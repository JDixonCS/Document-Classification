Gaussian Processes for Machine Learning
Matthias Seeger Department of EECS University of California at Berkeley 485 Soda Hall, Berkeley CA 94720-1776, USA mseeger@cs.berkeley.edu
February 24, 2004
Abstract Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated.
Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations [13, 78, 31]. The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.
1 Introduction and Overview: Gaussian Processes in a Nutshell
In this section, we introduce the basic reasoning behind non-parametric random field and Gaussian process models. Readers who have been exposed to these concepts may jump to the end of the section where an overview of the remaining sections is given. In most machine learning problems, we aim to generalise from a finite set of observed data, in the sense that our ability to predict uncertain aspects of a problem improves after making
Previously at: Institute for Adaptive and Neural Computation, University of Edinburgh, UK.
1

the observations. This is possible only if we postulate a priori a relationship between the variables we will observe and the ones we wish to predict. This relationship is uncertain itself, making generalisation a non-trivial problem. For example, in spatial statistics we observe the values of a function at certain locations and want to predict them at other ones. In temporal statistics, we might want to predict future values of a time series from its past. In the situations we are interested in here, the postulated relationship can be represented by an ensemble (or a distribution) of functions. It is helpful to imagine the observed data being "generated" by picking a function from the ensemble which gives rise to the sample (typically, observations themselves are imperfect or "noisy"). It is important to stress that this generative view can well be a crude abstraction of the mechanism we really hold capable of simulating the phenomenon, as long as its probabilistic inversion leads to satisfying predictions. This inversion is obtained by conditioning the generative ensemble on the observed data, which leads to a new adapted ensemble pinned down at observation points but still variable elsewhere.
In parametric statistics, we agree on a function class indexed by a finite number of parameters. A distribution over these parameters induces an ensemble over functions. Learning from observations means to modify this distribution so to adapt the ensemble to the data. If our a priori postulate is a very informed one (e.g. if the function class is motivated by a physical theory of the phenomenon), the parametric approach is the method of choice, but if many aspects of the phenomenon are unknown or hard to describe explicitly, nonparametric modelling can be more versatile and powerful. It is important to stress that our aim is solely to obtain accurate predictions together with valid estimates of uncertainty, not to "explain" the inner workings of the true generative process. In the latter case, nonparametric modelling is less applicable.
In non-parametric statistics, regularities of the relationship are postulated without requiring the ensemble to be concentrated on a easily describable class. For example, we may assume the ensemble to be stationary or isotropic (see Section 2), which allows us to infer properties of the generative ensemble even though our observations come from a single realisation thereof. We might also postulate smoothness so that nearby points (in space or time) have similar values with high probability, periodicity, boundary conditions, etc. In contrast to the parametric case, it is less clear how we can represent such a generative ensemble explicitly.
A random field is a mapping from an input space to real-valued random variables1, a natural generalisation of a joint distribution to an infinite index set. Like a joint distribution, we can try to describe the field by its low-order cumulants such as mean and covariance function, the latter being a bivariate form satisfying a positive semidefiniteness property akin to a covariance matrix of a joint distribution. If all cumulants above second order vanish, the random field is Gaussian: a Gaussian process. Importantly, properties such as stationarity, isotropy, smoothness, periodicity, etc. can be enforced via the choice of the covariance function. Furthermore, all finite-dimensional marginal distributions of the field are jointly Gaussian, and inference and prediction require little more than numerical linear algebra.
With this brief introduction, we hope to have motivated the reader to browse through the more detailed sections to follow. Section 2 defines Gaussian processes, introduces the important subclasses of stationary and isotropic GPs and develops two different views on GPs
1The extension to complex-valued random fields is straightforward. Since most machine learning applications require real-valued fields only, we concentrate on this case for simplicity.

prominent in machine learning. Some elementary GP models are introduced in Section 3. Approximate inference techniques for such models are discussed in Section 4 using a generic framework. Theoretical aspects of GPs can be understood by associating them with reproducing kernel Hilbert spaces (RKHS), as shown in Section 5. Traditionally, GP models have been used in the context of penalised maximum likelihood and spline smoothing which are motivated in Section 6. A variant of spline smoothing, the support vector machine has gained large popularity in the machine learning community, its relationship to Bayesian GP techniques is given in Section 7. GP models have been used extensively in spatial statistics, using an estimation procedure called kriging, as described in Section 8. The final Section 9 deals with the choice of the covariance function which is of central importance in GP modelling. We describe classes of standard kernels and their properties, show how kernels can be constructed from elementary parts, discuss methods for learning aspects of the kernel and finally illustrate classes of covariance functions over discrete index sets.
Readers more interested in practical machine learning aspects may want to skip over Sections 5 and 6 which contain more theoretical material not required to understand GP applications. We use notational conventions familiar to probability theorists which is introduced in Section A.1, but are careful to motivate the formalism in the more applied sections.
2 Gaussian Processes: The Process and the Weight Space
View
Gaussian process (GP) models are constructed from classical statistical models by replacing latent functions of parametric form (e.g. linear functions, truncated Fourier or Wavelet expansions, multi-layer perceptrons) by random processes with Gaussian prior. In this section, we will introduce GPs and highlight some aspects which are relevant to machine learning. We develop two simple views on GPs, pointing out similarities and key differences to distributions induced by parametric models. We follow [2], Chap. 1,2. A good introduction into the concepts required to study GP prediction is given in [74], Chap. 2. For concepts and vocabulary from general probability theory, we refer to [19, 6, 9].
Let X be an non-empty index set. For the main parts of this paper, X can be arbitrary, but here we assume that X is at least a group2 (and sometimes we assume it to be Rg). In a nutshell, a random process X  R is a collection of random variables (one for each x  X ) over a common probability space. The measure-theoretic definition is awkward, but basically the same as for a single variable. It can also be viewed as a function from the probability space and X into the reals. The functions X  R obtained for a fixed atomic event are called sample paths, and a random process can also be seen as the corresponding distribution over sample paths. If X  X is finite, we obtain a random variable  R|X| by evaluating the process at the points X, its distribution is called finite-dimensional distribution (f.d.d.). If we assume that a random process exists and consider the system of all f.d.d.'s, it is clear that it has to be symmetric and consistent: a permutation of the components of X must result in the distribution of an equally permuted random vector, and if X1  X2 = , the marginal distributions on the intersection starting from the ones for X1 and X2 must be identical. Formally, for every n  N>0, x1, . . . , xn  X , Borel sets B1, . . . , Bn and every
2Has an addition +, an origin 0 and a negation -.

permutation  of {1, . . . , n} we must have

µ (1),..., (n) (B(1) × · · · × B(n)) = µ 1,..., n (B1 × · · · × Bn) and
µ 1,..., n (B1 × · · · × Bn-1 × R) = µ 1,..., n-1 (B1 × · · · × Bn-1).
Importantly, Kolmogorov [28] proved that symmetry and consistency are also sufficient conditions for such a specification to guarantee the existence of a random process (in the concrete measure-theoretic sense) with these f.d.d.'s. The question about uniqueness of random processes is tricky, because two processes can be equivalent (u(x) = v(x) almost surely for every x; equivalent processes are called versions of each other), yet differ significantly w.r.t. almost sure properties of their sample paths. For example, one can construct a version of a smooth process whose sample paths are not differentiable at a finite number of points almost surely. In the context of machine learning applications we are interested here, sample path properties such as differentiability are of lesser importance, and we will focus on m.s. properties (to be introduced shortly) which can be characterised more directly and are invariant under change of version. In other words, we will in general identify a process with the equivalence class of all its versions or with a particularly "nice" member of this class,3 and the simple nature of the applications we are interested in here guarantees the admissability of this practice. We will see that global sample path properties of a process (in this sense) such as smoothness and average variability are directly related to corresponding m.s. properties. See Adler [2] for methods of studying sample path properties.
Let {Xn} be a sequence of real-valued random variables, and recall that Xn  X (n  ) in quadratic mean (or in mean square (m.s.)) if E[|Xn-X|2]  0. M.s. convergence is weaker than almost sure (a.s.) convergence, but turns out to be the most useful mode for discussing GP aspects we require here. In general, X and Y are m.s. equivalent if E[|X - Y |2] = 0. In a nutshell, for a property which is traditionally defined in terms of limits (such as continuity, differentiability, etc.) within R we can typically define the corresponding m.s. property for scalar random variables by substituting normal for m.s. convergence.
Suppose that u(x) is a random process. The first and second-order statistics of u(x) are its mean function m(x) = E[u(x)] and covariance function

K(x, x ) = E (u(x) - m(x))(u(x ) - m(x )) .

Obviously, both depend on the f.d.d.'s of the process only. The covariance function is cen-
tral to studying characteristics of the process in the mean square sense. It is a positive semidefinite4 function in the sense that for every n  N, x1, . . . , xn  X , z1, . . . , zn  R:

n

zizjK(xi, xj)  0.

(1)

i,j=1

This is clear because for X = i zi(u(xi) - m(xi)) we have E[|X|2]  0. Positive semidefiniteness means that for every finite set X  X the symmetric matrix K(X, X)  R|X|,|X| obtained by evaluating K on X × X is positive semidefinite. Note that this implies that K(x, x)  0 for all x. K is called positive definite if (1) holds with > whenever z = 0.

3As an example, a Wiener process (see Section 2.3) always has a version with continuous sample paths. 4This term is not uniquely used in the literature, it is sometimes replaced by non-negative definite or
even positive definite (which has a different meaning here).

The positive semidefiniteness of K leads to an important spectral decomposition which is discussed in Section 5. A positive semidefinite K will also be referred to as kernel, pointing out its role as kernel for a linear integral operator (see Section 5).

2.1 Stationary Processes

In many situations, the behaviour of the process does not depend on the location of the observer, and under this restriction a rich theory can be developed, linking local m.s. properties of the process to the behaviour of K close to the origin. A process is called strictly homogeneous (or strictly stationary) if its f.d.d.'s are invariant under simultaneous translation of their variables. This implies that m(x) is constant and K(x, x ) is a function of x - x (we write K(x, x ) = K(x - x ) in this case). A process fulfilling the latter two conditions is called (weakly) homogeneous (or (weakly) stationary). For a stationary process, the choice of the origin is not reflected in the statistics up to second order. If K(0) > 0,

(x)

=

K (x ) K (0)

is called correlation function. A stationary process has a spectral representation as a stochas-
tic Fourier integral (e.g., [2], Chap. 2; [19], Chap. 9; [88]), based on Bochner's theorem which (for X = Rg) asserts that (x) is positive semidefinite, furthermore uniformly continuous with (0) = 1, |(x)|  1 iff it is the characteristic function of a variable , i.e.

(x) = ei T dF ()

(2)

for a probability distribution function F (). If F () has a density f () (w.r.t. Lebesgue measure), f is called spectral density. This theorem allows to prove positive semidefiniteness of K by computing its Fourier transform and checking that it is non-negative. If so, it must be proportional to the spectral density. Note that since (x) is an even function, the spectral distribution is symmetric around 0, and if f () exists it is even as well.
The f.d.d.'s of a process determine its mean square properties, while this is not true in general for almost sure properties (such as continuity or differentiability of sample paths). Even stronger, for a zero-mean process, m.s. properties are usually determined entirely by the covariance function K(x, x ). For stationary processes, it is merely the behaviour of K(x) at the origin which counts: the m.s. derivative5 D u(x) exists everywhere iff D D K(x) exists at x = 0. Thus, the smoothness of the process in the m.s. sense grows with the degree of differentiability at 0. For example, a process with the RBF (Gaussian) covariance function K (27) is m.s. analytic, because K is analytic (differentiable up to any order) at 0.

2.2 Isotropic Processes

A stationary process is called isotropic if its covariance function K(x) depends on x only. In this case, the spectral distribution F is invariant under isotropic isomorphisms (e.g., rotations). Loosely speaking, second-order characteristics of an isotropic process are

5Here, D ¡

denotes a differential functional, such as 2/(x1x2).

the same from whatever position and direction they are observed. It is much simpler to characterise isotropic correlation functions than stationary ones in general. Let ( ) = (x) for  = x . The spectral decomposition (2) simplifies to

( ) = g/2-1( )dF ()

(3)

where F () = I{ } dF ( ) is a distribution function for   0 and

 (z)

=

( + 1) (z/2)

J

(z

),

where J (z) is a Bessel function of the first kind (see [2], Sect. 2.5). Recall that g is the dimensionality of the input space X = Rg. The right hand side in (3) is the Hankel transform

of order g/2 - 1 of F (see [74], Sect. 2.10). Alternatively, if the spectral density f () exists

and f () = f () for  =  , then dF () = Ag-1g-1f () d,6 so we can easily convert

to the spectral representation in terms of f (). Denote the set of ( ) corresponding to

isotropic correlation functions in Rg by Dg. Note that (3) characterises Dg (by Bochner's

theorem). It is clear that Dg+1  Dg, since an isotropic correlation function in Rg+1 re-

stricted to a g-dimensional subspace is in Dg. Beware that both F () and f () depend

on the dimension g for which ( ) is used to induce a correlation function (see (3)). Let

D = g1 Dg. Since

g/2-1 (2g)1/2x  e-x2 (g  ),

one can show that ( )  D iff ( ) = exp(- 22) dF () (this result is due to Schoenberg).

Note that the assumption of isotropy puts strong constraints on the correlation function,
especially for large g. For example, ( )  inf x g/2-1(x)  -1/g so large negative correlations are ruled out. If ( )  D, it must be non-negative. Furthermore, for large g ( ) is smooth on (0, ) while it may have a jump at 0 (additive white noise). If ( )  Dg and B  Rg,g is nonsingular, then
 (x) = ( Bx )

is a correlation function as well, called anisotropic. Examples of (an)isotropic covariance functions are given in Section 9.

2.3 Two Views on Gaussian Processes
A Gaussian process (GP) is a process whose f.d.d.'s are Gaussian. Since a Gaussian is determined by its first and second-order cumulants and these involve pairwise interactions only, its f.d.d.'s are completely determined by mean and covariance function. This means that for GPs, strong and weak stationarity are the same concept. GPs are by far the most accessible and well-understood processes (on uncountable index sets). It is clear that for every positive semidefinite function K there exists a zero-mean GP with K as covariance function (by Kolmogorov's theorem), so GPs as modelling tool are very flexible. Importantly, by choosing K properly we can encode properties of the function distribution implicitly as we desired in Section 1.
6Ag-1 = 2g/2/(g/2) is the surface area of the unit sphere in ¡ g .

In conjunction with latent variable modelling techniques, a wide variety of non-parametric models can be constructed (see Section 3). The fact that all f.d.d.'s are Gaussian with covariance matrices induced by K(x, x ) can be used to obtain approximations to Bayesian inference fairly straightforwardly (see Section 4), and these approximations often turn out to be much more accurate than for parametric models of equal flexibility (such as multilayer perceptrons). It is interesting to note that m.s. derivatives D u(x) of a GP are GPs again (if they exist), and
E D(1)u(x)D(2)u(x ) = D(1)D(2)K(x, x ),
thus derivative observations can be incorporated into a model in the same way as function value observations (for applications, see [46, 71]). Characteristics such as m.s. differentiability up to a given order can be controlled via the covariance function (see Section 2.1), an example is given in Section 9.
One of the most thoroughly studied GPs is the Wiener process (or Brownian motion, or continuous random walk) with covariance function K(x, x ) = 2 min{x, x } (here, X = R0; for multivariate generalisations to Brownian sheets, see [2], Chap. 8). It is characterised by u(0) = 0 a.s., E[|u(x + h) - u(x)|2] = 2h, h  0, and by having orthogonal7 increments: E[(u(x1) - u(x2))(u(x3) - u(x4))] = 0, x1  x2  x3  x4. Note that u(x) is not stationary, a stationary version with orthogonal increments is the Ornstein-Uhlenbeck process (see Section 9.1). The Wiener process is an example for a diffusion process. It has a large number of applications in mathematics, physics and mathematical finance. The property of orthogonal increments allows to define stochastic integrals (e.g., [19], Chap. 13) with a Wiener process as (random) measure. u(x) is m.s. continuous everywhere, but not m.s. differentiable at any point. In fact, a version of the Wiener process can be constructed which has continuous sample paths, but for every version sample paths are nowhere differentiable with probability 1. The Wiener process can be used to explicitly construct other GPs by means of stochastic integrals, the procedure is sketched in Section 6.
We now develop two elementary views on Gaussian processes, the process and the weight space view. While the former is usually much simpler to work with, the latter allows us to relate GP models to parametric linear models rather directly. We follow [85].8 The process view on a zero-mean GP u(x) with covariance function K is in the spirit of the GP definition given above. u(x) is defined implicitly, in that for any finite subset X  X it induces a f.d.d. N (0, K (X)) over the vector u = u(X) of process values at the points X. Here, K (X) = K (X, X) = (K(xi, xj))i,j where X = {x1, . . . , xn}. Kolmogorov's theorem guarantees the existence of a GP with this family of f.d.d.'s.9 In practice, many modelling problems involving an unknown functional relationship u(x) can be formulated such that only ever a finite number of linear characteristics of u(x) (e.g., evaluations or derivatives of u(x)) are linked to observations or predictive queries, and in such cases the process view boils down to dealing with the "projection" of the GP onto a multivariate Gaussian distribution, thus to simple linear algebra of quadratic forms.10
7Orthogonality implies independence since the process is Gaussian. 8We use the term "process view" instead of "function space view" employed in [85]. The relationship between GPs and associated spaces of smooth functions is a bit subtle and introduced only below in Section 5. 9If K is continuous everywhere, a version exists with continuous sample paths, but we do not require this here. 10In practice, some knowledge of numerical mathematics is required to avoid numerically instable proce-

GPs can also be seen from a weight space viewpoint, relating them to the linear model. In the Bayesian context this view was first suggested by O'Hagan [45] as a "localised regression model" (the weight space is finite-dimensional there) while the generalisation to arbitrary GP priors developed there uses the process view. This paper is among the first to address GP regression in a rigorous Bayesian context, while the equivalence between spline smoothing and Bayesian estimation of processes was noticed earlier by Kimeldorf and Wahba [27] (see Section 6). Recall the linear model

y = (x)T  + ,

(4)

where (x) is a feature map from the covariate x and  is independent Gaussian noise. Every GP whose covariance function satisfies weak constraints can be written as (4), albeit with possibly infinite-dimensional weight space. To develop this view, we use some facts which are discussed in detail below in Section 5. Under mild conditions on the covariance function K(x, x ) of u(x), we can construct a sequence

k
 1/2 (x),
=1
which converges to u(x) in quadratic mean (k  ).11 Here,  are i.i.d. N (0, 1) variables.  are orthonormal eigenfunctions of the operator induced by K with corresponding eigenvalues 1  2  · · ·  0, 1 2 < , in a sense made precise in Section 5. Thus, if  = ( ) and (x) = (1/2(x)) , then u(x) = (x)T  in quadratic mean, and (x)T (x ) = K(x, x ). This is the weight space view on GPs and allows to view a non-parametric regression model
y = u(x) + 
as direct infinite-dimensional generalisation of the linear model (4) with spherical Gaussian prior on . We say that (x) maps into a feature space which is typically (countably) infinite-dimensional. It is important to note that in this construction of the feature map (x) the individual components 1/2 (x) do not have the same scaling, in the sense that their norm in L2(µ) (the Hilbert space they are drawn from and that K operates on) is 1/2  0 (  ). They are comparable in a different (RKHS) norm which scales with the "roughness" of a function. Intuitively, as   , the graph of  becomes rougher and increasingly complicated, see Section 5 for details.
For all inference purposes which are concerned with f.d.d.'s of u(x) and its derivatives (or other linear functionals) only, the process and the weight space view are equivalent: they lead to identical results. However, we feel that often the process view is much simpler to work with, avoiding spurious infinities12 and relying on familiar Gaussian manipulations only. On the other hand, the weight space view is more frequently used at least in the machine learning literature, and its peculiarities may be a reason behind the perception that GP models are difficult to interpret. There is also the danger that false intuitions or conclusions
dures. Since most matrices to be dealt with are positive semidefinite, this is not too hard. Some reliable techniques are mentioned in Section 4.
11We only need pointwise m.s. convergence, although much stronger statements are possible under mild assumptions, e.g. [2], Sect. 3.3.
12Which seem to cancel out almost "magically" in the end from the weight space viewpoint, while infinities do not occur in the process view in the first place.

are developed from interpolating geometrical arguments from low-dimensional Euclidean space to the feature space.13 We should also note that a weight space representation of a
GP in terms of a feature map  is of course not unique. The route via eigenfunctions of the covariance operator is only one way to establish such.14 About the only invariant is that we always have (x)T (x ) = K(x, x ).

2.4 Gaussian Processes as Limit Priors of Parametric Models

We conclude this section by mentioning that one of the prime reasons for focusing current machine learning interest on GP models was a highly original different way of establishing a weight space view proposed in [42]. Consider a model

H
f (x) = vjh(x; u(j))
j=1

which could be a multi-layer perceptron (MLP) with hidden layer functions h, weights u(j) and output layer weights v. Suppose that u(j) have independent identical priors s.t. the resulting h(x; u(j)) are bounded almost surely over a compact region of interest. Also, vj  N (0, 2/H) independently. Then, for H  , f (x) converges in quadratic mean to a zero-mean GP with covariance function 2E [h(x; u)h(x ; u)]. Stronger conditions would
assure almost sure convergence uniformly over a compact region. The bottom line is that
if we take a conventional parametric model which linearly combines the outputs of a large
number of feature detectors, and if we scale the outputs s.t. each of them in isolation has
only a negligible contribution to the response, we might just as well use the corresponding
Gaussian process model. Neal [42] also shows that if a non-zero number of the non-Gaussian
feature outputs have a significant impact on the response with non-zero probability, then
the limit process is typically not Gaussian.

To conclude, the weight space view seems to relate non-parametric GP models with parametric linear models fairly directly. However, there are important differences in general. Neal showed that GPs are obtained as limit distributions of large linear combinations of features if each feature's contribution becomes negligible, while the output distributions of architectures which fit at least a few strong feature detectors are typically not Gaussian. Predictions from a GP model are smoothed versions of the data (in a sense made concrete in Section 6), i.e. interpolate by minimising general smoothness constraints encoded in the GP prior, as opposed to parametric models which predict by focusing on these functions (within the family) which are most consistent with the data. O'Hagan [45] discusses differences w.r.t. optimal design.

13Steinwart [75] gives the following example. For a universal covariance function (most kernels discussed

here have this property), any two finite disjoint subsets of X can be separated by a hyperplane in feature

space, and the distances of all points to the plane can be made to lie in an interval of arbitrarily small size.

Steinwart concludes that "any finite dimensional interpretation of the geometric situation in a feature space

of a universal kernel must fail". We strongly agree.

14For example, in Section 5 we discuss K's role as reproducing kernel, in the sense that K( , ) =

¡

¡

(K(·, ), K(·,

¡

¡

))K in some Hilbert space with inner product (·, ·)K . We could define  to map ¡

 K(·, ) ¡

and use the Hilbert space as weight space.

3 Some Gaussian Process Models

The simplest Gaussian process model is useful for regression estimation:

y = u + ,

where u = u(x) is a priori a zero-mean Gaussian process with covariance function K and  is independent N (0, 2) noise. Inference for this model is simple and analytically tractable,
because the observation process y(x) is zero-mean Gaussian with covariance K(x, x ) + 2 , .15 Given some i.i.d. data S = {(xi, yi) | i = 1, . . . , n}, let K = (K(xi, xj))i,j. Then, P (u) = N (0, K ) and

P (u|S) = N K (2I + K )-1y, 2(2I + K )-1K ,

(5)

where u = (u(xi))i. For some test point x distinct from the training points, u = u(x)  y | u, so that

P (u|x, S) = P (u|x, u)P (u|S) du = N u|k(x)T (2I + K )-1y, K(x, x) - k(x)T (2I + K )-1k(x) . Here, k(x) = (K(xi, x))i. We see that for this model, the posterior predictive process u(x) given S is Gaussian with mean function yT (2I + K )-1k(x) and covariance function
K(x, x ) - k(x)T (2I + K )-1k(x ).

Note that the mean function used for prediction is linear in the targets y for every fixed x. Furthermore, the posterior covariance function does not depend on the targets at all.
In practice, if only posterior mean predictions are required, the prediction vector  = (2I + K )-1y can be computed using a linear conjugate gradients solver which runs in O(n2) if the eigenvalue spectrum of K shows a fast decay. If predictive variances for many test points are required, the Cholesky decomposition16 2I +K = LLT should be computed, after which each variance computation requires a single back-substitution.

The pointwise predictive variance is never larger than the corresponding prior variance, but the shrinkage decreases with increasing noise level 2. The same result can be derived in the weight space view with u(x) = (x)T , applying the standard derivation of Bayesian
linear regression (e.g., [85]). Note that just as in parametric linear regression, the smoothed
prediction E[u|S] is a linear function of the observations y, as is the mean function of the
predictive process E[u(x)|S] (see also Section 8). Note also that if K(x, x )  0 as x - x
gets big, predictive mean and variance for points x far from all data tend to prior mean
0 and prior variance K(x, x). Second-level inference problems such as selecting values for hyperparameters (parameters of K and 2) or integrating them out are not analytically

15In the context of this model, it is interesting to note that if K is stationary and continuous everywhere

except

at

0,

it

is

the

sum

of

a

continuous

(stationary)

covariance

K

and

a

white

noise

covariance

 ¡

,¡

.

Furthermore, Scho¨nberg conjectured that if K is an isotropic bounded covariance function, it must be

¡ continuous except possibly at 0. 16A symmetric matrix is positive definite iff it has a (unique) Cholesky decomposition

T , where is

lower triangular with positive diagonal elements.

tractable and approximations have to be applied. Approximate model selection is discussed in Section 4.
We can generalise this model by allowing for an arbitrary "noise distribution" P (y|u), retaining the GP prior on u(x). The generative view is to sample the process u(·) from the prior, then yi  P (yi|u(xi)) independent from each other given u(·).17 The likelihood function factors as a product of univariate terms:

n

P (y|X , u(·)) = P (y|u) = P (yi|ui).

(6)

i=1

Since the likelihood depends on u(·) only via the finite set u, the predictive posterior process

can be written as

dP (u(·)|S)

=

P (u|S) P (u)

dP

(u(·)),

(7)

i.e. P (u(X)|S) = (P (u|S)/P (u))P (u(X)) for any finite X  X . The prior measure is "shifted" by multiplication with P (u|S)/P (u) depending on the process values u at the training points only. The predictive process is not Gaussian in general, but its mean and covariance function can be obtained from knowledge of the posterior mean and covariance matrix of P (u|S) as discussed in Section 4. For a test point x,

P (y|x, S) = E [P (y|u)]

where the expectation is over the predictive distribution of u = u(x). In this general model, first-level inference is not analytically tractable. In Section 4 a general approximate inference framework is discussed. Markov Chain Monte Carlo (MCMC) methods can be applied fairly straightforwardly, for example by Gibbs sampling from the latent variables u [43]. Such methods are attractive because the marginalisation over hyperparameters can be dealt with in the same framework. However, naive realisations may have a prohibitive running time due to the large number of correlated latent variables, and more advanced techniques can be difficult to handle in practice. While MCMC is maybe the most advanced and widely used class of approximate inference techniques, it is not discussed in any further detail here (see [41] for a review).

3.1 Generalised Linear Models. Binary Classification

A large class of models of this kind is obtained by starting from generalised linear models (GLMs) [44, 37] and replacing the parametric linear function xT  by a process u(x) with a GP prior. This can be seen as direct infinite-dimensional generalisation of GLMs by employing the weight space view (see Section 2). In the spline smoothing context, this framework is presented in detail in [18]. It employs noise distributions
P (y|u) = exp -1(y u - Z(u)) + c(y, ) ,

i.e. P (y|u) is in an exponential family with natural parameter u, sufficient statistics y/ and log partition function -1Z(u). Here,  > 0 is a scale hyperparameter. The linear model is

17This is generalised easily to allow for bounded linear functionals of the latent process u(·) instead of the

evaluation

functional

 ¡

i,

as

discussed

in

Section

5.

a special case with  = 2, u = µ = Eu[y] and Z(u) = (1/2)u2. A technically attractive feature of this framework is that log P (y|u) is strictly concave in u, leading to a strictly log-concave, unimodal posterior P (u|S). For binary classification and y  {-1, +1}, the GLM for the binomial noise distribution is logistic regression with the logit noise

P (y|u)

=

(y (u + b)),

(t)

=

1

1 + e-t

.

(8)

Here,  = 2 and Z(u) = 2 log cosh((u + b)/2). Another frequently used binary classification noise model is probit noise

P (y|u) = (y (u + b)) = E N(0,1) I{y(u+b)+ >0}

(9)

which can be seen as noisy Heaviside step and is not in the exponential family. Both noise models (8), (9) are strictly log-concave.

3.2 Models with C Latent Processes

We can also allow for a fixed number C  1 of latent variables for each case (x, y), i.e. C processes uc(x). The likelihood factors as

n
P (yi|u(i)),
i=1

u(i) = (uc(xi))c.

uc(x) is zero-mean Gaussian a priori with covariance function K (c). While it is theoretically possible to use cross-covariance functions for prior covariances between uc for different c, it may be hard to come up with a suitable class of such functions.18 Furthermore, the assumption that the processes uc are independent a priori leads to large computational savings, since the joint covariance matrix over the data assumes block-diagonal structure. Note that in this structure, we separate w.r.t. different c, while in block-diagonal structures coming from the factorised likelihood we separate w.r.t. cases i.

An important example using C latent processes is C-class classification. The likelihood
comes from a multinomial GLM (or multiple logistic regression). It is convenient to use a binary encoding for the class labels, i.e. y = c for class c  {1, . . . , C}.19 The noise is multinomial with

µ = E[y | u] = softmax(u) = 1T exp(u) -1 exp(u).

u  µ is sometimes called softmax mapping. Note that this mapping is not invertible, since we can add 1 to u for any  without changing µ. In other words, the parameterisation of the multinomial by u is overcomplete, due to the linear constraint yT 1 = 1 on y, and the corresponding GLM log partition function
Z(u) = log 1T exp(u)

is not strictly convex. The usual remedy is to constrain u by for example fixing uC = 0. This is fine in the context of fitting parameters by maximum likelihood, but may be problematic

18Hyperparameters may be shared between the prior processes, making them marginally dependent.

19We use vector notation for ,  C associated with a single case. This should not be confused with

¡

¡

the vector notation ,  n used above to group variables for all cases.

¡

¡

for Bayesian inference. As mentioned above, we typically use priors which are i.i.d. over the uc, so if we fix uC = 0, the induced prior on µ is not an exchangeable distribution (i.e. component permutations of u can have different distributions) and µC is singled out for no other than technical reasons. We think it is preferable in the Bayesian context to retain symmetry and accept that u  µ is not 1-to-1. Dealing with this non-identifiability during inference approximations is not too hard since softmax is invertible on any plane orthogonal to 1 and Z(u) is strictly convex on such. Anyway, this detail together with the two different blocking structures mentioned above renders implementations of approximate inference for the C-class model somewhat more involved than the binary case (see [86] for an example). Other examples for C-process models are ordinal regression ("ranking") models (see [37] for likelihood suggestions) or multivariate regression.
3.3 Robust Regression
GP regression with Gaussian noise can lead to poor results if the data is prone to outliers, due to the light tails of the noise distribution. A robust GP regression model can be obtained by using a heavy-tailed noise distribution P (y|u) such as a Laplace or even Student-t distribution. An interesting idea is to use the fact that the latter is obtained by starting with N (0,  -1) and to integrate out the precision  over a Gamma distribution (e.g., [42]). Thus, a robust model can be written as
y = u + ,   N (0,  -1),
where  is drawn i.i.d. from a Gamma distribution (whose parameters are hyperparameters). The posterior P (u|S,  ) conditioned on the precision values i is Gaussian and is computed in the same way as for the case i = -2 above.  can be sampled by MCMC, or may be chosen to maximise the posterior P ( |S). The marginal likelihood P (y| ) is Gaussian and can be computed easily. However, note that in the latter case the number of hyperparameters grows as n which might invalidate the usual justification of marginal likelihood maximisation (see Section 4).

4 Approximate Inference and Learning

We have seen in the previous section that the posterior process for a likelihood of the general form (6) can be written as "shifted" version (7) of the prior. About the only processes (in this context) which can be dealt with feasibly are Gaussian ones, and a general way of obtaining a GP approximation to the posterior process is to approximate P (u|S) by a Gaussian Q(u),20 leading to the process

dQ(u(·))

=

Q(u) P (u)

dP

(u(·))

(10)

which is Gaussian (recall from Section A.1 that this is a concise way of writing that Q(u(X)) = (Q(u)/P (u))P (u(X)) for every finite X  X ). An optimal way of choosing Q would be to minimise the relative entropy (Definition 1)

D[P (u(·)|S) Q(u(·))] = D[P (u|S) Q(u)].

(11)

20The conditioning on S in Q(·) is omitted for notational simplicity.

The equality is intuitively clear, since Q(u(·)), P (u(·)|S) and P (u(·)) are the same conditional on u. Formally, it follows from the fact that if dP (u(·)|S) dQ(u(·)), then

dP (u(·)|S)

=

P (u|S) Q(u)

dQ(u(·)),

and otherwise D[P (u|S) Q(u)] =  (recall our notation from Section A.1). At the minimum point (unique w.r.t. f.d.d.'s of Q) Q and P (·|S) have the same mean and covariance function. This is equivalent to moment matching and requires us to find mean and covariance matrix of P (u|S). Unfortunately, this is intractable in general for large datasets and non-Gaussian noise. Any other Gaussian approximation Q(u) leads to a GP posterior approximation Q(u(·)), and the intractable (11) can nevertheless be valuable as guideline.
Here, we are primarily interested in approximate inference methods for GP models which employ GP approximations (10) to posterior processes via

Q(u) = N (u | K , A).

(12)

Here, , A can depend on the data S, the covariance function K (often via the kernel matrix

K ) and on other hyperparameters. This class contains a variety of methods proposed in

the literature. Virtually all of these have a reduced O(n) parameterisation, since A has the

restricted form

A = K-1 + I ·,I DI I,· -1

(13)

with D  Rd,d diagonal with positive entries and I  {1, . . . , n}, |I| = d. For the methods
mentioned below in this section, d = n and I ·,I = I, but for sparse GP approximations (e.g., [13, 78, 31]) we have d n. In the latter case, \I = 0 and we use   Rd for simplicity, replacing  in (12) by I·,I.

From (10), the (approximate) predictive posterior distribution of u = u(x) at a test point x is determined easily as Q(u|x, S) = N (u|µ(x), 2(x)), where

µ(x) = kI (x)T ,

2(x) = K(x, x) - kI (x)T D1/2B-1D1/2kI (x),

(14)

B = I + D1/2KI D1/2.

Here, kI (x) = (K(xi, x))iI . More generally, the GP posterior approximation has mean function µ(x) and covariance function

K(x, x ) - kI (x)T D1/2B-1D1/2kI (x ).

The predictive distribution P (y|x, S) is obtained by averaging P (y|u) over N (u|µ(x), 2(x)). If this expectation is not analytically tractable, it can be done by Gaussian quadrature (e.g., [54], Sect. 4.5) if P (y|u) is smooth and does not grow faster than polynomial.
A simple and numerically stable way to determine the predictive variances is to compute the Cholesky decomposition B = LLT after which each variance requires one back-substitution with L. It is important to stress that while inference approximation in GP models often boils down to simple linear algebra, it is crucial in practice to choose representations and

procedures which are numerically stable. In the presence of positive definite matrices, techniques based on the Cholesky factorisation are known to be most stable.21 Furthermore, in our representation B is well-conditioned since all its eigenvalues are  1.
We will refer to  as prediction vector. More generally, as mentioned in Section 2, we can use derivative information or other bounded linear functionals of the latent process u(x) in the likelihood and/or for the variables to be predicted, using the fact that the corresponding finite set of scalar variables is multivariate Gaussian with prior covariance matrix derived from the covariance function K (as discussed in more detail in Section 5).
A generalisation to the multi-process models of Section 3 is also straightforward in principle. Here, u has dimension C n. Again A is restricted to the form (13), although D is merely block-diagonal with n (C × C) blocks on the diagonal. Moreover, if the processes are a priori independent, both K and K -1 consist of C (n × n) blocks on the diagonal. The general formulae for prediction (14) have to be modified for efficiency. The details are more involved and may depend on the concrete approximation method, C-process models are not discussed in further detail here.

4.1 Some Examples

A simple and efficient way of obtaining a Gaussian approximation Q(u|S) is via Laplace's method (also called saddle-point approximation), as proposed in [86] for binary classification with logit noise (8). To this end, we have to find the posterior mode u^ which can be done by a variant of Newton-Raphson (or Fisher scoring, see [37]). Each iteration consists of a weighted regression problem, i.e. requires the solution of an n × n positive definite linear system. This can be done approximately in O(n2) using a conjugate gradients solver. At the mode, we have

 = Y (-Y u^ ), D = (diag (-Y u^ ))(diag (Y u^ )),

(15)

where  is the logistic function (8) and Y = diag y. All n diagonal elements of D are positive. Recall that the Laplace approximation replaces the log posterior by a quadratic fitted to the local curvature at the mode u^ . For the logit noise the log posterior is strictly concave and dominated by the Gaussian prior far out, so in general a Gaussian approximation should be fairly accurate. On the other hand, the true posterior is significantly skewed, meaning that the mode can be quite distant from the mean (which would be optimal) and the covariance approximation via local curvature around the mode can be poor.
The expectation propagation (EP) algorithm [39] for GP models can significantly outperform the Laplace GP approximation in terms of prediction accuracy, but is also more costly.22 It is also somewhat harder to ensure numerical stability. On the other hand, EP is more general and can for example deal with discontinuous or non-differentiable log likelihoods. In fact, the special case of EP for Gaussian fields has been given earlier by Opper and Winther [48] under the name ADATAP, and EP can be seen as an iterative generalization of older Bayesian online learning techniques.
21Matrix inversion is often recommended in the GP machine learning literature. It is well known in numerical mathematics that inversion should be avoided whenever possible for reasons of stability, and in the context of our GP framework using a Cholesky decomposition is even more efficient.
22Partly due to its more complex iterative structure, but also because its elementary steps are smaller than for the Laplace technique and cannot be vectorised as efficiently.

A range of different variational approximations have been suggested in [16, 65, 24]. Note that for the variational method where Q(u|S) is chosen to minimise D[· P (u|S)], it is easy to see that the best Gaussian variational distribution has a covariance matrix of the form (13) (e.g., [64], Sect. 5.2.1).
Sparse approximations to GP inference are developed in [12, 13, 31]. While the original application was online learning, they are understood easier as "sparsifications" of EP (or ADATAP). While the approximations mentioned so far have training time scaling of O(n3), sparse inference approximations reduce this scaling to O(n d2) with adjustable d n. For many problems, sparse approximations attain sufficient accuracy in essentially linear time in n which allows the application in data-rich settings. The idea is to concentrate on a subset I  {1, . . . , n}, |I| = d of the training data which we call the active set, then to approximate the true likelihood P (y|u) of the model by a likelihood approximation Q(uI) which is a function of the components uI only. With this replacement, inference becomes linear in n (as can be seen from the formulae in this section which allow the use of an active set). The challenge is how to choose I and the form for Q(uI) in a way to best approximate the moments of the true posterior P (u|y), while staying within the resource limitations of O(n d2) time and O(n d) memory.23 Also, if P (y|u) is not Gaussian, the sparse technique has to be embedded in an inference approximation of the kind discussed in this section. Details on some sparse schemes can be found in [78, 13, 31], some generic schemes based on the EP algorithm and information-theoretic selection heuristics for I are described in [63]. Free Matlab software has been released by Lehel Csato´.24
4.2 Model Selection
So far we have only been concerned with first-level inference conditioned on fixed hyperparameters. A useful general method has to provide some means to select good values for these parameters or to marginalise over them (see Section 3). The latter is the correct way to proceed in a strict Bayesian sense and can be approximated by MCMC techniques, but often model selection is computationally more attractive. A frequently used general empirical Bayesian method for marginalising over nuisance hyperparameters is marginal likelihood maximisation or maximum likelihood II (also called evidence maximisation). This technique can be applied to the generic GP approximation described in this section, leading to a powerful generic way of adjusting hyperparameters via nonlinear optimization which scales linearly in the number of parameters. It is important to point out that such automatic model selection techniques are a strong advantage of Bayesian GP methods over other kernel machines such as SVMs (see Section 7) for which we do not know of selection strategies of similar power and generality.
If we denote the hyperparameters by , the marginal likelihood is P (S|) = P (y|), where the latent "primary" parameters u have been integrated out. If S is sufficiently large and  of rather small fixed dimension, the hyperposterior P (|S) frequently is highly concentrated around a mode ^ . Instead of using P (|S) to marginalise over , we replace the posterior by  ^ (), thus simply plug in ^ for . This is an example of a maximum a pos-
23Choosing I completely at random is possible, but performs poorly in situations such as classification where the influence of patterns on the posterior can be very different.
24See http://www.kyb.tuebingen.mpg.de/bs/people/csatol/ogp/index.html.

teriori (MAP) approximation.25 Finding ^ basically amounts to maximising the marginal likelihood, because the hyperprior P () is of a simple form. Conditions under which the hyperposterior is sufficiently peaked are hard to come by in general and will usually be overrestrictive for realistic models.26 Thus, while marginal likelihood maximisation does not solve the model selection problem in general, it has been shown to work well in many empirical studies featuring very different models, and its description as "plug-in" approximation to Bayesian marginalisation may lead to successful extensions in cases where the simple method fails.
Some readers might worry at this point that we propose to select  by maximising the likelihood P (y|), and maximum likelihood techniques are prone to overfitting. The key difference is that in the marginal likelihood, the primary "parameter" u(·) has been integrated out. While choosing primary parameters so as to maximise the likelihood often leads to overcomplicated fits that generalise badly, this is not true in general for marginal likelihood maximisation. A simple argument (yet not a proof) is that a value (1) leading to very complicated u(·) needs to assign mass P (u(·)|) to many more functions than a value (2) leading to simple u(·) (e.g. linear or low-order polynomial), so even if the likelihood of y is much higher for some of the complicated u(·), in the process of marginalisation the complicated functions are downweighted stronger in the integral for P (y|(1)) than are the simpler functions in the integral for P (y|(2)). This "Occam razor" effect has been analysed by MacKay [33]. However it is obviously possible to create situations in which marginal likelihood maximisation still leads to overfitting.27 As a general rule of thumb, the dimensionality of the hyperparameters  should not scale with n,28 and the Occam razor argument just given should intuitively apply to the situation (once more, we do not know of a definite test separating admissable from non-admissable cases in general).
We will focus on marginal likelihood maximisation as general model selection technique. The log marginal likelihood log P (y|) is as difficult to compute as the posterior P (u|S, ) and has to be approximated in general.29 It is easy to see that the variational lower bound

log P (y|)  EQ [log P (y|u, ) + log P (u|)] + H[Q(u)]

(16)

= EQ [log P (y|u, )] - D[Q(u) P (u|)].

holds for any distribution Q(u) (recall relative and differential entropy from Section A.2). The slack in the bound is the relative entropy D[Q(u) P (u|S, )]. Note that the posterior approximation Q(u) depends on  as well, but it is not feasible in general to obtain its exact gradient w.r.t. . Variational EM, an important special case of a lower bound maximisation algorithm is iterative, in turn freezing one of Q,  and maximising the lower bound w.r.t. the other (here, Q can be chosen from a family of variational distributions). Alternatively,

25Multimodality in the hyperposterior can arise from non-identifiability of the model though symmetries in

, i.e. there exist different

(1) ,

(2) s.t. P ( ¡

|{ ¡

i },

(1))  P ( ¡

|{ ¡

i },

(2)) for datasets of interest. In this

case, we can just pick any of the dominant modes ^ in the hyperposterior to arrive at the same predictions

as if we had chosen a peak train featuring all equivalent modes.

26Since we integrate out a variable of the same dimension of the training sample and the latter is

independent only conditional on the process u(·) (which is not in general a finite-dimensional variable), we

cannot use the central limit theorem directly to assert Gaussianity of P ( | ) as n gets large. ¡ 27For example, one could maliciously set = u(·).

28Although in special situations the technique may still be applicable, see [78] or Section 3.3.

29It is analytically tractable for a Gaussian likelihood, for example in the case of GP regression with

Gaussian noise discussed above it is log N ( |0, ¡

+

2 ¢

).

¡

Q can be chosen in a different way as approximation of the posterior P (u|S) (for example using the EP algorithm or sparse approximations). The deviation from the variational choice of Q (i.e. the one which maximises the lower bound over a family of candidates) can be criticised on the ground that other choices of Q can lead to decreases in the lower bound, so the overall algorithm does not increase its criterion strictly monotonically. On the other hand, Q chosen in a different way may lie outside families over which the lower bound can be maximised efficiently, thus may even result in a larger value than the variational maximiser within the family.30 Furthermore, the lower bound criterion can be motivated by the fact that its gradient
EQ( ) [ log P (y, u|)]
(ignoring the dependence of Q on ) approximates the true gradient
 log P (y|) = EP ( |S) [ log P (y, u|)]
at every point .
We close by mentioning an interesting point in which lower bound maximisation for GP models might deviate from the usual practice with parametric architectures. For the latter, it is customary to maximise the lower bound w.r.t.  while keeping Q completely fixed (the gradient of Q w.r.t.  is ignored). This makes sense as long as Q is independent of the prior distribution in the model, but in the context of approximate GP inference methods, the dependence of Q(u) on the GP prior (thus on ) is quite explicit (for example, the covariance of Q is (K-1 + D)-1 which depends strongly on the kernel matrix K , since D is merely a diagonal matrix). We argue that instead of keeping all of Q fixed during the maximisation for , we should merely ignore the dependence of the essential parameters , D on .31 This typically leads to a more involved gradient computation which is potentially closer to the true gradient. Alternatively, if this computation is beyond resource limits, further indirect dependencies on  may be ignored. We remark that the optimisation problem is slightly non-standard due to the lack of strict monotonicity, and given optimisers have to be modified to take this into account. Details can be found in [63], Sect. 4.5.3.
5 Reproducing Kernel Hilbert Spaces
The theory of reproducing kernel Hilbert spaces (RKHS) can be used to characterise the space of random variables obtained as bounded linear functionals of a GP on which any method of prediction from finite information must be based. Apart from that, RKHS provide a unification of ideas from a wide area of mathematics, most of which will not be mentioned here. The interested reader may consult [3]. Our exposition is taken from [80]. This section can be skipped by readers interested primarily in practical applications.
A reproducing kernel Hilbert space (RKHS) H is a Hilbert space of functions X  R for which all evaluation functionals  are bounded. This implies that there exists a kernel
30For example, even though the bound maximiser over all Gaussians has a covariance matrix of the form (13), finding it is prohibitively costly in practice and proposed variational schemes [16, 65, 24] use restricted subfamilies.
31There is no simple analytic formula for this dependence, so we cannot do better than ignoring it.

K(x, x ) s.t. K(·, x)  H for all x  X and

f (x) =  f = (K(·, x), f )

(17)

for all f  H, where (·, ·) is the inner product in H. To be specific, a Hilbert space is a vector

space with an inner product which is complete, in the sense that each Cauchy sequence converges to an element of the space. For example, a Hilbert space H can be generated from an inner product space of functions X  R by adjoining the limits of all Cauchy sequences to H. Note that this is a rather abstract operation and the adjoined objects need not be functions in the usual sense. For example, L2(µ) is obtained by completing the vector space of functions for which

f (x)2 dµ(x) < 

(18)

and can be shown to contain "functions" which are not defined pointwise.32 For an RKHS H such anomalies cannot occur, since the functionals  are bounded:33

|f (x)| = | f |  C f .

By the Riesz representation theorem (e.g., [20]) there exists a unique representer K  H such that (17) holds with K(·, x) = K . It is easy to see that the kernel K is positive semidefinite. K is called reproducing kernel (RK) of H, note that

K , K = K(·, x), K(·, x ) = K(x, x ).

It is important to note that in a RKHS, (norm) convergence implies pointwise convergence to a pointwise defined function, since

|fm(x) - f (x)| = |(K , fm - f )|  C fm - f .

On the other hand, for any positive semidefinite K there exists a unique RKHS H with RK K. Namely, the set of finite linear combinations of K(·, xi), xi  X with





 aiK(·, xi), bjK(·, xj) = aibjK(xi, xj)

i

j

i,j

is an inner product space which is extended to a Hilbert space H by adjoining all limits of Cauchy sequences. Since norm convergence implies pointwise convergence in the inner product space, all adjoined limits are pointwise defined functions and H is an RKHS with RK K. To conclude, a RKHS has properties which make it much "nicer" to work with than a general Hilbert space. All functions are pointwise defined, and the representer of the evaluation functional  is explicitly given by K(·, x).
32The existence of such functions in L2(µ) means that expressions such as (18) have to be interpreted with some care. Each element f  L2(µ) can be defined as the set of all equivalent Cauchy sequences which define f (two Cauchy sequences are equivalent if the sequence obtained by interleaving them is Cauchy as well). An expression E(f, g) should then be understood as the limit limn E(fn, gn) where fn  f, gn  g, etc. The existence of the limit has to be established independently. In the sequel, we will always use this convention.
33Bounded functionals are also called continuous.

5.1 RKHS by Mercer Eigendecomposition. Karhunen-Loeve Expansion
We have already mentioned that L2(µ) is not a RKHS in general, but for many kernels K it contains a (unique) RKHS as subspace. Recall that L2(µ) contains all functions f : X  R for which (18) holds. The standard inner product is

(f, g) = f (x)g(x) dµ(x).

Often, µ is taken as indicator function of a compact set such as the unit hypercube. A positive semidefinite K(x, x ) can be regarded as kernel (or representer) of a positive semidefinite linear operator K in the sense
(Kf )(x) = (K(·, x), f ).
 is an eigenfunction of K with eigenvalue  = 0 if
(K)(x) = (K(·, x), ) =  (x).
For K, all eigenvalues are real and non-negative. Furthermore, suppose K is continuous and
K(x, x )2 dµ(x)dµ(x ) < .

Then, by the Mercer-Hilbert-Schmidt theorems there exists a countable orthonormal sequence of continuous eigenfunctions   L2(µ) with eigenvalues 1  2  · · ·  0, and K can be expanded in terms of these:

K(x, x ) =  (x) (x ),

(19)

1

and 1 2 < , thus   0(  ). This can be seen as generalisation of the eigendecomposition of a positive semidefinite Hermitian matrix. Indeed, the reproducing
property of positive semidefinite kernels was recognised and used by E. H. Moore [40] to
develop the notion of general "positive Hermitian matrices". In this case, we can characterise the RKHS embedded in L2(µ) explicitly. For f  L2(µ), define the Fourier coefficients

f = (f, ).

Consider the subspace HK of all f  L2(µ) with 1 - 1f2 < . Then, HK is a Hilbert

space with inner product

(f, g)K

=

1

f g 

,

moreover the Fourier series 1 f converges pointwise to f .34 Since { (x)} are the Fourier coefficients of K(·, x) (using Equation 19), we have

(f, K(·, x))K = f (x) = f (x),
1
34In particular, f is defined pointwise.

thus K is the RK of HK. It is important to distinguish clearly between the inner products (·, ·) in L2(µ) and (·, ·)K in HK (see [89] for more details about the relationship of these inner products). While · measures "expected squared distance" from 0 (w.r.t. dµ), · K is a measure of the "roughness" of a function. For example, the eigenfunctions have  = 1, but  K = -1/2 thus becoming increasingly rough.35
The spectral decomposition of K leads to an important representation of a zero-mean GP u(x) with covariance function K: the Karhunen-Loeve expansion. Namely, the sequence

k

uk(x) = u (x),

(20)

=1

where u are independent N (0,  ) variables, converges to u(x) in quadratic mean (a stronger statement under additional conditions can be found in [2]). Moreover,

u = u(x) (x) dµ(x)
which is well defined in quadratic mean. We have already used this expansion in Section 2 to introduce the "weight space view". Note that since the variances  decay to 0, the GP can be approximated by finite partial sums of the expansion (see [89]).

5.2 Duality between RKHS and Gaussian Process

If u(x) is a zero-mean GP with covariance function K, what is the exact relationship between
u(x) and the RKHS with RK K? One might think that u(x) can be seen as distribution over HK, but this is wrong (as pointed out in [80], Sect. 1.1). In fact, for any version of u(x) sample functions from the process are not in HK with probability 1! This can be seen by noting that for the partial sums (20) we have

E

uk

2 K

=E

k u2 =1 

= k   (k  ).

Roughly speaking, HK contains "smooth", non-erratic functions from L2(µ), characteristics we cannot expect from sample paths of a random process. A better intuition about HK is that it will turn out to contain expected values of u(x) conditioned on a finite amount of
information, thus the posterior mean functions we are interested in.

The following duality between HK and a Hilbert space based on u(x) was noticed in [27] and is important in the context of theoretical analyses. Namely, construct a Hilbert space
HGP in the same way as above starting from positive semidefinite K, but replace K(·, xi) by u(xi) and use the inner product

(A, B)GP = E[AB],

thus





 aiu(xi), bju(xj) = aibjK(xi, xj).

i

j

GP

i,j

35In the same sense as high-frequency components in the usual Fourier transform.

HGP is a space of random variables, not functions, but it is isometrically isomorphic to HK under the mapping u(xi)  K(·, xi), with
(u(x), u(x ))GP = E[u(x)u(x )] = K(x, x ) = (K(·, x), K(·, x ))K .
For most purposes, we can regard HGP as RKHS with RK K. The space HGP is important in the context of inference on GP models we are interested in, because it contains exactly the random variables we condition on or would like to predict in situations where only a finite amount of information is available (from observations which are linear functionals of the process).
If L is a bounded linear functional on HK, it has a representer   HK with (x) = LK . The isometry maps  to a random variable Z  HGP which we formally denote by Lu(·). Note that
E [(Lu(·))u(x)] = (, K )K = (x) = LK .
More generally, if L(1), L(2) are functionals with representers  (1), (2) s.t. x  L(j)K are in HK, then
E (L(1)u(·))(L(2)u(·)) = ((1), (2))K = L(1)(K(·, x), (2))K = L(1)L(2)K(x, y).
Again, it is clear that Lu(·) is (in general) very different from the process obtained by applying L to sample paths of u(x). In fact, since the latter are almost surely not in HK, L does not even apply to them in general. The correct interpretation is in quadratic mean, using the isometry between HGP and HK. As an example, suppose that X = Rg and L = D is a differential functional evaluated at x. Then, we retrieve the observations in Section 2 about derivatives of a GP.
6 Penalised Likelihood. Spline Smoothing
The GP models we are interested in here have their origin in spline smoothing techniques and penalised likelihood estimation, and for low-dimensional input spaces spline kernels are widely used due to the favourable approximation properties of splines and computational advantages. A comprehensive account of spline smoothing and relations to Bayesian estimation in GP models is [80] which our exposition is mainly based on. Spline smoothing is a special case of penalised likelihood methods, giving another view on the reproducing kernel via the Green's function of a penalisation (or regularisation) operator which will be introduced below. This section can be skipped by readers interested primarily in practical applications.
In Section 5 we have discussed the duality between a Gaussian process and the RKHS of its covariance function. Apart from the Bayesian viewpoint using GP models, a different and more direct approach to estimation in non-parametric models is the penalised likelihood approach, the oldest and most widely used incarnations of which are spline smoothing methods. We will introduce the basic ideas for the one-dimensional model which leads to the general notion of regularisation operators, penalty functionals and their connections to RKHS. We omit all details, (important) computational issues and multidimensional generalisations, see [80] for details. A more elementary account is [18].

We will only sketch the ideas, for rigorous details see [80, 27]. Interpolation and smoothing

by splines originates from the work of Scho¨nberg [61]. A natural spline s(x) of order m on

[0, 1] is defined based on knots 0 < x1 < · · · < xn < 1. If k denotes the set of polynomials of order  k, then s(x)  2m-1 on [xi, xi+1], s(x)  m-1 on [0, x1] and on [xn, 1], and s  C2m-2 overall. Natural cubic splines are obtained for m = 2. Define the roughness

penalty

Jm(f ) =

1

f (m)(x)

2
dx.

0

Jm(f ) penalises large derivatives of order m by a large value, for example J2 is large for

functions of large curvature. Then, for some fixed function values the interpolant minimising

Jm(f ) over all f for which the latter is defined is a spline of order m. More precisely, f  Wm[0, 1], a so-called Sobolev space of all f  C m-1[0, 1] s.t. f (m-1) is absolutely

continuous on [0, 1]. If we consider the related smoothing problem of minimising the penalised

empirical risk

n

(yi - f (xi))2 + Jm(f ), f  Wm[0, 1],

(21)

i=1

it is clear that the minimiser is again a natural spline s(x) of order m (any other f  Wm[0, 1] can be replaced by the spline with the same values at the knots, this does not change the risk term and cannot increase Jm). Now, from Taylor's theorem:

f (x)

=

m-1 =0

x !

f ()(0)

+

1
Gm(x, t)f (m)(t) dt
0

with Gm(x, t) = (x - t)+m-1/(m - 1)! (here, u+ = uI{u0}). If f ()(0) = 0,  = 0, . . . , m - 1 then (Gm(x, ·), Dmf ) = f (x), thus Gm(x, t) is the Green's function for the boundary value problem Dmf = g. These functions f form a Hilbert space with inner product

1
(f, g)K = f (m)(t)g(m)(t) dt
0

which is a RKHS with RK

1

K(x, x ) = Gm(x, t)Gm(x , t) dt.

(22)

0

It is interesting to note that a zero-mean GP with covariance function K can be obtained as
(m - 1)-fold integrated Wiener process (introduced in Section 2.3). Let W (x) be a Wiener process on [0, 1] with W (0) = 0 a.s. and E[W (1)2] = 1 (its covariance function is min{x, x }). It is possible to define a stochastic integral against a process with independent increments.36
The process u(x) defined via the stochastic integral

u(x) = Gm(x, t) dW (t)
36See [19], Sect. 9.4 for an easy derivation. It is important to note that the stochastic integral is not the random variable arising from integrating over sample paths of the process, the latter integrals do not exist in many cases in which the stochastic integral can be constructed.

is a zero-mean GP with covariance function K. If W is chosen s.t. its sample paths are continuous, u(x) is in Wm[0, 1] and u()(0) = 0 for  < m. Since dGm/dx = Gm-1 and G1(x, t) = I{x>t}, u(m-1) and W are m.s. equivalent. Note that u(x) can be written as

x

x

x

u(x) = dW (t) dx1 . . .

dxm-1

0

t

xm-2

for m > 1.

The boundary values can be satisfied by taking the direct sum of the space with m-1. The latter is trivially an RKHS w.r.t. an inner product of choice: choose an orthonormal basis and define the kernel to be the sum of outer products of the basis functions. The kernel for the direct sum is the sum of K and the finite-dimensional kernel. Note that · K is only a seminorm on the full space because p K = 0 for p  m-1.

We only sketch the general case, see [80, 53] for details. We make use of the following duality between a RKHS and a regularisation (pseudodifferential) operator P on L2(µ). Let H be the Hilbert space of f s.t. Pf  L2(µ). For P, consider the operator37 PP. If this has a null space (such as m-1 in the example above), we restrict H to the orthogonal complement.
Now, the operator is positive definite and has an inverse (its Green's function) whose kernel K is the RK of H.38 The inner product is

(f, g)K = (Pf, Pg)

and the penalty functional is simply the squared RKHS norm. If G(t, u) exists s.t. (G(t, ·), Pf ) = f (t) for all f  H, the RK is given by

K(s, t) = (G(s, ·), G(t, ·)).

On the other hand, we can start from an RKHS with RK K and derive the corresponding regularisation operator P. This can give additional insight into the meaning of a covariance function (see [53, 70]). In fact, if K is stationary and continuous, we can use Bochner's theorem (2). Namely, if f () is the spectral density of K, we can take f ()-1/2 as spectrum of P.39 The one-dimensional example above is readily generalised to splines on the unit sphere or to thin plate splines in X = Rg, but the details get quite involved (see [80], Chap. 2).
Kimeldorf and Wahba [27] generalised this setup to a general variational problem in an RKHS, allowing for general bounded linear functionals Lif instead of f (xi) in (21). The minimiser is determined by n+M coefficients, where M is the dimension of the null space of the differential operator P associated with K (M = m + 1 in the spline case above). These can be computed by direct formulae given in [80], Sect. 1.3. In the more general penalised likelihood approach [80, 18], n function values or linear functionals of f are used as latent variables in a likelihood (see Section 3), to obtain for example non-parametric extensions of GLMs [18]. The penalised likelihood is obtained by adding the penalty functional to the likelihood, and just as above the minimiser is determined by n + M coefficients only (this representer theorem can be proved using the same argument as in the spline case above). In general, iterative methods are required to find values for these coefficients.
37P is the adjoint of P, i.e. (f, Pg) = (Pf, g). 38This construction via Green's functions is different from the one above involving Gm(x, t). Without going into details, it may help to consider the analogue of the finite-dimensional case (vectors and matrices
¡¢¡ ¡ instead of functions and operators): ¡ = ( T )-1 = T where = -1. £ 39P is not uniquely defined, but only PP (which has spectrum f ( )-1).

6.1 Bayesian View on Spline Smoothing

We close this section by reviewing the equivalence between spline smoothing and Bayesian

estimation for a GP model pointed out by Kimeldorf and Wahba [27]. Given a positive

semidefinite kernel K corresponding to a pseudodifferential operator with M -dimensional

null space, we can construct an RKHS H as follows. If H0 is the null space represented by

an orthonormal basis p and H1 the RKHS for K, let H be their direct sum. Consider the

model

M
F (x) = p(x) + b1/2u(x), yi = F (xi) + i,

=1

where u(x) is a zero-mean GP with covariance function K and i are independent N (0, 2). Furthermore,   N (0, aI) a priori. On the other hand, let f be the minimiser in H of the regularised risk functional

1 n

n
(yi - f (xi))2 +  P1f

2 H1

,

i=1

where P1 is the orthogonal projection onto H1. Kimeldorf and Wahba [27] show that f lies

in the span of {p |  = 1, . . . , M }  {K(·, xi) | i = 1, . . . , n} and give a numerical procedure for computing the coefficients. If we define F^a(x) = E[F (x) | y1, . . . , yn], then they show

that

lim
a

F^a(x)

=

f(x),



=

2 nb

for every fixed x. The proof (see [80], Chap. 1) is a straightforward application of the duality between the RKHS H1 and the Hilbert space based on u(x), as described in Section 5. The procedure of dealing with H0 and the improper prior on  is awkward but is not necessary if the RKHS H1 induced by K is rich enough.40

Finally, we note that a parametric extension of a non-parametric GP model can be sensible even if H1 is rich enough in principle, leading to semiparametric models (or partial splines). For details about such models, we refer to [18], Chap. 4 and [80], Chap. 6.

7 Maximum Entropy Discrimination. Large Margin Classifiers
We regard GPs as building blocks for statistical models in much the same way as a parametric family of distributions (see Section 3 for examples). Statistical methods to estimate unknown parameters in such models follow different paradigms, and in machine learning the following have been among the most popular.
1. Probabilistic Bayesian paradigm: This has been introduced in Section 3. As noted in Section 4, the (intractable) posterior process is typically approximated by a GP itself.
40This is not the case for spline kernels, for which f  H1 is constrained by the boundary conditions.

2. Large margin (discriminative) paradigm: Here, a "posterior" process is obtained by associating margin constraints with observed data, then searching for a process which fulfils these (soft) constraints and at the same time is close to the prior GP, in a sense made concrete in this section. Since the constraints are linear in the latent outputs, the "posterior" process is always a GP with the same covariance as the prior.

The relationship between Bayesian methods and penalised likelihood or generalised spline smoothing methods has been discussed in Section 6. Large margin methods are special cases of spline smoothing models with a particular loss function which does not correspond to a probabilistic noise model (e.g., [81, 65, 73]). Several attempts have been made to express large margin discrimination methods as approximations to Bayesian inference (e.g., [73, 65, 64]), but the paradigm separation suggested in [25] seems somewhat more convincing.
The connection between these two paradigms has been formulated in [25], this section is based on their exposition. The large margin paradigm has been made popular by the empirical success of the support vector machine (SVM) (see [59, 8] for background material). In the Bayesian GP setting (see Section 3), the likelihood P (y|u) of the observed data y can be seen to impose "soft constraints" on the predictive distribution, in the sense that functions of significant probability under the posterior must not violate many of them strongly. In the large margin paradigm whose probabilistic view has been called minimum relative entropy discrimination (MRED) [25], such constraints are enforced more explicitly.41 We introduce a set of latent margin variables  = (i)i  Rn, one for each datapoint. Along with the GP prior P (u(·)) on the latent function, we choose a prior P () over . The margin prior encourages large margins i, as is discussed in detail below. The minimum relative entropy distribution dQ(u(·), ) is defined as minimiser of D[Q P ], subject to the soft margin constraints

E(u(·), )Q [yiu(xi) - i]  0, i = 1, . . . , n.

(23)

Just as in the case of a likelihood function, these constraints depend on the values u = (u(xi))i of the random process u(·) only. It is well known in information theory (e.g., [22], Sect. 3.1) that the solution to this constrained problem is given by

n

dQ(u(·),  ) = Z()-1 exp

i (yiui - i) dP (u(·),  ),

(24)

i=1

where

n

Z() = E(u(·), )P exp

i (yiui - i) .

i=1

The value for the Lagrange multipliers  is obtained by minimising the convex function log Z() (sometimes called the dual criterion) under the constraints   0. Since the right hand side of (24) factorises between u(·) and  and the same holds for the prior P , we see that Q must factorise in the same way. Furthermore, it is immediate from (24) that Q(u(·)) is again a Gaussian process with the same covariance kernel K as P (u(·)) and with mean

41For notational simplicity, we do not use a bias term b here. The modifications to do so are straightforward. In the original SVM formulation, b can be seen to have a uniform (improper) prior.

function µ(x) = k(x)T Y , where Y = diag(yi)i. Due to the factorised form, we also have Z() = Zu(·)()Z () and

¡ Zu(·)() = E P e T

=

e

1 2

T ¡£¢¤¡

.

The form of Z depends on the choice of the prior P ( ) on the margin variables.
Jaakkola et. al. [25] give some examples of such priors which encourage large margins.
For example, if P () = i P (i), then P (i) should drop quickly for i < 1 in order to penalise small and especially negative margins (empirical errors). In order for (23) to be
a "soft constraint" only w.r.t. margin violations and also to mimic the SVM situation, we have to use P (i) = 0 for i > 1.42 If P (i)  e-c(1-i)I{i1}, then

Z

()



n i=1

1

e-i - i/c

,

and the complete dual criterion is

log

Z ( )

=

-

n i=1

(i

+

log(1

-

i/c))

+

1 T 2

Y

KY

,

  0.

(25)

Except for the potential term log(1 - i/c), this is identical to the SVM dual objective (see below).43 The so-called hard margin SVM for which margin constraints are enforced without allowing for violations, is obtained for c  . It converges only if the training data is indeed separable and is prone to over-complicated solutions. The effect of the potential term on the solution is limited (see [25]). It keeps i from saturating to c exactly (which happens in SVM for misclassified patterns). The dual criterion can be optimised using efficient algorithms such as SMO [52], although the nonlinear potential term introduces minor complications.44 Just like in SVM, sparsity in  is encouraged and can be observed in practice.

To conclude, MRED gives a complete probabilistic interpretation of the SVM, or at least of a close approximation thereof. Note that SVM classification cannot be seen as MAP approximation to Bayesian inference for a probabilistic model, because its loss function does not correspond to a proper negative log likelihood [65, 47, 73]. Interestingly, the MRED view points out limitations of this framework as opposed to a Bayesian treatment of a Gaussian process model with a proper likelihood. Recall from above that the margin constraints are linear in the latent outputs u, leading to the fact that the MRED "posterior" process Q(u(·)) has the same covariance kernel K as the prior. While the constraints enforce the predictive mean to move from 0 a priori to µ(x), the "predictive variances" are simply the prior ones, independent of the data. This suggests that if predictive variances (or error bars) are to be estimated besides simply performing a discrimination, then SVMs or other large margin discriminative methods may be less appropriate than probabilistic GP models. For more details on this argument, see [63], Sect. 4.7.2.

More important is the lack of practical methods for model selection with SVM. For Bayesian GP methods, a general model selection strategy is detailed in Section 4. Alternatively,

42As in the SVM setup, the choice of 1 as margin width is arbitrary, because the distance can be re-scaled
in terms of the prior variance. 43The potential term acts like a logarithmic barrier to enforce the constraints i < c (e.g., [7]). 44SMO makes use of the fact that the SVM criterion is quadratic with linear constraints.

hyperparameters can be marginalised over approximately using MCMC techniques [43]. In contrast, model selection for SVM is typically done using variants of cross validation, which severely limits the number of free parameters that can be adapted. While it is often claimed that learning-theoretical foundations count as distinctive advantage of SVM, similar or even superior guarantees can be given for approximate Bayesian GP techniques as well [67].
8 Kriging
An important and early application of Gaussian random field models has been termed kriging [35] after a South-African mining engineer D. Krige who developed methods for predicting spatial ore-grade distributions from sampled ore grades [30]. Optimal spatial linear prediction has its roots in earlier work by Wiener and Kolmogorov ("closeness in space" may have to be replaced by "closeness in time", since they were mainly concerned with time series). These fundamental ideas have been further developed in the fields of geostatistics [35] as kriging and in meteorology under the name objective analysis (see [11], Chap. 3 for references). We will not go into any details, but refer to [11], Chap. 3 and [74] (we follow the latter here). The basic model is the same as for semiparametric smoothing:
z(x) = m(x)T  + (x)
where m(x) is a known feature map and (x) is a zero-mean random field with covariance function K. In a nutshell, kriging is a minimum mean squared error prediction method for linear functionals of z(x) given observations z = (z(x1), . . . , z(xn))T at spatial locations xi  Rg. For example, if z(x) measures ore grade at x one might be interested in predicting
z(x) dx
B
over some area B  Rg. Since they focus on m.s. error and m.s. properties of z(x) in general, kriging methods typically depend on second-order properties of the process only, and (x) is often assumed to be a Gaussian field. Furthermore, we restrict ourselves to linear predictors 0 + T z . The optimal predictor of z(x) in the m.s. error sense is the conditional expectation which is linear in z if (x) is Gaussian and  is known:
K  = k, 0 = m(x) - M T  T 
where K = (K(xi, xj))i,j, k = (K(xi, x))i and M = (m(x1), . . . , m(xn))T . If  is unknown, a simple procedure is to plug in the generalised least squares estimate
^ = M T K-1M -1 M T K-1z
for ^ . This procedure can be motivated from several angles. If we restrict our attention to linear predictors of z(x) which are unbiased in the sense
E 0 + T z = 0 + T M  = E[z(x)] = m(x)T 

for any , the suggested approach minimises the m.s. error over these unbiased predictors. It is therefore called best linear unbiased predictor (BLUP). A Bayesian motivation can be constructed in the same way as mentioned in Section 6. Namely,  is given a Gaussian prior whose covariance matrix scales with a > 0 and (x) is a priori Gaussian. Then, the posterior mean for z(x) converges to the BLUP as a   (i.e. as the  prior becomes uninformative).
The equations behind the BLUP have been known long before and have been rediscovered in many areas of statistics. In practice, kriging methods are more concerned about inducing an appropriate covariance function (under the stationarity assumption) from observed data as well. The empirical semivariogram is a frequently used method for estimating the covariance function close to the origin. On the theoretical side, Stein [74] advocates the usefulness of fixed-domain asymptotics (a growing number of observations located within a fixed compact region) to understand the relationship between covariance model and behaviour of kriging predictors.45 By Bochner's theorem (2) a stationary covariance function is characterised by its spectral distribution F (). Stein points out that fixed-domain asymptotics depend most strongly on the spectral masses for large  , i.e. the high frequency components, much less so on the low frequency ones or the mean function m(x)T  (if m(x) is smooth itself, e.g. polynomials). Let f () be the spectral density, i.e. the Fourier transform of K(x). In general, the lighter the tails of f () the smoother (x) is in the m.s. sense. Stein advocates this expected smoothness as a central parameter of the GP prior and condemns the uncritical use of smooth (analytic) covariance functions such as the RBF (Gaussian) kernel (see Section 9). Another important concept highlighted by Stein (see also [80], Chap. 3) is the one of equivalence and orthogonality of GPs.46 Essentially, GPs with covariance functions of different form can be equivalent in which case it is not possible to unambiguously decide for one of them even if an infinite amount of observations in a fixed region are given. On this basis, one can argue that for a parametric family of covariance functions inducing equivalent GPs the parameters can just as well be fixed a priori since their consistent estimation is not possible. On the other hand, parameters s.t. different values lead to orthogonal GPs should be learned from data and not be fixed a priori.
Note that kriging models are more generally concerned with intrinsic random functions (IRF) [36], generalisations of stationary processes which are also frequently used in the spline smoothing context. In a nutshell, a k-IRF u(x) is a non-stationary random field based on a "spectral density" whose integral diverges on any neighborhood of the origin (e.g., has infinite pointwise variance). However, if c  Rn is a generalised divided difference (g.d.d.) for x1, . . . , xn in the sense that i cip(xi) = 0 for all polynomials p of total degree  k, then the variance of i ciu(xi) is finite and serves to define an "covariance function" K(x) which is k-conditionally positive semidefinite, namely
n
cicjK(xi - xj)  0
i,j=1
45Stein restricts his analysis to "interpolation", i.e. to situations where predictions are required only at locations which are in principle supported by observations (in contrast to "extrapolation" often studied in the time series context). This should not be confused with the distinction between interpolation and smoothing used in Section 6. All non-trivial kriging techniques are smoothing methods.
46Two probability measures are equivalent if they have the same null sets, i.e. are mutually absolutely continuous (see Section A.1). They are orthogonal if there is a null set of one of them which has mass 1 under the other. Gaussian measures are either orthogonal or equivalent.

for all g.d.d.'s c. In practice, one uses semi-parametric models where the latent process of interest is the sum of a k-IRF and a polynomial of total degree  k whose coefficients are parametric latent variables.47
In fact, IRFs do not add more generality w.r.t. high-frequency behaviour of the process since f () must be integrable on the complement of any 0-neighborhood, so the IRF can be written as the uncorrelated sum of a stationary and a non-stationary part, the latter with f () = 0 outside a 0-neighborhood (thus very smooth). IRFs are not discussed in any further detail here (see [36, 74]).

9 Choice of Kernel. Kernel Design

There is a tendency in the machine learning community to treat kernel methods as "black box" techniques, in the sense that covariance functions are chosen from a small set of candidates over and over again. If a family of kernels is used, it typically comes with a very small number of free parameters so that model selection techniques such as crossvalidation can be applied. Even though such approaches work surprisingly well for many problems of interest in machine learning, experience almost invariably has shown that much can be gained by choosing or designing covariance functions carefully depending on known characteristics of a problem (for an example, see [59], Sect. 11.4).

Establishing a clear link between kernel functions and consequences for predictions is very non-trivial and theoretical results are typically asymptotic arguments. As opposed to finitedimensional parametric models, the process prior affects predictions from a non-parametric model even in fixed-domain asymptotic situations (see Section 8). The sole aim of this section is to introduce a range of frequently used kernel functions and some of their characteristics, to give some methods for constructing covariance functions from simpler elements, and to show some techniques which can be used to obtain insight into the behaviour of the corresponding GP. Yaglom [88] gives extensive material, an accessible review is [1]. In the final part, we discuss some kernel methods over discrete spaces X .

It should be noted that positive definiteness of an arbitrary symmetric form or function is hard to establish in general. For example, the sensible approach of constructing a distance d(x, x ) between patterns depending on prior knowledge, then proposing

K(x, x ) = e-w d( , )2

(26)

as covariance function does not work in general because K need not be positive semidefinite, moreover there is no simple general criterion to prove that K is a covariance function.48 If
d(x, x ) can be represented in an Euclidean space, K is a kernel as we will see below. Note that if K(x, x ) of the form (26) is a kernel, so must be K(x, x )t for any t > 0.49 Kernels
with this property are called infinitely divisible. Scho¨nberg [60] managed to characterise
infinitely divisible kernels (26) by a property on d(x, x ) which unfortunately is just as hard to handle as positive semidefiniteness.50

47In fact, ( ) maps to a basis of k. As mentioned above, the BLUP is obtained as posterior expectation ¡

under an uninformative prior on the parametric coefficients.

48If d( , ) is stationary, one can try to compute the spectral density, but this will not be analytically

¡

¡

tractable in general.

49This is true in general only for t  ¡ >0 , see below.

50-d( , )2 must be conditionally positive semidefinite of degree 0 (see Section 8).

¡

¡

9.1 Some Standard Kernels

In the following, we provide a list of frequently used "standard kernels". Most of these will have a variance (scaling) parameter C > 0 in practice, sometimes an offset parameter
vb > 0, thus instead of K one uses C K or C K + vb. C scales the variance of the process, while a vb > 0 comes from the uncertainty of a bias parameter added to the process.51 In applications where the kernel matrix K is used directly in linear systems, it is advised to add a jitter term52  , to the kernel to improve the condition number of K . This amounts to a small amount of additive white noise on u(x) ( can be chosen quite small), but should
not be confused with measurement noise which is modelled separately (see Section 3). These
modifications are omitted in the sequel for simplicity.

The Gaussian (RBF) covariance function

K(x, x ) = exp

-

w 2

x-x

2

(27)

is isotropic for each X = Rg (i.e. D). w > 0 is an inverse squared length scale parameter, in the sense that w-1/2 determines a scale on which u(x) is expected to change significantly.
K(x) is analytic at 0, so u(x) is m.s. analytic. Stein [74] points out that

j

k =0

u(j)

(0)

xj j!

 u(x)

in quadratic mean for every x (a similar formula holds for X = Rg), so that u can be predicted perfectly by knowing all its derivatives at 0 (which depend on u on an neighborhood of 0 only). He criticises the wide-spread use of the Gaussian covariance function because its strong smoothness assumptions are unrealistic for many physical processes, in particular predictive variances are often unreasonably small given data. The spectral density (in R) is f () = (2w)-1/2 exp(-2/(2w)) with very light tails. On the other hand, Smola et. al. [70] recommend the use of the Gaussian covariance function for high-dimensional kernel classification methods because of the high degree of smoothness. It is interesting to note that in the context of using GPs for time series prediction, Girard et. al. [17] report problems with unreasonably small predictive variances using the Gaussian covariance function (although they do not consider other kernels in comparison). Figure 1 shows smoothed plots of some sample paths. Note the effect of the length scale w-1/2 and the high degree of smoothness.

We can consider the anisotropic version, called squared-exponential covariance function:

K(x, x ) = exp - 1 (x - x )T W (x - x ) .

(28)

2

Here, W is positive definite. Typically, W is a diagonal matrix with an inverse squared length scale parameter wj for each dimension. Full matrices W have been considered in [53, 79], and factor analysis-type matrices W are a useful intermediate (e.g., [4, 65]). An important application of the additional d.o.f.'s in (28) as compared to the Gaussian kernel is automatic relevance determination (ARD), as discussed below. Note that the squaredexponential covariance function for diagonal W can be seen as product of g one-dimensional

51For reasons of numerical stability, vb must not become too large.

52 In

the

context

of

kriging

(see

Section

8),

adding

 ¡

,¡

has been proposed by Math´eron to model the

so-called "nugget effect" (see [11], Sect. 2.3.1), but other authors have criticised this practice.

Gaussian (RBF)
4

3

2

1

0

-1

-2

-3

-4

0

0.2

0.4

0.6

0.8

1

Figure 1: Smoothed sample paths from GP with Gaussian covariance function. All have variance C = 1. Dash-dotted: w = 1. Solid: w = 102. Dashed: w = 502.

Gaussian kernels with different length scales, so the corresponding RKHS is a tensor product space built from RKHS's for one-dimensional functions (see Section 5).

The Mat´ern class of covariance functions (also called modified Bessel covariance functions)

is given by

K( )

=

1/2 2-1( + 1/2)2

( ) K ( ),

 = x-x ,

(29)

where  > 0,  > 0 and K(x) is a modified Bessel function (e.g., [74], Sect. 2.7). One can show that z K(z)  2-1() for z  0, so

K (0)

=

 1/2 ( ) ( + 1/2)2

.

K is isotropic for each X = Rg. An important feature of this class is that the m.s. smoothness
of u(x) can be regulated directly via . For example, u(x) is m times m.s. differentiable iff  > m. The spectral density in R is f () = (2 + 2)--1/2. For  = 1/2 + m we
obtain a process with rational spectral density, a continuous time analogue of an AR time series model. For  = 1/2, K( )  e- defines an Ornstein-Uhlenbeck process, a stationary
analogue to the Wiener process which also has independent increments. In general, for  = 1/2 + m we have K( )  e- p( ), where p(x) is a polynomial of order m (e.g., [74], Sect. 2.7). Note that if  = (w(2 + 1))1/2, then

2+1f ()  e-2/(2w) (  ),

thus K( ) converges to the Gaussian covariance function after appropriate re-scaling.

The Mat´ern class can be generalised to an anisotropic family in the same way as the Gaussian kernel. Figure 2 show some sample function plots for values  = 1/2, 3/2, 5/2, 10. Note the effect of  on the roughness of the sample paths. For  = 1/2 the paths are erratic even though the length scale is 1, i.e. the same as the horizontal region shown. For  = 3/2, the process is m.s. differentiable, for  = 5/2 twice so.

Matern (nu=1/2): Ornstein-Uhlenbeck

2

3

Matern (nu=3/2)

1.5 2
1 1
0.5 0
0 -1
-0.5
-2 -1

-1.5

-3

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

10

0.2

0.4

0.6

0.8

1

Matern (nu=5/2) 3

Matern (nu=10) 3

2

2

1

1

0

0

-1

-1

-2

-2

-3

-3

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

10

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Figure 2: Smoothed sample paths from GP with Mat´ern covariance function. All have
variance C = 1. Upper left: Ornstein-Uhlenbeck (Mat´ern,  = 1/2),  = 1. Upper right: Mat´ern,  = 3/2,  = 1 (dash-dotted),  = 102 (solid). Lower left: Mat´ern,  = 5/2,  = 1 (dash-dotted),  = 102 (solid). Lower right: Mat´ern,  = 10,  = 1 (dash-dotted),  = 102
(solid).

The exponential class of covariance functions is given by K( ) = e- ,   (0, 2].

The positive definiteness can be proved using the Mat´ern class (see [74], Sect. 2.7). For  = 1, we have the Ornstein-Uhlenbeck covariance function, for  = 2 the Gaussian one. Although it seems that the kernel varies smoothly in , the processes have quite different properties in the regimes   (0, 1),  = 1,   (1, 2) and  = 2. Continuous sample paths can be ensured for any   (0, 2], but differentiable sample paths can only be obtained for

 = 2 (in which case they are analytic).53 K( ) is not positive definite for  > 2. Figure 3 shows some sample path plots.
Exponential
3

2

1

0

-1

-2

-3

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Figure 3: Smoothed sample paths from GP with exponential covariance function. All have variance C = 1 and  = 102. Solid:  = 1.5. Dashed:  = 1.9. Dash-dotted:  = 2 (Gaussian).

We have derived the spline covariance function on [0, 1] (22) from first principles above.

This kernel is of interest because posterior mean functions in GP models (or minimisers of

the variational problem over the RKHS) are splines of order m, i.e. piecewise polynomials in C2m-2 (see Section 6) and associated computations are O(n) (where n is the number of

training points, or "knots") only. On the other hand, technical complications arise because spline kernels are RKs for subspaces of Wm[0, 1] only, namely of the functions which satisfy the boundary conditions (see Section 6). The operator induced by a spline kernel has a

null space spanned by polynomials, and in practice it is necessary to adjoin the correspond-

ing (finite-dimensional) space. The spline kernels are not stationary (they are supported

on [0, 1]), but we can obtain spline kernels on the circle by imposing periodic boundary

conditions on Wm[0, 1], leading to the stationary kernel

K(x, x ) =

2 (2 )2m

cos(2(x

-

x

)).

1

From this representation, it follows that the spectral density is

f () =

1 (2 )2m

2

(||)

1

which is discrete. Note that sample functions from u(x) are periodic with probability 1. In Wahba [80], Chap. 2 it is shown how to construct splines on the sphere by using the

53All these statements hold with probability 1, as usual.

iterated Laplacian, but this becomes quite involved. An equivalent to splines (in a sense) can be defined in Rg using thin-plate spline conditionally positive definite functions (see Section 8), see [80, 18] for details.
For kernel discrimination methods, polynomial covariance functions

K(x, x ) =

((

x

(xT x 2 + )(

+ x

)m 2+

))m/2

,

  0, m  N

are popular although they seem unsuitable for regression problems. The denominator normalises the kernel to K(x, x) = 1. Although this normalisation is not done in some applications, it seems to be recommended in general. Polynomial kernels without the normalising denominator can be seen to induce a finite-dimensional feature space of polynomials of total degree  m (if  > 0).54 It is interesting to note that this is exactly the RKHS we have to adjoin to one for a conditionally positive definite kernel of order m such as the thin-plate spline covariance function. On the other hand, in the spline case these polynomial parts are usually not regularised at all. By the Karhunen-Loeve expansion (see Section 5), we can write u(x) as expansion in all monomials of total degree  m with Gaussian random coefficients. The regularisation operator (see Section 6) for polynomial kernels is worked out in [70]. Note that K(x, x ) is not a covariance function for m  N, thus the kernel is not infinitely divisible. Figure 4 shows some sample path plots. These are polynomials and therefore analytic.

Polynomial
2

1.5

1

0.5

0

-0.5

-1

-1.5

-2

-2.5

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Figure 4: Sample paths from GP with polynomial covariance function. All have variance C = 1 and  = 0.05. Solid: m = 10. Dashed: m = 5.
The Euclidean inner product xT x is sometimes referred to as "linear kernel" in the
54The feature space of the normalised polynomial kernel consists of polynomials of total degree  m divided by ( 2 + )m/2.
¡

machine learning literature. GP models based on this kernel are nothing else than straightforward linear models (linear regression, logistic regression, etc.). It is clear from the weight space view (see Section 2) that a linear model can always be regarded as a GP model (or kernel technique), but this makes sense only if n < g, where n is the number of training points.55 Furthermore, the SVM with linear kernel is a variant of the perceptron method [55] with "maximal stability" [57] studied in statistical physics.
Finally, let us give an example of a function which is not a covariance function, the so-called "sigmoid kernel"
K(x, x ) = tanh axT x + b .
K is not positive semidefinite for any a, b (see [69]), it is nevertheless shipped in most SVM packages we know of. It springs from the desire to make kernel expansions look like restricted one-layer neural networks. The correct link between MLPs and GP models has been given by Neal (see Section 2), which involves taking the limit of infinitely large networks. A covariance function corresponding to a one-layer MLP in the limit has been given by Williams [84]. In practice, it is of course possible to fit expansions of kernels to data which are not covariance functions. However, the whole underlying theory of minimisation in a RKHS (see Sections 5 and 6) breaks down, as does the view as inference in a GP model. On the practical side, flawed results such as negative predictive variances can pop up when least expected. Even worse, most optimisation techniques (including SVM algorithms) rely on the positive semidefiniteness of matrices and may break down otherwise. In fact, the SVM optimisation problem is not convex and has local minima for general K.

9.2 Constructing Kernels from Elementary Parts
We can construct complicated covariance functions from simple restricted ones which are easier to characterise (e.g. stationary or (an)isotropic covariance functions, see Section 2). A large number of families of elementary covariance functions are known (e.g., [88]), some of which are reviewed in Section 9.1.
A generalisation of stationary kernels to conditionally positive semidefinite ones (stationary fields to IRFs) is frequently used in geostatistical models (see Section 8) but will not be discussed here. The class of positive semidefinite forms has formidable closure properties. It is closed under positive linear (so-called conic) combinations, pointwise product and pointwise limit. If K(v, v ) is a covariance function, so is

K~ (x, x ) = h(x; v)h(x ; v )K(v, v ) dvdv

(30)

(if K~ is finite everywhere). An important special case is K~ (x, x ) = a(x)K(x, x ) a(x ). For example, a given kernel (with positive variance everywhere) can always be modified to be constant on the diagonal by choosing a(x) = K(x, x)-1/2, this normalisation has been discussed in the context of the polynomial kernel above. Note that O'Hagan's "localised regression model" (Section 2) is also a special case of (30). A general way of creating a non-stationary covariance function K~ (y, y ) from a parametric model h(y; ) linear in  is to assume a GP prior on , then to integrate out the parameters (see [62] for details). Furthermore, suppose we do so with a sequence of models and priors to obtain a sequence
55Otherwise, running a kernel algorithm is wasteful and awkward due to a singular kernel matrix.

of kernels. If the priors are appropriately scaled, the pointwise limit exists and is a kernel again. Many standard kernels can be obtained in this way (e.g., [84]). Neal showed that if the model size goes to infinity and the prior variances tend to 0 accordingly, layered models with non-Gaussian priors will also tend to a GP (due to the central limit theorem; see Section 2).

Another important modification is embedding. If K(h, h ) is a covariance function and h(x)

is an arbitrary map, then

K~ (x, x ) = K(h(x), h(x ))

(31)

is a covariance function as well (this is a special case of (30)). For example, if we have
d(x, x ) = h(x) - h(x ) in some Euclidean space, then (26) is a valid kernel induced from
the Gaussian (RBF) kernel (27). The Fisher kernel [23] and mutual information kernels [66]
are examples. Embedding can be used to put rigid constraints on the GP. For example, if K is stationary in (31) and h(x) = h(x ), then u(x) = u(x ) almost surely.56 For h(x) = (cos((2/)x), sin((2/)x))T , sample paths of u(x) are -periodic functions.

Embedding can be used to create non-stationary kernels from elementary stationary ones. A more powerful mechanism starts from viewing (30) in a different way. Let K be the squared-exponential kernel (28), but suppose the input x is subject to noise:

x = t + ,   N (0, S (t)).

Here,  at different observed locations t are independent, and all noise variables are independent of the process u(·). The process v(t) = u(x) = u(t + ) is not Gaussian, but its mean and covariance function are determined easily: E[v(t)] = 0 and
E[v(t)v(t )]  E N (t +  | t +  , W -1) = N (t | t , W -1 + S (t) + S (t ))

which has the form of a squared-exponential kernel with covariance matrix which depends on t, t . A similar construction was used in [16] to create non-stationary covariance functions. This idea can be generalised considerably as shown in [49]. Define

Q(t, t ) =

(t - t )T

1 2

(S (t)

+

S (t

))

-1
(t - t ).

Note that Q is not a Mahalanobis distance, because the covariance matrix depends on t, t .

Now, if ( ) is an isotropic correlation function in D (recall Section 2.2), it is shown in

[49] that

Q(t, t ) = |S (t)|1/4|S (t )|1/4

1 2

(S

(t)

+

S (t

))

-1/2

(Q(t,

t

))

(32)

is a valid correlation function. The proof uses the characterisation

( ) = e-22 dF ()

56This is because the correlation (u( ), u( )) is 1, thus u( ) = a u( ) + b for fixed a, b, then a = 1, b = 0

¡

¡

¡

¡

because both variables have the same mean and variance.

of D (see Section 2.2), thus

(Q(t, t )) =

1 2

(S(t

)

+

S(t

))

1/2

 22

d/2

N

t

t

,

1 42

(S(t)

+

S(t

))

dF ().

The integral can now be written as



N (r | t, S~ (t, ))N (r | t , S~ (t , )) dr dF ()

which is positive semi-definite as a special case of (30).57 Equation (32) can be used to create many new families of non-stationary kernels from isotropic ones. Note that now there are two fields to estimate, u(·) and t  S(t). In principle, the latter one can be specified via GPs as well (see [49]), but inference becomes very costly. On the other hand, simpler parametric models may be sufficient. If unlabelled data is abundant, it is possible to learn the second field from this source only (see [62]). It is interesting to note that if t  S(t) is smooth, then m.s. properties of u(·) deducible from ( ) are transferred to the GP with correlation function Q (32).

9.3 Guidelines for Kernel Choice
Choosing a good kernel for a task depends on intuition and experience. On high-dimensional tasks where no suitable prior knowledge is available, the best option may be to explore simple combinations of the standard kernels listed above. If invariances are known, they may be encoded using the methods described in [59], Sect. 11.4. With approximate Bayesian GP inference, one can in principle use combinations of different kernels with a lot of free (hyper)parameters which can be adapted automatically.
For low-dimensional X , one can obtain further insight. Stein [74] points out the usefulness of studying fixed-domain asymptotics (see Section 8). In this respect, the tail behaviour of the spectral density (see Section 2) is important. The m.s. degree of differentiability (degree of smoothness) of the process depends on the rate of decay of f (). Stein recommends kernel families such as the Mat´ern class (29) which come with a degree of smoothness parameter . He also stresses the importance of the concept of equivalence and orthogonality of GPs (see Section 8). His arguments are of asymptotic nature, for example it is not clear whether  in the Mat´ern class can be learned accurately enough from a limited amount of data. Also, predictions from equivalent processes with different kernels can be different.58
There are ways of "getting a feeling" for the behaviour of a process by visualisation, which is an option if X = Rg is low-dimensional, g = 1, 2. We can draw "samples" from the process and plot them as follows (the plots in this section have been produced in this way). Let X  X be a fine grid59 over a domain of interest, n = |X| and u = u(X) 
57Namely, (30) applies especially to "diagonal kernels" K( , ) = f ( ) ( ) where f is positive. In our ¡
case, = ( T , )T . ¢ 58Stein argues (citing Jeffreys) that such differences cannot be important since they do not lead to con-
sistency in the large data limit (in a fixed domain). 59For fine grids and smooth kernels such as the Gaussian one, the Cholesky technique described here fails
due to round-off errors. The singular value decomposition (SVD) should be used in this case, concentrating on the leading eigendirections which can be determined reliably.

N (0, K (X)). We can sample u as u = Lv, v  N (0, I), where K (X) = LLT is the
Cholesky decomposition. If X is too large, u can be approximated using an incomplete
Cholesky factorisation of K (X) (see [87]). If g = 1, the process is isotropic and the grid is regularly spaced, K (X) has Toeplitz structure60 and its Cholesky decomposition can be computed in O(n2) (see [14]). Repeatedly sampling u and plotting (X, u) can give an idea about degree of smoothness, average length scales (Euclidean distance in X over which u(x)
is expected to vary significantly) or other special features of K.

9.4 Learning the Kernel

One promising approach for choosing a covariance function is to learn it from data and/or
prior knowledge. For example, given a parametric family of covariance functions, how can we choose61 the parameters in order for the corresponding process to model the observed
data well?

Model selection from a fixed family can be done by the empirical Bayesian method of

marginal likelihood maximisation, a generic approximation of which in the case of GP mod-

els is given in Section 4. Since this procedure typically scales linearly in the number of hyper-

parameters, elaborate and heavily parameterised families can be employed. An important

special case has been termed automatic relevance determination (ARD) by MacKay [34]

and Neal [42]. The idea is to introduce a hyperparameter which determines the scale of

variability of a related variable of interesting (with prior mean 0). For example, we might

set up a linear model (4) by throwing in a host of different features (components in (x)),

then place a N (|0, D) on the weights  where D is a diagonal matrix of positive hyper-

parameters. If we place a hyperprior on diag D which encourages small values, there is an

a priori incentive for di = Di,i to become very small, inducing a variance of i close to 0

which effectively switches off the effect of ii(x) on predictions. This is balanced against

the need to use at least some of the components of the model to fit the data well, leading

to an automatic discrimination between relevant and irrelevant components. In the context

of covariance functions, we can implement ARD with any anisotropic kernel (see Section 2)

of the form

K(x, x ) = K~ ((x - x )T W (x - x )),

where K~ is isotropic and W is diagonal and positive definite. An example is the squared-
exponential covariance function (28). Here, wi determines the scale of variability of the (prior) field as x moves along the i-th coordinate axis. If we imagine the field being restricted to a line parallel to this axis, wi-1/2 is the length scale of this restriction, i.e. a distance for which the expected change of the process is significant. If wi  0, this length scale is very large, thus the field will be almost constant along this direction (in regions of interest).
Thus, via ARD we can discriminate relevant from irrelevant dimensions in the input variable
x automatically, and predictions will not be influenced significantly by the latter.

In spatial statistics, semivariogram techniques (see [11], Sect. 2.3.1) are frequently used. For a stationary process, the (semi)variogram is (x - x ) = (1/2)Var[u(x) - u(x )]. It is estimated by averaged squared distances over groups of datapoints which are roughly the

60A matrix is Toeplitz if all its diagonals (main and off-diagonals) are constant. 61The proper Bayesian solution would be to integrate out the parameters, but even if this can be ap-
proximated with MCMC techniques, the outcome is a mixture of covariance functions leading to expensive
predictors.

same distance apart and fitted to parametric families by maximum likelihood. Stein [74] criticises the use of the empirical semivariogram as single input for choosing a covariance function and suggests a range of other techniques, including the empirical Bayesian approach mentioned above.
For classification models, the idea of local invariance w.r.t. certain groups of transformations is important. For example, the recognition of handwritten digits should not be influenced by translations or small-angle rotations of the bitmap.62 If a process is used as latent function in a classification problem, e.g. representing the log probability ratio between classes (see Section 3), then starting from some x and applying small transformations from a group w.r.t. which discrimination should remain invariant should not lead to significant changes in the process output (e.g. in the m.s. sense). To relate this notion to ARD above, varying x along such invariant directions should induce a coordinate of x (non-linear in general) which is irrelevant for prediction. Chapter 11 in [59] gives a number of methods for modifying a covariance function in order to incorporate invariance knowledge to some degree, also reviewing work in that direction which we omit here.
Finally, Minka [38] pointed out that instances of the "learning how to learn" or "prior learning" paradigm can be seen as learning a GP prior from multi-task data (see his paper for references). In fact, the setup is the one of a standard hierarchical model frequently used in Bayesian statistics to implement realistic prior distributions. We have access to several noisy samples and make the assumption that these have been sampled from different realisations of the latent process which in turn have been sampled i.i.d. from the process prior. Data of this sort is very valuable for inferring aspects of the underlying covariance function. In a simple multi-task scenario a multi-layer perceptron is fit to several samples by penalised maximum likelihood, sharing the same input-to-hidden weights but using different sets of hidden-to-output weights for each sample. The idea is that the hidden units might discover features which are important in general, while the combination in the uppermost layer is specific. If we place Gaussian priors on the hidden-to-output weights, this becomes a GP model with a covariance function determined by the hidden units. More generally, we can start from any parametric family of covariance functions and learn hyperparameters or even the hyperposterior from multi-task data using marginal likelihood maximisation together with the hierarchical sampling model. An approximate implementation of this idea has been reported in [51].
9.5 Kernels for Discrete Objects
As mentioned in Section 2, in principle the input space X is not restricted to be Rg or even a group. For example, Gaussian processes over lattices are important in vision applications (in the form of a Gaussian Markov random field with sparse structured inverse covariance matrix). For Gaussian likelihoods, the posterior mean can be determined most efficiently using a conjugate gradients solver63 and the embedded trees algorithm of Wainwright, Sudderth and Willsky [82] can be used to compute the marginal variances as well. Kernel methods, i.e. methods which use covariance matrices over variables determined from the "spatial" relationship of these (or associated covariates) have been proposed for a number
62Although a 180-degree rotation of a 6 results in a 9. 63Loopy belief propagation renders the correct mean as well if it converges [83], but is much slower and often numerically unstable.

of problems involving discrete spaces X (finite or countably infinite). Our aim in this section is no more than to give a few selected examples.
Kernels can be defined on the set of finite-length strings from a finite alphabet. Many string kernels have been proposed recently, but we will not try to review any of this work. Important applications of string kernels (or distance measures between sequences) arise from problems in DNA or RNA biology where statistical models have to be built for nucleotide sequences. Many proposed string kernels are special cases of convolution kernels introduced by Haussler [21]. Maybe the most interesting case discussed there is the extension of a hidden Markov random field (HMRF). The latter is a Markov random field (MRF) with observed variables x, latent variables u and clique potentials Cd(xd, ud) where xd, ud are subsets of components of x, u, and u is marginalised over. If we replace the clique potential by positive definite kernels Kd((xd, ud), (xd, ud)) and marginalise over u, u , the result is a covariance kernel which can also be seen as unnormalised joint generative distribution for (x, x ). If the original MRF has a structure which allows for tractable computation, the same algorithm can be used to evaluate the covariance function efficiently. For example, a hidden Markov model (HMM) for sequences can be extended to a pair-HMM in this way, emitting two observed sequences sharing the same latent sequence, and many string kernels arise as special cases of this construction.
In practice, string kernels (and more generally kernels obtained from joint probabilities under pair-HMRFs) often suffer from the "ridge problem": K(x, x) is much larger than K(x, x ) for many x for which a priori we would like to attain a significant correlation, especially if rather long sequences are compared. For example, in models involving DNA sequences we would like sequences to correlate strongly if they are homologous, i.e. encode for proteins of very similar function. In a standard string kernel, two sequences are strongly correlated if both can be obtained from a common "ancestor" latent sequence by few operations such as insertions and substitutions (this ancestor model is motivated by the evolution of genes and gives a good example for the pair-HMM setup). However, often homologous sequences differ quite substantially in regions on which the structure of the functional part of the protein does not depend strongly. Such "remote" homologies are the really interesting ones, since very close homologies can often be detected using simpler statistical techniques than process models based on string kernels. On the other hand, it may be possible to spot such homologies by going beyond string kernels and pair-HMRF constructions, for example building on the general framework given in [10] where kernels are obtained from finite transducers.
A conceptually simple way to obtain a kernel on X is to embed X in some Euclidean space Rg, then to concatenate the embedding with any of the known Rg kernels, for example the Gaussian one (27). An example is the Fisher kernel [23] which maps datapoints to their "Fisher scores" under a parametric model. There has been a surge of interest recently in automatic methods for parameterising low-dimensional non-linear manifolds (e.g., [77, 56]) by local Euclidean coordinates. Although these methods are non-parametric, they can be used to fit conventional parametric mixture models in order to obtain a parametric embedding which could then be used to obtain a kernel.
Recently, Kondor and Lafferty [29] proposed kernels on discrete objects using concepts from spectral graph theory (diffusion on graphs). If X is finite, a covariance function on X is simply a positive semidefinite matrix. If H is a symmetric generator matrix, the

corresponding exponential kernel is defined as

K() = exp(H ) =

j j!

H

j

,

  0.

(33)

j1

We

define

K (x ,

x

)

=

K

() ,

, where

we use elements

of

X

as

indices into

the

matrix K ().

K() is positive definite. In fact, it has the same eigenvectors as H , but the eigenspectrum is

transformed via   exp(). In practice, general exponential kernels cannot be computed

feasibly if X is large, in particular there is no general efficient way of computing kernel

matrices of K over points of interest. It might be possible to approximate marginalisations of K() by sampling. The kernel and generator matrices are linked by the heat equation

 K() 

=

H K().

It is interesting to note that every infinitely divisible covariance function K with scale parameter  on X has the form (33). Namely, if K is the covariance matrix for K, then H = K / at  = 0. Kondor and Lafferty are interested in diffusion kernels on graphs as special cases of exponential kernels. Here, the generator is the negative of the so-called graph Laplacian. The construction can be seen as stationary Markov chain (random walk) in continuous time on the vertices of the graph. The kernel K(x, x ) is the probability of being at x at time , given that the state at time 0 was x. This interpretation requires that H 1 = 0 which is true for the negative graph Laplacian and which implies that K () is (doubly) stochastic. The same equation describes heat flow or diffusion from an initial distribution. The idea is to describe the structure of X (in the sense of "closeness", i.e. close points should be highly correlated under the covariance function) in terms of local neighbourhood association which induce an (weighted or unweighted) undirected graph. Then, the correlation at some x with all other points is proportional to the distribution of a random walk started at x after time . Similar ideas have been used very effectively for non-parametric clustering or classification with partially labelled data [76]. Kondor and Lafferty give examples for graphs of special regular structures for which the diffusion kernel can be determined efficiently. These include certain special cases of string kernels (here, X is infinite and the analogue to Markov chains has to be treated more carefully). In situations where K cannot be determined by known simple recursive formulae, one could represent X by a representative sample including the training set (but also unlabelled data). If the generator matrix of the underlying graph (projected onto the representative sample in a sensible way) is sparse, its leading eigenvectors and eigenvalues could be approximated by sparse eigensolvers which would lead to an approximation of K () which is low-rank optimal w.r.t. the Frobenius norm. Kondor and Lafferty also note that on the graph given by a regular grid in Rg, the generator matrix converges towards the usual Laplacian operator and K towards the Gaussian kernel (27) as the mesh size approaches 0.

9.6 How Useful are Uncertainty Estimates?
In this section we have highlighted a number of powerful techniques of encoding prior knowledge in a covariance function or learning an appropriate kernel. For many problems in machine learning (especially in classification) one does not observe a big difference in generalisation error over a range of different common kernels, while significant differences arise

in the uncertainty estimates (predictive variances) for Bayesian GP techniques. Moreover, the discussion in Section 7 suggests that much of the additional complexity in Bayesian GP methods as compared to SVM arise exactly because such uncertainty estimates are desired as well. It is therefore important to ask how useful these estimates are in practice.
Strictly speaking, both frequentist confidence intervals and Bayesian uncertainty estimates are tied to assumptions which are likely to be violated in non-trivial real world situations. The former are conditioned on a null hypothesis which is certainly violated at some scale, the latter require the data to be generated by the model. In a Bayesian setting, different priors and models can be compared either to conclude that the predictions enjoy a certain robustness or to detect mismatches which should trigger a refinement.
In the case of GP models, the choice of the covariance function can have a significant effect on the uncertainty estimates. We demonstrate this fact using a simple one-dimensional regression task. Note that in GP regression with Gaussian noise, the error bars do not depend on the targets (this is different for non-Gaussian likelihoods, e.g. in classification). Data was sampled from a noisy sine wave around /2, (3/2), a single point at , the noise standard deviation was  = 0.05. We compare the RBF covariance function (27) with w = 4 against the Mat´ern kernel with different  and  = (w(2 + 1))1/2, the process variance was C = 1 in all cases. Recall that for the Mat´ern kernel,  controls the degree of m.s. differentiability of the process, while the RBF process is m.s. analytic. Figure 5 shows mean predictions and one standard deviation error bars (the noise level was set to the true value).
As expected, for the Ornstein-Uhlenbeck prior ( = 1/2) the mean prediction interpolates the data, the error bars grow to the maximum value 1 very rapidly away from the data. A Brownian motion process is not suitable as prior for a smoothing technique. The tendency to interpolate rather than smooth the data diminishes with growing , as does the speed with which the error bars grow to 1 away from data. Note also the very slim error bars for the RBF prediction in the data-rich regions, expressing the strong (prior) belief that the underlying function is smooth, thus close to the smooth mean prediction there. Stein [74] notes that predictions using the RBF covariance function often come with unrealistically small error bars.
In many situations, the uncertainty estimates themselves are of less importance than the quality of the decisions based on them. In the Bayesian context, decisions are made by substituting the predictive distribution inferred from data for the unknown truth. Utility values can be computed as expectations over the predictive distribution and "Bayesian optimal" decisions be made by comparing these for different alternatives. A simple example arises in binary classification if the task allows us to reject a certain fraction of the test patterns. The Bayesian optimal decision is to reject patterns for which the target predictive distribution P (y|x, D) is most uncertain (has highest entropy). A similar setting is treated heuristically with SVM discriminants rejecting those patterns for which the discriminant value is closest to zero. Note that in both cases, we are interested in the order relations of the scores over a test set rather than their numerical values. A study comparing both practices (the GP technique is a sparse IVM approximation [31] using the same amount of running time) has been done in [63], Sect. 4.7.2. It concludes that on the example considered the SVM reject strategy shows significant weaknesses compared to the approximate Bayesian IVM setup and that the additional work for obtaining uncertainty estimates can pay off.64
64It is shown that large wrong predictive means are often accompanied by large predictive variances,

1.5

1.5

1

1

0.5

0.5

0

0

-0.5

-0.5

-1

-1

-1.5

-1.5

0

1

2

3

4

5

6

70

1

2

3

4

5

6

7

1.5

1.5

1

1

0.5

0.5

0

0

-0.5

-0.5

-1

-1

-1.5

-1.5

0

1

2

3

4

5

6

70

1

2

3

4

5

6

7

Figure 5: Error bars for noisy sine regression task for different covariance functions. Mean prediction (solid), errors bars (dotted), true curve (dashed), data (dots). Upper left: RBF, w = 4. Upper right: Ornstein-Uhlenbeck (Mat´ern,  = 1/2),  = 2.8284. Lower left: Mat´ern,  = 3/2,  = 4. Lower right: Mat´ern,  = 3/2,  = 4.899.

Note that these shortcomings of SVM cannot be alleviated by posthoc transformations of the discriminant output (as suggested by [50]) because these leave order relations invariant.

10 Summary
In this paper, we described central properties of Gaussian processes and statistical models based on GPs together with efficient generic ways of approximate inference and model selection. The focus is less on giving algorithmic descriptions of concrete inference approximations and their variational optimisation problems, which may be found in the references provided. Instead we hope to have conveyed the basic concepts of latent variables and Gaussian random fields required to understand these non-parametric algorithms and to have highlighted some of the essential differences to parametric statistical models. By the
explaining the superior performance of the Bayesian score which combines these two quantities.

evolution of ever more powerful computers and the development of fast sparse inference approximations, we feel that GP models will become applicable to large-data problems which were previously restricted to parametric models. GP models are more powerful and flexible than simple linear parametric models and easier to handle than complicated ones such as multi-layer perceptrons, and the availability of fast algorithms should remove remaining obstacles of them becoming part of the standard toolbox of machine learning practitioners.
Acknowledgements
We thank Chris Williams for many discussions and important comments on early drafts, David Barber and Bernhard Scho¨lkopf for corrections and improvements, Bernhard Scho¨lkopf and the MPI Tu¨bingen for their hospitality in September 2003, furthermore Neil Lawrence, Ralf Herbrich, Lehel Csato´, Manfred Opper, Carl Rasmussen, Amos Storkey and Michael Tipping for discussions and comments. The author gratefully acknowledges support through a research studentship from Microsoft Research Ltd. during his postgraduate studies.
A Appendix
In section A.1, we describe the notational conventions used in this paper and some concepts from probability theory. In Section A.2 we collect some definitions.
A.1 Notation
Vectors a = (ai)i = (a1 . . . an)T (column by default) and matrices A = (ai,j)i,j are written in bold-face. If A  Rm,n, I  {1, . . . , m}, J  {1, . . . , n} are index sets,65 then AI,J denotes the |I| × |J| sub-matrix formed from A by selecting the corresponding entries (i, j), i  I, j  J.
Some special vectors and matrices are defined as follows: 0 = (0)i and 1 = (1)i the vectors of all zero and all ones, j = (i,j)i the j-th standard unit vector. Here, i,j = 1 if i = j, and 0 otherwise (Kronecker symbol). Furthermore, I = (i,j)i,j is the identity matrix. The superscript T denotes transposition. diag a is the matrix with diagonal a and 0 elsewhere. diag A is the vector containing the diagonal of A. tr A is the sum of the diagonal elements of A, tr A = 1T (diag A). |A| denotes the determinant of the square matrix A. For p > 1, a p denotes the p-norm of the vector a, a p = ( i |ai|p)1/p. If nothing else is said, · = · 2, the Euclidean norm. Relations are vectorised in Matlab style, as are scalar functions: a  b means that ai  bi for all i, and f (a) = (f (ai))i. We do not distinguish notationally between a random variable and its possible values. Vector or matrix random variables are written in the same way as vectors or matrices. If a distribution has a density, we generally use the same notation for the distribution and its density function. If x is a random variable, then E[x] denotes the expectation (or expected value) of x. If A is an event, then Pr{A} denotes its probability. The probability space will
65All index sets and sets of data points are assumed to be ordered, although we use a notation known from unordered sets.

usually be clear from the context, but for clarity we often use an additional subscript, e.g. PrS{A} or EP [x] (meaning that x  P ). By IA, we denote the indicator function of an event A, i.e. IA = 1 if A is true, IA = 0 otherwise. Note that Pr{A} = E[IA]. The delta distribution  places mass 1 onto the point x and no mass elsewhere,  (B) = I{ B}. Let X , Y, Z be sets of random variables, X , Y non-empty. We write X  Y | Z to denote the conditional independence of X and Y given Z: the conditional distribution of X given Y, Z does not depend on Y.
log denotes the logarithm to Euler's base e. The notation f (x)  g(x) means that f (x) = cg(x) for c = 0 constant w.r.t. x. We often use this notation with the left hand side being a density. By sgn x, we denote the sign of x, i.e. sgn x = +1 for x > 0, sgn x = -1 for x < 0, and sgn 0 = 0. The Landau O-notation is defined as g(n) = O(f (n)) iff there exists a constant c  0 such that g(n)  c f (n) for almost all n.
We use some probability-theoretic concepts and notation which might be unfamiliar to the reader. A measure is denoted by dµ(x), the Lebesgue measure in Rg is denoted by dx. If A is a measurable set ("event"), µ(A) = I{ A}dµ(x) denotes its mass under µ. A measure is finite if the mass of the whole space is finite, and a probability measure if this mass is 1. If dµ is a probability measure, we denote its distribution by µ. The events A of mass 0 are called null sets.66 For example, in Rg with Lebesgue measure (the usual "volume") all affine spaces of dimension < g are null sets. A property is almost surely (a.s.) true if the event of it being false is a null set. dµ1 is called absolutely continuous w.r.t. dµ2 if all null sets of dµ1 are null sets of dµ2 (the notation is dµ1 dµ2). The theorem of Radon and Nikodym states that dµ1 has a density f (x) w.r.t. dµ2, i.e.

µ1(A) = I{ A}f (x) dµ2(x)

for all measurable A, iff dµ1 dµ2. In this case,

f (x)

=

dµ1(x) dµ2(x)

is called Radon-Nikodym derivative or simply density w.r.t. dµ2.

A.2 Definitions

Definition 1 (Relative Entropy) Let P, Q be two probability measures on the same space with Q P , such that the density dQ/dP exists almost everywhere. The relative entropy is defined as

D[Q

P ] = EQ

log

dQ dP

=

log

dQ dP

dQ.

If Q is not absolutely continuous w.r.t. P , we set D[Q P ] = . It is always non-negative, and equal to 0 iff Q = P . The function (Q, P )  D[Q P ] is strictly convex.
66In order not to run into trouble, we always assume that our probability space is complete, meaning that its sigma-algebra contains all subsets of null sets.

If both Q and P have a density w.r.t. Lebesgue measure dw, then dQ/dP = Q(w)/P (w), the ratio of the densities.
If we fix a base measure P0 (finite, need not be a probability), the entropy can be defined as H[Q] = -D[Q P0]. For continuous distributions over Rg, the uniform (Lebesgue) measure is not finite. The usual remedy is to subtract off an infinite part of the entropy which does not depend on the argument Q, ending up with the differential entropy

H[Q] = - Q(w) log Q(w) dw.

(34)

Both entropy and differential entropy are concave functions (being the negative of convex ones).

References
[1] P. Abrahamsen. A review of Gaussian random fields and correlation functions. Technical report, Norwegian Computing Centre, 1997.
[2] R. J. Adler. Geometry of Random Fields. John Wiley & Sons, 1981.
[3] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68(3):337­404, 1950.
[4] D. Barber and C. Bishop. Ensemble learning for multi-layer networks. In M. Jordan, M. Kearns, and S. Solla, editors, Advances in Neural Information Processing Systems 10, pages 395­401. MIT Press, 1998.
[5] S. Becker, S. Thrun, and K. Obermayer, editors. Advances in Neural Information Processing Systems 15. MIT Press, 2003. To appear.
[6] P. Billingsley. Probability and Measure. John Wiley & Sons, 3rd edition, 1995.
[7] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2002. Available online at www.stanford.edu/~boyd/cvxbook.html.
[8] Christopher Burges. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2(2):121­167, 1998.
[9] K. L. Chung. A Course in Probability Theory. Academic Press, 2nd edition, 1974.
[10] C. Cortes, P. Haffner, and M. Mohri. Rational kernels. In Becker et al. [5]. To appear.
[11] N. Cressie. Statistics for Spatial Data. John Wiley & Sons, 2nd edition, 1993.
[12] Lehel Csato´, Ernest Fokou´e, Manfred Opper, Bernhard Schottky, and Ole Winther. Efficient approaches to Gaussian process classification. In Solla et al. [72], pages 251­ 257.
[13] Lehel Csato´ and Manfred Opper. Sparse on-line Gaussian processes. Neural Computation, 14:641­668, 2002.

[14] G. Cybenko and M. Berry. Hyperbolic Householder algorithms for factoring structured matrices. SIAM J. Matrix Anal. Appl., 11:499­520, 1990.
[15] T. Dietterich, S. Becker, and Z. Ghahramani, editors. Advances in Neural Information Processing Systems 14. MIT Press, 2002.
[16] Mark N. Gibbs. Bayesian Gaussian Processes for Regression and Classification. PhD thesis, University of Cambridge, 1997.
[17] A. Girard, C. Rasmussen, and R. Murray-Smith. Gaussian process priors with uncertain inputs -- application to multiple-step ahead time series forecasting. In Becker et al. [5]. To appear.
[18] P.J. Green and Bernhard Silverman. Nonparametric Regression and Generalized Linear Models. Monographs on Statistics and Probability. Chapman & Hall, 1994.
[19] Geoffrey Grimmett and David Stirzaker. Probability and Random Processes. Oxford University Press, 3rd edition, 2001.
[20] P. Halmos. Introduction to Hilbert Space and the Theory of Spectral Multiplicity. Chelsea, New York, 1957.
[21] David Haussler. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10, University of California, Santa Cruz, July 1999. See http://www.cse.ucsc.edu/~haussler/pubs.html.
[22] Shunsuke Ihara. Information Theory for Continuous Systems. World Scientific, 1st edition, 1993.
[23] T. S. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. In Kearns et al. [26], pages 487­493.
[24] Tommi Jaakkola and David Haussler. Probabilistic kernel regression models. In D. Heckerman and J. Whittaker, editors, Workshop on Artificial Intelligence and Statistics 7. Morgan Kaufmann, 1999.
[25] Tommi Jaakkola, Marina Meila, and Tony Jebara. Maximum entropy discrimination. In Solla et al. [72], pages 470­476.
[26] M. Kearns, S. Solla, and D. Cohn, editors. Advances in Neural Information Processing Systems 11. MIT Press, 1999.
[27] G. Kimeldorf and G. Wahba. A correspondence between Bayesian estimation of stochastic processes and smoothing by splines. Annals of Mathematical Statistics, 41:495­502, 1970.
[28] A. N. Kolmogorov. Foundations of the Theory of Probability. Chelsea, New York, 2nd edition, 1933. Trans. N. Morrison (1956).
[29] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In C. Sammut and A. Hofmann, editors, International Conference on Machine Learning 19. Morgan Kaufmann, 2002.

[30] D. Krige. A statistical approach to some basic mine valuation problems on the witwatersrand. Journal of the Chemical, Metallurgical and Mining Society of South Africa, 52:119­139, 1951.

[31] Neil D. Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian process methods: The informative vector machine. In Becker et al. [5]. See www.cs.berkeley.edu/~mseeger.

[32] T. Leen, T. Dietterich, and V. Tresp, editors. Advances in Neural Information Processing Systems 13. MIT Press, 2001.

[33] D. MacKay. Bayesian Methods for Adaptive Models. PhD thesis, California Institute of Technology, 1991.

[34] D. MacKay. Bayesian non-linear modeling for the energy prediction competition. In ASHRAE Transactions, volume 100, pages 1053­1062, 1994.

[35] G. Matheron. Principles of geostatistics. Economic Geology, 58:1246­1266, 1963.

[36] G. Matheron. The intrinsic random functions and their applications. Journal for Applied Probability, 5:439­468, 1973.

[37] P. McCullach and J.A. Nelder. Generalized Linear Models. Number 37 in Monographs on Statistics and Applied Probability. Chapman & Hall, 1st edition, 1983.

[38] T. Minka and R. Picard.

Learning how to learn is learn-

ing with point sets.

Unpublished manuscript. Available at

http://wwwwhite.media.mit.edu/~tpminka/papers/learning.html., 1997.

[39] Thomas Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Massachusetts Institute of Technology, January 2001.

[40] E. H. Moore. On properly positive hermitian matrices. Bull. Amer. Math. Soc., 23:59, 1916.

[41] R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report CRG-TR-93-1, University of Toronto, 1993. See www.cs.toronto.edu/~radford.

[42] R. M. Neal. Bayesian Learning for Neural Networks. Number 118 in Lecture Notes in Statistics. Springer, 1996.

[43] Radford M. Neal. Monte Carlo implementation of Gaussian process models for Bayesian classification and regression. Technical Report 9702, Department of Statistics, University of Toronto, January 1997.

[44] J. Nelder and R. Wedderburn. Generalized linear models. Journal of Roy. Stat. Soc. B, 135:370­384, 1972.

[45] A. O'Hagan. Curve fitting and optimal design. Journal of Roy. Stat. Soc. B, 40(1):1­42, 1978.

[46] A. O'Hagan. Some Bayesian numerical analysis. In J. Bernardo, J. Berger, A. Dawid, and A. Smith, editors, Bayesian Statistics 4, pages 345­363. Oxford University Press, 1992.
[47] M. Opper and O. Winther. Gaussian process classification and SVM: Mean field results and leave-one-out estimator. In Smola et al. [68].
[48] Manfred Opper and Ole Winther. Gaussian processes for classification: Mean field algorithms. Neural Computation, 12(11):2655­2684, 2000.
[49] C. Paciorek. Nonstationary Gaussian Processes for Regression and Spatial Modelling. PhD thesis, Carnegie Mellon University, Pittsburg, 2003.
[50] J. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Smola et al. [68].
[51] J. Platt, C. Burges, S. Swenson, C. Weare, and A. Zheng. Learning a Gaussian process prior for automatically generating music playlists. In Dietterich et al. [15], pages 1425­ 1432.
[52] John C. Platt. Fast training of support vector machines using sequential minimal optimization. In Scho¨lkopf et al. [58], pages 185­208.
[53] T. Poggio and F. Girosi. Networks for approximation and learning. Proceedings of IEEE, 78(9):1481­1497, 1990.
[54] William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numerical Recipes in C. Cambridge University Press, 2nd edition, 1992.
[55] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386­408, 1958.
[56] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323­2326, 2000.
[57] P. Rujan. A fast method for calculating the perceptron with maximal stability. Journal de Physique I, 3:277­290, 1993.
[58] B. Scho¨lkopf, C. Burges, and A. Smola, editors. Advances in Kernel Methods: Support Vector Learning. MIT Press, 1998.
[59] Bernhard Scho¨lkopf and Alexander Smola. Learning with Kernels. MIT Press, 1st edition, 2002.
[60] I. J. Scho¨nberg. Metric spaces and completely monotone functions. In Proc. Nat. Acad. Sci., volume 39, pages 811­841, 1938.
[61] I. J. Scho¨nberg. Spline functions and the problem of graduation. Annals of Mathematicals, 52:947­950, 1964.
[62] M. Seeger. Covariance kernels from Bayesian generative models. In Dietterich et al. [15], pages 905­912.

[63] M. Seeger. Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and Sparse Approximations. PhD thesis, University of Edinburgh, July 2003. See www.cs.berkeley.edu/~mseeger.
[64] Matthias Seeger. Bayesian methods for support vector machines and Gaussian processes. Master's thesis, University of Karlsruhe, Germany, 1999. See www.cs.berkeley.edu/~mseeger.
[65] Matthias Seeger. Bayesian model selection for support vector machines, Gaussian processes and other kernel classifiers. In Solla et al. [72], pages 603­609.
[66] Matthias Seeger. Covariance kernels from Bayesian generative models. Technical report, Institute for ANC, Edinburgh, UK, 2000. See www.cs.berkeley.edu/~mseeger.
[67] Matthias Seeger. PAC-Bayesian generalization error bounds for Gaussian process classification. Journal of Machine Learning Research, 3:233­269, October 2002.
[68] A. Smola, P. Bartlett, B. Scho¨lkopf, and D. Schuurmans, editors. Advances in Large Margin Classifiers. MIT Press, 1999.
[69] A. Smola, Z. O´ va´ri, and R. Williamson. Regularization with dot-product kernels. In Leen et al. [32], pages 308­314.
[70] A. Smola, B. Scho¨lkopf, and K.-R. Mu¨ller. The connection between regularization operators and support vector kernels. Neural Networks, 11:637­649, 1998.
[71] E. Solak, R. Murray-Smith, W. Leithead, and C. Rasmussen. Derivative observations in Gaussian process models of dynamic systems. In Becker et al. [5]. To appear.
[72] S. Solla, T. Leen, and K.-R. Mu¨ller, editors. Advances in Neural Information Processing Systems 12. MIT Press, 2000.
[73] Peter Sollich. Probabilistic methods for support vector machines. In Solla et al. [72], pages 349­355.
[74] M. Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer, 1999.
[75] I. Steinwart. On the influence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67­93, 2001.
[76] Martin Szummer and Tommi Jaakkola. Partially labeled classification with Markov random walks. In Dietterich et al. [15], pages 945­952.
[77] J. Tenenbaum, A. de Silva, and J. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319­2323, 2000.
[78] Michael Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211­244, 2001.
[79] F. Vivarelli and C. K. I. Williams. Discovering hidden features with Gaussian process regression. In Kearns et al. [26].
[80] Grace Wahba. Spline Models for Observational Data. CBMS-NSF Regional Conference Series. SIAM Society for Industrial and Applied Mathematics, 1990.

[81] Grace Wahba. Support vector machines, reproducing kernel Hilbert spaces and the randomized GACV. In Scho¨lkopf et al. [58], pages 69­88.
[82] M. Wainwright, E. Sudderth, and A. Willsky. Tree-based modeling and estimation of Gaussian processes on graphs with cycles. In Leen et al. [32], pages 661­667.
[83] Y. Weiss and W. Freeman. Correctness of belief propagation in Gaussian graphical models of arbitrary topology. In Solla et al. [72], pages 673­679.
[84] C. Williams. Computation with infinite neural networks. Neural Computation, 10(5):1203­1216, 1998.
[85] Christopher K. I. Williams. Prediction with Gaussian processes: From linear regression to linear prediction and beyond. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer, 1997.
[86] Christopher K. I. Williams and David Barber. Bayesian classification with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342­1351, 1998.
[87] S. Wright. Modified Cholesky factorizations in interior-point algorithms for linear programming. SIAM Journal on Optimization, 9(4):1159­1191, 1999.
[88] A. Yaglom. Correlation Theory of Stationary and Related Random Functions, volume I. Springer, 1987.
[89] H. Zhu, C. K. I. Williams, R. Rohwer, and M. Morciniec. Gaussian regression and optimal finite dimensional linear models. In C. Bishop, editor, Neural Networks and Machine Learning, volume 168 of NATO ASI series. Springer, 1998.


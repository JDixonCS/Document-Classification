AdaBoost
Jiri Matas and Jan S ochman
Centre for Machine Perception Czech Technical University, Prague
http://cmp.felk.cvut.cz

Presentation

Outline:

1

AdaBoost algorithm

2

3 · Why is of interest?
4

· How it works?

5

· Why it works?

6

AdaBoost variants

7

AdaBoost with a Totally Corrective Step (TCS)

8

Experiments with a Totally Corrective Step

9

10

11

12

13

14

15

16

Introduction

1

1990 ­ Boost-by-majority algorithm (Freund)

2

1995 ­ AdaBoost (Freund & Schapire)

3

4 1997 ­ Generalized version of AdaBoost (Schapire & Singer)
5

2001 ­ AdaBoost in Face Detection (Viola & Jones)

6

Interesting properties:

7

AB is a linear classifier with all its desirable properties.

8

9 AB output converges to the logarithm of likelihood ratio.
10 AB has good generalization properties.
11

AB is a feature selector with a principled strategy (minimisation of upper

12

bound on empirical error).

13

AB close to sequential decision making (it produces a sequence of gradually

more complex classifiers).

14

15

16

What is AdaBoost?

1

2 AdaBoost is an algorithm for constructing a "strong" classifier as linear

combination

3

T

f (x) = tht(x)

4

t=1

5

of "simple" "weak" classifiers ht(x).

6

7

8

9

10

11

12

13

14

15

16

What is AdaBoost?

1

2 AdaBoost is an algorithm for constructing a "strong" classifier as linear

combination

3

T

f (x) = tht(x)

4

t=1

5

of "simple" "weak" classifiers ht(x).

6

Terminology

7

ht(x) ... "weak" or basis classifier, hypothesis, "feature"

8

9 H(x) = sign(f (x)) ... `'strong" or final classifier/hypothesis

10

11

12

13

14

15

16

What is AdaBoost?

1

2 AdaBoost is an algorithm for constructing a "strong" classifier as linear

combination

3

T

f (x) = tht(x)

4

t=1

5

of "simple" "weak" classifiers ht(x).

6

Terminology

7

ht(x) ... "weak" or basis classifier, hypothesis, "feature"

8

9 H(x) = sign(f (x)) ... `'strong" or final classifier/hypothesis

10

Comments

11

The ht(x)'s can be thought of as features.

12

Often (typically) the set H = {h(x)} is infinite.

13

14

15

16

(Discrete) AdaBoost Algorithm ­ Singer & Schapire (1997)

Given: (x1, y1), ..., (xm, ym); xi  X , yi  {-1, 1}

1

2 Initialize weights D1(i) = 1/m
3

For t = 1, ..., T : 4

1. (Call WeakLearn), which returns the weak classifier ht : X  {-1, 1} with

5

minimum error w.r.t. distribution Dt;

6

2. Choose t  R,

7

3. Update

8

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

9 10

where Zt is a normalization factor chosen so that Dt+1 is a distribution

11

12 Output the strong classifier:
13

T

14

H(x) = sign

tht(x)

t=1

15

16

(Discrete) AdaBoost Algorithm ­ Singer & Schapire (1997)

Given: (x1, y1), ..., (xm, ym); xi  X , yi  {-1, 1}

1

Initialize weights D1(i) = 1/m

2

For t = 1, ..., T :

3

1. (Call WeakLearn), which returns the weak classifier ht : X  {-1, 1} with

4

minimum error w.r.t. distribution Dt;

5

2. Choose t  R, 3. Update

6

7

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

8

9 where Zt is a normalization factor chosen so that Dt+1 is a distribution
10

Output the strong classifier:

11

T

12

H(x) = sign

tht(x)

13

t=1

Comments

14

The computational complexity of selecting ht is independent of t.

15

All information about previously selected "features" is captured in Dt!

16

WeakLearn

1

Loop step: Call WeakLearn, given distribution Dt;

returns weak classifier ht : X  {-1, 1} from H = {h(x)}

2

Select a weak classifier with the smallest weighted error

3

ht = arg min j =
hj H

m i=1

Dt(i)[yi

=

hj (xi)]

4

5

Prerequisite: t < 1/2 (otherwise stop)

6

WeakLearn examples:

7

· Decision tree builder, perceptron learning rule ­ H infinite

8

· Selecting the best one from given finite set H

9

10

11

12

13

14

15

16

WeakLearn

1

Loop step: Call WeakLearn, given distribution Dt;

returns weak classifier ht : X  {-1, 1} from H = {h(x)}

2

Select a weak classifier with the smallest weighted error

3

ht = arg min j =
hj H

m i=1

Dt(i)[yi

=

hj (xi)]

4

5

Prerequisite: t < 1/2 (otherwise stop)

6

WeakLearn examples:

7

· Decision tree builder, perceptron learning rule ­ H infinite

8

· Selecting the best one from given finite set H

9

10 Demonstration example
11

Weak classifier = perceptron

12

13

14

15

·  N (0, 1)

·



1 r 83

e-1/2(r-4)2

16

WeakLearn

1

Loop step: Call WeakLearn, given distribution Dt;

returns weak classifier ht : X  {-1, 1} from H = {h(x)}

2

Select a weak classifier with the smallest weighted error

3

ht = arg min j =
hj H

m i=1

Dt(i)[yi

=

hj (xi)]

4

5

Prerequisite: t < 1/2 (otherwise stop)

6

WeakLearn examples:

7

· Decision tree builder, perceptron learning rule ­ H infinite

8

· Selecting the best one from given finite set H

9

10 Demonstration example

11

Training set

Weak classifier = perceptron

12

13

14

15

·  N (0, 1)

·  1 e-1/2(r-4)2

r 83

16

WeakLearn

1

Loop step: Call WeakLearn, given distribution Dt;

returns weak classifier ht : X  {-1, 1} from H = {h(x)}

2

Select a weak classifier with the smallest weighted error

3

ht = arg min j =
hj H

m i=1

Dt(i)[yi

=

hj (xi)]

4

5

Prerequisite: t < 1/2 (otherwise stop)

6

WeakLearn examples:

7

· Decision tree builder, perceptron learning rule ­ H infinite

8

· Selecting the best one from given finite set H

9

10 Demonstration example

11

Training set

Weak classifier = perceptron

12

13

14

15

·  N (0, 1)

·  1 e-1/2(r-4)2

r 83

16

AdaBoost as a Minimiser of an Upper Bound on the Empirical Error

The

main

objective

is

to

minimize

tr

=

1 m

|{i

:

H (xi)

=

yi}|

1

2

T

It can be upper bounded by tr(H)  Zt

3

t=1

4

5

6

7

8

9

10

11

12

13

14

15

16

AdaBoost as a Minimiser of an Upper Bound on the Empirical Error

The

main

objective

is

to

minimize

tr

=

1 m

|{i

:

H (xi)

=

yi}|

1

2

T

It can be upper bounded by tr(H)  Zt

3

t=1

4

How to set t?

5

Select t to greedily minimize Zt() in each step

6

Zt() is convex differentiable function with one extremum

7



ht(x)  {-1, 1} then optimal t =

1 2

log(

1+rt 1-rt

)

where rt =

m i=1

Dt(i)ht(xi)yi

8 9

10

Zt = 2 t(1 - t)  1 for optimal t

11

 Justification of selection of ht according to t

12

13

14

15

16

AdaBoost as a Minimiser of an Upper Bound on the Empirical Error

The

main

objective

is

to

minimize

tr

=

1 m

|{i

:

H (xi)

=

yi}|

1

T
It can be upper bounded by tr(H)  Zt

2

t=1

3

How to set t?

4

Select t to greedily minimize Zt() in each step

5

Zt() is convex differentiable function with one extremum

6



ht(x)  {-1, 1} then optimal t =

1 2

log(

1+rt 1-rt

)

7

where rt =

m i=1

Dt(i)ht(xi)yi

8

Zt = 2 t(1 - t)  1 for optimal t

9

 Justification of selection of ht according to t

10

Comments

11

The process of selecting t and ht(x) can be interpreted as a single

12

optimization step minimising the upper bound on the empirical error.

13

Improvement of the bound is guaranteed, provided that t < 1/2.

The process can be interpreted as a component-wise local optimization

14

(Gauss-Southwell iteration) in the (possibly infinite dimensional!) space of

15

¯ = (1, 2, . . . ) starting from. ¯0 = (0, 0, . . . ).

16

Reweighting

Effect on the training set

1

Reweighting formula:

2

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

=

exp(-yi m

t q=1

q

hq(xi))

t q=1

Zq

3 4

5

exp(-tyiht(xi))

< 1, yi = ht(xi) > 1, yi = ht(xi)

6

}

7

 Increase (decrease) weight of wrongly (correctly)

8

classified examples. The weight is the upper bound on the error of a given example!

9

10

11

12

13

14

15

16

Reweighting

Effect on the training set

1

Reweighting formula:

2

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

=

exp(-yi m

t q=1

q

hq(xi))

t q=1

Zq

3 4

5

exp(-tyiht(xi))

< 1, yi = ht(xi) > 1, yi = ht(xi)

6

}

7

 Increase (decrease) weight of wrongly (correctly)

8

classified examples. The weight is the upper bound on the error of a given example!

9

10

11

12

13

14

15

16

Reweighting

Effect on the training set

1

Reweighting formula:

2

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

=

exp(-yi m

t q=1

q

hq(xi))

t q=1

Zq

3 4

5

exp(-tyiht(xi))

< 1, yi = ht(xi) > 1, yi = ht(xi)

6

}

7

 Increase (decrease) weight of wrongly (correctly)

8

classified examples. The weight is the upper bound on the error of a given example!

9

10

11

12

13

14

15

16

Reweighting

Effect on the training set

1

Reweighting formula:

2

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

=

exp(-yi m

t q=1

q

hq(xi))

t q=1

Zq

3 4

5

exp(-tyiht(xi))

< 1, yi = ht(xi) > 1, yi = ht(xi)

6

}

7

 Increase (decrease) weight of wrongly (correctly)

8

classified examples. The weight is the upper bound on the error of a given example!

9

10

11

12

13

14

15

16

Reweighting

Effect on the training set

1

Reweighting formula:

2

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

=

exp(-yi m

t q=1

q

hq(xi))

t q=1

Zq

3 4

3

5

exp(-tyiht(xi))

< 1, yi = ht(xi) > 1, yi = ht(xi)

2.5 2

6

err

}

1.5

7

1

 Increase (decrease) weight of wrongly (correctly)

0.5

8

classified examples. The weight is the upper bound on the error of a given example!

0

-2

-1.5

-1

-0.5

0

0.5

1

1.5

2

yf(x)

9

10

11

12

13

14

15

16

Reweighting

Effect on the training set

1

Reweighting formula:

2

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

=

exp(-yi m

t q=1

q

hq(xi))

t q=1

Zq

3 4

3

5

exp(-tyiht(xi))

< 1, yi = ht(xi) > 1, yi = ht(xi)

2.5 2

6

err

}

1.5

7

1

 Increase (decrease) weight of wrongly (correctly)
classified examples. The weight is the upper bound on the error of a given example!

0.5

0

-2

-1.5

-1

-0.5

0

0.5

1

1.5

2

yf(x)

8 9

Effect on ht

10
e

t minimize Zt 

0.5

11

Dt+1(i) =

Dt+1(i)

12

i:ht(xi)=yi

i:ht(xi)=yi

13

Error of ht on Dt+1 is 1/2

14

Next weak classifier is the most "independent" one

15

1

2

3

4t

16

Summary of the Algorithm
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

Summary of the Algorithm

Initialization...

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

Summary of the Algorithm

Initialization...

1

For t = 1, ..., T :

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=1

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

5

6

7

8

9

10

11

12

13

14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=1

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

6

7

8

9

10

11

12

13

14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=1

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

Set

t

=

1 2

log(

1+rt 1-rt

)

6

7

8

9

10

11

12

13

14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=1

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

Set

t

=

1 2

log(

1+rt 1-rt

)

6

Update

7

8

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

9

10

11

12

13

14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=1

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

Set

t

=

1 2

log(

1+rt 1-rt

)

Update

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

Output the final classifier:

T

H(x) = sign

tht(x)

t=1

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

6 7 8 9 10 11 12 13 14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=2

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

Set

t

=

1 2

log(

1+rt 1-rt

)

Update

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

Output the final classifier:

T

H(x) = sign

tht(x)

t=1

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

6 7 8 9 10 11 12 13 14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=3

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

Set

t

=

1 2

log(

1+rt 1-rt

)

Update

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

Output the final classifier:

T

H(x) = sign

tht(x)

t=1

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

6 7 8 9 10 11 12 13 14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=4

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

Set

t

=

1 2

log(

1+rt 1-rt

)

Update

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

Output the final classifier:

T

H(x) = sign

tht(x)

t=1

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

6 7 8 9 10 11 12 13 14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=5

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

Set

t

=

1 2

log(

1+rt 1-rt

)

Update

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

Output the final classifier:

T

H(x) = sign

tht(x)

t=1

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

6 7 8 9 10 11 12 13 14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=6

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

Set

t

=

1 2

log(

1+rt 1-rt

)

Update

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

Output the final classifier:

T

H(x) = sign

tht(x)

t=1

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

6 7 8 9 10 11 12 13 14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t=7

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

Set

t

=

1 2

log(

1+rt 1-rt

)

Update

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

Output the final classifier:

T

H(x) = sign

tht(x)

t=1

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

6 7 8 9 10 11 12 13 14

15

16

Summary of the Algorithm

Initialization... For t = 1, ..., T :

t = 40

1

2

m

3

Find ht = arg min j = Dt(i)[yi = hj(xi)]

hj H

i=1

4

If t  1/2 then stop

5

Set

t

=

1 2

log(

1+rt 1-rt

)

Update

Dt+1(i)

=

Dt(i)exp(-tyiht(xi)) Zt

Output the final classifier:

T

H(x) = sign

tht(x)

t=1

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

6 7 8 9 10 11 12 13 14

15

16

Does AdaBoost generalize?

Margins in SVM

1

y( · h(x)) max min

2

(x,y)S

2

3

Margins in AdaBoost

y( · h(x))

4

max min

(x,y)S

1

5

Maximizing margins in AdaBoost

6

7

T
PS[yf (x)  ]  2T
t=1

1t -(1 - t)1+

 · h(x) where f (x) =
1

8 9

10 Upper bounds based on margin
11

 1 d log2(m/d)

1/2

12

PD[yf (x)  0]  PS[yf (x)  ] + O m

2

+ log(1/) 

13

14

15

16

AdaBoost variants

Freund & Schapire 1995

1

2

Discrete (h : X  {0, 1}) 3

Multiclass AdaBoost.M1 (h : X  {0, 1, ..., k})

4

Multiclass AdaBoost.M2 (h : X  [0, 1]k)

5

Real valued AdaBoost.R (Y = [0, 1], h : X  [0, 1])

6

7 Schapire & Singer 1997
8

Confidence rated prediction (h : X  R, two-class)

9

Multilabel AdaBoost.MR, AdaBoost.MH (different formulation of minimized

10

loss) 11

... Many other modifications since then (Totally Corrective AB, Cascaded AB)

12

13

14

15

16

Pros and cons of AdaBoost

Advantages

1

2 Very simple to implement
3 Feature selection on very large sets of features
4

Fairly good generalization

5

Disadvantages

6

7 Suboptimal solution for ¯
8

Can overfit in presence of noise

9

10

11

12

13

14

15

16

Adaboost with a Totally Corrective Step (TCA)

Given: (x1, y1), ..., (xm, ym); xi  X , yi  {-1, 1}

1

Initialize weights D1(i) = 1/m

2

For t = 1, ..., T :

3

1. (Call WeakLearn), which returns the weak classifier ht : X  {-1, 1} with

4

minimum error w.r.t. distribution Dt;

5

2. Choose t  R,

6

3. Update Dt+1

7

4. (Call WeakLearn) on the set of hm's with non zero 's . Update ..

8

Update Dt+1. Repeat till | t - 1/2| < , t.

9

Comments

10

All weak classifiers have t  1/2, therefore the classifier selected at t + 1 is 11

"independent" of all classifiers selected so far.

12

It can be easily shown, that the totally corrective step reduces the upper

bound on the empirical error without increasing classifier complexity.

13

The TCA was first proposed by Kivinen and Warmuth, but their t is set as in 14

stadard Adaboost.

15

Generalization of TCA is an open question.

16

Experiments with TCA on the IDA Database

1

Discrete AdaBoost, Real AdaBoost, and Discrete and Real TCA evaluated

2

Weak learner: stumps.

3

Data from the IDA repository (Ratsch:2000):

4

Input

Training Testing Number of

5

Banana

dimension patterns patterns realizations

2

400

4900 100

6

Breast cancer 9

200

77

100

7

Diabetes

8

German

20

468

300

100

700

300

100

8

Heart

13

170

100

100

9

Image segment 18

1300 1010 20

10

Ringnorm

20

400

7000 100

Flare solar

9

666

400

100

11

Splice

60

1000 2175 20

12

Thyroid

5

140

75

100

Titanic

3

150

2051 100

13

Twonorm

20

400

7000 100

14

Waveform

21

400

4600 100

15

Note that the training sets are fairly small

16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)
4
the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

5

6

7

8

9

10

11

12

13

14

15

16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

IMAGE

5

0.4

6

0.35

7

0.3

8

0.25

9

10
0.2
11
0.15
12

0.1

13

0.05

14

0

100

101

102

103

Length of the strong classifier

15 16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

FLARE

5

0.46

6

0.44

7

8
0.42
9
0.4
10

0.38

11

12
0.36
13

0.34

14

0.32

100

101

102

103

Length of the strong classifier

15 16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

GERMAN

5

0.32

6

0.3

7

0.28

8

0.26

9

10
0.24
11
0.22
12

0.2

13

0.18

14

0.16

100

101

102

103

Length of the strong classifier

15 16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

RINGNORM

5

0.4

6

0.35

7

0.3

8

0.25

9

10
0.2
11
0.15
12

0.1

13

0.05

14

0

100

101

102

103

Length of the strong classifier

15 16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

SPLICE

5

0.25

6

7
0.2
8

9
0.15
10

11
0.1
12

13
0.05
14

0

100

101

102

103

Length of the strong classifier

15 16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

THYROID

5

0.25

6

7
0.2
8

9
0.15
10

11
0.1
12

13
0.05
14

0

100

101

102

103

Length of the strong classifier

15 16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

TITANIC

5

0.24

6

0.235

7

8

0.23

9

10
0.225
11

0.22

12

13
0.215
14

0.21

100

101

102

103

Length of the strong classifier

15 16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

BANANA

5

0.45

6

0.4

7

0.35

8

0.3

9

10
0.25
11
0.2
12

0.15

13

0.1

14

0.05

100

101

102

103

Length of the strong classifier

15 16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

BREAST

5

0.31

6

0.3

7

8
0.29
9
0.28
10

0.27

11

12
0.26
13

0.25

14

0.24

100

101

102

103

Length of the strong classifier

15 16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

DIABETIS

5

0.35

6

0.3

7

8
0.25
9
0.2
10

0.15

11

12
0.1
13

0.05

14

0

100

101

102

103

Length of the strong classifier

15 16

Results with TCA on the IDA Database

1

Training error (dashed line), test error (solid line)

2

Discrete AdaBoost (blue), Real AdaBoost (green),

3

Discrete AdaBoost with TCA (red), Real AdaBoost with TCA (cyan)

the black horizontal line: the error of AdaBoost with RBF network weak classifiers from (Ratsch-ML:2000)

4

HEART

5

0.35

6

0.3

7

8
0.25
9
0.2
10

0.15

11

12
0.1
13

0.05

14

0

100

101

102

103

Length of the strong classifier

15 16

Conclusions

1

The AdaBoost algorithm was presented and analysed

2

A modification of the Totally Corrective AdaBoost was introduced

3

4 Initial test show that the TCA outperforms AB on some standard data sets.
5

6

7

8

9

10

11

12

13

14

15

16

3

2.5

2

err

1.5

1

0.5

0

-2

-1.5

-1

-0.5

0

0.5

1

1.5

2

yf(x)

e 0.5
t

3

2.5

2

err

1.5

1

0.5

0

-2

-1.5

-1

-0.5

0

0.5

1

1.5

2

yf(x)

3

2.5

2

err

1.5

1

0.5

0

-2

-1.5

-1

-0.5

0

0.5

1

1.5

2

yf(x)

e 0.5
t

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 0

5

10

15

20

25

30

35

40

IMAGE 0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

FLARE 0.46

0.44

0.42

0.4

0.38

0.36

0.34

0.32

100

101

102

103

Length of the strong classifier

GERMAN 0.32

0.3

0.28

0.26

0.24

0.22

0.2

0.18

0.16

100

101

102

103

Length of the strong classifier

RINGNORM 0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

SPLICE 0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

THYROID 0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

TITANIC 0.24

0.235

0.23

0.225

0.22

0.215

0.21

100

101

102

103

Length of the strong classifier

BANANA 0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

100

101

102

103

Length of the strong classifier

BREAST 0.31

0.3

0.29

0.28

0.27

0.26

0.25

0.24

100

101

102

103

Length of the strong classifier

DIABETIS 0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

HEART 0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

IMAGE 0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

FLARE 0.46

0.44

0.42

0.4

0.38

0.36

0.34

0.32

100

101

102

103

Length of the strong classifier

GERMAN 0.32

0.3

0.28

0.26

0.24

0.22

0.2

0.18

0.16

100

101

102

103

Length of the strong classifier

RINGNORM 0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

SPLICE 0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

THYROID 0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

TITANIC 0.24

0.235

0.23

0.225

0.22

0.215

0.21

100

101

102

103

Length of the strong classifier

BANANA 0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

100

101

102

103

Length of the strong classifier

BREAST 0.31

0.3

0.29

0.28

0.27

0.26

0.25

0.24

100

101

102

103

Length of the strong classifier

DIABETIS 0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier

HEART 0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

100

101

102

103

Length of the strong classifier


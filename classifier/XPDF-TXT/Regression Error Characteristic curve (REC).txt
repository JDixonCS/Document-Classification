43

Regression Error Characteristic

Curves

Jinbo Bi Kristin P. Bennett
Department of Mathematical Sciences, Rensselaer Polytechnic Institute,

BIJ2~RPI.EDU BENNEK~RPI.EDU
Troy~ NY12180 USA

Abstract
Receiver Operating Characteristic (ROC) curves provide a powerful tool for visualiTr ing and comparingclassification results. Regression Error Characteristic (REC) curves generalize ROCcurves to regression. REC curves plot the error tolerance on the xaxis versus the percentage of points predicted within the tolerance on the y-axis. The resulting curve estimates the cumulativedistribution function of the error. The RECcurve visually presents commonly-usedstatistics. The area-over-the-curve (AOC)is a biased estimate of the expected error. The R2 value can be estimated using the ratio of the AOC for a given model to the AOCfor the null model. Users can quickly assess the relative merits of manyregression functions by examining the relative position of their REC curves. The shape of the curve reveals additional information that can be used to guide modeling.
1. Introduction
Receiver Operating Characteristic (ROC)curves have proven to be a valuable way to evaluate the quality of a discriminant function for classification problems (Egan, 1975; Swets et al., 2000; Fawcett, 2003). ROC curves address manyof the limitations of comparing algorithms based on a single misclassification cost measure (Provost et al., 1998). AnROCcurve characterizes :the performanceof a binary classification model across all possible trade-offs betweenthe false negative and false positive classification rates. AnROCgraph allows the performanceof multiple classification functions to be visualized and comparedsimultaneously. ROCcurves can be used to evaluate both expected accuracy and variance information. ROCcurves are consistent for a given problemeven if the distribution

of positive and negative instances is highly skewed. The area under the ROCcurve (AUC)represents the expected performance as a single scalar. The AUChas a knownstatistical meaning: it is equivalent to the Wilcoxontest of ranks. Fundamentalsof interpreting ROCcurves are easily grasped. ROCcurves are effective tools for visualizing results for non-experts as well as experts and help them make more valid conclusions. For example, a non-expert can see that two functions have similar ROCcurves and can conclude that there is no significant difference betweenthe functions even though one mayhave a larger classification cost. Currently ROCcurves are limited to classification problems.
The goal of this paper is to devise a methodologyfor regression problems with similar benefits to those of ROCcurves. Our solution, the Regression Error Characteristic (REC)curve, plots the error tolerance on the x-axis versus the percentage of points predicted within the tolerance on the y-axis. Theresulting curve estimates the cumulative distribution function (CDF) the error. The error here is defined as the difference betweenthe predicted value f(x) and actual value y response for any point (x, y). It could be the squared residual (y - f(x)) 2 or absolute deviation lY - f(x)l depending on the error metric employed. Figure 1 provides an example of RECcurves generated for industrial data. See Section 3 for moredetails.
RECcurves behave much like ROCcurves.
¯ RECcurves facilitate visual comparisonof regression functions with each other and the null model.
¯ The curve area provides a valid measureof the expected performance of the regression model. The RECcurve estimates CDFof the error. The area over the curve (AOC)is a biased estimate of the expected error.
¯ The RECcurve is largely qualitatively invariant to choices of error metrics and scaling of the resid-

Proceedingsof the Twentieth Inter'national Conferenceon MachineLearning(ICML-2003)V, ~rashin~onDC,2003.

44

O.g
0.8 0.7 0.6
~ 0.5
< 0.4 0.3 0.2 0.1 0

~r / i" ¯ "j , -).:
0 : L .
mean(0.46898) SVM(10.22087) SVM(20.23462) SVM(30.27901) PLS(0.24517) KPLS(0.24517)
o., 0:6 0:8 ; ,:2
Absoludteeviation

Figure 1. RECcurve comparingresults of five models with the ,roll (mean) modelon real-world data.

1.0
o,t/A .............. ~ ...-~j......

..."..................

o ~.~ o.i[~,..,,,.o2[/ ........

o.,

o.~ ~

Falsepositiverate

Figure 2. Sample ROCcurves: A = almost perfect E = null model.

model.

hal. Scaling the response does not change the graph other than the labeling of the x-axis. Using various error metrics, such as the absolute deviation or squared error, does change the RECcurve, but the relative position of curves does not change qualitatively. A function that dominates another using the squared error will also dominate the alternative using the absolute deviation.
¯ RECcurves provide an effective strategy for presenting results to non-experts. One can readily see when regression fimctions are alike and when they are quite different. In our industrial consulting practice, REC curves provide a much more compelling presentation of regression results than alternatives such as tables of meansquared errors.
¯ The information represented in RECcurves can be used to guide tim modeling process based on the goals of the modeler. For example, good choices of e for the E-insensit.ive loss function in the SVMregression can he found using the curves.
This paper is organiz2d as follows. Webegin with a brief review of ROCcurves. In Section 3, we define what an REC curve is and give an example of how it is interpreted based on real world data. Weexamine the area-over-the-curve statistic in Section 4 and show howit provides a biased estinmte of the expectation of the error. In the next two sections, we explore the properties of RECcurves and their potential use by investigating curves for synthetic data with known characteristics. Weconclude with a summary and future work in the last section.

2. Review of ROC
Webriefly review the properties of ROCcurves. Readers should consult (Fawcett, 2003) for an excellent tutorial on ROCgraphs. For two-class discrimination problems, the ROCcurve for a discriminant function is constructed by varying the threshold or probability used to discriminate between classes for that function. The resulting pairs of false positive rates and true positive rates are plotted on the x and y axes respectively with lines interpolating between them.
Figure 2 illustrate~s ROCcurves plotted for several classification models. A classifier performs well if the ROCcurve climbs rapidly towards the upper left-hand corner. Function A in Figure 2 is an almost perfect classifier. The expected performance of a classifier can be characterized by the area under the P~OCcurve (AUC). The AUCfor a perfect classifier is 1. Random guessing would yield the diagonal line (labelled as E) with the AUCequal to 0.5. No valid classifier should have an AUCbelow 0.5. A classifier is said to dominate an alternative classifier if the corresponding curve is always above that for the alternative. In Figure 2, function B dominates functions C and D, and thus would be preferable. Function B is better than functions C and D for any choice of the cost function. Correspondingly, the AUCfor function B is larger than for functions C and D. Fnnction D does not dominate fimction C: but it has a higher AUCand is preferable overall. However, function C may be preferable if a low false positive rate is desirable. Note that error bars can also be included in ROCcurves to indicate variance information. We save presentation of ROC and RECcurves with error bars for future work.
Our goal is to develop a curve Lhat analogously characterizes the quality of recession models and main-

45

tains the benefits of ROCcurves. In regression, the analogousconcept to classification error is the residual y- f(x). Like error rates in classification, existing measuresof residuals such as meansquared error, meanabsolute deviation, _tt~ 2 and Q2, provide only a single snapshot of the performance of the regression model. In addition, performance metrics based on the absolute or squared residuals mayproduce different rankings of regression functions. Scatter plots of the predicted value versus the actual value allow users to visualize function performance across the range of data, but they are not practical for visualizing the performance of multiple functions simultaneously.
Typically, weprefer a regression function that fits most data within a desirable error tolerance. As in I~OC curves, the graph should characterize the quality of the regression modelfor different levels of error tolerance. The best function depends on the goals of the modeler. Wemight want to choose a model that fits almost all data with a loose tolerance versus another model with a tight tolerance that suffers catastrophic failures on a few points. IdeAlly, a single statistic such as AUCshould exist to provide a measureof expected performancewith well-understood statistical meaning. Preferably this statistic should correspondto an easily assessed geometric quantity such as an area.
3. Definition of REC Curves
Regression Error Characteristic (REC) curves meet these goals. RECcurves plot the error tolerance on the x-axis and the accuracy of a regression function on the y-axis. Accuracyis defined as the percentage of points that are fit within the tolerance. If wehave zero tolerance, only those points that the function fits exactly would be considered accurate. If we choose a tolerance that exceeds the maximumerror observed for the modelon all of the data, then all points would be considered accurate. Thus there is a clear trade-off between the error tolerance and the accuracy of the regression function. The concept of error tolerance is appealing because most regression data are inherently inaccurate, e.g. due to experimental and measurement errors.
RECcurves were initially motivated by the einsensitive loss function used in SVMregression methods (Vapnik, 1995). The e-insensitive loss (max{0, f(x)l- e}) has proven to be very robust, but there is no a priori way to pick e. The RECcurve based on absolute error metric considers an e-insensitive loss for all possible values of e - muchthe same as ROC curves plot the results for all possible misclassifieation costs. Further justification for this evaluation ap-

proach comes from statistics. The RECcurve is an estimate of the CDFof the error. This enables us to estimate the expected errors of various models.
Thebasic idea of the e-insensitive loss function is that residuals must be greater than a tolerance e before they are considered as errors. Supposewehave a set of m data points, (xl, Yl),.-., (Xm, Ym), wn here xi E R are the independent variables and y E R is the dependent or response variable. Wedefine the accuracy at tolerance e as:
acc(e:)=I{(x,y):loss(/(x), y0<e, =1,..-,m)l
m
Whenusing the squared error metric, the loss is defined as loss(f(x),y) --- (f(x) - y)2. When solute deviation is adopted, the loss is defined as loss(f(x),y) = If(x) - Yl. The RECcurve is structed by plotting ~ versus acc(e). The following algorithm is used to plot an RECcurve. Percentile plots can also be used to draw RECcurves, which is not discussedin this article.
REC Plot Algorithm Input: e~ = loss(f(xi), Yi), i -- 1,...,m. Weassume that the errors ei are sorted in ascending order and the "plot" commandinterpolates between the plotted points with a line. 1. eprev := O, correct := 0; 2. fori = 1tom 3. if ei > eprev then 4. plot(eprev, correct~m)
5. £prev :----- £i
6. end 7. correct := correct + 1 8. end 9. plot(era, correct~m)
As e increases, the accuracy increases. The acc(e) eventually goes to 1 when e becomes large enough. A practical problem is how to select the range of e whendrawing the I~ECplots for multiple models. The range is problem-specific even if the data are normalized. The range of e adjusts the appearance of REC curves whenwe draw curves for different modelsin one box. For a very wrongmodel, to achieve accuracy 1, we have to use large e. If the range is selected too large, the RECfigure becomesdifficult to read since the curves corresponding to better models will cluster in the upper left-hand corner. These are precisely the curves of interest since the corresponding models perform well.
To overcomethis difficulty wescale the RECcurve using a null model.In this paper the null modelis a constant function with the constant equal to the meanof

46

tile response of the training data, f(x) = 9. This mean Their MSEsare not. identical but there is no real difmodel is the best constant model assuming Ganssian ferenc~ in their performance. In this case, SVM1was noise. If the noise follows a Laplacian distribution, a hand tuned hased on testing information in order to

constant model equal to the median of the response estimate the be.st possible performance. Modelparamof the training data maybe preferable. In this paper eters for SVMw2 ere selected using cross-validation.

we only showresults for the null modelbased on the The KPLSparameter (number of latent variables) was

mean.Howeverdifferent null models can be defined if selected by a fixed policy (Rosipal &Trejo, 2001). One

priori knowledgeon the noise modelis accessible for could use this picture to argue with the managerthat

a problem at hand, and then RECcurves can be ex- SVM2and KPLSare doing almost ms well as the best

tended to scale with different null models.

possible model SVM1.SVM3illustrates what happens

Reasonable regression approaches produce regression models that are better than the null model. An REC graph looks like a rectangle box that contains several monotonically increasing curves (RECcurves) each corresponding to a regression model. The x-axis of

if a poor choice of the SVMparameters is made. Other experiments illustrated that KPLSwas faster and less sensitive to tuning. Based on the RECcurves, one might conclude that KPLSis a preferable method on these datasets.

the box usually starts with 0 since e E 0. Wedefine

the other end of the x-axis as the largest value of the 4. Area Over the REC Curve

errors, ei, obtained by the meanmodelon the sample data, denoted by ~. If on the sample data, a given modelachieves accuracy of 1 for e <: ~, then the full RECcurve is plotted within the box for that model. If the smallest value of e where the model achieves accuracy 1 is greater than ~., then the corresponding I~ECcurve is tr~mcatedat g. Henceit is possible that an RECcurve maynot reach accuracy 1 if the model performs worse than the mean model at high errors, whichis a sign of a poor model. Since we focus on the analysis of goodcurves, wealways plot from 0 to g.
Figure 1 illustrates the RECcurves plotted for five different models and the null modelfor a proprietary

In ROCcurves the area under the ROCcurve provides an estimate of the expected accuracy. Correspondingly the area over the RECcurve (AOC)is nmasureof the expected error for a regression model. If we think of the error as a randomvariable, the RECcurve is an estimate of the cumulative distribution function of this variable. Wenowshowthat calculating the expected value based on this estimate of the error probability function is equivalent to evaluating the area over the curve. Wealso showhowAOC can be used to estimate other commonsample or test statistics for regression models.

industrial problem. The numbers in the legend give AOCvalues for each model. Wepresent the results based on 50 randompartitions of data into 90%train-
ing and 10%test. Theresults for the 50 trials are av-
eraged by the simple strategy of including the results for every test point from every trial in the RECcurve.
This method is commonlyused in ROCcurves. However any alternative methodlike error bars for presenting cross-validated results developed for ROCcurves

4.1. Area Calculates Expected Error
For a given model, the error calculated using the squared error or absolute deviation, can be regarded as a randomvariable e > 0. The cumulative distribution of this randomvariable is defined as the probability of ¢ _< ~. Denote the CDFof the variable ~ ms P(e) and the correspondingprobability density fimction as p(¢). The RECcurve represents the empirical CDFofe with

could also be used. See (Fawcett, 2003) for a survey. The models were constructed using classic recession SVMtrained with different values of e (Vapnik, 1995), linear partial least squares (PLS) (Wold, 1966), kernel partial least squares (KPLS)(Rosipal &Trejo, 2001)1. All methods produced reasonable results that outperformed the null model. The results show that the SVM1model performs better than the rest. The KPLSand the SVM2models perform very similarly.

the probability P estimated by the frequency /3 for ei based on the training data. Assumethe errors ei observed on the mtraining points are sorted in ascending
order. Then e.~n is the maximumobserved error, and the frequency [~(ei) = i/m. By the Glivenko-Cantelli lemma,]5(e) converges to P(e) uniformly over all
ues of e (DeGroot, 1986). Wegive a rough sketch the argument about AOCassuming tim full RECcurve can be plotted in the figure, in other words, e,,, < i.

~Ttmresults wereobtained using the AnalyzeTM softwarecreated by MarkEmbrectltsat RensselaerPolytechnic Institute and distributed at www.drugrnining.comRe. sults are for RECdemonstrationonly so modelingdetails are
not provided.

The expectation of e is defined as E(e) = f ep(e)de. For any maximumobserved error e,, > 0, we can break the integral into two parts:

47

The sample max em converges to infinity or the maximumvalue of e if it exists, so the second term converges to 0 as the sample size m --* oc. Define
A(~i = ~i -- ei-1 and wi E (ei-1, ei). AssumeAei -o as rn --* c~. The expected value of ~ is equal to the R, iemann integral

the AOCstatistic can always be calculated using the sample ei even though they may not be plotted in the figure. Note AOCis a biased estimate since it always underestimates the actual expectation due to the drop of the second term of the integral (1). For good models

rt/

=

(2)

i=1

By the Mean Value Theorem, we can always choose wi E (ei-1, ei) such that

0.60"8~ "''''i/* "~" '''4"" ./" "

= P(ei) - P(ei-1) Ci -- £i-1

The above equation can thus be rewritten as Pi (ei)-P(e4-1)A~
E(~) --- ,~-l-i,mooi=Llmw4 e4 - e4-1 (3)
= l~iTmt'-~OZ0 wi(P(c,) - P(e,-1)).
4----1
The probability distribution P at e is unknown, but we can approximate this quantity based on the finite sample. The infinite series (3) can be truncated at m equal to the sample size. But this approximation underastimates E(e) since it does not estimate the portion integral (1) corresponding to e >em for the finite sample. Weuse the empirical distribution/5 estimated on the sample data to approximate P and take w4 -- e4,

0o

E

Figure 3. A RECcurve with em <_ ~. The area over the curve (AOC)estimates the E(e).

(better than the mean model in our illustration), the AOCis a reasonable estimate of the expected error. As the amount of data tends to infinity, AOCconverges to the expectation of e. Therefore we include AOCinformation in REC plots. In all RECfigures, the numbers in the legend represent the AOCvalues corresponding to each model.

Table 1. Comparison of AOCestimates and Mean estimates of the expected error E(e).

m

4----1 m--1

=

+ Y](4

-

(4=1
; mPm( )

--

+

)

\ i=l

where c0 = 0. The first term computes the area of the entire region in the box corresponding to this REC curve since fi(em) = 1. This is shown in Figure 3 as
the area in the left shadowed rectangle. The terms i2n the parentheses evaluate the area under the curve. Therefore E(e) can be approximated by the area over the curve (AOC) within the box as shown in Figure This analysis assumes that the entire RECcurve can be plotted. If a model has e,~ > ~, the RECcurve will be truncated. The area observed in the plot does not correspond to the full area over the curve. However

2Weactually calculate the area under the curve by the trapezoidal rule. The same analysis holds but is slightly more complicated for that case.

SE MODEL AOC MSE

SVM1
PLS MEAN

0.0875 0.0888 0.0991 0.1004
0.3548 0.3570

AD AOC MAD
0.2209 0.2215 0.2452 0.2463 0.4690 0.4704

Certainly there are other more commonestimates of E(e). For example, the sample mean, ~ ~i~1 el, also estimates the expected error. If e is calculated using the absolute deviation (AD), then the AOCis close the mean absolute deviation (MAD). If E is based the squared error (SE), the AOCapproaches the mean squared error (MSE). Table 1 illustrates the difference between AOCand mean estimates of the SE and AD on the industrial data presented in Figure 1. We can clearly see that the AOCestimate is biased because it always produces a lower estimate of the expected error than the MSEor MAD.The REC curve offers an alternative error estimate that can be visualized directly and that has potential of providing additional information.

48

4.2. Estimatlng Other Statistics
More statistics can be represented by RECcurve graphs. For RECcurves with the SE, the AOCfor the meanmodelis an estimate of the variance of the response y since it evaluate~ tile expectation El(y-~)2]. Oncewehave an estimate of the response variance, we can estimate other commonly-usedstatistics such as Q2 and R2. Widely used in chemometrics, Q2 is the ratio of the MSEover the sample re.sponse variance, so Q2can also be estimated using AOCby
Q2 = E,~=I(Y, - f(x~)) 2 AOCmodel ~;nl(Yi -- ~)2 ~AOC..... " (4)
This yields an estimate of R2 as well because R2 is often defined as 1 - Q2. Onecould also evaluate equivalent statistics defined based on the AD.
The significance of difference betweentwo RECcurves can be assessed by examining the maximumdeviation betweenthe twocurves across all values of E. This corresponds to the Kolmogorov-Smirnov(KS) two-sample test (DeGroot, 1986) for judging the hypothesis that the error e generated by two models f and g follows the same distribution. The KS-test requires no assumptions about the underlying CDFof z. Since I%EC curves represent the sample CDF/5(~), the statistics used in the KS-test such as D+ = supe(/5/(e)-/St(e)), D- = sup~(Pg(e) - P/(c)) and D = max{n+,D-}, can be estimated and visualized by the maximumvertical distance betweenthe two RECcurves corresponding respectively to the models f and g.

were used to construct y and the remaining 10 dimensions are just noise. The goal was to examinehowREC curves vary wheny is disturbed by additive noise. The
response y is const,'ucted via y = 0.5 ~°1 xi+{ where is the additive noise. Several noise modelswerecon-
sidered: Gaussian, Laplacian, and Weibull (DeGroot,
1986). To save space, Weibull noise data will be analyzed in the next section not here. Intuitively, the
distributio,~ of the residual dependson the regression modelf and the noise variable {. Figures 4 illustrates
RECcurves produced for the data with Gaussian and
Laplacian additive noise of mean0 and standard deviation 0.8. Each plot considers four functions: tile
true model0.5 y~}°1 xi, the mean(null) model, a randora model ~i=10l w=ixi where wi are randomly generated from [0, 1], and a biased model0.5 ~i1=01 x~ + 1.5. Tile AOCvalues corresponding to each curve are also presented beside the curves or in the legends.

Hencethe RECcurve facilitates the visualization of all these statistics simultaneouslyfor manyregression functions in a single graph. To deliver the same information, one wouldhave to examinelarge tables of
results and do mental calculations. Moreover, REC curves have the benefit of providing additional informationbesides all individual statistics.

Figure 4. RECcurves with Gaussian noise (above) and Laplaciannoise (below). Left: AD,right: SE.

5. Noise and Loss Models
Weexplore the relationship between the noise model and error metrics by investigating RECcurves on synthetic data with knownnoise distribution. The goals here are to examine if the model evaluation based on RECcurves is sensitive to choices of error metrics, and if RECcurves can help identify characteristics of regression models. Wedraw the RECcurves based on both SEand ADto illustrate the effect of error metrics on the appearance of RECcurves.
All experiments were conducted using 200 points randomlygenerated in a 20-dimensional space from a uniform distribution on [-1, 1]2°. Thefirst 10 dimensions

As expected: the true model dominates the other models. Similar to ROCcurves, an RECcurve dominates another if it is always above the other. The mean model dominates the random model indicating that the random model is poor. Analogous to the ROC curve: a random model can provide the bottom line for comparison although we use the mean model g as the worst ease in comparison rather than a random guess. The biased modelinitially performs the worst, but once the error tolerance is large enough, it outperforms the meanmodel. Laplacian noise generates moreoutliers than Gaussian noise. Whenoutliers are present, the top of the RECcurve will be flat and not reach 1 until the error tolerance is high. This can be observed in Figure 4(below) for Laplacian noise data.

49

1 o.9 o.8
03
~,o,5 o.s
8
~ o.4

B ~~'~I

f-"

i ..

ii it

~.;.

At")

i ¯ ,['"

!:
! .I . IJ :

a . ol ~i o.1
o'.5;

r l

..m..esavnm11-r.~214.58(7o1.9133)

I m-- LS10.5710)

I

,'.5 = =5 5 55 , ,.5
Absolutedeviation

i. ......-
~ ..': i .s.i :
~.....

o~ o8] o.71

- rr~an(1.2457)

-. ¯ t,.oe(0.c~927)

-- SVM~=0.8(0.66515)

.... $VM¢=1.810.91333)

.... LS10.57103)

o.,I

I 1,5 2 2.5 5 3,5 4 4.5 Absolutedeviation

0.5 1 1.5 2 2.5 3 8.5 4 Absolutedeviation

Figure 5. RECcurves with Laplacian noise for trained models. Left: RECcurves based on training data for LS and SVM(E = 1.8) models. Middle: Training RECcurves including the new SVMmodel with c ---- 0.8. Right: RECcurves generated based on test data for various models.

The curves for the biased model are of particular interest. Note how quickly the curve for the biased model climbs in the middle on contrary to the fiat part at the lower end. This characteristic nonconvex behavior of the RECcurve is caused by the position of the model relative to data. For example, the regression model is about in the middle of the data, but the data is far from the regrea~ion function on both sides. This case is rare in practice. More commonly: this kind of curve results from a bia.sed model for which the data largely lies on one side of the regression model rather than the other side. The biased model will exhibit poor accuracy when the tolerance e is small, but once e exceeds the bias the accuracy rapidly increases. Hence this characteristic shape in the RECcurve indicates that the model is likely biased. If the model should be unbiased, this could be a sign of potential problems. In SVMs, a bias can be introduced intentionally via the e-insensitive loss fimction. As observed in Figure 6, the REC curve for the SVMmodel (e = 0.1) has slight nonconvexity at the low end.
The same conclusion would be drawn for each noise model according to REC plots based on both AD and SE. Hence the evaluation of regression models using RECcurves is qualitatively invariant to the choices of error metrics. We prefer the AD-based REC curves because the plots tend to be more spaced-out and are convenient to distinguish the trend of different curves. In the next section, we only show AD-based plots.
6. Potential Use of RECCurves
RECcurves can help determine an appropriate e value for the e-insensitive loss in SVMsI. f the data is largely distributed within a band of width e about the true model with only a few points lying outside, the einsensitive loss is preferred in modeling. On this data,

a good regression model can fit the majority of data within a relatively small tolerance c. We expect a sharp jump in the RECcurve around the e value used in the insensitive loss. The curve becomesfiat after it hits that e value. Hence we can see an abrupt bend in the trend of the RECcurve, especially in SVMtraining I~EC curves as shown in Figure 5(middle).
Figure 6. RECcurves on Weibull noised data for trained models. Left: training, right: test.
We examine how RECplots could assist in finding a proper c value using synthetic data with Laplacian noise. Figure 5 presents R.EC curves for least squares (LS) and SVMmodels trained using 50 points (left middle) and tested on 150 points (right). Initially LS model performed better than SVMwith parameter e = 1.8. Inappropriate choices of e in the insensitive loss can hurt the performance of SVM.Figure 5(left) shows that the LS RECcurve exhibits a dramatic change of slope at about ~ = 0.8. To show this we draw two straight lines roughly tangent, to the lower part of the curve (the line A) and the high end of the curve (the line B), respectively. The two lines cross at about e = 0.8. The similar behavior can be also observed on the RECcurve for SVM(~ = 1.8). Based on this information observed on training I'¢EC curves, we trained a SVMwith e ---- 0.8 in the loss function.

5O

The new curve for SVM(e = 0.8) in Figure 5(middle) shows a significant enhancementin the SVMtraining accuracy. The generalization performancewasalso im-

consulting tables or other graphs. Experienced modelers can exploit the additional information contained in the RECcurve beyondsimple statistics. These proper-

proved for SVM(e. = 0.8) as shownin Figure 5(right). ties can be used to help diagnose problems with mod-

Thesametrick was also applied to synthetic data gen- els and to guide modelselection. For example, we use

erated with Weibull-distributed noise where the sign RECcurves to select values of the ¢.-insensitive loas of the noise was selected using a coin toss. The SVM function in regression. RECcurves can potentially

with e = 0.1 shownin Figure 6 chosen in the above way profit from the innovations and variations developed outperforms the LS model and the other SVMmodel. for ROCcurves. Different methodsfor averaging mod-

Weleave automating and validating this heuristic for els and incorporating error bars to represent variance selecting the e parameter in SVMtso fllture research. can be used. Theconvexhull of the set of points in the

Single statistim such as MSEand MADmay produce RECspace maybe useful for constructing ensembles different rankings whencomparingtwo models. In this of regression functions.

ease, the corresponding two RECcurves must cross.

Let. the best modelselected by MSEdiffer from that Acknowledgements

of MADS. uppose the curves based on the ADdo not cross, in other words, curve 1 is always abovecurve 2. Let r1 and r2 be the two vectors of absolute residuals corresponding to the two models sorted in as-

This work was supported by NSFgrant IIS-9979860. Manythanks to the reviewers for their vahmblesuggestions.

tending order. Since curve 1 dominates curve 2, it

implies (r~) 2 _<

1r(ri/<2)ri22, .1

< i Thus

< m, which in turns RECcurves based on the

implies SE also

do not cross. In addition, we have ~ r1 _< y~. r2 .and ~(r/1) ~ _< ~(ri2) 2, so the MADand MSEprovide the

References
DeOroot,M.H. (1986). Probability and statistics. MA: Addison-Wesley.

sameranking. By contradiction, the curves must cross Egan, J. P. (1975). Signal detection theory and ROC

if models are ranked differently by MSEand MAD.

analysis. In Series in cognition and perception~ New

If two RECcurves cross, howshould we rank the two

York:Scientific Press.

models? One solution is to evaluate the AOCvalues for the two models. If AOCvalues based on AD and SE both show preferences for the same model, we can trust this ranking. If the preferences are different, then the MSEand MADrankings will differ as well.

Fawcett, T. (2003). ROCgraphs: Notes and practical considerations for data mining researchers (Technical Report HPL-2003-4). Hewlett Packard, Palo Alto, CA.

Thus the ranking has to rely on moreinformation re- Provost, F., Fawcett, T., & Kohavi, R. (1998). The

vealed by RECvisualization. Moreover, there maybe

case against accuracy estimation for comparingin-

no significant difference in the curves. The maximum duction algorithms. Proceedingsof the Fifteenth In-

vertical distance between two RECcurves represents

ternational Conference on Machine Learning (pp.

the KS-test statistic used to assess the significance of

445-453). San Francisco: MorganKaufinann.

the difference betweenthe two error distributions. We leave investigation of the KS-test in RECcurves to future research.

Rosipal, R., & Trejo, L. (2001). Kernel partial least squares regression in reproducingkernel hilbert space. Journal of MachineLearning Research, 2, 97-

123. 7. Conclusions

Weproposed a newtechnique for evaluation and comparison of regression models. RECcurves depict the trade-off betweenerror tolerance versus the accuracy

Swets, J. A., Dawes, R. M., & Monahan, J. (2000). Better decisions through science. Scientific America, 283, 82-87.

of the functions. The RECcurve is an estimate of Vapnik,V. N. (1995). The nature of statistical learning

the error cumulative distribution function. Commonly theory. NewYork: Springer.

used measures of the distribution can be estimated using the geometryof the figure. For examplethe area
over the curve is a biased estimate of the expectederror. Using RECcurves, non-experts can quickly evaluate the relative merits of regression functions without

\Void, H. (1966). Estimation of principle components and related models by iterative least squares. Mul-
tivariate Analysis (pp. 391-420). NewYork: Academic Prexs.


NEURAL NETWORKS FOR OPTIMAL APPROXIMATION OF SMOOTH AND ANALYTIC FUNCTIONS
H. N. Mhaskar Department of Mathematics, California State University
Los Angeles, California, 90032, U.S.A.
Neural Computation, 8 (1996), 164- 177.
Abstract. We prove that neural networks with a single hidden layer are capable of providing an optimal order of approximation for functions assumed to possess a given number of derivatives, if the activation function evaluated by each principal element satisfies certain technical conditions. Under these conditions, it is also possible to construct networks that provide a geometric order of approximation for analytic target functions. The permissible activation functions include the squashing function (1 + e-x)-1 as well as a variety of radial basis functions. Our proofs are constructive. The weights and thresholds of our networks are chosen independently of the target function; we give explicit formulas for the coefficients as simple, continuous, linear functionals of the target function.
1. Introduction. In recent years, there has been a great deal of research in the theory of approximation of real valued functions using artificial neural networks with one or more hidden layers, with each principal element (neuron) evaluating a sigmoidal or radial basis function ([1, 2, 3, 5, 7, 8, 9, 15, 17, 18]). A typical density result shows that a network can approximate an arbitrary function in a given function class to any degree of accuracy. Such theorems are proved for instance in [5, 8] in the case of sigmoidal activation functions and in [16, 19] for radial basis functions. Very general theorems of this nature can be found in [9, 12].
A related important problem is the complexity problem; i.e., to determine the number of neurons required to guarantee that all functions, assumed to belong to a certain function class, can be approximated within a prescribed accuracy, . For example, the now classical result of Barron [1] shows that if the function is assumed to satisfy certain conditions expressed in terms of its Fourier transform, and each of the neurons evaluates a sigmoidal activation function, then at most O( -2) neurons are needed to achieve the order of approximation . An interesting aspect of this result is that the order of magnitude of the number of neurons is independent of the number of variables on which the function depends. Other bounds of this nature are obtained in [13] when the activation function is not necessarily sigmoidal.
A very common assumption about the function class is defined in terms of the number of derivatives that a function possesses. For example, one is interested in approximating all functions of s real variables having a continuous gradient. By a suitable normalization, one may assume that the gradient is bounded by 1. It is known (e.g. [6]) that any reasonable approximation scheme to provide an approximation order for all functions in this class must depend upon at least ( -s) parameters. In [10], we showed how to construct networks with two hidden layers, each neuron evaluating a bounded sigmoidal function, to accomplish such an approximation order with O( -s) neurons. Along with Micchelli [14] we have studied this problem in much greater detail. The best result known so far for networks with a single hidden layer is that O( -s-1 log(1/ )) neurons are enough if the activation function is the squashing function 1/(1 + e-x). In our work with Chui and Li [4] we have shown that if s > 1 and the approximation is required to be "localized", then at least ( -s log(1/ )) neurons are necessary, even if different neurons may evaluate different activation functions. A detailed discussion of the notion of localized approximation is not relevant within the context of this paper; we refer the reader to [4]. We made a conjecture in [11] that with a sigmoidal activation function, the number of neurons necessary to provide the approximation order to all functions in this class, with or without localization, cannot be O( -s).
In this paper, we disprove this conjecture. We prove that if the activation function satisfies certain technical conditions then the optimal order of approximation for this class (and other similar classes) can be achieved with a neural network with a single hidden layer. Our results will be formulated for neural networks more general than the traditional networks evaluating a univariate activation function. In particular, our results will include estimates on the order of approximation by generalized regularization networks introduced
1

in [7, 17, 18]. The precise definitions and results will be given in the next section. The proofs of all the new results in Section 2 will be given in Section 3.

2. Main Results. Let 1  d  s, n  1 be integers, f : Rs  R and  : Rd  R. A generalized

translation network with n neurons evaluates a function of the form

n k=1

ak (Ak (·)

+

bk

)

where

the

weights

Ak's are d × s real matrices, the thresholds bk  Rd and the coefficients ak  R (1  k  n). The set

of all such functions (with a fixed n) will be denoted by ;n,s. We are interested in approximating the target function f by elements of ;n,s on [-1, 1]s. In the case when d = 1, the class ;n,s denotes the
outputs of the classical neural networks with one hidden layer consisting of n neurons, each evaluating the

univariate activation function . In [7, 17, 18] Girosi, Poggio and Jones have pointed out the importance of

the study of the more general case considered here. They have demonstrated how such general networks arise

naturally in applications such as image processing and graphics as solutions of certain extremal problems.

Our approximations will not be constructed as in [7, 17, 18] as solutions of extremal problems, but rather will

be given explicitly. They will not provide the best approximation, but will nevertheless provide the optimal

order of approximation.

An additional advantage of our networks is that the weights Ak's and the thresholds bk's will be

determined independently of the target function f . We observe in this connection that the determination of

these quantities is typically a major problem in most traditional training algorithms such as backpropagation.

In fact, the only "training" required for our networks consists of evaluating the coefficients ak. We give

explicit formulas for these coefficients as linear combinations of the Fourier-Chebyshev coefficients of the

target function. Alternative formulas based on the values of the target function can also be given, but we

do not present these alternative constructions here, since a good discussion of this issue would require us to

elaborate upon some very techincal background material. From a practical perspective, we observe that we

are assuming that the target function can be sampled without noise at prescribed points. Our constructions

are extremely simple, use no optimization, and avoid all the problems, for example, local minima, stability,

etc., associated with the classical, optimization-based training paradigms such as backpropagation. We fully

expect the constructions to be robust under noise, but have not developed any theory to deal with this

question.

First, we introduce some notations. If A  Rs is Lebesgue measurable, and f : A  R is a measurable function, we define the Lp(A) norms of f as follows.

(2.1)

 

1/p
|f (x)|pdx ,

||f ||p,A

:=



A
ess sup

|f (x)|,

xA

if 1  p < , if p = .

The class of all functions f for which ||f ||p,A <  is denoted by Lp(A). It is customary (and in fact essential from a theoretical point of view) to adopt the convention that if two functions are equal almost everywhere in the measure-theoretic sense then they should be considered as equal elements of Lp(A). We make two notational simplifications. The symbol L(A) will denote the class of continuous functions on A. In this paper, we have no occasion to consider discontinuous functions in what is normally denoted by L(A), and
using this symbol for the class of continuous functions will simplify the statements of our theorems. Second, when the set A = [-1, 1]s, we will not mention the set in the notation. Thus, ||f ||p will mean ||f ||p,[-1,1]s etc. We measure the degree of approximation of f by the expression

(2.2)

E;n,p(f ) := inf{||f - g||p : g  ;n,s}.

The quantity E;n,p(f ) denotes the theoretically minimal error that can be achieved in approximating the function f in the Lp norm by generalized translation networks with n neurons each evaluating the activation function . The complexity problem is clearly equivalent to obtaining sharp estimates on E;n,p(f ).
In theoretical investigations of the degree of approximation, one typically makes an a priori assumption that the target function f , although itself unknown, belongs to some known class of functions. In this paper, we are interested in the Sobolev classes, which we define as follows. Let r  1 be an integer and Q be a cube in Rs. The class Wrp,s(Q) consists of all functions with r - 1 continuous partial derivatives on Q which in
2

turn can be expressed (almost everywhere on Q) as indefinite integrals of functions in Lp(Q). Alternatively, the class Wrp,s(Q) consists of functions which have, at almost all points of Q, all partial derivatives up to order r such that all of these derivatives are in Lp(Q). The Sobolev norm of f  Wrp,s(Q) is defined by

(2.3)

||f ||Wrp,s(Q) :=

||Dkf ||p,Q

0kr

where for the multi-integer k = (k1, . . . , ks)  Zs, 0  k  r means that each component of k is nonnegative

and does not exceed r, |k| :=

s j=1

|kj |

and

Dkf

=

|k|f xk11 · · · xkss

,

k  0.

Again, Wr,s(Q) will denote the class of functions which have continuous derivatives of order r and lower. As before, if Q = [-1, 1]s, we will not mention it in the notation. Thus, we write Wrp,s = Wrp,s([-1, 1]s) etc.
Since the target function itself is unknown, the quantity of interest is

(2.4)

E;n,p,r,s := sup{E;n,p(f ) : ||f ||Wrp,s  1}.

We observe that any function in Wrp,s can be normalized so that ||f ||Wrp,s  1. Hence, E;n,p,r,s measures the "worst case" degree of approximation by generalized translation networks with n neurons under the assumption that f  Wrp,s and is properly normalized.
Since any element of ;n,s depends upon (ds + d + 1)n parameters, the general results by DeVore,
Howard and Micchelli [6] indicate that

(2.5)

E;n,p,r,s  cn-r/s

The general results in [6] are not exactly applicable here since the definition of the degree of approximation does not preclude the possibility that the parameters involved in the approximation may be discontinuous functionals on the class in question. Therefore, (2.5) is only a conjecture, rather than a known fact. In our constructions below, the parameters are continuous functionals of the class and hence (2.5) is applicable and shows that the networks provide an optimal order of approximation subject to the continuity requirement.
In the sequel, we make the following convention regarding constants. The letters c, c1, c2, · · · will denote positive constants which may depend upon p, r, s and other explicitly indicated quantities. Their value may be different at different occurrences, even within a single formula.
We now formulate our main theorem.
Theorem 2.1. Let 1  d  s, r  1, n  1 be integers, 1  p  ,  : Rd  R be infinitely many times continuously differentiable in some open sphere in Rd. We further assume that there exists b in this sphere such that

(2.6)

Dk(b) = 0, k  Zd, k  0.

Then there exist d×s matrices {Aj}nj=1 with the following property. For any f  Wrp,s, there exist coefficients aj(f ) such that

(2.7)

n
||f - aj (f )(Aj (·) + b)||p  cn-r/s||f ||Wrp,s .
j=1

The functionals aj are continuous linear functionals on Wrp,s. In particular,

(2.8)

E;n,p,r,s  cn-r/s.

We observe that the condition (2.6) implies that  is not a polynomial. For the function (x) := cos x1 + cos x2, (d = 2), we have D(1,1)  0. Thus, when d > 1, the assumption (2.6) is stronger than the assumption that  is not a polynomial. We suspect that it is a stronger assumption also in the case when
d = 1. The following Proposition 2.2 shows that (2.6) is nevertheless satisfied by a large class of functions. In light of the first part of this proposition, we doubt that in the case when d = 1, a nonpolynomial function that
is infinitely many times differentiable but does not satisfy (2.6) would be of any practical interest whatever.

3

Proposition 2.2. Let d  1 be an integer and  : Rd  R be infinitely many times continuously differen-
tiable on an open sphere B. If (2.6) is not satisfied, i.e., at every point of B some derivative of  is zero, then for every closed sphere U  B, there exists a multi-integer r  0, a sphere N  U and functions hi,j,N of d - 1 real variables such that

(2.9)

d ri-1

(x) =

hi,j,N (x1, . . . , xi-1, xi+1, . . . , xd)xji ,

i=1 j=1

x  N.

If d = 1 and  is analytic in a (complex) neighborhood of some point in B but not a polynomial, then (2.6) is satisfied.

Some of the important examples where (2.6) is satisfied are the following, where for x  Rd, we write

d

||x|| :=

x2j 1/2:

j=1

(The sqashing function)

d = 1, (x) := (1 + e-x)-1,

(Generalized multiquadrics)

d  1, (x) := (1 + ||x||2),   Z,

(Thin plate splines)

d  1, q  Z, q > d/2, (x) :=

||x||2q-d log ||x||, ||x||2q-d ,

d even, d odd

and

(The Gaussian function)

d  1, (x) := exp(-||x||2).

If the target function is merely assumed to be in Lp rather than in Wrp,s, the estimate (2.7) leads to a similar estimate in terms of the modulus of smoothness of the function. This is a fairly standard argument
in approximation theory, and does not add any new insight to the problem. Since a formulation of this result
would require us to introduce a great deal more notation, we omit this apparent generalization.
The idea behind the proof of Theorem 2.1 is simple. It is well known that for every integer m  r, there exists a polynomial Pm(f ) of coordinatewise degree not exceeding m such that for every f  Wrp,s,

(2.10)

||f - Pm(f )||p  cm-r||f ||Wrp,s .

Following [9] we express each monomial in Pm(f ) in terms of a suitable derivative of . In turn, this derivative can be approximated by an appropriate divided difference, involving O(ms) evaluations of . A careful book-keeping then yields Theorem 2.1.
If the target function f is analytic in the poly-ellipse

(2.11)

E := {z = (z1, . . . , zs)  Cs : |zj + zj2 - 1|  , j = 1, . . . , s}

for some  > 1 and 1 < 1 <  then for every integer m  1 there exists a polynomial ([20]) Lm(f ) (different from the polynomials described above) with coordinatewise degree not exceeding m such that

(2.12)

||f

-

Lm(f )||p



c,1 -1 m

max |f (z)|.
zE

Approximating these polynomials by networks as above, we get the following Theorem 2.3.

4

Theorem 2.3. Let 1  d  s, n  1 be integers, 1 < 1 < , 1  p   and f be analytic in the poly-ellipse E. Further, let  be as in Theorem 2.1. Then

(2.13)

E;n,p(f )



c,1

-n1/s
1

max
zE

|f (z)|.

It is possible to obtain some estimates on the degree of approximation under substantially weaker assumptions on  than those assumed in Theorems 2.1, 2.3. One strategy, as in [9], would be to take the convolution of  with a suitable, infinitely many times continuously differentiable function; apply Theorem 2.1 (or Theorem 2.3) to the resulting function and use a quadrature formula. We have not yet worked out the details of this argument, but it seems unlikely that these estimates would be optimal under the weak assumptions on . Using the ideas in the proof of Theorem 2.1, it is also possible to obtain estimates for simultaneous approximation of derivatives of the target function. This would follow from the corresponding theorems in the theory of trigonometric approximation (cf. [14]). Although the technical details in these generalizations are expected to be of some interest, we do not wish to pursue these ideas further in this paper.

3. Proofs. In order to prove Theorem 2.1, we first recall some well known facts from the theory of trigonometric approximation. These will be used to construct the polynomial operator in (2.10). The subspace of 2-periodic functions in Lp([-, ]s) (respectively Wrp,s([-, ]s)) will be denoted by Lp (respectively Wrp,s). If g  Lp , its Fourier coefficients are defined by

(3.1)

1 g^(k) := (2)s

g(t)e-ik·tdt,
[-,]s

k  Zs.

The partial sums of the Fourier series of g are defined by

(3.2)

sm(g, t) :=

g^(k)eik·t,

-mkm

m  Zs, m  0, t  [-, ]s,

where the notation k  m means kj  mj, 1  j  s. The de la Valle´e Poussin operator is defined by

(3.3)

1

vn(g, t) := (n + 1)s

sm(g, t),

nm2n

n  Z, n  0, t  [-, ]s.

The de la Valle´e Poussin operator has the following important property.
Proposition 3.1. cf. ([22, 14]) If r  1, s, m  1 are integers, 1  p   and g  Wrp,s then vm(g) is a trigonometric polynomial of coordinatewise order at most 2m and

(3.4)

||g

- vm(g)||p,[-,]s



c mr

||g

||Wrp,s

Further,

(3.5)

|vm(g)(k)|  cm||g||Wrp,s
0k2m

where  := s/ min(p, 2).
The standard way to construct a periodic function from a function on [-1, 1]s is to make the substitution xj =: cos tj, 1  j  s, for x  [-1, 1]s and t  [-, ]s. Obviously, the integrals defining the Lp norms are no longer equal under this substitution. Therefore, we make the following construction.
According to [21, §VI.3.1], there exists a continuous linear operator T : Wrp,s  Wrp,s([-2, 2]s) such that the restriction of T (f ) to [-1, 1]s is (almost everywhere) equal to f . The continuity of the operator T means that

(3.6)

||T (f )||Wrp,s([-2,2]s)  c||f ||Wrp,s

5

for every f in Wrp,s. In practical applications, f itself may be defined on [-2, 2]s. We may then choose to

work with f itself rather than T (f ). However, the bounds in (2.7) will then depend upon ||f ||Wrp,s([-2,2]s)

rather than the value 1

||f on

|[|-W1rp,,s1.]sNaenxdt,

let  be an 0 outside of

infinitely many times continuously differentiable [-3/2, 3/2]s. Then the function T (f ) coincides

function that takes with f on [-1, 1]s,

is identically 0 outside [-3/2, 3/2]s and

(3.7)

||T (f )||Wrp,s([-2,2]s)  c||f ||Wrp,s .

In the sequel, we denote the extension T (f ) of f again by the symbol f . We define a 2-periodic function from the function f (extended as above) by the formula

(3.8)

f (t) := f (2 cos t1, . . . , 2 cos ts), t  [-, ]s.

Then f   Wrp,s and, using induction and the fact that f is identically 0 outside of [-3/2, 3/2]s, we conclude from (3.7) that

(3.9)

c1||f ||Wrp,s  ||f ||Wrp,s  c2||f ||Wrp,s .

Now, it is easy to check that for any integer m, vm(f ) is an even function and hence we may write

(3.10)

vm(f , t) =:

s
Vk(f ) cos(kjtj).

0k2m

j=1

For integer k  0, let Tk be the Chebyshev polynomial adapted to the interval [-2, 2], defined by (cf. [22])

(3.11)

Tk(2 cos t) = cos(kt), t  [-, ]

and for a multi-integer k  0, let

(3.12)

s
Tk(x) := Tkj (xj ),
j=1

x  Rs.

The polynomial Pm(f ) defined by

(3.13)

Pm(f, x) :=

Vk(f )Tk(x),

0k2m

x  Rs

is an algebraic polynomial of coordinatewise degree at most 2m and is related to vm(f ) by the formula Pm(f, (2 cos t1, . . . , 2 cos ts)) = vm(f , t), t  [-, ]s.

Consequently, we obtain from (3.4), (3.9) that

(3.14)

c ||f - Pm(f )||p  mr ||f ||Wrp,s .

Also, in view of (3.5), (3.9), we have

(3.15)

|Vk(f )|  cm||f ||Wrp,s .
0k2m

The next step in the proof of Theorem 2.1 is to construct an approximation to every polynomial. This is summarized in the following Lemma 3.2.

6

Lemma 3.2. Let  satisfy the conditions of Theorem 2.1, m  1 be an integer and k  0 be any multi-integer in Zs with max1js |kj |  m. Then for every > 0, there exists Gk,m,  ;(6m+1)s,s such that

(3.16)

||Tk - Gk,m, ||  .

The weights and thresholds of each Gk,m, may be chosen from a fixed set with cardinality not exceeding (6m + 1)s.

Proof. First, we consider the case when d = 1. The point b in (2.6) is a real number in this case and
accordingly, will be denoted by b. Let  be infinitely many times continuously differentiable on [b - , b + ].
s
For a multi-integer p = (p1, . . . , ps), and x  Rs, we write xp := xpjj , where 00 is interpreted as 1. From
j=1
the formula

(3.17)

p(w; x)

:=

 |p| w1p1 . . . wsps

(w

·x

+

b)

=

xp(|p|)(w

·x

+ b),

we conclude that

(3.18)

-1
xp = (|p|)(b) p(0; x).

Following the ideas in [9], we now replace the partial derivative p(0; x) by an appropriate divided difference.

For multi-integers p and r, we write

p

s

:=

pj .

r

j=1 rj

For any h > 0, the network defined by the formula

(3.19)

p,h(x) := h-|p|

(-1)|r| p  h(2r - p) · x + b r

0rp

is in ;(p1+1)···(ps+1),s and represents a divided difference for p(0; x). Further, we have

(3.20)

||p,h - p(0; ·)||  M;m,sh2,

max
1js

|pj |



m,

|h|  /(3ms)

where M;m,s is a positive constant depending only on the indicated variables. Now, we write Tk(x) := 0pk k,pxp, and choose



h := h;m,s := min

, min 3ms 0k2m

M;m,s

1/2

0pk (|p|)(b) -1|k,p|

.

Then (3.20) implies that the network Gk,m, defined by

(3.21)

-1

Gk,m, (x) :=

k,p (|p|)(b)

p,h;m,s (x),

0pk

x  [-1, 1]s

satisfies (3.16). For each k, the weights and thresholds in Gk,m, are chosen from the set {(h;m,sr, b) : r  Zs, |rj |  3m, 1  j  s}.
The cardinality of this set is (6m + 1)s. Therefore, Gk,m,  ;(6m+1)s,s.

7

Next, if d > 1, and b is as in (2.6), then we consider the univariate function

(x) := (x, b2, · · · , bs).

The function  satisfies all the hypothesis of Theorem 2.1, with b1 in place of b in (2.6). Taking into account the fact that (w · x + b1) = (Awx + b) with

w

Aw

:=



0 ...



,

0

any network in ;n,s is also a network in ;n,s. Therefore, the case d = 1 implies the lemma also when
d > 1. Proof of Theorem 2.1. Without loss of generality, we may assume that n  13s. Let m  1 be the largest integer such that (12m + 1)s  n. We define Pm(f ) = 0k2m Vk(f )Tk as in (3.13). In view of (3.15), the network

(3.22)

Nn(f, x) :=

Vk(f )Gk,2m,m-r- (x)

0k2m

is in ;n,s and satisfies

||Pm(f ) - Nn(f )||  cm-r||f ||Wrp,s.

Since ||g||p  2s/p||g|| for all Lebesgue measurable functions g on [-1, 1]s we get from (3.14) that

||f - Nn(f )||p  cn-r/s||f ||Wrp,s
as required. Further, it is quite clear that the coefficients Vk are continuous linear functionals on Lp. Hence, the continuity assertion follows.
We will prove Proposition 2.2 after the proof of Theorem 2.3. Proof of Theorem 2.3. Again, we may assume that n  7s and let m  1 be the largest integer such that (6m + 1)s  n. We write xj,n := cos((2j + 1)/(2m)), 0  j  m, and use the Lagrange interpolation polynomial Lm(f ) at the points {(xk1,m, . . . , xks,m)}, 0  k  m, in place of Pm(f ) in the proof of Theorem 2.1. According to [20], this polynomial satisfies (2.12). Theorem 2.3 then follows as an application of Lemma 3.2 in exactly the same way as Theorem 2.1.
We end this section with a proof of Proposition 2.2. Proof of Proposition 2.2. For multi-integer k  0, let

Zk := {x  U : Dk(x) = 0}.

Since (2.6) is not satisfied, we have U =

Zk. Now, each Zk is a closed set and U being a closed

kZd,k0
sphere, is a complete metric space. Therefore, Baire's category theorem implies that for some multi- integer r  0, Zr contains a nonempty interior. Hence, there exists an open sphere N  U such that Dr(x) = 0 for every x  N . The formula (2.9) expresses  as a solution of this differential equation on N . If d = 1, 

is analytic in a closed neighborhood U of some point x0  B and (2.6) is not satisfied, then we have proved that  is equal to a polynomial on some interval contained in U . The identity theorem of complex analysis

then shows that  itself is a polynomial.

4. Conclusions. We have constructed generalized translation networks with a single hidden layer that provide an optimal order of approximation for functions in Sobolev classes similar to the order obtained in the classical polynomial approximation theory. If the target function is analytic, then it is possible to get a geometric rate of approximation, again similar to polynomial approximation. The weights and thresholds of our networks are chosen independently of the target function. We give explicit formulas for the coefficients,

8

so that the "training" consists of calculating certain simple, coninuous linear functionals on the target function. The activation function for the network is fairly general, but has to satisfy certain smoothness conditions. Among the activation functions for which our theorems are applicable are the squashing function, the Gaussian function, thin plate splines and generalized multiquadric functions.
Acknowledgments. I wish to thank Professor F. Girosi and T. Poggio, MIT Artificial Intelligence Laboratories, for their kind encouragement in this research. The research was supported in part by National Science Foundation Grant DMS 9404513 and Air Force Office of Scientific Research Grant F49620-93-1-0150.
REFERENCES
1. A. R. Barron, Universal approximation bounds for superposition of a sigmoidal function, IEEE Trans. Information Theory, 39 (1993), 930-945.
2. A. R. Barron and R. L. Barron, Statistical learning networks: a unified view, in "Symposium on the Interface: Statistics and Computing Science", Reston, Virginia, April, 1988.
3. D. S. Broomhead and D. Lowe, Multivariable functional interpolation and adaptive networks, Complex Systems, 2 (1988), 321-355.
4. C. K. Chui, X. Li and H. N. Mhaskar, Some limitations on neural networks with one hidden layer, Submitted for publication.
5. G. Cybenko, Approximation by superposition of sigmoidal functions, Mathematics of Control, Signal and Systems, 2 (1989), 303-314.
6. R. DeVore, R. Howard and C. A. Micchelli, Optimal nonlinear approximation, Manuscripta Mathematica, 63 (1989), 469-478.
7. F. Girosi, M. Jones and T. Poggio, Regularization theory and neural networks architectures, Neural Computation, 7 (1995), 219-269.
8. K. Hornik, M. Stinchcombe and H. White, Multilayer feedforward networks are universal approximators, Neural Networks, 2 (1989), 359-366.
9. M. Leshno, V. Lin, A. Pinkus, and S. Schocken, Multilayer feedforward networks with a nonpolynomial activation function can approximate any function, Neural Networks, 6 (1993), 861-867.
10. H. N. Mhaskar, Approximation properties of a multilayered feedforward artificial neural network, Advances in Computational Mathematics, 1 (1993), 61-80.
11. H. N. Mhaskar, Approximation of real functions using neural networks, in Proc. of Int. Conf. on Computational Mathematics, New Delhi, India, 1993, (H. P. Dikshit and C. A. Micchelli Eds.), World Scientific Press, 1994.
12. H. N. Mhaskar and C. A. Micchelli, Approximation by superposition of a sigmoidal function and radial basis functions, Advances in Applied Mathematics, 13 (1992), 350-373.
13. H. N. Mhaskar and C. A. Micchelli, Dimension independent bounds on the degree of approximation by neural networks, IBM Journal of Research and Development, 38 (1994), 277-284.
14. H. N. Mhaskar and C. A. Micchelli, Degree of approximation by neural and translation networks with a single hidden layer, to appear in Advances in Applied Mathematics.
15. J. Moody and C. Darken, Fast learning in networks of locally tuned processing units, Neural Computation, 1(2) (1989), 282-294.
16. J. Park and I. W. Sandberg, Universal approximation using radial basis function networks, Neural Computation, 3 (1991), 246-257.
17. T. Poggio and F. Girosi, Networks for approximation and learning, in Proceedings of the IEEE, 78(9), (1990).
18. T. Poggio, F. Girosi and M. Jones, From regularization to radial, tensor and additive splines, in "Neural Networks for Signal Processing, III", 1993, (C. A. Kamm, G. M. Kuhn, B. Yoon, R. Chellappa, S. Y. Kung Eds.), IEEE, New York, 1993, pp.3- 10.
19. M. J. D. Powell, The theory of radial basis function approximation, in " Advances in Numerical Analysis III, Wavelets, Subdivision Algorithms and Radial Basis Functions", (W. A. Light Ed.), Clarendon Press, Oxford, 1992, pp. 105-210.
20. J. Siciak, On some extremal functions and their applications in the theory of analytic functions of several complex variables, Trans. Amer. Math. Soc., 105 (1962), 322-357.
9

21. E. M. Stein, "Singular integrals and differentiability properties of functions", Princeton Univ. Press, Princeton, 1970.
22. A. F. Timan, "Theory of Approximation of Functions of a Real Variable", Macmillan Co., New York, 1963.
10


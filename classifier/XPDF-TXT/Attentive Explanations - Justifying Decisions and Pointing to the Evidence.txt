Attentive Explanations: Justifying Decisions and Pointing to the Evidence

Dong Huk Park1 Lisa Anne Hendricks1 Zeynep Akata1,2

Bernt Schiele2

Trevor Darrell1 Marcus Rohrbach1

1UC Berkeley EECS, CA, United States 2Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbru¨cken, Germany

arXiv:1612.04757v1 [cs.CV] 14 Dec 2016

Abstract
Deep models are the defacto standard in visual decision models due to their impressive performance on a wide array of visual tasks. However, they are frequently seen as opaque and are unable to explain their decisions. In contrast, humans can justify their decisions with natural language and point to the evidence in the visual world which led to their decisions. We postulate that deep models can do this as well and propose our Pointing and Justification (PJ-X) model which can justify its decision with a sentence and point to the evidence by introspecting its decision and explanation process using an attention mechanism. Unfortunately there is no dataset available with reference explanations for visual decision making. We thus collect two datasets in two domains where it is interesting and challenging to explain decisions. First, we extend the visual question answering task to not only provide an answer but also a natural language explanation for the answer. Second, we focus on explaining human activities which is traditionally more challenging than object classification. We extensively evaluate our PJ-X model, both on the justification and pointing tasks, by comparing it to prior models and ablations using both automatic and human evaluations.
1. Introduction
Humans are surprisingly good at explaining their decisions, even though their explanations do not necessarily align with their initial reasoning [43] and they arguably do not have full conscious access to their decision process. Still, explaining one's decisions is an integral part of human communication, understanding, and learning. Therefore, we aim to build models that can justify their decisions, something which comes naturally to humans. Explanations can take many forms. For example, humans can explain their decisions using natural language, or by pointing to visual evidence. Thus, we propose a model which provides both textual justifications and exposes which image regions

Q: What sport is this?

A: Baseball <VQA-Att>

Textual Justification: The player is holding a bat
<Exp-Att>

Q: What sport is this?

A: Baseball <VQA-Att>

Textual Justification: The player is swinging a bat
<Exp-Att>

Figure 1: For a given question and corresponding image, we predict the answer and explain it by generating a natural language justification and introspect the model with two attention mechanisms, the first for the answer (vqa att) and the second for the explanation (exp att); e.g. we point to the evidence for the answer (baseball) and the explanation (holding versus swinging a bat) specific to the image.
are important for a decision by providing visualizations of attention.
Generating convincing justifications requires models to not only recognize objects, activities, and attributes, but discuss which visual elements are important for a decision. For example, consider Figure 1, in which two images of people playing baseball are shown. In both examples, the question "What sport is this?" is asked, and the model correctly answers "Baseball". Though both images share common visual elements (e.g., people and baseball bats), the textual explanations reflect the differences in the two images: while one justifies the answer "Baseball" with the fact that one player is "holding a bat", the other justifies the answer with the fact that one player is "swinging a bat".
In addition to producing textual explanations, we want to introspect the model and point to the evidence in the im-

1

age. Specifically, we design our model to use two attention mechanisms, one for predicting the answer and one for predicting the explanation, as the evidence for the answer might not always be the same as for the explanation. When generating the textual explanation, our model points to different image regions. E.g. for answering the question, it focuses on a larger area where the action is happening (Figure 1, middle), while for the explanation it focuses on the position of the bat (Figure 1, right). Furthermore, when our model answers the question, it points to different regions and aspects for different images (top: to the closest player, bottom: on the field and the player's legs).
Explaining decisions of visual AI systems is gaining increased interest as visual recognition models become more and more reliable, but at the same time frequently remain opaque. In this work we focus on both language and visual explanations that justify a decision by having access to the hidden state of the model, but do not necessarily have to align with the system's reasoning process. Instead we try to predict the verbal arguments humans give, and evaluate if our model points to the same visual evidence as humans do. Justification models which give textual explanations have been investigated in the context of fine-grained classification of 200 bird species [19]. However, we want to generalize it to more diverse categories and tasks. On the one hand we aim to explain human activities, as recognizing them is more challenging than objects due to their less clear class boundaries, low inter-class variability, and high intraclass variability. On the other hand, we propose to look at the visual question answering (VQA) task which includes a diverse set of visual recognition tasks, including person, object, attribute, activity, and scene recognition, counting, and understanding object interactions. Here, decisions do not only depend on the image, but also on the task which is formulated as a question.
[19] proposes a model which allows to learn how to generate explanations from captions. While this might be possible for birds where the vocabulary and type of explanations are very similar within and across classes, we expect this will not generalize to other scenarios. Thus, to learn models which are able to generate explanations, we propose and collect two complementary explanation datasets for VQA and activities rather than generic descriptions of activities [36] or image captions [9]. In addition to a textual justification we aim to point to the visual evidence by exposing part of the models' decision process when predicting the answer and the explanation. While our models learn this without supervision, we augment our explanation datasets with visual evidence (in form of image locations) which humans give both for their answers and explanations to evaluate how well our predicted attention aligns with humans pointing.

2. Related Work
Here, we review relevant work on explanations, visual question answering with attention and activity recognition.
Explanations. Early works on textual explanation are mostly template based. [39] proposes explanation systems for medical applications, [24, 40, 10] propose explanations as a feedback to improve simulated training for special teaching programs, [28] proposes a model to explain robot actions, [23] evaluates methods developed to explain Bayesian networks and [21] proposes a system that determines the motivation for a decision by recalling the situation in which the decision was made, and replaying the decision under variants of the original situation. Most recently [19] developed a deep network to generate natural language justifications of a fine-grained object classifier. However, unlike our model, it does not consider explaining decisions visually and the model is trained on descriptions rather than reference explanations.
In addition to providing textual explanations, [12, 6] have attempted to explain model decisions by finding discriminative visual patches that are related to the prediction. Other models [47, 14, 48] aim to understand which intermediate features are important for a classification decision, e.g. what does a certain neuron represent. Recently [17] proposed to tackle the problem of indicating the evidence of a prediction through guided backpropagation and occlusion. Our model provides visual justification of a prediction through an integrated attention mechanism.
[7] proposed breaking explanation models into two classes: introspection and justification systems. While introspection systems aim to convey exact details of a decision process, justification systems aim to provide evidence that a decision is sound. This division is also seen in human cognition. [43] points out that human reasoning process does not necessarily align with the explanations they give. In other words, the internal mechanism behind a decision may not be perfectly captured by the textual justification. However, the justification is still useful as it provides evidence for why a particular decision is made. Especially, it can be significantly valuable when deploying systems in real world scenarios where users may not have domain knowledge in deep networks or machine learning. In this paper, we propose a model that attempts to provide both justifications and introspection via natural language and attention respectively, thereby providing more comprehensive explanations for a classification decision.
The justifications and introspection that our model provides are multimodal explanations in the form of text and attention map­which many of the previous methods do not provide in combination­and this is an intuitive way to convey knowledge about what is important to the network without requiring domain knowledge.

2

Dataset

I

C

D (#w) E (#w)

CUB [42, 35]

11K 200 58K (17)

0

MSCOCO [26],VQA [3] 123K  3000 616K (10.6) 0

VQA-X (ours)

20K 3000

0 30K (8.1)

MHP [2, 34, 36]

25K 410 75K (15)

0

ACT-X (ours)

17K 367

0

20K (13)

Table 1: Statistics for CUB [42, 35], MHP [2, 36] and VQA [3] datasets: I = number of images, C = number of classes, D (#w) = Descriptions (average number of words) and E (#w) = Explanations (average number of words).

Visual Question Answering and Attention. Initial approaches to VQA used full-frame representations [29], but most recent approaches use some form of spatial attention [46, 45, 49, 8, 44, 38, 15]. We base our method on [15], i.e. winner of VQA 2016 challenge, and predict a latent weighting (attention) of spatially localized image features based on the question. The weighted image representation rather than the full frame representation is then used as a basis for answering the question. [11] shows that human gaze attention and attention in VQA systems are different. Concurrently to this work, [22] have explored the element-wise product for VQA just as we do in our method, however our model uses L2 normalization after the bilinear feature while [22] does not.
Activity Recognition. Recent work that tackles activity recognition in still images relies on a variety of cues, such as pose and global context, to achieve good results [34, 30]. However, although cues like pose may influence model performance, activity recognition models are not capable of indicating which factors influence a decision process. In contrast, explanations aim to reveal which parts of an image are important for a classification. For example, it might be clear that someone is doing yoga based purely on pose, whereas an activity like mountain biking might require context, such as outdoor scenery, to properly classify. Our explanations attempt to bridge this gap and justify which parts of an image, whether it be pose or context, are most important for a classification decision.
3. Visual Explanation Datasets
We start by introducing our two explanation datasets: Visual Question Answering Explanation (VQA-X) and MPI Human Pose Activity Explanation (ACT-X). A summary of dataset statistics is presented in Table 1.
VQA Explanation Dataset (VQA-X). The Visual Question Answering (VQA) dataset [3] contains open-ended questions about images which require understanding vision,

MSCOCO Description
A man on a snowboard is on a ramp.

VQA Explanation
Q: What is the person doing? A: Snowboarding
Because... they are on a snowboard in snowboarding outfit.

A man riding a snowboard down the side of a ramp.

Q: Is the person swimming? A: No
Because... the guy is nowhere near water.

A gang of biker police riding their bikes in formation down a street.
Many guys ride motorcycles in a line together

Q: Can these people arrest someone? A: Yes Because... they are Vancouver police.
Q: What kind of vehicles are these? A: Motorcycles
Because... they have two wheels and headlights and have one rider each.

Figure 2: Our VQA-X dataset contains MSCOCO images and explanations for the corresponding question and answer pairs. While MSCOCO descriptions are generic, our explanations are specific to the question, answer, and image.

natural language, and commonsense knowledge to answer. The dataset consists of approximately 200K MSCOCO images [27], with 3 questions per image and 10 answers per question. A question may be associated with multiple images and an answer may correspond to multiple questions. We select 20K question/answer (QA) pairs from the VQA training set and 2K QA pairs from the VQA validation set, which are later divided into 1K QA pairs each for validation and testing. Since there are multiple answers per question, we only consider questions where at least 8 of the answers agree and pick the most common answer as the ground truth. The QA pairs were selected based on few simple heuristics that would remove pairs that require trivial explanations, such as Q: "What is the color of the banana?" etc. We collected 1 explanation per data point for the training set and 5 explanations per data point for the validation and test sets. The annotators were asked to provide a proper sentence or clause that would come after the proposition "because" as explanations to the provided image, question, and answer triplet. Some examples for both descriptions, i.e. from MSCOCO dataset, and our explanations are presented in Figure 2. In comparison to the descriptions, our explanations focuses on the visual evidence that pertains to the question and answer instead of generally describing objects in the scene.
Action Explanation Dataset (ACT-X). The MPI Human Pose (MHP) dataset [2] contains 25K images extracted from videos downloaded from Youtube. From the MHP dataset, we selected all images that pertain to 367 activities, resulting in 17, 019 images total. For the training set of 15, 786 images, we collected a single explanation for the activity whereas for the remaining images, i.e. validation set, test

3

Description
A man in a black shirt and blue jeans is holding a glowing ball.

Explanation: I can tell this person is juggling
Because he holds two balls in one hand, while another ball is aloft just above the other hand.

A man standing wearing a pink shirt Because he has two balls in his

and grey pants near a ball.

hands while two are in the air.

A man in black shirt is holding a red Because he is holding a ball in one

ball in one hand and balancing

hand and balancing the other on his

another red ball on his shoulder.

shoulder.

a person wearing gray shirt is carrying some kind of orb in both of his hands

Because he is holding a ball in each hand and looking up at the other ball.

Figure 3: Our ACT-X dataset contains images from MHP [2] dataset and our activity explanations. For MHP, [36] collected one-sentence descriptions. Our explanations are task specific whereas descriptions are more generic.
set, we collected 4 explanations. During data annotation, we asked the annotators to complete the sentence "I can tell the person is doing (action) because.." where the action is the ground truth activity label. We also asked them to use at least 10 words and avoid mentioning the activity class in the sentence. MHP dataset also comes with single sentence descriptions provided by [36]. Some examples of descriptions and explanations can be seen in Figure 3. Please note that descriptions describe only the person in the scene, and the tools the person interacts with, not the background, e.g. he is holding a glowing ball. Our explanations are significantly different from descriptions, e.g. he is juggling because he is holding a glowing ball and two other glowing balls are in the air.
4. Pointing and Justification Model (PJ-X)
The goal of our work is to justify with natural language why a decision was made, and point to the evidence for both the decision and the textual justification provided by the model. We deliberately design our Pointing and Justification Model (PJ-X) to allow training these two tasks as well as the decision process jointly. Specifically we want to rely on natural language justifications and the classification labels as the only supervision. We design the model to learn how to point in a latent way. For the pointing we rely on an attention mechanism [4] which allows the model to focus on a spatial subset of the visual representation. As the model ignores all spatial visual features it does not (or insignificantly) attend to, this pointing also allows at the same time to introspect the model. Our model uses two different attentions, one to make predictions and another to generate explanations.
An overview of our double attention model is presented in Figure 4. Below we detail our explanation model for visual question answering and then highlight the difference

when predicting activities.
4.1. PJ-X for Visual Question Answering
We first detail how we predict the answer given image and question. Then, given additionally the answer, we generate the textual justification. In both cases we include a latent attention mechanism which allows to introspect where the model looks at or points to.

Learning to answer. In visual question answering the goal is to predict an answer given a question and an image. To be able to introspect the answering process we want the model to select the area of the image which gives the evidence for the answer. This can be achieved using an attention model. Here we follow the state-of-the-art MCB attention model [15], but replace the MCB with an elementwise multiplication. Adding a fully-connected layer for embedding the visual feature before element-wise multiplication and applying L2 normalization after that lead to similar performance, but much faster training. Comparison on the VQA dataset [3] between our model and the state-of-the-art model can be found in Section 5.2.
We extract spatial image features f I (I, n, m) from the last convolutional layer of ResNet-152 followed by 1 × 1 convolutions (f¯I ) giving a 2048 × N × M spatial image feature. We encode the question Q with a 2-layer LST M f Q(Q). We combine this and the spatial image feature using element-wise multiplication followed by L2 normalization and Dropout, and two more layers of 1×1 convolutions with ReLU in between, which operate on the spatial feature map location n  N and m  N :

f¯IQ(I, n, m, Q) =(W1f I (I, n, m) + b1) f Q(Q) (1)

f IQ(I, Q) =L2(f¯IQ(I, Q))

(2)

¯npo,mintA =f pointA(I, n, m, Q)

(3)

=W3(W2f IQ(I, Q) + b2) + b3 (4)

with ReLU (x) = max(x, 0). This process gives us a N × M attention map ¯n,m. We apply softmax to produce a normalized soft attention map, which aims to point at the evidence of the answer (pointA):

npo,mintA =

exp(¯npo,mintA)

N i=1

M j=1

exp(¯ip,ojintA)

(5)

The attention map is than used to take the weighted sum over the image features and this representation is once again combined with the LSTM feature to predict the answer y^ as

4

Answering With Attention

Softmax FC
Weighted Sum

CNN (ResNet-152)

.
What sport is this?

.

"Baseball"

LSTM

Embed
.

Because... "The player is swinging a bat"
.
LSTM

Conv

Weighted Sum

LSTM Previous Word Explaining With Attention

Figure 4: Our Pointing and Justification (PJ-X) architecture for attentive explanations.

a classification problem over all answers Y .

NM

f¯y(I, Q) =(

npo,mintAf I (I, n, m))

x=1 y=1

f y(I, Q) =W4f¯y(I, Q) + b4

p(y|I, Q) =Sof tmax(f y(I, Q))

y^ = argmax p(y|I, Q)
yY

f Q(Q) (6)
(7) (8) (9)

Learning to justify. We argue that to generate a textual justification for VQA, we should condition it on the question, the answer, and the image. For instance, to be able to explain "Because they are Vancouver police" in Figure 2, the model needs to see the question, i.e. "Can these people arrest someone?", the answer, i.e. "Yes" and the image, i.e. the "Vancouver police" banner on the motorcycles.
We model this by first using a second attention mechanism and then using the localized feature as input to a LSTM which generates the explanations. In this way we hope to uncover which parts of the image contain the evidence for the justification.
More specifically, the answer predictions are embedded in a d-dimensional space followed by tanh non-linearity and a fully connected layer:

f yEmbed(y^) =W6(tanh(W5y^ + b5)) + b6

(10)

To allow the model to learn how to attend to relevant
spatial location based on the answer, image, and question,
we combine this answer feature with Question-Image embedding f¯IQ(I, Q). After applying 1 × 1 convolutions,
element-wise multiplication followed by L2 normalization

and Dropout, the resulting multimodal feature is flattened to a 14 × 14 attention map similarly as the previous attention step.

f¯IQA(I, n, m, Q, y^) =(W7f¯IQ(I, Q, n, m) + b7) (11)

f yEmbed(y^))

(12)

f IQA(I, Q, y^) =L2(f¯IQA(I, Q, y^))

(13)

¯npo,mintX =f pointX (I, n, m, Q, y^)

(14)

=W9(W8f IQA(I, Q, y^) + b8) + b9

(15)

with Relu (x) = max(x, 0). This process gives us a N × M attention map ¯n,m. We apply softmax to produce a normalized soft attention map, which aims to point at the evidence of the generated explanation (pointX):

npo,mintX =

exp(¯npo,mintX )

N i=1

M j=1

exp(¯npo,mintX

)

(16)

Using this second attention map, we compute the attended visual representation, and merge it with the LSTM feature that encodes the question and the embedding feature that encodes the answer:

NM

f X (I, Q, y^) =(W10

npo,mintX f I (I, n, m) + b10)

x=1 y=1

(17)

(W11f Q(Q) + b11)

(18)

f yEmbed(y^)

(19)

5

This combined feature is then fed into an LSTM decoder to generate explanations that are conditioned on image, question, and answer.
It predicts one word wt at each time step t conditioned on the previous word and the hidden state of the LSTM:
ht = f LST M (f X (I, Q, y^), wt-1, ht-1) (20) wt = f pred(ht) = Sof tmax(Wpredht + bpred) (21)
4.2. PJ-X for Activity Recognition
Although activities may also have a question associated with them, e.g. "what is this person doing?", the question is always going to be the same for all the images, and thus the PJ-X for Activity Recognition omits the question encoding part from the PJ-X for VQA. As a result, the first step, defined in Equation 1 through Equation 9, which is used to predict an answer, is no longer conditioned on the question.
Just as in PJ-X for VQA, the PJ-X for Activity Recognition can be divided into two parts: activity classification and activity explanation. For activity classification, we deploy a similar pipeline used in the answering part of PJ-X for VQA. We extract image features from the last convolutional layer of ResNet-152, followed by 1 × 1 convolutions with ReLU, and one more layer of 1×1 convolutions which gives us a N × M attention map. We apply softmax to produce a normalized soft attention map, which is used to take the weighted sum over the ResNet features to create an attention feature. The attention feature is followed by a fully connected layer that gives us the activity predictions.
For activity explanation, we embed the predictions into 300-dim space followed by tanh non-linearity and a fully connected layer that produces a vector of 2048-dim, which we tile to 2048 × N × M . In order to generate a second attention map that is conditioned on the answer, we create a multimodal feature similarly as before. To allow the model to learn how to attend to relevant spatial location based on the activity, we need to combine the answer embedding with the N ×M spatial visual feature (feature after applying 1×1 convolutions to the ResNet feature) by element-wise multiplication followed by L2 normalization and Dropout. This multimodal feature is flattened to an attention map through two 1 × 1 convolutions with ReLU in between. As a result, we finally get the second 2048-dim attention feature, which is again combined with the answer embedding. This combined feature is then fed into an LSTM decoder to generate explanations that are conditioned on image and answer.
5. Experiments
In this section, we evaluate both textual justification and visual pointing tasks. For textual justification, we compare to ablations and related approaches on our VQA-X and ACT-X datasets based using automatic and human evalua-

tion for the generated explanations (Section 5.3). For visual pointing, we compare our attention maps to several baselines and report quantitative results with corresponding analysis (Section 5.4). Finally, we show qualitative results for both tasks (Section 5.5).
We begin with detailing our experimental setup (Section 5.1) and evaluate our VQA model on the VQA task [3] which is the basis for our explanation models (Section 5.2).
5.1. Experimental Setup
Dataset Splits. There are 3 data splits for the VQA-X dataset: training set (20K QA pairs with 1 explanation per pair, 18,357 images), validation set (1K QA pairs with 5 explanations per pair, 991 images), and test set (1K QA pairs with 5 explanations per pair, 1000 images). The ACTX dataset also consists of 3 splits with training set having 15,786 images with 1 explanation per image, the validation set having 580 images with 4 explanations per image, and finally the test set having 653 images with 4 explanations per image. We train all our models in the experiments on the training set, finetune hyperparameter settings on the validation set, and report results on the test set.
Model Training and Hyperparameters. For VQA our model is pre-trained on the VQA training set [3] to achieve state-of-the-art performance on predicting answers, but we fix the weights for fine-tuning on explanations as the dataset is significantly smaller. The spatial feature size of our model is N = M = 14, and for VQA we classify with the 3000 most frequently occurring answers on the training set (i.e. |Y | = 3000). For activity recognition, |Y | = 367. We set the answer embedding size as d = 300.
Ablations for Textual Justification. For our ablation studies, we re-implemented the state-of-the-art captioning model [13] with an integrated attention mechanism which we refer to as "Captioning Model". This model only uses images and does not use class labels, i.e. the answer in VQA-X and the activity label in ACT-X. It first takes in the 2048 × 14 × 14-dim ResNet feature and applies two convolutions with ReLU in between to create an attention map. The attention map is used to generate the attention feature over the ResNet features. The resulting attention feature is embedded once using a fully connected layer and this is passed to the LSTM to generate explanations. Comparisons with [19] were also made. [19] generates explanatory sentences by using ResNet features extracted from the entire image. Generated sentences are conditioned on both the image and class predictions. The explanations are trained with a discriminative loss which enforces the generated sentence to contain class-specific information. In the experiment, [19] is trained on descriptions. "Ours on Descriptions" is also trained on descriptions, but has an integrated attention mechanism that is used to generate textual expla-

6

nations as well as to point to the evidence of the explanation. "Ours w/o Exp-Attention" is similar to [19] in the sense that there is no attention mechanism for generating explanations, however, it does not use the discriminative loss and is trained on explanations instead of descriptions.
Baselines for Visual Pointing. In order to evaluate if the attention of our model corresponds to where humans think the evidence for the answer and justification is, we collect attention maps from humans for both VQA-X and ACT-X datasets. Human-annotated attention maps are collected via Amazon Mechanical Turk in two different ways. First, annotators are provided with an image, divided into 14 × 14 grid, and an answer (question and answer pair for VQAX, class label for ACT-X). They are asked to select up to 4 locations in the grid that most prominently show the visual evidence for the answer. For the second round, annotators are again given the image with the same grid and the answer, but they are also provided with the ground-truth explanation. This time, they are asked to select up to 4 locations that most prominently show the visual evidence for the explanation. For each image, we collect 3 attention maps of each type, totaling 6 attention maps per image. We call these two types of annotated attention maps answer-based ground truth (Ans-based GT) and explanation-based ground truth (Exp-based GT), respectively.
We compare our model against the following baselines: One Point Random: randomly attends to a single point in the grid. Uniform Map: generates attention map that is uniformly distributed over the 14×14 grid. Saliency Map [20]: attends to the most salient parts of the image.
Evaluation Metrics. For textual justification, we use BLEU-4 [32], METEOR [5], ROUGE [25], CIDEr [41] and SPICE [1] metrics to determine the degree of similarity between generated and ground truth sentences. Among these metrics, BLEU-4 counts the number of matches among the n-grams in generated and reference sentences. METEOR is computed by matching words, but unlike BLEU, it uses WordNet [31] to also match the synonyms. CIDEr counts common n-grams which are TF-IDF weighted which rewards sentences for mentioning phrases which are uncommon in the dataset. ROUGE measures the n-gram recall between the generated and reference sentence, whereas BLEU is a precision-based measure. SPICE metric maps reference and candidate captions in a dependency parse tree encoding the objects, attributes, and relations between them.
The metrics that we use for visual pointing are Weighted Overlap and Earth Mover's Distance (EMD)[37]. Weighted Overlap computes the weighted sum of two attention maps, i.e. the more the generated attention map agrees with the annotated map, the higher the score. However, since Weighted Overlap is a point-by-point comparison, it may not capture the spatial similarity of two attention maps. To address this

Method
MCB [15] Our VQA model

Training data Train Train+Val

62.5

64.2

63.0

64.8

Table 2: OpenEnded results on VQA dataset [3], test-dev. The columns indicate the accuracy of the model after being trained on training set and train+val set, respectively. Our model achieves slightly higher accuracy and the previous VQA challenge winner MCB [15].

issue, we use the Earth Mover's Distance (EMD) which measures the distance between two probability distributions over a region. It reflects the minimum amount of work that must be performed to transform one distribution into the other by moving "distribution mass." EMD matches spatial similarity better than Weighted Overlap in that it captures the notion of distance between two sets or distributions instead of two single points. We use the code from [33] to compute EMD.
5.2. Visual Question Answering Model
The VQA model that we use throughout the experiments is based on the state-of-the-art MCB model [15], but trains and evaluates faster (reduction by about 30%). The main difference between the two models is how they combine two different representations and create multimodal features.
Our VQA model encodes the image with ResNet and the question with an LSTM, just as in the MCB model. However, instead of doing Compact Bilinear Pooling [16] between the two representations, our model simply embeds the encoded image feature using 1 × 1 convolutions and applies element-wise multiplication between the embedding and the LSTM feature. Whereas the MCB model tries to create a rich multimodal feature by approximating the outer product of two representations, our model tries to learn the proper alignment of the representations so that when merged with element-wise multiplication, it creates a feature that is as powerful as the MCB feature. Similar to [15], the merged representation is normalized by applying signed square root and L2-normalization. This improved model leads to an increase of 0.5% when training on the training set, and 0.8% when training on train-val.
5.3. Textual Justification
We first discuss automatic evaluation and then human evaluation of our approach.
5.3.1 Automatic Evaluation
We present results of our ablation study on our VQA-X dataset in Table 3. Our general observation is that both at-

7

Approach

Attention for Training Data Explanation QA Conditioned B

Automatic evaluation

M

R

C

Human

S

eval

Ours on Descriptions Descriptions Yes

Yes

0.081 0.143 0.283 0.343 0.112 34.0

Captioning Model

Explanations Yes

No

Ours w/o Exp-Attention Explanations No

Yes

Ours

Explanations Yes

Yes

0.171 0.160 0.404 0.436 0.073 24.8 0.251 0.205 0.487 0.742 0.116 43.2 0.253 0.209 0.498 0.721 0.121 41.6

Table 3: VQA Explanations (VQA-X). Evaluated automatic metrics: BLEU-4 (B), METEOR (M), ROUGE (R), CIDEr (C) and SPICE (S). Reference sentence for human and automatic evaluation is always an explanation.

Approach

Attention for Training Data Explanation Act. Conditioned B

Automatic evaluation

M

R

C

Human

S

eval

[19]

Descriptions No

Yes

Ours on Descriptions Descriptions Yes

Yes

0.101 0.151 0.311 0.323 0.083 22.5 0.099 0.154 0.316 0.383 0.093 40.4

Captioning Model

Explanations Yes

No

Ours w/o Exp-Attention Explanations No

Yes

Ours

Explanations Yes

Yes

0.274 0.214 0.496 0.614 0.129 24.4 0.238 0.198 0.462 0.636 0.139 14.4 0.339 0.255 0.538 0.994 0.186 33.6

Table 4: Activity Explanations (ACT-X). Evaluated automatic metrics: BLEU-4 (B), METEOR (M), ROUGE (R), CIDEr (C) and SPICE (S). Reference sentence for human and automatic evaluation is always an explanation.

tention and our explanations help to improve the results. The details are as follows. Our explanations help as all the model variants that use explanations (i.e. "Captioning Model", "Ours w/o Exp-Attention" and "Ours") outperform the model trained on descriptions ( i.e. "Ours on Descriptions"). This is expected as MSCOCO sentences describe the entire scene and are generic, therefore they do not necessarily talk about the interesting aspects of the scene where the attention is focused on. On the other hand, the integrated attention mechanism helps improve the captioning accuracies in most metrics as "Ours" model outperforms "Ours w/o Exp-Attention". We conclude from this result that attention is indeed an important cue when the task is to generate natural language justifications of an answer.
Similarly, the results of our ablation study performed on our ACT-X dataset in Table 4 clearly show that "Ours" leads to the highest results in all evaluation metrics. The effect of attention can be validated by noticing that both "Ours" and "Captioning Model" which are trained with attention outperform "Ours w/o Exp-Attention" in every metric. "Ours" performs better than "Captioning Model" suggesting that conditioning sentence generation on the predicted activity is important. Similar to our observations on VQA-X dataset, our results on ACT-X dataset indicate that ground truth explanations are more discriminative of the action label compared to descriptions, as "Ours" performs significantly better than "Ours on Descriptions". This is expected as the descriptions on this dataset focuses on the clothing and the tools the person might be carrying, which are not necessar-

ily the important cues for recognizing activities. We also observe from Table 4 that our method outperforms [19].
5.3.2 Human Evaluation
As automatic evaluation of sentences are highly dependent on the reference descriptions and have shown to not always correspond well to human judgments, we also compare generated explanations through human evaluation. We randomly choose 250 images from the test sets of the VQAX and ACT-X datasets, respectively. We then ask 3 humans for each image to judge whether a generated explanation is better than, worse than, or equivalent to a ground truth explanation (we note that human judges do not know what explanation is ground truth and the order is randomized). We report the percentage of generated explanations which are equivalent to or better than ground truth human explanations, when at least 2 out of 3 human judges agree.
On the VQA-X dataset (Table 3), the "Captioning Model," which is the only model that is not conditioned on the question and answer pair, performs considerably worse than all other methods. Also, training with description data leads to worse performance (compare "Ours on Descriptions" to "Ours"). This is to be expected on the VQAX dataset since the descriptions frequently discuss content which is not relevant to questions. However, even though it is trained on a different type of sentence data, our model trained on descriptions ("Ours on Descriptions") still performs better than the "Captioning Model" suggesting that

8

Approach

VQA Explanations (VQA-X) Activity Explanations (ACT-X) Ans-based GT Exp-based GT Ans-based GT Exp-based GT

One Point Random Uniform Map Saliency Map [18, 20] Ours (ans-att) Ours (exp-att)

1.70 2.04 3.75 8.22 [5.72]

1.80 2.05 3.76 [8.02] 5.73

1.53 2.11 4.07 20.74 [4.30]

1.38 2.17 4.24 [17.14] 4.50

Table 5: Evaluation of pointing, with Weighted Overlap metric (in %, higher is better). Ans-based GT is an attention map generated by humans based on the answer/class labels. Exp-based GT is an attention map generated by humans based on the explanation labels. Ans-att denotes the attention map used to predict the answer whereas exp-att denotes the attention map used to generate explanations.

Approach

VQA Explanations (VQA-X) Activity Explanations (ACT-X) Ans-based GT Exp-based GT Ans-based GT Exp-based GT

One Point Random Uniform Map Saliency Map [18, 20] Ours (ans-att) Ours (exp-att)

6.29 5.06 4.29 3.75 [3.97]

6.30 5.07 4.28 [3.77] 3.96

6.21 4.75 3.96 3.89 [3.89]

6.10 4.60 3.86 [4.16] 3.78

Table 6: Evaluation of pointing, with Earth Mover's Distance (EMD) metric (lower is better). Ans-based GT is an attention map generated by humans based on the answer/class labels. Exp-based GT is an attention map generated by humans based on the explanation labels. Ans-att denotes the attention map used to predict the answer whereas exp-att denotes the attention map used to generate explanations.

conditioning explanation generation on questions and answers is very important for good performance on the VQAX dataset.
On the ACT-X dataset (Table 4), we find that attention is important for good performance, with models which exclude attention ([19] and "Ours w/o Exp-Attention") performing worse than other models trained with similar data. Similar to experiments on the VQA-X dataset, comparing the "Captioning Model" to our final model demonstrates that conditioning on the predicted activity is important for good performance. In contrast to the VQA-X dataset, "Ours on Descriptions" performs extremely well on human evaluation for a number of reasons. When collecting the ACT-X dataset, we instruct workers to avoid mentioning an activity class in the explanation. However, the descriptions for the same images frequently mention the activity being performed. Hence, as illustrated in Figure 5, sentences generated with models trained on description frequently mention an activity class. We believe that mentioning the activity class leads to low scores using automatic metrics (as collected reference explanations do not mention the activity class), but leads to higher human evaluation scores, especially since humans are asked to compare a generated result which includes the activity class in the explanation, to collected explanations which never include the activity class.

Additionally, there are 3x more descriptions than explanations which may lead to better grammar and sentence cohesiveness for models trained on description data.
5.4. Visual Pointing
We present the results on the visual pointing task on our VQA-X and ACT-X datasets in Table 5 and Table 6. We observe that our model outperforms all other methods in VQA-X for both metrics. For ACT-X, our attention map for predicting answer (ans-att) performs slightly worse than the Saliency Map against the explanation-based GT on EMD, but our model in general outperforms all other methods for both metrics.
It is important to note that our model generates two different attention maps for generating answers and explanations, and the highlighted numbers show that each attention map performs best in correspondence to what the ground-truth is conditioned on. Brackets around the numbers indicate the results of our model when comparing noncorresponding attention maps, that we add for completeness. When taking such correspondence in consideration, the weak performance of ans-att discussed above is not significant since it performs the best against the Ans-based GT.
The relatively high number on Weighted Overlap for Ours (ans-att) on ACT-X can be attributed to the fact that

9

ans-att is highly focused and peaky, resulting in higher scores when aligned correctly with the human-annotated maps.
Figure 5: Sentences generated by "Ours with Description" and "Ours." As discussed in Section 5.3.2, the model trained on descriptions mentions the activity class (noted in red) explicitly in the sentences.
5.5. Qualitative Results In this section we present our qualitative results on VQA-
X and ACT-X datasets demonstrating that our model generates high quality sentences and the attention maps point to relevant locations in the image. VQA-X. Figure 6 shows qualitative results on our VQA-X dataset. Our textual explanations are able to both capture common sense and discuss specific image parts important for answering a question. For example, when asked if a room looks clean, the explanation model is able to discuss what it means for something to be clean, i.e. "there is not clutter and it is spotlesss". When determining the kind of vehicle which requires discussing specific image parts, the textual explanation discusses the wheels and the fact that the vehicle has a motor.
Visually, we notice that our attention model is able to point to important visual evidence. For example, for the question "what is the bird doing?" the visual explanation focuses on the bird. For questions like "What game is this?" the model focuses on particular sports equipment which are important for the final answer. Moreover, supporting our initial claims, the attention map that leads to the correct answer and the attention map that leads to a relevant explanation look different, e.g. "monitor has a picture on it" requires looking at the image with a wider angle. Our final observation is that for the pointing task, our generated attention map agrees with the points that human observers deem important, e.g. "playing soccer". ACT-X. Figure 7 shows results on our ACT-X dataset. Textual explanations point to a variety of visual cues important for correctly classifying activities such as pose, e.g.

"crossed legs" for yoga, global context, e.g. "in a gym" for exercise class, and person-object interaction, e.g. "sitting in a kayak" for canoeing/kayaking. These explanations require determining which of many multiple cures are appropriate to justify a particular action.
Our model points to visual evidence important for understanding each human activity. For example to classify "jogging on a mini-trampoline" the model focuses both on the person, who is in a jogging pose, as well as the mini trampoline. Similarly, "boat rowing, stationary" or "biking, stationary" attends to the pose of the person as well as the instrument he is interacting with. Our attentive explanations also agree with human judgment, e.g. for "skipping rope", the position of the hands are highlighted by both humans and our attention module.
Figure 8 shows that explanations on the ACT-X dataset discuss small details important for differentiating between similar classes. For example, when explaining kayaking and windsurfing, it is important to mention the correct sporting equipment instead of image context.
Correct versus incorrect predicted answer/action. Figures 9 and 10 compare explanations when the answer or action label are correctly and incorrectly predicted. In addition to providing an intuition about why predictions are correct, our explanations frequently justify why the model makes incorrect predictions. For example, when incorrectly predicting whether one should stop or go (Figure 9, second row, right), the model outputs "Because the light is green" suggesting that the model has mistaken a red light for a green light, and furthermore, that green lights mean "go". When asked "What is the person doing?" (Figure 9, bottom row, right), the model incorrectly predicts "playing tennis". Though this prediction is incorrect, the explanation "Because he is holding a tennis racket" suggests what visual elements could have confused the VQA model. Figure 10 shows similar trends on the ACT-X dataset. For example, when incorrectly predicting the activity bicycling for an image depicting motor scooter, the explanation "Because he is riding a bicycle down a road with other riders behind him" suggests that the motor scooter may have been misclassified as a bicycle. We reiterate that our model justifies predictions and does not fully explain the inner-workings of deep architectures. However, these justifications demonstrate that our model can output intuitive explanations which could help those unfamiliar with deep architectures make sense of model predictions.
Same question/answer pair or image. Figures 11 and 12 demonstrate that both images and the question/answer pair are needed for good explanations. Figure 11 shows explanations for different images, but with the same question/answer pair. Importantly, explanation text and visualizations change to reflect image content. Figure 12 shows

10

What is the bird doing? Flying. Because it is up in the sky.
What game is this? Tennis Because the man is holding a tennis racket.

Is the laptop turned on? Yes Because there is a picture on the screen.
Does this room look clean? Yes. Because there is no clutter and it is spotless.

What type of vehicle is this person driving? Motorcycle Because it has two wheels and is motorized

What game are they playing? Soccer. Because they are kicking a soccer ball.

What kind of food is this? Pizza Because it is round and it has toppings.

Should the car be stopped at the light? Yes, Because the light is red.

Does this appear to be a sporting event? Yes Because there are people playing tennis

What is the baby elephant doing? Walking. Because the legs are moving forward on the ground.

What kind of animal is lying on the ground?

What are these boys doing? Reading.

Cow. Because it has four legs and looks like a cow.

Figure

6:

VQA-X

qualitative

results:

For the given Because they are looking
them

question at a book in front

o(f top),

we

report

the

predicted

answer

(middle)

and

the

generated

justification explanation (bottom).

Among visualizations, Does this fruit contain vitamin C?

original

image

(left),

attention

map

used to predict the Where is this picture taken? Airport.

answer

followed

by

the

attention

map

used

to generate the Yes Because it is a perfect color

eofxorpanlgae.nation

(middle)

and

pointing

of

human

aBnencaousteathteorerasre for planes otnhthee caonncrsetwe. er

and for the explanation (right). Figure best viewed with zoom.

Should a person carry an umbrella on this day? No. Because the sky is blue.

Callisthenics Because he is holding a handstand between each hand on the pole Is the pizza appropriate for a vegeterian? Yes. Because it is full of vegetables.

Sitting, teaching stretching or yoga Because she is sitting on a yoga mat What gamewiisththhiesr?legs crossed. Baseball Because the player is holding a bat.

Health club exercise classes Because she is in a gym lined on a trampoline in front of a group of people

Bicycling, stationary Because he is sitting on a stationary bike and pedaling it.

Jugging, on a mini tramp Because she is standing on a mini trampoline in a jogging pose
Rope skipping Because he is jumping up and down while swinging a rope over his head
Rowing, stationary Because he is sitting on a rowing machine and leaning back on the handles.

Caribbean dance Because she is wearing a caribbean dress and dancing in front of a group
Coaching Because he is standing on a volleyball court and showing players how to shoot
Skating, ice dancing Because she is wering an ice dancing costime and skating on the ice rink.

Figure 7: ACT-X qualitative results: We report the predicted activity (top) and the generated justification explanation (bot-

tom).

Among

the

visualizations,

original

image (left), the attention Volleyball, indoor Because he is standing on a volleyball

court and holding a volleyball in his

map used to generate the sentence (middleh)andasnd pointing of human

map

used

to

predict

the

activity

followed by the attention Backpacking Because he is walking up a trail with a

annotators for the activity and for the explanation back pack on his back. (right).

that when different questions are asked about the same im-

ages, explanations provide information which are specific

to the questions.

Boxing, punching bag Becayse he is standing in front of a punching bag with boxing gloves on

6. Conclusion Drums, sitting Because she is sitting behind a drum set and playing the drums with sticks. As a step towards explainable AI models, in this work
we introduced a novel attentive explanation model that is capable of providing natural language justifications of decisions as well as pointing to the evidence. We proposed two novel explanation datasets collected through crowdsourcing for visual question answering and activity recognition, i.e. VQA-X and ACT-X. We quantitatively demonstrated that

Canoeing, kayaking, rowing, competitive Because he is sitting in a kayak and rowing with a paddle
both attention and using reference explanations to train our model helps achieve high quality explanations. Further-
Fencing
more, we qualitatively demonstrated that our model Because he is wearing a fienscinag ble uniform and holding a sword in his
to locate the evidence as well as generatihnangds.sentences that a human might mistake for real.
Ashinaabe jingle dance Because he is in a costume in front of a crowd
Acknowledgements
This work was in part supported by DARPA; AFRL; DoD MURI award N000141110688; NSF awards IIS1212798, IIS-1427425, and IIS-1536003, and the Berkeley Artificial Intelligence Research (BAIR) Lab.

11

References
[1] P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice: Semantic propositional image caption evaluation. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.
[2] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
[3] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. Vqa: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015.
[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.
[5] S. Banerjee and A. Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, volume 29, pages 65­72, 2005.
[6] T. Berg and P. N. Belhumeur. How do you tell a blackbird from a crow? In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2013.
[7] O. Biran and K. McKeown. Justification narratives for individual classifications. In Proceedings of the AutoML workshop at ICML, volume 2014, 2014.
[8] K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Nevatia. Abc-cnn: An attention based convolutional neural network for visual question answering. arXiv:1511.05960, 2015.
[9] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollar, and C. L. Zitnick. Microsoft COCO captions: Data collection and evaluation server. arXiv:1504.00325, 2015.
[10] M. G. Core, H. C. Lane, M. Van Lent, D. Gomboc, S. Solomon, and M. Rosenberg. Building explainable artificial intelligence systems. In Proceedings of the national conference on artificial intelligence, volume 21, page 1766. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.
[11] A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Batra. Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions? In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.

[12] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. Efros. What makes paris look like paris? ACM Transactions on Graphics, 31(4), 2012.

[13] J. Donahue, L. A. Hendricks, M. Rohrbach, S. Venugopalan, S. Guadarrama, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016.

[14] V. Escorcia, J. C. Niebles, and B. Ghanem. On the relationship between visual attributes and convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.

[15] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach. Multimodal compact bilinear pooling for visual question answering and visual grounding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.

[16] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact bilinear pooling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

[17] Y. Goyal, A. Mohapatra, D. Parikh, and D. Batra. Towards Transparent AI Systems: Interpreting Visual Question Answering Models. In Proceedings of the Workshop on Visualization for Deep Learning at ICML, 2016.

[18] J. Harel and C. Koch.

A

saliency

implementation

in

matlab.

http://www.klab.caltech.edu/~harel/share/gbvs.php.

[19] L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, and T. Darrell. Generating visual explanations. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.

[20] X. Hou, J. Harel, and C. Koch. Image signature: Highlighting sparse salient regions. IEEE transactions on pattern analysis and machine intelligence, 34(1):194­ 201, 2012.

[21] W. L. Johnson. Agents that learn to explain themselves. In Proceedings of the Conference on Artificial Intelligence (AAAI), pages 1257­1263, 1994.

[22] J. Kim, K. W. On, J. Kim, J. Ha, and B. Zhang. Hadamard product for low-rank bilinear pooling. CoRR, abs/1610.04325, 2016.

[23] C. Lacave and F. J. D´iez. A review of explanation methods for bayesian networks. The Knowledge Engineering Review, 17(02):107­127, 2002.

[24] H. C. Lane, M. G. Core, M. Van Lent, S. Solomon, and D. Gomboc. Explainable artificial intelligence for

12

training and tutoring. Technical report, DTIC Document, 2005.
[25] C.-Y. Lin. Rouge: a package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, 2004.
[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla´r, and C. L. Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision (ECCV), pages 740­755. Springer, 2014.
[27] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla´r, and C. L. Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision (ECCV), 2014.
[28] M. Lomas, R. Chevalier, E. V. Cross II, R. C. Garrett, J. Hoare, and M. Kopack. Explaining robot actions. In ACM/IEEE HRI, 2012.
[29] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neurons: A neural-based approach to answering questions about images. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015.
[30] A. Mallya and S. Lazebnik. Learning models for actions and person-object interactions with transfer to question answering. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.
[31] G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller. Introduction to wordnet: An on-line lexical database*. International journal of lexicography, 3(4):235­244, 1990.
[32] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 311­318, 2002.
[33] O. Pele and M. Werman. Fast and robust earth mover's distances. In 2009 IEEE 12th International Conference on Computer Vision, pages 460­467. IEEE, September 2009.
[34] L. Pishchulin, M. Andriluka, and B. Schiele. Finegrained activity recognition with holistic and pose based features. In Proceedings of the German Confeence on Pattern Recognition (GCPR), pages 678­ 689. Springer, 2014.
[35] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning deep representations of fine-grained visual descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[36] S. Reed, Z. Akata, S. Mohan, S. Tenka, B. Schiele, and H. Lee. Learning what and where to draw. In

Advances in Neural Information Processing Systems (NIPS), 2016.
[37] Y. Rubner, C. Tomasi, and L. J. Guibas. A metric for distributions with applications to image databases. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 1998.
[38] K. J. Shih, S. Singh, and D. Hoiem. Where to look: Focus regions for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[39] E. H. Shortliffe and B. G. Buchanan. A model of inexact reasoning in medicine. Mathematical biosciences, 23(3):351­379, 1975.
[40] M. Van Lent, W. Fisher, and M. Mancuso. An explainable artificial intelligence system for small-unit tactical behavior. In NCAI, 2004.
[41] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4566­ 4575, 2015.
[42] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, Caltech, 2010.
[43] M. R. Wick and W. B. Thompson. Reconstructive expert system explanation. Artificial Intelligence, 54(12):33­70, 1992.
[44] C. Xiong, S. Merity, and R. Socher. Dynamic memory networks for visual and textual question answering. In Proceedings of the International Conference on Machine Learning (ICML), 2016.
[45] H. Xu and K. Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.
[46] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked attention networks for image question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[47] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In Proceedings of the European Conference on Computer Vision (ECCV). 2014.
[48] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Object detectors emerge in deep scene cnns. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.
[49] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7W: Grounded Question Answering in Images. In

13

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
14

Sailing. Because he is sitting in a boat and tying a boat with the water.

Kayaking. Because he is sitting in a kayak and paddling across the water.

Bicycling, racing, road. Because he is riding a bicycle down the hill with other riders behind him.

Windsurfing. Because he is standing on a windsurfing board and holding onto the sail.

Cicyling, BMX. Because he is riding a BMX bike toward a flight of stairs with a fishing foot.

Canoeing, kayaking, rowing, competitive. Because he is sitting in a kayak and rowing with a paddle. Unicycling. Because he is riding a unicycle on the street and holding a selfie stick.

Figure 8: ACT-X results with similar activities. Figure on the left: We show results with fine-grained activities all related to sailing, canoeing, kayaking and observe that both the fine-grained activities are correctly predicted and the explanations match the activity and the image. Figure on the right: We show results with fine-grained activities all related to bicycling and observe that both the fine-grained activities are correctly predicted and the explanations match the activity and the image.

Answer is correctly predicted:
What are these boys doing? GT & P = Reading. Because they are looking at a book in front of them
Does this fruit contain vitamin C? GT & P = Yes Because it is a perfect color of orange.
Is the pizza appropriate for a vegeterian? GT & P = Yes. Because it is full of vegetables.
What kind of animal is lying on the ground? GT & P = Cow. Because it has four legs and looks like a cow.
Where is this picture taken? GT & P = Airport. Because there are planes on the concrete.
Should a person carry an umbrella on this day? GT & P = No. Because the sky is blue.
What game is this? GT & P = Baseball Because the player is holding a bat.

Answer is incorrectly predicted:
Should we stop? GT = Yes. P = No. Because the light is green.
Is this a woman? GT = Yes. P = No. Because she is wearing a suit.
What is the bear doing? GT = Swimming. P = Eating. Because he is hungry and likes food.
What type of vegetable is pictured? GT = Tomato. P = Orange. Because it is round and orange.
What is the person doing? GT = Skiing. P = Snowboarding. Because the is riding a snowboard down a snowy mountain
What is he doing? GT = Jumping. P = Playing wii.. Because he is holding the controller up in the way.
What is the person doing? GT = Stretching. P = Playing Tennis. Because he is holding a tennis racket.

Figure 9: VQA-X results. Figure on the left: We show various qualitative results with correctly predicted answer and observe that the explanation justifies the answer accordingly. Figure on the right: We show results with incorrectly predicted answer and observe that although the answer is incorrect, our model can provide visual and textual explanations on why the model might be failing in those cases.

15

Action label is correctly predicted:
GT & P = Volleyball, indoor Because he is standing on a volleyball court and holding a volleyball in his hands
GT & P = Boxing, punching bag Becayse he is standing in front of a punching bag with boxing gloves on
GT & P = Drums, sitting Because she is sitting behind a drum set and playing the drums with sticks.
GT & P = Backpacking Because he is walking up a trail with a back pack on his back.
GT & P = Canoeing, kayaking, rowing Because he is sitting in a kayak and rowing with a paddle
GT & P = Fencing Because he is wearing a fencing uniform and holding a sword in his hands.
GT & P = Ashinaabe jingle dance Because he is in a costume in front of a crowd

Action label is incorrectly predicted:
GT = piano sitting, P = cello sitting Because he is sitting on a chair and playing cello with the water.
GT = snowshoeing, P = skiing Because he is wearing skis and skiing down a snowy slope.
GT = painting furniture, P = carpentry Because he is standing in a workshop and reaching on a board.
GT = sitting, playing instrument in church, P = violin, sitting Because he is sitting on a chair and has a violin up to his chin.
GT = manual labor, P = chopping wood Because he is sitting in front of a log and chopping it with an axe.
GT = motor scooter, P = bicycling Because he is riding a bicycle down a road with other riders behind him.
GT = slimnastics, jazzexercise, P = ballet, modern or jazz Because she is standing in a ballet studio with her arms spread wide.

Figure 10: ACT-X results. Figure on the left: We show various qualitative results with correctly predicted answer and observe that the explanation justifies the answer accordingly. Figure on the right: We show results with incorrectly predicted answer and observe that although the answer is incorrect, our model can provide visual and textual explanations on why the model might be failing in those cases.

What is the man doing? Surfing. Because the man is riding a wave on a surfboard.

What room is this? Kitchen. Because there is a stove and a blender.

Because he is on a surfboard.

Because there is a refrigerator and a sink in the room.

What game are they playing? Wii. Because the man is holding a wii remote.
Because she is holding a wiimote.

What kind of animal is this? Cow. Because it has four legs and looks like a cow.
Because they are grazing in a field like cows.

Figure 11: VQA-X results with the same question/answer pair. We select results with the same question and answer pair with two different images and show that although the QA pairs are the same, for different images our model generates different explanations. (Answers are correctly predicted)
16

What is the bird doing? Walking. Because they are on the ground
What is the color of the seats? Green. Because they color of the trees and forest indicate.
Does this particular fruit contain vitamin c? Yes. Because it is a perfect color.
What type of citrus fruit is this? Orange. Because it is round and orange and has a peel.

What activity are the people doing? Skiing. Because they are on skis and going down a slope.
Is it summer? No. Because there is snow.
Is this person going to get wet? Yes. Because he is crushing a wave right now.
What is the man doing? Surfing. Because he is on a surfboard.

Figure 12: VQA-X results with same image and different questions. We select results with the same image and different Q/A pairs and show that although the images are the same, our model is able to answer the questions differently and generate a different explanation accordingly. (Answers are correctly predicted)

Is this a competition? Yes. Because there are people gathered around a small area.
What are these people doing? Skiing. Because they are on skis and going down a slope.

Does this room look clean? Yes. Because it is spotless and there is no clutter.
Is the toilet bowl closed? Yes. The toilet seat is in the downward position and I can't see the bowl.

17


,sentence,label,data,regex
0,Ranking Documents Through Stochastic Sampling on Bayesian Network-based Models: A Pilot Study,0,,False
1,"Xing Tan1, Jimmy Xiangji Huang1 and Aijun An2",0,,False
2,"Information Retrieval and Knowledge Management Research Lab 1School of Information Technology, 2Department of Computer Science & Engineering",0,,False
3,"York University, Toronto, Canada",1,ad,True
4,"{xtan, jhuang}@yorku.ca, aan@cse.yorku.ca",0,,False
5,ABSTRACT,0,,False
6,"Using approximate inference techniques, we investigate in this paper the applicability of Bayesian Networks to the problem of ranking a large set of documents. Topology of the network is a bipartite. Network parameters (conditional probability distributions) are determined through an adoption of the weighting scheme tf -idf . Rank of a document with respect to a given query is defined as the corresponding posterior probability, which is estimated through performing Rejection Sampling. Experimental results suggest that performance of the model is at least comparable to the baseline ones such as BM 25. The framework of this model potentially offers new and novel ways in weighting documents. Integrating the model with other ranking algorithms, meanwhile, is expected to bring in performance improvement in document ranking.",1,ad,True
7,Keywords,0,,False
8,Info. Retrieval; Bayesian Networks; Stochastic Sampling,0,,False
9,1. INTRODUCTION,1,DUC,True
10,"Probabilistic Graphical Models [10] in the form of Bayesian Networks (BN) [6] are widely used to represent knowledge with uncertainties. In the recent years, computational technologies and tools for BN-based models are becoming increasingly powerful. That being the case, modeling problems in Information Retrieval (IR), in particular for document ranking, as probabilistic inference problems in BNbased models has achieved only limited success to date. Major reasons for such relatively small progress in BN-based approaches for document ranking are, for one, conceptually it is challenging to appropriately identify causalities in document ranking (i.e., deciding network topology) and then to accurately capture the uncertainties (i.e., deciding network parameters) in order to construct a BN model for IR; and two, computationally exact inference algorithms associated with the model is bound to be intractable as practically the size of the BN model in terms of nodes representing both the number of documents and vocabulary size in words, can be easily in a few millions.",0,,False
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.",1,ad,True
12,"SIGIR '16, July 17-21, 2016, Pisa, Italy",0,,False
13,c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00,0,,False
14,DOI: http://dx.doi.org/10.1145/2911451.2914750,0,,False
15,"In this paper, we investigate the applicability of BNs to the problem of ranking a large set of documents. We propose a model, which specifically takes into considerations both the appropriateness in the semantics of causalities, and the computational tractability of probabilistic inferences. Topology of the network is a bipartite. Conditional probability distributions are determined through adopting the weighting scheme tf -idf . Experimental results, obtained from working on both computer-generated and standard document sets suggest that performance of the model is at least comparable to the baseline ones such as BM 25 ([2, 11]).",1,ad,True
16,The remainder of this paper is organized as follows. Section 2 presents the background and preliminaries. Section 3 introduces the model. Experimental results are reported and analyzed in Section 4. Section 5 concludes the paper.,0,,False
17,2. PRELIMINARIES,0,,False
18,"In this section, document ranking in IR, and BN, are briefly reviewed.",0,,False
19,2.1 Document Ranking in IR,0,,False
20,"One of the fundamental tasks in IR is document-ranking: Given a set of documents D such that D ,"" {d1, . . . , dM } and |D| "","" M , a set of terms T "","" {t1, . . . , tN } and |T | "","" N , and a collection of query terms q such that q  T , documents in D need to be ranked in a complete order according to their respective relevance to q (other criteria such as """"diversity"""" might also be considered for ranking). To do this, a typical approach is to define a score function score(q, di) that returns a numeric score for each document di  D with respect to q. Documents can then be ranked on their scores in descending order. The top S elements will be selected to construct the set S, where |S| "", S.",0,,False
21,"Let tft,d denote term frequency, the number of occurrences of term t in document d; dft denotes document frequency, the number of documents in D that contain the term t; and idft ,"" log (M/dft) denotes inverse document frequency. Summing up on tft,d × idft for each term t  q with respect to d defines a baseline score function for document ranking: score(q, d) "","" tq(tft,d ×idft). Definitions for variant score functions of tf -idf such as BM25, can be found in [11]. In addition, collection frequency in D and its subset S, are defined as the total number of occurrences of t in D and in S, and denoted by cft and sft, respectively.""",1,ad,True
22,2.2 Bayesian Networks,0,,False
23,"A Bayesian Network is a directed acyclic graph where nodes correspond to random variables [6]. Pairs of nodes in the graph might be connected by a directed edge. For example, given two nodes X and Y in the graph, if X enters Y , it is said that X is a parent of Y . Effect of the par-",0,,False
24,961,0,,False
25,ents of a node X in the graph is quantified by a conditional probability distribution P (X|P arents(X)).,0,,False
26,"BNs are often used to carry out probabilistic inference: computing posterior distribution of a set of variables given a set of evidence variables ­ variables whose values are observed or assigned. Consider a simple example of Baby World, which consists of four random variables H, T , C, and R, corresponding to the variable facts that the baby is Hungary, Tired, Crying, and scReaming, respectively. A BN for this world is shown in Figure 1. After all four conditional probability distributions as listed in the figure are specified, we could compute, for example, the probability that the baby is hungry if we observe that it is crying but not screaming: P (H is true | C is true, and R is false).",0,,False
27,P (H) Hungry? H,0,,False
28,P (T ) T Tired?,0,,False
29,"P (C|H, T ) C Crying?",0,,False
30,"P (R|H, T ) scReaming? R",0,,False
31,Figure 1: A Bayesian Network: Baby World.,0,,False
32,3. MODEL,0,,False
33,"This section presents the model, explains how samplings are performed, and justifies the merits of our model.",0,,False
34,3.1 Network Topology and Conditional Probabilities,0,,False
35,"Suppose a user specifies a set of terms as a query q for a set of documents D, a subset S  D of documents need to be retrieved and ranked in a complete order. To be explained in this section, we formulate this problem into the problems of calculating posterior probability values in a BN-based model where the original query is treated as the observed evidence.",0,,False
36,"In our model, the probability space is induced accordingly by two sets of random variables D (document random variables) and T (term random variables), where Di  D for 1  i  M , and Tj  T for 1  j  N . Each document Di takes two values: V al(Di) ,"" {d1i , d0i }, which represents the values of """"Di selected with respect to a query"""" (d1i ) or not (d0i ); Similarly, V al(Tj) "","" {t1j , t0j }, which represents the values of term """"Tj is a query term"""" (t1j ) or not (t0j ). The BN, as shown in Figure 2, is a two-layer directed graph, which contains a node in the top layer for each document variable Di and a node in the bottom layer for each term variable Tj. In the graph, an edge from Di to Tj represents that term Tj appears in the document Di. We assume no edges between document variables in D, and no edges between term variables in T . In addition to the graph, two types of""",1,ad,True
37,D,0,,False
38,D1 D2,0,,False
39,... ...,0,,False
40,DM,0,,False
41,T,0,,False
42,T1,0,,False
43,T2,0,,False
44,... ...,0,,False
45,... ...,0,,False
46,TN,0,,False
47,Figure 2: A BN graph for document ranking.,0,,False
48,"probabilities need to be specified to capture the nature of the dependence of variables: P (Di), the prior distribution over a document Di, and P (Tj|D1, . . . , DM ), the conditional probability distribution of Tj given D1, D2, . . . , and DM . In our model, P (Di) represents the distribution that Di is selected in S or not, hence it is reasonable to define, for any",0,,False
49,"document Di in D, the probability that Di is eventually selected into S equals the ratio of the size of S to the size of D, i.e., P (d1i ) , S/M .",0,,False
50,"The conditional distribution P (Tj|D1, . . . , DM ) specifies the distribution over the term Tj, which depends on the actual content of S, the set of documents selected. That is to say, specifically, for each subset of D (totally 2M of them), we need to specify a distribution for t1j , the event that tj is actually a query term. Since the number of parents of a term in the network is not bounded by a constant, we know that exact inference here has exponential time complexity in worst cases. Nevertheless what we really need is to calculate, for any document variable Di where 1  i  M , the relevance of Di to evidence q~, i.e., the posterior probability of P (d1i |q~), the value of which can be estimated through stochastic sampling. For simplicity in explanation, we assume that q~ contains only one term t, without loss of generality.",0,,False
51,3.2 Estimating Posteriors Through Sampling,0,,False
52,General reasoning problems of probabilistic inference in,0,,False
53,"BNs, and even their corresponding approximate versions,",0,,False
54,"are NP-hard ([4], [5]). Due to this computational intractabil-",1,NP,True
55,"ity, one often turns to randomized sampling methods (e.g.,",0,,False
56,"Rejection Sampling [12], which is used in our current re-",0,,False
57,search) to approximate posterior probabilities. Asymptoti-,0,,False
58,cally accuracy of these sampling methods would usually be,0,,False
59,improved as the number of samples increases.,0,,False
60,Given a BN and its specified prior distributions for all n,0,,False
61,"variables {X1, X2, . . . , Xn}, where Xi  X for 1  i  n,",0,,False
62,forward sampling samples all nodes in X in an order consis-,0,,False
63,"tent with the topological structure of the BN, and the prob-",0,,False
64,"ability of a specific event, written as (x1, . . . , xn)1, generated",0,,False
65,from forward sampling equals to,0,,False
66,"n i,1",0,,False
67,P,0,,False
68,(xi|,0,,False
69,"parents(Xi)),",0,,False
70,"which in turn equals to the joint distribution P (x1, . . . , xn).",0,,False
71,"Suppose totally N samples are taken, and the number of oc-",0,,False
72,"currences of an event (x1, . . . , xn) equals to N(x1,...,xn), then the ratio N(x1,...,xn)/N is an approximation to P (x1, . . . , xn). With observation of evidence e for E, where E  X , the con-",0,,False
73,"ditional probability P (X , x|e) can be further estimated",0,,False
74,"through Rejection: first, all samples that do not match e",0,,False
75,"are rejected in N , to obtain N1; second, all samples in N1",0,,False
76,"and compatible with X ,"" x are put into N2; third, N2/N1""",0,,False
77,"is an estimate to P (X , x|e).",0,,False
78,Consider our model again and suppose there are P sam-,0,,False
79,"ples where |P| ,"" P . During sampling, we dynamically main-""",0,,False
80,"tain a vector of counters C, where |C| , M . All counters",0,,False
81,in C are initialized to zero. Our sampling strategy say for,0,,False
82,"the jth sample P j where 1  j  P : Step 1, each docu-",0,,False
83,ment variable is sampled according to the distribution S/M ;,0,,False
84,"Those selected document variables are put into Sj, thus the",0,,False
85,"expected value of |Sj| is S. Step 2, we accept this sample",0,,False
86,if and only if the collection frequency of Sj with respect to,0,,False
87,"term t proportionally exceeds the one of D. Formally, the",0,,False
88,sample is accepted iff,0,,False
89,sftj cft,0,,False
90,>,0,,False
91,S M,0,,False
92,.,0,,False
93,If sample P j,0,,False
94,is accepted,0,,False
95,"and Di  Sj, ci  C, which is the corresponding counter for",0,,False
96,"Di, would be increased by one.",0,,False
97,"After completion of sampling, the set P would be mutual-",0,,False
98,"exclusively partitioned into two sets, the set of accepted sam-",0,,False
99,"ples Paccepted, and the set of rejected ones Prejected. The",0,,False
100,vector C would be updated for |Paccepted| times. Values,0,,False
101,"1The term (x1, . . . , xn) is an abbreviation for (X1 ,"" x1, . . . , Xn "", xn).",0,,False
102,962,0,,False
103,"stored in the counters of C, are actually scores for their corresponding documents with respect to the query term t. The documents can thus be ranked according to their scores.",0,,False
104,3.3 Justification of Methodology,0,,False
105,"In the literature, considerable research in investigating potential linkages between BNs and IR in general, has been reported (most notably [1], [7], [13], [14]). The originality and value of our research contribution lies in the following facts.",0,,False
106,"Model Semantics. The model defines 2(M+N) different states, for different combinations of truth assignments to all random variables in D  T . A state specifies an instance of which random variables are true and which are false. For each state, its joint probability distribution theoretically can be calculated (although computationally it might be impractical). We are only interested in those states where statistically S out of D variables are true, since we only concern about the problem of selecting S out of D documents related to a given query.",0,,False
107,"Causality. In the model, document variables in D are designed, in consistence with common perceptions, to have direct causal influence on term in T . For example, causal relation ""Di  Tj"" is interpreted as: If Di is selected to be a member of S (i.e., Di  S) then the term Ti should be of interest to the user.",0,,False
108,"Scalability. The size of the problem of document ranking in practise is often in the magnitude of a few millions, if not more. A network built-up from these problems is large in size and multiply connected, making it dauntingly challenging to perform exact probabilistic inference. Consequently, it can be seen from the literature that experiments in earlier work (e.g., [7] and [1]), are restricted to cases with maximal a few thousand documents only. The development of BNs however has reached the point where approximate inference algorithms, such as randomized sampling, loopy propagation, or variational approximation, can make a practical difference. We adopt a direct sampling method in this research.",1,ad,True
109,"Bipartite Network Structure. The underlying undirected graph of the network is a bipartite: nodes are grouped into two types, only connections between nodes from two different types are allowed. Recently, Bayesian models on bipartite graphs have found their ways to modeling real-world applications in social networks, with appealing properties demonstrated [3]. It remains to be investigated how these results can be utilized into our own framework of BN for IR. Nevertheless, due to this simplicity of topological structure, additional features (e.g., ontological/relational/logical representation and reasoning: see [8] for a mosaic of such proposals) can be incorporated into the current model.",1,ad,True
110,4. EXPERIMENTAL RESULTS,0,,False
111,"In this section we compare the performance of our proposed model with the ones of tf -idf , BM 25 and Golden (to be explained in Section 4.1). We first work on a set of computer-generated random documents (DR1 , DR2 , DR3 and DR4 ) and then on D1 and D2, which are two subsets of WT2G, a standard TREC test collection.",1,WT,True
112,4.1 Documents Generation,0,,False
113,"To simplify matters, we assume that all generated documents in DR are with same document length, which equals to the size of the vocabulary TR. For any document Dr  DR, occurrences of terms in Dr follow a Normal Distribution N (µ, ), where the values of mean µ and the standard de-",0,,False
114,Term Frequency Term Frequency,0,,False
115,An Example Document,0,,False
116,3000,0,,False
117,Center Shifted,0,,False
118,3000,0,,False
119,2500,0,,False
120,2500,0,,False
121,2000,0,,False
122,"|D| , 100000",0,,False
123,2000,0,,False
124,"|D| , 100000",0,,False
125,"µ , 50",0,,False
126,"µ , 18",0,,False
127," , 17",0,,False
128," , 17",0,,False
129,1500,0,,False
130,1500,0,,False
131,1000,0,,False
132,1000,0,,False
133,500,0,,False
134,500,0,,False
135,0 0 10 20 30 40 50 60 70 80 90 100,0,,False
136,Terms,0,,False
137,0 0 10 20 30 40 50 60 70 80 90 100,0,,False
138,Terms,0,,False
139,Figure 3: An example document (created according,0,,False
140,"to a Normal Distribution with µ , 50 and  ,"" 17),""",0,,False
141,"and its variant with the center shifted to 18 (µ , 18).",0,,False
142,"viation  can be adjusted. However all documents in DR share the same µ and . Note that the smaller the value of , the terms cluster more closely to µ. The center of a given document is shifted to a random center before being stored into a vector. To illustrate the idea, an example document with µ ,"" 50,  "","" 17, |DR| "","" 10000, and |TR| "","" 100, and its variant with shifted center (now, µ "", 18) is respectively shown in the left subplot (and the right one) of Figure 3.",1,ad,True
143,"Accordingly we introduce the baseline ranking method Golden. That is, given an input query q and a document d, the score(q, d) is defined as the difference between the center of d, and q. When the value of score(q, d) is smaller (greater), it means document d is more (less) related to the term q. Golden is used as one of the three methods for comparisons in Figure 4.",0,,False
144,4.2 Experiments,0,,False
145,"We first work on four sets of randomly generated documents: DR1 , where  ,"" 1250, DR2 ,  "","" 1000, DR3 ,  "","" 833, DR4 ,  "","" 416. Between these sets from DR1 up to DR4 , documents are more closely clustered around their means, thus intuitively more reliable in the sense of IR. Collection size, document length, and vocabulary size, are set to be all equal: |DR| "", |TR| , 10000. Experiments related to a specific set is pictorially summarized in the corresponding sub-figure in Figure 4.",0,,False
146,"Consider Figure 4.a, for example, a term is queried against DR1 , three different ranking methods, i.e., Golden, BM25, and Rejection, return three different completely ordered sequences of 500 elements (the number 500 is obtained from |DR| × 0.05, where 0.05 is the pre-specified ratio, i.e., portion of all documents in DR1 need to be ranked). Results of pair-wise comparisons between these three methods are reported in Figure 4.a, where x-axis values indicate sample sizes and y-axis values indicate how many documents out of the 500 ranked ones are actually agreed between two given methods. For example, the red dot pointed by the arrow in Figure 4.a refers to the fact that totally 336 documents are shared by the 500 documents retrieved from applying BM25 on DR1 , and the 500 elements obtained from performing Rejection Sampling on DR1 (with the sample size equaling to 0.1 Million).",0,,False
147,"Documents in both D1 and D2 (Figure 5) are drawn from dataset collection WT2G where |D1| , |D2| ,"" 2500, |T1| "", 50961 and |T2| ,"" 127487. First 100 elements obtained from three different ranking methods, tf -idf , BM 25, and Rejection are pair-wise compared in Figure 5.""",1,WT,True
148,4.3 Brief Discussion,0,,False
149,"This section draws the major observations from the present experimental study, and discusses some implications.",0,,False
150,963,0,,False
151,Num. of Agreed Documents Num. of Agreed Documents,0,,False
152,"(a) DR1 and  , 1250",0,,False
153,500,0,,False
154,Golden-BM25,0,,False
155,450,0,,False
156,Golden-Rejection,0,,False
157,BM25-Rejection,0,,False
158,400,0,,False
159,350,0,,False
160,300 250,0,,False
161,200,0,,False
162,150,0,,False
163,100,0,,False
164,50,0,,False
165,0,0,,False
166,0,0,,False
167,1K,0,,False
168,10K 50K 0.1M 0.5M 1M 1.5M 2M,0,,False
169,Sample Size,0,,False
170,"(c) DR3 and  , 833",0,,False
171,500,0,,False
172,Golden-BM25,0,,False
173,450,0,,False
174,Golden-Rejection,0,,False
175,BM25-Rejection,0,,False
176,400,0,,False
177,350,0,,False
178,300,0,,False
179,250,0,,False
180,200,0,,False
181,150,0,,False
182,100,0,,False
183,50,0,,False
184,0,0,,False
185,0,0,,False
186,1K,0,,False
187,10K 50K 0.1M 0.5M 1M 1.5M 2M,0,,False
188,Sample Size,0,,False
189,"(b) DR2 and  , 1000",0,,False
190,500,0,,False
191,Golden-BM25,0,,False
192,450,0,,False
193,Golden-Rejection,0,,False
194,BM25-Rejection,0,,False
195,400,0,,False
196,350,0,,False
197,300,0,,False
198,250,0,,False
199,200,0,,False
200,150,0,,False
201,100,0,,False
202,50,0,,False
203,0,0,,False
204,0,0,,False
205,1K,0,,False
206,10K 50K 0.1M 0.5M 1M 1.5M 2M,0,,False
207,Sample Size,0,,False
208,"(d) DR4 and  , 416",0,,False
209,500,0,,False
210,Golden-BM25,0,,False
211,450,0,,False
212,Golden-Rejection,0,,False
213,BM25-Rejection,0,,False
214,400,0,,False
215,350,0,,False
216,300,0,,False
217,250,0,,False
218,200,0,,False
219,150,0,,False
220,100,0,,False
221,50,0,,False
222,0,0,,False
223,0,0,,False
224,1K,0,,False
225,10K 50K 0.1M 0.5M 1M 1.5M 2M,0,,False
226,Sample Size,0,,False
227,"Figure 4: Pair-wise comparisons among the ranking methods Golden, BM25, and Rejection on data-sets with different standard deviations.",0,,False
228,Num. of Agreed Documents,0,,False
229,(a) Data Set: D1,0,,False
230,100,0,,False
231,90,0,,False
232,80,0,,False
233,70,0,,False
234,60,0,,False
235,50,0,,False
236,40,0,,False
237,30,0,,False
238,20,0,,False
239,TfIdf-BM25,0,,False
240,10,0,,False
241,TfIdf-Rejection,0,,False
242,BM25-Rejection,0,,False
243,0,0,,False
244,0,0,,False
245,1K,0,,False
246,5k,0,,False
247,7.5k 10K 50K 0.1M 0.5M 1M,0,,False
248,Sample Size,0,,False
249,Num. of Agreed Documents,0,,False
250,(b) Data Set: D2,0,,False
251,100,0,,False
252,90,0,,False
253,80,0,,False
254,70,0,,False
255,60,0,,False
256,50,0,,False
257,40,0,,False
258,30,0,,False
259,20,0,,False
260,TfIdf-BM25,0,,False
261,10,0,,False
262,TfIdf-Rejection,0,,False
263,BM25-Rejection,0,,False
264,0,0,,False
265,0,0,,False
266,1K,0,,False
267,5k,0,,False
268,7.5k 10K 50K 0.1M 0.5M 1M,0,,False
269,Sample Size,0,,False
270,"Figure 5: Pair-wise comparisons among the ranking methods tf-idf, BM25, and Rejection on two datasets (2500 documents each) from WT2G.",1,WT,True
271,"For more closely clustered documents, in essence all methods agree more on their rankings (as shown in Figure 4, they agree most on the set DR4 , but least on DR1 ); Following the same argument, we should claim that D1 is more clustered than D2.",0,,False
272,"In our experimental settings, increasing sample size initially improves the performance sharply, but it tends to be leveling off after sample size is greater than certain value (e.g., 0.5M in the subplots of Figure 4). As the sample size increases, asymptotically the Rejection method agrees at least 80% with BM25 for almost all sets except for D2 (around 60% only). It seems that we should conclude that the proposed BN-based model can achieve competitive performance levels at relatively low cost in sampling.",0,,False
273,"Since in Rejection, estimating posterior probabilities are based on the tf -idf ranking scheme, Rejection is a stochastic variant to the tf -idf ranking method. BM 25, meanwhile, can be deemed as a refinement to tf -idf . Hence, it is not a surprise that BM 25 agrees largely with Rejection. The more intriguing observation is that, with standard data-sets, the two methods disagree at least on 20% of their rankings. The reason as we speculate is either Rejection behaves somewhat differently from BM 25 in practise, or sampling with the current settings can get trapped and do not converge. In order to unravel this intricacy, a further investigation is necessary and desirable.",0,,False
274,Num. of Agreed Documents Num. of Agreed Documents,0,,False
275,5. SUMMARY & FUTURE WORK,0,,False
276,"In this paper, document ranking in IR is transformed into the problems of estimating posterior probabilities through stochastic sampling on a BN designed for IR. Experimental results from this pilot study is quite encouraging in the sense that, with moderate sampling efforts, the model demonstrates its ranking capability comparable to BM 25. Additionally, graph-based structure and probability-based parameters of the model, together with other considerations in the model, suggest that new and novel weighing schemes for document ranking are conjecturally within a reach.",0,,False
277,"Among many possible avenues, our direct future research includes 1) further evaluating performance of the model on WT2G, WT10G, and other standard dataset collections; 2) testing on parametric settings other than the current one that is based on term-frequency; 3) testing other sampling strategies (e.g., Gibbs Sampling [10], and the most recent ones [9]) to improve sampling efficiency and performance.",1,WT,True
278,"Feedback to this work we have received encourages us to investigate in a broader context the relationships between the proposed BN model and other term weighting models ([15, 16]). With ease, most existing probability theory-based models in IR can actually be derived in this BN-based framework. It is thus rather promising to exploit this framework for a deeper understanding of existing term weighting models, and for the developments of new and better models in Information Retrieval.",1,ad,True
279,6. ACKNOWLEDGEMENTS,0,,False
280,"We gratefully acknowledge the anonymous reviewers for their insightful comments and suggestions. This research is supported by a Discovery grant from the Natural Sciences and Engineering Research Council of Canada (NSERC), an NSERC CREATE award in ADERSIM2 and an ORF-RE (Ontario Research Fund - Research Excellence) award in BRAIN Alliance3.",1,ad,True
281,7. REFERENCES,0,,False
282,[1],0,,False
283,[2] [3],0,,False
284,[4] [5] [6] [7] [8] [9],0,,False
285,[10] [11] [12] [13] [14] [15] [16],0,,False
286,"S. Acid, L. M. de Campos, J. M. Fern´andez-Luna, and J. F. Huete. An information retrieval model based on simple Bayesian networks. Int. J. Intell. Syst., 18(2):251­265, 2003.",0,,False
287,"M. Beaulieu, M. Gatford, X. Huang, S. Robertson, S. Walker, and P. Williams. Okapi at TREC-5. In Proc. of TREC, pages 143­166, 1996.",1,TREC,True
288,"F. Caron. Bayesian nonparametric models for bipartite graphs. In Advances in Neural Information Processing Systems 25, pages 2051­2059. Curran Associates, Inc., 2012.",0,,False
289,"G. F. Cooper. The computational complexity of probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42(2):393 ­ 405, 1990.",0,,False
290,"P. Dagum and M. Luby. Approximating probabilistic inference in Bayesian belief networks is NP-hard. Artificial Intelligence, 60(1):141 ­ 153, 1993.",1,NP,True
291,"P. A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge University Press, New York, NY, USA, 1st edition, 2009.",0,,False
292,"R. Fung and B. Del Favero. Applying Bayesian networks to information retrieval. Commun. ACM, 38(3):42­ff., Mar. 1995.",0,,False
293,"L. Getoor and B. Taskar. Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning). The MIT Press, 2007.",0,,False
294,"K. Kandasamy, J. G. Schneider, and B. P´oczos. Bayesian active learning for posterior estimation - IJCAI-15 distinguished paper. In Proc. of the 24th IJCAI, pages 3605­3611, 2015.",0,,False
295,"D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning. The MIT Press, 2009.",0,,False
296,"C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA, 2008.",0,,False
297,"S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall Press, Upper Saddle River, NJ, USA, 3rd edition, 2009.",1,ad,True
298,"H. Turtle and W. B. Croft. Inference networks for document retrieval. In Proc. of the 13th ACM SIGIR, pages 1­24, New York, NY, USA, 1990.",0,,False
299,"H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Trans. Inf. Syst., 9(3):187­222, July 1991.",0,,False
300,"J. Zhao, J. X. Huang, and B. He. CRTER: Using Cross Terms to Enhance Probabilistic IR. In Proc. of the 34th ACM SIGIR, pages 155­164, 2011.",0,,False
301,"J. Zhao, J. X. Huang, and Z. Ye. Modeling Term Associations for Probabilistic Information Retrieval. ACM Trans. Inf. Syst., 32(2):1­47, 2014.",0,,False
302,2http://www.yorku.ca/adersim 3http://brainalliance.ca,1,ad,True
303,964,0,,False
304,,0,,False

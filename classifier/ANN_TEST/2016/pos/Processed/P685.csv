,sentence,label,data,regex
0,Evaluating Retrieval over Sessions: The TREC Session Track 2011­2014,1,Session,True
1,Ben Carterette,0,,False
2,"University of Delaware, Newark, DE, USA",0,,False
3,carteret@cis.udel.edu,0,,False
4,Paul Clough,0,,False
5,Mark Hall,0,,False
6,"University of Sheffield,",0,,False
7,"Edge Hill University, Ormskirk,",0,,False
8,"Sheffield, UK",0,,False
9,UK,0,,False
10,p.d.clough@sheffield.ac.uk hallmark@edgehill.ac.uk,0,,False
11,Evangelos Kanoulas,0,,False
12,"University of Amsterdam, Amsterdam, The Netherlands",0,,False
13,e.kanoulas@uva.nl,0,,False
14,Mark Sanderson,0,,False
15,"RMIT University, Melbourne, Australia",0,,False
16,mark.sanderson@rmit.edu.au,0,,False
17,ABSTRACT,0,,False
18,"Information Retrieval (IR) research has traditionally focused on serving the best results for a single query-- so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of IR systems over sessions. This paper describes the TREC Session Track, which ran from 2010 through to 2014, which focussed on forming test collections that included various forms of implicit feedback. We describe the test collections; a brief analysis of the differences between datasets over the years; and the evaluation results that demonstrate that the use of user session data significantly improved effectiveness.",1,ad,True
19,1. INTRODUCTION,1,DUC,True
20,"One of the commonest IR system evaluation methodologies is the Cranfield approach [4] using test collections to conduct controlled, systematic, and repeatable evaluations [7]. The focus of such evaluation is on how well an IR system can locate and rank relevant documents from a single query. In practice, however, users typically reformulate queries in response to search results or as their information need alters over time [9]. Retrieval evaluation should compute system success over multiple query-response interactions [10].",0,,False
21,"The TREC Session Track1 was an attempt to evaluate IR systems over multi-query sessions. In 2010, the track produced test collections and evaluation measures for studying retrieval over sessions [11]; from 2011 on [12, 13, 1, 2], the track focused more on providing participants with user data with which to improve retrieval. The resulting collections consist of document collections, topics, and relevance assessments, as well as log data from user sessions.",1,TREC,True
22,1http://ir.cis.udel.edu/sessions/,0,,False
23,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy",1,ad,True
24,c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914675,0,,False
25,"The track's test collections are described here and compared: including studying the effects of the search engines used to build the collections, user variability, and topic analysis. Participant results indicate that certain types of search benefit significantly from exploiting session information.",0,,False
26,2. SESSION TRACK OVERVIEW,0,,False
27,"The aim of the track was to test if the retrieval effectiveness of a query could be improved by using previous queries, ranked results, and user interactions. We constructed four test collections comprising N sessions of varying length, each the result of a user attempting to satisfy one of T pre-defined topics. Each session numbered 1..i..N consisted of:",0,,False
28,· mi blocks of user interactions (the session's length); · the current query qmi in the session; · mi - 1 blocks of interactions in the session prior to the,0,,False
29,"current query, composed of: 1. the user queries in the session, q1, q2, ..., qmi-1; 2. the ranked list of URLs seen by the user for each of those queries; 3. the set of clicked URLs/snippets.",0,,False
30,"Ranking algorithms were evaluated on the current query under two conditions: A one-off ad hoc query; or a query using some or all of the prior logged data. The latter condition had several different sub-conditions that varied year to year: (""RL"" refers to Ranked List):",1,ad,True
31,"· RL1: The baseline condition: an ad hoc query · RL2-1: RL1 plus previous session queries · RL2-2: RL2-1 plus rankings (URLs, titles, snippets) · RL2-3: RL2-2 plus user data (clicks, dwell times) · RL3: Using all data in the session log (in particular,",1,ad,True
32,other sessions on the same topic),0,,False
33,The focus of the track was on the degree to which a group improved their retrieval system's baseline effectiveness (RL1) by incorporating some or all of the additional log data.,1,corpora,True
34,3. TEST COLLECTIONS,0,,False
35,"Table 1 shows statistics of the Session track collections. The ClueWeb09 collection was used in 2011 and 2012, and the ClueWeb12 collection in 2013 and 2014.",1,Session,True
36,"Topics: While not a part of a true log of user search activity, we felt it was important to define topic descriptions for overall sessions so as to make relevance assessing",0,,False
37,685,0,,False
38,Table 1: Four years of TREC Session Track test collections and evaluations,1,TREC,True
39,2011,0,,False
40,2012,0,,False
41,2013,0,,False
42,2014,0,,False
43,collection topic properties,0,,False
44,topic set size topic cat. dist.,0,,False
45,ClueWeb09,1,ClueWeb,True
46,62 known-item,0,,False
47,session properties user population,0,,False
48,search engine total sessions sessions per topic mean length (in queries) median time between queries relevance judgments topics judged total judgments evaluation by nDCG@10,0,,False
49,mean RL1 mean RL2-1 mean RL2-2 mean RL2-3,0,,False
50,mean RL3 max RL* - RL1,0,,False
51,U. Sheffield,0,,False
52,BOSS+CW09 filter 76 1.2 3.7,1,CW,True
53,68.5s,0,,False
54,"62 19,413",0,,False
55,0.3015 0.3083 0.2941 0.3077,0,,False
56,­§ 0.1800,0,,False
57,ClueWeb09,1,ClueWeb,True
58,"48 10 exploratory, 6 interpretive, 20 known-item, 12 known-subj",0,,False
59,U. Sheffield,0,,False
60,BOSS+CW09 filter 98 2.0 3.0,1,CW,True
61,66.7s,0,,False
62,"48 17,861",0,,False
63,0.1847 0.1950 0.2140 0.2303,0,,False
64,­§ 0.1770,0,,False
65,ClueWeb12,1,ClueWeb,True
66,"61 10 exploratory, 9 interpretive, 32 known-item, 10 known-subj",0,,False
67,U. Sheffield + IR researchers indri 133 2.2 3.7 72.2s,0,,False
68,"49 13,132",0,,False
69,0.1373 ­§ ­§,0,,False
70,0.1832 0.1834 0.1230,0,,False
71,ClueWeb12,1,ClueWeb,True
72,"60 15 exploratory, 15 interpretive, 15 known-item, 15 known-subj",0,,False
73,MTurk,0,,False
74,"indri 1,257 21.0",0,,False
75,3.7 25.6s,0,,False
76,"51 16,949",0,,False
77,0.1719 ­§ ­§,0,,False
78,0.1885 0.2002 0.1507,0,,False
79," 2011 topics were not categorized, but a retrospective analysis suggests most of them fit the ""known-item"" label best.  2014 topics were reused 2012 and 2013 topics. § The RL2-1 and RL2-2 conditions were eliminated for 2013 and 2014; the RL3 condition was introduced in 2013.",0,,False
80,"simpler. The challenge was to construct topics that were likely to require multiple query reformulations. In 2011, we did this by adapting multi-faceted TREC 2007 Question Answering track topics. Because of the nature of the QA track, many topics modelled ""fact-finding"" tasks answerable by a single document. In 2012-2013, we developed topics according to a task categorization scheme [15] with four classes: known-item; known-subject; interpretive; and exploratory. In 2014, we reused topics from 2012-2013 selecting fifteen topics from the four categories, biasing selection to topics that had longer user sessions and more clicks.",1,ad,True
81,"Sessions: Assessing the impact of session data on retrieval effectiveness required capturing user-system interactions, including queries, rankings, and clicks. We describe the users and search engines employed to generate the data.",1,Session,True
82,"Users: In 2011-2013, the primary user group were staff and students at the University of Sheffield. Using a universitywide email, we invited participants to search on as many topics as they had time for. In 2013 we solicited additional participants from the Session Track and SIG-IRList mailing lists. In 2014 we used a crowdsourcing platform (Mechanical Turk) taking a similar approach to past work for crowdsourcing interactions [18].",1,ad,True
83,"Search process: Users were shown a topic description, a search box for entering queries, and a list of ten ranked results with a pagination control to navigate to further results. Each retrieved item was represented by its title, URL, and snippet. Additionally, there was a ""Save"" button that users were instructed to use to collect those documents that helped them satisfy their information need. We experimented with additional components, such as a list of queries issued, but did not observe a difference in users' behaviour.",1,ad,True
84,Search engine: In 2011-2012 we used Yahoo!'s BOSS (Build your Own Search System) API to search the live web. We fil-,1,Yahoo,True
85,"tered URLs returned by BOSS against those in the ClueWeb09 collection so that users would only see pages that were present in the publicly-available corpus. A large number of pages returned by BOSS did not match any URL in ClueWeb09. In 2013-2014, we switched to indri search with a homebuilt index of ClueWeb12. The indri index included each of the twenty ClueWeb12 segments (ClueWeb12 00 through ClueWeb12 19) indexed using the Krovetz stemmer and no stopword list. The indexes searched contained only text from title fields, anchor text from incoming links (""inlink"" text), and page URLs. Each query was incorporated into an indri structured query language template and a retrieval score was computed from a query-likelihood model for the full document representation and three weighted combinations of query-likelihood field models with unordered-window within-field models. The ""inlink"" model was weighted 50 times higher than the title model, and 100 times higher than the URL model. This query template is the product of manual search and investigation of retrieved results.",1,ClueWeb,True
86,"The system logged all interactions with the user, including the queries issued, which documents were ranked (including URL, title, and snippet), which documents the user viewed, and which they saved as relevant to the task (note however that the latter are not the relevance judgments). This log data was then used to create the sessions.",0,,False
87,4. EVALUATION,0,,False
88,"We used topical relevance judgments in order to compute measures of effectiveness like nDCG@10 for each topic. Since the Session Track examines whether session log data can be exploited, the evaluation examined the change in effectiveness from the baseline (RL1) to using some data (RL2) to using a full query log (RL3). In addition, since each topic may be the subject of more than one session, and",1,Session,True
89,686,0,,False
90,X20X13XXXnav2X0-1XX44,0,,False
91,4 1,0,,False
92,3 0,0,,False
93,2 0,0,,False
94,1 0,0,,False
95,0 0,0,,False
96,-2 0,0,,False
97,key - 3 0 1 2 7 4 0,0,,False
98,hi - 2 0 4 28 52 14 2,0,,False
99,rel - 1 1 12 75 89 64 0,0,,False
100,not - 0 4 5 50 161 337 11,0,,False
101,junk -2 0 0 0 0 4 5,0,,False
102,Table 2: Agreement on relevance grades,1,ad,True
103,RL1 nDCG@10 0.1 0.2 0.3 0.4,0,,False
104,1 2011 6 2012 1 2013 6 2014,0,,False
105,0,0,,False
106,20,0,,False
107,40,0,,False
108,60,0,,False
109,80,0,,False
110,100,0,,False
111,run number,0,,False
112,Figure 1: Mean nDCG@10 (with error bars showing ±2 standard error) for all 108 submitted runs' RL1 baseline.,0,,False
113,"each session may use different queries, the evaluation was over sessions rather than over topics.",0,,False
114,"Documents were selected for judging by pooling the top-10 results from all the submitted RLs along with all documents that were retrieved and viewed by the users. TREC NIST assessors (not the original users) judged each pooled document with respect to the topic description. All original user actions were invisible to the assessors; judgments were made solely on the topical similarity to the topic description on a 6-grade scale. Over four years, 66,548 relevance judgments were made to 60,500 unique pages identified by URL: 33,686 pages from ClueWeb09 ; 26,814 from ClueWeb12. A total of 19,179 (29%) documents were judged relevant (grade 1 or higher) and 47,369 (71%) judged nonrelevant.",1,TREC,True
115,"Since the topics for 2014 were taken from the 2012 & 2013 Session Tracks and in the last two years the document collection was ClueWeb12, we have documents with multiple assessments. Table 2 shows assessor agreement. Assessors were much more likely to say a document judged non-relevant in 2013 was relevant in 2014 than vice versa.",1,Session,True
116,"Results: Figure 1 shows nDCG@10 for all groups' baseline RL1 submissions, sorted by nDCG@10 and coded by year. It is evident that 2011 had the best baseline effectiveness (average nDCG@10 of 0.30), followed by 2012 (0.18), then 2014 (0.17), and finally 2013 (0.14) had the lowest baseline effectiveness. The change from 2011 to 2012 reflects a shift to more difficult topics: the 2012 known-subject and interpretive topic categories proved to be significantly more difficult than the 2011 known-item topics. The change from 2012 to 2013 reflects a change in the underlying search technology from Yahoo! BOSS to the Indri-based system.",1,ad,True
117,"Figure 2 shows the improvement over each submitted run's RL1 baseline sorted by that improvement. Improvement from the RL1 baseline does not show any trend by year-- for 2011, the average improvement was 0.04, for 2012 it was 0.05, for 2013 it was 0.05, and for 2014 it dropped to 0.02.",0,,False
118,max change in nDCG@10 from RL1 baseline,0,,False
119,0.0,0,,False
120,0.1,0,,False
121,0.2,0,,False
122,-0.1,0,,False
123,2011 2012 2013 2014,0,,False
124,0,0,,False
125,20,0,,False
126,40,0,,False
127,60,0,,False
128,80,0,,False
129,100,0,,False
130,run number,0,,False
131,"Figure 2: Largest measured improvement in nDCG@10 from RL1 to any other condition for all 108 submitted runs, with error bars showing ±2 standard errors.",0,,False
132,"From these results, we conclude that it is possible to use session history to improve effectiveness over basic ad hoc retrieval, and moreover that it does not take a lot of session history to do so. Further evidence is offered in [6, 5, 17]. A study of particular interest due to the fact that it was conducted both over a Session track collection and a commercial search engine proprietary collection is that by Raman et al. [16]; the session collection enabled them to demonstrate the effectiveness of their algorithm in accordance to the proprietary test collection.",1,ad,True
133,5. ANALYSIS,0,,False
134,In this section we perform some basic analysis of the Session Track collections and evaluation results.,1,Session,True
135,"Topic categories: We investigated the degree to which systems were able to improve effectiveness for each of our four topic classes. We look at the average improvement from the RL1 baseline, and find the maximum average improvement to any other RL condition for each run.",0,,False
136,"The overall mean improvements are 0.04, 0.07, 0.04, and 0.05 for known-item, known-subject, exploratory, and interpretive respectively, though only the differences between known-subject and the others were statistically significant.This suggests that known-subject topics benefit most from access to session history, but the details are more subtle. Exploratory topics tend to have the largest improvements for individual systems: the five largest improvements in exploratory topics are 5­10% larger than the five largest in known-subject topics. Exploratory topics also show the greatest benefit from the use of more log data: from RL1 to RL2, exploratory topics only increase an average of 0.03 (compare to 0.05 for known-subject topics, the largest improvement), but from RL2 to RL3 they increase by 0.05 (compare to 0.04 for known-subject topics, the second-largest improvement).",0,,False
137,"Topic variability: Most IR test collections have only one instantiation of a topic (an exception is the TREC Query track). Since we may have multiple sessions for any given topic, the Session Track gives us a chance to analyze variability in effectiveness within topics.",1,TREC,True
138,"Figure 3 shows how much effectiveness varies over the different user sessions of a single topic. Each plot on the x-axis is a topic, the y-axis is a boxplot of the range in nDCG@10 changes from RL1 to any other RL. A taller box means more variability. A point or box plotted further up the y-axis indicates higher average change in nDCG@10 across the sessions",0,,False
139,687,0,,False
140,difference in nDCG@10 over sessions,0,,False
141,0.0,0,,False
142,0.5,0,,False
143,1.0,0,,False
144,1.5,0,,False
145,20122-011022-041742-041032-01142-22081220-1432-02144-24061220-1642-051242-031942-021642-01134-24071220-1542-04143-21021210-1722-031212-031042-051632-021112-021022-031432-041942-011522-011142-021442-031542-011022-021342-031012-051232-02182-24,0,,False
146,topic (ordered by median),0,,False
147,Figure 3: Variability over sessions and system effectiveness for selected topics.,0,,False
148,"of a topic. An extreme case is topic 24 from 2012 (the right most topic on the plot). There were five sessions recorded for this topic (numbers 48­52); one group improved from 0.00 in RL1 to 0.91 in RL3 on one session, but fell from 0.81 in RL1 to 0.00 in RL2 on another. Many other groups had similarly large differences across sessions on this topic.",1,ad,True
149,"The result indiactes that there is a substantial variability in topics, separate from the variability in system effectiveness, due to the way the users performs their search and formulates their query. Previous user study showed this as well [14]. It may be beneficial to include multiple versions of the same topic in standard test collections, so as to better capture interactions between topic and system variability.",0,,False
150,6. CONCLUSION,0,,False
151,"This paper describes the four test collections produced for the TREC Session Track that have been used to assess the use of implicit feedback on retrieval performance within sessions. The key result from the track is that aggregate data from all participant submissions shows that retrieval effectiveness was improved for ad hoc retrieval using data based on session history data. It also appears that the more detailed the session data, the greater the improvement.",1,TREC,True
152,"Through analyzing aspects of the test collections, such as topic categories and variability, we demonstrate how the resources can be used to investigate implicit feedback and offer reusable and publicly-accessible resources for evaluating IR systems across sessions.",0,,False
153,7. ACKNOWLEDGEMENTS,0,,False
154,"This work was supported in part by the Australian Research Council's Discovery Projects scheme (DP130104007), the National Science Foundation (NSF) under grant number IIS-1350799, and the Google Faculty Research Award scheme. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.",0,,False
155,8. REFERENCES,0,,False
156,"[1] B. Carterette, E. Kanoulas, A. Bah, M. Hall, and P. D. Clough. Overview of the TREC 2013 Session track. In Proceedings of TREC, 2013.",1,TREC,True
157,"[2] B. Carterette, E. Kanoulas, A. Bah, M. Hall, and P. D. Clough. Overview of the TREC 2014 Session track (notebook version). In Proceedings of TREC, 2014.",1,TREC,True
158,"[3] B. Carterette, E. Kanoulas, P. D. Clough, and M. Sanderson, editors. Proceedings of the ECIR 2011 Workshop on Information Retrieval Over Query Sessions, Available at http://ir.cis.udel.edu/ECIR11Sessions.",1,Query,True
159,"[4] C. W. Cleverdon. The significance of the cranfield tests on index languages. In Proceedings of SIGIR, pages 3­12, 1991.",0,,False
160,"[5] D. Guan and H. Yang. Is the first query the most important: An evaluation of query aggregation schemes in session search. In Proceedings of AIRS, pages 86­99. 2014.",0,,False
161,"[6] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In Proceedings of SIGIR, pages 453­462, 2013.",0,,False
162,"[7] D. Harman. Information Retrieval Evaluation. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2011.",0,,False
163,"[8] P. Ingwersen and K. J¨arvelin. The Turn: Integration of Information Seeking and Retrieval in Context (The Information Retrieval Series). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2005.",0,,False
164,"[9] B. J. Jansen, D. L. Booth, and A. Spink. Patterns of query reformulation during Web searching. J. Am. Soc. Inf. Sci. Technol., 60(7):1358­1371, July 2009.",0,,False
165,"[10] K. J¨arvelin. Explaining user performance in information retrieval: Challenges to ir evaluation. In Proceedings of ICTIR, pages 289­296, 2009.",0,,False
166,"[11] E. Kanoulas, B. Carterette, P. Clough, and M. Sanderson. Session track overview. In Proceedings of the 19th Text REtreival Conference (TREC), 2010.",1,Session,True
167,"[12] E. Kanoulas, B. Carterette, M. Hall, P. D. Clough, and M. Sanderson. Overview of the TREC 2011 Session track. In Proceedings of TREC, 2011.",1,TREC,True
168,"[13] E. Kanoulas, B. Carterette, M. Hall, P. D. Clough, and M. Sanderson. Overview of the TREC 2012 Session track. In Proceedings of TREC, 2012.",1,TREC,True
169,"[14] K. S. Kim. Information-seeking on the web: Effects of user and task variables. Library and Information Science Research, 23(3), 2011.",0,,False
170,"[15] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Inf. Process. Manage., 44(6):1822­1837, Nov. 2008.",0,,False
171,"[16] K. Raman, P. N. Bennett, and K. Collins-Thompson. Toward whole-session relevance: Exploring intrinsic diversity in web search. In Proceedings of SIGIR, pages 463­472, 2013.",0,,False
172,"[17] S. Zhang, D. Guan, and H. Yang. Query change as relevance feedback in session search. In Proceedings of SIGIR, pages 821­824, 2013.",1,Query,True
173,"[18] G. Zuccon, T. Leelanupab, S. Whiting, E. Yilmaz, J. Jose, and L. Azzopardi. Crowdsourcing interactions: Capturing query sessions through crowdsourcing. In Proceedings of ECIR, 2011.",0,,False
174,688,0,,False
175,,0,,False

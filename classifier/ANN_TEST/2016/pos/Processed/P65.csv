,sentence,label,data,regex
0,Document Retrieval Using Entity-Based Language Models,0,,False
1,Hadas Raviv,1,ad,True
2,Oren Kurland,0,,False
3,David Carmel,0,,False
4,"Technion, Israel",0,,False
5,"Technion, Israel",0,,False
6,"Yahoo Research, Israel",1,Yahoo,True
7,hadasrv@tx.technion.ac.il kurland@ie.technion.ac.il david.carmel@ymail.com,1,ad,True
8,ABSTRACT,0,,False
9,"We address the ad hoc document retrieval task by devising novel types of entity-based language models. The models utilize information about single terms in the query and documents as well as term sequences marked as entities by some entity-linking tool. The key principle of the language models is accounting, simultaneously, for the uncertainty inherent in the entity-markup process and the balance between using entity-based and term-based information. Empirical evaluation demonstrates the merits of using the language models for retrieval. For example, the performance transcends that of a state-of-the-art term proximity method. We also show that the language models can be effectively used for clusterbased document retrieval and query expansion.",1,ad,True
10,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,0,,False
11,Keywords: document retrieval; entity-based language models,0,,False
12,1. INTRODUCTION,1,DUC,True
13,"Most ad hoc document retrieval methods compare query and document representations. To address the potential vocabulary mismatch between a short query and documents relevant to the query, various semantic document-query similarity measures have been proposed [28].",1,ad,True
14,"Specifically, there is a growing body of work on retrieval methods that utilize information about entities in a repository (e.g., Wikipedia or Freebase) which appear in queries and documents (e.g., [46, 35, 39, 7, 13, 31, 45, 29, 33, 44]). Most of these methods expand the query with terms or entities related to those appearing (or marked) in it [46, 35, 39, 7, 13, 31, 45, 29]; other methods project queries and documents onto a latent or explicit entity space [14, 33, 44].",1,Wiki,True
15,"In this paper we take a step back, and address a more fundamental challenge regarding the use of entity-based information for document retrieval. We study whether using surface level entity-based query and document representations can help to improve retrieval effectiveness. By ""surface level"" we refer to representations based only on terms",1,ad,True
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.",1,ad,True
17,"SIGIR '16, July 17-21, 2016, Pisa, Italy",0,,False
18,c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00,0,,False
19,DOI: http://dx.doi.org/10.1145/2911451.2911508,0,,False
20,"in the text and markups of entities in it, along with raw corpus-based occurrence statistics. This is in contrast to expansion-based and projection-based representations that utilize also terms and entities related to those (marked) in the text and which often use auxiliary information about entities from the entity repository; e.g., textual descriptions of entities, entities' categories and inter-entity relations [46, 35, 39, 7, 13, 31, 45, 29, 33, 44]. Put in simpler words, the question we address is whether the markup of entities in a query and documents is, by itself, sufficient information for",1,ad,True
21,improving retrieval effectiveness. The reason for addressing the question just posed is two,1,ad,True
22,"fold. First, it will shed light on the effectiveness of using entities in their most basic capacity; that is, special tokens marked in queries and documents. Indeed, findings in past work on ad hoc retrieval regarding the merits of using surface level entity-based representations are inconclusive [16, 42, 47, 3, 14]. Second, such representations can be naturally used in existing retrieval approaches and tasks to improve performance; e.g., query expansion and cluster-based document retrieval as we show in this paper.",1,ad,True
23,"There are various potential merits in using surface level entity-based representations. For example, these can help to cope with the vocabulary mismatch problem; e.g., the entity United States of America can have different expressions in the text, including, ""U.S."", ""USA"", ""United States"" and more. Furthermore, expressions of entities in the text are variable-length n-grams that bear semantic meaning. Thus, entities can be used for effective modeling of term proximity information which goes beyond using fixed-length n-grams.",0,,False
24,"An important challenge in inducing entity-based representations is accounting for the uncertainty inherent in the entity-markup process (a.k.a. entity linking); that is, associating term sequences with entities in a repository. Specifically, a term sequence can potentially be associated with multiple entities; e.g., the term ""Lincoln"" can be associated with the U.S. president, the car, the 2012 movie, etc. The uncertainty in entity linking has significant impact on retrieval effectiveness as we show in this paper.",0,,False
25,"We present novel types of entity-based language models which consider both single terms in the text as well as term sequences marked as entities by an existing entity-linking tool. These language models are induced from the query and documents in the corpus and serve for retrieval in the language modeling framework. The main novelty of these language models is accounting, simultaneously, for (i) the uncertainty in entity linking -- specifically, the confidence levels of entity markups; and, (ii) the balance between using",0,,False
26,65,0,,False
27,"term-based and entity-based information. We demonstrate the importance of accounting for the mutual effects of these two aspects. For example, we show that using high recall entity markup, which is quite noisy, can help to significantly improve retrieval effectiveness if the noise is ""balanced"" by sufficient utilization of term-based information.",0,,False
28,"Empirical evaluation demonstrates the merits of using our entity-based language models for retrieval. The performance significantly transcends that of a state-of-the-art term proximity method: the sequential dependence model (SDM) [36, 19]. Integrating the language models with SDM yields further performance improvements. The language models are also effective for two additional retrieval paradigms: clusterbased document retrieval and query expansion.",1,ad,True
29,2. RELATED WORK,0,,False
30,"The work most related to ours is that on devising surface level entity-based document and query representations for document retrieval [21, 16, 42, 47, 3, 41, 14]. The findings about the merits of these representations have been inconclusive. The few cases where the representations were shown to be somewhat effective for retrieval were when entity markups were devised in extreme care and were of very high quality [47, 3, 14]. In contrast to this past work that focused on vector space models, we demonstrate the clear merits of using our entity-based language models for retrieval. Also, in contrast to previously proposed representations [21, 16, 42, 47, 3, 41, 14], our language models account simultaneously for the uncertainty in the entity-markup process, and the balance between using term-based and entity-based information. Consequently, a highly important aspect that further differentiates our approach from past work is the effective utilization of high recall, noisy, entity markups.",0,,False
31,"There is work on query expansion using entity-based information [43, 34, 30, 40, 8, 10, 18, 46, 6, 20, 7, 35, 39, 13, 31, 29, 45] and on projecting queries and documents onto an entity space to compare them [14, 33, 44]. There are two fundamental differences between all this past work and ours which focuses on surface level entity-based query and document representations. First, in these past methods, queries and documents are represented by external terms and entities which they do not contain1. Our surface level representations do not utilize such expansions. Second, auxiliary information about entities from the entity repository (e.g., textual descriptions of entities and their interrelations) is utilized in this past work, but not in our representations2.",0,,False
32,"We show that our entity-based language models can be used to create effective expanded query forms by ""plugging"" them into an existing query expansion method: the relevance model [26, 1]. The resultant approach, which simultaneously expands the query with both terms and entities, is conceptually reminiscent of some methods recently proposed by Dalton et al. [13]. In their work, queries are expanded, independently, using terms and entities. The retrieval scores at-",0,,False
33,"1Xiong and Callan [44] found that representing queries using only entities marked in them is of merit for their learningto-rank approach. However, features describing the queryentities relations rely on auxiliary information from the entity repository that is not used by our methods. 2The entity-linking process could use auxiliary information from the entity repository. However, our proposed representations utilize the entity markups simply as tokens with confidence levels, and do not use auxiliary information.",0,,False
34,tained by using multiple term-only and entity-only expanded query forms are fused using a learning-to-rank method [13]. We show that our language models can be used to further improve the effectiveness of such expansion-based approaches by improving the quality of the pseudo relevant document list used for query expansion.,0,,False
35,We also demonstrate the merits of using our language models for cluster-based document retrieval. Using entitybased representations for this task is novel to this study.,0,,False
36,"In some studies, concepts (entities) in verbose queries were automatically weighted [2, 22, 4, 5]. In contrast to our approach, weights (confidence levels) of entities in documents were not accounted for. We demonstrate the importance of accounting for the confidence level of entity markups in both queries and documents. Further tuning of entities' weights in our proposed language models, using some of these approaches [2, 22, 4, 5], is interesting future work.",0,,False
37,"There are language models that integrate word phrases and named entities based on their association with predefined classes [27, 23]. In contrast to our language models, which are not based on such classes, these language models were not designed and used for document retrieval.",0,,False
38,3. RETRIEVAL FRAMEWORK,0,,False
39,In what follows we present ad hoc document retrieval methods that rank documents in a corpus D in response to query q. The methods utilize information about entities mentioned in the query and in documents.,1,ad,True
40,"To mark entities in texts, we use some entity-linking tool that utilizes a repository (e.g., Wikipedia or Freebase) where entities have unique IDs. The entity-linking tool takes as input a text, query or document in our case, and marks variable length sequences of terms as potential entities in the repository. The entity markup of a term sequence is composed of entity ID and a confidence level in [0, 1]. The confidence level reflects the likelihood that the term sequence corresponds to the entity. The confidence level relies on the term sequence and its context; e.g., its neighboring terms or other term sequences marked as entities [15, 38]. Using high confidence level results in high precision entity markup while low confidence level results in high recall.",1,Wiki,True
41,"We assume that each position in a given text can be part of at most a single term sequence that is marked as an entity; i.e., the entity markups do not overlap. A specific occurrence of a term sequence in a text cannot be marked with more than one entity. Yet, a term sequence can appear several times in a text with different entity markups as the markups depend on the context of the sequence. Details of the entity linking tools we use are provided in Section 4.1.",0,,False
42,The retrieval methods we present in Section 3.2 use entitybased query and document language models. We now turn to define these language models.,0,,False
43,3.1 Entity-based language models,0,,False
44,"We define unigram entity-based language models over a token space T ; i.e., tokens are generated by the language model independently of each other. The token space,",0,,False
45,"T d,ef V  E",0,,False
46,(1),0,,False
47,is composed of the set V of all terms in the corpus D and the set E of entities in the entity repository which were marked at least once in a document in D with any confidence level.,0,,False
48,66,0,,False
49,"The language models we devise rely on a definition of pseudo counts for tokens. Two definitions of pseudo counts will be presented in Sections 3.1.1 and 3.1.2. Let pc(t, x) be the pseudo count of token t ( T ) in the text or text collection x. We define the pseudo length of x as:",0,,False
50,"pl(x) d,ef",0,,False
51,"X pc(t, x).",0,,False
52,"tT :pc(t,x)>0",0,,False
53,The maximum likelihood estimate (MLE) of token t ( T ) with respect to x is:,0,,False
54,xMLE (t),0,,False
55,"d,ef",0,,False
56,"pc(t, x) pl(x)",0,,False
57,.,0,,False
58,(2),0,,False
59,The MLE can be smoothed using Dirichlet priors [49]:,0,,False
60,"xDir(t) d,ef",0,,False
61,"pc(t,",0,,False
62,x) + µDMLE pl(x) + µ,0,,False
63,(t),0,,False
64,;,0,,False
65,(3),0,,False
66,µ is a smoothing parameter. We next describe two types of language models defined,0,,False
67,over T and induced using Equations 2 and 3. The language models differ by the definition of pseudo counts for tokens.,0,,False
68,3.1.1 Hard confidence-level thresholding,0,,False
69,"The hard confidence-level thresholding language model, HTLM in short, is based on fixing a threshold  ( [0, 1]) for entity markups. Entity-based information is used only for entity markups with confidence level   . In contrast, every term occurrence in a text, including those in entity markups with a confidence level <  , is accounted for.",1,LM,True
70,"To formally define a HTLM using Equations 2 and 3, we have to define pseudo counts for tokens from T in a text or text collection x. To that end, we lay down a few definitions. If t ( T ) is a term, then cterm(t, x) is the number of occurrences of t in x. Let M(x) denote the set of all entity markups in x; i.e., all occurrences of term sequences in x that were marked as entities with some confidence level. For a markup m ( M(x)), E(m) is the entity and (m) is the confidence level. The equivalence relation t1  t2 holds iff the entity tokens t1 and t2 are identical (i.e., have the same ID). The pseudo count of t ( T ) in x is based on (i) the raw count of t in x if t is a term; and, (ii) the number of entity markups of t in x with a confidence level   if t is an entity. Formally,",1,LM,True
71,"pcHT LM; (t, x) d,ef",1,LM,True
72,"( cterm(t, x)",0,,False
73,(1,0,,False
74,-,0,,False
75,),0,,False
76,P,0,,False
77,mM(x):E(m)t,0,,False
78,[(m),0,,False
79,],0,,False
80,(4),0,,False
81,if t  V; if t  E;,0,,False
82," ( [0, 1]) is a free parameter which controls the relative importance attributed to term and entity tokens;  is Kronecker's delta function: for statement s, [s] , 1 if s is true and [s] , 0 otherwise.",0,,False
83,"We note that using a Dirichlet smoothed HTLM (i.e., using Equation 4 in Equation 3) can still result in assigning zero probability to some tokens in T . These are entities with no corresponding markup of a term sequence in the corpus with confidence level   . We re-visit this point below.",1,LM,True
84,"If we set  ,"" 1 in Equation 4, then the resultant HTLM reduces to a standard unigram term-based language model. Setting  "","" 0 results in HTEntLM which is a unigram language model that assigns non-zero probability only to entities: if the MLE from Equation 2 is used, then these are""",1,LM,True
85,"the entities with at least one markup in x with a confidence level   ; if the Dirichlet smoothed language model is used (Equation 3), then these are the entities with at least one markup in the corpus with a confidence level   .",0,,False
86,3.1.2 Soft confidence-level thresholding,0,,False
87,"A potential drawback of HTLM is committing to a specific threshold  for entity markups. That is, information about entity markups with confidence level lower than  is ignored. Furthermore, all entity markups with confidence level   are counted equally as their confidence levels are ignored.",1,LM,True
88,"Thus, we now turn to present a soft confidence-level thresholding language model, STLM. STLM accounts for all markups of an entity and weighs them by the corresponding confidence levels. Specifically, the pseudo count of t ( T ) in the text or text collection x is defined as:",1,LM,True
89,(,0,,False
90,"pcST LM (t, x) d,ef",1,LM,True
91,"cterm(t, x)",0,,False
92,(1,0,,False
93,-,0,,False
94,),0,,False
95,P,0,,False
96,mM(x):E(m)t,0,,False
97,(m),0,,False
98,if t  V; if t  E;,0,,False
99,(5),0,,False
100," ( [0, 1]) is a free parameter that, as in HTLM, controls",1,LM,True
101,the relative importance attributed to term and entity to-,0,,False
102,"kens. Thus, STLM addresses the uncertainty inherent in",1,LM,True
103,the entity linking process by using expected entity occur-,0,,False
104,rence counts; the corresponding confidence levels serve for,0,,False
105,occurrence probabilities. These expected counts are then,0,,False
106,integrated with deterministic term counts.,0,,False
107,"If we set  ,"" 1 in Equation 5, then STLM reduces to""",1,LM,True
108,a standard unigram term-based language model as was the,0,,False
109,"case for HTLM. Setting  , 0 results in STEntLM. This",1,LM,True
110,language model assigns a non-zero probability only to en-,0,,False
111,tities that have at least one markup (with any confidence,0,,False
112,level) in x when using the MLE (Equation 2) or in the corpus,0,,False
113,when using the Dirichlet smoothed language model (Equa-,0,,False
114,"tion 3). We note that in contrast to the case for HTLM,",1,LM,True
115,there is no token in T that is assigned a zero probability by,0,,False
116,a Dirichlet smoothed STLM.,1,LM,True
117,3.2 Retrieval models,0,,False
118,We rank document d by the cross entropy between the language models induced from the query (q) and d [25]:,0,,False
119,X,0,,False
120,"CE (q || d) , - q(t) log d(t);",0,,False
121,(6),0,,False
122,tT,0,,False
123,higher values correspond to decreased similarity. Equation 6 is instantiated using the entity-based language,0,,False
124,"models described in Section 3.1. Following common practice [48], we use an unsmoothed maximum likelihood estimate for the query language model (Equation 2) and a Dirichlet smoothed document language model (Equation 3). We obtain four retrieval methods : HT3, HTOEnt, ST and STOEnt4, which utilize the HTLM, HTEntLM, STLM and",1,LM,True
125,"3In HT, the same confidence-level threshold, d, is used for all documents; the query threshold, q, can be different from d. Hence, an entity token assigned a non-zero probability by q could be assigned a zero probability by d; e.g., an entity marked in q with a confidence level  q but with no markup in the corpus with confidence level  d. In these cases, we zero the probability assigned to the entity token by q to avoid a log 0 in Equation 6. This is common practice in addressing term tokens that appear in a query but not in any document in the corpus. 4HTOEnt and STOEnt rely only on entity tokens. If all entities in E are assigned a zero probability by the unsmoothed",1,ad,True
126,67,0,,False
127,Table 1: TREC data used for experiments.,1,TREC,True
128,corpus,0,,False
129,# of docs,0,,False
130,data,0,,False
131,queries,0,,False
132,AP,1,AP,True
133,ROBUST,0,,False
134,WT10G GOV2 ClueB ClueBF,1,WT,True
135,"242, 918 528, 155 1, 692, 096 25, 205, 179 50, 220, 423",0,,False
136,Disks 1-3 Disks 4-5 (-CR),0,,False
137,WT10g GOV2 ClueWeb09 (Cat. B),1,WT,True
138,"51 - 150 301 - 450, 601 - 700 451 - 550 701 - 850",0,,False
139,1 - 200,0,,False
140,"STEntLM language models, respectively. HT and ST utilize entity and term tokens, while HTOEnt and STOEnt utilize only entity tokens, hence the ""O"" in the methods names.",1,LM,True
141,3.2.1 Score-based fusion,0,,False
142,"The HTLM and STLM language models integrate termbased and entity-based information at the language model level. Hence, the query-document comparison in Equation 6 simultaneously accounts for the appearance of the query terms and entities in a document.",1,LM,True
143,"An alternative approach is integrating term and entity information at the retrieval score level. Inspired by approaches in the vector-space model [42], and in work on using a latent entity space [33], we consider a method that fuses document retrieval scores produced by utilizing, independently, termonly (xterm) and entity-only (xent) language models induced from text x. Document d is scored by:",0,,False
144,CE `qterm || dterm´ + (1 - )CE `qent || dent´ ; (7),0,,False
145,the  parameter balances the score fusion5. The query language models are unsmoothed maximum likelihood estimates (Equation 2) and the document language models are Dirichlet smoothed (Equation 3).,0,,False
146,"Instantiating Equation 7 with an entity-only language model, HTEntLM or STEntLM, and with a standard unigram termbased language model yields the F-HT and F-ST methods, respectively. These are conceptually highly similar to the HT and ST methods which integrate term-based and entitybased information at the language-model level. However, HT and ST use a single smoothing parameter for both term and entity tokens (see Equation 3) while F-HT and F-ST can use a different smoothing parameter for each as they utilize separately term-only and entity-only language models. We could have used different smoothing parameters for entity and term tokens under the same language model, e.g., by applying term-specific smoothing [17], but we leave this exploration for future work.",1,LM,True
147,4. EVALUATION,0,,False
148,4.1 Experimental setup,0,,False
149,"Experiments were conducted using the TREC datasets specified in Table 1. AP and ROBUST are mostly composed of news articles. WT10G is a small, noisy, Web collection. GOV2 is a much larger Web collection composed of high quality pages crawled from the .gov domain. ClueB is the",1,TREC,True
150,"query language model, then no documents are retrieved. This can happen for example when inducing HTEntLM from the query with a high confidence-level threshold or inducing a STEntLM from a query which has no entity markups. 5The  in the score-based fusion model has a conceptually similar role to that of  in STLM and HTLM: balancing the use of term-based and entity-based information.",1,LM,True
151,English part of the Category B of the ClueWeb 2009 Web collection. ClueBF was created from ClueB by filtering from rankings suspected spam documents: those assigned a score below 50 by Waterloo's spam classifier [11].,1,ClueWeb,True
152,Data processing. Titles of TREC topics served for queries.,1,TREC,True
153,Tokenization and Porter stemming were applied using the Lucene toolkit (lucene.apache.org) which was used for experiments. Stopwords on the INQUERY list were removed from queries but not from documents.,0,,False
154,"Unless otherwise specified, the TagMe entity-linking tool (tagme.di.unipi.it) is used to annotate queries and documents. TagMe uses Wikipedia (a July 2014 dump) as the entity repository, and was shown to be highly effective and efficient in comparison to other publicly available entity-linking systems [12]. In Section 4.2.1 we also show the effectiveness of our methods using the Wikifier entity-linking tool6 [9, 12]. Wikifier was applied with an efficient configuration claimed to yield baseline entity linking effectiveness.",1,Wiki,True
155,"TagMe and Wikifier cannot process very long texts. Thus, we split documents into non-overlapping term-window passages. We terminate a passage at the first space that appears at least 500 characters after the beginning of the previous passage. We let the tools mark the passages independently. The tools are applied on the non-stemmed and non-stopped queries and documents. Entity markup of a term sequence includes an entity ID and a confidence level (in [0, 1]). We scan each text left to right and remove overlapping entity markups so that each position can be part of at most a single markup. If two markups overlap, we select the one with the higher confidence level. We break ties of confidence levels by selecting the markup which starts at the leftmost position.",1,Wiki,True
156,Baselines. We use standard term-based unigram language,0,,False
157,"model retrieval [25], denoted TermsLM, for reference. This is a special case of the HT, ST, F-HT and F-ST methods with  , 1. Documents are ranked by the cross entropy between the unsmoothed (MLE) query language model and Dirichlet smoothed document language models.",1,LM,True
158,"The HTCon method is a special case of HT with  , 0.5 and q , d ,"" 0 (q and d are the query and document thresholds, respectively). HTCon accounts uniformly for all entity mentions, and attributes the same importance to term and entity tokens. HTCon is conceptually reminiscent of methods representing documents and queries using concepts (e.g., from Wordnet) by concatenating with equal weights term-based and concept-based vector-space representations [41, 16, 42]. Accordingly, we consider F-HTCon: a special case of F-HT with  "", 0.5 and q , d , 0.",0,,False
159,"Additional baseline is the state-of-the-art sequential dependence model, SDM, from the Markov Random Field framework which utilizes term proximities [36, 19]. The comparison with SDM, and its integration with our STLM is presented in Section 4.2.3.",1,LM,True
160,Evaluation measures and free-parameters. Mean aver-,0,,False
161,"age precision at cutoff 1000 (MAP), precision of the top 10 documents (p@10) and NDCG@10 (NDCG) serve as evaluation measures. Statistically significant performance differences are determined using the two-tailed paired t-test with a 95% confidence level.",1,MAP,True
162,6cogcomp.cs.illinois.edu/page/demo view/Wikifier,1,Wiki,True
163,68,0,,False
164,"Table 2: Comparison of methods instantiated from Equation 6 using term-only (TermsLM) and entitybased language models. Bold: the best result in a row. 't', 'h', 'o', 'c' and 's' mark statistically significant differences with TermsLM, HT, HTOEnt, HTCon and ST, respectively.",1,LM,True
165,TermsLM HT HTOEnt HTCon ST STOEnt,1,LM,True
166,MAP,1,MAP,True
167,AP,1,AP,True
168,p@10,0,,False
169,NDCG,0,,False
170,MAP ROBUST p@10,1,MAP,True
171,NDCG,0,,False
172,MAP WT10G p@10,1,MAP,True
173,NDCG,0,,False
174,GOV2,0,,False
175,MAP p@10,1,MAP,True
176,NDCG,0,,False
177,ClueB,1,Clue,True
178,MAP p@10 NDCG,1,MAP,True
179,ClueBF,1,Clue,True
180,MAP p@10,1,MAP,True
181,NDCG,0,,False
182,20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8 17.1 22.7 16.5 18.8 33.6 24.3,0,,False
183,23.1t 44 2t,0,,False
184,. 45.3t 28 1t,0,,False
185,. 45 5t,0,,False
186,. 47 1t,0,,False
187,.,0,,False
188,21.9t 30.4t,0,,False
189,32.7 32.1t 57.3t 47.4t 18.7t 25.9t 18.7t 20 5t,0,,False
190,. 37.9t 28 4t,0,,False
191,.,0,,False
192,"15.6t,h 36.0h 37.6h 19.1t,h 35.7t,h 36.9t,h 13.3t,h 21.6t,h 21.2t,h 18.0t,h 39.4t,h 32.7t,h 14.0t,h",0,,False
193,"23.9 18.3 14.4t,h 29.2h 22.2h",0,,False
194,22.5o 43.4to 44.7to 27.4to 45.0to 46.3to 21.4to 30.5to 32.1o 30.6ho 56.8o,0,,False
195,46.9o,0,,False
196,18.5o 26.7to 19.2t,0,,False
197,19.9o 38.2to 28.4to,0,,False
198,"23.5to,c 17.5to,,hc,s",0,,False
199,"43.8to 38.3hc,s",0,,False
200,"45.5to 39.6hc,s",0,,False
201,"28.1to,c 21.4to,,hc,s",0,,False
202,"45.3to 38.0to,,hc,s",0,,False
203,"46.9to 39.2to,,hc,s",0,,False
204,"22.9to,,hc 16.7ho,c,s",0,,False
205,"31.6to 25.3ho,c,s",0,,False
206,"34.3to,c 25.4ho,c,s",0,,False
207,"32.2to,c 20.7to,,hc,s",0,,False
208,"57.7to 44.0to,,hc,s",0,,False
209,"47.9to 35.7to,,hc,s",0,,False
210,"19.5to 14.0tc,,hs",0,,False
211,27 4t .,0,,False
212,24.1,0,,False
213,19 3t .,0,,False
214,17.5,0,,False
215,"20.3to 14.4tc,,hs",0,,False
216,"37.9to 30.6hc,s",0,,False
217,"27.5to 22.8hc,s",0,,False
218,The free parameter values of all retrieval methods are set using 10-fold cross validation performed over the queries in a dataset. Query IDs are used to create the folds. The optimal parameter values for each of the 10 train sets are determined using a simple grid search applied to optimize MAP. The learned parameter values are then used for the queries in the corresponding test fold.,1,Query,True
219,"The value of the Dirichlet smoothing parameter, µ, is selected from {100, 500, 1000, 1500, 2000, 2500, 3000}. The parameter , used in HTLM, STLM, F-HT and F-ST, is set to values in {0, 0.1, . . . , 1}. The document (d) and query (q) entity-markup confidence level thresholds, used in HT, HTOEnt and F-HT, are set to values in {0, 0.1, . . . , 0.9}.",1,LM,True
220,4.2 Experimental results,0,,False
221,4.2.1 Entity-based language models,0,,False
222,"Table 2 presents the performance of the methods that use entity-based language models to instantiate Equation 6. Our first observation is that the HT and ST methods outperform the standard term-based language-model retrieval, TermsLM, in all relevant comparisons (6 corpora × 3 evaluation measures); most improvements are substantial and statistically significant. Furthermore, HT and ST outperform to a substantial and statistically significant degree their special cases which use only entity tokens: HTOEnt and STOEnt, respectively. These findings attest to the merits of using our proposed language models, HTLM and STLM, which integrate term-based and entity-based information.",1,LM,True
223,We also see in Table 2 that HT and ST outperform HTCon in most relevant comparisons; most MAP improvements for ST are statistically significant. Recall from Section 4.1 that HTCon represents past practice of concept-based representations: accounting uniformly for all entity mentions and attributing equal importance to entity and term tokens. Below we further study the importance of accounting for the,1,MAP,True
224,MAP,1,MAP,True
225,MAP,1,MAP,True
226,MAP,1,MAP,True
227,24.0 23.0 22.0 21.0 20.0 19.0 18.0 17.0 16.0 15.0,0,,False
228,0,0,,False
229,24.0,0,,False
230,AP,1,AP,True
231,HT ST,0,,False
232,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,0,,False
233, WT10G,1,WT,True
234,22.0,0,,False
235,20.0,0,,False
236,18.0,0,,False
237,16.0,0,,False
238,14.0 12.0,0,,False
239,0,0,,False
240,20.0,0,,False
241,HT ST,0,,False
242,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,0,,False
243,ClueB,1,Clue,True
244,19.0,0,,False
245,18.0,0,,False
246,17.0,0,,False
247,16.0,0,,False
248,15.0 14.00,0,,False
249,HT ST,0,,False
250,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,0,,False
251,MAP,1,MAP,True
252,MAP,1,MAP,True
253,MAP,1,MAP,True
254,30.0,0,,False
255,ROBUST,0,,False
256,28.0,0,,False
257,26.0,0,,False
258,24.0,0,,False
259,22.0,0,,False
260,20.0,0,,False
261,18.0 0,0,,False
262,34.0 32.0 30.0 28.0 26.0 24.0 22.0 20.0 18.0 16.0,0,,False
263,0,0,,False
264,22.0 21.0 20.0 19.0 18.0 17.0 16.0 15.0 14.00,0,,False
265,HT ST,0,,False
266,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,0,,False
267, GOV2,0,,False
268,HT ST,0,,False
269,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,0,,False
270, ClueBF,1,Clue,True
271,HT ST,0,,False
272,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,0,,False
273,"Figure 1: The effect of varying  on the MAP of HT and ST. For  ,"" 1, the methods amount to TermsLM (term-based language model retrieval). For  "","" 0, the methods use only entity tokens. The performance is reported for the test folds (i.e., all queries in a dataset) when fixing the value of  and using cross validation to set the values of all other free parameters. Note: figures are not to the same scale.""",1,MAP,True
274,"confidence level of entity markups, and attributing different weights to term and entity tokens as in HT and ST.",0,,False
275,"Table 2 shows that ST outperforms HT in most relevant comparisons, although rarely to a statistically significant degree. In addition, ST posts more statistically significant improvements over HTCon than HT. We note that HT depends on four free parameters (, q, d and µ) while ST depends only on two ( and µ). Furthermore, the values learned for q and d in HT using the training folds are very low, attesting to the merits of using high recall entity markup. (We revisit this point below.) Overall, these findings attest to the potential merits of using a soft-thresholding approach for the confidence level of entity markups (STLM) with respect to a hard-thresholding approach (HTLM); i.e., accounting for all entity markups in a text and weighing their impact by their confidence levels is superior to accounting, uniformly, for entity markups with a confidence level above a threshold.",1,ad,True
276,Terms vs. entities. Figure 1 depicts the MAP performance,1,MAP,True
277,"of HT and ST as a function of . Low and high values of  result in more importance attributed to entity-based and term-based information, respectively. For  ,"" 1, the two methods amount to TermsLM -- i.e., standard termbased language model retrieval. For  "","" 0, the methods use only entity-based information; specifically, HT reduces to HTOEnt and ST reduces to STOEnt.""",1,LM,True
278,"We see in Figure 1 that optimal performance is always attained for   {0, 1}. This finding echoes those based on Table 2. That is, HT and ST outperform TermsLM,",1,LM,True
279,69,0,,False
280,MAP MAP,1,MAP,True
281,35.0 30.0,0,,False
282,AP ROBUST,1,AP,True
283,HT,0,,False
284,WT10G GOV2,1,WT,True
285,CLUEB09 CLUEB09F,0,,False
286,35.0 30.0,0,,False
287,AP ROBUST,1,AP,True
288,HT,0,,False
289,WT10G GOV2,1,WT,True
290,CLUEB09 CLUEB09F,0,,False
291,25.0,0,,False
292,25.0,0,,False
293,20.0,0,,False
294,20.0,0,,False
295,15.0 0,0,,False
296,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,0,,False
297,q,0,,False
298,15.0 0,0,,False
299,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,0,,False
300,d,0,,False
301,"Figure 2: The effect of varying q and d on the MAP performance of HT. The values of free parameters, except for that in the x-axis, are set using cross validation as in Figure 1.",1,MAP,True
302,"and HTOEnt and STOEnt, respectively. Thus, we find that there is much merit in integrating term-based and entitybased information for representing queries and documents.",0,,False
303,"Figure 1 shows that the optimal value of  for HT is often higher than for ST. This can be attributed to the fact that HTLM, used to represent the query and documents in HT, uses a single confidence-level threshold for entity markups. Thus, potentially valuable information about entities is not utilized. As a result, HT calls for more reliance on termbased information to ""compensate"" for this potential information loss. In contrast, ST accounts for all entity markups, weighing their impact by their confidence levels. Hence, the ""risk"" in relying on entity-based information is lower7.",1,LM,True
304,"To further explore the effect of using a hard threshold for the confidence level of entity markups in HT, we present in Figure 2 its MAP performance as a function of q and d -- the query and document thresholds, respectively. Recall that low threshold corresponds to high recall markup. Figure 2 shows that low values of q and d lead to improved performance. This finding can be attributed to the fact that increasing the confidence-level threshold amounts to loosing potentially valuable information about appearances of entities in the query and documents. To compensate for the lower precision (i.e., noisier) markup caused by using a low threshold, more weight is put on term-based information as is evident in the relatively high optimal values of  presented in Figure 1. Specifically, we note that the learned values of , d, and q, averaged over the train folds, for AP, ROBUST, WT10G, GOV2, ClueB and ClueBF are (0.6, 0.01, 0.11), (0.7,0.1,0.01), (0.55,0.1,0.2), (0.77,0.1,0.01), (0.7,0.15,0), and (0.81, 0.17, 0) respectively; namely, relatively high values of  and low values of d and q lead to improved performance.",1,MAP,True
305,Entity linking. Our main evaluation is based on using TagMe,0,,False
306,"for entity linking. In Table 3 we compare the retrieval performance when using the entity markups of TagMe and Wikifier. Having Wikifier annotate large-scale collections is a challenging computational task. Thus, we present results only for AP, ROBUST and WT10G. We report MAP and NDCG; the performance patterns for p@10 are the same.",1,Wiki,True
307,"Table 3 shows that using ST, our best performing method from above, with Wikifier, results in performance that transcends (often, significantly) that of the standard term-based language model (TermsLM) when using all queries in a dataset",1,Wiki,True
308,"7Setting  on a per-query basis, in the spirit of work on fusing term-only-based and latent-entity-space-based retrieval scores [33], is a future direction we intend to explore.",0,,False
309,"Table 3: Comparing entity-linking tools. Either all queries in a dataset are used (""All Queries""), or only those marked with at least one entity by both TagMe and Wikifier (""Marked Queries""). Bold: best result in a column in a block; 't', 's', 'w' and 'e': statistically significant differences with TermsLM, TagMeST, Wikifier-ST and TagMe-STOEnt, respectively.",1,Wiki,True
310,AP,1,AP,True
311,ROBUST,0,,False
312,WT10G,1,WT,True
313,MAP NDCG MAP NDCG MAP NDCG,1,MAP,True
314,All Queries,0,,False
315,TermsLM 20.9 40.4,1,LM,True
316,TagMe ST,0,,False
317,23 5t 45 5t,0,,False
318,.,0,,False
319,.,0,,False
320,Wikifier ST,1,Wiki,True
321,23.3t 43.6,0,,False
322,25.0 28 1t,0,,False
323,. 27.2t,0,,False
324,43.5 46 9t,0,,False
325,. 45.6t,0,,False
326,19.1 22 9t,0,,False
327,". 19.7t,s",0,,False
328,30.3 34 3t,0,,False
329,. 30.9s,0,,False
330,Marked Queries,0,,False
331,TermsLM 22.2,1,LM,True
332,TagMe ST,0,,False
333,25 1t .,0,,False
334,Wikifier ST,1,Wiki,True
335,25 1t .,0,,False
336,"TagMe STOEnt 18.5tw,s",0,,False
337,"Wikifier STOEnt 17.5tw,s",1,Wiki,True
338,41.7 48 4t,0,,False
339,.,0,,False
340,46.2t 41.4s 39.1sw,0,,False
341,25.4 43.9 21.4 34.2,0,,False
342,28 8t 47 3t 24 8t 36 2,0,,False
343,.,0,,False
344,.,0,,False
345,.,0,,False
346,.,0,,False
347,28.0t 46.4t 21.9s 34.0,0,,False
348,"22.9tw,s 41.1sw 18.1s 28.1s",0,,False
349,"19.4tw,s,e 34.8tw,s,e 12.6tw,s,e 21.8tw,s",0,,False
350,"(the ""All Queries"" block). However, the performance of using TagMe is consistently better.",0,,False
351,"TagMe marks more queries with at least one entity than Wikifier: for AP, ROBUST and WT10G, Wikifier marked no entities in 17, 34 and 26 queries, respectively; TagMe did not mark entities in 0, 1 and 3 queries. (For GOV2 TagMe marked all queries with entities and for ClueB/ClueBF all queries except for one.) Recall that for queries with no marked entities, ST relies only on term-based information.",1,Wiki,True
352,"To refine the comparison of TagMe and Wikifier, we report the performance of ST and STOEnt8 -- the latter relies only on entity tokens -- with these two tools over only queries in which both marked at least one entity. As can be seen in the ""Marked Queries"" block in Table 3, TagMe still outperforms Wikifier in almost all relevant comparisons; for STOEnt, several improvements are statistically significant.",1,Wiki,True
353,"TagMe's superiority can be partially attributed to marking more entities (with confidence level > 0) on average than Wikifier: (2.4, 1.8, 2.0) with respect to (1.7, 1.2, 1.0) in queries over AP, ROBUST and WT10G; and, (157.2, 158.7, 207.0) with respect to (58.4, 50.5, 61.7) in documents.",1,Wiki,True
354,"To conclude, our methods are effective with both TagMe and Wikifier. Using TagMe yields better performance that can be partially attributed to higher recall entity markup.",1,Wiki,True
355,4.2.2 The score-based fusion methods,0,,False
356,"Table 4 presents the performance of the F-HT and FST methods from Section 3.2.1 that perform score fusion of term-only-based and entity-only-based retrieval scores. The performance of TermsLM (term-only language model), HT and ST that integrate term and entity information at the language model level, and that of F-HTCon which is a special case of F-HT (see Section 4.1), is presented for reference. We see that F-HT and F-ST substantially outperform TermsLM. (F-ST posts the best performance in most relevant comparisons in Table 4.) Both methods also outperform F-HTCon in most relevant comparisons.",1,LM,True
357,"8For queries for which a tool does not mark any entities, no documents are retrieved with STOEnt. Thus, we do not report the performance of STOEnt using all queries as the results are inherently biased in favor of TagMe which marks many more queries with entities than Wikifier.",1,Wiki,True
358,70,0,,False
359,"Table 4: Score-based fusion (""F-"" methods). Bold: best result in a row; 't', 'h', 's', 'f ' and 'c': statistically significant differences with TermsLM, HT, ST, F-HT and F-HTCon, respectively.",1,LM,True
360,TermsLM HT ST F-HT F-HTCon F-ST,1,LM,True
361,MAP,1,MAP,True
362,AP,1,AP,True
363,p@10,0,,False
364,NDCG,0,,False
365,MAP ROBUST p@10,1,MAP,True
366,NDCG,0,,False
367,MAP WT10G p@10,1,MAP,True
368,NDCG,0,,False
369,GOV2,0,,False
370,MAP p@10,1,MAP,True
371,NDCG,0,,False
372,ClueB,1,Clue,True
373,MAP p@10,1,MAP,True
374,NDCG,0,,False
375,ClueBF,1,Clue,True
376,MAP p@10,1,MAP,True
377,NDCG,0,,False
378,20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8,0,,False
379,17.1,0,,False
380,22.7 16.5 18.8 33.6 24.3,0,,False
381,23.1t 23.5t 23.1t,0,,False
382,44.2t 43.8t,0,,False
383,44 5t .,0,,False
384,45.3t 45.5t,0,,False
385,46 2t .,0,,False
386,28.1t 28.1t 28.1t 45.5t 45.3t 45.7t,0,,False
387,47.1t 46.9t 47.3t,0,,False
388,21.9t,0,,False
389,"22 9t,h .",0,,False
390,22.2t,0,,False
391,30.4t 31.6t 30.0,0,,False
392,32.7,0,,False
393,34 3t .,0,,False
394,32.7,0,,False
395,32.1t,0,,False
396,57.3t 47.4t,0,,False
397,32.2t,0,,False
398,57.7t 47.9t,0,,False
399,"33.5ts,h 58 6t",0,,False
400,. 48 7t,0,,False
401,.,0,,False
402,"18.7t 19.5t 19.6t,h",0,,False
403,25.9t 27.4t 26.4t,0,,False
404,18.7t 19.3t 19.1t,0,,False
405,"20.5t 20.3t 21.3t,h",0,,False
406,37.9t 37.9t,0,,False
407,39 6t .,0,,False
408,28.4t 27.5t 29.5ts,0,,False
409,22.5s 43.5t 45.1t 27.7t 45.2t 46.6t 21.6ts 30.4t,0,,False
410,"33.1 30.6hs,f",0,,False
411,57.0,0,,False
412,46.6 19.3t,0,,False
413,27.5t 19.9t,0,,False
414,19.7f,0,,False
415,36.5f 27.6,0,,False
416,"23 9t,h . f ,c",0,,False
417,44.2t 45.8t,0,,False
418,"28.4tc 46.7ts,c 47.8tc",0,,False
419,22.9tc 31 8t,0,,False
420,.,0,,False
421,33.7t,0,,False
422,"33.3ts,,hc 58.0t 48.2t",0,,False
423,"20 8t,h . s,f ,c",0,,False
424,"28 8t,h .f",0,,False
425,"20 5t,h .f",0,,False
426,"21.8ts,,hc 39.4tc 29.2ts",0,,False
427,"Table 5: Comparison and integration with SDM [36]. Bold: the best result in a row. 't', 's', 'f ' and 'm' mark statistically significant differences with TermsLM, ST, F-ST and SDM, respectively.",1,LM,True
428,AP ROBUST WT10G GOV2 ClueB ClueBF,1,AP,True
429,TermsLM ST F-ST SDM SDM+STLM,1,LM,True
430,MAP p@10 NDCG MAP p@10 NDCG MAP p@10 NDCG MAP p@10 NDCG,1,MAP,True
431,MAP p@10 NDCG MAP p@10 NDCG,1,MAP,True
432,20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8,0,,False
433,17.1 22.7 16.5 18.8 33.6 24.3,0,,False
434,23.5t,0,,False
435,43.8t 45.5t,0,,False
436,23 9t .,0,,False
437,44 2t .,0,,False
438,45 8t .,0,,False
439,21.6sf 40.6f 42.3f,0,,False
440,28.1t,0,,False
441,28 4t .,0,,False
442,"25.7tf,s",0,,False
443,45.3t,0,,False
444,"46 7t,s .",0,,False
445,43.9tf,0,,False
446,46.9t,0,,False
447,47 8t .,0,,False
448,"44.8tf,s",0,,False
449,22.9t 31.6t 34 3t,0,,False
450,.,0,,False
451,22.9t 31 8t,0,,False
452,. 33.7t,0,,False
453,20.2sf 27.7sf 30.7sf,0,,False
454,"32.2t 33.3t,s 32.1t",0,,False
455,57.7t 58.0t 58.3t,0,,False
456,47.9t 48.2t 48.4t,0,,False
457,19.5t 27.4t 19.3t,0,,False
458,"20.8t,s 28.8t 20.5t",0,,False
459,"18.2tf,s 23.8sf 16.9sf",0,,False
460,"20.3t 21.8t,s 20.2tf",0,,False
461,37.9t 39.4t 35.8tf,0,,False
462,"27.5t 29.2t,s 25.9tf",0,,False
463,23.9tm 44.2tm 45.8tm,0,,False
464,"28.3tm 45.7tf,m 47.1tf,m 23.1tm 31.6tm 34.0tm 34 7t,s",0,,False
465,". f ,m 61 4t,s",0,,False
466,". f ,m 50 6t,s",0,,False
467,". f ,m 21.5tm,s",0,,False
468,"30 8t,s . f ,m",0,,False
469,"21.9tm,s",0,,False
470,"22 7t,s . f ,m",0,,False
471,"42 8t,s . f ,m",0,,False
472,"32 2t,s . f ,m",0,,False
473,"In most relevant comparisons, F-HT outperforms HT and F-ST outperforms ST, although most performance differences are not statistically significant. The improvements can be attributed to the fact that F-HT and F-ST use a different smoothing parameter value for terms and entities while HT and ST use a joint one. (See Section 3.2.1 for details.)",0,,False
474,"The potential effectiveness of using different smoothing parameters for term and entity tokens stems from the different number of terms and entity markups in a document. The average number of terms in a document for AP, ROBUST, WT10G, GOV2, and ClueB (ClueBF) is 455.4, 474.8, 588.2, 904.7 and 813.6, respectively. The average number of entity markups with a confidence level > 0 is much lower: 157.2, 158.7, 207.0, 291.9 and 307.8.",1,AP,True
475,4.2.3 Comparison and integration with SDM,0,,False
476,We next compare our entity-based approach with the sequential dependence model (SDM) [36] which scores d by:,0,,False
477,"SSDM (d; q) d,""ef SSimS(d, q)+OSimO(d, q)+U SimU (d, q);""",0,,False
478,"the sum of the S, O and U parameters is 1; SimS(d, q), SimO(d, q) and SimU (d, q) are cross-entropy based similarity estimates of the document to the query, utilizing information about occurrences of unigram, ordered bigrams, and unordered bigrams, respectively, of q's terms in d; un-ordered bigrams are confined to 8-terms windows in documents.",0,,False
479,"Using entity tokens in our methods amounts to utilizing information about the occurrences of only some ordered variable-length n-grams of query terms in documents -- i.e., n-grams which constitute entities. Thus, in contrast to SDM, our methods do not utilize proximity information for query terms which are not in entity markups nor proximity information for unordered n-grams of query terms.",0,,False
480,"In addition, we study the merit of integrating entity-based information, specifically, our soft-thresholding language model STLM, with SDM. To that end, we augment the SDM scoring function with an entity-based document-query similar-",1,ad,True
481,"ity estimate, SimE(d, q). For this estimate, we use the score assigned to d by the STOEnt method; i.e., we use an entity-only language model since term-based information is accounted for in SimS(d, q). The resultant method, SDM+STLM, scores d by (S + O + U + E , 1):",1,LM,True
482,"SSDM+ST LM (d; q) d,""ef S SimS (d, q) + OSimO(d, q)+""",1,LM,True
483,"U SimU (d, q) + ESimE(d, q).",0,,False
484,"SDM+STLM can be viewed as a novel instantiation of a weighted dependence model (WSDM) [4] with a novel concept type (i.e., entity). If O , U ,"" 0, SDM+STLM amounts to our F-ST method (see Section 3.2.1).""",1,LM,True
485,"All free parameters of SDM and SDM+STLM: S, O, U , E and the Dirichlet smoothing parameter, µ, are set using cross validation as described in Section 4.1; S, O, U , and E are selected from {0, 0.1, . . . , 1} and µ is set to values in {100, 500, 1000, 1500, 2000, 2500, 3000}.",1,LM,True
486,"Table 5 shows that ST and F-ST outperform SDM, often statistically significantly, in most relevant comparisons (6 corpora × 3 evaluation measures). This implies that using variable length n-grams which potentially bear semantic meaning (entities) can yield better performance than using ordered and unordered bigrams which do not necessarily have semantic meaning. Recall that in contrast to SDM, ST and F-ST do not account for proximities between terms which do not constitute entities and for unordered bigrams.",1,corpora,True
487,"In most relevant comparisons, SDM+STLM outperforms SDM and ST (which utilizes STLM) and is as effective as, and often posts statistically significant improvements over, F-ST -- its special case that fuses unigram term-only and entity-only retrieval scores. The few cases where F-ST outperforms SDM+STLM could be attributed to potential overfitting effects due to the high number of free parameters of SDM+STLM and the relatively low number of queries.",1,LM,True
488,We also found that effective weights assigned to entityonly similarities in SDM+STLM (E) are much higher than those assigned to ordered (O) and un-ordered (U ) bigram,1,LM,True
489,71,0,,False
490,Table 6: Robustness analysis. Number of queries for which ST hurts (-) and improves (+) AP performance with respect to TermsLM and SDM.,1,Robust,True
491,AP ROBUST WT10G GOV2 ClueB ClueBF,1,AP,True
492,-+- + - + - + - + - +,0,,False
493,ST vs. TermsLM 38 61 75 173 31 63 50 99 54 137 75 112,1,LM,True
494,ST vs. SDM,0,,False
495,35 64 87 161 33 60 74 75 79 112 89 97,0,,False
496,"term-based similarities. Furthermore, effective values of O and U are lower and higher, respectively, for SDM+STLM than for SDM. These findings further attest to the merits of using entity-based similarities with respect to (ordered and un-ordered) bigram similarities, and show that un-ordered bigram, in contrast to ordered bigram, similarities could be complementary to entity-based similarities.",1,LM,True
497,4.2.4 Further analysis,0,,False
498,"We now turn to further analyze merits, and shortcomings, of using entity-based query and document representations. To that end, we focus on the ST method that utilizes STLM.",1,LM,True
499,"Table 6 presents performance robustness analysis: the number of queries for which ST improves or hurts average precision (AP) over TermsLM and SDM. In both cases, ST improves AP for more queries than it hurts; naturally, the differences with SDM are smaller than those with TermsLM.",1,AP,True
500,"One advantage of STLM is that it represents the query and documents using entities which constitute variable length n-grams with semantic meaning. A case in point, query #41 in ClueWeb, ""orange county convention center"", refers to the primary public convention center for the Central Florida region. TermsLM, SDM and ST ranked the Web home page for this entity second. However, at the third rank in the lists retrieved by TermsLM and SDM appears a Wikipedia page titled ""list of convention and exhibition centers"", which is not specific to the entity of concern. The average precision (AP) of TermsLM, SDM and ST for the query in the ClueB dataset was 9, 13, and 30, respectively, attesting to the merit of the correct identification of the entity in the query and its utilization by ST.",1,ad,True
501,"The ST method can suffer from incorrect entity identification in queries. For example, query #407 in ROBUST, ""poaching, wildlife preserves"", targets information about the impact of poaching on the world's various wildlife preserves. The entities identified by TagMe are ""poaching"", ""wildlife"" and ""preserves""; the latter refers to fruit preserves instead of nature preserves. Such erroneous entity identification can be attributed to the little context short queries provide. Consequently, the AP of ST for this query is only 8 while that of TermsLM and SDM is 31.4 and 30.0, respectively.",1,ad,True
502,4.3 Using entity-based language models in additional retrieval paradigms,1,ad,True
503,We next explore the effectiveness of using our entity-based language models in two additional retrieval paradigms: clusterbased document retrieval and query expansion.,1,ad,True
504,4.3.1 Cluster-based document retrieval,0,,False
505,"Let Dinit denote the list of top-n documents retrieved by TermsLM (standard language-model-based retrieval). Following common practice in work on cluster-based document retrieval [32, 24], we re-rank Dinit using information induced from nearest-neighbor clusters of documents in Dinit.",1,LM,True
506,Table 7: Cluster-based document re-ranking. Bold:,0,,False
507,"the best result in a row; 't', 's', '' and '' mark sta-",0,,False
508,"tistically significant differences with TermsLM, ST,",1,LM,True
509,"C-Term-Term and C-Term-Ent, respectively.",0,,False
510,TermsLM ST C-Term-TermC-Term-EntC-Ent-Ent,1,LM,True
511,AP,1,AP,True
512,p@10 NDCG,0,,False
513,ROBUST p@10 NDCG,0,,False
514,WT10G,1,WT,True
515,p@10 NDCG,0,,False
516,GOV2,0,,False
517,p@10 NDCG,0,,False
518,ClueB,1,Clue,True
519,p@10 NDCG,0,,False
520,ClueBF,1,Clue,True
521,p@10 NDCG,0,,False
522,39.6 40.8 42.2,0,,False
523,43.5,0,,False
524,28.6 31.2 53.4 45.0 23.7 17.2 32.1 22.9,0,,False
525,42.5 44.8t 44.3t,0,,False
526,45.5t,0,,False
527,30.6,0,,False
528,33.4 57.0t,0,,False
529,46.8 27.1t,0,,False
530,19.1 36.9t 27.8t,0,,False
531,43.2t 44.2t,0,,False
532,43.1,0,,False
533,44.2,0,,False
534,30.2 32.1 55.1 45.8 23.7 17.2 31.2s 23.1s,0,,False
535,44.3t,0,,False
536,44.9,0,,False
537,"46.0t 47.5t 3335.7.4tt,s 58 3t",0,,False
538,. 48 9t,0,,False
539,.,0,,False
540,"33 0t,s 24.9t,s",0,,False
541,. 3308.53tt,0,,False
542,.,0,,False
543,"46 5t,s .",0,,False
544,46 8t .,0,,False
545,"47 7t,s . ,",0,,False
546,"49 1t,s . ,",0,,False
547,"34 8t,s 36.3t,s",0,,False
548,". 57.9t 47.8t 3212..59tt,,ss 39 0t 29..6t",0,,False
549,"We use Sim(x, y) d,ef exp(-CE `xMLE || yDir´) to measure the similarity between texts x and y [24]; xMLE is an unsmoothed MLE induced from x and yDir is a Dirichlet smoothed language model induced from y. Each document",0,,False
550,"d ( Dinit) and the k - 1 documents d (d ,"" d) in Dinit that yield the highest Sim(d, d) constitute a cluster.""",0,,False
551,"We rank the (overlapping) clusters c, each contains k doc-",0,,False
552,q,0,,False
553,"uments, by:",0,,False
554,k,0,,False
555,Q,0,,False
556,dc,0,,False
557,S,0,,False
558,"im(q,",0,,False
559,d),0,,False
560,[32].,0,,False
561,This is a highly effective,0,,False
562,simple cluster ranking method [24]. To induce document,0,,False
563,"ranking, each cluster is replaced with its constituent docu-",0,,False
564,ments omitting repeats; documents in a cluster are ordered,0,,False
565,"by their query similarity: Sim(q, d).",0,,False
566,The document (re-)ranking procedure just described re-,0,,False
567,lies on the choice of the document language models used to,0,,False
568,"induce clusters (i.e., in Sim(d, d)) and the choice of docu-",0,,False
569,ment and query language models used to induce document-,0,,False
570,"query similarities (Sim(q, d)); the latter are used for rank-",0,,False
571,ing both clusters and documents within the clusters. We,0,,False
572,use C-Term-Term to denote the standard method that,0,,False
573,uses term-only language models for inducing clusters and,0,,False
574,"document-query similarities [32, 24]. The C-Term-Ent",0,,False
575,"method utilizes the same clusters used by C-Term-Term, but",0,,False
576,"uses our entity-based language model, STLM, for inducing",1,LM,True
577,document-query similarities to rank clusters and documents,0,,False
578,"in them. In the C-Ent-Ent method, STLM is used to both",1,LM,True
579,create clusters and induce document-query similarities. As a,0,,False
580,"reference comparison, we re-rank Dinit using the ST method",0,,False
581,that uses STLM but does not utilize clusters.,1,LM,True
582,As the main goal of cluster-based re-ranking is improv-,0,,False
583,"ing precision at top ranks [32, 24], we report p@10 and",0,,False
584,NDCG@10 (NDCG). Free-parameter values are set using,0,,False
585,cross validation; NDCG is the optimization criterion. Specif-,0,,False
586,"ically, n is selected from {50, 100}; k is in {5, 10}; and, ",0,,False
587,"(used in STLM) is in {0, 0.1, . . . , 1}; the Dirichlet smooth-",1,LM,True
588,ing parameter is set to 1000. Table 7 presents the results.,0,,False
589,"We see that all cluster-based methods (denoted ""C-X-Y"")",0,,False
590,almost always outperform the initial term-based document,0,,False
591,"ranking, TermsLM. C-Term-Ent substantially outperforms",1,LM,True
592,C-Term-Term. This attests to the merits of using STLM,1,LM,True
593,for inducing cluster ranking and within cluster document,0,,False
594,"ranking. In most relevant comparisons, C-Ent-Ent outper-",0,,False
595,forms (and is never statistically significantly outperformed,0,,False
596,"by) C-Term-Ent, attesting to the potential merits of using",0,,False
597,72,0,,False
598,Table 8: Query expansion. Bold: the best result,1,Query,True
599,"in a row. 't', 's', 'r', 'w', 'm' and 'n' mark sta-",0,,False
600,"tistically significant differences with TermsLM, ST,",1,LM,True
601,"RM3, WikiRM, SDM-RM and RMST, respectively.",1,Wiki,True
602,TermsLM ST RM3 WikiRMSDM-RM RMST RMST-ST,1,LM,True
603,MAP AP p@10,1,MAP,True
604,NDCG MAP,1,MAP,True
605,ROBU p@10,0,,False
606,ST NDCG MAP,1,MAP,True
607,WT p@10 10G NDCG,1,WT,True
608,MAP GOV2 p@10,1,MAP,True
609,NDCG MAP ClueB p@10 NDCG MAP Clue p@10 BF NDCG,1,MAP,True
610,20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8 17.1 22.7 16.5 18.8 33.6 24.3,0,,False
611,23.5t 24.1t 24.0t,0,,False
612,24.9t,0,,False
613,"24.6t 27.4tw,s,m ,r,n",0,,False
614,43.8t 42.5t 46.2t,0,,False
615,43.9t,0,,False
616,44.8t,0,,False
617,"46 8t,r .",0,,False
618,45.5t,0,,False
619,43.2,0,,False
620,"48 2t,r .",0,,False
621,45.6t,0,,False
622,45.0t,0,,False
623,"47.4t,r",0,,False
624,28.1t 28.3t 27.8t,0,,False
625,28.4t,0,,False
626,"29.0t 30.5tw,s,m ,r,n",0,,False
627,45.3t 43.6 44.6t,0,,False
628,"43.2 45.9tm,r 47.1tw,s,m ,r",0,,False
629,"46.9t 43.8s 46.1t,r 43.6sw 46.5tm,r 47.2tm,r",0,,False
630,22 9t .,0,,False
631,19.6s,0,,False
632,"21.9t,r",0,,False
633,20.0s,0,,False
634,"22.7tm,r",0,,False
635,"22.8tm,r",0,,False
636,31.6t 28.0s,0,,False
637,"34 2t,r .",0,,False
638,28.6w,0,,False
639,"31.7tm,r",0,,False
640,34 3t .,0,,False
641,30.1s,0,,False
642,"34 3t,r .",0,,False
643,30.5sw,0,,False
644,32.9,0,,False
645,31.1t 31.8s,0,,False
646,32.2t 32.4t 32.1t,0,,False
647,33.7tw,0,,False
648,33.1t,0,,False
649,"33 7t,s .",0,,False
650,57.7t 58.1t,0,,False
651,60 1t .,0,,False
652,58.0t,0,,False
653,59.6t,0,,False
654,58.5t,0,,False
655,47.9t 48.0t,0,,False
656,50 6t .,0,,False
657,47.6,0,,False
658,49.4t,0,,False
659,48.8t,0,,False
660,"19.5t 19.3t 21.9t,s,r 20.9t,r 20.7t,s,r 22.1tn,s,r",0,,False
661,27.4t,0,,False
662,30.6t,0,,False
663,"35 3t,s,r .",0,,False
664,"32.2tw,s",0,,False
665,"32.2tw,s 34.9tn,s,r",0,,False
666,"19.3t 22.6t,s 26.1t,s,r 24.3t,s 25.1t,s,r 27.1tn,s,r",0,,False
667,"20.3t 20.4t 21.0t 21.8t,s,r 20.8t 21.9tn,s",0,,False
668,37.9t 37.9t 38.5t,0,,False
669,"39 7t,r .",0,,False
670,38.2t,0,,False
671,38.4t,0,,False
672,27.5t 28.1t 28.2t,0,,False
673,"29.8t,r",0,,False
674,28.5t,0,,False
675,"30 3t,s .",0,,False
676,"entity-based information to also create clusters. However, only two improvements are statistically significant.",0,,False
677,"Finally, Table 7 shows that in almost all relevant comparisons, ST outperforms TermsLM (often, statistically significantly) and C-Term-Term and is outperformed by C-TermEnt and C-Ent-Ent. This shows that while there is merit in using STLM for direct ranking of documents as shown in Section 4.2.1, the performance can be further improved by using STLM for cluster-based document ranking.",1,LM,True
678,4.3.2 Query expansion,1,Query,True
679,"As noted in Section 2, there is much work on expanding queries with terms and entities using entity-based information. In contrast, our entity-based language models, when induced from the query, utilize only query terms and entities marked in the query. Hence, we study the effectiveness of using our language models to perform query expansion.",0,,False
680,We use the relevance model (RM3) [1] as a basis for instantiating expanded query forms. The probability assigned to token t by a relevance model RM is:,0,,False
681,RM (t),0,,False
682,"d,ef",0,,False
683,qMLE (t),0,,False
684,+,0,,False
685,(1,0,,False
686,-,0,,False
687,),0,,False
688,X,0,,False
689,dL,0,,False
690,dDir,0,,False
691,(t),0,,False
692,S(d; q),0,,False
693,P,0,,False
694,d L,0,,False
695,S(d,0,,False
696,;,0,,False
697,q),0,,False
698,;,0,,False
699,(8),0,,False
700, is a free parameter; L is a list of top-retrieved documents,0,,False
701,used to construct RM ; S(d; q) is d's score. Due to computa-,0,,False
702,"tional considerations, as in work on entity-based query ex-",0,,False
703,"pansion [13, 45] we use RM to re-rank an initially retrieved",0,,False
704,"document list; CE `RM || dDir´ serves for re-ranking. Using only terms as tokens, and applying standard language-",0,,False
705,model-based retrieval (TermsLM) over the corpus to create,1,LM,True
706,"L, yields the standard RM3 [1]. Creating L by applying",0,,False
707,"TermsLM over Wikipedia results in WikiRM [46], an ex-",1,LM,True
708,"ternal corpus expansion approach also used in [13, 45]. RM3",0,,False
709,and WikiRM re-rank a document list retrieved by TermsLM.,1,Wiki,True
710,(WikiRM is the only model where the list from which RM is,1,Wiki,True
711,"constructed, L, is not a sub-set of the list to be re-ranked.)",0,,False
712,"In both methods, S(d; q) d,""ef exp(-CE `qMLE || dDir´). The SDM-RM model [13] is constructed from, and used""",0,,False
713,"to re-rank, lists retrieved by the sequential dependence model",0,,False
714,"(SDM) [36]. dDir, and the resultant relevance model constructed by setting  ,"" 0 in Equation 8, are term-based unigram language models; S(d; q) is the exponent of the score assigned to d by SDM. Re-ranking is performed by linear interpolation of the SDM score assigned to d and CE `RM || dDir´, using a parameter . SDM-RM is, in fact, the highly effective Latent Concept Expansion method [37] without IDF-based weighting of expansion terms.""",0,,False
715,"The next two relevance models, defined over T (the termentity token space from Equation 1), are novel to this study. They utilize our STLM language model which integrates terms and entities at the language model level. RMST is inspired by methods proposed by Dalton et al. [13]9 by the virtue of using both terms and entities for query expansion. qMLE and dDir are our STLM language models. S(d; q) d,ef exp(-CE `qMLE || dDir´). The TermsLM method is applied over the corpus to create the initial list to be re-ranked (cf. [45]) and from which L is derived.",1,LM,True
716,"RMST-ST is constructed as RMST using STLM. The difference is that our entity-based ST method, rather than TermsLM, is used to create the initial list to be re-ranked and from which L is derived. The formal ease of using STLM in the relevance model (Equation 8), yielding RMST and RMST-ST, attests to the merits of using a single language model defined over terms and entities with respect to the alternative score-based fusion approach from Section 3.2.1.",1,LM,True
717,"The free parameters of all methods are set using cross validation. The number of expansion terms (i.e., those assigned the highest probability by RM ), the number of documents in L, and  are set to values in {10, 30, 50, 100}, {50, 100} and {0, 0.1, . . . , 1}, respectively. (Only for WikiRM, the number of documents in L is selected from {1, 5, 10, 30, 50, 100} following [46].) All lists that are re-ranked contain 1000 documents. The values of the free parameters of ST and SDM are selected from the ranges specified in Section 4.1. The Dirichlet smoothing parameter, µ, is selected from {100, 500, 1000, 1500, 2000, 2500, 3000}; for relevance model construction (Equation 8) the value 0 is also used (yielding unsmoothed MLE). To reduce the number of free-parameter values configurations, we use the same value of µ for creating L, for re-ranking and for constructing the relevance model, unless 0 is used for relevance model construction.",1,Wiki,True
718,"Table 8 presents the performance. Our ST method, which does not perform query expansion, is competitive with the term-based relevance model (RM3). We also see that RMST is an effective expansion method which often outperforms RM3 and SDM-RM. This finding echoes those from past work [13, 45] about the merits of using both terms and entities for query expansion. The best performing method in most relevant comparisons is RMST-ST which uses STLM to (i) create an effective initial list for re-ranking; (ii) create an effective list, L, for relevance model construction; and, (iii) induce ranking using the entity-based relevance model as in RMST. We conclude that our STLM language model can play different important roles in query expansion.",1,LM,True
719,"Table 8 shows that expansion using Wikipedia as an external corpus (WikiRM) is effective. Our RMST and RMSTST expansion methods (as well as ST) utilize entity tokens marked by TagMe (i.e., Wikipedia concepts), but do no use",1,Wiki,True
720,"9Various expansion methods, which utilize also auxiliary information about entities from the entity repository, were integrated in [13]. We do not use such auxiliary information.",0,,False
721,73,0,,False
722,"the text on their Wikipedia pages in contrast to WikiRM. Thus, integrating WikiRM with our methods, e.g., using score-based integration [13], is interesting future direction.",1,Wiki,True
723,5. CONCLUSIONS,0,,False
724,We presented novel entity-based language models induced using an entity linking tool. The models simultaneously account for the uncertainty in the entity-linking process and the balance between using term-based and entity-based information. We showed the merits of using the language models for document retrieval in several retrieval paradigms.,1,ad,True
725,Acknowledgments. We thank the reviewers for their comments. This paper is based upon work supported in part by a Yahoo! faculty research and engagement award.,1,Yahoo,True
726,6. REFERENCES,0,,False
727,"[1] N. Abdul-jaleel, J. Allan, W. B. Croft, O. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. Umass at TREC 2004: Novelty and hard. In Proc. of TREC-13, 2004.",1,ad,True
728,"[2] J. Allan, J. P. Callan, W. B. Croft, L. Ballesteros, J. Broglio, J. Xu, and H. Shu. Inquery at TREC-5. In Proc. of TREC-5, pages 119­132, 1996.",1,TREC,True
729,"[3] A. R. Aronson, T. C. Rindflesch, and A. C. Browne. Exploiting a large thesaurus for information retrieval. In Proc. of RIAO, volume 94, pages 197­216, 1994.",0,,False
730,"[4] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In Proc. of WSDM, pages 31­40, 2010.",0,,False
731,"[5] M. Bendersky, D. Metzler, and W. B. Croft. Parameterized concept weighting in verbose queries. In Proc. of SIGIR, pages 605­614, 2011.",0,,False
732,"[6] M. Bendersky, D. Metzler, and W. B. Croft. Effective query formulation with multiple information sources. In Proc. of WSDM, pages 443­452, 2012.",0,,False
733,"[7] W. C. Brand~ao, R. L. T. Santos, N. Ziviani, E. S. de Moura, and A. S. da Silva. Learning to expand queries using entities. JASIST, 65(9):1870­1883, 2014.",0,,False
734,"[8] G. Cao, J. Nie, and J. Bai. Integrating word relationships into language models. In Proc. of SIGIR, pages 298­305, 2005.",0,,False
735,"[9] X. Cheng and D. Roth. Relational inference for wikification. In Proc. of EMNLP, pages 1787­1796, 2013.",1,wiki,True
736,"[10] K. Collins-Thompson and J. Callan. Query expansion using random walk models. In Proc. of CIKM, pages 704­711, 2005.",1,Query,True
737,"[11] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 14(5):441­465, 2011.",0,,False
738,"[12] M. Cornolti, P. Ferragina, and M. Ciaramita. A framework for benchmarking entity-annotation systems. In Proc. of WWW, pages 249­260, 2013.",0,,False
739,"[13] J. Dalton, L. Dietz, and J. Allan. Entity query feature expansion using knowledge base links. In Proc. of SIGIR, pages 365­374, 2014.",0,,False
740,"[14] O. Egozi, S. Markovitch, and E. Gabrilovich. Concept-based information retrieval using explicit semantic analysis. ACM Transactions on Information Systems (TOIS), 29(2):8, 2011.",0,,False
741,"[15] P. Ferragina and U. Scaiella. Tagme: On-the-fly annotation of short text fragments (by Wikipedia entities). In Proc. of CIKM, pages 1625­1628, 2010.",1,Wiki,True
742,"[16] W. R. Hersh, D. H. Hickam, and T. Leone. Words, concepts, or both: optimal indexing units for automated information retrieval. In Proc. of SCAMC, page 644, 1992.",0,,False
743,"[17] D. Hiemstra. Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term. In Proc. of SIGIR, pages 35­41, 2002.",0,,False
744,"[18] M. Hsu, M. Tsai, and H. Chen. Combining wordnet and conceptnet for automatic query expansion: A learning approach. In Proc. of AIRS, pages 213­224, 2008.",0,,False
745,"[19] S. Huston and W. B. Croft. A comparison of retrieval models using term dependencies. In Proc. of CIKM, pages 111­120, 2014.",0,,False
746,"[20] A. Kotov and C. Zhai. Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries. In Proc. of WSDM, pages 403­412, 2012.",0,,False
747,"[21] R. Krovetz and W. B. Croft. Lexical ambiguity and information retrieval. ACM Transactions on Information Systems (TOIS), 10(2):115­141, 1992.",0,,False
748,"[22] G. Kumaran and J. Allan. A case for shorter queries, and helping users create them. In Proc. of NAACL, pages 220­227, 2007.",0,,False
749,"[23] H.-K. J. Kuo and W. Reichl. Phrase-based language models for speech recognition. In Proc. of EUROSPEECH, 1999.",0,,False
750,"[24] O. Kurland and E. Krikon. The opposite of smoothing: A language model approach to ranking query-specific document clusters. Journal of Artificial Intelligence Research (JAIR), 41:367­395, 2011.",0,,False
751,"[25] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proc. of SIGIR, pages 111­119, 2001.",0,,False
752,"[26] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. of SIGIR, pages 120­127, 2001.",0,,False
753,"[27] M. Levit, S. Parthasarathy, S. Chang, A. Stolcke, and B. Dumoulin. Word-phrase-entity language models: getting more mileage out of n-grams. In Proc. of INTERSPEECH, pages 666­670, 2014.",0,,False
754,"[28] H. Li and J. Xu. Semantic matching in search. Foundations and Trends in Information Retrieval, 7(5):343­469, 2014.",0,,False
755,"[29] R. Li, L. Hao, P. Zhang, D. Song, and Y. Hou. A query expansion approach using entity distribution based on markov random fields. In Proc. of AIRS, 2015.",0,,False
756,"[30] S. Liu, F. Liu, C. T. Yu, and W. Meng. An effective approach to document retrieval via utilizing wordnet and recognizing phrases. In Proc. of SIGIR, pages 266­272, 2004.",0,,False
757,"[31] X. Liu, F. Chen, H. Fang, and M. Wang. Exploiting entity relationship for query expansion in enterprise search. Information Retrieval Journal, 17(3):265­294, 2014.",0,,False
758,"[32] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proc. of ECIR, pages 454­462, 2008.",0,,False
759,"[33] X. Liu and H. Fang. Latent entity space: a novel retrieval approach for entity-bearing queries. Information Retrieval Journal, 18(6):473­503, December 2015.",0,,False
760,"[34] R. Mandala, T. Tokunaga, and H. Tanaka. Combining multiple evidence from different types of thesaurus for query expansion. In Proc. of SIGIR, pages 191­197, 1999.",0,,False
761,"[35] E. Meij, D. Trieschnigg, M. de Rijke, and W. Kraaij. Conceptual language models for domain-specific retrieval. Information Processing & Management, 46(4):448­469, 2010.",0,,False
762,"[36] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proc. of SIGIR, pages 472­479, 2005.",0,,False
763,"[37] D. Metzler and W. B. Croft. Latent concept expansion using markov random fields. In Proc. of SIGIR, pages 311­318, 2007.",0,,False
764,"[38] D. Milne and I. H. Witten. Learning to link with Wikipedia. In Proc. of CIKM, pages 509­518, 2008.",1,Wiki,True
765,"[39] D. Pan, P. Zhang, J. Li, D. Song, J. Wen, Y. Hou, B. Hu, Y. Jia, and A. N. D. Roeck. Using Dempster-Shafer's evidence theory for query expansion based on freebase knowledge. In Proc. of AIRS, pages 121­132, 2013.",0,,False
766,"[40] C. Shah and W. B. Croft. Evaluating high accuracy retrieval techniques. In Proc. of SIGIR, pages 2­9, 2004.",0,,False
767,"[41] P. Srinivasan. Query expansion and medline. Information Processing & Management, 32(4):431­443, 1996.",1,Query,True
768,"[42] E. M. Voorhees. Using wordnet to disambiguate word senses for text retrieval. In Proc. of SIGIR, pages 171­180, 1993.",0,,False
769,"[43] E. M. Voorhees. Query expansion using lexical-semantic relations. In Proc. of SIGIR, pages 61­69, 1994.",1,Query,True
770,"[44] C. Xiong and J. Callan. EsdRank: Connecting query and documents through external semi-structured data. In Proc. of CIKM, pages 951­960, 2015.",0,,False
771,"[45] C. Xiong and J. Callan. Query expansion with Freebase. In Proc. of ICTIR, pages 111­120, 2015.",1,Query,True
772,"[46] Y. Xu, G. J. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on Wikipedia. In Proc. of SIGIR, pages 59­66, 2009.",1,Query,True
773,"[47] Y. Yang and C. G. Chute. Words or concepts: the features of indexing units and their optimal use in information retrieval. In Proc. of SCAMC, page 685, 1993.",0,,False
774,"[48] C. Zhai. Statistical language models for information retrieval: A critical review. Foundations and Trends in Information Retrieval, 2(3):137­213, 2008.",0,,False
775,"[49] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. of SIGIR, pages 334­342, 2001.",1,ad,True
776,74,0,,False
777,,0,,False

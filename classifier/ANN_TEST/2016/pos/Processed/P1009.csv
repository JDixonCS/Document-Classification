,sentence,label,data,regex
0,Simple Dynamic Emission Strategies for Microblog Filtering,1,blog,True
1,"Luchen Tan, Adam Roegiest, Charles L. A. Clarke, and Jimmy Lin",0,,False
2,"David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada",1,ad,True
3,"{luchen.tan, aroegies, jimmylin}@uwaterloo.ca, claclark@gmail.com",0,,False
4,ABSTRACT,0,,False
5,"Push notifications from social media provide a method to keep up-to-date on topics of personal interest. To be effective, notifications must achieve a balance between pushing too much and pushing too little. Push too little and the user misses important updates; push too much and the user is overwhelmed by unwanted information. Using data from the TREC 2015 Microblog track, we explore simple dynamic emission strategies for microblog push notifications. The key to effective notifications lies in establishing and maintaining appropriate thresholds for pushing updates. We explore and evaluate multiple threshold setting strategies, including purely static thresholds, dynamic thresholds without user feedback, and dynamic thresholds with daily feedback. Our best technique takes advantage of daily feedback in a simple yet effective manner, achieving the best known result reported in the literature to date.",1,TREC,True
6,1. INTRODUCTION,1,DUC,True
7,"Filtering topical events from social media streams, such as Twitter, provides a means for users to keep up-to-date on topics of interest to them. If care is taken, these updates may even be pushed directly to the user through notifications on mobile devices or desktops. However, for push notifications to be successful, the user must be given means to control the frequency and volume of updates, avoiding indiscriminate and unwanted notifications. This frequency and volume depends both on the interests of the user, with topics of greater interest updated in greater volume, and on the topics themselves, with some topics naturally receiving updates more frequently than others.",1,Twitter,True
8,"We might update a user interested in polls for the 2016 U.S. presidential elections many times a day during the election cycle itself, but with updates stopping altogether after November 8. We might update a user interested in California residential water restrictions only when these restrictions change, perhaps a few times a year, but interest in the topic might persist for many years, as long as the user is a",0,,False
9,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.",1,ad,True
10,"SIGIR '16, July 17 - 21, 2016, Pisa, Italy",0,,False
11,c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00,0,,False
12,DOI: http://dx.doi.org/10.1145/2911451.2914704,0,,False
13,"resident of the state. For causal sports and entertainment topics (cricket or the Kardashians), a user may not desire more than a few of the most significant updates per day, regardless of events taking place. For topics of great personal importance (a tornado warning or friend's wedding), we might push all updates.",1,ad,True
14,"The TREC Microblog tracks provide an experimental forum for research groups working in this area. In 2015, the track [4] required participating groups to monitor the live ""spritzer"" stream provided by Twitter over a period of ten days in July, selecting tweets relevant to 225 pre-defined interest profiles, each expressed through statements modeled after TREC ad hoc topics. Each tweet returned by participating systems was accompanied by the clock time at which the system would have pushed it to the user. This information was then used by the track organizers to compute various official evaluation measures for a subset of 51 interest profiles. The evaluation measures considered both the relevance of selected tweets and the time at which they were putatively pushed. In addition, the measures accounted for retweets, near-duplicate tweets, and other redundancies, reflecting an expectation that a user would not want to receive notifications about the same thing over and over again.",1,TREC,True
15,"Along with this mobile notification scenario (called ""scenario A"") the evaluation supported a daily digest scenario (called ""scenario B""). At the end of each of the ten days, participating systems returned a ranked list of tweets from that day, just as if a user was sent a summary of the day's events by email. Standard evaluation measures for ranked retrieval may be applied, provided that they also appropriately account for redundant content.",0,,False
16,"To achieve good results for scenario A, systems must successfully address three requirements implicit in the task:",1,ad,True
17,"1. A requirement to score individual tweets with respect to relevance. As a simplification, the evaluation focused on topical relevance. Social signals, such as the prominence of the source or its connection to the user, were not considered, so that the relevance of a tweet is primarily determined by its content.",0,,False
18,"2. A requirement for novelty, so that the system does not push redundant information. This requirement was operationalized by the assessors considering tweets chronologically: if two textually similar tweets arrive at different times, the later tweet is considered redundant if it does not contain substantive information beyond that found in the earlier tweet [9]. Again, only tweet content is considered, so that a duplicate tweet from a more prominent or authoritative source would still be considered redundant.",0,,False
19,1009,0,,False
20,"3. A requirement to avoid pushing non-relevant information altogether. The evaluation measures for scenario A explicitly rewarded systems that avoided pushing non-relevant information. Thus, appropriate selection of thresholds was critical to success. In some cases, the ideal response for a given day was to push nothing, and the ""empty"" strategy of never pushing anything formed a challenging baseline that many systems failed to beat.",0,,False
21,"Since the first two requirements are inherent in any ranking task, we focus on the third requirement. After demonstrating the impact of ignoring the third requirement, we consider various strategies for establishing and maintaining thresholds for pushing tweets. We examine strategies under two assumptions: with and without user feedback. When feedback is not available, thresholds are established from historical information. When feedback is provided, it is limited to once-per-day judgments based on the scenario B output. Of particular importance is the establishment of a global score threshold, applied across all topics in the absence of feedback. Our best technique takes advantage of daily feedback in a simple yet effective manner, achieving the best known result reported in the literature to date.",1,ad,True
22,2. RELATED WORK,0,,False
23,"Filtering has been a longstanding subject for information retrieval research. Earlier work on Topic Detection and Tracking (TDT) investigated algorithms for the discovery of new topics and maintaining awareness when these topics reappear in newswire streams or broadcast news [2]. Experimental results from TREC from the mid-1990s to early2000s indicated that simple IR techniques can achieve high quality results in TDT domains [1, 3, 10, 11]. For example, Allan et al. [1] used the most common words from 1 to 16 training stories to generate short queries which were compared to documents using TF-IDF based similarity.",1,Track,True
24,"More recently, researchers have examined filtering and tracking problems in the context of social media. However, Twitter introduces several issues not present in previous topic tracking tasks, especially in relation to tweets. In particular, tweets have a maximum length of 140 characters and this length limitation implies that meaningful words rarely occur more than once, suggesting that TF-IDF weighting schemes may be less useful. Lin et al. [5] investigated the use of four language modeling smoothing techniques to filter tweets, mitigating issues with sparse terms, i.e., the zero-probability problem. Zhao and Tajima [12] framed a retweet recommendation problem as a multiplechoice secretary problem. They examined Twitter ""portal accounts"", which retweet selected tweets for their followers, and considered a number of strategies for tweet selection. They proposed and compared a number of online and nearonline decision methods, including a history-based threshold algorithm, a stochastic threshold algorithm, a time-interval algorithm, and an ""every k-tweets"" algorithm.",1,Twitter,True
25,3. TREC 2015 MICROBLOG TRACK,1,TREC,True
26,"Topics (called ""interest profiles"") provided to participants in the TREC 2015 Microblog track included a short querylike description of the information need, called the ""title"" in TREC parlance. For example, topic MB235 has the title ""California residential water restrictions"". Other fields in a topic elaborate on the information need, providing a more",1,TREC,True
27,"complete indication of what is and is not relevant. While track participants were permitted to use these other fields for filtering purposes, we focused on the more realistic task in which the user provides only the short query.",0,,False
28,"Track participants filtered the so-called Twitter ""spritzer"" stream over ten days in July 2015, selecting those relevant to 225 pre-defined interest profiles and recording their tweet ids for submission. In addition to the tweet ids, the push time was recorded for each tweet, indicating the time the system decided to push the notification. After all experimental runs were submitted to the track organizers, 51 of the interest profiles were selected for judging. Tweets were pooled and judged on a three-point scale. In evaluating a run, a tweet was considered redundant, providing no gain, if it did not contain substantial new information not found in previously pushed tweets [9].",1,Track,True
29,The primary evaluation measure for scenario A is expected latency-discounted gain:,0,,False
30,1N,0,,False
31,"ELG , N",0,,False
32,GiDi,0,,False
33,(1),0,,False
34,"i,1",0,,False
35,"For N pushed tweets, Gi is the gain associated with tweet i based on the assigned relevance grade, after adjusting for redundancy. The temporal discount applied to tweet i is Di ,"" max(0, (100 - Li)/100), where Li is the latency in minutes between the time the tweet appears in the stream and the time the system decides to push it. ELG is computed on a daily basis, over the tweets pushed by a system that day, with the system's overall score averaged across all topics and all days.""",1,ad,True
36,"ELG has an interesting discontinuity when a system decides not to push anything. For some topics on some days, when no relevant tweets appear in the stream, this is the correct action. On such days, a system pushing any tweet receives a score of zero (since none can possibly be relevant). To reward systems that push nothing on such days, the value of ELG is defined to be one. On the other hand, for systems that push nothing on days when relevant tweets appear in the stream, the value of ELG is defined to be zero. Since no relevant tweets appeared for many topics on many days, the ""empty"" strategy of never pushing anything receives a nonzero ELG score. Indeed, this strategy forms a challenging baseline which many participating systems failed to beat.",0,,False
37,4. BASELINE SYSTEM,0,,False
38,"The system used for the experiments reported in this paper was deployed for the TREC 2015 Microblog track [7], with the design of the system based on substantial pilot experiments conducted prior to the actual evaluation. The system achieved the best ELG score across the 32 automatic runs submitted by 14 participating groups.",1,TREC,True
39,"All experiments described in this paper are from post hoc runs using a replay mechanism over tweet data captured during the evaluation period. Following the TREC evaluation, we performed error analysis to understand the contributions of various system components, and have distilled our algorithm into simple strategies to address the three requirements listed in the introduction, detailed below.",1,hoc,True
40,"Relevance. Although we recognize that social signals and other non-content features are important for relevance, as a first step we only consider tweet content. For pre-processing, we apply language detection tools to eliminate non-English",0,,False
41,1010,0,,False
42,"tweets, tokenize the tweets using Twokenize,1 and then apply some simple tweet quality heuristics, e.g., eliminating tweets containing less than five tokens.",0,,False
43,"For matching against the tweet stream, titles were tokenized by splitting on space and punctuation, and stopwords were removed. Since relevant tweets may not contain all, or even any, of the title terms, we employed a pseudo-relevance feedback step to expand the title terms with 5 hashtags and 10 other terms. This was accomplished by querying Twitter's own search engine with title terms at the beginning of each day and extracting the top terms using pointwise KL-divergence [6, 8].",1,Twitter,True
44,"For relevance scoring, we applied a simple matching formula developed through pilot testing. Given the short length of tweets, many ""standard"" features such as query term frequency appear to have limited value. We found that (binary) query term occurrence appears to be the key feature, with the occurrence of title terms having greater importance than the occurrence of expansion terms. To achieve a balance, we score tweet relevance as follows:",0,,False
45,(wt,0,,False
46,·,0,,False
47,Nt,0,,False
48,+,0,,False
49,we,0,,False
50,·,0,,False
51,Ne),0,,False
52,·,0,,False
53,Nt |T |,0,,False
54,(2),0,,False
55,"where Nt is the number of title terms that appear in the tweet, Ne is the number of expansion terms that appear in the tweet, and |T | is the number of title terms. Based on pilot experiments, weights were set to wt , 3 and we , 1.",0,,False
56,"Novelty. We de-duplicated tweets by computing unigram overlap between each new tweet and the tweets previously pushed for a given topic across all days. Tweets with 60% or more overlap were discarded and not further considered. As with the other parameters, we based this overlap setting on pilot experiments conducted prior to the actual TREC 2015 evaluation.",1,Novelty,True
57,"Thresholding. Thresholds for pushing tweets were based on the relevance score in Equation (2). Each day, a threshold is selected, and only tweets with scores greater than or equal to the threshold are considered for delivery. In addition, the evaluation placed a limit of k ,"" 10 on the number of tweets that could be pushed each day. Once k tweets are pushed, all further system output is ignored.""",1,ad,True
58,"Because of its simplicity, Equation (2) has the interesting property that reasonable thresholds are the same across all queries. Our simplest thresholding strategy is thus to select a single static global threshold (GT) across all queries and days. A simple dynamic strategy (without feedback) is to consider the top k from the previous day, selecting as a threshold the score of the kth tweet. However, under this strategy, we do not lower the threshold below a global minimum, selecting as a threshold max(GT, top-k yesterday).",0,,False
59,"For our TREC 2015 experiments, we used a global threshold of GT ,"" 5 (determined based on previous pilot experiments). To better understand the impact of this threshold, Figure 1 shows the effectiveness of our system for these two strategies across a range of global threshold values. The baseline for this plot is the effectiveness of the empty strategy, i.e., never pushing anything, with low threshold values underperforming it. A global threshold of GT "", 6 slightly outperforms our default threshold of GT ,"" 5, which we retain for the remainder of this paper. We find that our simple strategy of dynamically adjusting the threshold without""",1,TREC,True
60,1http://www.ark.cs.cmu.edu/TweetNLP/,1,Tweet,True
61,0.50 0.45 0.40 0.35,0,,False
62,ELG for different global thresholds,0,,False
63,"Oracle Run Empty Run GT max(GT, top-k yesterday)",0,,False
64,0.30,0,,False
65,ELG,0,,False
66,0.25,0,,False
67,0.20,0,,False
68,0.15,0,,False
69,0.10,0,,False
70,0.05,0,,False
71,0.00,0,,False
72,oracle,0,,False
73,pty em,0,,False
74,"GT,0",0,,False
75,"GT,1",0,,False
76,"GT,2",0,,False
77,"GT,3",0,,False
78,"GT,4",0,,False
79,"GT,5",0,,False
80,"GT,6",0,,False
81,"GT,7",0,,False
82,"GT,8",0,,False
83,"GT,9",0,,False
84,"Figure 1: ELG for different global thresholds, the oracle run, and the empty run.",0,,False
85,feedback is not effective--the difference compared to using a simple global threshold is negligible for most settings.,0,,False
86,"The oracle run in Figure 1 represents the ELG achievable if we made an ideal selection of the global threshold for each topic at the beginning of each day. Clearly, substantial improvements can be achieved through better threshold selection. In the next section, we explore a dynamic emission strategy that uses feedback from each day's digest (scenario B) to select the threshold for the following day.",1,ad,True
87,5. FEEDBACK STRATEGIES,0,,False
88,"Under scenario B, participants submitted a ranked list of tweets for each topic for each day, providing a daily digest of events for the hypothetical user. Building on the baseline system described above, we use each day's digest to provide relevance feedback for determining thresholds for the following day. While in reality each system saved a ranked list during each day of the evaluation period for later submission, and thus system results were not judged until the evaluation concluded, here we assume that relevance information is provided at the end of each day and available for immediate use. We imagine a user interacting with the results once per day, providing feedback as a way of adjusting the filter for the next day. While in practice this daily interaction might be too onerous, the results provide a sense of what gains could be achieved with ongoing feedback.",1,ad,True
89,"For the feedback strategies described below, we used our official scenario B run submitted to TREC 2015 as the daily digest--this ensured that all tweets have relevance judgments. Our method of relevance scoring, Equation (2), often assigns the same score to several tweets. We use the term ""score block"" to denote a set of tweets with the same score. Accordingly, we say that for a score si, it has a corresponding score block SBi. To determine a dynamic threshold using relevance feedback, we begin by combining all feedback received into a single list of score blocks. At the end of day one for a particular topic, we have 10 tweets worth of feedback, on day two, 20 tweets worth, and so on.",1,TREC,True
90,"There are two edge cases to consider. In the first case, if there are no relevant tweets in any score block, we take",0,,False
91,1011,0,,False
92,"Baseline Strategies GT , 5 GT , 6 Feedback Strategies avg gain weighted avg gain weighted avg gain+r score",0,,False
93,"ELG 0.3191 0.3303 (p , 0.3819) ELG 0.3257 (p , 0.5664) 0.3510 (p , 0.0004) 0.3678 (p  0.0000)",0,,False
94,"Table 1: Effectiveness of various emission strategies; p-values are generated from a paired sign test with GT , 5.",0,,False
95,"the maximum of the global threshold (GT ) and the highest score seen so far plus wt, the weight assigned to a title term match. In the second case, if all tweets are relevant, we take the minimum score seen so far.",0,,False
96,"If we have a mix of relevant and non-relevant tweets, we first compute the average gain for each score as follows:",0,,False
97,gain(t),0,,False
98,avg,0,,False
99,gain(si),0,,False
100,",",0,,False
101,sj si tSBj,0,,False
102,|SBj |,0,,False
103,sj si,0,,False
104,"where gain(t) comes from the relevance judgments. We then weight each score's average gain by the proportion of relevant content provided by that score (i.e., the precision of that score):",0,,False
105,weight(si),0,,False
106,",",0,,False
107,|{t|t,0,,False
108,SBi,0,,False
109, gain(t) |SBj |,0,,False
110,>,0,,False
111,0}|,0,,False
112,sj,0,,False
113,"weighted avg gain(si) , avg gain(si) · weight(si)",0,,False
114,"While selecting a threshold that maximizes average gain doesn't perform particularly well, selecting a threshold that maximizes the weighted average gain significantly improves ELG (see Table 1). However, the weighted average gain method still returns too much non-relevant content. To further improve effectiveness, we use the relevance information to adjust the threshold based on the ratio between relevant and non-relevant content:",1,ad,True
115,"|{t|t  SBj  gain(t) , 0}|",0,,False
116,r,0,,False
117,score(si),0,,False
118,",",0,,False
119,sj si,0,,False
120,|{t|t,0,,False
121,SBj,0,,False
122,gain(t),0,,False
123,>,0,,False
124,0}|,0,,False
125,sj si,0,,False
126,"For use as a threshold, a score's r score must be less than some cutoff, . We can vary  to be more or less permissive of non-relevant tweets: via a coarse-grained parameter sweep, we find that  ,"" 1.75 achieves substantially improved results on ELG (see Table 1). Note that if no score achieves an r score less than or equal to , we set the threshold for the next day to be the maximum of the global threshold (GT ) and the highest score seen so far, across all days.""",0,,False
127,6. CONCLUSION,0,,False
128,"Simple techniques for content matching and novelty can achieve good effectiveness for microblog filtering, provided that care is taken to set appropriate thresholds to avoid pushing non-relevant information. Referring back to Figure 1, our most effective technique achieves an ELG of 0.3678, which is still substantially below what might be achieved if",1,blog,True
129,"the optimal threshold could be determined for each topic at the beginning of each day, i.e., the oracle with an ELG of 0.4709. However, we significantly improve upon our already highly-effective baseline (already the best automatic run at TREC 2015). In fact, our technique achieves the best known result reported in the literature to date, including manual runs. Our experiments highlight the importance of proper threshold setting, and demonstrate that systems can automatically set appropriate thresholds using simple yet effective feedback techniques. While dynamic thresholds can be set from the tweets of previous days without feedback, such a strategy provides little value, at least with the simple technique we tried.",1,ad,True
130,"Our experiments show that dynamic thresholding using feedback has the potential to produce substantial and significant gains. While we have explored only one approach to this idea, through end-of-day relevance judgments, we can imagine more realistic interfaces, which might for example allow incremental judgments as tweets are pushed. In addition, we hope to incorporate social signals and other noncontent features into the relevance and novelty components of our system, with the goal of retaining our simple approach to thresholding, while improving overall effectiveness.",1,ad,True
131,"Acknowledgments. This work was supported in part by the U.S. National Science Foundation under awards IIS1218043 and CNS-1405688 and the Natural Sciences and Engineering Research Council of Canada (NSERC). Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors.",1,ad,True
132,7. REFERENCES,0,,False
133,"[1] J. Allan, R. Papka, and V. Lavrenko. On-line new event detection and tracking. SIGIR, 1998.",0,,False
134,"[2] J. G. Fiscus and G. R. Doddington. Topic detection and tracking evaluation overview. Topic Detection and Tracking. Kluwer, Norwell, MA, 2002.",1,Track,True
135,"[3] L. S. Larkey, F. Feng, M. Connell, and V. Lavrenko. Language-specific models in multilingual topic tracking. SIGIR, 2004.",0,,False
136,"[4] J. Lin, M. Efron, G. Sherman, Y. Wang, and E. M. Voorhees. Overview of the TREC-2015 Microblog track. TREC, 2015.",1,TREC,True
137,"[5] J. Lin, R. Snow, and W. Morgan. Smoothing techniques for adaptive online language models: Topic tracking in tweet streams. SIGKDD, 2011.",1,ad,True
138,"[6] L. Tan and C. L. A. Clarke. Succinct queries for linking and tracking news in social media. CIKM, 2014.",0,,False
139,"[7] L. Tan, A. Roegiest, and C. L. A. Clarke. University of Waterloo at TREC 2015 Microblog Track. TREC, 2015.",1,TREC,True
140,"[8] T. Tomokiyo and M. Hurst. A language model approach to keyphrase extraction. ACL Workshop on Multiword Expressions, 2003.",0,,False
141,"[9] Y. Wang, G. Sherman, J. Lin, and M. Efron. Assessor differences and user preferences in tweet timeline generation. SIGIR, 2015.",0,,False
142,"[10] J. Yamron, I. Carp, L. Gillick, S. Lowe, and P. Van Mulbregt. Topic tracking in a news stream. DARPA Broadcast News Workshop, 1999.",1,ad,True
143,"[11] Y. Yang, T. Ault, T. Pierce, and C. W. Lattimer. Improving text categorization methods for event tracking. SIGIR, 2000.",0,,False
144,"[12] X. Zhao and K. Tajima. Online retweet recommendation with item count limits. Web Intelligence and Intelligent Agent Technologies, 2014.",0,,False
145,1012,0,,False
146,,0,,False

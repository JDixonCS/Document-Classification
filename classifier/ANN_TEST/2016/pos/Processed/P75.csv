,sentence,label,data,regex
0,Engineering Quality and Reliability in Technology-Assisted Review,0,,False
1,Gordon V. Cormack,0,,False
2,University of Waterloo,0,,False
3,gvcormac@uwaterloo.ca,0,,False
4,Maura R. Grossman,0,,False
5,"Wachtell, Lipton, Rosen & Katz",0,,False
6,mrgrossman@wlrk.com,0,,False
7,ABSTRACT,0,,False
8,"The objective of technology-assisted review (""TAR"") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.",1,ad,True
9,Keywords: Technology-assisted review; predictive coding; electronic discovery; e-discovery; test collections; relevance feedback; continuous active learning; reliability; quality; systematic review.,0,,False
10,The views expressed herein are solely those of the author and should not be attributed to her firm or its clients.,0,,False
11,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '16 July 17-21, 2016, Pisa, Italy c 2016 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-4069-4/16/07. DOI: http://dx.doi.org/10.1145/2911451.2911510",1,ad,True
12,1. INTRODUCTION,1,DUC,True
13,"A vexing question that has plagued the use of technologyassisted review (""TAR"") is ""when to stop""; that is, knowing when as much relevant information as possible has been found, with reasonable effort. We present a provably reliable method to achieve high recall using any search strategy that repeatedly retrieves documents and receives relevance feedback, continuing indefinitely until a decision is made to discontinue the review process. Amenable search strategies include traditional ranked retrieval,1 interactive searching and judging [8], move-to-front pooling [8], and continuos active learning (""CAL"") [5].",1,ad,True
14,"For the particular implementation of CAL supplied as the baseline model implementation (""BMI"") [7] for the TREC 2015 Total Recall Track [13], we present two stopping procedures that achieve superior empirical reliability for comparable effort, and comparable empirical reliability for less effort, relative to our provably reliable method.",1,TREC,True
15,"Our primary motivation is to provide quality assurance for TAR applications, including electronic discovery (""eDiscovery"") in legal matters [5], systematic review in evidencebased medicine [10], and the creation of test collections for information retrieval (""IR"") evaluation [14]. Since these applications generally require that a human review each relevant document, we assume for this study that the effort to provide relevance feedback for relevant documents is a sunk cost. On the other hand, the effort to assess and provide relevance feedback for non-relevant documents is wasted. We measure review effort in terms of the total number of documents reviewed, whether relevant or not. An ideal search would find all of the relevant documents with effort equal to precisely that number. An acceptable search would find most of the relevant documents with minimal wasted effort.",0,,False
16,"A reliable search method would achieve an acceptable search most of the time. More formally, if S is a random variable representing a search, and acceptable(s) is an indicator function denoting whether a particular search s has an acceptable result, we define:",0,,False
17,"reliability ,def Pr[acceptable(S) , 1] .",0,,False
18,"To this end, we define recall(s) and effort(s) to be the recall and effort associated with s. For simplicity, our primary",0,,False
19,"1To be amenable, a retrieval method must be able to rank the entire collection. Incomplete rankings or set-based results may be extended by adding the remaining documents in any order.",1,ad,True
20,75,0,,False
21,Collection,0,,False
22,At Home At Home At Home Kaine MIMIC II RCV1-v2 Filtering Intersection Robust-04 Robust-05,1,RCV1,True
23,Source,0,,False
24,TREC 2015 Total Recall TREC 2015 Total Recall TREC 2015 Total Recall TREC 2015 Total Recall TREC 2015 Total Recall Reuters TREC 2012 Filtering TREC 2012 Filtering TREC 2004 Robust TREC 2005 Robust,1,TREC,True
25,Description,0,,False
26,"Jeb Bush public email Hacker forums Local news Tim Kaine non-public email MIMIC II Clinical Database News subject categories NIST topics Conjunction of RCV1-v2 subject pairs Amalgam of TREC ad-hoc topics 50 legacy topics, new dataset",1,RCV1,True
27,# Docs,0,,False
28,"290,000 465,147 902,434 401,953",0,,False
29,"31,538 804,414 804,414 804,414 528,256 1,033,461",0,,False
30,# Topics,0,,False
31,10 10 10 4 19 103 50 50 249 50,0,,False
32,# Rel (R),0,,False
33,"227-17,135 179-9,517 23-2,094 14,341-166,118 180-19,182 5-381,327",0,,False
34,12-610 21-349 4-161 17-376,0,,False
35,"Table 1: Ten Evaluation Datasets. In our experiments, the three At Home datasets are treated as a single test collection, for a total of eight test collections.",0,,False
36,results use a goal-post definition [18] of acceptability:,0,,False
37,"acceptable(s) , 1 (recall(s)  0.70) . 0 (recall(s) < 0.70)",0,,False
38,Our primary results further assume that 95% reliability is sufficiently high.,0,,False
39,The methods and results detailed in this work are:,0,,False
40,"· The target method : a provably reliable method that chooses ten random relevant documents as a target, and employs an independent search method to retrieve documents without knowledge of the target set, until each document in the target set has been retrieved. We prove that this method achieves 95% reliability for a minimum threshold recall of 70%.",0,,False
41,"· The knee method : a geometric stopping procedure, based on the shape of the gain (i.e., recall versus effort) curve, that augments BMI to achieve similar empirical reliability to the target method, with substantially less effort. The knee method, in contrast to the target method, is practical regardless of the number of relevant documents in the collection.",0,,False
42,"· The budget method: a variant of the knee method that achieves superior empirical reliability to the target method, with similar effort.",0,,False
43,· Empirical validation: we assess the effectiveness and reliability of our methods on eight archival test collections consisting of 555 topics and 4.5 million documents (see Table 1).,0,,False
44,"· Quality evaluation: As an alternative to binary relevance and fixed recall and reliability thresholds, we argue and provide evidence that quality loss functions [18] provide more nuanced measures that better reflect the tensions among consistency, effectiveness, and efficiency.",0,,False
45,2. BACKGROUND,0,,False
46,"The modern literature on the effectiveness and reliability of high-recall retrieval is largely confined to the problem of constructing test collections for IR evaluation, and eDiscovery in legal matters. A 1985 study by Blair and Maron [2] showed that teams of lawyers and paralegals, using iterative Boolean searches, believed they had achieved 75% recall, when in fact they had achieved 20%. Blair [3] later",1,ad,True
47,"described the difficulty of measuring high recall in general, and the use of targeted searching, systematically constructed Boolean queries, and stratified sampling to estimate recall for the Blair and Maron study.",0,,False
48,"The Text Retrieval Conference (""TREC"") [21] first addressed the problem of IR evaluation for ""large"" datasets, which at the time of TREC's inception in 1992, contained on the order of 500,000 documents. TREC follows the Cranfield paradigm [20], which evaluates the results of subject systems against a gold standard that identifies every relevant document. For large datasets, the effort to render a human assessment for each document is prohibitive, thus occasioning the use of automated or semi-automated methods to limit the human review effort required to label the dataset. TREC saw the introduction of the ""pooling method,"" which selects the top-ranked documents from a number of independent retrieval efforts for assessment, and deems all other documents to be non-relevant. A number of studies (see [19]) indicate that this method fails to identify a substantial number of documents, but even so, the resulting gold standard yields a stable evaluation of the relative effectiveness of candidate systems, as measured by Kendall's  rank correlation. We are unaware of any studies that address the effectiveness of pool-based gold standards for evaluating high-recall retrieval, or for simulating interactive relevance feedback. Studies suggest that greedy or machine-learning methods to select the pool yield a more nearly complete gold standard [8, 14].",1,TREC,True
49,"Interactive searching and judging (""ISJ""), in which a searcher repeatedly formulates queries and examines the top results from a relevance-ranking search engine, has been shown to yield gold standards with comparable quality to the pooling method, with considerably less effort [8]. Continuous active learning (""CAL"") [5] is essentially the same as ISJ, but uses machine learning instead of, or in addition to, manually formulated queries to rank the documents for review. An approach similar to CAL was used in the TREC 2012 Filtering Track (see [17]) to construct the gold standard that was used for evaluation, and also to simulate relevance feedback. A subsequent study based on pooling showed that the CAL-like approach had achieved high recall, and high effectiveness, as measured by Kendall's  [17]. CAL achieved superior results at the TREC 2009 Legal Track [4], and remains state of the art for eDiscovery.",1,ad,True
50,"The TREC 2015 Total Recall Track [13] represents the first study of high-recall human-in-the-loop retrieval in which all aspects of human intervention are simulated, and",1,TREC,True
51,76,0,,False
52,"hence controlled. Fully automated or semi-automated retrieval systems were tested through their interaction with an evaluation server. At the outset, the evaluation server provided a document collection and a topic description, after which the system under test submitted potentially relevant documents from the collection to the evaluation server. In response, the evaluation server provided an assessment (derived from a pre-computed gold standard) for the submitted documents, and the process continued until the documents were exhausted or the system chose to stop.",0,,False
53,"Participants in the Total Recall Track were supplied with a CAL baseline model implementation2 (""BMI"") that, when connected to the evaluation server, performed all aspects of the task--other than deciding when to stop--without human intervention. Participating systems were allowed to run indefinitely, and were evaluated (primarily) on the quality of the ranking determined by the order in which the system presented documents to the server. Instead of actually terminating when they thought an acceptable result had been achieved, participants were invited to ""call their shot"" by indicating, in real time, when they would have stopped, had they been required to balance benefit with cost. The current study considers the addition of a call-your-shot mechanism to BMI, and, more generally, to any ranking system.",1,Track,True
54,"The TREC 2015 Total Recall Track contributed five fully labeled archival datasets. The Jeb Bush, Hacker Forums, and Local News datasets were used for the At Home task, in which participants ran their systems on their own platforms, connecting via the internet to the evaluation server, which was run by the track coordinators. The Kaine and MIMIC II datasets were used for the Sandbox task, in which participants encapsulated their systems as a virtual machine, which was run by the track coordinators, along with the evaluation server, isolated from the internet.",1,TREC,True
55,"The reliability of methods for constructing gold standards for IR evaluation has typically been evaluated by how well the resulting gold standard ranks the relative effectiveness of precision-oriented retrieval systems, where the objective is to find as much relevant information as possible at low rank. For this purpose, a calibrated estimate is not required; it is sufficient to determine whether one system achieves higher recall than another, and the actual numerical value is ascribed little meaning. A number of studies (see [23]) eschew recall altogether, assuming that the user's information need will be satisfied by a tiny fraction of a vast sea of relevant documents. Zobel et al. [23] suggest that recall is a poor effectiveness measure, even for the ""high-recall applications"" where the user seeks ""total recall,"" and that only an extensive ad-hoc effort using multiple queries and tools will satisfy the user that their information need has been met.",1,ad-hoc,True
56,"The reliability and effectiveness of TAR (also known as ""predictive coding"") is the subject of much interest in the legal community [9, 16]. A number of approaches to TAR, to deciding when to stop, and to quality assurance have been advanced, but no stopping procedure has previously been shown to be mathematically or empirically reliable. Perhaps the most commonly used approach to TAR involves the use of a supervised machine-learning algorithm trained using a set of documents from the collection (typically referred to as a ""seed set"") to partition the collection into a ""review set,"" which is subject to human review, and a ""null set,"" which",1,ad,True
57,2See http://plg.uwaterloo.ca/~gvcormac/trecvm/.,1,trec,True
58,"is not. This approach is referred to as either simple passive learning (""SPL"") or simple active learning (""SAL""), depending on whether or not the learning algorithm is involved in selecting the training documents [5]. Recently, CAL has been advanced as a superior alternative [5, 7].",1,ad,True
59,"Regardless of the TAR method used, the question remains of when to stop. For SPL and SAL, two questions must be answered: when to stop training; and how many documents should be included in the review set. For CAL, the sole question is when to stop. One approach that has been advanced is to draw a random hold-out set (referred to as a ""control set"") that is used to measure the effectiveness of the classifier, in order to determine when to stop training, and then to measure recall, so as to determine how many documents should comprise the review set. The control set must be large enough to contain a sufficient number of relevant documents to yield a precise estimate. Bagdouri et al. [1] note that the use of a control set constitutes sequential sampling, with the net effect that it yields a biased estimate of recall, and cannot be used for quality assurance. As an alternative, they propose ""certified text classification,"" in which part of the review budget is set aside to conduct a frequentist acceptance test that will accept or reject the classifier. Bagdouri et al. are concerned with the problem of testing whether the classifier has achieved a threshold level of F1; they do not consider recall, or how to proceed in the event that the classifier is rejected by the test.",1,ad,True
60,"The limitations of binary relevance may be of particular importance in evaluating the effectiveness and reliability of TAR systems. Binary relevance does not account for the differential importance of relevant documents, and there will necessarily be documents near the threshold about which competent assessors will disagree (see [19]). In evaluating the recall of a system against a gold standard, there will necessarily be uncertainty for some documents as to whether the system is correct, the gold standard is correct, or reasonable minds could disagree. If a system fails to meet a target recall threshold, is it because the system has missed important documents, because it has missed marginal documents about which reasonable minds could differ, or because it has missed documents that are incorrectly coded relevant in the gold standard? And, is the effort to remedy the shortfall proportionate to the importance of the missed documents?",0,,False
61,"Binary relevance and fixed recall targets are examples of traditional goal-post methods in quality engineering ([18]), where success or failure is a binary quantity, and reliability is the probability of success. In quality engineering, a quadratic loss function blends reliability and effectiveness into a single quality measure, with targets, but no arbitrary thresholds [18].",1,ad,True
62,3. GUARANTEED RELIABILITY,0,,False
63,"Our target method involves drawing a target set T of k random relevant documents from the collection; for simplicity we fix k ,"" 10, but a different number could be chosen. In order to draw T , we retrieve and review documents selected at random until k relevant documents are found, or the collection is exhausted. The underlying search strategy retrieves documents for review without knowledge of T , until every document in T has been found. This method achieves 95% reliability, as shown below.""",0,,False
64,Consider a document collection C and a function rel(d) indicating binary relevance. The number of relevant docu-,0,,False
65,77,0,,False
66,"ments in the collection is R ,"" |{d  C|rel(d)}|. A search strategy is a ranking on C where 1  rank(d)  |C| denotes the position of d in the ranking. It is important to note that the following argument holds for any such ranking, provided it is independent of T .""",0,,False
67,The retrieved set of the target method is the shortest prefix P of the ranking that contains T :,0,,False
68,"P , {d|rank(d)  max rank(d )} .",0,,False
69,d T,0,,False
70,Now consider the ranking relrank(d) of only relevant documents:,0,,False
71,"relrank(d) , d  C|rel(d)  rank(d )  rank(d) .",0,,False
72,The last retrieved document dlast is necessarily in T :,0,,False
73,"dlast , arg max rank(d) , arg max relrank(d) .",0,,False
74,dT,0,,False
75,dT,0,,False
76,The recall of our method is:,0,,False
77,recall,0,,False
78,",",0,,False
79,relrank(dlast) R,0,,False
80,.,0,,False
81,"Taking T to be a random variable, the method is reliable if:",0,,False
82,R,0,,False
83,10,0,,False
84,Pr[,0,,False
85,relrank(dlast R,0,,False
86,),0,,False
87,0.7],0,,False
88,0.95 .,0,,False
89,"Assuming large R, consider the problem of determining a cutoff c such that:",0,,False
90,Pr[,0,,False
91,R,0,,False
92,-,0,,False
93,relrank(dlast) R,0,,False
94,>,0,,False
95,c],0,,False
96,",",0,,False
97,0.05,0,,False
98,(1),0,,False
99,"Pr[R - relrank(dlast) > cR] , 0.05",0,,False
100,(2),0,,False
101,"For the condition in Equation 2 to hold, it must be the case that the [numerically] top-ranked cR documents are absent from T , which occurs with probability",0,,False
102,1-,0,,False
103,10 R,0,,False
104,cR,0,,False
105,", 0.05 .",0,,False
106,"It follows that: For all R > 10,",0,,False
107,c,0,,False
108,",",0,,False
109,log 0.05,0,,False
110,R log,0,,False
111,1,0,,False
112,-,0,,False
113,10 R,0,,False
114,.,0,,False
115,c,0,,False
116,<,0,,False
117,lim,0,,False
118,R,0,,False
119,R,0,,False
120,log 0.05,0,,False
121,log,0,,False
122,1,0,,False
123,-,0,,False
124,10 R,0,,False
125,", 0.299573 < 0.3 .",0,,False
126,(3),0,,False
127,"Combining (1) and (3), we have:",0,,False
128,Pr[,0,,False
129,R,0,,False
130,-,0,,False
131,relrank(dlast) R,0,,False
132,>,0,,False
133,0.3],0,,False
134,<,0,,False
135,0.05,0,,False
136,R,0,,False
137,10,0,,False
138,Pr[,0,,False
139,relrank(dlast) R,0,,False
140,0.7],0,,False
141,0.95,0,,False
142,.,0,,False
143,Reliability is obtained at the cost of supplemental review,0,,False
144,"effort inversely proportional to R, the number of relevant",0,,False
145,documents. The number of randomly selected documents,0,,False
146,that,0,,False
147,need,0,,False
148,to,0,,False
149,be,0,,False
150,reviewed,0,,False
151,to,0,,False
152,find,0,,False
153,k,0,,False
154,relevant,0,,False
155,ones,0,,False
156,is,0,,False
157,k,0,,False
158,|C| R,0,,False
159,",",0,,False
160,on,0,,False
161,average for R,0,,False
162,|C |.,0,,False
163,For k,0,,False
164,", 10",0,,False
165,and prevalence,0,,False
166,R |C|,0,,False
167," 1%,",0,,False
168,"the target method entails a review overhead of about 1,000",1,ad,True
169,additional documents. Lower prevalence entails substan-,1,ad,True
170,"tially more overhead, while higher prevalence entails less.",1,ad,True
171,1,0,,False
172,1,0,,False
173,0.9,0,,False
174,0.9,0,,False
175,0.8,0,,False
176,0.8,0,,False
177,0.7,0,,False
178,0.7,0,,False
179,0.6,0,,False
180,0.6,0,,False
181,0.5,0,,False
182,0.5,0,,False
183,0.4,0,,False
184,0.4,0,,False
185,0.3,0,,False
186,0.3,0,,False
187,0.2,0,,False
188,0.2,0,,False
189,0.1,0,,False
190,0.1,0,,False
191,0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,0,,False
192,0 0 0.1 0.2 0.3,0,,False
193,(a),0,,False
194,Fig. 2: KnFeiegdulerea1lg: oKrintheme Dfoetrecotniloinne[1k5n].ee detection. (a) depict,1,trec,True
195,"4pro.etrapteeEndMd4ic5PuIldaRergIrdCeiesAsta.LnTcRheeEfmrLoamIgAnyBituI,dLeIxTofYwthiethsethbearsmcaoxrirmesupmonddisttoanthcee",0,,False
196,"vaOluuerskannede tmheethcoodrrreeslpieosnodnintghetharsessuhmopldtiovnaltuheast(CwAitLh,Sin, 1). The kn",0,,False
197,"aiinsccrsoairnmdkaipnnclgyemwaoitrfheu-ltnihkceetliyporrnoebloeavbfailntihttyed-rolacenunkmginetgnhtspsrobinefcfiotprhleee,lesssuisdc-lceiekseedloysf the triangl",0,,False
198,"rwelietvhantthedopcouminetnstsa.sAvseartciconesse.qHueonwcee, vtheer,gaasinwcuersvhe opwloti-n Section IV",0,,False
199,"twawdinnirthdaghiwnlreheenaicgraMfh-lzrl eeosvrnlmoeogrpsseeluiordsp(eicer.aaelooln.n,kscc""eeomilsnnyaeatriasagnsripulnuypmaoalreuoldplsxrrteeifomclueisvnbaiaoectnnettgi""soe)dnncoaecustru,armtvlilhtyaeentdcutooosruneehtvssaeefvxtone,,rotofwfloinrke",0,,False
200,dat we,0,,False
201,bfoeerntrheetrinevoeids.y online data sets typical of computing systems.,0,,False
202,poAinEntWicdoerMarel sAgpao.inndTcuihnrgevetowEtoWhueldMrahnAakveaastlpowppherioc1ha.0cauhllndtiuolcsauenmsinenfltetesccthhioanndiques simila,0,,False
203,"btoeenthroetsreieveemd,palondyesdlopbey0.B0 othlleirnegafeterr.BaAnndsact[u1a5l ]gaainnd Geometri",0,,False
204,"cisMantulargonpvodvreoraiibtnrtydahgpb.miiclSAiatultyvlphyeparraodtansigvewkeeitrnehggeaa,ustlsgrtfeaorhnoreidmisotrhembttmhraiefsesaevciaddtfloeoaromrsln,edctauhthnheoadedntoagwmeelneirmoetdihisateyoabtdtelgeiocoonttlldioosogny",1,ad,True
205,[16]. Th describe,0,,False
206,"abcyhieAvelb7r0e%chrteceatll aaln.din70%thepirrecwisioonrkatosnompearrtainakl rb,aarsriiesrs [3], whic",0,,False
207,"tbdyyepriecixvahleasfuosrftrimvoeomdmerpannruecavlalisorseiufivseierwsw[o1[11r9k],].oorTnahseMmsOliogpNhetE,bTuepa[ct1oh7iet]vh.eadEt WMA is a roannlkin(seloapleg<orr)i,twhomuldthbaet 0u.s7e, santdwtoheexslpopoenaefntetiratlhlyatwraenikghted movin",1,ad,True
208,"(asvloeprea>gre)sw. ouTlhdeapfiprrosatchE, WbutMnoAt,eqcuaalll0e.dFoarrsrm,alilsvauluseesd to smoot wotshfeoBeRcuaoldsineneddpx|CopuEnet|,cWotwdutMerahteewaAx""o,ps,uelwloradpierheenrixcrcvpeaheatwicortii""t,sshlkonv,""epoieenepssw-llpoosppue0eetbd< >.r0larri,ccaadksnad1tohaff.osoesrtthtsa,elalwarRervioevwbrae-algteimdeevs.iatTioh""",1,ad,True
209,"sferrovmed athrart,foarndR is 1a0n0,estima6t.0e (owfitthhesuivtaabrileanscmeooitnh-arrival times",0,,False
210,"irtFtnaehiglrring)aeeabwstlihallmiysoty,ealtdtthghhoooeadostf.decaWiontrmwedripfoco+aarrvtmeo4adreld·uofaaftevhhrsoeirrgaahvhrbyaerlpyerouc,ttasholwele,dtshhiasaictcohtthhiaeacvctorhientmihepgveprrseeeuedscttaebehlnylraetatssnhmhdet-haeximmauxmimwuam",1,ad,True
211,"oalmdsowuenret uonfivetrimsale; thtoat wthaeistamfoerthtrheeshonldexwtoupldoiwnotrktoforarrive. If th aqpuoweinindttleyavurasrreiidvetefyosrdaoafuttareseermtst,phiiirnsiccaltulhderivenasglhutaohtleidon,teo(nsretehthaTteabwtleehr1se)u.sbhseo-ld is reache 4wim.1itphoorNutatonistseeeaitAntrgbibatuthteeemnoeefnxttthiasrraivlgaol,riEthWmMisAthdaetclEarWesMaAkndeoee.sOnno edxiiIrsfetwsceatlnwyienrrfleeeptcootisrottnopwpoahitnettrhe1e<tmhiien<imksnusmeuecrhapntohkianst,tsiusc--h6,tiwhtaetowtnohlueyrldedetermines i aalmkonsetecehrtaasinblyeestnoppapsresmedat.uAreslyadureestuolcth, aEnWce.MMAoreiosvoern, ly applicabl",1,ad,True
212,teihnffiosratnnaa¨isovnealaifnupnpecrotsiaeocnthtionwfgot.uhled,0,,False
213,entail size of,0,,False
214,quadratic computational the collection. To avoid,1,ad,True
215,"both eventualities, we evaluate  only at values of s arising",0,,False
216,"forfombattchheebsaitschpersopofodrtIoiIcoIun.maleKtnotNslEosegEle|DCctL|e,EdabsAytLhBeGMvOIa.RlTuIehTseHonfMums baerer",0,,False
217,"sfeewpKaroanfteetedhdeblyceaaninsdeibxdpaatoseneedvnatoliuanellsythfionercrnseoawstiiinollgnbintethevarivtaabtlh.leeR, eeplvaoetininvetblsyy of maximum cchuarnvcae.tuArenyinreasidduaatlasesqeut--enttihale-teksntienegsb--iasarise oaffpspetrobxyima ately the se coofnFpsoerorevinaactthisvevinaclhuaoeiccouef rsov,fewthetrheeavsahtloaulardetefolor cata. lonmlyaoxniemia, cihfotshene ucsu- rve is rotate ingdeaggreeoemsetcrliocc""kknweies-deeatebcotiuont ""(xalmgoinri,tyhmmin[1)5t],hriloluusgtrhattehde line forme by the points (xmin, ymin) and (xmax, ymax). We choose this lin because we want to preserve the overall behavior of the dat",0,,False
218,"78 set--using a line of best fit, for example, risks cutting off th end points due to a higher concentration of points in the middl",0,,False
219,"of the curve. After rotating about this line, the local maxima--",0,,False
220,Collection,0,,False
221,At Home Kaine MIMIC II RCV1-v2 Filtering Intersection Robust-04 Robust-05,1,RCV1,True
222,Target Method,0,,False
223,Reliability Recall Effort,0,,False
224,1.00,0,,False
225,0.91,0,,False
226,"44,079",0,,False
227,1.00,0,,False
228,"0.92 119,644",0,,False
229,1.00,0,,False
230,0.89,0,,False
231,"14,440",0,,False
232,0.96,0,,False
233,0.88,0,,False
234,"83,412",0,,False
235,0.98,0,,False
236,"0.93 133,788",0,,False
237,1.00,0,,False
238,"0.92 174,415",0,,False
239,0.89,0,,False
240,"0.94 169,907",0,,False
241,1.00,0,,False
242,"0.92 155,405",0,,False
243,Knee Method,0,,False
244,Reliability Recall Effort,0,,False
245,0.93,0,,False
246,0.93,0,,False
247,"5,244",0,,False
248,1.00,0,,False
249,"0.98 172,774",0,,False
250,1.00,0,,False
251,"0.97 19,387",0,,False
252,0.99,0,,False
253,"0.94 60,645",0,,False
254,1.00,0,,False
255,0.99,0,,False
256,"3,857",0,,False
257,0.98,0,,False
258,"0.96 153,638",0,,False
259,1.00,0,,False
260,1.00,0,,False
261,"7,575",0,,False
262,1.00,0,,False
263,0.96,0,,False
264,"8,444",0,,False
265,Budget Method,0,,False
266,Reliability Recall Effort,0,,False
267,"0.97 0.97 43,896 1.00 0.98 172,774 1.00 0.97 19,418 0.99 0.94 70,601 1.00 1.00 143,798 1.00 0.99 190,671 1.00 1.00 162,673 1.00 1.00 134,719",0,,False
268,"Table 2: Reliability, Mean Recall, and Mean Effort for the Target, Knee, and Budget Methods.",0,,False
269,"in Figure 1. We draw a line from the origin to the recall achieved at rank s, and compute the maximum perpendicular distance from this line to the gain curve. Our candidate value of i is the projection to the x-axis of the intersection between the perpendicular and the gain curve. Our rationale in choosing this point was that it would correctly choose the inflection point for an ideal curve, and would avoid anomalies associated with points very close to the origin or to rank s, while capturing our intuitive notion of a genuine tipping point.",0,,False
270,We calculated the slope ratio as:,0,,False
271,|{d|rank(d)irel(d)}|,0,,False
272,",",0,,False
273,i 1+|{d|i<rank(d)s}|,0,,False
274,.,0,,False
275,s-i,0,,False
276,"Smoothing was accomplished by adding 1 to the number of relevant documents beyond the knee. This choice avoided the singularity of no relevant documents beyond the knee, and generally penalized situations in which the chosen inflection point was close to s. No smoothing was applied to the numerator, as we were not concerned with occasional underestimates.",1,ad,True
277,4.2 Adjustment for Low Prevalence,0,,False
278,The case of R 100 is more problematic. Any correction for small R faces a dual problem:,0,,False
279,"1. the stopping procedure has no knowledge of the value of R, other than what can be estimated through relevance feedback from retrieved documents; and,",0,,False
280,"2. even if it were known that R was small, the sparsity of relevant documents compromises the reliability of our slope-ratio calculation.",0,,False
281,"The knee method relies entirely on the slope-ratio test, adjusted to compensate for low R. Initial tuning on the training collections from the TREC 2015 Total Recall Track indicated that a fixed lower bound  on the rank at which to stop, might be effective. For our submission to the TREC 2015 Total Recall At Home task, we conducted a parameter sweep of six combinations of   {100, 1000} and   {3, 6, 10}. Our results showed that combinations involving  , 100 or  ,"" 3 were unreliable, and we eliminated them from further consideration. Unsurprisingly, the combination of  "","" 1000,  "","" 10 proved most reliable, achieving the recall target for 29 of 30 topics (reliability "", 0.97 [0.830 - 0.999 95% c.i.]).",1,ad,True
282,"We observed that recall and reliability appeared to be lower for smaller R, while effort (especially for  ,"" 10) appeared to be disproportionately higher for large R. This observation led us to seek more reliable methods for small R,""",0,,False
283,"and to choose  ,"" 6 for large R. To aid in this endeavor, we used a non-public dataset consisting of about 300,000 documents reviewed by attorneys and labeled according to 63 criteria, with R ranging from 5 to 164, 000 (median 431). Based on tuning experiments using this dataset, we calibrated the slope-ratio cutoff as a function of relret, the number of relevant documents retrieved at any given rank:""",0,,False
284," ,"" 156 - min(relret, 150) .""",0,,False
285,"In other words, we set the threshold for the slope ratio to be 150 when no relevant documents have been retrieved, 6 whenever at least 150 relevant documents have been retrieved, and use linear interpolation between these values.",0,,False
286,"We further observed that with this adjustment, the choice of  , 100 versus  ,"" 1000 became less critical. The lower value occasionally achieved lower effort than the higher value, and occasionally failed when the higher value did not. We chose to retain the value of  "", 1000 from our earlier experiments.",1,ad,True
287,4.3 Effort Adjustment,0,,False
288,A variant of our knee method--the budget method --,0,,False
289,adjusts for small R by stopping only when a review budget,1,ad,True
290,"comparable to that of the target method has been expended,",0,,False
291,and the slope-ratio test   6 is also satisfied. This ad-,1,ad,True
292,"justment substantially delays termination for small R, thus",0,,False
293,ensuring reliability.,0,,False
294,The approach is predicated on the hypothesis that the,0,,False
295,supplemental review effort entailed by the target method,0,,False
296,would be better spent reviewing more documents retrieved,0,,False
297,by CAL. The target method entails the supplemental review,0,,False
298,of,0,,False
299,about,0,,False
300,10|C| R,0,,False
301,documents,0,,False
302,in,0,,False
303,order,0,,False
304,to,0,,False
305,find,0,,False
306,10,0,,False
307,relevant,0,,False
308,ones.,0,,False
309,"According to the probability-ranking principle, we would ex-",0,,False
310,pect CAL to find more relevant documents than random,0,,False
311,"selection,",0,,False
312,for,0,,False
313,any,0,,False
314,level,0,,False
315,of,0,,False
316,"effort,",0,,False
317,up,0,,False
318,to,0,,False
319,and,0,,False
320,beyond,0,,False
321,10|C| R,0,,False
322,.,0,,False
323,While the supplemental documents retrieved by the target,0,,False
324,"method provide a statistical estimate of R, the documents",0,,False
325,"retrieved by CAL provide a lower bound for R, and therefore",0,,False
326,an upper bound for the expected effort entailed by the target,0,,False
327,"method. At the outset, this upper bound is loose, but as the",0,,False
328,"review progresses, it tightens. The budget method retrieves",0,,False
329,documents using CAL until review effort exceeds this upper,0,,False
330,"bound and   6.0, or until 0.75|C| documents are retrieved.",0,,False
331,"For small R, the budget determines the stopping point.",0,,False
332,"For large R, enough relevant documents will likely be discov-",0,,False
333,ered to bound the review budget to an insubstantial fraction,0,,False
334,"of R, and the slope-ratio test will determine when to stop.",0,,False
335,"In any event, the review stops at 0.75|C|. This final cutoff",0,,False
336,is predicated on the probability-ranking principle: random,0,,False
337,79,0,,False
338,TREC 2015 Total Recall Track -- Athome Tasks,1,TREC,True
339,1000000 1,0,,False
340,Effort (Documents Reviewed),0,,False
341,0.8 100000,0,,False
342,0.6,0,,False
343,Recall,0,,False
344,0.4 10000,0,,False
345,0.2,0,,False
346,0,0,,False
347,1000,0,,False
348,23 26 66 76 113 179 182 227 252 255 265 506 589 629 661 1111 1256 1624 2036 2094 2299 2375 2375 3635 4542 4805 5725 5836 9517 17135,0,,False
349,Number of Relevant Documents in Collection,0,,False
350,Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method,0,,False
351,TREC 2015 Total Recall Track -- Athome Tasks 1000000,1,TREC,True
352,1,0,,False
353,Effort (Documents Reviewed),0,,False
354,0.8 100000,0,,False
355,0.6,0,,False
356,Recall,0,,False
357,0.4 10000,0,,False
358,0.2,0,,False
359,0,0,,False
360,1000,0,,False
361,23 26 66 76 113 179 182 227 252 255 265 506 589 629 661 1111 1256 1624 2036 2094 2299 2375 2375 3635 4542 4805 5725 5836 9517 17135,0,,False
362,Number of Relevant Documents in Collection,0,,False
363,Effort for Budget Method Effort for Target Method Recall for Budget Method Recall for Target Method,0,,False
364,Figure 2: TREC 2015 Total Recall At Home Collection.,1,TREC,True
365,"selection of 75% of the collection would, with high probability, achieve 70% recall; the top-ranked 75% should achieve even higher recall.",0,,False
366,5. EXPERIMENTS,0,,False
367,"Testing the reliability of our stopping methods occasioned the use of ""fully assessed"" test collections, with a large number of topics and documents, where by ""fully assessed,"" we mean that the pooling method, ISJ, or a rule base was used, and the resulting documents were labeled by a human assessor. From the limited number of collections that met these criteria, we selected the TREC 2015 Total Recall Track collections, the Reuters RCV1-v2 news dataset, the TREC 2002 Filtering Track collections, and the TREC 2004 and 2005 Robust Track collections, as detailed in Table 1. We used our Total Recall At Home participation to conduct an initial parameter sweep with six combinations, as well as final testing; the other datasets were used solely for testing.",1,TREC,True
368,"The first phase of our experiments took place within the context of the TREC 2015 Total Recall Track, which had three distinct phases: training, At Home, and Sandbox. We conducted our initial development and tuning during the training phase, and submitted the knee method for evaluation in the At Home phase, but not the Sandbox phase. We captured the sequence of documents retrieved by BMI in both the At Home and Sandbox phases, and later used them",1,TREC,True
369,Effort (Documents Reviewed),0,,False
370,Recall,0,,False
371,TREC 2015 Total Recall Track -- Kaine Collection,1,TREC,True
372,1,0,,False
373,1000000,0,,False
374,0.8,0,,False
375,100000 0.6,0,,False
376,0.4 10000,0,,False
377,0.2,0,,False
378,0,0,,False
379,1000,0,,False
380,14341 20083 131698 166118,0,,False
381,Number of Relevant Documents in Collection,0,,False
382,Surplus Effort for Knee Method Surplus Effort for Target Method,0,,False
383,Recall for Knee Method Recall for Target Method,0,,False
384,TREC 2015 Total Recall Track -- ICD-9 Codes,1,TREC,True
385,1,0,,False
386,1000000,0,,False
387,0.8,0,,False
388,100000 0.6,0,,False
389,Effort (Documents Reviewed),0,,False
390,Recall,0,,False
391,0.4 10000,0,,False
392,0.2,0,,False
393,0,0,,False
394,1000,0,,False
395,179 2141 2575 3452 3851 5066 5140 5867 6113 6815 7806 8025 8678 8724 11081 11222 15046 16780 19095,0,,False
396,Number of Relevant Documents in Collection,0,,False
397,Surplus Effort for Knee Method Surplus Effort for Target Method,0,,False
398,Recall for Knee Method Recall for Target Method,0,,False
399,Figure 3: TREC 2015 Total Recall Sandbox Collections.,1,TREC,True
400,"to simulate the effect of the stopping methods whose results are presented here. After conducting further tuning on our non-public collection of 300,000 documents with 63 topics, we froze all parameters, and ran BMI on the other evaluation datasets, capturing the order in which the documents were retrieved. We then simulated our stopping methods by applying them to the ranking.",0,,False
401,"Summary results showing reliability, average recall, and average effort for all collections are shown in Table 2. The overall reliability of the target method, the knee method, and the budget method are substantially higher than the target of 0.95. Considering reliability, alone, there is little to choose among the methods; but the recall achieved by the knee and budget methods is substantially higher, while the effort expended by the knee method is, for some datasets, dramatically lower.",0,,False
402,"As illustrated in Figures 2 through 6, R (the number of relevant documents) appears to be the principal determinant of effort. For small R, effort for the target and budget methods approaches the size of the collection, while effort for the knee method, with one notable exception, generally diminishes with R, approaching the floor of  ,"" 1000 that we chose for this study. On the other hand, for large R, the effort for all methods appears proportional to R.""",0,,False
403,"The top panel of Figure 2 compares recall and effort for the knee and target methods, for each topic in the At Home col-",0,,False
404,80,0,,False
405,Effort (Documents Reviewed),0,,False
406,Recall,0,,False
407,Reuters RCV1-v2 Subject Categories,1,Reuters,True
408,1,0,,False
409,1000000,0,,False
410,0.8,0,,False
411,100000 0.6,0,,False
412,0.4 10000,0,,False
413,0.2,0,,False
414,0,0,,False
415,1000,0,,False
416,5 844 2107 2636 4835 7406 11878 21280 32153 47708 204820,0,,False
417,Number of Relevant Documents in Collection Surplus Effort for Knee Method,0,,False
418,Surplus Effort for Target Method Recall for Knee Method,0,,False
419,Recall for Target Method,0,,False
420,Figure 4: Reuters RCV1-v2 Subject Codes.,1,Reuters,True
421,"lection, ordered by R. We see that 28 of the 30 recall points for the knee method (shown by the green curve) fall above 0.7, indicating reliability of 0.93, while all of the points for the target method (shown by the red curve) fall above 0.7, indicating reliability of 1.00 for this collection. We also see that the most of the recall points for the knee method fall above those for the target method, indicating higher median recall, and the (signed) area between the curves is positive, indicating higher mean recall. Per-topic effort is shown as a bar graph on a logarithmic scale spanning three orders of magnitude. For small R, the knee method entails about 100 times less effort than the target method, while for large R, the effort is comparable.",0,,False
422,"The bottom panel of Figure 2 follows the same format, comparing the budget method (shown in blue) to the target method (shown in red). While the budget method achieves higher recall than the target method for nearly all topics, that superiority is not reflected in higher reliability. Effort for the two methods is very similar. The same observations apply to the results for the other collections: For low R, recall for the budget method exceeds that of the target method, while effort is indistinguishable; for large R, recall and effort are indistinguishable from the knee method. Both methods are reliable.",0,,False
423,"For brevity, we show graphical results comparing only the knee and target methods for the other collections. Tabular results for all methods are presented in Table 2.",0,,False
424,"Figure 3 shows results for the Sandbox task of the TREC 2015 Total Recall Track, which was notable in that participants had no prior access to the datasets or the topics, and their retrieval systems had to run fully autonomously. The top panel shows our results for the Kaine collection, which consisted of about 400,000 documents from Tim Kaine's eight-year tenure as Governor of Virginia. These documents had been previously reviewed and labeled by the archivist at the Library of Virginia according to four statutory categories: ""record"" (versus ""non-record""), ""open record,"" ""restricted record,"" and ""pertaining to the Virginia Tech shooting."" Two of the topics had moderately high R  104, and two had very high R  105. For all topics, the knee method achieved higher recall at the expense of somewhat higher effort. The bottom panel shows our results for the MIMIC II collection, which consisted of about 30,000 medical records",1,TREC,True
425,Recall 12 23 33 49 59 72 91 112 168 321 610,0,,False
426,Effort (Documents Reviewed),0,,False
427,TREC 2002 Filtering Track -- Assessor Topics,1,TREC,True
428,1000000 1,0,,False
429,0.8 100000,0,,False
430,0.6,0,,False
431,0.4 10000,0,,False
432,0.2,0,,False
433,0,0,,False
434,1000,0,,False
435,Number of Relevant Documents in Collection,0,,False
436,Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method,0,,False
437,TREC 2002 Filtering Track -- Intersection Topics 1000000,1,TREC,True
438,1,0,,False
439,0.8 100000,0,,False
440,0.6,0,,False
441,0.4 10000,0,,False
442,0.2,0,,False
443,0,0,,False
444,1000,0,,False
445,Number of Relevant Documents in Collection,0,,False
446,Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method,0,,False
447,Figure 5: TREC 2002 Filtering Track Collections.,1,TREC,True
448,Recall 21 28 39 45 50 59 78 89 137 218 349,0,,False
449,Effort (Documents Reviewed),0,,False
450,"collected from a hospital intensive care unit. The documents consisted of nurses' notes, radiology reports, and discharge summaries. The ""topics"" consisted of ICD-9 diagnostic codes extracted from non-textual database records. With one exception (R ,"" 179), all topics had moderately high R. The knee method generally achieved higher recall than the target method, at the expense of somewhat higher effort for most topics.""",1,ad,True
451,"Figure 4 shows the results for the RCV1-v2 dataset, using the subject categories and descriptions published with the dataset as topics [11]. Over a very wide range 101 R 105, we observe a familiar pattern: The knee method has somewhat higher recall and lower variance, with dramatically lower effort, for small R.",1,RCV1,True
452,"Figure 5 shows results for two sets of topics created for the TREC 2002 Filtering Track. The top panel shows results for topics that were created and assessed by NIST for the track. All topics had low R 610; the majority had very low R  100. For all topics, including those with the lowest R 100, the knee method achieved near-perfect recall. Recall for the target method showed much higher variance, suggesting that its reliability is actually lower. The knee method entails order(s) of magnitude less effort. The lower panel shows results for intersection topics, each of which was the conjunction of two RCV1-v2 subject categories. If rel1(d) and rel2(d) indicate relevance for two RCV1-v2 topics, rel1(d)  rel2(d) indicates relevance for the intersection",1,TREC,True
453,81,0,,False
454,TREC 2004 Robust Track,1,TREC,True
455,1,0,,False
456,1000000,0,,False
457,0.8,0,,False
458,100000 0.6,0,,False
459,Recall 6 9 11 14 17 19 22 27 28 33 35 39 46 50 57 66 71 80 94 113 130 161 194 254,0,,False
460,Effort (Documents Reviewed),0,,False
461,0.4 10000,0,,False
462,0.2,0,,False
463,0,0,,False
464,1000,0,,False
465,Number of Relevant Documents in Collection,0,,False
466,Surplus Effort for Knee Method Surplus Effort for Target Method,0,,False
467,Recall for Knee Method Recall for Target Method,0,,False
468,TREC 2005 Robust Track 1000000,1,TREC,True
469,1,0,,False
470,Recall 9 21 32 42 52 60 65 71 83 88 97 109 111 121 127 151 153 163 165 177 183 232 242 280 356,0,,False
471,Effort (Documents Reviewed),0,,False
472,0.8 100000,0,,False
473,0.6,0,,False
474,0.4 10000,0,,False
475,0.2,0,,False
476,0,0,,False
477,1000,0,,False
478,Number of Relevant Documents in Collection,0,,False
479,Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method,0,,False
480,Figure 6: TREC 2004-2005 Robust Track Collections.,1,TREC,True
481,"topic. The intersection topics were reported as a failed experiment [17], since no system achieved reasonable results on them. The results show that, while the effort to achieve high recall for these anomalous topics is inordinately large, our stopping methods are reliable.",0,,False
482,"Figure 6 shows results for the TREC 2004 and 2005 Robust tracks. In 2004, the Robust Track aggregated 150 topics developed for the TREC 6, TREC 7, and TREC 8 Ad-Hoc tasks, 50 topics developed for the 2003 Robust Track, and 49 new topics, for a total of 249 topics. For 2005, 50 of these topics--those deemed to be ""difficult""--were reprised with a new dataset. The top panel reports our results for 2004; the bottom for 2005. The results further confirm that the target and knee methods both achieve high reliability, while the knee method entails dramatically less effort.",1,TREC,True
483,6. DIMINISHING LOSS,0,,False
484,"As evidenced by the results above, reliability does not capture certain important aspects of effectiveness or efficiency. Moreover, empirical measurements of reliability lack statistical power, while parametric estimates depend on assumptions regarding the distribution of recall values. Since the choices of acceptable recall and acceptable reliability are both somewhat arbitrary, bias due to incorrect distributional assumptions may be of little consequence. We suggest that reporting the mean µ and standard deviation  of recall",0,,False
485,"conveys more useful information, if not a provably accurate estimate of reliability. Such an estimate would have to be compared to one or more tacit thresholds to determine the reliability of the method; for example, assuming normality, any pair of µ and  such that µ - 1.64  0.70 would be 95% reliable. More generally, the value of Q ,"" µ - 1.64 is a quantitative measure of quality, which may be used to determine the threshold level of acceptable recall for which 95% reliability may be obtained. Alternatively, by substituting the appropriate z-score in place of 1.64, a threshold of reliability different from 95% may be tested.""",0,,False
486,"We suggest that reliability and recall should be supplanted by quality estimates based on loss functions, of which recall and reliability are special cases. We define Q ,"" 1 - loss, where loss is the mean value of a loss function over all topics. If""",0,,False
487,"loss ,"" 1 - recall , Q "","" recall ; if,""",0,,False
488,"loss ,"" 0 (recall  0.7) , Q "", reliability . 1 (recall < 0.7)",0,,False
489,A quadratic loss function such as:,1,ad,True
490,"lossr , (1 - recall)2",0,,False
491,(4),0,,False
492,"captures the desirability of consistently high recall, subsuming the roles of µ and  in the previous discussion. Our aspirational goal is to achieve 100% recall. Any shortfall is penalized, and larger shortfalls are penalized more heavily.",0,,False
493,"Quadratic loss further generalizes to other aspects of quality, such as graded relevance, facet relevance [6], and efficiency. For example, let a1, a2, . . ., an be categories of relevance, and rela(d) be the indicator function for category a. Define:",1,ad,True
494,recalla,0,,False
495,",",0,,False
496,|{d,0,,False
497,C|relret(d)  rela(d)}| |{d  C|rela(d)}|,0,,False
498,"lossa , (1 - recalla)2",0,,False
499,n,0,,False
500,n,0,,False
501,"loss ,"" ilossai , where 1 "", i .",0,,False
502,(5),0,,False
503,"i,i",0,,False
504,"i,1",0,,False
505,The,0,,False
506,choice,0,,False
507,of,0,,False
508,weights,0,,False
509,i,0,,False
510,is,0,,False
511,not,0,,False
512,critical;,0,,False
513,the,0,,False
514,value,0,,False
515,i,0,,False
516,",",0,,False
517,1 n,0,,False
518,for,0,,False
519,"all i will often suffice, as it rewards consistent recall over",0,,False
520,"all categories, with the effect that documents belonging to",0,,False
521,rarer categories are afforded more influence.,0,,False
522,"Review effort may also be modeled as a category of loss,",0,,False
523,"thus quantifying the notion of ""reasonable effort."" For the",0,,False
524,"problem as we have framed it, an ideal method would entail",0,,False
525,"effort , R. From the presentation of results in the TREC",1,TREC,True
526,"2015 Total Recall Track Overview [13], we observe that a",1,Track,True
527,"reasonable effort might entail effort ,"" aR + b, where a  1""",0,,False
528,"represents effort proportional to sunk review cost, and b ",0,,False
529,0 represents fixed overhead. We suggest that a  2 and,1,ad,True
530,b  1000 represent reasonable effort to achieve recall  0.70,0,,False
531,with 95% reliability. The use of a quadratic loss replaces the,1,ad,True
532,a and b thresholds by a soft target:,0,,False
533,"losse ,",0,,False
534,b2 |C |,0,,False
535,ef f ort R+b,0,,False
536,2,0,,False
537,.,0,,False
538,(6),0,,False
539,"losse may be used to measure efficiency in its own right, or treated as a category loss in (5).",0,,False
540,82,0,,False
541,Collection,0,,False
542,At Home Kaine MIMIC II RCV1-v2 Filtering Intersection Robust-04 Robust-05,1,RCV1,True
543,Target Method,0,,False
544,lossr,0,,False
545,0.0132 0.0815 0.1229 0.1475 0.1011 0.1057 0.0870 0.1141,0,,False
546,losse 0.0090 0.0016,0,,False
547,0.0734 0.0883 0.2110 0.2499 0.4136 0.2368,0,,False
548,lossre 0.0111 0.0577 0.1012 0.1216 0.1654 0.1919 0.2989 0.1858,0,,False
549,Knee Method,0,,False
550,lossr 0.0197 0.0252,0,,False
551,0.0516 0.0947 0.0181 0.0818 0.0430 0.0570,0,,False
552,losse 0.0000 0.0025 0.0862 0.0154 0.0079 0.2740 0.0481 0.0265,0,,False
553,lossre 0.0099 0.0179 0.0710 0.0678 0.0140 0.2022 0.0456 0.0445,0,,False
554,Budget Method,0,,False
555,lossr 0.0056 0.0252 0.0516 0.0824 0.0015 0.0159 0.0025 0.0087,0,,False
556,losse,0,,False
557,0.0108 0.0025 0.0866 0.0795 0.2278 0.2947 0.3865 0.1843,0,,False
558,lossre,0,,False
559,0.0082,0,,False
560,0.0179 0.0712 0.0809 0.1611 0.2087 0.2733 0.1305,0,,False
561,"Table 3: Root Mean Loss for Relevance, Effort, and Combined.",0,,False
562,Target Method,0,,False
563,lossr,0,,False
564,lossh,0,,False
565,0.0837 0.0504,0,,False
566,Knee Method,0,,False
567,lossr,0,,False
568,lossh,0,,False
569,0.0134 0.0021,0,,False
570,Budget Method,0,,False
571,lossr 0.0007,0,,False
572,lossh 0.0011,0,,False
573,Table 4: Root Mean Loss for Relevance and High Relevance.,0,,False
574,"In Table 3, we report, for each collection, the root mean",0,,False
575,"loss ( loss) over all topics for relevance loss as defined in (4); effort loss as defined in (6); as well as their unweighted average, lossre ,"" 0.5 · lossr + 0.5 · losse. The results show conclusively the superiority of the budget method in terms of lossr. They show the general superiority of the knee method in terms of losse, while calling to our attention three collections where the target method is more efficient. On inspection, we see that two of the three collections have exclusively or nearly exclusively topics with high prevalence. We further see that that the the target method's narrow margin of superiority in terms of lossr is offset by a wide margin of inferiority in losse, as reflected in lossre. For the intersection collection, no system achieved acceptable losse.""",0,,False
576,"The bottom line is that the quality loss results support our qualitative observation that the knee method affords the best balance between consistently high recall and consistently low effort; the budget method provides consistently higher recall at the expense of disproportionate effort for topics with few relevant documents; the target method, while provably reliable, yields empirical results that are generally inferior to the knee and budget methods.",0,,False
577,"To illustrate the use of quality loss for graded relevance, we used a subset of 84 topics from Robust-04, for which relevance assessments were available for the categories ""highly",1,ad,True
578,"relevant"" and ""relevant."" Table 4 shows lossr and lossh for these categories, respectively. The knee and target methods have lower lossh, than lossr, indicating they retrieve highly relevant documents more consistently than merely relevant documents. The budget method shows the opposite effect, but even so, is markedly superior to the target and knee methods. While we cannot draw any firm conclusions from this small experiment, the results do not support the proposition that TAR methods achieve high recall by ""bulking up"" on marginal documents at the expense of important ones (cf. [12]).",0,,False
579,7. DISCUSSION,0,,False
580,"To our knowledge, the target method is the first provably reliable method for TAR. The commonly used frequentist acceptance test (see [1, 9]) offers a p-value or confidence level which is a measure of the reliability of the test, not",0,,False
581,"the reliability of the TAR method, not the probability that a given result is acceptable, and not the probability that a TAR method will pass the acceptance test. In eDiscovery, it is common to calculate a frequentist recall estimate, with a 5% margin of error and 95% confidence, and deem the result acceptable if the estimate exceeds 75%. Calculating such an estimate requires a sample of about 385 random relevant documents, entailing 38.5 times as much surplus effort as the target method.",0,,False
582,"Our proof of reliability does not require that the target sample T be chosen at the outset, as long as it is independent of the retrieval method. The target method could be used as an acceptance test, such that the consequence of failing the test would be to continue to retrieve documents without knowledge of T , until all the documents in T are retrieved.",0,,False
583,"Over test collections like the ones used in this study, there can be little doubt (p  0.00) that the knee and budget methods are reliable, that the budget method is more reliable than both the knee and target methods, and that the knee method is the most efficient. As with any empirical work, the test collections constitute convenience samples and ongoing research is necessary to characterize the scope of TAR tasks to which our results may be generalized.",0,,False
584,"The target method is reliable regardless of the underlying review method; however, if the underlying method uses a human in the loop to formulate queries or to influence the selection of documents in any way, that human must be isolated from any knowledge of T . The simplest approach to accomplish this goal might be to complete all such human intervention before drawing T , and to rely on fully automated document selection thereafter. An alternative would be to establish an ""information barrier"" between those who draw T and those who conduct the search.",0,,False
585,"This work establishes the reliability of the knee and budget methods as applied to BMI. It remains to be determined how well these approaches would work--possibly with different tuning parameters--for other CAL methods, including hybrid systems in which a human is afforded influence in the selection of documents for review. It is not obvious how to adapt the knee or budget method to SPL or SAL, for which an essential question is when to stop training.",1,ad,True
586,"The target method, by design, targets less than 100% recall. It could be modified to continue past the point at which",0,,False
587,83,0,,False
588,"the last document in T is retrieved, thereby expending additional effort to increase the probability of achieving 100% recall. One might, for example, extrapolate from the distribution of rank(d  T ). The knee method, on the other hand, does target 100% recall, and only incidentally optimizes reliability. It appears that loss functions better characterize the tension among consistency, effectiveness, and efficiency, as compared to goal-post methods. Regardless of which measure is chosen for evaluation, systems should be tuned to optimize their suitability for their intended purpose, not the measure itself (cf. [22]).",1,ad,True
589,8. CONCLUSIONS,0,,False
590,"Reservations about the effectiveness and reliability of TAR have impeded its adoption for eDiscovery and other highrecall retrieval tasks. A primary area of concern has centered on the issue of ""when to stop,"" or knowing with reasonable certainty--and being able to show an adversary or the court--that a particular TAR effort has identified an acceptable amount of relevant information. Many approaches to validation in common use today are simply invalid, or require disproportionate effort compared to the information they yield, and are often misunderstood and misapplied [9, 16].",1,ad,True
591,"We offer a method to determine when to stop that is guaranteed to be reliable, for the price of reviewing a number of random documents that is an order of magnitude less than acceptance tests that estimate recall, but neither determine when to stop nor guarantee reliability. We provide two other methods that entail no effort beyond that required by the underlying TAR method and, while not providing a guarantee of reliability, consistently demonstrate better reliability, and better recall, when evaluated on eight test collections, comprising 555 topics and 4.5M documents. Of particular interest is the knee method which, in contrast to the other methods, is demonstrated to be reliable and efficient when the collection contains few relevant documents.",0,,False
592,"While our primary results are demonstrated using measures derived from traditional goal-post methods--binary relevance, a recall threshold, and a reliability floor--we describe how loss functions may be formulated to capture the tension among consistency, degrees of relevance, facets of relevance, and efficiency. We apply these formulae to show insights into our results that might not have been readily apparent from the goal-post measures.",1,ad,True
593,9. REFERENCES,0,,False
594,"[1] M. Bagdouri, W. Webber, D. D. Lewis, and D. W. Oard. Towards minimizing the annotation cost of certified text classification. In SIGIR 2013.",0,,False
595,"[2] D. Blair and M. E. Maron. An evaluation of retrieval effectiveness for a full-text document-retrieval system. Commun. ACM, 28(3):289­299, 1985.",0,,False
596,"[3] D. C. Blair. Stairs redux: Thoughts on the stairs evaluation, ten years after. J. Am. Soc. Inf. Sci., 47(1):4­22, Jan. 1996.",0,,False
597,"[4] G. Cormack and M. Mojdeh. Machine learning for information retrieval: TREC 2009 Web, Relevance Feedback and Legal Tracks. In TREC 2009.",1,TREC,True
598,[5] G. V. Cormack and M. R. Grossman. Evaluation of machine-learning protocols for technology-assisted review in electronic discovery. In SIGIR 2014.,0,,False
599,[6] G. V. Cormack and M. R. Grossman. Multi-faceted recall of continuous active learning for technology-assisted review. In SIGIR 2015.,0,,False
600,"[7] G. V. Cormack and M. R. Grossman. Autonomy and reliability of continuous active learning for technology-assisted review. arXiv:1504.06868, 2015.",0,,False
601,"[8] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In SIGIR 1998.",0,,False
602,"[9] M. R. Grossman and G. V. Cormack. Comments on ""The implications of Rule 26(g) on the use of technology-assisted review"". Fed. Cts. L. Rev., 7:285­312, 2014.",0,,False
603,"[10] C. Lefebvre, E. Manheimer, and J. Glanville. Searching for studies. Cochrane Handbook for Systematic Reviews of Interventions, 2008.",0,,False
604,"[11] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: A new benchmark collection for text categorization research. J. Mach. Learn. Res., 5:361­397, 2004.",1,RCV1,True
605,"[12] D. Remus and F. S. Levy. Can robots be lawyers? Computers, lawyers, and the practice of law. http://dx.doi.org/10.2139/ssrn.2701092, 2015.",0,,False
606,"[13] A. Roegiest, G. V. Cormack, M. R. Grossman, and C. L. A. Clarke. Notebook Draft TREC 2015 Total Recall Track Overview. In TREC 2015.",1,TREC,True
607,[14] M. Sanderson and H. Joho. Forming test collections with no system pooling. In SIGIR 2004.,0,,False
608,"[15] V. Satop¨a¨a, J. Albrecht, D. Irwin, and B. Raghavan. Finding a ""kneedle"" in a haystack: Detecting knee points in system behavior. In ICDCSW 2011.",0,,False
609,"[16] K. Schieneman and T. Gricks. The implications of Rule 26(g) on the use of technology-assisted review. Fed. Cts. L. Rev., 7:239­274, 2013.",0,,False
610,[17] I. Soboroff and S. Robertson. Building a filtering test collection for TREC 2002. In SIGIR 2003.,1,TREC,True
611,[18] G. Taguchi. Introduction to Quality Engineering: Designing Quality Into Products and Processes. 1986.,0,,False
612,"[19] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5):697­716, 2000.",0,,False
613,"[20] E. M. Voorhees. The philosophy of information retrieval evaluation. In Evaluation of cross-language information retrieval systems, pages 143­170. Springer, 2002.",0,,False
614,"[21] E. M. Voorhees and D. K. Harman. The Text REtrieval Conference. In E. M. Voorhees and D. K. Harman, editors, TREC: Experiment and Evaluation in Information Retrieval, pages 3­19. MIT Press, 2005.",1,TREC,True
615,"[22] E. Yilmaz and S. Robertson. On the choice of effectiveness measures for learning to rank. Information Retrieval, 13(3):271­290, 2010.",0,,False
616,"[23] J. Zobel, A. Moffat, and L. A. Park. Against recall: Is it persistence, cardinality, density, coverage, or totality? In ACM SIGIR Forum, volume 43, pages 3­8. ACM, 2009.",0,,False
617,84,0,,False
618,,0,,False

,sentence,label,data,regex
0,Axiomatic Analysis for Improving the Log-Logistic Feedback Model,0,,False
1,"Ali Montazeralghaem, Hamed Zamani, and Azadeh Shakery",1,ad,True
2,"School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Iran",0,,False
3,"Center for Intelligent Information Retrieval, College of Information and Computer Sciences,",0,,False
4,"University of Massachusetts Amherst, MA 01003",0,,False
5,"{ali.montazer,shakery}@ut.ac.ir",0,,False
6,zamani@cs.umass.edu,0,,False
7,ABSTRACT,0,,False
8,"Pseudo-relevance feedback (PRF) has been proven to be an effective query expansion strategy to improve retrieval performance. Several PRF methods have so far been proposed for many retrieval models. Recent theoretical studies of PRF methods show that most of the PRF methods do not satisfy all necessary constraints. Among all, the log-logistic model has been shown to be an effective method that satisfies most of the PRF constraints. In this paper, we first introduce two new PRF constraints. We further analyze the log-logistic feedback model and show that it does not satisfy these two constraints as well as the previously proposed ""relevance effect"" constraint. We then modify the log-logistic formulation to satisfy all these constraints. Experiments on three TREC newswire and web collections demonstrate that the proposed modification significantly outperforms the original log-logistic model, in all collections.",1,TREC,True
9,CCS Concepts,0,,False
10,·Information systems  Query representation; Query reformulation;,1,Query,True
11,Keywords,0,,False
12,Pseudo-relevance feedback; axiomatic analysis; theoretical analysis; query expansion; semantic similarity,0,,False
13,1. INTRODUCTION,1,DUC,True
14,"Pseudo-relevance feedback (PRF) refers to a query expansion strategy to address the vocabulary mismatch problem in information retrieval (IR). PRF assumes that a number of top-retrieved documents are relevant to the initial query. Based on this assumption, it updates the query model using these pseudo-relevant documents to improve the retrieval performance. PRF has been shown to be highly effective in many retrieval models [1, 5, 8, 10].",1,ad,True
15,Several PRF models with different assumptions and formulations have so far been proposed. Clinchant and Gaussi-,0,,False
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.",1,ad,True
17,"SIGIR '16, July 17-21, 2016, Pisa, Italy",0,,False
18,c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914768,0,,False
19,"er [2] theoretically analyzed a number of effective PRF models. To this end, they proposed five constraints (axioms) for PRF models and showed that the log-logistic feedback model [1] is the only PRF model (among the studied ones) that satisfies all the constraints. They also showed that its performance is superior to the other PRF methods, including the mixture model [10] and the geometric relevance model [9]. Effectiveness of the log-logistic model motivates us, in this paper, to study this state-of-the-art PRF model.",0,,False
20,"Recently, Pal et al. [8] proposed a sixth constraint for PRF models to improve the PRF performance in the divergence from randomness framework. This constraint, which is called ""relevance effect"", indicates that the terms in the feedback documents with high relevance scores (i.e., relevance of document to the initial query) should have higher weights in the feedback model compared to those with exactly similar statistics, but appear in the documents with lower relevance scores. Formally writing, if a term w occurs in two documents d1, d2  F (F denotes the set of feedback documents) such that d1 is more relevant to the initial query than d2. Then, we can say that the feedback weight of w given the F - {d1} feedback documents is lower than the weight of the same word in the F - {d2} feedback documents [8]. It can be shown that the log-logistic feedback model does not satisfy the relevance effect constraint.",0,,False
21,"In this paper, we propose two additional constraints for PRF models. The first constraint considers the semantic similarity of feedback terms to the initial query. Although previous work, such as [4], proposed similar constraints for retrieval models, to the best of our knowledge, it is the first time to study a semantic-related constraint for the PRF task. The second constraint indicates that the weight of each term w in the feedback model not only depends on the distribution of w in the feedback documents, but is also related to the distribution of the other terms in those documents. We further show that the log-logistic model does not satisfy the two proposed constraints. We then propose a modification to the log-logistic feedback model to satisfy the proposed constraints as well as the relevance effect constraint [8].",1,ad,True
22,"We evaluate the modified log-logistic model using three standard TREC collections: AP (Associated Press 1988-89), Robust (TREC 2004 Robust track), and WT10g (TREC 910 Web track). The experimental results demonstrate that the proposed method significantly outperforms the original log-logistic feedback model in all collections. The proposed method is also shown to be more robust than the original log-logistic model, especially in the web collection.",1,TREC,True
23,765,0,,False
24,2. METHODOLOGY,0,,False
25,"In this section, we introduce two constraints that (pseudo) relevance feedback methods should satisfy (in addition to those proposed in [2, 8]). We further analyze the log-logistic model, a state-of-the-art feedback model, and figure out that this model does not satisfy the proposed constraints as well as the ""relevance effect"" constraint introduced in [8]. Based on these observations, we modify the log-logistic feedback model to satisfy all the constraints.",1,ad,True
26,"We first introduce our notation. Let F W (w; F, Pw, Q) be the feedback weight function that assigns a real-value weight to each feedback term w for a given query Q. F and Pw respectively denote the set of feedback documents for the query Q and a set of term-dependent parameters. For simplicity, we henceforth use F W (w). In the following equations, T F and IDF denote term frequency and inverse document frequency, respectively. The notation | · | is also used for query/document length or size of a given set.",0,,False
27,2.1 Constraints,0,,False
28,"In this subsection, we introduce two constraints for feedback models.",0,,False
29,"[Semantic effect] Let Q be a single-term query (i.e., Q ,"" {q}), w1 and w2 be two terms such that IDF (w1) "","" IDF (w2), D  F : T F (w1, D) "","" T F (w2, D), and""",0,,False
30,"sem(q, w1) < sem(q, w2)",0,,False
31,"where sem(·, ·) denotes the semantic similarity of the given terms. Then, we can say:",0,,False
32,F W (w1) < F W (w2),0,,False
33,The intuition behind this constraint is that the feedback terms should be semantically similar to the initial query.,0,,False
34,"[Distribution effect] Let w1 and w2 be two vocabulary terms such that T F (w1, D1) ,"" T F (w2, D2), T F (w1, D2) "","" T F (w2, D1) "","" 0, and |D1| "","" |D2|, where D1 and D2 are two documents in the feedback set F . Also, assume that w1 and w2 do not occur in other feedback documents, and""",0,,False
35,U niqueT erms(D1) < U niqueT erms(D2),0,,False
36,"where U niqueT erms(·) denotes the number of unique terms in the given document. Then, we can say:1",0,,False
37,F W (w1) < F W (w2),0,,False
38,"In other words, this constraint implies that for computing the feedback weight of a term w, the distribution of other terms in the feedback documents should also be considered.",0,,False
39,2.2 Modifying the Log-Logistic Model,0,,False
40,The feedback weight of each term w in the log-logistic feedback model [1] is computed as follows:,0,,False
41,F W (w),0,,False
42,",",0,,False
43,1 |F |,0,,False
44,DF,0,,False
45,"F W (w, D)",0,,False
46,",",0,,False
47,1 |F |,0,,False
48,DF,0,,False
49,log(,0,,False
50,"t(w,",0,,False
51,D) w,0,,False
52,+,0,,False
53,w,0,,False
54,),0,,False
55,(1),0,,False
56,where w,0,,False
57,",",0,,False
58,Nw N,0,,False
59,(Nw,0,,False
60,and N,0,,False
61,denote the number of docu-,0,,False
62,ments in the collection that contain w and the total number,0,,False
63,"of documents in the collection, respectively), and t(w, D) ,",0,,False
64,"T F (w,",0,,False
65,D),0,,False
66,log(1,0,,False
67,+,0,,False
68,c,0,,False
69,avgl |D|,0,,False
70,),0,,False
71,(avgl,0,,False
72,denotes,0,,False
73,the,0,,False
74,average,0,,False
75,document,0,,False
76,length and c is a free hyper-parameter). It is shown that,0,,False
77,1The intuition behind this constraint comes from the definition of information in information theory literature.,0,,False
78,"the log-logistic model satisfies all the PRF constraints introduced in [2]. It can be easily proved that this model cannot satisfy the constraints proposed in this paper. In more detail, there is no semantic-related or relevance-related components in the log-logistic formulation and thus it cannot satisfy the proposed ""semantic effect"" and the ""relevance effect"" [8] constraints. In addition, the log-logistic formula does not consider the distribution of other terms in computing the weight of each term w, and thus it does not satisfy the ""distribution effect"" constraint.",1,ad,True
79,"To satisfy the ""semantic effect"" constraint, we modify the log-logistic feedback weight function as follows:",0,,False
80,F Wsem(w),0,,False
81,",",0,,False
82,F W (w),0,,False
83,1 |Q|,0,,False
84,"s(w, q) s(q, q)",0,,False
85,(2),0,,False
86,qQ,0,,False
87,"where s(·, ·) denotes the semantic similarity between the given two terms. The parameter  controls the effect of semantic similarity in the feedback weight function. The semantic weighting component comes from the query-growth function, which was previously proposed by Fang and Zhai [4]. Note that in Equation (2), we can ignore the 1/|Q| term and the  parameter, since they are equal for all terms and the feedback weighting function will be normalized. Several methods have so far been proposed to incorporate semantic similarity of terms in various retrieval tasks. In this paper, we consider the mutual information as a basic semantic similarity metric to compute s(·, ·). The mutual information (MI) of two terms w and w is computed as follows:",1,corpora,True
88,"I (Xw ,",0,,False
89,Xw ),0,,False
90,",",0,,False
91,"Xw ,Xw {0,1}",0,,False
92,"p(Xw ,",0,,False
93,Xw ),0,,False
94,log,0,,False
95,"p(Xw, Xw ) p(Xw)p(Xw )",0,,False
96,"where Xw and Xw are two binary random variables that represent the presence or absence of the terms w and w in each document. A simple way to compute the mutual information is to consider the whole collection; but, this choice may not be ideal for ambiguous terms. Another way is to compute the mutual information from the pseudo-relevant documents. However, the top-retrieved documents could be a biased corpus for this goal. Therefore, similar to [4], we extract the mutual information from a corpus containing the top m retrieved documents and r × m documents randomly selected from the collection, where r is a free parameter that controls the generality of mutual information scores.",0,,False
97,"To satisfy the ""distribution effect"" constraint, we re-define the function t(w, D) as follows:",0,,False
98,"t(w, D)",0,,False
99,",",0,,False
100,"t(w, D)",0,,False
101,log(,0,,False
102,|D| ut(D),0,,False
103,),0,,False
104,(3),0,,False
105,where ut(D) denotes the number of unique terms in the document D. A similar approach for modifying the raw TF formula was previously used in [7].,0,,False
106,"To satisfy the ""relevance effect"" constraint, we re-define the function F W (w, D) (see Equation (1)) as follows:",0,,False
107,"F W (w, D) ,"" F W (w, D)  RS(Q, D)""",0,,False
108,(4),0,,False
109,"where RS(Q, D) denotes the relevance score of the document D to the query Q. This function can be computed using the relevance score of D in the first ranking phase in PRF. A similar idea was previously proposed by Lavrenko and Croft [5]. They used the query likelihood similarity as a posterior probability in the relevance models. Lv and",0,,False
110,766,0,,False
111,ID AP Robust,1,AP,True
112,WT10g,1,WT,True
113,Collection,0,,False
114,Associated Press 88-89 TREC Disks 4 & 5 minus,1,TREC,True
115,Congressional Record,0,,False
116,TREC Web Collection,1,TREC,True
117,Table 1: Collections statistics.,0,,False
118,"Queries (title only) TREC 1-3 Ad Hoc Track, topics 51-200",1,TREC,True
119,"TREC 2004 Robust Track, topics 301-450 & 601-700 TREC 9-10 Web Track, topics 451-550",1,TREC,True
120,#docs 165k 528k,0,,False
121,1692k,0,,False
122,doc length 287 254,0,,False
123,399,0,,False
124,"#qrels 15,838 17,412",0,,False
125,5931,0,,False
126,Table 2: Performance of the proposed modifications and the baselines. Superscripts 0/1 denote that the MAP improvements over NoPRF/LL are statistically significant. The highest value in each column is marked in bold.,1,MAP,True
127,Method,0,,False
128,AP MAP P@10 RI,1,AP,True
129,Robust MAP P@10 RI,1,Robust,True
130,WT10g MAP P@10 RI,1,WT,True
131,NoPRF LL,0,,False
132,LL+Sem LL+Rel LL+Dis,0,,False
133,0.2642,0,,False
134,0.3385,0,,False
135,0.34220 0.34250 0.33860,0,,False
136,0.4260 0.4622,0,,False
137,0.4702 0.4681 0.4671,0,,False
138,­ 0.15,0,,False
139,0.18 0.20 0.16,0,,False
140,0.2490,0,,False
141,0.2829,0,,False
142,0.294001 0.289701 0.28310,0,,False
143,0.4237 ­ 0.4393 0.33,0,,False
144,0.4474 0.31 0.4490 0.35 0.4401 0.32,0,,False
145,0.2080 0.2127,0,,False
146,0.2247 0.228901 0.2194,0,,False
147,0.3030 ­ 0.3187 0.08,0,,False
148,0.3188 0.10 0.3289 0.17 0.3207 0.13,0,,False
149,LL+All 0.344501 0.4722 0.20 0.297901 0.4486 0.36 0.230001 0.3177 0.19,0,,False
150,Zhai [6] also used a similar technique to improve the divergence minimization feedback model [10].,0,,False
151,"Considering the aforementioned modifications, we can rewrite the log-logistic feedback weighting formula as follows:",0,,False
152,F W (w),0,,False
153,",",0,,False
154,1 |F |,0,,False
155,DF,0,,False
156,log(,0,,False
157,"t(w,",0,,False
158,D) w,0,,False
159,+,0,,False
160,w,0,,False
161,),0,,False
162,RS,0,,False
163,"(Q,",0,,False
164,D),0,,False
165,"s(w, q) s(q, q)",0,,False
166,(5),0,,False
167,qQ,0,,False
168,3. EXPERIMENTS,0,,False
169,3.1 Experimental Setup,0,,False
170,"We used three standard TREC collections in our experiments: AP (Associated Press 1988-89), Robust (TREC Robust Track 2004 collection), and WT10g (TREC Web Track 2001-2002). The first two collections are newswire collections, and the third collection is a web collection with more noisy documents. The statistics of these datasets are reported in Table 1. We consider the title of topics as queries. All documents are stemmed using the Porter stemmer. Stopwords are removed in all the experiments. We used the standard INQUERY stopword list. All experiments were carried out using the Lemur toolkit2.",1,TREC,True
171,3.1.1 Parameter Setting,0,,False
172,"The number of feedback documents, the number of feedback terms, the feedback coefficient and the parameter that controls the generally of mutual information scores (parameter r) are set using 2-fold cross validation over each collection. We sweep the number of feedback documents and feedback terms between {10, 25, 50, 75, 100}, the feedback coefficient between {0, 0.1, · · · , 1}, and the parameter r between {2, 4, 6, 8, 10}.",0,,False
173,3.1.2 Evaluation Metrics,0,,False
174,"To evaluate retrieval effectiveness, we use mean average precision (MAP) of the top-ranked 1000 documents as the",1,MAP,True
175,2http://lemurproject.org/,0,,False
176,"main evaluation metric. In addition, we also report the pre-",1,ad,True
177,cision of the top 10 retrieved documents (P@10). Statisti-,0,,False
178,cally significant differences of performance are determined,0,,False
179,using the two-tailed paired t-test computed at a 95% confi-,0,,False
180,dence level over average precision per query.,0,,False
181,"To evaluate the robustness of methods, we consider the ro-",0,,False
182,bustness,0,,False
183,index,0,,False
184,(RI),0,,False
185,[3],0,,False
186,which,0,,False
187,is,0,,False
188,defined,0,,False
189,as,0,,False
190,N+ -N- N,0,,False
191,",",0,,False
192,where,0,,False
193,N,0,,False
194,denotes the number of queries and N+/N- shows the num-,0,,False
195,ber of queries improved/decreased by the feedback method.3,0,,False
196,"The RI value is always in the [-1, 1] interval and the method",0,,False
197,with higher value is more robust.,0,,False
198,3.2 Results and Discussion,0,,False
199,"In this subsection, we first evaluate the proposed modifications to the log-logistic model. We further study the sensitivity of the proposed method to the free parameters.",0,,False
200,3.2.1 Evaluating the Modified Log-Logistic Model,0,,False
201,"We consider two baselines: (1) the document retrieval method without feedback (NoPRF), and (2) the original loglogistic feedback model (LL). Although several other PRF methods have already been proposed, since in this paper, we propose a modification of the log-logistic model, we do not compare the proposed method with other existing PRF models.",1,ad,True
202,"To study the effect of each constraint in the retrieval performance, we modify the log-logistic model based on each constraint, separately. LL+Sem, LL+Rel, and LL+Dis denote the modified log-logistic model based on the ""semantic effect"", the ""relevance effect"", and the ""distribution effect"" constraints, respectively. We also modify the log-logistic model by considering all of these constraints (called LL+All) as introduced in Equation (5). The results obtained by the baselines and those achieved by the proposed modifications are reported in Table 2. According to this table, LL outperforms the NoPRF baseline in all cases, which shows the effectiveness of the log-logistic model. The improvements on the WT10g collection is lower than those on the AP and the Robust collections. This observation demonstrates that the",1,WT,True
203,"3To avoid the influence of very small average precision changes in the RI value, we only consider the improvements/losses higher than 10% (relatively).",0,,False
204,767,0,,False
205,0.32  Robust WT10g,1,Robust,True
206,0.32  Robust WT10g,1,Robust,True
207,0.30,0,,False
208,0.30 ,0,,False
209,0.28,0,,False
210,0.28,0,,False
211,MAP MAP,1,MAP,True
212,0.26,0,,False
213,0.26,0,,False
214,0.24,0,,False
215,0.24,0,,False
216,0.22,0,,False
217,0.22,0,,False
218,25 50 75 100 125 150 175 200,0,,False
219,2,0,,False
220,4,0,,False
221,6,0,,False
222,8,0,,False
223,# of feedback terms,0,,False
224,r,0,,False
225,Figure 1: Sensitivity of the proposed method to the number of feedback terms and the parameter r.,0,,False
226,log-logistic model is less effective and robust in improving,0,,False
227,"the retrieval performance in the web collection, compared",0,,False
228,to the newswire collections. LL+Sem and LL+Rel perform,0,,False
229,"better than LL in terms of MAP and P@10, in all collec-",1,MAP,True
230,tions. The MAP improvements are statistically significant,1,MAP,True
231,"in many cases, especially in the LL+Rel method. Except",0,,False
232,"in one case (i.e., LL+Sem in Robust), both LL+Sem and",1,Robust,True
233,LL+Rel models are shown to be more robust than the LL,0,,False
234,baseline. It is worth noting that we use very simple modifi-,0,,False
235,"cations to satisfy these two constraints, and thus using more",0,,False
236,accurate methods to satisfy these constraints can potentially,0,,False
237,improve the performance. LL+Dis method in general per-,0,,False
238,forms comparable to or sometimes slightly better than the,0,,False
239,LL baseline. The results achieved on the WT10g collection,1,WT,True
240,"shows that LL+Dis can be more effective in noisy conditions,",0,,False
241,"such as web collections. Overall, although the theoretical",0,,False
242,"analysis shows that PRF methods should satisfy the ""distri-",0,,False
243,"bution effect"" constraint, it does not substantially affect the",0,,False
244,retrieval performance in the AP and the Robust collections.,1,AP,True
245,The,0,,False
246,reason,0,,False
247,is,0,,False
248,that,0,,False
249,the,0,,False
250,values,0,,False
251,of,0,,False
252,|D| ut(D),0,,False
253,(see,0,,False
254,Equation,0,,False
255,(3)),0,,False
256,are,0,,False
257,"very close to each other for different documents, especially",0,,False
258,"in newswire collections. Thus, our modification to the log-",0,,False
259,"logistic regarding the ""distribution effect"" constraint cannot",0,,False
260,substantially affect the retrieval performance.,0,,False
261,"As shown in Table 2, the LL+All method, which is our fi-",0,,False
262,"nal modification to the log-logistic model, outperforms both",0,,False
263,baselines in all collections in terms of MAP and P@10. The,1,MAP,True
264,MAP improvements are always statistically significant. The,1,MAP,True
265,LL+All method is also shown to be more robust than the,0,,False
266,"LL method, in particular in the WT10g collection.",1,WT,True
267,3.2.2 Parameter Sensitivity,0,,False
268,"In this set of experiments, we fix one of the parameters r (the generality control parameter for mutual information) and n (the number of feedback terms), and then sweep the other one to show the sensitivity of the method to the input parameters. The results are reported in Figure 1.4 According to this figure, the method is quite stable w.r.t. the changes in the values of these two parameters, especially for the parameter r. The results also indicate that by increasing the number of feedback terms, performance in the Robust collection generally increases, but in the WT10g collection it is not the case. The reason could be related to the noisy nature of this collection compared to the newswire collections.",1,Robust,True
269,"4For the sake of visualization, we only report the results for the Robust and the WT10g collections. The behaviour of the method in AP is similar to the Robust collection.",1,Robust,True
270,4. CONCLUSIONS AND FUTURE WORK,0,,False
271,"In this paper, we proposed two new constraints for pseudorelevance feedback models. The first constraint considers semantic similarity of the feedback terms to the initial query. The second constraint focuses on the effect of distribution of all terms in the feedback documents on each term. We further studied the log-logistic model, a state-of-the-art feedback model, and showed that this model does not satisfy the proposed constraints as well as the previously proposed ""relevance effect"" constraint [8]. We then modified the loglogistic model to satisfy all of these constraints. The proposed modification was evaluated using three TREC newswire and web collections. Experimental results suggest that the modified model significantly outperforms the original log-logistic model, in all collections.",1,TREC,True
272,"An interesting future direction is to study other feedback methods, such as the language model-based feedback methods, and modify them in order to satisfy the constraints. In this paper, we only consider simple approaches to satisfy the constraints, such as using mutual information for capturing semantic similarities. Future work can focus on more complex and accurate approaches to improve the retrieval performance.",0,,False
273,5. ACKNOWLEDGEMENTS,0,,False
274,"This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",0,,False
275,6. REFERENCES,0,,False
276,"[1] S. Clinchant and E. Gaussier. Information-based Models for Ad Hoc IR. In SIGIR '10, pages 234­241, 2010.",0,,False
277,"[2] S. Clinchant and E. Gaussier. A Theoretical Analysis of Pseudo-Relevance Feedback Models. In ICTIR '13, pages 6­13, 2013.",0,,False
278,"[3] K. Collins-Thompson. Reducing the Risk of Query Expansion via Robust Constrained Optimization. In CIKM '09, pages 837­846, 2009.",1,Query,True
279,"[4] H. Fang and C. Zhai. Semantic Term Matching in Axiomatic Approaches to Information Retrieval. In SIGIR '06, pages 115­122, 2006.",0,,False
280,"[5] V. Lavrenko and W. B. Croft. Relevance Based Language Models. In SIGIR '01, pages 120­127, 2001.",0,,False
281,"[6] Y. Lv and C. Zhai. Revisiting the Divergence Minimization Feedback Model. In CIKM '14, pages 1863­1866, 2014.",0,,False
282,"[7] J. H. Paik. A Novel TF-IDF Weighting Scheme for Effective Ranking. In SIGIR '13, pages 343­352, 2013.",0,,False
283,"[8] D. Pal, M. Mitra, and S. Bhattacharya. Improving Pseudo Relevance Feedback in the Divergence from Randomness Model. In ICTIR '15, pages 325­328, 2015.",0,,False
284,"[9] J. Seo and W. B. Croft. Geometric Representations for Multiple Documents. In SIGIR '10, pages 251­258, 2010.",0,,False
285,"[10] C. Zhai and J. Lafferty. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In CIKM '01, pages 403­410, 2001.",0,,False
286,768,0,,False
287,,0,,False

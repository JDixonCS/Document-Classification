,sentence,label,data,regex
0,Balancing Relevance Criteria through Multi-Objective Optimization,0,,False
1,Joost van Doorn1,0,,False
2,Daan Odijk1,0,,False
3,joost.vandoorn@student.uva.nl d.odijk@uva.nl,0,,False
4,"Diederik M. Roijers1,2",0,,False
5,Maarten de Rijke1,0,,False
6,diederik.roijers@cs.ox.ac.uk derijke@uva.nl,0,,False
7,"1Informatics Institute, University of Amsterdam, Amsterdam, The Netherlands 2Department of Computer Science, University of Oxford, Oxford, United Kingdom",0,,False
8,ABSTRACT,0,,False
9,"Offline evaluation of information retrieval systems typically focuses on a single effectiveness measure that models the utility for a typical user. Such a measure usually combines a behavior-based rank discount with a notion of document utility that captures the single relevance criterion of topicality. However, for individual users relevance criteria such as credibility, reputability or readability can strongly impact the utility. Also, for different information needs the utility can be a different mixture of these criteria. Because of the focus on single metrics, offline optimization of IR systems does not account for different preferences in balancing relevance criteria.",1,ad,True
10,"We propose to mitigate this by viewing multiple relevance criteria as objectives and learning a set of rankers that provide different tradeoffs w.r.t. these objectives. We model document utility within a gainbased evaluation framework as a weighted combination of relevance criteria. Using the learned set, we are able to make an informed decision based on the values of the rankers and a preference w.r.t. the relevance criteria. On a dataset annotated for readability and a web search dataset annotated for sub-topic relevance we demonstrate how trade-offs between can be made explicit. We show that there are different available trade-offs between relevance criteria.",1,ad,True
11,Keywords,0,,False
12,Multi-objective optimization; Learning to rank,0,,False
13,1. INTRODUCTION,1,DUC,True
14,"The primary goal of information retrieval (IR) systems is to satisfy the information need of a user. Search engines today are fairly successful in finding topically relevant pages. To achieve this most search engines are optimized to rank documents based on their topical relevance to the query. In an offline optimization setting relevance is typically determined by experts, and evaluated with utility-based metrics such as nDCG, which tends to focus on optimizing a single aspect of utility. In online optimization, feedback is collected implicitly for all relevance criteria. However, this approach may ignore differences between individual users and information",0,,False
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17 - 21, 2016, Pisa, Italy",1,ad,True
16,c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00,0,,False
17,DOI: http://dx.doi.org/10.1145/2911451.2914708,0,,False
18,"needs by aggregating across all users and queries. Often aggregation works well, but not always. E.g., users that have limited vocabularies (e.g., children) can benefit from search results optimized for their reading level. When people look for medical information on the web they would benefit from accurate and reliable information, more so than when looking for information on a Star Wars movie.",1,ad,True
19,"Utility depends on many factors aside from topicality; criteria such as credibility, reputability and readability are also important [14]. While their importance is typically secondary to topicality, there clearly is a benefit in many use cases. A learning to rank approach [16] can be used to learn an optimal ranker for a specified weighted preference over criteria. Similarly, data fusion techniques can combine ranked lists that are optimized for a certain notion of utility. But what if we want to optimize for multiple criteria, without knowing their relative importance beforehand? For instance, how should we balance relevance and readability if we do not know who our user will be? Or relevance and sub-topic relevance?",1,ad,True
20,"We draw inspiration from multi-objective optimization techniques to answer these questions, i.e., to find a set of rankers for which each solution is optimal for a different trade-off in the relevance criteria. We combine the multi-objective technique Optimistic Linear Support (OLS) [12] with multiple utility-based metrics in a learning-to-rank setting. We consider two scenarios with two relevance criteria for which we optimize a set of rankers. We evaluate our approach on two datasets, one annotated for relevance and readability, and one annotated for relevance and diversity. To learn our rankers we apply dueling bandit gradient descent with a point-wise ranking function. To optimize for diversity we subsequently apply MMR and cluster-based ranking.",1,ad,True
21,2. BACKGROUND,0,,False
22,"The concept of relevance is core to IR and a much debated subject. Park [9] gives an extensive analysis on the nature of relevance in IR, and argues that relevance is intrinsically related to the selection process by the user. Cooper [4] states that each query could represent a set of specific questions as part of the information need, where documents are relevant if they provide an answer to one of these specific questions. Schamber [14] identifies major criterion groups for relevance: aboutness, currency, availability, clarity and credibility. There is a general trend that relevance cannot be attributed to just one factor such as topicality, but is multi-factored [9].",0,,False
23,"Many metrics have been proposed to measure the effectiveness of an IR system; we focus on metrics based on the concept of utility [2, 4]. The utility of an IR system depends on all factors that determine the usefulness for each specific user. Cooper [4] defines utility as ""A catch all concept involving not only topic relatedness",0,,False
24,769,0,,False
25,Convex coverage set,0,,False
26,Value of objective 2 Scalarized value,0,,False
27,Dominated solutions,0,,False
28,Value of objective 1,0,,False
29,0,0,,False
30,w1,0,,False
31,10,0,,False
32,w1,0,,False
33,10,0,,False
34,w1,0,,False
35,1,0,,False
36,Figure 1: The points on the line Figure 2: Three steps of OLS on a two-objective problem. X-axis is weight of one objective (note:,0,,False
37,"represent solutions in the cover- w2 ,"" 1 - w1), y-axis the scalarized value. Blue area highlights difference between upper bound""",0,,False
38,"age set, the others are dominated. and convex value. As more solutions are added to the CCS, the difference is iteratively reduced.",1,ad,True
39,"but also quality, novelty, importance, credibility, and many other",0,,False
40,"things."" Utility-based metrics combine a notion of utility with spe-",0,,False
41,cific assumptions about user behavior [2]. Each document has a,0,,False
42,specific numerical utility value for an information need. Addition-,0,,False
43,"ally, a discount function is used on the document's rank, under the",0,,False
44,assumption that more effort is needed to reach lower ranked doc-,0,,False
45,"uments, and it is less likely for the user to reach these documents.",0,,False
46,"Many metrics, therefore, boil down to the same basic formula to",0,,False
47,"estimate the utility of the ranked list, composed of a sum of the",0,,False
48,product of document utility and a discount factor [3]:,0,,False
49,"M,",0,,False
50,"K k,1",0,,False
51,gain,0,,False
52,(relk,0,,False
53,),0,,False
54,×,0,,False
55,discount,0,,False
56,(k),0,,False
57,(1),0,,False
58,"Extensions focus on multiple criteria. E.g., Dai et al. [5] present",0,,False
59,an extension of nDCG for freshness and relevance. Zuccon [17],0,,False
60,proposes on an extension of rank-based precision for readability.,1,ad,True
61,"Similarly, diversity and novelty metrics also take into account",0,,False
62,multiple criteria in the form of subtopic relevance [3]. The underly-,0,,False
63,ing assumption is that there are multiple subtopics (or categories),0,,False
64,"for each query, and each user will be interested in results for at least",0,,False
65,one of these subtopics. Relevance assessments are provided for,0,,False
66,each of the subtopics belonging to a query separately. These are,0,,False
67,combined based on the probability p(i|q) of intent i being intended,0,,False
68,by the user for query q [1]. -nDCG extends (1) with a weighted,0,,False
69,"sum over subtopics, given pi as the probability of each subtopic:",0,,False
70,-nDCG,0,,False
71,",",0,,False
72,1 N,0,,False
73,"M i,1",0,,False
74,pi,0,,False
75,"K k,1",0,,False
76,gain,0,,False
77,k i,0,,False
78,×,0,,False
79,discount (k),0,,False
80,(2),0,,False
81,While there has been previous work that combines multiple rel-,0,,False
82,"evance criteria in the utility-based evaluation framework, to the",0,,False
83,"best of our knowledge, no previous work uses multi-objective opti-",0,,False
84,"mization techniques on information retrieval problems, i.e., existing",0,,False
85,methods do not return a set of alternative rankers with different,0,,False
86,available trade-offs with respect to the different relevance criteria.,1,ad,True
87,3. MULTI-OBJECTIVE OPTIMIZATION,0,,False
88,Scalarization function. We assume that a ranker has a value for,0,,False
89,"each different relevance criterion, i.e., each ranker has an associated",0,,False
90,"value vector V, with a value, Vi in each criterion i. We follow [11]",0,,False
91,and assume that the preference of an individual user with respect,0,,False
92,to these criteria can be expressed in terms of an unknown scalar-,0,,False
93,"ization function f , that collapses the value vector to a scalar utility:",0,,False
94,"f (V, w), where w is a vector that parameterizes f . We are unable",0,,False
95,"to observe this function directly. Instead, we aim to find a cover-",1,ad,True
96,"age set [11] of rankers, that contains an optimal trade-off for each",1,ad,True
97,"possible preference (i.e., f and w) that a user might have, see Fig. 1.",0,,False
98,We assume that f is linear (where weighted means:,0,,False
99,C i,0,,False
100,wi,0,,False
101,",",0,,False
102,1):,0,,False
103,"f (V, w) ,"" wT V,""",0,,False
104,(3),0,,False
105,"i.e., the utility for the user is a weighted sum over relevance criteria.",0,,False
106,Metrics as objectives. To formulate our own scalarization function,0,,False
107,we can combine (1) with (3):,0,,False
108,"M,",0,,False
109,"C i,1",0,,False
110,wi,0,,False
111,"K k,1",0,,False
112,gain,0,,False
113,i(dock,0,,False
114,),0,,False
115,×,0,,False
116,discount,0,,False
117,(k),0,,False
118,(4),0,,False
119,"This definition is similar to the definition of -nDCG of (2), where",0,,False
120,instead of a sum over topics we have a sum over C relevance criteria.,1,ad,True
121,"In -nDCG, the metric M would subsequently be normalized. It",0,,False
122,"is, however, not desirable to normalize the linear scalarized value",0,,False
123,function as this would remove the convex property of the value,0,,False
124,vector that is required for efficient optimization using OLS. The,0,,False
125,linear scalarization function does require values that are comparable,0,,False
126,"in their magnitude, therefore, the individual value functions are",0,,False
127,"normalized with normalization value Ni instead, giving:",1,ad,True
128,Vi,0,,False
129,",",0,,False
130,1 Ni,0,,False
131,"K k,1",0,,False
132,gain,0,,False
133,i,0,,False
134,(dock,0,,False
135,),0,,False
136,×,0,,False
137,discount,0,,False
138,(k),0,,False
139,(5),0,,False
140,"Convex coverage set. Because each criterion contributes positively to the scalarization function, and we are interested in the relative importance of each criterion, we can assume that w is a positive vector that sums to 1 in order to determine a coverage set. A coverage set that covers all possible linear scalarizations is called a convex coverage set (CCS) [11]. To compute the (approximate) CCS, we build on the Optimistic Linear Support (OLS) framework for multi-objective optimization [12]. Fig. 2 illustrates the OLS procedure; OLS computes a CCS by solving a multi-objective problem as a series of single-objective problems, i.e., problems that are scalarized using different w. At each iteration, OLS tries to find a new ranker, thereby incrementally building a CCS. We can use existing single-objective optimization techniques to find rankers for a given w. We use Dueling Bandit Gradient Descent (DBGD) [16].",1,ad,True
141,"Each ranker found in an iteration of OLS has an associated value vector V. For each w, the scalarized value of a ranker is Vw ,"" w·V. Given a partial CCS, i.e., the set S of rankers and associated V found so far, we define the scalarized value function as a function of w:""",0,,False
142,"VS(w) ,"" max w · V,""",0,,False
143,VS,0,,False
144,"i.e., the scalarized value function is the convex upper surface of the vectors in Fig. 2. OLS selects the next w from the corner weights of VS(w), i.e., those w where VS(w) changes slope. In Fig. 2 the corner weights evaluated in that iteration are indicated by the blue vertical lines. The maximal possible improvement in scalarized value on the partial CCS is indicated by the dashed lines above VS(w). Once it reaches a corner weight, OLS is provably optimal as long as the single-objective method it employs to solve the scalarized problems is exact [12]. In practice, exact singleobjective subroutines are not required; we can safely use DBGD, but with lesser guarantees of the optimality of the solution [13].",0,,False
145,"Reuse and iterated search scheme. A limitation of applying standard OLS is that for every corner weight DBGD needs to be run. This can be expensive, depending on the size of the dataset. However, this can be mitigated by hot-starting the single-objective optimization algorithm with parts of previously found solutions (following [13]). For each new corner weight, we multi-start DBGD, starting from the rankers that were found at the 3 closest corner weights so far. It is possible that DBGD does not find a new best",0,,False
146,770,0,,False
147,"solution, even though such a solution might still exist. If this is the case for a number of iterations, we take a random pertubation step. DBGD is stopped automatically after 40,000 iterations, or if no improvement has been found after 20 random pertubations. To our knowledge, this is the first time such a Multi-Start/Iterated Local Search scheme [7] has been combined with OLS.",0,,False
148,4. EXPERIMENTAL SET-UP,0,,False
149,"To demonstrate how multi-objective optimization for balancing multiple relevance criteria works in practice, we perform experiments on two datasets: (i) balancing readability and topical relevance in a health setting (CLEF eHealth 2015 task 2 [8]), and (ii) balancing diversity and topical relevance in a web search dataset annotated for sub-topic relevance (TREC 2012 Web Track diversity task). While our runs are competitive, our main goal is to find multiple solutions that balance different relevance criteria, which we report in the form of a CCS.",1,ad,True
150,"CLEF eHealth. CLEF eHealth 2015 task 2 provides annotations for two objectives. It is composed of 5 training queries and 67 test queries; annotations are provided for relevance only for the training queries, and both relevance and understandability for the test queries. As the extra understandability annotations are required to optimize for both relevance and understandability at the same time we only use the test set queries for optimization. To measure readability document text is extracted using boilerpipe [6]. Since simple readability metrics do not correlate well with actual readability in the medical domain [15], another approach to readability is required. We compiled a list of medical terms and counted their occurrences. This list was taken from an English Wikipedia page, for which all words contained in the 20k English word list from the Google Trillion Word Corpus were filtered out.1 Using this feature, and, additionally, the Coleman-Liau index, Gunning fog index, and document length, we trained an SVM to predict the understandability score. For the CLEF eHealth 2015 task 2 the usual metrics are RBP, uRBP and uRBPgr. In preliminary experiments we found a strong correlation between RBP and uRBP, like [17]. Hence, we optimize for nDCG using relevance annotations (nDCGrel), and also for nDCG using understandability annotations (nDCGread). We normalize nDCGread so that the value is in the same range as nDCGrel.",1,CLEF,True
151,TREC Web Diversity. The TREC 2012 Web Track diversity,1,TREC,True
152,"task comes with 50 queries, with sub topic relevance assessments",0,,False
153,provided for the first 20 documents produced by the participating,0,,False
154,systems. TREC 2010 and 2011 Web Track diversity task queries,1,TREC,True
155,were used for training. For diversity we use MMR and cluster-based,0,,False
156,ranking [10] with cosine similarity on TF-IDF vectors. We only,0,,False
157,apply MMR on the first T clusters. Documents are first scored,0,,False
158,"based on relevance, subsequently, MMR and cluster-based ranking",0,,False
159,"rerank the documents for diversity, which produces a rank i for each",0,,False
160,"document i. Using rank i, the final ranking is determined based on",0,,False
161,(1,0,,False
162,-,0,,False
163,wd )scorei,0,,False
164,+,0,,False
165,wd,0,,False
166,1 rank,0,,False
167,i,0,,False
168,",",0,,False
169,where,0,,False
170,wd,0,,False
171,is,0,,False
172,a,0,,False
173,parameter,0,,False
174,that,0,,False
175,balances,0,,False
176,diversity and relevance. The usual metrics reported for the TREC,1,TREC,True
177,Diversity task are nDCG and -nDCG. For optimization we use both,0,,False
178,nDCG and -nDCG to optimize for both relevance and diversity.,0,,False
179,"As clustering introduces a lot of randomness, we average over 5",0,,False
180,"runs of DBGD. For value functions Vrel and Vdiv, we normalize the",0,,False
181,"values of nDCG and -nDCG, respectively, to [0, 1].",0,,False
182,5. RESULTS,0,,False
183,"CLEF eHealth 2015 task 2. For this task, we simultaneously optimize for readability and relevance, using nDCG for both metrics.",1,CLEF,True
184,1For this list see: github.com/JoostvDoorn/moo-sigir2016,0,,False
185,0.40,0,,False
186,0.35,0,,False
187,Scalarized value,0,,False
188,0.30,0,,False
189,0.25,0,,False
190,0.20,0,,False
191,0.15,0,,False
192,0.0,0,,False
193,0.2,0,,False
194,0.4,0,,False
195,0.6,0,,False
196,0.8,0,,False
197,1.0,0,,False
198,Weight for Readabilit y,1,ad,True
199,"Figure 3: The CCS found on CLEF eHealth 2015, with a scalarized value based on relevance and readability. Absolute left having maximum weight on relevance, and absolute right maximum weight on readability.",1,CLEF,True
200,The scalarized value of a solution was calculated using a weighted interpolation between value functions Vrel and Vread (Eq. 3).,1,ad,True
201,"The convex coverage set (CCS), constructed using OLS, is shown in Fig. 3. OLS finds eight solutions of which six are not dominated. The set of solutions is reported in Table 1 with their RBP and uRBP scores. We note that our best uRBP score is above the second run for the original task and the best nDCGrel is in the top-5 out of 110 runs. We observe that we are able to find solutions that optimally combine the nDCGrel and nDCGread objectives given different preferences for readability. E.g., with a wread of 0.626, we obtain a 5% increase in nDCGread, with an 8% loss compared the best solution in terms of nDCGrel (wread ,"" 0.197). uRBP combines both objectives, and is highly correlated to RBP [17]. Due to this correlation, using RBP with uRBP would not find all rankers that offer the best available trade-offs between relevance and readability; the solution would be biased toward relevance. We therefore conclude that uRBP is not suitable for all possible preferences that a user might have.""",1,ad,True
202,Table 1: Evaluation of the solutions from the CCS on eHealth. wread nDCGrel nDCGread RBP uRBP uRBPgr 0.000 0.350 0.783 0.392 0.342 0.337 0.197 0.364 0.777 0.397 0.340 0.339 0.448 0.344 0.807 0.371 0.327 0.324 0.514 0.343 0.804 0.372 0.327 0.324 0.626 0.335 0.814 0.369 0.326 0.324 0.771 0.298 0.832 0.333 0.294 0.294 0.944 0.266 0.840 0.304 0.270 0.269 1.000 0.157 0.840 0.189 0.160 0.159,1,ad,True
203,"To further analyze the effect of different weights for the readability objective, we analyze the annotations at each position in the ranking averaged over topics. Fig. 4 shows for three solutions in the CCS. The ranker optimized for readability does not show documents with higher relevance annotations in the top positions, whereas the other rankers are able to place more relevant documents at the top (similarly for readability). We observe that each ranker is suitable for their specific scalarization function, and as such our method is effective in balancing different relevance criteria.",1,ad,True
204,"TREC 2012 diversity task. For this task, we simultaneously optimize for overall relevance and sub-topic relevance by linearly combining value functions based on nDCG and -nDCG. The CCS from OLS is shown in Fig. 5. The results from the points in the CCS on the TREC 2012 diversity task are shown in Table 2. Fewer solutions were found for the CCS compared to the readability task,",1,TREC,True
205,771,0,,False
206,1,0,,False
207,2,0,,False
208,3,0,,False
209,4,0,,False
210,5,0,,False
211,6,0,,False
212,7,0,,False
213,"wread , 0.197",1,ad,True
214,8 9 10,0,,False
215,"wread , 0.626 wread , 1.00",1,ad,True
216,2.6 2.4 2.2 2.0 1.8 1.6 0.0 0.2 0.4 0.6 0.8 1.0,0,,False
217,Readability,1,ad,True
218,Relevance,0,,False
219,Figure 4: Average readability and relevance annotations on each rank for three different solutions in the CCS.,1,ad,True
220,1.00,0,,False
221,0.95,0,,False
222,Scalarized value,0,,False
223,0.90,0,,False
224,0.85,0,,False
225,0.80,0,,False
226,0.75,0,,False
227,0.70,0,,False
228,0.65,0,,False
229,0.60,0,,False
230,0.0,0,,False
231,0.2,0,,False
232,0.4,0,,False
233,0.6,0,,False
234,0.8,0,,False
235,1.0,0,,False
236,Weight for Diversity,0,,False
237,"Figure 5: The CCS found on the TREC 2010 and 2011 datasets, with a scalarized value based on relevance and diversity. Absolute left having maximum weight on relevance, and absolute right maximum weight on diversity.",1,TREC,True
238,furthermore the differences between the values in Table 5 are also,0,,False
239,"quite small, suggesting only a small trade-off between the objectives.",1,ad,True
240,"As such this setting seems less suitable for our method. In terms of -nDCG, the solutions on the test set (TREC 2012) are midperformers compared to the original participants (the best solution is above the fourth of nine participants). The overall nDCG score",1,TREC,True
241,Table 2: Evaluation of the solutions from the CCS on the TREC 2012 dataset.,1,TREC,True
242,wdiv nDCG -nDCG 0.000 0.236 0.489 0.808 0.229 0.493 1.000 0.204 0.480,0,,False
243,"would have ranked second. During training (TREC 2010­2011),",1,TREC,True
244,the intermediate solution that OLS found obtains the same -nDCG,0,,False
245,"score with an increase in nDCG, compared to the solution optimized",0,,False
246,"only for -nDCG, see Fig. 5. We therefore conclude that our ap-",0,,False
247,"proach finds more balanced and better solutions, than if we would",0,,False
248,optimize for a single objective.,0,,False
249,6. DISCUSSION,0,,False
250,"We demonstrated how to optimize rankings for multiple objectives by proposing a multi-objective approach based on optimistic linear support and DBGD for learning to rank. Because DBGD may get stuck in a local minimum we proposed an iterated local search schema for DBGD, and reuse of rankers inside OLS in order to make our algorithm more efficient. Using this approach, we have found multiple optimal rankers on the CLEF eHealth 2015 task 2 and on the TREC diversity task that offer different trade-offs w.r.t. different relevance criteria. These multiple optimal rankers are more flexible than a one-size-fits-all ranker produced by a standard learning to rank approach, and our work therefore forms an important step for flexibly optimizing search when multiple criteria are in play.",1,CLEF,True
251,"As to future work, one important issue is exposing different solutions to the user, or using different solutions to select the desired one. Exposing the user to multiple solutions can be done using additional",1,ad,True
252,"user interface elements, or based on profiling of the user or adapting per query. The number of user interface controls provided in generic search engines is very minimal; specialized search engines are more likely to benefit from optimizing their controls based on these multiobjective criteria. Future work may also investigate what a good scalarization function is, as others may exist and be more suited, and which metrics are more suitable for linear combination. Many current evaluation metrics are highly correlated with relevance and as such may not always provide the flexibility to get a large CCS.",1,ad,True
253,"Acknowledgments. This research was supported by Ahold, Amsterdam Data Science, the Bloomberg Research Grant program, the Dutch national program COMMIT, Elsevier, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOXPol), the ESF Research Network Program ELIAS, the Royal Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project, the Microsoft Research Ph.D. program, the Netherlands eScience Center under project number 027.012.105, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.001.109, 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, the Yahoo Faculty Research and Engagement Program, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.",1,ad,True
254,REFERENCES,0,,False
255,"[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In WSDM'09, pages 5­14. ACM, 2009.",0,,False
256,"[2] B. Carterette. System effectiveness, user models, and user utility: a conceptual framework for investigation. In SIGIR'11, pages 903­912. ACM, 2011.",0,,False
257,"[3] C. L. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In WSDM'11, pages 75­84. ACM, 2011.",1,ad,True
258,"[4] W. S. Cooper. A definition of relevance for information retrieval. Information Storage and Retrieval, 7(1):19­37, 1971.",0,,False
259,"[5] N. Dai, M. Shokouhi, and B. D. Davison. Learning to rank for freshness and relevance. In SIGIR'11, pages 95­104. ACM, 2011.",0,,False
260,"[6] C. Kohlschütter, P. Fankhauser, and W. Nejdl. Boilerplate detection using shallow text features. In WSDM'10, pages 441­450. ACM, 2010.",0,,False
261,"[7] H. R. Lourenço, O. C. Martin, and T. Stützle. Iterated local search. Springer, 2003.",0,,False
262,"[8] J. Palotti, G. Zuccon, L. Goeuriot, L. Kelly, A. Hanbury, G. J. Jones, M. Lupu, and P. Pecina. CLEF eHealth evaluation lab 2015, task 2: Retrieving information about medical symptoms. In CLEF '15. Springer, 2015.",1,CLEF,True
263,"[9] T. K. Park. The nature of relevance in information retrieval: An empirical study. The Library Quarterly, 63(3):318­351, 1993.",0,,False
264,"[10] F. Raiber and O. Kurland. The Technion at TREC 2013 web track: Cluster-based document retrieval. Technical report, Technion, Israel, 2013.",1,TREC,True
265,"[11] D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley. A survey of multi-objective sequential decision-making. Journal of Artificial Intelligence Research, 48:67­113, 2013.",0,,False
266,"[12] D. M. Roijers, S. Whiteson, and F. A. Oliehoek. Computing convex coverage sets for faster multi-objective coordination. Journal of Artificial Intelligence Research, 52:399­443, 2015.",0,,False
267,"[13] D. M. Roijers, S. Whiteson, and F. A. Oliehoek. Point-based planning for multi-objective POMDPs. In IJCAI'15, 2015.",0,,False
268,"[14] L. Schamber and J. Bateman. User criteria in relevance evaluation: Toward development of a measurement scale. In ASIS'96, volume 33, pages 218­25. ERIC, 1996.",0,,False
269,"[15] X. Yan, D. Song, and X. Li. Concept-based document readability in domain specific information retrieval. In CIKM'06, pages 540­549. ACM, 2006.",1,ad,True
270,"[16] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In ICML'09, pages 1201­1208. ACM, 2009.",0,,False
271,"[17] G. Zuccon. Understandability biased evaluation for information retrieval. In ECIR'16. Springer, 2016.",0,,False
272,772,0,,False
273,,0,,False

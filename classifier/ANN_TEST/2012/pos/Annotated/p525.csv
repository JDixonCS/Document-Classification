,sentence,label,data,regex
0,Generating Reformulation Trees for Complex Queries,0,,False
1,Xiaobing Xue W. Bruce Croft,0,,False
2,Center for Intelligent Information Retrieval Computer Science Department,0,,False
3,"University of Massachusetts, Amherst, MA, 01003, USA",0,,False
4,"{xuexb,croft}@cs.umass.edu",0,,False
5,ABSTRACT,0,,False
6,"Search queries have evolved beyond keyword queries. Many complex queries such as verbose queries, natural language question queries and document-based queries are widely used in a variety of applications. Processing these complex queries usually requires a series of query operations, which results in multiple sequences of reformulated queries. However, previous query representations, either the ""bag of words"" method or the recently proposed ""query distribution"" method, cannot effectively model these query sequences, since they ignore the relationships between two queries. In this paper, a reformulation tree framework is proposed to organize multiple sequences of reformulated queries as a tree structure, where each path of the tree corresponds to a sequence of reformulated queries. Specifically, a two-level reformulation tree is implemented for verbose queries. This tree effectively combines two query operations, i.e., subset selection and query substitution, within the same framework. Furthermore, a weight estimation approach is proposed to assign weights to each node of the reformulation tree by considering the relationships with other nodes and directly optimizing retrieval performance. Experiments on TREC collections show that this reformulation tree based representation significantly outperforms the state-of-the-art techniques.",1,TREC,True
7,Categories and Subject Descriptors,0,,False
8,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,0,,False
9,General Terms,0,,False
10,"Algorithms, Experimentation, Performance",0,,False
11,Keywords,0,,False
12,"Reformulation Tree, Verbose Query, Information Retrieval",1,Query,True
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.",1,ad,True
14,1. INTRODUCTION,1,DUC,True
15,"Although short keyword queries are still very common in web search, the increasing diversity of search applications and information needs has led to increasing complexity in queries. For example, verbose (or long) queries have become more and more popular in web search. In community-based Q&A, users pose natural language questions as queries. In patent retrieval, the whole document (patent application) is considered as the query. These complex queries help users express their information need naturally and save the effort of picking keywords. However, processing these queries poses a significant challenge for search systems.",0,,False
16,"Dealing with complex queries usually requires a series of query operations. For example, a typical process of dealing with a verbose query can be described as follows. The system first selects a subset of query words from the original query to remove noisy information. Then, the generated subset query is further modified to handle vocabulary mismatch. Finally, weights are assigned to queries generated at each step. Depending on the application, the above process could become more complicated. For example, in cross-lingual retrieval, the original verbose query needs to be translated into a foreign language query before applying any further operation. The above process will generate multiple sequences of reformulated queries, where each sequence records a way of modifying the original query using several query operations. These reformulation sequences capture the relationships between the reformulated queries. Fig. 1 displays some examples of the reformulation sequences, where the subset query is selected from the original query at the first step of the sequence and the second step further substitutes the subset query.",0,,False
17,"However, previous query representations cannot model these reformulation sequences well. The ""bag of words"" representation is widely used in information retrieval. Using this representation, the original query is transformed into a set of weighted words. Some extensions to this representation introduce phrases [19] and latent words [14][20].",0,,False
18,Reformulation Sequence Qreductions iraqs foreign debtreduce iraqs foreign debt Qiraqs foreign debtiraqs foreign debts Qiraqs foreign debtiraqs external debt,0,,False
19,"Figure 1: The reformulation sequences generated for a verbose query Q ""any efforts proposed or undertaken by world governments to seek reduction of iraqs foreign debt""",0,,False
20,525,0,,False
21,"This representation does not explicitly model a reformulated query, which serves as the basis of the reformulation sequences. An example of the ""bag of words"" representation is shown in Fig. 2 (a). Recently, the ""query distribution"" representation [27] was proposed to transform the original query into a set of reformulated queries. For example, Xue et al [30] represents a verbose query as a set of subset queries. This representation indeed considers a reformulated query as the basic unit, but it fails to capture the relationships between the reformulated queries. Therefore, the sequences of reformulated queries still cannot be modeled using this representation. An example of the ""query distribution"" representation is shown in Fig. 2 (b).",0,,False
22,"In this paper, a novel query representation is proposed to transform a complex query into a reformulation tree, where the nodes at each level of this tree correspond to the reformulated queries generated using a specific query operation. Using this representation, a reformulation sequence is naturally modeled as a path from the root node to the leaf node. The construction of the reformulation tree simulates the process of applying a series of query operations to the complex query. Furthermore, weight is assigned to each node of the reformulation tree, which indicates the importance of the corresponding reformulated query. The estimation of the weight for a node considers not only the characteristics of this node itself, but also its relationships with other nodes. Different with previous reformulation models that treat retrieval models as independent steps, we estimate the weights on the reformulation tree by directly optimizing the performance of retrieval models, which considers the reformulation model and the retrieval model in a joint view.",0,,False
23,"Verbose queries, as a typical example of complex queries, have attracted much attention recently. Previous research on verbose queries either weights the query words in the original query [16, 15, 3] or selects the best subset of query words from the original query [12]. Relatively little research considers combining multiple query operations together for improving verbose queries. Therefore, as an implementation of the reformulation tree framework, a two-level tree structure is constructed for verbose queries, where the first level corresponds to the subset query selection operation and the second level corresponds to the query substitution operation. A weight estimation method is also described, which incorporates the relationships between different reformulated queries and directly optimizes the retrieval performance.",1,corpora,True
24,"Fig. 2 (c) shows an example reformulation tree. The first level of this tree consists of two subset queries extracted from the original query, i.e., ""reductions iraqs foreign debt"" and ""iraqs foreign debt"". At the second level, each subset query is further modified to generate query substitutions. For example, ""iraqs foreign debt"" has been modified to ""iraqs external debt"". Furthermore, weight is assigned to each node of this tree, which measures the importance of each reformulated query. Compared with other representations, the reformulation sequences as shown in Fig. 1 are captured using the reformulation tree.",0,,False
25,"The contributions of this paper can be summarized as four folds. First, a tree based query representation is proposed to deal with complex queries, which models a series of query operations and captures the relationships between the reformulated queries. Second, a specific implementation, i.e., the two-level reformulation tree, is introduced for verbose queries, which combines two important operations, subset",0,,False
26,"a) Bag of Words {0.09 reduction, 0.09 iraqs, 0.09 foreign, 0.09 debt, ...}",0,,False
27,"b) Query Distribution {0.55 seek reduction iraqs, 0.23 seek reduction iraqs debt,",1,Query,True
28,"0.05 undertaken iraqs debt, 0.03 efforts seek reduction iraqs ... }",0,,False
29,c) Reformulation Tree,0,,False
30,Subset Selection:,0,,False
31,Original Query 0.36,1,Query,True
32,reductions iraqs foreign debt 0.20,0,,False
33,iraqs foreign debt 0.12,0,,False
34,Query Substitution:,1,Query,True
35,reduce iraqs foreign debt iraqs foreign debts,0,,False
36,0.20,0,,False
37,0.08,0,,False
38,iraqs external debt 0.04,0,,False
39,"Figure 2: Different query representations for a verbose query ""identify any efforts proposed or undertaken by world governments to seek reduction of iraqs foreign debt""",0,,False
40,"query selection and query substitution. Third, a weight estimation method is designed by incorporating the relationships between different reformulated queries and directly optimizing retrieval performance. Fourth, detailed experiments are conducted to show that the tree-based representation outperforms other query representations for verbose queries.",1,corpora,True
41,2. RELATED WORK,0,,False
42,"In this section, we first describe previous work on complex queries, especially on verbose queries and then we review previous query representation approaches.",0,,False
43,2.1 Complex Query,1,Query,True
44,"As described in the introduction, complex queries have been widely used in different applications. Some examples include the verbose query, the natural language question query and the document-based query.",0,,False
45,Kumaran and Allan [11] studied shortening a verbose query through human interaction. Bendersky and Croft [2] discovered key concepts from a verbose query. These key concepts were combined with the original query to improve the retrieval performance. Kumaran and Carvalho [12] learned to automatically select subset queries using several query quality predictors. Balasubramanian et al [1] extent [12] for web long queries.,0,,False
46,"Lease et al [16] developed a regression model to assign weights to each query word in the verbose query by using the secondary features. Lease [15] further combined their regression model with the Sequential Dependence Model, which achieved significant performance improvement. Bendersky et al [3] proposed a unified framework to measure the weights of words, phrases and proximity features underlying a verbose query.",0,,False
47,"A natural language question query is widely used in a community-based Question and Answer service such as Yahoo! Answers and Quora. Previous work [8, 9, 24] studied effectively finding previously answered questions that are relevant to a new question asked by a user. Different retrieval models have been proposed to calculate the simi-",1,Yahoo,True
48,526,0,,False
49,"larity between questions. For example, a translation based retrieval model [8] were developed to deal with the vocabulary mismatch between the semantically related questions.",0,,False
50,"A document-based query allows users to directly submit a document as a query. A typical example is in patent retrieval [13], where the whole patent application is submitted to the search system in order to find the relevant patents. Many features are extracted from a patent application and these features are the basis of retrieving relevant patents.",0,,False
51,"In this paper, a tree-based representation is proposed to improve the complex query. A specific implementation for verbose queries is described. This implementation combines subset selection and query modification within the same framework, which has not been explored in previous work.",0,,False
52,2.2 Query Representation,1,Query,True
53,"In this section, we review two types of query representations, ""bag of words"" and ""query distribution"".",0,,False
54,"The ""bag of words"" representation transforms the original query into a set of terms, either weighted or not. These terms include the words and phrases from the original query and the latent words and phrases extracted from the corpus. For example, the relevance model approach [14] adds latent words to the original query, the sequential dependency model [19] detects the phrase structure, and the latent concept expansion model [20] uses proximity features and latent words. This type of representation does not consider how to use words and phrases to form actual reformulated queries. In other words, a reformulated query is not explicitly modeled in this representation.",1,ad,True
55,"The ""query distribution"" representation transforms the original query into a set of reformulated queries, where each query is assigned a probability. This probability helps measure the importance of the query. For example, Xue et al [30] represented a verbose query as a distribution of subset queries and a modified Conditional Random Field is proposed to estimate the probability for each subset query. This type of representation indeed considers the reformulated query as the basic unit, but it assumes independence between the reformulated queries. When multiple query operations are applied, this independence assumption usually does not hold.",0,,False
56,"In this paper, the proposed ""reformulation tree"" representation extends the ""query distribution"" representation by modeling the relationships between reformulated queries using the tree structure.",0,,False
57,"Some previous work also considers the relationships between queries. Boldi et al [4] proposed to build a query-flow graph that modeled web users' search behaviors. Specifically, the directed edges between two queries indicated that they were likely to belong to the same search mission. Mei et al [17] presented a general framework to model search sequences, where a search sequence is represented as a nested sequence of search objects. The above work focuses on short keyword queries and uses query logs to capture the relationships between the queries submitted within the same search session. In contrast, in this paper, we study complex queries and model the relationships between the reformulated queries. Furthermore, the construction of the reformulation tree proposed in this paper is closely related to the final retrieval performance, while previous work studies the query graph or sequence independently from the retrieval model.",0,,False
58,"Guo et al [6] proposed a CRF-based model for query refinement, which combines several tasks like spelling correction, stemming and phrase detection. This model focuses on morphological changes of keyword queries such as spelling correction and stemming, but does not consider complex queries.",0,,False
59,3. BACKGROUND,0,,False
60,"In this section, we summarize several basic concepts used in this paper.",0,,False
61,"A complex query (q) is more complicated than a short keyword query. Examples of complex queries include verbose queries, natural language question queries and documentbased queries. In this paper, we will focus on verbose queries.",0,,False
62,"A query operation (r) indicates a query processing technique. In this paper, we focus on two query operations, i.e. subset query selection and query substitution. Subset query selection [11, 12, 1, 30] selects a subset of query words from the original query. Query substitution [10, 26, 29] replaces the original query word with a new word. Other examples of query operations include query translation, query segmentation and so on.",1,Query,True
63,A reformulated query (qr) is the output of applying a query operation. A reformulation sequence is a sequence of reformulated queries by applying a series of query operations.,0,,False
64,A reformulation tree (T ) is a tree structure that organizes the reformulated queries generated by different query operations. Each path of T corresponds to a reformulation sequence.,0,,False
65,4. REFORMULATION TREE,0,,False
66,"In this section, we first describe the framework for generating the reformulation tree T . Then, we compare this treebased representation with previous query representations. Finally, the principle of the weight estimation is described.",0,,False
67,4.1 Framework,0,,False
68,"Suppose that n query operations {r1, r2, ..., rn} are required to process a complex query q. Then, q is transformed into a n-level tree T . Each node of T represents a reformulated query qr. From now on, if not explicitly indicated, we use qr to represent both a node of T and the corresponding reformulated query. The root node of T represents the original query q, which can be considered as a special reformulated query. The ith level of T are generated by applying the ith operation ri to the nodes at the (i - 1)th level. An arc is added between the nodes at the (i - 1)th level and the nodes at the ith level if the latter is the output of applying ri to the former. Therefore, each path of T corresponds to a reformulation sequence. Furthermore, weight w(qr) is assigned to each node of T , which measures the importance of the corresponding reformulated query qr.",1,ad,True
69,"When T is used for retrieval, the retrieval score of a document D is calculated using Eq. 1.",0,,False
70,"sc(T, D) ,"" w(qr)sc(qr, D)""",0,,False
71,(1),0,,False
72,qr T,0,,False
73,"where w(qr) is the weight assigned to the node corresponding to the reformulated query qr and sc(qr, D) is the retrieval score of using qr to retrieve D. In general, sc(T, D) is calculated by combining the retrieval score of using each",0,,False
74,527,0,,False
75,"reformulated query qr in T , where w(qr) is used as the combination weight. The calculation of sc(qr, D) depends on implementation.",0,,False
76,4.2 Comparisons of Query Representations,1,Query,True
77,"In this subsection, we compare different query representations using the example in the introduction. Fig. 2 displays the ""bag of words"" representation, the ""query distribution"" representation and the ""reformulation tree"" representation.",0,,False
78,"In the ""bag of words"" representation, the basic unit is words or phrases. This representation may tell you that the important words in the original query are ""reduction"", ""iraqs"" and ""debt"", but how these words can be used together to form a new query is not clear.",0,,False
79,"The ""query distribution"" representation extends the ""bag of words"" representation by explicitly modeling a reformulated query. For example, this representation lists the top ranked subset queries such as ""seek reduction iraqs"" and ""seek reduction iraqs debt"". However, the relationships between the reformulated queries are not captured using this representation.",0,,False
80,"When a series of query operations are applied, we need a representation that models the reformulation sequences generated using these operations. The ""reformulation tree"" representation is designed to solve this problem. For example, in Fig. 2, the subset queries and the query substitutions are organized into a tree structure. This structure indicates that we need to first select subset queries and then conduct query substitution. It captures the relationships between ""iraqs foreign debt"" and ""iraqs external debt"", since the latter is the output of applying the query subsitution operation to the former.",0,,False
81,4.3 Weight Estimation,0,,False
82,"The weight assigned to each node in the tree indicates the importance of the corresponding reformulated query. This weight should characterize both the features of this node itself and its relationships with other nodes. Therefore, the weight of qr, i.e., w(qr), is calculated in Eq. 2.",0,,False
83,"w(qr) , w(par(qr)) kfk(qr)",0,,False
84,(2),0,,False
85,k,0,,False
86,"where par(qr) denotes the parent node of qr. fk is the query feature extracted from qr and k is the parameter. Eq. 2 shows that the weight of qr is not only decided by its own query features {fk} but also by the weight of its parent node par(qr). Intuitively, if qr is important, its children should also receive high weights. In this way, the relationships between reformulated queries are incorporated into the weight",1,corpora,True
87,estimation. Note that Eq. 2 provides the principle of weight estima-,0,,False
88,tion. How to calculate w(qr) will depend on the implementation.,0,,False
89,5. REFORMULATION TREE FOR VERBOSE QUERIES,0,,False
90,"In this section, we describe a two-level reformulation tree for verbose queries. We first describe the query operations used to construct the reformulation tree, i.e. subset query selection and query substitution. Then, we introduce a stagebased weight estimation method to assign weight to each node. Finally, the retrieval model is described.",0,,False
91,5.1 Building Tree Structure,0,,False
92,"The construction of the reformulation tree for verbose queries consists of two steps: first, subset queries are selected from the original query; second, the subset queries generated in the previous step are further modified to generate query substitutions.",0,,False
93,"We follow Kumaran and Carvalho [12]'s method to generate subset queries. All query words from the original verbose query are considered. If the length of the verbose query is bigger than ten, we first rank all query words by their idf values and then pick the top ten words for the subset query generation. Then, all subset queries with length between three and six words are generated.",0,,False
94,"The passage analysis technique [29] is used to generate query substitutions. In order to replace one word from the original query, all the passages containing the rest of the query words are first extracted. Then, three methods are used to generate candidates for query substitution from these passages. Morph considers the morphologically similar words as candidates. Pattern considers the words matching the patterns extracted from the original query as candidates. Wiki considers the Wikipedia redirect pairs as the candidates. More details can be found in [29]. Finally, the top ranked candidates are used as query substitutions.",1,Wiki,True
95,"Given the above two query operations, the reformulation tree for the verbose query can be generated in this way. First, all subset queries with length between three to six are extracted from the original query. Each subset query is assigned a weight. How to estimate the weight will be described in the next subsection. According to this weight, we will pick the top ranked subset queries to construct the first level of the reformulation tree. SubNum is a parameter that controls the number of nodes at the first level. Second, among these SubNum subset queries, we further modify the top ModNum queries to generate query substitutions, which constructs the second level of the reformulation tree. ModNum is another parameter that controls the number of nodes that will be substituted.",0,,False
96,"For example, the reformulation tree used in the introduction (Fig. 2) can be constructed in two steps. This process is illustrated in Fig. 3. First, we pick the top two subset queries ""reductions iraqs foreign debt"" and ""iraqs foreign debt"" to construct the first level of the reformulation tree. Second, we modify these two subset queries respectively. For the first subset query, ""reduce iraqs foreign debt"" is generated by replacing ""reduction"" with ""reduce"". For the second subset query, two query substitutions, i.e. ""iraqs foreign debt"" and ""iraqs external debt"" are generated.",0,,False
97,5.2 Weight Estimation,0,,False
98,"Eq. 2 indicates that the weight of a node in the reformulation tree depends on both its intrinsic features and the weight of its parent node. However, how to estimate the weight is still unclear. In this part, we describe a stage-based weight estimation method. The learning-to-rank based parameter estimation strategy [28] is used as the basis, which transforms a query feature into a corresponding retrieval feature.",0,,False
99,"In the initial stage, the root node (the original query q) is assigned the weight 1, i.e. w(q) , 1.",0,,False
100,"In Stage I, after the subset queries qsub are generated, we calculate the weight of qsub using Eq. 3.",0,,False
101,528,0,,False
102,Original Query Step 1,1,Query,True
103,Original Query,1,Query,True
104,reductions iraqs foreign debt iraqs foreign debt Step 2 Original Query,1,Query,True
105,reductions iraqs foreign debt,0,,False
106,iraqs foreign debt,0,,False
107,reduce iraqs foreign debt iraqs foreign debts,0,,False
108,iraqs external debt,0,,False
109,Figure 3: The process of constructing a reformulation tree,0,,False
110,"w(qsub) , w(q) skubfksub(qsub)",0,,False
111,k,0,,False
112,",",0,,False
113,skub fksub (qsub ),0,,False
114,(3),0,,False
115,k,0,,False
116,Eq. 3 instantiates Eq. 2 by focusing on the subset queries. fksub is the query feature extracted from qsub and skub is the corresponding parameter. Since the root node is the parent,0,,False
117,"of every subset query, its weight w(q) ,"" 1, is used in Eq. 3. In order to estimate {skub} by directly optimizing the re-""",0,,False
118,"trieval performance, we transform each query feature fksub into the corresponding retrieval feature Fksub, where Fksub is calculated in Eq. 4.",0,,False
119,"Fksub({qsub}, D) ,",0,,False
120,"fksub(qsub)sc(qsub, D)",0,,False
121,(4),0,,False
122,qsub,0,,False
123,"where sc(qsub, D) is the retrieval score of using qsub to retrieve D. The calculation of sc(qsub, D) depends on the retrieval model. The retrieval feature Fksub combines the retrieval score of each subset query qsub using their corresponding query feature fksub(qsub) as the combination weight. In general, Fksub indicates how well documents are ranked if fksub is used as the weight to combine subset queries.",0,,False
124,"Now, we obtain a set of retrieval features {Fksub}. The problem of estimating {skub} to combine the query features {fksub} is transformed into the problem of combining the corresponding retrieval features {Fksub} to achieve the best retrieval performance. The latter problem is typically solved",0,,False
125,"using learning to rank techniques. In this paper, the ListNet method [5] is adopted to learn {skub} on the training set.",1,ad,True
126,"After obtaining {skub}, we can assign the weight for each subset query according to Eq. 3.",0,,False
127,"In Stage II, we assign weights to the substituted queries.",0,,False
128,The weight of a substituted query qmod is calculated using Eq. 5.,0,,False
129,"w(qmod) , w(qsub)",0,,False
130,m k odfkmod(qmod),0,,False
131,(5),0,,False
132,k,0,,False
133,Table 1: Summary of features,0,,False
134,Features for Subset Queries fksub(qsub) MI[11] mutual information,0,,False
135,SQLen[12] sub-query length,0,,False
136,QS[7] query scope,0,,False
137,QC[25] query clarity score,0,,False
138,SOQ[12] similarity to original query,0,,False
139,psg,0,,False
140,count of passages containing qsub,0,,False
141,KeyCpt[2] whether contains the key concept,0,,False
142,Features for Substituted Queries fkmod(qmod) Morph generated using Morph,0,,False
143,Pattern generated using Pattern,0,,False
144,Wiki,1,Wiki,True
145,generated using Wiki,1,Wiki,True
146,psg,0,,False
147,count of passages containing qmod,0,,False
148,seg-type the number of possible segmentations,0,,False
149,where qsub is the parent node of qmod. Compared with Eq.,0,,False
150,"3, the weights of the subset queries w(qsub) generated in",0,,False
151,"Stage I are incorporated in Eq. 5. Similarly, in order to estimate {m k od}, we transform fkmod",1,corpora,True
152,into the corresponding retrieval feature Fkmod using Eq. 6.,0,,False
153,"Fkmod({qmod}, D) ,",0,,False
154,"w(qsub)fkmod(qmod)sc(qmod, D)",0,,False
155,qmod,0,,False
156,"(6) where qsub is the parent node of qmod. In general, Fkmod tells how well the documents are ranked if fkmod is used as the weight to combine the substituted queries. Thus, the parameters {m k od} are learned by combining these retrieval features {Fkmod} using ListNet.",0,,False
157,5.3 Features,0,,False
158,"In this part, we describe the query features used to characterize the subset queries and the substituted queries.",0,,False
159,"The features used to characterize the subset queries are mainly query quality predictors. This type of features have been widely used in previous research [12][30]. Examples of query quality predictors include Mutual Information [11], Query Scope [7] and Query Clarity [25]. In addition, passage information is considered. The number of passages that contain a subset query provides strong evidence for the quality of a subset query. Whether a subset query contains key concepts is also considered as a feature. These key concepts were discovered by Bendersky et al [2].",1,Query,True
160,"The features used to characterize the substituted queries include the type of methods of generating substituted queries. As described in Section 5.1, ""Morph"" indicates using the morphologically similar words as candidates, ""Pattern"" indicates using the pattern based method and ""Wiki"" indicates using the Wikipedia redirect page. The passage information is also considered as one feature. Furthermore, the number of possible segmentations of a substituted query is used as another feature. This feature can be directly obtained using the passage analysis technique [29].",1,Wiki,True
161,The details of features are summarized in Table 1.,0,,False
162,5.4 Retrieval Model,0,,False
163,"The retrieval score of using a reformulation tree T can be calculated using Eq. 1. For example, given the reformulation tree shown in Fig. 2, the retrieval score can be calculated as follows:",0,,False
164,529,0,,False
165,"sc(T, D) ,"" 0.36 × sc(Original Query, D) "","" +0.2 × sc(""""reductions iraqs foreign debt"""", D) "","" +0.12 × sc(""""iraqs foreign debt"""", D) "","" +0.2 × sc(""""reduce iraqs foreign debt"""", D) "","" +0.08 × sc(""""iraqs foreign debts"""", D) "","" +0.04 × sc(""""iraqs external debt"""", D)""",1,Query,True
166,"In this paper, the sequential dependency model (SDM) [19] is used to calculate sc(qr, D), which has been widely used in previous work [2, 30] as a state-of-the-art technique. Using SDM, the score of a document can be calculated as follows:",0,,False
167,"sc(qr, D) , T",0,,False
168,log(P (t|D)) + O,0,,False
169,log(P (o|D)),0,,False
170,tT (qr ),0,,False
171,oO (qr ),0,,False
172,+ U,0,,False
173,log(P (u|D)),0,,False
174,(7),0,,False
175,uU (qr ),0,,False
176,"where T (qr) denotes a set of query words of qr, O(qr) denotes a set of ordered bigrams extracted from qr and U (qr) denotes a set of unordered bigrams extracted from qr. T , O and U are parameters controlling the weights of different parts and are usually set as 0.85, 0.1 and 0.05 [19]. P (t|D), P (o|D) and P (u|D) are calculated using the language modeling approach [22, 31].",0,,False
177,The SDM model can be easily implemented using the Indri query language [18]. Fig. 4 shows an example of Indri query for SDM model.,0,,False
178,6. EXPERIMENTS,0,,False
179,"Four TREC collections, Gov2, Robust04, ClueWeb (Category B) and Wt10g are used for experiments. Robust04 is a newswire collection, while the rest are web collections. The statistics of each collection are reported in Table 2. For each collection, two indexes are built, one not stemmed and the other stemmed using the Porter Stemmer[23]. Stemming transforms a word into its root form, which is conducted either during indexing or during query processing. The latter case treats stemming as a part of query reformulation, which has been shown effective for web search [21]. Both cases are considered in this paper using two types of indexes. No stopword removal is done during indexing. For each topic, the description part is used as the query. A short list of 35 stopwords and some frequent stop patterns (e.g., ""find information"") are removed from the description query.",1,TREC,True
180,"The query set of each collection is split into a training set and a test set. On the training set, the parameters k are learned. On the test set, the learned parameters k are used to assign weight to the reformulation tree generated from each test query. Specifically, ten-fold cross validation is used, where the query set is split into ten folds. Each time nine folds are used for training and one fold is used for test. This process repeats ten times.",0,,False
181,"Several baselines are compared. QL denotes the query likelihood language model [22, 31]. SDM denotes the sequential dependence model [19]. KC denotes the key concept method proposed by Bendersky et al [2]. Note that we do not report KC on ClueWeb, since the key concept query is not provided on ClueWeb in [2]. QL+SubQL and DM+SubQL [30] are the subset query distribution methods,",1,ClueWeb,True
182,Figure 4: Example of Indri query. qr : iraqs foreign debt SDM: #weight(,0,,False
183,0.85 #combine(iraqs foreign debt) 0.10 #combine(#1(iraqs foreign) #1(foreign debt)) 0.05 #combine(#uw8(iraqs foreign) #uw8(foreign debt)) ),0,,False
184,Table 2: TREC collections used in experiments,1,TREC,True
185,Name,0,,False
186,Docs,0,,False
187,Topics,0,,False
188,Gov2,1,Gov,True
189,"25,205,179 701-850",0,,False
190,"Robust04 528,155 301-450,601-700",1,Robust,True
191,"Wt10g 1,692,096 451-550",0,,False
192,"ClueWeb 50,220,423 1-100",1,ClueWeb,True
193,"which combine the original query with a distribution of subset queries. QL+SubQL uses QL for both the original query and the subset queries, while DM+SubQL uses SDM for the original query and uses QL for the subset queries. In this paper, QL+SubQL and DM+SubQL are trained using the global features mentioned in [30]. SDM, KC, QL+SubQL and DM+SubQL are the state-of-the-art techniques for verbose queries. SDM and KC can be classified as the ""bag of words"" representation, while QL+SubQL and DM+SubQL can be considered as the ""query distribution"" representation. Therefore, the comparisons with these baselines help show the advantages of the ""reformulation tree"" representation.",1,ad,True
194,"The proposed reformulation tree approach is denoted as RTree, which uses SDM as the underlying retrieval model. Two parameters are used during the tree construction, SubNum and ModNum, where SubNum denotes how many subset queries are kept in the reformulation tree and ModNum denotes among those kept subset queries how many are further modified to generate query substitutions. In this paper, SubNum takes all subset queries generated and ModNum is set as 10. The effect of these parameters will be explored in the following part of this paper.",0,,False
195,"The standard performance measures, mean average precision (MAP), precision at 10 (P10) and the normalized discounted cumulative gain at 10 (NDCG10), are used to measure retrieval performance. In order to improve readability, we report 100 times the actual values of these measures. The two-tailed t-test is conducted for significance.",1,MAP,True
196,The Lemur 4.10 toolkit is used to build the index and run the query. The ireval package provided in the toolkit is used for evaluation and significance test.,0,,False
197,6.1 Example,0,,False
198,"In Table 3, we show some examples of the generated reformulation trees. As mentioned previously, some stopwords and stop patterns are removed from the original query. Those words are kept to improve readability. Note that they are not used for retrieval and subset query generation.",1,ad,True
199,"Table 3 shows that the subset queries and the substituted queries are effectively combined within the same framework. For example, given the original query ""what allegations have been made about enrons culpability in the california energy crisis"", the reformulation tree first generates high quality subset queries ""enrons culpability california energy crisis"" and ""california energy crisis"" and then further modifies ""california energy crisis"" as two substituted queries ""california gas prices"" and ""california electricity crisis"".",1,ad,True
200,530,0,,False
201,Table 4: Comparisons of retrieval performance. denotes significantly different from the baseline.,0,,False
202,Gov2,1,Gov,True
203,Robust04,1,Robust,True
204,Wt10g,0,,False
205,ClueWeb,1,ClueWeb,True
206,MAP P10 NDCG10 MAP P10 NDCG10 MAP P10 NDCG10 MAP P10 NDCG10,1,MAP,True
207,QL,0,,False
208,22.46 49.13 35.94,0,,False
209,SDM,0,,False
210,23.98 51.01 38.81,0,,False
211,KC,0,,False
212,24.88 50.87 37.72,0,,False
213,QL+SubQL,0,,False
214,23.36 50.81 37.96,0,,False
215,DM+SubQL 24.82 53.36 40.55,0,,False
216,RTree,0,,False
217,26.70 53.96 40.78,0,,False
218,vs. QL,0,,False
219,18.9% 9.8% 13.5%,0,,False
220,vs. SDM,0,,False
221,11.3% 5.8% 5.1%,0,,False
222,vs. KC,0,,False
223,7.3% 6.1% 8.1%,0,,False
224,vs. QL+SubQL 14.3% 6.2% 7.4%,0,,False
225,vs. DM+SubQL 7.6% 1.1% 0.6%,0,,False
226,Non-stemmed Index 22.40 39.12 39.63 16.44 28.97 23.30 40.04 40.60 16.76 31.65 23.87 40.76 40.75 17.45 30.82 22.85 39.28 39.98 16.81 29.79 23.65 40.32 41.15 18.25 31.13 25.07 42.33 42.64 19.44 34.02 11.9% 8.2% 7.6% 18.2% 17.4% 7.6% 5.7% 5.0% 16.0% 7.5% 5.0% 3.9% 4.6% 11.4% 10.4% 9.7% 7.8% 6.7% 15.6% 14.2% 6.0% 5.0% 3.6% 6.5% 9.3%,0,,False
227,27.86 29.82 29.55 28.48 30.71 32.80 17.7% 10.0% 11.0% 15.2% 6.8%,0,,False
228,10.97 21.63 11.53 22.76 n/a n/a 11.01 21.84 11.54 21.94 12.94 26.43 18.0% 22.2% 12.2% 16.1% n/a n/a 17.5% 21.0% 12.1% 20.5%,0,,False
229,14.10 15.04 n/a 14.16 14.85 18.05 28.0% 20.0% n/a 27.5% 21.5%,0,,False
230,QL,0,,False
231,25.43 52.21,0,,False
232,SDM,0,,False
233,27.85 54.03,0,,False
234,KC,0,,False
235,27.52 53.83,0,,False
236,QL+SubQL,0,,False
237,26.19 53.36,0,,False
238,DM+SubQL 28.49 55.91,0,,False
239,RTree,0,,False
240,29.85 56.38,0,,False
241,vs. QL,0,,False
242,17.4% 8.0%,0,,False
243,vs. SDM,0,,False
244,7.2% 4.3%,0,,False
245,vs. KC,0,,False
246,8.5% 4.7%,0,,False
247,vs. QL+SubQL 14.0% 5.7%,0,,False
248,vs. DM+SubQL 4.8% 0.8%,0,,False
249,38.42 40.14 39.10 39.51 41.95 41.84 8.9% 4.2% 7.0% 5.9% -0.3%,0,,False
250,Porter-stemmed Index 25.49 43.13 42.89 19.61 32.68 26.83 44.94 44.78 20.87 35.77 25.97 41.65 42.29 21.01 34.02 25.81 43.01 43.23 20.11 32.06 26.99 44.86 44.94 21.98 35.05 28.00 45.10 45.30 23.80 37.42 9.8% 4.6% 5.6% 21.4% 14.5% 4.4% 0.4% 1.2% 14.0% 4.6% 7.8% 8.3% 7.1% 13.3% 10.0% 8.5% 4.9% 4.8% 18.3% 16.7% 3.7% 0.5% 0.8% 8.3% 6.8%,0,,False
251,31.31 33.21 32.29 30.93 33.45 35.84 14.5% 7.9% 11.0% 15.9% 7.1%,0,,False
252,12.64 23.57 13.01 24.90 n/a n/a 12.94 25.00 13.29 24.08 13.69 25.82 8.3% 9.5% 5.2% 3.7% n/a n/a 5.8% 3.3% 3.0% 7.2%,0,,False
253,15.46 15.87 n/a 16.37 15.58 18.09 17.0% 14.0% n/a 10.5% 16.1%,0,,False
254,"Table 3: Examples of the reformulation tree. The top ranked nodes are displayed. In the original query Q, the stopwords and stop structures are italicized.",0,,False
255,Q: what allegations have been made about enrons culpability in the california energy crisis 0.194 Q 0.133 enrons culpability california energy crisis 0.047 california energy crisis,1,ad,True
256,0.009 california gas prices 0.008 california electricity crisis,0,,False
257,Q: give information on steps to manage control or protect squirrels 0.148 Q 0.060 control protect squirrels 0.048 control squirrels,0,,False
258,0.013 control of ground squirrels 0.012 control squirrel,0,,False
259,Q: what is the state of maryland doing to clean up the chesapeake bay 0.089 Q 0.063 maryland chesapeake bay,0,,False
260,0.015 md chesapeake bay 0.009 maryland chesapeake bay watershed 0.034 chesapeake bay 0.007 chesapeake bay watershed 0.006 chesapeake bay river,0,,False
261,6.2 Retrieval Performance,0,,False
262,"The first experiment is conducted to compare the retrieval performance of the proposed RTree method with the baselines. The baseline methods include QL, SDM, KC, QL+Sub QL and DM+SubQL. The results are shown in Table 4. The best performance is bolded.",0,,False
263,"Table 4 shows that RTree outperforms all the baseline methods. Specifically, RTree performs better than the ""bag of word"" representations, SDM and KC. Using the nonstemmed index, RTree significantly improves SDM and KC on all four collections with respect to all three performance measures. For example, on ClueWeb, RTree improves SDM by 12.2% and 20.0% with respect to MAP and NDCG10, respectively. On Wt10g, RTree improves KC by 11.4% and 11.0% with respect to MAP and NDCG10, respectively. On the Porter-stemmed index, similar results are also observed. These results show that the ""reformulation tree"" representation is more effective than the ""bag of words"" representation on modeling verbose queries, since the former explicitly models the reformulated query, while the latter only considers words and phrases.",1,ClueWeb,True
264,"Furthermore, RTree also outperforms the ""query distribution"" representations, QL+SubQL and DM+SubQL. Using the non-stemmed index, RTree outperforms QL+SubQL and DM+SubQL on all four collections with respect to all three measures. Most of the improvements are significant. For example, on ClueWeb, RTree improves QL+SubQL by 17.5% and 27.5% with respect to MAP and NDCG10, respectively. Also, RTree improves DM+SubQL by 12.1% and 21.5% with respect to MAP and NDCG10, respectively. The results using the Porter-stemmed index are similar. These observations indicate that the ""reformulation tree"" representation is better than the ""query distribution"" representation,",1,ClueWeb,True
265,531,0,,False
266,number of queries,0,,False
267,70,0,,False
268,60,0,,False
269,50 SDM,0,,False
270,40 KC,0,,False
271,30 RTree,0,,False
272,20,0,,False
273,10,0,,False
274,0,0,,False
275,number of queries,0,,False
276,relative increase/decrease,0,,False
277,(a) Gov2,1,Gov,True
278,140 120 100,0,,False
279,80 60 40 20,0,,False
280,0,0,,False
281,SDM KC RTree,0,,False
282,number of queries,0,,False
283,relative increase/decrease,0,,False
284,(b) Robust04,1,Robust,True
285,50,0,,False
286,45,0,,False
287,40,0,,False
288,35 SDM,0,,False
289,30,0,,False
290,25,0,,False
291,KC,0,,False
292,20,0,,False
293,RTree,0,,False
294,15,0,,False
295,10,0,,False
296,5,0,,False
297,0,0,,False
298,number of queries,0,,False
299,relative increase/decrease,0,,False
300,(c) Wt10g,0,,False
301,35 30 25 20 15 10,0,,False
302,5 0,0,,False
303,SDM RTree,0,,False
304,relative increase/decrease,0,,False
305,(d) ClueWeb Figure 5: Analysis of relative increases/decreases of MAP over QL.,1,ClueWeb,True
306,since the former effectively combines different reformulation operations within the same framework.,0,,False
307,"It is not surprising that RTree brings more improvements over the baselines using the non-stemmed index than using the Porter-stemmed index, since some effect of query substitutions, especially those generated using the morphologically similar words, is already provided by the Porter stemmer.",1,ad,True
308,6.3 Further Analysis,0,,False
309,"Table 4 shows that RTree significantly outperforms the baseline methods. In this part, we make detailed comparisons between RTree and the baseline approaches.",0,,False
310,"First, we compare RTree with SDM and KC. Specifically, we analyze the number of queries each approach increases or decreases over QL. Fig 5 shows the histograms of SDM, KC and RTree based on the relative increases/decreases of MAP over QL. The non-stemmed index is used in Fig. 5. Similar results are observed using the Porter-stemmed index.",1,MAP,True
311,Table 5: Comparisons with QL+SubQL and,0,,False
312,"DM+SubQL. ""+"", "","" and -"""" denote that RTree""",0,,False
313,"performs better, equal or worse than QL+SubQL",0,,False
314,and DM+SubQL with respect to MAP.,1,MAP,True
315,RTree,0,,False
316,vs. QL+SubQL,0,,False
317,vs. DM+SubQL,0,,False
318,+,0,,False
319,",",0,,False
320,-,0,,False
321,+,0,,False
322,",",0,,False
323,-,0,,False
324,Gov2 71.81% 0.67% 27.52% 63.09% 0.67% 36.24%,1,Gov,True
325,Robust04 68.27% 0.00% 31.73% 63.05% 0.00% 36.95%,1,Robust,True
326,Wt10g 62.89% 2.06% 35.05% 62.89% 1.03% 36.08%,0,,False
327,ClueWeb 65.31% 2.04% 32.65% 70.41% 2.04% 27.55%,1,ClueWeb,True
328,Table 6: The effect of subset query selection and,0,,False
329,query substitution with respect to MAP,1,MAP,True
330,MAP,1,MAP,True
331,Gov2 Robust04 Wt10g ClueWeb,1,Gov,True
332,SDM,0,,False
333,23.98 23.30 16.76 11.53,0,,False
334,KC,0,,False
335,24.88 23.87 17.45,0,,False
336,,0,,False
337,QL+SubQL 23.36 22.85 16.81 11.01,0,,False
338,DM+SubQL 24.82 23.65 18.25 11.54,0,,False
339,RTree-Subset 25.80 24.76 18.11 11.73,0,,False
340,RTree,0,,False
341,26.70 25.07 19.44 12.94,0,,False
342,"Fig. 5 shows that RTree improves more queries than SDM and KC. For example, on Gov2, RTree improves 110 queries out of the total 150 queries, while SDM and KC improve 89 and 91, respectively. On Robust04, RTree improves 174 queries out of the total 250 queries, while SDM and KC improve 129 and 153 queries, respectively. At the same time, RTree also hurts less queries than SDM and KC. These observations indicate that RTree is more robust than both SDM and KC.",1,Gov,True
343,"Furthermore, we compare RTree with QL+SubQL and DM+SubQL. QL+SubQL and DM+SubQL only consider subset query selection, while RTree combines both subset query selection and query substitution. The comparisons between them indicate whether RTree effectively combines two query operations to improve verbose queries. Specifically, we analyze the percent of queries where RTree performs better than QL+SubQL and DM+SubQL, respectively. The results using the non-stemmed index are reported in Table 5.",0,,False
344,"Table 5 shows that RTree consistently outperforms QL+ SubQL and DM+SubQL for 60%-70% queries on all four collections. These results indicate that RTree provides an effective way to combine different query operations, which significantly improves the retrieval performance of verbose queries.",0,,False
345,6.4 Subset Selection vs. Query Substitution,1,Query,True
346,"RTree combines subset query selection and query substitution together using a two-level reformulation tree. Previous experiments have demonstrated the general effect of this approach. In this part, we split the effect of subset query selection and query substitution. Specifically, we propose a one-level reformulation tree, which only consists of subset queries. This one-level reformulation tree is denoted as RTree-Subset. The comparisons between RTree-Subset and other approaches using the non-stemmed index are shown in Table 6.",0,,False
347,"In Table 6, RTree-Subset outperforms the baseline methods, which indicates the effect of subset queries in the reformulation tree. When query substitutions are introduced, RTree further improves RTree-Subset. Thus, both subset selection and query substitution account for the performance",0,,False
348,532,0,,False
349,MAP,1,MAP,True
350,0.3 0.295,0,,False
351,0.29 0.285,0,,False
352,0.28 0.275,0,,False
353,0.27 5,0,,False
354,10,0,,False
355,20,0,,False
356,30,0,,False
357,all,0,,False
358,MAP,1,MAP,True
359,0.29 0.285,0,,False
360,0.28 0.275,0,,False
361,0.27 0.265,0,,False
362,0.26 5,0,,False
363,(a) Gov2,1,Gov,True
364,10,0,,False
365,20,0,,False
366,30,0,,False
367,all,0,,False
368,MAP,1,MAP,True
369,0.25 0.245,0,,False
370,0.24 0.235,0,,False
371,0.23 0.225,0,,False
372,0.22 5,0,,False
373,(b) Robust04,1,Robust,True
374,10,0,,False
375,20,0,,False
376,30,0,,False
377,all,0,,False
378,MAP,1,MAP,True
379,0.15 0.145,0,,False
380,0.14 0.135,0,,False
381,0.13 0.125,0,,False
382,0.12 5,0,,False
383,(c) Wt10g,0,,False
384,10,0,,False
385,20,0,,False
386,30,0,,False
387,all,0,,False
388,(d) ClueWeb,1,ClueWeb,True
389,Figure 6: The effect of the parameter SubNum. x-axis denotes SubNum and y-axis denotes MAP.,1,MAP,True
390,of RTree. RTree-Subset also performs better than other subset query selection methods such as QL+SubQL and DM+SubQL.,0,,False
391,6.5 Parameter Analysis,0,,False
392,"As described in Section 5.1, there are two parameters used during the process of constructing the reformulation tree, SubN um and M odN um. SubN um denotes the number of subset queries used in the reformulation tree and M odN um denotes the number of subset queries that are modified to generate query substitutions. In this subsection, we explore the effect of these two parameters. The Porter-stemmed index is used. Fig. 6 shows the effect of the parameter SubN um, where SubN um takes the values 5, 10, 20, 30 and ""all"", where M odN um is fixed as 10. Here, ""all"" indicates using all subset queries generated.",0,,False
393,"Fig. 6 shows that the best number of subset queries used in the reformulation tree is inconsistent. On Gov2, the performance becomes stable after using the top 20 subset queries. On Robust04 and Wt10g, the performance keeps increasing when more subset queries are considered. On ClueWeb, the performance drops when more than the top 20 queries are used. One possible explanation for these observations is provided. Robust04 and Wt10g are relatively",1,Gov,True
394,Table 7: The effect of the parameter M odN um with,0,,False
395,respect to MAP,1,MAP,True
396,M odN um Gov2 Robust04 Wt10g ClueWeb,1,Gov,True
397,3,0,,False
398,29.52 27.08 22.61 13.77,0,,False
399,5,0,,False
400,29.63 27.11 22.72 13.75,0,,False
401,10,0,,False
402,29.66 27.16 22.75 13.71,0,,False
403,"small collections, thus using more subset queries is likely to retrieve more relevant documents. However, when the size of the collection becomes very large such as Gov2, using more subset queries does not help much retrieval all relevant documents. If the collection is not only big but also contains much noise such as ClueWeb, using more subset queries even hurts the performance.",1,Gov,True
404,"Table 7 displays the retrieval performance when M odN um takes three different values, i.e. 3, 5 and 10, where SubN um is set as 10.",0,,False
405,"Table 7 shows that there is not much performance change when M odN um is bigger than 3, which indicates that modifying the top three subset queries is enough to achieve most of the performance of RTree.",0,,False
406,7. EFFICIENCY,0,,False
407,"We now discuss the efficiency of using the reformulation tree model for retrieval. The online cost of this model comes from three aspects, i.e. reformulated query generation, query feature extraction, and retrieval.",0,,False
408,"The efficiency of the reformulated query generation depends on the reformulation operations involved. For example, generating the subset queries is very efficient. In contrast, generating query substitutions using the passage analysis is more time consuming, since it needs to analyze a lot of passages.",0,,False
409,"The efficiency of the query feature extraction also depends on the query features used. Some query features are expensive such as query clarity, while some features are relatively cheap such as the frequency in query logs.",0,,False
410,"Both of these steps can be optimized if large scale query logs are available. We can limit the reformulated queries to those appearing in query logs. In this way, instead of generating queries, we simply search the query logs, which can be efficiently implemented using the index. Also, all query features can be precomputed, which speeds up the query feature extraction.",1,ad,True
411,"In terms of the efficiency of retrieval, Eq. 1 shows that the retrieval score of each reformulated query (sc(qr, D)) is required. At first glance, this appears to be inefficient, since we need to run multiple queries. However, this can be easily optimized. Eq. 7 shows that sc(qr, D) consists of the scores of the words and bigrams in qr. Since all the reformulated queries in the reformulation tree are generated from the same original query, they share many words and bigrams. Thus, the scores of these words and bigrams can be reused by different reformulated queries. For example, the words and bigrams in the subset queries all come from the original query. Thus, we only need to calculate the scores for every word and bigram in the original query and then reuse these scores for each subset query. Further, although query substitutions may introduce new words and bigrams, the number of these new words and bigrams is limited. For example, Table 7 shows that query substitutions generated",0,,False
412,533,0,,False
413,from the top three subset queries are sufficient to achieve good retrieval performance.,0,,False
414,8. CONCLUSION,0,,False
415,"Complex queries pose a new challenge to search systems. In order to combine different query operations and model the relationships between the reformulated queries, a new query representation is proposed in this paper, where the original query is transformed into a reformulation tree. A specific implementation is described for verbose queries, which combines subset query selection and query substitution within a principled framework. In the future, this query representation will be applied to other search tasks involving complex queries such as the cross-lingual retrieval and diversifying the search results.",0,,False
416,Acknowledgments,0,,False
417,"This work was supported in part by the Center for Intelligent Information Retrieval and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",0,,False
418,9. REFERENCES,0,,False
419,"[1] N. Balasubramanian, G. Kumaran, and V. Carvalho. Exploring reductions for long web queries. In SIGIR10, pages 571­578, 2010.",0,,False
420,"[2] M. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In SIGIR08, pages 491­498, Singapore, 2008.",0,,False
421,"[3] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In WSDM10, New York City, NY, 2010.",0,,False
422,"[4] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and S. Vigna. The query-flow graph: model and applications. In CIKM08, pages 609­618, 2008.",0,,False
423,"[5] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In ICML07, pages 129­136, 2007.",0,,False
424,"[6] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and discriminative model for query refinement. In SIGIR08, pages 379­386, Singapore, 2008.",0,,False
425,"[7] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In SPIRE04, pages 43­54, 2004.",0,,False
426,"[8] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar questions in large question and answer archives. In CIKM05, pages 84­90, 2005.",0,,False
427,"[9] V. Jijkoun and M. de Rijke. Retrieving answers from frequently asked questions pages on the web. In CIKM05, pages 76­83, 2005.",0,,False
428,"[10] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating query substitutions. In WWW06, pages 387­396, Ediburgh, Scotland, 2006.",1,ad,True
429,"[11] G. Kumaran and J. Allan. A case for shorter queries, and helping users creat them. In ACL07, pages 220­227, Rochester, New York, 2007.",0,,False
430,"[12] G. Kumaran and V. R. Carvalho. Reducing long queries using query quality predictors. In SIGIR09, pages 564­571, Boston, MA, 2009.",0,,False
431,"[13] L. Larkey. A patent search and classification system. In DL99, pages 179­187, Berkeley, CA, 1999.",0,,False
432,"[14] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR01, pages 120­127, New Orleans, LA, 2001.",0,,False
433,"[15] M. Lease. An improved Markov random field model for supporting verbose queries. In SIGIR09, pages 476­483, Boston, MA, 2009.",0,,False
434,"[16] M. Lease, J. Allan, and W. B. Croft. Regression rank: learning to meet the oppotunity of descriptive queries. In SIGIR05, pages 472­479, Salvador, Brazil, 2005.",1,ad,True
435,"[17] Q. Mei, K. Klinkner, R. Kumar, and A. Tomkins. An analysis framework for search sequences. In CIKM09, pages 1991­1994, 2009.",0,,False
436,"[18] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5):735­750, 2004.",0,,False
437,"[19] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In SIGIR05, pages 472­479, Salvador, Brazil, 2005.",1,ad,True
438,"[20] D. Metzler and W. B. Croft. Latent concept expansion using markov random fields. In SIGIR07, pages 311­318, Amsterdam, the Netherlands, 2007.",0,,False
439,"[21] F. Peng, N. Ahmed, X. Li, and Y. Lu. Context sensitive stemming for web search. In SIGIR07, pages 639­646, Amsterdam, the Netherlands, 2007.",0,,False
440,"[22] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR98, pages 275­281, Melbourne, Australia, 1998.",0,,False
441,"[23] M. F. Porter. An algorithm for suffix stripping. Program, 14(3):130­137, 1980.",0,,False
442,"[24] S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and Y. Liu. Statistical machine translation for query expansion in answer retrieval. In ACL07, pages 464­471, Prague, Czech Republic, June 2007. Association for Computational Linguistics.",0,,False
443,"[25] Y. Z. S. Cronen-Townsend and W. B. Croft. Predicting query performance. In SIGIR02, pages 299­306, 2002.",0,,False
444,"[26] X. Wang and C. Zhai. Mining term association patterns from search logs for effective query reformulation. In CIKM08, pages 479­488, Napa Valley, CA, 2008.",0,,False
445,"[27] X. Xue and W. B. Croft. Representing queries as distributions. In SIGIR10 Workshop on Query Representation and Understanding, pages 9­12, Geneva, Switzerland, 2010.",1,Query,True
446,"[28] X. Xue and W. B. Croft. Modeling subset distributions for verbose queries. In SIGIR11, pages 1133­1134, 2011.",0,,False
447,"[29] X. Xue, W. B. Croft, and D. A. Smith. Modeling reformulation using passage analysis. In CIKM10, pages 1497­1500, 2010.",0,,False
448,"[30] X. Xue, S. Huston, and W. B. Croft. Improving verbose queries using subset distribution. In CIKM10, pages 1059­1068, 2010.",0,,False
449,"[31] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In SIGIR01, pages 334­342, New Orleans, LA, 2001.",1,ad,True
450,534,0,,False
451,,0,,False

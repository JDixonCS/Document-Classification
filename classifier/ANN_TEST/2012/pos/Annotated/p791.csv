,sentence,label,data,regex
0,Category Hierarchy Maintenance: a Data-Driven Approach,0,,False
1,"Quan Yuan, Gao Cong, Aixin Sun, Chin-Yew Lin, Nadia Magnenat-Thalmann",1,ad,True
2,"School of Computer Engineering, Nanyang Technological University, Singapore 639798",0,,False
3,"{qyuan1@e., gaocong@, axsun@, nadiathalmann@}ntu.edu.sg",1,ad,True
4,"Microsoft Research Asia, Beijing, China 100080",0,,False
5,{cyl@microsoft.com},0,,False
6,ABSTRACT,0,,False
7,"Category hierarchies often evolve at a much slower pace than the documents reside in. With newly available documents kept adding into a hierarchy, new topics emerge and documents within the same category become less topically cohesive. In this paper, we propose a novel automatic approach to modifying a given category hierarchy by redistributing its documents into more topically cohesive categories. The modification is achieved with three operations (namely, sprout, merge, and assign) with reference to an auxiliary hierarchy for additional semantic information; the auxiliary hierarchy covers a similar set of topics as the hierarchy to be modified. Our user study shows that the modified category hierarchy is semantically meaningful. As an extrinsic evaluation, we conduct experiments on document classification using real data from Yahoo! Answers and AnswerBag hierarchies, and compare the classification accuracies obtained on the original and the modified hierarchies. Our experiments show that the proposed method achieves much larger classification accuracy improvement compared with several baseline methods for hierarchy modification.",1,ad,True
8,Categories and Subject Descriptors,0,,False
9,H.3.3 [Information Storage and Retrieval]: Information Filtering,0,,False
10,Keywords,0,,False
11,"Category Hierarchy, Hierarchy Maintenance, Classification",0,,False
12,1. INTRODUCTION,1,DUC,True
13,"With the exponential growth of textual information accessible, category hierarchy becomes one of the most widely-adopted and effective solutions in organizing large volume of documents. Hierarchy provides an organization of data by different levels of abstraction, in which each node (or category) represents a topic that is shared by the data in it. The connection between two nodes denotes supertype-subtype relation. Examples include Web directories provided by Yahoo! and Open Directory Project (ODP), hierarchies for community-based question-answering services by Yahoo! Answers (YA) and AnswerBag (AB), product hierarchies by online retailers like Amazon and eBay, as well as the hierarchies for news",1,ad,True
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.",1,ad,True
15,browsing at many news portal websites. Figure 1 shows a small portion of Yahoo! Answers hierarchy. Questions in the same category are usually relevant to the same topic.,1,Yahoo,True
16,"Hierarchy enables not only easy document browsing, but also searching of the documents within user defined categories or subtrees of categories. Additionally, hierarchy information can be utilized to enhance retrieval models to improve the search accuracy [4]. On the other hand, users' information needs can only be satisfied with the documents accessible through the hierarchy (but not the category hierarchy itself). That is, the usefulness of a hierarchy heavily relies on the effectiveness of the hierarchy in properly organizing the existing data, and more importantly accommodating the newly available data into the hierarchy. Given the fast growth of text data, continuously accommodating large volume of newly available text data into a hierarchy is nontrivial. Automatic text classification techniques are often employed for efficient categorization of newly available documents into category hierarchies. However, hierarchy often evolves at a much slower pace than its documents. Two major problems often arise after adding many documents into a hierarchy after some time.",1,ad,True
17,"· Structure Irrelevance. A category hierarchy may well reflect the topical distribution of its data at the time of construction. However, as new topics always emerge from the newly coming documents, there is no proper category in the hierarchy to accommodate these new documents, leading to putting these documents in less relevant categories. As the result, some categories contain less topically cohesive documents. Moreover, some categories become less discriminative with respect to the current data distribution. One example is the two categories Printers and Scanners in YA, for there emerged many questions about multi-functional devices which are related to both printers and scanners, leading to ambiguity between these two categories.",1,ad,True
18,"· Semantics Irrelevance. Semantics may change over time which calls for a better organization of the documents [17]. For instance, when creating the hierarchy, experts are more likely to put category Petroleum under Geography. However, after the disaster of BP Gulf Oil Spill, a lot of news articles in category Petroleum are about the responsibility of the Obama Administration. These documents have stronger connection to category Politics than Geography. It is therefore more reasonable to put these documents under Politics for better document organization.",0,,False
19,"These two problems not only hurt user experiences in accessing information through the hierarchy, but also result in poorer classification accuracy for the classifiers categorizing newly available documents because of the less topically cohesive categories [15].",0,,False
20,791,0,,False
21,,0,,False
22,,0,,False
23,Figure 1: Portion of Yahoo! Answers Hierarchy,1,Yahoo,True
24,"Consequently, the poorer classification accuracy further hurts user experience in browsing and searching documents through the hierarchy. This calls for category hierarchy maintenance, a task to modify the hierarchy to make it better reflect the topics of its documents, which in turn would improve the classification accuracy. Although a category hierarchy is relatively stable, many websites have modified or adjusted their hierarchies in the past. In May 2007, Yahoo! Answers added a new category Environment into her hierarchy, and added several categories like Facebook and Google under Internet later. By comparison, eBay adjusted her hierarchy more frequently, because there always emerge new types of items, like tablets and eReaders.",1,ad,True
25,"Hierarchy modification is nontrivial. Manual modification of category hierarchy is a tedious and difficult task, because it is hard to detect the semantic changes as well as the newly emerged topics. This motivates the data-driven automatic modification of a given hierarchy to cope with semantic changes and newly emerged topics. This is a challenging task because of at least two reasons, among others. First, the resultant modified category hierarchy (hereafter called modified hierarchy for short) should largely retain the semantics of the existing hierarchy and keep its category labels semantically meaningful. Second, the categories in the modified hierarchy shall demonstrate much higher topical cohesiveness, which in turn enables better classification accuracy in putting new documents into the modified hierarchy.",0,,False
26,"To the best of our knowledge, very few work has addressed the hierarchy modification problem (see Section 2). Tang et al. propose a novel approach to modifying the relations between categories aiming to improve the classification accuracy [17]. However, their proposed method does not change the leaf categories of the given hierarchy, and thus cannot solve the aforementioned problems. For example, the method may move the leaf category Petroleum to be child category of Politics. However, it is more reasonable to partition the documents in Petroleum into two categories: one being the child category of Geography, and the other child category of Politics. The method [17] fails to do so since it is unable to detect the newly emerged hidden topic ""Petroleum politics"".",1,ad,True
27,"In this paper, we propose a data-driven approach to modify a given hierarchy (also called as the original hierarchy) with reference to an auxiliary hierarchy using three operations (namely, sprout, merge, and assign). An auxiliary hierarchy is a category hierarchy that covers a similar set of topics as does the given hierarchy (e.g., the Yahoo! hierarchy and ODP can be used as auxiliary hierarchy to each other). Similar to the concept of bisociation [8], our approach discovers finer and more elaborate categories (also known as hidden topics) by projecting the documents in the given hierarchy to the auxiliary hierarchy. This operation, similar to a cross-product operation between the categories from the given hierarchy and the categories from the auxiliary hierarchy, is named sprout 1. The similar hidden topics are then merged to form new",1,Yahoo,True
28,1We would like to thank an anonymous reviewer for suggesting the connection with bisociation [8] and the name sprout,0,,False
29,"categories in the modified hierarchy. The assign operation rebuilds the parent-child relations in the modified hierarchy. The category labels in the modified hierarchy are either borrowed from or generated based on both the original and the auxiliary hierarchies. We emphasize that the reuse of category labels from original and auxiliary hierarchies largely ensures semantically meaningful category labels in the modified hierarchy. When such an auxiliary hierarchy is unavailable, the given hierarchy can be used as an auxiliary hierarchy. Because of the three operations (i.e., sprout, merged, and assign), we name our approach the SMA approach. The main contributions are summarized as follows.",0,,False
30,"1) We propose a novel data-driven approach SMA to automatically modify a category hierarchy making it better reflect the topics of its documents. The proposed approach exploits the semantics of the given hierarchy and an auxiliary hierarchy, to guide the modification of the given hierarchy.",0,,False
31,"2) We evaluate the proposed approach using data from three realworld hierarchies, Yahoo! Answers, Answerbag, and ODP. The user study shows that the modified hierarchy fits with the data better than the original one does. As we argue that the categories in the modified hierarchy are more topically cohesive compared to the original hierarchy, we employ text classification as an extrinsic evaluation. Our experimental results show that the classifiers trained on the modified hierarchy achieve much higher classification accuracy (measured by both macro-F1 and micro-F1), than the classifier built on the original hierarchy, or the classifiers modeled on the hierarchies generated by three baseline methods, including the state-of-the-art method in [17] and the hierarchy generation method in [2].",1,Yahoo,True
32,"The rest of this paper is organized as follows. Section 2 surveys the related work. We describe the research problem and overview the proposed approach in Section 3. The three operations are detailed in Section 4. The experimental evaluation and discussion of the results are presented in Section 5. Finally, we conclude this paper in Section 6.",0,,False
33,2. RELATED WORK,0,,False
34,"Hierarchy Generation. Hierarchy generation focuses on extracting a hierarchical structure from a set of categories, each containing a set of documents. The generation process can be either fully automatic [2, 5, 11] or semi-automatic [1, 7, 22]. The semi-automatic approaches involve interaction with domain experts in the hierarchy generation process. In the following, we review the fully automatic approaches in more detail.",0,,False
35,Aggarwal et al. use the category labels of documents to supervise hierarchy generation [2]. They first calculate the centroids of all categories and use them as the initial seeds. Similar categories are merged and clusters with few documents are discarded. The process is iterated to build the hierarchy. User study is employed to evaluate the quality of the generated hierarchy.,0,,False
36,"Punera et al. utilizes a divisive hierarchical clustering approach, which first splits the given set of categories into two sets of categories, and each such set is partitioned recursively until it contains only one category [11].",0,,False
37,"An algorithm for generating hierarchy for short text is proposed by Chuang et al. [5]. They first create a binary-tree hierarchy by hierarchical agglomerative clustering, and then construct a multiway-tree hierarchy from the binary-tree hierarchy. They use both classification measurement and user evaluation to evaluate the generated hierarchy.",0,,False
38,"Recently, Qi et al. employ genetic algorithms to generate hierarchy [12]. Given a set of leaf categories, a group of hierarchies are randomly generated as seeds, and genetic operators are applied",0,,False
39,792,0,,False
40,to each hierarchy to generate new ones. The newly generated hierarchies are evaluated and the hierarchies with poor classification accuracy are removed. The process is repeated until the classification accuracy is not improved.,0,,False
41,"Different from hierarchy generation which assumes a set of categories as input, our hierarchy modification method takes a hierarchy as the input. Hierarchy generation does not change the given categories hence it cannot solve the structure irrelevance problem.",0,,False
42,"Hierarchy Modification. Tang et al. present a method of modifying a hierarchy to improve the classification accuracy [17]. The method introduces three operations. The promote operation lifts a category to upper level; the merge operation generates a new parent for a category and its most similar sibling; the demote operation either demotes a category as a child of its most similar sibling, or makes the sibling a child of the category. For each category in the given hierarchy, promote operation is tested, followed by merge and demote operations, in a top-down manner. The operation comes into effect if it can improve the classification accuracy. The approach iterates the process until no improvement can be observed or some criterion is met. In experiments, this method outperforms clustering-based hierarchy generation method in terms of classification accuracy. However, this method does not change the leaf categories, leaving the topically incohesive leaf categories untouched. In addition, the method [17] has a high time-complexity. Due to the high time complexity of the method [17], Kiyoshi et al. propose an approach [10] to addressing the efficiency issue.",1,ad,True
43,"Discussion. With the existing work on either hierarchy generation or hierarchy modification, the leaf categories in the modified hierarchy (i.e., either generated or modified) remain unchanged. Clearly, without changing leaf categories, the topical incohesiveness among documents in the same leaf category remains unaddressed. Consequently, the likely poorer classification accuracy for these topically incohesive categories results in poorer document organization in the hierarchy. In this paper, we therefore propose an automatic approach to modify a given hierarchy where the leaf categories could be split or merged so as to better reflect the topics of the documents in the hierarchy.",1,ad,True
44,3. SMA APPROACH OVERVIEW,1,AP,True
45,"We observe that each category in a hierarchy may contain several ""hidden topics"", each of which is topically cohesive, e.g., category Computer contains hidden topics like Internet Programming, Operating Systems, etc. With more documents adding to a category hierarchy, new ""hidden topics"" emerge within a single category leading to topical incohesiveness among its documents (see Section 1). Our proposed approach, therefore, aims to find the hidden topics within each category and then sprout categories based on its hidden topics, merge similar hidden topics to form new categories, and then assign the parent-child relation among categories. We name our approach SMA after its three major operations.",1,ad,True
46,"The key challenges in the approach include: (i) How to detect the ""hidden topics"" at the appropriate granularity? (ii) How to evaluate the similarity between ""hidden topics""? and (iii) How to assign the parent-child relation between the unmodified and modified categories? Further, recall from Section 1, the modified hierarchy has to largely retain the semantics of the existing hierarchy, with meaningful category labels and topically cohesive categories. In the following, we give a high-level overview of the SMA approach and then detail the three major operations in the next Section.",0,,False
47,"The framework of our SMA algorithm is illustrated in both Figure 2 and Algorithm 1, where Hc is the category hierarchy to be modified, Hn is the modified hierarchy, Hn is the intermediate hi-",0,,False
48, ,0,,False
49,Figure 2: Overview of SMA,0,,False
50,Algorithm 1: SMA algorithm for hierarchy modification,0,,False
51,Input: Hc: category hierarchy to be modified Ha: auxiliary hierarchy  : minimum coverage ratio  : maximum loss ratio,0,,False
52,Output: Hn: modified category hierarchy,0,,False
53,1 Hn  Hc;,0,,False
54,2 h  number of levels of Hc;,0,,False
55,3 foreach Level from 2 to h of Hn do,0,,False
56,4 foreach Category Ci of Hn on level do,0,,False
57,5,0,,False
58,"Ci  ProjectedCategories(Ci, Ha,  ,  );",0,,False
59,6,0,,False
60,"Sprout(Ci, Ci , Hn, Ha);",0,,False
61,7 n  number of categories on level of Hc;,0,,False
62,"8 Merge( , n , Hn); 9 Assign( , Hn);",0,,False
63,"10 HnRelabel(Hc, Ha, Hn); 11 return Hn",0,,False
64,"erarchy during the modification process, and Ha is the auxiliary hierarchy.",0,,False
65,"Auxiliary Hierarchy. Briefly introduced in Section 1, an auxiliary hierarchy Ha is a hierarchy covering similar topics as the given hierarchy Hc. For example, Yahoo! hierarchy and ODP hierarchy can be auxiliary hierarchy to each other. Similarly, Yahoo! Answers and AnswerBag can be auxiliary hierarchy to each other.",1,Yahoo,True
66,"The auxiliary hierarchy Ha plays an essential role in finding hidden topics. Note that the hidden topics are not readily present in the auxiliary hierarchy, and our approach does not simply use the structure of auxiliary hierarchy as part of the modified hierarchy. Instead, they contain semantics from both the original hierarchy and the auxiliary hierarchy. We use the following example to illustrate. Suppose that the original hierarchy has two categories, Action movie and Comedy movie, and the auxiliary hierarchy contains two categories America and Asia. Our approach will find that Action movie has two hidden topics, namely American action movie and Asian active movie; Comedy movie also has two hidden topics, namely American comedy movie and Asian comedy movie.",1,ad,True
67,"The auxiliary hierarchy also plays an important role in guiding the merge operation, which is to merge similar hidden topics to generate the categories of the modified hierarchy. Continue with the earlier example, after merging the generated hidden topics, we may get new categories­American movie and Asian movie (if ""action vs. comedy"" is evaluated to be less discriminative compared with ""American vs. Asian""). The semantics of the hierarchy to be modified, together with the semantics of the auxiliary hierarchy, will be exploited to define the similarity between hidden topics.",0,,False
68,"Validated in our experiments (Section 5), our approach is equally applicable when the original hierarchy Hc is used as the auxiliary hierarchy to itself.",0,,False
69,"Algorithm Overview. Shown in Figure 2 and Algorithm 1, Hn is first initialized to Hc (line 1). In a top-down manner, the SMA algorithm modifies the hierarchy level by level. Note that the root",0,,False
70,793,0,,False
71,"category is the only category at level 1. Starting from level 2, for each category Ci in this level, the documents contained in Ci is projected to the auxiliary hierarchy Ha. A set of categories from Ha each of which contains a reasonable number of documents originally from Ci is identified to represent Ci's hidden topics (line 5). The two parameters, minimum coverage ratio  and maximum loss ratio  , adjust the number of hidden topics. New finer categories are then sprouted from Ci according to the hidden topics and the documents in category Ci are assigned to these finer categories (or hidden topics) (line 6). Given the expected number of categories n on level (line 7), the merge operation forms n number of new categories on level of the intermediate hierarchy Hn (line 8). If the current level is not the lowest level in the hierarchy, the parentchild relations between the modified categories and the unmodified categories on the next level are assigned (line 9). The last step in the SMA algorithm is to generate category labels with reference to both the original and auxiliary hierarchies (line 10).",1,ad,True
72,"4. SPROUT, MERGE, AND ASSIGN",0,,False
73,"We detail the three operations to address the challenges in the SMA framework (i.e., to identify hidden topics, evaluate the similarity between hidden topics, and assign the parent-child relation).",1,ad,True
74,4.1 Sprout Operation,0,,False
75,"The sprout operation first discovers the hidden topics for the documents in a category Ci and then sprouts the category. Without loss of generalization, a leaf category is represented by all documents belonging to the category; a non-leaf category is represented by all documents belonging to any of its descendent categories.",0,,False
76,4.1.1 Discovery of Hidden Topics,0,,False
77,"Ideally, for a category we find a set of its hidden topics, which are comprehensive and cohesive, and have no overlap. This is however a challenging task. We proceed to give an overview of the proposed method. Given a category Ci in the intermediate hierarchy Hn during the modification process (see Algorithm 1), we assign all its documents into the categories of the auxiliary hierarchy Ha, and get a set of candidate categories from Ha in a tree-structure. Each candidate category contains a number of documents from Ci. Then, with the consideration of both cohesion and separation, we select a set of categories from the tree as hidden topics. The selection process is modeled as an optimization problem. We now elaborate the details.",0,,False
78,"Document Projection. To assign documents from Ci to Ha, we represent a document by its word feature vector, and a category in Ha by its centroid vector. Based on cosine similarity between the document and the centroids, we recursively assign each document d  Ci to Ha from its root to a leaf category along a single path of categories. If a good number of documents from Ci are assigned to a category Ca in Ha, then the topic of Ca is relevant to Ci, and the semantics of Ca can be used to describe a hidden topic of Ci. Thus, multiple categories in Ca can be identified to describe all hidden topics of Ci. For example, large number of documents from category Programming assigned into two categories Security and Network in an auxiliary hierarchy, implies that Programming has two hidden topics: Network Programming and Security Programming. We have also tried to build a classifier on Ha to assign documents from Ci to Ha using Naive Bayes and support vector machine, respectively, and the set of generated hidden topics is almost the same.",0,,False
79,The process of assigning documents from a category Ci in Hn to categories in Ha is called projection. We denote the set of docu-,0,,False
80,"ments projected from category Ci to category Ca by (Ci  Ca). If Ca is a leaf category, then (Ci  Ca) denotes the set of documents from Ci that are projected into Ca; if Ca is a non-leaf category, then (Ci  Ca) denotes the set of documents projected into any of the descendent leaf categories of Ca in Ha.",0,,False
81,"Candidate Topic Tree. Based on the projection, we select categories from Ha to represent the hidden topics of Ci. A selected category can be either a leaf category or a non-leaf category. Before describing the selection process, we introduce the notions of major category and minor category. Let  denote the minimum coverage ratio parameter.",0,,False
82,Definition 1 (Major Category). A category Ca from Ha is a major category for category Ci if |(Ci  Ca)|/|Ci|   .,0,,False
83,Definition 2 (Minor Category). A category Ca from Ha is a minor category for category Ci if |(Ci  Ca)|/|Ci| <  .,0,,False
84, ,0,,False
85,  !,0,,False
86,"
#$ %

 


 


 
  


 
  
  






   '!(
!""'

'&#",0,,False
87,$% !,0,,False
88,Figure 3: Generate candidate topic tree for Ci using Ha,0,,False
89,"For example, suppose  ,"" 15%. As shown in Figure 3, category Ci is projected to the categories in Ha. In the left tree, in which each number besides a node represents the percentage of documents of Ci projected to the node, the nodes in dark color are major categories while the others are minor categories.""",0,,False
90,"Naturally, only the major categories are considered candidate categories to represent hidden topics of Ci because a good number of documents in Ci are projected into them. However, not all major categories need to be selected because of two reasons. First, let Cp be the parent of a major category Ca. By definition, the parent of a major category is also a major category. Selecting both Ca and Cp would lead to semantic overlap. Second, assume all Cp's other child categories are minor categories, but altogether those minor categories contain a large number of documents. Then selecting Ca but not Cp would lead to a significant loss of documents from Ci (hence semantic loss). We therefore define the notion of loss ratio.",1,ad,True
91,Definition 3 (Loss Ratio). The loss ratio of a leaf category is,0,,False
92,"defined as 0. For a non-leaf category Ca, let Ca be the set of minor",0,,False
93,categories among Ca's child categories. The loss ratio of Ca with,0,,False
94,"respect to Ci, the category being projected, is the ratio between",0,,False
95,the projected documents in all its child minor categories and Ca's,0,,False
96,"projected documents, i.e.,",0,,False
97,. C Ca |(CiC )|,0,,False
98,| (Ci Ca )|,0,,False
99,We set a threshold maximum loss ratio  . After projecting doc-,0,,False
100,"uments from Ci to categories in Ha, we only keep the major categories whose parent's loss ratio is smaller than  . Note that, if a",0,,False
101,"non-leaf category is not selected in the above process, the subtree",0,,False
102,"rooted at this category is not selected. After the selection, we obtain",0,,False
103,"a sub-hierarchy from Ha containing only eligible major categories,",0,,False
104,"which is called candidate topic tree for Ci, denoted by TCi . For example, suppose  , 30%. The candidate topic tree for Ci",0,,False
105,is shown on the right hand side of Figure 3. Although node C5 is,0,,False
106,"a major category, it is not part of the candidate topic tree since the",0,,False
107,"loss ratio ((10%+10%)/50% , 40%) of its parent node C3 is larger than  .",0,,False
108,794,0,,False
109,Hidden Topic Selection. We next present how to choose a set of,0,,False
110,"nodes from TCi to represent hidden topics of Ci. Ideally, we expect the hidden topics to be comprehensive but not overlap with each",0,,False
111,"other. Hence, we use tree-cut to define the selection [18].",0,,False
112,"Definition 4 (Tree-Cut). A tree-cut is a partition of a tree. It is a list of nodes in the tree, and each node represents a set of all leaf nodes in a subtree rooted by the node. The sets in a tree-cut exhaustively cover all leaf nodes of the tree, and they are mutually disjoint.",0,,False
113,"There exist many possible tree-cuts for TCi to generate hidden topics. Two example tree cuts for the candidate topic tree in Figure 3 are {C1, C2} and {C1, C3}. Among all possible tree-cuts, we aim to choose the tree-cut such that each resultant hidden topic (cate-",0,,False
114,gory) is cohesive and well separated from other categories in the,0,,False
115,"tree-cut. In the following, we prove that the tree-cut containing",0,,False
116,only leaf nodes of the candidate topic tree satisfies this require-,0,,False
117,"ment. Note that a leaf node in TCi is not necessary a leaf category in Ha. For example, in Figure 3, C3 is leaf node of the candidate topic tree, but not a leaf category in the auxiliary hierarchy.",0,,False
118,We proceed to show the proof. We first define the Sum of Square,0,,False
119,Error (SSE) of cohesion for a category Ca.,0,,False
120,"SSE(Ca) ,"" (d - ca)2,""",0,,False
121,dCa,0,,False
122,where d is a document and ca is the centroid of category Ca.,0,,False
123,"Given a set of categories {Ca} (1  a  k), the Total-SSE and",0,,False
124,"Total Sum of Square Between (Total-SSB), denoted by E and B",0,,False
125,"respectively, are E ,",0,,False
126,"k a,1",0,,False
127,SSE,0,,False
128,(Ca),0,,False
129,and,0,,False
130,B,0,,False
131,",",0,,False
132,"k a,1",0,,False
133,|Ca|(c,0,,False
134,-,0,,False
135,"ca)2,",0,,False
136,where c is the centroid of documents in all categories {Ca}. It is,0,,False
137,"verified that, given a set of documents, the sum of E and B is a",0,,False
138,"constant value [16]: E +B ,",0,,False
139,"k a,1",0,,False
140,"dCa (d-c)2. Thus, maximizing",0,,False
141,separation is equivalent to minimizing cohesion error. We therefore,0,,False
142,formulate the problem of selecting categories from TCi to represent hidden topics for category Ci as following:,0,,False
143,"Ci ,"" arg min SSE(Ca), where S is a tree-cut on TCi . (1)""",0,,False
144,S,0,,False
145,Ca S,0,,False
146,This problem can be reduced to the maximum flow problem by,0,,False
147,"viewing TCi as a flow network. Thus, it can be solved directly by Ford-Fulkerson method [6]. However, its complexity is relatively",0,,False
148,high. Note that we need to solve the optimization problem for every,0,,False
149,"category in the original hierarchy, and thus an efficient algorithm is",0,,False
150,essential.,0,,False
151,Lemma 1: The SSE of a category is not smaller than the Total-SSE,0,,False
152,"of its child categories. Proof: Suppose there is a category Cp with k child categories {Ci}ki,1.",0,,False
153,"For a child category Ci, the Sum of Square Distance (SSD) of its",0,,False
154,"data to a data point x is: SSD(Ci) , dCi (d - x)2. We get the mini-",0,,False
155,mum value when x,0,,False
156,",",0,,False
157,1 |Ci |,0,,False
158,dCi,0,,False
159,d,0,,False
160,",",0,,False
161,ci,0,,False
162,which,0,,False
163,let,0,,False
164,d dx,0,,False
165,"dCi (d-x)2 , 0.",0,,False
166,"Thus, when x is the mean of data in Ci (or ci), the SSD of Ci be-",0,,False
167,"comes SSE of Ci, and gets its minimum value. One step further, we",0,,False
168,have,0,,False
169,"(d - ci)2  (d - cp)2,",0,,False
170,dCi,0,,False
171,dCi,0,,False
172,where cp is the mean of data of Cp. This demonstrates that the SSE,0,,False
173,"of Ci is smaller than the SSD of data of Ci to the overall mean, and",0,,False
174,this result leads to,1,ad,True
175,k,0,,False
176,k,0,,False
177,(d - ci)2 ,0,,False
178,(d - cp)2.,0,,False
179,"i,1 dCi",0,,False
180,"i,1 dCi",0,,False
181,This demonstrates that the SSE of a category is not smaller than the,0,,False
182,Total-SSE of its child categories.,0,,False
183,Procedure ProjectedCategories,0,,False
184,Input: Ci: the category to be sprouted Ha: the auxiliary hierarchy  : minimum coverage ratio  : maximum loss ratio,0,,False
185,Result: Ci []: the list of projected categories for Ci,0,,False
186,1 Ci []  {root category of Ha}; 2 repeat,0,,False
187,3 Ca  Ci [].getNextUnprocessedCategory(); 4 C_List[]  child categories of Ca;,0,,False
188,5 M_List[]  {};,0,,False
189,6 foreach Category C of C_List[] do,0,,False
190,7,0,,False
191,if,0,,False
192,| (Ci C)| |Ci |,0,,False
193,then,0,,False
194,8,0,,False
195,M_List[].add(C);,1,ad,True
196,9,0,,False
197,if,0,,False
198,1-,0,,False
199,CM_List [] | (Ci C)| | (Ci Ca )|,0,,False
200,<,0,,False
201,then,0,,False
202,10,0,,False
203,Ci [].add(M_List[]);,1,ad,True
204,11,0,,False
205,Ci [].remove(Ca);,0,,False
206,12 else,0,,False
207,13,0,,False
208,mark Ca as processed,0,,False
209,14 until No more unprocessed category in Ci [] 15 return Ci [],0,,False
210,"Lemma 1 enables us to use an efficient method to solve Eq.1 as follows. According to Lemma 1, specializing a category by its child categories can reduce Total-SSE. That is, among all possible tree-cuts in TCi , the cut that contains only leaf categories has the minimum value of Total-SSE.",0,,False
211,"In summary, for a category Ci in Hn, we build a candidate topic tree TCi and the leaf nodes of TCi are used to represent the hidden topics of Ci. The pseudocode is given in Procedure 2 ProjectedCategories. As discussed, according to Lemma 1, we only need the leaf nodes of the candidate topic tree TCi as the result. Instead of explicitly building TCi and then finding TCi 's tree cut containing only leaf nodes, we find TCi 's leaf nodes directly in our procedure. More specifically, we start from the root category of Ha in a topdown manner (the root node is a major category by definition as its coverage ratio is 1). Each time we get a unprocessed categories Ca from the list of projected categories Ci [], and check its child categories (lines 3-4). The major categories among the child categories are put into a major category list (lines 6-8) for further testing on loss ratio. If the loss ratio of Ca is smaller than maximum loss ratio  , then Ca is replaced by its child major categories (lines 9-11); otherwise, Ca is selected as a candidate category (line 13). We iterate the process until all major categories are processed (line 14).",1,ad,True
212,4.1.2 Sprout Category,0,,False
213,"For a category Ci of Hn, we sprout it based on the projected categories Ci returned by Procedure ProjectedCategories. Recall that each of the projected category Ca  Ci represents a hidden topic of Ci and contains a good number of documents projected from Ci, i.e., (Ci  Ca). We sprout Ci with |Ci | number of categories. However, not all documents from Ci are contained in all these newly sprouted categories, i.e., CaCi |(Ci  Ca)|  |Ci|. For those documents in Ci but not contained in any of the newly sprouted categories, we assign them to their nearest sprouted categories. As the result, each document in Ci is now contained in one and only one sprouted category of Ci.",0,,False
214,795,0,,False
215,$X[LOLDU\ KLHUDUFK\,0,,False
216,&RPSXWHU,0,,False
217,6HFXULW\,0,,False
218,1HWZRUN,0,,False
219,3URWRFRO,0,,False
220,0RGLI\LQJ &RPSXWHU KLHUDUFK\,0,,False
221,&DEOH  +LGGHQ WRSLFV,0,,False
222,1HWZRUN,0,,False
223,3URJUDPPLQJ,0,,False
224, 6SOLW FDWHJRU\  0HUJH FDWHJRU\,0,,False
225,&RPSXWHU,0,,False
226,1HWZRUN 6HFXULW\,0,,False
227,1HWZRUN 1HWZRUN,0,,False
228,6HFXULW\,0,,False
229,3URWRFRO,0,,False
230,3URWRFRO &DEOH 3URJUDPPLQJ 3URJUDPPLQJ,0,,False
231,1HWZRUN 6HFXULW\ 6HFXULW\ 3URJUDPPLQJ,0,,False
232,&RPSXWHU,0,,False
233,1HWZRUN &DEOH,0,,False
234,1HWZRUN 3URWRFRO 3URWRFRO 3URJUDPPLQJ,0,,False
235,:$1,0,,False
236,:$1 6HFXULW\,0,,False
237, $VVLJQ FDWHJRU\ UHODWLRQ,0,,False
238,:$1 3URWRFRO 0RGLILHG KLHUDUFK\ EHIRUH UHODEHO,0,,False
239,"Figure 4: SMA operations by example. The hidden topics and sprouted categories for a category of the original hierarchy are in the same color. The Network and Programming categories have 3 and 2 hidden topics, respectively, leading to 5 sibling sprouted categories. These 5 categories are merged into 3 categories and the category WAN is reassigned to 2 of the merged categories.",1,ad,True
240,"Example 4.1: Shown in Figure 42, suppose after applying procedure ProjectedCategories, we find Network is projected to Security, Protocol, and Cable. According to the three hidden topics, we sprout Network into three categories Network Security, Network Protocol, and Network Cable.",0,,False
241,"The sprout operation may be reminiscent of the work on hierarchy integration, aiming to integrate a category from a source hierarchy into similar categories in the target hierarchy, which has a different purpose from our mapping. Most of proposals (e.g., [3, 21]) on hierarchy integration employ a hierarchical classifier built on the target hierarchy to classify each document in the source hierarchy into a leaf node of the target hierarchy, which is too fine a granularity to represent hidden topics as in our approach. Frameworks that can map a category to categories on proper levels in target hierarchy are proposed (e.g., [19]). However, they do not take the cohesion and separation between mapping categories into account, which are essential to find good hidden topics in our approach. Thus, they cannot be applied to our work.",0,,False
242,4.2 Merge Operation,0,,False
243,"The sprout operation in Section 4.1 generates a set of sprouted categories, each representing a hidden topic. The merge operation aims to combine the newly sprouted categories with similar hidden topics.",0,,False
244,"Suppose we are now working on level of the intermediate modified hierarchy Hn and we have a set of sprouted categories originated from the categories on level . Our task is to merge some of these sprouted categories such that the number of resultant categories on level is the same as before (i.e., n ). Note that the number of resultant categories can also be specified by users. To ease the presentation, the modified hierarchy has the same size as the given hierarchy in our discussion.",0,,False
245,"During merge, we need to consider an important constraint -- we can only merge categories under the same parent category. Thus, existing clustering algorithms need to be modified to accommodate such a constraint. Another key issue here is how to define the similarity between two sprouted categories by considering their semantics enclosed in Hc and Ha. In the following, we first define a similarity measure and then describe our merge method.",0,,False
246,We consider two aspects when defining the similarity for a pair of sprouted categories C1 and C2 on level : (i) the distribution of their,0,,False
247,"2For clarity, we recommend viewing all diagrams in this paper from a color copy.",0,,False
248,"documents over categories of Hc and Ha, and (ii) the similarity",0,,False
249,"between the categories within Hc and Ha, respectively.",0,,False
250,Let Lc be the set of categories on level in the original hierarch,0,,False
251,"Hc, Ls be the set of categories sprouted from Lc, and La be the",0,,False
252,set of projected categories in auxiliary hierarchy representing the,0,,False
253,"hidden topics of the categories in Lc. That is, La ,"" CiLc Ci . For a sprouted category Cs  Ls, its document distribution over""",0,,False
254,Lc is defined to be the ratios of its documents in each of the cate-,0,,False
255,"gories in Lc. That is, the document distribution of Cs can be mod-",0,,False
256,eled as a |Lc|-dimensional vector vcs. The j-th element of vcs is,0,,False
257,|CsC j | |Cs |,0,,False
258,"(i.e.,",0,,False
259,the,0,,False
260,portion,0,,False
261,of Cs's,0,,False
262,documents,0,,False
263,also,0,,False
264,contained,0,,False
265,"in Cj),",0,,False
266,"where Cj is the j-th category of Lc. Similarly, we get the data dis-",0,,False
267,tribution vector vas for Cs over La based on the ratio of documents,0,,False
268,in Cs projected to each of the categories in La. Because vcs and,0,,False
269,vas usually have different dimensionality for different sprouted cat-,0,,False
270,"egory Cs's, we extend vcs to be |Hc|-dimensional (each category is",0,,False
271,one dimension) by filling up zeros for corresponding categories in,0,,False
272,"Hc but not in Lc. Similarly vas is extended to be |Ha|-dimensional. We use two matrices Mc, Ma to represent the similarity between",0,,False
273,"categories of Hc and Ha, respectively. Mc is a |Hc|- by-|Hc| matrix and Ma is a |Ha|-by-|Ha| matrix. Each element mi j in a matrix",0,,False
274,represents the similarity in the corresponding hierarchy between,0,,False
275,"a pair of categories Ci and Cj, which is defined by Inverted Path",0,,False
276,Length [14]:,0,,False
277,mi j,0,,False
278,",",0,,False
279,"1 1+path(Ci,C j )",0,,False
280,",",0,,False
281,where,0,,False
282,"path(Ci, Cj) is",0,,False
283,the length,0,,False
284,of,0,,False
285,path between Ci and Cj in the hierarchy.,0,,False
286,Considering both document distribution and structural similar-,0,,False
287,"ity from the two hierarchies, the similarity between two sprouted",0,,False
288,categories C1 and C2 on level of Hn is defined as:,0,,False
289,"Sim(C1, C2) , (vTc1 · Mc · vc2) + (vTa1 · Ma · va2).",0,,False
290,"This similarity definition considers both the similarity estimated based on Hc and the similarity estimated based on Ha. With similarity between two sprouted categories defined, we proceed to detail the merge operation.",0,,False
291,"We first explain the notion of sibling sprouted category through an example. Let Ci1 and Ci2 be the two categories sprouted from category Ci, and Cj1 and Cj2 be the two categories sprouted from Cj. If Ci and Cj in Hm are both children of category Cp, then naturally, all the newly sprouted categories Ci1, Ci2, Cj1, Cj2 are children of Cp. These four example categories are known as sibling sprouted categories. All the five sprouted categories shown in Figure 4 are sibling sprouted categories.",0,,False
292,"The merge operation is as follows. We first calculate the similarity between sibling sprouted categories on level . Then, we pick up the pair of categories with the largest similarity, and merge them into a category, and recompute its similarities with its sibling sprouted categories. The process iterates until the number of remaining categories on equals n , the number of categories on level of the original hierarchy Hc. When all the sibling sprouted categories under the same parent node are merged into a single category, we shrink the single category into its parent node. Note that we cannot merge two sprouted categories on level if they have different parent node.",0,,False
293,"Example 4.2: Recall Example 4.1. We sprout Network into three categories Network Security, Network Protocol, and Network Cable. Suppose there is another category Programming on the same level of Network and sprouted into Security Programming, and Protocol Programming (see Figure 4). Based on the similarity, Network Security and Security Programming are merged together (both are about the Security topic), and Network Protocol and Protocol Programming are merged to generate a new category about protocol.",0,,False
294,796,0,,False
295,4.3 Assign Operation,0,,False
296,"After modifying categories (by sprout and merge) on level of H n, the original parent-child relations between the categories on level and the categories on next level + 1 do not hold any longer. Hence we need to reassign the parent-child relation.",0,,False
297,"Based on the fact that a non-leaf category of a hierarchy subsumes the data of all its descendants, we rebuild the children for each of the modified categories on by checking document containment. If the documents of a category on level are also contained in a category on level + 1, then the latter category is assigned to be the children of the former category. In other words, for each category C on , we calculate the intersection of documents between C and the categories on + 1. The intersections form new children for category C. Because one category has only one parent category, if a category has intersections with more than one category on level , the category will be split into multiple categories, each containing the intersection with one category on level .",0,,False
298,"Example 4.3: Recall Example 4.2. Suppose WAN was a child category of Network before sprout (see Figure 4). After sprout and merge, Network no longer exists and WAN lost its parent. We compare the documents of WAN and the two newly formed categories after merge (i.e., Network Security & Security Programming and Network Protocol & Protocol Programming). If WAN has overlap with both categories, then WAN have two hidden topics (about security and protocol). Thus, we divide WAN into two categories and assign them to different parent nodes, shown in Figure 4.",0,,False
299,4.4 Relabel,0,,False
300,"Unlike most of previous work, our approach is able to automatically generate readable labels for every modified category. By projecting documents from Hn to Ha, we can consider the three hierarchies Hn, Hc and Ha, which contain the same set of data. For every document in Hn, we trace its labels in Hc and Ha, and use them together as the label of the document; the semantic of the new label is the intersection of semantics of its two component labels. We then aggregate such labels for all documents in a category of Hn, and use them as candidate labels for the category. The candidate labels for a category are ranked according to the proportion of documents in their corresponding original categories from Hc and Ha. In this paper, the top-1 ranked label is chosen.",1,ad,True
301,"The labels generated in this way are mostly readable and semantically meaningful, as reflected in our user study (see Section 5.3) and case study (see Section 5.4). Nevertheless, a manual verification of the labels for the newly generated categories can be employed when the proposed technique is used in real applications.",1,ad,True
302,5. EXPERIMENTS,0,,False
303,"We designed two sets of experiments. The first set of experiment, similar to that in [17], is to evaluate whether the modified hierarchy improves the classification accuracy. Discussed in Section 1, if a category hierarchy better reflects the topics of its contained documents and each category in the hierarchy is topically cohesive, then better classification accuracy is expected than that on a hierarchy with less topically cohesive categories. The second set of experiments employs a user study to manually evaluate the semantic quality of the modified hierarchy following the settings in [2,5]. Finally, we report a case study comparing a part of Yahoo! Answers hierarchy with its modified hierarchy.",1,Yahoo,True
304,5.1 Data Set,0,,False
305,"We use data from three real-world hierarchies: Yahoo! Answers, AnswerBag, Open Directory Project, denoted by HYA, HAB and",1,Yahoo,True
306,Table 1: Statistics of the three hierarchies,0,,False
307,Hierarchy,0,,False
308,HYA,0,,False
309,HAB,0,,False
310,HODP,1,ODP,True
311,Number of documents,0,,False
312,"421,163 148,822 203,448",0,,False
313,Number of leaf nodes,0,,False
314,75,0,,False
315,195,0,,False
316,460,0,,False
317,Number of non-leaf nodes 40,0,,False
318,70,0,,False
319,98,0,,False
320,Height,0,,False
321,4,0,,False
322,5,0,,False
323,4,0,,False
324,"HODP, respectively. Since the modified category hierarchy contains a different set of leaf nodes, the labels for documents given in the original dataset do not stand in modified hierarchy. Manual annotation of documents in the modified hierarchy is therefore unavoidable. To make the annotation manageable, we selected the documents from two major topics Sports and Computers from these three hierarchies (because the annotators are familiar with both topics). Nevertheless, the number of documents in the two major topics in the three hierarchies ranges from 148,822 to 421,163, and the number of categories ranges from 115 to 558. These numbers are large enough for a valid evaluation. Table 1 reports the statistics on the three hierarchies.",1,ODP,True
325,"HYA: Obtained from the Yahoo! Webscope datatset3, HYA contains 421,163 documents (or questions) from Sports and Computers & Internet categories.",1,Yahoo,True
326,"HAB: We collected 148,822 questions from Recreation & Sports and Computers categories from AnswerBag to form HAB. Categories with fewer than 100 questions are pruned and all affected questions are moved to their parent categories.",0,,False
327,"HODP: The set of 203,448 documents from Sports and Computers categories are collected4 in HODP. Categories containing fewer than 15 documents or located on level 5 or deeper are removed in our experiments.",1,ODP,True
328,The preprocessing of the documents in all three hierarchies includes stopword removal and stemming. Terms occurred no more than 3 times across the datasets are also removed.,0,,False
329,5.2 Evaluation by Classification,0,,False
330,"The proposed SMA algorithm modifies a category hierarchy to better reflect the topics of its documents, which in turn should improve the classification performance. Following the experimental setting in [17], we evaluate the effectiveness of hierarchy modification by comparing the classification accuracies obtained by the same hierarchical classification model applied on the original category hierarchy and the modified hierarchy, respectively.",0,,False
331,"Another three methods for hierarchy modification are employed as the baselines, namely, Bottom Up Clustering (BUC), Hierarchical Acclimatization (HA) [17], and Supervised Clustering (SC) [2]. Table 2 gives a summarized comparison of the three baselines with the proposed SMA, and Section 5.2.1 briefs the baseline methods.",0,,False
332,"The modified hierarchies by all the methods evaluated in this paper have the same size as the original hierarchy (i.e., same number of levels, and same number of categories in each level). For each hierarchy modification method, we evaluate the percentage of classification accuracy increment obtained by the same classification model (e.g., Support Vector Machine) on the modified hierarchy over the original hierarchy. The classification accuracy is measured by both micro-average F1 (Micro-F1) and macro-averaged F1 (Macro-F1) [20]. The former gives equal weight to every document while the latter weighs categories equally regardless the number of documents in each category.",0,,False
333,"We remark that this is a fair evaluation for all the methods, each generating a hierarchy with the same size as that of the original",0,,False
334,3Available at http://research.yahoo.com/Academic_Relations. 4Available at http://www.dmoz.org/rdf.html.,1,ad,True
335,797,0,,False
336,Table 2: Comparison of baseline methods with SMA,0,,False
337,Aspect/Methods Utilize original hierarchy Change leaf category,0,,False
338,BUC HA[17] SC [2],0,,False
339,×,0,,False
340,×,0,,False
341,×,0,,False
342,×,0,,False
343,SMA ,0,,False
344,Utilize auxiliary hierarchy ×,0,,False
345,×,0,,False
346,× Optional,0,,False
347,"hierarchy, where the same classification method is applied to the modified hierarchies to evaluate the improvement of each modified hierarchy over the original one in terms of classification accuracy.",0,,False
348,5.2.1 Baseline Methods,0,,False
349,"Baseline 1: Bottom Up Clustering (BUC). In this method, each leaf category is represented by the mean vector of its contained documents. The categories are then clustered in a bottom-up manner using K-means to form a hierarchy.",0,,False
350,"Baseline 2: Hierarchical Acclimatization (HA). The HA algorithm is reviewed in Section 2, In simple words, it employs promote, demote and merge operations to adjust the internal structure, but leaves the leaf nodes unchanged [17].",1,ad,True
351,"Baseline 3: Supervised Clustering (SC). Given a set of documents with labels, SC first calculates the mean vector of each category as the initial centroid and then reassigns the documents to the categories based on the cosine similarity with their centroids. Then, similar categories are merged and minor categories are removed. These procedures are repeated, and during each iteration, a constant portion of features with smallest term-frequencies are set to zero (projected out) [2]. The process stops when the number of features left is smaller than a pre-defined threshold. This method cannot generate category labels. We take the most frequent words in a category to name it.",0,,False
352,5.2.2 Experiments on Yahoo! Answers,1,Yahoo,True
353,"From the data of HYA, we randomly selected 500 questions as test data (used for classification evaluation with manual annotations). To evaluate the possible improvement in classification accuracy, the same set of test documents are classified on the original (or unmodified) HYA, and the modified HnYA's. Two classifiers, multinominal Naive Bayes (NB) and Support Vector Machine (SVM) classifiers are used as base classifiers for hierarchical classification. We build Single Path Hierarchical Classifier (SPH) [9] as it performs better than other hierarchical classification methods for question classification according to the evaluation [13]. In the training phase of SPH, for each internal node of the category tree, SPH trains a classifier using the documents belonging to its descendent nodes. In the testing phase, a test document is classified from the root to a leaf node in the hierarchy along a single path.",0,,False
354,"SMA Settings. Recall that SMA uses auxiliary hierarchy in the modification process. We evaluated SMA with three settings, to modify HYA using HYA, HAB, and HODP as auxiliary hierarchy, respectively. The three settings are denoted by SMAYA|YA, SMAYA|AB, and SMAYA|ODP, respectively. The first setting is to evaluate the effectiveness of using the original hierarchy as auxiliary hierarchy, and the last two are to evaluate the effectiveness of using external hierarchies.",1,ODP,True
355,"Parameter Setting. Before evaluating the test documents on the modified hierarchies, we set the parameters required by SMA for hierarchy modification. Recall that SMA requires two parameters: minimum coverage ratio  and maximum loss ratio  . Usually parameters are set using a development set or through crossvalidation. In our case, however, there is no ground truth on how",0,,False
356,good a modified hierarchy is and manual assessment of every modified hierarchy for parameter tuning is impractical. We therefore adopt a bootstrapping like approach described below.,1,ad,True
357,"After the test data selected, the remaining data is used for hierarchy modification. We split the remaining data of HYA into 3 parts: P1, P2, P3, and the proportion of their sizes is 12:3:1. Using P1 for HYA and a given auxiliary hierarchy, we obtain a modified hierarchy HnYA. Naturally, all documents in P1 have category labels from hierarchy HnYA. We then build a classifier using all documents in P1 and their labels from HnYA. The classifier classifies documents in P2 and P3. Assume that the classifier gives reasonably good classification accuracy, then all documents in P2 and P3 have their category labels assigned according to HnYA. With these labels, we can evaluate the classification accuracy of documents in P3 by the classifier built using P2 on HnYA. Intuitively, if a hierarchy H1 better organizes documents than another hierarchy H2, then the classifier trained on H1 is expected to have higher classification accuracy for P3 than a classifier built on H2. We then select the parameters leading to the best classification accuracy for P3. In our experiments, the parameters (i.e.,  and  ) set for SMAYA|YA, SMAYA|AB, and SMAYA|ODP are (0.29 and 0.11), (0.17 and 0.17), (0.38 and 0.08), respectively.",1,ad,True
358,"Test Data Annotation. With the chosen parameters, each SMA setting generated a modified hierarchy using P1 as HYA and its corresponding auxiliary hierarchy. The preselected 500 test questions are used as test data to fairly evaluate the modified hierarchies by the three SMA settings and the baseline methods. Recall that the 500 questions are not included in the three parts (P1, P2, and P3) for parameter setting. Because BUC and HA do not change the leaf categories, the original labels of the 500 questions remain applicable. For SC and SMA, both changing leaf categories, we invited two annotators to label the 500 questions to their most relevant leaf categories in the modified hierarchies. We synthesized the results of the annotators, and assigned the labels for questions. If two annotators conflicted about a label, a third person made the final judgment. The dataset and their annotations are available online 5.",1,ad,True
359,"Classification Results. Classification accuracy measured by MicroF1 using NB and SVM as base classifiers for the six methods (i.e., three baselines BUC, HA, SC, and the three SMA settings) on modified hierarchies is reported in Figure 5(a). For comparison, the classification accuracy on the original (or unmodified) Yahoo! Answers hierarchy is also reported under column named HYA. Figure 5(b) reports Macro-F1.",1,Yahoo,True
360,"As shown in Figures 5(a) and 5(b), all the three settings of SMA achieve significant improvement over the results obtained on the original hierarchy. For example, using NB as the base classifier, SMAYA|YA improves Micro-F1 over the results on the original hierarchy by 41.0% and improves Macro-F1 by 40.3%. NB achieves better accuracy than SVM probably because NB was used as the base classifier for parameter setting. The three baseline modification methods only slightly improve the classification accuracy over the original hierarchy and even deteriorate the accuracy in some cases. Recall that SC and SMA modify leaf categories while BUC and HA only modify internal structures of hierarchy without changing leaf categories. All the methods that change leaf categories outperform the methods that keep leaf categories unchanged. Note that SMAYA|YA significantly outperforms SC. One possible reason could be that SMAYA|YA utilizes the semantics of the original hierarchy in hierarchy modification while SC does not.",0,,False
361,We observe that the auxiliary hierarchies employed by SMA have effect on the classification accuracy of the modified hierarchy.,0,,False
362,5 http://www.ntu.edu.sg/home/gaocong/datacode.htm,0,,False
363,798,0,,False
364,MicroF1,0,,False
365,0.9 NB,0,,False
366,0.85 SVM 0.8,0,,False
367,0.75 0.7,0,,False
368,0.65 0.6,0,,False
369,0.55 0.5 HYA,0,,False
370,0.8 NB,0,,False
371,0.75 SVM 0.7,0,,False
372,0.65 0.6,0,,False
373,0.55 0.5,0,,False
374,0.45 0.4 HYA,0,,False
375,BUC,0,,False
376,HA,0,,False
377,SC,0,,False
378,SMAYA|YA SMAYA|AB SMAYA|ODP,1,ODP,True
379,(a) Micro-F1,0,,False
380,BUC,0,,False
381,HA,0,,False
382,SC,0,,False
383,SMAYA|YA SMAYA|AB SMAYA|ODP,1,ODP,True
384,(b) Macro-F1,0,,False
385,MacroF1,0,,False
386,"Figure 5: Micro-F1 and Macro-F1 on the modified hierarchies by three baselines and three SMA settings, and on the original Yahoo! Answers hierarchy.",1,Yahoo,True
387,Table 3: Classification accuracy on modifying AnswerBag,0,,False
388,Measure/Hierarchy HAB SMAAB|YA Improvement(%),0,,False
389,Macro-F1 (NB),0,,False
390,0.3444 0.5638,0,,False
391,63.7%,0,,False
392,Macro-F1 (SVM) 0.3691 0.4933,0,,False
393,33.6%,0,,False
394,Micro-F1 (NB),0,,False
395,0.4671 0.6669,0,,False
396,42.8%,0,,False
397,Micro-F1 (SVM) 0.4371 0.5697,0,,False
398,30.3%,0,,False
399,"Measured by Macro-F1, SMAYA|YA without using an external hierarchy slightly outperforms its counterpart SMAYA|AB or SMAYA|ODP, which uses an external hierarchy; while in terms of Micro-F1, SMAYA|YA performs worse than do its counterparts.",1,ODP,True
400,5.2.3 Experiments on AnswerBag,0,,False
401,"In this set of experiments, SMA is used to modify the AnswerBag hierarchy. The main purpose is to evaluate whether SMA remains effective when the size of the auxiliary hierarchy is smaller than the one to be modified. Specifically, we use HYA as auxiliary hierarchy to modify HAB and evaluate the classification accuracy as we did in the earlier set of experiments. Note that the HAB has 265 categories which is more than twice of the 115 categories contained in HYA. The parameters  and  were set as 0.17 and 0.08, respectively, using the parameter setting approach described earlier. The classification accuracy is reported in Table 3. Observe that SMAAB|YA improves the classification accuracy (Macro- and Micro-F1) by 30% to 63%, compared with the result obtained before hierarchy modification. This demonstrates that the proposed SMA approach is effective for different hierarchies, even if the size of the auxiliary hierarchy is smaller than the hierarchy to be modified.",0,,False
402,5.3 User Study,0,,False
403,"A good category hierarchy must be semantically meaningful: (i) Its category labels should be easy to understand, facilitating data browsing; and (ii) Its category structure should reflect the topics of its data. We would like to note that it is challenging to evaluate these. We evaluate the modified hierarchy by SMAYA|AB through two types of user study by following the methods [2, 5], respectively.",0,,False
404,Table 4: Comparison on appropriateness of category labels,0,,False
405,Judgement,0,,False
406,Number of documents,0,,False
407,HnYA is better than HYA,0,,False
408,12,0,,False
409,HnYA is not as good as HYA,0,,False
410,1,0,,False
411,Both are equally good,0,,False
412,81,0,,False
413,Neither is good,0,,False
414,6,0,,False
415,Table 5: Averaged scores of HYA and HnYA (by SMAYA|AB),0,,False
416,Measure/Hierarchy HYA HnYA,0,,False
417,Cohesiveness,0,,False
418,5.00 6.00,0,,False
419,Isolation,0,,False
420,4.00 4.67,0,,False
421,Hierarchy,0,,False
422,5.00 5.33,0,,False
423,Navigation Balance 4.50 4.50,0,,False
424,Readability,1,ad,True
425,6.00 5.67,0,,False
426,"Through the study, we aim to quantify both the appropriateness of the category labels and the structure of the modified hierarchy.",0,,False
427,"Category Labels. Following a similar setting as in [2], we randomly selected 100 questions from the labeled test set originated from Yahoo! Answers. For each question, we gave the path of the categories in HYA from the second level category to the leaf category, and similarly the category path from HnYA (by SMAYA|AB). We asked three students to annotate which category path better reflects the topic of the question. Which hierarchy a category path was originated from was not provided to the annotators. Given a question, each volunteer is asked to rate each path from 1(lowest) to 5(highest) based on its quality. hen, we select one of the following choices based on the averaged ratings (rYA and rnYA for the two paths, respectively). (1)HnYA is better than HYA, if rnYA, rYA  [3, 5] and rnYA > rYA; (2)HnYA is not as good as HYA, if rnYA, rYA  [3, 5]; (3)Both are equally good, if rnYA, rYA  [3, 5] and rnYA ,"" rYA; (4)Neither is good, if rnYA, rYA  [1, 3). The statistics of the labels are reported in Table 4. The table shows that the number of questions having better labels in HnYA is larger than that in HYA although for majority of questions, the category paths from the two hierarchy are equally good. This result also suggests that the generated labels well reflects the content of categories.""",1,Yahoo,True
428,"Category Structure. Following the evaluation approaches in [5], we evaluate the quality of Yahoo! Answers hierarchy and the modified hierarchy by five measures. Cohesiveness: Judge whether the instances in each category are semantically similar. Since it is impractical to read all questions in a large category, we randomly select 50 questions from each category for cohesiveness evaluation. Isolation: Judge whether categories on the same level are discriminative from each other.We also use the 50 randomly selected questions to represent each category. Hierarchy: Judge whether the concepts represented by the categories become finer from top to bottom. Navigation Balance: Judge whether the number of child categories for each internal category is appropriate. Readability: Judge whether the concept represented by each category is easy to understand.",1,Yahoo,True
429,"We invited three students to evaluate the two hierarchies, HYA and HnYA (by SMAYA|AB) and assigned scores ranging from 0 to 7 on each measure. The mean of the scores is reported in Table 5.",0,,False
430,"The cohesiveness of the modified hierarchy is better than the original one. A possible reason is that our approach detected the hidden topics and merged the most similar ones together. The isolation of the modified hierarchy is slightly better. This is probably because the proposed method takes isolation into consideration. To find out the reasons that caused the relatively low isolation of the original hierarchy, we get the list of categories with low scores from",0,,False
431,799,0,,False
432, ,0,,False
433,,0,,False
434,&&&,0,,False
435, ,0,,False
436," !"" #",0,,False
437,!,0,,False
438, #,0,,False
439, $,0,,False
440,#,0,,False
441,,0,,False
442,"% "" ",0,,False
443,  ,0,,False
444,&&&,0,,False
445,   ,0,,False
446,Figure 6: Portion of Yahoo! Answers hierarchy and its modified hierarchy.,1,Yahoo,True
447,"the annotators. As an example, a number of questions that are related to motor-cycling were put under Other - Auto Racing by their askers, resulting in low isolation between the two categories. The modified hierarchy does not deteriorate hierarchy quality, navigation balance, and readability of the original hierarchy on average. In summary, the modified hierarchy is of high quality comparable to the original hierarchy generated by domain experts.",1,ad,True
448,5.4 Case Study,0,,False
449,"As a case study, we select three categories Software, Internet and Hardware from Yahoo! Answers as an example to illustrate the differences before and after modifying HYA. The modified hierarchy HnYA is by SMAYA|AB utilizing AnswerBag as auxiliary hierarchy.",1,Yahoo,True
450,"The two hierarchies are shown in Figure 6. We make the following observations: 1) Different from the original hierarchy, Software and Internet become three categories ­ Operating System & Application Software, Internet & E-mail and Internet Software. The third category is formed based on the overlapping part of the original two categories, which contains questions about instant messaging (IM) and blog software. This demonstrates that the proposed approach can discover and detach the overlapping hidden topics. 2) Two pairs of categories of the original hierarchy, (Laptops & Notebooks and Desktops), and (Printers and Scanners), are merged into two categories in the modified hierarchy, because of the high similarity between the categories within each pair. This shows that categories with high overlap in semantics are merged. 3) For Hardware, some hidden topics are discovered and new categories are formed, like Storage and CPU & Memory & Motherboard, whose questions come from Desktops, Add-ons and Other - Hardwares in the original hierarchy. These newly formed categories are more isolated from each other.",1,blog,True
451,6. CONCLUSION,0,,False
452,"Category hierarchy plays a very important role in organizing data automatically (through classifiers built on the hierarchy) or manually. However, with newly available documents added into a hierarchy, new topics emerge and documents within the same category become less topically cohesive. Thus the hierarchies suffer from problems of structure irrelevance and semantic irrelevance, leading to poor classification accuracy of the classifiers developed for automatically categorizing the newly available documents into the hierarchy, which in turn leads to poorer document organization. To address these problems, we propose a novel approach SMA to modify a hierarchy. SMA comprises three non-trivial operations (namely, sprout, merge, and assign) to modify a hierarchy. Experimental results demonstrate that SMA is able to generate a modified",1,ad,True
453,"hierarchy with better classification accuracy improvement over the original hierarchy than baseline methods. Additionally, user study shows that the modified category hierarchy is topically cohesive and semantically meaningful.",0,,False
454,7. ACKNOWLEDGEMENTS,0,,False
455,"Quan Yuan would like to acknowledge the Ph.D. grant from the Institute for Media Innovation, Nanyang Technological University, Singapore. Gao Cong is supported in part by a grant awarded by Microsoft Research Asia and by a Singapore MOE AcRF Tier 1 Grant (RG16/10).",0,,False
456,8. REFERENCES,0,,False
457,"[1] G. Adami, P. Avesani, and D. Sona. Bootstrapping for hierarchical document classification. In CIKM, pages 295­302, 2003.",0,,False
458,"[2] C. C. Aggarwal, S. C. Gates, and P. S. Yu. On the merits of building categorization systems by supervised clustering. In KDD, pages 352­356, 1999.",0,,False
459,"[3] R. Agrawal and R. Srikant. On integrating catalogs. In WWW, pages 603­612, 2001.",0,,False
460,"[4] X. Cao, G. Cong, B. Cui, C. S. Jensen, and Q. Yuan. Approaches to exploring category information for question retrieval in community question-answer archives. ACM Trans. Inf. Syst., 30(2):1­38, 2012.",0,,False
461,"[5] S.-L. Chuang and L.-F. Chien. A practical web-based approach to generating topic hierarchy for text segments. In CIKM, pages 127­136, 2004.",0,,False
462,"[6] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms, Second Edition. The MIT Press and McGraw-Hill Book Company, 2001.",0,,False
463,"[7] S. C. Gates, W. Teiken, and K.-S. F. Cheng. Taxonomies by the numbers: building high-performance taxonomies. In CIKM, pages 568­577, 2005.",0,,False
464,"[8] A. Koestler. The Act of Creation. Penguin Books, New York, 1964. [9] D. Koller and M. Sahami. Hierarchically classifying documents",0,,False
465,"using very few words. In ICML, pages 170­178, 1997. [10] K. Nitta. Improving taxonomies for large-scale hierarchical",0,,False
466,"classifiers of web documents. In CIKM, pages 1649­1652, 2010. [11] K. Punera, S. Rajan, and J. Ghosh. Automatically learning document",0,,False
467,"taxonomies for hierarchical classification. In WWW (Special interest tracks and posters), pages 1010­1011, 2005. [12] X. Qi and B. D. Davison. Hierarchy evolution for improved classification. In CIKM, pages 2193­2196, 2011. [13] B. Qu, G. Cong, C. Li, A. Sun, and H. Chen. An evaluation of classification models for question topic categorization. JASIST, 63(5):889­903, 2012. [14] G. Siolas and F. d'Alché Buc. Support vector machines based on a semantic kernel for text categorization. In IJCNN (5), pages 205­209, 2000. [15] A. Sun, E.-P. Lim, and Y. Liu. What makes categories difficult to classify?: a study on predicting classification performance for categories. In CIKM, pages 1891­1894, 2009. [16] P.-N. Tan, M. Steinbach, and V. Kumar. Introduction to Data Mining. Addison-Wesley, 2005. [17] L. Tang, J. Zhang, and H. Liu. Acclimatizing taxonomic semantics for hierarchical content classification from semantics to data-driven taxonomy. In KDD, pages 384­393, 2006. [18] N. Tomuro. Tree-cut and a lexicon based on systematic polysemy. In NAACL, 2001. [19] W. Wei, G. Cong, X. Li, S.-K. Ng, and G. Li. Integrating community question and answer archives. In AAAI, 2011. [20] Y. Yang and X. Liu. A re-examination of text categorization methods. In SIGIR, pages 42­49, 1999. [21] D. Zhang and W. S. Lee. Web taxonomy integration through co-bootstrapping. In SIGIR, pages 410­417, 2004. [22] L. Zhang, S. Liu, Y. Pan, and L. Yang. Infoanalyzer: a computer-aided tool for building enterprise taxonomies. In CIKM, pages 477­483, 2004.",0,,False
468,800,0,,False
469,,0,,False

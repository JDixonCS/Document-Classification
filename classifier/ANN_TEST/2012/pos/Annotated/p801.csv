,sentence,label,data,regex
0,"When Web Search Fails, Searchers Become Askers: Understanding the Transition",0,,False
1,"Qiaoling Liu§, Eugene Agichtein§, Gideon Dror, Yoelle Maarek, Idan Szpektor,",0,,False
2,"§Emory University, Atlanta, GA, USA Yahoo! Research, Haifa, Israel",1,Yahoo,True
3,"{qliu26, eugene}@mathcs.emory.edu, {gideondr, idan}@yahoo-inc.com, yoelle@ymail.com",0,,False
4,ABSTRACT,0,,False
5,"While Web search has become increasingly effective over the last decade, for many users' needs the required answers may be spread across many documents, or may not exist on the Web at all. Yet, many of these needs could be addressed by asking people via popular Community Question Answering (CQA) services, such as Baidu Knows, Quora, or Yahoo! Answers. In this paper, we perform the first large-scale analysis of how searchers become askers. For this, we study the logs of a major web search engine to trace the transformation of a large number of failed searches into questions posted on a popular CQA site. Specifically, we analyze the characteristics of the queries, and of the patterns of search behavior that precede posting a question; the relationship between the content of the attempted queries and of the posted questions; and the subsequent actions the user performs on the CQA site. Our work develops novel insights into searcher intent and behavior that lead to asking questions to the community, providing a foundation for more effective integration of automated web search and social information seeking.",1,ad,True
6,Categories and Subject Descriptors,0,,False
7,H.3.3 [Information Systems]: Information Storage and Retrieval,0,,False
8,Keywords,0,,False
9,"query analysis, community question answering",0,,False
10,1. INTRODUCTION,1,DUC,True
11,"While Web search engines have significantly progressed in effectiveness and efficiency over the last decade, there still exist certain user needs that cannot be satisfied. This could be due to a number of reasons, such as the difficulty of expressing a complex need as a short search query, the lack of existing relevant content on the Web (e.g., for unique or ""tail"" needs that keep appearing), and for more ""social"" needs, for which the user prefers to interact with a real human.",1,ad,True
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.",1,ad,True
13,"In fact, Hitwise in August 2011 reported that only 6680% of the searches are successful1, and Hassan et al. [15] obtained a similar success rate of search goals (73%) via human labeling. We argue that many such unsatisfied searches could be addressed by asking people via Community Question Answering (CQA) services, such as Baidu Knows, Quora, or Yahoo! Answers. It already happens in practice. For example, we have observed that about 2% of web search sessions performed by users who are also members of the Yahoo! Answers community, lead to a question posted to the community. Consider Figure 1a, which depicts a sample search submitted to a major search engine. The searcher is not satisfied with the results, and eventually posts a related question on the Yahoo! Answers site, which is then answered to the searcher's satisfaction. Understanding and improving the synergy between searching and community question answering is at the heart of this work.",1,ad,True
14,"Specifically, our goal is to better understand the behavior of these users, as well as characterize the types of Web searches that could be effectively handled by CQA sites. Insights acquired during such analysis should bring multiple benefits to both search engines and CQA systems. On one hand, search engines always need to better understand when searchers are unsatisfied by the returned results. More specifically, Web search engines would find value in analyzing the search session patterns of such unsuccessful queries, the associated underlying query intents, and possibly reflect these findings in search effectiveness metrics. One can even imagine new search experiences that would allow users to turn to the community for certain types of needs better addressed by people than by traditional Web search. Additionally, CQA systems could potentially improve the asking experience by taking advantage of the context provided by unsuccessful queries preceding a posted question. One can imagine several ways to leverage this context, such as automatically giving examples of irrelevant answers to clarify the question to the community.",1,ad,True
15,"To the best of our knowledge, our work is the first to perform a large-scale study of the transformation of searchers into askers. That is, we start our analysis with web search sessions, trace the searcher through her visit to a CQA site, and analyze the resulting questions posted for the community.",0,,False
16,"We focus on one of the most visited, and more mature, CQA systems existing today, namely Yahoo! Answers, which",1,Yahoo,True
17,1www.hitwise.com/us/about-us/ press-center/press-releases/ experian-hitwise-reports-google-share-of-searche/,0,,False
18,801,0,,False
19,(a),0,,False
20,(b),0,,False
21,Figure 1: Example search (a) followed by a question posted by the same user on the Yahoo! Answers site with a satisfactory answer from the community (b).,1,Yahoo,True
22,with more than 1 billion posted answers2 is highly visible,0,,False
23,such as occurrence of personal pronouns or sentiment,0,,False
24,in most search engines result pages. We built a corpus of,0,,False
25,indicators (Section 4).,0,,False
26,query-to-question transitions and studied it in order to un-,0,,False
27,derstand when and why searchers become askers. A privacy-,0,,False
28,preserving subset of the data has recently been made publicly available through Yahoo's Webscope program3.,1,ad,True
29,Research Question 3: How do searchers behave after transferring to the CQA site?,0,,False
30,"More specifically, our study is organized around the following three research questions, each associated with a set of hypotheses:",0,,False
31,· Hypothesis 5: We further hypothesize that the content and the topics of the questions posted after a search session differ substantially from the general ques-,0,,False
32,Research Question 1: When do searchers turn to CQA,0,,False
33,tion distribution (Section 5.1).,0,,False
34,for answers?,0,,False
35,· Hypothesis 6: We hypothesize that the question ses-,0,,False
36,"· Hypothesis 1: Queries and information needs of search sessions that lead to posting questions are hypothesized to share common characteristics, and differ from general web searches in words and information needs (Section 3.1).",1,ad,True
37,"sions after switching from searching, exhibit different characteristics than general question sessions and search sessions explored in depth in previous research work [17][10]. We study these different types of sessions in terms of duration and persistence for specific users and examine their behavior over time (Section 5.2).",0,,False
38,"· Hypothesis 2: We hypothesize that searchers who switch to CQA exhibit common search behavior. For instance, they tend to click more on CQA results on",0,,False
39,The rest of this paper is dedicated to answering the above questions and verifying the associated hypotheses.,0,,False
40,"the search result page, and their search sessions are",0,,False
41,"longer, allowing to characterize different types of users in the same spirit as [7] (Section 3.2).",0,,False
42,2. ACQUIRING DATA,0,,False
43,"In order to understand how searchers become askers, we",0,,False
44,collected a dataset that contains both the search session part,0,,False
45,Research Question 2: How do search queries relate to the associated questions posted on CQA sites?,0,,False
46,and asking session part of each user who conducted a search session that resulted in posting a question. Our dataset is derived from joining a sample of the query logs of the Yahoo!,1,Yahoo,True
47,· Hypothesis 3: Queries and questions follow different,0,,False
48,"search engine and the Yahoo! Answers question logs, both",1,Yahoo,True
49,"word distributions. More specifically, words in queries",0,,False
50,for June 2011.,0,,False
51,are hypothesized to follow different distributions than,0,,False
52,"To create this dataset, we first created a mapping of users",0,,False
53,"those appearing in questions, and a clear vocabulary",0,,False
54,"between the two logs, based on the clicks on the same Yahoo!",1,Yahoo,True
55,gap between these can be observed (Section 4).,0,,False
56,Answers question page following the same query on the same,0,,False
57,"time frame, as they appear in both logs. Then, we extracted",0,,False
58,· Hypothesis 4: Questions are typically more specific,0,,False
59,"user actions from the query and question logs, e.g. posting",0,,False
60,"than queries and include additional context (e.g., per-",1,ad,True
61,"queries and clicking on results from query logs, as well as",0,,False
62,sonal background) absent from the original queries.,0,,False
63,posting questions and re-viewing them from the question,0,,False
64,We hypothesize that these differences are reflected in,0,,False
65,logs. We constructed search sessions from these extracted,0,,False
66,"the lexicographic differences between questions and queries, actions, with a 30 minutes timeout as a session boundary.",0,,False
67,2http://yanswersblog.com/index.php/archives/2010/,1,blog,True
68,05/03/1-billion-answers-served/ 3http://webscope.sandbox.yahoo.com/,0,,False
69,"Question sessions have no temporal boundary, since every action in the session unambiguously refers to the question posted by the asker.",0,,False
70,802,0,,False
71,Table 1: Statistics of the constructed datasets. Description,0,,False
72,Total sampled search sessions SearchOnly sessions Search sessions that include question sessions SearchAsk sessions: search sessions with a single relevant question posted after searching,0,,False
73,Number of sessions,0,,False
74,"1,287,238 (100%) 1,233,279 (95.8%)",0,,False
75,53959 (4.2%) 21231 (1.65%),0,,False
76,Query length distribution (cumulative) 0.0 0.2 0.4 0.6 0.8 1.0,1,Query,True
77,"Once we had search sessions and question sessions, and mapping between some of them, we created two datasets. The first, termed SearchAsk dataset, contains search sessions that turned into question sessions. We only kept such sessions that resulted in posting one and only one question for simplicity of analysis later on. In addition, we only kept sessions in which the posted question is ""relevant"" to a previously issued query (if the query and the question share at least one non-stopword, they were considered relevant). By observing that some users actually searched for Yahoo! Answers to navigate to its home page before they posted a question there, we deleted such special navigational clicks and corresponding queries from the user action sequences. The second dataset, termed SearchOnly dataset, consists of search sessions that did not turn into question sessions. In both datasets, we only kept sessions for users that posted at least once in Yahoo! Answers, since these users are aware of the site and know how to post a question there, thus removing the potential investment of effort for newcomers to join the site, and filtering our the users that simply do not know where to ask questions.",1,ad,True
78,"Table 1 reports the statistics of the datasets we obtained. As shown in the table, 95.8% of all the search sessions are SearchOnly sessions, while SearchAsk sessions account for 1.65%. Despite the sparsity of the SearchAsk sessions, we still believe that understanding how searchers become askers in such sessions can be helpful for improving the search experience of these users and perhaps more users. Indeed, the two datasets allow us to investigate the differences between sessions in which users posted a question following attempted searches, mainly due to search failure or searcher frustration, and sessions in which users that have experience of asking questions on Yahoo! Answers did not bother or did not need to ask questions at that time. Recall, that the users in these datasets satisfy two conditions: (1) having clicked at least a Yahoo! Answers question page within this month; and (2) having asked at least one question on Yahoo! Answers within the month. Yet, we believe that such users still represent the general, though somewhat experienced, web searchers.",1,Yahoo,True
79,3. FROM SEARCHING TO ASKING: QUERY AND BEHAVIOR ANALYSIS,0,,False
80,"As a first step, we study the characteristics of queries leading to a question post on Yahoo! Answers (Section 3.1), and the characteristics of searcher behavior before question asking (Section 3.2).",1,ad,True
81,3.1 Characteristics of Queries leading to Questions,1,ad,True
82,"The first interesting question is which queries are more likely to be unsuccessful for automated search, but instead are more amenable to be answered by a CQA site. To get such queries, we examine each SearchAsk session, and",1,ad,True
83,SearchAsk SearchOnly,0,,False
84,1,0,,False
85,2,0,,False
86,5 10 20,0,,False
87,50,0,,False
88,Query length (number of words),1,Query,True
89,Figure 2: Distribution of query length,0,,False
90,Table 2: Statistics of words per query,0,,False
91,Avg # Avg # Avg % Avg,0,,False
92,words stop- stop- word,0,,False
93,words words length,0,,False
94,SearchAsk queries 6.5,0,,False
95,2.4,0,,False
96,28% 5.1,0,,False
97,SearchOnly queries 3.4,0,,False
98,0.72 11% 6.0,0,,False
99,"extract the queries that are issued before the question is posted, and are relevant to the question. We call such queries SearchAsk queries. For comparison, we also extract the queries in each SearchOnly session which are called SearchOnly queries. In the following, we explore how SearchAsk queries are different from SearchOnly queries in terms of length, words, frequency, and results.",0,,False
100,Query Length Distribution.,1,Query,True
101,"Figure 2 compares the distribution of query length (in terms of number of words in the query) for the SearchAsk and SearchOnly queries. We can see that SearchAsk queries tend to be longer than SearchOnly queries, as 85% of the SearchOnly queries contain at most 5 words, while about 50% of the SearchAsk queries contain more than 5 words. Therefore, searchers issuing longer queries are more likely to turn to Yahoo! Answers to post a relevant question.",1,Yahoo,True
102,"Table 2 compares the average word length per query and the average number of stopwords for the SearchAsk queries and SearchOnly queries. We can see that, on average, queries turning to questions tend to contain more words (but shorter words) than queries that do not turn to questions. The main reason could be that SearchAsk queries contain more stopwords (which are often short) than SearchOnly queries. Indeed, the percentage of stopwords in SearchAsk queries is over 2.5 times higher than in SearchOnly queries.",0,,False
103,803,0,,False
104,Query frequency distribution (cumulative) 0.5 0.6 0.7 0.8 0.9 1.0 Search Search Only Ask,1,Query,True
105,Query SERP with url from Yahoo! Answers Query SERP without url from Yahoo! Answers,1,Query,True
106,SearchAsk SearchOnly,0,,False
107,1,0,,False
108,10 100 1000 10000,0,,False
109,Query frequency in 1-month query log,1,Query,True
110,Figure 3: Distribution of query frequency,0,,False
111,Query Words Distribution.,1,Query,True
112,"To better understand the difference between the content of SearchAsk queries and SearchOnly queries, we compare their word distributions and show the main difference in Table 3. We can see that SearchOnly queries are more likely to be navigational, e.g., to reach websites like Facebook or YouTube, or to find information related to the searcher's common tasks such as looking up the weather, hunting for coupons, or finding a cooking recipe. In contrast, SearchAsk quries are more likely to start with question words (e.g., `how', `what'), and tend to use more verbose natural language to express the needs of the searchers (e.g., `want', `to', `know') rather than using only keywords.",1,YouTube,True
113,Query Frequency Distribution.,1,Query,True
114,"To verify the hypothesis from the above word distribution analysis that SearchAsk queries are more likely to be unique, we compute the frequency4 of SearchAsk queries and SearchOnly queries in our 1-month query log. Figure 3 shows the results. We can see that over 90% of SearchAsk queries are tail (actually unique) queries, indicating the variety of the needs of searchers and the ways to express them. In contrast, SearchOnly queries contain more popular queries, e.g., around 20% of SearchOnly queries occur in more than 100 search sessions.",0,,False
115,Query Results Distribution.,1,Query,True
116,"To better understand user needs behind SearchAsk queries, we further examine the results returned in their search engine result pages (SERPs). We found a significant difference between SearchAsk and SearchOnly queries based on whether a SERP contains a Yahoo! Answers question page. As shown in Figure 4, a Yahoo! Answers question page occurs in the SERPs for half of the queries that eventually turn to questions, but for only 13% of SearchOnly queries. It is clear that SearchAsk queries are more likely to have a Yahoo! Answers question page in the SERP. This is not surprising. First, having a Yahoo! Answers question page in search results indicates that the query could be relevant to an existing Yahoo! Answers question. Therefore, answers from a human might be more suitable to address the need behind the query, encouraging the searcher to post a question on Yahoo! Answers. Second, more impressions often leads to more clicks. After landing on the Yahoo! Answers",1,Yahoo,True
117,4The frequency of a query in this paper is computed as the number of search sessions containing the query.,0,,False
118,0.0,0,,False
119,0.2,0,,False
120,0.4,0,,False
121,0.6,0,,False
122,0.8,0,,False
123,1.0,0,,False
124,Probability,0,,False
125,Figure 4: Distribution of query results,0,,False
126,"site, the searcher might realize that a community might be able to answer her information need, and try posting a question.",0,,False
127,Summary of Query Characteristics.,1,Query,True
128,"As a summary of the above analysis, we conclude that queries that are more likely to fail in search and lead to a question post on Yahoo! Answers tend to be longer, and use more verbose natural language to express the searchers' needs. The needs behind such queries tend to be more unique and complex than those associated with SearchOnly queries.",1,ad,True
129,3.2 Searcher Behavior Before Asking Questions,0,,False
130,"To understand how searchers become askers, we analyze the searcher behavior in search sessions, with an associated question posted by the same user on Yahoo! Answers.",1,Yahoo,True
131,Last Action Before Question Asking.,0,,False
132,"First, we examine what searchers do right before they start question asking, i.e., we examine the last user action prior to a question being posted. We found that the last search action before question asking is a click on Yahoo! Answers question result in 47.8% of the sessions, a click on other result in 31.2% of the sessions, and a query in 17.4% of the sessions. We notice that in about half of the sessions, the searcher posts a question right after clicking on a Yahoo! Answers question page from the search engine results. There may be several reasons for this. First, such a click indicates that the query is relevant to the clicked question, and therefore it probably carries an information need that would benefit from a human response. Second, when the clicked Yahoo! Answers question page cannot satisfy the search need, it encourages the user to post a new question on Yahoo! Answers. Of course, it is also possible that a searcher had already decided to post a question when seeing the original SERP, and she then clicked on a Yahoo! Answers question result simply to navigate to the Yahoo! Answers site.",1,Yahoo,True
133,Distribution of Clicks.,0,,False
134,"To better understand the effects of clicking on a Yahoo! Answers question result on the transformation of searchers into askers, we compute and compare the likelihood of such clicks in SearchAsk and SearchOnly sessions. Figure 5 shows the results. First, 21% of SearchOnly sessions and 81% of SearchAsk sessions contain a Yahoo! Answers question page in the SERPs. Next, after seeing a Yahoo! Answers question page in the SERPs, 81% of the searchers who turned to",1,Yahoo,True
135,804,0,,False
136,Words First words Content words,0,,False
137,Words First words Content words,0,,False
138,Table 3: Frequent words in SearchAsk queries and SearchOnly queries More likely in SearchAsk queries,0,,False
139,"to, a, be, i, how, do, my, can, what, on, in, the, for, have, get, with, you, if, yahoo, it how, what, can, be, why, i, do, my, where, yahoo, if, when, 0000, a, will, 00, best, who, which, should yahoo, 00, use, 0, work, song, old, help, make, need, like, change, year, good, long, mail, answer, email, want, know",0,,False
140,"More likely in SearchOnly queries facebook, youtube, google, lyric, craigslist, free, online, new, bank, game, map, ebay, county, porn, tube, coupon, recipe, home, city, park facebook, youtube, google, craigslist, ebay, the, you, gmail, casey, walmart, amazon, *rnrd, justin, facebook.com, mapquest, netflix, face, fb, selena, home facebook, youtube, google, craigslist, lyric, free, bank, map, ebay, online, county, porn, tube, coupon, recipe, anthony, weather, login, park, ca",0,,False
141,Search Search Only Ask,0,,False
142,1+ clicks on Yahoo! Answers question result session SERPs with url from Yahoo! Answers session SERPs without url from Yahoo! Answers,1,Yahoo,True
143,1/1 Begin,0,,False
144,0.12 / 0.13,0,,False
145,0.15 / 0.03,0,,False
146,Click ques result,0,,False
147,0.47 / 0.21,0,,False
148,0.25 / 0.40,0,,False
149,0.06 / 0.01,0,,False
150,0.14 / 0.23,0,,False
151,Query,1,Query,True
152,0.04 / 0.05,0,,False
153,Ask /End,0,,False
154,0.0,0,,False
155,0.2,0,,False
156,0.4,0,,False
157,0.6,0,,False
158,0.8,0,,False
159,1.0,0,,False
160,Probability,0,,False
161,Figure 5: Distribution of clicks,0,,False
162,0.30 / 0.23,0,,False
163,0.51 / 0.43,0,,False
164,0.48 / 0.66,0,,False
165,Click other result,0,,False
166,0.30 / 0.25,0,,False
167,0.09 / 0.29,0,,False
168,"askers had clicked on a Yahoo! Answers question result while 19% of them hadn't; in contrast, 43% of the searchers in SearchOnly sessions seeing a Yahoo! Answers question result clicked on it while 57% of them didn't. Therefore, users in SearchAsk sessions are about twice as likely as in SearchOnly sessions to click on a Yahoo! Answers question page in the search results once seeing it. This indicates that searchers are more likely to post a question once clicking on a Yahoo! Answers question result.",1,ad,True
169,Transitions between Actions.,0,,False
170,"To better understand searcher actions, we further compute the probability of transitions between actions in SearchAsk and SearchOnly sessions respectively, and compare them in Figure 6. The transition probability between two actions ai and aj in SearchAsk (SearchOnly) sessions is computed using Maximum Likelihood estimation: P (ai, aj) ,"" Nai,aj /Nai , where Nai,aj is the number of transitions from action ai to action aj in all SearchAsk (SearchOnly) sessions, and Nai "","" ak Nai,ak . SearchAsk transition probabilities are shown in red before the slash symbol, while SearchOnly transition probabilities are shown in black after the slash symbol. If we look at the transitions for SearchOnly sessions from the figure, we can see that after issuing a query, the searcher is very likely to click on other result, then with perhaps more queries and clicks on other result, and then ends the session. Clicking on a Yahoo! Answers question result is very unlikely. However, in SearchAsk sessions, the searcher has a higher probability on clicking a Yahoo! Answers question result. After the click, the searcher in SearchAsk sessions would post a question on Yahoo! Answers for around half of the time.""",1,Yahoo,True
171,"Figure 6: Transition probabilities for actions in SearchAsk (in red, before the slash symbol) and SearchOnly (in black, after the slash symbol) sessions. Note that two other actions (Pagination and Click interface) are ignored for simplicity.",0,,False
172,Action Sequences Before Question Asking.,0,,False
173,"To better understand how searchers become askers, we examine the user action sequences in SearchAsk sessions before the question post, and compare them with action sequences in SearchOnly sessions. Table 4 shows a sample of top frequent user action sequences. The top frequent path in SearchOnly sessions indicates navigational needs of the searchers, i.e., they issue a query, click on a search result and leave the session. Such navigational cases account for 30% of total SearchOnly sessions. In contrast, the top frequent path in SearchAsk sessions indicates more ""social"" needs of the searchers, i.e., they issue a query, click on a search result of Yahoo! Answers question page, and then ask a question on Yahoo! Answers. Yet, the path distribution is more balanced for SearchAsk sessions. Moreover, clicks on Yahoo! Answers question results are common in the paths.",1,Yahoo,True
174,Session Size Distribution.,1,Session,True
175,"Finally, we compare the distribution of session sizes for SearchAsk and SearchOnly sessions. Session size can be measured in several ways, e.g., by the number of (unique) queries issued by the searcher in the session, by the number of actions performed in the session, or by the duration that the session lasts. We use the first option in this paper. The results are shown in Figure 7. While only one query",1,Session,True
176,805,0,,False
177,Table 4: Top frequent user action sequences in,0,,False
178,SearchAsk sessions and SearchOnly sessions (B: Be-,0,,False
179,"gin a session, Q: Query, Cqr: Click on a Yahoo! Answers question result, Cor: Click on other result, A: Ask a question, E: End a session)",1,Query,True
180,SearchAsk sessions Distribution,0,,False
181,B Q Cqr A B Q Cor A B Q Q Cqr A BQA,0,,False
182,10% 3.8% 3.3% 2.8%,0,,False
183,B Q Cor Q Cqr A,0,,False
184,2.0%,0,,False
185,SearchOnly sessions Distribution,0,,False
186,B Q Cor E B Q Cor Q Cor E BQE,0,,False
187,30.2% 7.1% 6.1%,0,,False
188,B Q Cor Cor E B Q Q Cor E,0,,False
189,3.6% 3.6%,0,,False
190,Table 5: Statistics of length difference between a,0,,False
191,query and its associated question (number of words).,0,,False
192,Median Avg Max,0,,False
193,|question| - |query| 42,0,,False
194,66 1431,0,,False
195,|subject| - |query|,0,,False
196,3,0,,False
197,4 27,0,,False
198,|content| - |query| 31,0,,False
199,55 1428,0,,False
200,Table 6: Overlap of content words (CW) between a query and its associated question.,1,CW,True
201,"CW?  CWquery CW? , CWquery CW?  CWquery CW?  CWquery CW?  CWquery",1,CW,True
202,"?,question",0,,False
203,31.4% 1.8% 0.7% 66.1%,0,,False
204,"?,subject",0,,False
205,14.6% 6.2% 3.7% 75.5%,0,,False
206,"?,content",0,,False
207,14% 0.4% 17.1% 68.5%,0,,False
208,Sessioin size distribution (cumulative) 0.0 0.2 0.4 0.6 0.8 1.0,0,,False
209,SearchAsk SearchOnly,0,,False
210,1,0,,False
211,2 3 5 10 20,0,,False
212,50,0,,False
213,Session size (number of unique queries),1,Session,True
214,Figure 7: Distribution of session size,0,,False
215,"is issued in the half of SearchOnly sessions, at least three different queries are issued in the half of SearchAsk sessions. The average session size is 2.5 for SearchOnly sessions and 3.8 for SearchAsk sessions. This shows that searchers tend to issue more queries in SearchAsk sessions, possibly because SearchOnly sessions contain more navigational needs, while SearchAsk sessions are associated with more difficult or complex needs, and thus require more effort in finding answers.",0,,False
216,4. QUERIES VS. QUESTIONS: CONTENT ANALYSIS,0,,False
217,"After discovering the unique attributes of queries that lead to asking a question, we next want to understand better the process of turning a search session, as captured by a query, into a question posted on Yahoo! Answers.",1,ad,True
218,"The most expected difference between queries and questions is their length. Table 5 shows these differences. From the table we can see that a question has 66 more words than its associated query on average. This indicates two things: first, as expected, questions are much more verbose, being natural language expressions, compared to the concise queries; second, since Yahoo! Answers questions are not",1,Yahoo,True
219,Figure 8: Word distributions over question words,0,,False
220,"limited in length, additional knowledge of the problem to be solved is added. Interestingly, the subject of the question is very close in length to the query, which shows that searchers still think in search-style writing for the subject. However, the content part of the question is significantly longer, and much more information is added in this question part.",1,ad,True
221,"We next look at word distribution differences, since they may point at the lexical gap between queries leading to questions and their associated posted questions. Figure 8 depicts the word occurrence distribution over word ranking by frequency for search-related questions. The most notable difference between the two distributions is that questions tend to be more personal and verbose, as captured by the abundant usage of the pronouns such as `I', `me', `it' and `this', connectives such as `but', `because', `recently', and `just', as well as sentiment indicators such as `help', `please', and `thanks'. Queries, on the other hand, tend to focus more on the things or actions that are searched for, with content words like `best', `free', `download' and `games' as well as question words like `how', `why' and `what' occurring more frequently than in the associated questions corpus. Interestingly, one to four digit figures, such as car model years,",1,ad,True
222,806,0,,False
223,Table 7: Examples showing semantics difference between the query and the question.,0,,False
224,ID Type of Query,1,Query,True
225,"Question (Category, Subject, and Content)",0,,False
226,context,0,,False
227,added,1,ad,True
228,1 N/A,0,,False
229,what to serve with Food & Drink>Cooking & Recipes,0,,False
230,chicken salad,1,ad,True
231,what can you serve with chicken salad?,1,ad,True
232,2 thought best nba players with- Sports>Basketball,0,,False
233,out a championship,0,,False
234,Greatest NBA players to never win championship?,0,,False
235,"Patrcik ewing, reggie miller, charles barkley, karl malone? Who else?",0,,False
236,3 task,0,,False
237,pt cruiser ac fix,0,,False
238,Cars & Transportation>Car Makes>Chrysler,0,,False
239,how much does it cost to fix an ac system in a pt cruiser?,0,,False
240,4 task,0,,False
241,"solve n^2-2n-3,5000 Education & Reference>Homework Help",0,,False
242,"Algebra question, Need Help Pls!!!!?",0,,False
243,An owner of a key rings company found that the profit earned (in thousands of,0,,False
244,"dollars) per day by selling n number of key rings is given by n^2 - 2n - 3, where",0,,False
245,n is the number of key rings in thousands. Find the number of key rings sold on,0,,False
246,a particular day when the total profit is $5000. Thanx,0,,False
247,5 limit,0,,False
248,chocolate croissant Dining Out>United States>San Jose,1,hoc,True
249,menlo park,0,,False
250,"Where can I get a good Chocolate Croissant near Menlo Park, CA?",1,hoc,True
251,"Something with thick, dark chocolate? And please, don't say La Boulanger.",1,hoc,True
252,"6 situation, chicago fried chicken Dining Out>United States>Chicago",0,,False
253,task,0,,False
254,Where can I get really good fried chicken in the Lakeview area in,0,,False
255,Chicago?,0,,False
256,I really want fried chicken after watchin a special on TV. But I cant find any place,0,,False
257,near me that has decent priced chicken thats not fast food and is homemade and,1,ad,True
258,delicious. Any one know of a place?,0,,False
259,"7 situation, douglas az",0,,False
260,Education & Reference>Higher Education (University +),0,,False
261,task,0,,False
262,Radiology schools in Arizona?,1,ad,True
263,"Does any one know any schools in az that offer radiology degree programs, I",1,ad,True
264,moved to Douglas az and don't know any schools near to study radiology. If any,1,ad,True
265,one can help that would be great :),0,,False
266,"8 attribute, how many bottles to Pregnancy & Parenting>Newborn & Baby",0,,False
267,"situation, buy for a newborn",0,,False
268,How many bottles should I purchase for my new baby? And what,0,,False
269,task,0,,False
270,brand is best?,0,,False
271,I am 9 mo. pregnant and still need to buy bottles. I will be trying to breast,0,,False
272,feed but I am unsure of how many bottles and what sizes I should buy. Is there,0,,False
273,anything else I will need for feeding and what brand do you recommend? Thanks!,0,,False
274,"also appear more in question-related queries than in their associated questions, probably since they capture much of the essence of the target information need.",0,,False
275,"To further understand the semantic difference between composing a query and its related question, we measured the distribution of query-question pairs in which the same words are used for both query and question, the pairs in which one is included in the other, and those pairs in which each contains words that do not occur in the other. Table 6 presents these statistics, while Table 7 provides examples of such pairs, annotated with the type of context added when switching from query to question, as been classified by [23], i.e., task, situation, attribute, limit, and thought.",1,ad,True
276,"Some interesting question composition patterns are evident from this analysis. First, in the majority of pairs (66%), both queries and questions contain unique words that do not occur in the other. This is somewhat surprising, since we would expect more complete inclusion of the query terms in the question. However, it seems that with the freedom of writing a free text question, searchers tend to rephrase some of the terms they used in their queries. For example, abbreviations and short terms are turned into their more complete forms, e.g. `AZ' into `Arizona' and `newborn' into `new baby' (see example 7 and 8 in Table 7). In addition, while 31% of the pairs do show complete inclusion of the query terms in the question, many times the query terms do not all appear in the question's subject or content, but spread in both question parts. Table 7 shows that most of the extensions of",1,ad,True
277,"the query into a question include additional details that are related to the search task. Yet, many times details of the personal situation are added, such as the state of mind, e.g. ""after watching a special on TV "" (example 6 in Table 7).",1,ad,True
278,"One interesting future research is to automatically generate questions from queries [25][26]. However, adding context information to the question, such as the situation or limit is a difficult challenge. Still, expanding the query expression to an explicit question form may be possible for many cases, e.g. examples 1 and 3 in Table 7.",1,ad,True
279,5. ASKING AFTER SEARCHING: QUESTION ANALYSIS,0,,False
280,"As our final analysis, we are interested in discovering unique activity patterns in Yahoo! Answers that searchers posting a question have, compared to typical asker behavior in Yahoo! Answers. Specifically, we first examine the differences in lexicon, that is whether different words are used when composing a question (Section 5.1). Then, we analyze the difference in asker behavior after posting such questions, in terms of ""traditional"" CQA activities (Section 5.2).",1,Yahoo,True
281,5.1 Characterizing Questions Posted after a Search Session,1,Session,True
282,"As expected, we find that there is a large difference between the word distribution for the corpus of all questions posted in June 2011 and the distribution of the corpus of",0,,False
283,807,0,,False
284,Table 8: Categories with largest differences in,0,,False
285,assignment probability between questions coming,0,,False
286,from search and general questions,0,,False
287,Categories more likely for Categories more likely for,0,,False
288,general questions,0,,False
289,questions following search,0,,False
290,Polls & Surveys (Entertain- Maintenance & Repairs,0,,False
291,ment & Music),0,,False
292,(Cars),0,,False
293,Singles & Dating,0,,False
294,Law & Ethics,0,,False
295,Religion & Spirituality,0,,False
296,Dogs (Pets),0,,False
297,Politics,0,,False
298,Pregnancy,0,,False
299,Friends,0,,False
300,Maintenance & Repairs,0,,False
301,(Home & Garden),0,,False
302,Mathematics,0,,False
303,Renting & Real Estate,0,,False
304,Diet & Fitness,0,,False
305,Accounts & Passwords,0,,False
306,"Lesbian, Gay, Bisexual, and Other - Yahoo! Mail",1,Yahoo,True
307,Transgendered,0,,False
308,Other - Beauty & Style,0,,False
309,Military,0,,False
310,Basketball,0,,False
311,Problems with Service,0,,False
312,Baby Names,0,,False
313,Garden & Landscape,0,,False
314,Adolescent,0,,False
315,Cooking & Recipes,0,,False
316,"questions posted by searchers. In addition, the entropy of generating a word from the search-related question corpus is much lower, showing a more focused vocabulary. But what are the reasons for this large difference? It turned out to be mainly topical.",1,ad,True
317,"To measure this topical difference between the two types of questions, we looked at the distribution of categories to which the questions in the two compared corpora are assigned. Table 8 shows the categories with largest differences in assignment probability, those that are preferred more in the general question corpus and in the search-related question corpus respectively. These lists show that searchers tend to ask informational questions [14] to get fact- or adviceoriented answers, such as how to fix the car or maintain one's garden, how to bake cookies, but also questions related to Yahoo products, such as Yahoo! Mail. On the other hand, regular askers are more likely to ask conversational questions [14] with a social flavor, such as discussions around music or sports events, politics and religions, and opinions on possible baby names. We manually labeled 100 questions randomly sampled from the search-related question corpus, and found none are conversational, showing a very different distribution compared to that 38% of Yahoo! Answers questions are conversational as reported in [14]. We conjecture that this is because searchers usually turn to search engines to find information instead of starting conversations. Another kind of questions that are less likely searched first over the web are personal questions, in which the asker is interested in adding very personal details. These include topics such as diet and fitness advices, dating and style opinions. Finally, there are questions that are too complex, for which the asker knows the answer cannot be found on the web. A good example are Math questions, such as example 4 in Table 7.",1,corpora,True
318,"To further investigate the differences between the two question types, we removed the strong bias caused by the different category distributions within the two corpora by sampling questions from the general question corpus based on the category distribution of the search-related question corpus. By comparing the word distribution between the sampled corpus and the search-related question corpus, we found that hardly no topical differences remained. That is, the topical variation in the two corpora is more or less completely captured by the level of assigned categories, without",1,corpora,True
319,Table 9: Statistics of words in SearchAsk questions,0,,False
320,and sampled general questions,0,,False
321,Avg. corpus,0,,False
322,Sampled general SearchAsk,0,,False
323,statistics,0,,False
324,questions,0,,False
325,questions,0,,False
326,# words,0,,False
327,78.3,0,,False
328,73.7,0,,False
329,# words per sentence,0,,False
330,13.5,0,,False
331,13.7,0,,False
332,# sentences,0,,False
333,5.8,0,,False
334,5.4,0,,False
335,% stopwords,0,,False
336,66.3,0,,False
337,65.0,0,,False
338,word length,0,,False
339,4.22,0,,False
340,4.17,0,,False
341,Table 10: Statistics about user follow-up activities around their posted questions.,0,,False
342,SearchAsk Ask Search,0,,False
343,Avg duration,0,,False
344,30h,0,,False
345,32h 19.4m,0,,False
346,Median duration,0,,False
347,2.2h,0,,False
348,3.7h 11.6m,0,,False
349,Avg #actions,0,,False
350,6.41,0,,False
351,7.45,0,,False
352,-,0,,False
353,Median #actions,0,,False
354,5,0,,False
355,5,0,,False
356,-,0,,False
357,"more subtle topical differences evident. Still, there may be stylish variations in question composition between searchers and typical askers. Table 9 provides the stylish statistics for the general-sampled and search-related question corpora. The significant difference between the two corpora is the number of words per question: for the same topics, general questions contain 6% more words compared to searchrelated questions. Yet, interestingly, this attribute is due to more sentences that are written on average per general question, while if we look at the number of words per sentence, we see that surprisingly search-related questions have slightly more words in each sentence. This could be related to more information-focused nature of the questions posted after a search session, and suggests further investigation.",1,corpora,True
358,5.2 Asker Follow-up Activity after a Search,0,,False
359,"As our final question behavior analysis, we wanted to test whether a searcher interacts more or less with Yahoo! Answers after posting the question. To that end, we measured both the number of actions that both searchers and regular askers perform around a specific question they posted, as well as the duration of this set of actions. Follow-up actions after a posted question include: browsing the question page (e.g. checking for new answers), adding more details to the question, selecting a best answer, reporting abusive answers, voting for answers, and deleting the question.",1,Yahoo,True
360,"Table 10 provides the average statistics of these actions, while Figures 9 and 10 depict the distribution of number of actions and their duration for searchers and regular askers. From the table we can see that searchers perform fewer yet similar number of actions as typical askers do, but in a much shorter duration. As can be seen by Figure 9, in terms of number of actions, the difference of about one more action on average for regular askers is small though constant. For the duration of the interaction, regular askers spend about 7% more time on average around the question, but looking at the median, the difference is substantially larger, with half the searchers spending 2.2 hours or less while the typical askers tend to spend about 68% more time, or 3.7 hours, at the median. As an interesting comparison, we also measured the average time the searchers spent searching before asking questions, to show the substantial difference between",0,,False
361,808,0,,False
362,Cumulative distribution function 0.0 0.2 0.4 0.6 0.8 1.0,0,,False
363,SearchAsk Ask,0,,False
364,1,0,,False
365,2,0,,False
366,5 10 20,0,,False
367,50,0,,False
368,# of user actions on question after it is posted,0,,False
369,Figure 9: Count of user actions in questions,0,,False
370,1.0,0,,False
371,0.8,0,,False
372,0.6,0,,False
373,Cumulative distribution function,0,,False
374,0.4,0,,False
375,0.2,0,,False
376,0.0,0,,False
377,SearchAsk Ask Search,0,,False
378,0,0,,False
379,10m 1h,0,,False
380,1d,0,,False
381,1m,0,,False
382,Duration of user actions on the question after it is posted,0,,False
383,Figure 10: Duration of user actions in questions,0,,False
384,"an interactive search session and an offline asking session, a difference that is clear to the searchers, since they are willing to spend several hours waiting for an answer to arrive, compared to a few minutes actively searching.",0,,False
385,"In summary, we showed that searchers are expecting a faster response time for their questions, which often aim to address practical problem solving tasks. On the other hand, general Yahoo! Answers askers are willing to put more effort in following up their questions. One possible reason for this behavior is that Yahoo! Answers site is often viewed by users from a more social perspective, as indicated by many users asking socially-focused (e.g., conversational) questions.",1,ad,True
386,6. RELATED WORK,0,,False
387,"As we study the transformation of unsatisfied searches into questions posted on a popular CQA site, our work is related to the work on query log analysis, searcher behavior and satisfaction prediction, and CQA question analysis.",0,,False
388,"On the search side, significant research has been done on analysis of queries and searcher behavior based on query logs. For example, understanding query intent and user goals has attracted much research effort [22][6]. Difficult queries [9][8], long and tail queries [4][5], and question-like",0,,False
389,"queries [21] have also received special research attention. Besides, searcher satisfaction and frustration [11][15][2][16] has also been actively studied, which utilized query log information for satisfaction prediction, such as relevance measures, as well as user behavior during the search session, including mouse clicks and time spent between user actions.",0,,False
390,"Donato et al. [10] identified the research missions that often associate with complex information needs and require collecting information from many pages. In our work we focused on studying the types of queries that arguably are difficult for a web search engine to satisfy, often require human to answer [19][20][18], and thus could be better handled by CQA sites. Liu et al. [18] argued that some of these needs can be satisfied with existing answers from CQA archives by harnessing the unique structure of such archives for detecting web searcher satisfaction. Our work in this paper further observed that many searchers not satisfied with search results finally posted a related question on a CQA site, which inspired our analysis of how searchers become askers.",0,,False
391,"White and Dumais [24][12] studied search engine switching behavior and developed models to predict the switching and its rationale. Although different types of searchers are focused on (they focused on searchers who turn to another search engine and issue more queries, while we focused on searchers who turn to CQA sites and post questions), we are both interested in characterizing the types of queries and searcher behavior that lead to the switchings. Our analysis shows both similar (e.g. longer sessions are more likely to involve a switch) and different characteristics (e.g. different last action before switching) compared to their study.",1,ad,True
392,"On the CQA side, there is also research effort devoted to question analysis, e.g. distinguishing conversational and informational questions [14], identifying high quality questions [3], and investigating the effects of contexts in questions on answer quality [23]. In our work, we use their classification of contextual factors to analyze the semantic difference between the query and question posted by the same user for the same need. There is also some previous work related to asker behavior analysis. For example, Adamic et al. [1] analyzed the content properties and user interaction patterns across different Yahoo! Answers categories. Gyongyi et al. [13] studied several aspects of user behavior in Yahoo! Answers, including users' activity levels, interests, and reputation. Yet, they did not study the effort that askers spend in tracing their posted questions as we studied in this paper.",1,Yahoo,True
393,7. CONCLUSIONS,0,,False
394,"Web search needs are becoming increasingly sophisticated, and the expectations have grown accordingly. As a result, quite a few search sessions end in posting a question in a Community Question Answering service, as the searcher realizes that such a service could better answer her need.",0,,False
395,"This work studies the unique properties of SearchAsk sessions: search sessions that turn into question composition. To the best of our knowledge, this paper presents the first large-scale analysis of the user transition from searching to asking. What makes our work unique is the study of the explicit connection between the search query and the corresponding question from the same user for the same need. It provides insights into some specific needs that searchers try to express on search engines, yet are not satisfied by search results, and turn to human answerers instead. We analyzed the various aspects of SearchAsk sessions, includ-",1,ad,True
396,809,0,,False
397,"ing the differences between general search-engine queries and those belonging to a SearchAsk session, the transformation of a query into a natural language question and the question composition patterns, as well as other asking behavior of searchers, compared to general askers in a CQA service.",0,,False
398,"Our findings may contribute both to search-engine optimization, as well as to better user experience in CQA sites. For example, we found out that searchers are not as patient as regular askers when waiting for answers to their questions. This finding may influence CQA sites to promote questions coming from searchers, if they want to retain their engagement. As another example, our analysis of the transitions between user actions in SearchAsk sessions, and especially the fact that question asking is typically preceded by viewing a CQA page, may help search engines. They might decide to detect such cases and explicitly promote the option of asking a question to the searcher, even before she resorts into doing it on her own. Furthermore, as this paper demonstrates, modeling the transformation of a query meant for an automated search engine into a fully specified question meant for human, provides a valuable tool for query intent and satisfaction analysis.",0,,False
399,"In future work, we intend to develop some of the directions mentioned above. One of the most intriguing ones in our view is for search engines to automatically trigger a dialog for posting questions in the right CQA forum, whenever a SearchAsk need is detected. This is just one application made possible by our study, which lays a foundation for more effective integration of automated web search and social information seeking.",1,ad,True
400,8. ACKNOWLEDGMENTS,0,,False
401,This work was supported by the National Science Foundation grant IIS-1018321 and by the Yahoo! Faculty Research and Engagement Program. The authors also would like to thank Elad Yom-Tov for the helpful discussion and the anonymous reviewers for their valuable comments.,1,Yahoo,True
402,9. REFERENCES,0,,False
403,"[1] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman. Knowledge sharing and yahoo! answers: everyone knows something. In WWW, 2008.",0,,False
404,"[2] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it if you can: a game for modeling different types of web search success using interaction data. In SIGIR, pages 345­354, 2011.",0,,False
405,"[3] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne. Finding high-quality content in social media. In WSDM, pages 183­194, 2008.",0,,False
406,"[4] P. Bailey, R. W. White, H. Liu, and G. Kumaran. Mining historic query trails to label long and rare search engine queries. ACM Trans. Web, 4:15:1­15:27, September 2010.",0,,False
407,"[5] M. Bendersky and W. B. Croft. Analysis of long queries in a large scale search log. In WSCD, 2009.",0,,False
408,"[6] A. Broder. A taxonomy of web search. SIGIR Forum, 36:3­10, September 2002.",0,,False
409,"[7] G. Buscher, R. W. White, S. Dumais, and J. Huang. Large-scale analysis of individual and task differences in search result page examination strategies. In WSDM, pages 373­382, 2012.",0,,False
410,"[8] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In SIGIR, 2006.",0,,False
411,"[9] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In SIGIR, 2002.",0,,False
412,"[10] D. Donato, F. Bonchi, T. Chi, and Y. Maarek. Do you want to take notes?: identifying research missions in yahoo! search pad. In WWW, pages 321­330, 2010.",1,ad,True
413,"[11] H. A. Feild, J. Allan, and R. Jones. Predicting searcher frustration. In SIGIR, pages 34­41, 2010.",0,,False
414,"[12] Q. Guo, R. W. White, Y. Zhang, B. Anderson, and S. T. Dumais. Why searchers switch: understanding and predicting engine switching rationales. In SIGIR, pages 335­344, 2011.",0,,False
415,"[13] Z. Gyongyi, G. Koutrika, J. Pedersen, and H. Garcia-Molina. Questioning yahoo! answers. Evolution, 2008.",0,,False
416,"[14] F. M. Harper, D. Moy, and J. A. Konstan. Facts or friends?: distinguishing informational and conversational questions in social q&a sites. In CHI, pages 759­768, 2009.",0,,False
417,"[15] A. Hassan, R. Jones, and K. L. Klinkner. Beyond dcg: user behavior as a predictor of a successful search. In WSDM, pages 221­230, 2010.",0,,False
418,"[16] S. B. Huffman and M. Hochster. How well does result relevance predict session satisfaction? In SIGIR, 2007.",0,,False
419,"[17] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM, 2008.",0,,False
420,"[18] Q. Liu, E. Agichtein, G. Dror, E. Gabrilovich, Y. Maarek, D. Pelleg, and I. Szpektor. Predicting web searcher satisfaction with existing community-based answers. In SIGIR, pages 415­424, 2011.",0,,False
421,"[19] M. R. Morris, J. Teevan, and K. Panovich. A comparison of information seeking using search engines and social networks. In ICWSM, 2010.",1,CW,True
422,"[20] M. R. Morris, J. Teevan, and K. Panovich. What do people ask their social networks, and why?: a survey study of status message q&a behavior. In CHI, 2010.",0,,False
423,"[21] B. Pang and R. Kumar. Search in the lost sense of ""query"": question formulation in web search queries and its temporal changes. In HLT, 2011.",0,,False
424,"[22] D. E. Rose and D. Levinson. Understanding user goals in web search. In WWW, pages 13­19, 2004.",0,,False
425,"[23] S. Suzuki, S. Nakayama, and H. Joho. Formulating effective questions for community-based question answering. In SIGIR, pages 1261­1262, 2011.",0,,False
426,"[24] R. W. White and S. T. Dumais. Characterizing and predicting search engine switching behavior. In CIKM, pages 87­96. ACM, 2009.",0,,False
427,"[25] S. Zhao, H. Wang, C. Li, T. Liu, and Y. Guan. Automatically generating questions from queries for community-based question answering. In IJCNLP, pages 929­937, 2011.",0,,False
428,"[26] Z. Zheng, X. Si, E. Y. Chang, and X. Zhu. K2q: Generating natural language questions from keywords with user refinements. In IJCNLP, 2011.",0,,False
429,810,0,,False
430,,0,,False

,sentence,label,data,regex
0,Unsupervised Linear Score Normalization Revisited,0,,False
1,Ilya Markov,0,,False
2,"University of Lugano Lugano, Switzerland",0,,False
3,ilya.markov@usi.ch,0,,False
4,Avi Arampatzis,0,,False
5,Democritus University of Thrace,0,,False
6,"Xanthi, Greece",0,,False
7,avi@ee.duth.gr,0,,False
8,Fabio Crestani,0,,False
9,"University of Lugano Lugano, Switzerland",0,,False
10,fabio.crestani@usi.ch,0,,False
11,ABSTRACT,0,,False
12,"We give a fresh look into score normalization for merging result-lists, isolating the problem from other components. We focus on three of the simplest, practical, and widelyused linear methods which do not require any training data, i.e. MinMax, Sum, and Z-Score. We provide theoretical arguments on why and when the methods work, and evaluate them experimentally. We find that MinMax is the most robust under many circumstances, and that Sum is-- in contrast to previous literature--the worst. Based on the insights gained, we propose another three simple methods which work as good or better than the baselines.",0,,False
13,Categories and Subject Descriptors,0,,False
14,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,0,,False
15,Keywords,0,,False
16,"Score Normalization, Distributed Retrieval",0,,False
17,1. INTRODUCTION,1,DUC,True
18,"Merging ranked-lists produced by several engines requires two steps: score normalization and combination. In distributed setups with disjoint collections, the combination step becomes trivial since each document is assigned a single score from one of the participating engines. This makes such setups ideal in isolating the normalization problem. Distributed setups usually include another step before normalization, i.e. resource selection (RS). While useful in improving efficiency, RS has been also shown to improve effectiveness significantly. We argue that this is due to the far-fromperfect quality of state-of-the-art normalization methods: in an ideal normalization, e.g. scores are normalized to probabilities of relevance, any kind of RS will hurt effectiveness by excluding sources with relevant documents; in such an ideal situation, the more systems one combines, the better the effectiveness. Consequently, the theoretical ceiling of effectiveness can only be achieved with an ideal normalization without RS, and distributed setups with disjoint collections are best for experimenting with normalization [1].",0,,False
19,"Previously proposed normalizations vary from linear to non-linear functions which may require or not training data for their estimation. While there is a rich literature on the subject, most experiments reported do not isolate the prob-",0,,False
20,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",0,,False
21,"lem as we described above [3, 4, 5]. We give a fresh look into the simplest normalization methods: linear functions which do not require any training or search engine cooperation. Being the most practical, they are also the fastest and generally have been proved to be robust and effective. Beyond an empirical comparison, we also give theoretical justifications on their implicit assumptions. To our knowledge, these assumptions have never been made explicit or they are `lost' in the large volume of the related literature. Based on the gained insights, we propose three alternative linear normalization methods.",1,ad,True
22,"In the experiments we use the gov2.1000 and gov2.30 splits of the TREC GOV2 dataset [2]. In gov2.1000 the largest 1000 hosts of the GOV2 dataset are treated as 1000 sources, and the number of sources that contain relevant documents is usually much less than 1000. In gov2.30 the collections of gov2.1000 are clustered into 30 sources; here, relevant documents appear in most of the sources. We use ten retrieval functions implemented by the Terrier toolkit 1, namely, BM25, tf-idf (Terrier and Lemur versions), language modeling (original and with Dirichlet smoothing), and a number of DFRbased functions (BM25, BB2, IFB2, InL2 and PL2). Retrieval functions are randomly assigned to sources. Topics 701-850 are used as queries. Operationally, engines return truncated rankings for efficiency reasons, thus, we only consider either the top 10 or 1000 results. Our results are summarized in Tables 1 and 2, which we will refer to throughout the paper. Statistical significance is measured with a paired t-test.  shows the significantly best baseline (MinMax, Sum or Z-Score) and  marks the modification that is significantly better than any baseline, both at the 0.05 level.",1,TREC,True
23,2. LINEAR SCORE NORMALIZATION,0,,False
24,"The MinMax method [3] scales the output score range per engine to [0, 1]. We argue that the most important assumption behind MinMax is that each source contains at least 1 relevant document, and that this document will mostlikely get ranked 1st. By assigning the same highest score to all 1st documents, they are ranked before any other in the merged list. Since it is also assumed that these documents are most-likely relevant, a high early precision is achieved which pushes higher other evaluation measures sensitive to early rank positions, e.g. MAP. Thus, MinMax owes its success to getting right the early ranks in the merged list. However, due to fact that the 1st document of each engine is assigned the same highest score, MinMax produces a roundrobin effect in merging which may impact effectiveness neg-",1,MAP,True
25,1http://terrier.org,0,,False
26,1161,0,,False
27,"atively as the number of engines increases. Indeed, MinMax shows high performance when run on 30 sources that contain sufficient relevant documents, i.e. the above assumption is satisfied. On the contrary, when run on 1000 sources, where the assumption is weakened by the sparseness of relevant documents and additionally the round-robin effect is more prominent, MinMax performance degrades considerably.",1,ad,True
28,"The Sum method [5] is similar to MinMax, but without using a fixed highest score eliminating the undesirable round-robin effect: the minimum score is shifted to 0 while the sum of all scores per ranked-list is scaled to 1: s ,"" s-min, snorm "","" s / i si. The intuition given in [4] is: under the assumption of exponentially-distributed scores, the normalization is equivalent to setting the means of score distributions of sources to be equal to each other. However, many studies on modeling scores show that the aforementioned assumption of exponentially-distributed scores does not hold in practice, especially for top-ranked documents. Our experimental results show that the Sum method almost always has the worst performance. This contradicts the results of the original work [5], however, the authors in the last-mentioned study used a meta-search setup and evaluated normalization and combination methods together--a setup that does not isolate the normalization problem.""",1,ad,True
29,"In [5] also the Z-Score method is proposed, which normalizes each score to its number of standard deviations that it is away from the mean score. However, as previously noted in [1], this method rather assumes a bell-like distribution of document scores (e.g. a Normal), where the mean would be a meaningful `neutral' score. However, in practice score distributions are highly skewed and clearly violate this assumption. Still if only the top documents are considered from each result list, they are likely to be relevant, and therefore the distribution of their scores may be close to that of relevant documents, i.e. likely a Normal. Indeed, our results show that when only the top-10 documents are retrieved from each source, Z-Score usually shows higher performance than when applied on the top-1000 documents.",0,,False
30,"So far we have seen that methods that assume particular score distribution shapes, such as Sum and Z-Score, are worse, and that MinMax which does not make such assumptions is better. In other words, the sum or the mean of scores do not seem to be a good statistics for normalization, and that we should be looking into the direction of MinMax for developing a better normalization. We also know that using a fixed highest score should be preferably avoided so as to eliminate the round-robin effect, as well as, to be able to down-weigh sources of no relevance (haunting the main assumption of MinMax). Another problem rarely mentioned that all three methods above have is that they are greatly affected by the minimum score seen, or else, the chosen truncation point of the rankings (e.g. the minimum score of top-10 documents is usually very different from the minimum score of top-1000).",0,,False
31,"We will first try to deal with the minimum-score problem. The lowest theoretical score of many scoring functions is 0. Thus, making the assumption that the most non-relevant documents usually score at 0, the minimum in MinMax function should also be set to 0. This way we obtain the Max method: snorm ,"" s/max. Our results show that Max, although simpler, almost always has the same or higher performance than that of the MinMax method.""",0,,False
32,Let us now try to deal with non-relevant sources which vio-,0,,False
33,MinMax Sum Z-Score Max MMStdv UV,0,,False
34,MAP 0.0700 0.0171 0.0367 0.0722 0.0632 0.0182,1,MAP,True
35,gov2.30,0,,False
36,gov2.1000,0,,False
37,p@10 p@100 MAP p@10 p@100,1,MAP,True
38,0.1953 0.1674 0.0027 0.0161 0.0140,0,,False
39,0.0134 0.0292 0.0004 0.0013 0.0019,0,,False
40,0.0819 0.0935 0.0014 0.0013 0.0054,0,,False
41,0.1953 0.1734 0.0023 0.0161 0.0140,0,,False
42,0.1738 0.1348 0.0038 0.0114 0.0088,0,,False
43,0.1248 0.0676 0.0006 0.0013 0.0009,0,,False
44,Table 1: Top 1000 documents are retrieved.,0,,False
45,MinMax Sum Z-Score,0,,False
46,Max MMStdv UV,0,,False
47,gov2.30 MAP p@10 0.0372 0.1966 0.0355 0.1785 0.0353 0.1839 0.0397 0.1953 0.0366 0.1604 0.0381 0.1980,1,MAP,True
48,gov2.1000 p@100 MAP p@10 p@100 0.1437 0.0027 0.0161 0.0140 0.1401 0.0029 0.0013 0.0140 0.1413 0.0031 0.0007 0.0162 0.1656 0.0028 0.0161 0.0140 0.1432 0.0025 0.0054 0.0139 0.1498 0.0049 0.0228 0.0225,1,MAP,True
49,Table 2: Top 10 documents are retrieved.,0,,False
50,late the main assumption of MinMax. Scoring functions aim,0,,False
51,at assigning scores in a way that relevant documents have,0,,False
52,very different scores from non-relevant documents. There-,0,,False
53,"fore, if the standard deviation  of scores in a ranked-list",0,,False
54,"is high, the list is likely to contain both relevant and non-",0,,False
55,"relevant documents. If  is low, the list is likely to contain",0,,False
56,either only relevant or only non-relevant documents. On,0,,False
57,"one hand, long ranked-lists (e.g. 1000 documents) are likely",0,,False
58,"to contain many non-relevant documents and, therefore, we",0,,False
59,prefer those that have high standard deviation (i.e. they also,0,,False
60,"contain many relevant documents). In this case, the follow-",0,,False
61,ing linear modification makes sense:,0,,False
62,snorm,0,,False
63,",",0,,False
64,s-min max-min,0,,False
65,.,0,,False
66,"We call it MM-Stdv. On the other hand, short ranked lists",0,,False
67,(e.g. 10 documents) may contain only relevant documents,0,,False
68,"and, therefore, we might prefer the lists with low standard",0,,False
69,deviation. Unit-variance linear modification (UV) is similar,0,,False
70,to Z-Score and scale-invariant but does not shift the mean,0,,False
71,"to 0: snorm , score/. Our results support both formulas",0,,False
72,depending on the situation.,0,,False
73,3. CONCLUSIONS,0,,False
74,"We isolated the normalization problem and found that MinMax is the most robust method under many circumstances, Z-Score and Sum may perform well when some conditions are met, and Sum is worse than previous literature suggested. Furthermore, we gave theoretical insights on why and when these methods work or fail, and proposed three new methods that work as good or better than the baselines.",0,,False
75,4. REFERENCES,0,,False
76,"[1] A. Arampatzis and J. Kamps. A signal-to-noise approach to score normalization. In Proceeding of the ACM CIKM, pages 797­806, 2009.",0,,False
77,"[2] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. In Proceedings of the ACM CIKM, pages 1277­1286. ACM, 2009.",0,,False
78,"[3] J. H. Lee. Analyses of multiple evidence combination. In Proceedings of the ACM SIGIR, pages 267­276. ACM, 1997.",0,,False
79,"[4] R. Manmatha and H. Sever. A formal approach to score normalization for meta-search. In Proceedings of the HLT, pages 98­103. MKP Inc., 2002.",0,,False
80,"[5] M. Montague and J. A. Aslam. Relevance score normalization for metasearch. In Proceedings of the ACM CIKM, pages 427­433. ACM, 2001.",0,,False
81,1162,0,,False
82,,0,,False

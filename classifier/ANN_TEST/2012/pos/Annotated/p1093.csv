,sentence,label,data,regex
0,Incorporating Statistical Topic Information in Relevance Feedback,1,corpora,True
1,Karla Caballero,0,,False
2,"UC, Santa Cruz Santa Cruz CA, USA",0,,False
3,karla@soe.ucsc.edu,0,,False
4,Ram Akella,0,,False
5,"UC, Santa Cruz Santa Cruz CA, USA",0,,False
6,akella@soe.ucsc.edu,0,,False
7,ABSTRACT,0,,False
8,"Most of the relevance feedback algorithms only use document terms as feedback (local features) in order to update the query and re-rank the documents to show to the user. This approach is limited by the terms of those documents without any global context. We propose to use statistical topic modeling techniques in relevance feedback to incorporate a better estimate of context by including global information about the document. This is particularly helpful for difficult queries where learning the context from the interactions with the user is crucial. We propose to use the topic mixture information obtained to characterize the documents and learn their topics. Then, we rank documents incorporating positive and negative feedback by fitting a latent distribution for each class of documents online and combining all the features using Bayesian Logistic Regression. We show results using the OHSUMED dataset for 3 different variants and obtain higher performance, up to 12.5% in Mean Average Precision (MAP).",1,corpora,True
9,Categories and Subject Descriptors,0,,False
10,"H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Relevance Feedback, Document Filtering",0,,False
11,General Terms,0,,False
12,"Algorithms, Experimentation",0,,False
13,Keywords,0,,False
14,"Relevance Feedback,Topic Models, Language Models",0,,False
15,1. INTRODUCTION,1,DUC,True
16,"Relevance feedback has been studied extensively in Information Retrieval as a form of incorporating feedback from the user to refine the results retrieved. The authors in [5] concluded that negative feedback is also valuable to improve the ranking. However, the need to capture broader context in difficult queries is still a challenge. The authors in [2] have showed that including global features and using clusters can improve the retrieval performance significantly. Thus, statistical topic modeling provides a robust and automatic method to incorporate context to the user feedback.",1,corpora,True
17,Main contact.,0,,False
18,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",0,,False
19,"Previous approaches have used statistical topic models to represent documents according to their latent topic content and use this representation in information retrieval [1, 4]. Authors in [4], use topics as a form of smoothing the Language Model used in retrieval. However, this approach does not address the incorporation of relevance feedback. Recent work from [1] explores the use of topics as a form to perform the query expansion for relevance feedback. However, this action might make the query noisier because the top topic terms might not contribute to a better discrimination of the relevant documents. In addition, these terms might not be distinctive across different topics.",1,ad,True
20,"We propose to include the topic information as feedback using the document topic mixture instead of the document word mixture. We first estimate the topic mixture for each document in the corpus using LDA and save it as meta data. Given an initial query we use a standard retrieval engine, Language Models for this case, to show the first set of documents to the user and obtain relevance judgments. Then, we assume that topic mixtures for feedback documents are observed. We then define two latent Dirichlet distributions: one for relevant documents and another for nonrelevant documents. We fit these distributions iteratively, by finding a sufficient statistic and maximizing the likelihood of observing this statistic. To score the documents, we use Bayesian Logistic Regression. This function results in a very efficient scoring function, and incorporates the benefits of active learning. Under this model, we incorporate positive and negative feedback, and context based on topics in the interaction without changing the query. We also provide efficient updates of the latent distributions based on topics.",1,ad,True
21,2. METHODOLOGY,0,,False
22,In this section we describe how we incorporate the topic,1,corpora,True
23,mixture of feedback documents as a global measure in con-,0,,False
24,"trast to query expansion. To achieve this, we estimate the",0,,False
25,"topic mixtures of the documents, i, for K topics in the cor-",0,,False
26,"pus using LDA off-line. Given a initial ranking, the user",0,,False
27,provides relevance feedback which is used to fit the latent,0,,False
28,"relevant/non-relevant distributions of topic mixtures. Thus,",0,,False
29,we assume two Dirichlet distributions: one for the relevant,0,,False
30,"set of documents R, and one for the non-relevant documents R¯ . Therefore, we have:",0,,False
31,P (i|R),0,,False
32,Dirichlet(R),0,,False
33,",",0,,False
34,(,0,,False
35,"K k,1",0,,False
36,"k,R",0,,False
37,),0,,False
38,"K k,1",0,,False
39,"(k,R)",0,,False
40,K,0,,False
41,"k,R -1 i,k k,1",0,,False
42,"for R and R¯ . Then, we calculate the log-probability of the document being generated by those distributions:",0,,False
43,1093,0,,False
44,K,0,,False
45,K,0,,False
46,K,0,,False
47,"LogP (i|) ,"" log ( k)- log (k)+ (k-1) log(i,k)""",0,,False
48,"k,1",0,,False
49,"k,1",0,,False
50,"k,1",0,,False
51,We denote these scores as P Ri and P R¯i respectively. To,0,,False
52,update the latent distributions of the relevant R and non-,0,,False
53,"relevant topics R¯, we use the topic content from the docu-",0,,False
54,"ments labeled by the user as relevant, R, and non-relevant,",0,,False
55,"R¯ , after each interaction. The Dirichlet distribution guar-",0,,False
56,antees a unique maximum when the Maximum Likelihood,0,,False
57,"(ML) is estimated for . Moreover, a sufficient statistics,",0,,False
58,"SS, can be estimated to update this distribution as more",0,,False
59,observations are available. We can update SS efficiently,0,,False
60,"without keeping previous document feedback. The initial value of the sufficient statistic SSk(0,R) for the relevant topic k and its update from the interaction j is described by:",0,,False
61,"S Sk(0,R)",0,,False
62,",",0,,False
63,1 NR(0),0,,False
64,"log i,k",0,,False
65,iR0,0,,False
66,"S Sk(j,R)",0,,False
67,",",0,,False
68,NR(j-1) NR(j),0,,False
69,S,0,,False
70,"Sk(j,R-1)",0,,False
71,+,0,,False
72,1 NR(j),0,,False
73,"log i,k",0,,False
74,iRj,0,,False
75,"where NR(j) ,"" NR(j-1) + |Rj |, and |Rj | is the total number of relevant documents at the j-th interaction. Given SSR(j) and SSR(¯j), we use the method proposed in [3] to calculate the ML estimator for (Rj) and (R¯j). In addition to these distributions, we use the topic-based Language Model PT W for document i as follows:""",1,ad,True
76,K,0,,False
77,"PT W,i(w|i, ^) , P (w|z ,"" k, ^k)P (z "", k|i)",0,,False
78,"k,1",0,,False
79,where ^ are the word mixture for the topics obtained from,0,,False
80,"LDA. Thus, the score ST W,i for query Q with terms q is defined as:",0,,False
81,"ST W,i ,",0,,False
82,"PT W,i(w ,"" q|i, ^)""",0,,False
83,qQ,0,,False
84,To combine the scores from the latent relevant/non-relevant,0,,False
85,"topic mixtures and the topic-based Language Model, we use",0,,False
86,"the Bayesian Logistic Regression approach [5]. Let yi ,"" {+1, -1} be the relevant/non-relevant label for document,""",0,,False
87,we have the score function:,0,,False
88,"P (yi|, di)",0,,False
89,",",0,,False
90,1,0,,False
91,+,0,,False
92,1 exp(-T diyi),0,,False
93,"where di is the feature vector scores: P Ri, P R¯i, ST W,i.  is a parameter vector assumed to be normally distributed and updated in a Bayesian form. Here, the distribution of  from the j-th iteration is taken as prior distribution for the next iteration. To approximate the posterior distribution we",0,,False
94,use the Laplace approximation as discussed in [5].,0,,False
95,3. RESULTS,0,,False
96,"We test our method using the OHSUMED dataset which consists of 196, 000 medical abstracts and 3, 506 relevance labels for 63 queries from the Document Filtering Track from TREC 4. As suggested in the track, we assume unobserved labels as non-relevant. We fit the LDA model using K ,"" 50 topics, which is the number of topics with highest performance based on Empirical Likelihood. To test the impact of topic information, we use standard Language Model (LM) with Dirichlet smoothing described in [4] as baseline. This score is used with Bayesian Logistic Regression. We test 3 variants of the model and the baseline: LM as baseline;""",1,Track,True
97,Table 1: Results of Topic feedback using 50 topics,0,,False
98,in the OHSUMED dataset,0,,False
99,Method,0,,False
100,P@10 MAP DiscGain,1,MAP,True
101,LM,1,LM,True
102,0.3968 0.4286 0.5660,0,,False
103,LM +ST W,1,LM,True
104,0.4206 0.4557,0,,False
105,LM +ST W +P R,1,LM,True
106,0.2968 0.3307,0,,False
107,LM +ST W +P R+P R¯ 0.4698 0.5141,1,LM,True
108,0.6315 0.5590 0.6580,0,,False
109,LM+ST W ; LM+ST W +P R; LM+ST W +P R+P R¯. We calculate the initial ranking using LM and asked for feedback,1,LM,True
110,"until we have at least one relevant and one non-relevant documents. We use 10 feedback documents and estimate precision at 10 (P@10), Mean Average Precision (MAP), and Discounted Gain (DiscGain). There are two relevance level labels available in the dataset, {1, 2}, that are assumed equally, {+1} for P@10 and MAP. However for DiscGain, we use both labels in the evaluation.",1,MAP,True
111,"Table 1 shows the results for the variants tested. We observe that the LM+ST W performs better than the baseline. This score is similar to the LDA-based retrieval proposed in [4] but the value of the linear combination parameters  is fitted based on the feedback as opposed to a corpus-wide parameter. When we incorporate only the score from the relevant distribution of topics P R, the performance decreases. However, when the score for the non-relevant distribution P R¯ is incorporated, the performance is the highest. This shows the value of negative feedback reported previously in [5]. We notice that, the combination of P R and P R¯ is equivalent to the log of the likelihood ratio test (probabilistic ranking principle) weighted by . This also explains why both scores should be included in the model.",1,LM,True
112,"We observe that the combination of the four scores improves the general performance by: 11.6% P@10, 12.8%",0,,False
113,"MAP, and 4.6% DiscGain respect topic-based language model (LM+ST W ). This demonstrates, the power of statistical topic modeling in relevance feedback.",1,MAP,True
114,4. CONCLUSION AND FUTURE WORK,0,,False
115,"We have presented a method to incorporate statistical topic information in relevance feedback without changing the query. Results show that including the mixture of topics in relevance feedback improves the performance by pruning the search space, and adding context to the query. As future work, we plan to incorporate a policy to decide when to update the parameters of the relevant and non-relevant topic distribution optimally.",1,corpora,True
116,5. ACKNOWLEDGMENTS,0,,False
117,This work is partially funded by CONACYT grant 207751 and SAP Gift Support,1,AP,True
118,6. REFERENCES,0,,False
119,"[1] D. Andrzejewski and D. Buttler. Latent topic feedback for information retrieval. In Proceedings of the 17th ACM SIGKDD conference, KDD '11, pages 600­608, 2011.",0,,False
120,"[2] K. S. Lee, W. B. Croft, and J. Allan. A cluster-based resampling method for pseudo-relevance feedback. In Proceedings of the SIGIR conference, pages 235­242, 2008.",0,,False
121,"[3] T. Minka. Estimating a dirichlet distribution. Technical report, 2003.",0,,False
122,"[4] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In Proceedings of the 29th ACM SIGIR conference, SIGIR '06, pages 178­185, 2006.",1,ad-hoc,True
123,"[5] Z. Xu and R. Akella. A bayesian logistic regression model for active relevance feedback. In Proceedings of the 31st ACM SIGIR conference, SIGIR '08, pages 227­234, 2008.",0,,False
124,1094,0,,False
125,,0,,False

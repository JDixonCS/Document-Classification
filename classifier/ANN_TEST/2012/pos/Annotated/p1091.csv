,sentence,label,data,regex
0,Impact of Assessor Disagreement on Ranking Performance,0,,False
1,Pavel Metrikov Virgil Pavlu Javed A. Aslam,0,,False
2,"College of Computer and Information Science Northeastern University, Boston, MA, USA",0,,False
3,"{metpavel, vip, jaa}@ccs.neu.edu",0,,False
4,ABSTRACT,0,,False
5,"We consider the impact of inter-assessor disagreement on the maximum performance that a ranker can hope to achieve. We demonstrate that even if a ranker were to achieve perfect performance with respect to a given assessor, when evaluated with respect to a different assessor, the measured performance of the ranker decreases significantly. This decrease in performance may largely account for observed limits on the performance of learning-to-rank algorithms.",0,,False
6,Categories and Subject Descriptors: H. Information Systems; H.3 Information Storage and Retrieval; H.3.3 Information Search and Retrieval:Retrieval models,0,,False
7,"General Terms: Experimentation, Measurement, Theory",0,,False
8,"Keywords: Inter-assessor Disagreement, Learning-to-Rank, Evaluation",0,,False
9,1. INTRODUCTION,1,DUC,True
10,"In both Machine Learning and Information Retrieval, it is well known that limitations in the performance of ranking algorithms can result from several sources, such as insufficient training data, inherent limitations of the learning/ranking algorithm, poor instance features, and label errors. In this paper we focus on performance limitations due solely to label ""errors"" which arise due to inter-assessor disagreement.",0,,False
11,"Consider a training assessor A that provides labels for training data and a testing assessor B that provides labels for testing data. Even if a ranker can produce a perfect list as judged by A, its performance will be suboptimal with respect to B, given inevitable inter-assessor disagreement. In effect, no ranking algorithm can simultaneously satisfy two or more disagreeing assessors (or users). Thus, there are inherent limitations in the performance of ranking algorithms, independent of the quality of the learning/ranking algorithm, the availability of sufficient training data, the quality of extracted instance features, and so on.",0,,False
12,"We model inter-assessor disagreement with a confusion matrix C, where cij corresponds to the (conditional) probability that a document labeled i by testing assessor B will be labeled j by training assessor A, for some given set of label grades such as {0, 1, 2, 3, 4}. Given such a model of interassessor disagreement, we ask the question, ""What is the",1,ad,True
13,9This work supported by NSF grant IIS-1017903.,0,,False
14,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",0,,False
15,"expected performance of a ranked list optimized for training assessor A but evaluated with respect to testing assessor B?"" We approach this question in two ways, via simulation and closed form approximation. In the former case, we use the confusion matrix C to probabilistically generate training labels A from testing labels B, optimally rank documents according to A, and evaluate with respect to B. In the latter case, we analytically derive a closed-form approximation to this limiting performance, as measured by nDCG.",0,,False
16,"Given a confusion matrix C modeling inter-assessor disagreement, one can apply our results to any learning-to-rank dataset. The limiting nDCG values obtained correspond to reasonable upper bounds on the nDCG performance of any learning-to-rank algorithm, even one given unlimited training data and perfect features. Considering the performance of existing algorithms on these datasets, and comparing with the upper bounds we derive, one can argue that learning-torank is approaching reasonable limits on achievable performance.",0,,False
17,2. ASSESSOR DISAGREEMENT MODEL,0,,False
18,"Much research in the IR community has focused on addressing the problem of system evaluation in the context of missing, incomplete, or incorrect document judgments [1]. Soboroff and Carterette [4] provide an in-depth analysis of the effect of assessor disagreement on the Million Query Track evaluation techniques. Both assessors and users often disagree on the degree of document relevance to a given query, and we model such disagreement with a confusion matrix C as described above. On data sets with multiple assessments per query-document pair, such as the TREC Enterprise Track [2], these confusion matrices can be directly estimated from data, and they can be obtained from user studies as well [3, 5].",1,ad,True
19,"For any ranked list returned by a system, the expected limiting nDCG due to assessor disagreement can be formulated as a function of (1) the disagreement model C and (2) the number of assessed documents and their distribution over the label classes. One way to compute this expected nDCG is numerical simulation: For every document d having testing label id in the ranked list, we randomly draw an alternative label jd with the probability cij; we then sort the ranked list in decreasing order of {jd} and evaluate its nDCG performance with respect to labels {id}. This simulation is repeated multiple times and the results averaged to obtain an accurate estimate of the expected limiting nDCG.",0,,False
20,"In our first experiment, we test whether the inter-assessor confusion matrix C alone can be used to estimate the limiting nDCG value. We do so by considering data sets that have multiple judgments per query-document pair, such as were collected in the TREC Enterprise Track where each",1,TREC,True
21,1091,0,,False
22,Real NDCG Bound Real NDCG Bound,0,,False
23,Real vs. Simulated NDCG Bounds (Enterprise C),0,,False
24,Real vs. Simulated NDCG Bounds (MSR C),0,,False
25,1,0,,False
26,1,0,,False
27,0.9,0,,False
28,0.9,0,,False
29,0.8,0,,False
30,0.8,0,,False
31,0.7,0,,False
32,0.7,0,,False
33,0.6,0,,False
34,SG,0,,False
35,0.6,0,,False
36,SG,0,,False
37,BG,0,,False
38,BG,0,,False
39,0.5,0,,False
40,GS,0,,False
41,0.5,0,,False
42,GS,0,,False
43,BS,0,,False
44,BS,0,,False
45,0.4,0,,False
46,GB,0,,False
47,0.4,0,,False
48,GB,0,,False
49,SB,0,,False
50,SB,0,,False
51,0.3,0,,False
52,0.3,0,,False
53,0.3,0,,False
54,0.4,0,,False
55,0.5,0,,False
56,0.6,0,,False
57,0.7,0,,False
58,0.8,0,,False
59,0.9,0,,False
60,1,0,,False
61,0.3,0,,False
62,0.4,0,,False
63,0.5,0,,False
64,0.6,0,,False
65,0.7,0,,False
66,0.8,0,,False
67,0.9,0,,False
68,1,0,,False
69,Simulated NDCG Bound,0,,False
70,Simulated NDCG Bound,0,,False
71,"Figure 1: Applying C ENT model (left) and C MSR model (right) to TREC enterprise data. X-axis is the simulated nDCG upper bound, while Y-axis is the actual nDCG assessor disagreement measured between 2 TREC assessors; pairs of assessor type (""Gold-Silver"" as GS) are indicated by colors.",1,TREC,True
72,"topic was judged by three assessors: a gold assessor G (expert on task and topic), a silver assessor S (expert at task but not at topic), and a bronze assessor B (expert at neither). Each G, S, and B set of assessments can take on the role of training or testing assessor, as described above, giving rise to six possible combinations: GS, GB, SG, SB, BG, BS. For each such combination, such as GS, the optimal ranked list can be computed with respect to G and evaluated with respect to S, resulting in a real suboptimal nDCG. The GS confusion matrix can also be computed from the data given and the simulation described above performed, yielding an estimated limiting nDCG. These actual and estimated limiting nDCG values can then be compared.",0,,False
73,"Using the TREC Enterprise data, Figure 1 compares the estimated limiting nDCG obtained through simulation with a confusion matrix (x-axis) with the real suboptimal nDCG (y-axis) obtained from different assessors. The left plot uses a confusion matrix CENT obtained from the TREC Enterprise data itself, as described above, while the right plot uses a confusion matrix CMSR obtained from a user study conducted by Microsoft Research [3]. Note that the more accurate confusion matrix yields better simulated results, as expected, and that the confusion matrix alone can be used to accurately estimate limiting nDCG values in most cases.",1,TREC,True
74,"Given that a confusion matrix alone can be used, via simulation, to estimate limiting nDCG performance, we next consider other data sets and their associated real or estimated confusion matrices. Yandex [5] conducted user studies to obtain confusion matrices specific to Russian search (CY anR) and to Ukrainian search (CY anU ), and these were shown to improve learning-to-rank performance if the learner was given such models as input. Table 1 presents the estimated limiting nDCG values when applying three different confusion matrices to two learning-to-rank data sets. For comparison, the last column in the table presents the actual best known performance of a learning algorithm on these data sets. Consider the difference between the estimated limiting nDCG bounds (middle three columns) and known ranking performance (last column): If CMSR is a good model of assessor disagreement for these data sets, then the known learning-to-rank performance is reasonably close to the limiting bound, and little improvement is possible. On the other hand, if CYanU is a better model of inter-assessor disagreement, then learning algorithms have room for improvement.",0,,False
75,3. CLOSED FORM APPROXIMATION,1,AP,True
76,"Let L ,"" {0, 1, 2, 3, 4} be the set of relevance grades, nk the number of documents with reference label k  L, n the total""",1,ad,True
77,Collection MSLR30K(SIM) MSLR30K(CFA),1,MSLR,True
78,Yahoo(SIM) Yahoo(CFA),1,Yahoo,True
79,C MSR 0.780 0.794 0.861 0.887,0,,False
80,C YanR 0.867 0.869 0.920 0.919,0,,False
81,C YanU 0.900 0.898 0.944 0.938,0,,False
82,LearnToRank 0.741,0,,False
83,0.801,0,,False
84,Table 1: nDCG upper bounds derived from disagreement models C applied to popular learning-torank data sets. SIM rows are simulated values; the CFA rows are closed form approx. Last column is best known learning-to-rank performance.,0,,False
85,"number of documents in the rank-list, and Prank(i, r) the probability that the rank of a given document with reference label i is r, as ordered by the alternative labels j. One can then show that the expected nDCG as measured by reference labels is",0,,False
86,E [nDC G],0,,False
87,",",0,,False
88,h,0,,False
89,I,0,,False
90,1 dealDC,0,,False
91,G,0,,False
92,·PiL,0,,False
93,ni,0,,False
94,·,0,,False
95,gain(i),0,,False
96,·,0,,False
97,Pn,0,,False
98,"r,1",0,,False
99,"i Prank (i,r)",0,,False
100,discount(r),0,,False
101,where,0,,False
102,"Prank(i, r)",0,,False
103,",",0,,False
104,P,0,,False
105,jL,0,,False
106,h cij,0,,False
107,·,0,,False
108,"Pr-1
h=0

Pn-h-1
s=r-1-h

ij (h,s) i",0,,False
109,s+1,0,,False
110,"and ij(h, s) is the probability that other s documents have",0,,False
111,"the same alternative label j, and other h documents have",0,,False
112,"alternative label higher than j, given that a particular doc-",0,,False
113,ument with reference label i has alternative label j. Com-,0,,False
114,"puting ij(h, s) straightforwardly is inefficient for even moderately long rank-lists, with a running time of O(n2|L|).",0,,False
115,We instead employ a closed form approximation (CFA),1,ad,True
116,"based on approximating , a sum-product of binomial con-",0,,False
117,"ditional distributions, with a Gaussian joint distribution of",0,,False
118,"two variables (h + s, s). This approximation becomes more",0,,False
119,accurate as rank-lists get longer. For a fixed i and j we have,0,,False
120,`h+s´,0,,False
121,s,0,,False
122,"N ij (µ,",0,,False
123,"),",0,,False
124,µ,0,,False
125,",",0,,False
126,`µh+s,0,,False
127,µs,0,,False
128,"´,",0,,False
129,and,0,,False
130,",",0,,False
131,"` h2+s cov h+s,s",0,,False
132,"cov s,h+s s2",0,,False
133,´,0,,False
134,where,0,,False
135,(1),0,,False
136,aij,0,,False
137,",",0,,False
138,P,0,,False
139,kj,0,,False
140,"cik ,",0,,False
141,(2),0,,False
142,µh+s,0,,False
143,",",0,,False
144,-aij,0,,False
145,+,0,,False
146,P,0,,False
147,kL,0,,False
148,nk,0,,False
149,·,0,,False
150,"akj ,",0,,False
151,(3),0,,False
152,µs,0,,False
153,",",0,,False
154,-cij,0,,False
155,+,0,,False
156,P,0,,False
157,kL,0,,False
158,nk,0,,False
159,·,0,,False
160,"ckj ,",0,,False
161,and,0,,False
162,h2+s,0,,False
163,",",0,,False
164,-aij,0,,False
165,·,0,,False
166,(1,0,,False
167,-,0,,False
168,aij ),0,,False
169,+,0,,False
170,P,0,,False
171,kL,0,,False
172,nk,0,,False
173,·,0,,False
174,akj,0,,False
175,·,0,,False
176,(1,0,,False
177,-,0,,False
178,akj ),0,,False
179,s2,0,,False
180,",",0,,False
181,-cij,0,,False
182,·,0,,False
183,(1,0,,False
184,-,0,,False
185,cij ),0,,False
186,+,0,,False
187,P,0,,False
188,kL,0,,False
189,nk,0,,False
190,·,0,,False
191,ckj,0,,False
192,·,0,,False
193,(1,0,,False
194,-,0,,False
195,ckj ),0,,False
196,"cov s,h+s",0,,False
197,",",0,,False
198,-cij,0,,False
199,·,0,,False
200,(1,0,,False
201,-,0,,False
202,aij ),0,,False
203,+,0,,False
204,P,0,,False
205,kL,0,,False
206,nk,0,,False
207,·,0,,False
208,ckj,0,,False
209,·,0,,False
210,(1,0,,False
211,-,0,,False
212,akj ).,0,,False
213,"We can approximate E[nDCG] in O(n2) time given that the ""spread"" of the Gaussian grows as O( n) per component. The CFA rows of Table 1 show closed form approximations for comparison with simulated nDCG upper bounds.",1,ad,True
214,4. CONCLUSION,0,,False
215,We present a simple probabilistic model of assessor disagreement and results which indicate that the performance of learning-to-rank algorithms may be approaching inherent limits imposed by such disagreement.,0,,False
216,5. REFERENCES,0,,False
217,"[1] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P.",0,,False
218,"de Vries, and E. Yilmaz. Relevance assessment: Are",0,,False
219,"judges exchangeable and does it matter? SIGIR, 2008.",0,,False
220,"[2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff. Overview of the TREC-2007 Enterprise Track.",1,TREC,True
221,"[3] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. Here or there: Preference judgments for relevance. In ECIR, 2008.",0,,False
222,"[4] B. Carterette and I. Soboroff. The effect of assessor error on ir system evaluation. In SIGIR, 2010.",0,,False
223,"[5] A. Gulin, I. Kuralenok, and D. Pavlov. Winning the transfer learning track of Yahoo!'s learning to rank challenge with YetiRank. Journal of Machine Learning Research, 2011.",1,Yahoo,True
224,1092,0,,False
225,,0,,False

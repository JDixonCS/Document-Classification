,sentence,label,data,regex
0,Rhetorical Relations for Information Retrieval,0,,False
1,Christina Lioma,0,,False
2,Computer Science University of Copenhagen,0,,False
3,Denmark,0,,False
4,c.lioma@diku.dk,0,,False
5,Birger Larsen,0,,False
6,Royal School of Library and Information Science,0,,False
7,Copenhagen Denmark,0,,False
8,blar@iva.dk,0,,False
9,Wei Lu,0,,False
10,School of Information Management,0,,False
11,Wuhan University China,0,,False
12,reedwhu@gmail.com,0,,False
13,ABSTRACT,0,,False
14,"Typically, every part in most coherent text has some plausible reason for its presence, some function that it performs to the overall semantics of the text. Rhetorical relations, e.g. contrast, cause, explanation, describe how the parts of a text are linked to each other. Knowledge about this socalled discourse structure has been applied successfully to several natural language processing tasks. This work studies the use of rhetorical relations for Information Retrieval (IR): Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a document's rhetorical relations be useful to IR?",0,,False
15,We present a language model modification that considers rhetorical relations when estimating the relevance of a document to a query. Empirical evaluation of different versions of our model on TREC settings shows that certain rhetorical relations can benefit retrieval effectiveness notably (> 10% in mean average precision over a state-of-the-art baseline).,1,TREC,True
16,Categories and Subject Descriptors,0,,False
17,H.3.3 [Information Search and Retrieval]: Retrieval Models; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing--linguistic processing,0,,False
18,Keywords,0,,False
19,"Rhetorical relations, discourse structure, retrieval model, probabilistic retrieval",0,,False
20,1. INTRODUCTION,1,DUC,True
21,"According to discourse analysis, every part in most coherent text tends to have some plausible reason for its presence, some function that it performs to the overall semantics of the text. Rhetorical relations, e.g. contrast, explanation, condition, are considered critical for text interpretation, because they signal how the parts of a text are linked to each other to form a coherent whole [23]. Unlike grammatical relations, which are generally explicitly manifest in",0,,False
22,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.",1,ad,True
23,Figure 1: Rhetorical relations example (from [11]).,0,,False
24,"language, rhetorical relations may be unstated. The goal of discourse analysis is therefore to infer rhetorical relations, and specifically to identify their span, constraints and function.",0,,False
25,"There is a large body of research on both descriptive and predictive models of rhetorical structure and discourse analysis in natural language text. For instance, annotation projects have taken significant steps towards developing semantic [12, 18] and discourse [5] annotated corpora. Some of these annotation efforts have already had a computational impact, making it possible to automatically induce semantic roles [15] and to automatically identify rhetorical relations [14], achieving near-human levels of performance on certain tasks [27]. In addition, applications of discourse analysis to automatic language processing tasks such as summarisation or classification (overviewed in section 2) indicate that rhetorical relations can enhance the performance of welltrained natural language processing systems.",1,corpora,True
26,"Motivated by these advances, this work brings perspectives from discourse analysis into Information Retrieval (IR) with the aim of investigating if and how rhetorical relations can benefit retrieval effectiveness. Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a document's rhetorical relations be useful to IR? For example, consider the rhetorical relations of the text shown in Figure 1 (borrowed from [11]). Should some of the terms in this sentence be given extra weight by an IR system, according to their rhetorical relations? Can some rhetorical relations be considered more informative and hence more useful for IR ranking than others? These questions have been posed before (see discussion in section 2), however to our knowledge this is the first time that a principled integration of rhetorical relations into a probabilistic IR model improves precision by > 10%.",1,ad,True
27,931,0,,False
28,"Reasoning about query - document relevance using the language modeling formalism [9], we present a model that conditions the probability of relevance between a query and a document on the rhetorical relations occurring in that document. We present an application of this model to an IR re-ranking task, where, given a list of documents initially retrieved for a query, the goal is to improve the ranking of the documents by refining their estimation of relevance to the query. Experimental evaluation of different versions of our model on TREC data and standard settings demonstrates that certain rhetorical relations can be beneficial to retrieval, with notable improvements to retrieval effectiveness (> 10% in mean average precision and other standard TREC evaluation measures over a state-of-the-art baseline).",1,TREC,True
29,2. RELATED WORK,0,,False
30,"Discourse analysis and rhetorical structures have been studied in the context of several automatic text processing applications. This has been partly enabled by the availability of discourse parsers - see [11, 14] for up-to-date overviews of discourse parsing technology. Studies of discourse analysis in relation to IR and its broader applications are briefly overviewed below. For a more general overview of discourse analysis approaches, see Wang et al. [33], section 2.",1,ad,True
31,Sun & Chai [28] investigate the role of discourse processing and its implication on query expansion for a sequence of questions in scenario-based context question answering (QA). They consider a sequence of questions as a mini discourse. An empirical examination of three discourse theoretic models indicates that their discourse-based approach can significantly improve QA performance over a baseline of plain reference resolution.,0,,False
32,"In a different task, Wang et al. [33] parse Web user forum threads to determine the discourse dependencies between posts in order to improve information access over Web forum archives. They present three different methods for classifying the discourse relationships between posts, which are found to outperform an informed baseline.",1,ad,True
33,"Heerschop et al. [16] perform document sentiment analysis (partly) based on a document's discourse structure. They hypothesise that by splitting a text into important and less important text spans, and by subsequently making use of this information by weighting the sentiment conveyed by distinct text spans in accordance with their importance, they can improve the performance of a sentiment classifier. A document's discourse structure is obtained by applying rhetorical structure theory on a sentence level. They report a 4.5% improvement in sentiment classification accuracy when considering discourse, in comparison to a nondiscourse based baseline. Similarly to this study, Somasundaran et al. [26] report improvements to opinion polarity classification when using discourse, and Morato et al. [24] report a positive dependence between classification performance and certain discourse variables. An overview of discourse analysis for opinion detection can be found in Zhou et al. [36].",0,,False
34,"In the area of text compression, Louis et al. [21] study the usefulness of rhetorical relations between sentences for summarisation. They find that most of the significant rhetorical relations are associated to non-discriminative sentences, i.e. sentences that are not important for summarisation. They report that rhetorical relations that may be intuitively perceived as highly salient do not provide strong indicators of",0,,False
35,"informativeness; instead, the usefulness of rhetorical relations is in providing constraints for navigating through the text's structure. These findings are compatible with the study of Clarke & Lapata [7] into constraining text compression on the basis of rhetorical relations. For a more indepth look into the impact of individual rhetorical relations to summarisation see Teufel & Moens [30].",1,ad,True
36,"In domain-specific IR, Yu et al. [34] focus on psychiatric document retrieval, which aims to assist users to locate documents relevant to their depressive problems. They propose the use of high-level discourse information extracted from queries and documents, such as negative life events, depressive symptoms and semantic relations between symptoms, to improve the precision of retrieval results. Their discourseaware retrieval model achieves higher precision than the vector space and Okapi models.",0,,False
37,"Closer to our work, Wang et al. [31] extend an IR ranking model by adding a re-ranking strategy based on document discourse. Specifically, their re-ranking formula consists of the original retrieval status value computed with the BM11 model, which is then multiplied by a function that linearly combines inverse document frequency and term distance for each query term within a discourse unit. They focus on one discourse type only (advantage-disadvantage) which they identify manually in queries, and show that their approach improves retrieval performance for these queries. Our work differs on several points. We use an automatic (not manual) discourse parser to identify rhetorical relations in the documents to be retrieved (not queries). We consider 15 rhetorical relations (not 1) and we study their impact to retrieval performance using a modification of the IR language model.",1,ad,True
38,"Finally, Suwandaratna & Perera [29] also present a reranking approach for Web search that uses discourse structure. They report a heuristic algorithm for refining search results based on their rhetorical relations. Their implementation and evaluation is partly based on a series of ad-hoc choices, making it hard to compare with other approaches. They report a positive user-based evaluation of their system for ten test cases.",1,ad-hoc,True
39,3. RANKING WITH RHETORICAL,0,,False
40,RELATIONS,0,,False
41,"There may be various ways of considering rhetorical relations in an IR setting. In this work, we view rhetorical relations as non-overlapping text spans, rather than a graph or a tree with structure and overlapping nodes [27]. We select a principled integration of rhetorical relation information into the retrieval model that ranks documents with respect to queries. The goal is to enable evidence about the rhetorical relations in a document to have a quantifiable impact upon the estimation of relevance of this document to a query, and to study that impact.",0,,False
42,3.1 Model Derivation,0,,False
43,"Let q be a query, d a document, D a collection of documents, and g a rhetorical relation in the collection (so that",0,,False
44,"g p(g|d) ,"" 1). In probabilistic IR, each d in D can be ranked by its probability p(d|q) of being relevant to q. Using Bayes' law:""",0,,False
45,"p(d|q) , p(q|d)p(d) ra,nk p(q|d)",0,,False
46,(1),0,,False
47,p(q),0,,False
48,932,0,,False
49,"where the right-hand side of Equation 1 is derived as follows: p(q) is dropped because it is fixed for all documents, and p(d) can be dropped on the assumption that it is uniform in the absence of any prior knowledge about any document. Using the language modeling approach to IR [9], p(q|d) can be interpreted as the probability of generating the terms in q from a model induced by d, or more simply how likely it is that the document is about the same topic as the query. p(q|d) can be estimated in different ways, for instance using Dirichlet, Jelinek-Mercer, or two-stage smoothing [35].",0,,False
50,We introduce into Equation 1 the probability of generating the query terms from a model induced by d and by its rhetorical relations g  d as follows:,0,,False
51,"p(q|d) ,"" p(q|d, g)p(g|d)""",0,,False
52,(2),0,,False
53,g,0,,False
54,"We now explain the two components in Equation 2. The first component, p(q|d, g), can be interpreted as the probability of generating the query terms from a model induced by d and g. We estimate p(q|d, g) as a simple mixture of the probabilities of generating q from d and g:",0,,False
55,"p(q|d, g) , (1 - ) · p(q|d) +  · p(q|g)",0,,False
56,(3),0,,False
57,"where p(q|d) is the (baseline) probability of relevance between q and d mentioned in the beginning of this section,  is a free parameter, and p(q|g) can be interpreted as the probability of generating q from a model induced by the rhetorical relation g, or more simply, the `likelihood of relevance' between the terms in the query and the terms in the rhetorical relation.",0,,False
58,"The second component of Equation 2, p(g|d), is the probability of the rhetorical relation given the document. Similarly to above, this can be interpreted as the probability of generating the terms in g from a model induced by d, or more simply the likelihood of relevance between the terms in the rhetorical relation and the terms in the document.",0,,False
59,3.2 Model Induction,0,,False
60,To make Equations 2-3 operational we need to compute p(q|g) and p(g|d). One simple way of doing so is using the respective maximum likelihood estimations:,0,,False
61,log,0,,False
62,p(q|g ),0,,False
63,",",0,,False
64,"|q| i,1",0,,False
65,"f (qi, g) |g |",0,,False
66,(4),0,,False
67,"where f (qi, g) is the frequency of the query term qi in g, and |g| is the number of terms in g.",0,,False
68,log,0,,False
69,p(g |d),0,,False
70,",",0,,False
71,|g |,0,,False
72,"f (gj , |d|",0,,False
73,d),0,,False
74,(5),0,,False
75,"j,1",0,,False
76,"where f (gj, d) is the frequency of the rhetorical relation",0,,False
77,"term gj in d, and |d| is the number of terms in d. In this",0,,False
78,"work, we use the above equations and, to compensate for",0,,False
79,"zero-frequency cases, we apply add-one smoothing.",1,ad,True
80,Alternative principled estimations of Equations 4-5 are,0,,False
81,"possible (e.g. Dirichlet, Good-Turing) and could poten-",0,,False
82,tially improve the performance reported in this work. For,0,,False
83,"instance, one could discount the frequencies in Equations",0,,False
84,4-5 by a respective collection model using Dirichlet smooth-,0,,False
85,"ing: log ps(q|g) ,",0,,False
86,"|q| f (qi,g )+·p(qi|)",0,,False
87,"i,1",0,,False
88,|g |+,0,,False
89,where,0,,False
90,would,0,,False
91,be the smoothing parameter and  would be the collec-,0,,False
92,tion of all rhetorical relations in D. A similarly Dirichlet,0,,False
93,smoothed alternative estimation of Equation 5 would be:,0,,False
94,"log ps(g|d) ,",0,,False
95,". |g | f (gj ,d)+·p(gj |D)",0,,False
96,"j,1",0,,False
97,|d|+,0,,False
98,We choose to use,0,,False
99,maximum likelihood instead of Dirichlet to avoid introduc-,1,ad,True
100,ing the extra Dirichlet smoothing parameter  when inves-,0,,False
101,tigating the effect of rhetorical relations upon retrieval.,0,,False
102,Another alternative would be to use Good-Turing smooth-,0,,False
103,"ing, however doing so would scale down the maximum like-",0,,False
104,lihood,0,,False
105,estimations,0,,False
106,in,0,,False
107,Equations,0,,False
108,4-5,0,,False
109,by,0,,False
110,a,0,,False
111,factor,0,,False
112,of,0,,False
113,1,0,,False
114,-,0,,False
115,E(1) |g |,0,,False
116,and,0,,False
117,1-,0,,False
118,E(1) |d|,0,,False
119,"respectively,",0,,False
120,where,0,,False
121,E(1) |g |,0,,False
122,(resp.,0,,False
123,E(1) |d|,0,,False
124,),0,,False
125,is,0,,False
126,the,0,,False
127,estimate of how many items in the numerator of Equation 4,0,,False
128,(resp. Equation 5) have occurred once in the sample of the,0,,False
129,denominator (see Gale & Sampson [13] for more on Good-,0,,False
130,"Turing smoothing). In effect, for Equation 4 this scaling",0,,False
131,down would reduce the probability of the query terms that,0,,False
132,"we have seen in g, making room for query terms that we",0,,False
133,"have not seen. For our setting this would not be necessary,",0,,False
134,because in practice most queries and most rhetorical rela-,0,,False
135,tions correspond to rather short text spans. Good-Turing,0,,False
136,smoothing might be better suited for larger samples [13].,0,,False
137,"Overall, the model presented in this section can be seen as",0,,False
138,a `basic model' for ranking documents (partly) according to,0,,False
139,their rhetorical relations. Different variations on this basic,0,,False
140,"model are certainly possible, however we choose to use the",0,,False
141,simple maximum likelihood version of this model for this,0,,False
142,exploratory investigation into the potential benefits of using,0,,False
143,rhetorical relations for IR.,0,,False
144,4. EVALUATION,0,,False
145,4.1 Experimental Setup,0,,False
146,"We evaluate our model on the task of re-ranking an initial list of documents, which has been retrieved in response to a query. Re-ranking is a well-known IR practice that can enhance retrieval performance notably [19]. The baseline of our experiments consists of the top 1000 documents retrieved for each query using a state-of-the-art retrieval model (language model with Dirichlet smoothing1 [9]). Our approach reranks these documents using Equation 2.",0,,False
147,4.1.1 Dataset and Pre-processing,0,,False
148,"We experiment with the TREC datasets of the Web 2009 (queries 1-50) and Web 2010 (queries 51-100) tracks, that contain collectively 100 queries and their relevance assessments on the Clueweb09 cat. B dataset2 (50,220,423 web pages in English crawled between January and February 2009). We choose these datasets because they are used widely in the community, allowing comparisons with stateof-the-art. We remove spam using the spam rankings of Cormack et al. [8] with the recommended setting of percentilescore < 70 indicating spam3.",1,TREC,True
149,"We consider a subset of this collection, consisting of the top 1000 documents that have been retrieved in response to each query by the baseline retrieval model on tuned settings",0,,False
150,1We also experimented with Jelinek-Mercer and two-stage smoothing for the baseline retrieval model. Dirichlet and two-stage gave higher scores. We chose Dirichlet over twostage because it includes one less parameter to tune. 2http://lemurproject.org/clueweb09.php/ 3Note that removing spam from Clueweb09 cat B. is known to give overall lower retrieval scores than keeping spam [3].,1,Clue,True
151,933,0,,False
152,"Table 1: Examples of the 15 rhetorical relations (in bold italics) of our dataset, identified by the SPADE",0,,False
153,discourse parser [27],0,,False
154,Rhetorical relation Example sentences with rhetorical relations italicised and bold,0,,False
155,attribution,0,,False
156,... the islands now known as the Gilbert Islands were settled by Austronesian-speaking people ...,0,,False
157,background,0,,False
158,... many whites had left the country when Kenyatta divided their land among blacks ...,1,ad,True
159,cause-result,0,,False
160,"... I plugged ""wives"" into the search box and came up with the following results ...",0,,False
161,comparison,0,,False
162,"... so for humans, it is stronger than coloured to frustrate these unexpected numbers ...",0,,False
163,condition,0,,False
164,... Conditional money based upon care for the pet ...,0,,False
165,consequence,0,,False
166,... voltage drop with the cruise control switch could cause erratic cruise control operation ...,0,,False
167,contrast,0,,False
168,"... Although it started out as a research project , the ARPANET quickly developed into ...",0,,False
169,elaboration,0,,False
170,... order accutane no prescription required ...,0,,False
171,enablement,0,,False
172,... The project will also offer exercise programs and make eye care services accessible ...,0,,False
173,evaluation,0,,False
174,... such advances will be reflected in an ever-greater proportion of grade A recommendations ...,1,ad,True
175,explanation,0,,False
176,"... the concept called as ""evolutionary developmental biology"" or shortly ""evo-devo"" ...",0,,False
177,manner-means,0,,False
178,"... Fill current path using even-odd rule, then paint the path ...",0,,False
179,summary,0,,False
180,"... Safety Last, Girl Shy, Hot Water, The Kid Brother, Speedy (all with lively orchestral scores) ...",0,,False
181,temporal,0,,False
182,... Take time out before you start writing ...,0,,False
183,topic-comment,0,,False
184,... Director Mark Smith expressed support for greyhound adoption ...,1,ad,True
185,"(described in section 4.1.2) using the Indri IR system4 for indexing and retrieval. For this subset, we strip HTML annotation using our in-house WHU-REAPER crawling and web parsing toolkit5. Rhetorical relations are identified using the freely available SPADE discourse parser [27]. Table 1 shows the 15 types of rhetorical relations identified by this process, with examples taken from the re-ranking dataset.",1,AP,True
186,4.1.2 Parameter Tuning,0,,False
187,"Two parameters are involved in these experiments: the Dirichlet smoothing parameter  of the retrieval model (used by both the baseline and our approach) and the mixture parameter  of our model. Both parameters are tuned using 5-fold cross validation for each query set separately; results reported are the average over the five test sets.  is tuned across {100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000, 10000} (using the range of Zhai & Lafferty [35]) and  is tuned across {0.1, 0.3, 0.5, 0.7, 0.9}.",0,,False
188,"Performance is reported and tuned separately for Mean Average Precision (MAP), Binary Preference (BPREF), and Normalised Discounted Cumulated Gain (NDCG). These measures contribute different aspects to the overall evaluation: BPREF measures the average precision of a ranked list; it differs from MAP in that it does not treat non-assessed documents as explicitly non-relevant (whereas MAP does) [4]. This is a useful insight, especially for a collection as large as Clueweb09 cat. B where the chances of retrieving non-assessed documents are higher. NDCG measures the gain of a document based on its position in the result list. The gain is accumulated from the top of the ranked list to the bottom, with the gain of each document discounted at lower ranks. This gain is relative to the ideal based on a known recall base of relevance assessments [17]. Finally, we test the statistical significance of our results using the t-test at 95% and 99% confidence levels [25].",1,MAP,True
189,4.2 Findings,0,,False
190,"Figure 2 shows the distribution of the rhetorical relations in our re-ranking dataset as a percentage of the total number of rhetorical relations. Elaboration, attribution",0,,False
191,4http://www.lemurproject.org/ 5Freely available by emailing the third author.,0,,False
192,"and background are the most frequent rhetorical relations, whereas topic-comment is the most infrequent. This happens because quite often in text a topic forms the nucleus of the discourse, which is then linked by a number of different rhetorical relations, for instance about its background, elaborating on an aspect, or attributing parts of it to some entity. As a result, several types of other rhetorical relations can correspond to a single topic-comment. Note that the distribution of rhetorical relations reported here is in agreement with the literature, e.g. Teufel & Moens [30] also report a 5% occurrence of contrast, albeit in the domain of scientific articles.",0,,False
193,4.2.1 Retrieval-Enhancing Rhetorical Relations,0,,False
194,"Table 2 shows the performance of our model against the baseline, for each rhetorical relation and evaluation measure. The baseline performance is among the highest reported in the literature for these setings; for instance Bendersky et al. [3] report MAP,0.1605 for a tuned language model baseline with the Web 2009 track queries on Clueweb cat. B without spam.",1,MAP,True
195,"We observe that different rhetorical relations perform differently across evaluation measures and query sets. The four rhetorical relations that improve performance over the baseline consistently for all evaluation measures and query sets (shaded rows in Table 2) are: background, cause-result, condition and topic-comment. Topic-comment is one of the overall best-performing rhetorical relations, which in simple terms means that boosting the weight of the topical part of a document improves its estimation of relevance.",1,ad,True
196,"A closer look at which rhetorical relations decrease performance presents a more uneven picture as no relations consistently underperform for all measures and query sets. Some relations, such as explanation and enablement for Web 2009, and summary and evaluation for Web 2010, are among the lowest performing, but are not under the baseline across all measures and both query sets. This implies that separating rhetorical relations into those that generally can enhance retrieval performance and those that cannot may not be straight-forward. Even though exploring the family likeness between useful relations and ones that give no mileage is an interesting discussion, in the rest of the pa-",0,,False
197,934,0,,False
198,Table 2: Retrieval performance with rhetorical relations and without (baseline). * (**) marks stat. signif-,0,,False
199,icance at 95% (99%) using the t-test. Bold means > baseline. % shows the difference from the baseline.,0,,False
200,Shaded rows indicate consistent improvements over the baseline at all times.,1,ad,True
201,rhetorical relation,0,,False
202,MAP,1,MAP,True
203,Web 2009 (queries 1-50) BPREF,0,,False
204,NDCG,0,,False
205,MAP,1,MAP,True
206,Web 2010 (queries 51-100) BPREF,0,,False
207,NDCG,0,,False
208,none (baseline),0,,False
209,0.1625,0,,False
210,0.3230,0,,False
211,0.3893,0,,False
212,0.0986,0,,False
213,0.2240,0,,False
214,0.2920,0,,False
215,attribution,0,,False
216,0.1654* +1.8% 0.3275** +1.4% 0.3927** +0.9% 0.0924,0,,False
217,-6.2% 0.2549** +13.8% 0.3008** +3.0%,0,,False
218,background,0,,False
219,0.1646 +1.3% 0.3291** +1.9% 0.3910 +0.4% 0.1086* +10.2% 0.2623** +17.1% 0.3070** +5.1%,0,,False
220,cause-result,0,,False
221,0.1626 +0.1% 0.3255** +0.8% 0.3900 +0.2% 0.1015 +2.9% 0.2491* +11.2% 0.3079 +5.4%,0,,False
222,comparison,0,,False
223,0.1610,0,,False
224,-0.9% 0.3251* +0.6% 0.3877,0,,False
225,-0.4% 0.1017 +3.1% 0.2282,0,,False
226,+1.9% 0.3040** +4.1%,0,,False
227,condition,0,,False
228,0.1632 +0.5% 0.3258** +0.9% 0.3903 +0.3% 0.0999 +1.3% 0.2470** +10.3% 0.2936 +0.5%,0,,False
229,consequence,0,,False
230,0.1602,0,,False
231,-1.4% 0.3250 +0.6% 0.3874,0,,False
232,-0.5% 0.0945,0,,False
233,-4.1% 0.2377* +6.1% 0.2840** -2.7%,0,,False
234,contrast,0,,False
235,0.1549* -4.6% 0.3269** +1.2% 0.3897 +0.1% 0.1103* +11.8% 0.2531** +13.0% 0.3069** +5.1%,0,,False
236,elaboration,0,,False
237,0.1556* -4.2% 0.3292** +1.9% 0.3866,0,,False
238,-0.7% 0.0951,0,,False
239,-3.5% 0.2598** +16.0% 0.3005** +2.9%,0,,False
240,enablement,0,,False
241,0.1601,0,,False
242,-1.4% 0.3240 +0.3% 0.3869*,0,,False
243,-0.6% 0.1010 +2.4% 0.2316* +3.4% 0.2992* +2.5%,0,,False
244,evaluation,0,,False
245,0.1632 +0.5% 0.3242 +0.4% 0.3886,0,,False
246,-0.2% 0.0814** -17.4% 0.2313* +3.3% 0.2902,0,,False
247,-0.6%,0,,False
248,explanation,0,,False
249,0.1546,0,,False
250,-4.9% 0.3259* +0.9% 0.3813,0,,False
251,-2.1% 0.1034 +4.9% 0.2645** +18.1% 0.3069** +5.1%,0,,False
252,manner-means,0,,False
253,0.1623,0,,False
254,-0.1% 0.3253* +0.7% 0.3884,0,,False
255,-0.2% 0.0986,0,,False
256,- 0.2324* +3.7% 0.2897,0,,False
257,-0.8%,0,,False
258,summary,0,,False
259,0.1626 +0.1% 0.3241 +0.3% 0.3879,0,,False
260,-0.4% 0.0862,0,,False
261,-12.6% 0.2220*,0,,False
262,-0.9% 0.2928 +0.3%,0,,False
263,temporal,0,,False
264,0.1615,0,,False
265,-0.6% 0.3262** +1.0% 0.3887,0,,False
266,-0.2% 0.0921,0,,False
267,-6.6% 0.2546** +13.7% 0.3052 +4.5%,0,,False
268,topic-comment,0,,False
269,0.1673 +3.0% 0.3375 +4.5% 0.3976* +2.1% 0.1090* +10.5% 0.2476* +10.5% 0.3009 +3.1%,0,,False
270,Table 3: Effect of the rhetorical relation to the re-,0,,False
271,trieval model as indicated by parameter  (see Equa-,0,,False
272,"tion 3), for the tuned runs of Table 2. Shaded",1,ad,True
273,rows indicate rhetorical relations that consistently,0,,False
274,improve performance over the baseline at all times.,0,,False
275,rhetorical,0,,False
276,Web 2009 (queries 1-50) Web 2010 (queries 51-100),0,,False
277,relation,0,,False
278,MAP BPREF NDCG MAP BPREF NDCG,1,MAP,True
279,attribution,0,,False
280,0.1 0.5,0,,False
281,0.1,0,,False
282,0.3 0.5,0,,False
283,0.3,0,,False
284,background,0,,False
285,0.2 0.6,0,,False
286,0.2,0,,False
287,0.3 0.7,0,,False
288,0.3,0,,False
289,cause-result,0,,False
290,0.3 0.7,0,,False
291,0.3,0,,False
292,0.5 0.7,0,,False
293,0.5,0,,False
294,comparison,0,,False
295,0.4 0.7,0,,False
296,0.4,0,,False
297,0.3 0.5,0,,False
298,0.3,0,,False
299,condition,0,,False
300,0.3 0.7,0,,False
301,0.3,0,,False
302,0.3 0.5,0,,False
303,0.3,0,,False
304,consequence,0,,False
305,0.5 0.7,0,,False
306,0.5,0,,False
307,0.5 0.7,0,,False
308,0.5,0,,False
309,contrast,0,,False
310,0.3 0.7,0,,False
311,0.3,0,,False
312,0.3 0.5,0,,False
313,0.3,0,,False
314,elaboration,0,,False
315,0.1 0.5,0,,False
316,0.1,0,,False
317,0.3 0.5,0,,False
318,0.3,0,,False
319,enablement,0,,False
320,0.1 0.9,0,,False
321,0.1,0,,False
322,0.3 0.5,0,,False
323,0.3,0,,False
324,evaluation,0,,False
325,0.5 0.7,0,,False
326,0.5,0,,False
327,0.5 0.7,0,,False
328,0.5,0,,False
329,explanation,0,,False
330,0.5 0.7,0,,False
331,0.5,0,,False
332,0.5 0.7,0,,False
333,0.5,0,,False
334,manner-means 0.5 0.7,0,,False
335,0.5,0,,False
336,0.5 0.7,0,,False
337,0.5,0,,False
338,summary temporal,0,,False
339,0.5 0.7,0,,False
340,0.5,0,,False
341,0.3 0.7,0,,False
342,0.3,0,,False
343,0.1 0.7,0,,False
344,0.1,0,,False
345,0.3 0.5,0,,False
346,0.3,0,,False
347,topic-comment 0.5 0.5,0,,False
348,0.5,0,,False
349,0.5 0.7,0,,False
350,0.5,0,,False
351,topic-comment evaluation consequence,0,,False
352,summary enablement,0,,False
353,explanation comparison manner-means cause-result,0,,False
354,temporal contrast condition,0,,False
355,background,0,,False
356,attribution elaboration,0,,False
357,0,0,,False
358,5,0,,False
359,10,0,,False
360,15,0,,False
361,20,0,,False
362,25,0,,False
363,% of all rhetorical relations,0,,False
364,Figure 2: % distribution of rhetorical relations in our dataset.,0,,False
365,per we focus on those rhetorical relations that consistently improve retrieval performance (for these datasets).,0,,False
366,"Improvements over the baseline are generally higher for Web 2010 than Web 2009, possibly because the former baseline is weaker, with potentially more room for improvement. An interesting trend is that more rhetorical relations improve performance according to BPREF than according to MAP and NDCG. As BPREF is the only of these evaluation measures that does not consider non-assessed documents as non-relevant, this indicates the presence of non-assessed documents in the ranking.",1,MAP,True
367,"The scores shown in Table 2 are averaged over tens of queries, meaning that they can be affected by outliers. Figure 3 presents a detailed per-query overview of the performance of each query in relation to the baseline for each of the 15 rhetorical relations6. The plotted points represent the difference in MAP between our approach and the baseline. Positive points indicate that our approach outperforms the baseline. The points are sorted.",1,MAP,True
368,"We observe that although the overall performance of the Web 2010 query set is lower than that of the Web 2009 query set, the improvements over the baseline of the 2010 set are consistently larger. Only in one case, topic-comment, do the plotted points clearly cross. Overall both query sets show similar plots with outliers at both ends of the scale. However, the 2009 query set tends to have a somewhat larger proportion of negative outliers, which goes some way towards explaining the lower improvements over the baseline observed for Web 2009. The Web 2010 set shows improvements over the baseline for most of the rhetorical relations and for the majority of the queries.",0,,False
369,4.2.2 Quantifying the Contribution of Rhetorical Relations to the Ranking,0,,False
370,"Exactly how much impact each rhetorical relation has on the ranking can be seen in Table 3. The table lists the  values for the best performing tuned runs from Table 2, where high  values mean that the rhetorical relations are given more weight in the ranking (see Equation 3). We see that none of the values are above 0.5 for MAP and NDCG, indicating that too much emphasis on the rhetorical rela-",1,MAP,True
371,"6Similar trends are observed in the corresponding figures for BPREF and NDCG, which are not included here for brevity.",0,,False
372,935,0,,False
373,attribution 0.08,0,,False
374,background 0.1,0,,False
375,cause-result 0.2,0,,False
376,0 -0.02,0,,False
377,0.1 0,0,,False
378,-0.1 0.2,0,,False
379,comparison contrast,0,,False
380,0,0,,False
381,0.1 0,0,,False
382,-0.1 0.1,0,,False
383,condition elaboration,0,,False
384,0 consequence,0,,False
385,0.05 0,0,,False
386,-0.1 enablement,0,,False
387,0.15,0,,False
388,0 -0.1,0,,False
389,evaluation 0.05,0,,False
390,0,0,,False
391,0 -0.05,0,,False
392,0.1 0,0,,False
393,explanation,0,,False
394,0.05 0 manner-means,0,,False
395,0.05 0,0,,False
396,-0.1 summary,0,,False
397,-0.2 temporal,0,,False
398,-0.1 topic-comment,0,,False
399,0.1,0,,False
400,0.05,0,,False
401,0.05,0,,False
402,0,0,,False
403,0,0,,False
404,0,0,,False
405,-0.05,0,,False
406,-0.1,0,,False
407,-0.05,0,,False
408,Web 2009 Web 2010,0,,False
409,"Figure 3: Sorted per-query difference in MAP between the baseline and our model (y-axis), for each rhetorical relation. The horizontal line marks the baseline. + and o mark the 2009 and 2010 query sets.",1,MAP,True
410,936,0,,False
411,"tions may not be beneficial to performance. Consistent with Table 2, BPREF follows a different trend than MAP and NDCG, which could be due to the fact that it is a different type of evaluation measure as discussed above in section 4.1.2. With BPREF, non-assessed documents are not explicitly penalised in the evaluation (as in MAP and NDCG) resulting in overall higher  values for best performing runs, typically of around 0.5-0.7.",1,MAP,True
412,"Further we observe that the rhetorical relations that consistently improve performance over the baseline, as indicated in Table 2, differ in  values for their best performing runs. For example,  , 0.2 - 0.3 for background and  ,"" 0.5 for topic-comment. This implies that, to use rhetorical relations successfully for IR, it is not sufficient to know which rhetorical relations should be considered in the ranking and which not; also knowledge about how much emphasis to put on each rhetorical relation is needed for optimal IR performance.""",0,,False
413,"Finally, note that the frequency of rhetorical relations does not affect their impact to retrieval. For instance, the three best performing rhetorical relations, topic-comment, background and cause-result constitute respectively approximately 1%, 11% and 5% of all rhetorical relations, as shown in Figure 2.",0,,False
414,5. OPTIMISED RANKING WITH RHETORICAL RELATIONS,0,,False
415,5.1 Rhetorical Relation Selection,0,,False
416,"The findings in section 4.2 show that some rhetorical relations can be more beneficial to retrieval performance than others. An ideal solution would not consider the lexical statistics of all rhetorical relations in a document, but rather it would select to include in the ranking only those rhetorical relations that have a higher likelihood of enhancing retrieval performance. This can be formulated as finding the optimal rhetorical relation ^g that maximises the expected retrieval scores according to an evaluation measure (e.g. MAP) for a query-document pair:",1,MAP,True
417,"^g ,"" arg max E[y|q, d]""",0,,False
418,(6),0,,False
419,g ,0,,False
420,where E denotes the expectation and y the retrieval score,0,,False
421,(rest of notation as defined in section 3).,0,,False
422,Bayesian decision theory allows to reason about this type,0,,False
423,"of expectation, for instance see [32]. In this work, we treat",0,,False
424,"this as a problem of Bayesian posterior inference, where the",0,,False
425,goal is to estimate the retrieval performance associated with,0,,False
426,"a rhetorical relation, given the observed retrieval scores it",0,,False
427,"fetches on a number of queries. Then, we can consider",0,,False
428,the rhetorical relation associated with the highest retrieval,0,,False
429,"performance as optimal. For this estimation, we split our",0,,False
430,dataset into different parts so that we use the observations,0,,False
431,from one to make inferences about the other (see section 5.2,0,,False
432,for details).,0,,False
433,"Let n ,"" 15 be the rhetorical relations shown in Table 2,""",0,,False
434,and xj be the number of queries for which retrieval with the jth rhetorical relation gets a retrieval score yj. For now,0,,False
435,we assume that all rhetorical relations may be expected to,0,,False
436,"have similar retrieval performance, with the jth rhetorical",0,,False
437,relation having an average performance ratio per query j,0,,False
438,(estimated,0,,False
439,as,0,,False
440,yj xj,0,,False
441,).,0,,False
442,Various,0,,False
443,densities,0,,False
444,can,0,,False
445,be,0,,False
446,used,0,,False
447,to,0,,False
448,fit,0,,False
449,simi-,0,,False
450,"lar data [22], one of which is the Poisson distribution. Let us",0,,False
451,"assume that, conditional on j, the retrieval scores yj have independent Poisson distributions with means jxj. Let us further assume that the j are independent realisations of a gamma variable with parameters  and , and that  itself has a prior gamma distribution with parameters  and . Thus",0,,False
452,"f (y|) , n (xj j )yj e-xj j j,1 yj !",0,,False
453,"(|) , n j -1 e-j ()",0,,False
454,"j,1",0,,False
455,"() ,  -1 e- ()",0,,False
456,"so that the joint probability density of the retrieval scores y, the average performance ratios , and  is",0,,False
457,n,0,,False
458,"f (y|)f (|)() , c {jyj +-1e-j (xj +)}·n+-1e-",0,,False
459,"j,1",0,,False
460,(7) where c is a constant of proportionality.,0,,False
461,"The conditional density of  can be computed by various numerical approximations, one of which is the Laplace method [2], which we use here. To find the conditional density of  we integrate over the j to obtain",0,,False
462,n,0,,False
463,"f (y, ) , c {(xj +)-(yj +)(yj +)}·n+-1e- (8)",0,,False
464,"j,1",0,,False
465,from which the marginal density of y is obtained by further integration to give,0,,False
466,n,0,,False
467,"f (y) , c (yj + ) · e-h()d",0,,False
468,(9),0,,False
469,"j,1",0,,False
470,0,0,,False
471,"where h() ,""  - (n +  - 1)log + (yj + )log(xj + ). Let I denote the integral in this expression. In this work,""",0,,False
472,"we take an uninformative prior for , with  , 0.1 and  , 1 and use  , 1.87. We then apply Laplace's method",0,,False
473,"to I, resulting in the approximate posterior density for , ~(|y) , I~-1e-h().",0,,False
474,"To calculate approximate posterior densities for j we integrate Equation 7 over i, i , j and then we apply Laplace's method to the numerator and denominator inte-",0,,False
475,grals of,0,,False
476,where,0,,False
477,(j |y),0,,False
478,",",0,,False
479,jyj +-1e-j xj,0,,False
480, 0,0,,False
481,e-hj,0,,False
482,()d,0,,False
483,(yj + ),0,,False
484, 0,0,,False
485,e-h()d,0,,False
486,"hj() , ( + j) - (n +  - 1)log + (yi + )log(xi + )",0,,False
487,"i,j",0,,False
488,"The resulting denominator is again I~1, while the numerator must be recalculated at each of a range of values for j. The output is the (posterior) expected retrieval performance associated with each rhetorical relation.",0,,False
489,5.2 Experiments,0,,False
490,"7These values are not tuned; they are the default values of this approach as illustrated in [10], chapter 11.3, pages 603-604.",0,,False
491,937,0,,False
492,"Table 4: Retrieval performance with optimal rhetorical relations (inferred, observed) and without rhetorical",0,,False
493,relations (baseline). (1)-(5) refers to the five randomised samplings used to infer the optimal rhetorical,0,,False
494,relations. Bold marks better than baseline.,0,,False
495,rhetorical relation,0,,False
496,MAP,1,MAP,True
497,Web 2009 (queries 1-50) BPREF,0,,False
498,NDCG,0,,False
499,MAP,1,MAP,True
500,Web 2010 (queries 51-100),0,,False
501,BPREF,0,,False
502,NDCG,0,,False
503,none (baseline),0,,False
504,0.1625,0,,False
505,0.3230,0,,False
506,0.3894,0,,False
507,0.0967,0,,False
508,0.2198,0,,False
509,0.2890,0,,False
510,optimalinferred (1) optimalinferred (2) optimalinferred (3) optimalinferred (4) optimalinferred (5) optimalobserved,0,,False
511,0.1879 0.1948 0.1984 0.1952 0.1950 0.2157,0,,False
512,+15.6% +19.9% +22.1% +20.1% +20.0% +32.7%,0,,False
513,0.3503 0.3585 0.3532 0.3479 0.3528 0.3660,0,,False
514,+8.5% +11.0%,0,,False
515,+9.3% +7.7% +9.2% +13.3%,0,,False
516,0.4224 0.4202 0.4169 0.4282 0.4287 0.4412,0,,False
517,+8.5% +7.9% +7.1% +10.0% +10.1% +13.3%,0,,False
518,0.1355 0.1285 0.1358 0.1360 0.1340 0.1474,0,,False
519,+40.1% +32.9% +40.0% +40.6% +38.6% +52.4%,0,,False
520,0.2859 0.2841 0.2906 0.2874 0.2865 0.2978,0,,False
521,+30.1% +29.3% +32.2% +30.8% +30.3% +35.5%,0,,False
522,0.3347 0.3394 0.3388 0.3336 0.3322 0.3569,0,,False
523,+15.8% +17.4% +17.2% +15.4% +14.9% +23.5%,0,,False
524,5.2.1 Setup,0,,False
525,"The observations required to make the above inference are triples of rhetorical relation - query number - retrieval score. To avoid overfitting, we pool randomly 50% of the observations from the 2009 Web query scores and 50% of the observations from the 2010 Web query scores. We use this pool to infer the expected retrieval performance of each rhetorical relation. We repeat this randomised pooling five times, each time randomly pertrubing the data, producing five different sets of observations. We then use each set to infer the expected best performing rhetorical relation per query, in accordance to Equation 6. Following this, we use the model introduced in section 3, Equation 2, to rank documents with respect to queries only for optimal (as inferred) rhetorical relations. We evaluate the above method using the same experimental settings described in section 4.1.",0,,False
526,5.2.2 Findings,0,,False
527,Table 4 shows the runs corresponding to the five differ-,0,,False
528,ent inferences of the best rhetorical relation that use our,0,,False
529,model (optimalinferred (1)-(5) respectively). We also report,0,,False
530,the optimal retrieval performance actually observed in the,0,,False
531,dataset when using the best rhetorical relation per query,0,,False
532,(optimalobserved). Optimal here means with respect to the,0,,False
533,"choice of rhetorical relation, not with respect to the Dirichlet",0,,False
534, parameter of the baseline retrieval model.,0,,False
535,Table 4 shows that our optimised ranking model for rhetor-,0,,False
536,ical relations is better than the baseline for any of the five,0,,False
537,random inferences on all three evaluation measures. The,0,,False
538,probability of getting such a positive result by chance is,0,,False
539,1 25,0,,False
540,<,0,,False
541,"0.05,",0,,False
542,and,0,,False
543,thus,0,,False
544,the,0,,False
545,improvements,0,,False
546,are,0,,False
547,statistically,0,,False
548,sig-,0,,False
549,nificant. The improvements over the baseline are consider-,0,,False
550,"able, a very promising finding given the relatively low num-",0,,False
551,ber of observations used for optimising the choice of rhetor-,0,,False
552,ical relations. Experiments involving larger query sets can,0,,False
553,be reasonably expected to perform on a par with state-of-,0,,False
554,the-art performance.,0,,False
555,"More generally, the improvements in Table 4 signal that",0,,False
556,rhetorical relations (derived automatically as shown in this,0,,False
557,work) could potentially be useful features for `linguistically-,0,,False
558,uninformed' learning-to-rank approaches.,0,,False
559,6. DISCUSSION,0,,False
560,6.1 Rhetorical Relation Distribution,0,,False
561,"The distribution of the 15 rhetorical relations we identified in our dataset is not the same for all rhetorical relations (see Figure 2). Some types, e.g. topic-comment, tend to be very sparse, whereas relations such as elaboration prevail.",0,,False
562,"This has no impact on the model presented in section 3, but it can bias the optimised inference of the model presented in section 5. The lower the occurrence of a rhetorical relation in the dataset, the fewer the observations of retrieval performance associated with it, and hence the weaker the predictions we can infer about whether it is optimal or not. A fairer setting would be to have the same number of `query - retrieval performance' observations for all rhetorical relations - however that would imply fiddling with the document distribution of our dataset significantly, potentially harming its quality as a test collection.",0,,False
563,6.2 Limitations,0,,False
564,"A general limitation of discourse analysis is that not all types of text are susceptible to it. For instance, legal text, contracts, or item lists often lack rhetorical structure. In this work, we made no effort to identify and exempt such types of text from the discourse parsing. We reasoned that, as the SPADE parser includes a first-step grammatical parsing, the initial grammatical parsing of these types of text would flag out ill-formed parts (e.g. missing a verb, or consisting of extremely long sentences), which would then be skipped by the discourse analysis. This was indeed the case, however at a certain efficiency cost. Overall processing speed for SPADE was approximately 19 seconds per document (including the initial grammatical parsing), on a machine of 9 GB RAM, 8 core processor at 2.27GHz. One way of improving this performance would be to update the first-step grammatical parsing. Currently this depends on the well-known Charniak parser [6], which is one of the best performing grammatical parsers, however no longer supported. Other state-of-theart faster grammatical parsers, e.g. the Stanford parser8, could be adapted and plugged into SPADE instead.",1,ad,True
565,"The choice of applying our model for re-ranking as opposed to ranking all documents was closely related to the efficiency concerns discussed above. Our model is not specific to re-ranking only, however, using SPADE on approximately 50 million documents was too expensive at this point. Improving the discourse parser's efficiency is something we are currently working on, with the aim to apply our model for full ranking and see if the conclusions drawn from this work hold.",0,,False
566,"Finally, the accuracy of the discourse parser was not considered in this work, apart from indications in the literature that SPADE is a generally well-performing parser [27]. Given that the default version of the parser we used is trained on news articles, one may reason that its accuracy could improve if we train it on the retrieval collection, or on doc-",0,,False
567,8http://nlp.stanford.edu/software/lex-parser.shtml,0,,False
568,938,0,,False
569,"uments of the same domain. Note that, parsing accuracy aside, rhetorical relations assignment is not an entirely unambiguous process, even to humans [23]. For the purposes of this work, this type of fine-grained ambiguity may however not be important to retrieval performance.",0,,False
570,6.3 Future Extensions,0,,False
571,"Future extensions include primarily making SPADE scalable on large collections of documents as discussed above, as well as using more than one rhetorical relation per document. For instance, the posterior probabilities estimated in section 5.1 could be used to weight the text in each rhetorical relation. If those posteriors are too flat, an exponent could make them peakier. As the exponent goes to infinity, the maximum relation model presented in section 5.2 would be recovered. In addition, we intend to refine the discourse analysis by considering the nucleus (i.e. central) versus satellite (i.e. peripheral) rhetorical relations for IR, as well as to improve the effectiveness of the discourse parser by training it on data of the same domain. As discussed in section 3.2, we will also investigate alternative estimations of Equations 2-3.",1,ad,True
572,"An interesting future research direction is the potential relation between rhetorical relations and user context: for instance, in a search session including several query reformulations, is there a correlation between the progression of the information need of the user and the rhetorical relations that the retrieval system should boost in a document (e.g. elaboration), as indicated by Sun & Chai [28]? Another interesting future extension of this work is in relation to evaluation measures of graded relevance measures on an inter-document level, as investigated in XML retrieval [20] for instance. If parts of a document can be regarded as more or less relevant, this may be reflected to their discourse structure. This might be especially useful for multi-threaded documents, such as multiple-user reviews and opinions, where the discourse relations tend to shift markedly. Finally, the current operationalisation of our model is simplistic in the sense that the term `rhetorical relation' is coerced into meaning `non-overlapping text fragment' and the actual relation between bits of text is discarded in the process. In future work we could apply fielded XML retrieval models in order to investigate nested structuring among rhetorical relations.",1,ad,True
573,7. CONCLUSIONS,0,,False
574,"Rhetorical relations, e.g. contrast, explanation, condition, indicate the different ways in which the parts of a text are linked to each other to form a coherent whole. This work studied two questions: Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a document's rhetorical relations be useful to IR? To address these, we presented a retrieval model that conditions the probability of relevance between a query and a document on the rhetorical relations occurring in that document. We applied that model to an IR re-ranking scenario for Web search. Experimental evaluation of different versions of our model on TREC data and standard settings demonstrated that certain rhetorical relations can be beneficial to retrieval, with >10% improvements to retrieval precision. Furthermore, we showed that these improvements over the baseline can improve significantly, when the optimal rhetorical relation per document is selected for retrieval.",1,ad,True
575,"Overall, three rhetorical relations were found to benefit",0,,False
576,"retrieval performance notably and consistently for different evaluation measures and query sets: background, causeresult and topic-comment. In retrospect, this is perhaps not surprising, since these are among the most salient discourse relations on an intuitive basis: the main topic or theme of a text, its background, causes and results [21]. Future extensions and research directions of this work include considering more than one rhetorical relation per document, applying our model for ranking all documents (as opposed to re-ranking only) and experimenting with alternative estimations of its components.",0,,False
577,8. ACKNOWLEDGMENTS,0,,False
578,"We thank Kasper Hornbæk, Jakob Grue Simonsen, Raf Guns, Qikai Cheng and the anonymous reviewers for helping improve this paper. Work partially funded by the Danish International Development Agency DANIDA (grant no. 10-087721) and the National Natural Science Foundation of China (grant no. 71173164).",0,,False
579,9. REFERENCES,0,,False
580,"[1] Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL. ACL, 2011.",0,,False
581,"[2] A. Azevedo-Filho and R. D. Shachter. Laplace's method approximations for probabilistic inference in belief networks with continuous variables. In R. L. de M´antaras and D. Poole, editors, UAI, pages 28­36. Morgan Kaufmann, 1994.",0,,False
582,"[3] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of web documents. In I. King, W. Nejdl, and H. Li, editors, WSDM, pages 95­104. ACM, 2011.",0,,False
583,"[4] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In M. Sanderson, K. J¨arvelin, J. Allan, and P. Bruza, editors, SIGIR, pages 25­32. ACM, 2004.",0,,False
584,"[5] L. Carlson, D. Marcu, and M. E. Okurowski. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Current Directions in Discourse and Dialogue, pages 85­112. Kluwer Academic Publishers, 2003.",1,ad,True
585,"[6] E. Charniak. A maximum-entropy-inspired parser. In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, pages 132­139, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.",0,,False
586,"[7] J. Clarke and M. Lapata. Discourse constraints for document compression. Computational Linguistics, 36(3):411­441, 2010.",0,,False
587,"[8] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. CoRR, abs/1004.5168, 2010.",0,,False
588,"[9] W. B. Croft and J. Lafferty. Language Modeling for Information Retrieval. Kluwer Academic Publishers, Norwell, MA, USA, 2003.",1,ad,True
589,"[10] A. C. Davison. Statistical Models. Cambridge University Press, New York, 2009.",0,,False
590,[11] D. A. duVerle and H. Prendinger. A novel discourse parser based on support vector machine classification.,0,,False
591,939,0,,False
592,"In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL '09, pages 665­673, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics.",0,,False
593,"[12] C. J. Fillmore, C. F. Baker, and S. Hiroaki. The framenet database and software tools. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages 1157­1160, 2002.",0,,False
594,"[13] W. A. Gale and G. Sampson. Good-turing frequency estimation without tears. Journal of Quantitative Linguistics, 2(3):217­237, 1995.",0,,False
595,"[14] S. Ghosh, R. Johansson, G. Riccardi, and S. Tonelli. Shallow discourse parsing with conditional random fields. In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP), pages 1071­1079, Chiang Mai, Thailand, 2011.",0,,False
596,"[15] D. Gildea and D. Jurafsky. Automatic labeling of semantic roles. In ACL. ACL, 2000.",0,,False
597,"[16] B. Heerschop, F. Goossen, A. Hogenboom, F. Frasincar, U. Kaymak, and F. de Jong. Polarity analysis of texts using discourse structure. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM '11, pages 1061­1070, New York, NY, USA, 2011. ACM.",0,,False
598,"[17] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422­446, 2002.",0,,False
599,"[18] P. Kingsbury and M. Palmer. From treebank to propbank. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages x­x, 2002.",0,,False
600,"[19] E. Krikon and O. Kurland. A study of the integration of passage-, document-, and cluster-based information for re-ranking search results. Inf. Retr., 14(6):593­616, 2011.",0,,False
601,"[20] M. Lalmas. XML Retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2009.",0,,False
602,"[21] A. Louis, A. K. Joshi, and A. Nenkova. Discourse indicators for content selection in summarization. In R. Fern´andez, Y. Katagiri, K. Komatani, O. Lemon, and M. Nakano, editors, SIGDIAL Conference, pages 147­156. The Association for Computer Linguistics, 2010.",0,,False
603,"[22] R. Manmatha, T. M. Rath, and F. Feng. Modeling score distributions for combining the outputs of search engines. In W. B. Croft, D. J. Harper, D. H. Kraft, and J. Zobel, editors, SIGIR, pages 267­275. ACM, 2001.",0,,False
604,"[23] W. C. Mann and S. A. Thompson. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8:243­281, 1988.",0,,False
605,"[24] J. Morato, J. Llorens, G. Genova, and J. A. Moreiro. Experiments in discourse analysis impact on information classification and retrieval algorithms. Inf. Process. Manage., 39:825­851, November 2003.",0,,False
606,"[25] M. D. Smucker, J. Allan, and B. Carterette. Agreement among statistical significance tests for information retrieval evaluation at varying sample sizes. In J. Allan, J. A. Aslam, M. Sanderson, C. Zhai, and J. Zobel, editors, SIGIR, pages 630­631. ACM, 2009.",0,,False
607,"[26] S. Somasundaran, G. Namata, J. Wiebe, and L. Getoor. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In EMNLP, pages 170­179. ACL, 2009.",0,,False
608,"[27] R. Soricut and D. Marcu. Sentence level discourse parsing using syntactic and lexical information. In HLT-NAACL, 2003.",0,,False
609,"[28] M. Sun and J. Y. Chai. Discourse processing for context question answering based on linguistic knowledge. Know.-Based Syst., 20:511­526, August 2007.",0,,False
610,"[29] N. Suwandaratna and U. Perera. Discourse marker based topic identification and search results refining. In Information and Automation for Sustainability (ICIAFs), 2010 5th International Conference on, pages 119­125, 2010.",0,,False
611,"[30] S. Teufel and M. Moens. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409­445, 2002.",0,,False
612,"[31] D. Y. Wang, R. W. P. Luk, K.-F. Wong, and K. L. Kwok. An information retrieval approach based on discourse type. In C. Kop, G. Fliedl, H. C. Mayr, and E. M´etais, editors, NLDB, volume 3999 of Lecture Notes in Computer Science, pages 197­202. Springer, 2006.",0,,False
613,"[32] J. Wang and J. Zhu. On statistical analysis and optimization of information retrieval effectiveness metrics. In F. Crestani, S. Marchand-Maillet, H.-H. Chen, E. N. Efthimiadis, and J. Savoy, editors, SIGIR, pages 226­233. ACM, 2010.",1,ad,True
614,"[33] L. Wang, M. Lui, S. N. Kim, J. Nivre, and T. Baldwin. Predicting thread discourse structure over technical web forums. In EMNLP [1], pages 13­25.",1,ad,True
615,"[34] L.-C. Yu, C.-H. Wu, and F.-L. Jang. Psychiatric document retrieval using a discourse-aware model. Artif. Intell., 173:817­829, May 2009.",0,,False
616,"[35] C. Zhai and J. D. Lafferty. Two-stage language models for information retrieval. In SIGIR, pages 49­56. ACM, 2002.",0,,False
617,"[36] L. Zhou, B. Li, W. Gao, Z. Wei, and K.-F. Wong. Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities. In EMNLP [1], pages 162­171.",0,,False
618,940,0,,False
619,,0,,False

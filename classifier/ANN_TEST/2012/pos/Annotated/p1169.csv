,sentence,label,data,regex
0,Utilizing Inter-Document Similarities in Federated Search,0,,False
1,Savva Khalaman,0,,False
2,Oren Kurland,0,,False
3,savvakh@tx.technion.ac.il kurland@ie.technion.ac.il,0,,False
4,"Faculty of Industrial Engineering and Management Technion -- Israel Institute of Technology Haifa 32000, Israel",0,,False
5,ABSTRACT,0,,False
6,"We demonstrate the merits of using inter-document similarities for federated search. Specifically, we study a resultsmerging method that utilizes information induced from clusters of similar documents created across the lists retrieved from the collections. The method significantly outperforms state-of-the-art results merging approaches.",0,,False
7,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,0,,False
8,"General Terms: Algorithms, Experimentation",0,,False
9,"Keywords: inter-document similarities, federated search",0,,False
10,1. INTRODUCTION,1,DUC,True
11,"Federated search is the task of retrieving documents from multiple (possibly non-overlapping) collections in response to a query [1]. The task is typically composed of three steps: attaining resource (collection) description, selecting resources (collections), and merging the results retrieved from the selected collections [1]. We focus on the resultsmerging step; specifically, we study the merits of using information induced from inter-document similarities.",0,,False
12,"While there is much work on utilizing inter-document similarities for the single-corpus retrieval setting, there is little work along that venue for federated retrieval. For example, clustering was used to transform a single-collection retrieval setting into that of multiple collections [8]. Clusters of sampled documents were used for performing query expansion in federated search [5]; yet, inter-document similarities were not used for results merging. Furthermore, it was shown that among the clusters created across the lists retrieved from different collections there are some that contain a high percentage of relevant documents [3]; still, a results merging method exploiting these clusters was not proposed",0,,False
13,"The only work, to the best of our knowledge, that uses inter-document-similarities for (direct) results merging is based on scoring a document by its similarity with other documents in the retrieved lists [6]. We show that the method we study substantially outperforms this approach.",0,,False
14,"The method we present for merging results in federated search is adapted from recent work on fusing lists that were retrieved from the same collection [4]. In contrast to the non-overlapping collections setting we explore here, the retrieved lists in this work [4] were (partially) overlapping and",1,ad,True
15,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",0,,False
16,"the merging (fusion) methods used to assign initial scores to documents exploited this overlap. The adapted method that we study integrates retrieval scores assigned by a state-ofthe-art results-merging approach (e.g., CORI [1] and SSL [7]) with information induced from clusters created from similar documents across the retrieved lists. Specifically, a document can provide relevance-status support to documents in the same list or in other lists that it is similar to. The resultant retrieval performance is substantially better than that of using only the initial retrieval scores assigned by the state-of-the-art merging approach.",1,ad,True
17,2. RESULTS-MERGING METHOD,0,,False
18,Suppose that some resource (collection) selection method,0,,False
19,was applied in response to query q [1]. We assume that docu-,0,,False
20,ment lists were retrieved from the selected (non-overlapping),0,,False
21,"collections and merged by some previously proposed resultsmerging algorithm [1, 7]. Let Di[nn]it denote the list of the n documents most highly ranked by the merging algorithm",0,,False
22,that assigns the (initial) score Finit(d; q) to document d. Our goal is re-ranking Di[nn]it to improve ranking effec-,0,,False
23,"tiveness. (Documents not in Di[nn]it remain at their original ranks.) To that end, we study the utilization of inter-",0,,False
24,document similarities by adapting a method proposed in,1,ad,True
25,"work on fusing lists retrieved from the same corpus [4]. Let Cl be the set of document clusters created from Di[nn]it using some clustering algorithm; c will denote a cluster. Then, the score assigned to document d ( Di[nn]it) by the Clust method is:",0,,False
26,"FClust(d; q) d,ef (1 X",0,,False
27,P,0,,False
28,- ) P,0,,False
29,d,0,,False
30,F (c; q),0,,False
31,Finit(d; q),0,,False
32,+,0,,False
33,Di[nn]it,0,,False
34,Finit(d P,0,,False
35,;,0,,False
36,q),0,,False
37,P,0,,False
38,"diPc Sim(di,",0,,False
39,d),0,,False
40,;,0,,False
41,"cCl c Cl F (c ; q) d Di[nn]it dic Sim(di, d )",0,,False
42,F (c; q),0,,False
43,"d,ef",0,,False
44,Q,0,,False
45,di c,0,,False
46,Finit(di; q);,0,,False
47,as,0,,False
48,all,0,,False
49,the,0,,False
50,clusters,0,,False
51,in,0,,False
52,Cl,0,,False
53,contain,0,,False
54,"the same number of documents (see details below), there is",0,,False
55,"no cluster-size bias incurred; Sim(·, ·) is the inter-document",0,,False
56,similarity measure used to create Cl;  is a free parameter.,0,,False
57,"Thus, d is highly ranked if (i) its (normalized) initial score",0,,False
58,"is high; and, (ii) it is similar to documents in clusters c",0,,False
59,that contain documents that were initially highly ranked.,0,,False
60,"In other words, similar documents provide relevance-status",0,,False
61,"support to each other via the clusters to which they belong. Note that if  ,"" 0, then cluster-based information is not used and FClust(d; q) is d's normalized initial score.""",0,,False
62,1169,0,,False
63,Uni Rel NRel Rep KM,0,,False
64,CORI SSL CORI SSL CORI SSL CORI SSL,0,,False
65,initial CRSC Clust initial CRSC Clust,0,,False
66,initial CRSC Clust initial CRSC Clust,0,,False
67,initial CRSC Clust initial CRSC Clust,0,,False
68,initial CRSC Clust initial CRSC Clust,0,,False
69,CORI SSL,0,,False
70,initial CRSC Clust initial CRSC Clust,0,,False
71,Queries: 51-100 p@5 p@10 MAP,1,MAP,True
72,.460,0,,False
73,.524 .536i .432 .492,0,,False
74,.504,0,,False
75,.420 .060 .474i .062 .498i .064i,0,,False
76,.410 .060,0,,False
77,.458 .062 .490i .064i,0,,False
78,.424 .384 .059,0,,False
79,.428 .476 .412,0,,False
80,.402 .061 .442i .063ic .384 .095,0,,False
81,.412 .404 .096 .480ic .454ic .099ic,0,,False
82,.452 .446 .064,0,,False
83,.476 .476 .066,0,,False
84,.552ic .516ic .069ic .464 .448 .065,0,,False
85,.488 .462 .067,0,,False
86,.536i .516ic .070ic,0,,False
87,.428 .408 .060,0,,False
88,.416 .418 .060,0,,False
89,.492c .474ic .064ic .444 .408 .061,0,,False
90,.448 .434 .060,0,,False
91,.476 .470i .064ic,0,,False
92,Queries: 201-250,0,,False
93,p@5 p@10 MAP,1,MAP,True
94,.468 .402 .135,0,,False
95,.484 .396 .138,0,,False
96,.496 .460ic .147i .480 .412 .152,0,,False
97,.512 .412 .150 .532 .478ic .165ic,0,,False
98,Queries: 101-150 p@5 p@10 MAP,1,MAP,True
99,.396 .376 .480ic .412 .400 .492c,0,,False
100,.432 .456 .496 .440 .452 .492,0,,False
101,.380 .054,0,,False
102,.372 .052 .454ic .059ic .396 .055 .346 .052 .468ic .060ic,0,,False
103,.370 .064 .442 .072i .466i .073i,0,,False
104,.376 .095 .436 .103i .474i .105i,0,,False
105,.416 .472 .500i .396,0,,False
106,.456 .504i,0,,False
107,.394 .414,0,,False
108,.450 .398 .412,0,,False
109,.452,0,,False
110,.054,0,,False
111,.057 .059ic .055,0,,False
112,.058 .060i,0,,False
113,.428 .432,0,,False
114,.496 .452 .396,0,,False
115,.508c,0,,False
116,.388 .050,0,,False
117,.424 .052 .474i .056ic .408 .052,0,,False
118,.418 .052 .484ic .058ic,0,,False
119,"Table 1: Results. Boldface: the best result per testbed, evaluation measure, and initial merging method; 'i' and 'c': statistically significant differences with initial and CRSC, respectively.",0,,False
120,3. EVALUATION,0,,False
121,"We conducted experiments with testbeds that are commonly used in work on federated search [1, 2, 5, 8, 7]: (i) Uni: Trec123-100col-bysource (Uniform), (ii) KM: Trec4kmeans (K-means), (iii) Rep: Trec123-2ldb-60col (Representative), (iv) Rel: Trec123-AP-WSJ-60col (Relevant), and (v) NRel: Trec123-FR-DOE-81col (non-relevant). Titles of the TREC topics 51-150 served for queries for all testbeds except for KM where the description fields of TREC topics 201-250 were used. Tokenization, Porter stemming, and stopword removal were applied using the Lemur toolkit (www.lemurproject.org), which was used for experiments. To acquire resource (collection) description, we adopt the query-based sampling method from [2] which was also used in [1, 7, 5]. Following common practice [1, 7], the 10 highest ranked collections are selected in the resource-selection phase using CORI's resource selection method. As in previous report [7], 1000 documents are retrieved from each selected collection using the INQUERY search engine. Then, the retrieved lists are merged using either CORI's merging method [1] or the single-model SSL merging approach [7]. The initial score, Finit(d; q), assigned to d by these methods is used in our Clust method.",1,AP,True
122,"To cluster Di[nn]it, we use a simple nearest-neighbors-based clustering approach [4]. For each d  Di[nn]it we create a cluster that is composed of d and the  - 1 ( , 5) documents d in Di[nn]it (d ,"" d) that yield the highest Sim(d, d ) d"",ef",0,,False
123,"

",0,,False
124,exp -D(p[d0](·)||p[dµ](·)) ; p[xµ] is the Dirichlet-smoothed un-,0,,False
125,igram language model induced from x with the smooth-,0,,False
126,"ing parameter  (, 1000); D is the KL divergence; the",0,,False
127,term-counts statistics used for smoothing language models,0,,False
128,is based on the query-based sampling mentioned above.,0,,False
129,The initial ranking induced by the CORI and SSL results-,0,,False
130,merging methods serves for a baseline. Additional reference,0,,False
131,comparison that we use is the Cross Rank Similarity Com-,0,,False
132,parison scoring,0,,False
133,(CRSC) approach [6] that,0,,False
134,P,0,,False
135,d with,0,,False
136,d,0,,False
137,"(,d)Di[nn]it",0,,False
138,P d,0,,False
139,"(,d",0,,False
140,re-ranks Di[nn]it,0,,False
141,"Sim(d ,d)",0,,False
142,")Di[nn]it Sim(d ,d",0,,False
143,here by ) . Our,0,,False
144,"Clust method incorporates two free parameters: n ( {10, 30,",1,corpora,True
145,"50, 100}), the number of documents in Di[nn]it, and  ( {0, 0.1, . . . , 1}), the interpolation parameter; CRSC only depends",0,,False
146,on n. We set the free-parameter values for each method us-,0,,False
147,ing leave-one-out cross validation performed over the queries,0,,False
148,per testbad; MAP@1000 serves as the optimization criterion,1,ad,True
149,"in the learning phase. In addition to MAP, we also present",1,ad,True
150,p@5 and p@10 performance numbers. Statistically signif-,0,,False
151,icant differences in performance are determined using the,0,,False
152,two-tailed paired t-test at a 95% confidence level.,0,,False
153,Results and Conclusions. We see in Table 1 that Clust,0,,False
154,"always outperforms the initial ranking that was induced by a state-of-the-art results-merging method; often, the improvements are statistically significantly. This finding attests to the merits of integrating the initial results-merging score with information induced from clusters of similar documents. Further exploration reveals that   {0, 1} often yields optimal performance. This shows that the integration just mentioned yields performance that is better than that of using each of the two integrated components alone. We also see that Clust consistently outperforms CRSC, which does not utilize the initial results-merging scores, nor uses a cluster-based approach.",0,,False
155,"Acknowledgments We thank the reviewers for their comments. This paper is based upon work supported in part by the Israel Science Foundation under grant no. 557/09. Any opinions, findings and conclusions or recommendations expressed here are the authors' and do not necessarily reflect those of the sponsors.",0,,False
156,4. REFERENCES,0,,False
157,"[1] J. Callan. Distributed information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 5, pages 127­150. Kluwer Academic Publishers, 2000.",1,ad,True
158,"[2] J. Callan and M. Connell. Query-based sampling of text databases. ACM Transactions on Information Systems, 19(2):97­130, 2001.",1,Query,True
159,"[3] F. Crestani and S. Wu. Testing the cluster hypothesis in distributed information retrieval. Information Processing and Management, 42(5):1137­1150, 2006.",0,,False
160,"[4] A. Khudyak Kozorovitsky and O. Kurland. Cluster-based fusion of retrieved lists. In Proceedings of SIGIR, pages 893­902, 2011.",0,,False
161,"[5] M. Shokouhi, L. Azzopardi, and P. Thomas. Effective query expansion for federated search. In Proceedings of SIGIR, pages 427­434, 2009.",0,,False
162,"[6] X. M. Shou and M. Sanderson. Experiments on data fusion using headline information. In Proceedings of SIGIR, pages 413­414, 2002.",1,ad,True
163,"[7] L. Si and J. Callan. A semisupervised learning method to merge search engine results. ACM Transactions on Information Systems, 21(4):457­491, October 2003.",0,,False
164,"[8] J. Xu and W. B. Croft. Cluster-based language models for distributed retrieval. In Proceedings of SIGIR, 1999.",0,,False
165,1170,0,,False
166,,0,,False

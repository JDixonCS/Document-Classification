,sentence,label,data,regex
0,Multi-Aspect Query Summarization by Composite Query,1,Query,True
1,"Wei Song1, Qing Yu2, Zhiheng Xu3, Ting Liu1, Sheng Li1, Ji-Rong Wen2",0,,False
2,"1School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China",0,,False
3,"{wsong, tliu, lisheng}@ir.hit.edu.cn",0,,False
4,"2 Microsoft Research Asia, Beijing, 100190, China",0,,False
5,"{qingyu, jrwen}@microsoft.com",0,,False
6,"3 Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China",1,ad,True
7,xuzhiheng19881130@gmail.com,0,,False
8,ABSTRACT,0,,False
9,"Conventional search engines usually return a ranked list of web pages in response to a query. Users have to visit several pages to locate the relevant parts. A promising future search scenario should involve: (1) understanding user intents; (2) providing relevant information directly to satisfy searchers' needs, as opposed to relevant pages. In this paper, we present a paradigm for dealing with informational queries. We aim to summarize a query's information from different aspects. Query aspects are aligned to user intents. The generated summaries for query aspects are expected to be both specific and informative, so that users can easily and quickly find relevant information. Specifically, we use a ""Composite Query for Summarization"" method, which leverages the search engine to proactively gather information by submitting multiple composite queries according to the original query and its aspects. In this way, we could get more relevant information for each query aspect and roughly classify information. By comparative mining the search results of different composite queries, it is able to identify query (dependent) aspect words, which help to generate more specific and informative summaries. The experimental results on two data sets, Wikipedia and TREC ClueWeb2009, are encouraging. Our method outperforms two baseline methods on generating informative summaries.",1,ad,True
10,Categories and Subject Descriptors,0,,False
11,H.3.m [Information Storage and Retrieval]: Miscellaneous,0,,False
12,General Terms,0,,False
13,"Algorithms, Experimentation",0,,False
14,This work was done when the first and third authors were visiting Microsoft Research Asia,0,,False
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.",1,ad,True
16,Keywords,0,,False
17,"Query aspect, Query summarization, Composite query, Mixture Model",1,Query,True
18,1. INTRODUCTION,1,DUC,True
19,"Nowadays, accessing information on the Internet through search engines has become a fundamental life activity. Current web search engines usually provide a ranked list of URLs to answer a query. This type of information access does a good job for dealing with simple navigational queries by leading users to specific websites. However, it is becoming increasingly insufficient for queries with vague or complex information need. Many queries serve just as the start of an exploration of related information space. Users may want to know about a topic from multiple aspects. Organizing the web content relevant to a query according to user intents would benefit user exploration. In addition, a list of URLs couldn't directly satisfy user information need. Users have to visit many pages and try to find relevant parts within long pages, since the information may be scattered across documents. The long-standing goal of search engines should be providing relevant information, as opposed to relevant documents, to directly satisfy searchers' needs.",1,ad,True
20,"This paper presents a novel search paradigm that the system should automatically discover information and present an informative overview for a query from multiple aspects. We target on dealing with informational queries. A query represents a centric topic, and the query aspects are aligned to user intents covering diverse information needs. The query aspects could be specified explicitly by users through an interface or automatically mined from search logs or other resources [4, 18, 22, 25]. In this paper, we use simple methods to do aspect mining and mainly focus on multi-aspect oriented query summarization: given a query and a set of aspects, generate a summary for each query aspect, which is expected to provide specific and informative content to users directly and helps for further exploration. Figure 1 shows an example of the system output.",1,ad,True
21,"We further formulate the multi-aspect oriented query summarization into 2 phases: information gathering and summary generation. Different from traditional text summarization where a set of documents to be summarized is given as a system input, we propose a ""Composite Query for Summarization"" method, which leverages the search engine to proactively gather related information. In addition to using the search result of the original query, we also composite a set of new queries and submit them to the search engine to",1,ad,True
22,325,0,,False
23,Figure 1: An example output of multi-aspect oriented query summarization.,0,,False
24,"collect query aspect related information. For example, by concatenating the original query and the keywords of an aspect as a query, we are able to get query dependent aspect information; by submitting the aspect keywords only as a query, we could get query independent aspect information. Our motivations are:",0,,False
25,"First, the search result of the original query may not contain enough information for all aspects that users care about, because the search engine returns documents only considering whether a document is relevant to the query keywords, rather than its aspects.",0,,False
26,"Second, for better aspect oriented exploration, the information for different query aspects should be as orthogonal as possible. It is important to distinguish the aspect specific information from the general information about the whole query. By using the composite queries, we could get more specific information for each aspect.",0,,False
27,"The flexible information gathering also helps for summary generation phase. By comparing the search results of different types of composite queries, query (dependent) aspect words can be identified without complex natural language processing, based on which more specific and informative summaries could be generated,",0,,False
28,The contributions of this paper can be summarized as follows:,0,,False
29,"· We formulate the multi-aspect based query summarization task. In this scenario, the system proactively discovers information and aims to provide multiple dimensional and direct information seeking in response to informational queries.",0,,False
30,"· We propose a ""Composite Query for Summarization"" method for proactive information gathering, which is a key point for our task and differs from traditional search result organization and textual summarization.",1,Query,True
31,"· We emphasize generating specific and informative summaries to directly address searchers' needs on different aspects. To achieve this, we propose a simple method to identify query aspect dependent words by comparing the search results of different types of composite queries.",1,ad,True
32,· We conduct experiments on both real web queries and,0,,False
33,large-scale pseudo queries based on Wikipedia1. Automatic evaluation and human judgements are used for measuring the quality of generated summaries.,1,Wiki,True
34,"The rest of the paper is organized as follows. First, we discuss related work in Section 2. In Section 3, we define the query aspect and briefly introduce optional approaches for query aspect mining. In Section 4, we detail the proposed ""composite query"" based method for both information gathering and aspect oriented summary generation. After that, we report our experimental results in Section 5. Section 6 states our conclusions.",0,,False
35,2. RELATED WORK,0,,False
36,2.1 Search Result Organization,0,,False
37,"Exploratory search becomes a new frontier in the search domain, which aims to provide additional support for information seeking beyond simple lookup [24]. Recent work has shown that well-organized search results are helpful for information exploration. For example, search result clustering [9, 11, 27], categorizing [1], facet based information exploration [6], representative queries [23] and tag clouds [10] are adopted for search result navigation. Clustering based approaches automatically group similar search result documents together [9, 11, 27]. Search result documents can also be classified into a manually constructed category taxonomy [1]. But the fixed hierarchy often lacks of flexibility to describe various user information needs. Faceted search aims to offer the ability for searchers to filter search results by specifying desired attributes [6]. However, the facets are usually pre-defined for some specific domain so that it is difficult to apply it to web search. Though most of the above methods organize search result documents into various aspects and improve user experience for information exploration, the content are still presented at document level, and users can't get relevant information directly.",1,ad,True
38,2.2 Document Summarization,0,,False
39,"Single document summarization techniques have been successfully applied in web search engines (snippet generation) [19, 20]. A span of text gives users a first sight of the topics of a document. For efficiency, sentence extraction strategy is used for generating query dependent summaries [5].",0,,False
40,"Comparing with single document summarization, multidocument summarization is expected to generate a global picture for a set of documents which is given as input [15, 26]. Recently researchers utilize latent topics for multiple document summarization. For example, subtopics from the narrative of a topic (a description of a topic, which is provided by the DUC summarization track) is used to enhance summarization [17]. Wang uses topic model to extract subtopics and select sentences by topic words [21]. However, the latent topics used in these papers are usually mined unsupervised. As a result, the topics may fit to the data collection, rather than align to user intents.",1,DUC,True
41,"Some work makes use of predefined aspects to provide (sentiment) summarization on reviews or comments [7, 14]. Our work is also inspired by [13], which incorporates user interaction into the summarization process. Given a corpus of documents, users predefine their interested facets and the",1,corpora,True
42,1http://en.wikipedia.org/,1,wiki,True
43,326,0,,False
44,"Figure 2: A snipping of returned documents for query ""Saving Private Ryan"" and two typical services provided by Bing Search.",0,,False
45,"system provides summaries according to the facets. The authors evaluate it on online reviews and Gene corpus (which are relatively ""clean"" data sets). In contrast, we focus on summarizing user intents related to a query rather than a given corpus. They don't consider the informativeness of the generated summaries, while one of our goals is to provide direct information to users.",0,,False
46,"Our work is based on query aspects but differs from existing work in several points. First, in our framework, query aspects could be mined from any resources but not limited to a set of documents to be summarized. Second, the traditional summarization task treats the documents as a given input to the system. However, in our scenario, we separate the information gathering and summarization generation phases. In this way, we view the whole web as a corpus and could proactively collect more related information for summarization. Third, we aim to generate both specific and informative content for each query aspect. Therefore, users could get relevant information directly.",1,ad,True
47,3. QUERY ASPECT,0,,False
48,"Multi-aspect oriented query summarization depends on query aspects. In this section, we define the query aspect and briefly discuss query aspect mining methods both in literature and in realistic way.",0,,False
49,"An aspect represents a distinct information need relevant to the original query. Recently, various methods have been proposed for automatically discovering query intents [2, 4, 22, 25]. The NTCIR-9 Intent Task was organized to explore and evaluate the technologies of mining and satisfying different user intents for a vague query [18]. In these work, a query aspect is represented in different ways, such as a set of search queries related to the original query [2, 25], a set of query qualifiers [22] or a single intent string [18]. These definitions are in fact very similar. The main differences are: (1) Whether distinguish the original query and the query qualifier. (2) Whether select an exemplar (label) to represent a set of queries related to the same intent.",0,,False
50,"Inspired by previous work, we define an aspect as a query",0,,False
51,"qualifier - keywords that are added to an original query to form a specific user intent. For example, ""reviews"" and ""actors"" could be seen as aspects for a movie. In this work, we mainly focus on multi-aspect oriented summary generation and use very simple method to mine query aspects. However, any existing method for mining query aspects could be incorporated.We can also use the services provided by search engines to get approximate query aspects. For example, search engines provide ""query suggestion"" or ""related searches"" features. Figure 2 shows a snipping of the search result from Bing Search page for query ""Saving Private Ryan"", a famous movie. Thus, the aspects could be easily identified using simple rules from related searches. We could also predefine some aspect templates for certain query classes, such as movie, travel, music, people, etc. We leave this as future work.",1,ad,True
52,4. MULTI-ASPECT ORIENTED QUERY SUMMARIZATION,0,,False
53,"Now, we suppose the aspects are given and aim to summarize a query according to its different aspects. We expect to generate both specific and informative summary for each aspect instead of a set of documents so that the users could get relevant information directly. First, we explain the meaning of specific and informative by an example. Suppose that for the query ""Saving Private Ryan"", one of the user information needs is to know the ""actors"" of this movie. There are some candidate sentences:",1,ad,True
54,"(i) ""A movie page covers information about new Steven Spielberg movie 'Saving Private Ryan' including actors, film makers and behind the scenes.""",0,,False
55,"(ii) ""Saving Private Ryan cast are listed here including the Saving Private Ryan actresses and actors featured in the film.""",0,,False
56,"(iii) ""The actors of Saving Private Ryan are Tom Hanks as Captain and several men Edward Burns, Barry Pepper...""",0,,False
57,"All the three sentences contain certain information about the aspect ""actors"". The first one talks about the general information about the query. It is Not specific to the desired aspect. The second sentence focuses on the desired aspect, however, it does not provide relevant information directly, only gives navigational information. We say it is specific but Not informative. The third sentence should be a good candidate which provides direct answers to the desired aspect, i.e., the names of the actors. It is both specific and informative.",0,,False
58,"As the example shows, the challenges of this task include: (1) Distinguish aspect specific information from general query information. (2) Identify informative content instead of navigational information only. We take the Composite Query for Summarization method to deal with above issue, which consists of 2 phases: information gathering and summary generation. First, we proactively get aspect specific information using composite queries. Then a mixture model is used to model different types of words which present query common information or aspect specific information. Finally, we rank the candidate sentences based on the mixture model and the redundancy in search results for generating summaries.",1,ad,True
59,327,0,,False
60,4.1 Information Gathering,0,,False
61,"Existing work on text summarization doesn't pay much attention on how to collect data. A natural way is to use query search result. However, there may be not enough information for certain query aspects, if we only use the search result of the original query. For example, some users wonder whether movie ""Saving Private Ryan"" tells a true story, but few top documents in the search result of ""Saving Private Ryan"" discuss this topic.",0,,False
62,"We present a composite query based method for information gathering. Formally, we denote the original query as Q and an aspect as Ak. For example, Q refers to the original query ""Saving Private Ryan"" and Ak refers to one aspect ""actors"". In information gathering phase, we composite a new query by concatenating the original query and the aspect words, denoted as Q + Ak. The composite query is ""Saving Private Ryan actors"". Therefore, we can submit the composite query to the search engine to get top ranked documents. Comparing with the search result of the original query, the search result of the composite query is much more specific for the query aspect. Also, we can submit the aspect Ak itself to the search engine to get information about the aspect which is query independent.",0,,False
63,"For a query with K aspects, we have a set of composite queries {Q, Q + A1, ..., Q + AK , A1, ..., AK }. We use the top returned documents for each composite query. The search result of Q (denoted as CQ) provides overall information about the query; the search result of Q + Ak (denoted as CQ+Ak ) provides the information about the aspect Ak of the query Q. The search result of Ak (denoted as CAk ) provides information about the aspect itself which is query independent. The idea of using composite queries is straightforward and the benefits are two folded: (1) We collect more aspect related data which may be not contained in original query's search result. (2) The search engine helps us roughly classify information according to the query aspects.",0,,False
64,"Based on the collected data for query aspects, we identify aspect words by comparing the search results of different types of composite queries. These words are then used for assisting summary generation.",0,,False
65,4.2 Summary Generation,0,,False
66,4.2.1 Modeling Search Result,0,,False
67,"We assume the desired information for query aspect Ak is embedded in collection CQ+Ak , which consists of 3 kinds of information: query general information, aspect information, irrelevant information. Correspondingly, the words in search results could be divided into 3 categories:",0,,False
68,"Query Common Words: They tend to occur frequently across multiple aspects, such as ""movie"", ""TV"", ""IMDB"" for ""Saving Private Ryan"".",1,Query,True
69,"Query Aspect Words: These words provide information for an aspect, such as ""cast"", ""list"" and ""Tom Hanks"" for the aspect ""actors"".",1,Query,True
70,"Global Background Words: These words distribute heavily on the Web. Mostly, they are stop words or high frequency non-discriminative words.",0,,False
71,Figure 3 shows 3 types of words and their relationship in search results of the original query and the composite,0,,False
72,CQ+A1,0,,False
73,Query Aspect Words,1,Query,True
74,CQ+A2,0,,False
75,Query Aspect Words,1,Query,True
76,CQ+AK,0,,False
77,Query  Aspect,1,Query,True
78,Words,0,,False
79,CQ,0,,False
80,Words For Other undefined,0,,False
81,Aspect,0,,False
82,Global Background Words,0,,False
83,Query Common Words,1,Query,True
84,Figure 3: The illustration of the relationship between the search results of different composite queries and different types of words.,0,,False
85,"queries. We assume that the query aspect words describing the aspect Ak of query Q will occur more in CQ+Ak , while the query common words will occur frequently across multiple aspects. Based on the collected data by using composite queries, the observations support the assumption. Therefore, we adopt a mixture model to describe each type of words. Formally, k represents the query aspect words model for aspect Ak. B represents the query common words model. G represents the global background words model which is to draw globally high frequency terms. All these models are multinomial probability distributions over vocabulary.",1,ad,True
86,The collection CQ+Ak could be generated by the mixture model. Each word w in CQ+Ak is generated according to:,0,,False
87,"pk(w) , Gp(w|G) + (1 - G)×",0,,False
88,(1),0,,False
89,(Bp(w|B) + (1 - B)p(w|k)),0,,False
90,"where pk(w) represents the probability of a term occurrence w in collection CQ+Ak , G and B are fixed parameters. The generative process could be seen as 2 steps: first decide whether this word is from G, and then decide it comes from B or k. To estimate the aspect word model k, we first estimate G and B. G is estimated using maximum likelihood estimator based on document frequency which is computed on a large collection of web pages. B is estimated by combining the search results of the original query and all query aspects, i.e., CQ  {CQ+Ak }. We use CQ to catch the general content of the query and the unknown aspects which are not defined explicitly or mined already. B could be estimated according to:",1,ad,True
91,p(w|B ),0,,False
92,",",0,,False
93," tf (w, CQ) + ktf (w, CQ+Ak ) w (tf (w, CQ) + k tf (w, CQ+Ak ))",0,,False
94,(2),0,,False
95,"where tf (w, ·) represents the term frequency in a collection. After deriving p(w|B) and p(w|G), p(w|k) could be estimated using the expectation maximization (EM) algorithm [3] by maximizing the log-likelihood of the collection CQ+Ak :",0,,False
96,"L(CQ+Ak ) ,"" log tf (w, CQ+Ak )pk(w)""",0,,False
97,(3),0,,False
98,w,0,,False
99,"For each term w in CQ+Ak , the updating formulas of the E-step and the M-step are shown below:",0,,False
100,328,0,,False
101,E-Step:,0,,False
102,pw (z,0,,False
103,",",0,,False
104,G),0,,False
105,",",0,,False
106,p(w|G ) p(w|G)+(1-)((1-B )p(w|k)+B p(w|B )),0,,False
107,pw (z,0,,False
108,",",0,,False
109,k),0,,False
110,",",0,,False
111,(1-B )p(w|k ) (1-B )p(w|k)+B p(w|B ),0,,False
112,M-Step:,0,,False
113,"p(w|k ) ,"" wtft(fw(w,C,QC+QA+kA)k(1)-(1p-wp(wz"",(Gz,))Gpw))(pzw, k(z),k) where z is a latent variable introduced to represent which type a word is assigned to. p(z , G) and p(z ,"" k) are corresponding probabilities. In this way, we distinguish the query aspect words from the query common words. The words with high probabilities in k represent the specific query aspect better.""",0,,False
114,"Next, we consider to identify more informative aspect words. We divide the query aspect words into 2 categories: query dependent aspect words which provide direct information for the aspect, such as ""Tom Hanks"" and ""Edward Burns"" for aspect ""actors""; query independent aspect words which are query independent and reflect the characteristics of the aspect itself, like ""actor"", ""actress"", and ""cast"" for aspect ""actors"". We distinguish these 2 types of query aspect words by the assumption that query dependent aspect words occur in CQ+Ak , and query independent aspect words occur in both CQ+Ak and CAk . The CAk is the search result of the aspect Ak itself, which contains many words related to the aspect. However, these words can be used for any query with such aspect, but don't bring direct information for a specific query. So we identify query dependent aspect words as QDWk ,"" {t|t  CQ+Ak and t  CAk }. The words occur in CQ+Ak that suggests they are related to the query aspect, but don't occur in CAk that indicates they are query dependent. The relative importance of the query dependent aspect words could be read out from p(w|k).""",1,ad,True
115,4.2.2 Sentence Selection,0,,False
116,"To summarize aspect Ak for query Q, we extract sentences from the content of the search result documents in CQ+Ak  CQ. The candidate sentences are then ranked based on their specificity, redundancy and informativeness. The top ranked sentences are used as a summary for the desired aspect.",0,,False
117,Candidate sentence filtering based on specificity. On-,0,,False
118,ly part of the sentences within the search result are related,0,,False
119,to the desired aspect. We select a candidate sentence for a,0,,False
120,desired aspect only if it is closer to the desired aspect than,0,,False
121,"to any other aspects. To measure this, we classify each sen-",0,,False
122,tence to one of the aspects:,0,,False
123,"k , argmax",0,,False
124,p(w|i),0,,False
125,(4),0,,False
126,"i{1,2,...,K,B,G} ws",0,,False
127,"where i is an estimated query aspect words model or the query common words model or the global background words model. A sentence within CQ+Ak  CQ is chosen as a candidate only if k equals to k. Thus, all the selected candidate sentences are more specific to the desired aspect.",0,,False
128,"Sentence clustering. The candidate sentences are selected from multi-documents. Redundancy is particular important. On one hand, the same information conveyed by sentences from different documents indicates its importance.",0,,False
129,"On the other hand, it is not good to show duplicate sentences to users. Due to the above reasons, the candidate sentences are grouped into clusters according to lexical features. We adopt a hierarchical clustering approach. Each single sentence is initiated as a cluster. If two clusters are close enough, they are merged. This procedure repeats until the smallest distance between all remaining clusters is larger than a threshold. Edit distance is used to measure the distance between two sentences. We use U (s) to represent the cluster, which the sentence s belong to. The size of this cluster U (s).size indicates the popularity of this cluster or the redundancy of the information this cluster conveys.",1,ad,True
130,Measuring informativeness. Since informative summaries,0,,False
131,"are expected, we measure the informativeness of a sentence",0,,False
132,based on:,0,,False
133,"inf o(s|k) , (1 - )",0,,False
134,p(w|k) + ,0,,False
135,p(w|k ),0,,False
136,"ws, wQDWk",0,,False
137,"ws, wQDWk",0,,False
138,(5),0,,False
139,where QDWk represents the query dependent aspect words for aspect Ak;  is a parameter to tune the impact of the query dependent aspect words.,0,,False
140,"Sentence ranking. In each cluster, we select one sentence with highest inf o(s|k) as the exemplar to represent the cluster. The exemplars selected from all clusters are ranked according to W eightk(s):",0,,False
141,"W eightk(s) , log(1 + U (s).size) × inf o(s|k) (6)",0,,False
142,5. EXPERIMENTS,0,,False
143,"In the experiments, we assume the query aspects are given and focus on evaluating the quality of generated summaries for query aspects. The data sets we used already contain aspects for each query. Our method and baseline methods take both query and aspects as input.",1,ad,True
144,5.1 Data Sets,0,,False
145,"To the best of our knowledge, few public data set can be used to evaluate the multi-aspect oriented query summarization. We constructed two data sets from well-known data sources, Wikepedia and TREC. We will introduce the data sets and experimental results in following sections.",1,TREC,True
146,5.1.1 Wikipedia Data,1,Wiki,True
147,"Each topic page in Wikipedia is composed of a title and a list of sub sections, which describe the topic from different aspects. For example, the title of a page is ""Saving Private Ryan"", and the page includes subheadings like ""Plot"", ""Cast"" and ""Production"". In our experiments, we treated the title of a page as a query, the meaningful subheadings (top level) as query aspects. We filtered out the meaningless subheadings like ""Notes"", ""References"" and ""Further Readings"" by rules. We also filtered out pages with less than 3 or larger than 10 aspects to avoid noise. We used the textual content under a subheading as the golden reference for the corresponding aspect. In all, we sampled 1000 pages (queries) from an English Wikipedia dump which was collected in January 2011. The statistics of the sampled data is listed in Table 1.",1,Wiki,True
148,329,0,,False
149,Table 1: The statistic of Wikipedia data set,1,Wiki,True
150,Topics,0,,False
151,1000,0,,False
152,Average Length of Topics (words) 2.15,0,,False
153,Average Aspects per Topic,0,,False
154,5.15,0,,False
155,Average Aspect Length (words) 798,0,,False
156,"We divided the sampled data into develop set and test set. The develop set containing 100 queries was used for parameter tuning. While the test set, which contains 900 queries, was used for comparing performance of different systems. Note that, since our method uses the search results of a search engine which may give Wikipedia pages as returned documents, we removed Wikipedia pages from the search results when doing experiments.",1,Wiki,True
157,5.1.2 TREC 2009 Web Track Data,1,TREC,True
158,The trec data is widely used for search related experiment evaluation. We use the public available query set of TREC 2009 Web track. One goal of TREC 2009 Web Track is evaluating the search result diversity. The data set includes 50 topics and each topic has 3 to 8 manually edited subtopics to be covered. Each subtopic is a description of an information need. Figure 4 shows an example topic provided by TREC 2009 Web track.,1,trec,True
159,"We treated each topic as a query and derived query aspects from its subtopic descriptions by simple rules. We first extracted all nouns from a description. Then we excluded those terms which occur in original query, then used the remaining terms as an aspect. For example, for the query ""Obama family tree"", ""mother information"" was used as one aspect. In all, we got 50 queries and 4.9 aspects for each query on average.",0,,False
160,5.2 Baselines,0,,False
161,"The proposed algorithm is denoted as Q-Composite. We compare it with 2 baselines. Baseline 1 is based on Ling et al [13], denoted as Ling-2008. This method first estimates an aspect prior distribution based on term co-occurrence in the corpus, then integrates the priors into a topic model, finally ranks sentences according to the distance between sentence language model and the aspect models. It is proved very effective for mining faceted summaries on relatively clean and formal data sets, like Gene corpus. But it is not oriented to the web search. Like traditional text summarization tasks, they just use a collection of documents related to the centric topic for summarization. We implemented this method and applied it to the multiple aspect based query summarization as a baseline. The aspect model of Ling-2008 was estimated on the search result of each original query and the sentences for each aspect were extracted from the search results of both the original query and the composite query, which was the same as the input of our method.The second baseline is based on the top sentences in snippets, which are provided by a search engine for each composite query Q+Ak, denoted as Snippet. The number of the top sentences depends on the total summarization length limit. Though it is simple, it is very strong. These snippets are selected from the top relevant documents of the composite query, so that they are more likely specific to the query aspect. In addition, most snippet generation algorithms are based on single document",1,ad,True
162,Figure 4: An example topic in TREC 2009 web track.,1,TREC,True
163,"summarization method, which tend to extract the sentences containing most relevant terms.",0,,False
164,5.3 Parameter Settings,0,,False
165,"There are several parameters in our method. We tuned the parameters of our method and baselines on the develop set. In our experiments, G was set to 0.95 in order to get more discriminative words. B was set to 0.8 to balance query common information and aspect specific information. The threshold used in sentence merging procedure was set to 0.7. The parameter  was set to 0.0, which means to rank sentences based on query dependent aspect words only. For each composite query, we used the top 50 documents from the search result. The words occurring in less than 3 documents were discarded.",0,,False
166,5.4 Experiment Design and Evaluation,0,,False
167,"Due to the different characteristics of the two data sets, we adopt different evaluation strategies and metrics.",1,ad,True
168,5.4.1 Evaluation on Wikipedia Data,1,Wiki,True
169,"For Wikipedia data, we generate the summaries based on real web data. We send both the original query and the composite queries to a commercial search engine and get the search result documents and snippets. For efficiency, we train the model using the snippets and extract sentences from the content of the documents. We use the ROUGE tool for evaluation on Wikipedia Data. ROUGE is a wellknown tool for evaluating both single and multi-document summarization [12]. Basically, it is a recall-like metric. A higher ROUGE value means that more useful information is found. ROUGE-1 metric has been proved highly consistent with human judgements, so we take it for evaluation in our experiments. At evaluating time, the golden reference for each aspect is taken from the content of corresponding subheading in a Wikipedia page. Since the extracted sentences for summarization have different length, we let each system generate top sentences and the first 200, 400 and 600 words are used for evaluation.",1,Wiki,True
170,5.4.2 Evaluation on TREC 2009 Data,1,TREC,True
171,"For TREC data set, we generate summaries from the corpus provided by TREC rather than the whole Web, namely the ClueWeb09. Our method depends on the search engine's search result, so we need index ClueWeb09 and build a small search engine. We use a simple ranking function to give search result based on BM25 [16], anchor text and stat-",1,TREC,True
172,330,0,,False
173,"Table 2: Labeling guide and examples. The query is ""Saving Private Ryan"" and the aspect is ""Actors""",0,,False
174,Label,0,,False
175,Gain Value Description,0,,False
176,Examples,0,,False
177,(Level),0,,False
178,Informative and spe- 5 cific,0,,False
179,The sentence focuses on the desired aspect and provides useful,0,,False
180,"The actors of ""Saving Private Ryan"" include Tom Hanks, Tom Sizemor, Edward Burns.",0,,False
181,information which can help user to,0,,False
182,know something about the query,0,,False
183,aspect.,0,,False
184,"Saving Private Ryan is a 1998 American war film,",0,,False
185,Informative but not 4,0,,False
186,The sentence conveys multi-aspect directed by Steven Spielberg and it follows,0,,False
187,specific,0,,False
188,information about the query. And Tom Hanks as Captain John H. Miller.,0,,False
189,it does provide useful information,0,,False
190,for the desired aspect.,0,,False
191,Specific but not in- 2 formative,0,,False
192,The sentence talks about the desired aspect but doesn't provide,0,,False
193,Saving Private Ryan Cast and Details on TVGuide.com.,0,,False
194,much detail information.,0,,False
195,Not about this aspect but about the query,0,,False
196,1,0,,False
197,Saving Private Ryan is a 1998 American It provides some information epic war film set during the invasion of Normandy in about some aspects of the query World War II.,0,,False
198,but not related to the desired as-,0,,False
199,pect.,0,,False
200,Alphabetized and searchable index of real and,0,,False
201,Not about this query 0,0,,False
202,"The sentence does not talk about fictional events, cast, and places related to",0,,False
203,the query.,0,,False
204,films.,0,,False
205,ic rank features. It generates snippets by selecting the top sentences which contain the most query terms.,0,,False
206,"Since the data does not provide golden reference at sentence level, we have to judge the quality of generated sentences manually. So it is necessary to clarify the standard for assessment. Ideally, a good query summary should make users get the desired information directly. In our scenario, we assess the summaries from two perspectives: specific and informative. First, we hope the summary can give specific information about an aspect rather than a general description covering multiple aspects. Second, it should give more direct information in contrast to navigational information so that users spend less time to obtain information.",0,,False
207,"Based on this standard, we asked labelers to label the generated sentences for 50 queries. For each system and each query aspect, the labelers had to evaluate the top 3 ranked sentences. Each sentence was assigned a gain value according to the guidelines shown in Table 2, which describes the labeling standard by using an example. Note that we skip the gain value 3, because we think that the ""informative and specific"" and ""informative but not specific"" sentences are useful to users for getting direct information, should be given higher bonus than other levels. The topic descriptions, as shown in Figure 4, were also presented to labelers as reference.",1,ad,True
208,The normalized Discounted Cumulative Gain (nDCG) [8] is used to evaluate the performance. The nDCG is a metric that gives higher weights to well ranked objects. The average nDCG over all the test query aspects is used to measure the overall performance.,0,,False
209,5.5 Experimental Results and Discussion,0,,False
210,"In this session, we present the experimental results on two data sets and analyze the performance of different systems and the impacts of key factors.",0,,False
211,Coverage,0,,False
212,Top search results of the composite queries,0,,False
213,0.5,0,,False
214,0.45,0,,False
215,0.4,0,,False
216,0.35,0,,False
217,0.3,0,,False
218,0.25,0,,False
219,0.2,0,,False
220,0.15,0,,False
221,0.1,0,,False
222,50,0,,False
223,100,0,,False
224,300,0,,False
225,500,0,,False
226,Top search results of the original queries,0,,False
227,Top1 Top5 Top10 Top30,0,,False
228,Figure 5: The average coverage of the search results of the original queries over the composite aspect queries.,0,,False
229,5.5.1 Coverage of the Search Results of the Original Queries,0,,False
230,Previous work focuses on organizing the search result of,0,,False
231,the original query into multiple aspects. We argue that the,0,,False
232,search result of an original query may not have enough infor-,0,,False
233,"mation covering all query aspects. To verify this, we conduct",0,,False
234,a simple experiment to measure the coverage of the search,0,,False
235,results of the original queries on the corresponding compos-,0,,False
236,ite queries. We sampled 100 queries from the Wikipedia,1,Wiki,True
237,"data set. For each original query Q, we retrieved the set",0,,False
238,"of top N URLs from a search engine, denoted as SQN . For",0,,False
239,"each composite query Q + Ak, we retrieved the set of top M",0,,False
240,"URLs from the same search engine, denoted as SQM+Ak . We",0,,False
241,measured,0,,False
242,the,0,,False
243,coverage,0,,False
244,of,0,,False
245,SQN,0,,False
246,over,0,,False
247,"SQM+Ak ,",0,,False
248,"i.e.,",0,,False
249,. . |SQ N SQ M+Ak |,0,,False
250,M,0,,False
251,The average coverage over all queries' aspects is shown in,0,,False
252,"Figure 5. Intuitively, the search result of Q + Ak should de-",0,,False
253,331,0,,False
254,"scribe the query aspect better. However, the top documents in SQM+Ak rarely appear in SQN . For example, more than 60% top 1 documents retrieved by composite queries are not in the top 100 returned documents for the corresponding original queries. When considering more top documents in SQM+Ak , the coverage is even smaller. These observations indicate that, at the document level, the search results of the original queries couldn't cover most relevant information related to query aspects. By using composite queries, we could get much more relevant information. Next, we evaluate the quality of the fine-grained information units generated by systems.",0,,False
255,5.5.2 System Comparisons,0,,False
256,"Figure 6 shows the performance comparisons of different systems on Wikipedia test set, varying the word number of summary length limit. We can see that Q-Composite outperforms both Ling-2008 and Snippet. The results on TREC 2009 data have the similar trend, which are shown in Figure 7. We have found favorable results for Q-Composite on both NDCG@1 and NDCG@3. This shows proposed method is effective to extract more informative and aspect specific sentences. Especially, Q-Composite gains great improvement on NDCG@1, which is important for presenting condensing information on result pages.",1,Wiki,True
257,"To gain more insights, we analyze the label level distributions of the generated summary sentences of the 3 systems on TREC 20009 data set, as shown in Figure 8. The Xaxis are label levels. The Y-axis is the distribution. We can see that our method provides more informative sentences (level 5 and level 4) compared with baselines. However, all systems still generate less specific and informative sentences than navigational sentences. This indicates the task is really challenging.",1,TREC,True
258,"Q-Composite performs better than Ling-2008. The reasons may include: (1) Ling-2008 estimates the aspect model on the search result of the original query. There may be not enough information covering all query aspects as shown in section 5.5.1. Therefore, for difficult aspects, it is unable to estimate accurate models. (2) In the search result of the original query, information related to multiple aspects often mixes together. It increases the difficulty to estimate discriminative aspect models. Therefore, it is more difficult to provide specific information for desired aspect. (3) The search result is so noisy that there are many navigational sentences. For example, sentences containing ""actors"" may also contain words like ""cast"", ""list"" and ""actress"". These words are very easy to have higher weights in aspect models and the sentences are ranked high as well. However, such sentences may only contain navigational information but can't provide direct information. Another reason affecting the performance of Ling-2008 may be that we did not implement the variation with regularization, which is more complex but reported having better performance than the basic algorithm with Dirichlet model priors. In contrast, by using composite query based method, we are able to get more aspect specific information and roughly classify the information. By distinguishing query dependent aspect words and query independent aspect words, we give bonus to sentences that are aspect specific but also contain more information beyond aspect words.",0,,False
259,"Snippet performs well on Wikipedia data set. It is reasonable, since the snippet generation algorithm favorites the",1,Wiki,True
260,ROUGE-1,0,,False
261,0.4,0,,False
262,0.35,0,,False
263,0.304,0,,False
264,0.334 0.3008.320,0,,False
265,0.3 0.265 0.258,0,,False
266,0.2707.287,0,,False
267,0.25,0,,False
268,0.239,0,,False
269,Q-Composite,0,,False
270,0.2 Ling-2008,0,,False
271,0.15,0,,False
272,Snippet,0,,False
273,0.1,0,,False
274,0.05,0,,False
275,0,0,,False
276,200,0,,False
277,400,0,,False
278,600,0,,False
279,words,0,,False
280,Figure 6: ROUGE-1 performance of Q-Composite and baseline systems on Wikipedia test data.,1,Wiki,True
281,Performance,0,,False
282,0.700 0.600 0.500 0.400 0.300 0.200 0.100 0.000,0,,False
283,0.282 0.190 0.146,0,,False
284,NDCG@1,0,,False
285,0.608,0,,False
286,0.570,0,,False
287,0.498,0,,False
288,Q-Composite Ling-2008 Snippet,0,,False
289,NDCG@3,0,,False
290,Figure 7: Performance comparisons between systems on TREC 2009 data set.,1,TREC,True
291,"sentences containing many query terms. Thus the generated summaries match many query aspect terms, which benefits ROUGE-1 metric, especially when the length of summaries is short. However, the snippets don't show much informative information. From the Figure 8, we can see that Snippet provides more level 2 sentences (specific but not informative), but very few level 4 and level 5 sentences. The generated sentences usually lack of detail description about the query aspect, mostly are just navigational sentences which often fail to satisfy user information need directly. Our method could get more aspect specific information by comparing the search results of multiple composite queries. Highlighting query dependent aspect words also helps select more informative sentences. Snippet generates less irrelevant sentences. One reason is that most sentences in snippets contain original query terms, while other methods don't have such constraint. Another reason may be that using composite queries may lead to topic drift, if the search results of the composite queries contain much noise.",1,ad,True
292,5.5.3 The Impact of Query Dependent Aspect Words,1,Query,True
293,"Our method distinguishes the query dependent aspect words and query independent aspect words. We examine the impact of these two types of words. We set the parameter  to be 0.5 in Equation 5 , which means we do not distinguish query dependent and independent aspect words. We denote it as Q-Composite-AVG. Since the human judgements on TREC 2009 data directly measure the informativeness of the generated summaries, we compare Q-Composite ( , 0.0) and Q-Composite-AVG on this data set. Figure 9 shows the level distributions of the generated top 1 sentences. We can",1,TREC,True
294,332,0,,False
295,Q-Composite,0,,False
296,0.5,0,,False
297,0.45,0,,False
298,Label level 0.4,0,,False
299,0.35,0,,False
300,Percentage,0,,False
301,0.3 Q-Composite,0,,False
302,0.25 Ling-2008,0,,False
303,0.2,0,,False
304,Snippet,0,,False
305,0.15,0,,False
306,0.1,0,,False
307,0.05,0,,False
308,0,0,,False
309,0,0,,False
310,1,0,,False
311,2,0,,False
312,4,0,,False
313,5 Label Levels,0,,False
314,Figure 8: Level distributions of systems on TREC 2009 data set.,1,TREC,True
315,Percentage,0,,False
316,0.4,0,,False
317,0.35,0,,False
318,0.3,0,,False
319,0.25,0,,False
320,0.2,0,,False
321,Q-Composite,0,,False
322,0.15,0,,False
323,Q-Composite-AVG,0,,False
324,0.1,0,,False
325,0.05,0,,False
326,0,0,,False
327,0,0,,False
328,1,0,,False
329,2,0,,False
330,4,0,,False
331,5,0,,False
332,Label Levels,0,,False
333,Figure 9: Level distributions of Q-Composite and Q-Composite-Avg on TREC 2009 data set.,1,TREC,True
334,"see that Q-Composite generates more informative sentences (level 4 and level 5). In contrast, the Q-Composite-AVG generates more specific but non-informative sentences (level 2). That is because Q-Composite favorites the words not only related to the aspect but also related to the original query. The results show that distinguishing query dependent aspect words and independent words is useful for identifying more informative sentences. However, we also see that QComposite selects slightly more irrelevant sentences. This is because some composite queries bring in more noise, which leads to topic drift.",1,ad,True
335,5.5.4 The Impact of Search Engine Result,0,,False
336,"Our method uses the search results returned by the search engine. In this section, we examine whether the quality of returned documents can affect system performance. We simulate some not very good results, by removing some documents from the search results or randomly picking documents. We test on the Wikipedia test set, since the evaluation can be done automatically. In details, we evenly remove 5 documents from the top 50 search results, denoted as remove5, namely the 1st, 11st, 21st, 31st and 41st documents. We construct the remove15 in the same way. We also randomly sample 50 documents from the top 1000 results (denoted as random) and select the last 50 documents (denoted as tail).",1,Wiki,True
337,"The experimental results are shown in Figure 10. When the search results are not so bad (remove5 or remove15), where most of the documents are relevant, the results are comparable. However, as the relevant documents reduce and noisy data increases, the models may be not very accurate. It shows worse results on random and tail. The results indi-",1,ad,True
338,ROUGE-1,0,,False
339,0.250 0.200 0.150 0.100 0.050 0.000,0,,False
340,0.221,0,,False
341,0.217,0,,False
342,0.217,0,,False
343,0.204,0,,False
344,Origin Remove5 Remove15 Random,0,,False
345,0.180 Tail,0,,False
346,"Figure 10: The impact of seach engine, on Wikipedia test set using ROUGE-1 performance.",1,Wiki,True
347,"cate our method depends on the quality of the search engine search results. For difficult composite queries, there may be no enough relevant candidate sentences for summarization. More noise may lead to topic drift as well.",1,ad,True
348,6. CONCLUSIONS AND FUTURE WORK,0,,False
349,"In this paper, we presented a multi-aspect oriented query summarization task. This task aims to summarize a query from multiple aspects which are aligned to user intents. Ideally, the users could get relevant information satisfying their information needs directly. Specifically, we formulated the task into 2 main phases: information gathering and summary generation. In the information gathering phase, we proposed a composite query based strategy, which proactively gets information based on the search engine. This strategy differs from traditional search result organization and text summarization, where the set of documents to be deal with is seen as a given system input. In the summary generation phase, we took into consideration the specificity, informativeness and redundancy for sentence selection. We conducted experiments on 2 data sets. Both automatic evaluation and manually judgements were explored. We emphasized that the quality of aspect oriented summaries should be evaluated according to their specificity and informativeness. The experimental results showed that by using composite queries, much more aspect relevant information could be got and our method outperformed 2 baselines for generating informative summaries.",1,ad,True
350,"The proposed method attempts to directly provide well organized and relevant information to users, as opposed to relevant documents. We have several possible directions of future work. First, in this paper we assume the query aspects are given. We would examine the system performance when using automatically mined query aspects. Second, more advanced methods could be exploited to integrate multiple sources of information related to a query for generating more informative summaries. Third, the composite query strategy could be applied for search result diversification by retrieving more aspect related documents.",1,ad,True
351,Acknowledgments,0,,False
352,"The 1st, 4th and 5th authors are supported by the National Natural Science Foundation of China under Grant No. 60736044, by the National High Technology Research and Development Program of China No. 2011ZX01042-001-001, by Key Laboratory Opening Funding of MOE-Microsoft Key",0,,False
353,333,0,,False
354,"Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, HIT.KLOF.2009020.",0,,False
355,7. REFERENCES,0,,False
356,"[1] H. Chen and S. T. Dumais. Bringing order to the web: automatically categorizing search results. In CHI, pages 145­152, 2000.",0,,False
357,"[2] V. Dang, X. Xue, and W. B. Croft. Inferring query aspects from reformulations using clustering. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM '11, pages 2117­2120, New York, NY, USA, 2011. ACM.",0,,False
358,"[3] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1­38, 1977.",0,,False
359,"[4] Z. Dou, S. Hu, K. Chen, R. Song, and J.-R. Wen. Multi-dimensional search result diversification. In Proceedings of the 4th ACM WSDM, pages 475­484, New York, NY, USA, 2011. ACM.",0,,False
360,"[5] J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. Multi-document summarization by sentence extraction. In Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization, pages 40­48, Stroudsburg, USA, 2000.",0,,False
361,"[6] M. A. Hearst. Clustering versus faceted categories for information exploration. Commun. ACM, 49:59­61, April 2006.",0,,False
362,"[7] M. Hu and B. Liu. Mining and summarizing customer reviews. In W. Kim, R. Kohavi, J. Gehrke, and W. DuMouchel, editors, Proceedings of the 10th ACM SIGKDD, Seattle, Washington, USA, August 22-25, 2004, pages 168­177. ACM, 2004.",0,,False
363,"[8] K. J¨arvelin and J. Kek¨al¨ainen. Ir evaluation methods for retrieving highly relevant documents. In Proceedings of the 23rd annual international ACM SIGIR, pages 41­48, New York, NY, USA, 2000. ACM.",0,,False
364,"[9] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram. A hierarchical monothetic document clustering algorithm for summarization and browsing search results. In Proceedings of the 13th international conference on WWW, pages 658­665, New York, NY, USA, 2004. ACM.",0,,False
365,"[10] B. Y.-L. Kuo, T. Hentrich, B. M. . Good, and M. D. Wilkinson. Tag clouds for summarizing web search results. In Proceedings of the 16th ACM WWW, pages 1203­1204, New York, NY, USA, 2007. ACM.",0,,False
366,"[11] D. J. Lawrie and W. B. Croft. Generating hierarchical summaries for web searches. In Proceedings of the 26th ACM SIGIR, pages 457­458, New York, NY, USA, 2003. ACM.",0,,False
367,"[12] C.-Y. Lin and E. Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Conference of the NAACL Volume 1, pages 71­78, Stroudsburg, PA, USA, 2003. Association for Computational Linguistics.",0,,False
368,"[13] X. Ling, Q. Mei, C. Zhai, and B. Schatz. Mining multi-faceted overviews of arbitrary topics in a text",0,,False
369,"collection. In Proceeding of the 14th ACM SIGKDD, pages 497­505, New York, NY, USA, 2008. ACM.",0,,False
370,"[14] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th international conference on World Wide Web, pages 171­180, New York, NY, USA, 2007. ACM.",1,blog,True
371,"[15] A. Nenkova, L. Vanderwende, and K. McKeown. A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization. In Proceedings of the 29th Annual International ACM SIGIR, Seattle, Washington, USA, pages 573­580. ACM, 2006.",0,,False
372,"[16] S. Robertson and H. Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3:333­389, April 2009.",0,,False
373,"[17] C. Shen, D. Wang, and T. Li. Topic aspect analysis for multi-document summarization. In Proceedings of the 19th ACM CIKM, pages 1545­1548, New York, NY, USA, 2010. ACM.",0,,False
374,"[18] R. Song, M. Zhang, T. Sakai, M. Kato, Y. Liu, M. Sugimoto, Q. Wang, and N. Orii. Overview of the ntcir-9 intent task. In NTCIR-9 Proceedings, pages 82­105. Morgan and Claypool, December 2011.",0,,False
375,"[19] A. Tombros and M. Sanderson. Advantages of query biased summaries in information retrieval. In Proceedings of the 21st ACM SIGIR, pages 2­10, New York, NY, USA, 1998. ACM.",0,,False
376,"[20] C. Wang, F. Jing, L. Zhang, and H.-J. Zhang. Learning query-biased web page summarization. In Proceedings of the 6th ACM CIKM, pages 555­562, New York, NY, USA, 2007. ACM.",0,,False
377,"[21] D. Wang, S. Zhu, T. Li, and Y. Gong. Multi-document summarization using sentence-based topic models. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 297­300, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics.",0,,False
378,"[22] X. Wang, D. Chakrabarti, and K. Punera. Mining broad latent query aspects from search sessions. In Proceedings of the 15th ACM SIGKDD, pages 867­876, New York, NY, USA, 2009. ACM.",1,ad,True
379,"[23] X. Wang and C. Zhai. Learn from web search logs to organize search results. In Proceedings of the 30th annual international ACM SIGIR, pages 87­94, New York, NY, USA, 2007. ACM.",0,,False
380,"[24] R. White and R. Roth. Exploratory search. beyond the query-response paradigm. In Synthesis Lectures on Information Concepts, Retrieval, and Services Series, Gary Marchionini (ed.), vol. 3. Morgan and Claypool, 2009.",1,ad,True
381,"[25] F. Wu, J. Madhavan, and A. Halevy. Identifying aspects for web-search queries. In Journal of Artificial Intelligence Research, pages 677­700, 2011 (40).",1,ad,True
382,"[26] W.-t. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. Multi-document summarization by maximizing informative content-words. In Proceedings of the 20th IJCAI, pages 1776­1782, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.",0,,False
383,"[27] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma. Learning to cluster web search results. In Proceedings of the 27th annual international ACM SIGIR, pages 210­217, New York, NY, USA, 2004. ACM.",0,,False
384,334,0,,False
385,,0,,False

,sentence,label,data,regex
0,Inferring Missing Relevance Judgments from Crowd Workers via Probabilistic Matrix Factorization,0,,False
1,Hyun Joon Jung,0,,False
2,Dept. of Electrical and Computer Engineering University of Texas at Austin,0,,False
3,hyunJoon@utexas.edu,0,,False
4,Matthew Lease,0,,False
5,School of Information Science University of Texas at Austin,0,,False
6,ml@ischool.utexas.edu,0,,False
7,ABSTRACT,0,,False
8,"In crowdsourced relevance judging, each crowd worker typically judges only a small number of examples, yielding a sparse and imbalanced set of judgments in which relatively few workers influence output consensus labels, particularly with simple consensus methods like majority voting. We show how probabilistic matrix factorization, a standard approach in collaborative filtering, can be used to infer missing worker judgments such that all workers influence output labels. Given complete worker judgments inferred by PMF, we evaluate impact in unsupervised and supervised scenarios. In the supervised case, we consider both weighted voting and worker selection strategies based on worker accuracy. Experiments on crowd judgments from the 2010 TREC Relevance Feedback Track show promise of the PMF approach merits further investigation and analysis.",1,TREC,True
9,Categories and Subject Descriptors,0,,False
10,I.2.6 [Artificial Intelligence]: Learning,0,,False
11,General Terms,0,,False
12,"Algorithms, Design, Experimentation, Performance",0,,False
13,Keywords,0,,False
14,"Crowdsourcing, label aggregation, matrix Factorization",0,,False
15,1. INTRODUCTION,1,DUC,True
16,"Crowdsourced relevance judging offers potential to reduce time, cost, and effort of relevance judging [1] and benefit from greater diversity of crowd judges. However, quality of judgments from non-workers continues to be a concern, motivating continuing work in quality assurance methods based on statistical label aggregation methods or greater attention to human factors. A common approach is to collect multiple, redundant judgments from workers and aggregate them via methods like majority voting (MV) or expectation maximization (EM) to produce consensus labels [4].",0,,False
17,"Because each crowd worker typically judges only a small number of examples, collected judgments are typically sparse and imbalanced, with relatively few workers influencing output consensus labels. MV is completely susceptible to this problem. EM addresses this indirectly: while only workers",1,ad,True
18,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",0,,False
19,Figure 1: Crowdsourcing workers judgments (Left) are copied to a sparse worker-task matrix (Middle). Missing judgments are inferred via PMF (Right).,0,,False
20,"labeling an example vote on it, global worker judgments are used to infer class priors and worker confusion matrices.",0,,False
21,"We propose to tackle this issue directly by adopting a collaborative filtering approach, which routinely deals with the issue of each user rating only a small number of items (e.g., movies, books, etc.) vs. the complete set. In particular, we employ probabilistic matrix factorization (PMF), which induces a latent feature vector for each worker and example [6] in order to infers unobserved worker judgments for all examples. Figure 1 depicts our approach graphically.",1,ad,True
22,"We are not familiar with any prior work investigating PMF, or collaborative filtering approaches more generally, toward crowdsourcing quality assurance. Related prior work has investigated other ways to infer bias corrected labels in place of raw labels [4], as well as inference of missing labels by estimating a unique classifier for each worker [3].",0,,False
23,"Probabilistic Matrix Factorization (PMF). Suppose we have M tasks (examples to be labeled), N workers, and a label matrix R in which Rij indicates the label of worker i for task j. Let U  RDM and V  RDN be latent feature matrices for workers and tasks, with column vectors Ui and Vj representing D-dimensional worker-specific and taskspecific latent feature vectors, respectively. The conditional probability distribution over the observed labels R  RNM is given by Equation 1. Indicator Iij equals 1 iff worker i labeled task j. We place zero-mean spherical Gaussian priors on worker and task feature vectors (Equations 2 and 3).",0,,False
24,NM,0,,False
25,"p(R|U, V, 2) ,",0,,False
26,"[N (Rij |UiT Vj , 2)]Iij",0,,False
27,(1),0,,False
28,"i,1 j,1",0,,False
29,N,0,,False
30,"p(U |U2 ) ,"" [N (Ui|0, U2 I)]""",0,,False
31,(2),0,,False
32,"i,1",0,,False
33,M,0,,False
34,"p(V |V2 ) ,"" [N (Vj |0, V2 I)]""",0,,False
35,(3),0,,False
36,"j,1",0,,False
37,1095,0,,False
38,Method 1 2 3 4 5 6 7,0,,False
39,Supervised No No No Yes Yes Yes Yes,0,,False
40,Worker Labels raw (sparse) raw (sparse) PMF (complete) raw (sparse) raw (sparse) raw (sparse) PMF (complete),0,,False
41,Label Aggregation MV EM MV WV,0,,False
42,"Filtering(,0.67) WV & Filtering(,0.67) WV & Filtering(,0.7)",0,,False
43,ACC 0.603 0.644 0.643 0.642 0.752 0.750 0.673,0,,False
44,Rank 4 3 3 3 1 1 2,0,,False
45,RMSE 0.63 0.596 0.598 0.598 0.498 0.500 0.571,0,,False
46,Rank 4 3 3 3 1 1 2,0,,False
47,SPE 0.332 0.418 0.440 0.900 0.838 0.848 0.542,0,,False
48,Rank 6 4 5 1 2 2 3,0,,False
49,"Table 1: Results of PMF-based inference of missing worker labels. For the unsupervised case, majority voting (MV) with PMF (Method 3) is compared to MV and EM approaches using input (sparse) worker labels (Methods 1-2). With supervision, we compare weighted voting (WV) and/or filtering with and without PMF. Ranks shown indicate statistically significant differences at p <, 0.05 using a two-tailed paired t-test.",0,,False
50,"To estimate model parameters, we maximize the log-posterior over task and worker features with fixed hyper-parameters. Maximizing the posterior with respect to U and V is equivalent to minimizing squared error with L2 regularization:",0,,False
51,1N 2,0,,False
52,M,0,,False
53,Iij (Rij,0,,False
54,-,0,,False
55,UiT,0,,False
56,Vj )2,0,,False
57,+,0,,False
58,U 2,0,,False
59,N,0,,False
60,Ui,0,,False
61,2 F,0,,False
62,+,0,,False
63,V 2,0,,False
64,M,0,,False
65,Vj,0,,False
66,2 F,0,,False
67,"i,1 j,1",0,,False
68,"i,1",0,,False
69,"i,1",0,,False
70,"where U ,"" U /, V "","" V /, and""",0,,False
71,2 F,0,,False
72,denotes,0,,False
73,the,0,,False
74,Frobe-,0,,False
75,nius Norm. We use gradient descent to find a local mini-,1,ad,True
76,"mum of the objective for U and V . Finally, we infer missing",0,,False
77,worker judgments in the worker-task matrix R by taking,0,,False
78,"the scalar product of U and V. Note that as in [4], we also",0,,False
79,replace actual labels with bias-corrected inferred labels.,0,,False
80,Label Aggregation. Given the complete set of inferred,0,,False
81,"worker relevance judgments in matrix R, we next aggregate",0,,False
82,worker judgments to induce consensus labels. We consider,0,,False
83,"both unsupervised supervised scenarios. In the former, we",0,,False
84,consider majority voting with raw (sparse) labels (Method,0,,False
85,"1), expectation maximization with raw labels (Method 2),",0,,False
86,"and PMF-based MV (Method 3). In the supervised case, we",0,,False
87,"measure each worker's accuracy based on expert judgments,",0,,False
88,with labels of anti-correlated workers flipped such that ac-,0,,False
89,curacy is always  50%. We use supervision in two distinct,0,,False
90,"ways: weighted voting (WV) and worker filtering, in which",0,,False
91,only workers with accuracy   participate in voting.,0,,False
92,2. EVALUATION,0,,False
93,"Experiments are performed on crowd judgments collected in the 2010 TREC Relevance Feedback Track [2] from Amazon Mechanical Turk. 762 crowd workers judged 19033 querydocument tasks (examples), and 89624 judgments were collected. Our worker-task matrix thus has 762 columns (workers) and 19,033 rows (tasks); only 89,624 out of 14,503,146 labels (0.6%) are observed, so data is extremely sparse. 3,275 expert relevance judgments by NIST are partitioned into training (2,275) and test (1,000) sets. The test set is evenlybalanced between relevant and non-relevant classes.",1,TREC,True
94,"Parameters. For dimensionality of task and worker latent feature vectors, we consider D  10, 30, 50 and select D ,"" 30 based on cross-validation on the entire set of labels (unsupervised). We similarly tune regularization parameter   {0.001, 0.01, 0.1, 0.5} and select  "","" 0.1. We tune the worker filtering threshold   [0.6, 0.99] by cross-validation on the training set using a linear sweep with step-size 0.01.""",0,,False
95,"Metrics and Results. Table 1 reports accuracy (ACC), RMSE, and specificity achieved by each method.",0,,False
96,Unsupervised Methods. Method 2 of PMF with ma-,0,,False
97,jority voting (MV) outperforms the MV baseline (Method 1) and performs equivalently to EM (Method 2).,0,,False
98,"Supervised vs. Unsupervised Methods. While supervised methods tend to dominate, unsupervised EM and PMF both match performance of the supervised weighted voting (WV) method without filtering or PMF (Method 4).",0,,False
99,"Supervised Methods. Worker filtering is clearly seen to provide the greatest benefit, and surprisingly performs better without PMF than with PMF (Methods 6 vs. 7). When filtering is used, use of WV is not seen to further improve performance (Methods 5 vs. 6). We do see PMF-based modeling outperform non-PMF modeling when worker filtering is not employed (Methods 7 vs. 4).",0,,False
100,3. CONCLUSION,0,,False
101,"While unsupervised consensus labeling accuracy with PMF only matched EM performance, PMF is advantageous in that once complete worker judgments are inferred, they might be used for a variety of other purposes, such as better routing or recommending appropriate tasks to workers.",1,ad,True
102,"Intuitively, an accurate worker's empirical label distribution should resemble the actual class prior. This suggests an alternative, more weakly supervised scenario to consider in which class priors are known while example labels are not. In the unsupervised case, we might instead simply examine the distribution of empirical priors for each worker and detect outliers [5]. In future work, we plan to investigate these ideas further in combination with those described here.",1,ad,True
103,4. REFERENCES,0,,False
104,"[1] O. Alonso, D. Rose, and B. Stewart. Crowdsourcing for relevance evaluation. SIGIR Forum, 42(2):9­15, 2008.",0,,False
105,"[2] C. Buckley, M. Lease, and M. D. Smucker. Overview of the TREC 2010 Relevance Feedback Track (Notebook). In Proc. of the 19th Text Retrieval Conference, 2010.",1,TREC,True
106,"[3] S. Chen, J. Zhang, G. Chen, and C. Zhang. What if the irresponsible teachers are dominating? a method of training on samples and clustering on teachers. In 24th AAAI Conference, pages 419­424, 2010.",0,,False
107,"[4] P. Ipeirotis, F. Provost, and J. Wang. Quality management on amazon mechanical turk. In Proceedings of the ACM SIGKDD workshop on human computation, pages 64­67. ACM, 2010.",0,,False
108,"[5] H. J. Jung and M. Lease. Improving Consensus Accuracy via Z-score and Weighted Voting. In AAAI Workshop on Human Computation (HComp), 2011.",0,,False
109,"[6] R. Salakhutdinov and et al. Probabilistic matrix factorization. In NIPS 2008, volume 20, January 2008.",0,,False
110,1096,0,,False
111,,0,,False

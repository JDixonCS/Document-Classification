,sentence,label,data,regex
0,Time to Judge Relevance as an Indicator of Assessor Error,0,,False
1,Mark D. Smucker,0,,False
2,Department of Management Sciences University of Waterloo,0,,False
3,mark.smucker@uwaterloo.ca,0,,False
4,Chandra Prakash Jethani,0,,False
5,"Yahoo! Inc. Sunnyvale, CA, USA",1,Yahoo,True
6,jethani@yahoo-inc.com,0,,False
7,ABSTRACT,0,,False
8,"When human assessors judge documents for their relevance to a search topic, it is possible for errors in judging to occur. As part of the analysis of the data collected from a 48 participant user study, we have discovered that when the participants made relevance judgments, the average participant spent more time to make errorful judgments than to make correct judgments. Thus, in relevance assessing scenarios similar to our user study, it may be possible to use the time taken to judge a document as an indicator of assessor error. Such an indicator could be used to identify documents that are candidates for adjudication or reassessment.",1,ad,True
9,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,0,,False
10,"Keywords: Relevance judging, assessor error detection",0,,False
11,1. INTRODUCTION,1,DUC,True
12,"We have two motivations for wanting to better understand assessor behavior and assessor error. First, knowledge of assessor behavior should help IR researchers in the construction of more accurate relevance assessing systems. Second, while traditional retrieval metrics are relatively insensitive to assessor error [6], newer metrics that sample documents for judging, can be dramatically affected by assessor error [1, 7]. A single sampled document can represent thousands of documents in the final estimation, and a single document falsely judged to be relevant can greatly inflate the estimated number of relevant documents. Being able to predict which documents might have incorrect judgments could be useful for improving assessor consistency by asking assessors to judge certain documents a second time [1].",1,ad,True
13,"We have previously reported on relevance judging errors made by the 48 participants who took part in our user study [5], but we have not previously reported on what behaviors correlate with errors.",1,ad,True
14,"In this paper, we present results for a relevance assessing scenario where there was not an incentive for users to work faster. In this scenario, where the assessor is not paid per judgment, but is paid for the time spent, we have found an interesting result: users take more time to make mistakes than they do to make correct relevance judgments.",0,,False
15,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",0,,False
16,2. MATERIALS AND METHODS,0,,False
17,"The user study presented participants with fixed ranked lists of documents. Given a ranked list, participants were instructed to search for and save relevant documents. The user interface was web-based and consisted of two web pages. The first page showed 10 document summaries in a format similar to a web search engine's results page. Clicking on a summary took the participant to a page that displayed the full document and allowed the participant to save the document as relevant. We did not require relevance judgments to be made. We treat viewing a full a document and not saving it as a decision to judge the document as non-relevant. Participants could view the next 10 results by clicking on a link at the bottom of the document summaries page.",1,ad,True
18,The study used 8 topics from the 2005 TREC Robust track. The documents came from the AQUAINT newswire collection. Each participant searched ranked lists with a uniform precision of 0.6 or 0.3. We use the NIST relevance judgments as truth.,1,TREC,True
19,Participants worked on 4 search topics for 10 minutes each. The study used a fully balanced design. Participants were instructed to work as fast as possible while making as few mistakes as possible. We use data from the second phase of the study. All participants had already completed phase 1 of the study and gained experience with relevance judging before participating in the second phase. The result lists contained near-duplicate documents. We use only the first judgment on a set of near-duplicates.,1,ad,True
20,"We measured the time the participants spent on the full document page before saving the document as relevant or leaving the page. It is well-known that large variations in behavior are caused by the search topic and the user. To allow comparisons of times across users and topics, we standardized a participant's times on a topic to have a mean of zero and a standard deviation of 1.",0,,False
21,"We average a participant's times on a topic, and then average across topics for that participant to produce a participant average. We finally average across participants to produce average participant times.",0,,False
22,3. RESULTS AND DISCUSSION,0,,False
23,"When a participant judges a non-relevant document to be relevant, a false positive error is made. Likewise, when a relevant document is judged to be non-relevant, a false negative error is made. Table 1 and Figure 1 show the main results. These results are based on 3614 judgments (2706 correct judgments, 511 false negative errors, 397 false positive errors).",1,ad,True
24,1153,0,,False
25,Average Participant Time to Judge Doc. Relevance Correct False Positive p-value False Negative p-value,0,,False
26,Standardized Times,0,,False
27,-0.05 ± 0.01 0.26 ± 0.07 < 0.001 0.13 ± 0.08,0,,False
28,0.02,0,,False
29,Raw Times in Seconds,0,,False
30,28 ± 2,0,,False
31,39 ± 4,0,,False
32,0.03,0,,False
33,33 ± 3,0,,False
34,0.16,0,,False
35,"Table 1: Average participant times to judge document relevance for both standardized times and raw times in seconds. Shown with the average is the standard error of the mean. We used a two-sided, Student's t-test to measure the statistical significance of the difference between the time to make correct judgments and each of the error conditions: false positive and false negative errors. For the standardized times, both types of errors took a statistically significant longer time (p < 0.05).",0,,False
36,Average Participant Standardized Time to Judge -1.0 -0.5 0.0 0.5 1.0 1.5,0,,False
37,False Positive,0,,False
38,Correct,0,,False
39,False Negative,0,,False
40,"Figure 1: Average participant standardized times to judge document relevance for false positive, correct, and false negative judgments.",0,,False
41,"We found that on average, participants took significantly longer to judge documents when they made an error compared to when they were correct in their judgment. A priori, one might guess that participants would make errors by not taking enough time, but in contrast, we find here that time appears to indicate difficulty in making a correct judgment.",1,ad,True
42,"We hypothesize that the study participants make judgment errors according to the following process. First the participants identify a topically relevant document based on the document's summary and then click on the summary to view the full document. Participants then study the document to determine if it is relevant or not to the specifics of the search topic. The more difficult it is to determine the document's relevance, the longer the participant takes. At some point, the participant decides to save the document as relevant or abandon the document without saving it. False negatives result from not finding the relevant material in the document, and false positives are the result of the final decision being a guess.",0,,False
43,"There are many other factors that can cause a user to take longer to judge a document. For example, we have shown that the longer a document, the longer it takes to judge its relevance [3]. Is it just that mistakes are being made on longer documents? To investigate this, we modeled the probability that a judgment was an error using logistic regression. For the model, we considered time, document length measured in words, and the rank of the document in the results list. We found that when both time and document length were used together in the model, document length was not a useful predictor of errors. On the other hand, when we included the interaction between time and document length, this interaction was helpful. In other",1,ad,True
44,"words, if the time to judge is too long given the length of the document, this is an indicator of error. We also found that ranks above about 50 were predictive of making errors. Reaching a rank over 50 in 10 minutes is likely indicative of a participant who is not carefully judging documents. For ranks less than 50, rank was not predictive of errors.",0,,False
45,"The relevance assessing setup may have a significant effect on assessor behavior. In preliminary analyses of separate studies where assessors had an incentive to work faster [4] and relevant documents were highly prevalent (90% relevant) [2], we have seen false negatives be long errors and false positives be short errors. In this paper, both types of errors were long errors. In future work we intend to analyze these studies in more detail by using this paper's method of standardizing times.",1,ad,True
46,4. CONCLUSION,0,,False
47,"We found in a relevance assessing scenario where the assessor is trusted and paid by time spent, and not by number of judgments made, that all things equal, the longer an assessor takes to make a relevance judging decision, the more likely a mistake will be made. We hypothesize that time to judge a document relative to other documents, gives an indication of the difficulty of judging the document.",1,ad,True
48,5. ACKNOWLEDGMENTS,0,,False
49,"David Hu wrote the software to compute the sets of nearduplicate documents. This work was supported in part by NSERC, in part by Amazon, and in part by the University of Waterloo.",0,,False
50,6. REFERENCES,0,,False
51,"[1] B. Carterette and I. Soboroff. The effect of assessor error on IR system evaluation. In SIGIR, pp. 539­546, 2010.",0,,False
52,"[2] C. Jethani. Effect of prevalence on relevance assessing behavior. Master's thesis, University of Waterloo, 2011.",0,,False
53,"[3] C. Jethani and M. D. Smucker. Modeling the time to judge document relevance. In Proceedings of the SIGIR'10 Workshop on the Simulation of Interaction, 2010.",0,,False
54,"[4] M. D. Smucker and C. Jethani. The crowd vs. the lab: A comparison of crowd-sourced and university laboratory participant behavior. In Proceedings of the SIGIR 2011 Workshop on Crowdsourcing for Information Retrieval, 2011.",0,,False
55,"[5] M. D. Smucker and C. Jethani. Measuring assessor accuracy: a comparison of NIST assessors and user study participants. In SIGIR, pp. 1231­1232, 2011.",0,,False
56,"[6] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. IPM, 36:697­716, September 2000.",0,,False
57,"[7] W. Webber, D. W. Oard, F. Scholer, and B. Hedin. Assessor error in stratified evaluation. In CIKM, pp. 539­548, 2010.",0,,False
58,1154,0,,False
59,,0,,False

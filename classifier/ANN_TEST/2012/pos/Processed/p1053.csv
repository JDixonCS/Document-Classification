,sentence,label,data,regex
0,Effect of Written Instructions on Assessor Agreement,0,,False
1,William Webber,0,,False
2,College of Information Studies University of Maryland,0,,False
3,United States of America wew@umd.edu,0,,False
4,Bryan Toth and Marjorie Desamito ,0,,False
5,"Eleanor Roosevelt High School Greenbelt, Maryland",0,,False
6,United States of America bryan.n.toth@gmail.com magicaura2000@yahoo.com,0,,False
7,ABSTRACT,0,,False
8,"Assessors frequently disagree on the topical relevance of documents. How much of this disagreement is due to ambiguity in assessment instructions? We have two assessors assess TREC Legal Track documents for relevance, some to a general topic description, others to detailed assessment guidelines. We find that detailed guidelines lead to no significant increase in agreement amongst assessors or between assessors and the official qrels.",1,TREC,True
9,Categories and Subject Descriptors,0,,False
10,H.3.4 [Information Storage and Retrieval]: Systems and software--performance evaluation.,0,,False
11,Keywords,0,,False
12,"Retrieval experiment, evaluation, e-discovery",0,,False
13,General Terms,0,,False
14,"Measurement, performance, experimentation",0,,False
15,1. INTRODUCTION,1,DUC,True
16,"Assessors frequently disagree on the relevance of a document to a topic. Voorhees [2000] finds that TREC adhoc assessors have mutual F1 scores of around 0.6, while Roitblat et al. [2010] report mutual F1 as low as 0.35 for professional e-discovery reviewers. Such low agreement is of serious practical concern in e-discovery, where large-scale, delegated manual review is still widely used. Possible causes of disagreement include assessor error and ambiguity in instructions. We examine whether detailed relevance guidelines increase agreement amongst assessors and with the guideline author, and find no significant increase in either form of agreement",1,TREC,True
17,2. METHODS AND MATERIALS,0,,False
18,"We measure inter-assessor agreement by Cohen's , for which 1 is perfect and 0 is random agreement [Cohen, 1960]. Unassessable documents (too long or misrendered) are ignored. A trial experiment of 75 documents per treatment, on Topic 301 from the TREC 2010 Legal Track, indicated that a sample size of 215 documents per treatment, with even proportions relevant and irrelevant, was required to achieve 80% power for a true  delta of 0.23, being the difference in agreement with official assessments between first and second tercile assessors in the TREC 2009 Legal Track. Work performed while interns at the University of Maryland.",1,TREC,True
19,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",0,,False
20,Message type,0,,False
21,Messages,0,,False
22,> 1 relevant document,0,,False
23,5,0,,False
24,Relevant appealed,0,,False
25,170,0,,False
26," unappealed

38

Irrelevant appealed

58

 unappealed returned",0,,False
27,32,0,,False
28,  unreturned,0,,False
29,16,0,,False
30,Total,0,,False
31,319,0,,False
32,Documents,0,,False
33,rel irrel unass,0,,False
34,13 12,0,,False
35,0,0,,False
36,170 82,0,,False
37,4,0,,False
38,38 7,0,,False
39,0,0,,False
40,0 78,0,,False
41,1,0,,False
42,0 44,0,,False
43,1,0,,False
44,0 17,0,,False
45,1,0,,False
46,221 240,0,,False
47,7,0,,False
48,"Table 1: Number and types of messages and documents sampled from Topic 204 for re-assessment. A message is classed as ""relevant"" if it contains a single relevant document (body or attachment). Counts of relevant, irrelevant, and unassessable documents are using the official, post-appeal assessments.",0,,False
49,"Topic 204 from the interactive task of the TREC 2009 Legal Track [Hedin et al., 2009] was used for the full experiment. The corpus is the EDRM Enron emails. Whole messages were sampled, but each email body and attachment was separately assessed. A stratified sample was taken, as described in Table 1. The strata were divided evenly and randomly into two batches. Each batch was assessed in document id order, with the parts of a message being assessed sequentially, as in TREC.",1,TREC,True
50,"At TREC, a senior lawyer called the topic authority develops the topic, writes the detailed guidelines, and adjudicates appeals against first-round assessments. The appeals process for this topic was thorough [Webber, 2011], and the majority of sampled documents were appealed; we regard the assessments as an accurate representation of the topic authority's conception of relevance. We measure agreement for each batch between the two experimental assessors and the official, post-appeal assessments.",1,TREC,True
51,"The latter two authors of this paper acted as experimental assessors. Each assessor assessed all documents in each batch. For the first batch, assessors were given the 42-word topic statement to guide their assessments; for the second, they received the 5-page detailed relevance guidelines. A third pass was then made, in which the two assessors jointly reviewed both batches, in light of the detailed guidelines, and tried to agree on a conception of relevance.",1,ad,True
52,3. EXPERIMENTAL RESULTS,0,,False
53,"Table 2 shows the results of our experiments. The provision of detailed assessment guidelines (Batch 2) did not improve agreement, significantly or otherwise, over topic-only instructions (Batch 1), either amongst assessors or with the official assessments, in either the full or the trial experiment. Message-level analysis (in",0,,False
54,1053,0,,False
55,Batch,0,,False
56,1 2 Jnt-1 Jnt-2,0,,False
57,Full experiment,0,,False
58,AvB AvO BvO,0,,False
59,0.519 0.528 0 .992,0,,False
60,0 .950,0,,False
61,0.454ab,0,,False
62,0.555 0.677a 0.665b,0,,False
63,0.710 0.637 0.686 0.674,0,,False
64,Trial experiment,0,,False
65,AvB AvO BvO,0,,False
66,0.229 0.275,0,,False
67,- -,0,,False
68,0.557 0.439,0,,False
69,- -,0,,False
70,0.417 0.294,0,,False
71,- -,0,,False
72,"Table 2: Cohen's  values between official and two experimental assessors, for full and trial experiments, on single-assessed Batch 1 (with topic statement only), single-assessed Batch 2 (with detailed guidelines), and (for full experiment only) jointassessed Batches 1 and 2 (with topic guidelines and consultation between assessors). Columnar value pairs significantly different at  , 0.05 (excepting inter-experimenter joint review) are marked by superscripts.",0,,False
73,Assessors,0,,False
74,A v. B A v. Official B v. Official,0,,False
75,Confidence interval,0,,False
76,"[-0.155, 0.173] [-0.061, 0.263] [-0.211, 0.065]",0,,False
77,"Table 3: Two-tailed 95% normal-approximation confidence intervals on the true change in  between Batch 1 and Batch 2 amongst different assessor pairs, for the full experiment.",0,,False
78,"which a message is relevant if any part of it is relevant) gives similar results. Inter-assessor  values are high for the full experiment's joint assessment, since assessors reached agreement on all save a handful of documents (1 for Batch 1, and 5 for Batch 2). Assessor A's agreement with the official assessments increases significantly under joint review, but this may be due to Assessor A's assessments moving closer to Assessor B's; Assessor A's self-agreement on Batch 1 is 0.399 post-consultation, whereas Assessor B's is 0.739.",0,,False
79,"Table 3 gives 95% confidence intervals on the true change in  values with the addition of assessor guidelines. A substantial improvement is still plausible in agreement between Assessor A and the official assessments, but not for Assessor B and official, nor for inter-assessor agreement.",1,ad,True
80,"Agreement between the original TREC assessors and the authoritative assessment on the documents examined in our experiment is 0.102 for Batch 1 and 0.024 for Batch 2, much lower than for our experimental assessors; however, this is a biased comparison, since sampling was heavily weighted towards appealed documents. Over the 7,289 documents sampled for assessment at TREC, though, the original assessors achieved a  of 0.320, still well below that of the experimental assessors. The relatively high reliabilty of the assessor is reflected in their high mutual F1 scores (Table 4).",1,TREC,True
81,"Qualitatively, the experimental assessors described the full experiment topic description by itself as being clear, and the detailed guidelines as being very clear and easy to relate to the documents. As can be seen in Table 2, agreement for this topic was generally higher than for the trial experiment.",0,,False
82,4. DISCUSSION,0,,False
83,"Our initial, seemingly common-sense, hypothesis was that more detailed instructions would raise agreement between assessors and the authoritative conception of relevance, and therefore amongst assessors themselves. The results of this experiment have failed to confirm this hypothesis, or even to show a general trend in this direction. The only significant improvement occurred when Asses-",0,,False
84,Batch,0,,False
85,1 2,0,,False
86,AvB,0,,False
87,0.679 0.769,0,,False
88,AvO,0,,False
89,0.648 0.791,0,,False
90,BvO,0,,False
91,0.828 0.823,0,,False
92,Table 4: Assessor mutual F1 scores for the full experiment.,0,,False
93,"sor A consulted with Assessor B, but that may be attributable to the former's assessments moving closer to the latter's. Indeed, confidence intervals indicate that a substantial increase in agreement is not plausible, except possibly between one assessor and the official view. We can conclude that, for this topic and these assessors, the provision of more detailed assessment guidelines did not lead to any marked increase in assessor reliability.",1,ad,True
94,"It is also notable that our experimental assessors, who were high school students with no legal training, appear to have produced assessments much more in line with the authoritative conception of relevance than the original TREC assessors, who were legally trained, professional document reviewers.",1,TREC,True
95,"Our findings are not reassuring for the widespread practice of using delegated manual review in e-discovery. If assessors do no better with detailed guidelines than with a general outline of the topic, then there is an irreducible loss of signal in transmitting the relevance conception of an authoritative reviewer into the minds of other assessors. E-discovery practice is moving towards the use of statistical classification tools [Grossman and Cormack, 2011]; it may well be that the lawyer overseeing a case is better able to convey their conception of relevance by personally training a machine classifier, than by instructing delegated human reviewers.",1,ad,True
96,Acknowledgments.,0,,False
97,"Venkat Rangan of Symantec eDiscovery provided the TIFF images of Enron documents used in the TREC 2009 Legal Track assessments. Maura Grossman and Gord Cormack advised on the choice of TREC topics. This material is based upon work supported by the National Science Foundation under Grant No. 1065250. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",1,TREC,True
98,References,0,,False
99,"Jacob Cohen. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37­46, 1960.",0,,False
100,"Maura R. Grossman and Gordon V. Cormack. Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Richmond Journal of Law and Technology, 17(3):11:1­ 48, 2011.",0,,False
101,"Bruce Hedin, Stephen Tomlinson, Jason R. Baron, and Douglas W. Oard. Overview of the TREC 2009 legal track. In Ellen Voorhees and Lori P. Buckland, editors, Proc. 18th Text REtrieval Conference, pages 1:4:1­ 40, Gaithersburg, Maryland, USA, November 2009. NIST Special Publication 500-278.",1,TREC,True
102,"Herbert L. Roitblat, Anne Kershaw, and Patrick Oot. Document categorization in legal electronic discovery: computer classification vs. manual review. Journal of the American Society for Information Science and Technology, 61(1):70­80, 2010.",0,,False
103,"Ellen Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5): 697­716, September 2000.",0,,False
104,"William Webber. Re-examining the effectiveness of manual review. In Proc. SIGIR Information Retrieval for E-Discovery Workshop, pages 2:1­8, Beijing, China, July 2011.",0,,False
105,1054,0,,False
106,,0,,False

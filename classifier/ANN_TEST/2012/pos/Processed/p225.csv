,sentence,label,data,regex
0,Efficient In-Memory Top-k Document Retrieval,0,,False
1,J. Shane Culpepper,0,,False
2,"School of CS & IT RMIT University Melbourne, VIC, 3001,",0,,False
3,Australia shane.culpepper@rmit.edu.au,0,,False
4,Matthias Petri,0,,False
5,"School of CS & IT RMIT University Melbourne, VIC, 3001,",0,,False
6,Australia matthias.petri@rmit.edu.au,0,,False
7,Falk Scholer,0,,False
8,"School of CS & IT RMIT University Melbourne, VIC, 3001,",0,,False
9,Australia falk.scholer@rmit.edu.au,0,,False
10,ABSTRACT,0,,False
11,"For over forty years the dominant data structure for ranked document retrieval has been the inverted index. Inverted indexes are effective for a variety of document retrieval tasks, and particularly efficient for large data collection scenarios that require disk access and storage. However, many efficiency-bound search tasks can now easily be supported entirely in-memory as a result of recent hardware advances.",1,ad,True
12,"In this paper we present a hybrid algorithmic framework for inmemory bag-of-words ranked document retrieval using a self-index derived from the FM-Index, wavelet tree, and the compressed suffix tree data structures, and evaluate the various algorithmic trade-offs for performing efficient queries entirely in-memory. We compare our approach with two classic approaches to bag-of-words queries using inverted indexes, term-at-a-time (TAAT) and document-at-atime (DAAT) query processing. We show that our framework is competitive with state-of-the-art indexing structures, and describe new capabilities provided by our algorithms that can be leveraged by future systems to improve effectiveness and efficiency for a variety of fundamental search operations.",1,ad,True
13,Categories and Subject Descriptors,0,,False
14,"H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing--indexing methods; H.3.2 [Information Storage and Retrieval]: Information Storage--file organization; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-- query formulation, retrieval models, search process; I.7.3 [Document and Text Processing]: Text Processing--index generation",0,,False
15,Keywords,0,,False
16,"Text Indexing, Text Compression, Data Storage Representations, Experimentation, Measurement, Performance",0,,False
17,1. INTRODUCTION,1,DUC,True
18,"Top-k retrieval algorithms are important for a variety of real world applications, including web search, on-line advertising, relational databases, and data mining. Efficiently ranking answers to",1,ad,True
19,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.",1,ad,True
20,"queries in large data collections continues to challenge researchers as the collection sizes grow, and the ranking metrics become more intricate. Despite recent hardware advances, inverted indexes remain the tool of choice for processing efficiency-bound search tasks [17]. However, large memory systems also provide new opportunities to explore another class of indexing algorithms derived from the suffix array to potentially improve the efficiency of various inmemory ranked document retrieval tasks [25, 27].",1,ad,True
21,"In this paper we present a hybrid algorithmic framework for inmemory bag-of-words ranked document retrieval using a self-index derived from the FM-Index, wavelet tree, and the compressed suffix tree data structures [12, 20, 27, 22], and evaluate the various algorithmic trade-offs for performing efficient in-memory ranked querying. We compare our approach with two classic approaches to bag-of-words queries using inverted indexes, term-at-a-time (TAAT) and document-at-a-time (DAAT) query processing. We show that our framework is competitive with state-of-the-art indexing structures, and describe new capabilities provided by our algorithms that can be leveraged by future systems to improve efficiency and effectiveness for various document retrieval tasks.",1,ad,True
22,"Our contributions. Firstly, we propose a hybrid approach to solv-",0,,False
23,"ing a subset of important top-k document retrieval problems ­ bagof-words queries. Secondly, we present a comprehensive efficiency analysis comparing in-memory inverted indexes with top-k selfindexing algorithms for bag-of-words queries on text collections an order of magnitude larger than any other prior experimental study. To our knowledge, this is the first comparison of this new algorithmic framework for realistic sized text collections using a standard similarity metric ­ BM25. Finally, we describe how our algorithmic framework can be extended to efficiently and effectively support other fundamental document retrieval tasks.",0,,False
24,2. PROBLEM OVERVIEW,0,,False
25,"In this paper, we investigate the use of self-indexing algorithms to solve the top-k document search problem. A document collection T is a contiguous string drawn from an alphabet , where  ,"" || is the number of distinct """"terms"""" or strings. In practice,  can be characters (UTF8 or ASCII), bytes, integers, or even phrases. Each document in T is separated by a unique end of document symbol defined to be lexicographically smaller than any s  .""",0,,False
26,"DEFINITION 1. A top-k document search takes a query q  , an integer 0 < k  d, and a text T   partitioned into d documents {D1, D2, . . . , Dd}, and returns the top-k documents ordered by a similarity measure S(q, Di).",0,,False
27,"In this work, we focus primarily on bag-of-words queries, so our baseline S(q, Di) ranking function is BM25. Our S(q, Di) ranking function has the following formulation:",0,,False
28,225,0,,False
29,"BM25 , log",0,,False
30,tq,0,,False
31,N - ft + 0.5 ft + 0.5,0,,False
32,· TFBM25,0,,False
33,TFBM25,0,,False
34,",",0,,False
35,"fd,t",0,,False
36,+ k1,0,,False
37,"fd,t · (k1 + 1) · ((1 - b) + (b · d/avg))",0,,False
38,"Here, N is the number of documents in the collection, ft is the number of distinct document appearances of t, fd,t is the number of occurrences of term t in document d, k1 ,"" 1.2, b "","" 0.75, d is the number of symbols in the dth document, and avg is the average of d over the whole collection. The free parameters k1 and b can be tuned for specific collections to improve effectiveness, but we use the standard Okapi parameters suggested by Robertson et al. [36].""",0,,False
39,3. ALGORITHMS,0,,False
40,"We now present an overview of the key data structures and algorithms used in our framework. Here, we only outline the key properties and features of wavelet trees and suffix arrays used in our search engine; for a more in-depth tutorial, see, for example Navarro and Mäkinen [30] and the references therein. Efficient operations in succinct data structures depend on two fundamental operations over a bitvector B[0, n - 1]:",0,,False
41,"RANK0/1(B, i): Return the number of 0's/1's in B[0, i]. SELECT0/1(B, i): Return the position of the ith of 0's/1's in B.",0,,False
42,Both operations can be performed in constant time. A simple constant time RANK0/1 solution uses o(n) space in addition to storing B [23]. More space efficient RANK0/1 algorithms are possible [35].,1,ad,True
43,3.1 Wavelet Trees,0,,False
44,"Efficient RANKs and SELECTs over an alphabet of size  > 2 can be performed using a wavelet tree [20]. A wavelet tree decomposes the RANKs and SELECTs operations over [0,  - 1] into RANK0/1 and SELECT0/1 operations on a binary alphabet using a binary tree. The root of the tree represents the whole alpha-",0,,False
45,bet. Its children represent each half of the alphabet of the par-,0,,False
46,"ent node. Each leaf node in the tree represents one symbol in [0,  - 1]. When answering the RANKs query for a specific symbol s, we perform RANK0/1 operations at each level in the tree until we arrive at the leaf node representing s. The overall RANKs(T , i) can be computed by combining the RANK0/1 results at each tree level in O(log ) time. Any symbol s ,"" T [i] is also computed in time O(log ) with a similar algorithm; we call this operation ACCESS(T , i). Using a succinct representation of RANK0/1 and SELECT0/1 [35], a wavelet tree requires nH0 + o(n log ) bits of space, where H0  log  is the zero-order entropy of T .1""",0,,False
47,"Wavelet trees are a surprisingly versatile data structure, and have",0,,False
48,"attractive time and space bounds for many primitive operations in self-indexing algorithms [14]. As a result, many top-k document retrieval approaches rely heavily on wavelet trees. A subset of im-",0,,False
49,portant wavelet tree operations include:,0,,False
50,"RANKs(T , s, sp, ep): Return the number of occurrences",0,,False
51,"of symbol s in a range T [sp, ep].",0,,False
52,"SELECTs(T , s, j, sp, ep): Return the position of the jth oc-",0,,False
53,currence of symbol s in a range,0,,False
54,"T [sp, ep].",0,,False
55,"ACCESS(T , i):",0,,False
56,Return symbol T [i].,0,,False
57,1We assume logarithms are in base 2.,0,,False
58,"RMQ(T , sp, ep): RQQ(T , k, sp, ep):",1,MQ,True
59,"Return the smallest symbol s in a range T [sp, ep]. Return the kth smallest symbol s in a range T [sp, ep].",0,,False
60,3.2 Self-indexing,0,,False
61,"A suffix array SA[0, n-1] over T stores the offsets to all suffixes in T in lexicographical order. Any pattern P of length m occurring in T is a prefix of one or more suffixes in SA. These suffixes, due",0,,False
62,"to the lexicographical order within SA, are grouped together in a",0,,False
63,"range SA[sp, ep]. To determine SA[sp, ep], we perform two binary searches over SA and T . Each binary search comparison requires up to m symbol comparisons in T , for a total of O(m log n) time.",0,,False
64,Using additional auxiliary data structures this cost can be reduced to O(m + log n) [25]. Suffix array construction is a well studied,1,ad,True
65,"problem, and many solutions with various time and space trade-offs exist in the literature [34]. However, searching for a pattern P in T using only a suffix array requires O(n log n) bits to store both T and SA, which in practice is at least 5 times the text size.",1,ad,True
66,By replacing T with the Burrows-Wheeler Transform (BWT),1,WT,True
67,"permuted text, the key operations of a basic SA can be emulated with much less space, close to the size of T in compressed form.",0,,False
68,The Burrows-Wheeler Transform [7] ­ also known as the block-,0,,False
69,"sorting transform ­ produces a permutation of a string T , denoted T BWT , by sorting the n cyclic rotations of T into full lexicographical order, and taking the last column of the resulting n × n matrix. The resulting string T BWT tends to be more compressible as symbols are grouped together based on their context in T , which makes",1,WT,True
70,"the BWT an important part in many state of the art compression systems [26]. To produce T BWT for a given text T , it is not necessary to construct M as there is a duality between T BWT and the SA over a text T : T BWT[i] , T [SA[i] - 1 mod n].",1,WT,True
71,"The original text T can be recovered from T BWT in linear time without the need for any additional information. To recover T from only T BWT we first recover the first column, F , in M by sorting the last column (L ,"" T BWT), in lexicographical order. By mapping the symbols in L to their respective positions in F so L[i] "","" F [j] (usually referred to as the LF mapping, j "", LF(i)) we can recover T backwards as T [n - 1] , T BWT[0] , $ and T [j - 1] , T BWT [LF(i)] if and only if T [j] , T BWT[i]. Since F is simply a sort of the n characters of the string in lexicographical",1,WT,True
72,"order, it can be represented succinctly as a lookup table of alphabet",0,,False
73,characters along with the count of all symbols that appear before,0,,False
74,the current character c. The LF mapping is computed using the,0,,False
75,equation,0,,False
76,"LF(i) ,"" LF(i, c) "","" C[c] + RANKs(T BWT , i)""",1,WT,True
77,(1),0,,False
78,"where c is the symbol T BWT [i], and C[c] stores the number of symbols in T BWT smaller than c.",1,WT,True
79,Performing a search in T using the BWT permuted text is straight-,1,WT,True
80,forward. Recall that all rows are sorted in lexicographical order in,0,,False
81,"M. Therefore, for a pattern P, all occurrences of P in T must have a corresponding row in M within a range sp, ep . To determine the range within M, we first determine the range spm, epm within M that corresponds to Pm using C[ ]. Then, for each sym-",0,,False
82,"bol j ,"" m-1 . . . 0 in P, we iteratively find spj , epj by calculat-""",0,,False
83,"ing the number of rows within spj+1, epj+1 that are preceded by the symbol Pj in T . For a given row j, the LF mapping can be used to determine the row in M representing the symbol preceding j in T . The preceding row is determined by counting the number of occurrences of c , T BWT[j] before the current row and ranking these",1,WT,True
84,"occurrences within C[s]. Assume we have located spj+1, epj+1 ,",0,,False
85,226,0,,False
86,"which corresponds to the rows prefixed by P[j + 1, m]. Then",0,,False
87,"spj ,"" LF(spj+1 - 1, pj)""",0,,False
88,(2),0,,False
89,will calculate the position in F of the first occurrence of Pj within,0,,False
90,"spj+1, epj+1 , and thus compute the start of our range of rows within M that correspond to P[j, m]. Similarly, we compute",0,,False
91,"epj ,"" LF(epj+1, pj) - 1.""",0,,False
92,(3),0,,False
93,"Once the area sp, ep is determined, self-indexes offer a way to find any occurrence position SA[j], for sp  j  ep. This is accomplished by sampling T at regular intervals, and marking",0,,False
94,"positions of SA that point to sampled text positions in a bitmap E[0, n - 1]. Sampled suffix array positions are stored in an array G[RANK1(E, j)] , SA[j] if E[j] ,"" 1. Given a target value SA[j], the successive values i "","" 0, 1, . . . are evaluated until E[LFi(j)] "","" 1, producing the desired answer of SA[j] "","" SA[LFi(j)]+i. If every th text position is sampled, we guarantee i can be found for every 0  i < , and sampling requires O((n/) log n) extra bits for G (and for E in compressed form [35]), and computes any entry of SA within  applications of LF.""",0,,False
95,"Similarly, in order to recover any text substring T [l, r - 1] (including the whole T ), we can use the same sampling of text position multiples of , and store H[i] ,"" SA-1[i · ]. Thus, we extend the range to T [l, r - 1], for r "",  · r/ and display from the suffix array position j ,"" SA-1[r]. Then, we can display the area backwards as T BWT[j], T BWT[LF(j)], T BWT[LF2(j)], . . .. Each""",1,WT,True
96,"step requires one RANKs and one ACCESS operation, which has the same cost as LF. Therefore, we can display T [l, r - 1] within O(r - l + ) probes of LF.",0,,False
97,In practice self-indexes can be reduced to a wavelet tree over T BWT with auxiliary information to emulate F (the C array) and,1,WT,True
98,"the sampling information. This representation of a self-index is referred to as an FM-Index [12, 13]. A wavelet tree built over T BWT uses nHk(T ) + o(n log ) bits [24] for any k   log(n) - 1 and constant  < 1, so the space requirements are reasonable. Here Hk(T )  Hk-1(T )  . . .  H0(T )  log  is the k-th order entropy of T [26], a measure of the performance of any compressor using k-th order statistical modeling on T . Many",1,WT,True
99,other self-indexing variations exist with different time / space trade-,1,ad,True
100,"offs [30, 15]. In principle, any of these approaches are compatible",0,,False
101,"with the framework we present here, as long as the method returns",0,,False
102,"a sp, ep range of matching suffixes.",0,,False
103,4. TOP-K DOCUMENT RETRIEVAL USING,0,,False
104,SELF-INDEXES,0,,False
105,"In order to efficiently solve the top-k document search problem, unadorned self-indexing algorithms are not sufficient. Two approaches to enhance the self-index have been proposed. The first is to use a document array, that is, a mapping between every suffix in T to its corresponding document identifier [10, 18, 27, 41]. The second is to store, in addition to the global self-index, one selfindex of each individual document in the collection [22, 37]. These alternatives offer different theoretical frameworks that are not directly comparable, but experimental studies [10, 31] have consistently shown that the first approach offers better space and time performance in practice.",1,ad,True
106,"Representing the document array with a single wavelet tree can provide additional important advantages. For example, the list of distinct documents where a substring P appears, with the corresponding term frequencies, can be obtained without any additional structure [18], in O(log d) time per document retrieved, once the self-index has given the suffix array range of P. This information",1,ad,True
107,top-k,0,,False
108,prestored,0,,False
109,top-k,0,,False
110,top-k,0,,False
111,top-k,0,,False
112,g,0,,False
113,g,0,,False
114,D 31421432124342132143,0,,False
115,fringe sp,0,,False
116,fringe ep,0,,False
117,"Figure 1: Precomputed top-k results over fixed intervals g stored in a skeleton succinct suffix tree using the HSV approach. Only the fringe leaves are processed for a given sp, ep range.",0,,False
118,"can then be used to calculate simple TF×IDF based S(q, Di) metrics at query time [37]. In addition, several other operations such as Boolean intersection can be performed efficiently using only the wavelet tree over the document array [19].",1,ad,True
119,"Culpepper et al. [10] showed how to use the same wavelet tree to find the top-k documents (with raw term frequency weights) for a string P. Among all of the strategies proposed, the heuristic algorithm GREEDY worked best in practice. Despite the lack of worstcase theoretical guarantees, they show unadorned wavelet trees are efficient in time and space for this task. Hon et al. [22] presented a technique with worst-case performance guarantees. The HSV approach builds on the same document listing strategy originally proposed by Sadakane [37]. While HSV was originally described using individual self-indexes for each document as in Sadakane's approach [37], the method can be applied on top of either documentlisting solution in practice. The key insight of the HSV method is to precompute the top-k results for the lowest suffix tree nodes in a predetermined sampling range. Figure 1 shows a HSV tree over D. In this example, a sp, ep range of size g is precalculated and stored in a succinct suffix tree. An arbitrary query sp, ep is received. The bulk of the query result is already precomputed as sp, ep . The remainder of the query can then be processed using RQQ queries over the fringe ranges to generate the final top-k counts.",1,ad,True
120,"Using g samples guarantees that any suffix array interval sp, ep for a given P falls into one of three categories: (1) The range is completely covered by the sampled interval, and the top-k answer is precomputed; (2) The range is partially covered, and at most 2g fringe leaves must to be processed at query time and merged with the sample; or (3) The range is too small to be covered. For Case (3), the complete sp, ep range must be processed at runtime, but is guaranteed to be smaller than 2g.",0,,False
121,"Navarro and Valenzuela [31] demonstrated that implementing HSV over a document array, and using the GREEDY approach of Culpepper et al. [10] to speed up document listing, is more efficient than using either GREEDY or HSV in isolation. The hybrid approach requires additional space to support HSV on top of GREEDY, but efficiency is significantly improved by limiting the number of rank queries required at query time. This approach as well as other trade-offs are explored more fully in this paper.",1,ad,True
122,227,0,,False
123,22,0,,False
124,ft Ft,0,,False
125,acacia ft Ft,0,,False
126,"d fd,t",0,,False
127,p*,0,,False
128,"d fd,t",0,,False
129,p*,0,,False
130,"d fd,t",0,,False
131,p*,0,,False
132,"d fd,t",0,,False
133,p*,0,,False
134,avenue ft Ft Term Map,0,,False
135,"d fd,t",0,,False
136,p*,0,,False
137,"d fd,t",0,,False
138,p*,0,,False
139,Posting Lists,0,,False
140,Document Map WTX001-B01-1 307 WTX001-B01-2 14 WTX001-B01-3 2004 WTX001-B01-4 92 WTX001-B01-5 174 WTX001-B01-6 747,1,WT,True
141,Document Cache,0,,False
142,Document Storage,0,,False
143,"Figure 2: The three fundamental components of an inverted index. Each term in the vocabulary is mapped to a posting list of d, fd,t tuples. For each tuple, the position offsets p are also stored to support phrase or term proximity queries.",0,,False
144,5. INVERTED INDEXES,0,,False
145,"Traditional approaches to the top-k document search problem rely on inverted indexes. Inverted indexes have been the dominant data structure for a variety of ranked document retrieval tasks for more than four decades [44]. Despite various attempts to displace inverted indexes from their dominant position for document ranking tasks over the years, no alternative has been able to consistently produce the same level of efficiency, effectiveness, and time / space trade-offs that inverted indexes can provide (see, for instance Zobel et al. [45]).",1,ad,True
146,"Figure 2 shows a typical inverted indexing system. The system contains three key components: (1) Term Map - The vocabulary of terms, along with the number of documents containing one or more occurrence of the term (ft), the number of occurrences of the term in the collection (Ft), and a pointer to the corresponding posting list. (2) Posting Lists - An ordered list of tuples, d, fd,t , containing the document identifier and the frequency of the term in document d. For each tuple, the ordered position offsets, p are also maintained in order to support phrase queries. For indexes that do not require phase queries, p can be omitted. (3) Document Storage - A document map to match d to the document name, and a pointer to the document in a document cache.",0,,False
147,"Ranked document retrieval requires that only the top-k documents are returned, and, as a result, researchers have proposed many heuristic approaches to improve top-k efficiency [1, 4, 5, 6, 32, 38]. These approaches can be classified in two general categories: term-at-a-time (TAAT) and document-at-a-time (DAAT). Each of these approaches have various advantages and disadvantages.",1,ad,True
148,5.1 Term-at-a-Time Processing (TAAT),0,,False
149,"For TAAT processing, a fixed number of accumulators are allocated, and the rank contribution incrementally calculated for each query term in increasing document order. When inverted files are stored on disk, the advantages of this method are clear. The inverted file for each term can be read into memory, and processed sequentially. However, when k is small relative to the total number of matching documents in collection, TAAT can be inefficient, particularly when the number of terms in the query increases, since all of the inverted lists must be processed before knowing the full rank score of each document. In early work, Buckley and Lewit [6] proposed using a heap of size k to allow posting lists to be evaluated in TAAT order. Processing is terminated when the sum of the contributions of the remaining lists cannot displace the minimum score in the heap.",1,ad,True
150,"Moffat and Zobel [29] improved on this pruning approach with two heuristics: STOP and CONTINUE. The STOP strategy is somewhat similar to the method of Buckley and Lewit, but the terms are processed in order of document frequency from least frequent to most frequent. When the threshold of k accumulators is reached, processing stops. In contrast, the CONTINUE method allows the current accumulators to be updated, but new accumulators cannot be added. These accumulator pruning strategies only approximate the true top-k result list.",1,ad,True
151,"If approximate results are acceptable, the TAAT approach can be made even more efficient using impact ordering [1, 2, 33]. The key idea of impact ordering is to precompute the TF for each document a term appears in. Next, quantize the TF values into a variable number of buckets, and sort the buckets (or blocks) for each term in decreasing impact order. Now, the top-k representative can be generated by sequentially processing each of the highest ranking term contribution blocks until a termination threshold is reached. The authors refer to this blockwise processing method as scoreat-a-time processing. Despite not using the full TF contribution for each term, Anh and Moffat [1] demonstrate that the effectiveness of impact ordered indexes is not significantly reduced, but efficiency is dramatically improved.",1,ad,True
152,5.2 Document-at-a-Time Processing (DAAT),0,,False
153,"The alternative approach is to process all of the terms simultaneously, one document at a time [8]. The advantage of this approach is that the final rank score is known as each document is processed, so it is relatively easy to maintain a heap containing exactly k scores. The disadvantage is that all of the term posting lists are cycled through for each iteration of the algorithm requiring nonsequential disk reads for multi-word queries. However, our focus in this paper is in-memory ranked retrieval, so DAAT tends to work very well in practice.",1,ad,True
154,"Pruning strategies to further increase efficiency also exist for DAAT processing. The most widely used pruning strategy for DAAT is MAXSCORE. Turtle and Flood [40] observed that the BM25 TF component can never exceed k1 + 1 ,"" 2.2. So, the total score contribution for any term is at most 2.2 · log(N/Nt). Using this observation, Turtle and Flood present an algorithm that allows posting values below the threshold to be skipped. As the minimum bounding score in the heap slowly increases, more and more postings can be omitted. Enhanced DAAT pruning strategies similar in spirit to MAXSCORE have been shown to further increase efficiency [4, 38].""",0,,False
155,228,0,,False
156,T TATA$ATAT$TTAT$AATT$ BWT T A T T T $ T T T $ A A T A A T A $ A $,1,WT,True
157,C WTt 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0,1,WT,True
158,$0 A4,0,,False
159,10011111010,0,,False
160,000000000,0,,False
161,T 11,0,,False
162,$,0,,False
163,A,0,,False
164,T,0,,False
165,FM-Index,0,,False
166,sp,0,,False
167,ep,0,,False
168,D,0,,False
169,31421432124342132143,0,,False
170,WTd 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1,1,WT,True
171,0101011010,0,,False
172,0110101010,0,,False
173,1,0,,False
174,2,0,,False
175,3,0,,False
176,4,0,,False
177,Document Array Wavelet Tree,0,,False
178,"Figure 3: Given the text collection T ,"" TATA$ATAT$TTAT$AATT$ of four documents, a self-indexing system requires two wavelet trees. The first wavelet tree supports backwards search over the BWT permuted text, and the second supports statistical calculations over the document array. Note that only the items in grey are stored and used for query operations.""",1,WT,True
179,"Turtle and Flood also describe a similar approach to improve the efficiency of TAAT strategies. However, the TAAT variant is more complex than the DAAT approach as it requires an ordered candidate list of k documents to be maintained. The candidate list is used to skip document postings in each term list which could not possibly displace the current top-k documents once the heap contains k items.",0,,False
180,"Fontoura et al. [17] compare several TAAT and DAAT based inmemory inverted indexing strategies. The authors present novel adaptations of MAXSCORE and WAND [4] to significantly improve query efficiency of in-memory inverted indexes. The authors go on to show further efficiency gains in DAAT style processing by splitting query terms into two groups: rare terms and common terms. The exact split is based on a fixed threshold selected at query time. For our baselines, we use WAND for DAAT query processing, and MAXSCORE for TAAT query processing.",1,ad,True
181,6. SELF-INDEXING APPROACHES,1,AP,True
182,"We now describe our general approach to in-memory indexing and retrieval. Figure 3 shows the key components of our retrieval system: an FM-Index and the document array wavelet tree, WTd. In addition, our system requires a Document Map to map document identifiers to human readable document names (or URLs). No document cache is required and the original documents or snippets around each match can be recreated directly from the FM-Index by extracting the required text positions using the suffix array sampling. Only the items in grey are stored and used for characterbased top-k document retrieval. All other components are shown for illustration purposes only.",1,WT,True
183,ALGORITHM GREEDY-TAAT,0,,False
184,INPUT OUTPUT,1,NP,True
185,A sorted list t of q terms. A list of k documents in rank order.,0,,False
186,1: Initialize a max-heap R  {},0,,False
187,2: for i  1 to q do,0,,False
188,"3: Determine sp, ep for term ti 4: Ai  GREEDY(sp, ep, k)",0,,False
189,5: end for,0,,False
190,6: for i  1 to q do,0,,False
191,7: for j  1 to k do,0,,False
192,8:,0,,False
193,if Ai[j]  R then,0,,False
194,9:,0,,False
195,"UPDATE(R, Ai[j], score)",0,,False
196,10:,0,,False
197,else,0,,False
198,11:,0,,False
199,"ADD(R, Ai[j], score)",0,,False
200,12:,0,,False
201,end if,0,,False
202,13: end for 14: end for 15: return R[1 . . . k],0,,False
203,"FUNCTION GREEDY (sp, ep, k)",0,,False
204,"1:   WTd.root 2: A max-heap, sorted by ep - sp, h  PUSH(, [sp, ep])",1,WT,True
205,3: A priority queue PQ  {}.,0,,False
206,4: i  0,0,,False
207,"5: while h ,  and i < k do",0,,False
208,"6: , [sp, ep]  POP(h)",0,,False
209,7: if  is leaf then,0,,False
210,8:,0,,False
211,"PQ  ENQUEUE(.docid, ep - sp + 1)",0,,False
212,9:,0,,False
213,ii+1,0,,False
214,10: else,0,,False
215,11:,0,,False
216,"[s0, e0]  [RANK0(B, sp), RANK0(B, ep)]",0,,False
217,12:,0,,False
218,"[s1, e1]  [RANK1(B, sp), RANK1(B, ep)]",0,,False
219,13:,0,,False
220,"if e0 - s0 > 0 then h  PUSH(.left, [s0, e0])",0,,False
221,14:,0,,False
222,end if,0,,False
223,15:,0,,False
224,"if e1 - s1 > 0 then h  PUSH(.right, [s1, e1])",0,,False
225,16:,0,,False
226,end if,0,,False
227,17: end if,0,,False
228,18: end while,0,,False
229,19: return PQ,0,,False
230,"A simple bag-of-words search using a self-index retrieval system is outlined in Algorithm GREEDY-TAAT. Recall that the sp and ep range for any string can be found using a backwards search in the BWT permuted text using only a wavelet tree over T BWT and C. So, the sp, ep for each query term in Line (3) can be calculated in O(|ti| log ) time using an FM-Index. Now, a wavelet tree over the document array WTd can be used to retrieve exactly k documents in frequency order for each term using GREEDY or QUANTILE [10]. This algorithm is analogous to TAAT processing, and is referred",1,WT,True
231,to as GREEDY-TAAT. Note that Function GREEDY can also be augmented with HSV as described in Section 4 to further increase the efficiency of constructing Ai for each query term.,0,,False
232,"We also present several variations on this general strategy. First, we consider the addition of HSV style precomputations over WTd as described by Navarro and Valenzuela [31]. Instead of storing the top-k most frequent symbols in the skeleton suffix tree, we store",1,ad,True
233,229,0,,False
234,Query length,1,Query,True
235,|q| 1 2 3 4 5 6 7 8,0,,False
236,random sample,0,,False
237,TREC 7 & 8 queries,1,TREC,True
238,Total Matches Average ni,0,,False
239,Queries ('000),0,,False
240,('000),0,,False
241,100,0,,False
242,9.9,0,,False
243,9.9,0,,False
244,100,0,,False
245,24.8,0,,False
246,12.5,0,,False
247,100 104.5,0,,False
248,38.5,0,,False
249,100 238.1,0,,False
250,69.0,0,,False
251,100 351.2,0,,False
252,95.1,0,,False
253,100 408.7,0,,False
254,107.8,0,,False
255,100 463.8,0,,False
256,126.2,0,,False
257,100 489.8,0,,False
258,148.3,0,,False
259,800 234.9,0,,False
260,70.0,0,,False
261,TREC WT10G queries,1,TREC,True
262,Total Matches Average ni,0,,False
263,Queries ('000),0,,False
264,('000),0,,False
265,100,0,,False
266,3.3,0,,False
267,4.7,0,,False
268,100,0,,False
269,68.6,0,,False
270,42.5,0,,False
271,100 292.8,0,,False
272,123.2,0,,False
273,100 601.1,0,,False
274,166.6,0,,False
275,100 866.5,0,,False
276,228.5,0,,False
277,100 1041.9,0,,False
278,280.4,0,,False
279,100 1149.7,0,,False
280,319.6,0,,False
281,100 1171.8,0,,False
282,339.2,0,,False
283,800 621.5,0,,False
284,181.2,0,,False
285,"Table 1: Statistics of the queries used in experiments (sampled based on query length, or sampled from the filtered MSN query log), reporting the number of queries run, the mean number of documents that contained one or more of the query terms, and the mean length of the inverted lists processed.",0,,False
286,"the top-k most important symbols sorted by term impact for each interval g to improve effectiveness. In order to capture k-values commonly used in IR systems (k ,"" 10, 100, 1000), we prestore values of any k that is a power of 2 up to 8192 in term contribution order. Note that we go higher than 1024 since the values of k necessary to ensure good effectiveness can be greater than the desired k.""",0,,False
287,"Observe that in typical bag-of-words query processing over English text, the size of the vocabulary is often small relative to the total size of the collection. As such, we also present a new hybrid approach to top-k bag-of-words retrieval using a Term Map and WTd. If we assume the vocabulary is fixed for each collection, then the sp, ep range for each term can be precalculated and retrieved using a term map, as in the inverted indexing solution. This means that the FM-Index component is no longer necessary when processing bag-of-words queries. We refer to these hybrid approaches as SEM-GREEDY and SEM-HSV. These methods reduce the overall space requirements of our approach, but also limit the full functionality of some auxiliary operations. For example, the text can no longer be reproduced directly from the index, so snippets cannot be generated on the fly, and phrase queries are no longer natively supported. However, for classic bag-of-words queries, our hybrid approach provides an interesting trade-off to consider. Our final variation is to support a term-based self-index. We refer to this approach as FM-TERM.",1,WT,True
288,"It is also possible to support a DAAT query processing strategy in our retrieval system, but this would require efficiently supporting RMQ over the document array. Our approach currently supports a generalization of RMQ ­ RQQ. But, the cost of RQQ is O(log d) per k value extracted, while constant time solutions for RMQ currently exist [16]. However, an RMQ style approach as presented by Fischer and Heun [16] incurs an additional 2n bits of space, and so we do not explore the possibility further in this work.",1,MQ,True
289,"Also note the current top-k bag-of-words approach shown in GREEDY-TAAT is based entirely on the frequency counts of each item. This means that our current implementation only approximates the top-k items. This is a well-known problem in the inverted indexing domain. This limitation holds for any character-based bag-of-words self-indexing system that does frequency counting at query time since we can not guarantee that item k + 1 in any of the term lists does not have a higher score contribution than any item currently in the top-k intermediate list. A method of term contribution precalculation is required in order to support BM25 or language-model ranking. Without the term contribution scoring,",0,,False
290,"WAND and MAXSCORE enhancements are not possible, and therefore every document in the sp, ep must be evaluated in order to guarantee the final top-k ordering. However, this limitation can be mitigated by using HSV since we can precalculate the impact contribution for each sample position and store this value instead of storing only the frequency ordering. Top-k guarantees are also possible using a term-based self-indexing system where each distinct term is mapped to an integer using HSV or other succinct representations of term contribution preprocessing. In future work, we intend to fully examine all of the possibilities for top-k guarantees using self-indexes in various bag-of-words querying scenarios.",1,ad,True
291,"When using character-based self-indexing approaches for bagof-words queries, there is another disadvantage worth noting. For self-indexes, there is an efficiency trade-off between locating the top-k fd,t values and accurately determining ft since the index can extract exactly k fd,t values without processing every document. For a fixed vocabulary, ft is easily precomputed, and can be stored in the term map with the sp, ep pairs. But, in general it is not straightforward to determine ft for arbitrary strings over WTd without auxiliary algorithms and data structures to support calculating the value on the fly. The FM-HSV approach allows us to prestore ft for each sampled interval which can be be used to calculate ft over sp, ep more efficiently by only processing potential fringe leaves. Calculating ft using only WTd for arbitrary strings in near constant time using no additional space remains an open problem.",1,ad,True
292,7. EXPERIMENTS,0,,False
293,"In order to test the efficiency of our approach, two experimental collections were used. For a small collection, we used the TREC 7 and 8 ad hoc datasets. This collection is composed of 1.86 GB of newswire data from the Financial Times, Federal Register, LA Times, and Foreign Broadcast Information Service, and consists of around 528,000 total documents [42]. For a larger in-memory collection, we used the TREC WT10G collection. This collection consists of 10.2 GB of markup text crawled from the internet, totalling 1,692,096 documents [21].",1,TREC,True
294,"All of the algorithms described in this paper were implemented using C/C++ and compiled with gcc 4.6.1 with -O3 optimizations. For our baselines, we have implemented the in-memory variant of WAND as described by Fontoura et al. [17] for DAAT, and an inmemory variant of MAXSCORE for TAAT. Experiments were run on a single system with 2× Intel Xeon E5640 Processors with a 12 MB smart cache, 144 GB of DDR3 DRAM, and running Ubuntu Linux 11.10. Times are reported in milliseconds unless otherwise",0,,False
295,230,0,,False
296,800,0,,False
297,600,0,,False
298,400,0,,False
299,msec,0,,False
300,200,0,,False
301,0,0,,False
302,FM,0,,False
303,SE,0,,False
304,FM-H,0,,False
305,SE-H,0,,False
306,AD AT TAAT,0,,False
307,"k,""10,k'"",80",0,,False
308,FM,0,,False
309,SE,0,,False
310,FM-H,0,,False
311,SE-H,0,,False
312,AD AT TAAT,0,,False
313,"k,""100,k'"",800",0,,False
314,FM,0,,False
315,SE,0,,False
316,FM-H,0,,False
317,SE-H,0,,False
318,AD AT TAAT,0,,False
319,"k,""1000,k'"",8000",0,,False
320,"Figure 4: Efficiency for 1,000 randomly sampled MSN queries against the TREC 7 & 8 collection.",1,TREC,True
321,600,0,,False
322,400,0,,False
323,msec,0,,False
324,200,0,,False
325,0,0,,False
326,FM,0,,False
327,SE,0,,False
328,FM-H,0,,False
329,SE-H,0,,False
330,AD AT TAAT,0,,False
331,"k,""10,k'"",20",0,,False
332,FM,0,,False
333,SE,0,,False
334,FM-H,0,,False
335,SE-H,0,,False
336,AD AT TAAT,0,,False
337,"k,""100,k'"",200",0,,False
338,FM,0,,False
339,SE,0,,False
340,FM-H,0,,False
341,SE-H,0,,False
342,AD AT TAAT,0,,False
343,"k,""1000,k'"",2000",0,,False
344,"Figure 5: Efficiency for 1,000 randomly sampled MSN queries against the the TREC WT10G collection.",1,TREC,True
345,"noted. All efficiency runs are reported as the mean and median of 10 consecutive runs of a query, and all necessary information is preloaded into memory.",1,ad,True
346,"Note that we do not carry out an full evaluation of the effectiveness of the algorithms presented in this paper. In previous work, we showed that the BM25 ranking and query evaluation framework used in our approach can be as effective as other state-of-the-art open source search engines when using k > k, and do not repeat those experiments here [11]. In these experiments, we use the minimum k values that result in retrieval performance that is comparable to the effectiveness obtained through exhaustive processing. In all experiments we use k ,"" 8  k for the TREC 7 & 8 dataset, and k "","" 2  k for the TREC WT10G dataset. These values for k give results for the MAP and P@10 effectiveness measures that are not statistically significantly different compared to exhaustive processing, for both collections (paired t-test, p > 0.05). We intend to pursue additional efficiency and effectiveness trade-offs in future work.""",1,TREC,True
347,7.1 Experimental Setup,0,,False
348,"In order to test the efficiency of our algorithms, queries of varying length were extracted from a query log supplied by Microsoft.",0,,False
349,"Each query was tested against both TREC collections, and the filter-",1,TREC,True
350,ing criteria used was that every word in the query had to appear in at,1,ad,True
351,"least 10 distinct documents, resulting in a total of 656,172 unique",0,,False
352,"queries for the TREC 7 & 8 collection, and a total of 793,334 unique",1,TREC,True
353,queries for the TREC WT10G collection. From the resulting filtered,1,TREC,True
354,"query sets, two different query samples were derived.",0,,False
355,"First, 1000 queries of any length were randomly sampled from",0,,False
356,"each set, to represent a generic query log run. The 1,000 sampled",0,,False
357,"queries for TREC 7 & 8 have an average query length of 4.224, and",1,TREC,True
358,the average query length of the WT10G sample set is 4.265 words,1,WT,True
359,"per query. For the second set of experiments, 100 queries for each",0,,False
360,query length 1 to 8 were randomly sampled from the same MSN,0,,False
361,query sets. Table 1 shows the statistical properties of the sampled,0,,False
362,"queries that were used in the second experimental setup, including",0,,False
363,the average number of documents returned for each query for each,0,,False
364,"query length, and the average length of postings lists processed for",0,,False
365,"each query, computed as (",0,,False
366,"|q| i,1",0,,False
367,ni)/|q|.,0,,False
368,7.2 Average Query Efficiency,1,Query,True
369,"In order to test the efficiency of our algorithms, two experiments were performed on each of the collections. The first experiment is designed to measure the average efficiency for each algorithm,",0,,False
370,231,0,,False
371,"trec-7&8k',8*k",1,trec,True
372,"k,10",0,,False
373,200,0,,False
374,AD AT,0,,False
375,100,0,,False
376,50,0,,False
377,FM-H/SE-H,0,,False
378,10,0,,False
379,FM/SE,0,,False
380,TAAT,0,,False
381,0.1,0,,False
382,200 100,0,,False
383,50 10,0,,False
384,"k,100",0,,False
385,"k,1000",0,,False
386,MedianQryTim [ems],0,,False
387,"wt10gk',2*k",0,,False
388,0.1,0,,False
389,1 2 34 5 6 7 81 2 34 5 6 7 81 2 34 5 6 7 8,0,,False
390,"Figure 6: Efficiency of query length of 1 to 8 on the TREC 7 & 8 collection (top row) and TREC WT10G collection (bottom row) for k,""10,100 and 1000. For each query length, 100 randomly sampled queries are used from the MSN query log set.""",1,TREC,True
391,"given a sampling of normal queries. For this experiment, the length of the queries was not bounded during sampling, and had an average query length of just over 4 words per query as mentioned in Section 7.1.",1,ad,True
392,"Figures 4 and 5 show the relative efficiency of each method averaged over 1,000 randomly sampled MSN queries for TREC 7 & 8, and TREC WT10G. Each boxplot summarizes the time values as follows: the solid line indicates the median; the box shows the 25th and 75th percentiles; and the whiskers show the range, up to a maximum of 1.5 times the interquartile range, with outliers beyond this shown as separate points. In both figures, the following abbreviations are used for the algorithms: FM-GREEDY (FM), SEM-GREEDY (SE), FM-HSV (FM-H), SEM-HSV (SE-H), DAAT , and TAAT.",1,TREC,True
393,"We report the timings for all of the self-indexing methods using the character-based indexes. We also ran the same experiments using our term-based indexes, but the performance was identical. This result is not surprising since the dominant cost in the selfindexing method is traversing the wavelet tree over the document array, and is dictated by the depth of the wavelet tree and not the overall length. Since the depth depends only on the number of documents, both approaches consistently produce similar running times. So, the only efficiency difference between character-based and term-based indexes is in space-usage which is discussed in Section 7.",0,,False
394,"We see that the self-indexing methods which must calculate all frequency scores (FM and SE) incur the most overhead as k increases. This is largely due to the multiplicative effect of collating many consecutive k values. For example, when collating frequency values from WTd, the number of rank operations is proportional to the depth of the wavelet tree. In the case of the TREC WT10G collection, which contains around 1.6 million documents, the depth of the wavelet tree is 24. So, the number of random rank probes in wavelet tree begins to significantly degrade the performance for larger k.",1,ad,True
395,"This effect can be marginalized by augmenting WTd with HSV. Since an HSV style index has a portion of each sp, ep ranges, only the fringe positions for each range need to be calculated at runtime, reducing the total number of page faults. For all k, the HSV indexes are efficient and remarkably resilient to outliers. In general, the WAND variant of DAAT is more efficient for large values of k, but can perform poorly for certain queries. For example, the query ""point out the steps to make the world free of pollution"" on the WT10G collection consistently performed poorly in our DAAT framework.",1,WT,True
396,7.3 Efficiency based on Query Length,1,Query,True
397,"We now break down the efficiency of each of our algorithms relative to two parameters: k and q, where q is the number of terms in a query. Figure 6 shows the average of 10 runs of 100 queries per query length, q. For one-word queries, for all values of k, the inverted indexing approaches DAAT and TAAT are superior. This is not surprising since only a single term posting must be traversed to calculate BM25, and the algorithms have excellent locality of access. Still, the HSV variant is the most efficient for small k.",0,,False
398,"For |q| > 1, the results also depend on k. For k , 10 and k ,"" 100, the self-indexing methods are more efficient than TAAT since the methods can extract exactly k values. Since the sample rates in the lower regions of the HSV methods are close to k, very little work needs to be done by the indexes. The WAND-based DAAT method remains remarkably efficient for all values of k. As k increases, the performance of the HSV-based approaches begins to degrade since the sample size for the precalculated top-k orderings grows exponentially. The performance degradation at large k is equivalent to Case (3) as described in Section 4. In essence, most of the sp, ep ranges turn out to be much smaller than any of the samples, so the complete sp, ep range must be computed at runtime, reducing the performance to FM-GREEDY when an appropriate sample is not available. Note that the performance of HSV for TREC 7 & 8 is worse than for WT10G for two reasons. First, k""",1,ad,True
399,232,0,,False
400,20000 15000 10000,0,,False
401,trec-7&8 Components,1,trec,True
402,FM WTD DocMap HON SEMap InvFile PosOffsets DocCache,1,WT,True
403,wt10g,0,,False
404,S zi (eM)B,0,,False
405,5000,0,,False
406,0 FM,0,,False
407,SE,0,,False
408,FM-Term,0,,False
409,AD ATTAAT,0,,False
410,FM,0,,False
411,SE,0,,False
412,FM-Term,0,,False
413,AD ATTAAT,0,,False
414,Figure 7: Space usage for each component in the three indexing approaches presented in this paper for the TREC 7 & 8 collection (left) and the TREC WT10G collection (right). The dashed line in both graphs represents the total space usage of the original uncompressed text for the collection.,1,TREC,True
415,"is four times larger in TREC 7 & 8 resulting in fewer sample points. Secondly, if only a partial match is found, the self-index approach must retrieve 8 times more intermediate documents for scoring than in the inverted indexing approaches.",1,TREC,True
416,Note that none of our self-indexing approaches currently employ MAXSCORE or other methods to guide scoring. In principle our approach could also benefit from similar enhancements. We intend to explore the benefits and drawbacks of various early termination and impact scoring approaches for self-indexes in future work.,0,,False
417,7.4 Space Usage,0,,False
418,"We now address the issue of space usage for the different algorithmic approaches. Inverted indexes are designed to take advantage of a myriad of different compression techniques. As such, our baselines also support several state-of-the-art byte and word aligned compression algorithms [3, 9, 28, 39, 43]. So, when we report the space usage for an inverted index, the numbers are reported using compressed inverted indexes and compressed document collections.",1,ad,True
419,"Figure 7 presents a break down of space usage for each component of the inverted indexing and self-indexing approaches. From a functionality perspective, there are several different componentization schemes to consider. First, consider the comparison of an inverted index method (including the term map, the postings list with p offsets, the document map, and the compressed document cache) with an FM-Index (including WTd, the document map, and any other precomputed values ­ for instance the HSV enhancement). We consider these two in-memory indexes as functionally equivalent, as both can support bag-of-words or phrase queries, and can recreate snippets or even the original uncompressed document. The character based variant FM is significantly larger, but able to support a range of special character and arbitrary substring queries that term-based indexes do not support. Therefore, the term-based self-indexing variant FM-TERM is much closer to the inverted indexing variant in space usage and functionality.",1,WT,True
420,"The second alternative are indexes that support only bag-of-words queries. Now, an inverted index method requires only the term map, the postings list without p offsets, and the document map. The character-based self-indexes are essentially the same, but the FM-",0,,False
421,"Index component is replaced with a term map component. Note that the FM component of FM-TERM is only required for phrase queries, and can also be dropped if only bag-of-words queries are required. When considering all of the current self-indexing options presented in this paper, using an FM-Index component instead of a term map appears to offer the most flexible configuration for character-based self-indexes, while the term-based variant is competitive in both time, space, and functionality with an inverted index.",1,ad,True
422,8. CONCLUSION,0,,False
423,We have presented an algorithmic framework for in-memory bagof-words query processing that is efficient in practice. We have compared and contrasted our framework with industry and academic standard inverted indexing algorithms. Our approach shows great promise for advancing the state-of-the-art in exciting new directions.,1,ad,True
424,"However, several challenges must be overcome before these algorithms can reach widespread acceptance. For instance, recent work has dramatically reduced the space required for self-indexing algorithms, there are still opportunities to further reduce space usage in self-indexes. Another shortcoming of bag-of-words querying with self-indexing algorithms is providing top-k guarantees. While good solutions exist for providing top-k guarantees on singleton pattern queries, optimally merging multiple queries remains problematic.",1,ad,True
425,"However, self-indexing algorithms can also efficiently provide functionality that is notoriously inefficient, and sometimes even impossible, using inverted indexes. In addition to basic bag-of-words queries, our approach has the capability to perform phrase queries of any length, as well as the ability to support complex statistical calculations at query time, with no additional indexing costs. In fact, phrase queries were shown to be significantly faster using FM-GREEDY than when using inverted indexing approaches in prior work [10]. Self-indexes also inherently preserve term proximity. So, not only can each term be found quickly, but the nterms surrounding the keyword can quickly be extracted, tabulated, and used for on-the-fly statistical calculations. Applications of this functionality include more efficient relevance feedback algorithms, construction of higher order language models in ranking metrics, or term dependency extraction and query expansion. In summary, all of the disadvantages outlined in this paper for self-indexing paper warrant further research, as the potential benefits of this new approach are compelling indeed.",1,ad,True
426,"In future work, we will explore new algorithmic approaches to reduce space usage, and to further improve efficiency for larger values of k. We also intend to investigate the combination of efficient phrase querying and proximity calculations to produce and evaluate novel ranking metrics. Finally, we will design and evaluate new approaches to support distributed in-memory query processing in order to scale our system to terabyte size collections.",0,,False
427,9. ACKNOWLEDGMENTS,0,,False
428,This work was supported in part by the Australian Research Council. We thank Gonzalo Navarro and Daniel Valenzuela for insightful discussions on the HSV method. We also thank Alistair Moffat for valuable feedback on a draft of this paper.,0,,False
429,References,0,,False
430,"[1] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In SIGIR, pages 372­379, 2006.",0,,False
431,233,0,,False
432,"[2] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. In SIGIR, pages 35­42, 2001.",0,,False
433,"[3] N. R. Brisaboa, A. Fariña, G. Navarro, and M. F. Esteller. (S, C)-dense coding: An optimized compression code for natural language text databases. In SPIRE, volume 2857 of LNCS, pages 122­136, 2003.",0,,False
434,"[4] A. Z. Broder, D. Carmel, H. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In CIKM, pages 426­434, 2003.",0,,False
435,"[5] E. W. Brown. Fast evaluation of structured queries for information retrieval. In SIGIR, pages 30­38, 1995.",0,,False
436,"[6] C. Buckley and A. F. Lewit. Optimization of inverted vector searches. In SIGIR, pages 97­110, 1985.",0,,False
437,"[7] M. Burrows and D. J. Wheeler. A block-sorting lossless data compression algorithm. Technical Report 124, Digital Equipment Corporation, Palo Alto, California, May 1994.",0,,False
438,"[8] S. Büttcher, C. L. A. Clarke, and G. V. Cormack. Information Retrieval: Implementing and evaluating search engines. MIT Press, Cambridge, Massachusetts, 2010.",0,,False
439,"[9] J. S. Culpepper and A. Moffat. Enhanced byte codes with restricted prefix properties. In SPIRE, volume 3772 of LNCS, pages 1­12, November 2005.",0,,False
440,"[10] J. S. Culpepper, G. Navarro, S. J. Puglisi, and A. Turpin. Top-k ranked document search in general text databases. In ESA, Part II, LNCS 6347, pages 194­205, 2010.",0,,False
441,"[11] J. S. Culpepper, M. Yasukawa, and F. Scholer. Language independent ranked retrieval with NeWT. In ADCS, pages 18­25, December 2011. See http://goanna.cs.rmit.edu. au/~e76763/publications/cys11- adcs.pdf.",1,WT,True
442,"[12] P. Ferragina and G. Manzini. Opportunistic data structures with applications. In FOCS, pages 390­398, 2000.",0,,False
443,"[13] P. Ferragina and G. Manzini. Indexing compressed text. Journal of the ACM, 52(4):552­581, 2005. A preliminary version appeared in FOCS 2000.",0,,False
444,"[14] P. Ferragina, R. Giancarlo, and G. Manzini. The myriad virtues of wavelet trees. Information and Computation, 207:849­866, 2009.",1,ad,True
445,"[15] P. Ferragina, R. Gonzaález, G. Navarro, and R. Venturini. Compressed text indexes: From theory to practice. Journal of Experimental Algorithmics, 13:12.1­12.31, 2009.",0,,False
446,"[16] J. Fischer and V. Heun. Space-efficient preprocessing schemes for range minimum queries on static arrays. SIAM Journal on Computing, 40(2):465­492, 2011.",0,,False
447,"[17] M. Fontoura, V. Josifovski, J. Liu, S. Venkatesan, X. Zhu, and J. Zien. Evaluation strategies for top-k queries over memory-resident inverted indexes. Proceedings of the VLDB Endowment, 4(12):1213­1224, 2011.",0,,False
448,"[18] T. Gagie, S. J. Puglisi, and A. Turpin. Range quantile queries: Another virtue of wavelet trees. In SPIRE, pages 1­6, 2009.",0,,False
449,"[19] T. Gagie, G. Navarro, and S. Puglisi. New algorithms on wavelet trees and applications to information retrieval. Theoretical Computer Science, 426:25­41, 2012.",0,,False
450,"[20] R. Grossi, A. Gupta, and J. S. Vitter. Higher-order entropy-compressed text indexes. In SODA, pages 841­850, 2003.",0,,False
451,"[21] D. Hawking. Overview of the TREC-9 web track. In TREC-8, pages 87­102, 1999.",1,TREC,True
452,"[22] W.-K. Hon, R. Shah, and J. S. Vitter. Space-efficient framework for top-k string retrieval problems. In FOCS, pages 713­722, 2009.",0,,False
453,"[23] G. Jacobson. Succinct static data structures. PhD thesis, Carnegie Mellon University, 1988.",0,,False
454,"[24] V. Mäkinen and G. Navarro. Implicit compression boosting with applications to self-indexing. In SPIRE, LNCS 4726, pages 229­241, 2007.",0,,False
455,"[25] U. Manber and E. W. Myers. Suffix arrays: A new method for on-line string searches. SIAM J. Comp, 22(5):935­948, 1993.",0,,False
456,"[26] G. Manzini. An analysis of the Burrows-Wheeler transform. Journal of the ACM, 48(3):407­430, May 2001.",0,,False
457,"[27] S. Mithukrishnan. Efficient algorithms for document retrieval problems. In SODA, pages 657­666, 2002.",0,,False
458,"[28] A. Moffat and V. N. Anh. Binary codes for non-uniform sources. In DCC, pages 133­142, 2005.",0,,False
459,"[29] A. Moffat and J. Zobel. Self indexing inverted files for fast text retrieval. ACM TOIS, 14(4):349­379, 1996.",0,,False
460,"[30] G. Navarro and V. Mäkinen. Compressed full-text indexes. ACM Computing Surveys, 39(1):2­1 ­ 2­61, 2007.",0,,False
461,"[31] G. Navarro and D. Valenzuela. Space-efficient top-k document retrieval. In SEA, LNCS 7276, pages 307­319, 2012.",0,,False
462,"[32] M. Persin. Document filtering for fast ranking. In SIGIR, pages 339­348, 1994.",0,,False
463,"[33] M. Persin, J. Zobel, and R. Sacks-Davis. Filtered document retrieval with frequency sorted indexes. JASIST, 47(10):749­764, 1996.",0,,False
464,"[34] S. J. Puglisi, W. F. Smyth, and A. H. Turpin. A taxonomy of suffix array construction algorithms. ACM Comp. Surv., 39(2):4.1­4.31, 2007.",0,,False
465,"[35] R. Raman, V. Raman, and S. S. Rao. Succinct indexable dictionaries with applications to encoding k-ary trees and multisets. In SODA, pages 233­242, 2002.",0,,False
466,"[36] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In TREC-3, 1994.",1,TREC,True
467,"[37] K. Sadakane. Succinct data structures for flexible text retrieval systems. J. Discr. Alg., 5(1):12­22, 2007.",1,ad,True
468,"[38] T. Strohman, H. Turtle, and W. B. Croft. Optimization strategies for complex queries. In SIGIR, pages 219­225, 2005.",0,,False
469,"[39] A. Trotman. Compressing inverted files. Information Retrieval, 6(1): 5­19, 2003.",0,,False
470,"[40] H. Turtle and J. Flood. Query evaluation: strategies and optimizations. Information Processing and Management, 31(6): 831­850, 1995.",1,Query,True
471,"[41] N. Välimäki and V. Mäkinen. Space-efficient algorithms for document retrieval. In CPM, LNCS 4580, pages 205­215, 2007.",0,,False
472,"[42] E. M. Voorhees and D. K. Harman. Overview of the Eighth Text REtrieval Conference (TREC-8). In TREC-8, pages 1­24, 1999.",1,TREC,True
473,"[43] H. Yan, S. Ding, and T. Suel. Compressing term positions in web indexes. In SIGIR, pages 147­154, 2009.",0,,False
474,"[44] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Comp. Surv., 38(2):6­1 ­ 6­56, 2006.",0,,False
475,"[45] J. Zobel, A. Moffat, and K. Ramamohanarao. Inverted files versus signature files for text indexing. ACM TODS, 23(4):453­490, 1998.",0,,False
476,234,0,,False
477,,0,,False

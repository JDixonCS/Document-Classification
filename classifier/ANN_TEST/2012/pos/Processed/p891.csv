,sentence,label,data,regex
0,On Per-topic Variance in IR Evaluation,0,,False
1,Stephen E. Robertson,0,,False
2,"Microsoft Research 7 JJ Thomson Avenue Cambridge CB3 0FB, UK",0,,False
3,stephenerobertson@hotmail.co.uk,0,,False
4,Evangelos Kanoulas,0,,False
5,Information School University of Sheffield,0,,False
6,"Sheffield, UK",0,,False
7,ekanoulas@gmail.com,0,,False
8,ABSTRACT,0,,False
9,"We explore the notion, put forward by Cormack & Lynam and Robertson, that we should consider a document collection used for Cranfield-style experiments as a sample from some larger population of documents. In this view, any pertopic metric (such as average precision) should be regarded as an estimate of that metric's true value for that topic in the full population, and therefore as carrying its own per-topic variance or estimate precision or noise. As in the two mentioned papers, we explore this notion by simulating other samples from the same large population. We investigate different ways of performing this simulation. One use of this analysis is to refine the notion of statistical significance of a difference between two systems (in most such analyses, each per-topic measurement is treated as equally precise). We propose a mixed-effects model method to measure significance, and compare it experimentally with the traditional t-test.",1,ad,True
10,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval],0,,False
11,"General Terms: Experimentation, Measurement, Performance",0,,False
12,"Keywords: information retrieval, evaluation, statistical precision, significance testing, mixed-effects model, simulation",0,,False
13,1. INTRODUCTION,1,DUC,True
14,"In two recent papers, by Cormack and Lynam [3] and Robertson [9], it has been suggested that in Cranfield-style test-collection-based IR evaluations, we should in general consider the document collection used as a sample from a population (in addition to considering the set of topics or queries thus). Such a view suggests (among other things) that each per-topic measurement (such as of Average Precision) should be considered as carrying its own statistical precision, or noise, and that this noise might be different for every topic. This potentially complicates the issue of establishing statistical significance in IR tests. Looked at in a",1,ad,True
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.",1,ad,True
16,"more global way, the set of measurements that we take in a test, averaged over a set of topics, are the result not of a simple sampling of queries/topics, but of the interaction between a sampling of queries/topics and a separate sampling of documents. In [3] this question was investigated in an empirical way, via bootstrap experiments on experimental results; [9] was concerned with more theoretical issues involving the nature of IR evaluation metrics, and appealed only to very limited simulation experiments based on simple models. In some sense the present paper takes a position between these two, with experiments based on a variety of models starting from bootstrapping from empirical results.",0,,False
17,"The primary question being investigated in this paper is the question of how we should think about the reliability of individual per-topic measurements. The question is tricky, and the paper does not arrive at any firm conclusions. However, it is hoped to shed some light on the question. Specifically, we consider tests of statistical significance for differences between systems. An approach that can make use of per-topic noise is the mixed-effects models. We propose and investigate a linear mixed-effects models approach to significance.",0,,False
18,2. BACKGROUND,0,,False
19,2.1 Cormack & Lynam,0,,False
20,"In this paper [3], a TREC collection of documents was split randomly into two parts. For each topic, the evaluation result (in the form of the value of a metric M such as average precision, AP ) in first half of the collection was taken as predicting the value of M in the second half. In order to estimate the variance (or statistical precision) associated with this prediction, the authors took repeated bootstrap samples from the first half of the collection, and re-measured M on each sample. The distribution of M values over samples (single topic, single original set of results) provides an indication of the statistical precision required. They concluded that this procedure does indeed adequately model the variance inherent in an AP estimate for an individual topic in a document collection of a certain size, regarded as a sample from an infinite collection of potential documents (so that the split collection is seen as two independent samples from this population).",1,TREC,True
21,"Certain technical issues arise: first, it is better to use M , logit(AP ) rather than M ,"" AP , as it has better distributional properties; second, in the bootstrap sampling it is necessary to treat very small values of R, the total number""",1,AP,True
22,891,0,,False
23,"of relevant documents in the observed collection, in special ways. These are discussed further below.",0,,False
24,2.2 Robertson,0,,False
25,"In this paper [9], different metrics M are investigated theoretically, with a view to establishing whether it makes sense to pose the infinite-document-population question in terms of each metric. Some metrics turn out not to be understandable in these terms; a metric as defined on a given finite population may not be generalisable to an infinite population. In particular, for example, there appears to be no equivalent to NDCG defined on an infinite population. AP , however, can be so generalised.",1,AP,True
26,"Even if it is generalisable, we should ask the question: is the metric value observed on the given finite population a good estimate of its `true' value in the infinite population? Some limited simulation experiments, using some previously proposed models of score distributions for relevant and nonrelevant documents, are made in order to investigate this question. It appears from both theoretical arguments and these simulation experiments that some such metrics may provide biased estimates of their true population values. AP seems to be one of these: AP observed on a given finite document population is likely to be an over-estimate of `true' AP on the infinite population of which the observed finite population is a sample. The reason postulated for the bias in AP is as follows. The infinite population equivalent is defined as a particular form of integral over (area under) the infinite-population recall-precision curve, which we expect to be smooth. However, in estimating it for an observed sample, we choose to measure the height of the recall-precision curve only at the points in the ranking where relevant documents occur, i.e. exactly at the peaks of the noisy sample curve.",1,ad,True
27,2.3 Topics and documents,0,,False
28,"The usual approach to system evaluation in IR experiments is to choose a metric defined on the results of searching on an individual topic or query, and take an average of this metric over the set of topics, for the system concerned (there are many such metrics in common use, including AP and NDCG). The usual approach to statistical significance is to consider the results for the individual topics as the (indivisible) units of measurement. So suppose, for example, that the measure is RPrec and the significance test is the sign test, the fact that system A performs better than system B by RPrec on topic 1 counts as a definitive result. The significance question arises only at the aggregation-overtopics level (`does system A perform better than system B on significantly more topics?'). We do not ask the question as to whether or in what sense the individual topic 1 result was significant. This approach effectively assumes that the topics were sampled from some population of (possible or actual) topics, but that the document collection is fixed in stone for all time.",1,AP,True
29,"Stated like this, it must be clear that this (the document collection part) is a rather drastic assumption. In fact it is in part mitigated by the general view in IR research that a result is only good if it works on multiple test collections. But this is a rather crude approach to the problem. It would be good to have a much better understanding of what each individual topic result is telling us, and of how best to draw general conclusions from the topic sample × document sam-",0,,False
30,ple results that we have. A start in exploring these issues is made in the two papers cited above; the present paper attempts to take this analysis a step further.,1,ad,True
31,"A basic premise of the approach taken in the two cited papers is that the population of documents looks different from the point of view of each separate topic ­ in particular, with regard to relevance. The simulations in both papers are based on modelling the document population separately for each topic. An alternative approach would be to model the population of documents independently of the topics, and then model the interaction of each topic with that population. Such an approach would be interesting indeed but very hard. It would also have to model the interaction of each system with the population, separately applied to each topic. The big advantage of the per-topic model of the population is that both relevance and system responses (e.g. scores) can be taken as given. Furthermore, it serves to emphasise an important aspect of the document sample view of IR experiments. While the entire document collection is typically very large (from which one might be tempted to conclude that statistical precision issues do not arise), from the point of view of each topic we see a (typically much smaller) sets of relevant or retrieved documents, on which all the measures depend. Precision at cutoff 5, for example, should be seen as a very imprecise measurement (in the statistical sense) for a single topic.",1,ad,True
32,2.4 Variation between topics,0,,False
33,"It is well understood in the IR research community that there is a great deal of variation between topics. For example, the analysis of variance in the paper by Banks et al. [1] finds the topic effect to be greater than the system effect (as well as a significant interaction effect). The Robust track at TREC was devoted to investigating hard topics, in the recognition that there are significant numbers of topics that appear to be generally hard for systems to perform well on.",1,Robust,True
34,"If it is accepted that the document collection should be regarded as sampled in some way from a population, then as indicated above, each per-topic measurement has some builtin error of estimation because the measurement is based on the sample rather than the population. One of the aims of this paper is to get a handle on this estimation error. In particular, we ask the question: does this estimation error vary substantially between topics? If this variation is substantial, should this affect how we analyse test results?",0,,False
35,2.5 Scores and simulations,0,,False
36,"The method of simulation used in [3] is based on bootstrap ideas. We start with a given system and topic and the corresponding set of results (documents retrieved and ranked) from a real test collection of documents. Then we model the process of drawing other samples from the notional infinite population of documents by repeated sampling with replacement from these actual documents. The method used in [9] is a parametric approach based on score distributions. On the back of previous work on fitting distributions to the scores of relevant and non-relevant documents respectively, these distributions are then taken as representing the notional population of documents. We can repeatedly sample from the distributions rather than from any specific documents.",0,,False
37,It is clear that the score-distribution approach implies a smoothing effect which the bootstrap method does not have.,0,,False
38,892,0,,False
39,"Modelling the population as consisting only of instances exactly like the ones we have already seen seems to go against at least part of the basic idea of samples and populations ­ that any sample we take consists only of examples. We should in some sense expect the population to include other kinds of examples, not represented directly in the sample we have. On the other hand, the use of score distributions typically involves strong assumptions about the nature of the distributions, and also involves some dependence on the particular scoring methods used by systems. Systems differ greatly not only in the kinds of scores they produce, but also in whether they produce a consistent set of scores at all. For example, some systems may do an implicit AND on the query terms, and then use a scoring function which is defined only within the AND set. Others may rank by rule or on the basis of local comparisons which do not result in an overall score. These facts clearly limit the scope of score-distribution modelling ­ as against the advantage of smoothness.",1,ad,True
40,"In this paper, accepting the limitations of the score distribution approach, we nevertheless use it to investigate the smoothness issues. That is, we try to characterise how assuming a smoother population might affect what we can infer from the sample. But we look at a range of scoredistribution methods, of which one extreme (no smoothing) corresponds to the Cormack and Lynam bootstrap method. Thus taking the scores of the retrieved real documents as given and performing bootstrap samples on them is equivalent to bootstrapping from the documents themselves1. Then we introduce various levels of smoothing, from minimal Gaussian noise to fully-fledged parametric distribution fitting.",0,,False
41,An alternative method not pursued in this paper might be to smooth by introducing noise into the documents themselves. Some such methods have been used in the context of query performance prediction [11].,0,,False
42,3. MOTIVATING HYPOTHESIS,0,,False
43,"To restate the hypothesis that motivates this paper, as well as the two earlier ones:",0,,False
44,A test document collection should be thought of as a sample from some hypothetical universe of possible documents.,0,,False
45,"As is common in statistical analysis of any experimental results, we assume that the sample that we have is in some way representative of the universe, which we take to be large or infinite. The primary object of statistical analysis is to draw inferences about this universe. This is not to say that the test collection is representative of other real documents in the world, but that such a view is a prerequisite for basic statistical understanding. If we do not know how to draw inferences correctly for this hypothetical universe, then we certainly do not know how to draw inferences about the real world.",0,,False
46,The primary concern of this paper is with the variance (error) associated with each observation that we make (a given metric on a given system-topic pair). Since we cannot normally make multiple repeated observations on different test,0,,False
47,"1We note that the assumption that a system would always give the same score to a given document, even in the context of a varying collection, is debatable: there is some discussion in [4]. We do not pursue this question in the present paper.",0,,False
48,"collections sampled from the same universe, the only way we can discover how such repetitions might be distributed is by simulation.",0,,False
49,4. DATA AND METRICS,0,,False
50,"The document collection used in the experiments reported here is the one contained in TREC Disk 4 and 5, excluding the Congressional Record sub-collection, together with the 50 topics used for the TREC 8 Ad Hoc track. The system runs used are a new set, similar to that used in [6], consisting of a series of 44 runs on the Terrier system. Four different weighting models were used (BM25, the Hiemstra language model, PL2 and TF*IDF), and eleven different query versions (title, title+description, title+description+narrative, and eight query expansion runs using pseudo relevance feedback, expanding the query by between 4 and 512 terms). This set may be characterised in various ways: (a) all the systems (models) generate fairly traditional forms of scores; (b) all runs use the same initial parsing (including Porter stemming and stopword removal); (c) the collection of runs constitutes a systematic matrix of certain variables (weighting schemes and forms of query); (d) all other system variables are held constant.",1,TREC,True
51,"In the event, four pairs of runs gave the exact same AP values and thus only one run from each pair was used in the experiments while the other was discarded. Thus the analysis below is based on 40 runs.",1,AP,True
52,"The primary metric used in this analysis is logit(AP ), where AP is average precision. As indicated above, following [3], the distributional properties of logit(AP ) seem to be better than those of AP ; in particular, because (theoretical) range of logit(AP ) is (-, ), it makes sense to model distributions of repeated logit(AP ) measurements as Gaussian; attempting to do the same with AP measurements results in some logical inconsistencies. In [3], some evidence is presented that such distributions of logit(AP ) measurements are indeed approximately Gaussian. However, it should be noted that this does not resolve the issue raised in [9], that of the bias in estimation from finite samples, which applies to logit(AP ) as well as to AP . The use of logit necessitates some smoothing, to avoid infinities at either end ­ we follow previous practice in using an epsilon correction. However, we note that a new more heavily smoothed version of logit(AP ), yaAP, is proposed in [8]. This has better distributional properties than logit(AP ) itself, and may therefore be a better candidate metric for the work reported here; however, we have not yet rerun the analyses with yaAP.",1,AP,True
53,"The 44 runs have MAPs ranging from 0.273 to 0.131, with a significant bunching to the top end (31 of the runs have M AP > 0.24). If instead of taking the mean of AP , we take the mean of logit(AP ) and then convert it back to the AP scale by reversing the transformation, we see a different ranking of runs and rather lower `AP ' values, ranging from 0.188 to 0.034 (this is consistent with previous reported results using GMAP). Bunching of runs is not quite so strong but still evident.",1,MAP,True
54,5. SIMULATION MODELS,0,,False
55,5.1 General simulation principles,0,,False
56,"In general, we take the results from each topic and system run (the scores of the top 1000 ranked documents), split into",0,,False
57,893,0,,False
58,"relevant and non-relevant. This is taken as the results from searching a sample from some (assumed large or infinite) population of documents, and we wish to simulate what the search results from other samples from this same large population of documents would look like. Thus we need to infer a model of the whole population of documents and resample from it. We avoid sampling from the entire supposed population of documents (which would be intractable) by sampling separately for the results of each topic. What follows is a brief description of the method, which is similar to that used in [3], except that it separates the sampling of relevant and non-relevant documents/scores, in order to allow various smoothing methods. It involves a number of approximations that deserve more discussion than is possible in the present paper.",0,,False
59,"Suppose that the size of the original test collection is N , this topic has R known relevant documents, of which r are retrieved in the top 1000 by this system run. For the purposes of the present experiment, we will assume that all samples from the large population of documents are the same size, N . This assumption is not used directly, but is implicit in what follows. We first take a stochastic decision about how many relevant to include in our sample. This theoretically has two steps: deciding how many reldocs should be in the entire sampled collection, and deciding how many should be in the top 1000 retrieved set. The first step can be based on the observed generality of the topic (proportion of the original test collection that is relevant, R/N ), and the second can be based similarly on the proportion of these retrieved by the system (r/R). In practice we combine these two steps, and take a Poisson approximation because R/N at least is assumed to be small. So we take a sample from a Poisson distribution with mean r, to define the number rs of relevant retrieved for this topic in this simulation run. We ignore instances where this number is zero (further discussion below).",0,,False
60,"Next we take rs samples from some model of the distribution of relevant document scores, and 1000-rs samples from some model of the distribution of non-relevant scores. The details of how we do this vary between models. We sort the 1000 resulting scores into descending order, and treat them as the ranked document scores from a new test collection, sampled similarly to the original from the same large population. Since these scores are marked as `relevant' or `not relevant', we can apply any IR metric to this ranked list.",0,,False
61,5.2 Analysis,0,,False
62,"For each topic and each run (assume S systems or runs, and T topics), we repeat the process described above some number C of times. This results in C simulated values of the metric M , for each topic and each run. In simulation, these values represent the outcome of C different test collections, all of the same size, all taken from the same infinite population of documents, all searched for the same topic by the same system/run (meaning in this case the same query formulation and the same weighting and scoring method). We note that we could also vary the size of the simulated collection to investigate the effect of such variation on evaluation results.",0,,False
63,"In what follows, we use these C ×T ×S values of M in two different ways. First, we compare the mean and variance of M over C , 1000 samples with the observed value of M for the real test collection. This is used to validate the simula-",0,,False
64,"tion model as producing reasonable unbiased results; it also serves to demonstrate the large variation between the variances of different topics, thus confirming our initial premise that different topics provide measurements with different associated precision (or estimation error or noise). Second, we take C , 10 samples and submit them to an analysis of variance process. This provides us with a way of conducting statistical significance tests which take account of this variation between topics.",0,,False
65,5.3 Simulation models,0,,False
66,"As indicated, the simulation models vary in how they smooth the actual scores obtained from the system in modelling the two required distributions for each topic. In the BST (Bootstrap) simulation, we sample with replacement from the list of actual scores of the top 1000 documents, split into relevant and non-relevant. In this simulation, the scores are in effect labels only; it is therefore more-or-less equivalent to the bootstrap method used in [3]. One difference is in the handling of the zero cases. These turned out to be a problem in [3], because (a) the corpus used (TREC 6) included 5 topics with R < 5, and (b) the corpus was split randomly into two parts for their experiments. In the present corpus, there is one topic for which R ,"" 6, and the next lowest value is 13; furthermore, we do not split the collection. Therefore it has been assumed to be safe not to treat these cases in any special way, and to simply ignore the bootstrap samples which would have rs "", 0.",1,TREC,True
67,"In the KDE (kernel density estimation) method, we add a small amount of noise (following some simple distributional assumption) to each datapoint, and mix the results into a single smooth distribution. Different noise models may be used, but a common form (used here) is Gaussian ­ this can therefore be thought of as a Gaussian mixture model with one Gaussian for each observed score. In the present paper, we make use of a Perl module [5], applied separately to the relevant and non-relevant scores for each topic/returned document combination. There are various methods of determining the variance to be applied to the noise model; these are not discussed further in the present paper.",1,ad,True
68,"In the GMG (Gaussian Mixture and Gamma) model, developed by Kanoulas et al [6], the relevant distribution is modelled as a mixture of Gaussians (usually 1, 2 or 3 Gaussians, determined by the data), and the non-relevant as a Gamma distribution. Estimated parameters provided by the fitting process are the means and variances of the Gaussians, their relative preponderance, and the shape and scale parameters of the Gamma, as discussed in the cited paper.",0,,False
69,"As will be clear, simulation of IR evaluation evaluations is a complicated process, deserving of much more discussion. In the present paper, simulation is a means to an end, and such discussion is deferred to a later paper.",0,,False
70,6. RESULTS OF THE SIMULATIONS,0,,False
71,"We present some results, as follows. In Figure 1 (simulation model BST, run 11), each point represents a single topic; the x-axis is the observed logit(AP ) on the real test collection, and the y-axis is the value of the same metric in the samples. The points represent the means over all C ,"" 1000 samples, and the bars represent one standard deviation either side of the mean. We note that (a) the simulated results are very close to the observed ones; (b), more importantly, the standard-deviation bars are very variable.""",1,AP,True
72,894,0,,False
73,Figure 1: Bootstrap simulation of run 11.,0,,False
74,"In relation to (a), we note also that this is as expected, since the bootstrap simulation uses the real results very directly; we have made no attempt to prevent overfitting. However, we can clearly see consistency; it is entirely plausible, from the means at least, that the observed results were drawn from the distributions indicated. Once again, we defer more detailed discussion of the fit to observation to a future paper devoted to simulation. Run 11 is the best-performing run according to this metric; Figure 2 shows similar graphs for three more runs, distributed over the ranking of runs, with very similar characteristics.",1,ad,True
75,"We note also that the per-topic standard deviation values are similar for different runs. Taking pairwise correlations between runs over the set of topic standard deviations, for the above 5 runs, the correlation values range from 0.60 to 0.97 (the bottom of the range is raised significantly if we ignore the worst-performing run). Thus (on the whole) the same topics have the least precise estimates, or the greatest noise, on the different runs. This result may well be affected by the choice of runs for the present experiment: a more varied set of runs might indicate less commonality in this respect.",0,,False
76,"Figure 3 shows one run under the Kernel Density simulation and two under the Gaussian Mixture and Gamma simulation. The variation from the observations is slightly greater, though not by very much, as a result of the smoothing; the variances are also broadly similar.",1,ad,True
77,7. COMPARATIVE EVALUATION,0,,False
78,"So far we have introduced methods to simulate effectiveness measurements over different document corpora, samples from a very large (or infinite) document collection. Hence, the effectiveness of a system/run not only varies across topics but it further varies within each topic. The question that remains to be answered is how can we account for this new source of variability (within topic variability) and dis-",1,corpora,True
79,tinguish it from the variability across topics when evaluating retrieval systems in a comparative evaluation exercise.,0,,False
80,"Comparative systems-based evaluation typically uses a statistical hypothesis test such as the t-test to infer the significance of the difference in the effectiveness of two retrieval systems. However, the t-test is not adequate to handle multiple sources of variability into a systems-based evaluation. On the other hand, mixed-effects models, which will be introduced in this section, can incorporate arbitrary sources of variability into an analysis [7]. 2",1,ad,True
81,"We start with a simple model of a single source of variance (topic effect) and extend it to a model that better corresponds to our experiments of repeated measurements per topic. To illustrate the analysis, we take as example the simulated results for a single pair of systems.",0,,False
82,"The linear model that accounts only for the topic effect is,",0,,False
83,"yij , i + bj + ij",0,,False
84,(1),0,,False
85,"bj  N (0, 12), ij  N (0, 2)",0,,False
86,"where yij is the value of an evaluation measure calculated on topic j for system i, i is the effect of system i, bj is the effect of topic j, and ij is the residual error. The system effect is a so-called ""fixed effect"" since the systems we would like to compare are fixed and not a sample from some system distribution. On the other hand the topic effect is a ""random effect"", assuming that the topic is actually sampled from a population of topics.",0,,False
87,"The assumption behind this model is that the random variable, bj, is independent and identically normally distributed with zero mean and 12 variance. The residual error is also assumed to be independent and identically normally distributed with zero mean and 2 variance.",0,,False
88,In the statistical programming environment R the following procedures fit the same linear effect model (Eq. 1) and result in equivalent inferences:,0,,False
89,"t.test(y ~ system, paired,""TRUE, data"",data)",0,,False
90,"lme(y ~ system, data,""data, random"",~1|topic)",0,,False
91,"In all cases our data consists of an evaluation measure calculated over a set of topics for the two systems under comparison. This is stored in a data frame data with three fields, the measurement y, the system id system, and the topic id topic. The first line runs a paired t-test; in the second one the response variable y is explicitly written as a function of a fixed effect (system) and a random effect (Error(topic)). The last line is explicitly fitting a mixedeffects model using the function lme in the nlme package.",0,,False
92,"The mixed-effects model in Eq. 1 models the effects of topics over the effectiveness scores of the compared systems. However, it fails to model a far more important effect, that of the interaction between the systems and the topics; that is the fact that a system may find a query hard while the same query is considered easy by a different system. The effect of this interaction in the above model is conflated with the residual error, since we only have one measurement per system-topic. In the case of repeated measurements this interaction effect can be decomposed from the random error and modeled separately,",0,,False
93,2Mixed-effects models has been recently discussed by Carterette et al. [2]. A similar discussion is provided in this section.,0,,False
94,895,0,,False
95,"Figure 2: Bootstrap simulation of runs 19, 38, 29",0,,False
96,"Figure 3: Kernel Density Estimation simulation of run 11, and Gaussian Mixture and Gamma simulation of runs 11, 19",0,,False
97,"yijk , i + bj + cij + ijk",0,,False
98,(2),0,,False
99,"bj  N (0, 12), cij  N (0, 22), ijk  N (0, 2)",0,,False
100,"where yijk is the value of an evaluation measure calculated on topic j for system i on the document collection sample k, i is the effect of system i, bj is the effect of topic j, cij is the interaction effect, and ijk is the residual error. As it can be seen the interaction effect is a random effect normally distributed with zero mean and 22 variance. We can fit Eq. 2 in R by,",0,,False
101,"> lme1 <- lme(y~system, data,""df, random"",~1|topic/system)",0,,False
102,"The output of R's lme function is,",0,,False
103,> summary(lme1),0,,False
104,Linear mixed-effects model fit by REML,0,,False
105,Data: df,0,,False
106,AIC,0,,False
107,BIC logLik,0,,False
108,2173.5 2197.719 -1081.75,0,,False
109,Random effects: Formula: ~1 | topic (Intercept),0,,False
110,StdDev: 1.539644,0,,False
111,Formula: ~1 | system %in% topic (Intercept) Residual,0,,False
112,StdDev: 0.6191864 0.6386645,0,,False
113,Fixed effects: y ~ system,0,,False
114,Value Std.Error DF t-value p-value,0,,False
115,(Intercept) -1.3445 0.2438470 846 -5.514077 0.0000,0,,False
116,system2,0,,False
117,0.0999 0.1343512 46 0.744112 0.4606,0,,False
118,"Let's focus on the Random effects section of the output. We can observe that 12, the variance of the topic effect distribution, is estimated to 1.539644, 22, the variance of the system/topic interaction effect distribution is estimated to 0.6191864, while the variance of the error, 2, is estimated to 0.6386645.3 If we move to the Fixed effects section",0,,False
119,"3Note that even though system %in% topic appears as a nested effect of the systems within each topic, the function in fact calculates the interaction effect, because of the crossed",0,,False
120,896,0,,False
121,"of the above results, we can observe that the effect of the first system is estimated to -1.3445 (intercept), while the difference to the second system of the pair under comparison is estimated to 0.0999 (system2). The lme function, further performs a significance test to examine whether this difference is statistically significant. The p-value over the system2 line above designates that the difference between the two systems is not statistically significant.",0,,False
122,"So far, we have assumed that the errors are independent and identically normally distributed, with mean zero and variance 2, and they are independent of the random effects. This implies that error over all system/topic pairs are i.i.d. which is the opposite of what we have demonstrated through Figures 1 and 3. To further examine whether this assumption is valid we first plot the boxplots of residual errors grouped by system/topic pairs (Figure 4). This plot is useful for verifying that the errors are centered at zero (i.e., E[] ,"" 0), have constant variance across groups (V ar(ijk) "","" 2), and are independent of the group levels. Figure 4 indicates that the residuals are indeed centered at zero, but that the variability changes with system/topic group.4""",0,,False
123,We can further verify this by plotting the standardized residuals against the fitted values (left hand-side plot in Figure 5). As one can observe residuals vary with different variance across the different groups.,0,,False
124,"A more general model to better represent our measurements allows different variances by system/topic groups for the errors. We can define such a model by,",0,,False
125,"yijk , i + bj + cij + ijk",0,,False
126,(3),0,,False
127,"bj  N (0, 12), cij  N (0, 22), ijk  N (0, i2j )",0,,False
128,"This model is called ""heteroscedastic"", while the model of equal variances per system/topic group (Eq. 2) is call ""homoscedastic"". The difference between the heteroscedastic model in Eq. 3 and the homoscedastic model in Eq. 2 is the fact that the heteroscedastic model allows for different variances i2j for each system i topic j group of measurements. This can be expressed in lme by a varIdent parameter that defines the grouping factors.",0,,False
129,"> lme2 <- lme(y~system, data,""df, random"",""~1|topic/system, weights"",varIdent(form,~1|topic*system))",0,,False
130,"The results of fitting the heteroscedastic model in the data can be viewed below,",0,,False
131,> summary(lme2),0,,False
132,Linear mixed-effects model fit by REML,0,,False
133,Data: df,0,,False
134,AIC,0,,False
135,BIC logLik,0,,False
136,1080.872 1555.559 -442.4358,0,,False
137,Random effects: Formula: ~1 | topic (Intercept),0,,False
138,StdDev: 1.447164,0,,False
139,"design of our experiment, in which the two systems under comparison are run against all topics. 4For clarity purposes we only plot the residuals of the two systems against the first 10 topics.",0,,False
140,Formula: ~1 | system %in% topic (Intercept) Residual,0,,False
141,StdDev: 0.4537618 0.186183,0,,False
142,Variance function:,0,,False
143,Structure: Different standard deviations per stratum,0,,False
144,Formula: ~1 | topic * system,0,,False
145,Parameter estimates:,0,,False
146,1*1,0,,False
147,1*2,0,,False
148,2*1,0,,False
149,2*2 ...,0,,False
150,1.0000000 1.6108387 1.3969085 1.5405710 ...,0,,False
151,Fixed effects: y ~ system,0,,False
152,Value Std.Error DF t-value p-value,0,,False
153,(Intercept) -1.4385 0.22266286 846 -6.460817 0.0000,0,,False
154,system2,0,,False
155,0.1834 0.09844907 46 1.863342 0.0688,0,,False
156,"Apart from the random and fixed effects section, there is a Variance function section. The values of this section give the ratio of the standard error of each system/topic group to the standard error of the first system/topic group. Thus, for instance, the standard error for the second system when run against the first topic is about 60% more than that of the first system when run against the first topic, while the standard error for the first system run against the second topic is 40% more than the standard error running against the first topic. The standard error of the first system/topic group is itself presented as the Residual standard error, in the random effect section (0.186183).",0,,False
157,"We can confirm the adequacy of the heteroscedastic fit by re-examining plots of the standardized residuals versus the fitted values by system/topic. As expected, the standardized residuals in each block now have about the same variability, Figure 5 (right).",1,ad,True
158,The need for a heteroscedastic model can be formally tested with the anova method.,0,,False
159,"anova(lme1,lme2)",0,,False
160,Model df AIC BIC logLik Test L.Ratio p-value,0,,False
161,lme1,0,,False
162,1 5 2173 2197 -1081,0,,False
163,lme2,0,,False
164,2 98 1080 1555 -442 1 vs 2 1278.62 <.0001,0,,False
165,"The anova method compares the two models in terms of Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and Log Likelihood (logLik), with the smaller the better in all cases. Further it performs a significance test on the likelihood ratio of the two models. The very small p-value of the likelihood ratio statistic confirms that the heteroscedastic model explains the data significantly better than the homoscedastic model.",0,,False
166,8. EXPERIMENTAL ANALYSIS,0,,False
167,"We have seen that the proposed view of evaluation experiments suggests that not all individual topic measurements have the same status. That is, each one carries its own noise variance, and these vary significantly between topics. This suggests in turn that the common approaches to statistical significance in comparing systems are over-simplified. Tests like the t-test assign the same status (give the same weight) to every individual topic measurement.",0,,False
168,"The view proposed by Cormack and Lynam [3] is that each topic should in some sense be regarded as a separate experiment, and that the summarisation of results over sets",0,,False
169,897,0,,False
170,Figure 4: Boxplots of residuals for homoscedastic mixed-effects model by system/topic.,0,,False
171,Figure 5: Standardized residuals versus fitted values per system/topic for the homoscedastic (left) and heteroscedastic (right) mixed-effects model.,0,,False
172,898,0,,False
173,Figure 6: P-values for the different significance tests (GMG simulations).,0,,False
174,"of topics is analogous to a meta-analysis of a group of experiments. We proposed an approach to statistical significance testing that takes a step in this direction, using mixed-effects models. For each topic/run pair, we run the simulation 10 times, to obtain 10 values of the metric. In mixed-effects terms, we regard these values as repeated measurements, in the sense that the measurements have been taken at random from a population of possible values (i.e. a population of possible test collections). We are not interested in the differences between these individual values within the groups.",0,,False
175,"The effect of taking multiple values per topic/run combination is to indicate to the mixed-effects model how reliable or unreliable each topic is in determining the effectiveness of this run. One obvious issue is that these measurements are not real ­ they are the result of simulations. We can appeal to the evidence of Section 6, that they are consistent with the observed results. We need to use the simulations rather than the directly observed results because only here do we have the necessary repetitions.",0,,False
176,"We compare the (heteroscedastic and homoscedastic) mixedeffects models with the t-test. Since we use simulated data for mixed-effects models, we also use the same data for the t-test: this is performed on the means of the 10 simulated samples for each topic. Again, since this is not the usual way to run a t-test on IR evaluation data, we also run it in the usual way, on the observed results.",0,,False
177,"In order to measure the power of the test, we follow Sakai [10] in performing all possible pairwise tests, sorting by pvalue, and plotting the number of pairs against the p-values: see Figure 6 for the Gaussian Mixture and Gamma simulation; similar plots are obtained by the other two simulation model (note that in this section we focus on the GMG simulations, which show the most discrepancy from the observed results). All these are 2-sided tests. We see that the all four tests appear to be equally powerful, as indicated by how much of the curve is below a significance threshold such as 0.05. Hence, decomposing and separately modeling the precision of each topic separately does not lead to a change in the power of the test.",1,ad,True
178,What is more interesting however is to examine the disagreements between the p-values calculated by the different tests that could lead in disagreements between which system,1,ad,True
179,"pairs each test consider as significantly different in performance. Figure 7 presents the scatter plot of the p-values for the t-test on the means of the simulated measurements, the homoscedastic mixed-effects model and the heteroscedastic mixed-effects model against the t-test over the actual logit AP values (without any simulation). This again uses the evaluation scores over the Gaussian Mixture and Gamma simulation. Each point is a pair of systems under comparison. As it can be seen from the plots, there is a significant disagreement in the p-values computed by the different tests (but note that the visual effect is somewhat exaggerated because of the large number of superimposed points in the lower left). The t-test over the actual logit AP values agrees best with the heteroscedastic model. This is expected since the t-test over the mean of the repeated measurements per topic and the homoscedastic model do not handle correctly the extra source of variability. The points in the areas to the left and the bottom of the dashed lines in the plots of Figure 7 represent system pairs that have been found significantly different by at least one of the contrasted tests. If we focus at the heteroscedastic model vs t-test plot the percentage of disagreement between the two tests with respect to those system pairs that are found significant by at least one of the tests reaches about 20%. Examining the system pairs for which the two tests disagree is of particular interest; however we leave it as future work.",1,AP,True
180,"We also show contingency tables indicating agreement or otherwise between the tests at the threshold of 0.05 (Table 1). The results over all three simulation methods are summarized in these tables. Similarly to the Figures 6 and 7 we see that all test are comparable regarding their power, while there are certain system pairs for which different tests disagree with each other. Note that there is a disagreement in the results of the t-test on actual values across the three simulating methods; e.g. in the case of BST and KDE 303 comparisons have been found statistically significant according to the t-test, while for the case of GMG 318 are statistically significant. When simulating sampling the corpus there are topics for which the simulations produce the same logitAP value at every iteration. These topics are excluded when comparing two systems since the homoscedastic and heteroscedastic models cannot address zero variance within a topic, even though the t-test is not affected by this. Different simulation methods lead to different sets of zero variance topics and hence the t-test across different simulation methods is run over different topic sets.",1,AP,True
181,9. CONCLUSIONS,0,,False
182,"The experiments reported here reinforce the notion that we need to worry about the statistical status and validity of evaluation results on individual topics. The usual approach in IR evaluation, of regarding each individual topic measurement as an indivisible atom, is not consistent with our use of particular collections of documents. The size of the document collections we use is actually no insulation against statistical issues, since measurements for individual topics typically depend on the small sets of documents judged relevant.",0,,False
183,"A good approach to this issue has the potential to inform the measurement of statistical significance in IR evaluations. We provide some evidence that it will affect significance tests and inferences of retrieval quality. The evidence presented is by no means complete, and many other developments and",0,,False
184,899,0,,False
185,(BST),0,,False
186,Figure 7: Scatter plots of p-values for the different tests (GMG simulation),0,,False
187,Heterosced. 10 samples,0,,False
188,p < 0.05 p  0.05,0,,False
189,t-test on actual values,0,,False
190,p < 0.05 p  0.05,0,,False
191,287,0,,False
192,15,0,,False
193,16,0,,False
194,462,0,,False
195,t-test on mean over 10 samples,0,,False
196,p < 0.05 p  0.05,0,,False
197,t-test on actual values,0,,False
198,p < 0.05 p  0.05,0,,False
199,291,0,,False
200,2,0,,False
201,12,0,,False
202,475,0,,False
203,(GMG),0,,False
204,Heterosced. p < 0.05 288,0,,False
205,10 samples p  0.05,0,,False
206,37,0,,False
207,20 435,0,,False
208,t-test on mean p < 0.05 281,0,,False
209,9,0,,False
210,over 10 samples p  0.05,0,,False
211,44,0,,False
212,446,0,,False
213,(KDE),0,,False
214,Heterosced. p < 0.05 292,0,,False
215,10 samples p  0.05,0,,False
216,11,0,,False
217,22 455,0,,False
218,t-test on mean p < 0.05 288,0,,False
219,3,0,,False
220,over 10 samples p  0.05,0,,False
221,15,0,,False
222,474,0,,False
223,"Table 1: Contingency table on the agreement of different significance tests for 780 pairs of systems, on 3 simulations: (BST) Bootstrap, (GMG) Gaussian mixture + gamma, (KDE) Kernel Density Estimation.",0,,False
224,"experiments are required. One future direction would be to investigate whether we can characterise the high-variance topics in some simple way. However, the work reported here is seen as a step towards better understanding of this particular source of error in IR evaluation.",0,,False
225,Acknowledgments,0,,False
226,"We would like to thank Jaap Kamps for valuable discussions on an earlier version of this paper. Further, we gratefully acknowledge the support provided by the European Commission grant FP7-PEOPLE-2009-IIF-254562.",0,,False
227,10. REFERENCES,0,,False
228,"[1] David Banks, Paul Over, and Nien-Fan Zhang. Blind men and elephants: Six approaches to TREC data. Inf. Retr., 1(1-2):7­34, 1999.",1,TREC,True
229,"[2] Ben Carterette, Evangelos Kanoulas, and Emine Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM 2011, pages 611­620. ACM, 2011.",0,,False
230,"[3] Gordon V. Cormack and Thomas R. Lynam. Statistical precision of information retrieval evaluation. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 533­540. ACM, 2006.",0,,False
231,"[4] David Hawking and Stephen E. Robertson. On collection size and retrieval effectiveness. Inf. Retr., 6(1):99­105, 2003.",0,,False
232,[5] Philipp Jarnet. Statistics-KernelEstimation.,0,,False
233,http://search.cpan.org/~janert/ Statistics-KernelEstimation/.,0,,False
234,"[6] Evangelos Kanoulas, Virgiliu Pavlu, Keshi Dai, and Javed A. Aslam. Modeling the score distributions of relevant and non-relevant documents. In Proceedings of Second International Conference on the Theory of Information Retrieval, ICTIR 2009, pages 152­163. Springer, 2009.",0,,False
235,"[7] Jose C. Pinheiro and Douglas M. Bates. Mixed-Effects Models in S and S-PLUS. Statistics and computing. Springer-Verlag New York, Inc., 2000.",0,,False
236,"[8] Stephen Robertson. On smoothing average precision. In Proceedings of the 34th European Conference on IR Research, ECIR 2012, pages 158­169. Springer, 2012.",0,,False
237,"[9] Stephen E. Robertson. On document populations and measures of ir effectiveness. In Studies in theory of information retrieval (Proceedings of ICTIR 2007), pages 9­12, 2007.",0,,False
238,"[10] Tetsuya Sakai. Evaluating evaluation metrics based on the bootstrap. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 525­532. ACM, 2006.",0,,False
239,"[11] Vishwa Vinay, Ingemar J. Cox, Natasa Milic-Frayling, and Kenneth R. Wood. On ranking the effectiveness of searches. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 398­404. ACM, 2006.",0,,False
240,900,0,,False
241,,0,,False

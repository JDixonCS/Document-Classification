,sentence,label,data,regex
0,Short Research Papers 1B: Recommendation and Evaluation,0,,False
1,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
2,Unbiased Low-Variance Estimators for Precision and Related Information Retrieval Effectiveness Measures,0,,False
3,Gordon V. Cormack,0,,False
4,University of Waterloo,0,,False
5,ABSTRACT,0,,False
6,"This work describes an estimator from which unbiased measurements of precision, rank-biased precision, and cumulative gain may be derived from a uniform or non-uniform sample of relevance assessments. Adversarial testing supports the theory that our estimator yields unbiased low-variance measurements from sparse samples, even when used to measure results that are qualitatively different from those returned by known information retrieval methods. Our results suggest that test collections using sampling to select documents for relevance assessment yield more accurate measurements than test collections using pooling, especially for the results of retrieval methods not contributing to the pool.",0,,False
7,"ACM Reference Format: Gordon V. Cormack and Maura R. Grossman. 2019. Unbiased Low-Variance Estimators for Precision and Related Information Retrieval Effectiveness Measures. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10. 1145/3331184.3331355",0,,False
8,1 INTRODUCTION,1,DUC,True
9,"The thesis of this work is that information retrieval (IR) test collections [5] should use relevance assessments for documents selected by statistical sampling, not depth-k pooling or dynamic pooling methods like hedge [1]. To this end, we define the dynamic (dyn) estimator1 for precision at cutoff (P@k), which is easily generalized to rank-biased precision (RBP) and discounted cumulative gain (DCG). In contrast to the well-known inferred (inf ) and extended inferred (xinf ) estimators [8], dyn is unbiased. In comparison to the statistical (stat) estimators [4], dyn has substantially lower variance for a given assessment budget.",0,,False
10,"When used to estimate mean P@k (MP@k) and other measures over a set of topics interpreted as a sample of a larger population of topics, the dyn estimator, coupled with an amenable sampling strategy, and a larger sample of topics, can achieve lower variance than pooling methods, for a given assessment budget.",0,,False
11,"dyn is a Horvitz-Thompson estimator [3] for the difference between the true value of P@k and a learned prior estimate. In the special case where the prior estimate is fixed at 0, dyn P@k is equivalent to stat P@k, which is also unbiased but, according to the theory underlying this work, higher variance.",0,,False
12,1See open source dyn_eval implementation at cormack.uwaterloo.ca/sample.,0,,False
13,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6172-9/19/07. https://doi.org/10.1145/3331184.3331355",1,ad,True
14,Maura R. Grossman,0,,False
15,University of Waterloo,0,,False
16,"inf P@k and xinf P@k, in contrast, compute a separate esti-",0,,False
17,"mate for each stratum of sampled assessments. However, no such",0,,False
18,estimate is possible whenever a retrieval result to be measured,0,,False
19,contains none of the assessed documents from a particular stratum.,0,,False
20,"To resolve this singularity, inf uses Lindstone smoothing, which",0,,False
21,effectively substitutes a default constant value c when the estimate,0,,False
22,would otherwise be 0 . The published descriptions of inf and xinf,0,,False
23,0,0,,False
24,"use c , 1 ; the reference implementation used for this study uses",0,,False
25,2,0,,False
26,c,0,,False
27,",",0,,False
28,1 3,0,,False
29,.,0,,False
30,The,0,,False
31,net,0,,False
32,effect,0,,False
33,is,0,,False
34,that,0,,False
35,inf,0,,False
36,measurements are biased toward,0,,False
37,"the value c, with small values of P@k being overestimated, and",0,,False
38,large values underestimated.,0,,False
39,The theory that an estimator is unbiased may be falsified by iden-,0,,False
40,"tifying any possible combination of topics, documents, relevance",0,,False
41,"assessment sample, and retrieval results, for which it yields a biased",0,,False
42,estimate. The experiment described here takes as ground truth the,0,,False
43,"documents, topics, and relevance assessments from the TREC 8",1,TREC,True
44,"Ad Hoc test collection [6], assuming the topics to be a random",0,,False
45,sample drawn from a population of topics with precisely the same,0,,False
46,"mean and sample variance, and the assessments to be complete and",0,,False
47,"infallible. To measure bias, we consider two sets of retrieval results:",0,,False
48,"the 129 ""TREC runs"" submitted to TREC for evaluation, and 129",1,TREC,True
49,dual runs engineered to have precisely the same true P@k as the,0,,False
50,"TREC runs, in 1-1 correspondence. The dual runs were formed by",1,TREC,True
51,randomly permuting the ranks of the relevant documents in each,0,,False
52,"run, while preserving the ranks of non-relevant documents.",0,,False
53,"The results of this adversarial testing show no bias for dyn or stat, small but significant bias for xinf (which subsumes inf ), and",1,ad,True
54,"very large bias, both within and between the TREC and dual runs,",1,TREC,True
55,for depth-k pooling and hedge. To address the argument that biased,1,ad,True
56,"measurements do not matter as long as they accurately rank the relative effectiveness of the runs, we calculate median  correla-",0,,False
57,tion between the rankings achieved by repeated measurements of,0,,False
58,"MP@k versus ground truth, for the TREC runs, the dual runs, and",1,TREC,True
59,their union.,0,,False
60,2 MEASURING P@K,0,,False
61,"Given a document d  D and a topic t  T , binary relevance",0,,False
62,"rel(d) , 1 indicates that an infallible assessor would judge d rel-",0,,False
63,"evant to t; rel(d) , 0 indicates otherwise. Given a ranked list of",0,,False
64,"documents r ,"" r1r2 · · · rn from D and a topic t, our concern is how""",0,,False
65,1,0,,False
66,"best to measure P@k , k",0,,False
67,"min(k , n ) i ,1",0,,False
68,rel(ri,0,,False
69,"),",0,,False
70,understanding,0,,False
71,that,0,,False
72,the,0,,False
73,infallible assessor is a hypothetical entity whose judgments can at,0,,False
74,"best be approximated by real assessors, under controlled conditions,",0,,False
75,"rendering rel(d) for a subset J of all possible d. In this work, we",0,,False
76,"assume the fiction that rel(d) ,",0,,False
77,rel(d ),0,,False
78,(d  J ) .,0,,False
79,0,0,,False
80,(d / J ),0,,False
81,If J is a statistical sample of D drawn without replacement such,0,,False
82,"that each d  D is drawn with prior probability  (d) , Pr[d  J ] >",0,,False
83,945,0,,False
84,Short Research Papers 1B: Recommendation and Evaluation,0,,False
85,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
86,"0, we have the unbiased stat estimator:",0,,False
87,stat P@k,0,,False
88,",",0,,False
89,1 k,0,,False
90,"min(k , n ) i ,1",0,,False
91,rel(ri )  (ri ),0,,False
92,.,0,,False
93,"The dyn estimator harnesses a model M(d) estimating Pr[rel(d) ,",0,,False
94,1]:,0,,False
95,"min(k , n )",0,,False
96,1,0,,False
97,1,0,,False
98,"dyn P@k , k",0,,False
99,"i,1 M(ri ) +  (ri ) ·",0,,False
100,rel(ri ) - M(ri ) 0,0,,False
101,(ri  J ) . (ri / J ),0,,False
102,"Provided M(d) is independent of the outcome d  J , dyn P@k is unbiased. If J is a stratified sample drawn without replacement, this constraint is met when M(d) is derived from {rel(d )|d   J \ strat(d)}, where strat(d) is the stratum from which d is drawn.",0,,False
103,"A given IR method yields a different ranking r (t), with a particular P@k, denoted P@k(t), for any given topic t  T . MP@k ,"" E[P@k(T )] quantifies the effectiveness of the method. Given a uniform random sample T from T , the dyn unbiased estimate of""",0,,False
104,MP@k is,0,,False
105,"1 dyn MP@k , |T | t T dyn P@k(t) .",0,,False
106,3 STRATIFIED SAMPLING,0,,False
107,"The simplest sampling strategy that we consider divides J into equal-sized strata with equal sampling rates. For this strategy, M is learned using cross-validation, holding out each stratum in turn,",0,,False
108,"and using the remaining strata for training. In the present study we used logistic regression to maximize (logit) likelihood L(d) over the training examples. We calibrated these estimates by adding a constant prior (log odds) p, and converting to probability:",1,ad,True
109,"1 M(d) , 1 + exp(-(p + L(d))) . p was determined numerically to solve",0,,False
110,"M(d) ,",0,,False
111,d,0,,False
112,"r el (d )  (d) , E",0,,False
113,d,0,,False
114,"rel(d) ,",0,,False
115,so that the predicted number of relevant documents in the training examples would match the sample estimate.,0,,False
116,4 PROPORTIONAL TO SIZE SAMPLING,0,,False
117,"Variance can be reduced using a model P(d) that predicts Pr[rel(d)] prior to determining  (d), and, as nearly as possible, makes  (d)  P(d). P may be derived from any relevance-ranking method. In",0,,False
118,the present study we used reciprocal-rank fusion of the rankings submitted to TREC. We partitioned D into N strata of exponentially,1,TREC,True
119,"increasing size, so that higher-ranked documents were assigned to smaller strata. An equal number of documents n were drawn from each stratum Si , and the exponential growth rate  calculated numerically to cover D when the size of the smallest stratum |S0|, s:",0,,False
120,N -1,0,,False
121," , min  . s · s · (1 + )i  |D|",0,,False
122,"i ,0",0,,False
123,"|Si |, s · (1 +  )i (i  0..N - 2)",0,,False
124,N -2,0,,False
125,"|SN -2 |, |D|- |Si | .",0,,False
126,"i ,0",0,,False
127,5 TESTING BIAS AND VARIANCE,0,,False
128,Error err is the difference between a measurement est and ground,0,,False
129,"truth tru. Bias b is the average of err over repeated measurements. MSE is the average of err2. Variance e2rr ,"" MSE - b2. To test our claim that dyn MP@k is unbiased, ground truth need not be perfect,""",0,,False
130,"and the retrieval results to be measured need not be derived from real IR systems, as long as they are independent of J .",0,,False
131,"In evaluating an estimator, it is necessary to consider that MP@k is defined over a population T , whereas dyn MP@k and other estimators are derived from a set of topics T , deemed to be a random sample of T . The expectation, and therefore the bias of an estimate, is the same, whether we consider it to be an estimate of MP@k ,"" E P@k(T ) or of E P@k(T ). On the other hand, even ground truth for T has non-zero variance T2 when used as an estimator for MP@k :""",0,,False
132,T2,0,,False
133,",",0,,False
134,1 |T |,0,,False
135,Var P@k(T ) ,0,,False
136,1 |T |-1,0,,False
137,Var P@k(T ) .,0,,False
138,"The inequality holds in expectation, and for the purposes of ground",0,,False
139,"truth, we deem it equal:",0,,False
140,T2,0,,False
141,",",0,,False
142,1 |T |-1,0,,False
143,Var P@k(T ) .,0,,False
144,The overall variance of an estimator is therefore,0,,False
145,"e2st ,"" e2rr + T2 . Similarly, RMSE depends on what is being estimated:""",0,,False
146,"RMSerr ,",0,,False
147,2,0,,False
148,b,0,,False
149,+e2rr,0,,False
150,",",0,,False
151,"RMSET ,"" T ,""",0,,False
152,"RMSEest , b2 +e2rr + T2 .",0,,False
153,6 EXPERIMENT,0,,False
154,"Using the TREC 8 data as ground truth, we compared the bias and variance of dyn MP@10 to competing statistical and non-statistical estimators with two different assessment budgets: 100 assessments per topic, and 400 assessments per topic. We also compared the effect of quadrupling the number of topics, as an alternative to quadrupling the per-topic assessment budget, given a larger assessment budget.",1,TREC,True
155,"Ground truth was derived from the 86, 830 relevance assessments (qrels) from the TREC 8 Ad Hoc task. For each of 50 topics, documents were selected for assessment using depth-100 pooling for 71 of the 129 runs. For each topic, we defined ground truth rel(d) , 1 if d was assessed and judged relevant; otherwise rel(d) ,"" 0. An unbiased estimator should be able to estimate measurements with regard to this ground truth. For reference, we compared RMSET achieved by the estimators to ground-truth RMSET calculated using the 86, 830 ground-truth assessments (1, 737, on average, per topic).""",1,TREC,True
156,"To compute dyn, stat, and trec_eval2 estimates, we used our own implementation, available for download as the dyn_eval toolkit,1 which is input/output compatible with trec_eval, and has",1,trec,True
157,2See trec.nist.gov/trec_eval/.,1,trec,True
158,946,0,,False
159,Short Research Papers 1B: Recommendation and Evaluation,0,,False
160,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
161,Estimator,0,,False
162,xinf stat dyn xinf stat dyn depth-5 hedge,0,,False
163,xinf stat dyn xinf stat dyn depth-20 hedge,0,,False
164,TREC,1,TREC,True
165,Sample,0,,False
166,Uniform Uniform Uniform,0,,False
167,PPS PPS PPS Non-random Non-random,0,,False
168,Uniform Uniform Uniform,0,,False
169,PPS PPS PPS Non-random Non-random,0,,False
170,Exhaustive,0,,False
171,Systematic Bias RMS Summary over TREC Runs,1,TREC,True
172,b,0,,False
173,b,0,,False
174,err,0,,False
175,b,0,,False
176,100 assessments per topic,0,,False
177,RMSerr,0,,False
178,-0.0041 0.0004 0.0707 0.0443 0.0834,0,,False
179,0.0019 0.0010 0.0078 0.1154 0.1156,0,,False
180,-0.0006 0.0008 0.0061 0.0941 0.0943,0,,False
181,0.0427 0.0001 0.0859 0.0103 0.0865,0,,False
182,0.0008 0.0003 0.0029 0.0311 0.0312,0,,False
183,0.0002 0.0002 0.0031 0.0282 0.0284,0,,False
184,-0.0258 0.0000 0.0349 0.0000 0.0349,0,,False
185,-0.0296 0.0000 0.0330 0.0000 0.0330,0,,False
186,400 assessments per topic,0,,False
187,0.0017 0.0003 0.0113 0.0365 0.0382,0,,False
188,0.0002 0.0005 0.0032 0.0514 0.0515,0,,False
189,-0.0002 0.0003 0.0027 0.0383 0.0384,0,,False
190,0.0277 0.0001 0.0517 0.0065 0.0521,0,,False
191,0.0002 0.0001 0.0011 0.0107 0.0107,0,,False
192,0.0000 0.0001 0.0007 0.0082 0.0082,0,,False
193,-0.0011 0.0000 0.0034 0.0000 -0.0014 0.0000 0.0029 0.0000,0,,False
194,1737 assessments per topic 0.0000 0.0000 0.0000 0.0000,0,,False
195,0.0034 0.0029,0,,False
196,0.0000,0,,False
197,"RMSE |T |, 50 |T |, 200",0,,False
198,0.0928 0.1226 0.1027 0.0955 0.0513 0.0496 0.0535 0.0523,0,,False
199,0.0768 0.0616 0.0515 0.0883 0.0256 0.0248 0.0403 0.0386,0,,False
200,0.0558 0.0656 0.0559 0.0661 0.0420 0.0415 0.0408 0.0407,0,,False
201,0.0294 0.0328 0.0280 0.0556 0.0209 0.0206 0.0205 0.0204,0,,False
202,0.0402 0.0202,0,,False
203,Table 1: TREC Runs,1,TREC,True
204,Estimator,0,,False
205,xinf stat dyn xinf stat dyn depth-5 hedge,0,,False
206,xinf stat dyn xinf stat dyn depth-20 hedge,0,,False
207,TREC,1,TREC,True
208,Sample,0,,False
209,Uniform Uniform Uniform,0,,False
210,PPS PPS PPS Non-random Non-random,0,,False
211,Uniform Uniform Uniform,0,,False
212,PPS PPS PPS Non-random Non-random,0,,False
213,Exhaustive,0,,False
214,Systematic Bias RMS Summary over Dual Runs,0,,False
215,b,0,,False
216,b,0,,False
217,b,0,,False
218,err,0,,False
219,100 assessments per topic,0,,False
220,RMSerr,0,,False
221,-0.0043 0.0004 0.0709 0.0454 0.0842,0,,False
222,-0.0001 0.0010 0.0112 0.1163 0.1168,0,,False
223,-0.0018 0.0009 0.0093 0.0997 0.1001,0,,False
224,-0.0653 0.0001 0.1311 0.0116 0.1316,0,,False
225,0.0018 0.0006 0.0071 0.0734 0.0008 0.0004 0.0045 0.0465,0,,False
226,0.0737 0.0468,0,,False
227,-0.2202 0.0000 0.2381 0.0000 0.2381,0,,False
228,-0.1277 0.0000 0.1375 0.0000 0.1375,0,,False
229,400 assessments per topic,0,,False
230,0.0006 0.0003 0.0114 0.0356 0.0374,0,,False
231,-0.0020 0.0004 0.0049 0.0510 0.0512,0,,False
232,-0.0012 0.0004 0.0036 0.0405 0.0407,0,,False
233,-0.0261 0.0001 0.0698 0.0092 0.0704,0,,False
234,-0.0006 0.0002 0.0027 0.0264 0.0266,0,,False
235,-0.0003 0.0001 0.0016 0.0162 0.0163,0,,False
236,-0.1088 0.0000 0.1196 0.0000 0.1196,0,,False
237,-0.0526 0.0000 0.0574 0.0000 0.0574,0,,False
238,1737 assessments per topic 0.0000 0.0000 0.0000 0.0000,0,,False
239,0.0000,0,,False
240,"RMSE |T |, 50 |T |, 200",0,,False
241,0.0935 0.1237 0.1081 0.1377 0.0842 0.0619 0.2415 0.1434,0,,False
242,0.0772 0.0626 0.0546 0.1327 0.0425 0.0311 0.2389 0.1390,0,,False
243,0.0552 0.0654 0.0575 0.0813 0.0485 0.0438 0.1263 0.0703,0,,False
244,0.0292 0.0329 0.0288 0.0728 0.0243 0.0218 0.1213 0.0609,0,,False
245,0.0402 0.0202,0,,False
246,Table 2: Dual Runs,0,,False
247,"been verified to produce identical results. To render xinf estimates, we used the reference implementation from TREC.3",1,TREC,True
248,3See trec.nist.gov/data/clinical/sample_eval.pl.,1,trec,True
249,"For statistical estimation, we considered two sampling strategies: equal-probability sampling, and probability proportional-to-size sampling (PPS) with 20 strata, as described in Section 4. For nonstatistical estimation, we considered two strategies: depth-k pooling",0,,False
250,947,0,,False
251,Short Research Papers 1B: Recommendation and Evaluation,0,,False
252,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
253,Measure,0,,False
254,"MRBP ( , 0.9) MAP",1,MAP,True
255,NDCG,0,,False
256,b 0.0000 0.0015 0.0032,0,,False
257,dyn ,0,,False
258,b,0,,False
259,0.0001 0.0000 0.0001,0,,False
260,RMSE 0.0360 0.0283 0.0323,0,,False
261,TREC RMSE 0.0351 0.0280 0.0312,1,TREC,True
262,"Table 3: dyn MRBP, MAP, and NDCG using PSS and 400 assessments per topic, compared to TREC ground truth.",1,MAP,True
263,"and hedge, a best-of-breed dynamic pooling method. For depth-k pooling, we were not able to enforce a strict budget of 100 or 400 assessments per topic; as proxies we used k , 5 and k ,"" 20, with 110 and 385 assessments per topic, on average.""",0,,False
264,"For each of the 129 TREC runs and each of 129 dual runs, we measured MP@10 100 times using each statistical estimation strategy, calculating bias, variance, and error as described in Section 5 above. For the non-statistical strategies, we measured MP@10 only once, since er r , 0.",1,TREC,True
265,Results over sets of runs are summarized by:,0,,False
266,"· b: The mean bias over all runs, an indicator of systematic bias;",0,,False
267,·  : the (im)precision of the systematic bias estimate;,0,,False
268,b,0,,False
269,"· RMS b: run-specific bias, over and above systematic bias; · RMS err : the (im)precision of the P@10(T ) estimate; · RMS err: the (in)accuracy of the P@10(T ) estimate; · RMSEest: the (in) accuracy of the P@10(T ) estimate.",0,,False
270,"Tables 1 and 2 shows these summary statistics for the TREC and dual runs. The results before the top break in each table use a budget of 100 assessments per topic; the results after the break use a budget of 400 assessments per topic, and the TREC gold standard uses 1, 737 assessments per topic.",1,TREC,True
271,"We see that, as predicted, the stat and dyn estimates are unbiased, while xinf shows small but significant bias, and the non-statistical methods show substantial bias in favour of the TREC runs. PPS improves the dyn and stat estimates, but harms xinf. For a total assessment budget of 20, 000 documents, which is at the low end of typical TREC efforts, a budget of 100 assessments per topic for 200 topics allows dyn to surpass the accuracy of exhaustive assessment for 50 topics, for less than a quarter of the assessment effort. A budget of 400 assessments per topic for 50 topics, yields insubstantially different accuracy from exhaustive assessment. A budget of 400 assessments per topic for 200 topics yields insubstantially different accuracy from exhaustive assessment for 200 topics, with less than half the effort of exhaustive assessment for 50 topics.",1,TREC,True
272,"7 RBP, MAP, AND NDCG",1,MAP,True
273,"The dyn MRBP estimator is a straightforward modification to dyn MP@k. dyn MAP and dyn NDCG are somewhat biased because they divide by a normalization factor, which is also an estimate. But the normalization factor is invariant between runs, and therefore has minimal impact. Table 3 shows the results for these estimators compared to exhaustive assessment. As expected, the dyn MRBP shows no significant bias, while dyn MAP and dyn MNDCG show",1,MAP,True
274,"significant but insubstantial bias, with the net effect that all estimates have comparable accuracy to exhaustive assessment.",0,,False
275,8 DISCUSSION AND CONCLUSION,0,,False
276,"It has been argued that bias does not matter as long as runs are properly ranked. We combined the TREC and dual runs, and ranked them using dyn MP@10, depth-20 pooling, and hedge, achieving rank correlations  ,"" 0.934,  "","" 0.715, and  "","" 0.826, respectively. In contrast, when only the TREC runs are considered, the correlations are  "","" 0.972,  "","" 0.996, and  "","" 0.995. These results call into question the viability of using rank correlation over known runs as a measure of test collection accuracy.  , like RMSE and other proposed measures, conflates bias and variance. We really have no idea whether differences in these measures reflect bias or variance.""",1,TREC,True
277,"An unbiased estimator like dyn MP@k, dyn MRBP, or dyn DCG imposes no limit on the number of topics that could be assessed, splitting the assessment budget among them. Practical considerations like the overhead of obtaining and vetting topics are likely to dominate. A minimally biased estimator like dyn MAP or dyn NDCG constrains the number of topics that may be used to a number such that its bias is insubstantial compared to variance. Even so, the optimal number of topics appears to be substantially higher than 50.",1,ad,True
278,"xinf, the most commonly used estimator, shows significant bias, occasioning substantial effort to discover amenable sampling strategies [7]. stat, on the other hand, shows substantial variance. Our design of dyn was directly inspired by these estimators, adopting the ""default value"" (albeit learned rather than constant) of xinf, and the Horvitz-Thompson estimator of stat. Our approach to stratification and our motivating application was Dynamic Sampling [2], from which dyn derives its name. Our results suggest that dyn should yield better results than stat for this purpose.",1,ad,True
279,"There is no limit on the number of documents or the number of relevant documents per topic to which dyn may be applied. To keep variance reasonable, it is necessary to identify a sample space that contains substantially all of the relevant documents. Twenty years ago, depth-100 pooling was found to be adequate--if imperfect--for a collection of one-half million documents and topics with 100 or fewer relevant documents. Since that time, the number of documents and the number of relevant documents have increased, while assessment budgets have decreased. Statistical sampling offers a solution.",1,ad,True
280,REFERENCES,0,,False
281,"[1] Aslam, J. A., Pavlu, V., and Savell, R. A unified model for metasearch and the efficient evaluation of retrieval systems via the hedge algorithm. In SIGIR 2003.",0,,False
282,"[2] Cormack, G. V., and Grossman, M. R. Beyond pooling. In SIGIR 2018. [3] Horvitz, D. G., and Thompson, D. J. A generalization of sampling without",0,,False
283,"replacement from a finite universe. Journal of the American Statistical Association 47, 260 (1952), 663­685. [4] Pavlu, V., and Aslam, J. A practical sampling strategy for efficient retrieval evaluation. Northeastern University (2007). [5] Sanderson, M., et al. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval 4, 4 (2010), 247­375. [6] Voorhees, E., and Harman, D. Overview of the eighth text retrieval conference. In TREC 8 (1999). [7] Voorhees, E. M. The effect of sampling strategy on inferred measures. In SIGIR 2014. [8] Yilmaz, E., Kanoulas, E., and Aslam, J. A. A simple and efficient sampling method for estimating AP and NDCG. In SIGIR 2008.",1,TREC,True
284,948,0,,False
285,,0,,False

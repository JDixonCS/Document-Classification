,sentence,label,data,regex
0,Session 5A: Conversation and Dialog,1,Session,True
1,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
2,Asking Clarifying Questions in Open-Domain Information-Seeking Conversations,0,,False
3,Mohammad Aliannejadi,1,ad,True
4,Università della Svizzera italiana (USI) mohammad.alian.nejadi@usi.ch,1,ad,True
5,Fabio Crestani,0,,False
6,Università della Svizzera italiana (USI) fabio.crestani@usi.ch,0,,False
7,ABSTRACT,0,,False
8,"Users often fail to formulate their complex information needs in a single query. As a consequence, they may need to scan multiple result pages or reformulate their queries, which may be a frustrating experience. Alternatively, systems can improve user satisfaction by proactively asking questions of the users to clarify their information needs. Asking clarifying questions is especially important in conversational systems since they can only return a limited number of (often only one) result(s).",0,,False
9,"In this paper, we formulate the task of asking clarifying questions in open-domain information-seeking conversational systems. To this end, we propose an offline evaluation methodology for the task and collect a dataset, called Qulac, through crowdsourcing. Our dataset is built on top of the TREC Web Track 2009-2012 data and consists of over 10K question-answer pairs for 198 TREC topics with 762 facets. Our experiments on an oracle model demonstrate that asking only one good question leads to over 170% retrieval performance improvement in terms of P@1, which clearly demonstrates the potential impact of the task. We further propose a retrieval framework consisting of three components: question retrieval, question selection, and document retrieval. In particular, our question selection model takes into account the original query and previous question-answer interactions while selecting the next question. Our model significantly outperforms competitive baselines. To foster research in this area, we have made Qulac publicly available.",1,TREC,True
10,1 INTRODUCTION,1,DUC,True
11,"While searching on the Web, users often fail to formulate their complex information needs in a single query. As a consequence, they may need to scan multiple result pages or reformulate their queries. Alternatively, systems can decide to proactively ask questions to clarify users' intent before returning the result list [9, 33]. In other words, a system can assess the level of confidence in the results and decide whether to return the results or ask questions from the users to clarify their information need. The questions can be aimed",0,,False
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331265",1,ad,True
13,Hamed Zamani,0,,False
14,University of Massachusetts Amherst zamani@cs.umass.edu,0,,False
15,W. Bruce Croft,0,,False
16,University of Massachusetts Amherst croft@cs.umass.edu,0,,False
17,"Figure 1: Example conversations with clarifying questions from our dataset, Qulac. As we see, both users, Alice and Robin, issue the same query (""dinosaur""), however, their actual information needs are completely different. With no prior knowledge, the system starts with the same clarifying question. Depending on the user's answers, the system selects the next questions in order to clarify the user's information need. The tag ""No answer"" shows that the asked question is not related to the information need.",0,,False
18,"to clarify ambiguous, faceted or incomplete queries [44]. Asking clarifying questions is especially important in conversational search systems for two reasons: (i) conversation is the most convenient way for natural language interactions and asking questions [22] and (ii) a conversational system can only return a limited number of results, thus being confident about the retrieval performance becomes even more important. Asking clarifying questions is a possible solution for improving this confidence. Figure 1 shows an example of such a conversation selected from our dataset. We see that both users, Alice and Robin, issue the same query, ""dinosaur."" Assuming that the system does not have access to any prior personal or contextual information, the conversation starts with the same clarifying question. The rest of the conversation, however, depends on the users' responses. In fact, the users' responses aid the system to get a better understanding of the underlying information need.",0,,False
19,"A possible workflow for an information system with clarifying questions is shown in Figure 2. As we can see, Alice initiates a conversation by submitting her query to the system. The system then retrieves a list of documents and estimates its confidence on the",0,,False
20,475,0,,False
21,Session 5A: Conversation and Dialog,1,Session,True
22,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
23,"result list (i.e., ""Present Results?""). If the system is not sufficiently confident to present the results to the user, it then starts the process of asking clarifying questions. As the first step, it generates a list of candidate questions related to Alice's query. Next, the system selects a question from the candidate question list and asks it from the user. Based on Alice's answer, the system retrieves new documents and repeats the process.",0,,False
24,"In this paper, we formulate the task of selecting and asking clarifying questions in open-domain information-seeking conversational systems. To this end, we propose an offline evaluation framework based on faceted and ambiguous queries and collect a novel dataset, called Qulac,1 building on top of the TREC Web Track 2009-2012 collections. Qulac consists of over 10K questionanswer pairs for 198 TREC topics consisting of 762 facets. Inspired from successful examples of crowdsourced collections [2, 4], we collected clarifying questions and their corresponding answers for every topic-facet pair via crowdsourcing. Our offline evaluation protocol enables further research on the topic of asking clarifying questions in a conversational search session, providing a benchmarking methodology to the community.",1,TREC,True
25,"Our experiments on an oracle model show that asking only one good question leads to over 100% retrieval performance improvement. Moreover, the analysis of the oracle model provides important intuitions related to this task. For instance, we see that asking clarifying questions can improve the performance of shorter queries more. Also, clarifying questions exhibit a more significant effect on improving the performance of ambiguous queries, compared to faceted queries. We further propose a retrieval framework following the workflow of Figure 2, consisting of three main components as follows: (i) question retrieval; (ii) question selection; and (iii) document retrieval. The question selection model is a simple yet effective neural model that takes into account both users' queries and the conversation context. We compare the question retrieval and selection models with competitive term-matching and learning to rank (LTR) baselines, showing their ability to significantly outperform the baselines. Finally, to foster research in this area, we have made Qulac publicly available.2",1,ad,True
26,2 RELATED WORK,0,,False
27,"While conversational search has roots in early Information Retrieval (IR) research, the recent advances in automatic voice recognition and conversational agents have created increasing interest in this area. One of the first works in conversational IR dates back to 1987 when Croft and Thompson [16] proposed I3R that acted as an expert intermediary system, communicating with the user in a search session. A few years later Belkin et al. [7] characterized information-seeking strategies for conversational IR, offering users choices in a search session based on case-based reasoning. Since then researchers in the fields of IR and natural language processing (NLP) have studied various aspects of this problem. Early works focused on rule-base conversational systems [45, 47], while another line of research has investigated spoken language understanding approaches [1, 18, 29] for intelligent dialogue agents in the domain of flight [19] and train trip information [5]. The challenge was to understand the user's request and query a database",1,ad,True
28,"1Qulac, pronounced ku:l2k, means blizzard and wonderful in Persian. 2Code and data are available at https://github.com/aliannejadi/qulac.",1,ad,True
29,Figure 2: A workflow for asking clarifying questions in an,0,,False
30,open-domain conversational search system.,0,,False
31,"of flight or train schedule information accordingly. The recent advances of conversational agents have attracted research in various aspects of conversational information access [3, 6, 40, 49]. One line of research analyzes data to understand how users interact with voice-only systems [39]. Radlinski and Craswell [33] proposed a theoretical framework for conversational search highlighting the need for multi-turn interactions with users for narrowing down their specific information needs. Also, Trippas et al. [42] studied conversations of real users to identify the commonly-used interactions and inform a conversational search system design. Moreover, research on query suggestion is relevant to our work if we consider suggesting queries as a means of clarifying users' intent in a traditional IR setting [33]. Result diversification and personalizing is one of the key components for query suggestion [20], especially when applied to small-screen devices. In particular, Kato and Tanaka [21] found that presenting results for one facet and suggesting queries for other facets is more effective on such devices.",1,ad,True
32,"Research on clarifying questions has attracted considerable attention in the fields of NLP and IR. People have studied humangenerated dialogues on question answering (QA) websites, analyzing the intent of each utterance [32] and, more specifically, clarifying questions [9]. Kiesel et al. [22] studied the impact of voice query clarification on user satisfaction and found that users like to be prompted for clarification. Much work has been done on interacting with users for recommendation. For instance, Christakopoulou et al. [12] designed a system that can interact with users to collect more detailed information about their preferences in venue recommendation. Also, Sun and Zhang [40] utilized a semi-structured user query with facet-value pairs to represent a conversation history and proposed a deep reinforcement learning framework to build a personalized conversational recommender system. Focusing on clarifying questions, Zhang et al. [53] automatically extracted facet-value pairs from product reviews and considered them as questions and answers. They proposed a multi-memory network to ask questions for improved e-commerce recommendation. Our work is distinguished from these studies by formulating the problem of asking clarifying questions in an open-domain information-seeking conversational setting where several challenges regarding extracting topic facets [23] are different from a recommendation setting.",0,,False
33,"In the field of NLP, researchers have worked on question ranking [34] and generation [35, 46] for conversation. These studies rely on large amount of data from industrial chatbots [31, 46], query logs [37], and QA websites [34, 35, 41]. For instance, Rao and Daumé",0,,False
34,476,0,,False
35,Session 5A: Conversation and Dialog,1,Session,True
36,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
37,"[34] proposed a neural model for question selection on a simulated dataset of clarifying questions and answers extracted from QA websites such as StackOverflow. Later, they proposed an adversarial training for generating clarifying questions for a given product description on Amazon [35]. Also, Wang et al. [46] studied the task of question generation for an industrial chatbot. Unlike these works, we study the task of asking clarification question in an IR setting where the user's request is in the form of short queries (vs. a long detailed post on StackOverflow) and the system should return a ranked list of documents.",1,ad,True
38,3 PROBLEM STATEMENT,0,,False
39,"A key advantage of a conversational search system is its ability to interact with the user in the form question and answer. In particular, a conversational search system can proactively pose questions to the users to understand their actual information needs more accurately and improve its confidence in the search results. We illustrate the workflow of a conversational search system, focusing on asking clarifying questions.",1,ad,True
40,"As depicted in Figure 2, once the user submits a query to the system, the Information Need Representation module generates and passes their information need to the Retrieval Model, which returns a ranked list of documents. The system should then measure its confidence in the retrieved documents (i.e., Present Results? in Figure 2). In cases where the system is not sufficiently confident about the quality of the result list, it passes the query and the context (including the results list) to the Question Generation Model to generate a set of clarifying questions, followed by the Question Selection Model whose aim is to select one of the generated questions to be presented to the user. Next, the user answers the question and the same procedure repeats until a stopping criterion is met. Note that when the user answers a question, the complete session information is considered for selecting the next question. In some cases, a system can decide to present some results, followed by asking a question. For example, assume a user submits the query ""sigir 2019"" and the system responds ""The deadline of SIGIR 2019 is Jan. 28. Would you like to know where it will be held?"" As we can see, while the system is able to return an answer with high confidence, it can still ask further questions [50]. In this work, we do not study this scenario; however, one can investigate it for exploratory search.",1,ad,True
41,3.1 A Facet-Based Offline Evaluation Protocol,0,,False
42,"The design of an offline evaluation protocol is challenging because conversation requires online interaction between a user and a system. Hence, an offline evaluation strategy requires humangenerated answers to all possible questions that a system would ask, something that is impossible to achieve in an offline setting. To circumvent this problem, we substitute the Question Generation Model in Figure 2 with a large bank of questions, assuming that it consists of all possible questions in the collection. Although this assumption is not absolutely realistic, it reduces the complexity of the evaluation significantly as human-generated answers to a limited set of questions can be collected offline, facilitating offline evaluation.",0,,False
43,"In this work, we build our evaluation protocol on top of the TREC Web track's data. TREC has released 200 search topics, each",1,TREC,True
44,"of which being either ""ambiguous"" or ""faceted.""3 Clarke et al. [13] defined these categories as follows: ""... Ambiguous queries are those that have multiple distinct interpretations. ... On the other hand, facets reflect underspecified queries, with different aspects covered by the subtopics..."" The TREC collection is originally designed to evaluate search result diversification. In contrast, here we build various conversation scenarios based on topic facets.",1,TREC,True
45,"Formally, let T ,"" {t1, t2, . . . , tn } be the set of topics (queries) that initiates a conversation. Moreover, we define F "","" {f1, f2, . . . , fn } as the set of facets with fi "","" { f1i , f2i , . . . , fmi i } defining different facets of ti , where mi denotes the number of facets for ti . Further, let Q "","" {q1, q2, . . . , qn } be the set of clarifying questions belonging to every topic, where qi "","" {qi1, qi2, . . . , qzi i } consists of all clarifying questions that belong to ti ; zi is the number of clarifying questions for ti . Here, our aim is to provide the users' answers to all clarifying questions considering all topics and their corresponding facets. Therefore, let A(t, f , q)  a define a function that returns answer a for a given topic t, facet f , and question q. Hence, to enable offline evaluation, A requires to return an answer for all possible values of t, f , and q. In this work, T and F are borrowed from the TREC Web track 2009-2012 data. Q is then collected via crowdsourcing and A(t, f , q) is also modeled by crowdsourcing (see Section 4). It is worth noting that we also borrow the relevance assessments of the TREC Web track, after breaking them down to the facet level. For instance, suppose the topic """"dinosaur"""" has 10 relevant documents, 6 of which are labeled as relevant to the first facet, and 4 to the second facet. In Qulac, the topic """"dinosaur"""" is broken into two topic-facet pairs together with their respective relevance judgments.""",1,TREC,True
46,4 DATA COLLECTION,0,,False
47,"In this section, we explain how we collected Qulac (Questions for lack of clarity), that is, to the best of our knowledge, the first dataset of clarifying questions in an IR setting. As we see in Figure 1, each topic is coupled with a facet. Therefore, the same question would receive a different answer based on the user's actual information need. We follow a four-step strategy to build Qulac. In the first step we define the topics and their corresponding facets. In the second step, we collect a number of candidate clarifying questions (Q) for each query through crowdsourcing. Then, in the third step, we assess the relevance of the questions to each facet and collect new questions for those facets that require more specific questions. Finally, in the last step, we collect the answers for every queryfacet-question triplet, modeling A. In the following subsections, we elaborate on every step of our data collection procedure.",0,,False
48,4.1 Topics and Facets,0,,False
49,"As we discussed earlier, the problem of asking clarifying questions is particularly interesting in cases where a query can be interpreted in various ways. An example is shown in Figure 1 where two different users issue the same query for different intents. Therefore, any data collection should contain an initial query and description of its facet, describing the user's information need. In other words, we define a target facet for each query. Faceted and ambiguous queries make an ideal case to study the effect of clarifying questions in a conversational search system for the following reasons: (i) the user",0,,False
50,"3In this work, we use the term ""facet"" to refer to the subtopics of both faceted and ambiguous topics.",0,,False
51,477,0,,False
52,Session 5A: Conversation and Dialog,1,Session,True
53,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
54,"information need is not clear from the query; (ii) multiple facets of the same query could satisfy the user's information need; (iii) asking clarifying questions related to any of the facets provide a high information gain. Therefore, we choose the TREC Web track's topics4 [15] as the basis for Qulac. In other words, we take the topics of TREC Web track 09-12 as initial user queries. We then break each topic into its facets and assume that each facet describes the information need of a different user (i.e., it is a topic). As we see in Table 1, the average facet per topic is 3.85 ± 1.05. Therefore, the initial 198 TREC topics5 leads to 762 topic-facet pairs in Qulac. Consequently, for each topic-facet pair, we take the relevance judgements associated with the respective facet.",1,TREC,True
55,4.2 Clarifying Questions,0,,False
56,"It is crucial to collect a set of reasonable questions that address multiple facets of every topic6 while containing sufficient negative samples. This enables us to study the effect of retrieval models under the assumption of having a functional question generation model. Therefore, we asked human annotators to generate questions for a given query based on the results they observed on a commercial search engine as well as query auto-complete suggestions.",1,ad,True
57,"To collect clarifying questions, we designed a Human Intelligence Task (HIT) on Amazon Mechanical Turk.7 We asked the workers to imagine themselves acting as a conversational agent such as Microsoft Cortana where an imaginary user had asked them about a topic. Then, we described the concept of facet to them, supporting it with multiple examples. Finally, we asked them to follow the steps below to figure out the facets of each query and generate questions accordingly:",1,ad,True
58,(1) Enter the same query in a search engine of their choice and scan the results in the first three pages. Reading the title of the results as well as scanning the snippets would give them an idea of different facets of the query on the Web.,1,ad,True
59,"(2) For some difficult queries such as ""toilet,"" scanning the results would not help in identifying the facets. Therefore, inspired by [8], we asked the workers to type the query in the search box of the search engine, and press the space key after typing the query. Most commercial search engines provide a list of query auto-complete suggestions. Interestingly, in most cases the suggested queries reflect various aspects of the same query.",0,,False
60,"(3) Finally, we asked them to generate six questions related to the query, aiming to address the facets that they had figured out.",1,ad,True
61,"We assigned two workers to each HIT, resulting in 12 questions per topic in the first round. In order to preserve language diversity of the questions, we limited each worker to a maximum of two HITs. HITs were available to workers residing in the U.S. with an approval rate of over 97%. After collecting the clarifying questions, in the next step, we explain how we verified them for quality assurance.",0,,False
62,4.3 Question Verification and Addition,0,,False
63,"In this step, we aim to address two main concerns: (i) how good are the collected clarifying questions? (ii) are all facets addressed by at least one clarifying question? Given the high complexity of",1,ad,True
64,4 https://trec.nist.gov/data/webmain.html 5 The official TREC relevance judgements cover 198 of the topics. 6Candidate clarifying questions should also address out-of-collection facets. 7 http://www.mturk.com,1,trec,True
65,Table 1: Statistics of Qulac.,0,,False
66,# topics,0,,False
67,198,0,,False
68,# faceted topics,0,,False
69,141,0,,False
70,# ambiguous topics,0,,False
71,57,0,,False
72,# facets Average facet per topic Median facet per topic # informational facets # navigational facets,0,,False
73,762 3.85 ± 1.05 4 577 185,0,,False
74,# questions # question-answer pairs Average terms per question Average terms per answer,0,,False
75,"2,639 10,277 9.49 ± 2.53 8.21 ± 4.42",0,,False
76,"this step, we appointed two expert annotators for this task. We instructed the annotators to read all the collected questions of each topic, marking invalid and duplicate questions. Moreover, we asked them to match a question to a facet if the question was relevant to the facet. A question was considered relevant to a facet if its answer would address the facet. Finally, in order to make sure that all facets were covered by at least one question, we asked the annotators to generate an additional question for the facets that needed more specific questions. The outcome of this step is a set of verified clarifying questions, addressing all the facets in the collection.",1,ad,True
77,4.4 Answers,0,,False
78,"After collecting and verifying the questions, we designed another HIT in which we collected answers to the questions for every facet. The HIT started with detailed instructions of the task, followed by several examples. The workers were provided with a topic and a facet description. Then we instructed them to assume that they had submitted the query with their actual information need being the given facet. Then they were required to write the answer to one clarifying question that was presented to them. To avoid the bias of other questions for the same facet, we included only one question in each HIT. If a question required information other than what workers were provided with, we instructed the workers to identify it with a ""No answer"" tag. Each worker was allowed to complete a maximum of 100 HITs to guarantee language diversity. Workers were based in the U.S. with an approval rate of 95% or greater.",1,ad,True
79,"Quality check. During the course of data collection, we performed regular quality checks on the collected answers. The checks were done manually on 10% of submissions per worker. In case we observed any invalid submissions among the sampled answers of one user, we then studied all the submissions of the same user. Invalid submissions were then removed from the collection and the worker was banned from the future HITs. Finally, we assigned all invalid answers to other workers to complete. Moreover, we employed basic behavioral check techniques in the design of the HIT. For example, we disabled copy/paste features of text inputs and tracked workers' keystrokes. This enabled us to detect and reject low-quality submissions.",0,,False
80,5 SELECTING CLARIFYING QUESTIONS,0,,False
81,"In this section, we propose a conversational search system that is able to select and ask clarifying questions and rank documents based on the user's responses. The proposed system retrieves a set of questions for a given query from a large pool of questions,",0,,False
82,478,0,,False
83,Session 5A: Conversation and Dialog,1,Session,True
84,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
85,"containing all the questions in the collection. At the second stage, our proposed model, called NeuQS, aims to select the best question to be posed to the user based on the query and the conversation context. This problem is particularly challenging because the conversational interactions are in natural language, highly depending on the previous interactions between the user and the system (i.e., conversation context).",0,,False
86,"As mentioned earlier in Section 3, a user initiates the conversation by submitting a query. Then the system should decide whether to ask a clarifying question or present the results. At every stage of the conversation, the previous questions and answers exchanged between the user and the system are known to the model. Finally, the selected question and its corresponding answer should be incorporated in the document retrieval model to enhance the retrieval performance.",1,corpora,True
87,"Formally, for a given topic t let h ,"" {(q1, a1), (q2, a2), . . . , (q |h|, a |h| )} be the history of clarifying questions and their corresponding answers exchanged between the user and the system (i.e., context). Here, the ultimate goal is to predict q, that is the next question that the system should ask from the user. Moreover, let a be the user's answer to q. The answer a is unknown to the question selection model, however, the document retrieval model retrieves documents once the system receives the answer a. In the following, we describe the question retrieval model, followed by the question selection and the document retrieval models.""",0,,False
88,5.1 Question Retrieval Model,0,,False
89,"We now describe our BERT8 Language Representation based Question Retrieval model, called BERT-LeaQuR. We aim to maximize the recall of the retrieved questions, retrieving all relevant clarifying questions to a given query in the top k questions. Retrieving all relevant questions from a large pool of questions is challenging, because questions are short and context-dependent. In other words, many questions depend on the conversation context and the query. Also, since conversation is in the form of natural language, termmatching models cannot effectively retrieve short questions. For instance, some relevant clarifying questions for the query ""dinosaur"" are: ""Are you looking for a specific web page?"" ""Would you like to see some pictures?""",0,,False
90,"Yang et al. [51] showed that neural models outperform termmatching models for question retrieval. Inspired by their work, we learn a high-dimensional language representation for the query and the questions. Formally, BERT-LeaQuR estimates the probability p(R ,"" 1|t, q), where R is a binary random variable indicating whether the question q should be retrieved (R "", 1) or not (R ,"" 0). t and q denote the query (topic) and the candidate clarifying question, respectively. The question relevance probability in the BERT-LeaQuR model is estimated as follows:""",0,,False
91,"p(R ,"" 1|t, q) "",""  T (t ), Q (q) ,""",0,,False
92,(1),0,,False
93,"where T and Q denote topic representation and question representation, respectively.  is the matching component that takes the aforementioned representations and produces a question retrieval score. There are various ways to implement any of these components.",0,,False
94,8BERT: Bidirectional Encoder Representations from Transformers,0,,False
95,"We implement T and Q similarly using a function that maps a sequence of words to a d-dimensional representation (V s  Rd ). We use the BERT [17] model to learn these representation functions. BERT is a deep neural network with 12 layers that uses an attention-based network called Transformers [43]. We initialize the BERT parameters with the model that is pre-trained for the language modeling task on Wikipedia and fine-tune the parameters on Qulac with 3 epochs. BERT has recently outperformed state-of-the-art models in a number of language understanding and retrieval tasks [17, 27]. We particularly use BERT in our model to incorporate the knowledge from the vast amount of unlabeled data while learning the representation of queries and questions. In addition, BERT shows promising results in modeling short texts.",1,Wiki,True
96,"The component is modeled using a fully-connected feed-forward network with the output dimensionality of 2. Rectified linear unit (ReLU) is employed as the activation function in the hidden layers, and a softmax function is applied on the output layer to compute the probability of each label (i.e., relevant or non-relevant). To train BERT-LeaQuR, we use a cross-entropy loss function.",0,,False
97,5.2 Question Selection Model,0,,False
98,"In this section, we introduce a Neural Question Selection Model",0,,False
99,(NeuQS) which selects questions with a focus on maximizing the,0,,False
100,precision at the top of the ranked list. The main challenge in the,0,,False
101,question selection task is to predict whether a question has diverged,0,,False
102,from the query and conversation context. In cases where a user has,0,,False
103,"given a negative answer(s) to previous question(s), the model needs",0,,False
104,"to diverge from the history. In contrast, in cases where the answer",0,,False
105,"to the previous question(s) is positive, questions on the same topic",0,,False
106,"that ask for more details are preferred. For example, as we saw",0,,False
107,"in Figure 1, when Robin answers the first question positively (i.e.,",0,,False
108,"being interested in dinosaur books), the second question tries to",0,,False
109,narrow down the information to a specific type of dinosaur.,0,,False
110,NeuQS incorporates multiple sources of information. In partic-,1,corpora,True
111,"ular, it learns from the similarity of a query, a question and the",0,,False
112,context as well as retrieval and performance prediction signals. In,0,,False
113,"particular, NeuQS outputs a relevance score for a given query t,",0,,False
114,"question q, and conversation context h. Formally, NeuQS can be",0,,False
115,defined as follows:,0,,False
116,"score ,""  T (t ), H (h), Q (q), (t, h, q),  (t, h, q) , (2)""",0,,False
117,"where  is a scoring function for a given query representation T (t ), context representation H (h), question representation Q (q), retrieval representation (t, h, q), and query performance representa-",0,,False
118,"tion  (t, h, q). Various strategies can be employed to model each of",0,,False
119,the components of NeuQS.,0,,False
120,We model the components T and Q similarly to Section 5.1.,0,,False
121,"Further, the context representation component H is implemented",0,,False
122,as follows:,0,,False
123,"H (h) ,",0,,False
124,1 |h|,0,,False
125,|h|,0,,False
126,"QA (qi , ai ) ,",0,,False
127,i,0,,False
128,(3),0,,False
129,"where QA (q, a) is an embedding function of a question q and",0,,False
130,"answer a. Moreover, the retrieval representation (t, h, q)  Rk",0,,False
131,"is implemented by interpolating the retrieval score of the query,",0,,False
132,context and question (see Section 5.3) and the score of the top k,0,,False
133,"retrieved documents is used. Finally, the query performance prediction (QPP) representation component  (t, h, q)  Rk consists",0,,False
134,479,0,,False
135,Session 5A: Conversation and Dialog,1,Session,True
136,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
137,"of the performance prediction score of the ranked documents at different ranking positions (for a maximum of k ranked documents). We employed the  QPP model for this component [28]. We take the representations from the [CLS] layer of the pre-trained uncased BERT-Base model (i.e., 12-layer, 768-hidden, 12-heads, 110M parameters). To model the function  we concatenate and feed T (t ), H (h), Q (q), (t, h, q), and  (t, h, q) into a fully-connected feed-forward network with two hidden layers. We use ReLU as the activation function in the hidden layers of the network. We use a pointwise learning setting using a cross-entropy loss function.",1,ad,True
138,5.3 Document Retrieval Model,0,,False
139,"Here, we describe the model that we use to retrieve documents given a query, conversation context, and current clarifying question as well as user's answer. We use the KL-divergence retrieval model [24] based on the language modeling framework [30] with Dirichlet prior smoothing [52] where we linearly interpolate two likelihood models: one based on the original query, and one based on the questions and their respective answers.",0,,False
140,"For every term w of the original query t, conversation context h, the current question q, and answer a, the interpolated query probability is computed as follows:",0,,False
141,"p(w |t, h, q, a) ,""  × p(w |t ) + (1 -  ) × p(w |h,q,a ) , (4)""",0,,False
142,"where t denotes the language model of the original query, and h,q,a denotes the language model of all questions and answers that have been exchanged in the conversation.  determines the",0,,False
143,weight of the original query and is tuned on the development set.,0,,False
144,"Then, the score of document d is calculated as follows:",0,,False
145,"p(d |t, h, q, a) ,",0,,False
146,"p(wk |t, h, q, a) log(p(wk |d ) ,",0,,False
147,(5),0,,False
148,wk ,0,,False
149,where  is the set of all the terms present in the conversation. We use,0,,False
150,Dirichlet's smoothing for terms that do not appear in d. We use the,0,,False
151,document retrieval model for two purposes: (i) ranking documents,0,,False
152,after the user answers a clarifying question; (ii) ranking documents,0,,False
153,"of a candidate question as part of the NeuQS (see Section 5.2). Hence,",0,,False
154,the model does not see the answer in the latter case.,0,,False
155,6 EXPERIMENTS,0,,False
156,6.1 Experimental Setup,0,,False
157,"Dataset. We evaluate BERT-LeaQuR and NeuQS on Qulac, following a 5-fold cross-validation. We follow two strategies to split the data, (i) Qulac-T: we split the train/validation/test sets based on topics. In this case, the model has not seen the test topics in the training data; (ii) Qulac-F: here we split the data based on their facets. Thus, the same test topic might appear in the training set, but with a different facet.",0,,False
158,"In order to study the effect of multi-turn conversations with clarifying questions, we expand Qulac to include multiple artificially generated conversation turns. To do so, for each instance, we consider all possible combinations of questions to be asked as the context of conversation. Take t1 as an example where we select a new question after asking the user two questions. Assuming that t1 has four questions, all possible combinations of questions in the conversation context would be: (q1, q2), (q1, q3), (q1, q4), (q2, q3), (q2, q4), (q3, q4). Notice that the set of candidate clarifying questions for",0,,False
159,Table 2: Performance of question retrieval model. The superscript * denotes statistically significant differences compared to all the baselines (p < 0.001).,0,,False
160,Method,0,,False
161,MAP Recall@10 Recall@20 Recall@30,1,MAP,True
162,QL BM25 RM3 LambdaMART RankNet BERT-LeaQuR,0,,False
163,0.6714 0.6715 0.6858 0.7218 0.7304,0,,False
164,0.8349*,0,,False
165,0.5917 0.5938 0.5970 0.6220 0.6233,0,,False
166,0.6775*,0,,False
167,0.6946 0.6848 0.7091 0.7234 0.7314,0,,False
168,0.8310*,0,,False
169,0.7076 0.7076 0.7244 0.7336 0.7500,0,,False
170,0.8630*,0,,False
171,"each multi-turn example would be the ones that have not appeared in the context. The number of instances grows significantly as we enlarge the length of the conversation, leading to a total of 907,366 instances in the collection. At each turn of the conversation, we select the question from all candidate questions of the same topic and facet, having the same conversation history. In other words, they share the same context. Since the total number of unique conversational contexts is 75,200, a model should select questions for 75,200 contexts from all 907,366 candidate questions.",1,ad,True
172,"Question retrieval evaluation metrics. We consider four metrics to evaluate the effectiveness of question retrieval models: mean average precision (MAP) and recall for the top 10, 20, and 30 retrieved questions (Recall@10, Recall@20, Recall@30). Our choice of measures is motivated by the importance of achieving high recall for this task.",1,MAP,True
173,"Question selection evaluation metrics. Effectiveness is measured considering the performance of retrieval after adding the selected question to the retrieval model as well as the user answer. Five standard evaluation metrics are considered: mean reciprocal rank (MRR), precision of the top 1 retrieved document (P@1), and normalized discounted cumulative gain for the top 1, 5, and 20 retrieved documents (nDCG@1, nDCG@5, nDCG@20). We use the relevance assessments as they were released by TREC. However, we modify them in such a way to evaluate the performance with respect to every facet. For instance, if one topic consists of 4 facets it is then broken into 4 different topics each inheriting its own relevance judgements from the TREC assessments.",1,ad,True
174,"The choice of evaluation metrics is motivated by considering three different aspects of the task. We choose MRR to evaluate the effect of asking clarifying questions on ranking the first relevant document. We report P@1 and nDCG@1 to measure the performance for scenarios where the system is able to return only one result. This is often the case with voice-only conversational systems. Moreover, we report nDCG@5 and nDCG@20 as conventional ranking metrics to measure the impact of asking clarifying questions in a traditional Web search setting. Notice that nDCG@20 is the preferred evaluation metric for the ClueWeb collection due to the shallow pooling performed for relevance assessments [14, 26].",1,ad,True
175,Statistical test. We determine statistically significant differences using the two-tailed paired t-test with Bonferroni correction at a 99.9% confidence interval (p < 0.001).,0,,False
176,Compared methods. We compare the performance of our question retrieval and selection models with the following methods:,0,,False
177,480,0,,False
178,Session 5A: Conversation and Dialog,1,Session,True
179,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
180,Table 3: Performance comparison with baselines. WorstQuestion and BestQuestion respectively determine the lower and upper bounds. The superscript * denotes statistically significant differences compared to all the baselines (p < 0.001).,0,,False
181,Method,0,,False
182,OriginalQuery  -QPP LambdaMART RankNet NeuQS,1,Query,True
183,Qulac-T Dataset,0,,False
184,MRR P@1 nDCG@1 nDCG@5 nDCG@20,0,,False
185,0.2715 0.3570 0.3558 0.3573,0,,False
186,0.3625*,0,,False
187,0.1842 0.2548 0.2537 0.2562,0,,False
188,0.2664*,0,,False
189,0.1381 0.1960 0.1945 0.1979,0,,False
190,0.2064*,0,,False
191,0.1451 0.1938 0.1940 0.1943,0,,False
192,0.2013*,0,,False
193,0.1470 0.1812 0.1796 0.1804,0,,False
194,0.1862*,0,,False
195,MRR,0,,False
196,0.2715 0.3570 0.3501 0.3568 0.3641*,0,,False
197,Qulac-F Dataset,0,,False
198,P@1 nDCG@1,0,,False
199,0.1842 0.2548 0.2478 0.2559,0,,False
200,0.2682*,0,,False
201,0.1381 0.1960 0.1911 0.1986,0,,False
202,0.2110*,0,,False
203,nDCG@5,0,,False
204,0.1451 0.1938 0.1896 0.1944 0.2018*,0,,False
205,nDCG@20,0,,False
206,0.1470 0.1812 0.1773 0.1809 0.1867*,0,,False
207,WorstQuestion 0.2479 0.1451 BestQuestion 0.4673 0.3815,0,,False
208,0.1075 0.3031,0,,False
209,0.1402 0.2410,0,,False
210,0.1483 0.2077,0,,False
211,0.2479 0.4673,0,,False
212,0.1451 0.3815,0,,False
213,0.1075 0.3031,0,,False
214,0.1402 0.2410,0,,False
215,0.1483 0.2077,0,,False
216,"· Question retrieval: ­ BM25, RM3, QL: we index all the questions using Galago.9 Then, for a given query we retrieve the documents using BM25 [38], RM3 [25], and QL [30] models. ­ LambdaMART, RankNet: for every query-question pair, we use the scores obtained by BM25, RM3, and QL as features to train LambdaMART [48] and RankNet [10] implemented in RankLib.10 For every query, we consider all irrelevant questions as negative samples.",0,,False
217,"· Question selection: ­ OriginalQuery reports the performance of the document retrieval model only with the original query (Eq. (4) with  ,"" 1). ­  -QPP: we use a simple yet effective query performance predictor,  [28] as an estimation of a question's quality. We calculate the  predictor of the document retrieval model with the following input: original query, the context, and candidate questions. We then select the question with the highest  value. ­ LambdaMART, RankNet: we consider the task of question selection as a ranking problem where a list of candidate questions should be ranked and the one with the highest rank is chosen. Therefore, we use LambdaMART [48] and RankNet [10] as two LTR baselines. The list of features are: (i) a flag determining if a question is open or not; (ii) a flag indicating if the answer to the last question in the context is yes or no; (iii)  [28] performance predictor of the current question; (iv) the Kendall's  correlation of the ranked list at 10 and 50 of the original query and the current question; (v) the Kendall's  correlation of the ranked list at 20 and 50 of the current question and previous question-answer pairs in the context; (vi) Similarity of the current question and the query based on their BERT representations; (vii) Similarity of the current question and previous question-answer pairs in the context based on their BERT representations. ­ BestQuestion, WorstQuestion: in addition to all the baselines, we also report the retrieval performance when the worst and the best question is selected for an instance. BestQuestion (WorstQuestion) selects the candidate question for which the MRR value of the retrieval model is the maximum (minimum).""",1,Query,True
218,9 https://sourceforge.net/p/lemur/galago/ 10 https://sourceforge.net/p/lemur/wiki/RankLib/,1,wiki,True
219, MRR,0,,False
220,1.0 0.5 0.0 -0.5,0,,False
221,faceted ambiguous nav.,0,,False
222,inf.,0,,False
223,1,0,,False
224,2,0,,False
225,3,0,,False
226,4,0,,False
227,5 10,0,,False
228,topic type,0,,False
229,facet type,0,,False
230,# query terms,0,,False
231,"Figure 3: Impact of topic type, facet type, and query length",0,,False
232,"on the performance of BestQuestion oracle model, com-",0,,False
233,pared to OriginalQuery.,1,Query,True
234,"Note that the retrieval scores are calculated knowing the selected question and its answer (i.e., oracle model). Our goal is to show the upper and lower bounds.",0,,False
235,6.2 Results and Discussion,0,,False
236,"Question retrieval. Table 2 shows the results of question retrieval for all the topics. As we see, BERT-LeaQuR is able to outperform all baselines. It is worth noting that the model's performance gets better as the number of retrieved documents increases. This indicates that BERT-LeaQuR is able to capture the relevance of query and questions when they lack common terms. In fact, we see that all term-matching retrieval models such as BM25 are significantly outperformed in terms of all evaluation metrics.",0,,False
237,"Oracle question selection: performance. Here we study the performance of an oracle model, i.e. assuming that an oracle model is aware of the answers to the questions. The goal is to show to what extent clarifying questions can improve the performance of a retrieval system. As we see in the lower rows of Table 3 selecting best questions (BestQuestion model) helps the model to achieve substantial improvement, even in the case that the retrieval model is very simple. This shows the high potential gain of asking good clarifying questions on the performance of a conversational system. Particularly, we examine the relative improvement of the system after asking only one question and observe that BestQuestion achieves over 100% relative improvement in terms of different evaluation metrics (MRR: 0.2820  0.5677, P@1: 0.1933  0.4986, nDCG@1: 0.1460  0.3988, nDCG@5: 0.1503  0.2793, nDCG@20: 0.1520  0.2265). It is worth mentioning that we observe the highest relative improvements in terms of nDCG@1 (,173%) and P@1 (,""158%), exhibiting a high potential impact on voice-only conversational systems.""",0,,False
238,Oracle question selection: impact of topic type and length. We analyze the performance of BestQuestion based on the number,0,,False
239,481,0,,False
240,Session 5A: Conversation and Dialog,1,Session,True
241,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
242,NeuQS,0,,False
243,LambdaMART,0,,False
244,RankNet,0,,False
245,-QPP,0,,False
246,OriginalQuery,1,Query,True
247,Qulac-T,0,,False
248,0.36 0.34,0,,False
249,Qulac-F 0.36,0,,False
250,Qulac-T,0,,False
251,0.22 0.21 0.20 0.19,0,,False
252,Qulac-F,0,,False
253,Qulac-T,0,,False
254,0.19,0,,False
255,0.18,0,,False
256,Qulac-F,0,,False
257,MRR,0,,False
258,MRR nDCG@1 nDCG@20,0,,False
259,0.32,0,,False
260,0.34,0,,False
261,0.18,0,,False
262,0.17,0,,False
263,0.17,0,,False
264,0.30,0,,False
265,0.32,0,,False
266,0.28,0,,False
267,1,0,,False
268,2,0,,False
269,31,0,,False
270,0.302,0,,False
271,3,0,,False
272,# conversation turns,0,,False
273,# conversation turns,0,,False
274,0.16 0.15,0,,False
275,1,0,,False
276,2,0,,False
277,31,0,,False
278,2,0,,False
279,3,0,,False
280,# conversation turns,0,,False
281,# conversation turns,0,,False
282,0.16,0,,False
283,1,0,,False
284,2,0,,False
285,31,0,,False
286,2,0,,False
287,3,0,,False
288,# conversation turns,0,,False
289,# conversation turns,0,,False
290,"Figure 4: Performance0.c28omparison with the baselines for different number of conversation turns (k  {1, 2, 3}).",0,,False
291,1,0,,False
292,2,0,,False
293,3,0,,False
294,of query terms and topic type. We see tha#t tchoenrveelrastaitvioenimtuprrnosvement,0,,False
295,Impact of clarifying questions on facets. We study the differ-,0,,False
296,of BestQuestion is negatively correlated with the number of query,0,,False
297,ence of MRR between NeuQS and OriginalQuery on all facets. Note,1,Query,True
298,"terms (Pearson's r ,"" -0.2, p  0.001), suggesting that shorter""",0,,False
299,that for every facet we average the performance of NeuQS at dif-,0,,False
300,"queries require clarification in more cases. Also, comparing the",0,,False
301,ferent conversation turns. Our goal is to see how many facets are,0,,False
302,"topic types (ambiguous vs. faceted), we see a significant difference",0,,False
303,impacted positively by asking clarifying questions. NeuQS is im-,0,,False
304,in the relative improvement. The average MRR for ambiguous,0,,False
305,proves the effectiveness of retrieval by selecting relevant questions,0,,False
306,"topics is 0.3858, compared with the faceted topics with average",0,,False
307,for a considerable number of facets on both data splits. In partic-,0,,False
308,MRR of 0.2898. The difference was statistically significant (2-way,0,,False
309,"ular, the performance for 45% of the facets is improved by asking",0,,False
310,"ANOVA, p  0.001).",0,,False
311,"clarifying questions, whereas the performance for 19% is worse.",0,,False
312,"Question selection. Table 3 presents the results of the document retrieval model taking into account a selected question together with its answer. We see that all models outperform OriginalQuery, confirming that asking clarifying questions is crucial in a conversation, leading to high performance gain. For instance, compared to OriginalQuery, a model as simple as  -QPP achieves a 31% relative improvement in terms of MRR. Also, NeuQS consistently outperforms all the baselines in terms of all evaluation metrics on both data splits. All the improvements are statistically significant. Moreover, NeuQS achieves a remarkable improvement in terms of both P@1 and nDCG@1. These two evaluation metrics are particularly important for voice-only conversational systems where the system must return only one result to the user. The obtained improvements highlight the necessity and effectiveness of asking clarifying questions in a conversational search system, where they are perceived as natural means of interactions with users.",1,Query,True
313,"Impact of data splits. We compare the performance of models on both Qulac-T and Qulac-F data splits. We see that the LTR baselines perform worse on Qulac-F. Notice that the performance difference of LambdaMART among the splits is statistically significant in terms of all evaluation metrics (p < 0.001). RankNet, on the other hand, exhibits a more robust performance, i.e., the difference of its performance on the two splits is not statistically significant. Unlike the baselines, NeuQS exhibits a significant improvement in terms of all evaluation metrics on Qulac-F (p < 0.05), except for nDCG@5. This suggests that the baseline models are prone to overfitting on queries and conversations in the training data. As mentioned, Qulac-F's train and test sets may have some queries and questions in common, hurting models that are weak at generalization.",0,,False
314,"Impact of number of conversation turns. Figure 4 shows the performance of NeuQS as well as the baselines for different conversation turns. We evaluate different models at k turns (k  {1, 2, 3}). We see that the performance of all models improves as the conversation advances to multiple turns. Also, we see that all the models consistently outperform the OriginalQuery baseline at different number of turns. Finally, we see that NeuQS exhibits robust performance, outperforming all the baselines at different turns.",1,ad,True
315,"Case study: failure and success analysis. Finally, we analyze representative cases of failure and success of our proposed framework. We list three cases where selecting questions using NeuQS improves the retrieval performance, as well as three other examples in which the selected questions lead to decreased performance. MRR reports the difference of the performance of NeuQS and OriginalQuery in terms of MRR. As we see, the first three examples show the selected questions that hurt the performance (i.e., MRR < 0.0). The first row is an example where the user's response to the question is negative; however, the user provides additional information about their information need (i.e., facet). We see that even though the user has provided additional information, the performance drops. This is perhaps due to existence of no common terms between the additional information (i.e., ""dog is too hot"") and the facet (i.e., ""excessive heat on dogs""). This is more evident when we compare this example with a successful answer: ""No, I would like to know the effects of excessive heat on dogs."" The second row of the table shows a case where the answer to the question is positive, but there is no common terms between the question and the facet. Again, the intuition here is that the retrieval model is not able to take advantage of additional information when it has no terms in common with relevant documents. The third row of the table shows another failure example where the selected question is not relevant to the facet and the user provides no additional information. This is a typical failure example where the system does not get any positive feedback, but could still use the negative feedback to improve the ranking. This can be done by diverging from the documents that are similar to the negative feedback.",1,ad,True
316,"As for the success examples, we have listed three types. The first example (""east ridge high school"") is where the system is able to ask an open question. Open questions are very hard to formulate for open-domain information-seeking scenarios; however, it is more likely to get useful feedback from users in response to such questions. The fifth row shows an example of a positive feedback. The performance gain, in this case, is perhaps due to the existence of term ""biography"" in the question which would match with relevant documents. It is worth noting that the question and the query in this example have no common terms. This highlights the importance",0,,False
317,482,0,,False
318,Session 5A: Conversation and Dialog,1,Session,True
319,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
320,Table 4: Failure and success examples of NeuQS. Failure and success are measured by the difference in performance of NeuQS and OriginalQuery in terms of MRR (MRR).,1,Query,True
321,Query,1,Query,True
322,Facet Description,0,,False
323,Selected Question,0,,False
324,User's Answer,0,,False
325,dog heat,0,,False
326,"What is the effect of excessive heat Would you like to know how to care No, I want to know what happens",0,,False
327,on dogs?,0,,False
328,for your dog during heat?,0,,False
329,when a dog is too hot.,0,,False
330,"sit and reach How is the sit and reach test prop- Do you want to know how to per- Yes, I do.",0,,False
331,test,0,,False
332,erly done?,0,,False
333,form this test?,0,,False
334,alexian broth- Find Alexian Brothers hospitals. ers hospital,0,,False
335,"Are you looking for our schedule of No, I don't need that. classes or events?",0,,False
336,MRR -0.86,0,,False
337,-0.75,0,,False
338,-0.54,0,,False
339,east ridge high Information about the sports pro- What information about East Ridge I'm looking for information about,0,,False
340,school,0,,False
341,gram at East Ridge High School in High School are you looking for? their sports program.,0,,False
342,"Clermont, Florida",0,,False
343,euclid,0,,False
344,Find information on the Greek Do you want a biography?,0,,False
345,Yes.,0,,False
346,mathematician Euclid.,0,,False
347,"rocky moun- Who are the sports reporters for the Would you like to read recent news No, I just want a list of the reporters",1,ad,True
348,tain news,0,,False
349,Rocky Mountain News?,0,,False
350,about the Rocky Mountain News? who write the sports for the Rocky,0,,False
351,Mountain News.,0,,False
352,+0.96 +0.93 +0.88,0,,False
353,"of employing a language-representation-based question retrieval model (e.g., BERT-LeaQuR) as opposed to term-matching IR models. The last example shows a case where the answer is negative, but the user is engaged in the conversation and provides additional information about the facet. We see that the answer contains keywords of the facet description (i.e., ""reporters,"" ""sports""), improving the score of relevant documents that contain those terms.",1,ad,True
354,7 LIMITATIONS AND FUTURE WORK,0,,False
355,"Every data collection comes with some limitations. The same is valid for Qulac. First, the dataset was not collected from actual conversations. This decision was mainly due to the unbalanced workload of the two conversation participants. In our crowdsourcing HITs, the task of question generation required nearly 10 times more effort compared to the task of question answering. This makes it challenging and more expensive to pair two workers as participants of the same conversation. There are some examples of this approach in the literature [11, 36]; however, they address the task of reading comprehension, a task that is considerably simpler than identifying topic facets. A possible future direction is to provide a limited number of pre-generated questions (say 10) to the workers to select from, so that the complexity of the task would be significantly reduced.",1,ad,True
356,"Furthermore, Qulac is built for single-turn conversations (i.e., one question; one answer). Even though there are questions that can be asked after one another to form a multi-turn conversation, our data collection approach does not guarantee the existence of multi-turn conversations that involve the same participants. Also, we believe that the quality of generated clarifying questions highly depends on how well the selected commercial search engine is able to diversify the result list. We aimed to minimize this bias by asking workers to scan at least three pages of the result list. Also, the questions added by expert annotators guarantees the coverage of all facets (see Section 4.3). Finally, as we mentioned, faceted and ambiguous queries are good examples of topics that a conversational system needs to clarify; however, this task cannot be limited only to such queries. One can collect a similar data for exploratory",1,ad,True
357,"search scenarios, where asking questions can potentially lead to more user engagement while doing exploratory search.",1,ad,True
358,"In this work, our main focus was on question selection. There are various directions that can be explored in the future. One interesting problem is to explore various strategies of improving the performance of the document retrieval model as new information is added to the model. Moreover, we assumed the number of conversation turns to be fixed. Another interesting future direction is to model the system's confidence at every stage of the conversation so that the model is able to decide when to stop asking questions and present the result(s).",1,ad,True
359,8 CONCLUSIONS,0,,False
360,"In this work, we introduced the task of asking clarifying questions in open-domain information-seeking conversations. We proposed an evaluation methodology which enables offline evaluation of conversational systems with clarifying questions. Also, we constructed and released a new data collection called Qulac, consisting of 762 topic-facet pairs with over 10K question-answer pairs. We further presented a neural question selection model called NeuQS along with models on question and document retrieval. NeuQS was able to outperform the LTR baselines significantly. The experimental analysis provided many insights of the task. In particular, experiments on the oracle model demonstrated that asking only one good clarifying question leads to over 150% relative improvement in terms of P@1 and nDCG@1. Moreover, we observed that asking clarifying questions improves the model's performance for a substantial percentage of the facets. In some failure cases, we saw that a more effective document retrieval model can potentially improve the performance. Finally, we showed that, asking more clarifying questions leads to better results, once again confirming the effectiveness of asking clarifying questions in a conversational search system.",1,ad,True
361,483,0,,False
362,Session 5A: Conversation and Dialog,1,Session,True
363,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
364,ACKNOWLEDGMENTS,0,,False
365,This work was supported in part by the RelMobIR project of the,0,,False
366,"Swiss National Science Foundation (SNSF), in part by the Center",0,,False
367,"for Intelligent Information Retrieval, and in part by NSF grant",0,,False
368,"IIS-1715095. Any opinions, findings and conclusions or recommen-",0,,False
369,dations expressed in this material are those of the authors and do,0,,False
370,not necessarily reflect those of the sponsors.,0,,False
371,REFERENCES,0,,False
372,"[1] Mohammad Aliannejadi, Masoud Kiaeeha, Shahram Khadivi, and Saeed Shiry Ghidary. 2014. Graph-Based Semi-Supervised Conditional Random Fields For Spoken Language Understanding Using Unaligned Data. In ALTA. 98­103.",1,ad,True
373,"[2] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. 2018. In Situ and Context-Aware Target Apps Selection for Unified Mobile Search. In CIKM. 1383­1392.",1,ad,True
374,"[3] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. 2018. Target Apps Selection: Towards a Unified Search Framework for Mobile Devices. In SIGIR. 215­224.",1,ad,True
375,[4] Omar Alonso and Maria Stone. 2014. Building a Query Log via Crowdsourcing. In SIGIR. 939­942.,1,Query,True
376,"[5] Harald Aust, Martin Oerder, Frank Seide, and Volker Steinbiss. 1995. The Philips automatic train timetable information system. Speech Communication 17, 3-4 (1995), 249­262.",0,,False
377,[6] Seyed Ali Bahrainian and Fabio Crestani. 2018. Augmentation of Human Memory: Anticipating Topics that Continue in the Next Meeting. In CHIIR. 150­159.,0,,False
378,"[7] Nicholas J Belkin, Colleen Cool, Adelheit Stein, and Ulrich Thiel. 1995. Cases, scripts, and information-seeking strategies: On the design of interactive information retrieval systems. Expert systems with applications 9, 3 (1995), 379­395.",0,,False
379,"[8] Jan R. Benetka, Krisztian Balog, and Kjetil Nørvåg. 2017. Anticipating Information Needs Based on Check-in Activity. In WSDM. 41­50.",0,,False
380,"[9] Pavel Braslavski, Denis Savenkov, Eugene Agichtein, and Alina Dubatovka. 2017. What Do You Mean Exactly?: Analyzing Clarification Questions in CQA. In CHIIR. 345­348.",0,,False
381,"[10] Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. 2005. Learning to rank using gradient descent. In ICML. 89­96.",1,ad,True
382,"[11] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question Answering in Context. In EMNLP. 2174­2184.",0,,False
383,"[12] Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards Conversational Recommender Systems. In KDD. 815­824.",1,ad,True
384,"[13] Charles L. A. Clarke, Nick Craswell, and Ian Soboroff. 2009. Overview of the TREC 2009 Web Track. In TREC.",1,TREC,True
385,"[14] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Ellen M. Voorhees. 2011. Overview of the TREC 2011 Web Track. In TREC.",1,TREC,True
386,"[15] Charles L. A. Clarke, Nick Craswell, and Ellen M. Voorhees. 2012. Overview of the TREC 2012 Web Track. In TREC.",1,TREC,True
387,"[16] W. Bruce Croft and R. H. Thompson. 1987. I3R: A new approach to the design of document retrieval systems. JASIS 38, 6 (1987), 389­404.",0,,False
388,"[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 (2018).",0,,False
389,"[18] Yulan He and Steve J. Young. 2005. Semantic processing using the Hidden Vector State model. Computer Speech & Language 19, 1 (2005), 85­106.",0,,False
390,"[19] Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS Spoken Language Systems Pilot Corpus. In HLT. 96­101.",0,,False
391,"[20] Di Jiang, Kenneth Wai-Ting Leung, Lingxiao Yang, and Wilfred Ng. 2015. Query suggestion with diversification and personalization. Knowl.-Based Syst. 89 (2015), 553­568.",1,Query,True
392,"[21] Makoto P. Kato and Katsumi Tanaka. 2016. To Suggest, or Not to Suggest for Queries with Diverse Intents: Optimizing Search Result Presentation. In WSDM. 133­142.",0,,False
393,"[22] Johannes Kiesel, Arefeh Bahrami, Benno Stein, Avishek Anand, and Matthias Hagen. 2018. Toward Voice Query Clarification. In SIGIR. 1257­1260.",1,Query,True
394,[23] Weize Kong and James Allan. 2013. Extracting query facets from search results. In SIGIR. 93­102.,0,,False
395,"[24] John Lafferty and Chengxiang Zhai. 2001. Document Language Models, Query Models, and Risk Minimization for Information Retrieval. In SIGIR. 111­119.",1,Query,True
396,[25] Victor Lavrenko and W. Bruce Croft. 2001. Relevance-Based Language Models. In SIGIR. 120­127.,0,,False
397,"[26] Xiaolu Lu, Alistair Moffat, and J. Shane Culpepper. 2016. The effect of pooling and evaluation depth on IR metrics. Inf. Retr. Journal 19, 4 (2016), 416­445.",0,,False
398,"[27] Harshith Padigela, Hamed Zamani, and W. Bruce Croft. 2019. Investigating the Successes and Failures of BERT for Passage Re-Ranking. arXiv:1903.06902 (2019).",1,ad,True
399,[28] Joaquín Pérez-Iglesias and Lourdes Araujo. 2010. Standard Deviation as a Query Hardness Estimator. In SPIRE. 207­212.,1,Query,True
400,"[29] Roberto Pieraccini, Evelyne Tzoukermann, Z. Gorelov, Jean-Luc Gauvain, Esther Levin, Chin-Hui Lee, and Jay Wilpon. 1992. A speech understanding system based on statistical representation of semantics. In ICASSP. 193­196.",0,,False
401,[30] Jay M. Ponte and W. Bruce Croft. 1998. A Language Modeling Approach to Information Retrieval. In SIGIR. 275­281.,0,,False
402,"[31] Minghui Qiu, Liu Yang, Feng Ji, Wei Zhou, Jun Huang, Haiqing Chen, W. Bruce Croft, and Wei Lin. 2018. Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce. In ACL (2). 208­213.",0,,False
403,"[32] Chen Qu, Liu Yang, W. Bruce Croft, Johanne R. Trippas, Yongfeng Zhang, and Minghui Qiu. 2018. Analyzing and Characterizing User Intent in Informationseeking Conversations. In SIGIR. 989­992.",0,,False
404,[33] Filip Radlinski and Nick Craswell. 2017. A Theoretical Framework for Conversational Search. In CHIIR. 117­126.,1,ad,True
405,[34] Sudha Rao and Hal Daumé. 2018. Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information. In ACL (1). 2736­2745.,0,,False
406,[35] Sudha Rao and Hal Daumé III. 2019. Answer-based Adversarial Training for Generating Clarification Questions. arXiv:1904.02281 (2019).,0,,False
407,"[36] Siva Reddy, Danqi Chen, and Christopher D. Manning. 2018. CoQA: A Conversational Question Answering Challenge. arXiv:1808.07042 (2018).",0,,False
408,"[37] Gary Ren, Xiaochuan Ni, Manish Malik, and Qifa Ke. 2018. Conversational Query Understanding Using Sequence to Sequence Modeling. In WWW. 1715­1724.",1,Query,True
409,"[38] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In TREC. 109­126.",1,TREC,True
410,"[39] Damiano Spina, Johanne R. Trippas, Lawrence Cavedon, and Mark Sanderson. 2017. Extracting audio summaries to support effective spoken document search. JASIST 68, 9 (2017), 2101­2115.",0,,False
411,[40] Yueming Sun and Yi Zhang. 2018. Conversational Recommender System. In SIGIR. 235­244.,0,,False
412,"[41] Zhiliang Tian, Rui Yan, Lili Mou, Yiping Song, Yansong Feng, and Dongyan Zhao. 2017. How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models. In ACL (2). 231­236.",0,,False
413,"[42] Johanne R. Trippas, Damiano Spina, Lawrence Cavedon, Hideo Joho, and Mark Sanderson. 2018. Informing the Design of Spoken Conversational Search: Perspective Paper. In CHIIR. 32­41.",0,,False
414,"[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. arXiv:1706.03762 (2017).",0,,False
415,"[44] Alexandra Vtyurina, Denis Savenkov, Eugene Agichtein, and Charles L. A. Clarke. 2017. Exploring Conversational Search With Humans, Assistants, and Wizards. In CHI Extended Abstracts. 2187­2193.",0,,False
416,"[45] Marilyn A. Walker, Rebecca J. Passonneau, and Julie E. Boland. 2001. Quantitative and Qualitative Evaluation of Darpa Communicator Spoken Dialogue Systems. In ACL. 515­522.",0,,False
417,"[46] Yansen Wang, Chenyi Liu, Minlie Huang, and Liqiang Nie. 2018. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders. In ACL (1). 2193­2203.",0,,False
418,"[47] Jason D. Williams, Antoine Raux, Deepak Ramachandran, and Alan W. Black. 2013. The Dialog State Tracking Challenge. In SIGDIAL. 404­413.",1,Track,True
419,"[48] Qiang Wu, Christopher J. C. Burges, Krysta Marie Svore, and Jianfeng Gao. 2010. Adapting boosting for information retrieval measures. Inf. Retr. 13, 3 (2010), 254­270.",0,,False
420,"[49] Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to Respond with Deep Neural Networks for Retrieval-Based Human-Computer Conversation System. In SIGIR. 55­64.",0,,False
421,"[50] Rui Yan, Dongyan Zhao, and Weinan E. 2017. Joint Learning of Response Ranking and Next Utterance Suggestion in Human-Computer Conversation System. In SIGIR. 685­694.",0,,False
422,"[51] Liu Yang, Hamed Zamani, Yongfeng Zhang, Jiafeng Guo, and W. Bruce Croft. 2017. Neural Matching Models for Question Retrieval and Next Question Prediction in Conversation. arXiv:1707.05409 (2017).",0,,False
423,"[52] Chengxiang Zhai and John Lafferty. 2017. A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval. SIGIR Forum 51, 2 (2017), 268­276.",0,,False
424,"[53] Yongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, and W. Bruce Croft. 2018. Towards Conversational Search and Recommendation: System Ask, User Respond. In CIKM. 177­186.",0,,False
425,484,0,,False
426,,0,,False

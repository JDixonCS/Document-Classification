,sentence,label,data,regex
0,Short Research Papers 3C: Search,0,,False
1,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
2,Selecting Discriminative Terms for Relevance Model,0,,False
3,Dwaipayan Roy,0,,False
4,"Indian Statistical Institute, Kolkata dwaipayan_r@isical.ac.in",0,,False
5,Sumit Bhatia,0,,False
6,"IBM Research, Delhi, India sumitbhatia@in.ibm.com",0,,False
7,Mandar Mitra,0,,False
8,"Indian Statistical Institute, Kolkata mandar@isical.ac.in",0,,False
9,ABSTRACT,0,,False
10,"Pseudo-relevance feedback based on the relevance model does not take into account the inverse document frequency of candidate terms when selecting expansion terms. As a result, common terms are often included in the expanded query constructed by this model. We propose three possible extensions of the relevance model that address this drawback. Our proposed extensions are simple to compute and are independent of the base retrieval model. Experiments on several TREC news and web collections show that the proposed modifications yield significantly better MAP, precision, NDCG, and recall values than the original relevance model as well as its two recently proposed state-of-the-art variants.",1,ad,True
11,CCS CONCEPTS,0,,False
12,· Information systems  Query representation; Query reformulation.,1,Query,True
13,"ACM Reference Format: Dwaipayan Roy, Sumit Bhatia, and Mandar Mitra. 2019. Selecting Discriminative Terms for Relevance Model. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331357",0,,False
14,1 INTRODUCTION,1,DUC,True
15,"Query expansion (QE) is a popular technique used for handling the vocabulary mismatch problem in information retrieval [6]. In pseudo-relevance feedback (PRF) based query expansion methods, the top-ranked documents retrieved by using the original query are used to select terms for query expansion. As a result, they alleviate the need to use external sources of information for query expansion making PRF based retrieval models [1, 10, 12] as amongst the most widely used query expansion methods in practice.",1,Query,True
16,RM3 [8] is one of the most commonly used method for PRF and experiments reported by Lv and Zhai [11] demonstrated its robustness on different collections when compared with other feedback methods. They also found that RM3 term weighing is prone to select generic words as expansion terms that cannot help identify relevant documents. This occurrence of noise is a well-known shortcoming of PRF methods in general due to presence of non-relevant documents in the pseudo-relevant set (as precision is often less than one) [10]. The pseudo-relevant document set contains considerable,0,,False
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331357",1,ad,True
18,"noise that can lead to the expanded query drifting away from the original query [10]. Cao et al. [2] studied the utility of terms selected for expansion and found that only about a fifth of the expansion terms identified for query expansion contributed positively to an improvement in retrieval performance. Rest of the terms either had no impact on retrieval performance or led to the reduction in final retrieval performance. Background Work: In order to prevent the inclusion of common terms in expanded queries, different methods have been proposed to compute better term weights for PRF models such as incorporating proximity of expansion terms to query terms [12], classifying expansion terms as good and bad by using features derived from term distributions, co-occurrences, proximity, etc. [2]. Parapar and Barreiro [15] proposed RM3-DT, an alternative estimation of term weighting in RM by subtracting the collection probability of the term from its document probability, thus giving a high weight to terms that have a higher probability of being present in the pseudorelevant document set rather than the whole collection.",1,ad,True
19,"Clinchant and Gaussier [4] described an axiomatic framework and discussed five axioms for characterizing different PRF models. Building upon this framework, various improvements and modifications of the original relevance model have been proposed [1, 13, 14] to select better terms for query expansion and compute better weights by incorporating new constraints and assumptions in the computation of scores of terms present in the set of feedback documents. Recently, Montazeralghaem et al. [14] proposed two additional constraints to consider interdependence among previously established characteristics of pseudo-relevant feedback (PRF) models. The first constraint (TF-IDF constraint) considers the interrelationship of TF and IDF on PRF models and the second constraint (relevance score constraint) focuses on the interdependence of the feedback weight of selected terms and the relevance scores of documents containing the term. They proposed RM3+All, an extension of the RM3 model that accounts for these constraints. Our Contributions: We propose three alternate heuristics for selecting the terms for query expansion that involve minimal computations on top of relevance model. Our selection process is simple and re-ranks the terms present in feedback documents based on the ratio of likelihoods of these terms being generated by the relevant and non-relevant document sets. We perform a thorough empirical evaluation of the proposed methods on standard TREC collections, ranging from TREC ad hoc collections to ClueWeb09 and compare the performance two state-of-the-art baselines (RM3DT [15] and RM3+All [14]). Despite being simple, the proposed methods significantly outperform RM3, RM3-DT, and RM3+ALL in terms of retrieval performance, are more robust and are computationally more efficient. We make our implementation available as a Lucene plugin1 to enable further reuse and replication of our experiments.",1,corpora,True
20,1 https://github.com/dwaipayanroy/rm3idf,0,,False
21,1253,0,,False
22,Short Research Papers 3C: Search,0,,False
23,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
24,2 PROPOSED METHOD,0,,False
25,2.1 Intuition Behind Proposed Approaches,0,,False
26,"According to the relevance model (RM) [9], for a given query Q ,"" {q1, . . . , qk }, the feedback weight of an expansion term w is computed as follows.""",0,,False
27,"FWrm (w ) , P (w | R) , P (w |d ) P (q |d )",0,,False
28,(1),0,,False
29,d D,0,,False
30,q Q,0,,False
31,"Here, D is the pseudo-relevant document set.",0,,False
32,"In RM3 [8], the feedback weight is estimated by linearly interpolating the query likelihood model of w (P(w |Q)) with the original relevance model R to obtain R as follows.",0,,False
33,"FWrm3(w ) , P (w | R) ,  P (w |Q ) + (1 -  )P (w | R)",0,,False
34,(2),0,,False
35,"Here,   [0, 1] is the interpolation parameter, and P(w |Q) is estimated using maximum likelihood estimation.",0,,False
36,"Let us consider a term t that is present in a large number of documents in the collection. If t is present in the pseudo-relevant document set, the value of P(t |R) (according to Equation 1) is likely to be quite high, thereby increasing the possibility of selecting t as an expansion term [4, 5, 7]. However, since t is also present in a large number of documents outside the pseudo-relevant document set, this term lacks the discriminating power to distinguish between relevant and non-relevant documents. Furthermore, during retrieval with the expanded query, such terms may adversely affect the computation time by unnecessarily increasing the query length. Additionally, if the term weights in the expanded query are sum-normalized (as done by Lavernko and Croft [9], Jaleel et al. [8]), these common terms may together reduce the weights of important terms in the expanded query. In sum, the inability of the RM3 model to filter out such terms makes it vulnerable to a potential degradation in retrieval performance. Intuitively, terms with a high discriminative power are expected to have different distributions over the set of relevant and non-relevant documents. Mathematically, this difference can be captured by the ratio of the occurrence probabilities of the term t being selected from the relevance (R) and non-relevance (N ) classes, similar to the ideas by Robertson and Zaragoza [16]. Formally, for a given query Q, we consider two separate language models, associated respectively with the relevance class (R, estimated following RM [9]) and non-relevance class (N ) of the query. To estimate N , we choose the commonly-adopted alternative of using the whole document collection C since, for a typical query Q, most of the documents in the collection are used to be non-relevant. Based on this intuition, we now describe three approaches to compute the feedback term weights to select and weigh terms for query expansion.",1,ad,True
37,2.2 RM3+1,0,,False
38,"In our first proposed approach, we compute the weights of candidate expansion terms by using P(w |R)/P(w |N ). The probability of selection of a term from the relevance class (P(w |R)) can be computed using Equation (1). We adopt the common practice of approximating the non-relevance class is by using the collection model C. Note that the basic intuition behind incorporating the non-relevance model as a factoring component is to identify terms with discriminating features that can also be considered as an estimation of rareness. Formally, the feedback weight of term w in the",1,ad,True
39,Document Collection #Docs Collection Type,0,,False
40,Query Query Set Fields,1,Query,True
41,Query Dev Test Ids Set Set,1,Query,True
42,"TREC Disk 1, 2",1,TREC,True
43,News,0,,False
44,"741,856",0,,False
45,Title,0,,False
46,"TREC 1 ad-hoc TREC 2, 3 ad-hoc",1,TREC,True
47,51-100  101-200,0,,False
48,"TREC Disks 4, 5 News exclude CR",1,TREC,True
49,TREC 6 ad-hoc 301-350 ,1,TREC,True
50,"528,155 Title TREC 7, 8 ad-hoc 351-450",1,TREC,True
51,TREC Robust 601-700,1,TREC,True
52,GOV2 Web,0,,False
53,"25,205,179 Title",0,,False
54,"TREC Terabyte 1 701-750  TREC Terabyte 2,3 751-850",1,TREC,True
55,WT10G CW09B-S70,1,WT,True
56,Web,0,,False
57,"1,692,096 50,220,423",0,,False
58,Title,0,,False
59,WT10G ClueWeb09,1,WT,True
60,451-550 ,0,,False
61,1-200,0,,False
62,Table 1: Dataset Overview,0,,False
63,expanded query is computed as follows:,0,,False
64,FW (w ),0,,False
65,",",0,,False
66,P (w | R) P (w | N),0,,False
67,P (w | R) P (w | C),0,,False
68,P (w | R)  r (w ),0,,False
69,(3),0,,False
70,where r (w)  1/P(w |C) can be interpreted as a measure of rareness of w. We may replace r (w) by any other measure of a term's rareness.,0,,False
71,"For our experiments, we use the traditional inverse document frequency factor in place of r (w) (since this yields better retrieval",1,ad,True
72,"effectiveness). Next, the weights are sum-normalized and the top N terms with the highest FW (w) values are selected as expansion",0,,False
73,"terms. The normalized feedback weights (denoted NFW ) are then combined with the query likelihood model Q. Mathematically, the final weight of a term w in the expanded query is given by:",0,,False
74,"FWrm3+1 (w ) ,  P (w |Q ) + (1 -  )  NFW (w )",0,,False
75,(4),0,,False
76,"If Dirichlet smoothing is used (as recommended in [18]), then, dur-",0,,False
77,"ing retrieval using the expanded query Q, the score of a document d is computed in practice as follows:",0,,False
78,"Score(d,",0,,False
79,Q),0,,False
80,",",0,,False
81,NFW rm31+ (q),0,,False
82,q Q,0,,False
83,×,0,,False
84,log,0,,False
85,µ .P (q | C) + |d |.P (q |d ) P (q | C) µ + |d |,0,,False
86,(5),0,,False
87,"where NFW (q) represents the normalized weight of q in the expanded query, µ is the smoothing parameter, and |d | represents the number of indexed words in d.",0,,False
88,2.3 RM3+2,0,,False
89,"In RM3+1 , a term w is weighted on the basis of a linear combination between the query language model (P(w |Q)) and the ratio between",0,,False
90,term selection probability from RM (Equation (1)) and from the,0,,False
91,"non-relevance model. As an alternative approach, we consider the RM3 model (instead of RM in RM3+1 ) to approximate the relevance class, i.e, a term w is weighted by the probability ratio of how",1,ad,True
92,likely w is from the query-relevance model (RM3) and from the,0,,False
93,"non-relevance model. Formally, candidate terms are weighted using",0,,False
94,a P,0,,False
95,ratio similar to (w |R)/P(w |N ),0,,False
96,"the one used in RM3+1 , but unlike to formulate the expansion term",0,,False
97,"RM3+1 , we use weight. Here,",0,,False
98,"P(w |R) is approximated by Equation 2, and P(w |N ) is estimated",0,,False
99,"as in RM3+1 . Thus, the weight of term w in the expanded query is",0,,False
100,computed as follows.,0,,False
101,P (w | R) P (w | R),0,,False
102,"FW rm32+ (w ) , P (w | N) , P (w | C)",0,,False
103,(6),0,,False
104,  P (w |Q )  r(w ) + (1 -  )P (w | R)  r(w ),0,,False
105,"Note that in this approach, the final score for a document incorporates the rareness information ""twice"": first, as a part of the computation of FW (q) (Equation (6)), and again during retrieval because of the factor of P(q|C) in the denominator in Equation (5).",1,corpora,True
106,1254,0,,False
107,Short Research Papers 3C: Search,0,,False
108,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
109,"This overemphasis on the rareness of query terms may promote highly rare (but noisy) terms at the cost of useful, but more frequent query terms leading to a possible drop in retrieval effectiveness.",1,ad,True
110,2.4 RM3+3,0,,False
111,"To address the potential issues due to double application of rareness information we adopt an approach similar to Carpineto et al. [3] where candidate expansion terms are ranked using one weighing function, and a different function is used to determine the final weight of the selected expansion terms. Specifically, we first rank the candidate expansion terms using Equation (6), and select the top N terms. Since rareness information is used in Equation (6), we hope that this will avoid the problem of selecting low-IDF words as expansion terms. Next, the final feedback weight for the selected terms is computed using Equation (2) instead of Equation (6). This avoids the use of a ""double"" IDF factor during final retrieval. Formally, the feedback term weight is computed as follows.",1,ad,True
112,"FW rm33+ (w) , P(w |R) , P(w |Q) + (1 -  )P(w |R) (7)",0,,False
113,3 EXPERIMENTS,0,,False
114,3.1 Datasets and Experimental Settings,0,,False
115,"We evaluate the proposed modifications to RM3 using standard TREC news and web collections. For parameter tuning, we split the topic sets into development and testing sets as summarized in Table 1.All the collections are stemmed using Porter's stemmer and stopwords are removed prior to indexing using Apache Lucene. For the initial retrieval using the original, unexpanded queries, we used the Dirichlet smoothed language model.All the baselines and the proposed methods were implemented using Lucene.",1,TREC,True
116,"Baselines: For comparing with expansion based methods, we choose three baselines ­ (i) RM3 [8], (ii) RM3DT [15], a variant of RM3 that promotes divergent terms, and (iii) RM3+All [14], a recently proposed state-of-the-art modification of RM3.",0,,False
117,"Parameter Tuning: There are three parameters associated with the RM3 based QE method: number of documents (M), number of terms (N ), and smoothing parameter () that adjusts the importance of query terms. The parameters M and N were varied in the range {10, 15, 20} and {30, 40, 50, 60, 70} respectively and  was varied in the range {0.1, . . . , 0.9} in steps of 0.1. The parameters were tuned individually for each of the methods on the development topics, and applied on the corresponding test topics (see Table 1). There are no additional parameters associated with RM3DT, RM3+All, and the proposed methods. The smoothing parameter µ is varied in the range {100, 200, 500, 1000, 2000, 5000} and set to 1000 at which the optimal performance is observed on development topic sets. All our experiments were performed on a VM with Intel Xeon 2.80GHz CPU containing 80 cores, and 100GB of RAM.",1,ad,True
118,3.2 Results and discussion,0,,False
119,"Retrieval Performance: Table 2 presents the results of retrieval experiments with all the baselines over all the test collections. We observe that for all the test collections, all of the three proposed modifications outperform the traditional RM3 [8], as well as the",1,ad,True
120,"two baselines, for most of the evaluation measures. We also note",0,,False
121,that the second proposed compared to RM3+1 and,0,,False
122,RmMet3h+3o.dF(oRrMT3R+2E)Cis,0,,False
123,"seen to be less effective news topics, the mean",0,,False
124,"average precision (MAP), recall and NDCG at rank 10 are almost",1,MAP,True
125,always seen to be significantly better for both RM3+1 and RM3+3 than,0,,False
126,"the baselines. Improvement is also observed for P@10, however",0,,False
127,they are not significant for most of the topic sets. Results for TREC,1,TREC,True
128,"Terabyte track 2 and 3 topics exhibit similar improvements over the baselines. For ClueWeb09 topics, RM3+3 achieves significant improvements in MAP, recall, and P@10 values over RM3 and RM3DT. Compared to RM3+All, RM3+3 achieves significantly better MAP and recall values. We also report robustness index (RI) [17]",1,Terabyte,True
129,to compare the robustness of methods when compared with the,0,,False
130,"original RM3 model. We observe that the performance of both RM3+1 and RM3+3 is consistently more robust than the baselines. RM3DT achieves negative RI values for Terabyte 2,3 and ClueWeb",1,Terabyte,True
131,collections indicating that more queries suffered in terms of retrieval,0,,False
132,"performance than the queries that gained in retrieval performance. On the other hand, RM3+3 consistently achieved best RI values across all collections, except for the ClueWeb collection where it is a very close second. Thus, in terms of retrieval performance, RM3+3 achieves overall best performance among the baselines and the three proposed heuristics, followed by RM3+1 .",1,ClueWeb,True
133,"Computational Latency: For each test collection, Table 4 reports",0,,False
134,the total time taken by different methods to execute all the queries,0,,False
135,"(averaged over ten runs). Note that, we only compare the elapsed",0,,False
136,time for expansion term selection as the actual retrieval times,0,,False
137,(traversing inverted lists) would be common for all the methods.,0,,False
138,"We observe that the execution time for the three proposed methods are consistently less than RM3+All. Further, RM3+3 takes the least amount of additional time over RM3 for finding expansion terms for",1,ad,True
139,"Terabyte 2,3 and ClueWeb collections. Reading results in Tables 4 and 2 together, we conclude that the RM3+3 method, for most cases, achieves best retrieval performance, is robust, and computes the",1,Terabyte,True
140,expansion terms faster than other methods studied.,0,,False
141,"Qualitative Analysis: We compare expansion terms selected by RM3, RM3DT, RM3+All and RM3+3 (best among the proposed modifications). Table 3 presents the top 15 expansion terms for topic",0,,False
142,nuclear proliferation (from TREC123 collection) for those,1,TREC,True
143,"methods. Observe that as discussed before, RM3 is prone to se-",0,,False
144,lecting common terms (low IDF) as exemplified by year in the,0,,False
145,"list.On the other hand, RM3+All selects terms with significantly",0,,False
146,"high IDF values. For example, expansion terms kalayeh, qazvin",0,,False
147,"and yongbyon have collection frequency 1, 4 and 8 respectively",0,,False
148,"in TREC 123 collection. However, such extremely rare terms may",1,TREC,True
149,"often be due to noise rather than topical relevance to the original query. In contrast, RM3+3 strikes a balance by preventing frequent terms from occupying top weights as well as not prioritizing very",0,,False
150,rare terms.,0,,False
151,4 CONCLUSION,0,,False
152,We proposed three modifications to RM3 model to promote selection of discriminative terms for query expansion. A thorough empirical evaluation was performed using TREC news and Web collections and results compared with two state-of-the-art RM3 variants for promoting high IDF terms for query expansion. The,1,TREC,True
153,1255,0,,False
154,Short Research Papers 3C: Search,0,,False
155,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
156,Metrics,0,,False
157,TREC23,1,TREC,True
158,MAP P@10 Recall NDCG@10,1,MAP,True
159,RI,0,,False
160,TREC78Rb,1,TREC,True
161,MAP P@10 Recall NDCG@10,1,MAP,True
162,RI,0,,False
163,"MAP P@10 Terabyte 2,3 Recall NDCG@10",1,MAP,True
164,RI,0,,False
165,CW09B,1,CW,True
166,MAP P@10 Recall NDCG@10,1,MAP,True
167,RI,0,,False
168,LM,1,LM,True
169,0.2325 0.4990 0.6142 0.5065,0,,False
170,-,0,,False
171,0.2550 0.4372 0.7172 0.4406,0,,False
172,-,0,,False
173,0.2918 0.5680 0.7076 0.4943,0,,False
174,-,0,,False
175,0.1065 0.2258 0.4494 0.1693,0,,False
176,-,0,,False
177,RM3,0,,False
178,0.29360 0.54400 0.67340 0.54490,0,,False
179,-,0,,False
180,0.29060 0.45130 0.78280 0.4478,0,,False
181,-,0,,False
182,0.32010 0.60100 0.73640 0.51550,0,,False
183,-,0,,False
184,0.1081 0.23010 0.45390 0.1699,0,,False
185,-,0,,False
186,RM3DT,0,,False
187,0.29890 0.54380 0.67830 0.54400,0,,False
188,0.11,0,,False
189,"0.29310, 1 0.45910 0.78810 0.45030, 1",0,,False
190,0.12,0,,False
191,0.31680 0.59950 0.73850 0.51340,0,,False
192,-0.05,0,,False
193,0.1078 0.22970 0.45140,0,,False
194,0.1695,0,,False
195,-0.01,0,,False
196,RM3+All,0,,False
197,"0.29960, 1 0.54100 0.67960 0.55110, 1, 2",0,,False
198,0.16,0,,False
199,"0.29460, 1, 2 0.46170 0.78910 0.45490, 1",0,,False
200,0.19,0,,False
201,"0.32120, 2 0.60040 0.73910 0.52310, 1, 2",0,,False
202,0.14,0,,False
203,0.1081 0.23210 0.45910,0,,False
204,0.1702,0,,False
205,0.10,0,,False
206,"RM3+1 0.30540, 1, 2, 3",0,,False
207,"0.55400 0.69060, 1, 2, 3 0.55380, 1, 2, 3",0,,False
208,0.12,0,,False
209,"0.29980, 1 0.47040, 1, 2",0,,False
210,"0.79630 0.45510, 1",0,,False
211,0.11,0,,False
212,"0.33720, 1, 2, 3 0.61300",0,,False
213,"0.75030, 1, 2, 3 0.52340, 1, 2",0,,False
214,0.24,0,,False
215,"0.11370, 1, 2, 3 0.23180",0,,False
216,"0.47020, 1, 2, 3",0,,False
217,0.1692,0,,False
218,0.23,0,,False
219,"RM3+2 0.30300, 1 0.54900 0.68590, 1, 2, 3 0.55100, 1",0,,False
220,0.11,0,,False
221,"0.29520, 1 0.47140, 1, 2, 3",0,,False
222,0.79230 0.45600,0,,False
223,0.06,0,,False
224,"0.32370, 2 0.61200 0.73390 0.52020, 2",0,,False
225,0.02,0,,False
226,0.1080,0,,False
227,0.2328,0,,False
228,0.4487,0,,False
229,0.1664,0,,False
230,0.00,0,,False
231,"RM3+3 0.30750, 1, 2, 3",0,,False
232,"0.54100 0.68300, 1, 2, 3",0,,False
233,0.53990,0,,False
234,0.30,0,,False
235,"0.30360, 1, 2, 3 0.46780, 1, 2 0.79760, 1, 2, 3 0.45770, 1, 2",0,,False
236,0.27,0,,False
237,"0.33230, 1, 2, 3 0.61700, 1, 2, 3 0.74460, 1, 2 0.52520, 1, 2",0,,False
238,0.32,0,,False
239,"0.11480, 1, 2, 3 0.23740, 1, 2 0.46870, 1, 2, 3",0,,False
240,0.1737,0,,False
241,0.22,0,,False
242,"Table 2: Performance of the proposed methods and different baselines. Statistically significant improvements (measured by paired t-Test with 95% confidence) over LM, RM3, RM3DT and RM3+ALL are indicated by superscript 0, 1, 2 and 3, respectively, with maximum improvement in bold-faced.",1,LM,True
243,nuclear proliferation,0,,False
244,RM3 RM3DT RM3-All,0,,False
245,"nuclear. prolifer, weapon, 10, nation, year, soviet, 1, state, spread, date, regim, call, goal, limit nuclear, prolifer, intellig, cia, gate, mr, countri, weapon, soviet, iraq, chemic, effort, commun, agenc nuclear, prolifer, treati, weapon, farwick, pakistan, ntg, spector, qazvin, signatori, southasia, argentina, kalayeh, yongbyon, mcgoldrick",1,ad,True
246,"RM3+3 nuclear, prolifer, treati, weapon, soviet, intern, signatori, nation, armamaent, assess, review, regim, union, spread, peac",1,ad,True
247,"Table 3: Top 15 expansion terms selected by RM3, RM3+All and RM3+3",0,,False
248,Topics,0,,False
249,RM3,0,,False
250,Latency,0,,False
251,RM3DT RM3+ALL,0,,False
252,RM3+1,0,,False
253,RM3+2,0,,False
254,RM3+3,0,,False
255,TREC23,1,TREC,True
256,314 320 (1%) 390 (24%) 325 (3%) 353 (12%) 349 (11%),0,,False
257,TREC78Rb 756 773 (2%) 847 (12%) 769 (1%) 818 (8%) 778 (2%),1,TREC,True
258,"Tb 2,3",0,,False
259,997 1005 (0.7%) 1046 (4%) 1004 (0.7%) 1009 (1%) 1002 (0.5%),0,,False
260,CW09B 1547 1789 (15%) 1897 (22%) 1825 (17%) 1806 (16%) 1743 (12%),1,CW,True
261,Table 4: Average computational latency in milliseconds for different methods on test queries. Percentage increase over RM3 (in parentheses); lowest increase in bold.,0,,False
262,results suggested that the proposed heuristics (especially RM3+3 ) yield significant improvements in performance when compared,0,,False
263,"with the baselines. Further, the improvements are more robust and",0,,False
264,the proposed heuristics are computationally more efficient than the,0,,False
265,baselines.,0,,False
266,REFERENCES,0,,False
267,[1] M. Ariannezhad et al. 2017. Iterative Estimation of Document Relevance Score for Pseudo-Relevance Feedback. In Proc. of ECIR. 676­683.,1,ad,True
268,"[2] G. Cao, J. Nie, J. Gao, and S. Robertson. 2008. Selecting Good Expansion Terms for Pseudo-relevance Feedback. In Proc. of 31st ACM SIGIR. ACM, 243­250.",0,,False
269,"[3] C. Carpineto, R. de Mori, G. Romano, and B. Bigi. 2001. An information-theoretic approach to automatic query expansion. ACM Trans. Inf. Syst. 19, 1 (2001), 1­27.",0,,False
270,"[4] Stéphane Clinchant and Eric Gaussier. 2013. A Theoretical Analysis of PseudoRelevance Feedback Models. In Proc. of ICTIR 2013. Article 6, 8 pages.",0,,False
271,[5] Ronan Cummins. 2017. Improved Query-Topic Models Using Pseudo-Relevant PóLya Document Models. In Proc. of the ACM ICTIR 2017. 101­108.,1,Query,True
272,"[6] G. Furnas, T. Landauer, L. Gomez, and S. Dumais. 1987. The Vocabulary Problem in Human-system Communication. Commun. ACM 30, 11 (1987), 964­971.",0,,False
273,[7] H. Hazimeh and C. Zhai. 2015. Axiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance Feedback. In Proc. of ICTIR 2015. 141­150.,0,,False
274,"[8] N. A. Jaleel, J. Allan, W. B. Croft, F. Diaz, L. S. Larkey, X. Li, M. D. Smucker, and C. Wade. 2004. UMass at TREC 2004: Novelty and HARD. In Proc. TREC '04.",1,ad,True
275,"[9] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proc. of 24th SIGIR (SIGIR '01). ACM, New York, NY, USA, 120­127.",0,,False
276,"[10] Kyung Soon Lee, W B. Croft, and J. Allan. 2008. A cluster-based resampling method for pseudo-relevance feedback. In SIGIR. 235­242.",0,,False
277,[11] Y. Lv and C. Zhai. 2009. A Comparative Study of Methods for Estimating Query Language Models with Pseudo Feedback. In Proc. of 18th ACM CIKM. 1895­1898.,1,Query,True
278,"[12] Y. Lv and C. Zhai. 2010. Positional relevance model for pseudo-relevance feedback. In Proc. of 33rd ACM SIGIR. ACM, 579­586.",0,,False
279,"[13] A. Montazeralghaem, H. Zamani, and A. Shakery. 2016. Axiomatic Analysis for Improving the Log-Logistic Feedback Model. In Proc. of 39th ACM SIGIR. 765­768.",0,,False
280,"[14] A. Montazeralghaem, H. Zamani, and A. Shakery. 2018. Theoretical Analysis of Interdependent Constraints in Pseudo-Relevance Feedback. In SIGIR. 1249­1252.",0,,False
281,[15] Javier Parapar and Álvaro Barreiro. 2011. Promoting Divergent Terms in the Estimation of Relevance Models. In Proc. of Third ICTIR'11. 77­88.,0,,False
282,"[16] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (April 2009), 333­389.",0,,False
283,"[17] T. Sakai, T. Manabe, and M. Koyama. 2005. Flexible pseudo-relevance feedback via selective sampling. ACM TALIP 4, 2 (2005), 111­135.",0,,False
284,"[18] C. Zhai and J. Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM TOIS. 22, 2 (2004), 179­214.",0,,False
285,1256,0,,False
286,,0,,False

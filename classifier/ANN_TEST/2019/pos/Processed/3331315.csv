,sentence,label,data,regex
0,Short Research Papers 2B: Recommendation and Evaluation,0,,False
1,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
2,A New Perspective on Score Standardization,0,,False
3,Julián Urbano,0,,False
4,Delft University of Technology The Netherlands,0,,False
5,urbano.julian@gmail.com,0,,False
6,Harlley Lima,0,,False
7,Delft University of Technology The Netherlands,0,,False
8,h.a.delima@tudelft.nl,0,,False
9,Alan Hanjalic,0,,False
10,Delft University of Technology The Netherlands,0,,False
11,a.hanjalic@tudelft.nl,0,,False
12,ABSTRACT,0,,False
13,"In test collection based evaluation of IR systems, score standardization has been proposed to compare systems across collections and minimize the effect of outlier runs on specific topics. The underlying idea is to account for the difficulty of topics, so that systems are scored relative to it. Webber et al. first proposed standardization through a non-linear transformation with the standard normal distribution, and recently Sakai proposed a simple linear transformation. In this paper, we show that both approaches are actually special cases of a simple standardization which assumes specific distributions for the per-topic scores. From this viewpoint, we argue that a transformation based on the empirical distribution is the most appropriate choice for this kind of standardization. Through a series of experiments on TREC data, we show the benefits of our proposal in terms of score stability and statistical test behavior.",1,TREC,True
14,KEYWORDS,0,,False
15,"Evaluation, test collection, score standardization, statistical testing",0,,False
16,"ACM Reference Format: Julián Urbano, Harlley Lima, and Alan Hanjalic. 2019. A New Perspective on Score Standardization. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331315",0,,False
17,1 INTRODUCTION,1,DUC,True
18,"In the traditional Cranfield paradigm for test collection based evaluation in Information Retrieval (IR), systems are evaluated and compared by assessing their effectiveness on the set of topics contained in a collection. Specifically, an effectiveness measure like Average Precision is used to score every system with every topic, and the per-system mean scores over topics are often used as the single indicator of performance to rank systems [4]. It is well known in the IR literature that this paradigm does not allow to compare the performance of systems tested on different collections. The main reason for this is the very large variability we find in topic difficulty [1, 6, 8]. A system with a good score on one collection may very well achieve a low score on another. Even when comparing systems using the same collection, not all topics contribute equally to the final score because of their differences in difficulty (see for",1,ad,True
19,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331315",1,ad,True
20,"instance the bottom-left plot in Figure 1). Therefore, the observed",0,,False
21,differences in mean scores may be disproportionately due to a few,0,,False
22,"topics in the collection [2, 9].",0,,False
23,"To mitigate this problem, Webber et al. [9] proposed a two-step",0,,False
24,standardization process to look at scores relative to the difficulty of,0,,False
25,"the topic. First, given a raw effectiveness score x of some system",0,,False
26,"on some topic, a traditional z-score is computed",1,ad,True
27,"x -µ z,""  ,""",0,,False
28,(1),0,,False
29,where µ and  are the mean and standard deviation of the system,0,,False
30,scores for the topic. The effect is twofold: whether the topic is easy,0,,False
31,"or hard (high or low µ), the distribution of z-scores is centered at",0,,False
32,zero; and whether systems perform similarly for the topic or not,0,,False
33,"(low or high  ), the z-scores have unit variance. Thanks to this first",0,,False
34,"step, all topics contribute equally to the final scores.",0,,False
35,The second step is a transformation of the z-score so that the final,0,,False
36,"standardized score is bounded between 0 and 1, as is customary",0,,False
37,in IR measures. Webber et al. [9] propose to use the cumulative,0,,False
38,"distribution function (cdf ) of the standard normal distribution,",0,,False
39,which naturally maps z-scores on to the unit interval:,0,,False
40,", (z).",0,,False
41,(2),0,,False
42,"Recently, Sakai [3] proposed a simple linear transformation of the z-score instead of the non-linear transformation applied by :",1,ad,True
43,", Az + B.",0,,False
44,(3),0,,False
45,"On the grounds of Chebyshev's inequality, they further suggested A, 0.15 and B ,"" 0.5 so that at least 89% of the scores will fall within [0.05, 0.95]. Furthermore, and to ensure that standardized scores always stay within the unit interval, they proposed to simply censor""",0,,False
46,"between 0 and 1, computing ,"" max(min(1, Az + B), 0) in reality. In this paper we show that the standardizations by Webber et al. [9] and Sakai [3] are actually special cases of a general class of standardizations consisting in assuming a specific distribution for the per-topic scores, and that they differ only in what distribution they assume. From this new perspective, we argue that the empirical distribution is actually the most appropriate choice because of its properties. We also carry out two experiments on TREC data that show how our proposal behaves better than both raw scores and the previous standardization schemes.""",1,TREC,True
47,2 SCORE STANDARDIZATION,0,,False
48,"Let F be the distribution of scores by some population of systems on some particular topic and according to some specific measure like AP. If we knew this distribution, we could standardize a raw score x simply by computing , F (x) ,"" P(X  x), which naturally bounds""",1,AP,True
49,"between 0 and 1. The reasoning is that the cdf actually tells us where x is with respect to the rest of scores that one may expect for the topic, precisely computing the fraction of systems with lower",0,,False
50,1061,0,,False
51,Short Research Papers 2B: Recommendation and Evaluation,0,,False
52,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
53,1062,0,,False
54,Short Research Papers 2B: Recommendation and Evaluation,0,,False
55,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
56,"with slight deviations if censoring is needed. In the case of E-std, we see nearly constant mean and variance. This is achieved by design, because, in general, if X  F , then Y ,"" F (X ) follows a standard uniform. Therefore, E-std produces standardized scores that are uniformly distributed, ensuring "", 0.5 and s , 1/12.",0,,False
57,"The point still remains that µ and  are also unknown. The way around this limitation is to estimate them from a previous set of systems (called standardization systems by [3, 9]). Thus, given the scores X1, . . . , Xn of these systems, the estimates are the per-topic sample mean µ^ ,X and standard deviation ^ ,""sX . For E-std, these are precisely the data used in eq. (7) to standardize. In principle, these standardization systems should represent the system population of interest, which ultimately determines the topic difficulty through the per-topic distributions. In our view, the most reasonable choice would be the state of the art systems, which in a TREC collection are arguably the set of systems participating in the track.""",1,TREC,True
58,3 EXPERIMENTS,0,,False
59,"This section reports on two experiments to assess the effect of standardization. In the first one, we consider system comparisons using the same test collection (within-collection), while in the second one we consider comparisons between systems evaluated on different collections (between-collection). Comparisons will be made between results produced by the raw scores, N-std, U-std and E-std. For completeness, we also evaluate the standardization scheme that simply computes the z-score as in eq. (1) and therefore produces unbounded scores. This scheme is called z-std.",1,ad,True
60,"The data used in our experiments are the TREC 2004 Robust (RB) and TREC 2006 Terabyte (TB) collections. The RB data contains 110 systems evaluated on the 99 TREC 2003­2004 Robust topics. The TB data contains 61 systems on the 149 TREC 2004­2006 Terabyte topics. In terms of measures, we use AP and nDCG.",1,TREC,True
61,3.1 Within-Collection Comparisons,0,,False
62,"In order to investigate the effect of standardization on withincollection comparisons, we proceed as follows. We randomly sample 50 topics from the full set and compute the raw scores and the standardized scores as per each of the standardization schemes. From these data we compute three statistics. First, we compute the correlation between the ranking of systems by raw scores and the ranking by standardized scores, using Kendall's  and Yilmaz's ap [10]1. A high correlation indicates that the standardized scores are not much different from the raw scores, so in principle we look for lower coefficients. The third indicator evaluates the statistical power of the evaluation. In particular, we run a 2-tailed paired t-test between every pair of systems and, under the assumption that the null hypothesis is indeed false, look for schemes that maximize power. The process is repeated 10,000 times with both the RB and TB datasets, on both AP and nDCG.",1,TB,True
63,"Figure 2 shows the results for a selection of collection-measure combinations2. The two plots in the first row show the distributions of  correlations. As expected, U-std and z-std perform very similarly because the former is simply a linear transformation of",0,,False
64,"1In particular, we compute b and ap,b to deal with tied systems. See [5] for details. 2All plots, along with data and code to reproduce results, are available from https://github.com/julian-urbano/sigir2019-standardization.",0,,False
65,N-std U-std z-std E-std,0,,False
66,Robust - AP,1,Robust,True
67,20,0,,False
68,Terabyte - AP,1,Terabyte,True
69,N-std U-std z-std E-std,0,,False
70,Density,0,,False
71,0 5 10,0,,False
72,Density 0 5 15 25,0,,False
73,Density 0 5 10 15,0,,False
74,Density 0 5 10 15 20 25,0,,False
75,0.86,0,,False
76,0.90,0,,False
77,0.94,0,,False
78,Robust - nDCG,1,Robust,True
79,N-std U-std z-std E-std,0,,False
80,0.98,0,,False
81,0.86,0,,False
82,0.90,0,,False
83,0.94,0,,False
84,Terabyte - nDCG,1,Terabyte,True
85,N-std U-std z-std E-std,0,,False
86,0.98,0,,False
87,0.75,0,,False
88,0.80,0,,False
89,0.85 0.90 ap,0,,False
90,0.95,0,,False
91,1.00,0,,False
92,Robust - AP,1,Robust,True
93,0.75,0,,False
94,0.80,0,,False
95,0.85 0.90 ap,0,,False
96,0.95,0,,False
97,1.00,0,,False
98,Terabyte - nDCG,1,Terabyte,True
99,Power 0.55 0.65 0.75,0,,False
100,Power 0.45 0.55 0.65,0,,False
101,0.001,0,,False
102,raw N-std U-std z-std E-std,0,,False
103,0.005,0,,False
104,0.020 0.050,0,,False
105,Significance level ,0,,False
106,0.001,0,,False
107,raw N-std U-std z-std E-std,0,,False
108,0.005,0,,False
109,0.020 0.050,0,,False
110,Significance level ,0,,False
111,Figure 2: Within-collection comparisons. First row:  correlation between rankings of systems with raw and standardized scores (lower is better); rugs mark the means. Second row: ap correlation (lower is better). Third row: power of paired t-tests at various  levels (higher is better).,0,,False
112,"the latter; differences come from the necessity to censor outliers in U-std. Indeed, because they are both a linear transformation of the raw scores, they produce the most similar rankings. N-std results in slightly lower correlations, but E-std sets itself clearly apart from the others, yielding significantly lower  scores. The plots in the second row show even clearer differences in terms of ap . We see that U-std and z-std are almost identical, but more importantly we see that N-std and E-std are even further away, likely because they eliminate outliers that could affect the top ranked systems.",0,,False
113,"The two plots in the last row show statistical power for a range of significance levels. We can first observe that all standardization schemes achieve higher power than the raw scores, showing a clear advantage of the standardization principle. Once again, U-std and z-std perform nearly identically, and both are outperformed by N-std and, specially, E-std.",1,ad,True
114,3.2 Between-Collection Comparisons,0,,False
115,"Here we study how standardization affects between-collection comparisons. In this case, we randomly sample two disjoint subsets of 50 topics each and compute raw and standardized scores on both subsets. Because topics are sampled from the full set, both results can be regarded as coming from two different collections having different topics from the same population. In this case we are not interested in how standardized scores compare to raw scores, but rather on how stable the results are between both sets of topics, so we compute the following four statistics. First, we compute the  and ap correlations between both rankings. We seek high correlations, indicating high score stability across topic sets. Third, for",0,,False
116,1063,0,,False
117,Short Research Papers 2B: Recommendation and Evaluation,0,,False
118,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
119,"every system we run a 2-tailed unpaired t-test between both sets. By definition, the null hypothesis is true because we are comparing a system to itself simply on a different sample, so we expect as many Type I errors as the significance level . Finally, we run another test between every system on one collection and every other system on the other collection, looking again to maximize statistical power under the assumption that all systems are different and thus null hypotheses are false. As before, this process is repeated 10,000 times with both the RB and TB datasets, on both AP and nDCG.",1,TB,True
120,"Figure 3 shows the results for a selection of collection-measure combinations. The plots in the first two rows show that standardization generally produces more stable results, as evidenced by raw scores yielding lower correlations. U-std and z-std perform very similarly once again, and E-std generally outperforms the others, producing slightly more stable comparisons between collections. An exception can be noticed for ap on the TB dataset, which requires further investigation.",1,TB,True
121,"The third row of plots show the Type I error rates. We can see that all scoring schemes behave just as expected by the significance level . This evidences on the one hand the robustness of the t-test [7] (recall the diversity of distributions from the boxplots in Figure 1), and on the other hand that standardization neither harms nor helps from the point of view of Type I errors (this is rather a characteristic of the test). Finally, the last two plots show the power achieved by the tests when comparing different systems. Here we first notice that all standardization schemes are substantially more powerful than the raw scores, achieving about twice as much power. While the results are very similar in the RB set, we see clear differences in the TB set, with E-std once again outperforming the other schemes.",1,TB,True
122,4 CONCLUSIONS,0,,False
123,"In this paper we revisit the problem of score standardization to make IR evaluation robust to variations in topic difficulty. We introduced a new scheme for standardization based on the distributions of pertopic scores, and showed that previous methods by Webber et al. [9] and Sakai [3] are special cases of this scheme. From this point of view we propose the empirical distribution as an alternative, and discuss a number of points that highlight its superiority.",0,,False
124,"In experiments with TREC data, we showed that, even though the raw and standardized rankings are the same topic by topic, the rankings by mean scores may differ considerably. In addition, standardization achieves higher statistical power. Thus, standardization offers an alternative and quite different view on system comparisons. However, it is important to note that these comparisons are made on a different scale altogether, so one may not just use standardized scores to make statements about raw scores. Nonetheless, standardization with the empirical distribution is arguably more faithful to our notion of relative system effectiveness.",1,TREC,True
125,"Future work will follow three main lines. First, we will study additional datasets and measures for generality. However, because TREC collections are usually limited to 50 topics, we also plan on using recent simulation methods so that we can analyze more data [7]. Finally, we will study the stability of E-std for varying numbers of systems. This is interesting because, even though the empirical function converges to the true distribution, it is unclear how large the set of systems needs to be for the results to be stable.",1,ad,True
126,Density,0,,False
127,Density 0 2 4 6 8 10,0,,False
128,Robust - nDCG,1,Robust,True
129,raw N-std U-std z-std E-std,0,,False
130,10 15,0,,False
131,Terabyte - nDCG,1,Terabyte,True
132,raw N-std U-std z-std E-std,0,,False
133,5,0,,False
134,0,0,,False
135,0.65,0,,False
136,0.70 0.75 0.80 0.85 ,0,,False
137,Robust - AP,1,Robust,True
138,raw N-std U-std z-std E-std,0,,False
139,0.90,0,,False
140,0.95,0,,False
141,0.65,0,,False
142,0.70 0.75 0.80 0.85 ,0,,False
143,Terabyte - AP,1,Terabyte,True
144,raw N-std U-std z-std E-std,0,,False
145,0.90,0,,False
146,0.95,0,,False
147,Density 02468,0,,False
148,Density 02468,0,,False
149,0.050,0,,False
150,0.5,0,,False
151,0.6,0,,False
152,0.7,0,,False
153,0.8,0,,False
154,0.9,0,,False
155,0.5,0,,False
156,0.6,0,,False
157,0.7,0,,False
158,0.8,0,,False
159,0.9,0,,False
160,ap,0,,False
161,ap,0,,False
162,Robust - AP,1,Robust,True
163,Terabyte - AP,1,Terabyte,True
164,0.050,0,,False
165,raw N-std U-std z-std E-std,0,,False
166,raw N-std U-std z-std E-std,0,,False
167,Type I error rate,0,,False
168,Type I error rate,0,,False
169,0.001 0.005,0,,False
170,0.001 0.005,0,,False
171,0.001,0,,False
172,0.005,0,,False
173,0.020,0,,False
174,Significance level ,0,,False
175,Robust - nDCG,1,Robust,True
176,0.050,0,,False
177,0.001,0,,False
178,0.005,0,,False
179,0.020 0.050,0,,False
180,Significance level ,0,,False
181,Terabyte - nDCG,1,Terabyte,True
182,Power 0.4 0.5 0.6 0.7,0,,False
183,Power 0.3 0.4 0.5 0.6,0,,False
184,0.001,0,,False
185,raw N-std U-std z-std E-std,0,,False
186,0.005,0,,False
187,0.020 0.050,0,,False
188,Significance level ,0,,False
189,0.001,0,,False
190,raw N-std U-std z-std E-std,0,,False
191,0.005,0,,False
192,0.020 0.050,0,,False
193,Significance level ,0,,False
194,Figure 3: Between-collection comparisons. First row:  correlation between the rankings of systems produced by the two collections (higher is better); rugs mark the means. Second row: ap correlation (higher is better). Third row: Type I error rate of unpaired t-tests at various  levels (diagonal is better). Fourth row: statistical power (higher is better).,0,,False
195,ACKNOWLEDGMENTS,0,,False
196,Work carried out on the Dutch national e-infrastructure (SURF Cooperative) and funded by European Union's H2020 programme (770376-2 TROMPA). Eva.say( Hello World! );,0,,False
197,REFERENCES,0,,False
198,"[1] D. Bodoff. 2008. Test Theory for Evaluating Reliability of IR Test Collections. Information Processing and Management 44, 3 (2008), 1117­1145.",0,,False
199,"[2] J. Guiver, S. Mizzaro, and S. Robertson. 2009. A Few Good Topics: Experiments in Topic Set Reduction for Retrieval Evaluation. ACM TOIS 27, 4 (2009), 1­26.",0,,False
200,[3] T. Sakai. 2016. A Simple and Effective Approach to Score Standardization. In ACM ICTIR. 95­104.,0,,False
201,"[4] M. Sanderson. 2010. Test Collection Based Evaluation of Information Retrieval Systems. Foundations and Trends in Information Retrieval 4, 4 (2010), 247­375.",0,,False
202,[5] J. Urbano and M. Marrero. 2017. The Treatment of Ties in AP Correlation. In SIGIR ICTIR. 321­324.,1,AP,True
203,"[6] J. Urbano, M. Marrero, and D. Martín. 2013. On the Measurement of Test Collection Reliability. In ACM SIGIR. 393­402.",0,,False
204,[7] J. Urbano and T. Nagler. 2018. Stochastic Simulation of Test Collections: Evaluation Scores. In ACM SIGIR.,0,,False
205,"[8] E. Voorhees. 2005. Overview of the TREC 2005 Robust Retrieval Track. In TREC. [9] W. Webber, A. Moffat, and J. Zobel. 2008. Score Standardization for Inter-",1,TREC,True
206,"collection Comparison of Retrieval Systems. In AMC SIGIR. 51­58. [10] E. Yilmaz, J.A. Aslam, and S. Robertson. 2008. A New Rank Correlation Coefficient",0,,False
207,for Information Retrieval. In AMC SIGIR. 587­594.,0,,False
208,1064,0,,False
209,,0,,False

,sentence,label,data,regex
0,Short Research Papers 2B: Recommendation and Evaluation,0,,False
1,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
2,Quantifying Bias and Variance of System Rankings,0,,False
3,Gordon V. Cormack,0,,False
4,University of Waterloo gvcormac@uwaterloo.ca,0,,False
5,ABSTRACT,0,,False
6,"When used to assess the accuracy of system rankings, Kendall's  and other rank correlation measures conflate bias and variance as sources of error. We derive from  a distance between rankings in Euclidean space, from which we can determine the magnitude of bias, variance, and error. Using bootstrap estimation, we show that shallow pooling has substantially higher bias and insubstantially lower variance than probability-proportional-to-size sampling, coupled with the recently released dynAP estimator.",1,AP,True
7,"ACM Reference Format: Gordon V. Cormack and Maura R. Grossman. 2019. Quantifying Bias and Variance of System Rankings. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331356",0,,False
8,1 INTRODUCTION,1,DUC,True
9,"Since large-scale test collections were first applied in the context of TREC [10], concern has been expressed regarding the extent to which they afford ""fair"" or ""unbiased"" rankings of informationretrieval (IR) systems, and the extent to which those rankings are ""stable"" or ""reliable"" [3, 14]. We frame the problem in terms of measurement accuracy [11], where bias b and standard deviation  are orthogonal, dimensionally meaningful meta-measurements of (lack of) fairness and (lack of) stability, and RMS error RMSE ,",1,TREC,True
10,"b2 +  2 quantifies overall (lack of) accuracy. This work derives an amenable definition of b and  from Kendall's  rank correlation (§2). The same derivation applies to any of the plethora of rank-similarity scores that have been been employed in ad-hoc strategies to evaluate the fairness and stability of test collections [4, 7, 12, 16, 17]. We show how to measure |b |,  , and RMSE using bootstrap re-sampling (§3). Using the TREC 8 Ad Hoc test collection [15] as a reference standard, we evaluate four techniques--two statistical and two non-statistical--for building test collections constrained by an assessment budget (§4). Our evaluation reveals substantial differences in bias that would be masked by RMSE or  alone. Orthogonal measurements of b and  allow us to predict the effect of different budget-allocation strategies, which are borne out by the bootstrap results (§4.1). Through the use of adversarial testing, we show that one method is substantially unbiased, even when ranking results that are dissimilar to the TREC submissions (§4.2).",1,ad-hoc,True
11,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6172-9/19/07. https://doi.org/10.1145/3331184.3331356",1,ad,True
12,Maura R. Grossman,0,,False
13,University of Waterloo maura.grossman@uwaterloo.ca,0,,False
14,"We use bootstrap sampling to compare the official TREC 2018 Common Core test collection [1] to an alternate statistical collection created in advance by the University of Waterloo [9], along with its companion dynAP estimator1 [6]. We find that there are two sources of bias between the collections: (i) different relevance assessments for the same documents; and (ii) additional documents in the TREC collection selected using shallow sampling methods (§5).",1,TREC,True
15,"This work contributes a new methodology for test-collection evaluation, and uses that methodology to show that shallow pooling introduces bias far beyond what is shown by rank-correlation measures over submitted TREC results. Of the statistical estimators, the older and more well established infAP [18] shows clear bias, while dynAP shows negligible bias. The results indicate that a statistical test collection can yield comparable accuracy over 50 topics, and considerably better accuracy over more, compared to exhaustive assessment.",1,TREC,True
16,2 BIAS AND VARIANCE FOR RANKING,0,,False
17,"In this paper, a ""test result"" [11] is a ranking of systems according to",0,,False
18,"their measured effectiveness. Closeness of agreement is quantified by a rank-similarity coefficient like Kendall's  , with the maximum value 1 denoting equality.",0,,False
19,"We interpret system rankings to be points in Euclidean space, where  (x, y) ,"" 1 -  (x, y) is the the distance between x and y, whose location in space is unspecified. For test results X and Y , we define the expected squared distance  between them:""",0,,False
20,"(X , Y ) ,"" E  2(X , Y ) . The variance  2 of a given X is one-half the squared distance be-""",0,,False
21,tween itself and an independent and identically distributed test,0,,False
22,result:,0,,False
23, 2(X ),0,,False
24,",",0,,False
25,1,0,,False
26,(X,0,,False
27,",",0,,False
28,X,0,,False
29,),0,,False
30,(i .i .d .,0,,False
31,X,0,,False
32,",",0,,False
33,X,0,,False
34,),0,,False
35,.,0,,False
36,2,0,,False
37,"When G is the gold-standard ranking, squared bias b2 is the amount",0,,False
38,by which the squared distance between X and G exceeds that which,0,,False
39,is attributable to variance:,0,,False
40,"b2(X ) ,"" (X , G) -  2(X ) -  2(G) .""",0,,False
41,It follows that mean-squared error,0,,False
42,"MSE(X ) ,"" b2(X ) +  2(X ) . Bias b(X ) is a vector whose direction is unspecified; however, its magnitude |b |(X ) is sufficient for our purpose.""",0,,False
43,"It is worth noting that the ground truth ranking is a random variable G, rather than the particular outcome G ,  derived for the particular topics and assessments of the reference test collection from which G is derived. Under the assumption underlying virtually all reported statistical tests--that the set of topics in a collection",0,,False
44,is a random sample of a population of topics--the distributions of,0,,False
45,1See cormack.uwaterloo.ca/sample/.,0,,False
46,1089,0,,False
47,Short Research Papers 2B: Recommendation and Evaluation,0,,False
48,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
49,"G and X may estimated by bootstrap re-sampling (see [8]). Other random factors, notably the choice of documents for assessment and the assessor's relevance determinations, may be simulated in conjunction with the bootstrap.",0,,False
50,"When presenting empirical results, we report magnitude of bias |b|, standard deviation  , and root-mean-squared error RMSE.",0,,False
51,3 THE BOOTSTRAP,1,AP,True
52,"To estimate bias and variance, we conduct repeated tests in which random factors associated with the selection of topics, and the selection of documents to assess, are simulated. For statistical methods like infAP and dynAP, we first repeat the sampling method 100 times for each of nt topics in a reference collection. For each sample, we measure AP (not MAP, at this point) for each of ns available system results, saving the results in an nt × 100 × ns table. For nonstatistical methods, like pooling and hedge [2], repeated per-topic measurements are unnecessary, so the table contains nt × 1 × ns measurements.",1,AP,True
53,"We then draw 1, 000 samples of nt topics, with replacement, from the nt topics of the reference collection. Typically, but not necessarily, nt ,"" nt , thus preserving the variance of the mean. For each topic t within a sample, and for each system s, an AP measurement is drawn from the table at position (t, r , s), where r is chosen at random. For each system, the nt measurements are averaged to yield a MAP score. The resulting MAP scores are used to rank the ns systems.""",1,AP,True
54," is estimated as the empirical distribution of  (A, B), where A is the set of 1, 000 bootstrap rankings for one collection, and B is the set of 1, 000 rankings for another. Estimates of  and |b| follow.",0,,False
55,4 EXPERIMENT 1: ASSESSMENT BUDGETS,0,,False
56,"For our first experiment, we assume the TREC 8 Ad Hoc test collection to yield ground truth. We are concerned with ranking not only the 129 systems whose results were submitted to TREC 8 for evaluation, but also dissimilar systems. To simulate the results of dissimilar systems, we engineered an additional 129 results, each derived from a distinct TREC result by randomly permuting the order of the relevant documents. As a consequence, corresponding submitted and engineered results have identical AP and MAP scores, and identical ground-truth ranks.",1,TREC,True
57,"Using the ground-truth ranking for the submitted results, and additionally, the ground-truth ranking for the submitted and engineered results combined, we evaluate the bias, variance, and MSE of rankings derived from fewer assessments of the TREC collection.",1,ad,True
58,"The TREC collection contains 50 topics and 86,830 assessments, an average of 1,737 assessments per topic. Initially, we evaluated four methods of reducing assessment cost seventeen-fold to about 5,000 assessments:",1,TREC,True
59,"· The well-known infAP estimator, applied to a probabilityproportional-to-size (PPS) sample for each topic consisting of five documents drawn from each of 20 strata, for a total of 5,000 assessments;",1,AP,True
60,"· The recently released dynAP estimator, applied to the same PPS sample, for a total of 5,000 assessments;",1,AP,True
61,"· The trec_eval evaluator, applied to a variable number of documents per topic selected by depth-5 pooling, for a total of 5,542 assessments (avg. 111 per topic);",1,trec,True
62,"· trec_eval, applied to 100 documents per topic, selected by hedge, for a total of 5,000 assessments.",1,trec,True
63,"We then evaluated the effect of quadrupling the assessment budget in two ways: (i) by increasing the number of topics from 50 to 200; and (ii) by increasing the number of assessed documents per topic from 100 to 400. We further evaluated a budget of 80,000--about the same as at TREC--by quadrupling both the number of topics and the number of judgments per topic.",1,ad,True
64,"Table 1 shows ranking accuracy with respect to the submitted TREC results; Table 2 shows ranking accuracy with respect to the submitted results augmented by the engineered results. The topleft panel of each table shows accuracy for 5, 000 assessments. The result of quadrupling the number of topics is shown to the right, while the results of quadrupling the number of assessments per topic is shown below. The bottom row shows the accuracy of the reference TREC collection, under the assumption that it is unbiased.",1,TREC,True
65,"For 5, 000 assessments and the TREC results, the RMSE results show little to choose between dynAP and hedge, with between 15% and 20% higher error than the reference collection. The |b | and  results show that dynAP has less than one-quarter the bias, but hedge has lower variance, with the net effect that their overall error is similar.",1,TREC,True
66,4.1 Effect of More Topics,0,,False
67,"From these results we can predict predict the effect of quadrupling the number of topics, which is borne out by the top-right panel: |b | is essentially unchanged, while  is approximately halved. The net effect is that RMSE for dynAP is approximately halved, about 17% higher than for the reference collection.  for hedge is similarly halved but |b | is unchanged, so RMSE is reduced by only one-third, about 55% higher than for the reference collection.",1,ad,True
68,"From the initial results we can only bound the effect of quadrupling the number of assessments per topic: |b| and  will both generally be reduced, but b2 +  2 cannot fall below  for the reference collection. The bottom-left panel confirms this prediction, except for the fact that the bootstrap estimate of RMSE for hedge is slightly smaller than for the reference collection. This apparently preposterous result result may be explained by random error in the bootstrap estimate. Overall, this panel suggests that, with a budget of 400 assessments per topic, hedge has slightly lower overall error, but still four-times higher bias, than dynAP. Nevertheless, the results for hedge--and all other methods--are far superior when the same overall budget of 20, 000 is apportioned over 200 topics with 100 assessments per topic, instead of 50 topics with 400 assessments per topic.",1,ad,True
69,"The effect of quadrupling the number of topics, with a budget of 400 assessments per topic, is the same as with a budget of 100 assessments per topic: |b | is invariant, while  is halved. Overall, for a budget of 80, 000 assessments, dynAP achieves an RMSE score that is insubstantially different from that achieved by a reference collection with 331,320 assessments.",1,ad,True
70,1090,0,,False
71,Short Research Papers 2B: Recommendation and Evaluation,0,,False
72,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
73,infAP dynAP depth-5 hedge,1,AP,True
74,infAP dynAP depth-20 hedge,1,AP,True
75,Reference,0,,False
76,|b |,0,,False
77,RMSE,0,,False
78,"5,000 assessments, 50 topics",0,,False
79,0.0309 0.0952 0.1001 0.0117 0.0907 0.0914,0,,False
80,0.0800 0.0839 0.1160,0,,False
81,"0.0500 0.0806 0.0948 20,000 assessments, 50 topics",0,,False
82,0.0113 0.0826 0.0833,0,,False
83,0.0055 0.0806 0.0808,0,,False
84,0.0381 0.0783 0.0871,0,,False
85,"0.0159 0.0777 0.0794 86,830 assessments, 50 topics",0,,False
86,0 0.0797 0.0797,0,,False
87,|b |,0,,False
88,RMSE,0,,False
89,"20,000 assessments, 200 topics",0,,False
90,0.0293 0.0488 0.0124 0.0458,0,,False
91,0.0569 0.0475,0,,False
92,0.0815 0.0419 0.0916,0,,False
93,"0.0498 0.0398 0.0638 80,000 assessments, 200 topics",0,,False
94,0.0110 0.0419 0.0033 0.0408,0,,False
95,0.0433 0.0409,0,,False
96,0.0387 0.0394 0.0552,0,,False
97,"0.0154 0.0394 0.0423 331,320 assessments, 200 topics",0,,False
98,0 0.0405 0.0405,0,,False
99,Table 1: Accuracy of ranking TREC system results. () RMSE values less than Reference are explained by chance error in the bootstrap estimate.,1,TREC,True
100,infAP dynAP depth-5,1,AP,True
101,|b |,0,,False
102, RMSE,0,,False
103,"5,000 assessments, 50 topics",0,,False
104,0.1510 0.0939 0.1778,0,,False
105,0.0319 0.1078 0.1125,0,,False
106,0.3071 0.0695 0.3149,0,,False
107,|b |,0,,False
108,RMSE,0,,False
109,"20,000 assessments, 200 topics",0,,False
110,0.1517 0.0480 0.1591,0,,False
111,0.0347 0.0560 0.0658,0,,False
112,0.3007 0.0360 0.3029,0,,False
113,hedge,0,,False
114,infAP dynAP depth-20,1,AP,True
115,"0.1263 0.0773 0.1481 20,000 assessments, 50 topics 0.0495 0.0847 0.0981 -0.0145 0.0856 0.0843 0.2013 0.0688 0.2127",0,,False
116,"0.1267 0.0389 0.1325 80,000 assessments, 200 topics 0.0503 0.0430 0.0662 -0.0093 0.0442 0.0432 0.1961 0.0338 0.1990",0,,False
117,"hedge 0.0866 0.0780 0.1166 86,830 assessments, 50 topics",0,,False
118,Reference 0 0.0797 0.0797,0,,False
119,"0.0869 0.0398 0.0956 331,320 assessments, 200 topics",0,,False
120,0 0.0405 0.0405,0,,False
121,"Table 2: Accuracy of ranking TREC and dissimilar system results. () Cases where the b2(X ) < 0 are explained by chance error in the bootstrap estimate, and reported as - -b2(X ).",1,TREC,True
122,4.2 Adversarial Testing,0,,False
123,Table 2 affirms the same predictions regarding assessment-budget,0,,False
124,"allocation, but tells a different story regarding the accuracy of the",0,,False
125,test collections. When the engineered results are ranked with the,0,,False
126,"TREC results, bias is roughly tripled for all methods subject to a 5, 000-document assessment budget, while variance is increased moderately. For infAP, depth-5, and hedge, bias dominates variance,",1,TREC,True
127,calling into question whether these methods provide reasonable,0,,False
128,"accuracy, regardless of their RMSE. The dynAP results represent a closer call. |b | is one-third of  , rendering it a small but noticeable component of RMSE. Quadrupling the number of topics exacerbates the influence of |b|, but still yields RMSE scores better than the reference collection for 50 topics.",1,AP,True
129,"Quadrupling the per-topic assessment budget dramatically reduces |b | for dynAP, to the point that the bootstrap estimate of b2 is negative (shown as |b |,"" - -b2), indicating that we are unable to distinguish |b | from zero. When the number of topics is also quadrupled, we are still unable to distinguish |b| from zero, while  and RMSE are about 10% greater than RMSE for the reference collection with 200 topics; and half the RMSE for the reference""",1,ad,True
130,collection with 50 topics.,0,,False
131,5 EXPERIMENT 2: ALTERNATE ASSESSMENTS,0,,False
132,"Before TREC 2018, the University of Waterloo used Dynamic Sampling [5], and 19, 161 of their own assessments to create relevance assessments (irels) as input the dyn estimator, thereby forming a test collection for the 2018 Common Core Track, which was released, along with the dyn estimator, as UWcore181 [9]. To form the official TREC test collection, NIST (re)assessed the 19, 161 documents, and 5, 767 additional documents selected by a combination of depth-10 and move-to-front pooling, to create the official TREC relevance assessments (qrels) as input to trec_eval. To examine the impact on system ranking of using Waterloo versus TREC assessments, we compare UWcore18 to UWcore18N, in which the Waterloo assessments are replaced by NIST assessments. To examine the impact of using an additional 5, 767 TREC assessments, while eschewing dyn in favour of trec_eval, we compare UWcore18N to the NIST test collection.",1,TREC,True
133,"Table 3 shows the accuracy with which the three test collections rank 69 of the 73 system results submitted to TREC, excluding the four submitted by Waterloo. The top panel shows inter-collection",1,TREC,True
134,1091,0,,False
135,Short Research Papers 2B: Recommendation and Evaluation,0,,False
136,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
137,G,0,,False
138,UWcore18 UWcore18N,0,,False
139,NIST,0,,False
140,UWcore18,0,,False
141,0.0067 0.0364 0.0710,0,,False
142,UWcore18N --|b |-- 0.0887 0.0064 0.0369,0,,False
143,NIST,0,,False
144,0.0752 0.0364 0.0084,0,,False
145,UWcore18 UWcore18N,0,,False
146,NIST,0,,False
147,0.0828 0.1186 0.1089,0,,False
148,--RMSE-- 0.1210 0.0826 0.0903,0,,False
149,0.1090 0.0869 0.0794,0,,False
150,"Table 3: Accuracy results for alternative TREC 2018 Common Core Collections. The top panel shows pairwise bias; the bottom panel shows RMSE. () In the top panel, asymmetry and differences from 0 on the diagonal are explained by chance error in the bootstrap estimate.",1,TREC,True
151,"bias |b| using each collection as ground truth to measure the bias of each other collection, including itself. The top panel should be symmetric, and its diagonal should be 0. Deviations from this prediction may be attributed to random error in the bootstrap estimates. The diagonal of the bottom panel shows  for each collection, while the non-diagonal elements show inter-system RMSE. Our theory predicts that the bottom panel will be symmetric only if the  values are equal, which they nearly are.",0,,False
152,"There is substantial bias--about equal to  --between UWcore18 and UWcore18N, indicating that some systems score better according to the Waterloo assessments, while others score better according to NIST's. We are not in a position to render an opinion on which set of assessments is better and suggest that evaluation using the two sets of assessments should be considered to be separate experiments, and both results considered when comparing system results.",0,,False
153,"Bias between UWcore18N and NIST, while smaller in magnitude, may be more of a concern, because it reflects different measurements of ostensibly the same value. The results from our first experiment show that shallow pooling methods exhibit strong bias, while dynAP does not. Together, these results suggest that the intercollection bias |b |,"" 0.0364 may be attributed in large part to the NIST collection. If this were the case, it would offset an apparent advantage in  for NIST.""",1,AP,True
154,"We posit that better results would have been achieved, had the budget of 5, 767 NIST assessments allocated to shallow pooing been used to assess an additional 15 topics using Dynamic Sampling. Using our bootstrap simulation, we project that UWcore18N, if extended to 65 topics, would achieve  ,"" 0.0724, lower than the official 50-topic collection.""",1,ad,True
155,"The evidence suggests the current UWcore18N test collection, notwithstanding its slightly higher variance, is preferable to the official TREC test collection, because it is less likely to be biased, particularly with respect to novel IR methods.",1,TREC,True
156,6 DISCUSSION AND LIMITATIONS,0,,False
157,"Kutlu et al. [12] provide an excellent survey of rank-similarity measures and their shortcomings, in support of a ""significance",0,,False
158,"aware"" approach that takes into account the results of pairwise significance tests. Our approach exposes the variances of G and X directly--separate from bias--rather than obliquely through an amalgam of test results, distilled into a dimensionless overall score.",0,,False
159,"Previous work has evaluated fairness and stability separately, by",0,,False
160,perturbing the topics or the results to be ranked so as to calculate a,0,,False
161,"separate summary measure for each [3, 14]. Our bootstrap sample was inadequate to quantify b when  ",1,ad,True
162,"|b | 0. Further analytic and empirical work is needed to compute confidence intervals for the bootstrap estimates. In theory, the",0,,False
163,"variance of the variance estimates can be determined from the distributions of G and X , but those estimates should be verified by meta-experiments.",0,,False
164,Our experiments rely on on an assumption--contradicted by the,1,ad,True
165,evidence [13]--that an assessor will give the same relevance deter-,0,,False
166,"mination for a given document, regardless of the sampling strategy.",0,,False
167,Whether real assessor behaviour would have any substantive posi-,0,,False
168,tive or negative effect on rankings remains to be determined.,0,,False
169,Orthogonal estimates of bias and variance have predictive ability,0,,False
170,"lacking in a single score conflating the two, or separate uncalibrated",0,,False
171,"scores. Harnessing this predictive ability, we offer evidence that",0,,False
172,"shallow pooling methods introduce unreasonable amounts of bias,",0,,False
173,"while offering hardly lower variance than dynAP, which represents",1,AP,True
174,"a substantial improvement over infAP. Based on this evidence, we",1,AP,True
175,have reason to suggest that the UWcore18 or UWcore18N statisti-,0,,False
176,cal test collections are more accurate than the official TREC 2018,1,TREC,True
177,Common Core test collection.,0,,False
178,REFERENCES,0,,False
179,"[1] Allan, J., Harman, D., Voorhees, E., and Kanoulas, E. TREC 2018 Common Core Track overview. In TREC 2018.",1,TREC,True
180,"[2] Aslam, J. A., Pavlu, V., and Savell, R. A unified model for metasearch, pooling, and system evaluation. In CIKM 2003.",0,,False
181,"[3] Buckley, C., and Voorhees, E. M. Evaluating evaluation measure stability. In SIGIR 2000.",0,,False
182,"[4] Carterette, B. On rank correlation and the distance between rankings. In SIGIR 2009.",0,,False
183,"[5] Cormack, G. V., and Grossman, M. R. Beyond pooling. In SIGIR 2018. [6] Cormack, G. V., and Grossman, M. R. Unbiased low-variance estimators for",0,,False
184,"precision and related information retrieval effectiveness measures. In SIGIR 2019. [7] Cormack, G. V., and Lynam, T. R. Power and bias of subset pooling strategies.",0,,False
185,"In SIGIR 2007. [8] Cormack, G. V., and Lynam, T. R. Statistical precision of information retrieval",0,,False
186,"evaluation. In SIGIR 2006. [9] Cormack, G. V., Zhang, H., Ghelani, N., Abualsaud, M., Smucker, M. D.,",0,,False
187,"Grossman, M. R., Rahbariasl, S., and Grossman, M. R. Dynamic sampling meets pooling. In SIGIR 2019. [10] Harman, D. K. The TREC Test Collections. In TREC: Experiment and Evaluation in Information Retrieval (2005), E. M. Voorhees and D. K. Harman, Eds., pp. 21­52. [11] International Organization for Standardization. ISO 5725-1: 1994: Accuracy (Trueness and Precision) of Measurement Methods and Results-Part 1: General Principles and Definitions. International Organization for Standardization Geneva, Switzerland, 1994. [12] Kutlu, M., Elsayed, T., Hasanain, M., and Lease, M. When rank order isn't enough: New statistical-significance-aware correlation measures. In CIKM 2018. [13] Roegiest, A., and Cormack, G. V. Impact of review-set selection on human assessment for text classification. In SIGIR 2016. [14] Sakai, T. On the reliability of information retrieval metrics based on graded relevance. Inf. Process. Manag. 43, 2 (2007), 531­548. [15] Voorhees, E., and Harman, D. Overview of the Eighth Text REtrieval Conference. In TREC 8 (1999). [16] Voorhees, E. M. Variations in relevance judgments and the measurement of retrieval effectiveness. Inf. Process. Manag. 36, 5 (2000). [17] Yilmaz, E., Aslam, J. A., and Robertson, S. A new rank correlation coefficient for information retrieval. In SIGIR 2008. [18] Yilmaz, E., Kanoulas, E., and Aslam, J. A. A simple and efficient sampling method for estimating AP and NDCG. In SIGIR 2008.",1,TREC,True
188,1092,0,,False
189,,0,,False

,sentence,label,data,regex
0,"Short Research Papers 2A: AI, Mining, and others",0,,False
1,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
2,A study on the Interpretability of Neural Retrieval Models using DeepSHAP,1,AP,True
3,Zeon Trevor Fernando,0,,False
4,"L3S Research Center Hannover, Germany",0,,False
5,fernando@l3s.de,0,,False
6,Jaspreet Singh,0,,False
7,"L3S Research Center Hannover, Germany",0,,False
8,singh@l3s.de,0,,False
9,Avishek Anand,0,,False
10,"L3S Research Center Hannover, Germany",0,,False
11,anand@l3s.de,0,,False
12,ABSTRACT,0,,False
13,"A recent trend in IR has been the usage of neural networks to learn retrieval models for text based adhoc search. While various approaches and architectures have yielded significantly better performance than traditional retrieval models such as BM25, it is still difficult to understand exactly why a document is relevant to a query. In the ML community several approaches for explaining decisions made by deep neural networks have been proposed ­ including DeepSHAP which modifies the DeepLift algorithm to estimate the relative importance (shapley values) of input features for a given decision by comparing the activations in the network for a given image against the activations caused by a reference input. In image classification, the reference input tends to be a plain black image. While DeepSHAP has been well studied for image classification tasks, it remains to be seen how we can adapt it to explain the output of Neural Retrieval Models (NRMs). In particular, what is a good ""black"" image in the context of IR? In this paper we explored various reference input document construction techniques. Additionally, we compared the explanations generated by DeepSHAP to LIME (a model agnostic approach) and found that the explanations differ considerably. Our study raises concerns regarding the robustness and accuracy of explanations produced for NRMs. With this paper we aim to shed light on interesting problems surrounding interpretability in NRMs and highlight areas of future work.",1,adhoc,True
14,"ACM Reference Format: Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study on the Interpretability of Neural Retrieval Models using DeepSHAP. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184. 3331312",1,AP,True
15,1 INTRODUCTION,1,DUC,True
16,Deep neural networks have achieved state of the art results in several NLP and computer vision tasks in the last decade. Along with this spurt in performance has come a new wave of approaches trying to explain decisions made by these complex machine learning models. Explainablilty and interpretability are key to deploying,1,ad,True
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331312",1,ad,True
18,"NNs in the wild and having them work in tandem with humans. Explanations can help debug models, determine training data bias and understand decisions made in simpler terms in order to foster trust. Recently in IR, models such as DRMM [5], MatchPyramid [10], PACRR-DRMM [8] and others have shown great promise in ranking for adhoc text retrieval. While these models do improve state-ofthe-art on certain benchmarks, it is sometimes hard to understand why exactly these models are performing better. With the increased scrutiny on automated decision making systems, including search engines, it is vital to be able to explain decisions made. In IR however, little to no work has been done on trying to explain the output of complex neural ranking models.",1,ad,True
19,"In the ML community, several post-hoc non-intrusive methods have been suggested recently which enable us to train highly accurate and complex models while also being able to get a sense of their rationale. One of the more popular approaches to producing explanations is to determine the input feature attributions for a given instance and it's prediction according to a given model. The output of such a method is typically visualized as a heat map over the input words/pixels. Several approaches have been proposed in this direction for image and text classification but their applicability to adhoc text retrieval and ranking remains unexplored. In this paper we study the applicability of one such method designed specifically for neural networks ­ DeepSHAP [7], to explain the output of 3 different neural retrieval models. DeepSHAP is a modification of the DeepLift [15] algorithm to efficiently estimate the shapley values over the input feature space for a given instance. The shapley value is a term coined by Shapley [14] in cooperative game theory to refer to the contribution of a feature in a prediction. More specifically, shapley values explain the contribution of an input feature towards the difference in the prediction made vs the average prediction value.",1,hoc,True
20,"The objective of our work is to utilize DeepSHAP to explain NRMs which should ideally be a trivial pursuit since they are standard neural networks. However, in our experiments, we found that DeepSHAP's explanations are highly dependent on a reference input which is used to compute the average prediction. This is inline with recent work that suggests approaches like DeepLIFT lack robustness [4]. In this work, we ponder on the question, what makes a good reference input distribution for neural rankers? In computer vision, a plain black image is used as the reference input but what is the document equivalent of such an image in IR? Furthermore, we found that explanations produced by the model introspective DeepSHAP are considerably different from the model agnostic approach ­ LIME [12]. Although both models produce local explanations, the variability is concerning.",1,AP,True
21,1005,0,,False
22,"Short Research Papers 2A: AI, Mining, and others",0,,False
23,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
24,2 RELATED WORK,0,,False
25,"There are two main approaches to interpretability in machine learning models: model agnostic and model introspective approaches. Model agnostic approaches [12, 13] generate post-hoc explanations for the original model by treating it as a black box by learning an interpretable model on the output of the model or by perturbing the inputs or both. Model introspective approaches on one hand include ""interpretable"" models such as decision trees [6], attentionbased networks [20], and sparse linear models [19] where there is a possibility to inspect individual model components (path in a decision tree, feature weights in linear models) to generate useful explanations. On the other hand, there are gradient-based methods like [16] that generates attributions by considering the partial derivative of the output with respect to the input features. Following this, there were many works [1, 2, 7, 15] that generate attributions by inspecting the neural network architectures.",1,hoc,True
26,"Interpretability in ranking models Recently there have been few works focused on interpretability [17, 18] and diagnosis of neural IR models [9, 11]. In the diagnostic approaches, they use the formal retrieval constraints (""axioms"") defined for traditional retrieval models to find the differences between neural IR and learningto-rank approaches with hand-crafted features through a manual error analysis [9] or build diagnostic datasets based on the axioms to empirically analyse these models [11]. In [18] they built a explainable search system (EXS) that adapts a local model agnostic interpretability approach (LIME [12]) to explain the relevance of a document for a query for various neural IR models. In [17] they propose an approach that understands the query intent encoded by NRMs by learning a simple ranking model with an expanded query that approximates the original ranking.",1,ad,True
27,"To the best of our knowledge, this is the first work that looks at model introspective interpretability specifically for NRMs.",0,,False
28,3 DEEPSHAP FOR IR,1,AP,True
29,"DeepSHAP is a local model-introspective interpretability method to approximate the shapley values using DeepLIFT [15]. DeepLIFT explains the difference in output/prediction from some `reference' output with respect to the difference of the input (to explain) from a `reference' input. The authors define a function analogous to partial derivatives to compute the feature importance scores and use the chain rule to backpropagate the activation differences from the output layer to the original input. The choice of reference input depends on domain specific knowledge; For example, in digit classification task on the MNIST dataset, they use a reference input of all-zeros as that is the background of the images. For object detection in images, a plain black image is often used.",1,AP,True
30,"In the context of IR, DeepSHAP can be used to explain why a document is relevant to query (according to a given NRM) by computing the shapley values for words in the document. The words with high shapley values indicate that they are important towards this prediction of relevance. However, to accurately compute the shapley values using DeepSHAP a reference input is needed. What makes a good background image in the context of IR?",1,AP,True
31,"Unlike classification tasks, in ranking we have at least 2 inputs which are in most cases the query and document tokens. In this work we fix the reference input for the query to be same as that of",0,,False
32,the query-document instance to be explained and experiment with various reference inputs for the document. The intuition behind doing so is to gain an average reference output in the locality of the query.,0,,False
33,The various document reference inputs that we considered in our experiments are:,0,,False
34,"OOV The reference document consists of `OOV' tokens. For, DRMM and MatchPyramid models the embedding vector for `OOV' comprises of all-zeros which is similar to the background image used for MNIST. But for PACRR-DRMM, it is the average of all the embedding vectors in the vocabulary.",0,,False
35,"IDF lowest The reference document is constructed by sampling words with low IDF scores. These words are generally stopwords or words that are similar to stop-words so they should, in general, be irrelevant to the query.",0,,False
36,QL lowest The reference document comprises of sampled words with low query-likelihood scores that are derived from a language model of the top-1000 documents.,0,,False
37,COLLECTION rand doc The reference document is randomly sampled from the rest of the collection minus the top-1000 documents retrieved for the query.,0,,False
38,TOPK LIST rand doc from bottom The reference document is randomly sampled from the bottom of the top-1000 documents retrieved.,0,,False
39,These variants were designed based on the intuition that the reference input document would comprise of words that are irrelevant to the query and thus DeepSHAP should be able to pick the most important terms from the input document that explain relevance to the query.,1,AP,True
40,4 EXPERIMENTAL SETUP,0,,False
41,"In our experiments, we aim to answer the following research questions:",0,,False
42,· Are DeepSHAP explanations sensitive to the type of reference input in the case of NRMs?,1,AP,True
43,· Can we determine which reference input produces the most accurate local explanation?,0,,False
44,"To this end, we describe the experimental setup we used to address these questions. We describe the various NRM's we considered and how we used LIME to evaluate the correctness of explanations produced by DeepSHAP.",1,ad,True
45,4.1 Neural Retrieval Models,0,,False
46,DRMM [5] This model uses a query-document term matching count histogram as input into a feed forward neural network (MLP) to output a relevance score along with a gating mechanism that learns query term weights.,0,,False
47,MatchPyramid [10] This model uses a query-document interaction matrix as input to a 2D CNN to extract matching patterns. The output is then fed into a MLP to get a relevance score.,0,,False
48,"PACRR-DRMM [8] This model creates a query-document interaction matrix that is fed into multiple 2D CNNs with different kernel sizes to extract n-gram matches. Then, after k-max pooling across each q-term, the document aware q-term encodings are fed into a MLP, like in DRMM to obtain a relevance score.",0,,False
49,1006,0,,False
50,"Short Research Papers 2A: AI, Mining, and others",0,,False
51,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
52,Figure 1: Confusion matrices of various DeepSHAP background document methods comparing Jaccard similarities.,1,AP,True
53,4.2 Evaluation,0,,False
54,"To conduct the experiments, we used the Robust04 test collection from TREC. We used Lucene to index and retrieve documents. Next, we trained the NRMs using their implementations in MatchZoo1 [3]. All the hyparameters were tuned using the same experimental setup as described in the respective papers. We chose to study explanations for the distinguished set of hard topics2 (50) from the TREC Robust Track 2004. We generate the explanations from LIME and DeepSHAP for the top-3 documents retrieved for each query and use only these for our quantitative experiments.",1,Robust,True
55,"Evaluating explanations Since no ground truth explanations are available for a neural model, we use LIME based explanations as a proxy. We found that it can approximate the locality of a query document pair well, using a simple linear model that achieved an accuracy of over 90% across all NRMs considered in our experiments. To produce the explanations from LIME we used the implementation found in 3 along with the score-based modification suggested in [18]. The primary parameters for training a LIME explanation model are the number of perturbed samples to be considered and the number of words for the explanation. The number of perturbed samples is set to 5000 and the number of words is varied based on the experiment. We used the DeepSHAP implementation provided here 4. Note that we ignore the polarity of the explanation terms provided by both methods in our comparison since the semantics behind the polarities in LIME and DeepSHAP are different. We are more interested in the terms chosen as the explanations in both cases.",1,AP,True
56,5 RESULTS AND DISCUSSION,0,,False
57,5.1 Effect of reference input document,0,,False
58,"Figure 1 illustrates the overlap between the explanation terms produced when varying the reference input. Immediately we observe that the overlap between explanations produced is low; below 50% in most cases and consistently across all NRMs. Each reference input method has its own distinct semantics and this is reflected by the low overlap scores. We also find that there is no consistent trend across NRMs. For MatchPyramid, OOV and QL have highest overlap whereas for PACRR-DRMM its OOV and COL that have highest overlap even though both models share similarities in input",0,,False
59,1 https://github.com/N TMC- Community/MatchZoo/tree/1.0 2 https://trec.nist.gov/data/robust/04.guidelines.html 3 https://github.com/marcotcr/lime 4 https://github.com/slundberg/shap,1,trec,True
60,"representation and parts of the model architecture. Table 3 shows explanations for DRMM across variants. Once again we see how the explanations can differ significantly if we are not careful in selecting the reference input. For IR, finding the background image seems to be a much harder question.",0,,False
61,Our results show how explanations are highly sensitive to the reference input for NRMs chosen in our experiments. This is also indication that a single reference input method may not be the best for every NRM.,0,,False
62,5.2 Accuracy of reference input methods,0,,False
63,"To help identify which reference input method is most accurate in explaining a given query-document pair, we compared the LIME explanations for the same against it's corresponding DeepSHAP explanations. In general we found that DeepSHAP produces more explanation terms whereas LIME's L1 regularizer constrains the explanations to only the most important terms. Additionally, the discrepancy between the explanations can be attributed to LIME being purely local, whereas DeepSHAP has some global context since it looks at activations for the whole network which may have captured some global patterns. Hence, to estimate which reference input surfaces the most `ground truth' explanation terms we only computed recall at top 50 and 100 (by shapley value magnitude) DeepSHAP explanation terms (in Table 1).",1,AP,True
64,"The first interesting insight is that some NRMs are easier to explain whereas others are more difficult. PACRR-DRMM consistently has a recall less than 0.7, whereas the DeepSHAP explanations of DRMM effectively capture almost all of the LIME explanation terms. When comparing reference input variants within each NRM we find that there is no consistent winner. For DRMM, QL is the best which indicates that sampling terms which are relatively generic for this query in particular is a better `background image' than sampling generic words from the collection (IDF).",1,AP,True
65,"In the case of MatchPyramid, TOPK LIST is the worst performing but it is more difficult to distinguish between the approaches here. The best approach surprisingly is OOV. This can be attributed to how MatchPyramid treats OOV terms. The OOV token is represented by an all-zeros embedding vector that is used for padding the input interaction matrix whereas in DRMM, OOV tokens are filtered out. These preprocessing considerations prove to be crucial when determining the right input reference. Moving on to PACRRDRMM, we once again find that QL is the best method even though DeepSHAP struggles to find all the LIME terms.",1,ad,True
66,1007,0,,False
67,"Short Research Papers 2A: AI, Mining, and others",0,,False
68,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
69,"Table 1: Comparison of recall measures at top-k (50, 100) terms from DeepSHAP without polarity against the top-k (10, 20, 30) ground-truth terms from LIME for ROBUST04 difficult queries (50)",1,AP,True
70,SHAP variants OOV IDF QL COLLECTION TOPK LIST.,1,AP,True
71,DRMM,0,,False
72,MatchPyramid,0,,False
73,PACRR-DRMM,0,,False
74,top-10,0,,False
75,top-20,0,,False
76,top-30,0,,False
77,top-10,0,,False
78,top-20,0,,False
79,top-30,0,,False
80,top-10,0,,False
81,top-20,0,,False
82,top-30,0,,False
83,recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100,0,,False
84,0.789 0.905 0.672 0.845 0.615 0.812 0.793 0.843 0.656 0.726 0.566 0.640 0.582 0.582 0.388 0.388 0.299 0.299,0,,False
85,0.830 0.917 0.723 0.871 0.658 0.841 0.795 0.832 0.653 0.711 0.565 0.633 0.633 0.633 0.446 0.446 0.362 0.362,0,,False
86,0.894 0.955 0.754 0.892 0.670 0.856 0.765 0.821 0.638 0.711 0.556 0.636 0.643 0.643 0.462 0.462 0.367 0.367,0,,False
87,0.760 0.881 0.673 0.841 0.620 0.815 0.783 0.824 0.639 0.709 0.552 0.630 0.621 0.621 0.429 0.429 0.343 0.343,0,,False
88,0.639 0.821 0.606 0.794 0.578 0.788 0.759 0.811 0.624 0.702 0.545 0.627 0.625 0.625 0.425 0.425 0.340 0.340,0,,False
89,Table 2: Comparison of mean squared error (MSE) and accuracy (ACC) of LIME's linear model across various NRMs. Low MSE and high accuracy shows that it is able to fit and generalize in the query-document locality.,0,,False
90,NRM,0,,False
91,DRMM MatchPyramid PACRR-DRMM,0,,False
92,TRAIN MSE,0,,False
93,0.00631 0.01827 0.00165,0,,False
94,Linear Regression TEST MSE TRAIN ACC,0,,False
95,0.00633 0.01839 0.00160,0,,False
96,0.92662 0.90367 0.98857,0,,False
97,TEST ACC,0,,False
98,0.92654 0.90387 0.98980,0,,False
99,Table 3: An example of words selected by LIME and SHAP methods for the query `cult lifestyles' and document `FBIS3-843' which is about clashes between cult members and student union's activists at a university in Nigeria. Words unique to a particular explanation method are highlighted in bold.,1,AP,True
100,LIME,0,,False
101,OOV,0,,False
102,IDF,0,,False
103,QL,0,,False
104,COL.,0,,False
105,TOPK,0,,False
106,cult style followers elite saloon student home members march september,0,,False
107,cult followers,0,,False
108,black fraternities degenerate,0,,False
109,sons academic american,1,ad,True
110,tried household,0,,False
111,cult style followers suspects belong reappearing household black fraternities degenerate,0,,False
112,cult style elite saloon final march friday september arms closed,0,,False
113,cult black fraternities degenerate sons followers style home household avoid,0,,False
114,cult numbers english college university fallouts buccaneers feudings activists troubles,0,,False
115,6 CONCLUSION AND FUTURE WORK,0,,False
116,In this paper we suggested several reference input methods for DeepSHAP that take into account the unique semantics of document ranking and relevance in IR. Through quantitative experiments we found that it is indeed sensitive to the reference input. The distinct lack of overlap in most cases was surprising but in line with recent works on the lack of robustness in interpretability approaches. We also tried to evaluate which reference method is more accurate by comparing against LIME. Here we found that reference input method selection is highly dependent on the model at hand. We believe that this work exposes new problems when dealing with model introspective interpretability for NRMs. A worthwhile endeavor will be to investigate new approaches that explicitly take into account the discreteness of text and the model's preprocessing choices when generating explanations.,1,AP,True
117,This work was supported by the Amazon research award on,0,,False
118,`Interpretability of Neural Rankers'.,0,,False
119,REFERENCES,0,,False
120,"[1] Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. 2017. ""What is relevant in a text document?"": An interpretable machine learning approach. PLOS ONE 12 (2017), 1­23.",0,,False
121,"[2] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. 2015. On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. PLOS ONE 10 (2015), 1­46.",0,,False
122,"[3] Yixing Fan, Liang Pang, Jianpeng Hou, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2017. MatchZoo: A Toolkit for Deep Text Matching. (2017). arXiv:1707.07270",0,,False
123,"[4] Amirata Ghorbani, Abubakar Abid, and James Y. Zou. 2019. Interpretation of Neural Networks is Fragile. In AAAI '19.",0,,False
124,"[5] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In CIKM '16. ACM, 55­64.",1,hoc,True
125,"[6] Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan. 2015. Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics 9, 3 (2015), 1350­1371.",1,ad,True
126,[7] Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems 30. 4765­4774.,0,,False
127,"[8] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-Query Interactions. In EMNLP '18.",1,Query,True
128,"[9] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, and Xueqi Cheng. 2017. A Deep Investigation of Deep IR Models. arXiv preprint (2017). arXiv:1707.07700",0,,False
129,"[10] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2016. Text Matching As Image Recognition. In AAAI'16. 2793­2799.",0,,False
130,"[11] Daan Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach to Diagnosing Neural IR Models. In ECIR '19. 489­503.",0,,False
131,"[12] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. ""Why Should I Trust You?"": Explaining the Predictions of Any Classifier. In KDD '16. ACM, 1135­1144.",0,,False
132,"[13] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: HighPrecision Model-Agnostic Explanations. In AAAI '18.",0,,False
133,"[14] Lloyd S Shapley. 1953. A value for n-person games. Contributions to the Theory of Games 2, 28 (1953), 307­317.",0,,False
134,"[15] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating Activation Differences. arXiv preprint (2017). arXiv:1704.02685",0,,False
135,"[16] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR Workshop (2014).",0,,False
136,[17] Jaspreet Singh and Avishek Anand. 2018. Interpreting search result rankings through intent modeling. arXiv preprint (2018). arXiv:1809.05190,0,,False
137,"[18] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (WSDM '19). ACM, 770­773.",0,,False
138,"[19] Berk Ustun and Cynthia Rudin. 2016. Supersparse Linear Integer Models for Optimized Medical Scoring Systems. Machine Learning 102, 3 (2016), 349­391.",0,,False
139,"[20] Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In International Conference on Machine Learning - Volume 37 (ICML'15). 2048­2057.",0,,False
140,1008,0,,False
141,,0,,False

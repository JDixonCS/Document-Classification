,sentence,label,data,regex
0,Short Research Papers 3B: Recommendation and Evaluation,0,,False
1,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
2,Dynamic Sampling Meets Pooling,0,,False
3,"Gordon V. Cormack, Haotian Zhang, Nimesh Ghelani, Mustafa Abualsaud, Mark D. Smucker, Maura R. Grossman, Shahin Rahbariasl, and Amira Ghenai",0,,False
4,"University of Waterloo, Ontario, Canada",1,ad,True
5,ABSTRACT,0,,False
6,"A team of six assessors used Dynamic Sampling (Cormack and Grossman 2018) and one hour of assessment effort per topic to form, without pooling, a test collection for the TREC 2018 Common Core Track. Later, official relevance assessments were rendered by NIST for documents selected by depth-10 pooling augmented by move-to-front (MTF) pooling (Cormack et al. 1998), as well as the documents selected by our Dynamic Sampling effort. MAP estimates rendered from dynamically sampled assessments using the xinfAP statistical evaluator are comparable to those rendered from the complete set of official assessments using the standard trec_eval tool. MAP estimates rendered using only documents selected by pooling, on the other hand, differ substantially. The results suggest that the use of Dynamic Sampling without pooling can, for an order of magnitude less assessment effort, yield informationretrieval effectiveness estimates that exhibit lower bias, lower error, and comparable ability to rank system effectiveness.",1,TREC,True
7,"ACM Reference Format: Gordon V. Cormack, Haotian Zhang, Nimesh Ghelani, Mustafa Abualsaud, Mark D. Smucker, Maura R. Grossman, Shahin Rahbariasl, and Amira Ghenai. 2019. Dynamic Sampling Meets Pooling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331354",0,,False
8,1 INTRODUCTION,1,DUC,True
9,"This work evaluates the first in situ application of Dynamic Sampling (DS) [3] to construct a test collection for large-scale information retrieval (IR) evaluation (see [9]). In contrast to the de facto standard pooling method, DS estimates IR effectiveness measures like MAP by applying a statistical estimator to a sample of a very large universe of documents, drawn independently from any system to be evaluated. DS promises to yield unbiased estimates not only for the methods known at the time of construction, but also for methods yet-to-be invented. Prior to this effort, DS had been validated only through post-hoc simulation.",1,MAP,True
10,"In preparation for the TREC 2018 Common Core Track [2], we built a test collection by employing a combination of Continuous Active Learning (CAL) [5], Interactive Search and Judging (ISJ) [7], and DS to identify a sample of 19,161 documents from a universe of 39,214, each of which we assessed to be relevant or not relevant. The 19,161 documents were shared with NIST, where they were",1,TREC,True
11,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6172-9/19/07. https://doi.org/10.1145/3331184.3331354",1,ad,True
12,"again assessed, along with 8,902 documents identified by depth-10 pooling, and 1,010 identified by move-to-front (MTF) pooling [7] to form the 26,233 official TREC relevance assessments (qrels).",1,TREC,True
13,"A primary concern was the speed with which we could render the assessments necessary to form a viable test collection, so as to conform to our particular resource constraints, as well as those of anyone else who might wish to employ our method. After an exploratory phase in which one author used a rudimentary CAL tool and search engine to find as many relevant documents as possible in about 13 minutes per topic, a separate team of five of the authors spent about 45 minutes per topic using HiCAL 1 [1, 12], which we adapted for this purpose, to conduct further searches and to draw a dynamic stratified sample of 300 additional documents per topic for assessment. In total, about 50 person-hours (i.e., one hour per topic) were devoted to compiling our relevance assessments.",1,ad,True
14,"To evaluate the outcome of our DS effort, we consider the following five questions:",0,,False
15,"· How nearly all of the relevant documents were included in the universe of documents identified using DS, from which the sample was drawn?",0,,False
16,"· What is the accuracy of MAP estimates derived from the sample, measured by the average difference (bias) and RMS difference between the estimates and ground truth, per the official NIST assessments?",1,MAP,True
17,"· What is the accuracy with which systems are ranked according to MAP estimates, measured by the rank correlation ( or AP ), with the ranking afforded by ground truth?",1,MAP,True
18,· How do these outcomes compare to the NIST depth-10 and MTF pooling efforts?,0,,False
19,"· How might our or NIST's efforts have yielded better outcomes, with a different allocation of assessment effort?",0,,False
20,"Overall, our results indicate that assessment budgets would be better spent if allocated to DS rather than to pooling.",0,,False
21,2 SPEEDY ASSESSMENT,0,,False
22,"In the first of three distinct assessment phases, one of the authors used CAL, as implemented in the TREC 2015 Total Recall Track Baseline Implementation (BMI),2 to identify potentially relevant paragraphs, which were rendered for assessment, along with the document containing them, on an ASCII terminal. Of 3,601 documents rendered during this phase, 1,391 were assessed relevant. For a few topics where no relevant documents were seen within the first several dozen rendered documents, the assessor reverted to a search engine,3 finding a total of 47 more relevant documents over all of these topics. Nevertheless, the assessor found no relevant documents for several topics, and fewer than ten relevant",1,TREC,True
23,1See https://github.com/hical. 2See http://cormack.uwaterloo.ca/trecvm/. 3See http://stefan.buettcher.org/cs/wumpus/index.html.,1,trec,True
24,1217,0,,False
25,Short Research Papers 3B: Recommendation and Evaluation,0,,False
26,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
27,Method: DS NIST Depth-10 Depth-10 + MTF,0,,False
28,Avg. Coverage: 0.88 0.82 0.47,0,,False
29,0.49,0,,False
30,Min. Coverage: 0.58 0.48 0.11,0,,False
31,0.13,0,,False
32,Max. Coverage: 1.00 1.00 1.00,0,,False
33,1.00,0,,False
34,Table 1: Coverage of document-selection methods.,0,,False
35,"documents for 20 topics. Phase 1 consumed a total of 11.1 hours (i.e., 13.2 minutes per topic).",0,,False
36,"The second and third phases were conducted by a team of five different authors, who used the HiCAL system to render assessments for 10 topics each. Phase 2 involved the use of HiCAL's search and full-document-display mode to find more documents relevant to the 20 topics for which the first phase had found fewer than 10. Phase 2 was allocated 30 minutes of assessment time for each of these 20 topics (i.e., 12 minutes per topic overall).",1,ad,True
37,"Phase 3 involved the use of HiCAL's CAL and paragraph-onlydisplay mode to present paragraphs, excerpted from potentially relevant documents, for assessment. HiCAL was modified to present excerpts from only a sample of documents, selected using DS with strategy D25 [3] (see Section 7), and a budget of 300 assessments per topic. The initial training set consisted of all assessments from phases 1 and 2. Assessment time for phase 3 averaged 33 minutes per topic, bringing the total for all three phases to about one hour per topic.4",0,,False
38,3 COVERAGE,0,,False
39,"We define coverage to be the fraction of all relevant documents in the DS universe. In other words, coverage is the recall of the DS effort, before sampling and before relevance assessment. In order to estimate coverage of our assessment strategy compared to others, it is necessary to estimate the number of relevant documents for each topic. To this end, DS provides an unbiased statistical estimate of the number of relevant documents in the universe it identifies as the sample space from which its sample is drawn. Here, the size of the universe was 39,214 documents, from which a sample of 19,161 was drawn. Pooling and MTF independently identified 1,305 documents--of which 503 were relevant--from the DS universe, which were counted directly and removed from the sample space. Pooling and MTF identified 404 additional relevant documents outside the universe. Overall, our best estimate indicates that there are 5,457 relevant documents in the corpus.",1,ad,True
40,"Table 1 shows the average, minimum, and maximum coverage over 50 topics achieved by the construction methods under consideration. The DS universe covers 88% of all relevant documents, on average, and 58%, in the worst case, for this collection. The NIST qrels have lower coverage because, although they include more assessments, they do not constitute a statistical sample and cannot be extrapolated to a larger population. The depth-10 pool, whether augmented or not by MTF, has substantially inferior recall.",0,,False
41,"4Due to a system misconfiguration, assessors spent several additional minutes in a false start for phase 3, the results of which were discarded.",1,ad,True
42,4 ACCURACY,0,,False
43,Figure 1 shows scatterplots and summary statistics comparing the MAP estimates afforded by DS using the xinfAP estimator5 [11],1,MAP,True
44,"to the official TREC results, for each of the runs submitted to the",1,TREC,True
45,"TREC 2018 Common Core Track. Points denoted ""M"" represent",1,TREC,True
46,"so-called manual runs; points denoted ""R"" indicate runs that relied",0,,False
47,on legacy relevance assessments for the same topics but a different,0,,False
48,"corpus; points denoted "" "" indicate fully automatic runs. We see",0,,False
49,"that our DS assessments yield low bias6 (b , 0.02) and RMS error",0,,False
50,(RMSE,0,,False
51,",",0,,False
52,"0.02),",0,,False
53,with,0,,False
54,negligible,0,,False
55,variance,0,,False
56,( 2,0,,False
57,",",0,,False
58,2,0,,False
59,b,0,,False
60,-,0,,False
61,RMSE2,0,,False
62,0).,0,,False
63," , 0.88 and AP , 0.83 correlation scores [10] are typical of those",1,AP,True
64,arising from inter-assessor disagreement.,0,,False
65,The top right panel shows the result of substituting NIST as-,0,,False
66,"sessments in place of our own, to eliminate inter-assessor disagreement.  , 0.95 and AP ,"" 0.92 are substantially higher, while bias essentially vanishes, exposing a small variance as evidenced by RMSE "", 0.01.",1,AP,True
67,"The bottom two panels show results for depth-10 pooling, and",0,,False
68,depth-10 pooling augmented by MTF. Rank correlations are slightly,0,,False
69,"lower, while bias and error are substantially higher.",0,,False
70,5 STATISTICAL GROUND TRUTH,0,,False
71,"The method we employed to estimate coverage (Section 3) yields a revised but statistically valid sample of the DS universe, augmented by 404 relevant documents outside the DS universe. Statistical estimates derived from this sample using xinfAP--which we dub statistical ground truth--should, in theory, be more accurate than the official ground truth derived from from the same assessments using trec_eval.",1,AP,True
72,"We can evaluate the extent to which the statistical and official ground truths agree. The left panel of Figure 2 shows strong but imperfect agreement. The right panel of Figure 2 and the top-right panel of Figure 1, evaluate accuracy of the same NIST-assessed DS sample, according to the statistical and official ground truth, respectively. The statistical ground truth indicates much higher accuracy. If the statistical ground truth is indeed the gold standard, this result suggests that the dynamic sample alone--without any documents discovered by pooling or MTF--may also yield ground truth more accurate than the official ground truth.",0,,False
73,6 INCREASING COVERAGE,0,,False
74,"We used a fixed budget of 300 assessments for the third phase of our assessments, which had previously been shown to achieve good results for the D25 sampling strategy [3] (see Section 7). Arguably good results were achieved here; however, Table 1 indicates that coverage for at least one topic was less than 60%. We investigated whether the cause of occasionally poor coverage was assessor disagreement or inadequacy of the assessment budget.",1,ad,True
75,"Figure 3 shows coverage for each of the 50 topics, as well as the average, as a function of the total number of assessments for the three phases. We see that the slope for most of the curves is near-zero when the budget is exhausted. But for several of the topics--notably those with coverage of less than about 80%--the slope is noticeably positive, indicating that, had the budget been extended",1,ad,True
76,5,0,,False
77,trec.nist.gov/data/clinical/sample_eval.pl 6The arithmetic mean of error.,1,trec,True
78,1218,0,,False
79,Short Research Papers 3B: Recommendation and Evaluation,0,,False
80,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
81,0.5,0,,False
82,0.5,0,,False
83,q Automatic R Feedback M Manual,0,,False
84,q Automatic R Feedback M Manual,0,,False
85,0.4,0,,False
86,"infAP as per DS irels, NIST assessments",1,AP,True
87,0.4,0,,False
88,infAP as per DS irels and assessments,1,AP,True
89,0.3,0,,False
90,0.2,0,,False
91,"Tau , 0.88 Tau AP , 0.83 RMSE , 0.02 Bias , -0.02",1,AP,True
92,MM,0,,False
93,q,0,,False
94,RMRRRRRRRRRR,0,,False
95,q q q,0,,False
96,q RqqRqqqRqRRRRRqqRRqqq q qRqRqRRRR R,0,,False
97,q R,0,,False
98,R,0,,False
99,qq,0,,False
100,qqq,0,,False
101,0.1,0,,False
102,0.2,0,,False
103,0.3,0,,False
104,M M,0,,False
105,"Tau , 0.95",0,,False
106,"Tau AP , 0.92 RMSE , 0.01 Bias , 0.00",1,AP,True
107,RMRRRRRRRRR qqq,0,,False
108,qRqqRqRRRRqRqRqqqRqRRRRqqRR,0,,False
109,qR,0,,False
110,q,0,,False
111,qq,0,,False
112,R qR,0,,False
113,qq,0,,False
114,qqq,0,,False
115,0.1,0,,False
116,0.0,0,,False
117,0.0,0,,False
118,0.5,0,,False
119,0.4,0,,False
120,MAP as per qrels Depth-10 + MTF,1,MAP,True
121,0.3,0,,False
122,MAP as per qrels Depth-10,1,MAP,True
123,0.0,0,,False
124,0.1,0,,False
125,0.2,0,,False
126,0.3,0,,False
127,0.4,0,,False
128,0.5,0,,False
129,MAP as per NIST qrels and assessments,1,MAP,True
130,q Automatic R Feedback M Manual,0,,False
131,MM,0,,False
132,"Tau , 0.93 Tau AP , 0.89 RMSE , 0.05 Bias , 0.04",1,AP,True
133,q qq qR R,0,,False
134,qqq RqqRqRRRRqRqqRqqqqRqqRRRRqRR q,0,,False
135,q q R,0,,False
136,RMRRRRRRRRRR,0,,False
137,0.2,0,,False
138,0.3,0,,False
139,0.4,0,,False
140,0.5,0,,False
141,0.0,0,,False
142,0.1,0,,False
143,0.2,0,,False
144,0.3,0,,False
145,0.4,0,,False
146,0.5,0,,False
147,MAP as per NIST qrels and assessments,1,MAP,True
148,q Automatic R Feedback M Manual,0,,False
149,"Tau , 0.94 Tau AP , 0.91 RMSE , 0.05 Bias , 0.05",1,AP,True
150,M M MRRRRRRRRRR R,0,,False
151,RqqRqRRRRqRqqRqqqqRqqRRRRqqRRqqqq q q R,0,,False
152,q qq,0,,False
153,qR R,0,,False
154,0.2,0,,False
155,0.1,0,,False
156,0.1,0,,False
157,q q,0,,False
158,qqq,0,,False
159,q q,0,,False
160,qqq,0,,False
161,0.0,0,,False
162,0.0,0,,False
163,0.0,0,,False
164,0.1,0,,False
165,0.2,0,,False
166,0.3,0,,False
167,0.4,0,,False
168,0.5,0,,False
169,MAP as per NIST qrels and assessments,1,MAP,True
170,0.0,0,,False
171,0.1,0,,False
172,0.2,0,,False
173,0.3,0,,False
174,0.4,0,,False
175,0.5,0,,False
176,MAP as per NIST qrels and assessments,1,MAP,True
177,"Figure 1: Accuracy of MAP estimates using DS vs. pooling methods, compared to official TREC 2018 Common Core Track evaluation. The top-left panel shows results for DS with relevance assessments by the authors; the top-right panel shows results for DS with official relevance assessments by NIST. The bottom-left panel shows results for the depth-10 pool with relevance assessments by NIST; the bottom-right shows results for the depth-10 pool augmented by MTF, with relevance assessments by NIST.",1,MAP,True
178,0.5,0,,False
179,0.5,0,,False
180,q Automatic R Feedback M Manual,0,,False
181,q Automatic R Feedback M Manual,0,,False
182,0.4,0,,False
183,"infAP as per DS irels, NIST assessments",1,AP,True
184,0.4,0,,False
185,MAP as per NIST qrels and assessments,1,MAP,True
186,0.3,0,,False
187,0.2,0,,False
188,"Tau , 0.96 Tau AP , 0.94 RMSE , 0.01 Bias , 0.00",1,AP,True
189,M M,0,,False
190,RMRRRRRRRRR,0,,False
191,qqq qRRqRRqRRRqRqqRqRqqRqRRRqRq Rq,0,,False
192,R qR,0,,False
193,q qq,0,,False
194,q q,0,,False
195,qq,0,,False
196,0.1,0,,False
197,0.2,0,,False
198,0.3,0,,False
199,M M,0,,False
200,"Tau , 0.98",0,,False
201,"Tau AP , 0.97 RMSE , 0.00 Bias , 0.00",1,AP,True
202,MRRRRRRRR R qq,0,,False
203,qRRqRRqRRRqRqqRqRqqRqRRRqRq,0,,False
204,Rq,0,,False
205,q,0,,False
206,qq,0,,False
207,R qR,0,,False
208,qq,0,,False
209,qq,0,,False
210,0.1,0,,False
211,0.0,0,,False
212,0.0,0,,False
213,0.0,0,,False
214,0.1,0,,False
215,0.2,0,,False
216,0.3,0,,False
217,0.4,0,,False
218,0.5,0,,False
219,"infAP as per DS+NIST irels, NIST assessments",1,AP,True
220,0.0,0,,False
221,0.1,0,,False
222,0.2,0,,False
223,0.3,0,,False
224,0.4,0,,False
225,0.5,0,,False
226,"infAP as per DS+NIST irels, NIST assessments",1,AP,True
227,"Figure 2: Accuracy of official NIST qrels (left) and DS with NIST assessments (right), according to statistical ground truth.",0,,False
228,1219,0,,False
229,Short Research Papers 3B: Recommendation and Evaluation,0,,False
230,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
231,1.0,0,,False
232,0.9,0,,False
233,0.8,0,,False
234,0.7,0,,False
235,System Recall,0,,False
236,0.6,0,,False
237,0.5,0,,False
238,0.4,0,,False
239,0.3,0,,False
240,0.2,0,,False
241,0.1,0,,False
242,0.0 0.0,0,,False
243,0.2,0,,False
244,0.4,0,,False
245,0.6,0,,False
246,Review Effort (% Assessments),0,,False
247,50 topics Averaged,0,,False
248,0.8,0,,False
249,1.0,0,,False
250,Figure 3: Coverage as a function of overall assessment effort.,0,,False
251,"for these topics, higher coverage would have been achieved. We also note that several topics achieving ostensibly high coverage have substantial slope when the budget is exhausted, suggesting a shortfall in the ground truth estimate of the number of relevant documents. These results suggest that using the ""Knee Method"" as a stopping criterion, which has been shown to work well for CAL [4], might be preferable to a fixed assessment budget.",0,,False
252,"A similar approach might also have yielded better results for MTF, which was constrained by a restrictive assessment budget [2].",0,,False
253,7 DISCUSSION AND LIMITATIONS,0,,False
254,"For brevity, we present MAP results estimated by xinfAP, consistent with common practice. In a companion article [6], Cormack and Grossman show that better estimates of MAP, as well as precision at cutoff (P@k), rank-biased-precision (RBP), and normalized discounted cumulative gain (NDCG) may be derived from a DS test collection, using the purpose-built dyn_eval estimator.",1,MAP,True
255,"Estimates derived from DS assume that the DS universe includes substantially all relevant documents; DS yields an unbiased, nonuniform statistical sample from which MAP and other evaluation measures are derived. Hence, the effectiveness of any run--not just a member of an evaluation pool--may be evaluated using a DS collection.",1,MAP,True
256,"One can increase coverage by increasing the size of the universe, at the expense of higher variance. The evidence presented here suggests the the DS universe does indeed contain a substantial majority of the relevant documents. Future work may explore the influence of three parameters that balance the tension between coverage, sampling budget, and skew of the sampling rate in favour of likely relevant documents. The sampling budget of 300 was occasioned by our target of one hour of assessment time per topic. Given the fact that we had no pool of runs, we were further constrained to use content-only features for the learner. Within these constraints, strategies D12, D25, and D50, reflecting different tradeoffs between coverage and sampling rate, appeared equally good [3], and we chose the median. Further investigation might yield a better tradeoff. If it were logistically feasible, a flexible assessment budget with an average of 300 documents per topic, and an amenable stopping criterion, might have yielded better results, for the same overall assessment budget.",1,ad,True
257,We have measured the effect of assessor disagreement only on the MAP estimates derived from identical sets of assessments. Our,1,MAP,True
258,"results show that DS conducted by one set of assessors (the authors) can achieve high coverage in the eyes of another (the NIST assessors). If NIST assessors had conducted the DS assessments, would coverage have been higher, and, if so, how much higher?",1,ad,True
259,"Finally, we note that the relevance determinations of assessors are influenced by context; in particular, the order and richness of the documents they are shown [8]. DS assessors are shown the most-likely relevant documents first, which suggests they are likely be more stringent in their judgment of relevance, at least at the outset. As the density of relevant documents inevitably decreases, some assessors may have a tendency to ""reach"" and thus, be more likely to judge a document relevant than at the outset. Our design controls for this possible effect with respect to the NIST assessments for the DS sample, because the NIST assessors were unaware of the order in which the DS documents were discovered, or, indeed, whether a document was identified by pooling, by DS, or both. Our assessments with respect to the DS sample, and NIST's assessments with respect to MTF, might have been so influenced.",0,,False
260,8 CONCLUSION,0,,False
261,"Independent of NIST assessments or any pool of submitted runs, a small team of researchers spent 50 hours to create a set of sampled relevance assessments that effectively scores and ranks systems according to MAP over 50 topics. This level of effort represents an order-of-magnitude reduction in human effort, compared to typical efforts like TREC, and does not rely on pooling, which is both logistically challenging and a potential source of bias against systems not contributing to the pool. DS avoids this source of bias and, as both theory and empirical evidence show, does not introduce bias against the TREC runs that had no influence on the DS selection strategy, or our relevance assessments.",1,MAP,True
262,ACKNOWLEDGMENTS,0,,False
263,Special thanks to Ellen M. Voorhees for integrating the results of our DS process into the official NIST assessment effort.,0,,False
264,REFERENCES,0,,False
265,"[1] Abualsaud, M., Ghelani, N., Zhang, H., Smucker, M. D., Cormack, G. V., and Grossman, M. R. A system for efficient high-recall retrieval. In SIGIR 2018.",0,,False
266,"[2] Allan, J., Harman, D., Kanoulas, E., and Voorhees, E. TREC 2018 Common Core Track overview. In TREC 2018.",1,TREC,True
267,"[3] Cormack, G. V., and Grossman, M. R. Beyond pooling. In SIGIR 2018. [4] Cormack, G. V., and Grossman, M. R. Engineering quality and reliability in",0,,False
268,"technology-assisted review. In SIGIR 2016. [5] Cormack, G. V., and Grossman, M. R. Evaluation of machine-learning protocols",0,,False
269,"for technology-assisted review in electronic discovery. In SIGIR 2014. [6] Cormack, G. V., and Grossman, M. R. Unbiased low-variance estimators for",0,,False
270,"precision and related information retrieval effectiveness measures. In SIGIR 2019 (2019). [7] Cormack, G. V., Palmer, C. R., and Clarke, C. L. Efficient construction of large test collections. In SIGIR 1998. [8] Roegiest, A., and Cormack, G. V. Impact of review-set selection on human assessment for text classification. In SIGIR 2016. [9] Sanderson, M., et al. Test collection based evaluation of information retrieval systems. Foundations and Trends® in Information Retrieval 4, 4 (2010). [10] Yilmaz, E., Aslam, J. A., and Robertson, S. A new rank correlation coefficient for information retrieval. In SIGIR 2008. [11] Yilmaz, E., Kanoulas, E., and Aslam, J. A. A simple and efficient sampling method for estimating AP and NDCG. In SIGIR 2008. [12] Zhang, H., Abualsaud, M., Ghelani, N., Smucker, M. D., Cormack, G. V., and Grossman, M. R. Effective user interaction for high-recall retrieval: Less is more. In CIKM 2018.",1,AP,True
271,1220,0,,False
272,,0,,False

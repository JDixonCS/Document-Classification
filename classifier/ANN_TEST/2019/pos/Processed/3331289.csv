,sentence,label,data,regex
0,Short Research Papers 1B: Recommendation and Evaluation,0,,False
1,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
2,An Analysis of Query Reformulation Techniques for Precision Medicine,1,Query,True
3,"Maristella Agosti, Giorgio Maria Di Nunzio, Stefano Marchesin",0,,False
4,"Department of Information Engineering University of Padua, Italy",1,ad,True
5,"{maristella.agosti,giorgiomaria.dinunzio,stefano.marchesin}@unipd.it",0,,False
6,ABSTRACT,0,,False
7,"The Precision Medicine (PM) track at the Text REtrieval Conference (TREC) focuses on providing useful precision medicine-related information to clinicians treating cancer patients. The PM track gives the unique opportunity to evaluate medical IR systems using the same set of topics on two different collections: scientific literature and clinical trials. In the paper, we take advantage of this opportunity and we propose and evaluate state-of-the-art query expansion and reduction techniques to identify whether a particular approach can be helpful in both scientific literature and clinical trial retrieval. We present those approaches that are consistently effective in both TREC editions and we compare the results obtained with the best performing runs submitted to TREC PM 2017 and 2018.",1,TREC,True
8,CCS CONCEPTS,0,,False
9,· Information systems  Specialized information retrieval; Ontologies; Query reformulation.,1,Query,True
10,KEYWORDS,0,,False
11,Medical IR; query reformulation; precision medicine,0,,False
12,"ACM Reference Format: Maristella Agosti, Giorgio Maria Di Nunzio, Stefano Marchesin. 2019. An Analysis of Query Reformulation Techniques for Precision Medicine. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184. 3331289",1,Query,True
13,1 MOTIVATIONS,0,,False
14,"Medical Information Retrieval (IR) helps a wide variety of users to access and search medical information archives and data [4]. In [7, chapter 2], a classification of textual medical information is proposed: 1) Patient-specific information which applies to individual patients. This type of information can be structured, as in the case of an Electronic Health Record (EHR), or can be free narrative text. 2) Knowledge-based information that has been derived and organized from observational or experimental research. In the case of clinical research, the information is most commonly provided by books",0,,False
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331289",1,ad,True
16,"and journals, but can take a wide variety of other forms, including computerized media. Therefore, the design of effective tools to access and search textual medical information requires, among other things, enhancing the query through expansion and/or rewriting techniques that leverage the information contained within knowledge resources. In this context, Sondhi et al. [12] identified some challenges arising from the differences between general retrieval and medical case-based retrieval. In particular, state-of-the-art retrieval methods, combined with selective query term weighing based on medical thesauri and physician feedback, improve performance significantly [3, 13].",0,,False
17,"In 2017 and 2018, the Precision Medicine (PM) [10] track1 at the Text REtrieval Conference (TREC)2 focused on an important use case in clinical decision support: providing useful precision medicine-related information to clinicians treating cancer patients. This track gives a unique opportunity to evaluate medical IR systems since the experimental collection is composed of a set of topics (synthetic cases created by precision oncologists) for two different collections that target two different tasks: 1) retrieving biomedical articles addressing relevant treatments for a given patient, and 2) retrieving clinical trials for which a patient ­ described in the information need ­ is eligible.",1,TREC,True
18,"The objective of our study is to take advantage of this opportunity and evaluate several state-of-the-art query expansion and reduction techniques to examine whether a particular approach can be helpful in both scientific literature and clinical trials retrieval. Given the large number of participating research groups to this TREC track, we are able to compare the best experiments submitted to the PM track based on the results which were obtained applying our approach in the last two years. The experimental analysis shows that there are some common patterns in query reformulation that allow the retrieval system to achieve top performing results in both tasks.",1,ad,True
19,"The rest of the paper is organized as follows: Section 2 describes the approach used to evaluate different query reformulation techniques. Section 3 presents the experimental setup and compares the results obtained using our approach with the best performing runs from TREC PM 2017 and 2018. Finally, Section 4 reports some final remarks and concludes the paper.",1,TREC,True
20,2 APPROACH,1,AP,True
21,"The approach we propose for query expansion/reduction in a PM task comprises three steps, plus an additional fourth step required only for the retrieval of clinical trials. The steps are: (i) indexing, (ii) query reformulation, (iii) retrieval and (iv) filtering.",1,ad,True
22,1 http://www.trec- cds.org/ 2 https://trec.nist.gov/,1,trec,True
23,973,0,,False
24,Short Research Papers 1B: Recommendation and Evaluation,0,,False
25,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
26,"Indexing Step. We create the following fields to index clinical trials collections: <docid>, <text>, <max_age>, <min_age> and <gender>. Fields <max_age>, <min_age> and <gender> contain information extracted from the eligibility section of clinical trials and are required for the filtering step. The <text> field contains the entire content of each clinical trial -- and therefore also the information stored within the fields described above.",0,,False
27,"To index scientific literature collections, we create the following fields: <docid> and <text>. As for clinical trials, the <text> field contains the entire content of each target document.",0,,False
28,Query Reformulation Step. The approach relies on two types of query reformulation techniques: query expansion and query reduction.,1,Query,True
29,"Query expansion: We perform a knowledge-based a priori query expansion. First, we rely on MetaMap [2], a state-of-the-art medical concept extractor, to extract from each query field all the Unified Medical Language System (UMLS)3 concepts belonging to the following semantic types4: Neoplastic Process (neop), Gene or Genome (gngm) and Cell or Molecular Dysfunction (comd). The gngm and comd semantic types are related to the query <gene> field, while neop is related to the <disease> field. For those collections where an additional <other> field is included -- which considers other potential factors that may be relevant -- MetaMap is used on <other> with no restriction on the semantic types, as its content does not consistently refer to any particular semantic type.",1,Query,True
30,"Second, for each extracted concept, we consider all its name variants contained into the following knowledge sources: National Cancer Institute5 (NCI), Medical Subject Headings6 (MeSH), SNOMED CT7 (SNOMEDCT) and UMLS Metathesaurus8 (MTH). All knowledge sources are manually curated and up-to-date.",1,ad,True
31,"The expanded queries consist in the union of the original terms with the set of name variants. For example, consider a query only containing the word ""melanoma"" -- which is mapped to the UMLS concept C0025202. The set of name variants for the concept ""melanoma"" contains, among many others: cutaneous melanoma; malignant melanoma; malignant melanoma (disorder); etc. Therefore, the final expanded query is the union of the original term ""melanoma"" with all its name variants.",0,,False
32,"Additionally, we expand queries that do not mention any kind of blood cancer (e.g. ""lymphoma"" or ""leukemia"") with the term solid. This expansion proved to be effective in [5] where the authors found that a large part of relevant clinical trials do not mention the exact disease. A more general term like solid tumor is preferable and more effective.",0,,False
33,"Query reduction: We reduce original queries by removing, whenever present, gene mutations from the <gene> field. To clarify, consider a topic where the <gene> field mentions ""BRAF (V600E)"". After the reduction process, the <gene> field becomes ""BRAF"". The reduction process aims to mitigate the over-specificity of topics, since the information contained in a topic is too specific compared to those contained in the target documents [8].",1,Query,True
34,3 https://www.nlm.nih.gov/research/umls/ 4 https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml 5 https://www.cancer.gov/ 6 https://www.ncbi.nlm.nih.gov/mesh/ 7 http://www.snomed.org/ 8 https://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/,0,,False
35,"Additionally, we remove the <other> field from those collections that include it -- since it contains additional factors that are not necessarily relevant, thus representing a potential source of noise in retrieving precise information for patients.9",1,ad,True
36,"Retrieval Step. We use BM25 [11] as retrieval model. Additionally, query terms obtained through query expansion are weighted lower than 1.0 to avoid introducing too much noise in the retrieval process [6].",0,,False
37,"Filtering Step. The eligibility section in clinical trials comprises, among others, three important demographic aspects that a patient needs to satisfy to be considered eligible for the trial, namely: minimum age, maximum age and gender; where minimum age and maximum age are the minimum and the maximum age, respectively, required for a patient to be considered eligible for the trial, while gender is the required gender.",0,,False
38,"Therefore, after the retrieval step, we filter out from the list of candidate trials those for which a patient is not eligible -- i.e. his/her demographic data (age and gender) does not satisfy the three aforementioned eligibility criteria aforementioned. In those cases where part of the demographic data is not specified, a clinical trial is kept or discarded on the basis of the remaining demographic information. For instance, if the clinical trial does not specify a required minimum age, then it is kept or discarded based on its maximum age and gender required values.",0,,False
39,3 SETUP AND EVALUATION,0,,False
40,"In this section, we describe the experimental collections and the setup used to conduct and evaluate our approach. Then, we compare the results obtained with our approach with those of the best performing systems from TREC PM 2017 and 2018. All these systems make use of external knowledge sources to enhance retrieval performance; moreover, most of them are complex multi-stage retrieval systems, like those proposed in [5, 8], while the approach we present is quite simple and straightforward ­ facilitating its reproducibility.10",1,TREC,True
41,"Experimental Collections. Both tasks in TREC PM use the same set of topics, but with two different collections: scientific literature, clinical trials.",1,TREC,True
42,"Topics consists of 30 and 50 synthetic cases created by precision oncologists in 2017 and 2018, respectively. In 2017, topics contain four key elements in a semi-structured format: (1) disease (e.g. a type of cancer), (2) genetic variants (primarily present in tumors), (3) demographic information (e.g. age, gender), and (4) other factors (which could impact certain treatment options). In 2018, topics contain three of the four key elements used in 2017: (1) disease, (2) genetic variants, and (3) demographic information.",0,,False
43,"Scientific Literature consists of a set of 26,759,399 MEDLINE11 abstracts, plus two additional sets of abstracts: (i) 37,007 abstracts from recent proceedings of the American Society of Clinical Oncology (ASCO), and (ii) 33,018 abstracts from recent proceedings of the American Association for Cancer Research (AACR). These",1,ad,True
44,"9In a personal communication with the organizers of the track, we have been informed that it was difficult to convince the oncologists why the other field was even necessary. 10Source code available at: https://github.com/stefano-marchesin/TREC_PM_qreforms 11 https://www.nlm.nih.gov/bsd/pmresources.html",1,TREC,True
45,974,0,,False
46,Short Research Papers 1B: Recommendation and Evaluation,0,,False
47,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
48,"additional datasets were added to increase the set of potentially relevant treatment information. In fact, precision medicine is a fastmoving field where keeping up-to-date with the latest literature can be challenging due to both the volume and velocity of scientific advances. Therefore, when treating patients, it may be helpful to present the most relevant scientific articles for an individual patient. Relevant literature articles can guide precision oncologists to the best-known treatment options for the patient's condition.",1,ad,True
49,"Clinical Trials consists of a total of 241,006 clinical trial descriptions, derived from ClinicalTrials.gov12 -- a repository of clinical trials in the U.S. and abroad. When none of the available treatments are effective on oncology patients, the common recourse is to determine if any potential treatments are undergoing evaluation in a clinical trial. Therefore, it would be helpful to automatically identify the most relevant clinical trials for an individual patient. Precision oncology trials typically use a certain treatment for a certain disease with a specific genetic variant (or set of variants). Such trials can have complex inclusion and/or exclusion criteria that are challenging to match with automated systems.",1,ad,True
50,"Experimental Setup. We use Whoosh,13 a pure Python search engine library, for indexing, retrieval and filtering steps. For BM25, we keep the default values k1 , 1.2 and b ,"" 0.75 provided by Whoosh ­ as we found them to be a good combination [1]. For query expansion, we rely on MetaMap to extract and disambiguate concepts from UMLS. We summarize the procedure used for each experiment below. Indexing""",0,,False
51,"· Index clinical trials using the following created fields: <docid>, <text>, <max_age>, <min_age> and <gender>;",0,,False
52,· Index scientific abstracts using the following created fields: <docid> and <text>.,0,,False
53,Query reformulation,1,Query,True
54,"· Use MetaMap to extract from each query field the UMLS concepts restricted to the following semantic types: neop for <disease>, gngm/comd for <gene> and all for <other>;",0,,False
55,"· Extract from the concepts all name variants belonging to NCI, MeSH, SNOMED CT and MTH knowledge sources;",0,,False
56,"· Expand (or not) topics that do not mention ""lymphoma"" or ""leukemia"" with the term solid;",0,,False
57,"· Reduce (or not) queries by removing, whenever present, gene mutations from the <gene> field;",0,,False
58,· Remove (or not) the <other> field.,0,,False
59,Retrieval,0,,False
60,"· Adopt any combination of the reformulation strategies; · Weigh expanded terms with a value k  {0, 0.1, 0.2, ..., 1}; · Perform a search using expanded queries with BM25.",0,,False
61,Filtering,0,,False
62,· Filter out clinical trials for which the patient is not eligible.,0,,False
63,"Evaluation Measures. We use the official measures adopted in the TREC PM track: inferred nDCG (infNDCG), R-precision (Rprec) and Precision at rank 10 (P_10). Precision at rank 5 and at rank 10 were used only for the Clinical Trials task 2017 and are not",1,ad,True
64,12 https://clinicaltrials.gov/ 13 https://whoosh.readthedocs.io/en/latest/intro.html,1,ad,True
65,reported in this work for space reasons. The inferred nDCG was not computed for the task Clinical Trials 2017 since the sampled relevance judgments are not available.,0,,False
66,"Comparison. In Table 1, we report the results of our experiments (upper part) and compare them with the top performing participants at TREC 2017 and 2018 (lower part). Given the large number of experiments, we decided to present the top 5 runs ordered by P_10 for each year and for each task. Each line shows a particular combination (yes or no values) of semantic types (neop, comd, gngm), usage and expansion of <other> field (oth, oth_exp), query reduction (orig), and expansion using weighted solid (tumor) keyword. We use the symbol `·' to indicate that the features oth, oth_exp are not applicable for year 2018 due to the absence of the <other> field in 2018 topics. We report the results for both Scientific Literature (sl) and Clinical Trials (ct) tasks. We highlight in bold the top 3 scores for each measure, and we use the symbols  and  to indicate two combinations that performed well in both 2017 and 2018. For the TREC PM participants, we select those participants who submitted runs in both years and reached the top 10 performing runs in at least two measures [9, 10]. The results reported in the lower part of Table 1 indicate the best score obtained by a particular run for a specific measure; the best results of a participant are often related to different runs. The symbol `-' means that the measure is not available, while `<' indicates that none of the runs submitted by the participant achieved the top 10 performing runs. For comparison, we add for each measure the lowest score required to enter the top 10 TREC results list, and the score obtained by the best combination of our approach -- indicated by the line number ­ as if we were participants of these tracks.",1,TREC,True
67,"In 2018, there is a clear distinction in terms of performances among the combinations that achieve the best results for the sl and the ct tasks. For the sl task, considering the semantic type neop expansion without using the umbrella term solid provides the best performances for all the measures considered. On the other hand, two of the best three runs for the ct task (line 5 and 9), use no semantic type expansion, but rely on the solid (tumor) expansion with weight 0.1.",0,,False
68,"In 2017, the situation is completely different. Lines 12 and 13 show two combinations that are in the top 3 performing runs for both sl and ct. These two runs use query reduction and a weighted 0.1 solid (tumor) expansion. The use of a weighted 0.1 solid expansion as well as a reduced query (orig ,"" n) seems to improve performances consistently for all measures in 2017. The semantic type gngm seems more effective than neop, while comd does not seem to have any positive effect at all.""",0,,False
69,Another element that shows how difficult these two tasks are is the fact that top performing systems in 2017 do not achieve the same results in 2018. Our study therefore helps researchers to select (or remove) semantic types to build strong baselines for both tasks.,0,,False
70,4 CONCLUSIONS AND FINAL REMARKS,0,,False
71,"In this paper, we proposed and evaluated several state-of-the-art query expansion and reduction techniques for scientific literature and clinical trials retrieval. The experimental analysis showed that no clear pattern emerges for both tasks. In general, a query expansion approach using a selected set of semantic types helps the",0,,False
72,975,0,,False
73,Short Research Papers 1B: Recommendation and Evaluation,0,,False
74,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
75,Semantic Type,0,,False
76,Field Other,0,,False
77,line year neop comd gngm oth oth_exp orig solid,0,,False
78,1 2018 y,0,,False
79,y,0,,False
80,n,0,,False
81,·,0,,False
82,·,0,,False
83,y,0,,False
84,n,0,,False
85,2 2018 y,0,,False
86,n,0,,False
87,n,0,,False
88,·,0,,False
89,·,0,,False
90,y,0,,False
91,n,0,,False
92,3 2018 y,0,,False
93,n,0,,False
94,y,0,,False
95,·,0,,False
96,·,0,,False
97,y,0,,False
98,n,0,,False
99,4 2018 n,0,,False
100,n,0,,False
101,n,0,,False
102,·,0,,False
103,·,0,,False
104,y,0,,False
105,n,0,,False
106,5 2018 n,0,,False
107,n,0,,False
108,n,0,,False
109,·,0,,False
110,·,0,,False
111,y 0.1,0,,False
112,6 2018 n,0,,False
113,y,0,,False
114,n,0,,False
115,·,0,,False
116,·,0,,False
117,y,0,,False
118,n,0,,False
119,7 2018 y,0,,False
120,n,0,,False
121,n,0,,False
122,·,0,,False
123,·,0,,False
124,n,0,,False
125,n,0,,False
126,8 2018 n,0,,False
127,n,0,,False
128,y,0,,False
129,·,0,,False
130,·,0,,False
131,y,0,,False
132,n,0,,False
133,9 2018 n,0,,False
134,n,0,,False
135,n,0,,False
136,·,0,,False
137,·,0,,False
138,n 0.1,0,,False
139,10 2018 y,0,,False
140,n,0,,False
141,y,0,,False
142,·,0,,False
143,·,0,,False
144,n,0,,False
145,n,0,,False
146,11 2017 y,0,,False
147,n,0,,False
148,y,0,,False
149,n,0,,False
150,n,0,,False
151,n 0.1,0,,False
152,12 2017 n,0,,False
153,n,0,,False
154,y,0,,False
155,n,0,,False
156,n,0,,False
157,n 0.1,0,,False
158,13 2017 n,0,,False
159,n,0,,False
160,n,0,,False
161,n,0,,False
162,n,0,,False
163,n 0.1,0,,False
164,14 2017 y,0,,False
165,n,0,,False
166,n,0,,False
167,n,0,,False
168,n,0,,False
169,n 0.1,0,,False
170,15 2017 n,0,,False
171,n,0,,False
172,n,0,,False
173,n,0,,False
174,n,0,,False
175,n,0,,False
176,n,0,,False
177,16 2017 y,0,,False
178,n,0,,False
179,y,0,,False
180,n,0,,False
181,n,0,,False
182,y 0.1,0,,False
183,17 2017 n,0,,False
184,n,0,,False
185,y,0,,False
186,n,0,,False
187,n,0,,False
188,y,0,,False
189,n,0,,False
190,sl P_10 0.5660 0.5640 0.5480 0.5460 0.5440 0.5440 0.5420 0.5340 0.5300 0.5140 0.5033 0.4900,0,,False
191,0.4800 0.4767 0.4733 0.4733 0.4633,0,,False
192,ct P_10 0.5540 0.5600 0.5660 0.5680 0.5740 0.5540 0.5700 0.5640,0,,False
193,0.5820 0.5680 0.3759 0.3931,0,,False
194,0.4034 0.3862 0.3931 0.3828 0.3862,0,,False
195,sl infNDCG,0,,False
196,0.4912 0.4961 0.4941 0.4876 0.4877 0.4853 0.4636 0.4877 0.4635 0.4572 0.3984 0.3881 0.3931 0.3974 0.3943 0.3567 0.3442,0,,False
197,ct infNDCG,0,,False
198,0.5266 0.5264 0.5292 0.5411 0.5403 0.5403 0.5345 0.5337,0,,False
199,0.5446 0.5393,0,,False
200,-,0,,False
201,sl Rprec 0.3288 0.3288 0.3266 0.3240 0.3247 0.3236 0.3180 0.3229 0.3148 0.3144 0.2697 0.2677,0,,False
202,0.2728 0.2714 0.2732 0.2329 0.2254,0,,False
203,ct Rprec 0.4098 0.4138 0.4116 0.4197 0.4179 0.4130 0.4134 0.4106,0,,False
204,0.4205 0.4122 0.3206 0.3263,0,,False
205,0.3361 0.3202 0.3241 0.3253 0.3243,0,,False
206,18 2018 19 2018 20 2018 21 2018 22 2018,0,,False
207,2018 2018 23 2017 24 2017 25 2017 26 2017 27 2017 2017 2017,0,,False
208,TREC PM Participant Identifier UTDHLTRI UCAS udel_fang NOVASearch Poznan,1,TREC,True
209,Top 10 threshold Best combination of our approach,0,,False
210,UTDHLTRI udel_fang NOVASearch,1,TD,True
211,Poznan UCAS Top 10 threshold Best combination of our approach,0,,False
212,0.6160 0.5980 0.5800,0,,False
213,< < 0.5800 (1) 0.5660 0.6300 0.5067 < < < 0.4667 (11) 0.5033,0,,False
214,0.5380 0.5460 0.5240 0.5520 0.5580 0.5240 (9) 0.5820 0.4172,0,,False
215,< 0.3966 0.3690 0.3724 0.3586 (13) 0.4034,0,,False
216,0.4797 0.5580 0.5081,0,,False
217,< < 0.4710 (2) 0.4961 0.4647 0.3897 < < < 0.3555 (11) 0.3984,0,,False
218,0.4794 0.5347 0.5057 0.4992 0.4894 0.4736 (9) 0.5446,0,,False
219,-,0,,False
220,< 0.3654 0.3289,0,,False
221,< < 0.2992 (1) 0.3288 0.2993 0.2503 < < 0.2282 0.2282 (15) 0.2732,0,,False
222,0.3920 0.4005 0.3967 0.3931 0.4101 0.3658 (9) 0.4205,0,,False
223,(13) 0.3361,0,,False
224,Table 1: Results for the TREC PM tasks 2017 and 2018. Details are reported in Section 3.,1,TREC,True
225,"retrieval of scientific literature, while a query reduction approach without expansion, but a small weighted solid (tumor) keyword expansion, improves performances of the clinical trials task. Nevertheless, we found that a particular combination (marked as ) performs well in both tasks ­ in particular the clinical trials task ­ and could have been one of the top 10 performing runs across many evaluation measures in both TREC PM 2017 and 2018. Therefore, this run can be considered as a baseline on which stronger multi-stage systems can be built.",1,TREC,True
226,ACKNOWLEDGMENTS,0,,False
227,"The authors thank Ellen Vorhees and Kirk Roberts for their helpful insights regarding the interpretation of the data collection. The work was partially supported by the ExaMode project,14 as part of the European Union H2020 research and innovation program under grant agreement no. 825292.",0,,False
228,REFERENCES,0,,False
229,"[1] M. Agosti, G.M. Di Nunzio, and S. Marchesin. 2018. The University of Padua IMS Research Group at TREC 2018 Precision Medicine Track. In Proc. of the Twenty-Seventh Text REtrieval Conference, TREC 2018, Gaithersburg, Maryland, USA, Nov. 14-16, 2018.",1,ad,True
230,"[2] A.R. Aronson. 2001. Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program. In Proc. of the AMIA Symposium. American Medical Informatics Association, 17­21.",0,,False
231,"[3] L. Diao, H. Yan, F. Li, S. Song, G. Lei, and F. Wang. 2018. The Research of Query Expansion Based on Medical Terms Reweighting in Medical Information Retrieval. EURASIP Jour. on Wireless Communications and Networking 2018, 1 (04 May 2018), 105. https://doi.org/10.1186/s13638-018-1124-3",1,Query,True
232,14 htttp://www.examode.eu/,0,,False
233,"[4] L. Goeuriot, G.J.F. Jones, L. Kelly, H. Müller, and J. Zobel. 2016. Medical Information Retrieval: Introduction to the Special Issue. Information Retrieval Journal 19, 1 (01 Apr 2016), 1­5. https://doi.org/10.1007/s10791-015-9277-8",0,,False
234,"[5] T.R. Goodwin, M.A. Skinner, and S.M. Harabagiu. 2017. UTD HLTRI at TREC 2017: Precision Medicine Track. In Proc. of the Twenty-Sixth Text REtrieval Conference, TREC 2017, Gaithersburg, Maryland, USA, Nov. 15-17, 2017.",1,TD,True
235,"[6] H. Gurulingappa, L. Toldo, C. Schepers, A. Bauer, and G. Megaro. 2016. SemiSupervised Information Retrieval System for Clinical Decision Support. In Proc. of the Twenty-Fifth Text REtrieval Conference, TREC 2016, Gaithersburg, Maryland, USA, Nov. 15-18, 2016.",1,TREC,True
236,"[7] W. Hersh. 2009. Information Retrieval: A Health and Biomedical Perspective. 2009 Springer Science + Business Media, LLC, New York, NY, USA.",0,,False
237,"[8] M. Oleynik, E. Faessler, A. Morassi Sasso, A. Kappattanavar, B. Bergner, H. Freitas",0,,False
238,"da Cruz, J.P. Sachs, S. Datta, and E. Böttinger. 2018. HPI-DHC at TREC 2018: Precision Medicine Track. In Proc. of the Twenty-Seventh Text REtrieval Conference, TREC 2018, Gaithersburg, Maryland, USA, Nov. 14-16, 2018. [9] K. Roberts, D. Demner-Fushman, E.M. Voorhees, W.R. Hersh, S. Bedrick, and A.J. Lazar. 2018. Overview of the TREC 2018 Precision Medicine Track. In Proc. of the Twenty-Seventh Text REtrieval Conference, TREC 2018, Gaithersburg, Maryland, USA, Nov. 14-16, 2018. [10] K. Roberts, D. Demner-Fushman, E.M. Voorhees, W.R. Hersh, S. Bedrick, A.J.",1,HP,True
239,"Lazar, and S. Pant. 2017. Overview of the TREC 2017 Precision Medicine Track. In Proc. of the Twenty-Sixth Text REtrieval Conference, TREC 2017, Gaithersburg, Maryland, USA, Nov. 15-17, 2017. [11] S. Robertson and H. Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333­389.",1,TREC,True
240,"[12] P. Sondhi, J. Sun, C. Zhai, R. Sorrentino, and M.S. Kohn. 2012. Leveraging Medical",0,,False
241,"Thesauri and Physician Feedback for Improving Medical Literature Retrieval for Case Queries. Jour. of the American Medical Informatics Association: JAMIA 19, 5 (Sep-Oct 2012), 851­858. https://doi.org/10.1136/amiajnl-2011-000293",0,,False
242,"[13] D. Zhu, S. Wu, B. Carterette, and H. Liu. 2014. Using Large Clinical Corpora for Query Expansion in Text-Based Cohort Identification. Jour. of Biomedical Informatics 49 (2014), 275 ­ 281. https://doi.org/10.1016/j.jbi.2014.03.010",1,Query,True
243,976,0,,False
244,,0,,False

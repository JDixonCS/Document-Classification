,sentence,label,data,regex
0,Short Research Papers 3B: Recommendation and Evaluation,0,,False
1,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
2,Help Me Search: Leveraging User-System Collaboration for Query Construction to Improve Accuracy for Difficult Queries,1,Query,True
3,Saar Kuzi,0,,False
4,skuzi2@illinois.edu University of Illinois at Urbana-Champaign,0,,False
5,Anusri Pampari,0,,False
6,anusri@stanford.edu Stanford University,0,,False
7,ABSTRACT,0,,False
8,"In this paper, we address the problem of difficult queries by using a novel strategy of collaborative query construction where the search engine would actively engage users in an iterative process to continuously revise a query. This approach can be implemented in any search engine to provide search support for users via a ""Help Me Search"" button, which a user can click on as needed. We focus on studying a specific collaboration strategy where the search engine and the user work together to iteratively expand a query. We propose a possible implementation for this strategy in which the system generates candidate terms by utilizing the history of interactions of the user with the system. Evaluation using a simulated user study shows the great promise of the proposed approach. We also perform a case study with three real users which further illustrates the potential effectiveness of the approach.",1,ad,True
9,"ACM Reference format: Saar Kuzi, Abhishek Narwekar, Anusri Pampari, and ChengXiang Zhai. 2019. Help Me Search: Leveraging User-System Collaboration for Query Construction to Improve Accuracy for Difficult Queries. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, Paris, France, July 21­25, 2019 (SIGIR '19), 4 pages. https://doi.org/10.1145/3331184.3331362",1,Query,True
10,1 INTRODUCTION,1,DUC,True
11,"The current search engines generally work well for popular queries where a large amount of click-through information can be leveraged. Such a strategy may fail for long-tail queries, which are entered by only a small number of users. Thus, for such queries, a search engine generally would have to rely mainly on matching the keywords in the query with those in documents. Unfortunately, such a method would not work well when the user's query does not include the ""right"" keywords. Users in such cases would often end up repeatedly reformulating a query, yet they still could not find the relevant",0,,False
12,This work was done while the author was a student at UIUC.,0,,False
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331362",1,ad,True
14,Abhishek Narwekar,0,,False
15,narweka@amazon.com Amazon Alexa,0,,False
16,ChengXiang Zhai,0,,False
17,czhai@illinois.edu University of Illinois at Urbana-Champaign,0,,False
18,"documents. Unfortunately, there are many such queries, making it a pressing challenge for search engines to improve their accuracy.",0,,False
19,"In this paper, we address this problem and propose a general strategy of collaborative query construction where the search engine would actively engage users in an iterative process to revise a query. The proposed strategy attempts to optimize the collaboration between the user and the search engine and is based on the following assumptions: (1) Ideal query: For any difficult query, there exists an ideal query that, if constructed, would work well. This assumption allows us to re-frame the problem of how to help users as the problem of how to construct an ideal query. (2) Usersystem collaboration: User-system collaboration can be optimized by leveraging the strength of a search engine in ""knowing"" all the content in the collection and the strength of a user in recognizing a useful modification for the query among a set of candidates. (3) User effort: When facing a difficult query, the user would be willing to make some extra effort to collaborate with the search engine.",1,ad,True
20,"Our main idea is to optimize the user-system collaboration in order to perform a sequence of modifications to the query with the goal of reaching an ideal query. While the proposed strategy includes multiple ways to edit the query, we initially focus on studying a specific editing operator where the system suggests terms to the user to be added to the query at each step based on the history of interactions of the user with the system.",1,ad,True
21,"We perform an evaluation with a simulated user which demonstrates the great promise of this novel collaborative search support strategy for improving the accuracy of difficult queries with minimum effort from the user. The results also show that suggesting terms based on user interaction history improves effectiveness without incurring additional user effort. Finally, we conduct a case study with three real users that demonstrates the potential effectiveness of our approach when real users are involved.",1,ad,True
22,2 RELATED WORK,0,,False
23,"The main novelty of our work is the idea of collaborative construction of an ideal query, specific algorithms for iterative query expansion, and the study of their effectiveness for difficult queries.",0,,False
24,"Previous works have studied approaches for interactive query expansion (e.g., [2, 4, 10]). According to these works, the user needs to select terms to be added to each query independently. Our framework is more general both in performing a sequence of query modifications to optimize the user-system collaboration and in allowing potentially other query modifications than simply adding terms.",1,ad,True
25,1221,0,,False
26,Short Research Papers 3B: Recommendation and Evaluation,0,,False
27,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
28,"Furthermore, we propose methods which suggest terms to the user based on the history of interactions of the user with the system.",0,,False
29,"On the surface, our approach is similar to query suggestion already studied in previous works [3]. However, there are two important differences: (1) The suggested queries in our approach are expected to form a sequence of queries incrementally converging to an ideal query whereas query suggestion is done for each query independently. (2) The suggested queries in our method are composed of new terms extracted from the text collection, but the current methods for query suggestion tend to be able to only suggest queries taken from a search log.",1,ad,True
30,"Other works focused on developing query suggestion approaches for difficult queries [6, 11]. In general, ideas from past works on query suggestion can be used in our approach for generating the set of query modifications that are suggested to the user.",0,,False
31,"There is a large body of work on devising approaches for automatic query reformulation. One common method is to automatically add terms to the user's query [5]. Other approaches include, for example, substitution or deletion of terms [12]. The various ideas, which are suggested by works in this direction, can be integrated into our collaborative approach by devising sophisticated methods for term suggestion.",1,ad,True
32,3 COLLABORATIVE QUERY CONSTRUCTION,0,,False
33,"Our suggested Collaborative Query Construction (CQC) approach is based on the Ideal Query Hypothesis (IQH), which states that for any information need of a user, there exists an ideal query that would allow a retrieval system to rank all the relevant documents above the non-relevant ones. The IQH implies that if a user has perfect knowledge about the document collection, then the user would be able to formulate an ideal query. The IQH is reasonable because it is generally possible to uniquely identify a document by just using a few terms that occur together in it but not in others. This point was also referred to in previous work as the perfect query paradox [8]. We note that the IQH may not always hold; for example, when there are duplicate documents. Nevertheless, it provides a sound conceptual basis for designing algorithms for supporting users in interactive search. Based on the IQH, the problem of optimizing retrieval accuracy can be reduced to the problem of finding the ideal query. Thus, based on this formulation, the main reason why a search task is difficult is that the user does not have enough knowledge to formulate the ideal query. In this paper, we address this problem by helping a user to construct an ideal query.",1,Query,True
34,"Our collaborative query construction process is represented by a sequence of queries, Q1, Q2, ..., Qn , where Q1 is the user's initial query, Qn is an ideal query, and Qi+1 is closer to Qn than Qi and the gap between Qi and Qi+1 is small enough for the user to recognize the improvement of Qi+1 over Qi . From the system's perspective, at any point of this process, the task is to suggest a set of candidate queries, while the user's task is to choose one of them. In this paper, we focus on a specific approach in which the query refinement is restricted to only adding one extra term to the query at each step. That is, a single collaborative iteration of revising a query Qi would be as follows: (1) Present the user a list of m candidate terms, Ti (not already selected). (2) The user selects a term, t  Ti . (3) Qi+1 , Qi {t }. (4) Qi+1 is used to retrieve a result list Di+1.",1,ad,True
35,One advantage of using such an approach is that the gap be-,1,ad,True
36,tween two adjacent queries is expected to be small enough for the,1,ad,True
37,"user to recognize the correct choice. Furthermore, although this",0,,False
38,"implementation strategy is very simple, theoretically speaking, the",0,,False
39,process can guarantee the construction of any ideal query that,0,,False
40,contains all the original query terms if the system can suggest ad-,1,ad,True
41,ditional terms in the ideal query but not in the original query and,0,,False
42,the user can recognize the terms to be included in the ideal query.,0,,False
43,"We assume that the original query terms are all ""essential"" and",0,,False
44,"should all be included in the ideal query. While true in general, in",0,,False
45,"some cases this assumption may not hold, which would require the",0,,False
46,"removal or substitution of terms in the initial query. In this paper,",0,,False
47,"however, we focus on term addition as our first strategy and leave",1,ad,True
48,the incorporation of other operations for future work.,1,corpora,True
49,"Following the game-theoretic framework for interactive IR [13],",0,,False
50,our approach can be framed as the following Bayesian decision,0,,False
51,problem where the goal is to decide a candidate set of terms Ti to,0,,False
52,suggest to the user in response to the current query Qi :,0,,False
53,Ti,0,,False
54,",",0,,False
55,arg min,0,,False
56,T V -Qi,0,,False
57,"L(T , Hi , Q , U )p(Q |Hi , U )dQ ;",0,,False
58,Q,0,,False
59,(1),0,,False
60,where (1) Ti is a candidate set of terms to be presented to the user (a subset of the vocabulary V ). (2) Hi is all the information from the history of interactions of the user with the system. (3) Q is a unigram language model representing a potential ideal query. (4) U,0,,False
61,"denotes any relevant information about the user. (5) L(T , Hi , Q , U ) is a loss function assessing whether T is a good choice for Hi , U , and Q . (6) p(Q |Hi , U ) encodes the current belief about the ideal",0,,False
62,"query. The integral indicates the uncertainty about the ideal query,",0,,False
63,which can be expected to be reduced as we collect more information,0,,False
64,from the user.,0,,False
65,While in general we need to assess the loss of an entire can-,0,,False
66,"didate set T , in the much simplified method that we will actually",0,,False
67,"explore, we choose T by scoring each term and then applying a",0,,False
68,"threshold to control the number of terms. That is, we assume that",0,,False
69,the loss function on a term set T can be written as an aggregation,0,,False
70,of the loss on each individual term. As an additional simplifica-,1,ad,True
71,"tion, we approximate the integral with the mode of the posterior probability about the ideal query, ^ Q . Thus, our decision problem would become to compute the score of each term t, not already selected by the user, as follows: s(t) ,"" -L(t, Hi , ^ Q , U ); where ^ Q "","" arg maxQ p(Q |Hi , U ). Computationally, the algorithm boils down to the following two steps: (1) Given all of the observed information Hi and U , compute ^ Q . (2) Use ^ Q along with Hi and U to score each term in the vocabulary but not already in Qi .""",1,ad,True
72,4 TERM SCORING,0,,False
73,"According to the previous section, the optimal scoring function s(t) is based on the negative loss -L(t, Hi , ^ Q , U ). Intuitively, the loss of word t is negatively correlated with its probability according to ^ Q . We thus simply define our scoring function as s(t) ,"" p(t |^ Q ). That is, our problem is now reduced to infer ^ Q given all of the observed information Hi and U .""",0,,False
74,"Next, we suggest a model for inferring ^ Q , which is based on Pseudo-Relevance Feedback (PRF). This model is an extension of",0,,False
75,the relevance model RM1 [7] to incorporate Hi (We leave the incorporation of U for future work as such data is not available to us.):,1,corpora,True
76,1222,0,,False
77,Short Research Papers 3B: Recommendation and Evaluation,0,,False
78,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
79,"p(t |^ Q ) ,",0,,False
80,"p(t |d) · p(d |Q1, Hi ).",0,,False
81,(2),0,,False
82,d Di,0,,False
83,p(t |d) is estimated using the maximum likelihood approach. We,0,,False
84,"approximate p(d |Q1, Hi ) using a linear interpolation:",0,,False
85,"p(d |Q1, Hi ) , (1 -  ) · p(d |Q1) +  · p(d |Hi );",0,,False
86,(3),0,,False
87,"p(d |Q1) is proportional to the reciprocal rank of d w.r.t Q1;   [0, 1].",0,,False
88,"In order to estimate p(d |Hi ), two types of historical information are considered: (1) The terms selected by the user previously (HiT ). (2) The result lists presented to the user previously (HiD ). We combine these two components as follows: p(d |Hi ) , p(d |HiD ) · p(HiD |Hi ) + p(d |HiT ) · p(HiT |Hi ). (We assume p(HiD |Hi ) , p(HiT |Hi ).)",0,,False
89,"In order to estimate p(d |HiD ), we assume that documents which appear in the result list presented to the user in the current iteration,",0,,False
90,"and that were absent in the previous result list, represent aspects",0,,False
91,of the information need that are more important to the user. We,0,,False
92,thus,0,,False
93,"estimate p(d |HiD ) p(d |HiD ) ,",0,,False
94,as follows: 1,0,,False
95,rankDi (d) ·,0,,False
96,Z,0,,False
97,D,0,,False
98,d,0,,False
99,Di,0,,False
100,\ Di-1;,0,,False
101,(4),0,,False
102,"p(d |HiD ) , 0 for all other documents; rankDi (d) is the rank of document d in the result list Di ; ZD is a normalization factor.",0,,False
103,We estimate p(d |HiT ) such that high importance is attributed to documents in which terms that were previously selected by the,0,,False
104,user are prevalent.,0,,False
105,i -1,0,,False
106,"p(d |HiT ) ,"" p(d |tj , HiT ) · p(tj |HiT );""",0,,False
107,(5),0,,False
108,"j ,1",0,,False
109,"tj is the term selected by the user in the j'th iteration. p(d |tj , HiT ) is set to be proportional to the score of d with respect to tj as calculated by the system's ranking method. Assuming that terms",0,,False
110,selected in more recent iterations are more important than older,0,,False
111,"ones, we estimate p(tj |HiT ) as: p(tj |HiT )",0,,False
112,",",0,,False
113,exp(-µ ·(i ZT,0,,False
114,-j,0,,False
115,)),0,,False
116,;,0,,False
117,ZT,0,,False
118,is a,0,,False
119,normalization factor; µ is a free parameter and is set to 0.5.,0,,False
120,"To conclude, we assign a probability to each term which is a lin-",0,,False
121,ear interpolation of its probabilities in the documents in the result,0,,False
122,"list, where the interpolation weights are influenced by: (1) the rank",0,,False
123,"of the document, (2) the presence of the document in the previous",0,,False
124,"list, and (3) the frequency of terms that were previously selected.",0,,False
125,"Query representation: According to our approach, the query Qi is composed of the original query Q1 and the terms selected by the user. The terms in Qi are weighted based on a probability distribution such that the probability of a term t in V is: p(t |Qi ) , i ·pmle (t |Q1)+(1-i )·p(t |H ); p(t |H ) is proportional to the weight that was assigned to the term by the scoring method if this term",1,Query,True
126,"was previously selected, and is set to 0 otherwise; pmle (t |Q1) is the maximum likelihood estimate of t in Q1; i  [0, 1].",0,,False
127,5 EVALUATION,0,,False
128,"The evaluation of the proposed strategy has two challenges: (1) The proposed approach is of interactive nature. (2) We are interested in focusing on difficult queries. We address these challenges by constructing a new test collection based on an existing collection that would focus on difficult queries and experimenting with simulated users. Finally, we perform a case study with three real users. Experimental setup: We use the ROBUST document collection",1,ad,True
129,Table 1: Simulated user performance. Statistically significant differences with RM3 are marked with asterisk. All differences with the initial query are statistically significant.,0,,False
130,Initial RM3 CQC,0,,False
131,p @5,0,,False
132,.000 .036 .057,0,,False
133,Single Term,0,,False
134,p@10 M RR success@10,0,,False
135,.000 .053,0,,False
136,.000,0,,False
137,.040 .083,0,,False
138,.238,0,,False
139,.090 .127,0,,False
140,.457,0,,False
141,p @5,0,,False
142,.000 .040 .137,0,,False
143,Five Terms,0,,False
144,p@10 M RR success@10,0,,False
145,.000 .053,0,,False
146,.000,0,,False
147,.049 .090,0,,False
148,.219,0,,False
149,.136 .209,0,,False
150,.447,0,,False
151,"(TREC discs 4 and 5-{CR}). The collection is composed of 528,155",1,TREC,True
152,"newswire documents, along with 249 TREC topics which their titles",1,TREC,True
153,"serve as queries (301-450, 601-700). Stopword removal and Krovetz",0,,False
154,stemming were applied to both documents and queries. The Lucene,0,,False
155,toolkit was used for experiments (lucene.apache.org). The BM25,0,,False
156,model was used for ranking [9]. We use the following strategy to,0,,False
157,construct our test set. We first perform retrieval for all queries.,0,,False
158,"Then, we remove from the collection the relevant documents that",0,,False
159,are among the top 10 documents in each result list. After doing,0,,False
160,"that, we remain with 105 queries for which p@10 , 0 when per-",0,,False
161,forming retrieval over the modified collection. We use these queries,0,,False
162,"for our evaluation, along with the modified collection. We report",0,,False
163,"performance in terms of precision (p@ {5, 10}) and Mean Recip-",0,,False
164,"rocal Rank (MRR@1000), which is more meaningful than Mean",0,,False
165,Average Precision in the case of such difficult queries (it measures,0,,False
166,how much effort a user has to make in order to reach the very,0,,False
167,first relevant document). We also report the fraction of queries for,0,,False
168,"which a method resulted in p@10 > 0, denoted success@10. The",0,,False
169,two-tailed paired t-test at 95% confidence level is used in order to,0,,False
170,determine significant differences in performance.,0,,False
171,"Our approach involves free parameters, which are set to effective",0,,False
172,ones following some preliminary experiments. We should point,0,,False
173,out that our research questions are mainly about how promising,0,,False
174,"the proposed approach is as a novel interaction strategy, which is",0,,False
175,generally orthogonal to the optimization of these parameters. The,0,,False
176,"number of terms suggested to the user, m, is set to 5. The number",0,,False
177,of documents used in our PRF-based term scoring method is set,0,,False
178,"to 100. The interpolation parameter in Equation 3, , is set to 0.8.",0,,False
179,"The value of i , the weight given to the original query, is set to",0,,False
180,"max(0.4,",0,,False
181,|Q1 |Qi,0,,False
182,| |,0,,False
183,);,0,,False
184,we,0,,False
185,chose,0,,False
186,this,0,,False
187,weighting,0,,False
188,function,0,,False
189,as,0,,False
190,to,0,,False
191,attribute,0,,False
192,high importance to the original query when a small amount of,0,,False
193,expansion is used. We compare the performance of our approach,0,,False
194,"with that of using the original query, and of using an automatic",0,,False
195,query expansion approach in which a set of terms is automatically,0,,False
196,added to the original query once. We set the number of expansion,1,ad,True
197,terms to be equal to the number of terms that were added by the,1,ad,True
198,user in the collaborative process. We use the RM3 [1] expansion,0,,False
199,model (free parameters are set as in the collaborative approach).,0,,False
200,Simulation study: In order to do a controlled study of our ap-,0,,False
201,"proach, we experiment with a simulated user. Given a list of term",0,,False
202,"suggestions, the simulated user chooses a term with the highest",0,,False
203,"t f .id f score in the relevant documents. Specifically, for each query",0,,False
204,we concatenate all relevant documents and compute t f .id f based,0,,False
205,"on the single concatenated ""relevant document"". Our main result for",0,,False
206,the simulated user experiment is presented in Table 1. We report the,0,,False
207,performance when a single term or five terms are added. According,1,ad,True
208,"to the results, the collaborative approach (CQC) is very effective.",0,,False
209,"Specifically, after adding a single term to the query, users are able to",1,ad,True
210,1223,0,,False
211,Short Research Papers 3B: Recommendation and Evaluation,0,,False
212,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
213,p@10 success@10,0,,False
214,p@10 success@10,0,,False
215,CQC-Q,0,,False
216,CQC-H,0,,False
217,0.13,0,,False
218,CQC,0,,False
219,CQC-Q,0,,False
220,CQC-H,0,,False
221,0.52,0,,False
222,CQC,0,,False
223,0.50 0.12,0,,False
224,0.48 0.11,0,,False
225,0.46 0.10,0,,False
226,0.44,0,,False
227,0.09,0,,False
228,1,0,,False
229,2,0,,False
230,3,0,,False
231,4,0,,False
232,5,0,,False
233,Additional Terms,0,,False
234,1,0,,False
235,2,0,,False
236,3,0,,False
237,4,0,,False
238,5,0,,False
239,Additional Terms,0,,False
240,Figure 1: Performance of the different model components.,0,,False
241,"see a noticeable improvement on the first page of search results in about 45% of these difficult queries that did not return any relevant document initially (success@10). Furthermore, our approach outperforms the initial query to a statistically significant degree for all evaluation measures (for both number of terms) and is also much more effective than RM3. Our term scoring method utilizes both the original query and the user interaction history. We are interested in examining the relative importance of these individual components. Setting  , 0 in Equation 3 results in a model that uses only the original query (CQC-Q). Setting  ,"" 1 results in a model that uses only user history (CQC-H). The results are presented in Figure 1. Focusing on p@10, we can see that all components are very effective. Comparing the different components, we can see that CQC-H is outperformed by CQC-Q for a small number of terms, and the opposite holds for a large number. In terms of success@10, we can see that all model components achieve the highest performance when two terms are added, with CQC-H being the best performing one. Interestingly, success@10 decreases as more terms are added. That is, while adding more terms to the query can improve the average performance, it results in a less robust approach. Case study with real users: We are interested in examining whether real users can recognize the """"good"""" terms suggested by the system. To gain some initial understanding regarding this issue, we conducted a case study with three real users. We note that the conclusions that can be drawn from this study are limited due the small number of users. Yet, this study is still useful for getting some intuition regarding the utility of the approach. Each participant performed three iterations of the collaborative process for 30 queries. Specifically, we selected queries that achieved the highest performance in terms of p@10 after adding a single term by the simulated user. We chose these queries as we are interested to study the following research question: given a term scoring method that can provide effective terms, can the user identify them? For each query, the user was presented with the initial query, a text describing the topic, and the guidelines regarding how a relevant document should look like (all are part of the TREC topics). After issuing a query, the users are presented with a result list of 10 documents (a title and a short summary of 5 sentences are presented).""",1,ad,True
242,"In Figure 2, we compare the performance of the real users with that of the simulated user. According to the results, retrieval performance can be very good when terms are selected by real users. Specifically, all users reach success@10 of around 0.5. That is, after adding a single term, at least one relevant result is obtained for about 50% of the queries. In Table 2, we present examples of queries along with the terms that were selected by a single real user and a simulated user. We also report the performance that resulted from adding a term. The first query serves as an example where the real user outperforms the simulated user by a better choice of terms. The second query is an example where the simulated user",1,ad,True
243,0.35,0,,False
244,Simulated User,0,,False
245,Real User (1),0,,False
246,0.30,0,,False
247,Real User (2),0,,False
248,0.8,0,,False
249,Real User (3),0,,False
250,0.25,0,,False
251,0.6 0.20,0,,False
252,0.15,0,,False
253,0.4,0,,False
254,0.10,0,,False
255,0.2,0,,False
256,Simulated User,0,,False
257,Real User (1) 0.05,0,,False
258,Real User (2),0,,False
259,Real User (3),0,,False
260,0.00,0,,False
261,0.0,0,,False
262,0,0,,False
263,1,0,,False
264,2,0,,False
265,3,0,,False
266,0,0,,False
267,1,0,,False
268,2,0,,False
269,3,0,,False
270,Additional Terms,0,,False
271,Additional Terms,0,,False
272,Figure 2: Real users vs. Simulated user. Table 2: Query Examples. The performance of the query,1,Query,True
273,(p@10) after adding a term is reported in the brackets.,1,ad,True
274,curbing population growth Stirling engine antibiotics ineffectiveness,0,,False
275,Real User Simulated Real User Simulated Real User Simulated,0,,False
276,plan (0.0) china (0.1) company (0.0),0,,False
277,cfc (0.9) infection (0.2),0,,False
278,drug (0.1),0,,False
279,family (0.2) economic (0.1) financial (0.0),0,,False
280,hcfc (1.0) research (0.2) pharmaceutical (0.2),0,,False
281,birth (0.6) rate (0.2) group (0.0) hyph (1.0) study (0.2) product (0.1),0,,False
282,"outperforms the real user presumably by recognizing the correct technical terms. Finally, the last query is an example where both users achieve similar performance, but using different terms.",0,,False
283,6 CONCLUSIONS AND FUTURE WORK,0,,False
284,"We proposed and studied a novel strategy for improving the accuracy of difficult queries by having the search engine and the user collaboratively expand the original query. Evaluation with simulated users and a case study with real users show the great promise of this strategy. In future work, we plan to devise more methods for term scoring, incorporate more operations for query modification, and perform a large-scale user study. Acknowledgments. This material is based upon work supported by the National Science Foundation under grant number 1801652.",1,corpora,True
285,REFERENCES,0,,False
286,"[1] Nasreen Abdul-Jaleel, James Allan, W Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D Smucker, and Courtney Wade. [n. d.]. UMass at TREC 2004: Novelty and HARD. Technical Report.",1,ad,True
287,"[2] Peter Anick. 2003. Using terminological feedback for web search refinement: a log-based study. In Proceedings of SIGIR. ACM, 88­95.",0,,False
288,"[3] Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Mendoza. 2004. Query recommendation using query logs in search engines. In International Conference on Extending Database Technology. Springer, 588­596.",1,ad,True
289,"[4] Nicholas J. Belkin, Colleen Cool, Diane Kelly, S-J Lin, SY Park, J Perez-Carballo, and C Sikora. 2001. Iterative exploration, design and evaluation of support for query reformulation in interactive information retrieval. Information Processing & Management 37, 3 (2001), 403­434.",0,,False
290,[5] Claudio Carpineto and Giovanni Romano. 2012. A Survey of Automatic Query Expansion in Information Retrieval. ACM Comput. Surv. (2012).,1,Query,True
291,"[6] Van Dang and Bruce W Croft. 2010. Query reformulation using anchor text. In Proceedings of WSDM. ACM, 41­50.",1,Query,True
292,"[7] Victor Lavrenko and W Bruce Croft. 2001. Relevance based language models. In Proceedings of SIGIR. ACM, 120­127.",0,,False
293,[8] David Dolan Lewis. 1992. Representation and learning in information retrieval. Ph.D. Dissertation. University of Massachusetts at Amherst.,0,,False
294,"[9] Stephen E Robertson and Steve Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of SIGIR. Springer-Verlag New York, Inc., 232­241.",0,,False
295,"[10] Ian Ruthven. 2003. Re-examining the potential effectiveness of interactive query expansion. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval. ACM, 213­220.",0,,False
296,"[11] Yang Song and Li-wei He. 2010. Optimal rare query suggestion with implicit user feedback. In Proceedings of WWW. ACM, 901­910.",0,,False
297,"[12] Xuanhui Wang and ChengXiang Zhai. 2008. Mining term association patterns from search logs for effective query reformulation. In Proceedings of CIKM. ACM, 479­488.",0,,False
298,"[13] ChengXiang Zhai. 2016. Towards a game-theoretic framework for text data retrieval. IEEE Data Eng. Bull. 39, 3 (2016), 51­62.",0,,False
299,1224,0,,False
300,,0,,False

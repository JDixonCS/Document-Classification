,sentence,label,data,regex
0,Session 4B: Queries,1,Session,True
1,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
2,"Information Needs, Queries, and Query Performance Prediction",1,Query,True
3,Oleg Zendel,0,,False
4,olegzendel@campus.technion.ac.il Technion,0,,False
5,Anna Shtok,0,,False
6,annie.shtok@gmail.com,0,,False
7,Fiana Rabier,0,,False
8,fiana@verizonmedia.com Yahoo Research,1,Yahoo,True
9,Oren Kurland,0,,False
10,kurland@ie.technion.ac.il Technion,0,,False
11,J. Shane Culpepper,0,,False
12,shane.culpepper@rmit.edu.au RMIT University,0,,False
13,ABSTRACT,0,,False
14,"The query performance prediction (QPP) task is to estimate the effectiveness of a search performed in response to a query with no relevance judgments. Existing QPP methods do not account for the effectiveness of a query in representing the underlying information need. We demonstrate the far reaching implications of this reality using standard TREC-based evaluation of QPP methods: their relative prediction quality patterns vary with respect to the effectiveness of queries used to represent the information needs. Motivated by our findings, we revise the basic probabilistic formulation of the QPP task by accounting for the information need and its connection to the query. We further explore this connection by proposing a novel QPP approach that utilizes information about a set of queries representing the same information need. Predictors instantiated from our approach using a wide variety of existing QPP methods post prediction quality that substantially transcends that of applying these methods, as is standard, using a single query representing the information need. Additional in-depth empirical analysis of different aspects of our approach further attests to the crucial role of query effectiveness in QPP.",1,TREC,True
15,"ACM Reference Format: Oleg Zendel, Anna Shtok, Fiana Rabier, Oren Kurland, and J. Shane Culpepper. 2019. Information Needs, Queries, and Query Performance Prediction. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21­25, 2019, Paris, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184. 3331253",1,Query,True
16,1 INTRODUCTION,1,DUC,True
17,"Ad hoc (query-based) retrieval effectiveness can significantly vary across queries for a variety of retrieval methods [10]. This fact enabled a large body of work on query performance prediction (QPP) [10] whereby the goal is to estimate search effectiveness in the absence of human relevance judgments. There are two common approaches to this problem. Pre-retrieval predictors analyze the query and the corpus prior to retrieval time [14, 22, 23, 29, 38, 39, 48, 59]; e.g., queries containing terms with high IDF (inverse document",1,hoc,True
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/3331184.3331253",1,ad,True
19,"frequency) values should presumably be more effective [14, 23]. Post-retrieval predictors use information induced from the result list of top-retrieved documents [1, 2, 5, 9, 11, 14­18, 20, 22, 25, 30, 31, 33­ 37, 41­44, 49, 51, 56, 57, 60, 61]. For example, some post-retrieval predictors are based on analyzing the retrieval scores of documents in the result list (e.g., [17, 35, 36, 41, 49, 61]).",0,,False
20,"A careful examination of past work in this area reveals that most current approaches rely on predicting the performance across a set of queries, where each query represents a unique information need; often on a single corpus using a single retrieval method. This is largely an artifact of the test collections commonly used to evaluate new techniques. For example, the TREC [52] initiative has historically defined an information need as a topic using a title, description, and narrative. The title is commonly viewed as a keyword query that a user might issue to a search engine, and the description and narrative more clearly define the user's real information need. While QPP methods have been shown to be effective in this setting, many (specifically, pre-retrieval methods) were found to be ineffective in predicting the relative effectiveness of different queries that represent the same information need (a.k.a., query variations or reformulations) [48]1. Consequently, Thomas et al. [48] postulated that QPP methods essentially predict informationneed performance rather than query performance. Indeed, current QPP approaches do not explicitly account for the information need, nor for the extent to which a query effectively represents it for retrieval2. In fact, this is also the case for the formal fundamental probabilistic basis of most prediction methods [31, 43], where coupling an information need with the query used to represent it does not allow to account for the effectiveness of the query in representing the need for retrieval.",1,TREC,True
21,"We show that the implications of this coupling are more farreaching. Specifically, using the UQV datasets [3, 8], where humangenerated query variations are available for TREC topics, we show that the relative prediction quality patterns of various QPP methods, when evaluated in the standard setting of different queries representing different information needs, vary with respect to the effectiveness of queries used to represent the underlying need.",1,UQV,True
22,"Motivated by our empirical findings that attest to the importance of modeling the information need and its connection with queries used to represent it, we revise the probabilistic formalism of the QPP task [43] which is the basis for most QPP methods. Our formalism",0,,False
23,1Pre-retrieval methods are also not effective in predicting the performance of different document lists retrieved for the same query using different retrieval methods [31]. Some work [50] demonstrated the limited merit of using pre-retrieval methods to select between personalized and original query variants. 2The main exception we are aware of is the work of Sondak et al. [44].,0,,False
24,395,0,,False
25,"Session 4B: Queries SIGIR '19, July 21­25, 2019, Paris, France",1,Session,True
26,"SIGIR '19, July 21­25, 2019, Paris, France O. Zendel et al.",0,,False
27,"reflects a transition from addressing the basic question of ""What is the probability that this retrieved list is relevant (effective) for this query?"" [43] to addressing the question of ""What is the probability that this retrieved list is relevant (effective) for a (latent) information need that is represented by this query?"".",1,ad,True
28,"We use our revised probabilistic formalism to address a novel prediction challenge: query performance prediction using reference queries. That is, how can we estimate retrieval effectiveness for a given query using knowledge derived from additional (reference) queries that represent the same information need. Our proposed approach, which can be instantiated using any existing queryperformance predictor, accounts for the association between the query and the reference queries and the predicted performance for the latter. Extensive empirical evaluation attests to the clear merits of our approach. Specifically, the prediction quality is substantially better than that of using existing predictors which do not utilize reference queries.",1,ad,True
29,"Summary of Contributions. In this work, we show that the effectiveness of a query in representing the underlying information need has a significant impact on QPP quality. We then define a probabilistic QPP framework which encapsulates the fundamental relationship between queries and an information need, and show how it can be easily instantiated using a variety of QPP approaches to improve prediction quality. An extensive empirical evaluation shows that combining information from multiple reference queries dramatically improves prediction quality for queries regardless of their effectiveness in representing the information need, and independent of the QPP method used to instantiate our approach.",0,,False
30,2 RELATED WORK,0,,False
31,"The prediction framework presented by Carmel et al. [11] for estimating topic/information need difficulty accounts for the fact that the information need can be represented by different queries. However, in contrast to our work, the model instantiated from the framework predicts performance for a single query without using information from alternative queries that represent the same information need, and without accounting for how well the query represents the information need.",0,,False
32,"Previous work exists on predicting performance for a single query by estimating the presumed extent to which it represents the underlying information need [44]. This predictor was also used by Roitman et al. [36], where the estimate is based on properties of a pseudo-feedback-based relevance model induced from the retrieved list [27]. There has also been work on ranking query reformulations [19, 37, 54] by estimating the extent to which each reformulation represents the information need. The task we pursue is different from these previous lines of work: we predict performance for a query using information from other queries representing the same information need. The estimator from Sondak et al. [44] is used as a baseline reference comparison in this work.",0,,False
33,Other relevant recent work on using reference document lists for QPP was proposed by Shtok et al. [43]. The reference lists are retrieved by using different retrieval methods for a set of queries. Performance is then predicted based on the similarity between the retrieved list for a query and its corresponding reference lists. Our prediction model uses reference queries and assumes that the,0,,False
34,"retrieval method is fixed. Some of the estimates derived in this work result in predictors that utilize reference document lists retrieved for different queries; hence, these specific predictors are similar in spirit to those proposed by Shtok et al. [43]. However, we utilize estimates (predictors) for the retrieval effectiveness of the reference queries, while the (novel) models devised and evaluated in [43] do not account for the effectiveness of the reference lists. Furthermore, the work of Shtok et al. [43] couples the query with the information need as in prior work on QPP, while our focus in this work is on explicitly de-coupling the two.",0,,False
35,"Query Variations. There is a long history of research that explores the relationship between a single query and the information needs it represents. Early work exploring the impact of query representation and effectiveness was presented by Belkin et al. [6]. Shortly thereafter, Belkin et al. [7] operationalized these insights to significantly improve retrieval effectiveness by fusing the results from multiple query variations for a single information need.",1,Query,True
36,"A more recent seminal study of query reformulation in user search sessions in a production search engine also explored the importance of subtle query variations on the overall effectiveness of search results returned to the user [24]. Subsequent studies have shown that query variations can be used in learning-to-rank models to significantly boost query performance [19, 40]. Sheldon et al. [40] and other recent studies [55] have shown that highly-effective query variations, rewrites, and suggestions can be generated offline using random walks over a bipartite graph constructed over click data extracted from large search engine query logs [13]. In the absence of access to commercial search engine logs, effective manual query variations can also be generated using crowdsourcing [3, 4, 8]. We use the UQV query collections [3, 8] as the query variations are publicly available, and hence the results are reproducible.",1,UQV,True
37,"Using human query variations, Thomas et al. [48] showed empirically that existing pre-retrieval predictors essentially predict information-need difficulty rather than query difficulty. Their work inspires this work, where we attempt to de-couple information needs and queries in modeling the QPP task. We strengthen their findings by showing that the relative prediction quality patterns of various query-performance predictors (both pre- and post- retrieval ones) vary with respect to the effectiveness of queries used to represent the information needs. In contrast to our work, a model of the QPP task that accounts for the information need was not presented and the challenge of predicting performance using reference queries was not addressed. That is, Thomas et al. [48] show that it can be very hard to make a prediction that distinguishes the performance of queries representing a single information need, while we show that using multiple representations for an information need can significantly improve the quality of prediction for the performance of a single query for an information need.",1,ad,True
38,"Supervised Query Performance Prediction. It is also possible to improve prediction quality using supervised [12, 21, 26, 31, 33­ 35, 56] or weakly supervised [57] techniques. In general, these approaches, and others [42, 51, 61], integrate various unsupervised query performance prediction methods. In this work, we do not explore the use of query variations in supervised QPP techniques. Rather, we demonstrate the merits of our approach with many unsupervised pre- and post-retrieval predictors that are commonly the",1,Query,True
39,396,0,,False
40,"Session 4B: Queries Information Needs, Queries, and Query Performance Prediction",1,Session,True
41,a,0,,False
42,QC,0,,False
43,M,0,,False
44,I,0,,False
45,b,0,,False
46,Q CM,0,,False
47,R,0,,False
48,L,0,,False
49,R,0,,False
50,L,0,,False
51,"Figure 1: Graphical model representation of (a) the probabilistic model underlying existing query performance prediction methods, and (b) the extended model that accounts for the information need.",0,,False
52,basis for these supervised approaches. Extending our new framework to a supervised learning framework is an interesting problem that is orthogonal to our own.,0,,False
53,3 PREDICTION FRAMEWORK,0,,False
54,"The query performance prediction (QPP) task is to estimate the effectiveness of a search performed in response to a query in the absence of human relevance judgments [10]. As noted by Raiber and Kurland [31], the task can be framed as estimating the probability:",0,,False
55,"p(R ,"" 1|Q, C, M),""",0,,False
56,(1),0,,False
57,"where R, Q, C and M are random variables that take as values relevance status (1 stands for relevant), queries, corpora and retrieval methods, respectively. That is, the task is to estimate the probability for a relevance event, discussed below, given the ranked retrieval of a query over a corpus.",1,corpora,True
58,"Previous work on QPP is based, in spirit, on fixing the values of C and M, and estimating Equation 1 [31]. That is, prediction is performed over different queries used for retrieval with the same retrieval method on the same corpus. Since the choice of a query, retrieval method and corpus entails a retrieved list, the QPP task pursued in past work can be framed, as recently suggested by Shtok et al. [43], as estimating:",0,,False
59,"p(R ,"" 1|Q, L),""",0,,False
60,(2),0,,False
61,where L is a random variable that takes retrieved lists as values. Figure 1 (a) depicts the graphical model of dependence relations between the random variables used in Equations 1 and 2.,0,,False
62,"Estimating Equation 2 for a specific query and retrieved list amounts to addressing the question [43]:""What is the probability that this retrieved list is relevant to this query?"". This question is an extension, from a single document to a ranked document list, of the question underlying probabilistic retrieval: ""What is the probability that this document is relevant to this query?"" [46]. A relevance status for a retrieved list can be operationally defined in terms of document relevance [43]; e.g., by thresholding a documentrelevance-based evaluation measure applied to the retrieved list. It was shown that many post-retrieval QPP methods can be derived from Equation 2 [43].",1,ad,True
63,3.1 A missing Piece: The Information Need,0,,False
64,A more careful examination of past work on QPP reveals the following: evaluation of prediction quality was performed by using,0,,False
65,"SIGIR '19, July 21­25, 2019, Paris, France SIGIR '19, July 21­25, 2019, Paris, France",0,,False
66,"queries representing different information needs -- specifically, TREC topics served as information needs, and each topic was represented by a title, which was treated as a query. Various predictors, including pre-retrieval methods that analyze the query and the corpus but not the retrieved list, were shown to yield high prediction quality in this evaluation setting. However, recent work shows that pre-retrieval predictors are ineffective in predicting the relative performance of different queries used to represent the same information need [48]. Now, these two prediction tasks -- for queries representing the same or different information needs -- cannot be differentiated at the model level using Equations 1 and 2, since the underlying information need is not explicitly accounted for.",1,TREC,True
67,"Furthermore, the (implicit) coupling of the information need with the query in previous work on QPP has led to ignoring the fact that different queries representing the same information need can exhibit very different performance characteristics. As a case in point, consider the WIG predictor [61] which uses the (corpus normalized) mean retrieval score of the top retrieved documents as a prediction value. If the query does not effectively represent the information need, then high retrieval scores which attest to improved match between the document and the query3 need not necessarily indicate high effectiveness. Moreover, the effectiveness of the query in representing the information need for retrieval can depend on the corpus and retrieval method. For example, a query that can be relatively ineffective for a simple bag-of-words ranking function such as Okapi BM25 [32] or query likelihood [45] might still be highly effective if a retrieval method which captures higher level term dependencies is used (e.g., [28]).",1,WIG,True
68,"Given the observations discussed above, we revise the basic probabilistic modeling of the QPP task from Equations 1 and 2 to account for the information need. The QPP task becomes estimating:",0,,False
69,"p(R ,"" 1|I , Q, C, M) "", p(R ,"" 1|I, Q, L);""",0,,False
70,(3),0,,False
71,"I is a random variable that takes information needs as values; and, recall that the value of L is uniquely determined by the values of Q, C and M. Figure 1 (b) depicts the dependencies between the random variables. Note that an assignment to I induces a probability distribution over assignments of Q; this is the probability that a user selects a specific query to represent an information need. The retrieval effectiveness of this selection depends on the retrieval method and corpus that together with the query determine the retrieved list.",0,,False
72,"Assignments of the random variables in Equation 3 result in a novel revised fundamental question of the QPP task: ""What is the probability that this retrieved list is relevant to this (latent) information need given that the need is represented by this query?.""",0,,False
73,3.2 QPP using Reference Queries,0,,False
74,"To further explore the importance of accounting for the connection between the information need and the query in the QPP task, we pursue the following novel challenge: predicting retrieval performance for a given query using information about additional queries that presumably represent the same information need.",1,ad,True
75,3WIG is mainly used for ranking functions that are based on the surface-level similarity between the query and documents.,1,WIG,True
76,397,0,,False
77,"Session 4B: Queries SIGIR '19, July 21­25, 2019, Paris, France",1,Session,True
78,"Formally, let q be the query used for retrieval (Q , q) and i",0,,False
79,"be the information need it represents (I ,"" i). Let Qi be a set of queries, henceforth referred to as reference queries, that represent i; i.e., q  Qi . p(Q "", q|I , i) > 0 (q q).",0,,False
80,"We predict query performance by estimating p(R , 1|I ,"" i, Q "",",0,,False
81,"q, C ,"" c, M "", m) from Equation 3 using the reference queries in Qi",0,,False
82,as disjoint proxies for the information need:,0,,False
83,"p^Ref (R ,"" 1|i, q, c, m) d"",ef",0,,False
84,"p^(R ,"" 1|i, q, q, c, m)p^(q|i, q, c, m);""",0,,False
85,q Qi,0,,False
86,(4),0,,False
87,"herein, p^ denotes an estimate for p; p^Ref is an estimate that utilizes",0,,False
88,reference queries; we omit random variables from formulations,0,,False
89,(except for the relevance status) for brevity.,0,,False
90,"We now examine the estimates on the right hand side of Equation 4. p^(q|i, q, c, m) is an estimate for the likelihood that the reference query q is the one selected to represent the information need",0,,False
91,"i. Based on the relationships between random variables assumed in Figure 1 (b), the likelihood p(q|i, q, c, m) does not depend on the",0,,False
92,"corpus, retrieval method or on other queries used to represent the need4. In addition, we use the query q as a signal about the (latent)",1,ad,True
93,information need to derive the following approximation which is inspired by the relevance-model framework [27]5:,0,,False
94,"p^(q|i, q, c, m) , p^(q|i)  p^(q|q).",0,,False
95,(5),0,,False
96,"Below we use inter-query association measures to derive p^(q|q). We next examine p^(R ,"" 1|i, q, q, c, m) in Equation 4. This is an""",0,,False
97,"estimate for the probability of a relevance event given two queries that represent the information need i. The actual retrieved list is not specified as it can be produced in several ways; e.g., the queries can be concatenated to yield a single query used for retrieval, or the lists retrieved for the two queries can be fused to produce a single list. To devise a generic estimate which potentially applies to different approaches of fusing information about the two queries for retrieval, we make the following basic assumption: the retrieval effectiveness of using two queries representing the same information need is based, among other factors, on the extent to which each is an effective representative of the information need with respect to the corpus and retrieval method used. Specifically, the estimate we use is a linear interpolation (fusion), with a free parameter , of the estimates for a relevance event for the two queries:",0,,False
98,"p^(R ,"" 1|i, q, q, c, m) d"",ef (1 - )p^(R ,"" 1|i, q, c, m)+ p^(R "","" 1|i, q, c, m); (6)""",0,,False
99,"Plugging the estimates from Equations 5 and 6 in Equation 4 and assuming that p^(q|q) is a probability distribution over q  Qi , we",0,,False
100,arrive at:,0,,False
101,"p^Ref (R ,"" 1|i, q, c, m) d"",ef (1 - )p^(R ,"" 1|i, q, c, m)+""",0,,False
102,"p^(R ,"" 1|i, q, c, m)p^(q|q).""",0,,False
103,q Qi,0,,False
104,(7),0,,False
105,"4For simplicity, we assume that queries are generated independently for an information need. Note that this conditional independence does not contradict our use of the queries",1,ad,True
106,as disjoint events in Equation 4. Disjoint events cannot be independent. 5The probability of generating a term from a relevance model is approximated by the,0,,False
107,probability to generate it given the observed query [27].,0,,False
108,"SIGIR '19, July 21­25, 2019, Paris, France O. Zendel et al.",0,,False
109,Equation 7 is based on backing off from a direct estimate for a rele-,0,,False
110,"vance event, p^(R ,"" 1|i, q, c, m), to a mixture-based estimate that uses""",0,,False
111,"additional queries representing the same information need. More specifically, the higher the association (p^(q|q)) of the given query (q) with reference queries (q) that are effective representatives of the information need -- i.e., those with high p^(R ,"" 1|i, q, c, m) --""",1,ad,True
112,the higher the estimate for a relevance event for q.,0,,False
113,3.3 Deriving Specific Predictors,0,,False
114,"We next derive specific predictors based on Equation 7. First, follow-",0,,False
115,"ing standard practice in past work on QPP using probabilistic models [25, 33, 35, 43], we use the values P(q) and P(q), assigned by an existing performance predictor P to q and q, for p^(R ,"" 1|i, q, c, m) and p^(R "","" 1|i, q, c, m), respectively. The predictor uses information induced from the query (q or q) and the corpus (c) and might""",0,,False
116,also use information induced from the document list retrieved for the query using the retrieval method m. The prediction principles,0,,False
117,underlying existing predictors were derived from Equations 1 and,0,,False
118,"2 in recent work [43]. Now, if the query is coupled with the information need, as assumed in past work, then p^(R ,"" 1|i, q, c, m) and p^(R "","" 1|i, q, c, m) become p^(R "","" 1|q, c, m) and p^(R "","" 1|q, c, m),""",0,,False
119,"respectively, and we indeed resort to Equation 2. Our next goal is devising the estimate p^(q|q). To this end, we",0,,False
120,"use inter-query association measures, A, as described below. The",0,,False
121,"resultant prediction value we use, following Equation 7, is:",0,,False
122,PRef (q),0,,False
123,"d,e f",0,,False
124,(1,0,,False
125,-,0,,False
126,)P,0,,False
127,(q),0,,False
128,+,0,,False
129,|,0,,False
130,1 Qi,0,,False
131,|,0,,False
132,q Qi,0,,False
133,"P(q)A(q, q).",0,,False
134,(8),0,,False
135,"We do not normalize the inter-query association values to form a probability estimate p^(q|q) over q  Qi , as such normalization",0,,False
136,resulted in substantially degraded prediction quality. (Actual num-,1,ad,True
137,bers are omitted as they convey no additional insight.) This is not a,1,ad,True
138,"surprise: one cannot expect Qi to include all potential queries that represent i. As a result, normalization negatively distorts the esti-",0,,False
139,mate for the true relative extent to which the reference queries are,0,,False
140,"associated with the given query, and thereby the extent to which",0,,False
141,they represent the information need. This badly affects prediction,1,ad,True
142,across information needs -- the standard prediction quality evalua-,0,,False
143,"tion paradigm that we subscribe to in this paper. Furthermore, we",1,ad,True
144,show in Section 4 that the average association of the given query,0,,False
145,with the reference queries is already a descent basis for prediction.,1,ad,True
146,"(We further discuss this in Section 3.3.1.) Hence, normalization with",0,,False
147,respect to the associations negatively affects prediction quality. On,0,,False
148,"the other hand, in practice, we might have a different number of",0,,False
149,reference queries for each information need. To avoid the resulting,0,,False
150,"bias, we use the",0,,False
151,1 | Qi |,0,,False
152,normalization factor in Equation,0,,False
153,8.,0,,False
154,"The first inter-query association measure, A, we consider is",0,,False
155,"the Jaccard coefficient between q and q; the resultant predictor,",0,,False
156,"based on Equation 8, is Ref-Jaccard[P]. The second measure is",0,,False
157,the ratio between the overlap (in terms of number of documents) at the top-k ranks of the document lists retrieved for q and q,0,,False
158,from c using the retrieval method m; k is a free parameter; the,0,,False
159,resultant predictor is denoted Ref-Overlap[P]. The third measure,0,,False
160,is the Rank-Biased-Overlap (RBO) [53] between the lists retrieved for q and q computed at rank k with parameter p. In contrast to,0,,False
161,"the overlap measure, RBO also accounts for the ranks at which",0,,False
162,398,0,,False
163,"Session 4B: Queries Information Needs, Queries, and Query Performance Prediction",1,Session,True
164,"documents appear; the resulting predictor is Ref-RBO[P]. We note that the fact that the overlap and RBO measures are based on information induced from the corpus and the retrieval method does not contradict the fact that, according to Figure 1, queries are generated only based on the information need. The same way users might utilize knowledge and assumptions about the corpus/retrieval method (e.g., the language and style used in the corpus) to formulate queries, the prediction method can utilize all information at hand so as to predict the use of a specific query given an example of another query representing the same information need.",1,ad,True
165,3.3.1 Special-case predictors. To study the contribution to pre-,0,,False
166,diction quality of the two factors that govern the utilization of,0,,False
167,"reference queries in Equation 8 -- i.e, the predicted performance",0,,False
168,of the reference queries and their association with the query --,0,,False
169,"we consider two predictors that are special cases of our prediction model. The first, named OnlyAsso, assumes that p^(R ,"" 1|i, q, c, m) in Equation 7 is the same constant for all q  Qi ; i.e., the reference queries are assumed to be equi-effective, and prediction is only""",0,,False
170,"based on the association between the given query and the reference queries in Qi . Following Equation 8, the predicted value is:",0,,False
171,"POnlyAsso (q) d,ef",0,,False
172,(1,0,,False
173,-,0,,False
174,)P,0,,False
175,(q),0,,False
176,+,0,,False
177,|,0,,False
178,1 Qi,0,,False
179,|,0,,False
180,"A(q, q).",0,,False
181,q Qi,0,,False
182,(9),0,,False
183,"The second predictor assumes that all reference queries are associated to the same extent with q and hence are equi-likely to represent the information need i. The resultant prediction value, following Equation 8, is:",0,,False
184,"POnlyRef (q) d,ef",0,,False
185,(1 - )P(q) +  1 |Qi |,0,,False
186,P (q ).,0,,False
187,q Qi,0,,False
188,(10),0,,False
189,"This prediction method, termed OnlyRef, is based on the following assumption: the unweighted average of the performance-prediction values assigned to queries representing the information need is a good approximation to the performance value that should be predicted for any query (q) representing the need.",0,,False
190,4 EVALUATION,0,,False
191,4.1 Experimental Setup,0,,False
192,"Two TREC datasets were used for experiments. The first is ROBUST which is composed of 528, 155 (mainly) news articles and is associated with 249 TREC topics6 (301-450 and 600-700). The second dataset is the Category B of the ClueWeb12 collection (CW12 hereafter), which is composed of around 50 million web pages, and is associated with 100 TREC topics (201-300). The set of queries per TREC topic includes the original topic title and additional query variations [3, 8]: human generated queries that represent the topic.7 We removed duplicate query variations per topic and queries with out-of-vocabulary terms. On average, there are 12.75 (with standard deviation of 6.81) and 46.11 (with standard deviation of 18.66) unique variations per topic for ROBUST and CW12, respectively.",1,TREC,True
193,6One topic was removed from the original set due to absence of relevant documents,0,,False
194,in the relevance judgments files. 7The variations are available at https://tinyurl.com/robustuqv and https://tinyurl.com/,0,,False
195,clue12uqv.,0,,False
196,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
197,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
198,We applied Krovetz stemming to all queries and documents; stopwords on the INQUERY list were removed only from queries. The Indri toolkit (www.lemurproject.org) was used for experiments.,0,,False
199,"Following common practice in work on QPP [10, 31, 35, 43, 57], we use language-model-based retrieval: the query-likelihood model [45] was used to retrieve the document lists; retrieval scores are log query likelihood; document language models were Dirichlet smoothed with the smoothing parameter set to 1000 [58].",0,,False
200,"Our proposed framework is not limited to specific predictors; we aim to demonstrate consistent patterns on a set of commonly used post- and pre-retrieval predictors. We consider six pre-retrieval predictors that were shown to be highly effective in past work [22]: AvgIDF [15], MaxIDF [15], AvgSCQ [59], MaxSCQ [59], AvgVAR [59] and MaxVAR [59]. The post-retrieval predictors considered are Clarity [14], NQC [41], WIG [61], QF (query feedback) [60] and their UEF [42] counterparts. In addition, we report the performance for SMV [47] and the recently proposed RSD [36] predictor that was instantiated in our experiments using WIG.",1,WIG,True
201,"To measure prediction quality, we use Pearson correlation between the true AP (at cutoff 1000) values attained for queries using relevance judgments and the values assigned to them by a predictor [10]. We also used Kendall's Tau for evaluation [10] for all of the experiments shown. All trends were consistent across both correlation measures, and due to space limitations, the Kendall's Tau results are omitted as they provide no additional insight.",1,AP,True
202,"The free-parameter values of the predictors were set using the train-test approach used in prior QPP work [25, 31, 35, 57]. Specifically, we randomly split the topics into two equal-sized sets, where each of the two sets served in turn as the test fold. The parameter values yielding the highest Pearson correlation (see above) over the training fold were applied to the test fold. The reported prediction quality of the split is the average prediction quality of the two test folds. The partitioning procedure was repeated 30 times and we report the average prediction quality over these 30 splits. Statistically significant differences are computed using the two-tailed paired t-test at a 95% confidence level with Bonferroni correction.",0,,False
203,"The number of top-retrieved documents in all the post-retrieval predictors except for UEF was selected from {5, 10, 25, 50, 100, 250, 500, 1000}. The number of documents used to construct relevance model #1 (RM1) [27] for Clarity and QF was set to values in the same range. For UEF we set the number of top-retrieved documents to values in {25, 50, 100, 250, 500, 1000}, used Pearson correlation to measure inter-list similarity as recommended by Shtok et al. [42], and constructed RM1 using the same number of documents used for measuring the similarity. The overlap between two retrieved lists in QF was computed at the following cutoff values: {5, 10, 25, 50, 100}. For Clarity, QF, UEF and RSD, RM1 was constructed from unsmoothed document-language models and the number of terms was clipped to 100 [31]; for RSD we also experimented with an unclipped RM1 that was reported to be effective in prior work [44]. In addition, for RSD we used 100 sampled lists. For the list-based inter-query association measures used in our approach, Overlap and RBO, the cutoff k is selected from {5, 10, 25, 50, 100, 250, 500}; this range of values was also used to set the sampled-list size in RSD. The value of p in RBO is set to 0.95 following prior recommendations [43]. The value of  was selected from {0, 0.1, . . . , 1}.",1,ad,True
204,399,0,,False
205,Session 4B: Queries,1,Session,True
206,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
207,"Table 1: Prediction quality with respect to query effectiveness: TREC title queries (Title) and query variations with the maximal (Max), median (Med) and minimum (Min) AP per topic. The top (bottom) block presents previously proposed pre-retrieval (post-retrieval) predictors. The best prediction quality per corpus and predictor is boldfaced. The highest number in each column is underlined.",1,TREC,True
208,ROBUST,0,,False
209,CW12,1,CW,True
210,Title Max Med Min Title Max Med Min,0,,False
211,AvgIDF AvgSCQ AvgVar MaxIDF MaxSCQ MaxVar,0,,False
212,.357 .415 .314 .133 .432 .458 .327 .233 .243 .339 .281 .220 .440 .469 .328 .247 .405 .466 .378 .247 .421 .453 .337 .214 .396 .412 .391 .235 .377 .326 .369 .297 .338 .385 .375 .297 .400 .373 .424 .347 .420 .456 .441 .354 .372 .378 .374 .290,0,,False
213,Clarity,0,,False
214,.409 .384 .460 .417 .029 .130 .213 .189,0,,False
215,NQC,0,,False
216,.477 .551 .435 .166 .509 .548 .393 .181,0,,False
217,WIG,1,WIG,True
218,.475 .511 .454 .391 .535 .549 .500 .416,0,,False
219,SMV,0,,False
220,.424 .535 .411 .159 .462 .520 .320 .183,0,,False
221,RSD,0,,False
222,.489 .521 .376 .185 .549 .574 .325 .249,0,,False
223,QF,0,,False
224,.487 .391 .356 .429 .280 .405 .248 .248,0,,False
225,UEF(Clarity) .522 .517 .541 .468 .276 .294 .263 .292,0,,False
226,UEF(NQC) .523 .558 .444 .236 .438 .435 .355 .288,0,,False
227,UEF(WIG) .509 .385 .367 .352 .441 .387 .333 .348,1,WIG,True
228,UEF(QF),0,,False
229,.495 .444 .435 .451 .345 .298 .263 .324,0,,False
230,Table 2: Main result: Prediction quality of Ref-RBO when,0,,False
231,predicting performance for queries which are TREC's topic titles. The baseline is applying P to the title query as is standard. `' marks statistically significant differences with the,1,TREC,True
232,baseline. The best result in a column is underlined.,0,,False
233,P,0,,False
234,AvgIDF AvgSCQ AvgVar MaxIDF MaxSCQ MaxVar,0,,False
235,Clarity NQC WIG SMV RSD QF UEF(Clarity) UEF(NQC) UEF(WIG) UEF(QF),1,WIG,True
236,ROBUST,0,,False
237,baseline,0,,False
238,.357 .243 .405 .396 .338 .420,0,,False
239,.409 .477 .475 .424 .489 .487 .522 .523 .509 .495,0,,False
240,Ref-RBO,0,,False
241,.603 .595 .604 .611 .601 .606,0,,False
242,.598 .586 .590 .584 .568 .588 .603 .579 .589 .575,0,,False
243,CW12,1,CW,True
244,baseline,0,,False
245,.432 .440 .421 .377 .400 .372,0,,False
246,.029 .509 .535 .462 .549 .280 .276 .438 .441 .345,0,,False
247,Ref-RBO,0,,False
248,.669 .696 .673 .627 .675 .637,0,,False
249,.613 .664 .691 .646 .650 .662 .578 .658 .658 .594,0,,False
250,4.2 Experimental Results,0,,False
251,"4.2.1 Query effectiveness. The classic QPP task is to predict retrieval effectiveness for queries, where each represents a different",1,Query,True
252,"SIGIR '19, July 21­25, 2019, Paris, France O. Zendel et al.",0,,False
253,1.0,0,,False
254,Variations,0,,False
255,0.8,0,,False
256,Median,0,,False
257,Title,0,,False
258,0.6,0,,False
259,ROBUST,0,,False
260,AP,1,AP,True
261,0.4,0,,False
262,0.2,0,,False
263,0.0 0,0,,False
264,50,0,,False
265,100,0,,False
266,150,0,,False
267,200,0,,False
268,249,0,,False
269,Topic,0,,False
270,1.0,0,,False
271,Variations,0,,False
272,0.8,0,,False
273,Median Title,0,,False
274,CW12,1,CW,True
275,0.6,0,,False
276,AP,1,AP,True
277,0.4,0,,False
278,0.2,0,,False
279,0.0 0,0,,False
280,50,0,,False
281,100,0,,False
282,Topic,0,,False
283,Figure 2: The retrieval effectiveness (AP@1000) of query variations and TREC title queries for each topic. Each point on the x-axis is a different topic; the topics are ordered on the x-axis by the median effectiveness -- represented by the curves -- of all known variants for the topic.,1,AP,True
284,"information need (topic). In most work to date, a TREC topic title was used as the representative query for a topic. In Table 1 we study the prediction quality when queries of different effectiveness are used to represent each topic: the query variation with the highest AP (Max), median8 AP (Med) and lowest AP (Min) per topic9.",1,TREC,True
285,"Overall, we see that the best prediction quality is in most cases attained when the variation with the maximal AP (Max) represents the topic. (Refer to the boldfaced numbers.) The lowest prediction quality is almost always observed when the chosen query is the one with the minimal AP (Min). More generally, we see that prediction quality varies considerably depending on the effectiveness of the queries used. For example, for ROBUST, the best predictor for title queries is UEF(NQC) (0.523) whereas for Min queries it is UEF(Clarity) (0.468); i.e., the decision about the best predictor to use also appears to depend on the effectiveness of the query used to represent the information need (topic). To shed some light on this phenomenon, in Figure 2 we present the retrieval effectiveness (AP@1000) attained for the title queries, and all additional query",1,AP,True
286,"8For topics with an even number of query variations, the larger of the two middle values was chosen. 9Query variations with a zero AP were omitted for this analysis; there are 71 such variations in CW12 representing 31 topics and 34 in ROBUST representing 11 topics.",1,Query,True
287,400,0,,False
288,Session 4B: Queries,1,Session,True
289,"Information Needs, Queries, and Query Performance Prediction",1,Query,True
290,"Table 3: Prediction quality when using different inter-query association measures in our approach. `b' and `r': statistically significant differences with baseline and Ref-RBO in a column (except for OnlyAsso), respectively. `o': statistically significant differences with OnlyAsso in a row. Bold: best result in a column per dataset.",0,,False
291,ROBUST,0,,False
292,MaxIDF AvgSCQ WIG UEF(Clarity) OnlyAsso,1,WIG,True
293,baseline,0,,False
294,.396 .243 .475,0,,False
295,.522,0,,False
296,-,0,,False
297,Ref-Jaccard Ref-Overlap Ref-RBO Ref-Geo OnlyRef,0,,False
298,.....646331819934153rbobrbroboo,0,,False
299,....535296945413rbobroboo .290rb,0,,False
300,.506rbo .588b .590b .475ro .471rb,0,,False
301,.....655550592333912rbobrbroboo,0,,False
302,.269 .582 .588 .187,0,,False
303,-,0,,False
304,CW12,1,CW,True
305,MaxIDF AvgSCQ WIG UEF(Clarity) OnlyAsso,1,WIG,True
306,baseline,0,,False
307,.377 .440 .535,0,,False
308,.276,0,,False
309,-,0,,False
310,Ref-Jaccard Ref-Overlap Ref-RBO Ref-Geo,0,,False
311,....656372270773rbobrrbooo,0,,False
312,....766420934868rbobrrboo,0,,False
313,..760133rbrboo .691b .534ro,0,,False
314,....655225772185rbobrrbooo,0,,False
315,.504 .720 .685 .071,0,,False
316,OnlyRef,0,,False
317,.383r .490rb .537r,0,,False
318,.447rb,0,,False
319,-,0,,False
320,"variations per topic. We see that the difference in retrieval effectiveness among query variations per topic can be quite striking. While for some queries the AP is nearly 0, most topics have at least one variant with an AP higher than 0.3. In addition, we see that using the TREC title queries for retrieval does not necessarily yield median retrieval effectiveness per topic. For some topics, the performance can be much better or much worse than the median. In other words, the relative effectiveness of the title queries in representing the underlying information need (topic) varies across topics.",1,AP,True
321,"4.2.2 Main result: Using reference queries for prediction. We now turn to study the merits of using reference queries to predict retrieval effectiveness. In this section, and in Sections 4.2.3-4.2.6, prediction quality is evaluated using standard practice in work on QPP [10]; that is, prediction is performed for a set of queries, each of which is the title of a different TREC topic. In Section 4.2.7 we evaluate prediction quality when each topic is represented by a query variation which is not necessarily the topic title.",1,TREC,True
322,"Table 2 presents the prediction quality of Ref-RBO, which was derived from Equation 8 using RBO as the inter-query association measure. We show in Section 4.2.3 that RBO is one of the most effective inter-query association measures among those considered. Hence, Ref-RBO is the main instantiation of our approach that we focus on throughout this section. We hasten to point out that the prediction quality patterns we report for Ref-RBO are consistent with those attained when using the other inter-query association measures, as exemplified in Section 4.2.3.",0,,False
323,"We can see in Table 2 that for all the considered predictors P, for both datasets, Ref-RBO, which relies on reference queries, substantially and statistically significantly outperforms the baseline:",0,,False
324,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
325,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
326,ROBUST Ref-RBO,0,,False
327,0.6,0,,False
328,0.5,0,,False
329,Pearson,0,,False
330,0.4 0.3,0,,False
331,0.0 0.7,0,,False
332,AvgSCQ MaxIDF UEF(Clarity) WIG,1,WIG,True
333,0.2,0,,False
334,0.4,0,,False
335,0.6,0,,False
336,0.8,0,,False
337,1.0,0,,False
338,CW12 Ref-RBO,1,CW,True
339,0.6,0,,False
340,Pearson,0,,False
341,0.5,0,,False
342,AvgSCQ,0,,False
343,0.4,0,,False
344,MaxIDF,0,,False
345,UEF(Clarity),0,,False
346,0.3,0,,False
347,WIG,1,WIG,True
348,0.0,0,,False
349,0.2,0,,False
350,0.4,0,,False
351,0.6,0,,False
352,0.8,0,,False
353,1.0,0,,False
354,Figure 3: The effect of  on prediction quality (Equation 8).,0,,False
355,"applying the predictor P to the title query, as is standard, without using reference queries. Note that this baseline is a specific case of our approach when setting  , 0 in Equation 8. The best prediction quality attained for Ref-RBO (refer to the underlined numbers) surpasses the best results attained by any baseline by a large margin. We can clearly conclude that there is much merit in using our QPP approach that utilizes reference queries.",0,,False
356,"4.2.3 Inter-query association measures. Table 3 presents the prediction quality of our approach with the various inter-query association measures proposed in Section 3.3. Hereafter, due to space limitations, we only report the results for two pre-retrieval and two post-retrieval predictors which yield the best prediction quality (per collection) in Table 2 when used in our approach.",0,,False
357,"In addition, we present the results for the two special cases of our approach: OnlyAsso, which assumes that reference queries are effective to the same extent (Equation 9), and OnlyRef, which assumes that reference queries are uniformly distributed (Equation 10). We also experimented with a previously proposed estimate for p(q|i) [44], termed Geo. We report the prediction quality of using Equation 8, where Geo is used instead of the inter-query association measure; the resulting predictor is denoted Ref-Geo.",1,ad,True
358,We can see in Table 3 that our approach statistically significantly outperforms the baseline in the vast majority of cases regardless of the inter-query association measure employed. The best prediction quality is almost always attained for ROBUST by Ref-RBO and for CW12 by Ref-Overlap. The lowest numbers are observed for RefGeo; this is presumably because Geo is not effective in predicting,1,CW,True
359,401,0,,False
360,"Session 4B: Queries SIGIR '19, July 21­25, 2019, Paris, France",1,Session,True
361,Pearson,0,,False
362,ROBUST Ascending,0,,False
363,0.7,0,,False
364,0.6,0,,False
365,0.6 0.5,0,,False
366,0.5,0,,False
367,Pearson,0,,False
368,0.4,0,,False
369,AvgSCQ,0,,False
370,0.4,0,,False
371,MaxIDF,0,,False
372,0.3,0,,False
373,UEF(Clarity),0,,False
374,WIG,1,WIG,True
375,0.3,0,,False
376,0,0,,False
377,10,0,,False
378,20,0,,False
379,30,0,,False
380,40,0,,False
381,0,0,,False
382,# of reference queries,0,,False
383,CW12 Ascending,1,CW,True
384,0.8,0,,False
385,0.6,0,,False
386,0.7,0,,False
387,Pearson,0,,False
388,Pearson,0,,False
389,0.5,0,,False
390,0.6,0,,False
391,AvgSCQ,0,,False
392,0.5,0,,False
393,0.4,0,,False
394,MaxIDF,0,,False
395,0.4,0,,False
396,UEF(Clarity),0,,False
397,0.3,0,,False
398,WIG,1,WIG,True
399,0.3,0,,False
400,0,0,,False
401,10,0,,False
402,20,0,,False
403,30,0,,False
404,40,0,,False
405,0,0,,False
406,# of reference queries,0,,False
407,"SIGIR '19, July 21­25, 2019, Paris, France O. Zendel et al.",0,,False
408,ROBUST Descending,0,,False
409,AvgSCQ MaxIDF UEF(Clarity) WIG,1,WIG,True
410,10,0,,False
411,20,0,,False
412,30,0,,False
413,40,0,,False
414,# of reference queries,0,,False
415,CW12 Descending,1,CW,True
416,AvgSCQ MaxIDF UEF(Clarity) WIG,1,WIG,True
417,10,0,,False
418,20,0,,False
419,30,0,,False
420,40,0,,False
421,# of reference queries,0,,False
422,Figure 4: The effect of the number of reference queries on the prediction quality of Ref-RBO (Equation 8) when reference queries are added in Ascending or Descending order of AP effectiveness.,1,ad,True
423,"performance for queries representing the same information need in our approach -- i.e., the reference queries we use.",0,,False
424,"We also see in Table 3 that even when all reference queries are assumed to be associated with the query to the same extent (OnlyRef), or when we do not use prediction for the reference queries in our approach (OnlyAsso), the prediction quality of our approach surpasses that of the baseline in the vast majority of the cases; nonetheless, using both estimates in our approach results in better prediction quality in the vast majority of cases.",0,,False
425,"As already noted, pre-retrieval predictors are typically more efficient than post-retrieval predictors because they can be computed before the retrieval is performed. All the predictors instantiated in our framework are post-retrieval. The only exception is when Ref-Jaccard is used together with a pre-retrieval predictor P. Interestingly, this combination results in very high prediction quality. A case in point, for CW12, the prediction quality of RefJaccard[AvgSCQ] (.608) surpasses that of all the baseline pre- and post-retrieval predictors presented in Table 2.",1,ad,True
426,"4.2.4 The effect of  on prediction quality. In Figure 3 we study the effect of  on the prediction quality of Ref-RBO. (Recall that  , 0 amounts to the baseline: applying an existing predictor directly to the query as is standard.) Similar patterns were observed for the other inter-query association measures considered. We see",0,,False
427,"that the best prediction quality is always attained for   0.7, i.e., when a high weight is given to the reference queries; yet, in most cases the optimal  is (slightly) smaller than 1, attesting to the additional contribution to prediction quality of also accounting for the prediction performed directly to the query.",1,ad,True
428,"4.2.5 The effectiveness of the reference queries. Thus far, all the available query variations served as the reference queries in our framework regardless of their retrieval effectiveness. In what follows, we divide the variations into two halves: the queries with the highest (High) and lowest (Low) AP values per topic. In Table 4 we study the merits of using each of the two sets (in comparison to using all variations) as reference queries. We observe the following: (i) using High yields better prediction quality than using Low or using all the variations; the differences are statistically significant in the vast majority of cases; and, (ii) using each of the sets (including Low) is superior to not using reference queries at all (i.e., the baseline); that is, using even poor variations as reference queries can be beneficial for prediction using our approach.",1,AP,True
429,4.2.6 Varying the number of reference queries. We next study the effect of the number of reference queries on prediction quality. In Figure 4 we show the prediction quality of Ref-RBO as a function of the number of reference queries used; the queries are added one by,1,ad,True
430,402,0,,False
431,Session 4B: Queries,1,Session,True
432,"Information Needs, Queries, and Query Performance Prediction",1,Query,True
433,"Table 4: Prediction quality with respect to the effectiveness of reference queries. Low and High: using the set of queries with the highest and lowest AP values, respectively. All: using all queries. `b' and `a' mark statistically significant difference with the baseline and All, respectively. `l' marks statistically significant differences between Low and High.",1,AP,True
434,ROBUST,0,,False
435,CW12,1,CW,True
436,P,0,,False
437,Quantile baseline Ref-RBO baseline Ref-RBO,0,,False
438,AvgSCQ,0,,False
439,Low .243 High .243 All .243,0,,False
440,MaxIDF,0,,False
441,Low .396 High .396 All .396,0,,False
442,Low .522 UEF(Clarity) High .522,0,,False
443,All .522,0,,False
444,Low .475,0,,False
445,WIG,1,WIG,True
446,High .475,0,,False
447,All .475,0,,False
448,.495ab .440 .634abl .440 .595b .440 .525ab .377 .647abl .377 .611b .377 .562ab .276 .639abl .276 .603b .276 .533ab .535 .652abl .535 .590b .535,0,,False
449,.555ab .730abl .696b .492ab .671abl .627b .413ab .636abl .578b .577ab .721abl .691b,0,,False
450,"one either in ascending or descending order of AP effectiveness10. We can see that the highest prediction quality is attained when using a relatively small number of highly effective queries: only one query is needed in ROBUST and about five queries are required in CW12. However, when using less effective reference queries, a much larger number of queries is needed to reach the highest prediction quality. For CW12, prediction quality gradually improves as more queries are added. For ROBUST, a plateau is attained after adding about twenty queries.",1,AP,True
451,"4.2.7 Putting it all together. Thus far, we demonstrated the clear merits of our approach when predicting performance for titles of TREC topics which served for queries. In Table 1 we showed that prediction quality of existing predictors varies considerably depending on the effectiveness of the query for which performance is predicted. Obviously, the TREC title query might be the best or the worst in terms of representing the topic (information need). Indeed, Figure 2 showed that in some cases, the effectiveness of the title query can be quite different than the median effectiveness of variants representing the topic. So, an important question we consider next is whether our approach is effective in predicting performance for queries of varying effectiveness in terms of representation of the underlying information need. It is important to differentiate this question from the one we explored in Section 4.2.5: the impact on prediction quality of using reference queries of different effectiveness to predict the performance of title queries.",1,TREC,True
452,"Table 5 present the results for Ref-RBO when prediction is performed for the queries with the highest (Max), median (Med) and lowest (Min) AP per topic. We present for reference the prediction quality of predicting performance for title queries. All variations",1,AP,True
453,"10If the number of variations (on the x-axis) exceeds the number of variations for a specific topic, all variations available for this topic are used as reference queries.",0,,False
454,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
455,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
456,"Table 5: Prediction quality of Ref-RBO for queries with the maximal (Max), median (Med) and minimal (Min) AP, and for the title queries. `' marks statistically significant differences with the baseline: applying P directly to the query as is standard. Best result in a column in a block is boldfaced.",1,AP,True
457,P AvgSCQ MaxIDF UEF(Clarity) WIG,1,WIG,True
458,ROBUST,0,,False
459,CW12,1,CW,True
460,baseline Ref-RBO baseline Ref-RBO,0,,False
461,Max .339,0,,False
462,.583,0,,False
463,.469,0,,False
464,.747,0,,False
465,Med .281,0,,False
466,.631,0,,False
467,.328,0,,False
468,.660,0,,False
469,Min .220,0,,False
470,.724,0,,False
471,.247,0,,False
472,.695,0,,False
473,Title .243,0,,False
474,.595,0,,False
475,.440,0,,False
476,.696,0,,False
477,Max .412,0,,False
478,.588,0,,False
479,.326,0,,False
480,.697,0,,False
481,Med .391,0,,False
482,.627,0,,False
483,.369,0,,False
484,.614,0,,False
485,Min .235,0,,False
486,.694,0,,False
487,.297,0,,False
488,.688,0,,False
489,Title .396,0,,False
490,.611,0,,False
491,.377,0,,False
492,.627,0,,False
493,Max .517,0,,False
494,.583,0,,False
495,.294,0,,False
496,.659,0,,False
497,Med .541,0,,False
498,.644,0,,False
499,.263,0,,False
500,.557,0,,False
501,Min .468,0,,False
502,.698,0,,False
503,.292,0,,False
504,.725,0,,False
505,Title .522,0,,False
506,.603,0,,False
507,.276,0,,False
508,.578,0,,False
509,Max .511,0,,False
510,.595,0,,False
511,.549,0,,False
512,.774,0,,False
513,Med .454,0,,False
514,.644,0,,False
515,.500,0,,False
516,.698,0,,False
517,Min .391,0,,False
518,.714,0,,False
519,.416,0,,False
520,.688,0,,False
521,Title .475,0,,False
522,.590,0,,False
523,.535,0,,False
524,.691,0,,False
525,available for a topic are used as reference queries. We can conclusively see that our approach substantially outperforms the baseline regardless of the effectiveness of the query for which prediction is performed. This finding attests to the robustness of our approach with respect to existing predictors.,0,,False
526,5 CONCLUSIONS AND FUTURE WORK,0,,False
527,"We demonstrated important connections between a query, an information need, and the prediction quality achievable with many commonly used query performance predictors. Specifically, we showed that the relative prediction quality patterns of existing predictors can substantially vary with respect to the effectiveness of the queries for which performance is predicted.",0,,False
528,"Accordingly, we reformulated the probabilistic foundation of the query-performance-prediction (QPP) task by explicitly accounting for the underlying information need and its connection to queries used to represent it. We then presented a novel QPP approach that incorporates additional information from the information need, in the form of queries that can represent it. The approach, which can be instantiated using any existing performance predictor, dramatically improves prediction quality irrespective of the effectiveness of the query for which prediction is performed, or of the QPP method used to instantiate the approach.",1,corpora,True
529,"We intend to continue our exploration of the relationship between predicting performance for queries representing different information needs and predicting performance for queries representing the same information need, the latter of which remains as a grand challenge for the IR community.",0,,False
530,403,0,,False
531,Session 4B: Queries,1,Session,True
532,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
533,Acknowledgements. We thank the reviewers for their comments.,0,,False
534,"This work was partially supported by the Israel Science Foundation (grant no. 1136/17), the Australian Research Council's Discovery Projects Scheme (DP170102231), a Google Faculty Award, and an Amazon Research Award.",0,,False
535,REFERENCES,0,,False
536,"[1] G. Amati, C. Carpineto, and G. Romano. 2004. Query difficulty, robustness, and selective application of query expansion. In Proc. of ECIR. 127­137.",1,Query,True
537,[2] J. A. Aslam and V. Pavlu. 2007. Query Hardness Estimation Using Jensen-Shannon Divergence Among Multiple Scoring Functions. In Proc. of ECIR. 198­209.,1,Query,True
538,"[3] P. Bailey, A. Moffat, F. Scholer, and P. Thomas. 2016. UQV100: A Test Collection with Query Variability. In Proc. of SIGIR. 725­728.",1,UQV,True
539,"[4] P. Bailey, A. Moffat, F. Scholer, and P. Thomas. 2017. Retrieval Consistency in the Presence of Query Variations. In Proc. of SIGIR. 395­404.",1,Query,True
540,[5] N. Balasubramanian and J. Allan. 2010. Learning to select rankers. In Proc. of SIGIR. 855­856.,0,,False
541,"[6] N. J. Belkin, C. C., W. B. Croft, and J. P. Callan. 1993. The effect of multiple query representations on information retrieval system performance. In Proc. of SIGIR. 339­346.",0,,False
542,"[7] N. J. Belkin, P. Kantor, E.A. Fox, and J.A. Shaw. 1995. Combining evidence of multiple query representation for information retrieval. Information Processing and Management 31, 3 (1995), 431­448.",0,,False
543,[8] R. Benham and J. S. Culpepper. 2017. Risk-Reward Trade-offs in Rank Fusion. In Proc. of ADCS. 1­8.,1,ad,True
544,"[9] Y. Bernstein, B. Billerbeck, S. Garcia, N. Lester, F. Scholer, and J. Zobel. 2005. RMIT University at TREC 2005: Terabyte and Robust Track. In Proc. of TREC-14.",1,TREC,True
545,[10] D. Carmel and E. Yom-Tov. 2010. Estimating the Query Difficulty for Information Retrieval. Morgan & Claypool Publishers.,1,Query,True
546,"[11] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. 2006. What makes a query difficult?. In Proc. of SIGIR. 390­397.",0,,False
547,"[12] A.-G. Chifu, L. Laporte, J. Mothe, and Md Z. Ullah. 2018. Query Performance Prediction Focused on Summarized Letor Features. In Proc. of SIGIR. 1177­1180.",1,Query,True
548,[13] N. Craswell and M. Szummer. 2007. Random walks on the click graph. In Proc. of SIGIR. 239­246.,0,,False
549,"[14] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. 2002. Predicting query performance. In Proc. of SIGIR. 299­306.",0,,False
550,"[15] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. 2004. A Language Modeling Framework for Selective Query Expansion. Technical Report IR-338. Center for Intelligent Information Retrieval, University of Massachusetts.",1,Query,True
551,[16] R. Cummins. 2011. Predicting Query Performance Directly from Score Distributions. In Proc. of AIRS. 315­326.,1,Query,True
552,"[17] R. Cummins. 2014. Document Score Distribution Models for Query Performance Inference and Prediction. ACM Transactions on Information Systems 32, 1 (2014), 2.",1,Query,True
553,"[18] R. Cummins, J. M. Jose, and C. O'Riordan. 2011. Improved query performance prediction using standard deviation. In Proc. of SIGIR. 1089­1090.",0,,False
554,"[19] V. Dang, M. Bendersky, and W. B. Croft. 2010. Learning to rank query reformulations. In In Proc. of SIGIR. 807­808.",0,,False
555,[20] F. Diaz. 2007. Performance prediction using spatial autocorrelation. In Proc. of SIGIR. 583­590.,0,,False
556,"[21] C. Hauff, L. Azzopardi, and D. Hiemstra. 2009. The Combination and Evaluation of Query Performance Prediction Methods. In Proc. of ECIR. 301­312.",1,Query,True
557,"[22] C. Hauff, D. Hiemstra, and F. de Jong. 2008. A survey of pre-retrieval query performance predictors. In Proc. of CIKM. 1419­1420.",0,,False
558,[23] B. He and I. Ounis. 2004. Inferring Query Performance Using Pre-retrieval Predictors. In Proc. of SPIRE. 43­54.,1,Query,True
559,"[24] R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Generating query substitutions. In Proc. of WWW. 387­396.",1,ad,True
560,"[25] O. Kurland, A. Shtok, S. Hummel, F. Raiber, D. Carmel, and O. Rom. 2012. Back to the Roots: A Probabilistic Framework for Query-performance Prediction. In Proc. of CIKM. 823­832.",1,Query,True
561,"[26] K. Kwok, L. Grunfeld, H. Sun, P. Deng, and N. Dinstl. 2004. TREC 2004 Robust Track Experiments using PIRCS. In Proc. of TREC-13.",1,TREC,True
562,[27] V. Lavrenko and W. B. Croft. 2001. Relevance-Based Language Models. In Proc. of SIGIR. 120­127.,0,,False
563,[28] D. Metzler and W. B. Croft. 2005. A Markov random field model for term dependencies. In Proc. of SIGIR. 472­479.,0,,False
564,[29] J. Mothe and L. Tanguy. 2005. Linguistic features to predict query difficulty. In ACM SIGIR 2005 Workshop on Predicting Query Difficulty - Methods and Applications. http://www.haifa.il.ibm.com/sigir05-qp/papers/Mothe.pdf,1,Query,True
565,"SIGIR '19, July 21­25, 2019, Paris, France",0,,False
566,O. Zendel et al.,0,,False
567,[30] J. Pérez-Iglesias and L. Araujo. 2010. Standard Deviation as a Query Hardness Estimator. In Proc. of SPIRE. 207­212.,1,Query,True
568,[31] F. Raiber and O. Kurland. 2014. Query-performance prediction: Setting the expectations straight. In Proc. of SIGIR. 13­22.,1,Query,True
569,"[32] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. 1994. Okapi at TREC-3. In Proc. of TREC-3.",1,TREC,True
570,[33] H. Roitman. 2018. An Extended Query Performance Prediction Framework Utilizing Passage-Level Information. In Proc. of ICTIR. 35­42.,1,Query,True
571,[34] H. Roitman. 2018. Query Performance Prediction using Passage Information. In Proc. of SIGIR. 893­896.,1,Query,True
572,"[35] H. Roitman, S. Erera, O. S. Shalom, and B. Weiner. 2017. Enhanced Mean Retrieval Score Estimation for Query Performance Prediction. In Proc. of ICTIR. 35­42.",1,Query,True
573,"[36] H. Roitman, S. Erera, and B. Weiner. 2017. Robust Standard Deviation Estimation for Query Performance Prediction. In Proc. of ICTIR. 245­248.",1,Robust,True
574,"[37] H. Scells, L. Azzopardi, G. Zuccon, and B. Koopman. 2018. Query Variation Performance Prediction for Systematic Reviews. In Proc. of SIGIR. 1089­1092.",1,Query,True
575,[38] F. Scholer and S. Garcia. 2009. A case for improved evaluation of query difficulty prediction. In Proc. of SIGIR. 640­641.,0,,False
576,"[39] F. Scholer, H. E. Williams, and A. Turpin. 2004. Query association surrogates for Web search. JASIST 55, 7 (2004), 637­650.",1,Query,True
577,"[40] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. 2011. LambdaMerge: merging the results of query reformulations. In Proc. of WSDM. 795­804.",0,,False
578,"[41] A. Shtok, O. Kurland, and D. Carmel. 2009. Predicting query performance by query-drift estimation. In Proc. of ICTIR. 305­312.",0,,False
579,"[42] A. Shtok, O. Kurland, and D. Carmel. 2010. Using statistical decision theory and relevance models for query-performance prediction. In Proccedings of SIGIR. 259­266.",0,,False
580,"[43] A. Shtok, O. Kurland, and D. Carmel. 2016. Query Performance Prediction Using Reference Lists. ACM Trans. Inf. Syst. 34, 4 (2016), 19:1­19:34.",1,Query,True
581,"[44] M. Sondak, A. Shtok, and O. Kurland. 2013. Estimating query representativeness for query-performance prediction. In Proc. of SIGIR. 853­856.",0,,False
582,[45] F. Song and W. B. Croft. 1999. A general language model for information retrieval. In Proc. of SIGIR. 279­280.,0,,False
583,"[46] K. Sparck Jones, S. Walker, and S. E. Robertson. 2000. A probabilistic model of information retrieval: development and comparative experiments - Part 1. Information Processing and Management 36, 6 (2000), 779­808.",0,,False
584,[47] Y. Tao and S. Wu. 2014. Query Performance Prediction By Considering Score Magnitude and Variance Together. In Proc. of CIKM. 1891­1894.,1,Query,True
585,"[48] P. Thomas, F. Scholer, P. Bailey, and A. Moffat. 2017. Tasks, Queries, and Rankers in Pre-Retrieval Performance Prediction. In Proc. of ADCS. 11:1­11:4.",0,,False
586,"[49] S. Tomlinson. 2004. Robust, Web and Terabyte Retrieval with Hummingbird Search Server at TREC 2004. In Proc. of TREC-13.",1,Robust,True
587,"[50] Eduardo Vicente-López, Luis M. Campos, Juan M. Fernández-Luna, and Juan F. Huete. 2018. Predicting IR Personalization Performance Using Pre-retrieval Query Predictors. J. Intell. Inf. Syst. 51, 3 (2018), 597­620.",1,Query,True
588,"[51] V. Vinay, I. J. Cox, N. Milic-Frayling, and K. R. Wood. 2006. On ranking the effectiveness of searches. In Proc. of SIGIR. 398­404.",0,,False
589,[52] E. M. Voorhees and D. K. Harman. 2005. TREC: Experiments and evaluation in information retrieval. The MIT Press.,1,TREC,True
590,"[53] W. Webber, A. Moffat, and J. Zobel. 2010. A Similarity Measure for Indefinite Rankings. ACM Trans. Inf. Syst. 28, 4, Article 20 (Nov. 2010), 38 pages.",0,,False
591,"[54] M. Winaver, O. Kurland, and C. Domshlak. 2007. Towards robust query expansion: Model selection in the language model framework to retrieval. In Proc. of SIGIR. 729­730.",0,,False
592,"[55] D. Yin, Y. Hu, J. Tang, T. Daly, M. Zhou, H. Ouyang, J. Chen, C. Kang, H. Deng, C. Nobata, J.-M. Langlois, and Y. Chang. 2016. Ranking relevance in yahoo search. In Proc. of KDD. 323­332.",0,,False
593,"[56] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. 2005. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In Proc. of SIGIR. 512­519.",0,,False
594,"[57] H. Zamani, W. B. Croft, and J. S. Culpepper. 2018. Neural Query Performance Prediction using Weak Supervision from Multiple Signals. In Proc. of SIGIR. 105­ 114.",1,Query,True
595,[58] C.-X. Zhai and J. D. Lafferty. 2001. A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval. In Proc. of SIGIR. 334­342.,0,,False
596,"[59] Y. Zhao, F. Scholer, and Y. Tsegay. 2008. Effective Pre-retrieval Query Performance Prediction Using Similarity and Variability Evidence. In Proc. of ECIR. 52­64.",1,Query,True
597,[60] Y. Zhou and W. B. Croft. 2006. Ranking robustness: a novel framework to predict query performance. In Proc. of CIKM. 567­574.,0,,False
598,[61] Y. Zhou and W. B. Croft. 2007. Query performance prediction in web search environments. In Proc. of SIGIR. 543­550.,1,Query,True
599,404,0,,False
600,,0,,False

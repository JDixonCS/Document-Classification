,sentence,label,data
,,,
0,Short Research Papers 1B: Recommendation and Evaluation,null,null
,,,
1,,null,null
,,,
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
3,,null,null
,,,
4,Understanding the Interpretability of Search Result Summaries,null,null
,,,
5,,null,null
,,,
6,Siyu Mi,null,null
,,,
7,"Department of Computer Science, Virginia Tech siyu6@vt.edu",null,null
,,,
8,ABSTRACT,null,null
,,,
9,"We examine the interpretability of search results in current web search engines through a lab user study. Particularly, we evaluate search result summary as an interpretable technique that informs users why the system retrieves a result and to which extent the result is useful. We collected judgments about 1,252 search results from 40 users in 160 sessions. Experimental results indicate that the interpretability of a search result summary is a salient factor influencing users' click decisions. Users are less likely to click on a result link if they do not understand why it was retrieved",null,null
,,,
10,KEYWORDS,null,null
,,,
11,Interpretability; search result summary; click behavior.,null,null
,,,
12,ACM Reference Format: Siyu Mi and Jiepu Jiang. 2019. Understanding the Interpretability of Search Result Summaries. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval,null,null
,,,
13,1 INTRODUCTION,null,null
,,,
14,"Machine learning techniques are becoming increasingly powerful today, but they are also more and more sophisticated and difficult to understand for human beings. What we need is not only accurate models but also models that are explainable to us. Understanding how the model works may also help the user make better decisions and further improve the model.",null,null
,,,
15,"Despite many discussions of explainable AI and machine learning recently [3, 4, 9, 11, 13], few previous work explicitly examined the interpretability of search results. Some latest studies [14, 15] applied explainable techniques such as LIME [12] to interpret search result ranking. However, it remains unclear how helpful these techniques are in terms of helping search engine users. Also, we believe current search engines do have already offered some interpretable functionalities, but no previous work examined them in such a way.",null,null
,,,
16,"We note that information retrieval, among many research fields with extensive use of machine learning, is in fact one of the earliest",null,null
,,,
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331306",null,null
,,,
18,,null,null
,,,
19,Jiepu Jiang,null,null
,,,
20,"Department of Computer Science, Virginia Tech jiepu@vt.edu",null,null
,,,
21,"to offer interpretations for system decisions and outputs. Particularly, a query-biased search result summary [16] delivers two important information to search engine users:",null,null
,,,
22,"· Why the system retrieves a search result -- we use transparency to refer to the ability of a summary to interpret this information. Through selecting sentences with high coverage of query terms and highlighting keywords in URLs and snippets, search result summaries inform users keyword matching and term frequency are important criterion for retrieving and ranking search results.",null,null
,,,
23,"· To which extent a result would be useful -- we use assessability to refer to the ability of a summary to explain search result relevance. We believe assessability is a unique aspect of interpretability offered by search result summary, as many other machine learning applications directly present system outputs to end users.",null,null
,,,
24,"We report results from a lab user study for evaluating the interpretability of search result summaries in existing web search engines. We recruit participants to work on different search tasks using an experimental search system, where the results and summaries came from the API of a commercial web search engine. We collect their judgments regarding both the interpretability and the",null,null
,,,
25,· Participants have high transparency and assessability ratings for current search engine's summaries.,null,null
,,,
26,· The summaries' transparency and assessability judgments positively correlate with each other and usefulness ratings.,null,null
,,,
27,· Results of a regression analysis suggests that the transparency and assessability of summaries have significant effects on users' click decisions when they browse a SERP.,null,null
,,,
28,2 USER STUDY,null,null
,,,
29,2.1 Experimental Design,null,null
,,,
30,We conducted a lab user study to evaluate the interpretability of search results in web search engines. We instructed participants to work on assigned search tasks in an experimental system and make judgments about the retrieved results afterward. We recorded users' search behavior and collected search result judgments.,null,null
,,,
31,Our study used a 2×2 within-subject design to balance different types of search tasks. The tasks come from the TREC session tracks [1] and were categorized into four types by the targeted task product and goal based on Li and Belkin's faceted classification framework [8]. The targeted task product is either factual,Y,TREC
,,,
32,,null,null
,,,
33,989,null,null
,,,
34,,null,null
,,,
35,Short Research Papers 1B: Recommendation and Evaluation,null,null
,,,
36,,null,null
,,,
37,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
38,,null,null
,,,
39,Figure 1: Screenshots for search and judgment pages.,null,null
,,,
40,The experimental system sends user requests to Bing API and returns Bing search results and query suggestions. Figure 1,null,null
,,,
41,"For each task, we asked participants to collect information using our experimental search system to address the problem stated in the task description. We instructed the participants that they could issue different queries and click on multiple result links. The participants could request to finish a session if they believe they had finished the task requirements. Our system would also terminate a session automatically after 10 minutes. On average, the participants spent 262 seconds in a search session.",null,null
,,,
42,2.2 Search Result Judgments,null,null
,,,
43,We collected two types of judgments after each session: Summary Judgments. We asked participants to evaluate search result summaries retrieved during the search session. Table 1 shows the summary judgment questions2. We included two interpretability questions,null,null
,,,
44,Figure 1,null,null
,,,
45,"1 Bing search API returned highlighted URLs and snippets. The highlighted words may have not appeared in the search query. 2 During the training session, we instructed participants that they needed to answer the questions based on the whole search result abstract",null,null
,,,
46,,null,null
,,,
47,Table 1: Search result summary judgment questions.,null,null
,,,
48,,null,null
,,,
49,Transparency Assessability Usefulness,null,null
,,,
50,,null,null
,,,
51,"By looking at the snippet, I can understand why the search engine returned this result for my keywords ""$q"". By looking at the snippet, I can tell if the result is useful or not without opening the link. By looking at the snippet, I expect the result webpage to include useful information for the search task.",null,null
,,,
52,,null,null
,,,
53,Table 2: Summary of collected result judgments.,null,null
,,,
54,,null,null
,,,
55,"Priority of Summary Judgments 1 Clicked results 2 Not clicked, possibly viewed 3 Not clicked, possibly not viewed",null,null
,,,
56,,null,null
,,,
57,Judged/Total,null,null
,,,
58,,null,null
,,,
59,judgment questions regarding the result document. Here we only focus on summary judgments and do not report result judgments.,null,null
,,,
60,"Priority of Judgments. A user could issue multiple queries and retrieve a large number of results during a session. It is impractical to require participants to judge all the retrieved results due to the time constraints of a lab study. Thus, we generated a priority list of judgments as in Table 2. We gave clicked results the highest priority because they are connected with more search behaviors",null,null
,,,
61,Participants judged results in the priority list one after another until they spent 10 minutes on judgments. We shuffled the sequence of results within each priority group. We did not set any time limits for judging a result to ensure participants have enough time.,null,null
,,,
62,2.3 Collected Data,null,null
,,,
63,The user study included 40 participants,null,null
,,,
64,"In total, we collected user behavior and result judgments from 160 sessions",null,null
,,,
65,3 ANALYSIS,null,null
,,,
66,We examine the collected search result summary judgments in this section. We particularly focus on the following research questions:,null,null
,,,
67,,null,null
,,,
68,990,null,null
,,,
69,,null,null
,,,
70,Short Research Papers 1B: Recommendation and Evaluation,null,null
,,,
71,,null,null
,,,
72,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
73,,null,null
,,,
74,Figure 2: Distribution of search result summary ratings.,null,null
,,,
75,,null,null
,,,
76,Table 3: Spearman's correlation of judgments,null,null
,,,
77,,null,null
,,,
78,Transparency Assessability Usefulness,null,null
,,,
79,,null,null
,,,
80,Transparency,null,null
,,,
81,,null,null
,,,
82,1,null,null
,,,
83,,null,null
,,,
84,-,null,null
,,,
85,,null,null
,,,
86,-,null,null
,,,
87,,null,null
,,,
88,Assessability,null,null
,,,
89,,null,null
,,,
90,0.483,null,null
,,,
91,,null,null
,,,
92,1,null,null
,,,
93,,null,null
,,,
94,-,null,null
,,,
95,,null,null
,,,
96,Usefulness,null,null
,,,
97,,null,null
,,,
98,0.617,null,null
,,,
99,,null,null
,,,
100,0.476,null,null
,,,
101,,null,null
,,,
102,1,null,null
,,,
103,,null,null
,,,
104,All the correlations are statistically significant at 0.001 level.,null,null
,,,
105,,null,null
,,,
106,· RQ1: Are search result summaries from current search engines transparent and assessable to users?,null,null
,,,
107,· RQ2: How do transparency and assessability relate to each other and usefulness?,null,null
,,,
108,· RQ3: Does the interpretability of search result summary influence click decisions?,null,null
,,,
109,"When reporting results, we use ns for ""not significant at 0.05 level"" and , , and  for p < 0.05, 0.01, and 0.001, respectively.",null,null
,,,
110,3.1 Interpretability and Search Task,null,null
,,,
111,Participants rated the search result summaries in our experiments,null,null
,,,
112,Figure 2 plots the distribution of the collected judgments. 87% of the judged summaries received a transparency rating as high as 4,null,null
,,,
113,"3.2 Interpretability, Relevance, and Usefulness",null,null
,,,
114,"The collected transparency and assessability judgments positively correlate with each other. They also positively correlate with usefulness, but with different strengths.",null,null
,,,
115,Table 3 reports the Spearman's correlation of the three judgments among all the assessed search result summaries,null,null
,,,
116,Figure 3 and Figure 4 further disclose some details of the correlation between each pair of judgments. We divided the judged summaries into groups based on transparency ratings and assessability ratings,null,null
,,,
117,"As Figure 4 shows, the differences of the ""high"" assessability group with the other two",null,null
,,,
118,,null,null
,,,
119,Figure 3: Comparison of summaries with low,null,null
,,,
120,Figure 4: Comparison of summaries with low,null,null
,,,
121,Figure 3. This further explains the stronger correlation between transparency and usefulness comparing to other pairs of judgments.,null,null
,,,
122,3.3 Interpretability and Click Behavior,null,null
,,,
123,"We continue to examine the relationship between interpretability and click decisions. We only consider summary judgments for clicked results and the ""possibly viewed"" results",null,null
,,,
124,"Figure 5 plots the ""click rates"" for summaries with different levels of transparency and assessability. Here the click rate is calculated as: # judged and clicked summaries / # judged summaries. Figure 5 suggests some possible connections between the two interpretability measures and click decisions. However, it remains unclear whether the different click rates are simply because the two interpretability measures are correlated with usefulness",null,null
,,,
125,"We further performed a logistic regression analysis for users' click decisions among the clicked and ""possibly viewed"" results. We used a multilevel regression model because the data violates the independence assumption for regular logistic regression--we have multiple judgments within a session and a user can perform",null,null
,,,
126,,null,null
,,,
127,991,null,null
,,,
128,,null,null
,,,
129,Short Research Papers 1B: Recommendation and Evaluation,null,null
,,,
130,,null,null
,,,
131,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
132,,null,null
,,,
133,Figure 5: Click rates for summaries with different levels of transparency and assessability,null,null
,,,
134,"multiple sessions. We model user and session as random effects and examine the list of variables in Table 5 as fix effects. The list of independent variables of interests included: · Interpretability judgments ­ transparency and assessability. · Usefulness [5, 6, 10] ­ it is widely assumed that users would click",null,null
,,,
135,"on a result link if the summary looks useful. This is also the fundamental basis for using click as implicit feedback. · The rank of the summary on the original SERP ­ Joachim et al. [7] hypothesized that users are more likely to click on top-ranked results regardless of relevance/usefulness due to a trust bias. · The number of query terms matched in the summary ­ Both Yue et al. [17] and Clarke et al. [2] examined the attractiveness bias of click behavior. Here we use keyword matching as measures for attractiveness. We separately look into the title, URL, and snippet of summaries. We examined results using different text preprocessing methods and found they do not much influence the conclusions. The reported results used the Krovetz stemming and removed stop words",null,null
,,,
136,4 CONCLUSION,null,null
,,,
137,The interpretability of artificial intelligence systems has attracted a lot of attention these days. We believe IR system is one of the earliest to provide interpretable techniques to users--search result summary informs users why a result was retrieved and to which extent the result would be useful,null,null
,,,
138,,null,null
,,,
139,Table 4: Multilevel regression: click as dependent variable.,null,null
,,,
140,,null,null
,,,
141,Independent Variables Rank on the SERP Transparency Assessability Usefulness # query terms in title # query terms in URL # query terms in snipepet,null,null
,,,
142,,null,null
,,,
143,Estimate Std. Error Sig.,null,null
,,,
144,,null,null
,,,
145,0.046,null,null
,,,
146,,null,null
,,,
147,0.04,null,null
,,,
148,,null,null
,,,
149,0.419,null,null
,,,
150,,null,null
,,,
151,0.16,null,null
,,,
152,,null,null
,,,
153,,null,null
,,,
154,,null,null
,,,
155,0.388,null,null
,,,
156,,null,null
,,,
157,0.12,null,null
,,,
158,,null,null
,,,
159,,null,null
,,,
160,,null,null
,,,
161,0.547,null,null
,,,
162,,null,null
,,,
163,0.11,null,null
,,,
164,,null,null
,,,
165,,null,null
,,,
166,,null,null
,,,
167,0.065,null,null
,,,
168,,null,null
,,,
169,0.07,null,null
,,,
170,,null,null
,,,
171,0.186,null,null
,,,
172,,null,null
,,,
173,0.08,null,null
,,,
174,,null,null
,,,
175,,null,null
,,,
176,,null,null
,,,
177,-0.051,null,null
,,,
178,,null,null
,,,
179,0.03,null,null
,,,
180,,null,null
,,,
181,technique in IR systems through a lab user study. Our findings are,null,null
,,,
182,illuminating in several different ways:,null,null
,,,
183,"First, our results suggest that search result summary plays an",null,null
,,,
184,important role in explaining system's decisions,null,null
,,,
185,particular result) and outputs,null,null
,,,
186,from the high transparency and assessability ratings by users.,null,null
,,,
187,"Second, our results disclose a new salient factor--the interpretabil-",null,null
,,,
188,ity of search result summary--influencing users' click decisions.,null,null
,,,
189,This suggests search engines can improve the interpretability of,null,null
,,,
190,search results to optimize users' click decisions. Another important,null,null
,,,
191,implication is that click models may also need to take into account,null,null
,,,
192,interpretability to better model click data.,null,null
,,,
193,"Third, we also recommend that new explainable techniques for",null,null
,,,
194,search engines and search results should be fully compared with,null,null
,,,
195,existing query-biased search result summary.,null,null
,,,
196,REFERENCES,null,null
,,,
197,"[1] B. Carterette, P. Clough, M. Hall, E. Kanoulas, and M. Sanderson. Evaluating retrieval over sessions: The TREC session track 2011-2014. In SIGIR '16, pages 685­688, 2016.",null,null
,,,
198,"[2] C. L. A. Clarke, E. Agichtein, S. Dumais, and R. W. White. The influence of caption features on clickthrough patterns in web search. In SIGIR '07, pages 135­142, 2007.",null,null
,,,
199,"[3] F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608, 2017.",null,null
,,,
200,"[4] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi. A survey of methods for explaining black box models. ACM Computing Surveys, 51(5):93:1­93:42, 2019.",null,null
,,,
201,"[5] J. Jiang, D. He, and J. Allan. Comparing in situ and multidimensional relevance judgments. In SIGIR '17, pages 405­414, 2017.",null,null
,,,
202,"[6] J. Jiang, D. He, D. Kelly, and J. Allan. Understanding ephemeral state of relevance. In CHIIR '17, pages 137­146, 2017.",null,null
,,,
203,"[7] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR '05, pages 154­161, 2005.",null,null
,,,
204,"[8] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Information Processing & Management, 44(6):1822­1837, 2008.",null,null
,,,
205,"[9] Z. C. Lipton. The mythos of model interpretability. In 2016 ICML Workshop on Human Interpretability in Machine Learning, pages 96­100, 2016.",null,null
,,,
206,"[10] J. Mao, Y. Liu, K. Zhou, J.-Y. Nie, J. Song, M. Zhang, S. Ma, J. Sun, and H. Luo. When does relevance mean usefulness and user satisfaction in web search? In SIGIR '16, pages 463­472, 2016.",null,null
,,,
207,"[11] T. Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence, 267:1­38, 2019.",null,null
,,,
208,"[12] M. T. Ribeiro, S. Singh, and C. Guestrin. ""Why should I trust you?"": Explaining the predictions of any classifier. In KDD '16, pages 1135­1144, 2016.",null,null
,,,
209,"[13] M. T. Ribeiro, S. Singh, and C. Guestrin. Anchors: High-precision model-agnostic explanations. In AAAI '18, pages 1527­1535, 2018.",null,null
,,,
210,"[14] J. Singh and A. Anand. Posthoc interpretability of learning to rank models using secondary training data. In 2018 SIGIR Workshop on ExplainAble Recommendation and Search, 2018.",null,null
,,,
211,"[15] J. Singh and A. Anand. EXS: Explainable search using local model agnostic interpretability. In WSDM '19, pages 770­773, 2019.",null,null
,,,
212,"[16] A. Tombros and M. Sanderson. Advantages of query biased summaries in information retrieval. In SIGIR '98, pages 2­10, 1998.",null,null
,,,
213,"[17] Y. Yue, R. Patel, and H. Roehrig. Beyond position bias: Examining result attractiveness as a source of presentation bias in clickthrough data. In WWW '10, pages 1011­1018, 2010.",null,null
,,,
214,,null,null
,,,
215,992,null,null
,,,
216,,null,null
,,,
217,,null,null

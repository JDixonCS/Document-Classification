,sentence,label,data
,,,
0,Session 5A: Conversation and Dialog,null,null
,,,
1,,null,null
,,,
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
3,,null,null
,,,
4,Asking Clarifying Questions in Open-Domain Information-Seeking Conversations,null,null
,,,
5,,null,null
,,,
6,Mohammad Aliannejadi,null,null
,,,
7,Università della Svizzera italiana,null,null
,,,
8,Fabio Crestani,null,null
,,,
9,Università della Svizzera italiana,null,null
,,,
10,ABSTRACT,null,null
,,,
11,"Users often fail to formulate their complex information needs in a single query. As a consequence, they may need to scan multiple result pages or reformulate their queries, which may be a frustrating experience. Alternatively, systems can improve user satisfaction by proactively asking questions of the users to clarify their information needs. Asking clarifying questions is especially important in conversational systems since they can only return a limited number of",null,null
,,,
12,"In this paper, we formulate the task of asking clarifying questions in open-domain information-seeking conversational systems. To this end, we propose an offline evaluation methodology for the task and collect a dataset, called Qulac, through crowdsourcing. Our dataset is built on top of the TREC Web Track 2009-2012 data and consists of over 10K question-answer pairs for 198 TREC topics with 762 facets. Our experiments on an oracle model demonstrate that asking only one good question leads to over 170% retrieval performance improvement in terms of P@1, which clearly demonstrates the potential impact of the task. We further propose a retrieval framework consisting of three components: question retrieval, question selection, and document retrieval. In particular, our question selection model takes into account the original query and previous question-answer interactions while selecting the next question. Our model significantly outperforms competitive baselines. To foster research in this area, we have made Qulac publicly available.",null,null
,,,
13,1 INTRODUCTION,null,null
,,,
14,"While searching on the Web, users often fail to formulate their complex information needs in a single query. As a consequence, they may need to scan multiple result pages or reformulate their queries. Alternatively, systems can decide to proactively ask questions to clarify users' intent before returning the result list [9, 33]. In other words, a system can assess the level of confidence in the results and decide whether to return the results or ask questions from the users to clarify their information need. The questions can be aimed",null,null
,,,
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331265",null,null
,,,
16,,null,null
,,,
17,Hamed Zamani,null,null
,,,
18,University of Massachusetts Amherst zamani@cs.umass.edu,null,null
,,,
19,W. Bruce Croft,null,null
,,,
20,University of Massachusetts Amherst croft@cs.umass.edu,null,null
,,,
21,"Figure 1: Example conversations with clarifying questions from our dataset, Qulac. As we see, both users, Alice and Robin, issue the same query",null,null
,,,
22,"to clarify ambiguous, faceted or incomplete queries [44]. Asking clarifying questions is especially important in conversational search systems for two reasons:",null,null
,,,
23,"A possible workflow for an information system with clarifying questions is shown in Figure 2. As we can see, Alice initiates a conversation by submitting her query to the system. The system then retrieves a list of documents and estimates its confidence on the",null,null
,,,
24,,null,null
,,,
25,475,null,null
,,,
26,,null,null
,,,
27,Session 5A: Conversation and Dialog,null,null
,,,
28,,null,null
,,,
29,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
30,,null,null
,,,
31,result list,null,null
,,,
32,"In this paper, we formulate the task of selecting and asking clarifying questions in open-domain information-seeking conversational systems. To this end, we propose an offline evaluation framework based on faceted and ambiguous queries and collect a novel dataset, called Qulac,1 building on top of the TREC Web Track 2009-2012 collections. Qulac consists of over 10K questionanswer pairs for 198 TREC topics consisting of 762 facets. Inspired from successful examples of crowdsourced collections [2, 4], we collected clarifying questions and their corresponding answers for every topic-facet pair via crowdsourcing. Our offline evaluation protocol enables further research on the topic of asking clarifying questions in a conversational search session, providing a benchmarking methodology to the community.",Y,TREC
,,,
33,"Our experiments on an oracle model show that asking only one good question leads to over 100% retrieval performance improvement. Moreover, the analysis of the oracle model provides important intuitions related to this task. For instance, we see that asking clarifying questions can improve the performance of shorter queries more. Also, clarifying questions exhibit a more significant effect on improving the performance of ambiguous queries, compared to faceted queries. We further propose a retrieval framework following the workflow of Figure 2, consisting of three main components as follows:",null,null
,,,
34,2 RELATED WORK,null,null
,,,
35,While conversational search has roots in early Information Retrieval,null,null
,,,
36,"1Qulac, pronounced ku:l2k, means blizzard and wonderful in Persian. 2Code and data are available at https://github.com/aliannejadi/qulac.",null,null
,,,
37,,null,null
,,,
38,Figure 2: A workflow for asking clarifying questions in an,null,null
,,,
39,open-domain conversational search system.,null,null
,,,
40,"of flight or train schedule information accordingly. The recent advances of conversational agents have attracted research in various aspects of conversational information access [3, 6, 40, 49]. One line of research analyzes data to understand how users interact with voice-only systems [39]. Radlinski and Craswell [33] proposed a theoretical framework for conversational search highlighting the need for multi-turn interactions with users for narrowing down their specific information needs. Also, Trippas et al. [42] studied conversations of real users to identify the commonly-used interactions and inform a conversational search system design. Moreover, research on query suggestion is relevant to our work if we consider suggesting queries as a means of clarifying users' intent in a traditional IR setting [33]. Result diversification and personalizing is one of the key components for query suggestion [20], especially when applied to small-screen devices. In particular, Kato and Tanaka [21] found that presenting results for one facet and suggesting queries for other facets is more effective on such devices.",null,null
,,,
41,Research on clarifying questions has attracted considerable attention in the fields of NLP and IR. People have studied humangenerated dialogues on question answering,null,null
,,,
42,"In the field of NLP, researchers have worked on question ranking [34] and generation [35, 46] for conversation. These studies rely on large amount of data from industrial chatbots [31, 46], query logs [37], and QA websites [34, 35, 41]. For instance, Rao and Daumé",null,null
,,,
43,,null,null
,,,
44,476,null,null
,,,
45,,null,null
,,,
46,Session 5A: Conversation and Dialog,null,null
,,,
47,,null,null
,,,
48,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
49,,null,null
,,,
50,"[34] proposed a neural model for question selection on a simulated dataset of clarifying questions and answers extracted from QA websites such as StackOverflow. Later, they proposed an adversarial training for generating clarifying questions for a given product description on Amazon [35]. Also, Wang et al. [46] studied the task of question generation for an industrial chatbot. Unlike these works, we study the task of asking clarification question in an IR setting where the user's request is in the form of short queries",null,null
,,,
51,3 PROBLEM STATEMENT,null,null
,,,
52,"A key advantage of a conversational search system is its ability to interact with the user in the form question and answer. In particular, a conversational search system can proactively pose questions to the users to understand their actual information needs more accurately and improve its confidence in the search results. We illustrate the workflow of a conversational search system, focusing on asking clarifying questions.",null,null
,,,
53,"As depicted in Figure 2, once the user submits a query to the system, the Information Need Representation module generates and passes their information need to the Retrieval Model, which returns a ranked list of documents. The system should then measure its confidence in the retrieved documents",null,null
,,,
54,3.1 A Facet-Based Offline Evaluation Protocol,null,null
,,,
55,"The design of an offline evaluation protocol is challenging because conversation requires online interaction between a user and a system. Hence, an offline evaluation strategy requires humangenerated answers to all possible questions that a system would ask, something that is impossible to achieve in an offline setting. To circumvent this problem, we substitute the Question Generation Model in Figure 2 with a large bank of questions, assuming that it consists of all possible questions in the collection. Although this assumption is not absolutely realistic, it reduces the complexity of the evaluation significantly as human-generated answers to a limited set of questions can be collected offline, facilitating offline evaluation.",null,null
,,,
56,"In this work, we build our evaluation protocol on top of the TREC Web track's data. TREC has released 200 search topics, each",null,null
,,,
57,,null,null
,,,
58,"of which being either ""ambiguous"" or ""faceted.""3 Clarke et al. [13] defined these categories as follows: ""... Ambiguous queries are those that have multiple distinct interpretations. ... On the other hand, facets reflect underspecified queries, with different aspects covered by the subtopics..."" The TREC collection is originally designed to evaluate search result diversification. In contrast, here we build various conversation scenarios based on topic facets.",null,null
,,,
59,"Formally, let T = {t1, t2, . . . , tn } be the set of topics",null,null
,,,
60,4 DATA COLLECTION,null,null
,,,
61,"In this section, we explain how we collected Qulac",null,null
,,,
62,4.1 Topics and Facets,null,null
,,,
63,"As we discussed earlier, the problem of asking clarifying questions is particularly interesting in cases where a query can be interpreted in various ways. An example is shown in Figure 1 where two different users issue the same query for different intents. Therefore, any data collection should contain an initial query and description of its facet, describing the user's information need. In other words, we define a target facet for each query. Faceted and ambiguous queries make an ideal case to study the effect of clarifying questions in a conversational search system for the following reasons:",null,null
,,,
64,"3In this work, we use the term ""facet"" to refer to the subtopics of both faceted and ambiguous topics.",null,null
,,,
65,,null,null
,,,
66,477,null,null
,,,
67,,null,null
,,,
68,Session 5A: Conversation and Dialog,null,null
,,,
69,,null,null
,,,
70,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
71,,null,null
,,,
72,information need is not clear from the query;,null,null
,,,
73,4.2 Clarifying Questions,null,null
,,,
74,"It is crucial to collect a set of reasonable questions that address multiple facets of every topic6 while containing sufficient negative samples. This enables us to study the effect of retrieval models under the assumption of having a functional question generation model. Therefore, we asked human annotators to generate questions for a given query based on the results they observed on a commercial search engine as well as query auto-complete suggestions.",null,null
,,,
75,"To collect clarifying questions, we designed a Human Intelligence Task",null,null
,,,
76,(1) Enter the same query in a search engine of their choice and scan the results in the first three pages. Reading the title of the results as well as scanning the snippets would give them an idea of different facets of the query on the Web.,null,null
,,,
77,"(2) For some difficult queries such as ""toilet,"" scanning the results would not help in identifying the facets. Therefore, inspired by [8], we asked the workers to type the query in the search box of the search engine, and press the space key after typing the query. Most commercial search engines provide a list of query auto-complete suggestions. Interestingly, in most cases the suggested queries reflect various aspects of the same query.",null,null
,,,
78,"(3) Finally, we asked them to generate six questions related to the query, aiming to address the facets that they had figured out.",null,null
,,,
79,"We assigned two workers to each HIT, resulting in 12 questions per topic in the first round. In order to preserve language diversity of the questions, we limited each worker to a maximum of two HITs. HITs were available to workers residing in the U.S. with an approval rate of over 97%. After collecting the clarifying questions, in the next step, we explain how we verified them for quality assurance.",null,null
,,,
80,4.3 Question Verification and Addition,null,null
,,,
81,"In this step, we aim to address two main concerns:",null,null
,,,
82,4 https://trec.nist.gov/data/webmain.html 5 The official TREC relevance judgements cover 198 of the topics. 6Candidate clarifying questions should also address out-of-collection facets. 7 http://www.mturk.com,Y,TREC
,,,
83,,null,null
,,,
84,Table 1: Statistics of Qulac.,null,null
,,,
85,,null,null
,,,
86,# topics,null,null
,,,
87,,null,null
,,,
88,198,null,null
,,,
89,,null,null
,,,
90,# faceted topics,null,null
,,,
91,,null,null
,,,
92,141,null,null
,,,
93,,null,null
,,,
94,# ambiguous topics,null,null
,,,
95,,null,null
,,,
96,57,null,null
,,,
97,,null,null
,,,
98,# facets Average facet per topic Median facet per topic # informational facets # navigational facets,null,null
,,,
99,,null,null
,,,
100,762 3.85 ± 1.05 4 577 185,null,null
,,,
101,,null,null
,,,
102,# questions # question-answer pairs Average terms per question Average terms per answer,null,null
,,,
103,,null,null
,,,
104,"2,639 10,277 9.49 ± 2.53 8.21 ± 4.42",null,null
,,,
105,,null,null
,,,
106,"this step, we appointed two expert annotators for this task. We instructed the annotators to read all the collected questions of each topic, marking invalid and duplicate questions. Moreover, we asked them to match a question to a facet if the question was relevant to the facet. A question was considered relevant to a facet if its answer would address the facet. Finally, in order to make sure that all facets were covered by at least one question, we asked the annotators to generate an additional question for the facets that needed more specific questions. The outcome of this step is a set of verified clarifying questions, addressing all the facets in the collection.",null,null
,,,
107,,null,null
,,,
108,4.4 Answers,null,null
,,,
109,"After collecting and verifying the questions, we designed another HIT in which we collected answers to the questions for every facet. The HIT started with detailed instructions of the task, followed by several examples. The workers were provided with a topic and a facet description. Then we instructed them to assume that they had submitted the query with their actual information need being the given facet. Then they were required to write the answer to one clarifying question that was presented to them. To avoid the bias of other questions for the same facet, we included only one question in each HIT. If a question required information other than what workers were provided with, we instructed the workers to identify it with a ""No answer"" tag. Each worker was allowed to complete a maximum of 100 HITs to guarantee language diversity. Workers were based in the U.S. with an approval rate of 95% or greater.",null,null
,,,
110,"Quality check. During the course of data collection, we performed regular quality checks on the collected answers. The checks were done manually on 10% of submissions per worker. In case we observed any invalid submissions among the sampled answers of one user, we then studied all the submissions of the same user. Invalid submissions were then removed from the collection and the worker was banned from the future HITs. Finally, we assigned all invalid answers to other workers to complete. Moreover, we employed basic behavioral check techniques in the design of the HIT. For example, we disabled copy/paste features of text inputs and tracked workers' keystrokes. This enabled us to detect and reject low-quality submissions.",null,null
,,,
111,,null,null
,,,
112,5 SELECTING CLARIFYING QUESTIONS,null,null
,,,
113,"In this section, we propose a conversational search system that is able to select and ask clarifying questions and rank documents based on the user's responses. The proposed system retrieves a set of questions for a given query from a large pool of questions,",null,null
,,,
114,,null,null
,,,
115,478,null,null
,,,
116,,null,null
,,,
117,Session 5A: Conversation and Dialog,null,null
,,,
118,,null,null
,,,
119,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
120,,null,null
,,,
121,"containing all the questions in the collection. At the second stage, our proposed model, called NeuQS, aims to select the best question to be posed to the user based on the query and the conversation context. This problem is particularly challenging because the conversational interactions are in natural language, highly depending on the previous interactions between the user and the system",null,null
,,,
122,"As mentioned earlier in Section 3, a user initiates the conversation by submitting a query. Then the system should decide whether to ask a clarifying question or present the results. At every stage of the conversation, the previous questions and answers exchanged between the user and the system are known to the model. Finally, the selected question and its corresponding answer should be incorporated in the document retrieval model to enhance the retrieval performance.",null,null
,,,
123,"Formally, for a given topic t let h = {(q1, a1),",null,null
,,,
124,,null,null
,,,
125,5.1 Question Retrieval Model,null,null
,,,
126,"We now describe our BERT8 Language Representation based Question Retrieval model, called BERT-LeaQuR. We aim to maximize the recall of the retrieved questions, retrieving all relevant clarifying questions to a given query in the top k questions. Retrieving all relevant questions from a large pool of questions is challenging, because questions are short and context-dependent. In other words, many questions depend on the conversation context and the query. Also, since conversation is in the form of natural language, termmatching models cannot effectively retrieve short questions. For instance, some relevant clarifying questions for the query ""dinosaur"" are: ""Are you looking for a specific web page?"" ""Would you like to see some pictures?""",null,null
,,,
127,"Yang et al. [51] showed that neural models outperform termmatching models for question retrieval. Inspired by their work, we learn a high-dimensional language representation for the query and the questions. Formally, BERT-LeaQuR estimates the probability p(R = 1|t, q), where R is a binary random variable indicating whether the question q should be retrieved",null,null
,,,
128,,null,null
,,,
129,"p(R = 1|t, q) =  T",null,null
,,,
130,,null,null
,,,
131,-1,null,null
,,,
132,,null,null
,,,
133,"where T and Q denote topic representation and question representation, respectively.  is the matching component that takes the aforementioned representations and produces a question retrieval score. There are various ways to implement any of these components.",null,null
,,,
134,,null,null
,,,
135,8BERT: Bidirectional Encoder Representations from Transformers,null,null
,,,
136,,null,null
,,,
137,We implement T and Q similarly using a function that maps a sequence of words to a d-dimensional representation,null,null
,,,
138,The component is modeled using a fully-connected feed-forward network with the output dimensionality of 2. Rectified linear unit,null,null
,,,
139,,null,null
,,,
140,5.2 Question Selection Model,null,null
,,,
141,,null,null
,,,
142,"In this section, we introduce a Neural Question Selection Model",null,null
,,,
143,,null,null
,,,
144,(NeuQS) which selects questions with a focus on maximizing the,null,null
,,,
145,,null,null
,,,
146,precision at the top of the ranked list. The main challenge in the,null,null
,,,
147,,null,null
,,,
148,question selection task is to predict whether a question has diverged,null,null
,,,
149,,null,null
,,,
150,from the query and conversation context. In cases where a user has,null,null
,,,
151,,null,null
,,,
152,"given a negative answer(s) to previous question(s), the model needs",null,null
,,,
153,,null,null
,,,
154,"to diverge from the history. In contrast, in cases where the answer",null,null
,,,
155,,null,null
,,,
156,"to the previous question(s) is positive, questions on the same topic",null,null
,,,
157,,null,null
,,,
158,"that ask for more details are preferred. For example, as we saw",null,null
,,,
159,,null,null
,,,
160,"in Figure 1, when Robin answers the first question positively",null,null
,,,
161,,null,null
,,,
162,"being interested in dinosaur books), the second question tries to",null,null
,,,
163,,null,null
,,,
164,narrow down the information to a specific type of dinosaur.,null,null
,,,
165,,null,null
,,,
166,NeuQS incorporates multiple sources of information. In partic-,null,null
,,,
167,,null,null
,,,
168,"ular, it learns from the similarity of a query, a question and the",null,null
,,,
169,,null,null
,,,
170,context as well as retrieval and performance prediction signals. In,null,null
,,,
171,,null,null
,,,
172,"particular, NeuQS outputs a relevance score for a given query t,",null,null
,,,
173,,null,null
,,,
174,"question q, and conversation context h. Formally, NeuQS can be",null,null
,,,
175,,null,null
,,,
176,defined as follows:,null,null
,,,
177,,null,null
,,,
178,score =  T,null,null
,,,
179,,null,null
,,,
180,where  is a scoring function for a given query representation T,null,null
,,,
181,,null,null
,,,
182,tion ,null,null
,,,
183,,null,null
,,,
184,the components of NeuQS.,null,null
,,,
185,,null,null
,,,
186,We model the components T and Q similarly to Section 5.1.,null,null
,,,
187,,null,null
,,,
188,"Further, the context representation component H is implemented",null,null
,,,
189,,null,null
,,,
190,as follows:,null,null
,,,
191,,null,null
,,,
192,H,null,null
,,,
193,,null,null
,,,
194,1 |h|,null,null
,,,
195,,null,null
,,,
196,|h|,null,null
,,,
197,QA,null,null
,,,
198,i,null,null
,,,
199,,null,null
,,,
200,-3,null,null
,,,
201,,null,null
,,,
202,where QA,null,null
,,,
203,,null,null
,,,
204,"answer a. Moreover, the retrieval representation",null,null
,,,
205,,null,null
,,,
206,"is implemented by interpolating the retrieval score of the query,",null,null
,,,
207,,null,null
,,,
208,context and question,null,null
,,,
209,,null,null
,,,
210,"retrieved documents is used. Finally, the query performance prediction",null,null
,,,
211,,null,null
,,,
212,479,null,null
,,,
213,,null,null
,,,
214,Session 5A: Conversation and Dialog,null,null
,,,
215,,null,null
,,,
216,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
217,,null,null
,,,
218,of the performance prediction score of the ranked documents at different ranking positions,null,null
,,,
219,,null,null
,,,
220,5.3 Document Retrieval Model,null,null
,,,
221,,null,null
,,,
222,"Here, we describe the model that we use to retrieve documents given a query, conversation context, and current clarifying question as well as user's answer. We use the KL-divergence retrieval model [24] based on the language modeling framework [30] with Dirichlet prior smoothing [52] where we linearly interpolate two likelihood models: one based on the original query, and one based on the questions and their respective answers.",null,null
,,,
223,"For every term w of the original query t, conversation context h, the current question q, and answer a, the interpolated query probability is computed as follows:",null,null
,,,
224,,null,null
,,,
225,"p(w |t, h, q, a) =  × p(w |t ) +",null,null
,,,
226,,null,null
,,,
227,"where t denotes the language model of the original query, and h,q,a denotes the language model of all questions and answers that have been exchanged in the conversation.  determines the",null,null
,,,
228,weight of the original query and is tuned on the development set.,null,null
,,,
229,"Then, the score of document d is calculated as follows:",null,null
,,,
230,,null,null
,,,
231,"p(d |t, h, q, a) =",null,null
,,,
232,,null,null
,,,
233,"p(wk |t, h, q, a) log(p(wk |d ) ,",null,null
,,,
234,,null,null
,,,
235,-5,null,null
,,,
236,,null,null
,,,
237,wk,null,null
,,,
238,,null,null
,,,
239,where  is the set of all the terms present in the conversation. We use,null,null
,,,
240,,null,null
,,,
241,Dirichlet's smoothing for terms that do not appear in d. We use the,null,null
,,,
242,,null,null
,,,
243,document retrieval model for two purposes:,null,null
,,,
244,,null,null
,,,
245,after the user answers a clarifying question;,null,null
,,,
246,,null,null
,,,
247,of a candidate question as part of the NeuQS,null,null
,,,
248,,null,null
,,,
249,the model does not see the answer in the latter case.,null,null
,,,
250,,null,null
,,,
251,6 EXPERIMENTS,null,null
,,,
252,6.1 Experimental Setup,null,null
,,,
253,"Dataset. We evaluate BERT-LeaQuR and NeuQS on Qulac, following a 5-fold cross-validation. We follow two strategies to split the data,",null,null
,,,
254,"In order to study the effect of multi-turn conversations with clarifying questions, we expand Qulac to include multiple artificially generated conversation turns. To do so, for each instance, we consider all possible combinations of questions to be asked as the context of conversation. Take t1 as an example where we select a new question after asking the user two questions. Assuming that t1 has four questions, all possible combinations of questions in the conversation context would be:",null,null
,,,
255,,null,null
,,,
256,Table 2: Performance of question retrieval model. The superscript * denotes statistically significant differences compared to all the baselines,null,null
,,,
257,,null,null
,,,
258,Method,null,null
,,,
259,,null,null
,,,
260,MAP Recall@10 Recall@20 Recall@30,null,null
,,,
261,,null,null
,,,
262,QL BM25 RM3 LambdaMART RankNet BERT-LeaQuR,null,null
,,,
263,,null,null
,,,
264,0.6714 0.6715 0.6858 0.7218 0.7304,null,null
,,,
265,0.8349*,null,null
,,,
266,,null,null
,,,
267,0.5917 0.5938 0.5970 0.6220 0.6233,null,null
,,,
268,0.6775*,null,null
,,,
269,,null,null
,,,
270,0.6946 0.6848 0.7091 0.7234 0.7314,null,null
,,,
271,0.8310*,null,null
,,,
272,,null,null
,,,
273,0.7076 0.7076 0.7244 0.7336 0.7500,null,null
,,,
274,0.8630*,null,null
,,,
275,,null,null
,,,
276,"each multi-turn example would be the ones that have not appeared in the context. The number of instances grows significantly as we enlarge the length of the conversation, leading to a total of 907,366 instances in the collection. At each turn of the conversation, we select the question from all candidate questions of the same topic and facet, having the same conversation history. In other words, they share the same context. Since the total number of unique conversational contexts is 75,200, a model should select questions for 75,200 contexts from all 907,366 candidate questions.",null,null
,,,
277,Question retrieval evaluation metrics. We consider four metrics to evaluate the effectiveness of question retrieval models: mean average precision,null,null
,,,
278,Question selection evaluation metrics. Effectiveness is measured considering the performance of retrieval after adding the selected question to the retrieval model as well as the user answer. Five standard evaluation metrics are considered: mean reciprocal rank,null,null
,,,
279,"The choice of evaluation metrics is motivated by considering three different aspects of the task. We choose MRR to evaluate the effect of asking clarifying questions on ranking the first relevant document. We report P@1 and nDCG@1 to measure the performance for scenarios where the system is able to return only one result. This is often the case with voice-only conversational systems. Moreover, we report nDCG@5 and nDCG@20 as conventional ranking metrics to measure the impact of asking clarifying questions in a traditional Web search setting. Notice that nDCG@20 is the preferred evaluation metric for the ClueWeb collection due to the shallow pooling performed for relevance assessments [14, 26].",null,null
,,,
280,Statistical test. We determine statistically significant differences using the two-tailed paired t-test with Bonferroni correction at a 99.9% confidence interval,null,null
,,,
281,Compared methods. We compare the performance of our question retrieval and selection models with the following methods:,null,null
,,,
282,,null,null
,,,
283,480,null,null
,,,
284,,null,null
,,,
285,Session 5A: Conversation and Dialog,null,null
,,,
286,,null,null
,,,
287,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
288,,null,null
,,,
289,Table 3: Performance comparison with baselines. WorstQuestion and BestQuestion respectively determine the lower and upper bounds. The superscript * denotes statistically significant differences compared to all the baselines,null,null
,,,
290,,null,null
,,,
291,Method,null,null
,,,
292,OriginalQuery  -QPP LambdaMART RankNet NeuQS,null,null
,,,
293,,null,null
,,,
294,Qulac-T Dataset,null,null
,,,
295,,null,null
,,,
296,MRR P@1 nDCG@1 nDCG@5 nDCG@20,null,null
,,,
297,,null,null
,,,
298,0.2715 0.3570 0.3558 0.3573,null,null
,,,
299,0.3625*,null,null
,,,
300,,null,null
,,,
301,0.1842 0.2548 0.2537 0.2562,null,null
,,,
302,0.2664*,null,null
,,,
303,,null,null
,,,
304,0.1381 0.1960 0.1945 0.1979,null,null
,,,
305,0.2064*,null,null
,,,
306,,null,null
,,,
307,0.1451 0.1938 0.1940 0.1943,null,null
,,,
308,0.2013*,null,null
,,,
309,,null,null
,,,
310,0.1470 0.1812 0.1796 0.1804,null,null
,,,
311,0.1862*,null,null
,,,
312,,null,null
,,,
313,MRR,null,null
,,,
314,0.2715 0.3570 0.3501 0.3568 0.3641*,null,null
,,,
315,,null,null
,,,
316,Qulac-F Dataset,Y,null
,,,
317,,null,null
,,,
318,P@1 nDCG@1,null,null
,,,
319,,null,null
,,,
320,0.1842 0.2548 0.2478 0.2559,null,null
,,,
321,0.2682*,null,null
,,,
322,,null,null
,,,
323,0.1381 0.1960 0.1911 0.1986,null,null
,,,
324,0.2110*,null,null
,,,
325,,null,null
,,,
326,nDCG@5,null,null
,,,
327,0.1451 0.1938 0.1896 0.1944 0.2018*,null,null
,,,
328,,null,null
,,,
329,nDCG@20,null,null
,,,
330,0.1470 0.1812 0.1773 0.1809 0.1867*,null,null
,,,
331,,null,null
,,,
332,WorstQuestion 0.2479 0.1451 BestQuestion 0.4673 0.3815,null,null
,,,
333,,null,null
,,,
334,0.1075 0.3031,null,null
,,,
335,,null,null
,,,
336,0.1402 0.2410,null,null
,,,
337,,null,null
,,,
338,0.1483 0.2077,null,null
,,,
339,,null,null
,,,
340,0.2479 0.4673,null,null
,,,
341,,null,null
,,,
342,0.1451 0.3815,null,null
,,,
343,,null,null
,,,
344,0.1075 0.3031,null,null
,,,
345,,null,null
,,,
346,0.1402 0.2410,null,null
,,,
347,,null,null
,,,
348,0.1483 0.2077,null,null
,,,
349,,null,null
,,,
350,"· Question retrieval: ­ BM25, RM3, QL: we index all the questions using Galago.9 Then, for a given query we retrieve the documents using BM25 [38], RM3 [25], and QL [30] models. ­ LambdaMART, RankNet: for every query-question pair, we use the scores obtained by BM25, RM3, and QL as features to train LambdaMART [48] and RankNet [10] implemented in RankLib.10 For every query, we consider all irrelevant questions as negative samples.",null,null
,,,
351,· Question selection: ­ OriginalQuery reports the performance of the document retrieval model only with the original query,null,null
,,,
352,9 https://sourceforge.net/p/lemur/galago/ 10 https://sourceforge.net/p/lemur/wiki/RankLib/,null,null
,,,
353,,null,null
,,,
354,MRR,null,null
,,,
355,,null,null
,,,
356,1.0 0.5 0.0 -0.5,null,null
,,,
357,,null,null
,,,
358,faceted ambiguous nav.,null,null
,,,
359,,null,null
,,,
360,inf.,null,null
,,,
361,,null,null
,,,
362,1,null,null
,,,
363,,null,null
,,,
364,2,null,null
,,,
365,,null,null
,,,
366,3,null,null
,,,
367,,null,null
,,,
368,4,null,null
,,,
369,,null,null
,,,
370,5 10,null,null
,,,
371,,null,null
,,,
372,topic type,null,null
,,,
373,,null,null
,,,
374,facet type,null,null
,,,
375,,null,null
,,,
376,# query terms,null,null
,,,
377,,null,null
,,,
378,"Figure 3: Impact of topic type, facet type, and query length",null,null
,,,
379,,null,null
,,,
380,"on the performance of BestQuestion oracle model, com-",null,null
,,,
381,,null,null
,,,
382,pared to OriginalQuery.,null,null
,,,
383,,null,null
,,,
384,Note that the retrieval scores are calculated knowing the selected question and its answer,null,null
,,,
385,,null,null
,,,
386,6.2 Results and Discussion,null,null
,,,
387,,null,null
,,,
388,"Question retrieval. Table 2 shows the results of question retrieval for all the topics. As we see, BERT-LeaQuR is able to outperform all baselines. It is worth noting that the model's performance gets better as the number of retrieved documents increases. This indicates that BERT-LeaQuR is able to capture the relevance of query and questions when they lack common terms. In fact, we see that all term-matching retrieval models such as BM25 are significantly outperformed in terms of all evaluation metrics.",null,null
,,,
389,"Oracle question selection: performance. Here we study the performance of an oracle model, i.e. assuming that an oracle model is aware of the answers to the questions. The goal is to show to what extent clarifying questions can improve the performance of a retrieval system. As we see in the lower rows of Table 3 selecting best questions",null,null
,,,
390,,null,null
,,,
391,Oracle question selection: impact of topic type and length. We analyze the performance of BestQuestion based on the number,null,null
,,,
392,,null,null
,,,
393,481,null,null
,,,
394,,null,null
,,,
395,Session 5A: Conversation and Dialog,null,null
,,,
396,,null,null
,,,
397,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
398,,null,null
,,,
399,NeuQS,null,null
,,,
400,,null,null
,,,
401,LambdaMART,null,null
,,,
402,,null,null
,,,
403,RankNet,null,null
,,,
404,,null,null
,,,
405,#NAME?,null,null
,,,
406,,null,null
,,,
407,OriginalQuery,null,null
,,,
408,,null,null
,,,
409,Qulac-T,null,null
,,,
410,0.36 0.34,null,null
,,,
411,,null,null
,,,
412,Qulac-F 0.36,null,null
,,,
413,,null,null
,,,
414,Qulac-T,Y,null
,,,
415,0.22 0.21 0.20 0.19,null,null
,,,
416,,null,null
,,,
417,Qulac-F,Y,null
,,,
418,,null,null
,,,
419,Qulac-T,Y,null
,,,
420,0.19,null,null
,,,
421,0.18,null,null
,,,
422,,null,null
,,,
423,Qulac-F,Y,null
,,,
424,,null,null
,,,
425,MRR,null,null
,,,
426,MRR nDCG@1 nDCG@20,null,null
,,,
427,,null,null
,,,
428,0.32,null,null
,,,
429,,null,null
,,,
430,0.34,null,null
,,,
431,,null,null
,,,
432,0.18,null,null
,,,
433,,null,null
,,,
434,0.17,null,null
,,,
435,,null,null
,,,
436,0.17,null,null
,,,
437,,null,null
,,,
438,0.3,null,null
,,,
439,,null,null
,,,
440,0.32,null,null
,,,
441,,null,null
,,,
442,0.28,null,null
,,,
443,,null,null
,,,
444,1,null,null
,,,
445,,null,null
,,,
446,2,null,null
,,,
447,,null,null
,,,
448,31,null,null
,,,
449,,null,null
,,,
450,0.302,null,null
,,,
451,,null,null
,,,
452,3,null,null
,,,
453,,null,null
,,,
454,# conversation turns,null,null
,,,
455,,null,null
,,,
456,# conversation turns,null,null
,,,
457,,null,null
,,,
458,0.16 0.15,null,null
,,,
459,,null,null
,,,
460,1,null,null
,,,
461,,null,null
,,,
462,2,null,null
,,,
463,,null,null
,,,
464,31,null,null
,,,
465,,null,null
,,,
466,2,null,null
,,,
467,,null,null
,,,
468,3,null,null
,,,
469,,null,null
,,,
470,# conversation turns,null,null
,,,
471,,null,null
,,,
472,# conversation turns,null,null
,,,
473,,null,null
,,,
474,0.16,null,null
,,,
475,,null,null
,,,
476,1,null,null
,,,
477,,null,null
,,,
478,2,null,null
,,,
479,,null,null
,,,
480,31,null,null
,,,
481,,null,null
,,,
482,2,null,null
,,,
483,,null,null
,,,
484,3,null,null
,,,
485,,null,null
,,,
486,# conversation turns,null,null
,,,
487,,null,null
,,,
488,# conversation turns,null,null
,,,
489,,null,null
,,,
490,Figure 4: Performance0.c28omparison with the baselines for different number of conversation turns,null,null
,,,
491,,null,null
,,,
492,1,null,null
,,,
493,,null,null
,,,
494,2,null,null
,,,
495,,null,null
,,,
496,3,null,null
,,,
497,,null,null
,,,
498,of query terms and topic type. We see tha#t tchoenrveelrastaitvioenimtuprrnosvement,null,null
,,,
499,,null,null
,,,
500,Impact of clarifying questions on facets. We study the differ-,null,null
,,,
501,,null,null
,,,
502,of BestQuestion is negatively correlated with the number of query,null,null
,,,
503,,null,null
,,,
504,ence of MRR between NeuQS and OriginalQuery on all facets. Note,null,null
,,,
505,,null,null
,,,
506,terms,null,null
,,,
507,,null,null
,,,
508,that for every facet we average the performance of NeuQS at dif-,null,null
,,,
509,,null,null
,,,
510,"queries require clarification in more cases. Also, comparing the",null,null
,,,
511,,null,null
,,,
512,ferent conversation turns. Our goal is to see how many facets are,null,null
,,,
513,,null,null
,,,
514,topic types,null,null
,,,
515,,null,null
,,,
516,impacted positively by asking clarifying questions. NeuQS is im-,null,null
,,,
517,,null,null
,,,
518,in the relative improvement. The average MRR for ambiguous,null,null
,,,
519,,null,null
,,,
520,proves the effectiveness of retrieval by selecting relevant questions,null,null
,,,
521,,null,null
,,,
522,"topics is 0.3858, compared with the faceted topics with average",null,null
,,,
523,,null,null
,,,
524,for a considerable number of facets on both data splits. In partic-,null,null
,,,
525,,null,null
,,,
526,MRR of 0.2898. The difference was statistically significant,null,null
,,,
527,,null,null
,,,
528,"ular, the performance for 45% of the facets is improved by asking",null,null
,,,
529,,null,null
,,,
530,"ANOVA, p  0.001).",null,null
,,,
531,,null,null
,,,
532,"clarifying questions, whereas the performance for 19% is worse.",null,null
,,,
533,,null,null
,,,
534,"Question selection. Table 3 presents the results of the document retrieval model taking into account a selected question together with its answer. We see that all models outperform OriginalQuery, confirming that asking clarifying questions is crucial in a conversation, leading to high performance gain. For instance, compared to OriginalQuery, a model as simple as  -QPP achieves a 31% relative improvement in terms of MRR. Also, NeuQS consistently outperforms all the baselines in terms of all evaluation metrics on both data splits. All the improvements are statistically significant. Moreover, NeuQS achieves a remarkable improvement in terms of both P@1 and nDCG@1. These two evaluation metrics are particularly important for voice-only conversational systems where the system must return only one result to the user. The obtained improvements highlight the necessity and effectiveness of asking clarifying questions in a conversational search system, where they are perceived as natural means of interactions with users.",null,null
,,,
535,Impact of data splits. We compare the performance of models on both Qulac-T and Qulac-F data splits. We see that the LTR baselines perform worse on Qulac-F. Notice that the performance difference of LambdaMART among the splits is statistically significant in terms of all evaluation metrics,null,null
,,,
536,Impact of number of conversation turns. Figure 4 shows the performance of NeuQS as well as the baselines for different conversation turns. We evaluate different models at k turns,null,null
,,,
537,,null,null
,,,
538,"Case study: failure and success analysis. Finally, we analyze representative cases of failure and success of our proposed framework. We list three cases where selecting questions using NeuQS improves the retrieval performance, as well as three other examples in which the selected questions lead to decreased performance. MRR reports the difference of the performance of NeuQS and OriginalQuery in terms of MRR. As we see, the first three examples show the selected questions that hurt the performance",null,null
,,,
539,"As for the success examples, we have listed three types. The first example",null,null
,,,
540,,null,null
,,,
541,482,null,null
,,,
542,,null,null
,,,
543,Session 5A: Conversation and Dialog,null,null
,,,
544,,null,null
,,,
545,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
546,,null,null
,,,
547,Table 4: Failure and success examples of NeuQS. Failure and success are measured by the difference in performance of NeuQS and OriginalQuery in terms of MRR,null,null
,,,
548,,null,null
,,,
549,Query,null,null
,,,
550,,null,null
,,,
551,Facet Description,null,null
,,,
552,,null,null
,,,
553,Selected Question,null,null
,,,
554,,null,null
,,,
555,User's Answer,null,null
,,,
556,,null,null
,,,
557,dog heat,null,null
,,,
558,,null,null
,,,
559,"What is the effect of excessive heat Would you like to know how to care No, I want to know what happens",null,null
,,,
560,,null,null
,,,
561,on dogs?,null,null
,,,
562,,null,null
,,,
563,for your dog during heat?,null,null
,,,
564,,null,null
,,,
565,when a dog is too hot.,null,null
,,,
566,,null,null
,,,
567,"sit and reach How is the sit and reach test prop- Do you want to know how to per- Yes, I do.",null,null
,,,
568,,null,null
,,,
569,test,null,null
,,,
570,,null,null
,,,
571,erly done?,null,null
,,,
572,,null,null
,,,
573,form this test?,null,null
,,,
574,,null,null
,,,
575,alexian broth- Find Alexian Brothers hospitals. ers hospital,null,null
,,,
576,,null,null
,,,
577,"Are you looking for our schedule of No, I don't need that. classes or events?",null,null
,,,
578,,null,null
,,,
579,MRR -0.86,null,null
,,,
580,-0.75,null,null
,,,
581,-0.54,null,null
,,,
582,,null,null
,,,
583,east ridge high Information about the sports pro- What information about East Ridge I'm looking for information about,null,null
,,,
584,,null,null
,,,
585,school,null,null
,,,
586,,null,null
,,,
587,gram at East Ridge High School in High School are you looking for? their sports program.,null,null
,,,
588,,null,null
,,,
589,"Clermont, Florida",null,null
,,,
590,,null,null
,,,
591,euclid,null,null
,,,
592,,null,null
,,,
593,Find information on the Greek Do you want a biography?,null,null
,,,
594,,null,null
,,,
595,Yes.,null,null
,,,
596,,null,null
,,,
597,mathematician Euclid.,null,null
,,,
598,,null,null
,,,
599,"rocky moun- Who are the sports reporters for the Would you like to read recent news No, I just want a list of the reporters",null,null
,,,
600,,null,null
,,,
601,tain news,null,null
,,,
602,,null,null
,,,
603,Rocky Mountain News?,null,null
,,,
604,,null,null
,,,
605,about the Rocky Mountain News? who write the sports for the Rocky,null,null
,,,
606,,null,null
,,,
607,Mountain News.,null,null
,,,
608,,null,null
,,,
609,2.77,null,null
,,,
610,,null,null
,,,
611,of employing a language-representation-based question retrieval model,null,null
,,,
612,7 LIMITATIONS AND FUTURE WORK,null,null
,,,
613,"Every data collection comes with some limitations. The same is valid for Qulac. First, the dataset was not collected from actual conversations. This decision was mainly due to the unbalanced workload of the two conversation participants. In our crowdsourcing HITs, the task of question generation required nearly 10 times more effort compared to the task of question answering. This makes it challenging and more expensive to pair two workers as participants of the same conversation. There are some examples of this approach in the literature [11, 36]; however, they address the task of reading comprehension, a task that is considerably simpler than identifying topic facets. A possible future direction is to provide a limited number of pre-generated questions",null,null
,,,
614,"Furthermore, Qulac is built for single-turn conversations",Y,Qulac
,,,
615,,null,null
,,,
616,"search scenarios, where asking questions can potentially lead to more user engagement while doing exploratory search.",null,null
,,,
617,"In this work, our main focus was on question selection. There are various directions that can be explored in the future. One interesting problem is to explore various strategies of improving the performance of the document retrieval model as new information is added to the model. Moreover, we assumed the number of conversation turns to be fixed. Another interesting future direction is to model the system's confidence at every stage of the conversation so that the model is able to decide when to stop asking questions and present the result(s).",null,null
,,,
618,8 CONCLUSIONS,null,null
,,,
619,"In this work, we introduced the task of asking clarifying questions in open-domain information-seeking conversations. We proposed an evaluation methodology which enables offline evaluation of conversational systems with clarifying questions. Also, we constructed and released a new data collection called Qulac, consisting of 762 topic-facet pairs with over 10K question-answer pairs. We further presented a neural question selection model called NeuQS along with models on question and document retrieval. NeuQS was able to outperform the LTR baselines significantly. The experimental analysis provided many insights of the task. In particular, experiments on the oracle model demonstrated that asking only one good clarifying question leads to over 150% relative improvement in terms of P@1 and nDCG@1. Moreover, we observed that asking clarifying questions improves the model's performance for a substantial percentage of the facets. In some failure cases, we saw that a more effective document retrieval model can potentially improve the performance. Finally, we showed that, asking more clarifying questions leads to better results, once again confirming the effectiveness of asking clarifying questions in a conversational search system.",Y,Qulac
,,,
620,,null,null
,,,
621,483,null,null
,,,
622,,null,null
,,,
623,Session 5A: Conversation and Dialog,null,null
,,,
624,,null,null
,,,
625,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
626,,null,null
,,,
627,ACKNOWLEDGMENTS,null,null
,,,
628,This work was supported in part by the RelMobIR project of the,null,null
,,,
629,Swiss National Science Foundation,null,null
,,,
630,"for Intelligent Information Retrieval, and in part by NSF grant",null,null
,,,
631,"IIS-1715095. Any opinions, findings and conclusions or recommen-",null,null
,,,
632,dations expressed in this material are those of the authors and do,null,null
,,,
633,not necessarily reflect those of the sponsors.,null,null
,,,
634,REFERENCES,null,null
,,,
635,"[1] Mohammad Aliannejadi, Masoud Kiaeeha, Shahram Khadivi, and Saeed Shiry Ghidary. 2014. Graph-Based Semi-Supervised Conditional Random Fields For Spoken Language Understanding Using Unaligned Data. In ALTA. 98­103.",null,null
,,,
636,"[2] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. 2018. In Situ and Context-Aware Target Apps Selection for Unified Mobile Search. In CIKM. 1383­1392.",null,null
,,,
637,"[3] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. 2018. Target Apps Selection: Towards a Unified Search Framework for Mobile Devices. In SIGIR. 215­224.",null,null
,,,
638,[4] Omar Alonso and Maria Stone. 2014. Building a Query Log via Crowdsourcing. In SIGIR. 939­942.,null,null
,,,
639,"[5] Harald Aust, Martin Oerder, Frank Seide, and Volker Steinbiss. 1995. The Philips automatic train timetable information system. Speech Communication 17, 3-4",null,null
,,,
640,[6] Seyed Ali Bahrainian and Fabio Crestani. 2018. Augmentation of Human Memory: Anticipating Topics that Continue in the Next Meeting. In CHIIR. 150­159.,null,null
,,,
641,"[7] Nicholas J Belkin, Colleen Cool, Adelheit Stein, and Ulrich Thiel. 1995. Cases, scripts, and information-seeking strategies: On the design of interactive information retrieval systems. Expert systems with applications 9, 3",null,null
,,,
642,"[8] Jan R. Benetka, Krisztian Balog, and Kjetil Nørvåg. 2017. Anticipating Information Needs Based on Check-in Activity. In WSDM. 41­50.",null,null
,,,
643,"[9] Pavel Braslavski, Denis Savenkov, Eugene Agichtein, and Alina Dubatovka. 2017. What Do You Mean Exactly?: Analyzing Clarification Questions in CQA. In CHIIR. 345­348.",null,null
,,,
644,"[10] Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. 2005. Learning to rank using gradient descent. In ICML. 89­96.",null,null
,,,
645,"[11] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question Answering in Context. In EMNLP. 2174­2184.",null,null
,,,
646,"[12] Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards Conversational Recommender Systems. In KDD. 815­824.",null,null
,,,
647,"[13] Charles L. A. Clarke, Nick Craswell, and Ian Soboroff. 2009. Overview of the TREC 2009 Web Track. In TREC.",null,null
,,,
648,"[14] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Ellen M. Voorhees. 2011. Overview of the TREC 2011 Web Track. In TREC.",null,null
,,,
649,"[15] Charles L. A. Clarke, Nick Craswell, and Ellen M. Voorhees. 2012. Overview of the TREC 2012 Web Track. In TREC.",null,null
,,,
650,"[16] W. Bruce Croft and R. H. Thompson. 1987. I3R: A new approach to the design of document retrieval systems. JASIS 38, 6",null,null
,,,
651,"[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805",null,null
,,,
652,"[18] Yulan He and Steve J. Young. 2005. Semantic processing using the Hidden Vector State model. Computer Speech & Language 19, 1",null,null
,,,
653,"[19] Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS Spoken Language Systems Pilot Corpus. In HLT. 96­101.",null,null
,,,
654,"[20] Di Jiang, Kenneth Wai-Ting Leung, Lingxiao Yang, and Wilfred Ng. 2015. Query suggestion with diversification and personalization. Knowl.-Based Syst. 89",null,null
,,,
655,"[21] Makoto P. Kato and Katsumi Tanaka. 2016. To Suggest, or Not to Suggest for Queries with Diverse Intents: Optimizing Search Result Presentation. In WSDM. 133­142.",null,null
,,,
656,"[22] Johannes Kiesel, Arefeh Bahrami, Benno Stein, Avishek Anand, and Matthias Hagen. 2018. Toward Voice Query Clarification. In SIGIR. 1257­1260.",null,null
,,,
657,[23] Weize Kong and James Allan. 2013. Extracting query facets from search results. In SIGIR. 93­102.,null,null
,,,
658,"[24] John Lafferty and Chengxiang Zhai. 2001. Document Language Models, Query Models, and Risk Minimization for Information Retrieval. In SIGIR. 111­119.",null,null
,,,
659,[25] Victor Lavrenko and W. Bruce Croft. 2001. Relevance-Based Language Models. In SIGIR. 120­127.,null,null
,,,
660,"[26] Xiaolu Lu, Alistair Moffat, and J. Shane Culpepper. 2016. The effect of pooling and evaluation depth on IR metrics. Inf. Retr. Journal 19, 4",null,null
,,,
661,"[27] Harshith Padigela, Hamed Zamani, and W. Bruce Croft. 2019. Investigating the Successes and Failures of BERT for Passage Re-Ranking. arXiv:1903.06902",null,null
,,,
662,,null,null
,,,
663,[28] Joaquín Pérez-Iglesias and Lourdes Araujo. 2010. Standard Deviation as a Query Hardness Estimator. In SPIRE. 207­212.,null,null
,,,
664,"[29] Roberto Pieraccini, Evelyne Tzoukermann, Z. Gorelov, Jean-Luc Gauvain, Esther Levin, Chin-Hui Lee, and Jay Wilpon. 1992. A speech understanding system based on statistical representation of semantics. In ICASSP. 193­196.",null,null
,,,
665,[30] Jay M. Ponte and W. Bruce Croft. 1998. A Language Modeling Approach to Information Retrieval. In SIGIR. 275­281.,null,null
,,,
666,"[31] Minghui Qiu, Liu Yang, Feng Ji, Wei Zhou, Jun Huang, Haiqing Chen, W. Bruce Croft, and Wei Lin. 2018. Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce. In ACL",null,null
,,,
667,"[32] Chen Qu, Liu Yang, W. Bruce Croft, Johanne R. Trippas, Yongfeng Zhang, and Minghui Qiu. 2018. Analyzing and Characterizing User Intent in Informationseeking Conversations. In SIGIR. 989­992.",null,null
,,,
668,[33] Filip Radlinski and Nick Craswell. 2017. A Theoretical Framework for Conversational Search. In CHIIR. 117­126.,null,null
,,,
669,[34] Sudha Rao and Hal Daumé. 2018. Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information. In ACL,null,null
,,,
670,[35] Sudha Rao and Hal Daumé III. 2019. Answer-based Adversarial Training for Generating Clarification Questions. arXiv:1904.02281,null,null
,,,
671,"[36] Siva Reddy, Danqi Chen, and Christopher D. Manning. 2018. CoQA: A Conversational Question Answering Challenge. arXiv:1808.07042",null,null
,,,
672,"[37] Gary Ren, Xiaochuan Ni, Manish Malik, and Qifa Ke. 2018. Conversational Query Understanding Using Sequence to Sequence Modeling. In WWW. 1715­1724.",null,null
,,,
673,"[38] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In TREC. 109­126.",null,null
,,,
674,"[39] Damiano Spina, Johanne R. Trippas, Lawrence Cavedon, and Mark Sanderson. 2017. Extracting audio summaries to support effective spoken document search. JASIST 68, 9",null,null
,,,
675,[40] Yueming Sun and Yi Zhang. 2018. Conversational Recommender System. In SIGIR. 235­244.,null,null
,,,
676,"[41] Zhiliang Tian, Rui Yan, Lili Mou, Yiping Song, Yansong Feng, and Dongyan Zhao. 2017. How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models. In ACL",null,null
,,,
677,"[42] Johanne R. Trippas, Damiano Spina, Lawrence Cavedon, Hideo Joho, and Mark Sanderson. 2018. Informing the Design of Spoken Conversational Search: Perspective Paper. In CHIIR. 32­41.",null,null
,,,
678,"[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. arXiv:1706.03762",null,null
,,,
679,"[44] Alexandra Vtyurina, Denis Savenkov, Eugene Agichtein, and Charles L. A. Clarke. 2017. Exploring Conversational Search With Humans, Assistants, and Wizards. In CHI Extended Abstracts. 2187­2193.",null,null
,,,
680,"[45] Marilyn A. Walker, Rebecca J. Passonneau, and Julie E. Boland. 2001. Quantitative and Qualitative Evaluation of Darpa Communicator Spoken Dialogue Systems. In ACL. 515­522.",null,null
,,,
681,"[46] Yansen Wang, Chenyi Liu, Minlie Huang, and Liqiang Nie. 2018. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders. In ACL",null,null
,,,
682,"[47] Jason D. Williams, Antoine Raux, Deepak Ramachandran, and Alan W. Black. 2013. The Dialog State Tracking Challenge. In SIGDIAL. 404­413.",null,null
,,,
683,"[48] Qiang Wu, Christopher J. C. Burges, Krysta Marie Svore, and Jianfeng Gao. 2010. Adapting boosting for information retrieval measures. Inf. Retr. 13, 3",null,null
,,,
684,"[49] Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to Respond with Deep Neural Networks for Retrieval-Based Human-Computer Conversation System. In SIGIR. 55­64.",null,null
,,,
685,"[50] Rui Yan, Dongyan Zhao, and Weinan E. 2017. Joint Learning of Response Ranking and Next Utterance Suggestion in Human-Computer Conversation System. In SIGIR. 685­694.",null,null
,,,
686,"[51] Liu Yang, Hamed Zamani, Yongfeng Zhang, Jiafeng Guo, and W. Bruce Croft. 2017. Neural Matching Models for Question Retrieval and Next Question Prediction in Conversation. arXiv:1707.05409",null,null
,,,
687,"[52] Chengxiang Zhai and John Lafferty. 2017. A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval. SIGIR Forum 51, 2",null,null
,,,
688,"[53] Yongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, and W. Bruce Croft. 2018. Towards Conversational Search and Recommendation: System Ask, User Respond. In CIKM. 177­186.",null,null
,,,
689,,null,null
,,,
690,484,null,null
,,,
691,,null,null
,,,
692,,null,null

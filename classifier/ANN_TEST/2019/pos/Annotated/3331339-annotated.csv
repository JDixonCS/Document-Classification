,sentence,label,data
,,,
0,Short Research Papers 2C: Search,null,null
,,,
1,,null,null
,,,
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
3,,null,null
,,,
4,The Impact of Score Ties on Repeatability in Document Ranking,null,null
,,,
5,,null,null
,,,
6,Jimmy Lin1 and Peilin Yang,null,null
,,,
7,"1 David R. Cheriton School of Computer Science, University of Waterloo jimmylin@uwaterloo.ca",null,null
,,,
8,,null,null
,,,
9,ABSTRACT,null,null
,,,
10,"Document ranking experiments should be repeatable. However, the interaction between multi-threaded indexing and score ties during retrieval may yield non-deterministic rankings, making repeatability not as trivial as one might imagine. In the context of the open-source Lucene search engine, score ties are broken by internal document ids, which are assigned at index time. Due to multi-threaded indexing, which makes experimentation with large modern document collections practical, internal document ids are not assigned consistently between different index instances of the same collection, and thus score ties are broken unpredictably. This short paper examines the effectiveness impact of such score ties, quantifying the variability that can be attributed to this phenomenon. The obvious solution to this non-determinism and to ensure repeatable document ranking is to break score ties using external collection document ids. This approach, however, comes with measurable efficiency costs due to the necessity of consulting external identifiers during query evaluation.",null,null
,,,
11,ACM Reference Format: Jimmy Lin and Peilin Yang. 2019. The Impact of Score Ties on Repeatability in Document Ranking. In 42nd Int'l ACM SIGIR Conference on Research and Development in Information Retrieval,null,null
,,,
12,1 INTRODUCTION,null,null
,,,
13,"It should generate no controversy to assert that repeatability of document ranking experiments in information retrieval research is a desirable property. To be precise, running the same ranking model over the same document collection with the same queries should yield the same output every time. Yet, this simple property is not trivial to achieve in modern retrieval engines that take advantage of multi-threaded indexing. In this paper, we explore corner cases that yield non-repeatable rankings: observed non-determinism is attributable to score ties, or documents in the collection that receive the same score with respect to a particular ranking model.",null,null
,,,
14,"Anserini, an open-source information retrieval toolkit built on Lucene [10, 11], provides the context for our study. The system evolved from previous IR reproducibility experiments [2, 5] where",null,null
,,,
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331339",null,null
,,,
16,,null,null
,,,
17,"Lucene exhibited a good balance between efficiency and effectiveness compared to other open-source search engines. A large user and developer base, numerous commercial deployments at scale, as well as a vibrant ecosystem provide additional compelling arguments for building an IR research toolkit on top of Lucene.",null,null
,,,
18,"The multi-threaded indexer that Anserini implements on top of Lucene is able to rapidly index large modern document collections-- for example, building a simple non-positional index on the popular ClueWeb small collections each takes around an hour on a typical server [11]. A consequence of the multi-threaded design is that documents are inserted into the index in a non-deterministic order, which means that different index instances over the same collection may be substantively different. This has implications for documents that receive the same score at retrieval time--by default, the Lucene query evaluation algorithm breaks ties by an internal document id, which is assigned based on document insertion order. Since these internal document ids are not stable across different index instances, document ranking experiments may not be repeatable.",Y,ClueWeb
,,,
19,"Is this a big deal? We argue yes, from a number of perspectives. While arbitrary tie-breaking behavior has a relatively small impact on simple ""bag of words"" queries",null,null
,,,
20,The goal of this paper and our contribution is a detailed study of the impact of score ties from the perspective of repeatability across a number of different test collections for a specific search engine. We empirically characterize differences in effectiveness that can be attributed to arbitrary interleaving of documents ingested during multi-threaded indexing. The solution to repeatable document ranking is fairly obvious: ties should be broken deterministically by external collection document ids,null,null
,,,
21,2 EXPERIMENTAL DESIGN,null,null
,,,
22,"All experiments in this paper were conducted with Anserini v0.1.0, which is based on Lucene 6.3.0; all code necessary to replicate our experiments are available on GitHub at http://anserini.io/.",null,null
,,,
23,,null,null
,,,
24,1125,null,null
,,,
25,,null,null
,,,
26,Short Research Papers 2C: Search,null,null
,,,
27,,null,null
,,,
28,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
29,,null,null
,,,
30,"To examine the impact of score ties across a diverse range of document types, we considered three newswire collections, two tweet collections, and two web collections:",null,null
,,,
31,"· TREC 2004 Robust Track, on TREC Disks 4 & 5. · TREC 2005 Robust Track, on the AQUAINT Corpus. · TREC 2017 Common Core Track, on the New York Times Anno-",Y,TREC
,,,
32,tated Corpus. · TREC 2011/2012 Microblog Tracks,Y,TREC
,,,
33,tion) and TREC 2013/2014 Microblog Tracks,Y,TREC
,,,
34,"For each document collection, we used Anserini to build five separate indexes from scratch. For each index, we performed a retrieval run using topics from the corresponding test collections. In each of these runs, Anserini used Lucene's default tie-breaking technique based on arbitrarily-assigned internal document ids--which as we have noted above, is not consistent between index instances due to multi-threading. Differences in effectiveness between these runs quantify the impact of score ties.",null,null
,,,
35,"In Anserini, we have modified the query evaluation algorithm to use the external collection id to break score ties, which means that retrieval results are repeatable across different index instances. This is accomplished via the Sort abstraction in Lucene, which allows the developer to specify how ranked lists are sorted. Naturally, the default is by the score produced by the ranking model. For newswire and web collections, we added lexicographic sort order of collection document ids as the tie breaker. For tweets, ties are broken by reverse chronological order",null,null
,,,
36,"With the exception of tweet collections, we considered the following ranking models: BM25 and query likelihood, and the RM3 query expansion technique applied to both. For tweet collections, we only considered query likelihood since BM25 is known not to be effective. In Anserini, RM3 is implemented as a two-stage process: a relevance model is estimated from documents in an initial ranked list, which then forms an expanded query that retrieves the final results. Thus, there are two source of variability due to score ties--when applying a rank cutoff in the initial retrieval as well as the final ranking.",null,null
,,,
37,"All runs retrieved up to 1000 hits and were evaluated in terms of standard retrieval metrics: for newswire and tweet collections, we computed average precision",null,null
,,,
38,,null,null
,,,
39,3 RESULTS,null,null
,,,
40,The results of our experiments on the newswire collections are shown in Table 1. Under the columns with the names of the metric,null,null
,,,
41,"We see that the variability attributed to tie-breaking behavior yields minor effectiveness differences, usually in the fourth decimal place, but sometimes in the third decimal place. Overall, observed variability in AP is smaller than P30 because for AP, the differences come from documents that straddle the rank cutoff of 1000, where score contributions are small. Results show that RM3 can exhibit",null,null
,,,
42,"In absolute terms, the observed score variability is fairly small. However, to put these differences in perspective, incremental advances in many popular NLP and IR shared tasks today, for example, SQuAD and MS MARCO, are measured in the third decimal place. Leaving aside whether such leaderboard-driven research is good for the community as a whole, we note that the amount of variability observed in our experiments can approach the magnitude of differences in successive advances in ""the state of the art"".",null,null
,,,
43,"More importantly, as argued in the introduction, this variability makes regression testing--which is a cornerstone of modern software development--very difficult. Typically, in regression testing, for floating point values the developer specifies a tolerance when comparing test results with expected results, for example, to compensate for precision errors. In our case, it is not clear how the developer should set this tolerance. A value too large would fail to catch genuine bugs, while a value too small would cause frequent needless failures, defeating the point of regression testing.",null,null
,,,
44,"As discussed in the previous section, the solution to repeatable document ranking is relatively straightforward--instead of depending on the internal document id to break score ties, we should use external",null,null
,,,
45,,null,null
,,,
46,1126,null,null
,,,
47,,null,null
,,,
48,Short Research Papers 2C: Search,null,null
,,,
49,,null,null
,,,
50,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
51,,null,null
,,,
52,Model,null,null
,,,
53,BM25 BM25+RM3 QL QL+RM3,null,null
,,,
54,,null,null
,,,
55,"TREC 2004 Robust Track topics, Disks 4 & 5",Y,TREC
,,,
56,,null,null
,,,
57,AP,null,null
,,,
58,,null,null
,,,
59,min--max,null,null
,,,
60,,null,null
,,,
61,P30,null,null
,,,
62,,null,null
,,,
63,min--max,null,null
,,,
64,,null,null
,,,
65,0.2501 0.2757 0.2468 0.2645,null,null
,,,
66,,null,null
,,,
67,0.2498 -- 0.2501 0.2756 -- 0.2757 0.2464 -- 0.2469 0.2643 -- 0.2644,null,null
,,,
68,,null,null
,,,
69,0.0003 0.0001 0.0005 0.0002,null,null
,,,
70,,null,null
,,,
71,0.3123 0.3256 0.3083 0.3153,null,null
,,,
72,,null,null
,,,
73,0.3120 -- 0.3124 0.3253 -- 0.3257 0.3076 -- 0.3080 0.3149 -- 0.3151,null,null
,,,
74,,null,null
,,,
75,,null,null
,,,
76,0.0004 0.0004 0.0007 0.0004,null,null
,,,
77,,null,null
,,,
78,Model,null,null
,,,
79,BM25 BM25+RM3 QL QL+RM3,null,null
,,,
80,,null,null
,,,
81,"TREC 2005 Robust Track topics, AQUAINT Collection",Y,TREC
,,,
82,,null,null
,,,
83,AP,null,null
,,,
84,,null,null
,,,
85,min--max,null,null
,,,
86,,null,null
,,,
87,P30,null,null
,,,
88,,null,null
,,,
89,min--max,null,null
,,,
90,,null,null
,,,
91,0.2003 0.2511 0.2026 0.2480,null,null
,,,
92,,null,null
,,,
93,0.2000 -- 0.2006 0.2506 -- 0.2513 0.2019 -- 0.2026 0.2471 -- 0.2483,null,null
,,,
94,,null,null
,,,
95,0.0006 0.0007 0.0005 0.0012,null,null
,,,
96,,null,null
,,,
97,0.3660 0.3873 0.3713 0.4007,null,null
,,,
98,,null,null
,,,
99,0.3660 -- 0.3673 0.3860 -- 0.3880 0.3693 -- 0.3720 0.4007 -- 0.4013,null,null
,,,
100,,null,null
,,,
101,,null,null
,,,
102,0.0013 0.0020 0.0027 0.0006,null,null
,,,
103,,null,null
,,,
104,"TREC 2017 Common Core Track topics, New York Times Collection",Y,TREC
,,,
105,,null,null
,,,
106,Model,null,null
,,,
107,,null,null
,,,
108,AP,null,null
,,,
109,,null,null
,,,
110,min--max,null,null
,,,
111,,null,null
,,,
112,P30,null,null
,,,
113,,null,null
,,,
114,min--max,null,null
,,,
115,,null,null
,,,
116,,null,null
,,,
117,,null,null
,,,
118,BM25 BM25+RM3 QL QL+RM3,null,null
,,,
119,,null,null
,,,
120,0.1996 0.2633 0.1928 0.2409,null,null
,,,
121,,null,null
,,,
122,0.1997 -- 0.1998 0.2632 -- 0.2635 0.1929 -- 0.1929 0.2408 -- 0.2409,null,null
,,,
123,,null,null
,,,
124,0.0002 0.0003 0.0001 0.0001,null,null
,,,
125,,null,null
,,,
126,0.4207 0.4893 0.4327 0.4647,null,null
,,,
127,,null,null
,,,
128,0.4213 -- 0.4220 0.4867 -- 0.4893 0.4327 -- 0.4333 0.4640 -- 0.4647,null,null
,,,
129,,null,null
,,,
130,0.0007 0.0026 0.0006 0.0007,null,null
,,,
131,,null,null
,,,
132,Table 1: Variability in effectiveness attributed to score ties on three newswire collections. The first column for each metric,null,null
,,,
133,,null,null
,,,
134,"TREC 2011 and 2012 Microblog Track topics, Tweets2011 Collection",Y,TREC
,,,
135,,null,null
,,,
136,Model,null,null
,,,
137,,null,null
,,,
138,AP,null,null
,,,
139,,null,null
,,,
140,min--max,null,null
,,,
141,,null,null
,,,
142,P30,null,null
,,,
143,,null,null
,,,
144,min--max,null,null
,,,
145,,null,null
,,,
146,,null,null
,,,
147,,null,null
,,,
148,QL,null,null
,,,
149,,null,null
,,,
150,0.2787 0.2761 -- 0.2770 0.0026 0.3673 0.3636 -- 0.3667 0.0037,null,null
,,,
151,,null,null
,,,
152,QL+RM3 0.3178 0.3157 -- 0.3173 0.0021 0.3954 0.3929 -- 0.3975 0.0046,null,null
,,,
153,,null,null
,,,
154,"TREC 2013 and 2014 Microblog Track topics, Tweets2013 Collection",Y,TREC
,,,
155,,null,null
,,,
156,Model,null,null
,,,
157,,null,null
,,,
158,AP,null,null
,,,
159,,null,null
,,,
160,min--max,null,null
,,,
161,,null,null
,,,
162,P30,null,null
,,,
163,,null,null
,,,
164,min--max,null,null
,,,
165,,null,null
,,,
166,,null,null
,,,
167,,null,null
,,,
168,QL,null,null
,,,
169,,null,null
,,,
170,0.3357 0.3345 -- 0.3348 0.0012 0.5429 0.5406 -- 0.5423 0.0023,null,null
,,,
171,,null,null
,,,
172,QL+RM3 0.3692 0.3695 -- 0.3699 0.0007 0.5484 0.5528 -- 0.5542 0.0058,null,null
,,,
173,,null,null
,,,
174,"Table 2: Variability in effectiveness attributed to score ties on tweet collections, organized in the same way as Table 1.",null,null
,,,
175,,null,null
,,,
176,"TREC 2010--2012 Web Track topics, ClueWeb09b",Y,"TREC, ClueWeb09b"
,,,
177,,null,null
,,,
178,Model,null,null
,,,
179,,null,null
,,,
180,NDCG@20,null,null
,,,
181,,null,null
,,,
182,min--max,null,null
,,,
183,,null,null
,,,
184,,null,null
,,,
185,,null,null
,,,
186,BM25 BM25+RM3 QL QL+RM3,null,null
,,,
187,,null,null
,,,
188,0.1407 0.1524 0.1211 0.1340,null,null
,,,
189,,null,null
,,,
190,0.1405 -- 0.1408 0.1524 -- 0.1525 0.1210 -- 0.1212 0.1340 -- 0.1342,null,null
,,,
191,,null,null
,,,
192,0.0003 0.0001 0.0002 0.0002,null,null
,,,
193,,null,null
,,,
194,"TREC 2013 and 2014 Web Track topics, ClueWeb12-B13",Y,"TREC, Web Track, ClueWeb12-B13"
,,,
195,,null,null
,,,
196,Model,null,null
,,,
197,,null,null
,,,
198,NDCG@20,null,null
,,,
199,,null,null
,,,
200,min--max,null,null
,,,
201,,null,null
,,,
202,,null,null
,,,
203,,null,null
,,,
204,BM25 BM25+RM3 QL QL+RM3,null,null
,,,
205,,null,null
,,,
206,0.1216 0.1080 0.1146 0.0920,null,null
,,,
207,,null,null
,,,
208,0.1216 -- 0.1216 0.1077 -- 0.1083 0.1146 -- 0.1154 0.0920 -- 0.0926,null,null
,,,
209,,null,null
,,,
210,0.0000 0.0006 0.0008 0.0006,null,null
,,,
211,,null,null
,,,
212,"Table 3: Variability in effectiveness attributed to score ties on web collections, organized in the same way as Table 1.",null,null
,,,
213,,null,null
,,,
214,"The efficiency costs of repeatable experiments are quantified in Table 4 for the three largest collections used in our experiments. Here, we report average query evaluation latency",null,null
,,,
215,"For simple bag-of-words queries, we observe a measurable slowdown in query latency, which quantifies the cost of repeatability. Across the web collections, this slowdown is approximately 20%, but for tweets the latency costs are a bit higher, most likely due to more prevalent score ties. Not surprisingly, query evaluation with RM3 is much slower due to its two-stage process: here, however, the behavior between tweet and web collections diverge. For web",null,null
,,,
216,,null,null
,,,
217,1127,null,null
,,,
218,,null,null
,,,
219,Short Research Papers 2C: Search,null,null
,,,
220,,null,null
,,,
221,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
222,,null,null
,,,
223,"TREC 2013 and 2014 Microblog Track topics, Tweets2013",Y,TREC
,,,
224,,null,null
,,,
225,Model Non-Repeatable Repeatable,null,null
,,,
226,,null,null
,,,
227,,null,null
,,,
228,,null,null
,,,
229,QL QL+RM3,null,null
,,,
230,,null,null
,,,
231,0.46s 1.05s,null,null
,,,
232,,null,null
,,,
233,0.58s +26% 2.00s +90%,null,null
,,,
234,,null,null
,,,
235,"TREC 2010--2012 Web Track topics, ClueWeb09b",Y,TREC 2010--2012
,,,
236,,null,null
,,,
237,Model,null,null
,,,
238,,null,null
,,,
239,Non-Repeatable Repeatable,null,null
,,,
240,,null,null
,,,
241,,null,null
,,,
242,,null,null
,,,
243,BM25 BM25+RM3,null,null
,,,
244,,null,null
,,,
245,0.18s 3.92s,null,null
,,,
246,,null,null
,,,
247,0.23s +23% 4.18s +7%,null,null
,,,
248,,null,null
,,,
249,"TREC 2013--2014 Web Track topics, ClueWeb12-B13",Y,TREC
,,,
250,,null,null
,,,
251,Model,null,null
,,,
252,,null,null
,,,
253,Non-Repeatable Repeatable,null,null
,,,
254,,null,null
,,,
255,,null,null
,,,
256,,null,null
,,,
257,BM25 BM25+RM3,null,null
,,,
258,,null,null
,,,
259,0.22s 4.23s,null,null
,,,
260,,null,null
,,,
261,0.26s +18% 4.61s +9%,null,null
,,,
262,,null,null
,,,
263,"Table 4: Latency differences between non-repeatable and repeatable document ranking, where repeatability is achieved by consistently breaking ties using external document ids.",null,null
,,,
264,,null,null
,,,
265,"collections, the slowdown is less compared to bag-of-words queries because a significant amount of time is spent reading document vectors from the index and estimating relevance models during query evaluation. As a result, the amount of time actually spent in the postings traversal inner loop is proportionally smaller. Tweets, however, are much shorter, and so estimating relevance models is relatively fast. The larger expanded queries require consulting more postings and scoring more documents, thus leading to large slowdowns for repeatable runs.",null,null
,,,
266,"We acknowledge that these results are implementation specific, tied to the exact mechanism by which external document ids are consulted. Our current implementation uses existing Lucene abstractions for controlling the sort order of results, but greater efficiencies might be possible with a more invasive modification of Lucene internals. Nevertheless, our broader point remains true: repeatability inevitably comes at some cost in performance.",null,null
,,,
267,,null,null
,,,
268,4 RELATED WORK,null,null
,,,
269,"We are, of course, not the first to have noticed score ties in document ranking and to examine their impact. Cabanac et al. [3] studied the behavior of the widely-used trec_eval tool",null,null
,,,
270,"While most of the above cited papers focus on the implications of scoring ties for IR evaluation, others have examined different aspects of the phenomenon. For example, Wu and Fang [9] used score",null,null
,,,
271,,null,null
,,,
272,"ties to prioritize relevance signals in document ranking. Z. Yang et al. [12] explored different levels of score rounding as a way to accelerate query processing--for example, taking advantage of approximate scoring regimes. This relates to impact quantization [1], and JASS [6] is an example of a recent system that exploits approximate scoring for anytime ranking.",null,null
,,,
273,"This study builds on previous work, but examines a new angle that to our knowledge has not been explored--the impact of score ties from the perspective of experimental repeatability. Recent ACM guidelines1 articulate the role of repeatability as an important foundation of scientific methodology with computational artifacts. Without repeatability, replicability and reproducibility are not possible. Building on this thread, our work makes a contribution towards firmly establishing repeatability in IR experiments using Lucene.",null,null
,,,
274,5 CONCLUSIONS,null,null
,,,
275,"The conclusions from our examination of score ties are fairly clear: Although absolute differences in effectiveness metrics are relatively small--in the third decimal place at the most--these differences nevertheless pose challenges for regression testing. Without rigorous regression testing, it is difficult to put progress on solid footing in terms of software engineering best practices, since developers cannot be certain if a new feature introduced a bug. Fortunately, the solution to repeatable runs is fairly straightforward, which we have implemented: score ties should be broken by external collection ids. However, this comes with a measurable efficiency cost in terms of slowdown in query evaluation. As a concrete recommendation for navigating this tradeoff, we suggest that non-repeatable runs are acceptable for prototyping, but any permanent contributions to a codebase must pass slower regression tests that make repeatability a requirement.",null,null
,,,
276,Acknowledgments. This work was supported in part by the Natural Sciences and Engineering Research Council,null,null
,,,
277,REFERENCES,null,null
,,,
278,"[1] V. Anh, O. de Kretser, and A. Moffat. 2001. Vector-Space Ranking with Effective Early Termination. In SIGIR. 35­42.",null,null
,,,
279,"[2] J. Arguello, M. Crane, F. Diaz, J. Lin, and A. Trotman. 2015. Report on the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results",null,null
,,,
280,"[3] G. Cabanac, G. Hubert, M. Boughanem, and C. Chrisment. 2010. Tie-Breaking Bias: Effect of an Uncontrolled Parameter on Information Retrieval Evaluation. In CLEF. 112­123.",null,null
,,,
281,[4] N. Ferro and G. Silvello. 2015. Rank-Biased Precision Reloaded: Reproducibility and Generalization. In ECIR. 768­780.,null,null
,,,
282,"[5] J. Lin, M. Crane, A. Trotman, J. Callan, I. Chattopadhyaya, J. Foley, G. Ingersoll, C. Macdonald, and S. Vigna. 2016. Toward Reproducible Baselines: The Open-Source IR Reproducibility Challenge. In ECIR. 408­420.",null,null
,,,
283,[6] J. Lin and A. Trotman. 2015. Anytime Ranking for Impact-Ordered Indexes. In ICTIR. 301­304.,null,null
,,,
284,[7] F. McSherry and M. Najork. 2008. Computing Information Retrieval Performance Measures Efficiently in the Presence of Tied Scores. In ECIR. 414­421.,null,null
,,,
285,"[8] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. 2011. Overview of the TREC-2011 Microblog Track. In TREC.",null,null
,,,
286,[9] H. Wu and H. Fang. 2013. Tie Breaker: A Novel Way of Combining Retrieval Signal. In ICTIR. 72­75.,null,null
,,,
287,"[10] P. Yang, H. Fang, and J. Lin. 2017. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In SIGIR. 1253­1256.",null,null
,,,
288,"[11] P. Yang, H. Fang, and J. Lin. 2018. Anserini: Reproducible Ranking Baselines Using Lucene. Journal of Data and Information Quality 10, 4",null,null
,,,
289,"[12] Z. Yang, A. Moffat, and A. Turpin. 2016. How Precise Does Document Scoring Need to Be? In AIRS. 279­291.",null,null
,,,
290,1 https://www.acm.org/publications/policies/artifact-review-badging,null,null
,,,
291,,null,null
,,,
292,1128,null,null
,,,
293,,null,null
,,,
294,,null,null

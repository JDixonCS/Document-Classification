,sentence,label,data
,,,
0,Short Research Papers 2C: Search,null,null
,,,
1,,null,null
,,,
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
3,,null,null
,,,
4,"Critically Examining the ""Neural Hype"": Weak Baselines and the Additivity of Effectiveness Gains from Neural Ranking Models",null,null
,,,
5,,null,null
,,,
6,"Wei Yang,1 Kuang Lu,2 Peilin Yang, and Jimmy Lin1",null,null
,,,
7,"1 David R. Cheriton School of Computer Science, University of Waterloo 2 Department of Electrical and Computer Engineering, University of Delaware",null,null
,,,
8,,null,null
,,,
9,ABSTRACT,null,null
,,,
10,"Is neural IR mostly hype? In a recent SIGIR Forum article, Lin expressed skepticism that neural ranking models were actually improving ad hoc retrieval effectiveness in limited data scenarios. He provided anecdotal evidence that authors of neural IR papers demonstrate ""wins"" by comparing against weak baselines. This paper provides a rigorous evaluation of those claims in two ways: First, we conducted a meta-analysis of papers that have reported experimental results on the TREC Robust04 test collection. We do not find evidence of an upward trend in effectiveness over time. In fact, the best reported results are from a decade ago and no recent neural approach comes close. Second, we applied five recent neural models to rerank the strong baselines that Lin used to make his arguments. A significant improvement was observed for one of the models, demonstrating additivity in gains. While there appears to be merit to neural IR approaches, at least some of the gains reported in the literature appear illusory.",Y,TREC Robust04
,,,
11,"ACM Reference Format: Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically Examining the ""Neural Hype"": Weak Baselines and the Additivity of Effectiveness Gains from Neural Ranking Models. In 42nd Int'l ACM SIGIR Conference on Research and Development in Information Retrieval",null,null
,,,
12,1 INTRODUCTION,null,null
,,,
13,"In a recent SIGIR Forum opinion piece, Lin [11] criticized the state of information retrieval research, making two main points. First, he lamented the ""neural hype"" and wondered that for ""classic"" ad hoc retrieval problems",null,null
,,,
14,"In this paper, we attempt a rigorous evaluation of these claims. Focusing specifically on the test collection from the TREC 2004 Robust Track, a meta-analysis of the literature shows no upward trend in reported effectiveness over time. The best reported results",Y,TREC
,,,
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331340",null,null
,,,
16,,null,null
,,,
17,"on the collection are from a decade ago, and no recent paper",null,null
,,,
18,"As a follow up, we applied a number of recent neural ranking models from the MatchZoo toolkit [5] to rerank the strong baselines that Lin used to make his arguments. Out of five neural models, one was able to significantly improve upon Lin's results. In other words, the effectiveness gains from one neural model is additive with respect to a strong baseline--which provides evidence that neural IR can lead to ""real"" improvements. Nevertheless, four out of the five models examined were not able to significantly beat the baseline, suggesting that gains attributable to neural approaches are not as widespread as the literature suggests. The absolute average precision values we report are among the highest for neural models that we are aware of, although in absolute terms they are still much lower than the best known results.",null,null
,,,
19,2 META-ANALYSIS,null,null
,,,
20,The broader context of Lin's article is a recent series of papers that reflects a general angst,null,null
,,,
21,"We began by conducting a meta-analysis to rigorously examine Lin's criticism. His argument specifically focused on document ranking models that could be trained with commonly-available evaluation resources; specifically, such models should not require behavioral log data. As he argued, the test collection from the TREC 2004 Robust Track",Y,TREC
,,,
22,"We exhaustively examined every publication from 2005 to 2018 in the following venues to identify those that reported results on Robust04: SIGIR, CIKM, WWW, ICTIR, ECIR, KDD, WSDM, TOIS, IRJ, IPM, and JASIST. This was supplemented by Google Scholar searches to identify a few additional papers not in the venues indicated above. Our meta-analysis was conducted in January 2019, but after the paper acceptance we included a few more papers. A number of exclusion criteria were applied, best characterized as discarding corner cases--for example, papers that only used a subset of the topics or papers that had metrics plotted in a graph. In total,",null,null
,,,
23,,null,null
,,,
24,1129,null,null
,,,
25,,null,null
,,,
26,Short Research Papers 2C: Search,null,null
,,,
27,,null,null
,,,
28,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
29,,null,null
,,,
30,AP,null,null
,,,
31,,null,null
,,,
32,0.400 0.375 0.350,null,null
,,,
33,,null,null
,,,
34,non-neural models neural models TREC best TREC median Anserini RM3,Y,TREC
,,,
35,,null,null
,,,
36,0.325,null,null
,,,
37,,null,null
,,,
38,0.3,null,null
,,,
39,,null,null
,,,
40,0.275,null,null
,,,
41,,null,null
,,,
42,0.25,null,null
,,,
43,,null,null
,,,
44,0.225,null,null
,,,
45,,null,null
,,,
46,0.2,null,null
,,,
47,,null,null
,,,
48,5,null,null
,,,
49,,null,null
,,,
50,6,null,null
,,,
51,,null,null
,,,
52,7,null,null
,,,
53,,null,null
,,,
54,8,null,null
,,,
55,,null,null
,,,
56,9,null,null
,,,
57,,null,null
,,,
58,10,null,null
,,,
59,,null,null
,,,
60,11,null,null
,,,
61,,null,null
,,,
62,12,null,null
,,,
63,,null,null
,,,
64,13,null,null
,,,
65,,null,null
,,,
66,14,null,null
,,,
67,,null,null
,,,
68,15,null,null
,,,
69,,null,null
,,,
70,16,null,null
,,,
71,,null,null
,,,
72,17,null,null
,,,
73,,null,null
,,,
74,18,null,null
,,,
75,,null,null
,,,
76,19,null,null
,,,
77,,null,null
,,,
78,Year,null,null
,,,
79,,null,null
,,,
80,"Figure 1: Visualization results on Robust04, where baseline and best AP scores are represented by empty and filled circles.",Y,Robust04
,,,
81,,null,null
,,,
82,"we examined 130 papers; of these, 109 papers contained extractable average precision values that formed the basis of the results reported below. Note that some papers did not report AP, and thus were excluded from consideration. All papers and associated data are publicly available for verification and further analysis.1",null,null
,,,
83,"For each of the 109 papers, we extracted the highest average precision score achieved on Robust04 by the authors' proposed methods, regardless of experimental condition",Y,Robust04
,,,
84,"A visualization of our meta-analysis is presented in Figure 1. For each paper, we show the baseline and the best result as an empty circle and a filled circle",null,null
,,,
85,Our meta-analysis shows that researchers still frequently compare against weak baselines: In 36 papers,null,null
,,,
86,1 https://github.com/lintool/robust04- analysis,null,null
,,,
87,,null,null
,,,
88,"The highest AP we encountered was by Cormack et al. [4] in 2009, at 0.3686. Across over a decade's worth of publications, we see no obvious upward trend in terms of effectiveness.",null,null
,,,
89,"Focusing specifically on the neural approaches, 8 out of 18 papers",null,null
,,,
90,It is noted that not all papers purport to advance retrieval effectiveness,null,null
,,,
91,3 EXAMINING ADDITIVITY,null,null
,,,
92,"Beyond revealing comparisons to weak baselines as widespread, Armstrong et al. [3] further examined why exactly this was methodologically problematic. Such comparisons lead to improvements that ""don't add up"" because of non-additive gains. The prototypical research paper on ad hoc retrieval proposes an innovation and compares it to a baseline that does not include the innovation; as expected, the innovation leads to increases in effectiveness. In this way, researchers collectively introduce dozens of different innovations, all of which improve on their respective baselines.",null,null
,,,
93,"The key question, however, is whether the effectiveness gains of these innovations are additive. This might not occur, for example, if they exploit the same relevance signals. To put more precisely, does an improvement over a weak baseline still hold if applied to a strong baseline? If the answer is no, then gains over weak baselines may be illusory, and from a methodological perspective, we should",null,null
,,,
94,,null,null
,,,
95,1130,null,null
,,,
96,,null,null
,,,
97,Short Research Papers 2C: Search,null,null
,,,
98,,null,null
,,,
99,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
100,,null,null
,,,
101,"not accept gains as ""real"" and ""meaningful"" unless they improve over strong baselines. Armstrong et al. [3] presented some evidence that many improvements are not additive, a finding which has been confirmed and expanded on by Kharazmi et al. [10]. However, the debate is not fully settled, as Akcay et al. [2] demonstrated additivity in search result diversification after better parameter tuning.",null,null
,,,
102,"In the second part of our study, we explicitly examine the additivity hypothesis with respect to recent neural ranking models. Specifically, we applied neural ranking models on top of the strong baselines that Lin used to make his arguments, which showed that a well-tuned implementation of query expansion based on RM3 [1] beats the average precision reported in two recent neural IR papers, anonymously referred to as ""Paper 1"" and ""Paper 2"".",null,null
,,,
103,3.1 Experimental Setup,null,null
,,,
104,"We began by replicating Lin's results with the Anserini toolkit [23], using exactly the same experimental settings",null,null
,,,
105,"On top of Lin's runs, we applied a number of neural ranking models from MatchZoo",null,null
,,,
106,"The neural models were deployed in a reranking setup, where the output of the models were linearly interpolated with scores from the RM3 baseline: score =  · scoreNN +",null,null
,,,
107,"Following established practice, all models were trained using only the documents in the baseline RM3 runs that appear in the Robust04 relevance judgments. We used word vectors pre-trained on the Google News corpus",Y,Robust04
,,,
108,2 http://anserini.io,null,null
,,,
109,,null,null
,,,
110,Condition,null,null
,,,
111,BM25 [7] DRMM [7],null,null
,,,
112,,null,null
,,,
113,AP,null,null
,,,
114,0.255 0.279,null,null
,,,
115,,null,null
,,,
116,2-fold results from Paper 1,null,null
,,,
117,,null,null
,,,
118,Paper 1,null,null
,,,
119,,null,null
,,,
120,0.2971,null,null
,,,
121,,null,null
,,,
122,BM25+RM3,null,null
,,,
123,,null,null
,,,
124,0.2987,null,null
,,,
125,,null,null
,,,
126,#NAME?,null,null
,,,
127,,null,null
,,,
128,0.2993,null,null
,,,
129,,null,null
,,,
130,#NAME?,null,null
,,,
131,,null,null
,,,
132,0.2988,null,null
,,,
133,,null,null
,,,
134,#NAME?,null,null
,,,
135,,null,null
,,,
136,0.3126,null,null
,,,
137,,null,null
,,,
138,#NAME?,null,null
,,,
139,,null,null
,,,
140,0.3033,null,null
,,,
141,,null,null
,,,
142,#NAME?,null,null
,,,
143,,null,null
,,,
144,0.3021,null,null
,,,
145,,null,null
,,,
146,5-fold results from Paper 2,null,null
,,,
147,,null,null
,,,
148,Paper 2,null,null
,,,
149,,null,null
,,,
150,0.272,null,null
,,,
151,,null,null
,,,
152,BM25+RM3,null,null
,,,
153,,null,null
,,,
154,0.3033,null,null
,,,
155,,null,null
,,,
156,#NAME?,null,null
,,,
157,,null,null
,,,
158,0.3026,null,null
,,,
159,,null,null
,,,
160,#NAME?,null,null
,,,
161,,null,null
,,,
162,0.2995,null,null
,,,
163,,null,null
,,,
164,#NAME?,null,null
,,,
165,,null,null
,,,
166,0.3152,null,null
,,,
167,,null,null
,,,
168,#NAME?,null,null
,,,
169,,null,null
,,,
170,0.3036,null,null
,,,
171,,null,null
,,,
172,#NAME?,null,null
,,,
173,,null,null
,,,
174,0.3051,null,null
,,,
175,,null,null
,,,
176,NDCG@20,null,null
,,,
177,0.418 0.431,null,null
,,,
178,0.4398 0.4467 0.4455 0.4646 0.4423 0.4471,null,null
,,,
179,0.4514 0.4491 0.4468 0.4718 0.4441 0.4502,null,null
,,,
180,,null,null
,,,
181,Table 1: Experimental results applying neural models to rerank a strong baseline;  indicates statistical significance.,null,null
,,,
182,,null,null
,,,
183,3.2 Results,null,null
,,,
184,"Our experimental results are shown in Table 1. Of all the neural models we examined in MatchZoo, only the original DRMM paper evaluated on Robust04; the first two rows show the DRMM results and their BM25 baseline",null,null
,,,
185,The second and third blocks of Table 1 report results from the two-fold and five-fold cross-validation conditions to match Paper 1 and Paper 2. Results from Paper 1 and Paper 2 are provided for reference,null,null
,,,
186,Experiments show that reranking our strong baseline with neural models yields small improvements in many cases.3 Statistical significance of metric differences was assessed using a paired twotailed t-test: the only observed significant difference is with DRMM,null,null
,,,
187,"3The reader might wonder how it is possible that a neural model actually makes results worse, since a setting of  = 1.0 would ignore the neural model scores. However, due to cross-validation, this may not be the learned parameter.",null,null
,,,
188,,null,null
,,,
189,1131,null,null
,,,
190,,null,null
,,,
191,Short Research Papers 2C: Search,null,null
,,,
192,,null,null
,,,
193,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
194,,null,null
,,,
195,3.3 Discussion,null,null
,,,
196,"We specifically tackle a number of shortcomings and limitations of our study. First, only the five models implemented in MatchZoo were examined, and the quality of those implementations might be questioned. We concede this point, and so our findings apply to only the MatchZoo implementations of the various neural models. Nevertheless, MatchZoo has gained broad acceptance in the community as a solid experimental platform on which to explore neural ranking tasks.",null,null
,,,
197,"The next obvious objection is that we've only examined these particular five neural ranking models. This, of course, is valid criticism, but an exhaustive study of all models would be impractical. We argue that the models selected are representative of the types of approaches pursued by researchers today, and that these results suffice to support at least some tentative generalizations.",null,null
,,,
198,"The next criticism we anticipate concerns our evidence combination method, simple linear interpolation of scores. While there are much more sophisticated approaches to integrating multiple relevance signals, this approach is commonly used [6, 16, 19, 24, 26]. In a separate experiment where we explicitly ignored the retrieval scores, effectiveness was significantly lower. We leave open the possibility of better evidence aggregation methods, but such future work does not detract from our findings here.",null,null
,,,
199,"Another possible criticism of our study is the limited data condition, since we are training with only TREC judgments. Surely, the plethora of training data that comes from behavioral logs must be considered. While we do not dispute the effectiveness of neural approaches given large amounts of data, exploring the range of data conditions under which those models work is itself interesting. We note a stark contrast here: for NLP tasks, researchers have been able to extract gains from neural approaches with only ""modest"" amounts of data",Y,TREC
,,,
200,"Finally, there is a modeling decision worth discussing: In our experiments, all models except for DRMM truncate the length of the input document to the first K tokens",null,null
,,,
201,,null,null
,,,
202,4 CONCLUSIONS,null,null
,,,
203,We believe that our study supports the following conclusions: At,null,null
,,,
204,"least with respect to the Robust04 test collection, it does not appear",Y,Robust04
,,,
205,that the IR community as a whole has heeded the admonishments,null,null
,,,
206,of Armstrong et al. [3] from a decade ago. Our meta-analysis shows,null,null
,,,
207,that comparisons to weak baselines still pervade the literature. The,null,null
,,,
208,high water mark on Robust04 in terms of average precision was,null,null
,,,
209,"actually set in 2009, and no reported results since then",null,null
,,,
210,otherwise) come close. Focusing specifically on five neural ranking,null,null
,,,
211,"models in MatchZoo, we find that only one is able to significantly",null,null
,,,
212,improve upon a well-tuned RM3 run in a reranking setup on this,null,null
,,,
213,"collection. That is, at least under this limited data scenario, effec-",null,null
,,,
214,tiveness gains from most neural ranking models do not appear to,null,null
,,,
215,be additive. While neural networks no doubt represent an exciting,null,null
,,,
216,"direction in information retrieval, we believe that at least some of",null,null
,,,
217,the gains reported in the literature are illusory.,null,null
,,,
218,Acknowledgments. This work was supported in part by the Natu-,null,null
,,,
219,ral Sciences and Engineering Research Council,null,null
,,,
220,REFERENCES,null,null
,,,
221,[1] Abdul-Jaleel et al. 2004. UMass at TREC 2004: Novelty and HARD. TREC. [2] Akcay et al. 2017. On the Additivity and Weak Baselines for Search Result,null,null
,,,
222,Diversification Research. ICTIR. [3] Armstrong et al. 2009. Improvements That Don't Add Up: Ad-hoc Retrieval,null,null
,,,
223,Results Since 1998. CIKM. [4] Cormack et al. 2009. Reciprocal Rank Fusion Outperforms Condorcet and Indi-,null,null
,,,
224,vidual Rank Learning Methods. SIGIR. [5] Fan et al. 2017. MatchZoo: A Toolkit for Deep Text Matching. arXiv:1707.07270. [6] Ganguly et al. 2015. Word Embedding Based Generalized Language Model for,null,null
,,,
225,Information Retrieval. SIGIR. [7] Guo et al. 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. CIKM. [8] Hu et al. 2014. Convolutional Neural Network Architectures for Matching Natural,null,null
,,,
226,Language Sentences. NIPS. [9] Huang et al. 2013. Learning Deep Structured Semantic Models for Web Search,null,null
,,,
227,"using Clickthrough Data. CIKM. [10] Kharazmi et al. 2016. Examining Additivity and Weak Baselines. TOSI 34, 4",null,null
,,,
228,"(2016), Article 23. [11] Lin. 2018. The Neural Hype and Comparisons Against Weak Baselines. SIGIR",null,null
,,,
229,"Forum 52, 2",null,null
,,,
230,arXiv:1807.03341. [13] MacAvaney et al. 2019. CEDR: Contextualized Embeddings for Document Rank-,null,null
,,,
231,ing. arXiv:1904.07094. [14] Mitra and Craswell. 2017. Neural Models for Information Retrieval.,null,null
,,,
232,arXiv:1705.01509. [15] Mitra et al. 2017. Learning to Match using Local and Distributed Representations,null,null
,,,
233,of Text for Web Search. WWW. [16] Rao et al. 2019. Multi-Perspective Relevance Matching with Hierarchical,null,null
,,,
234,"ConvNets for Social Media Search. AAAI. [17] Sculley et al. 2018. Winner's Curse? On Pace, Progress, and Empirical Rigor. ICLR",null,null
,,,
235,Workshops. [18] Shen et al. 2014. Learning Semantic Representations using Convolutional Neural,null,null
,,,
236,Networks for Web Search. WWW. [19] Van Gysel et al. 2018. Neural Vector Spaces for Unsupervised Information,null,null
,,,
237,"Retrieval. TOIS 36, 4",null,null
,,,
238,Positional Sentence Representations.. AAAI. [21] Xiong et al. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. SIGIR. [22] Yang et al. 2016. aNMM: Ranking Short Answer Texts with Attention-Based,null,null
,,,
239,Neural Matching Model. CIKM. [23] Yang et al. 2018. Anserini: Reproducible Ranking Baselines Using Lucene. JDIQ,null,null
,,,
240,"10, 4",null,null
,,,
241,NAACL. [25] Yang et al. 2019. Simple Applications of BERT for Ad Hoc Document Retrieval.,null,null
,,,
242,arXiv:1903.10972. [26] Zamani and Croft. 2016. Embedding-based query language models. ICTIR.,null,null
,,,
243,,null,null
,,,
244,1132,null,null
,,,
245,,null,null
,,,
246,,null,null

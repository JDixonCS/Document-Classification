,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Document Expansion Using External Collections,0,,False
3,Garrick Sherman,0,,False
4,School of Information Sciences University of Illinois at Urbana-Champaign,0,,False
5,gsherma2@illinois.edu,0,,False
6,ABSTRACT,0,,False
7,"Document expansion has been shown to improve the effectiveness of information retrieval systems by augmenting documents' term probability estimates with those of similar documents, producing higher quality document representations. We propose a method to further improve document models by utilizing external collections as part of the document expansion process. Our approach is based on relevance modeling, a popular form of pseudo-relevance feedback; however, where relevance modeling is concerned with query expansion, we are concerned with document expansion. Our experiments demonstrate that the proposed model improves ad-hoc document retrieval effectiveness on a variety of corpus types, with a particular benefit on more heterogeneous collections of documents.",1,ad-hoc,True
8,1 INTRODUCTION,1,DUC,True
9,"Relevance modeling is an extremely influential pseudo-relevance feedback technique in which we assume that both queries and documents are observations sampled from a relevance model (RM) [8], which is a probability distribution over terms in relevant documents. Because we do not have true relevance feedback, relevance modeling makes use of the query likelihood, P (Q |D), to quantify the degree to which words in each document should contribute to the final model R. However, since no document is perfectly representative of its underlying generative model, we may be reasonably concerned that our estimate of P (Q |D) is the result of chance. That is, there is no guarantee that D is a representative sample from R. The quality of our RM, therefore, may benefit from a higher quality document representation than that which is estimated from the text of D.",0,,False
10,"We employ two techniques to attempt to improve our document language models: document expansion and the use of external document collections. Expandeded documents are expected to exhibit less random variation in term frequencies, improving probability estimates. We hope that estimates may be further refined by expanding documents using external collections, thereby avoiding any term bias exhibited by relevant documents in an individual collection.",0,,False
11,Our study differs from prior work in a few important ways. Previous investigations into document expansion have tended to use only,0,,False
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 https://doi.org/10.1145/3077136.3080716",1,ad,True
13,Miles Efron,0,,False
14,School of Information Sciences University of Illinois at Urbana-Champaign,0,,False
15,mefron@illinois.edu,0,,False
16,"the target collection to expand documents, while our work explores the use of one or more distinct collections. Conversely, most existing work involving external corpora in ad-hoc information retrieval has focused on query expansion; we are interested in incorporating external collections for purposes of document expansion.",1,corpora,True
17,2 RELATED WORK 2.1 Document Expansion in IR,0,,False
18,"Document expansion has been well studied in information retrieval literature, e.g. [10, 11, 13, 16]. For example, Liu & Croft propose a method of retrieval that uses document clusters to smooth document language models [10]. Tao et al. propose a similar approach but place each document at the center of its own cluster; this helps to ensure that the expansion documents are as closely related to the target document as possible [13].",0,,False
19,"Our approach takes as its starting point that of Efron, Organisciak & Fenlon [4], who issue very short microblog documents as pseudo-queries. They employ a procedure closely related to relevance modeling [8] to expand the original document using those microblog documents retrieved for the pseudo-query. We explore the application and adaptation of their work to different scenarios. First, Efron, Organisciak & Fenlon are concerned with microblog retrieval, in which documents are extremely short--perhaps as small as a keyword query. In contrast, we are interested in performing document expansion with more typical full-length documents, such as those found in news and web corpora. Second, while their work used only the target document collection, we propose an extension of their method that allows for multiple expansion corpora. Finally, we investigate pairing document expansion with query expansion, which their work suggests may be problematic in the microblog domain.",1,blog,True
20,2.2 Incorporating External Collections,1,corpora,True
21,"The incorporation of external collections into document retrieval is a similarly common theme in the ad-hoc IR literature, particularly with respect to query expansion [2, 3, 9, 15, 17]. Of particular relevance to our work is that of Diaz & Metzler, whose mixture of relevance models is the basis of our Eq. 5 [3]. Their model simply interpolates RMs built on different collections, weighting each by a query-independent quantity P (c). Though our work bears similarities, Diaz & Metzler are interested in query expansion, whereas we apply the technique as one piece in a document expansion model.",1,corpora,True
22,1045,0,,False
23,Short Research Paper,0,,False
24,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
25,3 DOCUMENT EXPANSION PROCEDURE,0,,False
26,3.1 Underlying Retrieval Model,0,,False
27,"Throughout this paper we rely on the language modeling retrieval framework [7]. More specifically, we employ query likelihood (QL) and relevance modeling for ranking.",0,,False
28,"3.1.1 Query Likelihood. Given a query Q and a document D, we rank documents on P (Q |D ), where D is the language model (typically a multinomial over the vocabulary V ) that generated the text of document D. Assuming independence among terms and a uniform distribution over documents, each document is scored by",1,Query,True
29,"P (Q |D) ,",0,,False
30,"P (w |D )c (w,Q )",0,,False
31,(1),0,,False
32,w Q,0,,False
33,"where c (w,Q ) is the frequency of word w in Q. We follow standard procedures for estimating P (w |D ) in Eq. 1, estimating a smoothed language model by assuming that document language models in a",0,,False
34,given collection have a Dirichlet prior distribution:,0,,False
35,P^(w |D ),0,,False
36,",",0,,False
37,"c (w,D) + µP^(w |C) |D| + µ",0,,False
38,(2),0,,False
39,"where P^(w |C) is the maximum likelihood estimate of the probability of seeing word w in a ""background"" collection C (typically C is the corpus from which D is drawn), and µ  0 is the smoothing hyperparameter.",0,,False
40,3.1.2 Relevance Modeling. Relevance modeling is a form of pseudo-relevance feedback that uses top ranked documents to estimate a language model representing documents relevant to a query [8].,0,,False
41,"Assuming uniform document prior probabilities, relevance models take the form of",0,,False
42,"P (w |R) , P (w |D)P (Q |D)",0,,False
43,(3),0,,False
44,D C,0,,False
45,where P (Q |D) is calculated as in Eq. 1 and essentially weights word w in D by the query likelihood of the document. Relevance models,0,,False
46,are most efficient and robust when calculated over only the top ranked documents and limited to the top terms. These parameters are referred to as f bDocs and f bT erms respectively in Table 1,0,,False
47,below.,0,,False
48,"Because relevance models are prone to query drift, it is often",0,,False
49,desirable to linearly interpolate an RM with the original query,0,,False
50,model to improve effectiveness:,0,,False
51,"P (w |Q ) , (1 -  )P (w |R) + P (w |Q ).",0,,False
52,(4),0,,False
53," is a mixing parameter controlling the influence of the original query. This form of relevance model is known as ""RM3.""",0,,False
54,3.2 Expanding with Document Pseudo-Queries,0,,False
55,"To expand a document D, we begin by treating the text of D as a pseudo-query which we pose against a collection of documents CE . To transform a document into a pseudo-query we apply two transformations. First we remove all terms from D that appear",0,,False
56,"in the standard Indri stoplist1. Next, we prune our pseudo-query by retaining only the 0 < k  K most frequent words in the stopped text of D, where K is the total number of unique terms in D. The integer variable k is a parameter that we choose empirically.",0,,False
57,These are the non-stop words with the highest probabilities in a maximum likelihood estimate of D's language model and are,0,,False
58,therefore a reasonable representation of the topic of the document.,0,,False
59,"Though some information may be lost with stopping, with a large enough k we hope to nevertheless capture the general topic of a",0,,False
60,"document; for example, a document about Hamlet's famous speech",0,,False
61,"may not be represented by the terms ""to be or not to be,"" but the",0,,False
62,"terms ""Shakespeare,"" ""Hamlet,"" ""speech,"" etc. will likely represent the document's subject sufficiently. Let QD be the pseudo-query for D, consisting of the text of D after our two transformations are",0,,False
63,applied.,0,,False
64,"We rank related documents, called expansion documents, by running QD over a collection CE . More formally, we rank the documents in CE against D using Eq. 1, substituting QD for the query and Ei --the ith expansion document--for the document. Let i be the log-probability for expansion document Ei with respect to D given by Eq. 1.",0,,False
65,"We now have a ranked list of tuples {(E1,1), (E2,2), ..., (EN ,N )} relating expansion document Ei to D with log-probability i . We take the top n documents where 0  n  N . We call these top documents ED and designate them as our expansion documents for D. Finally, we exponentiate each i and normalize our retrieval scores so they sum to 1 over the n retained documents. Assum-",0,,False
66,"ing a uniform prior over documents, we now have a probability distribution over our n retained documents: P (E|D).",0,,False
67,"Since this procedure does not depend on the query, we may compute ED once at indexing time and reuse our expansion documents across queries.",0,,False
68,4 DOCUMENT EXPANSION RETRIEVAL MODEL,0,,False
69,We would now like to incorporate our expansion documents into,1,corpora,True
70,a retrieval model over documents. We assume that a query is gen-,0,,False
71,"erated by a mixture of the original document language model D and language models E j representing the expansion documents in each corpus Cj  {C1,C2, ...,Cn }. We assume that E j can be estimated using the text of the expansion documents ED j in corpus Cj . This mixture model may be expressed as:",0,,False
72,|Q |,0,,False
73,n,0,,False
74,n,0,,False
75,"P^ (Q |D) , (1 -  ED j )P (qi |D) +  ED j P (qi |ED j ) (5)",0,,False
76,"i ,1",0,,False
77,"j ,1",0,,False
78,"j ,1",0,,False
79,where 0 ,0,,False
80,"n j ,1",0,,False
81,ED,0,,False
82,j,0,,False
83,1. We estimate P (qi |ED j ),0,,False
84,in expectation:,0,,False
85,"P (qi |ED j ) ,",0,,False
86,P (qi |E)P (E|D).,0,,False
87,(6),0,,False
88,E ED j,0,,False
89,"Like P (qi |D), we estimate the probability of qi given expansion document E, P (qi |E), as a Dirichlet-smoothed query likelihood. By virtue of our expansion document scoring and normalization, we also have P (E|D). This general model may be used with any number",0,,False
90,of expansion corpora.,1,corpora,True
91,1 http://www.lemurproject.org/stopwords/stoplist.dft,0,,False
92,1046,0,,False
93,Short Research Paper,0,,False
94,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
95,4.1 Relevance Modeling with Expanded Documents,0,,False
96,"Given our motivating intuition that document expansion allows for the more accurate estimation of document language models, we would expect that an RM computed using expanded documents should be more accurate than a standard RM. We therefore compute an RM3 as in Eqs. 3 and 4, substituting the expanded document for the original.",0,,False
97,5 EVALUATION,0,,False
98,5.1 Data,0,,False
99,"Although Eq. 5 allows for an arbitrary number of collections, for now we limit ourselves to two: the collection that the document appears in (the ""target"" collection) and Wikipedia2. We expect the latter, as a general encyclopedia, to yield relatively unbiased probability estimates. We build an Indri [12] index over the Wikipedia page text.",1,Wiki,True
100,We test our approach using TREC datasets:,1,TREC,True
101,· The AP newswire collection [5] from TREC disks 1 and 2 with topics 101-200.,1,AP,True
102,"· The robust 2004 [14] topics, numbering 250, from TREC disks 4 and 5.",1,TREC,True
103,· The wt10g collection [1] with the 100 topics from the 2000 and 2001 TREC Web tracks.,1,TREC,True
104,"These datasets provide a good range of collection types, from relatively homogeneous with well-formed documents (AP) to heterogeneous with varied document quality (wt10g).",1,AP,True
105,5.2 Runs,0,,False
106,"For each collection, we produce eight runs representing a combination of expansion source and query expansion model. Expansion source refers to the collection(s) used for document expansion, while the query expansion model refers to unexpanded queries (QL) or expanded queries (RM3).",0,,False
107,We produce runs with expansion documents from:,0,,False
108,"· no expansion, called baseline; · the target collection itself, called self ; · Wikipedia, called wiki; or · a mixture of the previous two, called combined.",1,Wiki,True
109,"For each source, both the QL and RM3 variations are compared. Stop words are removed from the query. For efficiency, we re-",0,,False
110,trieve the top 1000 documents using the default Indri QL implementation and re-rank these documents based on their expanded representations as described in Section 4.,0,,False
111,5.3 Parameters,0,,False
112,"The parameters required for our approach, their meanings, and the values used in our experiments are shown in Table 1.",0,,False
113,"For this work, we set k heuristically. In principle, k may equal the length of the document; however, this would increase computation time significantly, so we have set it to a smaller value for efficiency. The parameter n is also set heuristically; see Section 6.1 for a discussion of the sensitivity of our model to the setting of n.",0,,False
114,2 http://en.wikipedia.org,1,wiki,True
115,Table 1: Parameter settings for the document expansion procedure and retrieval model,0,,False
116,Param. k n,0,,False
117, ED,0,,False
118,µ f bDocs f bT erms,0,,False
119,Meaning,0,,False
120,The maximum number of document terms to use in constructing QD . The maximum number of expansion documents in ED . One of several related mixing parameters controlling the weights of P (q|D) and P (q|ED ) Used for Dirichlet smoothing of both P (q|D) and P (q|E). The number of feedback documents to use for RM3 runs.,0,,False
121,The number of terms per document to use for RM3 runs.,0,,False
122,Mixing parameter controlling the weights of the original query and relevance model for RM3 runs.,0,,False
123,Value 20 10,0,,False
124,0.0-1.0,0,,False
125,2500 20 20 0.0-1.0,0,,False
126,"The values of  ED and  , are determined using 10-fold crossvalidation. In the training stage, we sweep over parameter values in intervals of 0.1. The concatenated results of each test fold form a complete set of topics.",0,,False
127,6 RESULTS,0,,False
128,Retrieval effectiveness of each run is shown in Table 2. We measure effectiveness with mean average precision (MAP) and normalized discounted cumulative gain at 20 (nDCG@20) [6]. Each metric is optimized with 10-fold cross-validation.,1,MAP,True
129,"The results confirm that document expansion provides benefit over a baseline query likelihood run--no run performs significantly worse than the baseline, and most runs improve over the baseline QL run.",0,,False
130,"Performance of RM3 runs is more surprising with improvement over the baseline RM3 occurring more rarely compared to improvement over the baseline QL. The data suggest that RM3 runs may be more effective in more heterogeneous collections: there are three RM3 improvements in robust and six in wt10g, compared to only one in AP. This makes intuitive sense since a homogeneous collection would be expected to receive less benefit from query expansion. We can also see that an RM3 run typically improves over its QL counterpart, demonstrating that relevance modeling continues to operate effectively with the introduction of document expansion.",1,AP,True
131,"In general, wiki runs perform similarly to combined runs. However, the strong performance of combined runs is visible when query expansion is ignored: five out of six combined QL runs show statistically significant improvement over wiki QL runs. In one case (wt10g measuring nDCG@20) the combined QL run even outperforms the wiki RM3 run with statistical significance.",1,wiki,True
132,6.1 Sensitivity to n,0,,False
133,"Figure 1 shows sweeps over several values of n, the number of expansion documents, for the self and wiki QL runs using our",1,wiki,True
134,1047,0,,False
135,Short Research Paper,0,,False
136,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
137,MAP nDCG@20,1,MAP,True
138,0.26 0.24 0.22 0.20 0.18,0,,False
139,0,0,,False
140,Expansion Source self wiki Collection AP wt10g robust 0.45,1,wiki,True
141,0.40,0,,False
142,0.35,0,,False
143,10,0,,False
144,20,0,,False
145,30,0,,False
146,40,0,,False
147,Number of expansion documents,0,,False
148,0.30,0,,False
149,50,0,,False
150,0,0,,False
151,10,0,,False
152,20,0,,False
153,30,0,,False
154,40,0,,False
155,50,0,,False
156,Number of expansion documents,0,,False
157,"Figure 1: Sweeps over the number of expansion documents, n ,"" {0, 1, 5, 10, 50}, for the self and wiki QL runs.""",1,wiki,True
158,"established cross-validation procedure with identical folds. The sensitivity to n is not pronounced at n  5, and what little variation exists is not consistent across collections. We therefore set n to 10, an apparently safe value, for all other runs. This is a convenient result since it allows for more efficient document expansion.",0,,False
159,7 CONCLUSIONS,0,,False
160,"The results indicate that our approach for document expansion works well in general and especially in concert with traditional relevance modeling. We find that we can improve on traditional document expansion by incorporating external collections into the expansion process. In the future, we plan to investigate how important the choice of external collection is to the retrieval effectiveness of our model.",1,ad,True
161,REFERENCES,0,,False
162,"[1] P. Bailey, N. Craswell, and D. Hawking. Engineering a multi-purpose test collection for web retrieval experiments. Information Processing and Management, 39(6):853­871, 2003.",0,,False
163,"[2] M. Bendersky, D. Metzler, and B. W. Croft. Effective query formulation with multiple information sources. WSDM '12, 2012.",0,,False
164,"[3] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In SIGIR '06, pages 154­161, 2006.",1,corpora,True
165,"[4] M. Efron, P. Organisciak, and K. Fenlon. Improving retrieval of short texts through document expansion. In SIGIR '12, pages 911­920, 2012.",0,,False
166,"[5] D. Harman. Overview of the first text retrieval conference (TREC-1). In TREC '92, pages 1­20, 1992.",1,TREC,True
167,"[6] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. TOIS, 20(4):422­446, 2002.",0,,False
168,"[7] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In SIGIR '01, pages 111­119, 2001.",0,,False
169,"[8] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR '01, pages 120­127, 2001.",0,,False
170,"[9] Y. Li, W. Luk, K. Ho, and F. Chung. Improving weak ad-hoc queries using Wikipedia as external corpus. SIGIR '07, pages 797­798, 2007.",1,ad-hoc,True
171,"[10] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In SIGIR '04, pages 186­193, 2004.",0,,False
172,"[11] A. Singhal and F. Pereira. Document expansion for speech retrieval. In SIGIR '99, pages 34­41, 1999.",0,,False
173,"[12] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, pages 2­6, 2005.",0,,False
174,"[13] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion. In NAACL '06, pages 407­414, 2006.",0,,False
175,"[14] E. M. Voorhees. Overview of the TREC 2004 robust track. In TREC '132, 2013. [15] W. Weerkamp, K. Balog, and M. de Rijke. A generative blog post retrieval model",1,TREC,True
176,"that uses query expansion based on external collections. In ACL '09, pages 1057­1065, 2009. [16] X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In SIGIR '06, pages 178­185, 2006. [17] Y. Xu, G. J. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on Wikipedia. SIGIR '09, 2009.",1,ad-hoc,True
177,Table 2: Performance of runs using various expansion,0,,False
178,sources with (RM3) and without (QL) query expansion. Statistical significance at p  0.05 is marked with a variety of symbols; underlining further designates p  0.01:  indicates improvement over the baseline QL run;  and  indicate im-,0,,False
179,provement and decline respectively with respect to the baseline RM3 run;  indicates improvement over the QL run with the same expansion source; S and W indicate improvement,0,,False
180,"over the self and wiki sources, respectively, of the same run",1,wiki,True
181,type. Bolded runs are the highest raw score for an evaluation,0,,False
182,metric in a given collection.,0,,False
183,Corpus AP,1,AP,True
184,Robust wt10g,1,Robust,True
185,Exp. Source Baseline,0,,False
186,Self Wiki Combined Baseline Self Wiki Combined Baseline Self Wiki Combined,1,Wiki,True
187,Run,0,,False
188,QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3,0,,False
189,MAP,1,MAP,True
190,0.2337 0.3310 0.2694 0.3295 0.2644 0.3334 0.2774W S 0.3342S,0,,False
191,0.2183 0.2639 0.2369 0.2591 0.2326 0.2674S 0.2417W 0.2672S,0,,False
192,0.1683,0,,False
193,0.1651,0,,False
194,0.1660 0.1694 0.1780S 0.2089S 0.1759S 0.2061S,0,,False
195,nDCG@20,0,,False
196,0.4170 0.4855 0.4519 0.4876W 0.4582 0.4811 0.4734SW 0.4789,0,,False
197,0.3867 0.3908 0.4036 0.3894 0.4040 0.4201S  0.4156W S 0.4205S,0,,False
198,0.2816 0.2834,0,,False
199,0.2936 0.2758 0.3029 0.3085S  0.3148SW 0.3082S,0,,False
200,1048,0,,False
201,,0,,False

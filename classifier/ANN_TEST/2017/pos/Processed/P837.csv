,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Learning To Rank Resources,0,,False
3,Zhuyun Dai,0,,False
4,Carnegie Mellon University zhuyund@cs.cmu.edu,0,,False
5,Yubin Kim,0,,False
6,Carnegie Mellon University yubink@cs.cmu.edu,0,,False
7,Jamie Callan,0,,False
8,Carnegie Mellon University callan@cs.cmu.edu,0,,False
9,ABSTRACT,0,,False
10,"We present a learning-to-rank approach for resource selection. We develop features for resource ranking and present a training approach that does not require human judgments. Our method is well-suited to environments with a large number of resources such as selective search, is an improvement over the state-of-the-art in resource selection for selective search, and is statistically equivalent to exhaustive search even for recall-oriented metrics such as MAP@1000, an area in which selective search was lacking.",1,MAP,True
11,KEYWORDS,0,,False
12,"selective search, resource selection, federated search",0,,False
13,"ACM Reference format: Zhuyun Dai, Yubin Kim, and Jamie Callan. 2017. Learning To Rank Resources. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: 10.1145/3077136.3080657",0,,False
14,2 RELATED WORK,0,,False
15,"ere are three main classes of resource selection algorithms: termbased, sample-based, and supervised approaches. Term-based algorithms models the language distribution of each shard. At query time, they determine the relevance of a shard by comparing the query to the stored language model [1, 12]. Sample-based algorithms estimate the relevance of a shard by querying a small sample index of the collection, known as the centralized sample index (CSI) [9, 11, 13, 14]. Supervised methods use training data to learn models to evaluate shard relevance, with most methods training a classi er per shard [2, 4]. However, training a classi er for every shard is expensive in selective search, where shards number in hundreds.",0,,False
16,"us, supervised methods have not been used for selective search. Techniques that train a single classi er would be more suitable for selective search. Balog [3] trained a learning-to-rank algorithm for a TREC task and Hong et al. [6] learned a joint probabilistic classi er. e la er is used as a baseline in this work.",1,TREC,True
17,1 INTRODUCTION,1,DUC,True
18,"Selective search is a federated search architecture where a collection is clustered into topical shards. At query time, a resource selection algorithm is used to select a small subset of shards to search.",0,,False
19,"Recent work showed that while selective search is equivalent to exhaustive search for shallow metrics (e.g. P@10), it performs worse for recall-oriented metrics (e.g. MAP) [5]. is is a problem because modern retrieval systems apply re-ranking operations to a base retrieval, which can require deep result lists [10].",1,MAP,True
20,"In this paper, we present learning to rank resources, a resource selection method based on learning-to-rank. While learning-to-rank has been widely studied for ranking documents, its application to ranking resources has not been studied in depth. We take advantage of characteristics of the resource ranking problem that are distinct from document ranking; we present new features; and we propose a training approach that uses exhaustive search results as the gold standard and show that human judgments are not necessary.",1,ad,True
21,"Our approach is suitable for e ciently ranking the hundreds of shards produced by selective search and is an improvement over the state-of-the-art in resource selection for selective search. In addition, our approach is statistically equivalent to exhaustive search in MAP@1000, a deep recall-oriented metric.",1,ad,True
22,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080657",1,ad,True
23,3 MODEL,0,,False
24,"Let q denote a query and (q, si ) denote features extracted from the ith shard for the query. e goal of learning-to-rank is to",0,,False
25,"nd a shard scoring function f ((q, s)) that can minimize the loss",0,,False
26,function de,0,,False
27,ned as:,0,,False
28,L(f ),0,,False
29,",",0,,False
30,q,0,,False
31,Q,0,,False
32,l,0,,False
33,"(q,",0,,False
34,f,0,,False
35,)dP,0,,False
36,(q).,0,,False
37,"We use l(q, f )",0,,False
38,",",0,,False
39,"si >qsj 1{ f ((q, si )) < f ((q, sj ))} , where si >q sj denotes shard",0,,False
40,pairs for which si is ranked higher than sj in the gold standard,0,,False
41,shard ranking w.r.t query q.,0,,False
42,"We used SV Mr ank [7], which optimizes pair-wise loss. List-wise",0,,False
43,"algorithms such as ListMLE [16] produced similar results, thus we",0,,False
44,only report results with SV Mr ank .,0,,False
45,e training process requires a gold standard shard ranking for,0,,False
46,"each training query. We propose two de nitions of the ground truth,",0,,False
47,"relevance-based and overlap-based. In the relevance-based approach,",0,,False
48,the optimal shard ranking is determined by the number of relevant,0,,False
49,"documents a shard contains. us, the training data require queries",0,,False
50,"with relevance judgments, which can be expensive to obtain. e",0,,False
51,overlap-based approach assumes that the goal of selective search,0,,False
52,is to reproduce the document ranking of exhaustive search. e,0,,False
53,optimal shard ranking is determined by the number of documents,0,,False
54,in a shard that were ranked highly by exhaustive search. is does,0,,False
55,not require manual relevance judgments.,0,,False
56,4 FEATURES,0,,False
57,4.1 ery-Independent Information,0,,False
58,Shard Popularity: Indicates how o en the shard had relevant (relevance-based) or top-ranked (overlap-based) documents for training queries. It is query-independent and acts as a shard prior.,1,ad,True
59,837,0,,False
60,Short Research Paper,0,,False
61,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
62,4.2 Term-Based Statistics,0,,False
63,"Term-based features can be easily precomputed, thus are e cient. Taily Features: One feature is the Taily [1] score calculated for query q and shard s. However, Taily scores can vary greatly across shards and queries. For robustness, we add two additional features. If shard s is ranked rs for query q, the inverse rank is 1/rs , which directly describes the importance of s relative to other shards. e binned rank is ceilin (rs /b), where b is a bin-size. We use b ,"" 10, meaning that every 10 consecutive shards are considered equally relevant. is feature helps the model to ignore small di erences between shards with similar rankings. Champion List Features: For each query term, the top-k best documents were found. e number of documents each shard contributes to the top-k was stored for each shard-term pair. For multi-term queries, the feature values of each query term were summed. We use two values of k "","" {10, 100}, generating 2 features.""",1,ad,True
64,"ery Likelihood Features: e log-likelihood of a query with respect to the unigram language model of each shard is: L(q|s) ,",0,,False
65,"t q log p(t |s), where p(t |s) is the shard language model, the average of all document language models p(t |d) in the shard. Document language model p(t |d) is estimated using MLE with Jelinek-Mercer smoothing. ery likelihood, inverse query likelihood, and binned query likelihood features are created for body, title, and inlink representations, yielding a total of 9 features.",0,,False
66,"ery Term Statistics: e maximum and minimum shard term frequency across query terms, e.g. st fmax (q, s) ,"" maxt q st f (t, s), where st f (t, s) is the frequency of term t in shard s. We include the maximum and minimum of st f · id f where id f is the inverse document frequency over the collection. ese 4 features are created for body, title, and inlink representations, yielding 12 features. Bigram Log Frequency: e frequency of each bigram of the query in a shard is b fq (s) "","" b q log b fb (s), where b fb (s) is the frequency of bigram b in shard s. is feature can estimate term correlation. To save storage, we only store bigrams that appear more than 50 times in the collection.""",0,,False
67,4.3 Sample-Document (CSI-Based) Features,0,,False
68,"ese features are based on retrieval from the centralized sample index (CSI), which may provide term co-occurrence information. CSI retrieval is expensive, and thus is slower to calculate. Rank-S and ReDDE Features: Similar to Taily features, the shard scores given by Rank-S [9] and ReDDE [13], as well as the inverse rank and binned rank features for a total of 6 features. Average Distance to Shard Centroid: e distance between the top-k documents retrieved from the CSI to their respective shards' centroids. Intuitively, if the retrieved documents are close to the centroid, the shard is more likely to contain other similar, highlyscoring documents. For multiple documents from the same shard, the distances are averaged. We use two distance metrics: KL divergence and cosine similarity Note that because KL divergence measures distance rather than similarity, we use the inverse of the averaged KL divergence as the metric. We generated features for k ,"" {10, 100} and also a feature measuring the distance between the shard's centroid to its single highest scoring document in the top 100 of the CSI results, for a total of 6 features.""",0,,False
69,5 EXPERIMENTAL METHODOLOGY,0,,False
70,"Datasets: Experiments were conducted with ClueWeb09-B and Gov2. ClueWeb09-B (CW09-B) consists of 50 million pages from the ClueWeb09 dataset. Gov2 is 25 million web pages from the US government web domains. For relevance-based models, 200 queries from the TREC 09-12 Web Track topics were used for CW09-B, and 150 queries from the TREC 04-06 Terabyte Track topics were used for Gov2. Models were trained by 10-fold cross-validation. For overlap-based models, training queries were sampled from the AOL and Million ery Track query logs. Models were tested with the TREC queries. Optimal shard ranking for the overlap method was de ned by the number of documents each shard contains that were within the top N ,"" 2K retrieved from exhaustive search. We found N  [1K, 3K] produced stable results. Proposed methods and baselines: We used three sources of training data: relevance-based training data (L2R-TREC), and overlapbased training data (L2R-AOL and L2R-MQT). We used linear SV Mr ank , where C was chosen by cross validation. Our method was compared against state-of-the-art unsupervised models (Taily [1], ReDDE [13], and Rank-S [9]); and a supervised model Jnt [6]. Jnt was trained and tested using TREC queries with 10-fold cross-validation. Evaluation Metrics: Search accuracy was measured by P@10, NDCG@30 and MAP@1000. To test the proposed methods' superiority to baselines, a query-level permutation test with p < 0.05 was used. To test the equivalence to exhaustive search, a non-inferiority test [15] was used to assert that results of the more e cient selective search were at least as accurate as exhaustive search. e equivalence is established by rejecting the null hypothesis that selective search is at least 5% worse than exhaustive search with a 95% con dence interval. Selective Search Setup: We used 123 shards for CW09-B and 199 shards for Gov2 [5]. A 1% central sample index (CSI) was created for ReDDE and Rank-S baselines and CSI based features. Jnt followed the original implementation and used a 3% CSI. Search Engine Setup: Retrieval was performed with Indri, using default parameters. eries were issued using the sequential dependency model (SDM) with parameters (0.8, 0.1, 0.1). For CW09-B, documents with a Waterloo spam score below 50 were removed 1.""",1,ClueWeb,True
71,6 EXPERIMENTS,0,,False
72,6.1 Overall Comparison,0,,False
73,Our method was compared to four baselines and exhaustive search. We tested shard rank cuto s from 1­8% of total shards; 10 for CW09-B and 16 for Gov2. e automatic cuto s of Rank-S and Taily performed similarly to xed cuto s and are not shown. Shard rankings by L2R enabled more accurate search than all baselines in both datasets (Figure 1). e search accuracy of L2R models is higher than the baselines at nearly every shard rank cuto .,1,CW,True
74,"Table 1 compares L2R models and the baselines at two shard rank cuto s. e rst cuto is the point where the shallow metrics (P@10 and NDCG@30) stablize: 4 for CW09-B and 6 for Gov2. e second cuto is where MAP@1000 become stable: 8 for CW09-B and 12 for Gov2. L2R models improve over the baselines at both shard cuto s. For shallow metrics, L2R reaches exhaustive search",1,CW,True
75,1h ps://plg.uwaterloo.ca/ gvcormac/,0,,False
76,838,0,,False
77,Short Research Paper,0,,False
78,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
79,"at the rst cuto . Furthermore, searching the rst 8 out of 12 shards ranked by L2R is statistically non-inferior to searching all shards exhaustively, even for the recall-oriented MAP@1000. All the baselines have a 10% gap from exhaustive search in MAP@1000.",1,MAP,True
80,6.2 E ect of Training Data,0,,False
81,"One might expect the relevance-based model (L2R-TREC) to be better than overlap-based models (L2R-AOL and AOL-MQT), because it uses manual relevance judgments. A model trained with overlap data might favor shards that contain false-positive documents. However, there is li le di erence between the two training methods. L2R-TREC was statistically be er than TREC or AOL for MAP@1000 in Gov2, but the relative gain is only 2%; in all other cases, there is no statistically signi cant di erences among the three models. Furthermore, models trained with relevance and overlap data agreed on which features are important (not shown due to space constraints).",1,TREC,True
82,"is analysis indicates that unlike learning to rank document models, we can train a learning to rank resource selector on a new dataset before we have relevance judgments.",0,,False
83,6.3 ery Length,0,,False
84,"We compare L2R-MQT to the baselines using MAP@1000 for queries with di erent lengths on CW09-B, shown in Figure 1. Gov2 and other training data produced similar results and are not shown. For single-term queries, existing methods are already equivalent to or be er than exhaustive search, and L2R-MQT retains this good performance. e advantage of L2R-MQT comes from multi-term queries, where the best baseline Jnt still has a 10% gap from exhaustive search. For these queries, the improvement of L2R-MQT over the Taily is expected, because Taily does not model term co-occurrence. However, L2R-MQT also out-performs ReDDE and Rank-S, which account for term co-occurrence by retrieving documents from the CSI, but are limited by only having a sample view of the collection. L2R draws evidence from both the sample and the whole collection. Jnt also fuses sample- and term-based features, but most of its features are derived from ReDDE or Taily-like methods and do not carry new information. L2R improved over Jnt by using novel features that encode new evidence.",1,MQ,True
85,"Figure 1: MAP@1000 for queries on CW09-B, grouped by query length. Parentheses on the X axis present the number of queries in each group. T is the shard rank cuto .",1,MAP,True
86,6.4 Feature Analysis,0,,False
87,"e L2R approach uses three classes of features: query-independent, term-based, and sample-document (CSI). ese three feature classes have substantially di erent computational costs and contributions.",0,,False
88,"Fast vs. Slow features: Sample-document (CSI-based) features have a high computational cost, because they search a sample (typically 1-2%) of the entire corpus. Term-based features have a low computational cost, because they lookup just a few statistics per query term per shard. Costs for query-independent features are lower still. e third experiment compares a slow model that uses all features (ALL) to a fast version that does not use sample-document features (FAST).",0,,False
89,"We estimate the resource selection cost by the amount of data retrieved from storage. For CSI-based features, the cost is the size of postings of every query term in the CSI. For term-based features, the cost is the amount of su cient statistics required to derive all term-based features. e query-independent feature only looks up the shard popularity, so the cost is one statistic per shard.",0,,False
90,"Table 2 compares FAST with ALL and baselines by their accuracy and average resource selection cost per query. ReDDE results were similar to Rank-S and are not shown. Taily has been the state-ofthe-art term-based (`faster') resource selection algorithm. However, FAST is substantially more accurate. FAST also outperformed Jnt with over 100× speed up. Compared to ALL, FAST is 67 times faster on CW09-B and 34 times faster on Gov2. Although FAST has slightly lower search accuracy than ALL, the gap is not large and is not statistically signi cant, indicating that the information from the CSI features can be covered by the more e cient features.",1,CW,True
91,"We conclude that a resource ranker composed of only queryindependent and term-based features is as accurate as exhaustive search and a ranker that includes CSI features. CSI features improve accuracy slightly, but at a signi cant additional computational cost.",1,ad,True
92,"Importance of Feature Types: We investigate the contribution of other types of features: query-independent features and term-based features, where the term-based features were sub-divided into unigram and bigram features. Table 3 presents the results for the leave-one-out analysis conducted on FAST. On CW09-B, removing any feature set from FAST led to lower performance. is indicates that each set of features covers di erent types of information, and all are necessary for accurate shard ranking. Among these features, unigram features were most important because CW09-B has many single-term queries. On Gov2, the only substantial di erence is observed when bigram features are excluded.",1,CW,True
93,7 CONCLUSION,0,,False
94,"is paper investigates a learning-to-rank approach to resource selection for selective search. Much a ention has been devoted to learning-to-rank documents, but there has been li le study of learning-to-rank resources such as index shards. Our research shows that training data for this task can be generated automatically using a slower system that searches all index shards for each query. is approach assumes that the goal of selective search is to mimic the accuracy of an exhaustive search system, but with lower computational cost. is assumption is not entirely true--we would like selective search to also be more accurate--but it is convenient and e ective.",0,,False
95,839,0,,False
96,Short Research Paper,0,,False
97,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
98,"Table 1: Search accuracy comparison between 3 L2R models and baselines at two rank cuto s for two datasets. : statistically signi cant improvement compared to Jnt, the best resource selection baseline. : non-inferiority to exhaustive search .",0,,False
99,Method,0,,False
100,Redde Rank-S Taily Jnt L2R-TREC L2R-AOL L2R-MQT Exh,1,TREC,True
101,P @10,0,,False
102,0.355 0.350 0.346 0.370 0.374 0.374 0.382,0,,False
103,0.372,0,,False
104,"T,4 NDCG @30",0,,False
105,0.262 0.259 0.260,0,,False
106,0.269 0.281 0.281  0.285 ,0,,False
107,0.288,0,,False
108,CW09-B,1,CW,True
109,MAP @1000 0.176 0.175 0.172 0.178 0.192 0.191 0.193 0.208,1,MAP,True
110,P@10,0,,False
111,0.363 0.360 0.346 0.367 0.377 0.375 0.375 0.372,0,,False
112,"T,8",0,,False
113,NDCG,0,,False
114,@30 0.275,0,,False
115,0.268,0,,False
116,0.260 0.277 0.286  0.287  0.286 ,0,,False
117,0.288,0,,False
118,MAP @1000,1,MAP,True
119,0.187 0.183 0.175,0,,False
120,0.192 0.202  0.202  0.202 ,0,,False
121,0.208,0,,False
122,P,0,,False
123,@10 0.580,0,,False
124,0.570,0,,False
125,0.518 0.582 0.593 0.593 0.586,0,,False
126,0.585,0,,False
127,"T,6 NDCG @30",0,,False
128,0.445 0.440 0.403,0,,False
129,0.459 0.469 0.470  0.465,0,,False
130,0.479,0,,False
131,Gov2,1,Gov,True
132,MAP @1000 0.267 0.263 0.235 0.278 0.299 0.291 0.292 0.315,1,MAP,True
133,P,0,,False
134,@10 0.587 0.585,0,,False
135,0.530 0.588 0.591 0.587 0.593,0,,False
136,0.585,0,,False
137,"T,12",0,,False
138,NDCG,0,,False
139,@30 0.4600 0.461,0,,False
140,0.418 0.465 0.475  0.470 0.474 ,0,,False
141,0.479,0,,False
142,MAP @1000,1,MAP,True
143,0.289 0.286 0.256,0,,False
144,0.292 0.313  0.307  0.309 ,0,,False
145,0.315,0,,False
146,Table 2: E ectiveness and e ciency of FAST features. ALL uses all features. FAST does not use sample-document features. T: shard rank cuto . : non-inferiority to exhaustive.,0,,False
147,Cw09 -B,0,,False
148,"(T,8) Gov2",1,Gov,True
149,"(T,12)",0,,False
150,Method,0,,False
151,Redde Taily Jnt ALL FAST Redde Taily Jnt ALL FAST,0,,False
152,P,0,,False
153,@10 0.363,0,,False
154,0.346 0.367 0.375 0.373 0.579,0,,False
155,0.518 0.588 0.593 0.587,0,,False
156,NDCG,0,,False
157,@30 0.275,0,,False
158,0.260 0.277 0.286 0.285 0.445,0,,False
159,0.403 0.465 0.474 0.471,0,,False
160,MAP @1000 0.187 0.175 0.192 0.202 0.201 0.289 0.256 0.292 0.309 0.310,1,MAP,True
161,Average Cost,0,,False
162,"156,180 470",0,,False
163,"468,710 158,529",0,,False
164,"2,349 105,080",0,,False
165,"758 315,875 108,306",0,,False
166,"3,226",0,,False
167,Table 3: Performance of L2R-MQT using feature sets constructed with leave-one-out. `- X' means the feature was excluded from FAST. Text in bold indicates the lowest value in the column.,1,MQ,True
168,CW09 -B,1,CW,True
169,"(T,8)",0,,False
170,Gov2,1,Gov,True
171,"(T,12)",0,,False
172,Feature Set,0,,False
173,FAST - Unigram - Bigram - Independent,0,,False
174,FAST - Unigram - Bigram - Independent,0,,False
175,P@10 0.373,0,,False
176,0.303 0.364 0.368 0.592 0.592,0,,False
177,0.582 0.591,0,,False
178,NDCG@30 0.285,0,,False
179,0.226 0.275 0.282 0.471 0.468,0,,False
180,0.462 0.471,0,,False
181,MAP@1000 0.201,1,MAP,True
182,0.138 0.187 0.199 0.310 0.301,0,,False
183,0.296 0.303,0,,False
184,"We show that the learned resource selection algorithm produces search accuracy comparable to exhaustive search down to rank 1,000. is paper is the rst that we know of to demonstrate results that are statistically signi cantly equivalent to exhaustive search for MAP@1000 on an index that does not have badly skewed shard sizes. Accuracy this deep in the rankings opens up the possibility of using a learned reranker on results returned by a selective search system, which was not practical in the past.",1,MAP,True
185,"Most prior research found that sample-document algorithms such as ReDDE and Rank-S are a li le more accurate than term-based algorithms such as Taily for selective search resource selection; however, sample-document resource selection algorithms have far",0,,False
186,higher computational costs that increases query latency in some con gurations [8]. is work suggests that sample-document features provide only a small gain when combined with other types of features. It may no longer be necessary to choose between accuracy and query latency when using a learned resource ranker.,0,,False
187,8 ACKNOWLEDGMENTS,0,,False
188,"is research was supported by National Science Foundation (NSF) grant IIS-1302206. Yubin Kim is the recipient of the Natural Sciences and Engineering Research Council of Canada PGS-D3 (438411). Any opinions, ndings, and conclusions in this paper are the authors' and do not necessarily re ect those of the sponsors.",1,ad,True
189,REFERENCES,0,,False
190,"[1] R. Aly, D. Hiemstra, and T. Demeester. Taily: shard selection using the tail of score distributions. pages 673­682, 2013.",0,,False
191,"[2] J. Arguello, F. Diaz, J. Callan, and J. Crespo. Sources of evidence for vertical selection. pages 315­322, 2009.",0,,False
192,"[3] K. Balog. Learning to combine collection-centric and document-centric models for resource selection. In TREC, 2014.",1,TREC,True
193,"[4] S. Cetintas, L. Si, and H. Yuan. Learning from past queries for resource selection. pages 1867­1870, 2009.",0,,False
194,"[5] Z. Dai, X. Chenyan, and J. Callan. ery-biased partitioning for selective search. pages 1119­1128, 2016.",0,,False
195,"[6] D. Hong, L. Si, P. Bracke, M. Wi , and T. Juchcinski. A joint probabilistic classi cation model for resource selection. pages 98­105. ACM, 2010.",0,,False
196,"[7] T. Joachims. Training linear SVMs in linear time. In Proc. SIGKDD, pages 217­226, 2006.",0,,False
197,"[8] Y. Kim, J. Callan, J. S. Culpepper, and A. Mo at. Load-balancing in distributed selective search. pages 905­908, 2016.",1,ad,True
198,"[9] A. Kulkarni, A. S. Tigelaar, D. Hiemstra, and J. Callan. Shard ranking and cuto estimation for topically partitioned collections. pages 555­564, 2012.",0,,False
199,"[10] C. Macdonald, R. L. T. Santos, and I. Ounis. e whens and hows of learning to rank for web search. Inf. Retr., 16(5):584­628, 2013.",0,,False
200,"[11] I. Markov and F. Crestani. eoretical, qualitative, and quantitative analyses of small-document approaches to resource selection. 32(2):9, 2014.",0,,False
201,"[12] H. No elmann and N. Fuhr. Evaluating di erent methods of estimating retrieval quality for resource selection. pages 290­297, 2003.",0,,False
202,"[13] L. Si and J. P. Callan. Relevant document distribution estimation method for resource selection. pages 298­305, 2003.",0,,False
203,"[14] P. omas and M. Shokouhi. SUSHI: scoring scaled samples for server selection. pages 419­426, 2009.",0,,False
204,"[15] E. Walker and A. S. Nowacki. Understanding equivalence and noninferiority testing. Journal of General Internal Medicine, 26(2):192­196, 2011.",0,,False
205,"[16] F. Xia, T.-Y. Liu, J. Wang, W. Zhang, and H. Li. Listwise approach to learning to rank: theory and algorithm. In Proc. ICML, pages 1192­1199, 2008.",0,,False
206,840,0,,False
207,,0,,False

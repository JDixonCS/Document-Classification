,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,On the Benefit of Incorporating External Features in a Neural Architecture for Answer Sentence Selection,1,corpora,True
3,"Ruey-Cheng Chen, Evi Yulianti, Mark Sanderson, W. Bruce Cro ",0,,False
4,"RMIT University, Melbourne, Australia University of Massachuse s, Amherst, MA, USA",0,,False
5,"{ruey-cheng.chen,evi.yulianti,mark.sanderson}@rmit.edu.au,cro @cs.umass.edu",0,,False
6,ABSTRACT,0,,False
7,"Incorporating conventional, unsupervised features into a neural architecture has the potential to improve modeling e ectiveness, but this aspect is o en overlooked in the research of deep learning models for information retrieval. We investigate this incorporation in the context of answer sentence selection, and show that combining a set of query matching, readability, and query focus features into a simple convolutional neural network can lead to markedly increased e ectiveness. Our results on two standard question-answering datasets show the e ectiveness of the combined model.",1,corpora,True
8,CCS CONCEPTS,0,,False
9,·Information systems  Information retrieval; estion answering; ·Computing methodologies  Neural networks;,0,,False
10,KEYWORDS,0,,False
11,"Answer sentence selection, external features, convolutional neural networks",0,,False
12,"ACM Reference format: Ruey-Cheng Chen, Evi Yulianti, Mark Sanderson, W. Bruce Cro . 2017. On the Bene t of Incorporating External Features in a Neural Architecture for Answer Sentence Selection. In Proceedings of SIGIR '17, Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080705",1,corpora,True
13,1 INTRODUCTION,1,DUC,True
14,"Deep learning approaches have recently become a central methodology in the research of question answering (QA). Many recent a empts in this area have focused on utilizing neural architectures, such as convolutional neural networks (CNN) [8, 19], long shortterm memory networks [12], or a ention mechanisms [15, 18], to explicitly model high-level question-answer structures. ese advances outperform conventional approaches, which are based on engineered heuristics. However, whether deep learning will completely remove the need for such features remains an open question.",1,ad,True
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080705",1,ad,True
16,"e lack of understanding as to whether the features are needed in a neural architecture is what we address in this paper. While the development of new models for question answering has been moving rapidly ahead in the past few years, a wealth of proven useful results from prior art [2, 9, 10, 16] were usually ignored. Recently, there has been some evidence pointing a value in using features [11, 19] in neural network models. Commonly used neural network substructures, such as multilayer perceptrons [3], have the capability to combine a large number of external features, so incorporating all available signals in a neural network could improve e ectiveness and provide robust measurement of any e ect [1].",1,ad,True
17,"In this paper, we expand on past work using an extensive set of experiments. We demonstrate that, by incorporating a list of 21 common text features into a state-of-the-art CNN model, one can achieve an e ectiveness comparable to the currently best reported results on the TREC QA dataset and the WikiQA data.",1,corpora,True
18,2 EXTERNAL FEATURES,0,,False
19,"Our hypothesis is that one can assist relevance modeling with a set of external features to capture aspects of the data that are different to those captured in a neural network model. e chosen set of features should also cover basic signals that can be easily reproduced and implemented. In our experiments, we se le on a set of simple features known to be useful for question answering. We focus on retrieval and readability features. ere are two motivations for this approach: 1) such features are be er understood by information retrieval practitioners, and 2) they are relatively cheap to implement. us, we precluded the use of some sophisticated NLP features in our experiments, such as convolutional tree kernel [11] or syntactic similarity [10].",1,ad,True
20,"e full list of features is given in Table 1, they are divided into three categories.",0,,False
21,"Lexical and semantic matching. e rst group of features address non-factoid question answering, described Yang et al. [16], were rst selected, which cover topical relevance and semantic relatedness measures. e reference package released by Yang et al. was used in our implementation. For computing the language model and BM25 scores, we empirically set both µ in the language model and the average document length in BM25 to a xed value of ten.",1,ad,True
22,"Readability. e second group of features focus on text readability [5], which is a set of seven common surface readability features, such as number of words/syllables per sentence or the complexword ratio, plus one feature representing the notable Dale-Chall readability formula. We did not include other readability indices as",1,ad,True
23,1017,0,,False
24,Short Research Paper,0,,False
25,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
26,Table 1: List of unsupervised features used in this study,0,,False
27,Lexical and semantic matching,0,,False
28,Length,0,,False
29,Number of terms in the sentence,0,,False
30,ExactMatch Whether query is a substring,0,,False
31,Overlap,0,,False
32,Fraction of query terms covered,0,,False
33,OverlapSyn Fraction of query synonyms covered,0,,False
34,LM,1,LM,True
35,Language model score,0,,False
36,BM25,0,,False
37,BM25 score,0,,False
38,ESA,0,,False
39,Cosine similarity with the query ESA vector,0,,False
40,TAGME,0,,False
41,Overlap between query/sentence entities,0,,False
42,Word2Vec,0,,False
43,Cosine similarity with query word vectors,0,,False
44,Readability CPW SPW WPS CWPS CWR LWPS LWR DaleChall,1,ad,True
45,Number of characters per word Number of syllables per word Number of words per sentence Number of complex words per sentence Fraction of complex words Number of long words per sentence Fraction of long words (> 7 chars),0,,False
46,e Dale-Chall readability index,1,ad,True
47,Focus MatchedNGram,0,,False
48,"Maximum semantic relatedness between head question k-gram and any answer n-gram. See (1); 4 variants of (k, n) were used",1,ad,True
49,most of these indices can be represented as a linear combination of the surface features.,0,,False
50,"Focus. e third group of features used four parameterized variants of a newly proposed feature MatchedNGram to account for the matching between question head words and the potential answer n-gram. e feature takes the maximum of the semantic similarity between the rst k question words and any n-grams in the answer, using the cosine similarity between question/answer word vectors as the similarity measure. Given k and n, the feature is de ned as follows:",1,ad,True
51,"MatchedNGram(Q, A) , max cos",0,,False
52,l,0,,False
53,"k i ,1",0,,False
54,qi,0,,False
55,",",0,,False
56,l +n-1,0,,False
57,"j,l aj",0,,False
58,.,0,,False
59,(1),0,,False
60,"is simple feature explicitly looks for best matching answer n-grams with respect to question head phrases, such as ""who invented"", ""how many"", or ""what year did"". Word embeddings are leveraged in the computation of the similarity measure. Also, not all combinations of (k, n) are found e ective. In our experiments, we empirically chose four best con gurations of (k, n), which are {(k, n) | k  {2, 3}, n  {2, 3}}, based on their e ectiveness within a learning-to-rank model.",1,ad,True
61,3 EXPERIMENTS,0,,False
62,"Our evaluation was based on two widely used question answering benchmarks: the TREC QA and the WikiQA datasets. e rst benchmark was originally developed for the task of identifying correct answer factoids in retrieved passages. We used the version prepared by Wang et al. [13], with 1,229 questions in the larger",1,TREC,True
63,"training set, 82 in the dev set, and 100 in the test set. No further ltering was performed on the data (the ""raw"" se ing [7]).1 e second benchmark, WikiQA, was created by Yang et al.",1,Wiki,True
64,"over the English Wikipedia summary passages and the Bing query logs [17], with crowdsourced annotations. is new benchmark is developed to counter biases introduced in the creation TREC QA: the reliance on using lexical overlap with the question as the sole indication of a candidate answer. Hence, this dataset is by design made more challenging for retrieval based methods. Some major follow-up works [7, 18] used a split that includes questions with all positive labels, a version slightly di erent from the split distributed in the original data. We used the same split as in Rao et al. and used 873 questions in the training set, 126 in the dev set, and 243 in the test set [7].",1,Wiki,True
65,3.1 Neural Network Con guration,0,,False
66,"For the choice of a base system, which would serve as the experimental control, we chose to implement a bi-CNN architecture as proposed in Severyn and Moschi i [8]. is state-of-the-art model is preferred over other candidates, i.e., a ention-based CNN [18], for the ease of implementation and parameter optimization. is architecture is fairly robust and in most cases overly excessive parameter tuning is not required.",0,,False
67,"Convolutional Neural Networks. Our implementation follows closely to the experimental se ing in the original paper. Two sets of word embeddings were used: one with 50 dimensions, developed on top of English Wikipedia and the AQUAINT corpus [8], and the other a 300-dimension pre-trained model released by the word2vec project, using 100-billion words from Google News. e sparse word overlapping indicator features were also used in the convolutional layer [11]. e proposed 21 features were incorporated in the fully-connected layer which also combines pooled representations for the question and the answer sentences. e size of the kernel is set to 100 throughout the experiments. We used hyperbolic tangent tanh as the activation function and max pooling in the pooling layer. e network is trained by using stochastic gradient descent with mini batches. e batch size is set to 50 and AdaDelta update [20] was used with  ,"" 0.95. Early stopping was deployed tracking the change of dev set e ectiveness, and as a result the training almost always stopped in 5 to 10 epochs. We also experimented with dropout in two experimental runs by sweeping through a small set of dropout rates {0.1, 0.2, . . . , 0.9}.""",1,Wiki,True
68,"e a ention mechanism can also a ect the e ectiveness of the neural network model. To control for this variable, we implemented a simple a ention layer in the base CNN model to approximate the ABCNN-1 model, which is the simplest form of a ention mechanism proposed in Yin et al [18]. In mathematical terms, an a ention layer takes a question-side feature map Fq  Rnq ×d and an answerside feature map Fa  Rna ×d as input. Here, nq and na denote the maximum question/answer sentence length, respectively, and d denotes the dimension of the word embeddings.",0,,False
69,"1Some previous work chose to remove questions that contain no answers and led to two inconsistent data splits ""raw"" an ""clean"", so results on one split are not directly comparable to those on the other [7].",0,,False
70,1018,0,,False
71,Short Research Paper,0,,False
72,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
73,Table 2: E ectiveness results on TREC QA and WikiQA datasets. Best-performing runs in each word-embedding group are un-,1,TREC,True
74,derlined and the overall best result on individual benchmarks printed in boldface. Relative improvements (+/-%) are measured,0,,False
75,"against the group control (base system). Signi cant di erences with respect to bagged LambdaMART and the group control are indicated by /and /, respectively, for p < 0.05/p < 0.01 using the paired t-test.",0,,False
76,TREC QA,1,TREC,True
77,WikiQA,1,Wiki,True
78,System,0,,False
79,A n? Drop? MAP,1,MAP,True
80,MRR,0,,False
81,S@1,0,,False
82,MAP,1,MAP,True
83,MRR,0,,False
84,S@1,0,,False
85,Runs (AQUAINT/Wikipedia),1,AQUAINT,True
86,CNN,0,,False
87,×,0,,False
88,× 76.2,0,,False
89,80.9,0,,False
90,73.7,0,,False
91,66.0,0,,False
92,67.4,0,,False
93,52.3,0,,False
94,Combined Model,0,,False
95,×,0,,False
96,× 77.9 (+2.2%) 82.2 (+1.6%) 74.7 (+1.4%) 67.2 (+1.8%) 68.5 (+1.6%) 53.9 (+3.1%),0,,False
97,Combined Model,0,,False
98,×,0,,False
99,78.2 (+2.6%) 83.7 (+3.5%) 76.8 (+4.2%) 64.7 (-2.0%) 65.7 (-2.5%) 48.6 (-7.1%),0,,False
100,CNN Combined Model Combined Model,0,,False
101,× 75.4,0,,False
102,79.9,0,,False
103,71.6,0,,False
104,65.3,0,,False
105,66.8,0,,False
106,52.7,0,,False
107,× 77.2 (+2.4%) 81.1 (+1.5%) 72.6 (+1.4%) 70.0 (+7.2%) 71.4 (+6.9%) 58.4 (+10.8%),0,,False
108,77.3 (+2.5%) 82.0 (+2.6%) 74.7 (+4.3%) 69.0 (+5.7%) 70.9 (+6.1%) 58.4 (+10.8%),0,,False
109,Runs (Google News),0,,False
110,CNN,0,,False
111,×,0,,False
112,Combined Model,0,,False
113,×,0,,False
114,Combined Model,0,,False
115,×,0,,False
116,CNN Combined Model Combined Model,0,,False
117,× 76.1,0,,False
118,82.3,0,,False
119,75.8,0,,False
120,67.3,0,,False
121,69.1,0,,False
122,57.2,0,,False
123,× 73.8 (-3.0%) 79.2 (-3.8%) 70.5 (-7.0%) 69.2 (+2.8%) 70.2 (+1.6%) 56.0 (-2.1%),0,,False
124,74.8 (-1.7%) 80.1 (-2.7%) 71.6 (-5.5%) 69.2 (+2.8%) 70.7 (+2.3%) 56.4 (-1.4%),0,,False
125,× 75.0,0,,False
126,81.1,0,,False
127,73.7,0,,False
128,66.3,0,,False
129,68.3,0,,False
130,54.7,0,,False
131,× 76.5 (+2.0%) 82.5 (+1.7%) 74.7 (+1.4%) 69.4 (+4.7%) 71.2 (+4.2%) 57.6 (+5.3%),0,,False
132,76.3 (+1.7%) 82.5 (+1.7%) 74.7 (+1.4%) 67.9 (+2.4%) 69.7 (+2.0%) 56.0 (+2.4%),0,,False
133,Reference methods Bagged LambdaMART LSTM [12] CNN [8] aNMM [15] ABCNN-3 [18] PairwiseRank + SentLevel [7],0,,False
134,75.7,0,,False
135,81.3,0,,False
136,72.6,0,,False
137,63.0,0,,False
138,63.8,0,,False
139,46.5,0,,False
140,71.3,0,,False
141,79.1,0,,False
142,--,0,,False
143,--,0,,False
144,74.6,0,,False
145,80.8,0,,False
146,--,0,,False
147,--,0,,False
148,75.0,0,,False
149,81.1,0,,False
150,--,0,,False
151,--,0,,False
152,--,0,,False
153,--,0,,False
154,69.2,0,,False
155,71.1,0,,False
156,78.0,0,,False
157,83.4,0,,False
158,70.1,0,,False
159,71.8,0,,False
160,"e matrix A  Rnq ×na representing the ""a ention"" is computed internally to the layer as follows:",0,,False
161,"Ai, j , 1 +",0,,False
162,"1 Fq [i, :] - Fa [j, :]",0,,False
163,",",0,,False
164,(2),0,,False
165,"with · being the euclidean distance function. en, the layer",0,,False
166,"generates two new a ention-based feature maps, Fq and Fa , which are to be combined in the follow-up convolutional layers:",0,,False
167,"Fq , AWq Fa ,"" AT Wa ,""",0,,False
168,(3),0,,False
169,where Wq  Rna ×d and Wa  Rnq ×d denote model weights which are to be learned from the data.,0,,False
170,3.2 Baselines,0,,False
171,"A number of published results were included as reference runs in the experiments [8, 12, 15, 18], including a recently proposed neural model PairwiseRank [7]. For comparing with learning-to-rank systems, a Bagged LambdaMART model trained using RankLib2 over the 21 sentence features is included. e bagged LambdaMART was trained by optimizing NDCG@20 with subsampling rate 0.7, feature sampling rate 0.3, using 300 bags.",0,,False
172,2h ps://www.lemurproject.org/ranklib.php,0,,False
173,3.3 Results,0,,False
174,"Table 2 gives the e ectiveness results for the TREC QA and the WikiQA datasets. e e ectiveness of answer selection is measured by Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and Success at 1 (S@1), using the trec eval package following Severyn and Moschi i [8]. e experimental runs are divided into four groups, according to the word embeddings in use (AQUAINT/Wikipedia or Google News) and whether the a ention mechanism is enabled. In each group, the original CNN model is the experimental control, and runs with external features combined (denoted as Combined Model) are the treatments.",1,TREC,True
175,"On both sets of data, the PairwiseRank model gives the best results. e baseline bagged LambdaMART model appears to be strong on the TREC QA data, beating a number of neural network models, but it does not appear particularly e ective on the more challenging WikiQA dataset.",1,TREC,True
176,"Our base CNN model is found to be superior to the learning-torank model, and it gave be er results than the original implementation [8] on the TREC QA benchmark. In general, the Combined Model reliably improves the base CNN model. All experimental con gurations appear to bene t from the inclusion of the 21 extra features, with two exceptions: runs using GoogleNews embeddings on the TREC QA benchmark, and one of the dropout runs on the",1,TREC,True
177,1019,0,,False
178,Short Research Paper,0,,False
179,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
180,"WikiQA data. On the TREC QA benchmark, we saw an increase of 1.3%­4.3% in the three evaluation metrics. On WikiQA, the increase is around 1.6%­7.2% in MAP/MRR and 2.4%­10.8% in S@1.",1,Wiki,True
181,"ese increases are in most cases consistent with each other, except that in one particular con guration a decreased S@1 is observed alongside improved MAP and MRR scores.",1,MAP,True
182,"e AQUAINT/Wikipedia embeddings appear to have a slight advantage over the Google News embeddings. e best performing runs on both benchmarks using this embedding achieved a marked increased in e ectiveness compared to the best known results. On the TREC QA benchmark, however, the Combined Model with dropout surpassed the PairwiseRank model [7] in both MAP and MRR. On WikiQA, the Combined Model with the a ention mechanism outperformed ABCNN-3 (i.e., a stronger variant of ABCNN-1), with the achieved e ectiveness only marginally below the e ectiveness of the PairwiseRank model. In most cases, we found that the Combined Model works the best when the a ention mechanism is used together without dropout. We conjecture that in this case the a entional CNN model works di erently to the data, as external features on their own tend to t certain aspects well enough.",1,AQUAINT,True
183,"Based on these results, we conclude that, for answer sentence selection, combining the proposed external features into a convolutional neural architecture has a bene t of improving overall modeling e ectiveness. is improvement is evident even when the neural architecture is slightly altered to perform advanced neural functions such as a ention mechanism or dropout. e evidence is that the highly tuned convolutional neural architecture failed to model certain aspects in the data, which can be captured with a set of simple features. is points to a limitation of neural network methodology previously not mentioned in research on question answering.",1,ad,True
184,4 RELATED WORK,0,,False
185,"ere is a rich body of work in question answering focused on answer sentence selection [7, 8, 11, 12, 14, 15, 17­19]. Most of these e orts address the architectural issues in neural network models. Yu et al. [19] utilize a CNN architecture to model question-answer pairs, and this approach was taken by Yang et al [17] and Severyn and Moschi i [8], who later expanded the network architecture into a bi-CNN model. Wang et al. [14] decomposed vectors into similar/dissimilar components and used a two-channel CNN to capture the signals. e a ention mechanism was investigated in Yin et al. [18] and Yang et al. [15]. He and Lin [4] used a Bidirectional LSTM to model the context of input sentences. Rao et al. [7] proposed a pairwise ranking method that uses two bi-CNN architectures to perform sentence-level pairwise ranking.",1,ad,True
186,"Feature engineering has been a popular methodology for modeling question-answer structure, and is still actively used in nonfactoid question answering or answer re-ranking [10, 16]. One commonality between these specialized tasks is a focus on retrieving or ranking passage-level answers [6]. Surdeanu et al. [10] proposed using a translation model to capture the mapping between the high-level linguistic representations of the question and the answer. Yang et al. [16] proposed using query matching, semantic, and context features to select answer sentences for non-factoid questions.",0,,False
187,5 CONCLUSIONS,0,,False
188,We provide empirical evidence to support the use of conventional,0,,False
189,features in deep learning models on the task of answer sentence,0,,False
190,selection. We show that a convolutional neural network model ben-,0,,False
191,e ts from a group of commonly used text features and outperforms,0,,False
192,the best published result on a commonly used question answering,0,,False
193,benchmark. e fact that neural networks can still bene t from,0,,False
194,these conventional features may point to new possibilities in the,0,,False
195,"evolution of new neural architectures. In future work, we will seek",0,,False
196,"to expand this analysis to other neural architectures, such as LSTM-",0,,False
197,"CNN or recurrent neural networks, and other question answering",0,,False
198,benchmarks that are more recent and larger.,0,,False
199,REFERENCES,0,,False
200,"[1] Timothy G. Armstrong, Alistair Mo at, William Webber, and Justin Zobel. 2009. Improvements at Don'T Add Up: Ad-hoc Retrieval Results Since 1998. In Proceedings of CIKM '09. ACM, New York, NY, USA, 601­610.",1,hoc,True
201,"[2] Ma hew W. Bilo i, Jonathan Elsas, Jaime Carbonell, and Eric Nyberg. 2010. Rank Learning for Factoid estion Answering with Linguistic and Semantic Constraints. In Proceedings of CIKM '10. ACM, New York, NY, USA, 459­468.",0,,False
202,"[3] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Ma Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to Rank Using Gradient Descent. In Proceedings of ICML '05. ACM, New York, NY, USA, 89­96.",1,ad,True
203,[4] Hua He and Jimmy Lin. 2016. Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement. In Proceedings of NAACL '16. 937­948.,0,,False
204,"[5] Tapas Kanungo and David Orr. 2009. Predicting the Readability of Short Web Summaries. In Proceedings of WSDM '09. ACM, New York, NY, USA, 202­211.",1,ad,True
205,"[6] Mostafa Keikha, Jae Hyun Park, W. Bruce Cro , and Mark Sanderson. 2014. Retrieving Passages and Finding Answers. In Proceedings of ADCS '14. ACM, New York, NY, USA, Article 81, 4 pages.",0,,False
206,"[7] Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks. In Proceedings of CIKM '16. ACM, 1913­1916.",0,,False
207,"[8] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In Proceedings of SIGIR '15. ACM, 373­382.",0,,False
208,"[9] Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai, Jingjing Liu, and Ming-Wei Chang. 2015. Open Domain estion Answering via Semantic Enrichment. In Proceedings of WWW '15. 1045­1055.",0,,False
209,"[10] Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to Rank Answers to Non-Factoid estions from Web Collections. Computational Linguistics 37, 2 (April 2011), 351­383.",0,,False
210,"[11] Kateryna Tymoshenko, Daniele Bonadiman, and Alessandro Moschi i. 2016. Convolutional Neural Networks vs. Convolution Kernels: Feature Engineering for Answer Sentence Reranking. In Proceedings of NAACL '16. 1268­1278.",1,ad,True
211,[12] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in estion Answering. In Proceedings of ACL '15. 707­712.,0,,False
212,"[13] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A asi-Synchronous Grammar for QA. In Proceedings of EMNLP '07. 22­32.",0,,False
213,"[14] Zhiguo Wang, Haitao Mi, and Abraham I ycheriah. 2016. Sentence Similarity Learning by Lexical Decomposition and Composition. In Proceedings of COLING 2016. Osaka, Japan, 1340­1349.",0,,False
214,"[15] Liu Yang, Qingyao Ai, Jiafeng Guo, and W. Bruce Cro . 2016. aNMM: Ranking Short Answer Texts with A ention-Based Neural Matching Model. In Proceedings of CIKM '16. ACM, 287­296.",0,,False
215,"[16] Liu Yang, Qingyao Ai, Damiano Spina, Ruey-Cheng Chen, Liang Pang, W. Bruce Cro , Jiafeng Guo, and Falk Scholer. 2016. Beyond Factoid QA: E ective Methods for Non-factoid Answer Sentence Retrieval. In Proceedings of ECIR '16. Springer International Publishing, 115­128.",0,,False
216,"[17] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain estion Answering. In Proceedings of EMNLP '15. Lisbon, Portugal, 2013­2018.",1,Wiki,True
217,"[18] Wenpeng Yin, Hinrich Schtze, Bing Xiang, and Bowen Zhou. 2015. ABCNN: A ention-Based Convolutional Neural Network for Modeling Sentence Pairs. arXiv:1512.05193 [cs] (Dec. 2015).",0,,False
218,"[19] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep Learning for Answer Sentence Selection. arXiv:1412.1632 [cs] (Dec. 2014).",0,,False
219,[20] Ma hew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 (2012).,1,ad,True
220,1020,0,,False
221,,0,,False

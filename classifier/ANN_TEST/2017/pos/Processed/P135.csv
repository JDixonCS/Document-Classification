,sentence,label,data,regex
0,Session 2A: Search Interaction 1,1,Session,True
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,A Study of Snippet Length and Informativeness,0,,False
3,"Behaviour, Performance and User Experience",0,,False
4,David Maxwell,0,,False
5,"School of Computing Science University of Glasgow Glasgow, Scotland",0,,False
6,d.maxwell.1@research.gla.ac.uk,0,,False
7,Leif Azzopardi,0,,False
8,"Computer & Information Sciences University of Strathclyde Glasgow, Scotland leif.azzopardi@strath.ac.uk",0,,False
9,Yashar Moshfeghi,0,,False
10,"School of Computing Science University of Glasgow Glasgow, Scotland",0,,False
11,Yashar.Moshfeghi@glasgow.ac.uk,0,,False
12,ABSTRACT,0,,False
13,"e design and presentation of a Search Engine Results Page (SERP) has been subject to much research. With many contemporary aspects of the SERP now under scrutiny, work still remains in investigating more traditional SERP components, such as the result summary. Prior studies have examined a variety of di erent aspects of result summaries, but in this paper we investigate the in uence of result summary length on search behaviour, performance and user experience. To this end, we designed and conducted a withinsubjects experiment using the TREC AQUAINT news collection with 53 participants. Using Kullback-Leibler distance as a measure of information gain, we examined result summaries of di erent lengths and selected four conditions where the change in information gain was the greatest: (i) title only; (ii) title plus one snippet; (iii) title plus two snippets; and (iv) title plus four snippets. Findings show that participants broadly preferred longer result summaries, as they were perceived to be more informative. However, their performance in terms of correctly identifying relevant documents was similar across all four conditions. Furthermore, while the participants felt that longer summaries were more informative, empirical observations suggest otherwise; while participants were more likely to click on relevant items given longer summaries, they also were more likely to click on non-relevant items. is shows that longer is not necessarily be er, though participants perceived that to be the case ­ and second, they reveal a positive relationship between the length and informativeness of summaries and their a ractiveness (i.e. clickthrough rates). ese ndings show that there are tensions between perception and performance when designing result summaries that need to be taken into account.",1,ad,True
14,CCS CONCEPTS,0,,False
15,·Information systems Search interfaces;,0,,False
16,"ACM Reference format: David Maxwell, Leif Azzopardi, and Yashar Moshfeghi. 2017. A Study of Snippet Length and Informativeness. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080824",0,,False
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080824",1,ad,True
18,1 INTRODUCTION,1,DUC,True
19,"Interactive Information Retrieval (IIR) is a complex, non-trivial process where a searcher undertakes a variety of di erent actions during a search session [16]. Core to their experience and success is the Search Engine Results Page (SERP), with its presentation and design over the years having been subject to much research. With more complex components now becoming commonplace in modern day Web search engines (such as the information card [5, 36] or social annotations [35]), much work however still remains on examining how more traditional SERP components (such as result summaries) are designed and presented to end users.",1,ad,True
20,"Result summaries have traditionally been viewed as the `ten blue links' with the corresponding URL of the associated document, and one or more textual snippets of keywords-in-context from the document itself, approximately 130-150 characters (or two lines) in length [15]. Numerous researchers have explored result summaries in a variety of di erent ways, such as: examining their length [11, 19, 38]; the use of thumbnails [47, 52]; their a ractiveness [9, 14]; and the generation of query-biased snippets [41, 48].",1,ad,True
21,"e performance of users has broadly been evaluated in a limited fashion (e.g. by examining task completion times). In this work, we are interested in how the length and information content of result summaries a ects SERP interactions and a user's ability to select relevant over non-relevant items. Prior research has demonstrated that longer result summaries tend to lower completion times for informational tasks (where users need to nd one relevant document) [11], but does this hold in other contexts, speci cally for ad-hoc retrieval, where users need to nd several relevant items? Furthermore, how does the length and information associated with longer result summaries a ect the user's ability to discern the relevant from the non-relevant?",1,ad,True
22,"is work therefore serves as an investigation into the e ects of search behaviour and search performance when we vary (i) result summary snippet lengths, and by doing so (ii) the information content within the summaries. To this end, a within-subjects crowdsourced experiment (n ,"" 53) was designed and conducted. Under ad-hoc topic retrieval, participants used four di erent search interfaces, each with a di erent size of result summary. Findings allow us to address the two main research questions of this study. RQ1 How does the value of information gain represented as snippet length a ect behaviour, performance and user experience? RQ2 Does information gain ­ again represented as snippet length ­ affect the decision making ability and accuracy (identifying relevant documents) of users? We hypothesise that longer and more informative snippets will enable users to make be er quality decisions (i.e. higher degrees of accurately identifying relevant content).""",1,ad-hoc,True
23,135,0,,False
24,Session 2A: Search Interaction 1,1,Session,True
25,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
26,2 RELATED WORK,0,,False
27,"As previously mentioned, the design and presentation of SERPs has been examined in depth. Researchers have examined various aspects of SERPs, and how the designs of such aspects in uence the behaviour of users. Here, we provide a summary of the various aspects that have been investigated. Speci cally, we focus upon: the layout of SERPs; the size of SERPs; how snippet text is generated; and how much text should be presented within each result summary ­ the la er being the main focus of this work.",0,,False
28,2.1 SERP Layouts and Presentation,0,,False
29,"Early works regarding the presentation of result summaries [6, 12] examined approaches to automatically categorise result summaries for users, similar to the categorisation approach employed by early search engines. Chen and Dumais [6] developed an experimental system that automatically categorised result summaries on-the- y as they were generated. For a query, associated categories were then listed as verticals, with associated document titles provided underneath each category header. Traditional result summaries were then made available when hovering over a document title. Subjects of a user study found the interface easier to use than the traditional `ten blue links' approach - they were 50% faster at nding information displayed in categories. is work was then extended by Dumais et al. [12], where they explored the use of hover text to present additional details about search results based upon user interaction. Searching was found to be slower with hover text, perhaps due to the fact that explicit decisions about when to seek additional information (or not to) were required.",1,ad,True
30,"Alternatives to the traditional, linear list of result summaries have also been trialled (like grid-based layouts [8, 20, 40]). For example, Krammerer and Beinhaur [20] examined di erences in user behaviour when interacting with a standard list interface, compared against a tabular interface (title, snippet and URL stacked horizontally in three columns for each result), and a grid-based layout (result summaries placed in three columns). Users of the grid layout spent more time examining result summaries. e approach demonstrated promise in overcoming issues such as position bias [10], as observed by Joachims et al. [17].",1,ad,True
31,"Marcos et al. [34] performed an eye-tracking user study examining the e ect of user behaviour while interacting with SERPs ­ and whether the richness of result summaries provided on a SERP (i.e. result summaries enriched with metadata from corresponding pages) impacted upon the user's search experience. Enriched summaries were found to help capture a user's a ention. Including both textual and visual representations of a document when presenting results could have a positive e ect on relevance assessment and query reformulation [18]. Enriched summaries were also examined by Ali et al. [2] in the context of navigational tasks. Striking a good balance between textual and visual cues were shown to be er support user tasks, and search completion time.",1,ad,True
32,2.2 Generating Snippet Text,0,,False
33,"Users can be provided with an insight by result summaries as to whether a document is likely to be relevant or not [14]. Consequently, research has gone into examining di erent kinds of snippets, and how long a snippet should be. Work initially focused",0,,False
34,"upon how these summaries should be generated [30, 31, 39, 48, 51]. ese early works proposed the idea of summarising documents",0,,False
35,"with respect to the query (query-biased summaries) or keywords-incontext ­ as opposed to simply extracting the representative or lead sentences from the document [29]. Tombros and Sanderson [48] showed that subjects of their study were likely to identify relevant documents more accurately when using query-biased summaries, compared to summaries simply generated from the rst few sentences of a given document. ery-biased summaries have also been recently shown to be preferred on mobile devices [45].",1,ad,True
36,"When constructing snippets using query-biased summaries, Rose et al. [41] found that a user's perceptions of the result's quality were in uenced by the snippets. If snippets contained truncated sentences or many fragmented sentences (text choppiness), users perceived the quality of the results more negatively, regardless of length. Kanungo and Orr [21] found that poor readability also impacts upon how the resultant snippets are perceived. ey maintain that readability is a crucial presentation a ribute that needs to be considered when generating a query-biased summary. Clarke et al. [9] analysed thousands of pairs of snippets where result A appeared before result B, but result B received more clicks than result A. As an example, they found results with snippets which were very short (or missing entirely) had fewer query terms, were not as readable, and a racted fewer clicks. is led to the formulation of several heuristics relating to document surrogate features, designed to emphasise the relationship between the associated page and generated snippet. Heuristics included: (i) ensuring that all query terms in the generated snippet (where possible); (ii) withholding the repeating of query terms in the snippet if they were present in the page's title; and (iii) displaying (shortened) readable URLs.",1,ad,True
37,"Recent work has examined the generation of snippets from more complex angles ­ from manipulating underlying indexes [4, 49] to language modelling [14, 32], as well as using user search data to improve the generation process [1, 42]. Previous generation approaches also may not consider what parts of a document searchers actually nd useful. Ageev et al. [1] incorporated into a new model post-click searcher behaviour data, such as mouse cursor movements and scrolling over documents, producing behaviour-biased snippets. Results showed a marked improvement over a strong text-based snippet generation baseline. Temporal aspects have also been considered ­ Svore et al. [46] conducted a user study, showing that users preferred snippet text with trending content in snippets when searching for trending queries, but not so for general queries.",1,corpora,True
38,2.3 Results per Page,0,,False
39,"Today, a multitude of devices are capable of accessing the World Wide Web (WWW) ­ along with a multitude of di erent screen resolutions and aspect ratios. e question of how many result summaries should be displayed per page ­ or results per page (RPP) ­ therefore becomes hugely important, yet increasingly di cult to answer. Examining behavioural e ects on mobile devices when interacting with SERPs has a racted much research as of late (e.g. [24­ 26]), and with each device capable of displaying a di erent number of results above-the-fold, recent research has shown that the RPP value can in uence the behaviour of searchers [17, 25]. Understanding this behaviour can help guide and inform those charged with designing contemporary user interfaces.",0,,False
40,136,0,,False
41,Session 2A: Search Interaction 1,1,Session,True
42,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
43,"In a Google industry report, Linden [33] however stated that users desired more than 10RPP, despite the fact that increasing the RPP yielded a 20% drop in tra c; it was hypothesised that this was due to the extra time required to dispatch the longer SERPs. is drop however be a ributed to other reasons. Oulasvirta et al. [37] discusses the paradox of choice [43] in the context of search, where more options (results) ­ particularly if highly relevant ­ will lead to poorer choice and degrade user satisfaction. In terms of user satisfaction, modern search engines can therefore be a victim of their own success, presenting users with choice overload. Oulasvirta et al. [37] found that presenting users with a six-item search result list was associated with higher degrees of satisfaction, con dence with choices and perceived carefulness than an a list of 24 items.",1,ad,True
44,"Kelly and Azzopardi [22] broadly agreed with the ndings by Oulasvirta et al. [37]. Here, the authors conducted a betweensubjects study with three conditions, where subjects were assigned to one of three interfaces - the baseline interface, showing 10RPP (the `ten blue links'), and two interfaces displaying 3RPP and 6RPP respectively. eir ndings showed that individuals using the 3RPP and 6RPP interfaces spent signi cantly longer examining top-ranking results and were more likely to click on higher ranked documents than those on the 10RPP interface. Findings also suggested that subjects using the interfaces showing fewer RPP found it comparatively easier to nd relevant content than those using the 10RPP interface. However, no signi cant di erence was found between the number of relevant items found across the interfaces. Currently, 10RPP is still considered the de-facto standard [15].",1,ad,True
45,2.4 Snippet Lengths: Longer or Shorter?,0,,False
46,"Snippet lengths have been examined in a variety of ways. A user study by Paek et al. [38] compared a user's preference and usability against three di erent interfaces for displaying result summaries. With question answering tasks, the interfaces: displayed a normal SERP (i.e. a two line snippet for each summary, with a clickable link); an instant interface, where an expanded snippet was displayed upon clicking it; and a dynamic interface, where hovering the cursor would trigger the expanded snippet. e instant view was shown to allow users to complete the given tasks in less time than the normal baseline, with half of participants preferring this approach.",0,,False
47,"Seminal work by Cutrell and Guan [11] explored the e ect of di erent snippet lengths (short: 1 line, medium: 2-3 lines; and long: 6-7 lines). ey found that longer snippets signi cantly improved performance for informational tasks (e.g. `Find the address for Newark Airport.'). Users performed be er for informational queries as snippet length increased. is work was followed up by Kaisser et al. [19]. ey conducted two experiments that estimated the preferred snippet length according to answer type (e.g. nding a person, time, or place), and comparing the results of the preferred snippet lengths to users' preferences to see if this could be predicted.",1,ad,True
48,"e preferred snippet length was shown to depend upon the type of answer expected, with greater user satisfaction shown for the snippet length predicted by their technique.",0,,False
49,"More contemporary work has begun to examine what snippet sizes are appropriate for mobile devices. Given smaller screen sizes, this is important ­ snippet text considered acceptable on a computer screen may involve considerable scrolling/swiping on",0,,False
50,"a smaller screen. Kim et al. [27] found that subjects using longer snippets on mobile devices exhibited longer search times and similar search accuracy under informational tasks1. Longer reading times and frequent scrolling/swiping (with more viewport movements) were exhibited. Longer snippets did not therefore appear to be very useful on a small screen ­ an instant or dynamic snippet approach (as per Paek et al. [38]) may be useful for mobile search, too.",1,ad,True
51,"e presentation of result summaries has a strong e ect on the ability of a user to judge relevancy [14]. Relevant documents may be overlooked due to uninformative summaries ­ but conversely, non-relevant documents may be examined due to a misleading summary. However, longer summaries also increase the examination cost, so there is likely a trade-o between informativeness/accuracy and length/cost. e current, widely accepted standard for result summaries are two query-based snippets/lines [15]. is work examines whether increasing and decreasing the length (and consequently the informativeness) of result summary snippets a ects user accuracy and costs of relevance decisions in the context of ad-hoc topic search, where multiple relevant documents are sought.",1,ad,True
52,3 EXPERIMENTAL METHOD,0,,False
53,"To address our two key research questions outlined in Section 1, we conducted a within-subjects experiment. is allowed us to explore the in uence of snippet length and snippet informativeness on search behaviours, performance and user experience. Subjects used four di erent search interfaces, each of which varied the way in which result summaries were presented to them.",1,ad,True
54,"To decide the length and informativeness of the result summaries, we performed a preliminary analysis to determine the average length (in words) and informativeness (as calculated by the Kullback-Leibler distance [28] to measure information gain, or relative entropy) of result summaries with the title and varying numbers of snippet fragments (0­10). e closer the entropy value is to zero, the more information gained. Figure 1 plots the number of words, the information gain, and the information gain per word2. It is clear from the plot that a higher level of information gain was present in longer snippets. However, as the length increases with each additional snippet fragment added, the informativeness per word decreased. Consequently, for this study, we selected the four di erent interface conditions in the region where informativeness had the highest change, i.e. from zero to four. e conditions we selected for the study were therefore:",1,ad,True
55,T0 where only the title for each result summary were presented;,0,,False
56,"T1 where for each result summary, a title and one query-biased snippet fragment were presented;",0,,False
57,T2 where a title and two query-biased snippet fragments were presented; and,0,,False
58,"T4 where a title and four query-biased snippet fragments were presented,",0,,False
59,"where our independent variable is snippet informativeness, controlled by the length. Figure 2 provides an example of the di erent",0,,False
60,"1 e tasks considered by Kim et al. [27] were similar to those de ned by Cutrell and Guan [11], where a single relevant document was sought. 2To obtain these values, we submi ed over 300 queries from a previous study (refer to Azzopardi et al. [3]) conducted on similar topics and on the same collection to the search system that we used.",0,,False
61,137,0,,False
62,Session 2A: Search Interaction 1,1,Session,True
63,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
64,"result summaries in each condition. e remainder of this section details our methodology for this experiment, including a discussion of: the corpus, topics and system used (Subsection 3.1); how we generated snippets (Subsection 3.2); the behaviours we logged (Subsection 3.3); how we obtained the opinions of subjects regarding their experience (Subsection 3.4); and further details on our study, including measures taken for quality control (Subsection 3.5).",0,,False
65,"3.1 Corpus, Search Topics and System",0,,False
66,"For this experiment, we used the TREC AQUAINT test collection. Using a traditional test collection provided us with the ability to easily evaluate the performance of subjects. e collection contains over one million newspaper articles from the period 1996-2000. Articles were gathered from three newswires: the Associated Press (AP); the New York Times (NYT); and Xinhua.",1,TREC,True
67,"We then selected a total of ve topics from the TREC 2005 Robust Track, as detailed by Voorhees [50]. e topics selected were:  341 (Airport Security);  347 (Wildlife Extinction);  367 (Piracy);  408 (Tropical Storms); and  435 (Curbing Population Growth). We selected topic  367 as a practice topic so that subjects could familiarise themselves with the system. ese topics were chosen based upon evidence from a previous user study with a similar setup, where it was shown that the topics were of similar di culty [23]. For each subject, the remaining four topics were assigned to an interface (one of T0, T1, T2 or T4) using a Latin-square rotation.",1,TREC,True
68,"To ground the search tasks, subjects of the experiment were instructed to imagine that they were newspaper reporters, and were required to gather documents to write stories about the provided topics. Subjects were told to nd as many relevant documents as they could during the allo ed time, which was 10 minutes per topic ­ herea er referred to as a search session. With the traditional components of a SERP, such as the query box and result summaries present (refer to Figure 3), subjects were instructed to mark documents they considered relevant by clicking on the `Mark as Relevant' bu on within the document view ­ accessed by clicking on a result summary he or she thought was relevant. Coupled with a two minute period to familiarise themselves with the system (using topic  367), subjects spent approximately 4550 minutes undertaking the complete experiment when pre- and post-task surveys were accounted for.",1,ad,True
69,"For the underlying search engine, we used the Whoosh Information Retrieval (IR) toolkit 3. We used BM25 as the retrieval algorithm (b ,"" 0.75), but with an implicit ANDing of query terms to restrict the set of retrieved documents to only those that contained all the query terms provided. is was chosen as most search systems implicitly AND terms together.""",0,,False
70,3.2 Snippet Generation,0,,False
71,"For interfaces T2 and T4, each result summary presented to the subjects required one or more textual snippets from the corresponding document. ese snippet fragments were query-biased [48], and were generated by scoring sentences according to BM25 and selecting fragments from those sentences. Fragments were then extracted",0,,False
72,3Whoosh can be accessed at h ps://pypi.python.org/pypi/Whoosh/.,0,,False
73,Words,0,,False
74,150 100,0,,False
75,50 00 -2 -3 -4 -5 0,0,,False
76,Length and Informativeness,0,,False
77,1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10,0,,False
78,IG,0,,False
79,IG / Word,0,,False
80,0,0,,False
81,-0.2,0,,False
82,-0.4 0 1 2 3 4 5 6 7 8 9 10,0,,False
83,"Number of Snippet Fragments Figure 1: Plots showing the length (in words), informativeness (in information gain, IG) and the information gain (IG) per word for title, plus 0 to 10 snippets. e closer the value is to zero, the more information that is gained.",0,,False
84,"from the ordered series of sentences, by identifying query terms within those sentences with a window of 40 characters from either side of the term. Figure 2 provides a complete, rendered example of the result summaries generated by each of the four interfaces. Each result summary contains a document title, a newswire source (acting as a replacement for a document URL), and, if required, one or more textual snippets.",0,,False
85,3.3 Behaviours Logged,0,,False
86,"In order for us to address our research questions, our experimental system was required to log a variety of behavioural a ributes for each subject as they performed the variety of actions that take place during a search session. Search behaviours were operationalised over three types of measures: (i) interaction, (ii) performance, and (iii) the time spent undertaking various search activities. All behavioural data was extracted from the log data produced by our system, and from the TREC 2005 Robust Track QRELs [50]. All data was recorded with the interface and topic combination used by the subject at the given time.",1,ad,True
87,"Interaction measures included the number of queries issued, the number of documents viewed, the number of SERPs viewed, and the greatest depths in the SERPs to which subjects clicked on ­ and hovered over ­ result summaries.",0,,False
88,"Performance measures included a count of the documents marked as relevant by the subject, the number of documents marked that were also TREC relevant ­ as well as TREC non-relevant, and P@k",1,TREC,True
89,138,0,,False
90,Session 2A: Search Interaction 1,1,Session,True
91,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
92,T0 Venezuela Declares 42 Species in Danger of Extinction Xinhua News Service,0,,False
93,T1,0,,False
94,"Venezuela Declares 42 Species in Danger of Extinction ...the mammals in danger of extinction are the giant cachicamo, Margarita and...",0,,False
95,Xinhua News Service,0,,False
96,T2,0,,False
97,Venezuela Declares 42 Species in Danger of Extinction,0,,False
98,"...of animals in danger of extinction and banned game hunting of another 105...affecting the population of existing wildlife, such as the irrational exploitation...",0,,False
99,Xinhua News Service,0,,False
100,T4,0,,False
101,Venezuela Declares 42 Species in Danger of Extinction,0,,False
102,"16 (Xinhua) ­ Venezuela declared 42 wildlife species of animals in danger of...of animals in danger of extinction and banned game hunting of another 105...affecting the population of existing wildlife, such as the irrational exploitation...the mammals in danger of extinction are the giant cachicamo, Margarita and...",0,,False
103,Xinhua News Service,0,,False
104,"Figure 2: Examples of the result summaries generated by each of the four interfaces used in this study. e same document above is used ­ with the circle denoting what interface is being shown (of T0, T1, T2 or T4). Each of the result summaries consists of a title (in blue, underlined), none, one or more snippet fragments (in black, with fragments separated by ellipses), and a newswire source (in green).",0,,False
105,measurements for the performance of the subject's issued queries for a range of rankings.,0,,False
106,"Time-Based measures included the time spent issuing queries, examining SERPs ­ as well as examining result summaries4 ­ and the time spent examining documents. All of these times added together yielded the total search session time, which elapsed once 10 minutes had been reached.",1,ad,True
107,"From this raw data, we could then produce summaries of a search session, producing summarising measures such as the number of documents examined by searchers per query that they issued. We could also calculate from the log data probabilities of interaction, including a given subject's probability of clicking a result summary link, given that it was TREC relevant (P (C |R)) or TREC nonrelevant (P (C |N )) ­ or the probability of marking a document that was clicked, given it was either TREC relevant (P (M |R)) or TREC non-relevant (P (M |N )). Actions such as hover depth over result summaries were inferred from the movement of the mouse cursor, which in prior studies has been shown to correlate strongly with the user's gaze on the screen [7, 44].",1,TREC,True
108,3.4 Capturing User Experiences,0,,False
109,"To capture user experiences, we asked subjects to complete both pre- and post-task surveys for each of the four interface conditions.",0,,False
110,4Result summary times were approximated by dividing the total recorded SERP time by the number of snippets hovered over with the mouse cursor. We believe this is a reasonable assumption to make ­ the timings of hover events proved to be unreliable due to occasional network latency issues beyond our control.,0,,False
111,"Figure 3: Screenshot of the experimental search interface, showing the SERP view, complete with query box (with query `wildlife extinction') and the associated result summaries. In this example screenshot, interface T2 ­ presenting two snippets per result summary ­ is shown.",0,,False
112,"Pre-task surveys consisted of ve questions, each of which was on a seven-point Likert scale (7 ­ strongly agree to 1 ­ strongly disagree). Subjects were sought for their opinions on their: (i) prior knowledge of the topic; (ii) the relevancy of the topic to their lives; (iii) their desire to learn about the topic; (iv) whether they had searched on this topic before; and (v) the perceived di culty to search for information on the topic.",1,ad,True
113,"e same Likert scale was used for post-task surveys, where subjects were asked to judge the following statements: (clarity) ­ the result summaries were clear and concise; (con dence) ­ the result summaries increased my con dence in my decisions; (informativeness) ­ the result summaries were informative; (relevance) ­ the results summaries help me judge the relevance of the document; (readable) ­ the result summaries were readable; and (size) ­ the result summaries were an appropriate size and length.",1,ad,True
114,"At the end of the experiment, subjects completed an exit survey. From ve questions, they were asked to pick which of the four interfaces was the closest t to their experience. We sought opinions on what interface: (most informative) ­ yielded the most informative result summaries; (least helpful) ­ provided the most unhelpful summaries; (easiest) ­ provided the easiest to understand summaries; (least useful) ­ provided the least useful result summaries; and (most preferred) ­ the subject's preferred choice for the tasks that they undertook.",0,,False
115,3.5 Crowdsourced Subjects & ality Control,0,,False
116,"As highlighted by Zuccon et al. [54], crowdsourcing provides an alternative means for capturing user interactions and search behaviours from traditional lab-based user studies. Greater volumes of data can be obtained from more heterogeneous workers at a lower cost ­ all within a shorter timeframe. Of course, pitfalls of a crowdsourced approach include the possibility of workers completing tasks as e ciently as possible, or submi ing their tasks without performing the requested operations [13]. Despite these issues, it has been shown that there is li le di erence in the quality between crowdsourced and lab-based studies [54]. Nevertheless, quality control is a major component of a well-executed crowdsourced experiment [5]. Here, we detail our subjects and precautions taken.",1,ad,True
117,139,0,,False
118,Session 2A: Search Interaction 1,1,Session,True
119,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
120,"e study was run over the Amazon Mechanical Turk (MTurk) platform. Workers from the platform performed a single Human Intelligence Task (HIT), which corresponded to the entire experiment. Due to the expected length of completion for the study (45-50 minutes), subjects who completed the study in full were reimbursed for their time with US$9; a typically larger sum (and HIT duration) than most crowdsourced experiments. A total of 60 subjects took part in the experiment, which was run between July and August, 2016. However, seven subjects were omi ed due to quality control constraints (see below). In all, of the 53 subjects who satis ed the expected conditions of the experiment, 28 were male, with 25 female.",0,,False
121,"e average age of our subjects was 33.8 years (min , 22; max , 48; stde ,"" 7.0), with 19 of the subjects possessing a bachelor's degree or higher, and all expressing a high degree of search literacy, with all subjects stating that they conducted at least ve searches for information online per week. With 53 subjects, each searching over four topics, this meant a total of 212 search sessions were logged.""",0,,False
122,"We examined extra precautionary measures to ensure the integrity of the log data that was recorded. Precautions were taken from several angles. First, workers were only permi ed to begin the experiment on the MTurk platform that: (i) were from the United States, and were native English speakers; (ii) had a HIT acceptance rate of at least 95%; and (iii) had at least 1000 HITs approved. Requiring (ii) and (iii) reduced the likelihood of recruiting individuals who would not complete the study in a satisfactory manner. Recruits were forewarned about the length of the HIT, which was considerably longer than other crowdsourced experiments.",1,ad,True
123,"We also ensured that the computer the subject was a empting the experiment on had a su ciently large screen resolution (1024x768 or greater) so as to display all of the experimental interface on screen. With the experiment being conducted in a Web browser popup window of a xed size, we wanted to ensure that all subjects would be able to see the same number of results on a SERP within the popup window's viewport. As the experiment was conducted via a Web browser, we wanted to ensure that only the controls provided by the experimental apparatus were used, meaning that the popup window had all other browser controls disabled to the best of our ability (i.e. history navigation, etc.). e experimental system was tested on several major Web browsers, across di erent operating systems. is gave us con dence that a similar experience would be had across di erent system con gurations.",1,ad,True
124,"We also implemented a series of log post-processing scripts a er completion of the study to further identify and capture individuals who did not perform the tasks as instructed. It was from here that we identi ed the seven subjects that did not complete the search tasks in a satisfactory way ­ spending less than three of the ten minutes searching. ese subjects were excluded from the study, reducing the number of subjects reported from 60 to 53. Finally, results are reported based upon the rst 360 seconds as some of the remaining subjects didn't fully use all 600 seconds.",0,,False
125,4 RESULTS,0,,False
126,"Both search behaviour and user experience measures were analysed by each interface. To evaluate these data, ANOVAs were conducted using the interfaces as factors; main e ects were examined with  , 0.05. Bonferroni tests were used for post-hoc analysis. It should",1,hoc,True
127,"Table 1: Characters, words and Information Gain (IG) across each of the four interface conditions. An ANOVA test reveals signi cant di erences, with follow-up tests (refer to Section 4) showing that each condition is signi cantly different to others. ere are clearly diminishing returns in information gain as snippet length increases. An IG value closer to zero denotes a higher level of IG. In the table, IG/W. denotes IG per word.",0,,False
128,T0,0,,False
129,T1,0,,False
130,T2,0,,False
131,T4,0,,False
132,Words 6.58±0.01* 25.21±0.06* 44.29±0.10* 77.06±0.13* Chars. 37.37±0.05* 103.29±0.17* 168.36±0.23* 284.78±0.31*,0,,False
133,IG,0,,False
134,-6.35±0.01* -3.59±0.00* -3.00±0.00* -2.67±0.00*,0,,False
135,IG/W. -1.17±0.00* -0.18±0.00* -0.08±0.00* -0.04±0.00*,0,,False
136,Time Per Snippet 3,0,,False
137,Seconds,0,,False
138,2,0,,False
139,1 T0,0,,False
140,T1,0,,False
141,T2,0,,False
142,T4,0,,False
143,Interface Condition,0,,False
144,"Figure 4: Plot showing the mean time spent examining result summaries across each of the four interfaces examined. Note the increasing mean examination time as the snippet length increases, from T0T4.",0,,False
145,be noted that the error bars as shown in the plots for Figures 4 and 5 refer to the standard error.,0,,False
146,"To check whether the interfaces were di erent with respect to snippet length and information gain, we performed an analysis of the observed result summaries. Table 1 summarises the number of words and characters that result summaries contained on average. As expected, the table shows an increasing trend in words and characters as snippet lengths increase. Information gain for each snippet was then calculated using the KullbackLeibler distance [28] to measure information gain (e.g. relative entropy). Statistical testing showed that the di erences between snippet length (F (3, 208) ,"" 1.2x105, p < 0.001) and information gain (F (3, 208 "","" 2.6x105, p < 0.001)) were signi cant. Follow up tests revealed that this was the case over all four interfaces, indicating that our conditions were di erent on these dimensions. ese""",0,,False
147,ndings provide some justi cation for our choices for the number of snippet fragments present for each interface ­ a diminishing increase in information gain a er four snippets suggested that there wouldn't be much point generating anything longer.,0,,False
148,4.1 Search Behaviours,0,,False
149,"Interactions. Table 2 presents the mean (and standard deviations) of the number of queries issued, the number of SERPs viewed per query, documents clicked per query, and the click depth per query over each of the four interfaces examined. Across the four di erent interfaces, there were no signi cant di erences reported between",0,,False
150,140,0,,False
151,Session 2A: Search Interaction 1,1,Session,True
152,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
153,"Table 2: Summary table of both interaction and performance measures over each of the four interfaces evaluated. For each measure examined, no signi cant di erences are reported across the four interfaces.",0,,False
154,Number of eries Number of SERP Pages per ery Number of Docs Clicked per ery Depth per ery,0,,False
155,P@10 Number of Documents Marked Relevant Number of TREC Rels Found Number of Unjudged Docs Marked Relevant,1,TREC,True
156,T0,0,,False
157,3.72± 0.34 2.87± 0.29 4.23± 0.55 24.47± 2.96 0.25± 0.02 6.68± 0.66 2.58± 0.34 1.85± 0.32,0,,False
158,T1,0,,False
159,3.19± 0.35 2.69± 0.23 4.83± 0.54 22.87± 2.47 0.23± 0.02 7.00± 0.63 2.28± 0.25 2.08± 0.29,0,,False
160,T2,0,,False
161,3.30± 0.35 2.43± 0.13 5.14± 0.66 20.02± 1.46 0.27± 0.02 6.49± 0.58 2.47± 0.28 1.98± 0.24,0,,False
162,T4,0,,False
163,3.28± 0.31 2.40± 0.20 4.76± 0.62 19.40± 2.04 0.25± 0.03 7.60± 0.79 2.66± 0.32 1.68± 0.32,0,,False
164,"Table 3: Summary table of times over each of the four interfaces evaluated. Signi cant di erences exist between T0 and T4 (identi ed by the *, where  , 0.05) on a follow-up Bonferroni test.",0,,False
165,Time per ery Time per Document Time per Result Summary*,0,,False
166,T0 8.29± 0.57 17.31± 2.12 1.63 ± 0.13*,0,,False
167,T1 7.99± 0.57 22.82± 6.03 2.21± 0.21,0,,False
168,T2 9.42± 0.79 17.19± 1.86 2.35± 0.23,0,,False
169,T4 8.12± 0.48 18.99± 2.13 2.60 ± 0.27*,0,,False
170,"any of these measures. e number of queries issued follows a slight downward trend as the length of result summaries increases (3.72 ± 0.34 for T0 to 3.28 ± 0.31 for T4), as too does the number of SERPs examined, and the number of documents examined per query. e depth to which subjects went to per query however follows a downward trend ­ as the length of snippets increases, subjects were likely to go to shallower depths when examining result summaries (24.47 ± 2.96 for T0 to 19.4 ± 2.04 for T4).",0,,False
171,"Interaction probabilities all showed an increasing trend as snippet length increased over the four interfaces, as shown in Table 4. Although no signi cant di erences were observed over the four interfaces and the di erent probabilities examined, trends across all probabilities show an increase as the snippet length increases. An increase of both the probability of clicking result summaries on the SERP (P (C)) and marking the associated documents (P (M )) as relevant were observed. When these probabilities are examined in more detail by separating the result summaries clicked and documents marked by their TREC relevancy (through use of TREC QRELs), we see increasing trends for clicking and marking ­ both for TREC relevant (P (C |R) and P (M |R) for clicking and marking, respectively) and TREC non-relevant documents (P (C |N ) and P (M |N )). is interesting nding shows that an increase in snippet length does not necessarily improve the accuracy of subjects ­ simply the likelihood that they would consider documents as relevant.",1,TREC,True
172,"Performance. Table 2 also reports a series of performance measures over the four conditions, averaged over the four topics examined. We report the mean performance of the queries issued with P@10, the number of documents marked relevant, and the number of documents marked relevant that were TREC relevant. Like the interaction measures above, no signi cant di erences were observed",1,TREC,True
173,"over the four interfaces for each of the performance measures examined. e performance of queries issued by subjects was very similar across all four conditions (P@10  0.25), along with the number of documents identi ed by subjects as relevant (6.49 ± 0.58 for T2 to 7.6 ± 0.79 for T4), and the count of documents marked that were actually TREC relevant (2.28 ± 0.25 for T1 to 2.66 ± 0.32 for T4). We also examined the number of documents marked that were not assessed (unjudged) by the TREC assessors, in case one interface surfaced more novel documents. On average, subjects marked two such documents, but again there was no signi cant di erences between interfaces.",1,TREC,True
174,"Time-Based Measures. Table 3 reports a series of selected interaction times over each of the four evaluated interfaces. We include: the mean total query time per subject, per interface; the mean time per query; the mean time spent examining documents per query; and the mean time spent examining result summaries per query. No signi cant di erences were found between the mean total query time, the time per query and the time per document. However, a signi cant di erence did exist for the time spent per result summary. A clear upward trend in the time spent examining snippets can be seen in Figure 4 as result summaries progressively got longer, from 1.63 ± 0.13 for T0 to 2.6 ± 0.27 for T4, which was signi cantly di erent (F (3, 208) ,"" 3.6, p "","" 0.014). A follow-up Bonferroni test showed that the signi cant di erence existed between T0 and T4. is suggests that as result summary length increases, the amount of time spent examining result summaries also increases (an intuitive result). is also complies with trends observed regarding examination depths. When the length of result summaries increased, subjects were likely to examine result summaries to shallower depths.""",0,,False
175,141,0,,False
176,Session 2A: Search Interaction 1,1,Session,True
177,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
178,Table 4: Table illustrating a summary of interaction probabilities over each of the four interfaces evaluated. Note the increasing trends for each probability from T0  T4 (short to long snippets). Refer to Section 4.1 for an explanation of what each probability represents.,0,,False
179,T0,0,,False
180,P (C) P (C |R) P (C |N ),0,,False
181,0.20± 0.02 0.28± 0.03 0.18± 0.02,0,,False
182,P (M) P (M |R) P (M |N ),0,,False
183,0.61± 0.04 0.66± 0.06 0.55± 0.04,0,,False
184,T1 0.25± 0.02 0.34± 0.03 0.23± 0.02 0.68± 0.04 0.69± 0.05 0.65± 0.04,0,,False
185,T2 0.26± 0.03 0.35± 0.03 0.25± 0.03 0.65± 0.03 0.67± 0.05 0.58± 0.04,0,,False
186,T4 0.28± 0.03 0.40± 0.04 0.24± 0.03 0.71± 0.03 0.66± 0.05 0.67± 0.04,0,,False
187,"Table 5: Summary table of the recorded observations for the post-task survey, indicating the preferences of subjects over six criteria and the four interfaces, where  indicates that T0 was signi cantly di erent from the other conditions. In the table, Conf. represents Con dence, Read. represents Readability, Inform. represents Informativeness, and Rel. represents Relevancy.",1,ad,True
188,T0,0,,False
189,T1,0,,False
190,Clarity 4.16± 0.27*,0,,False
191,Conf. 3.71± 0.26*,0,,False
192,Read. 5.18± 0.31*,1,ad,True
193,Inform. 4.20± 0.30*,0,,False
194,Rel.,0,,False
195,3.84± 0.28*,0,,False
196,Size,0,,False
197,4.00± 0.31*,0,,False
198,5.00± 0.21 4.66± 0.26 6.32± 0.17 5.38± 0.24 4.89± 0.25 4.94± 0.25,0,,False
199,T2,0,,False
200,5.06± 0.24 4.75± 0.24 6.46± 0.14 5.27± 0.24 5.08± 0.24 5.21± 0.22,0,,False
201,T4,0,,False
202,5.40± 0.20 5.06± 0.25 6.36± 0.14 5.62± 0.20 5.36± 0.20 5.36± 0.19,0,,False
203,Table 6: Table presenting responses from the exit survey completed by subjects. e survey is discussed in Section 3.4.,0,,False
204,Most Informative Least helpful Easiest Least Useful Most Preferred,0,,False
205,T0 T1 T2 T4,0,,False
206,1,0,,False
207,4,0,,False
208,20 29,0,,False
209,46,0,,False
210,5,0,,False
211,1,0,,False
212,2,0,,False
213,4,0,,False
214,4,0,,False
215,24 22,0,,False
216,49,0,,False
217,4,0,,False
218,0,0,,False
219,1,0,,False
220,3,0,,False
221,5,0,,False
222,20 26,0,,False
223,4.2 User Experience,0,,False
224,"Task Evaluations. Table 5 presents the mean set of results from subjects across the four interfaces, which were answered upon completion of each search task. e survey questions are detailed in Section 3.4. Using the seven-point Likert scale for their responses (with 7 indicating strongly agree, and 1 indicating strongly disagree), signi cant di erences were found in all question responses (clarity F (3, 208) ,"" 5.22, p "","" 0.001, con dence F (3, 208) "","" 5.3, p "","" 0.001,""",0,,False
225,"readable F (3, 208) ,"" 9.25, p < 0.001, informative F (3, 208) "","" 5.22, p "","" 0.001, relevance F (3, 208) "","" 6.44, p < 0.001, and size F (3, 208) "","" 7.28, p < 0.001). Follow-up Bonferroni tests however showed that the signi cant di erence existed only between T0 and the remaining three interfaces, T1, T2 and T4. A series of discernible trends can be observed throughout the responses, with subjects regarding longer snippets as more concise, and a higher degree of clarity (4.16±0.27 for T0 to 5.4±0.2 for T4). is perceived clarity also made subjects feel more con dent that the longer result summaries helped them make be er decisions as to whether they were relevant to the given topic ­ interaction results presented above however di er from this, where the overall probability of marking documents increased, regardless of the document/topic TREC relevancy judgement. Other notable trends observed from the results included an increase in how informative subjects perceived the result summaries to be ­ again, with longer summaries proving more informative. Subjects also reported a general increase in satisfaction of the length of the presented result summaries/snippets ­ although, as mentioned, no signi cant di erence existed between the three interfaces that generated snippets (T1, T2 and T4).""",1,ad,True
226,"System Evaluations. Upon completion of the study, subjects completed the exit survey as detailed in Section 3.4. Responses from the subjects are presented in Table 6. From the results, subjects found result summaries of longer lengths (i.e. those generated by interfaces T2 and T4) to be the most informative, and those generated by T0 ­ without snippets ­ to be the least helpful and useful.",0,,False
227,"e longer result summaries were also consistently favoured by subjects, who preferred them over the result summaries generated by interfaces T0 and T1. Subjects also found the result summaries of longer length easier to use to satisfy the given information need.",0,,False
228,"From the results, it is therefore clear that a majority of subjects preferred longer result summaries to be presented on SERPs, generated by interfaces T2 and T4. Figure 5 provides summary plots, showing general trends across the four interfaces, examining observed interactions and reported experiences.",0,,False
229,5 DISCUSSION AND FUTURE WORK,0,,False
230,"In this paper, we investigated the in uence of result summary length on search behaviour and performance. Using the KullbackLeibler distance [28] as a measure of information gain, we examined result summaries of di erent lengths, selected a series of snippet lengths where there was a signi cant di erence in information gain between them, which yielded the con gurations for our four experimental conditions, T0, T1, T2 and T4. We conducted a crowdsourced user study comprising of 53 subjects, each of whom undertook four search tasks, using each of the four interfaces.",0,,False
231,"Our work was focused around addressing our two research questions, which explored RQ1 how the value of information gain (represented by snippet length) a ected search behaviour and user experience; and RQ2 whether information gain a ected the decision making ability and accuracy of users. Addressing RQ1 rst in terms of search behaviour, there was li le di erence ­ but we did observe the following trends: as summary length increases, participants: issued fewer queries; examined fewer pages; but clicked more documents, i.e. they spent more of their time assessing documents at higher ranks. Second, our results show that in terms",1,ad,True
232,142,0,,False
233,Session 2A: Search Interaction 1,1,Session,True
234,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
235,#,0,,False
236,#,0,,False
237,#,0,,False
238,Actions,0,,False
239,Query Count 5,1,Query,True
240,4,0,,False
241,3,0,,False
242,2 T0,0,,False
243,T1,0,,False
244,T2,0,,False
245,T4,0,,False
246,Pages per Query 4,1,Query,True
247,3,0,,False
248,2 T0,0,,False
249,T1,0,,False
250,T2,0,,False
251,T4,0,,False
252,Docs per Query 6,1,Query,True
253,4,0,,False
254,2 T0,0,,False
255,T1,0,,False
256,T2,0,,False
257,T4,0,,False
258,Depth per Query 30,1,Query,True
259,20,0,,False
260,10 T0,0,,False
261,T1,0,,False
262,T2,0,,False
263,T4,0,,False
264,Docs Marked Relevant 10,0,,False
265,8,0,,False
266,6,0,,False
267,T0,0,,False
268,T1,0,,False
269,T2,0,,False
270,T4,0,,False
271,Rel Count 3,0,,False
272,2.5,0,,False
273,2 T0,0,,False
274,T1,0,,False
275,T2,0,,False
276,T4,0,,False
277,Interface Condition,0,,False
278,P( M | N ),0,,False
279,P( M | R ),0,,False
280,P( M ),0,,False
281,P( C | N ),0,,False
282,P( C | R ),0,,False
283,P( C ),0,,False
284,Probabilities,0,,False
285,P( Click ),0,,False
286,0.4,0,,False
287,0.3,0,,False
288,0.2,0,,False
289,T0,0,,False
290,T1,0,,False
291,T2,0,,False
292,T4,0,,False
293,P( Click | Relevant),0,,False
294,0.4,0,,False
295,0.3,0,,False
296,0.2,0,,False
297,T0,0,,False
298,T1,0,,False
299,T2,0,,False
300,T4,0,,False
301,P( Click | Non-Relevant),0,,False
302,0.4,0,,False
303,0.3,0,,False
304,0.2,0,,False
305,T0,0,,False
306,T1,0,,False
307,T2,0,,False
308,T4,0,,False
309,P( Mark ) 0.8,0,,False
310,0.6,0,,False
311,T0,0,,False
312,T1,0,,False
313,T2,0,,False
314,T4,0,,False
315,P( Mark | Relevant ) 0.8,0,,False
316,0.6 T0,0,,False
317,0.8,0,,False
318,T1,0,,False
319,T2,0,,False
320,T4,0,,False
321,P( Mark | Non-Relevant ),0,,False
322,0.6,0,,False
323,T0,0,,False
324,T1,0,,False
325,T2,0,,False
326,T4,0,,False
327,Interface Condition,0,,False
328,Post-Task,0,,False
329,Clarity,0,,False
330,6,0,,False
331,4,0,,False
332,T0,0,,False
333,T1,0,,False
334,T2,0,,False
335,T4,0,,False
336,Confidence,0,,False
337,6,0,,False
338,4,0,,False
339,T0,0,,False
340,T1,0,,False
341,T2,0,,False
342,T4,0,,False
343,Readable,1,ad,True
344,6,0,,False
345,4,0,,False
346,T0,0,,False
347,T1,0,,False
348,T2,0,,False
349,T4,0,,False
350,Informativeness,0,,False
351,6,0,,False
352,4,0,,False
353,T0,0,,False
354,T1,0,,False
355,T2,0,,False
356,T4,0,,False
357,Relevance,0,,False
358,6,0,,False
359,4,0,,False
360,T0,0,,False
361,T1,0,,False
362,T2,0,,False
363,T4,0,,False
364,Size,0,,False
365,6,0,,False
366,4,0,,False
367,T0,0,,False
368,T1,0,,False
369,T2,0,,False
370,T4,0,,False
371,Interface Condition,0,,False
372,#,0,,False
373,#,0,,False
374,#,0,,False
375,"Figure 5: Plots, showing a variety of measures and survey results from subjects across the four interfaces examined. From le to right: actions associated with the subjects' search behaviours and performance; probabilities of interaction; and post-task survey responses, using a seven-point Likert scale (7 - strongly agree to 1 - strongly disagree).",0,,False
376,"of experience, subjects broadly preferred longer summaries. e participants felt that longer summaries were more clear, informative, readable ­ and interestingly ­ gave them more con dence in their relevance decisions. With respect to RQ2, we again observed li le di erence in subjects' decision making abilities and accuracy between the four interfaces. While subjects perceived longer snippets to help them infer relevance more accurately, our empirical evidence shows otherwise. In fact, it would appear that longer result summaries were more a ractive, increasing the information scent of the SERP [53]. is may account for the increase in clicks on the early results, without the bene ts, however: accuracy of our subjects did not improve with longer snippets; nor did they nd more relevant documents. Increased con dence in the result summaries (from T0  T4) may have led to a more relaxed approach at marking content as relevant ­ as can be seen by increasing click and mark probabilities for both relevant and non-relevant content. It is also possible that the paradox of choice [37] could play a role in shaping a user's preferences. For example, in the condition with longer result summaries (T4), users viewed fewer results/choices than on other conditions. is may have contributed to their feelings of greater satisfaction and increased con dence in their decisions.",1,ad,True
377,"ese novel ndings provide new insights into how users interact with result summaries in terms of their experiences and search behaviours. Previous work had only focused upon task completion times and accuracy of the rst result while not considering their experiences (e.g. [11, 19]). Furthermore, these past works were performed in the context of Web search where the goal was to nd one document. However, we acknowledge that our work also has limitations. Here, we examined out research questions ­ with respect to topic search within a news collection ­ to explore how behaviour and performance changes when searching for multiple relevant documents. It would be interesting to examine this in other search contexts, such as product search, for example. News article titles also can be cra ed di erently from documents in other domains. Summaries in this domain may perhaps be more important than in other domains, and so the e ects and in uences are likely to be larger. Furthermore, we only considered how behaviours changed on the desktop, rather than on other devices where users are more likely to be sensitive to such changes (e.g. [25, 27]). For example, during casual leisure search, multiple relevant documents on tablet devices are o en found, and so it would be interesting to perform a follow up study in this area.",1,ad,True
378,143,0,,False
379,Session 2A: Search Interaction 1,1,Session,True
380,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
381,"To conclude, our ndings show that longer result summaries,",0,,False
382,"while containing a greater amount of information content, are not",0,,False
383,necessarily be er in terms of decision making ­ although subjects,0,,False
384,perceived this to be the case. We also show a positive relationship,0,,False
385,between the length and informativeness of result summaries and,0,,False
386,their a ractiveness (clickthrough rates). ese results show that the,0,,False
387,experience and perceptions of users ­ and the actual performance,0,,False
388,"of those users ­ is di erent, and when designing interfaces, this",0,,False
389,needs be taken into account.,0,,False
390,Acknowledgments Our thanks to Alastair Maxwell and Stuart Mackie,0,,False
391,"for their comments, the 53 participants of this study, and the anonymous",0,,False
392,reviewers for their feedback. e lead author is nancially supported by,1,ad,True
393,"the UK Government through the EPSRC, grant . 1367507.",1,Gov,True
394,REFERENCES,0,,False
395,"[1] M. Ageev, D. Lagun, and E. Agichtein. Improving search result summaries by using searcher behavior data. In Proc. 35th ACM SIGIR, pages 13­22, 2013.",0,,False
396,"[2] H. Ali, F. Scholer, J. A. om, and M. Wu. User interaction with novel web search interfaces. In Proc. 21st OZCHI, pages 301­304.",0,,False
397,"[3] L. Azzopardi, D. Kelly, and K. Brennan. How query cost a ects search behavior. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in IR, pages 23­32, 2013.",0,,False
398,"[4] H. Bast and M. Celikik. E cient index-based snippet generation. ACM Trans. Inf. Syst., 32(2):6:1­6:24, Apr. 2014.",0,,False
399,"[5] H. Bota, K. Zhou, and J. M. Jose. Playing your cards right: e e ect of entity cards on search behaviour and workload. In Proc. 1st ACM CHIIR, pages 131­140, 2016.",1,ad,True
400,"[6] H. Chen and S. Dumais. Bringing order to the web: Automatically categorizing search results. In Proc. 18th ACM CHI, pages 145­152, 2000.",0,,False
401,"[7] M. C. Chen, J. R. Anderson, and M. H. Sohn. What can a mouse cursor tell us more?: Correlation of eye/mouse movements on web browsing. In Proc. 19th ACM CHI Extended Abstracts, pages 281­282, 2001.",0,,False
402,"[8] F. Chieriche i, R. Kumar, and P. Raghavan. Optimizing two-dimensional search results presentation. In Proc. 4th ACM WSDM, pages 257­266, 2011.",0,,False
403,"[9] C. L. A. Clarke, E. Agichtein, S. Dumais, and R. W. White. e in uence of caption features on clickthrough pa erns in web search. In Proc. 30th ACM SIGIR, pages 135­142, 2007.",0,,False
404,"[10] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In Proc. 1st ACM WSDM, pages 87­94, 2008.",0,,False
405,"[11] E. Cutrell and Z. Guan. What are you looking for?: an eye-tracking study of information usage in web search. In Proc. 25th ACM CHI, pages 407­416, 2007.",0,,False
406,"[12] S. Dumais, E. Cutrell, and H. Chen. Optimizing search by showing results in context. In Proc. 19th ACM CHI, pages 277­284, 2001.",0,,False
407,"[13] H. Feild, R. Jones, R. Miller, R. Nayak, E. Churchill, and E. Velipasaoglu. Logging the search self-e cacy of amazon mechanical turkers. In Proc. CSE SIGIR Workshop, pages 27­30, 2010.",0,,False
408,"[14] J. He, P. Duboue, and J.-Y. Nie. Bridging the gap between intrinsic and perceived relevance in snippet generation. In Proc. of COLING 2012, pages 1129­1146, 2012.",0,,False
409,"[15] M. Hearst. Search user interfaces. Cambridge University Press, 2009. [16] P. Ingwersen and K. Ja¨rvelin. e Turn: Integration of Information Seeking and",0,,False
410,"Retrieval in Context. 2005. [17] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately inter-",0,,False
411,"preting clickthrough data as implicit feedback. In Proc. 28th ACM SIGIR, pages 154­161, 2005. [18] H. Joho and J. M. Jose. A comparative study of the e ectiveness of search result presentation on the web. In Proc. 28th ECIR, pages 302­313, 2006. [19] M. Kaisser, M. A. Hearst, and J. B. Lowe. Improving search results quality by customizing summary lengths. In Proc. 46th ACL, pages 701­709, 2008. [20] Y. Kammerer and P. Gerjets. How the interface design in uences users' spontaneous trustworthiness evaluations of web search results: comparing a list and a grid interface. In Proc. of the Symp. on Eye-Tracking Research & Applications, pages 299­306, 2010. [21] T. Kanungo and D. Orr. Predicting the readability of short web summaries. In Proc. 2nd ACM WSDM, pages 202­211, 2009. [22] D. Kelly and L. Azzopardi. How many results per page?: A study of serp size, search behavior and user experience. In Proc. 38th ACM SIGIR, pages 183­192, 2015. [23] D. Kelly, K. Gyllstrom, and E. W. Bailey. A comparison of query and term suggestion features for interactive searching. In Proc. 32nd ACM SIGIR, pages 371­378, 2009.",1,Track,True
412,"[24] J. Kim, P. omas, R. Sankaranarayana, and T. Gedeon. Comparing scanning behaviour in web search on small and large screens. In Proc. 17th ADCS, pages 25­30, 2012.",0,,False
413,"[25] J. Kim, P. omas, R. Sankaranarayana, T. Gedeon, and H.-J. Yoon. Eye-tracking analysis of user behavior and performance in web search on large and small screens. J. of the Assoc. for Information Science and Technology, 2014.",0,,False
414,"[26] J. Kim, P. omas, R. Sankaranarayana, T. Gedeon, and H.-J. Yoon. Pagination versus scrolling in mobile web search. In Proc. 25th ACM CIKM, pages 751­760, 2016.",0,,False
415,"[27] J. Kim, P. omas, R. Sankaranarayana, T. Gedeon, and H.-J. Yoon. What snippet size is needed in mobile web search? In Proc. 2nd ACM CHIIR, pages 97­106, 2017.",0,,False
416,"[28] S. Kullback and R. A. Leibler. On information and su ciency. e Annals of Mathematical Statistics, 22:79­86, 1951.",0,,False
417,"[29] J. Kupiec, J. Pedersen, and F. Chen. A trainable document summarizer. In Proc. 18th ACM SIGIR, pages 68­73, 1995.",0,,False
418,"[30] T. Landauer, D. Egan, J. Remde, M. Lesk, C. Lochbaum, and D. Ketchum. Enhancing the usability of text through computer delivery and formative evaluation: the superbook project. Hypertext: A psychological perspective, pages 71­136, 1993.",0,,False
419,"[31] L. Leal-Bando, F. Scholer, and A. Turpin. ery-biased summary generation assisted by query expansion. J. Assoc. for Info. Sci. and Tech., 66(5):961­979, 2015.",0,,False
420,"[32] Q. Li and Y. P. Chen. Personalized text snippet extraction using statistical language models. Pa ern Recogn., 43(1):378­386, Jan. 2010.",0,,False
421,"[33] G. Linden. Marissa mayer at web 2.0, November 2006. h p:// glinden.blogspot. com/ 2006/ 11/ marissa-mayer-at-web-20.html.",1,blog,True
422,"[34] Marcos, M-C. and Gavin, F. and Arapakis, I. E ect of snippets on user experience in web search. In Proc. 16th Intl. Conf. on HCI, pages 47:1­47:8, 2015.",0,,False
423,"[35] A. Muralidharan, Z. Gyongyi, and E. Chi. Social annotations in web search. In Proc. 30th ACM CHI, pages 1085­1094, 2012.",0,,False
424,"[36] V. Navalpakkam, L. Jentzsch, R. Sayres, S. Ravi, A. Ahmed, and A. Smola. Measurement and modeling of eye-mouse behavior in the presence of nonlinear page layouts. In Proc. 22nd WWW, pages 953­964, 2013.",0,,False
425,"[37] A. Oulasvirta, J. Hukkinen, and B. Schwartz. When more is less: e paradox of choice in search engine use. In Proc. 32nd ACM SIGIR, pages 516­523, 2009.",1,ad,True
426,"[38] T. Paek, S. Dumais, and R. Logan. Wavelens: A new view onto internet search results. In Proc. 22nd ACM CHI, pages 727­734, 2004.",0,,False
427,"[39] J. Pedersen, D. Cu ing, J. Tukey, et al. Snippet search: A single phrase approach to text access. In Proc. 1991 Joint Statistical Meetings, 1991.",0,,False
428,"[40] M. L. Resnick, C. Maldonado, J. M. Santos, and R. Lergier. Modeling on-line search behavior using alternative output structures. In Proc. Human Factors and Ergonomics Soc. Annual Meeting, volume 45, pages 1166­1170, 2001.",1,ad,True
429,"[41] D. E. Rose, D. Orr, and R. G. P. Kantamneni. Summary a ributes and perceived search quality. In Proc. 16th WWW, pages 1201­1202, 2007.",0,,False
430,"[42] D. Savenkov, P. Braslavski, and M. Lebedev. Search snippet evaluation at yandex: lessons learned and future directions. Multilingual & Multimodal Information Access Evaluation, pages 14­25, 2011.",0,,False
431,"[43] B. Schwartz. e Paradox of Choice: Why More Is Less. Harper Perennial, 2005. [44] M. Smucker, X. Guo, and A. Toulis. Mouse movement during relevance judging:",1,ad,True
432,"Implications for determining user a ention. In Proc. 37th ACM SIGIR, pages 979­982, 2014. [45] N. V. Spirin, A. S. Kotov, K. G. Karahalios, V. Mladenov, and P. A. Izhutov. A comparative study of query-biased and non-redundant snippets for structured search on mobile devices. In Proc. 25th ACM CIKM, pages 2389­2394, 2016. [46] K. M. Svore, J. Teevan, S. T. Dumais, and A. Kulkarni. Creating temporally dynamic web search snippets. In Proc. 35th ACM SIGIR, pages 1045­1046, 2012. [47] J. Teevan, E. Cutrell, D. Fisher, S. M. Drucker, G. Ramos, P. Andre´, and C. Hu. Visual snippets: Summarizing web pages for search and revisitation. In Proc. 27th ACM CHI, pages 2023­2032, 2009. [48] A. Tombros and M. Sanderson. Advantages of query biased summaries in information retrieval. In Proc. 21st ACM SIGIR, pages 2­10, 1998. [49] A. Turpin, Y. Tsegay, D. Hawking, and H. E. Williams. Fast generation of result snippets in web search. In Proc. 30th ACM SIGIR, pages 127­134, 2007. [50] E. M. Voorhees. Overview of the trec 2005 robust track. In Proc. TREC-14, 2006. [51] R. W. White, J. M. Jose, and I. Ruthven. A task-oriented study on the in uencing e ects of query-biased summarisation in web searching. Info. Processing & Management, 39(5):707­733, 2003. [52] A. Woodru , R. Rosenholtz, J. B. Morrison, A. Faulring, and P. Pirolli. A comparison of the use of text summaries, plain thumbnails, and enhanced thumbnails for web search tasks. J. Am. Soc. Inf. Sci. Technol., 53(2):172­185, 2002. [53] Wu, W-C. and Kelly, D. and Sud, A. Using information scent and need for cognition to understand online search behavior. In Proceedings of the 37th International ACM SIGIR Conference, SIGIR '14, pages 557­566, 2014. [54] G. Zuccon, T. Leelanupab, S. Whiting, E. Yilmaz, J. Jose, and L. Azzopardi. Crowdsourcing interactions: using crowdsourcing for evaluating interactive information retrieval systems. Information Retrieval, 16(2):267­305, 2013.",1,ad,True
433,144,0,,False
434,,0,,False

,sentence,label,data,regex
0,Session 4B: Retrieval Models and Ranking 2,1,Session,True
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Information Retrieval Meets Game Theory: The Ranking Competition Between Documents' Authors,0,,False
3,Nimrod Raifer,0,,False
4,"Technion, Israel nimo@campus.technion.ac.il",0,,False
5,Fiana Raiber,0,,False
6,"Yahoo Research, Israel ana@yahoo-inc.com",1,Yahoo,True
7,Moshe Tennenholtz,0,,False
8,"Technion, Israel moshet@ie.technion.ac.il",0,,False
9,ABSTRACT,0,,False
10,"In competitive search se ings as the Web, there is an ongoing ranking competition between document authors (publishers) for certain queries. e goal is to have documents highly ranked, and the means is document manipulation applied in response to rankings. Existing retrieval models, and their theoretical underpinnings (e.g., the probability ranking principle), do not account for post-ranking corpus dynamics driven by this strategic behavior of publishers. However, the dynamics has major e ect on retrieval e ectiveness since it a ects content availability in the corpus. Furthermore, while manipulation strategies observed over the Web were reported in past literature, they were not analyzed as ongoing, and changing, post-ranking response strategies, nor were they connected to the foundations of classical ad hoc retrieval models (e.g., content-based document-query surface level similarities and document relevance priors). We present a novel theoretical and empirical analysis of the strategic behavior of publishers using these foundations. Empirical analysis of controlled ranking competitions that we organized reveals a key strategy of publishers: making their documents (gradually) become similar to documents ranked the highest in previous rankings. Our theoretical analysis of the ranking competition as a repeated game, and its minmax regret equilibrium, yields a result that supports the merits of this publishing strategy. We further show that it can be predicted with high accuracy, and without explicit knowledge of the ranking function, whether documents will be promoted to the highest rank in our competitions. e prediction utilizes very few features which quantify changes of documents, speci cally with respect to those previously ranked the highest.",1,ad,True
11,KEYWORDS,0,,False
12,ad hoc retrieval; game theory; ranking competition,1,ad,True
13, e paper is based on work done while the author was at the Technion.,0,,False
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080785",1,ad,True
15,Oren Kurland,0,,False
16,"Technion, Israel kurland@ie.technion.ac.il",0,,False
17,1 INTRODUCTION,1,DUC,True
18,"Ad hoc document retrieval models are o en based on the assumption of a xed document corpus -- i.e., corpus dynamics is not accounted for. e core challenge is estimating the relevance of a document to the query. e probability ranking principle (PRP) [25] is the theoretical foundation of this practice: to maximize user utility, documents should be ranked by their relevance probabilities.",1,hoc,True
19,"In practice, document corpora are not static as documents are changed, created or removed. Some of the corpus dynamics, specifically, in competitive search se ings (e.g., the Web), results from ranking incentives of document authors, henceforth referred to as publishers. at is, publishers might modify documents to promote them in rankings induced for queries of interest. ese modi cations are referred to as search engine optimization (SEO) [16]. Spam ltering, and more generally, using document quality measures as features in learning-to-rank methods [4], are examples of approaches for rank-penalizing documents that have gone through unwarranted modi cations (a.k.a., black-hat SEO [16]).",1,corpora,True
20,"However, existing retrieval approaches, and their theoretical foundations, do not account for future corpus dynamics driven by rankings. For example, it was recently shown that the PRP is sub-optimal in competitive retrieval se ings [3] as it can lead to decreased content breadth in the corpus, among other issues.",1,ad,True
21,"To estimate post-ranking corpus dynamics, speci cally, that caused by responses of publishers to rankings (i.e., document modi cations), analysis of the strategic behavior of publishers is called for. While types and techniques of SEO strategies were discussed in past work [16], these were not studied as response strategies with respect to rankings induced for speci c queries. Rather, they were presented as general actions observed on the Web (e.g., keyword stu ng and content copying).",0,,False
22,"Furthermore, there are no studies, to the best of our knowledge, that analyze publishers' strategies with respect to retrieval models and their foundations; namely, the e ect, over time, on features used for ranking. Such analysis is important for incorporating strategy predictions (estimates) in, and addressing their e ects on, retrieval approaches. A case in point, it was shown that if the actual writing quality of publishers for topics is known, then this information can be used in non-deterministic retrieval models to promote content breadth in the corpus, and therefore improve search e ectiveness along time [3]. More generally, analysis of the strategic behavior of publishers is crucial for se ing theoretical foundations for handling post-ranking corpus dynamics. e same way user modeling is",1,corpora,True
23,465,0,,False
24,Session 4B: Retrieval Models and Ranking 2,1,Session,True
25,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
26,"important for interactive information retrieval models [30], modeling the strategic behavior of publishers in response to induced rankings is important for addressing post-ranking corpus dynamics in retrieval models.",1,ad,True
27,"We present a novel initial theoretical and empirical analysis of the (temporal) strategic behavior of publishers in terms of changes they introduce to documents in response to induced rankings. e analysis is performed in the context of classical ad hoc retrieval models in two respects. First, we focus on content-based retrieval and accordingly content manipulation. Analyzing post-ranking strategies of changing hypertext, hyperlinks and a ecting clicks or any additional signal that can be used for relevance estimation is outside the scope of this paper. Nevertheless, we note that (i) content-based relevance estimates (e.g., Okapi BM25 and languagemodel-based estimates) are among the most important ones used in learning-to-rank approaches applied over Web data [20]; (ii) content manipulation techniques are quite pervasive, speci cally, over the Web [16]; and, (iii) for experiments we use a state-of-the-art learning-to-rank approach applied with content-based estimates. Second, we empirically study content manipulation in terms of the building blocks of classical, content-based, retrieval methods.",1,ad,True
28,ese include document-query surface-level similarities [14] and query-independent document relevance priors [4].,0,,False
29,"Performing empirical analysis of the ""ranking competition"" between publishers whose incentive is to have their documents ranked high, even if assuming the availability of a large-scale log of a search engine, is a major challenge due to the numerous dynamic aspects that a ect this competition. Over the Web, pages emerge and disappear, the search engine's index coverage changes rapidly, the ranking function, as well as estimates it utilizes, change throughout time and across sets of users and queries. Furthermore, di erent publishers cannot necessarily employ the same document modi cations, and many modi cations are not content-based as the ranking function also considers non content-based relevance signals.",0,,False
30,"Given that our goal, as described above, is to study the strategic behavior of publishers in the scope of the foundations of classical content-based retrieval models, we performed controlled empirical analysis by organizing ranking competitions between students in a course. Two basic conditions were set in these competitions. First, the students were not aware of the ranking function, nor of the actual features it used. Second, the students were incentivized to write quality documents that would be ranked high by the ranking function. As shown below, the dataset allowed to gain interesting and important observations about potential strategic behavior of publishers in a ranking competition.",0,,False
31,"An important observation that emerged in the competition analysis that we present is that publishers were gradually making their documents become more similar, in several respects, to those most highly ranked in previous rankings1. An interesting fundamental question that follows is whether this competing strategy can be theoretically justi ed given the information available to publishers: observations of past rankings and li le to no knowledge of the ranking function. To address this question, we present a novel game theoretic analysis of the ranking competition as a repeated",1,ad,True
32,"1 is strategy is conceptually reminiscent of the black-hat weaving and stitching content-based SEO techniques applied over the Web [6, 16] where content from legitimate pages is copied to spam pages so as to promote them.",0,,False
33,game [1]. Our main theoretical result with respect to the minmax regret equilibrium of the game [17] provides formal support to the merits of this publishing (competing) strategy.,0,,False
34,"In addition to analyzing the ranking competition theoretically and empirically, we set as a goal to predict whether a document would be ranked the highest given that this was not the case in the previous ranking; the predictor does not have explicit knowledge of the ranking function. Interestingly, relying on very few features that quantify the extent to which the document was changed and became similar to a document previously ranked the highest can yield high accuracy prediction. ese features are inspired by the cluster hypothesis [18], and more speci cally, one of the important operational premises that it gave rise to: ""similar documents should receive similar retrieval scores"" [9]. us, in lack of knowledge of the ranking function, the predictor essentially uses inter-document similarities as proxies for retrieval score similarities.",1,ad,True
35,Our contributions can be summarized as follows.,0,,False
36,· We present the rst dataset of query-based ranking competitions between publishers. e focus is on content manipulation.,0,,False
37,· We present an empirical analysis of publishers' strategies employed in the competitions.,0,,False
38,· We present a novel game theoretic analysis of the ranking competition as a repeated game. e main result of analyzing the minmax equilibrium of the game provides formal support to the merits of a key strategy employed by publishers in our games.,0,,False
39,"· We show that, in our se ing, it is possible to predict with high accuracy whether a document will be promoted to the highest rank in the next ranking. e prediction is based on very few features and does not rely on explicit knowledge of the ranking function.",0,,False
40,2 RELATED WORK,0,,False
41,"ere is much work on identifying, characterizing and addressing unwarranted (a.k.a. black-hat SEO [16]) actions of publishers [6]. In contrast, we focus on the strategic behavior of publishers when applying legitimate content-based manipulations.",1,ad,True
42,"Studies of the dynamic aspects of interactive retrieval focus on changes of queries and the ranking function (e.g., [15, 27, 30, 31]). Changes of clickthrough pa erns were also studied [27]. e dynamics of the collection as a result of the ranking competition, which is our focus, was not addressed.",1,ad,True
43,"ere has been work on studying and predicting the dynamics of the Web collection (e.g., [23, 26]), where the main operational goals were improving crawling policies and personalizing content delivery. Past versions of a Web page were used to improve its representation for ranking [13]. However, in contrast to our work, the dynamics has not been studied with respect to the ranking competition between publishers.",0,,False
44,"Recently, the publishers' ranking competition was analyzed using a game theoretic approach [3]. In contrast to our work, the assumption was that publishers have full knowledge of the ranking function, the competition was not analyzed as a repeated game, and no empirical analysis was presented.",0,,False
45,466,0,,False
46,Session 4B: Retrieval Models and Ranking 2,1,Session,True
47,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
48,"e merits of non-deterministic ranking functions from [3] were argued using a simulation of a ranking competition between publishers who stu query terms in documents [2]. In contrast with our work, publishers were assumed to know the basic (Okapi BM25) ranking function, there was no theoretical analysis of the competition and no analysis of non-simulated (real) ranking competitions.",0,,False
49,"A game theoretic approach was used to devise query-based ranking mechanisms that (i) maximize social welfare for ambiguous queries, by diversifying search results that are assumed to be scanned using random sequential search [12]; and (ii) balance relevance and monetization [11]. In contrast to our work, the competition between documents' authors (publishers) was not studied.",0,,False
50,"Game theoretical analysis has also been applied for adversarial classi cation [8, 10] and for optimizing learning-to-rank functions in non-adversarial retrieval se ings [28]. We address the competitive (adversarial) ad hoc retrieval se ing using di erent theoretical and empirical analyses.",1,ad,True
51,3 GAME THEORETIC ANALYSIS,0,,False
52,We analyze the ranking competition as a repeated game [1]. Analyzing the minmax regret equilibrium of the game yields a formal result that helps to explain a key strategy employed by publishers in the ranking competitions we organized as described in Section 5.,0,,False
53,"In what follows we assume that a query q, and some document ranking function (details below), have been xed. Let N ,"" {1, 2, . . . , n} be a set of n publishers (documents' authors) that would like to have their documents ranked high for q. Let Di be a nite set of documents that publisher i can (or might) write to convey the information she wants to share. For ease of presentation, and to avoid technical tie-breaking issues, assume Di  Dj "",""  for any i, j  N , i j. Let D "", ni,1Di be the set of all documents that can be wri en by the publishers.",0,,False
54,"We assume a complete linear ordering over D, denoted <. Such ordering can be based, for example, on a single (numeric) feature in a document representation2. Alternatively, the distance, under some representation, to a document which serves as a reference point (e.g., a document ranked the highest at some point) can serve to induce the ordering. us, for ease of exposition we can associate D with elements in the interval [0, 1]. A document ranking function for q is a mapping r : D  +. For simplicity (and avoiding tiebreaking), we assume r (di ) r (dj ) for any di , dj  D.",0,,False
55,"De nition 3.1. RSP(D1, . . . , Dn ) ,"" RSP(D) denotes the single peak ranking functions. ese are functions r de ned over D, such that for no d  D, there are di , dj  D, di < d < dj such that r (di ) > r (d) and r (dj ) > r (d).""",0,,False
56,"For example, linear learning-to-rank functions [20] are single peak with respect to each feature. e negative KL divergence used in the language modeling framework [19] is a single peak function over the multinomial distributions in the simplex by the virtue of being a concave function. However, the most e ective ranking functions (e.g., those utilizing non-linear learning-to-rank methods) are not single peak. Nevertheless, it is important to keep in mind that we analyze the dynamics from the point of view of publishers",0,,False
57,"2In this case, the analysis below applies to each feature in a document representation assuming that the values of others were xed.",0,,False
58,who have no information about the ranking function except for,0,,False
59,"that inferred by observing induced rankings. at is, publishers",0,,False
60,"may assume, and act based on the belief, that the ranking function",0,,False
61,"is single peak. Indeed, as shown in Section 5, the participants",0,,False
62,(publishers) in our ranking competitions can be viewed as searching,0,,False
63,for the structure of a single-peak ranking function for various,0,,False
64,"features, although the ranking function is not single-peak.",0,,False
65,Below we care only about the relative ranking of documents in,0,,False
66,"D; thereby, we consider the possible total ordering induced by the",0,,False
67,ranking function over D; there are nitely many such orderings.,0,,False
68,With a slight abuse of notation we will therefore refer to RSP(D),0,,False
69,as the set of possible single-peak orderings of documents in D.,0,,False
70,Let D0,0,,False
71,",",0,,False
72,"{d 0 ,",0,,False
73,1,0,,False
74,.,0,,False
75,.,0,,False
76,". , dn0 }",0,,False
77,be,0,,False
78,an,0,,False
79,initial,0,,False
80,set,0,,False
81,of,0,,False
82,documents,0,,False
83,where,0,,False
84,di0  Di is the initial document published by publisher i. We assume that each i  N possess no information at the beginning,0,,False
85,"about the function r  RSP(D), beyond knowing it is a single-peak",0,,False
86,"ordering. Consider t rounds, l ,"" 1, 2, . . . , t, in each of which each""",0,,False
87,"player i chooses a document di  Di , and obtains a utility of 1 if di",0,,False
88,"is ranked rst and 0 otherwise. Herein, a publisher or her document",0,,False
89,"is called ""winner"" if the document was the highest ranked; all other",0,,False
90,"publishers and their documents are called ""losers"". Let TO(D) be the set of possible total ordering over D. Notice that selecting di  Di for every i  N determines an ordering over the selected documents by the single-peak function r  RSP(D). e strategy of i at round l is de ned as a function from the history of previously",0,,False
91,"selected actions and outcomes (i.e., orderings) of all publishers, to",0,,False
92,"the document selected by publisher i. e outcome at each round can be associated with a subset R  RSP(D) of the possible single peak functions, as it rules out particular orderings. Henceforth, R is referred to as the knowledge state, as it captures the set of currently",0,,False
93,possible single peak orderings based on the observations received. e publishers ranking game just described is a repeated game [1].,0,,False
94,"In a repeated game, the same game is repeatedly played in rounds",0,,False
95,"(iterations). Speci cally, at each round a publisher publishes a",0,,False
96,"document, but a strategy in each round may relate to all information",0,,False
97,"observed so far; e.g., the documents published and rankings induced",0,,False
98,"in previous iterations. Accordingly, given the initial document set",0,,False
99,"D0, and the total number of rounds t, the set of possible strategies for player i is denoted Si (t, di0).3 e utility Ui (t, di0, s1, . . . , sn ), where sj  Sj (t, dj0) for every j  N , is the sum of utilities of player i in rounds 1, 2, . . . , t given the corresponding strategies.",0,,False
100,We now introduce a slight modi cation to the utility obtained by player i in a round to capture the cost of modifying documents. is,0,,False
101,cost re ects both the actual e ort involved in changing a document,0,,False
102,"and the ""penalty"" incurred by potentially dri ing from the actual",0,,False
103,"document i planned to publish. Assume there is some negligible cost C, i.e., C |D| < 1, where eC > 0 is the cost for changing document d to document d in distance e (assume standard distance on [0, 1]) in a single round. Formally, the utility of publisher i in round l will be",0,,False
104,based on its ranking (either rst or not) minus the cost of changing the document wri en in round l - 1.,0,,False
105,"Given the game described above, a major challenge is to de ne an",0,,False
106,appropriate solution concept which predicts behavior in the game.,0,,False
107,e classical solution concept in game theory is the celebrated Nash,0,,False
108,"3Si (t, d 0) encodes all possible documents published by i at any round of the game given the previous potential orderings.",0,,False
109,467,0,,False
110,Session 4B: Retrieval Models and Ranking 2,1,Session,True
111,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
112,"equilibrium, which is a strategy pro le, one for each player, for which unilateral deviations are not bene cial (i.e., any single player cannot gain by deviating from her strategy assuming the others stick to their strategies). is solution concept always exists in",0,,False
113,"nite games with complete information if players are allowed to use mixed strategies, and has been also extended to games with incomplete information where there are Bayesian assumptions about the actual game being played. However, our se ing does not exhibit such stylized assumptions, and we need to appeal to other solution concepts. In particular, in a minmax regret equilibrium [17], we consider strategy pro les such that each player (publisher) minimizes her regret when compared to the best response she could have played had she known the exact environment state (e.g., the exact ranking function) assuming others stick to their strategies; and this holds for all players simultaneously.",1,ad,True
114,"Given a strategy pro le s ,"" (s1, . . . , sn ) the regret of i is maxxUi (t, di0, x, s-i ) - Ui (t, di0, s); s-i denotes the strategy pro le applied by all players except for i. A strategy pro le s "","" (s1, . . . , sn ) is minmax regret equilibrium if for every i, si minimizes regret given s-i [17]. Given the de ned publishers game we can now show that:""",0,,False
115,T rium.,0,,False
116,3.2. Any publishers game has a minmax regret equilib-,0,,False
117,"P . We construct the following equilibrium. Let R be the knowledge state at the beginning of a given round l. At the beginning of round 1 all ranking functions in RSP(D) are possible, while",0,,False
118,at each following round the knowledge state can only shrink in,0,,False
119,"terms of the number of ranking functions it contains. Let Vl  [0, 1]",0,,False
120,be the set of documents which correspond to possible peaks of the,0,,False
121,functions in the knowledge state R; let dil-1 be the most recent,0,,False
122,document published by i. Let,0,,False
123,l i,0,,False
124,Di,0,,False
125, Vl,0,,False
126,such that |,0,,False
127,l i,0,,False
128,- dil -1 |,0,,False
129,is minimal; if two documents have this minimal distance one is,0,,False
130,arbitrarily selected. e document published by i in round l would,0,,False
131,be,0,,False
132,l i,0,,False
133,.,0,,False
134,(In,0,,False
135,the,0,,False
136,rst round it is di0.) We now prove that this strategy,0,,False
137,of i minimizes its regret.,0,,False
138,"Let Vt be the knowledge state at the beginning of the last round t; Vt may result from arbitrary publishers' behavior in rounds 1, 2, . . . , t - 1. No publisher j i will publish a document not in Vt as",0,,False
139,"otherwise she cannot win (i.e., this strategy would be dominated). Hence, i's publishing a document out of Vt is dominated by publish-",0,,False
140,"ing the previous document. ( is has no cost, and publishing out",0,,False
141,"of Vt cannot result in a win.) On the other hand, since any  Vt",0,,False
142,"can be a winner, the worst regret would be for not publishing",0,,False
143,t i,0,,False
144,as de ned above.,0,,False
145,is is because,0,,False
146,t i,0,,False
147,might,0,,False
148,be,0,,False
149,the,0,,False
150,winner,0,,False
151,from,0,,False
152,this,0,,False
153,"point on, by the virtue of being in Vt , but it incurs minimal cost.",0,,False
154,"us, minimizing regret in the last round is achieved by selecting",0,,False
155,t i,0,,False
156,as,0,,False
157,prescribed.,0,,False
158,By,0,,False
159,"induction,",0,,False
160,using,0,,False
161,the,0,,False
162,argument,0,,False
163,from,0,,False
164,above,0,,False
165,results in i's strategy minimizing regret in every round.,0,,False
166,Two corollaries follow the proof:,0,,False
167,C,0,,False
168,3.3. e above constructed equilibrium is also a sub-,0,,False
169,game perfect equilibrium.,0,,False
170,"Namely, if an arbitrary sequence of documents has been selected up to round l < t, then in the remaining game, given the information provided so far on the potential peaks, following each player's",0,,False
171,strategy in the remaining rounds is still a minmax regret equilibrium.,0,,False
172,C,0,,False
173,3.4. Losers at round l - 1 will publish in round l,0,,False
174,"documents that become closer to (i.e., more similar) to that of the",0,,False
175,winner from round l - 1.,0,,False
176,"P . Assume wlog that a publisher who lost round l - 1 published dj  [0, 1] that satis es dj < dw where dw  [0, 1] was the winning document. As selecting any dj < dj is dominated given the knowledge state gathered, and the regret for publishing dj > dw is higher than that of publishing dj such that dw > dj > dj , we",0,,False
177,get the corresponding phenomenon. Notice that this will also imply,0,,False
178,that current winners will not change their documents.,0,,False
179,"us, Corollary 3.4 helps to explain a key strategy employed by publishers in the competitions we organized as we show below; namely, mimicking the winners.",0,,False
180,4 DATA,0,,False
181,"As discussed in Section 1, our goal is to analyze content-based ranking competitions so as to shed light on the strategic behavior of publishers. Since there are no publicly available datasets that can be used to that end, we organized repeated ranking competitions. e resulting dataset is available at h ps://github.com/asrcdataset/asrc. (See details in Appendix A.) We next describe the essentials of the competition.",0,,False
182,"Fi y two senior-undergrad and grad students in an information retrieval course were the publishers. e competition included 31 di erent repeated matches, each of which was with respect to a di erent TREC's ClueWeb09 query. Each student participated in three matches. Five students competed against each other in all matches except for one in which six students competed.",1,ad,True
183,"e competition was run for eight rounds; i.e., there were eight matches per query. Before the rst round, an example of a relevant document was provided for each match (query). Students were incentivized by course-grade rewards to edit their documents along the rounds so as to have them ranked as high as possible.4 As from the second round, students participating in a match were presented with the ranking of documents submi ed in the previous round by all competitors in the same match.",1,ad,True
184,"All documents were unstructured plain text of up to 150 terms. e document ranking model was based on the state-of-the-art LambdaMART [29] learning-to-rank approach integrating three classes of features. e rst are query-dependent features, such as eryTermsRatio (ratio of query terms appearing in a document) and LMIR.DIR (language-model-based similarity of a document to the query). e second class of features are query-independent document quality measures [4, 21], including Entropy (entropy of the term distribution in a document) and StopwordsRatio (stopwords to non-stopwords ratio in a document). Increased entropy and occurrence of stopwords a est to content breadth and hence to high prior probability of relevance [4].",1,LM,True
185,"e feature in the third class, SimInit, was used to incentivize students to write documents that dri from the initial relevant document shared by all students competing in the same match: it is",0,,False
186,4Students were assigned with unique IDs and all data was anonymized.,0,,False
187,468,0,,False
188,Session 4B: Retrieval Models and Ranking 2,1,Session,True
189,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
190,# of publishers # of publishers,0,,False
191,10 8 6 4 2 0 0 1 2 3 4 5 6 7 8 9 10 11 # of matches won,0,,False
192,50 40 30 20 10,0,,False
193,0 1234567 # of consecutive matches won,0,,False
194,Figure 1: e number of (consecutive) matches won (x-axis) by a given number of publishers (y-axis).,0,,False
195,"based on the language-model similarity of a document to the initial relevant document. We note that in practical scenarios, publishers would rarely change their documents so they will not include the information originally intended for sharing. Indeed, in the theoretical analysis presented in Section 3, a cost was assigned to changes of documents. Yet, as we show below, the conclusions we draw about strategies are aligned with our theoretical results.",0,,False
196,"Documents (manually) classi ed as keyword stu ed were penalized in the ranking. Information about the ranking function and the features it utilizes was not disclosed to students. e resulting collection contains (i) 1279 documents: 31 initial relevant documents and 1248 documents created by students, 897 of which are unique5; (ii) keyword stu ng annotations; and (iii) exhaustive relevance judgments. Appendix A provides additional details of the collection and ranking model.",1,ad,True
197,5 EMPIRICAL ANALYSIS,0,,False
198,"In the following analysis, winner (loser) is a document (or publisher thereof) which was (not) ranked rst in a match.",0,,False
199,5.1 Analysis of wins,0,,False
200,"Figure 1 (le ) presents the number of matches won by a given number of publishers (students). e competition included 248 distinct matches (8 rounds × 31 matches per round). Each student was assigned with exactly 3 queries; hence, the maximum number of matches a student could win is 24. We see that only two of the students did not win even a single match, a esting to the students' engagement in the competition. e maximum number of matches won was 11, less than half of the maximal possible number of wins, indicating that the competition was dynamic.",0,,False
201,Figure 1 (right) presents the number of consecutive matches won by a given number of students; the maximum is the number of rounds (eight). We see that most students could retain the rst rank for at most three rounds. Only a small number of students retained the rst rank in more than four rounds. is nding further a ests to the strong competition held between the students.,0,,False
202,5.2 Analysis of strategies,0,,False
203,"By Corollary 3.4, to win matches, losers in previous rounds will publish documents that become similar to that of the winner from the preceding round. Accordingly, we next analyze the similarity",0,,False
204,"5Several students submi ed the same document over a few rounds; e.g., if the document was the highest ranked in a previous round.",0,,False
205,of documents that did not win a match (losers) to the winner over a series of rounds in which these documents remained losers. e similarity to the winner is estimated with respect to some of the features used to rank documents which were presented in Section 4.,0,,False
206,"e eryTermsRatio and LMIR.DIR features quantify the querydocument match; LMIR.DIR is a representative query-document surface-level similarity estimate [14]. e Entropy and StopwordsRatio features are among the most e ective query independent content-based document relevance priors reported in the literature [4]. Hence, the analysis of the strategic behavior of publishers we present next relies on estimates that constitute the foundations of classical content-based ad hoc retrieval approaches.",1,LM,True
207,Figure 2 depicts the average values of the features for documents that were losers in at least four consecutive rounds before winning a match6. We distinguish between documents whose feature value four rounds before winning a match was lower than or equal to that of the winner (LW) and those whose feature value was higher than that of the winner (L>W). We also present the average feature values of winners (W).,0,,False
208,"We see in Figure 2 that the average feature values of winners remain relatively stable along the competition; thus, winner documents, o en wri en by di erent publishers, tend to be quite similar along a few dimensions (features).",0,,False
209,"Figure 2 also shows that, in general, Entropy o en decreases along rounds and eryTermsRatio increases. is a ests to high content repetition in winner documents that might result from high occurrence of query terms. SimInit decreases which is potentially due to our rewarding diversi cation with respect to the initial relevant document.",0,,False
210,"More generally, we observe a clear trend throughout the competition: feature values of loser documents which became winners were becoming closer, o en gradually converging, to those of winners from previous rounds regardless of their initial values. at is, in lack of knowledge of features used for document ranking, losers were mimicking winners and thereby indirectly a ecting these features. is nding is in accordance with Corollary 3.4.",1,ad,True
211,6 PREDICTING WINNERS,0,,False
212,"Given that loser publishers apply the strategy of mimicking the winners, an interesting challenge rises: leveraging aspects of this strategy to predict, without using explicit knowledge of the ranking function, which loser publisher in round l - 1 will win round l assuming that a previous loser indeed wins this round.7",0,,False
213,"For prediction, we represent each document as a feature vector and de ne two sets of features (details below) that quantify the extent to which the document becomes more similar to the winner of the previous round. e features in the rst set are estimates of this similarity on a macro level, where documents are treated as",0,,False
214,"6Similar trends were observed for other features used by the ranking model and for losers that lost in at least three or ve consecutive rounds. ese results are omi ed as they convey no further insight. 7Predicting which publisher will win round l regardless if it won round l - 1 is a challenge for future work. As stated in the proof of Corollary 3.4, and as observed in the competitions, winners did not tend to change their documents. is is a fundamental di erence with the dynamics of loser documents which makes this prediction task challenging. For example, many of the dynamics-based features de ned below for predicting whether a loser will turn to a winner are degenerated for winner documents which do not change.",0,,False
215,469,0,,False
216,Session 4B: Retrieval Models and Ranking 2,1,Session,True
217,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
218,QueryTermsRatio 1.0,1,Query,True
219,LMIR.DIR 7.0,1,LM,True
220,Entropy 4.4,0,,False
221,StopwordsRatio 1.0,0,,False
222,SimInit 0.8,0,,False
223,LW,0,,False
224,0.8,0,,False
225,5.0,0,,False
226,4.2,0,,False
227,0.8,0,,False
228,0.6,0,,False
229,L>W,0,,False
230,W,0,,False
231,0.6,0,,False
232,3.0,0,,False
233,4.0,0,,False
234,0.6,0,,False
235,0.4,0,,False
236,0.4 -4 -3 -2 -1 0,0,,False
237,1.0 -4 -3 -2 -1 0,0,,False
238,3.8 -4 -3 -2 -1 0,0,,False
239,0.4 -4 -3 -2 -1 0,0,,False
240,0.2 -4 -3 -2 -1 0,0,,False
241,"Figure 2: Averaged feature values of documents that were losers in at least four consecutive rounds before becoming winners, and whose feature values four rounds before winning were either lower or equal (`LW') or higher (`L>W') than those of the winner. `W': averaged feature value of the corresponding winners. x-axis: (minus) number of rounds before a document won a match. e values of LMIR.DIR are scaled by 100.",1,LM,True
242,whole units. e features in the second set are micro level similarity estimates that allow to analyze the potential actions taken by publishers to make their documents similar to the winner.,0,,False
243,6.1 Features,0,,False
244,"e features in the rst set, henceforth Macro features, are estimates of the bag-of-terms textual similarities (denoted SIM) between the document in round l (D), the document wri en by the same publisher in the previous round l - 1 (PD) and the winner of the previous round l - 1 (PW). e Cosine between tf.idf vector representations of documents is the similarity estimate. ree estimates are used: SIM(D,PD), SIM(D,PW) and SIM(PD,PW).",0,,False
245,"Using these inter-document similarity measures is inspired by the cluster hypothesis [18] which states that ""closely associated documents tend to be relevant to the same requests"". More speci cally, an important operational manifestation of the cluster hypothesis is the premise that e ective retrieval methods should assign similar documents with similar retrieval scores [9]. Based on the premise, given that the predictor we devise has no explicit knowledge of the ranking method used, inter-document similarities can potentially serve as proxies for similarities between retrieval scores.",0,,False
246,"e features in the second set, henceforth Micro features, focus on potential actions of publishers to make their documents similar to PW, the winner of the previous round. A document becomes similar to the winner, based on a bag-of-terms representation, if terms from the winner are added and terms not in the winner are removed. Accordingly, given a set S of terms, ADD(PW) and RMV(PW) are the number of unique terms t  S used in PW that were added to, or removed from, the document, respectively. Similarly, ADD( PW) and RMV( PW) are the number of unique terms t  S not used in PW that were added to or removed from the document, respectively. We de ne three term sets S: (i) query terms ( ery), (ii) frequent terms, speci cally, stopwords (Stopwords), and (iii) non-frequent terms not in the query ( ery Stopwords).8",1,ad,True
247,"Overall, we use 15 features: 3 Macro (SIM(D,PD), SIM(D,PW), SIM(PD,PW)) and 12 Micro ({ADD(PW), RMV(PW), ADD( PW), RMV( PW) } × { ery, Stopwords, ery Stopwords}).",0,,False
248,"e Macro features, which quantify temporal inter-document similarity changes, are ranking-model agnostic. e Micro features",0,,False
249,8A term is considered a stopword if it is among the 100 most frequent alphanumeric terms in the ClueWeb09 Category B corpus.,1,ClueWeb,True
250,"are based on temporal changes of addition/deletion of terms. While term-based information (e.g., query-terms occurrence) would be used by any reasonable ranker, the prediction model uses no explicit knowledge about how this information is used by the non-linear ranker applied in the competition, nor about other features used for ranking.",1,ad,True
251,6.2 Prediction setup,0,,False
252,"In each round of the competition, queries for which the winner of the previous round remained the winner were discarded, as our goal is to predict which loser publisher in a previous round will win the current round. us, the number of queries considered in each round ranges from 6 to 26 (out of all 31 queries).",0,,False
253,"We used the features9 from Section 6.1 for binary classi cation with logistic regression (LReg), linear SVM (LSVM), polynomial SVM (PSVM) and random forests (RForest) via the scikit-learn library [22]; the two classes are winner and loser. To train the classi ers and set hyper-parameter values, we used leave-one-out cross validation over rounds. e documents submi ed by students with respect to all considered queries in a round served for testing; those submi ed in the remaining six rounds, excluding the rst, served for training. Prediction was performed per query: the document in the current round which was wri en by a loser publisher from the previous round and which was assigned the highest classi cation score was predicted the winner; all other documents were predicted to be losers.",0,,False
254,"Prediction e ectiveness is measured using Accuracy: the percentage of documents correctly predicted as winners or losers, and F1: harmonic mean of Precision and Recall.10 Values are averaged over queries and test folds. Statistically signi cant e ectiveness di erences are determined using the two-tailed paired t-test with p  0.05 applied over queries.",0,,False
255,"e hyper-parameter values of the classi ers were selected to optimize Accuracy over the train set. For LReg, LSVM and PSVM, the value of the regularization parameter is in {1, 10, 50, 100}.11",0,,False
256,"e degree of the polynomial SVM (PSVM) was in {2, 3, 4, 5}. e number of trees and leaves for RForest were selected from {10, 50,",0,,False
257,"9Feature values were min-max normalized per query. 10 Precision is the fraction of correctly predicted winners out of all documents predicted to be winners. Recall is the fraction of winners correctly predicted as winners. 11LReg, LSVM and PSVM were trained with L1 regularization.",0,,False
258,470,0,,False
259,Session 4B: Retrieval Models and Ranking 2,1,Session,True
260,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
261,"Table 1: Prediction e ectiveness of the four classi ers (LReg, LSVM, PSVM, RForest) and the baselines. e performance di erences between each of the classi ers and each of the baselines are statistically signi cant. All the di erences with RForest are statistically signi cant. Bold: the best result in a row. Note: F1 of AllLosers is 0 due to zero Recall.",0,,False
262,Random Majority AllWinners AllLosers LReg LSVM PSVM RForest,0,,False
263,Accuracy 0.627,0,,False
264,F1,0,,False
265,0.242,0,,False
266,0.685 0.363,0,,False
267,0.247 0.396,0,,False
268,0.753 0.000,0,,False
269,0.849 0.859 0.867 0.695 0.712 0.730,0,,False
270,0.878 0.752,0,,False
271,"100, 500} and {10, 20, 30}, respectively. All other hyper-parameters were set to their default values [22].",0,,False
272,6.3 Prediction e ectiveness,0,,False
273,"Main result. We compare the prediction e ectiveness of the aforementioned classi ers with that of four baselines. All prediction algorithms predict as winner(s) documents whose publishers lost the previous round. (i) Random: a single winner is randomly selected; (ii) Majority: the document whose publisher won the majority of past rounds for the query is predicted the winner (ties are broken arbitrarily); (iii) AllWinners: all documents are predicted winners, in which case only one document per query is correctly predicted; and, (iv) AllLosers: all documents are predicted losers, in which case all but one of the documents are correctly predicted as losers. e results are presented in Table 1. Although the four classi ers (LReg, LSVM, PSVM and RForest) utilize no knowledge of the ranking model, they predict with high e ectiveness the winner of the current round. Moreover, the di erences in prediction e ectiveness between each of the classi ers and each of the four baselines are substantial and statistically signi cant. ese ndings a est to the ability to predict winners from previous losers in our competitions based on macro-level and micro-level manipulation strategies of publishers.",0,,False
274,"Among the four classi ers, the lowest performance is posted by LReg, while the highest is posted by RForest. Hence, in the analysis to follow we focus on RForest.",0,,False
275,"Feature analysis. We next study the relative e ectiveness of the sets of features used in RForest. Recall that the 15 features belong to two sets: Macro and Micro. e Micro features belong to three subsets: ery, Stopwords and ery Stopwords. In Table 2 we compare the prediction e ectiveness of training RForest using di erent combinations of these (sub)sets of features. We present for reference the e ectiveness of the Majority, AllWinners and AllLosers baselines. We see that using even a single (sub)set of features yields prediction e ectiveness that statistically signi cantly surpasses that of the baselines. Among the three subsets of Micro features, the query-term-based features ( ery) are the most e ective. Integrating all three subsets leads to prediction e ectiveness that always statistically signi cantly surpasses that of using either one or two of the subsets. We also see that using Micro features alone leads to slightly higher e ectiveness than using only Macro features; the di erence is not statistically signi cant. Yet, combining both sets yields the highest prediction e ectiveness. ese",1,ad,True
276,"ndings suggest that the Micro and Macro features, as well as the three subsets of Micro features, are complementary to some extent.",0,,False
277,"Table 2: Using subsets of features for prediction. All di erences with respect to Majority, AllWinners, AllLosers and Macro+Micro are statistically signi cant. Bold: best result in a column.",0,,False
278,Majority AllWinners AllLosers,0,,False
279,ery Stopwords,0,,False
280,"ery Stopwords ery+Stopwords ery+ ery Stopwords Stopwords+ ery Stopwords Micro , ery+ Stopwords+ ery Stopwords Macro Macro+ ery Macro+Stopwords Macro+ ery Stopwords",0,,False
281,Macro+Micro (all features),0,,False
282,Accuracy,0,,False
283,0.685 0.247 0.753,0,,False
284,0.821 0.809 0.796 0.826 0.825 0.813 0.837 0.836 0.851 0.849 0.847,0,,False
285,0.878,0,,False
286,F1,0,,False
287,0.363 0.396 0.000,0,,False
288,0.635 0.594 0.587 0.650 0.648 0.617 0.673 0.671 0.702 0.694 0.692,0,,False
289,0.752,0,,False
290,We next study the e ectiveness of individual features. Table 3 presents the Accuracy of ablation tests performed upon RForest.12 We also report MRR: the mean di erence between the reciprocal ranks of the actual winner when documents are ranked in descending and ascending order of individual feature values. We rst see that removing any single feature statistically signi cantly hurts Accuracy. is a ests to the complementary nature of the features.,0,,False
291,"e negative MRR of SIM(D,PD) indicates, as expected, that to win a match, a loser publisher should change her document with respect to the previous round. e positive MRR of SIM(PD,PW) and SIM(D,PW) suggest that the document should be similar to the winner (from the previous round) in the previous and current rounds so as to win the match. is nding is aligned with Corollary 3.4.",0,,False
292,"e MRR of features in the ery and Stopwords subsets indicate that adding (removing) query terms is always good (bad) practice for becoming the winner, regardless of whether these terms were used by the winner. is nding is further supported by the observations about eryTermsRatio in Section 5.2. In contrast, removing (adding) frequent terms, i.e., stopwords, is always good (bad) practice, regardless of the use of stopwords by the winner. e MRR of features in the ery Stopwords subset, which refers to terms that are neither query terms nor stopwords, imply that to",1,ad,True
293,12Similar pa erns were observed for F1. ese results are omi ed as they convey no additional insight.,1,ad,True
294,471,0,,False
295,Session 4B: Retrieval Models and Ranking 2,1,Session,True
296,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
297,Table 3: Ablation tests: Accuracy of RForest when trained without one feature. RForest's Accuracy with all features is 0.878. All di erences with RForest are statistically signi cant. MRR: the mean reciprocal ranks di erence of the winner when ranking documents in descending and ascending order of feature values.,0,,False
298,Macro Features,0,,False
299,Micro Features,0,,False
300,Feature,0,,False
301,"SIM(D,PD) SIM(D,PW) SIM(PD,PW)",0,,False
302,Ablation,0,,False
303,0.829 0.837 0.820,0,,False
304,MRR,0,,False
305,-0.136 0.168 0.184,0,,False
306,Feature,0,,False
307,ADD(PW) RMV(PW) ADD( PW) RMV( PW),0,,False
308,ery,0,,False
309,Ablation MRR,0,,False
310,0.844 0.851 0.840 0.834,0,,False
311,0.130 -0.043,0,,False
312,0.104 -0.620,0,,False
313,Stopwords,0,,False
314,Ablation MRR,0,,False
315,0.840 0.843 0.856 0.847,0,,False
316,-0.219 0.043,0,,False
317,-0.023 0.060,0,,False
318,ery Stopwords,0,,False
319,Ablation MRR,0,,False
320,0.841 0.857 0.849 0.837,0,,False
321,0.183 -0.081 -0.053,0,,False
322,0.029,0,,False
323,"win a match a document should become more similar to the winner by adding and not removing terms that were used by the winner (positive MRR of ADD(PW) and negative MRR of RMV(PW)), as well as removing and not adding terms that were not used by the winner (positive MRR of RMV( PW) and negative MRR of ADD( PW)). ese manipulations which do not directly a ect the query-document similarity estimates a ect other features used by the ranking model (e.g., Entropy).",1,ad,True
324,7 CONCLUSIONS,0,,False
325,"We presented an initial theoretical and empirical study of the strategic behavior of publishers (documents' authors) in query-based ranking competitions. e publishers' goal is promoting their documents in rankings using li le available information, mainly about past rankings. Analysis of ranking competitions that we organized revealed that to achieve their goal, publishers were making their documents similar to those ranked the highest in previous rounds. A game theoretic analysis of the competition yielded a result that provides formal support to the merits of this strategy. We also showed that high accuracy prediction of whether a document will be promoted to the rst rank in our competitions can be achieved using very few features which quantify document changes.",0,,False
326,Acknowledgments We thank the reviewers for their comments. is work was supported in part by a Google Faculty Research,0,,False
327,Award.,0,,False
328,A THE RANKING COMPETITIONS,0,,False
329,"We next discuss the competition guidelines provided to students (Section A.1), the incentives for participating in the competition (Section A.2), the queries and examples of relevant documents (Section A.3) and the ranking function used (Section A.4).",0,,False
330,A.1 Guidelines,0,,False
331,"To alleviate the task for students, and to increase their engagement in the competition, the length of all documents was limited to 150 terms. Students were instructed to write unstructured plain text documents.",0,,False
332,Duplication of other documents (determined based on a bagof-terms comparison) resulted in the duplicate document being ranked last. e students were permi ed to copy parts of other documents from the competition or the Web. Students were guided,0,,False
333,"to write documents of the highest quality avoiding slang and informal language. e use of black hat SEO techniques [16], such as keyword stu ng, was discouraged by telling the students that the ranking function will penalize low quality documents, partly based on human annotations. We informed the students that they could use the provided examples of relevant documents, but that the documents they create need not necessarily be relevant.",0,,False
334,A.2 Incentives,0,,False
335,"e incentive for participating in the competition was earning extra credit points for the exam. For each query, a student earned two thirds of a point if her document was ranked rst for a query in a match. A third of a point was given to all other students competing with respect to the same query (i.e., the same match).",0,,False
336,In the rst half of the competition many students did not (signi cantly) update their documents even if these were not ranked,0,,False
337,"rst. erefore, we further incentivized the students by changing the reward mechanism as from the h round. e student whose document was ranked rst for a query was reworded one point. Students whose documents were ranked second and third were rewarded two thirds and third of a point, respectively. Students whose documents were ranked lower did not receive any credit.",0,,False
338,A.3 eries and initial relevant documents,0,,False
339,"We used the titles of 31 topics selected from 1-200 from TREC 20092012 as queries. e preference was selecting queries with clear commercial intent, since they were more likely to stir up competition as is the case on the Web. at is, having a document ranked high ( rst) with respect to these queries should lead to increased (monetary) pro ts to the document's publisher on the Web. e selected queries focused mostly on topics related to products or services. Examples include ""used car parts"", ""cheap internet"" and ""gmat prep classes"". e queries were randomly assigned to students ensuring that two students will not compete against each other in more than two di erent matches; the assignments were not changed throughout the competition.",1,TREC,True
340,"As already noted, for each query we provided a single example of a relevant document. e goal was to provide the students with information regarding the underlying information need as the queries are very short. To produce these relevant documents, we",1,ad,True
341,rst used the TREC topic description as a query in a commercial search engine. We extracted from the highly ranked documents,1,TREC,True
342,472,0,,False
343,Session 4B: Retrieval Models and Ranking 2,1,Session,True
344,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
345,% of keyword stuffed documents % of relevant documents,0,,False
346,12,0,,False
347,100,0,,False
348,3,0,,False
349,8,0,,False
350,75,0,,False
351,4,0,,False
352,4,0,,False
353,50,0,,False
354,5,0,,False
355,0 12345678 round,0,,False
356,25 12345678 round,0,,False
357,"Figure 3: e percentage of documents annotated as keyword stu ed (le ) and relevant (right) by at least 3, 4 or 5 annotators, averaged over queries per each of the eight competition rounds.",0,,False
358,candidate window passages of up to 150 terms. e passages were annotated for relevance by four annotators. We kept extracting passages for each query until a passage was judged relevant by at least three annotators. is passage then served as the initial relevant document example for all students competing for the query.,0,,False
359,A.4 Ranking model,0,,False
360,We next describe the ranking model used for all queries in each round of every match in the competition.,0,,False
361,"A.4.1 Learning-to-rank. We used a learning-to-rank (LTR) approach with 25 features to rank the documents. Most of the features (22) were all those used in Microso 's learning-to-rank datasets13 for the ""whole document"" except for the Boolean Model, Vector Space Model and LMIR.ABS features. As noted above, the documents in our competition are unstructured plain text. us, all the features are computed only for the entire document. Since documents in our competitive se ing are prone to manipulation, we used three additional features which were shown to be highly e ective for spam classi cation [21] and Web retrieval [4]: (i) the ratio between the number of stopwords and non-stopwords in a document, (ii) the percentage of stopwords in a stopword list that appear in the document, and (iii) the entropy of the term distribution in a document [4]. For the two stopword-based features, the list of stopwords was composed of the 100 most frequent alphanumeric terms in the ClueWeb09 Category B corpus [21].",1,LM,True
362,"e ClueWeb09 category B dataset with queries 1-200 was used to learn the LTR model. Speci cally, the model was applied upon the 1000 documents most highly ranked by using LMIR.DIR, i.e., the negative cross entropy between the unsmoothed and Dirichletsmoothed (with µ ,"" 1000) unigram language models induced from the query and documents, respectively14. We used LambdaMART [29] via the RankLib library15 to integrate the di erent features. e number of trees and leaves were selected from {100, 250, 500, 750, 1000} and {10, 25, 50}, respectively. NDCG@5 served for optimization when learning the model. In each round of the competition, we added the (unjudged) documents submi ed by students in all matches to the ClueWeb09 Category B corpus to""",1,ClueWeb,True
363,"13www.research.microso .com/en-us/projects/mslr 14We deliberately did not remove suspected spam documents from the initial document ranking, e.g., using Waterloo's spam classi er [7]. is practice allows learning a model using low quality (e.g., spam) documents. 15 www.lemurproject.org/ranklib.ph",0,,False
364,"have more updated values of corpus statistics, e.g., inverse document frequency (idf). Yet, we did not re-train the ranker. e Indri toolkit was used for indexing and retrieval16. We applied Krovetz stemming upon queries and documents and removed stopwords on the INQUERY list only from queries. e LMIR.JM feature was used with  ,"" 0.1; for BM25, we set k1 "", 1.2 and b , 0.75.",1,LM,True
365,"A.4.2 Results diversification. To encourage students to considerably change their documents rather than introduce minor modi cations to the initially provided relevant document, starting from the second round, they were advised to diversify their documents with respect to the relevant document. To further encourage diversi cation, we applied the MMR method [5] with respect to the initial relevant document dinit. Accordingly, the score assigned to document d with respect to query q is score(q, d) d,""ef rank(d, LT R) - (1 - )rank(d, dinit), where  "","" 0.5, rank(d, LT R) is the rank of d in a ranking of all the documents in a match induced by the LTR method and rank(d, dinit) is the rank of d in a ranking created based on the similarity with dinit; here, the rank of the lowest ranked document is 1. e similarity with dinit was computed using LMIR.DIR treating d as the query.""",1,ad,True
366,"A.4.3 Keyword stu ing. Keyword stu ng [16], speci cally of query terms, is one of the most applicable manipulation approaches the students could employ to promote their unstructured plain text documents in rankings. To avoid rewarding excessive keyword stu ng, and to encourage writing of high quality documents, each document was manually classi ed as keyword stu ed or not17.",0,,False
367,e annotation was performed via CrowdFlower18; each document was judged by ve annotators from English speaking countries19.,0,,False
368,"e inter-annotator agreement for keyword stu ng, computed using the free-marginal multi-rater kappa measure [24], is 0.88. A document classi ed as keyword stu ed by at least four annotators was rank-penalized: with probability 0.5 it was swapped with the next document in the ranking. If several consecutively ranked documents were keyword stu ed, then only the lowest ranked document was penalized.",0,,False
369,"In Figure 3 (le ) we present for each round the percentage of documents classi ed as keyword stu ed by at least three, four or",0,,False
370,ve annotators averaged over queries. We can see a mostly downward trend until the h round. In the h round we observe the lowest percentage of keyword stu ed documents. Starting from the,0,,False
371,"h round the percentage of keyword stu ed documents gradually increases. We hypothesis that in the rst half of the competition students' engagement gradually decreased. In the second half, as from the h round in which the rewards for having a document ranked high substantially increased, students started using manipulated texts even more so as to have their documents ranked high. In the h round, there might have been some confusion due to the introduction of a new reward mechanism.",1,ad,True
372,"16 www.lemurproject.org/indri 17A document was annotated as keyword stu ed if it contained excessive repetition of words which seemed unnatural or arti cially introduced. 18www.crowd ower.com 19Annotators were also instructed to classify documents as spam if they were hard to understand, non-cohesive, did not make any sense or were useless to anyone seeking information. Yet, none of the documents was classi ed as spam.",0,,False
373,473,0,,False
374,Session 4B: Retrieval Models and Ranking 2,1,Session,True
375,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
376,22,0,,False
377,95,0,,False
378,1,0,,False
379,16,0,,False
380,80,0,,False
381,MAP@5 NDCG@5,1,MAP,True
382,2,0,,False
383,10,0,,False
384,65,0,,False
385,3,0,,False
386,4 12345678,0,,False
387,round,0,,False
388,50 12345678 round,0,,False
389,"Figure 4: e MAP@5 (le ) and NDCG@5 (right) performance of the ranking induced by the retrieval method in each round. Binary relevance judgments were induced for computing MAP@5 by considering a document relevant if its relevance grade was at least 1, 2 or 3.",1,MAP,True
390,A.5 Ranking e ectiveness,0,,False
391,"All documents in the collection were judged for relevance. Annotators were presented with both the title and description of each TREC topic, and were asked to classify a document as relevant if it satis ed the information need stated in the description. As was the case with keyword stu ng annotation, each document was judged by ve annotators from English speaking countries via CrowdFlower. e inter-annotator agreement rate, computed using the free-marginal multi-rater kappa measure [24], was 0.67. Four-scale graded relevance judgments were generated using the annotations as follows. A document judged relevant by less than three annotators was labeled as non-relevant (0). Documents judged relevant by at least three, four or ve annotators were labeled as marginally relevant (1), fairly relevant (2) and highly relevant (3), respectively.",1,TREC,True
392,"As noted above, to address the potential manipulation of documents by students, the retrieval method used in the competition (i) was based on a learning-to-rank approach with multiple features, (ii) incorporated highly e ective document-quality measures and (iii) penalized keyword stu ed documents. Figure 3 (right) presents the percentage of documents classi ed relevant by at least three, four or ve annotators per round averaged over queries. We see that, in general, the percentage of relevant documents decreased over the course of the competition. While many of the documents were judged relevant by at least three annotators, far fewer documents were judged relevant by at least four or ve annotators. is",1,ad,True
393,nding a ests to the negative e ects of SEO. In Figure 4 we present the MAP@5 and NDCG@5 e ectiveness,1,MAP,True
394,"of the document ranking induced by the retrieval method in each of the eight competition rounds. We see that the e ectiveness of the ranking has gradually decreased over rounds, which can be partially a ributed to the fact that fewer relevant documents were generated by students as seen in Figure 3. We also see that in the",1,ad,True
395,"rst two rounds the e ectiveness of the ranking was much higher than that in the rounds to follow. We found that in the rst two rounds students used the initially provided relevant documents without signi cantly changing them. A er the second round, in which the retrieval method was changed by applying diversi cation with respect to the given relevant document (see Section A.4.2), students started diversifying their documents by introducing noise, using non-relevant information and applying content manipulation.",0,,False
396,REFERENCES,0,,False
397,[1] R. Aumann and M. Maschler. 1995. Repeated Games with Incomplete Information. MIT Press.,0,,False
398,[2] Ran Ben-Basat and Elad Kravi. 2016. e ranking game. In Proceedings of the 19th International Workshop on Web and Databases. 7.,1,ad,True
399,"[3] Ran Ben-Basat, Moshe Tennenholtz, and Oren Kurland. 2015. e Probability Ranking Principle is Not Optimal in Adversarial Retrieval Se ings. In Proceedings of ICTIR. 51­60.",0,,False
400,"[4] Michael Bendersky, W. Bruce Cro , and Yanlei Diao. 2011. ality-biased ranking of web documents. In Proceedings of WSDM. 95­104.",0,,False
401,"[5] Jaime G. Carbonell and Jade Goldstein. 1998. e Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. In Proceedings of SIGIR. 335­336.",1,ad,True
402,"[6] Carlos Castillo and Brian D. Davison. 2010. Adversarial Web Search. Foundations and Trends in Information Retrieval 4, 5 (2010), 377­486.",0,,False
403,"[7] Gordon V. Cormack, Mark D. Smucker, and Charles L. A. Clarke. 2011. E cient and e ective spam ltering and re-ranking for large web datasets. Informaltiom Retrieval Journal 14, 5 (2011), 441­465.",0,,False
404,"[8] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. 2004. Adversarial Classi cation. In Proceedings of KDD. 99­108.",0,,False
405,[9] Fernando Diaz. 2005. Regularizing Ad Hoc Retrieval Scores. In Proceedings of CIKM. 672­679.,0,,False
406,[10] Ran El-Yaniv and Mordechai Nisenson. 2010. On the Foundations of Adversarial Single-Class Classi cation. CoRR (2010).,0,,False
407,"[11] K r Eliaz and Ran Spiegler. 2011. A simple model of search engine pricing. e Economic Journal 121, 556 (2011), F329­F339.",0,,False
408,"[12] K r Eliaz and Ran Spiegler. 2016. Search design and broad matching. American Economic Review 106, 3 (2016), 563­586.",1,ad,True
409,[13] Jonathan L. Elsas and Susan T. Dumais. 2010. Leveraging temporal dynamics of document content in relevance ranking. In Proceedings of WSDM. 1­10.,0,,False
410,"[14] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal study of information retrieval heuristics. In Proceedings of SIGIR. 49­56.",0,,False
411,"[15] Norbert Fuhr. 2008. A probability ranking principle for interactive information retrieval. Information Retrieval 11, 3 (2008), 251­265.",0,,False
412,"[16] Zolta´n Gyo¨ngyi and Hector Garcia-Molina. 2005. Web Spam Taxonomy. In Proceedings of AIRWeb 2005, First International Workshop on Adversarial Information Retrieval on the Web. 39­47.",0,,False
413,[17] Nathanael Hya l and Craig Boutilier. 2004. Regret Minimizing Equilibria and Mechanisms for Games with Strict Type Uncertainty. In Proceedings of UAI. 268­277.,0,,False
414,"[18] N. Jardine and C. J. van Rijsbergen. 1971. e use of hierarchic clustering in information retrieval. Information Storage and Retrieval 7, 5 (1971), 217­240.",0,,False
415,"[19] John D. La erty and Chengxiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR. 111­119.",0,,False
416,"[20] Tie-Yan Liu. 2011. Learning to Rank for Information Retrieval. Springer. I­XVII, 1­285 pages.",0,,False
417,"[21] Alexandros Ntoulas, Marc Najork, Mark Manasse, and Dennis Fe erly. 2006. Detecting spam web pages through content analysis. In Proceedings of WWW. 83­92.",0,,False
418,"[22] Fabian Pedregosa, Gae¨l Varoquaux, Alexandre Gramfort, Vincent Michel,",0,,False
419,"Bertrand irion, Olivier Grisel, Mathieu Blondel, Peter Pre enhofer, Ron",0,,False
420,"Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau,",0,,False
421,"Ma hieu Brucher, Ma hieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825­2830.",0,,False
422,[23] Kira Radinsky and Paul N. Benne . 2013. Predicting content change on the web. In Proceedings of WSDM. 415­424.,1,ad,True
423,"[24] Justus J. Randolph. 2016. Online Kappa Calculator (2008). Retrieved February 6,",0,,False
424,h p://justus.randolph.name/kappa. (2016). [25] Stephen E. Robertson. 1977. e Probability Ranking Principle in IR. Journal,0,,False
425,"of Documentation (1977), 294­304. Reprinted in K. Sparck Jones and P. Wille (eds), Readings in Information Retrieval, pp. 281­286, 1997. [26] Ae´cio S. R. Santos, Bruno Pasini, and Juliana Freire. 2016. A First Study on Temporal Dynamics of Topics on the Web. In Proceedings of WWW. 849­854. [27] Marc Sloan and Jun Wang. 2012. Dynamical information retrieval modelling: a portfolio-armed bandit machine approach. In Proceedings WWW. 603­604. [28] Hong Wang, Wei Xing, Kaiser Asif, and Brian D. Ziebart. 2015. Adversarial Prediction Games for Multivariate Losses. In Proceedings of NIPS. 2728­2736. [29] Qiang Wu, Christopher J. C. Burges, Krysta Marie Svore, and Jianfeng Gao. 2010. Adapting boosting for information retrieval measures. Information Retrieval 13, 3 (2010), 254­270. [30] Grace Hui Yang, Marc Sloan, and Jun Wang. 2016. Dynamic Information Retrieval Modeling. Morgan & Claypool Publishers. [31] Yinan Zhang and Chengxiang Zhai. 2015. Information Retrieval as Card Playing: A Formal Model for Optimizing Interactive Retrieval Interface. In Proceedings of SIGIR. 685­694.",1,ad,True
426,474,0,,False
427,,0,,False

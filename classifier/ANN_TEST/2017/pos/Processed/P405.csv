,sentence,label,data,regex
0,Session 4A: Evaluation 2,1,Session,True
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Comparing In Situ and Multidimensional Relevance Judgments,0,,False
3,Jiepu Jiang,0,,False
4,"Center for Intelligent Information Retrieval, University of Massachuse s",0,,False
5,Amherst jpjiang@cs.umass.edu,0,,False
6,Daqing He,0,,False
7,"School of Computing and Information, University of Pi sburgh",0,,False
8,dah44@pi .edu,0,,False
9,James Allan,0,,False
10,"Center for Intelligent Information Retrieval, University of Massachuse s",0,,False
11,Amherst allan@cs.umass.edu,0,,False
12,ABSTRACT,0,,False
13,"To address concerns of TREC-style relevance judgments, we explore two improvements. e rst one seeks to make relevance judgments contextual, collecting in situ feedback of users in an interactive search session and embracing usefulness as the primary judgment criterion. e second one collects multidimensional assessments to complement relevance or usefulness judgments, with four distinct alternative aspects examined in this paper--novelty, understandability, reliability, and e ort.",1,ad,True
14,"We evaluate di erent types of judgments by correlating them with six user experience measures collected from a lab user study. Results show that switching from TREC-style relevance criteria to usefulness is fruitful, but in situ judgments do not exhibit clear bene ts over the judgments collected without context. In contrast, combining relevance or usefulness with the four alternative judgments consistently improves the correlation with user experience measures, suggesting future IR systems should adopt multi-aspect search result judgments in development and evaluation.",1,TREC,True
15,"We further examine implicit feedback techniques for predicting these judgments. We nd that click dwell time, a popular indicator of search result quality, is able to predict some but not all dimensions of the judgments. We enrich the current implicit feedback methods using post-click user interaction in a search session and achieve be er prediction for all six dimensions of judgments.",0,,False
16,KEYWORDS,0,,False
17,Relevance judgment; search experience; implicit feedback.,0,,False
18,1 INTRODUCTION,1,DUC,True
19,"Test collection-based IR evaluation relies on human assessments of search result quality. e most popular method is the Cran eldstyle relevance judgments [9], such as the approach used in TREC [10], where assessors (usually trained experts) judge a preassigned set of search results one a er another using criteria that focus on topical relevance. is method had achieved great success but also a racted criticism such as focusing solely on topical relevance and ignoring real users' perceptions of the usefulness of results in a particular search context. We examine two directions to improve this status quo.",1,TREC,True
20,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080840",1,ad,True
21,"One direction is to incorporate context into assessments. at is, the value of a search result depends on the scenario and context of accessing the result. Belkin et al. [5] proposed to evaluate interactive search systems by the usefulness of each interaction for accomplishing a search task. We can apply this model to search result judgments--to assess the usefulness of a click (the perceived usefulness of a clicked result). is intrinsically requires us to switch from relevance to usefulness as the primary judgment criteria, and to collect in situ judgments to take into account the particular time and context of accessing a search result.",1,corpora,True
22,"Two recent e orts [25, 36] examined this direction. Kim et al. [25] collected users' in situ feedback of clicked results a er they had",1,ad,True
23,"nished examining the results. However, they restricted the in situ feedback to ""thumbs-up"" or ""thumbs-down"". Mao et al. [36] asked users to assess the usefulness of the clicked results a er a search session without considering the particular context. Both studies reported improved correlations with search experience measures comparing to TREC-style relevance judgments by external assessors. However, neither study excluded the in uence of the di erence between searchers and external assessors on relevance judgments.",1,TREC,True
24,"e other direction is to use a combination of multiple aspects of judgments. Many previous studies tried to complement relevance with seemingly reasonable dimensions, such as novelty [6, 55], understandability [41, 56], credibility [39, 46, 51, 53], readability [42, 49], e ort [20, 50, 54], freshness [11], etc. Multidimensional judgments are also popular approaches used in user-centric evaluation models [19, 27, 52]. However, most previous IR studies had only examined one particular alternative dimension to relevance, and they had not veri ed the value of multidimensional judgments by correlating with user experience measures.",1,ad,True
25,"We evaluate and compare these two directions. We collected users' search result judgments from six dimensions (relevance, usefulness, novelty, understandability, reliability, and e ort) in two se ings--an in situ one that happened right a er users had nished examining a clicked search result (called in situ judgments), and a context-independent one collected a er a search session (called post-session judgments). We evaluate the two types of judgments on six dimensions by correlating with six search experience measures collected from a laboratory user study. We also examined implicit feedback methods for predicting these judgments.",1,ad,True
26,We examine the following questions in the rest of this article:,0,,False
27,· Do in situ judgments be er correlate with search experience measures than context-independent (post-session) ones? Do multiple dimensions of judgments help relevance/usefulness judgments be er correlate with search experience measures? Which dimensions of judgments should we collect to improve a particular user experience measure? Section 3 seeks answers to these questions.,0,,False
28,405,0,,False
29,Session 4A: Evaluation 2,1,Session,True
30,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
31,Figure 1: A screenshot of the search interface and the in situ judgments interface.,0,,False
32,· Can we e ectively predict di erent search result judgments using implicit feedback signals? Section 4 and Section 5 examine techniques for addressing this challenge.,1,ad,True
33,2 USER STUDY,0,,False
34,We designed a user study to collect search result judgments. e user study asked participants to work on di erent tasks in an experimental search system. We recorded users' search behavior and collected their in situ and post-session search result judgments.,0,,False
35,2.1 Experiment Design,0,,False
36,e user study employed a 2×2 within-subject design to balance di erent types of search tasks. e tasks come from the TREC session tracks [7] and were categorized into four types by the targeted task product and goal based on Li and Belkin's faceted classi cation framework [28]. e targeted task product is either factual (to locate facts) or intellectual (to enhance the user's understanding of a topic). e goal of a task is either speci c (clear and fully developed) or amorphous (an ill-de ned or unclear goal that may evolve along with the user's exploration).,1,TREC,True
37,We divided participants into groups of four. Participants in the same group worked on the same four tasks (one task for each type) but using a di erent sequence (rotated using a Latin square). We assigned di erent tasks to di erent groups to increase task diversity.,0,,False
38,"For each task, the participants went through two stages: · Search stage (10 minutes). e participants performed an in-",0,,False
39,"teractive search session to address the task. ey could submit and reformulate any queries and click on any search results. After clicking on a result's link, the participants switched to the result webpage in a new browser tab. When they had nished examining the result and turned back to the SERP, the participant needed to provide in situ judgments on the clicked results before they could resume the search session. Figure 1 shows the screenshots of the search interface and the in situ judgments. · Judgment stage (about 10 minutes). e participants rated their search experience in the session and nished post-session judgments on each result they visited in the session. Section 2.2 introduces details of the in situ and post-session judgments. As Figure 1 shows, the interface of the experimental system is similar to popular web search engines. e system redirected users' queries to Google and returned ltered Google search results. e system only showed the ""10-blue links"", vertical search results",1,ad,True
40,"(except image verticals), and related queries. Other SERP elements were removed to simplify the user study. e system displayed results in the same way they would appear on Google. e main di erence between our system and Google in SERP design was that our system showed task description on the top of a SERP (to help participants recall task requirements) and we showed related searches on the right side of a SERP.",0,,False
41,"e participants spent about 100 minutes to nish an experiment. First, they worked on a training task (including all the steps) for 10 minutes. en, they worked on four formal tasks, spending about 20 minutes on each task. We required the participants to take a 5-minute break a er two formal tasks to reduce fatigue.",0,,False
42,2.2 Collecting Search Result Judgments,0,,False
43,We collected search result judgments in two di erent scenarios: · In situ judgments ­ participants assessed a clicked result when,0,,False
44,they had nished examining it and turned back to the SERP. · Post-session judgments ­ the judgments collected a er a search,1,ad,True
45,session (in the judgment stage). e in situ judgments measure the participants' perceptions of,0,,False
46,"the clicked result at (roughly) the same time and contexts they visit the result. e approach is similar to Kim et al. [25], except that we adopted di erent measures to assess search results. In the search stage, we instructed the participants to examine results as they would normally do when using a search engine in their daily lives. For example, they did not need to fully read a result and they could abandon examining. Particularly, they were instructed that during the in situ judgments, they should not revisit the result for the purpose of answering the judgment questions (and we did not o er a link for revisiting in the in situ judgment interface). is is to ensure that the in situ judgments only measure participants' perceptions of the latest click activity.",1,ad,True
47,"e post-session judgments resemble the TREC-style relevance judgments, where the assessors judge results without a particular search context and in a random order--they are asked to judge a set of results one a er another in detail. In our post-session judgments, the assessors are real searchers. We asked them to judge the set of results they visited in the session. We instructed them to examine the results in a be er detail in the post-session judgments. e system also required participants to revisit each clicked result and spend at least 30 seconds to judge a result.",1,TREC,True
48,406,0,,False
49,Session 4A: Evaluation 2,1,Session,True
50,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
51,Table 1: estions for collecting search result judgments and users' search experience.,0,,False
52,Search Result Judgments,0,,False
53,Topical Relevance (TRel),0,,False
54,Usefulness (Usef) Novelty (Nov) E ort (Effort) Understandability (Under) Reliability (Relia),1,Novelty,True
55,"estion & Options How relevant is this webpage? · Key (3): this page or site is dedicated to the topic; authoritative and comprehensive; it is worthy of being a top result. · Highly Relevant (2): the content of this page provides substantial information on the topic. · Relevant (1): the content of this page provides some information on the topic, which may be minimal. · Not Relevant or Spam (0). In Situ: How much useful information did you get from this web page? From 1 (none) to 7 (a lot of). Post-session: How much useful information did this web page provide for the task? From 1 (none) to 7 (a lot of). How much new information did you get from this web page? From 1 (none) to 7 (a lot of). How much e ort did you spend on this web page? From 1 (none) to 7 (a lot of). How di cult was it for you to follow the content of this web page? From 1 (very di cult) to 7 (very easy). How trustworthy is the information in this web page? From 1 (not at all trustworthy) to 7 (very trustworthy).",0,,False
56,Search Experience Measures Satisfaction (Sat) Frustration (Frus) System Helpfulness (Help) Goal Success (Succ) Session E ort (S.Eff) Di culty (Diff),1,Session,True
57,estion & Options How satis ed were you with your search experience? From 1 (very unsatis ed) to 7 (very satis ed). How frustrated were you with this task? From 1 (not frustrated) to 7 (very frustrated). How well did the system help you in this task? From 1 (very badly) to 7 (very well). How well did you ful ll the goal of this task? From 1 (very badly) to 7 (very well). How much e ort did this task take? From 1 (minimum) to 7 (a lot of). How di cult was this task? From 1 (very easy) to 7 (very di cult).,1,ad,True
58,We collected users' in situ and post-session judgments of six different measures. Table 1 shows the detailed questions and options.,0,,False
59,"· TREC relevance (TRel) ­ the de facto standard of relevance judgments due to the popularity of TREC test collections. We collected TRel using the criteria of the latest TREC web track [10]. As Table 1 shows, the criteria focus on topical relevance. We excluded the relevance level Nav (the correct homepage of a navigational query) from the original TREC criteria because our search tasks do not include navigational search.",1,TREC,True
60,"· Usefulness (Usef) ­ Following Belkin et al.'s model [5] and Mao et al.'s study [36], we collected users' perceptions regarding the usefulness of the clicked results.",0,,False
61,"· Novelty (Nov) ­ Novelty was o en assessed algorithmically in previous studies based on sub-topic or ""nugget"" level relevance judgments [8, 40, 43, 55]. In contrast, we collect users' explicit novelty judgments.",1,Novelty,True
62,· Understandability (Under) ­ the easiness of understanding the content of the result. Recent studies incorporated understandability into search result ranking [41] and evaluation [56].,1,corpora,True
63,"· Reliability (Relia) ­ the reliability, credibility, and trustworthy of the information presented in the result [39, 46, 51] (here we do not distinguish the three constructs).",0,,False
64,· E ort ­ Yilmaz et al. [54] and Verma et al. [50] examined e ort as a dimension of evaluating search result.,0,,False
65,"e following table summarizes the measures collected in in situ and post-session judgments. We only collected TRel in post-session judgments because the TREC criteria do not consider context. We only collected Nov and Effort during in situ judgments because the participants of a pilot study reported confusions assessing the two measures twice. In the rest of this paper, we will use .i and .p su xes to denote in situ and post-session judgments, respectively. For example, Usef.i denotes users' in situ usefulness judgments.",1,TREC,True
66,"Except for TRel, we collected judgments using a 7-point Likert scale, because a previous study [48] showed that assessors approximate the optimal level of con dence when using a 7-point scale for relevance judgments. TRel used a di erent scale so that it is",0,,False
67,TREC relevance (TRel) Usefulness (Usef) Novelty (Nov) E ort (Effort) Understandability (Under) Reliability (Relia),1,TREC,True
68,In Situ (.i),0,,False
69,Post-session (.p),0,,False
70,consistent with the TREC web track (as a representative example of the state-of-the-art relevance judgment methods).,1,TREC,True
71,2.3 Search Experience Measures,0,,False
72,"In the judgment stage, participants rated their search experience in a session. We collected six representative user experience measures used in previous studies of information retrieval and recommender systems--satisfaction (Sat) [17, 21, 26, 35, 36, 45], goal success (Succ) [1, 18], frustration (Frus) [12, 13], task di culty (Diff) [4, 15, 29, 31, 32], the helpfulness of the system (Help) [19] and the total e ort spent (S.Eff) [27]. Table 1 includes the questions.",0,,False
73,2.4 Rationale of Experiment Design,0,,False
74,"e way we balance di erent types of tasks is similar to previous studies [22, 24, 30, 33, 36]. However, we acknowledge that the selected tasks cannot cover all varieties. It is also worth noting that the TREC session track tasks [7] are more complex than regular web search requests such as navigational search.",1,TREC,True
75,"Our study aims to collect both in situ judgments and user behaviors related to the clicked results. is poses challenges to the experiment design. On the one hand, we hope to collect accurate in situ judgments, which o en requires multi-item measurements [27, 52]. On the other hand, interrupting participants for in situ judgments breaks the ow of search session and can a ect their subsequent search behaviors. To balance between the two purposes, we made a few compromises in experiment design, e.g., we only collected six popular dimensions of judgments, and we simply used one question to measure each dimension.",1,ad,True
76,407,0,,False
77,Session 4A: Evaluation 2,1,Session,True
78,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
79,Table 2: Spearman's correlation () matrix of di erent judgments for the 727 unique clicks.,0,,False
80,In Situ Judgments,0,,False
81,Post-session Judgments,0,,False
82,Usef.i Novelty Effort Under.i Relia.i TRel Usef.p Under.p,1,Novelty,True
83,Novelty,1,Novelty,True
84,0.67,0,,False
85,-,0,,False
86,-,0,,False
87,-,0,,False
88,-,0,,False
89,-,0,,False
90,-,0,,False
91,-,0,,False
92,In Situ,0,,False
93,E ort,0,,False
94,0.22,0,,False
95,Understandability,0,,False
96,0.20,0,,False
97,0.24,0,,False
98,-,0,,False
99,0.14 -0.45,0,,False
100,-,0,,False
101,-,0,,False
102,-,0,,False
103,-,0,,False
104,-,0,,False
105,-,0,,False
106,-,0,,False
107,Reliability,0,,False
108,0.42,0,,False
109,0.37,0,,False
110,0.05,0,,False
111,0.26,0,,False
112,-,0,,False
113,-,0,,False
114,-,0,,False
115,-,0,,False
116,Topical Relevance,0,,False
117,0.63,0,,False
118,0.46,0,,False
119,0.16,0,,False
120,0.14,0,,False
121,0.42,0,,False
122,-,0,,False
123,-,0,,False
124,-,0,,False
125,Post-session,0,,False
126,Usefulness Understandability,0,,False
127,0.72 0.20,0,,False
128,0.52,0,,False
129,0.16,0,,False
130,0.18 -0.36,0,,False
131,0.18 0.68,0,,False
132,0.43 0.83 0.29 0.18,0,,False
133,0.24,0,,False
134,-,0,,False
135,Reliability,0,,False
136,0.43,0,,False
137,0.38,0,,False
138,0.04,0,,False
139,0.22,0,,False
140,0.82 0.48,0,,False
141,0.51,0,,False
142,0.31,0,,False
143,e reported values are estimated from 1000 bootstrap samples (we used strati ed sampling to balance user and task dependency).,0,,False
144,"While examining search behaviors, we excluded the time spent on answering in situ judgments from dwell time. On average the participants spent 57.1 seconds on a clicked result and 12.1 seconds to answer the ve in situ judgment questions.",0,,False
145,2.5 Collected Data,0,,False
146,We recruited 28 participants (16 are female) through iers posted on the campuses of two universities in the United States. We required participants to be English native speakers to exclude the in uence of language uency on relevance judgments [16]. All the participants were undergraduate or graduate students studying di erent elds.,1,ad,True
147,"ey were reimbursed $15 per hour. We collected 112 sessions by 28 participants on 28 tasks. Each participant worked on four unique tasks and each task was performed by four unique users. In total, we collected 537 queries (4.8 per session) and 736 clicks (6.6 per session) on 727 unique sessionURL pairs (9 cases of revisiting). We exclude the 9 cases of revisiting from the analysis (about 1% of the data) to simply the analysis.",0,,False
148,3 IN SITU VS. POST-SESSION JUDGMENTS,0,,False
149,3.1 Correlation of Di erent Judgments,0,,False
150,"Table 2 reports the correlation of di erent judgments, which are generally consistent with previous studies. For example, relevance and usefulness positively correlate with novelty and reliability [52], understandability negatively correlates with e ort [50], etc. We examined the relationship of the judgments in another article [23].",0,,False
151,"Note that Mao et al. [36] reported a weak correlation (0.332) of searchers' post-session usefulness judgments and external assessors' relevance judgments. However, Table 2 shows that TRel and Usef.p are strongly correlated ( ,"" 0.83) when both of them are assessed by searchers. is suggests that the low correlation reported by Mao et al. [36] may be mostly due to the disparity between searchers and external assessors, rather than the di erence between using relevance or usefulness as the judgment criteria.""",0,,False
152,3.2 Correlating with User Experience,0,,False
153,"We evaluate di erent search result judgments by correlating with (regressing) users' search experience measures in a session. is is based on the assumption that the ""quality"" of the clicked results in a session can in uence users' search experience in that session--thus, a reasonable search result judgment (assumed to indicate certain ""quality""), or a reasonable set of judgments, should also correlate with users' search experience in a session.",0,,False
154,3.2.1 Regression Analysis. We use multilevel regression analysis,0,,False
155,to examine the relationship between the judgments of the clicked,0,,False
156,results and users' search experience in a session. e dependent,0,,False
157,variables (DVs) are each of the six search experience measures. e,0,,False
158,independent variables (IVs) include the statistics of judgments re-,0,,False
159,"garding the clicked results in a session (such as the mean, maximum,",0,,False
160,"and minimum ratings). For TRel, Usef.i, and Usef.p, we include",0,,False
161,"the mean, maximum, and minimum ratings of the clicked results in",0,,False
162,a session as IVs in the regression analysis. For other search result,0,,False
163,"judgments, we only include the maximum and minimum ratings of",0,,False
164,the clicked results as IVs. is is because the mean ratings of the,0,,False
165,"other measures o en highly correlate with those of TRel and Usef,",0,,False
166,causing multicollinearity issues for regression analysis.,0,,False
167,"For each user experience measure (the DV), we examine six",0,,False
168,di erent models that include di erent judgments as IVs.,0,,False
169,· Unidimensional & Context-independent ­ Model 1 and 2,0,,False
170,only include context-independent search result judgments from,0,,False
171,a single dimension--Model 1 includes the statistics of TRel and,0,,False
172,Model 2 includes those of Usef.p.,0,,False
173,· Unidimensional & In Situ ­ Model 3 includes in situ judg-,0,,False
174,ments from a single dimension (the statistics of Usef.i) as IVs.,0,,False
175,· Multidimensional & Context-independent ­ Model 4 and,0,,False
176,5 extend Model 1 and 2 to include other dimensions of judg-,0,,False
177,"ments (the statistics of Under.p, Relia.p, Nov, and Effort).",0,,False
178,Note that Model 4 and 5 include two in situ judgments (Nov,0,,False
179,and Effort) because we did not collect post-session judgments,0,,False
180,on these two dimensions (as discussed in § 2.2).,0,,False
181,· Multidimensional & In Situ ­ Model 6 extends Model 3 to,0,,False
182,"include other dimensions of judgments (the statistics of Under.i,",0,,False
183,"Relia.i, Nov, and Effort).",0,,False
184,Contextindependent,0,,False
185,In Situ,0,,False
186,Unidimensional,0,,False
187,1 TRel only 2 Usef.p only,0,,False
188,3 Usef.i only,0,,False
189,Multidimensional,0,,False
190,4 TRel + others 5 Usef.p + others,0,,False
191,6 Usef + others,0,,False
192,"All six models also include the same set of control variables, including: gender (Male or Female), age (four levels; 0 for 18­24, 1 for 25­30, 2 for 31­40, and 3 for Over 40), highest degree obtained or expected (Undergraduate or Graduate), the expertise of using web search engines (SE Expertise) rated using a Likert scale from 1 (very badly) to 5 (very well), task product and goal, user's familiarity with the topic of the task (Topic Familiarity) rated using a Likert scale from 1 (very unfamiliar) to 7 (very familiar), and the number of clicks (# clicks) and queries (# queries) in the session.",1,ad,True
193,408,0,,False
194,Session 4A: Evaluation 2,1,Session,True
195,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
196,Table 3: e adjusted R2 of di erent regression models.,1,ad,True
197,Models,0,,False
198,Sat Frus Succ S.Eff Help,0,,False
199,Base (control only) 0.12 0.06 0.11 0.06 0.11,0,,False
200,1 TRel,0,,False
201,0.23 0.09 0.18 0.10 0.16,0,,False
202,2 Usef.p,0,,False
203,0.25 0.15 0.36 0.17 0.18,0,,False
204,3 Usef.i,0,,False
205,0.29 0.14 0.35 0.16 0.22,0,,False
206,4 TRel + others 0.31 0.25 0.33 0.33 0.31,0,,False
207,4 vs. 1,0,,False
208,**,0,,False
209,**,0,,False
210,**,0,,False
211,**,0,,False
212,**,0,,False
213,5 Usef.p + others 0.30 0.26 0.42 0.37 0.31,0,,False
214,5 vs. 2,0,,False
215,**,0,,False
216,**,0,,False
217,**,0,,False
218,**,0,,False
219,**,0,,False
220,6 Usef.i + others 0.30 0.27 0.34 0.37 0.27,0,,False
221,6 vs. 3,0,,False
222,**,0,,False
223,**,0,,False
224,*,0,,False
225,* and ** indicate p < 0.05 and p < 0.01 by F-test.,0,,False
226,Diff,0,,False
227,0.03 0.09 0.18 0.22 0.21,0,,False
228,** 0.26,0,,False
229,** 0.33,0,,False
230,**,0,,False
231,We examine multicollinearity between variables using variance,0,,False
232,"in ation factor (VIF). e IVs of all models satisfy VIF < 4, the com-",0,,False
233,monly suggested threshold (4­10) for concerns of multicollinearity issues [37]. Table 3 reports the adjusted R2 of the six models for,1,ad,True
234,regressing the six dimensions of search experience.,0,,False
235,"3.2.2 TREC Relevance vs. Usefulness. We rst compare TREC relevance criteria (TRel) and post-session usefulness judgments (Usef.p). is is a revisit of Mao et al.'s study [36], which compared searchers' usefulness judgments and external assessors' relevance judgments. Here we collected both judgments from real searchers, removing the in uence caused by the di erence between searchers and external annotators in relevance judgments. e regression analysis suggest that switching from TREC relevance to usefulness is fruitful, consistently enhancing the ability of the regression models to correlate with user experience (by adjusted R2).",1,TREC,True
236,"Models 1 and 2 include the mean, maximum, and minimum TRel or Usef.p ratings of the clicked results. Model 2 consistently explains the six search experience measures be er than Model 1 (by adjusted R2). We note that usefulness (Usef.p) seems to be particularly be er than TREC relevance (TRel) in terms of correlating with goal success (Succ), with adjusted R2 , 0.36 vs 0.18.",1,ad,True
237,"Models 4 and 5 further include other dimensions of judgments as IVs. is helps compare TRel and Usef.p judgments with other search result judgments as controls. Still, we consistently observe that Model 5 explains the six search experience measures be er than or as well as model 4 . ese results verify that usefulness is indeed a be er criteria of relevance judgments than TREC-style relevance (in terms of correlating with users' search experience).",1,TREC,True
238,"3.2.3 In Situ vs. Context-independent (Post-session) Judgments. We further compare in situ and post-session judgments in both unidimensional and multidimensional se ings. Results suggest in situ usefulness judgments have be er correlations with a few (but not all) user experience measures than post-session usefulness judgments. However, a er combining search result judgments from di erent dimensions, in situ judgments show limited advantages over post-session ones.",1,ad,True
239,"Models 3 and 2 include the mean, maximum, and minimum Usef.i or Usef.p ratings of the clicked results as IVs. Results show Model 3 explains satisfaction (Sat), helpfulness (Help), and task di culty (Di ) slightly be er than Model 2 , with about 0.04 di erence in adjusted R2.",1,ad,True
240,"We further compare in situ and post-session judgments in a multidimensional se ing, using a combination of Usef.p/Usef.i",0,,False
241,"and other four judgments as IVs (Models 5 and 6 ). Results show that the post-session multidimensional model ( 5 ) be er correlates with search success (Succ) than the in situ one (adjusted R2 0.42 vs 0.34), but the la er also be er correlates with task di culty (adjusted R2 0.26 vs. 0.21). Overall, no evidence suggests either model is consistently be er than another in terms of correlating with users' search experience measures.",1,ad,True
242,"Even though Model 3 (Usef.i only) performs slightly be er than Model 2 (Usef.p only), results suggest limited advantages of in situ judgments over post-session ones in terms of correlating with search experience measures. We suspect a possible reason is that a 10-minute session is not long enough to trigger su cient di erences between in situ and post-session judgments. Although we expect to observe a greater di erence between in situ and post-session judgments in longer sessions, we believe a substantial proportion of web search sessions are no longer than 10 minutes, which may not bene t much from in situ judgments. In addition, it also requires a more complex experiment design to collect in situ judgments.",1,ad,True
243,"3.2.4 Unidimensional vs. Multidimensional Judgments. We further compare models using a combination of multiple aspects of judgments (Models 4 , 5 , and 6 ) with those using a single dimension (Models 1 , 2 , and 3 ). Results suggest that it is almost always helpful (enhancing the correlation with most of the six search experience measures signi cantly) to complement either relevance or usefulness with the alternative dimensions.",0,,False
244,"Models 4 and 5 explain all six dimensions of search experience measures signi cantly be er than Models 1 and 2 , suggesting that multidimensional judgments are almost always helpful for TREC-style relevance judgments (TRel) and post-session usefulness judgments (Usef.p). We also note that in situ usefulness judgments (Usef.i) worked particularly well for correlating with users' satisfaction (Sat) and goal success (Succ), such that combining with more dimensions of judgments adds li le to the model.",1,TREC,True
245,"Results demonstrate that multidimensional search result judgments are helpful, complementing unidimensional judgments and yielding be er correlation with search experience measures. is also suggests the advantages of multidimensional search result judgments over the in situ one--the former can consistently improve relevance/usefulness to be er correlate with almost all user experience measures, while the la er shows limited advantages.",1,ad,True
246,3.3 Which Dimensions To Judge?,0,,False
247,"A crucial issue of information retrieval is deciding which criteria to use to rank search results. We come to initial answers by looking into the standardized coe cients () of Model 5 (Table 4) as an example due to its superiority over other models. e standardized coe cient  stands for the magnitude of change in the DV (relative to its standard deviation) caused by one-unit change in the IV (relative to the IV's standard deviation) while other variables being equal. e coe cients of the model indicate how changes in the ""quality"" of the clicked results will (theoretically) a ect users' search experience in a session. Table 4 suggests that: · To enhance user satisfaction, a search system should present",0,,False
248,useful and novel results--both Usef.p (mean) and Nov (max) show signi cant positive e ects on Sat in Model 5 .,0,,False
249,409,0,,False
250,Session 4A: Evaluation 2,1,Session,True
251,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
252,Table 4: Multilevel regression: standardized coe cients () of independent variables for Model 5 ­ Usef.p + others.,0,,False
253,Independent,0,,False
254,DV: session-level search experience,0,,False
255,Variables,0,,False
256,Sat Frus Succ S.Eff Help Diff,0,,False
257,Gender: Male Age Degree: Graduate SE Expertise,1,ad,True
258,0.10 -0.05 -0.03,0,,False
259,0.12,0,,False
260,0.19 0.09 0.06 0.00 0.16 0.00 -0.03 -0.02 -0.11 0.00 0.05 0.01 0.13 -0.08 0.23 0.04 0.08 0.01 0.12 -0.00,0,,False
261,Product: Factual Goal: Speci c Topic Familiarity,0,,False
262,0.02 -0.02 -0.06 -0.09 -0.00 0.02 0.02 -0.07 0.04 0.05 0.07 0.01 0.10 -0.23 0.17 -0.20 0.19 -0.24,0,,False
263,# clicks,0,,False
264,0.20 -0.12 0.17 -0.07 0.18 -0.01,0,,False
265,# queries,0,,False
266,-0.36 0.17 -0.25 0.16 -0.35 -0.02,0,,False
267, Usef.p (mean) 0.23 -0.38 0.36 -0.36 0.08 -0.43,0,,False
268, Usef.p (max)  Usef.p (min),0,,False
269,0.16 0.09 0.22 -0.07 0.11 -0.08 0.01 0.19 -0.04 0.19 0.01 0.18,0,,False
270, Nov (max)  Nov (min),0,,False
271,0.24 -0.10 0.18 -0.09 -0.01 -0.20 -0.08 -0.06,0,,False
272,0.25 -0.11 0.07 -0.00,0,,False
273, Under.p (max)  Under.p (min),0,,False
274,0.09 -0.27 0.16 -0.08,0,,False
275,0.30 -0.15 0.14 -0.26,0,,False
276,0.14 -0.22 0.29 -0.27,0,,False
277, Relia.p (max) -0.13 -0.08 0.01 0.05 -0.03 0.06,0,,False
278, Relia.p (min) 0.06 0.01 -0.05 0.08 -0.07 0.04,0,,False
279, Effort (max) -0.12 0.16 0.08 0.28 -0.13 0.02,0,,False
280, Effort (min) Adjusted R2,0,,False
281,0.21 0.04 0.12 0.01 0.25 0.02 0.30 0.26 0.42 0.37 0.31 0.26,0,,False
282,"Light and dark shadings indicate p < 0.05 and 0.01, respectively.",1,ad,True
283,"· To reduce user frustration, a search system should o er results that are useful and easy-to-understand--both Usef.p (mean) and Under.p (max) show signi cant negative e ects on Frus.",0,,False
284,"· To help users successfully reach the goal (Succ), a search system should retrieve useful, novel, and easy-to-understand results-- Usef.p (mean), Nov (max), and Under.p (max) show signi cant positive e ects on Succ.",0,,False
285,"· To reduce the total e ort of a search session, the system should retrieve easy-to-understand results and avoid those requiring too much e ort--Under.p (min) shows a signi cant negative e ect on S.Eff and Effort (max) shows a positive one.",0,,False
286,"· To be er help users in a session (enhance the helpfulness of the system), a system should retrieve novel and easy-to-understand results--both Nov (max) and Under.p (max) show signi cant positive e ects on Help.",0,,False
287,"· To reduce the perceived task di culty, we need to retrieve useful and easy-to-understand results--both Usef.p (mean) and Under.p (min) show signi cant negative e ects on Diff. e coe cients suggest that the mean usefulness of the clicked",0,,False
288,"results is helpful for explaining all six search experience measures (has statistically signi cant coe cients). In addition, novelty, understandability, and e ort also signi cantly relate to many di erent search experience measures, suggesting they are useful complements to usefulness in search result judgments. In contrast, reliability shows no signi cant e ect on any of the six user experience measures in Model 5 . However, we suspect this is because the top-ranked results returned by Google are mostly reliable ones, which makes reliability a less important judgment measure among the clicked results.",1,ad,True
289,Table 5: Statistics of the absolute di erence of two users' ratings on the same results (||).,0,,False
290,Usef.i Effort Nov Relia.i Under.i TRel Usef.p Relia.p Under.p,0,,False
291,| | mean (SD) 1.55 (1.45) 1.52 (1.25) 1.60 (1.47) 1.23 (1.21) 1.18 (1.23) 0.63 (0.68) 1.53 (1.54) 1.38 (1.35) 1.08 (1.31),0,,False
292,"|| , 0 25.9% 22.9% 26.4% 31.3% 34.8% 48.3% 29.9% 30.8% 38.8%",0,,False
293,||  1 58.2% 57.7% 54.2% 67.2% 68.2% 89.1% 60.7% 62.2% 76.6%,0,,False
294,||  2 79.1% 76.1% 78.1% 86.1% 88.1% 100.0% 77.6% 81.1% 90.5%,0,,False
295,3.4 Variability of Judgments,0,,False
296,"We further examine the variability of judgments among di erent searchers, because in many practical scenarios we may have to train and evaluate retrieval systems based on relevance judgments made by external assessors. We suspect di erent users may have a greater degree of inconsistencies in their in situ judgments than their post-session ones (due to the contextual nature of the former). However, results do not support this conjecture well.",1,ad,True
297,"We examine the absolute di erence of two users' ratings on the same result. Table 5 reports the mean absolute di erence and the distribution. e mean absolute di erence for in situ and postsession usefulness judgments (Usef.i and Usef.p) are very close (1.55 vs. 1.53). e mean absolute di erence of post-session reliability judgments (Relia.p) is slightly higher than that for in situ ones (Relia.i) (1.38 vs. 1.23), but that for post-session understandability judgments (Under.p) is also slightly lower than the in situ ones (Under.i, 1.08 vs. 1.18). Overall, no evidence suggests that either in situ or post-session judgments is more or less consistent than the other across di erent users.",0,,False
298,"Further, we note that di erent users' reliability and understandability judgments seem more consistent than those for usefulness, e ort, and novelty judgments, regardless of performed in an in situ se ing or a post-session one. is suggests that usefulness, e ort, and novelty judgments may su er from inter-rate consistency by a greater extent, while inter-rate agreement is less likely a concern for understandability and reliability judgments. However, since users judged TRel by a di erent scale, it remains unclear how do the other ve judgments compare with standard TREC relevance judgments in terms of inter-rate consistency.",1,TREC,True
299,3.5 Summary,0,,False
300,"To sum up, this section discloses both opportunities and challenges for future search result judgments. · Opportunity ­ Since a combination of multidimensional judg-",0,,False
301,"ments explains user experience measures be er than using relevance or usefulness alone, we expect that an appropriate ranking of search results by multiple criteria may potentially yield be er user experience as well. e results in Table 4 also help select ranking criteria according to a targeted user experience measure. · Challenge ­ Extending current judgments from a single dimension to multiple aspects largely increases the cost of judgments.",0,,False
302,is is a crucial issue for the scalability of multidimensional judgments. e following sections address this concern by predicting multidimensional judgments using implicit feedback techniques.,1,ad,True
303,410,0,,False
304,Session 4A: Evaluation 2,1,Session,True
305,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
306,Table 6: Implicit feedback features and their correlation with di erent search result quality measures.,0,,False
307,Pearson's r with search result judgments,0,,False
308,Click Dwell Time Features,0,,False
309,Note,0,,False
310,TRel Usef.p Nov Effort Under.p Relia.p,0,,False
311,T1 Click dwell time (log).,0,,False
312,0.38,0,,False
313,0.43 0.41,0,,False
314,0.36,0,,False
315,0.12,0,,False
316,0.34,0,,False
317,T2 T3 T4 T5,0,,False
318,(t - µ )/ . t is the result's dwell time; µ is average click dwell time;  is the standard deviation of click dwell time. T3-5 are based on personalized versions of µ and  .,0,,False
319,all clicks by user by task by length,0,,False
320,0.31,0,,False
321,0.34 0.30,0,,False
322,0.32,0,,False
323,0.31,0,,False
324,0.36 0.38,0,,False
325,0.32,0,,False
326,0.31,0,,False
327,0.35 0.30,0,,False
328,0.32,0,,False
329,0.29,0,,False
330,0.33 0.29,0,,False
331,0.31,0,,False
332,0.06 0.09 0.06 0.06,0,,False
333,0.24 0.24 0.24 0.24,0,,False
334,Follow-up ery Features,0,,False
335,Q1,0,,False
336,Q2,0,,False
337,e number of terms in the next query found in the URL/title/body,0,,False
338,Q3 of the result.,0,,False
339,URL title body,0,,False
340,TRel,0,,False
341,-0.04 -0.03 -0.03,0,,False
342,Usef.p,0,,False
343,0.03 -0.00 -0.03,0,,False
344,Nov,0,,False
345,-0.01 -0.00,0,,False
346,0.10,0,,False
347,Effort,0,,False
348,-0.02 0.01 0.06,0,,False
349,Under.p,0,,False
350,-0.02 -0.00,0,,False
351,0.03,0,,False
352,Relia.p,0,,False
353,0.03 -0.02 -0.02,0,,False
354,Q4,0,,False
355,e percentage of terms in the next query found in the,0,,False
356,Q5 URL/title/body of the result.,0,,False
357,Q6,0,,False
358,URL title body,0,,False
359,0.02 0.09 0.02 -0.04,0,,False
360,0.01,0,,False
361,0.08,0,,False
362,0.07 0.10 0.06 -0.00,0,,False
363,0.05,0,,False
364,0.07,0,,False
365,0.17,0,,False
366,0.18 0.21,0,,False
367,0.04,0,,False
368,0.13,0,,False
369,0.18,0,,False
370,Q7,0,,False
371,e number of newly added query terms in the next query refor-,1,ad,True
372,Q8 mulation found in the URL/title/body of the result.,0,,False
373,Q9,0,,False
374,URL title body,0,,False
375,0.03 -0.01 -0.03 -0.06 0.07 0.04 0.00 -0.07 0.07 0.07 0.13 -0.04,0,,False
376,0.02,0,,False
377,0.02,0,,False
378,0.01 -0.00,0,,False
379,0.04 -0.02,0,,False
380,Q10,0,,False
381,e number of removed query terms in the next query reformula-,0,,False
382,Q11 tion found in the URL/title/body of the result.,0,,False
383,Q12,0,,False
384,URL title body,0,,False
385,0.01 0.01 -0.00 -0.07 -0.04 -0.12,0,,False
386,0.06 0.09 0.06 -0.09,0,,False
387,0.03 -0.06,0,,False
388,0.08,0,,False
389,0.07 0.06,0,,False
390,0.01,0,,False
391,-0.09,0,,False
392,-0.04,0,,False
393,Q13,0,,False
394,e mean/max/min log likelihood scores between the full content,0,,False
395,Q14 of the result and follow-up queries.,0,,False
396,Q15,0,,False
397,mean max min,0,,False
398,0.22 0.23 0.22 -0.03,0,,False
399,0.19,0,,False
400,0.23,0,,False
401,0.22 0.23 0.21 -0.03,0,,False
402,0.17,0,,False
403,0.18,0,,False
404,0.15 0.19 0.19 -0.01,0,,False
405,0.17,0,,False
406,0.19,0,,False
407,Follow-up Click Features,0,,False
408,TRel Usef.p Nov Effort Under.p Relia.p,0,,False
409,C1,0,,False
410,e mean/max/min similarity between the title of the result and,0,,False
411,C2 the titles of clicked results in follow-up searches.,0,,False
412,C3,0,,False
413,mean max min,0,,False
414,0.04,0,,False
415,0.05 0.12,0,,False
416,0.02,0,,False
417,0.04,0,,False
418,0.01,0,,False
419,0.05,0,,False
420,0.04 0.09,0,,False
421,0.00,0,,False
422,0.06,0,,False
423,0.00,0,,False
424,0.06,0,,False
425,0.08 0.10,0,,False
426,0.01,0,,False
427,-0.01,0,,False
428,0.03,0,,False
429,C4,0,,False
430,e mean/max/min similarity between the snippet of the result,0,,False
431,C5 and the snippets of clicked results in follow-up searches.,0,,False
432,C6,0,,False
433,mean max min,0,,False
434,-0.00 -0.04,0,,False
435,0.08,0,,False
436,0.01 -0.06,0,,False
437,0.11,0,,False
438,0.01 -0.06,0,,False
439,0.10,0,,False
440,0.04 -0.05,0,,False
441,0.07,0,,False
442,0.02,0,,False
443,0.03,0,,False
444,0.01 -0.04,0,,False
445,0.06,0,,False
446,0.09,0,,False
447,C7,0,,False
448,e mean/max/min similarity between the full content of the result,0,,False
449,C8 and the full contents of SAT clicks (dwell time > 30s) in follow-up,0,,False
450,C9 searches.,0,,False
451,C10,0,,False
452,e mean/max/min similarity between the title of the result and,0,,False
453,C11 the titles of skipped results in follow-up searches.,0,,False
454,C12,0,,False
455,mean max min mean max min,0,,False
456,0.19,0,,False
457,0.23 0.09,0,,False
458,0.01,0,,False
459,-0.02,0,,False
460,0.10,0,,False
461,0.20 0.20 0.05 -0.00,0,,False
462,0.00,0,,False
463,0.08,0,,False
464,0.12 0.17 0.07 -0.00 -0.01,0,,False
465,0.07,0,,False
466,0.09,0,,False
467,0.11 0.11,0,,False
468,0.02,0,,False
469,0.05,0,,False
470,0.05,0,,False
471,0.11 0.09 0.06 -0.01,0,,False
472,0.05,0,,False
473,0.02,0,,False
474,0.09 0.13 0.13 -0.05,0,,False
475,0.00,0,,False
476,0.05,0,,False
477,C13,0,,False
478,e mean/max/min similarity between the snippet of the result,0,,False
479,C14 and the snippets of skipped results in follow-up searches.,0,,False
480,C15,0,,False
481,mean max min,0,,False
482,0.01,0,,False
483,0.03 0.03,0,,False
484,0.11,0,,False
485,-0.05,0,,False
486,0.03,0,,False
487,0.02 0.01 -0.01 -0.00,0,,False
488,0.02 -0.02,0,,False
489,0.10,0,,False
490,0.12 0.12,0,,False
491,0.13,0,,False
492,-0.05,0,,False
493,0.11,0,,False
494,"Light and dark shadings indicate the correlation is signi cant at 0.05 and 0.01 levels, respectively.",1,ad,True
495,4 PREDICTION,0,,False
496,"is section introduces our techniques for predicting multidimensional judgments of clicked results from search logs. We model the prediction task as a regression problem--the input is features related to a target click, the output is the predicted judgment score of the clicked result. We use gradient boosted regression trees (GBRT) for prediction. Table 6 lists the prediction features. Due to the limited space, we only report results for predicting TRel and the judgments included in Model 5 --Usef.p, Nov, Effort, Under.p, and Relia.p. However, the described approach can also e ectively predict other search result judgments as well.",1,ad,True
497,4.1 Click Dwell Time Features,0,,False
498,"Click dwell time (T1) is one of the most widely used implicit feedback measure. As Table 6 shows, T1 does not correlate much with understandability, but it still has 0.3­0.4 correlations (signi cant at 0.01 level) with other measures.",0,,False
499,T2­T5 measure the deviation of a click's dwell time from the mean dwell time (µ) of a group of clicks (normalized by the standard deviation  ). T2 computes µ and  based on all clicks in the training sets. T3 is based on clicks by the same user. T4 is based on clicks in sessions with the same task type. T5 is based on clicks on documents with similar length (we divide the clicked results into ten bins by length and compute µ and  of a click based on its bin).,0,,False
500,4.2 Follow-up ery Features,0,,False
501,Follow-up query features are based on the intuition that a clicked result may in uence follow-up query reformulation in a session.,0,,False
502,"us, we can infer the quality of a click from queries issued a er the clicked result in the same session.",0,,False
503,Q1­Q6 match the terms in the immediate follow-up query with the target click. Q7­Q12 match the newly added and removed terms in the immediate follow-up query reformulation with the target click. Q13­Q15 match the target click with all follow-up queries.,1,ad,True
504,411,0,,False
505,Session 4A: Evaluation 2,1,Session,True
506,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
507,"Many of the follow-up query features (such as Q6 and Q13­Q15) have signi cant correlations with the search result quality measures, con rming that the intuition is reasonable. We also note that Q6 and Q13­Q15 have stronger correlations with understandability than click dwell time features.",0,,False
508,4.3 Follow-up Click Features,0,,False
509,"Similar to follow-up query features, we may also infer the quality of a target click based on follow-up clicks in a session.",0,,False
510,"C1­C6 measure the similarity between the target click and followup clicks. C7­C9 measure the similarity with follow-up satisfactory (SAT) clicks. C10­C15 measure the similarity with follow-up skipped results (unclicked results ranked higher than a clicked result). Some features have signi cant correlations with the search result quality measures, suggesting they may be useful predictors.",0,,False
511,4.4 Prior-to-click Features (Baseline),0,,False
512,"Prior-to-click features include the existing techniques that predict search result quality measures using information available before users clicking on the result. In this paper, they serve as the baseline for the implicit feedback features. We include a full list of prior-toclick features in an online appendix1.",0,,False
513,"We incorporate di erent prior-to-click features for predicting di erent measures. e shared features for all six measures include the rank of the result by Google search, ad hoc search models (QL, BM25, DFR [3], and SDM [38]), and session search models [14, 47].",1,corpora,True
514,e unique features for predicting each measure are: · TRel ­ a subset of LETOR features [34]. · Usef.p ­ a subset of LETOR features [34] and a subset of the,0,,False
515,"usefulness features by Mao et al. [36] that do not rely on postclick information. · Nov ­ the similarity of the click with previous clicks and higher ranked results in the same SERP (motivated by previous work on novelty-based search result diversi cation [6, 43, 44, 55]). · Effort ­ Yilmaz et al. [54] and Verma et al. [50]. · Under.p ­ Palo i et al. [41, 42]. · Relia.p ­ Olteanu et al. [39] and Wawer et al. [51]. Our prior-to-click features are representatives of the state-of-theart techniques for predicting each dimension of judgments without using implicit feedback. However, we did not include features that we do not have the resource to calculate, which include link structure based features and social media popularity features such as Twi er mention. Note this may reduce the e ectiveness of predicting reliability since the excluded features take about 1/3 of the features by Olteanu et al. [39] and Wawer et al. [51].",0,,False
516,5 EVALUATION,0,,False
517,5.1 Experiment Settings,0,,False
518,"We evaluate prediction (regression) by the Pearson's correlation between the predicted values and actual judgments (prediction correlation) and the root mean square error (RMSE) of the predicted values. Note that the RMSE for predicting di erent measures is not comparable-- rst, TREC relevance ranges from 0­3 while others from 1­7; second, their distributions vary a lot. Here we only report",1,TREC,True
519,1 h p://ciir.cs.umass.edu/downloads/mdrel/,1,ad,True
520,prediction correlation for its easy interpretability. e results of RMSE is highly consistent with that using prediction correlation.,0,,False
521,"e dataset for evaluation includes multidimensional judgments on the 727 unique clicked results. We use 10-fold cross validation for evaluation (using eight folds for training, one for validation, and one for testing). We randomly shu e the dataset 10 times and apply 10-fold cross-validation for each random shu ing of the whole dataset--this generates prediction results on 10 × 10 ,"" 100 test folds in total (note that we are not using a 100-fold cross validation). We report the mean and standard deviation (SD) of prediction correlation on the test folds. We note that the prediction correlation reported in this section is di erent from and cannot be compared with the correlation in Table 6, which are computed for the whole dataset without cross validation.""",0,,False
522,5.2 Click Dwell Time Features,0,,False
523,"Current techniques for inferring search result quality from logs rely on click dwell time. Results ( 1 in Table 7) suggest the click dwell time features work reasonably well for predicting usefulness, novelty, and e ort, but they have di culties inferring the understandability and reliability of results.",0,,False
524,"e click dwell time features ( 1 ) are e ective predictors for usefulness, novelty, and e ort. For these three measures, the predicted values have about 0.3­0.4 mean Pearson's correlation with the actual judgments, which is comparable to that for predicting TREC relevance (mean r ,"" 0.35). However, the click dwell time features perform much worse for predicting understandability and reliability. On average the predicted and actual judgments have only 0.10 and 0.22 correlation, suggesting it is necessary to incorporate new implicit feedback signals.""",1,TREC,True
525,5.3 Follow-up ery and Click Features,0,,False
526,We extend click dwell time to include signals from follow-up search activities. Results suggest the new features are helpful.,0,,False
527,"e follow-up query ( 2 ) and click features ( 3 ) alone have limited prediction capability. However, combining them with the click dwell time features ( 4 ) consistently produces be er prediction than using click dwell time features alone ( 1 ): except for e ort, the prediction correlation for the other ve measures using feature set 4 is signi cantly be er than that for click dwell time features ( 1 ). is indicates that follow-up queries and clicks indeed provide useful implicit feedback that are complementary to click dwell time.",0,,False
528,"e follow-up query and click features are particularly helpful for predicting reliability. Combining them with the click dwell time features improves the mean correlation of prediction from 0.22 to 0.36. e new features are also helpful for predicting TREC relevance and usefulness as well. is partly con rms our intuition--the quality of a clicked result may in uence follow-up search activities, making it possible to infer the quality of a clicked result based on what happened a erward in the session.",1,TREC,True
529,"e new features also improved the mean prediction correlation for understandability from 0.10 to 0.20. However, we note the combination of all implicit feedback features still does not work well for predicting understandability (mean r ,"" 0.20). is suggests that, compared with other judgments, it is more challenging to predict understandability based on the implicit feedback information.""",0,,False
530,412,0,,False
531,Session 4A: Evaluation 2,1,Session,True
532,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
533,Table 7: e e ectiveness of di erent features for predicting multidimensional search result judgments.,0,,False
534,Mean (SD) Pearson's r between true and predicted judgments over the test folds,0,,False
535,Features,0,,False
536,TRel,0,,False
537,Usef.p,0,,False
538,Nov,0,,False
539,Effort,0,,False
540,Under.p,0,,False
541,Relia.p,0,,False
542,1 Click Dwell Time,0,,False
543,0.35 (0.11),0,,False
544,0.40 (0.11),0,,False
545,0.42 (0.11),0,,False
546,0.31 (0.10),0,,False
547,0.10 (0.14),0,,False
548,0.22 (0.13),0,,False
549,2 Follow-up ery,0,,False
550,0.19 (0.11),0,,False
551,0.17 (0.14),0,,False
552,0.13 (0.13),0,,False
553,0.12 (0.11),0,,False
554,0.14 (0.12),0,,False
555,0.19 (0.12),0,,False
556,3 Follow-up Click,0,,False
557,0.15 (0.12),0,,False
558,0.20 (0.11),0,,False
559,0.11 (0.12),0,,False
560,0.14 (0.11),0,,False
561,0.11 (0.12),0,,False
562,0.17 (0.12),0,,False
563,4 All ( 1 + 2 + 3 ),0,,False
564,0.39 (0.09),0,,False
565,0.46 (0.08),0,,False
566,0.45 (0.09),0,,False
567,0.33 (0.11),0,,False
568,0.20 (0.13),0,,False
569,0.36 (0.12),0,,False
570,1 vs. 4,0,,False
571,**,0,,False
572,**,0,,False
573,*,0,,False
574,**,0,,False
575,**,0,,False
576,5 Prior-to-click,0,,False
577,0.36 (0.10),0,,False
578,0.29 (0.10),0,,False
579,0.28 (0.11),0,,False
580,0.13 (0.12),0,,False
581,0.20 (0.14),0,,False
582,0.18 (0.13),0,,False
583,4 vs. 5,0,,False
584,**,0,,False
585,**,0,,False
586,**,0,,False
587,**,0,,False
588,**,0,,False
589,6 All+Prior-to-click,0,,False
590,0.45 (0.08),0,,False
591,0.49 (0.09),0,,False
592,0.47 (0.09),0,,False
593,0.39 (0.09),0,,False
594,0.26 (0.12),0,,False
595,0.40 (0.11),0,,False
596,5 vs. 6,0,,False
597,**,0,,False
598,**,0,,False
599,**,0,,False
600,**,0,,False
601,**,0,,False
602,**,0,,False
603,* and ** indicate the di erence is statistically signi cant at 0.05 and 0.01 levels by two-tail paired t -test.,0,,False
604,5.4 Comparing to Prior-to-click Features,0,,False
605,An important application of implicit feedback techniques is to infer relevance labels from search logs. Aggregating inferred relevance labels or implicit feedback signals from past search logs may help rank search results in the future [2]. We examine whether or not implicit feedback techniques can serve a similar purpose for multidimensional judgments.,0,,False
606,"e combination of the implicit feedback features and the priorto-click features ( 6 ) generated signi cantly be er prediction results on all the six judgments than using the prior-to-click features alone ( 5 ). is suggests that the implicit feedback features are indeed helpful and complementary to the prior-to-click features for predicting these judgments. We also note that the improvements in mean prediction correlation can be as large as over 0.2 (such as for predicting reliability and e ort). However, even combining the two sets of features still cannot adequately predict understandability (mean r , 0.26).",1,ad,True
607,6 DISCUSSION AND CONCLUSION,0,,False
608,"A crucial issue of information retrieval is deciding which criteria to use to rank search results. We compared two seemingly reasonable directions for improving current TREC-style relevance judgments. One direction is to collect in situ search result judgments. e other one is to complement a single dimension of judgments (such as relevance or usefulness) by combining with other aspects. We found that the la er direction seems more e ective and versatile-- using a combination of di erent dimensions of judgments, we can almost always improve correlation with user experience measures.",1,TREC,True
609,"We envision future search engines should rank results by multiple aspects. We also o ered initial suggestions on which criteria to adopt and when to adopt them. We further examined and improved implicit feedback techniques for predicting multiple judgments, addressing the scalability concern of applying multidimensional judgments in real web search applications.",1,ad,True
610,Our study makes the following contributions: · We evaluated and compared in situ usefulness judgments with,0,,False
611,"regular relevance/usefulness judgments by searchers. We show that using usefulness as the judgment criteria is fruitful, but in situ judgments do not show clear bene ts over regular ones. · We evaluate multidimensional search result judgments considering four alternative aspects other than relevance/usefulness. We show that multidimensional judgments be er correlate with user",0,,False
612,"experience measures than using relevance/usefulness judgments alone. We also note that multidimensional judgments is a be er direction for improving TREC-style relevance judgments. · Our study also discloses the connections between di erent user experience measures and various dimensions of search result judgments. is o ers practical suggestions for system design, such as the appropriate dimensions to judge search results for the purpose of improving a particular user experience measure. · We successfully generalize implicit feedback signals to include follow-up searches and clicks in a search session to help click dwell time be er predict multidimensional judgments. To the best of our knowledge, we are also the rst to examine the e ectiveness of implicit feedback approaches for predicting novelty, understandability, reliability, and e ort.",1,TREC,True
613,Our work also sheds lights on a few critical areas for exploration in the future:,0,,False
614,"An important line of future work is to provide more accurate criteria for search result ranking and evaluation. Based on a regression analysis, we have already o ered initial suggestions on what criteria to use and when to use them, as discussed in Section 3.3. We note that, with a su ciently large dataset, one can possibly learn a prediction model for search experience measures by taking multidimensional judgments of results as input. Such a model can further address issues such as what are the proper weights to put on di erent aspects when ranking search results. It may also solve the discrepancy between o ine evaluation measures and user experience measures, and ultimately serve as a be er objective function for training ranking models.",1,ad,True
615,"Another important application is to perform multidimensional ranking of search results based on implicit feedback signals and other information. We have already demonstrated that implicit feedback approaches can infer judgments of usefulness, novelty, e ort, and reliability with reasonable accuracy comparing to those for relevance labels. Aggregating such inferred judgments from past search logs may serve as useful features for performing multidimensional search result ranking in the future. However, we also note that our current technique needs to be improved to be er infer understandability of results from search logs.",1,ad,True
616,"We do admit certain limitations in our current study. First, our analysis and experiments are solely based on data collected from one laboratory user study, which is limited in both scale and representativeness. We suggest that further studies employ larger",1,ad,True
617,413,0,,False
618,Session 4A: Evaluation 2,1,Session,True
619,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
620,"datasets to verify our ndings. Second, it is worth noting that our way of collecting in situ judgments in uenced users' natural search behaviors. We observed in our log that users spent on average 12.1 seconds to nish the in situ judgments. us some particular user behavior pa erns may vary when applied to another scenario (without interrupting users for in situ judgments). ird, we also note that we only collected search result judgments for the clicked results, while it remains unclear to which extent the ndings can be generalized to the unclicked ones. Last but not least, the collected post-session judgments are more or less in uenced by the search session and the in situ judgments (although we meant to collect context-independent judgments such as to compare with contextual ones). It is also worth noting that our post-session judgments are not fully representative of the existing TREC-style approach.",1,TREC,True
621,7 ACKNOWLEDGMENT,0,,False
622,"is work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor.",0,,False
623,REFERENCES,0,,False
624,"[1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it if you can: A game for modeling di erent types of web search success using interaction data. In SIGIR '11, pages 345­354, 2011.",0,,False
625,"[2] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In SIGIR '06, pages 19­26, 2006.",1,corpora,True
626,"[3] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.",0,,False
627,"[4] J. Arguello. Predicting search task di culty. In ECIR '14, pages 88­99, 2014. [5] N. J. Belkin, M. J. Cole, and J. Liu. A model for evaluation of interactive infor-",0,,False
628,"mation retrieval. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation, 2009. [6] J. Carbonell and J. Goldstein. e use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR '98, pages 335­336, 1998. [7] B. Cartere e, P. Clough, M. Hall, E. Kanoulas, and M. Sanderson. Evaluating retrieval over sessions: e TREC session track 2011-2014. In SIGIR '16, pages 685­688, 2016. [8] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ cher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR '08, pages 659­666, 2008. [9] C. W. Cleverdon. e evaluation of systems used in information retrieval. In Proceedings of the International Conference on Scienti c Information, pages 687­ 698, 1959. [10] K. Collins- ompson, C. Macdonald, P. Benne , F. Diaz, and E. Voorhees. TREC 2014 web track overview. In TREC 2014, 2014. [11] N. Dai, M. Shokouhi, and B. D. Davison. Learning to rank for freshness and relevance. In SIGIR '11, pages 95­104, 2011. [12] H. A. Feild and J. Allan. Modeling searcher frustration. In HCIR '09, pages 5­8, 2009. [13] H. A. Feild, J. Allan, and R. Jones. Predicting searcher frustration. In SIGIR '10, pages 34­41, 2010. [14] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In SIGIR '13, pages 453­462, 2013. [15] J. Gwizdka. Revisiting search task di culty: Behavioral and individual di erence measures. In ASIS&T '08, 2008. [16] P. Hansen and J. Karlgren. E ects of foreign language and task scenario on relevance assessment. J. Doc., 61(5):623­639, 2005. [17] A. Hassan. A semi-supervised approach to modeling web search satisfaction. In SIGIR '12, pages 275­284, 2012. [18] A. Hassan, R. Jones, and K. L. Klinkner. Beyond DCG: User behavior as a predictor of a successful search. In WSDM '10, pages 221­230, 2010. [19] R. Hu and P. Pu. A study on user perception of personality-based recommender systems. In UMAP '10, pages 291­302, 2010. [20] J. Jiang and J. Allan. Adaptive e ort for search evaluation metrics. In ECIR '16, pages 187­199, 2016.",1,TREC,True
629,"[21] J. Jiang, A. Hassan Awadallah, X. Shi, and R. W. White. Understanding and predicting graded search satisfaction. In WSDM '15, pages 57­66, 2015.",1,ad,True
630,"[22] J. Jiang, D. He, and J. Allan. Searching, browsing, and clicking in a search session: Changes in user behavior by task and over time. In SIGIR '14, pages 607­616, 2014.",0,,False
631,"[23] J. Jiang, D. He, D. Kelly, and J. Allan. Understanding ephemeral state of relevance. In CHIIR '17, pages 137­146, 2017.",0,,False
632,"[24] D. Kelly, J. Arguello, A. Edwards, and W.-c. Wu. Development and evaluation of search tasks for IIR experiments using a cognitive complexity framework. In ICTIR '15, pages 101­110, 2015.",0,,False
633,"[25] J. Y. Kim, J. Teevan, and N. Craswell. Explicit in situ user feedback for web search results. In SIGIR '16, pages 829­832, 2016.",0,,False
634,"[26] J. Kiseleva, E. Crestan, R. Brigo, and R. Di el. Modelling and detecting changes in user satisfaction. In CIKM '14, pages 1449­1458, 2014.",0,,False
635,"[27] B. P. Knijnenburg, M. C. Willemsen, Z. Gantner, H. Soncu, and C. Newell. Explaining the user experience of recommender systems. User Modeling and User-Adapted Interaction, 22(4-5):441­504, 2012.",0,,False
636,"[28] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Inf. Process. Manage., 44(6):1822­1837, 2008.",0,,False
637,"[29] C. Liu, J. Liu, and N. J. Belkin. Predicting search task di culty at di erent search stages. In CIKM '14, pages 569­578, 2014.",0,,False
638,"[30] J. Liu, J. Gwizdka, C. Liu, and N. J. Belkin. Predicting task di culty for di erent task types. In ASIS&T '10, 2010.",0,,False
639,"[31] J. Liu, C. Liu, M. Cole, N. J. Belkin, and X. Zhang. Exploring and predicting search task di culty. In CIKM '12, pages 1313­1322, 2012.",0,,False
640,"[32] J. Liu, C. Liu, J. Gwizdka, and N. J. Belkin. Can search systems detect users' task di culty?: Some behavioral signals. In SIGIR '10, pages 845­846, 2010.",0,,False
641,"[33] J. Liu, C. Liu, X. Yuan, and N. J. Belkin. Understanding searchers' perception of task di culty: Relationships with task type. In ASIS&T '11, 2011.",0,,False
642,"[34] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. LETOR: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR 2007 workshop on learning to rank for information retrieval, pages 3­10, 2007.",0,,False
643,"[35] Y. Liu, Y. Chen, J. Tang, J. Sun, M. Zhang, S. Ma, and X. Zhu. Di erent users, di erent opinions: Predicting search satisfaction with mouse movement information. In SIGIR '15, pages 493­502, 2015.",0,,False
644,"[36] J. Mao, Y. Liu, K. Zhou, J.-Y. Nie, J. Song, M. Zhang, S. Ma, J. Sun, and H. Luo. When does relevance mean usefulness and user satisfaction in web search? In SIGIR '16, pages 463­472, 2016.",0,,False
645,"[37] S. Menard. Applied Logistic Regression Analysis. Sage, 1997. [38] D. Metzler and W. B. Cro . A markov random eld model for term dependencies.",0,,False
646,"In SIGIR '05, pages 472­479, 2005. [39] A. Olteanu, S. Peshterliev, X. Liu, and K. Aberer. Web credibility: Features",0,,False
647,"exploration and credibility prediction. In ECIR '13, pages 557­568, 2013. [40] P. Over. e TREC interactive track: An annotated bibliography. Inf. Process.",1,TREC,True
648,"Manage., 37(3):369­381, 2001. [41] J. Palo i, L. Goeuriot, G. Zuccon, and A. Hanbury. Ranking health web pages",0,,False
649,"with relevance and understandability. In SIGIR '16, pages 965­968, 2016. [42] J. Palo i, G. Zuccon, and A. Hanbury. e in uence of pre-processing on the",0,,False
650,"estimation of readability of web documents. In CIKM '15, pages 1763­1766, 2015. [43] D. Ra ei, K. Bharat, and A. Shukla. Diversifying web search results. In WWW",1,ad,True
651,"'10, pages 781­790, 2010. [44] R. L. Santos, C. Macdonald, and I. Ounis. On the role of novelty for search result",0,,False
652,"diversi cation. Inf. Retr., 15(5):478­502, 2012. [45] A. Schuth, K. Hofmann, and F. Radlinski. Predicting search satisfaction metrics",1,ad,True
653,"with interleaved comparisons. In SIGIR '15, pages 463­472, 2015. [46] J. Schwarz and M. Morris. Augmenting web pages and search results to support",0,,False
654,"credibility assessment. In CHI '11, pages 1245­1254, 2011. [47] X. Shen, B. Tan, and C. Zhai. Context-sensitive information retrieval using",0,,False
655,"implicit feedback. In SIGIR '05, pages 43­50, 2005. [48] R. Tang, W. M. Shaw, Jr., and J. L. Vevea. Towards the identi cation of the optimal",0,,False
656,"number of relevance categories. J. Am. Soc. Inf. Sci., 50(3):254­264, 1999. [49] J. van Doorn, D. Odijk, D. M. Roijers, and M. de Rijke. Balancing relevance",0,,False
657,"criteria through multi-objective optimization. In SIGIR '16, pages 769­772, 2016. [50] M. Verma, E. Yilmaz, and N. Craswell. On obtaining e ort based judgements for",0,,False
658,"information retrieval. In WSDM '16, pages 277­286, 2016. [51] A. Wawer, R. Nielek, and A. Wierzbicki. Predicting webpage credibility using",0,,False
659,"linguistic features. In WWW '14 Companion, pages 1135­1140, 2014. [52] Y. Xu and Z. Chen. Relevance judgment: What do information users consider",0,,False
660,"beyond topicality? J. Am. Soc. Inf. Sci. Technol., 57(7):961­973, 2006. [53] Y. Yamamoto and K. Tanaka. Enhancing credibility judgment of web search",0,,False
661,"results. In CHI '11, pages 1235­1244, 2011. [54] E. Yilmaz, M. Verma, N. Craswell, F. Radlinski, and P. Bailey. Relevance and",1,ad,True
662,"e ort: An analysis of document utility. In CIKM '14, pages 91­100, 2014. [55] C. X. Zhai, W. W. Cohen, and J. La erty. Beyond independent relevance: Methods",0,,False
663,"and evaluation metrics for subtopic retrieval. In SIGIR '03, pages 10­17, 2003. [56] G. Zuccon. Understandability biased evaluation for information retrieval. In",0,,False
664,"ECIR '16, pages 280­292, 2016.",0,,False
665,414,0,,False
666,,0,,False

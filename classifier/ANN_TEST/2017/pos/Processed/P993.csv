,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Enhancing Recurrent Neural Networks with Positional Attention for Question Answering,0,,False
3,"Qin Chen1, Qinmin Hu1, Jimmy Xiangji Huang2, Liang He1,3 and Weijie An1",0,,False
4,"1Department of Computer Science & Technology, East China Normal University, Shanghai, China 2Information Retrieval & Knowledge Management Research Lab, York University, Toronto, Canada",1,ad,True
5,"3Shanghai Engineering Research Center of Intelligent Service Robot, Shanghai, China",0,,False
6,"{qchen,wjan}@ica.stc.sh.cn,{qmhu,lhe}@cs.ecnu.edu.cn,jhuang@yorku.ca",0,,False
7,ABSTRACT,0,,False
8,"Attention based recurrent neural networks (RNN) have shown a great success for question answering (QA) in recent years. Although significant improvements have been achieved over the nonattentive models, the position information is not well studied within the attention-based framework. Motivated by the effectiveness of using the word positional context to enhance information retrieval, we assume that if a word in the question (i.e., question word) occurs in an answer sentence, the neighboring words should be given more attention since they intuitively contain more valuable information for question answering than those far away. Based on this assumption, we propose a positional attention based RNN model, which incorporates the positional context of the question words into the answers' attentive representations. Experiments on two benchmark datasets show the great advantages of our proposed model. Specifically, we achieve a maximum improvement of 8.83% over the classical attention based RNN model in terms of mean average precision. Furthermore, our model is comparable to if not better than the state-of-the-art approaches for question answering.",1,corpora,True
9,1 INTRODUCTION,1,DUC,True
10,"Recurrent neural networks (RNN) have been widely used for question answering (QA) due to its good performance [8­10]. In the RNN based QA models, each word in a question or an answer sentence is represented with a hidden vector first. Then, all the hidden vectors are aggregated for sentence representations. Afterwards, the best answer is selected from a candidate answer pool according to the sentence similarity.",0,,False
11,"One major challenge in RNN is how to aggregate the hidden vectors for sentence representations. Recently, the attention mechanism has shown its effectiveness for the attentive sentence representations in many NLP tasks including QA [1, 8, 9, 14]. In particular, a weight is automatically generated for each word via attention, and the sentence is represented as the weighted sum of the hidden vectors. Various attention mechanisms have been proposed in previous studies. In [8], the attentive weights of the words in answers relied on the hidden representation of questions. Santos et al. [1] proposed",0,,False
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 https://doi.org/10.1145/3077136.3080699",1,ad,True
13,"a two-way attention mechanism, where the attentive weights for the question (answer) were influenced by the answer (question) representation according to the word-by-word interaction matrix. However, these attentions relied on the hidden vectors, which may excessively concern the words near the end of the sentence due to the abundant semantic accumulation over the word sequence in RNN. To alleviate the attention bias problem, Wang et al. [9] proposed three inner attention methods, which added the attention information before the hidden representations and achieved the state-of-the-art performance in QA.",1,ad,True
14,"To the best of our knowledge, all the previous attention mechanisms neglect the positional context, which has been extensively studied for performance boosting in information retrieval (IR). In [3] and [17], the occurrence positions of the query terms were modeled with various kernels and then integrated into traditional IR models to enhance the retrieval performance. Inspired by the effectiveness of the positional context in IR, we attempt to incorporate it into classical attentions to enhance the performance of RNN based QA. Specifically, it is assumed that if a word in the question (i.e., question word) occurs in an answer sentence, it will have an influence on the neighboring context. In other words, the neighboring words should be given more attention than those far away since they may contain more question relevant information. Based on this assumption, we propose a Positional Attention based RNN (RNN-POA) model, which models the position-aware influence of the question word for answers' attentive representations. To be specific, we first present a position-aware influence propagation strategy, in which the influence of the question word is propagated to other positions in the answer sentence by a distance-sensitive kernel. Then, a position-aware influence vector for each word is generated in the hidden space, according to the accumulated influence propagated by all the question words occurring in the answer. After that, the position-aware influence vector is integrated into the classical attention mechanism for answers' attentive representations. We perform experiments on two publicly available benchmark datasets, namely TREC-QA and WikiQA. The results show that our positional attention can significantly outperform the classical attention which does not involve any position information. Furthermore, the performance of our proposed RNN-POA model is comparable to the state-of-the-art approaches for question answering.",1,ad,True
15,"The main contributions of our work are as follows: (1) as far as we know, it is the first attempt to investigate the effectiveness of the positional context for answers' attentive representations; (2) we propose a positional attention based RNN model, which has been proved to be effective to boost the QA performance; (3) our positional attention approach can help alleviate the attention bias",0,,False
16,993,0,,False
17,Short Research Paper,0,,False
18,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
19,"problem by utilizing the position information of the question words, instead of the semantic accumulated hidden representations.",1,ad,True
20,2 PROPOSED MODEL,0,,False
21,2.1 Framework of RNN-POA,0,,False
22,"Figure 1(b) shows the framework of our positional attention based RNN (RNN-POA) model for answer representations. To have a better comparison, the classical attention based question representation is also shown in Figure 1(a). We adopt the bidirectional long shortterm memory (BLSTM) [2] model for sentence modeling, which takes the pre-trained word embeddings as the input, and generates the hidden vectors by recurrent updates [2].",1,ad,True
23,"To obtain the composite representations of the questions, we adopt the classical attention used in [14], which solely relies on the hidden vectors for the attentive weight generation. Regarding to the answer, we propose a positional attention approach, and perform additional steps upon the classical attention as follows: (1) find the occurrence positions of the question words in an answer sentence; (2) propagate the influence of the question words to other positions with our position-aware influence propagation strategy; (3) generate the position-aware influence vector for each word in the answer sentence according to the propagated influence; (4) incorporate the position-aware influence vector into the classical attention mechanism.",1,ad,True
24,"With the attentive representations of both questions and answers, various similarity functions can be utilized to measure the relevance between them. We utilize the Manhattan distance similarity function with l1 norm (Formula (1)), which performs slightly better than the other alternatives such as cosine similarity as indicated in [5]:",0,,False
25,"sim(rq , ra ) , exp(-||rq - ra ||1)",0,,False
26,(1),0,,False
27,Hidden Vector,0,,False
28,ra,0,,False
29,Positional Attention,0,,False
30,rq Attention,0,,False
31,Hidden Vector,0,,False
32,Word Embedding,0,,False
33,Word Embedding Find occurrence positions,0,,False
34,Word Question,0,,False
35,Position-aware Influence Propagation,0,,False
36,Answer,0,,False
37,Word,0,,False
38,Question,0,,False
39,(a),0,,False
40,Position-aware Influence Vector (b),0,,False
41,Figure 1: (a) question representation with classical attention based RNN; (b) answer representation with positional attention based RNN.,0,,False
42,2.2 Position-aware Influence Propagation,0,,False
43,"Based on our previous assumption, a question word will have an influence on the neighboring context if it occurs in an answer sentence. Here we model the position-aware influence propagation with the Gaussian kernel, which has been proved to be effective for",0,,False
44,"position modeling in IR [3, 4, 17]:",0,,False
45,Kernel (u),0,,False
46,",",0,,False
47,exp(,0,,False
48,-u2 2 2,0,,False
49,),0,,False
50,(2),0,,False
51,where u is the distance between the question word and the current,0,,False
52,"word,  is a parameter which restricts the propagation scope, and",0,,False
53,Kernel (u) denotes the obtained influence corresponding to the,0,,False
54,distance of u based on the kernel.,0,,False
55,Note that the position-aware influence is diminishing when the,0,,False
56,"distance increases. In particular, when u ,"" 0 (i.e., the current word""",0,,False
57,"is exactly a question word), the maximum propagated influence is",0,,False
58,"obtained. As to the propagation scope  , the optimal value may vary",0,,False
59,"from words to words. In this paper, we apply a constant  value",0,,False
60,"for all question words, and focus on incorporating the positional",1,corpora,True
61,context into attentions.,0,,False
62,2.3 Position-aware Influence Vector,0,,False
63,"In this section, in order to model the influence in a high-dimensional space for attentions, we will demonstrate how to obtain the positionaware influence vector for each word in an answer sentence. First, we assume that the influence for a specific distance follows the Gaussian distributions over the hidden dimensions. Then, an influence base matrix K is defined based on the assumption, where each column denotes the influence base vector corresponding to a specific distance. More formally, each element of K is defined as:",0,,False
64,"K(i, u)  N (Kernel (u),  )",0,,False
65,(3),0,,False
66,"where K(i, u) denotes the influence corresponding to the distance",0,,False
67,"of u in the i's dimension, and N is the normal density with an",0,,False
68,expected value of Kernel (u) and standard deviation of  .,0,,False
69,"With the influence base matrix, the influence vector for a word",0,,False
70,at a specific position is obtained by accumulating the influence of,0,,False
71,all the question words occurring in the answer:,0,,False
72,"pj , Kcj",0,,False
73,(4),0,,False
74,"where pj denotes the accumulated influence vector for the word at position j, and cj is a distance count vector which measures the",0,,False
75,"count of question words with various distances. Specifically, for the",0,,False
76,"word at position j, the count of question words with a distance of",0,,False
77,"u, namely cj (u), is calculated as:",0,,False
78,"cj (u) , [(j - u)  pos (q)] + [(j + u)  pos (q)] (5)",0,,False
79,q Q,0,,False
80,"where Q represents a question containing multiple question words, q is a word in Q, pos (q) denotes the set of q's occurrence positions in an answer sentence, and [·] is an indicator function which equals",0,,False
81,to 1 if the condition satisfies and otherwise equals to 0.,0,,False
82,2.4 Positional Attention,0,,False
83,"In most previous attention mechanisms, the attentive weight of",0,,False
84,"a word relies on the hidden representations, while the position",0,,False
85,"information is not well investigated. In this section, we propose",0,,False
86,"a positional attention approach, which incorporates the position-",1,corpora,True
87,aware influence of the question words into answers' attentive rep-,0,,False
88,"resentations. Specifically, the attentive weight of a word at position j in the answer sentence is formulated as:",0,,False
89,"j ,",0,,False
90,"exp(e (hj , pj ))",0,,False
91,l k,0,,False
92,",1",0,,False
93,exp,0,,False
94,(e,0,,False
95,(hk,0,,False
96,",",0,,False
97,pk,0,,False
98,),0,,False
99,),0,,False
100,(6),0,,False
101,994,0,,False
102,Short Research Paper,0,,False
103,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
104,"where hj is the hidden vector at position j based on RNN, pj is the accumulated position-aware influence vector obtained by Formula (4), l denotes the sentence length, and e (·) is a score function which measures the word importance based on the hidden vector and the",0,,False
105,"position-aware influence vector. More formally, the score function",0,,False
106,is defined as:,0,,False
107,"e (hj , pj ) , vT tanh(WH hj + WP pj + b)",0,,False
108,(7),0,,False
109,"where WH and WP are matrices, b is a bias vector, tanh is the hyperbolic tangent function, v is a global vector and vT denotes its",0,,False
110,"transpose. WH , WP , b and v are the parameters. With the obtained attentive weights, an answer sentence is rep-",0,,False
111,resented by the weighted sum of all the hidden vectors:,0,,False
112,l,0,,False
113,"ra , j hj",0,,False
114,(8),0,,False
115,"j ,1",0,,False
116,3 EXPERIMENTS,0,,False
117,3.1 Experimental Setup,0,,False
118,"Datasets and Evaluation Metrics. We conduct experiments on two public question answering datasets: TREC-QA and WikiQA. TREC-QA was created by Wang et al. [11] based on the TREC QA track data. WikiQA [13] is an open domain question-answering dataset in which all answers are collected from the Wikipedia. Each dataset is split into 3 parts, i.e., train, dev and test, and the statistics are presented in Table 1. To evaluate the model performance, we adopt the mean average precision (MAP) and mean reciprocal rank (MRR), which are the primary metrics used in QA [1, 9].",1,TREC,True
119,"Table 1: Dataset Statistics. ""Avg QL"" and ""Avg AL"" denote the average length of questions and answers.",0,,False
120,Dataset,0,,False
121,# of quetions Avg QL (train/dev/test) (train/dev/test),0,,False
122,Avg AL (train/dev/test),0,,False
123,TREC-QA 1162/65/68 7.57/8.00/8.63 23.21/24.9/25.61 WikiQA 873/126/243 7.16/7.23/7.26 25.29/24.59/24.59,1,TREC,True
124,"Parameter Settings. For the word embeddings, we use the 100dimensional GloVe [6] word vectors1. The parameters in BLSTM are shared between questions and answers, which has been shown to be effective to improve the performance [9]. The dimension of the hidden vectors and the position-aware influence vectors is set to 50. Regarding to the propagation scope  (in Formula (2)), we investigate a list of values ranging from 5 to 55 with an interval of 10. The value of  (in Formula (3)) is empirically set to 0.1. We adopt the cross-entropy loss as the training objective, and utilize the Adadelta [16] algorithm for parameter update. The optimal parameters are obtained based on the best MAP performance on the development (dev) set.",1,ad,True
125,3.2 Effect of Positional Attention,0,,False
126,"To investigate the effect of our positional attention approach, two basic baselines which do not involve the position information, namely average pooling (""AVG"") [10] and the classical attention (""ATT"") used in [14], are integrated into the BLSTM based RNN model for",0,,False
127,1 http://nlp.stanford.edu/data/glove.6B.zip,0,,False
128,"comparisons. Table 2 shows the performance of various models. Statistical significant tests are performed based on the paired t-test at the 0.05 level. The symbols as  and represent significant improvements over ""AVG"" and ""ATT"" respectively. We observe that the classical attention mechanism slightly outperforms the average pooling method by capturing part of the informative words in answers. However, it does not pay specifical attention to the question words and their surrounding context, which loses some useful information for question answering. In our positional attention approach, the importance of the question word and the surrounding context is explicitly highlighted via the position-aware influence propagation of the question words. Therefore, we can achieve significant improvements over the two baselines on both QA datasets, and the maximum improvement is as high as 8.83% in terms of MAP.",1,MAP,True
129,Table 2: Performance of various models.,0,,False
130,Model,0,,False
131,TREC-QA MAP MRR,1,TREC,True
132,RNN-AVG 0.7064,0,,False
133,RNN-ATT 0.7180 RNN-POA 0.7814*,0,,False
134,0.8086,0,,False
135,0.8121 0.8513*,0,,False
136,WikiQA MAP MRR,1,Wiki,True
137,0.6889,0,,False
138,0.6961 0.7212*,0,,False
139,0.6999,0,,False
140,0.7085 0.7312*,0,,False
141,3.3 Performance Comparisons,0,,False
142,"To further evaluate the effectiveness of our proposed model, we compare it with the recent work in QA. Table 3 and Table 4 summarize the results on TREC-QA and WikiQA respectively. For TREC-QA, five strong baselines are used for comparisons: (1) a combination of the BLSTM model and BM25 model [10]; (2) inner attention based RNN models which added the attention information before the hidden representations [9]; (3) a convolutional neural network (CNN) based architecture using both the hidden features and the statistical features for ranking [7]; (4) a learning-to-rank method which leveraged the word alignment features and lexical features for ranking [12]; (5) an extended LSTM framework which incorporated CNN and built the attention matrix after sentence representations [8]. As to WikiQA, in addition to [8] and [9] mentioned above, we make a comparison with other two strong baselines: (1) a bigram CNN model with average pooling [13]; (2) a CNN model which used an interactive attention matrix for the attentive representations [15].",1,TREC,True
143,"It is observed that we achieve the new state-of-the-art performance on TREC-QA in terms of both MAP and MRR. Regarding to WikiQA, our RNN-POA model outperforms all the strong baselines except the inner attention based RNN models [9]. This validates our previous assumption that the words close to the question words should be given more attention than those far away. In addition, it has been proved to be effective for incorporating the positional context into answers' attentive representations.",1,TREC,True
144,3.4 Investigation of Propagation Scope ,0,,False
145,"The parameter  (in Formula (2)) controls the influence propagation scope of the question words. For a certain distance, the propagated position-aware influence increases when  grows. Figure 2 plots the MAP and MRR metrics over a set of  values ranging from 5 to 55 with a step of 10. We observe that the general tendency for each",1,MAP,True
146,995,0,,False
147,Short Research Paper,0,,False
148,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
149,Table 3: Performance comparisons on TREC-QA. The work marked with  used the cleaned dataset.,1,TREC,True
150,System,0,,False
151,MAP MRR,1,MAP,True
152,Wang and Nyberg (2015) [10] Wang et al. (2016) [9]  Severyn and Moschitti (2015) [7] Wang and Ittycheriah (2015) [12] Santos et al. (2016) [8],0,,False
153,0.7134 0.7369 0.7459 0.7460 0.7530,0,,False
154,0.7913 0.8208 0.8078 0.8200 0.8511,0,,False
155,RNN-POA,0,,False
156,0.7814 0.8513,0,,False
157,Table 4: Performance comparisons on WikiQA,1,Wiki,True
158,System,0,,False
159,MAP MRR,1,MAP,True
160,Yang et al. (2015) [13] Santos et al. (2016) [8] Yin et al. (2015) [15] Wang et al. (2016) [9],0,,False
161,0.6520 0.6886 0.6921 0.7341,0,,False
162,0.6652 0.6957 0.7108 0.7418,0,,False
163,RNN-POA,0,,False
164,0.7212 0.7312,0,,False
165,"evaluation metric is similar. Specifically, the performance increases when  grows at first. Then, it decreases and tends to be stable when  becomes larger. On the whole, a  value between 15 and 35 is recommended to be a reliable setting in our experiments.",0,,False
166,(a) MAP,1,MAP,True
167,(b) MRR,0,,False
168,Figure 2: Impact of the propagation scope ,0,,False
169,3.5 A Case Study,0,,False
170,"To have an intuitive understanding of our proposed RNN-POA model, we draw a word heatmap for a case based on the classical attention and our positional attention respectively in Figure 3. Obviously, to answer this question, we should focus on the words ""George Warrington"". However, the classical attention cares more about some irrelevant words such as ""market"". Although these words have some semantic relations with the words in the question (e.g., the word ""market"" co-occurs frequently with the words ""chief "" and ""executive""), the semantically related words are not necessarily useful for question answering. In contrast, our positional attention approach concerns more about the question words such as ""Amtrak"", ""president"", ""chief "" and ""executive"", as well as the surrounding context such as ""George Warrington"", which provides more valuable clues for question answering. That is why our proposed positional attention approach can achieve much better performance than the classical attention method.",0,,False
171,4 CONCLUSIONS,0,,False
172,"In this paper, we propose a positional attention based RNN (RNNPOA) model, which incorporates the positional context of the question words into answers' attentive representations. The experimental results on two benchmark datasets show the overwhelming",1,NP,True
173,Q: who is the president or chief executive of amtrak?,0,,False
174,"Long term success here has to do with doing it right ,
getting it right and increasing market share. said RNN-ATT George Warrington, Amtrak's president and chief",0,,False
175,executive.,0,,False
176,"Long term success here has to do with doing it right ,
getting it right and increasing market share. said RNN-POA George Warrington, Amtrak's president and chief",0,,False
177,executive.,0,,False
178,Figure 3: An example of RNN-ATT and RNN-POA.,0,,False
179,"superiority of our proposed model over the basic baselines which do not incorporate any position information. Furthermore, compared with the state-of-the-art approaches in question answering, our proposed model can achieve a considerable performance by merely incorporating the position information into the classical attention mechanism. In the future, we will investigate more strategies to model the position influence of the question words.",1,corpora,True
180,ACKNOWLEDGMENTS,0,,False
181,"This research is supported by the National Key Technology Support Program (No.2015BAH01F02), Science and Technology Commission of Shanghai Municipality (No.16511102702), and Shanghai Municipal Commission of Economy and Information Under Grant Project (No.201602024). This research is also supported by a Discovery grant from the Natural Sciences and Engineering Research Council (NSERC) of Canada and an NSERC CREATE award.",1,ad,True
182,REFERENCES,0,,False
183,"[1] Cicero Nogueira dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016. Attentive pooling networks. arXiv preprint arXiv:1602.03609 (2016).",0,,False
184,"[2] Alex Graves and Jürgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks 18, 5 (2005), 602­610.",0,,False
185,"[3] Baiyan Liu, Xiangdong An, and Jimmy Xiangji Huang. 2015. Using term location information to enhance probabilistic information retrieval. In SIGIR. 883­886.",0,,False
186,"[4] Jun Miao, Jimmy Xiangji Huang, and Zheng Ye. 2012. Proximity-based rocchio's model for pseudo relevance. In SIGIR. 535­544.",0,,False
187,[5] Jonas Mueller and Aditya Thyagarajan. 2016. Siamese Recurrent Architectures for Learning Sentence Similarity. In AAAI. 2786­2792.,0,,False
188,"[6] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word Representation. In EMNLP. 1532­1543.",0,,False
189,[7] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to rank short text pairs with convolutional deep neural networks. In SIGIR. 373­382.,0,,False
190,"[8] Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2015. LSTM-based deep learning models for non-factoid answer selection. arXiv:1511.04108 (2015).",0,,False
191,"[9] Bingning Wang, Kang Liu, and Jun Zhao. 2016. Inner attention based recurrent neural networks for answer selection. In ACL. 1288­1297.",0,,False
192,[10] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering.. In ACL. 707­712.,0,,False
193,"[11] Mengqiu Wang, Noah A Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA. In EMNLP-CoNLL. 22­32.",0,,False
194,[12] Zhiguo Wang and Abraham Ittycheriah. 2015. FAQ-based Question Answering via Word Alignment. arXiv preprint arXiv:1507.02628 (2015).,0,,False
195,"[13] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain Question Answering.. In EMNLP. 2013­2018.",1,Wiki,True
196,"[14] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification. In NAACL-HLT. 1480­1489.",0,,False
197,"[15] Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. 2015. ABCNN: Attention-based convolutional neural network for modeling sentence pairs. In arXiv preprint arXiv:1512.05193.",0,,False
198,[16] Matthew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 (2012).,1,ad,True
199,"[17] Jiashu Zhao, Jimmy Xiangji Huang, and Ben He. 2011. CRTER: using cross terms to enhance probabilistic information retrieval. In SIGIR. 155­164.",0,,False
200,996,0,,False
201,,0,,False

,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Gauging the ality of Relevance Assessments using Inter-Rater Agreement,0,,False
3,Tadele T. Damessie,1,ad,True
4,"RMIT University Melbourne, Australia",0,,False
5,Falk Scholer,0,,False
6,"RMIT University Melbourne, Australia",0,,False
7,ABSTRACT,0,,False
8,"In recent years, gathering relevance judgments through non-topic originators has become an increasingly important problem in Information Retrieval. Relevance judgments can be used to measure the eectiveness of a system, and are oen needed to build supervised learning models in learning-to-rank retrieval systems. e two most popular approaches to gathering bronze level judgments ­ where the judge is not the originator of the information need for which relevance is being assessed, and is not a topic expert ­ is through a controlled user study, or through crowdsourcing. However, judging comes at a cost (in time, and usually money) and the quality of the judgments can vary widely. In this work, we directly compare the reliability of judgments using three dierent types of bronze assessor groups. Our rst group is a controlled Lab group; the second and third are two dierent crowdsourcing groups, CF-Document where assessors were free to judge any number of documents for a topic, and CF-Topic where judges were required to judge all of the documents from a single topic, in a manner similar to the Lab group. Our study shows that Lab assessors exhibit a higher level of agreement with a set of ground truth judgments than CF-Topic and CF-Document assessors. Inter-rater agreement rates show analogous trends. ese nding suggests that in the absence of ground truth data, agreement between assessors can be used to reliably gauge the quality of relevance judgments gathered from secondary assessors, and that controlled user studies are more likely to produce reliable judgments despite being more costly.",0,,False
9,1 INTRODUCTION,1,DUC,True
10,"Gathering relevance judgments using humans is a key component in building Information Retrieval test collections. However, human interpretation of ""relevance"" is an inherently subjective process [11]. According to Tang and Solomon [16], judging relevance is a dynamic, multidimensional process likely to vary between assessors, and sometimes even with a single assessor at dierent stages of the process. For example, Scholer et al. [13] found that 19% of duplicate",0,,False
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permied. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: hp://dx.doi.org/10.1145/3077136.3080729",1,ad,True
12,ao P. Nghiem,0,,False
13,"RMIT University Melbourne, Australia",0,,False
14,J. Shane Culpepper,0,,False
15,"RMIT University Melbourne, Australia",0,,False
16,document pairings were judged inconsistently in the TREC-7 and TREC-8 test collections. Understanding the factors that lead to such variation in relevance assessments is crucial to reliable test collection development.,1,TREC,True
17,"To address this issue, Bailey et al. [3] proposed three classes of judges ­ gold, silver and bronze ­ based on the expertise of the assessor. Gold judges are topic originators as well as subject experts; whereas silver judges are subject experts but not topic originators. Bronze judges are neither topic originators nor subject experts. But are all judges in a single class really the same? Secondary assessors who are neither topic creators nor experts are all bronze assessors, but there are in fact many dierent types of assessors who fall into this class. As assessment at the bronze level is now becoming a common practice in IR, in particular with the growing popularity of crowdsourcing, we set up an experiment to investigate the homogeneity of assessment quality using three dierent variants of bronze judges. e classes used in this study are:",1,ad,True
18,"· Lab: is group of assessors carried out a relevance assessment task in a monitored lab environment, with a requirement to assess a pre-determined number of 30 documents in relation to a single search topic.",0,,False
19,· CF-Topic: is group of assessors are an exact replica of the Lab group task except that the task was administered using the CrowdFlower crowdsourcing platform.,1,ad,True
20,"· CF-Document: is group of assessors performed the task using CrowdFlower just as the CF-Topic group, but unlike the other two groups, each participant could judge as few (minimum 1) or as many (maximum 30) documents as they liked for a topic.",0,,False
21,Our main research question can formally be stated as:,0,,False
22,Research estion: Are there dierences in the quality of relevance judgments gathered from dierent sub-classes of bronze-level judges?,0,,False
23,2 RELATED WORK,0,,False
24,"e subjective nature of relevance is likely to result in disagreement between judges [11, 15]. Voorhees [18] was among the rst to study this phenomenon, and quantied agreement in relevance assessment using overlap between primary TREC assessors and two secondary assessors on 48 topics. A total of 30% of the documents judged relevant by the primary assessor were judged non-relevant, and less than 3% of the documents initially judged as non-relevant by the primary assessor were judged relevant by the secondary assessors.",1,TREC,True
25,1089,0,,False
26,Short Research Paper,0,,False
27,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
28,Number of documents judged,0,,False
29,0,0,,False
30,20,0,,False
31,40,0,,False
32,60,0,,False
33,80,0,,False
34,100,0,,False
35,Assessors,0,,False
36,Figure 1: Distribution of number of documents judged per assessor by the CF-Document group.,0,,False
37,"Sormunen [14] also compared judgments from a group of 9 master's students using a 4-point ordinal relevance scale with TREC binary assessments on 38 topics. Around 25% of the documents originally judged as relevant by the TREC assessors were re-assessed as non-relevant, and around 1% of the documents originally judged as non-relevant were re-assessed as relevant. Al-Maskari et al. [1] also ran an experiment on 56 TREC-8 topics using 56 participants in an interactive search task. e study found a 37% dierence between TREC and non-TREC assessors. at is, out of the 2, 262 documents judged relevant by the non-TREC assessors, 834 of the documents were judged non-relevant by the TREC assessors. In both studies, there is a clear dierence between the TREC assessors who are topic originators, and the non-TREC assessors who oen are not.",1,TREC,True
38,"To address dierences between TREC judges and secondary assessors, Bailey et al. [3] identied three classes of judges ­ gold, silver and bronze ­ as discussed in Section 1. Bailey et al. found that assessments generated by silver judges were oen comparable to gold judges, but that extra care was needed when using bronze level judgments. However, the study did not prescribe exactly how this might be accomplished. In this study, we focus on dierent types of bronze level of assessors, as they now represent the most common class of judges outside of evaluation campaigns such as TREC which are being employed in large scale assessment gathering initiatives.",1,ad,True
39,3 METHODS AND DATASETS,0,,False
40,e TREC 7 and 8 datasets are used in this study. We focus on,1,TREC,True
41,"topics from these collections since they are widely believed to be among the most complete collections available [10], and provide",0,,False
42,a strong ground truth when aempting to quantify reliability in re-assessment exercises. Our work builds on two previous stud-,0,,False
43,"ies using the same topic conguration, and which provide further",0,,False
44,"details about the user study conguration [5, 6]. We use 4 dierent topics: the 2 highest and 2 lowest performing topics from the",0,,False
45,"dataset were selected using the average precision of each topic, averaged over the 110 runs submied to TREC 2004 Robust track.",1,TREC,True
46,"is approach, called average-average-precision (AAP), was initially described by Carteree et al. [4], and used to quantify topic",1,AP,True
47,diculty. Topic #365 (el nino) and #410 (schengen agreement),0,,False
48,"have the 2 highest AAP scores, and topic #378 (euro opposition)",1,AP,True
49,and #448 (,0,,False
50,) are the 2 lowest AAP scoring topics in,1,AP,True
51,ship losses,0,,False
52,"the collection. For assessment, 30 documents were chosen for each",0,,False
53,"topic, in proportion to an existing distribution of graded document relevance judgments made by Sormunen [14].",1,ad,True
54,"A total of 32, 40 and 43 assessors judged documents in the Lab, CFTopic and CF-Document experimental groups, respectively. For all crowdsourcing experiments, a mandatory explanation of relevance assignment per document was required, and manually checked as a quality control, to ensure that crowdsourcing participants were performing assessments in good faith. A total of 10 assessors, 5 from CF-Topic and 5 from CF-Document failed the sanity check, and their data was removed from the nal evaluation. All crowdsourcing experiments were conducted using the CrowdFlower platform in a manner similar to previously run studies [2].",0,,False
55,"e setup for the CF-Document group was designed to be as exible as possible, with assessors free to judge any number of the 30 documents for any of the 4 topics which were assigned randomly by the system. is setup introduces challenges during nal data analysis, however, since assessors judged an unequal number of documents, as shown in Figure 1, and a comparison of agreement between assessors with the same level of precision requires an incomplete balanced block design to be constructed as described by Fleiss [7]. is results in a sparse matrix of relevance scores for the maximum number of unique documents (30 per topic in our case) across the 121 unique assessors who contributed judgments.",0,,False
56,"Krippendor's Alpha ( ) is a chance-corrected measure of agreement, and not aected by dierences in sample sizes or missing values, and therefore appropriate for analysis of our experimental data [8]. Cohen's Kappa ( ) which is more suited for categorical data [17] is also used to quantify assessment quality against a gold standard. e values produced by these metrics is between 1 and 1, where a level of 0 indicates agreement at the level predicted by chance, 1 signies perfect agreement between raters, and a negative score occurs when agreement is less than what is expected by chance alone.",0,,False
57,4 RESULTS AND DISCUSSION,0,,False
58,"Assessor reliability ­ measured by the mean pairwise agreement between each assessor and the Sormunen gold standard assessments ­ is used to assess the quality of the assessments from each experimental group. is analysis is then compared with a measure of assessment quality using only inter-rater agreement, in the absence of any ground truth.",0,,False
59,"Assessor Reliability. e pairwise overall average reliability score of the Lab, CF-Topic and CF-Document groups, measured using [Krippendor's , Cohen's ] is [0.687, 0.581], [0.407, 0.236] and [0.561, 0.522] respectively. e scores are calculated on binary foldings of the 4-level graded relevance levels ­ non-relevant (0), marginally relevant (1), relevant (2) and highly relevant (3). e marginally relevant (1) and non-relevant (0) judgments are binarized as non-relevant and the others as relevant as recommended by Scholer and Turpin [12].",1,ad,True
60,"e results in Table 1 indicate Lab and CF-Document assessors are more reliable than CF-Topic assessors. e statistical signicance of the dierences is evaluated using an unpaired two-tailed t-test across the individual pairwise agreement scores, and reported in Table 2. For both and , the overall paern from highest to lowest reliability score measured using the Sormunen judgments",0,,False
61,1090,0,,False
62,Short Research Paper,0,,False
63,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
64,"Table 1: Average pairwise agreement between judges and Sormunen gold standard judgments, measured across All and individual topics using Krippendor's Alpha ( ) on a 4-levels of ordinal scale and Cohen's Kappa( ) on a binary scale.",0,,False
65,Krippendor's Alpha ( ),0,,False
66,Cohen's Kappa( ),0,,False
67,Lab CF-Topic CF-Document Lab CF-Topic CF-Document,0,,False
68,All,0,,False
69,0.687 0.407,0,,False
70,0.561,0,,False
71,0.581 0.236,0,,False
72,0.522,0,,False
73,el nino schengen agreement euro opposition ship losses,0,,False
74,0.843 0.622 0.665 0.617,0,,False
75,0.531 0.057 0.437 0.561,0,,False
76,0.725 0.380 0.377 0.704,0,,False
77,0.761 0.558 0.436 0.565,0,,False
78,0.277 0.111 0.112 0.416,0,,False
79,0.599 0.410 0.391 0.666,0,,False
80,"Table 2: Statistical signicance of Table 1 results, evaluated using an unpaired two-tailed t-test for all bronze assessors. Results for Krippendor's Alpha ( ) are shown below the diagonal line with ratings on a 4-level ordinal scale, while results for Cohen's Kappa ( ) are shown above the diagonal line with ratings on a binary scale, aening 0 and 1 to 0; and 2 and 3 to 1.",0,,False
81,Lab CF-Topic CF-Document,0,,False
82,"Lab [ , 0.687/ , 0.581]",0,,False
83,"95% CI 0.123, 0.435 Lab (M,""0.687, SD"",0.214) CF-Topic (M,""0.407, SD"",0.390) t (65) ,"" 3.583, p < 0.001 95% CI 0.142, 0.266 Lab (M"",""0.687, SD"",0.214) CF-Document (M,""0.561, SD"",0.345) t (68) ,"" 1.793, p "", 0.077",0,,False
84,"CF-Topic 95% CI 0.211, 0.479 Lab (M,""0.581, SD"",0.308) CF-Topic (M,""0.236, SD"",0.244) t (65) ,"" 5.082, p < 0.001 [ "", 0.407/ , 0.236]",0,,False
85,"95% CI 0.325, 0.018 CF-Topic (M,""0.407, SD"",0.390) CF-Document (M,""0.561, SD"",0.345) t (71) ,"" 1.781, p "", 0.079",0,,False
86,CF-Document,0,,False
87,"95% CI 0.099, 0.216 Lab (M,""0.581, SD"",0.308) CF-Document (M,""0.522, SD"",0.347) t (68) ,"" 0.739, p "", 0.462",0,,False
88,"95% CI 0.426, 0.144 CF-Document (M,""0.522, SD"",0.347) CF-Topic (M,""0.236, SD"",0.244) t (71) ,"" 4.026, p < 0.001""",0,,False
89,"[ , 0.561/ , 0.522]",0,,False
90,"as a baseline is: Lab, CF-Document and CF-Topic respectively. One explanation for this trend might be that the Lab study is a more directed environment, and assessors know that they are being closely monitored the entire time. is could contribute to longer periods of focus, resulting in a higher overall agreement with the gold standard, and therefore a presumed higher overall quality of obtained judgments.",0,,False
91,"When comparing only the two crowdsourcing groups, the CFDocument assessors show higher reliability. is is a somewhat surprising result, since the judges assess fewer documents and therefore spend less time overall forming a notion of relevance for a particular topic. However, this lack of ""domain knowledge"" might be counteracted by task completion time: an assessor in CF-Topic had to judge all 30 documents to get paid, and when an assessor encounters long or dicult documents at the tail of an assessment list, the likely outcome is that the assessor becomes less motivated to get any single judgment exactly right. Fatigue and motivation are known to inuence relevance judgment outcomes [9, 19], and perhaps contribute to the drop in quality. In contrast, CFDocument assessors may perceive that less eort is required on their behalf to judge a single topic-document pair before geing paid. ese ""micro"" transactions could very well be a strong motivator for crowdsourced assessors, despite having an implicit startup cost in understanding the task at hand that is amortized when judging multiple documents for the same topic. We plan to study this phenomenon in more detail in future work.",1,ad,True
92,"Figure 2 and Figure 3 give further insight on the reliability levels (agreement with the gold standard) of individual CF-Topic and CFDocument assessors, respectively. Results for the Lab group were omied due to space limitations; the reliability score for this group was consistently well above > 0.2, with no negative scores for any assessors. A number of assessors in CF-Topic showed lower levels of agreement with the gold standard than expected by chance alone for 2 of the topics as shown in Figure 2. Reliability for the other 2 topics in this group is similar to the trend observed for the Lab assessors. Only one assessor's relative performance in the CF-Document setup deviated signicantly from the others, as shown in Figure 3. We plan to further investigate the reasons for why such low reliability scores were observed for some individual assessors in these groups in followup work. Note that all of these assessors passed manual sanity control measures, and appeared to be performing judgments in good faith.",0,,False
93,"Agreement. As can been seen in Table 3, overall agreement is higher in Lab, followed by CF-Document and CF-Topic, which are in the same relative order as the reliability scores when comparing against a gold standard, suggesting that inter-rater reliability is a reasonable proxy for the quality of judgments.",0,,False
94,"To further establish our belief of assessor reliability, we computed the median of the multiple assessments made for each document in each experimental group, and computed the Krippendor's Alpha ( ) agreement between individual assessors and this score, shown in Table 3 (right). e overall trend is again consistent with the ndings of Table 1.",1,ad,True
95,1091,0,,False
96,Short Research Paper,0,,False
97,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
98,"Table 3: Inter-rater agreement (le) and majority vote (right) measured between assessors in the Lab, CF-Topic and CF-Document groups using Krippendor's alpha ( ) across All and individual topics with ratings on a 4-level ordinal scale. e number of assessors for inter-rater agreement is shown in parenthesis next to each value.",0,,False
99,Topic All,0,,False
100,el nino schengen agreement euro opposition ship losses,0,,False
101,Inter-rater agreement,0,,False
102,Lab,0,,False
103,CF-Topic CF-Document,0,,False
104,0.657 (32) 0.426 (35) 0.530 (121),0,,False
105,0.845 (8) 0.634 (8) 0.565 (8) 0.558 (8),0,,False
106,0.394 (8) 0.170 (8) 0.464 (9) 0.377 (10),0,,False
107,0.682 (31) 0.500 (29) 0.431 (29) 0.471 (32),0,,False
108,Lab,0,,False
109,0.787,0,,False
110,0.917 0.691 0.867 0.710,0,,False
111,Majority vote CF-Topic CF-Document,0,,False
112,0.544,0,,False
113,0.663,0,,False
114,0.608 0.436 0.537 0.605,0,,False
115,0.771 0.542 0.599 0.799,0,,False
116, score  score,0,,False
117,a,0,,False
118,1.0 0.8 0.6 0.4 0.2 0.0 -0.2 -0.4 -0.6,0,,False
119,Assessors,0,,False
120,b,0,,False
121,1.0 0.8 0.6 0.4 0.2 0.0 -0.2 -0.4 -0.6,0,,False
122,Assessors,0,,False
123, score,0,,False
124,Figure 2: Reliability of CF-Topic assessors when compared with the Sormunen judgments using Krippendor's Alpha ( ) for the topics: (a) El nino; and (b) Schengen agreement.,0,,False
125,1,0,,False
126,0.8,0,,False
127,0.6,0,,False
128,0.4,0,,False
129,0.2,0,,False
130,0,0,,False
131,-0.2,0,,False
132,-0.4,0,,False
133,-0.6 Assessors,0,,False
134,Figure 3: Reliability of CF-Document assessors when compared to the Sormunen judments using Krippendor's Alpha ( ).,0,,False
135,"Geing gold standard relevance labels is rarely possible in a live judging scenario, but it is possible to compute inter-rater agreement between assessors, and use this to establish the quality of assessments. Our experiments conrm that using agreement between judges to gauge the quality of relevance judgments collected is indeed one possible approach to controlling the quality of judgments gathered by bronze level assessors.",0,,False
136,5 CONCLUSION,0,,False
137,"is study analyzed the quality of relevance judgments generated in three (of many possible) dierent sub-classes of bronze assessors, using Krippendor's Alpha ( ) and Cohen's Kappa ( ). e results of both metrics conrm the existence of assessment quality dierences among the three sub-classes of bronze assessors, warranting",0,,False
138,"further study. Nevertheless, inter-rater agreement can be a reliable",0,,False
139,tool to benchmark the quality of relevance judgments when gold,0,,False
140,standard judgments are not readily available.,1,ad,True
141,Acknowledgment. is work was supported by the Australian Research Council's Discovery Projects Scheme (DP170102231 and DP140103256).,0,,False
142,REFERENCES,0,,False
143,"[1] A. Al-Maskari, M. Sanderson, and P. Clough. 2008. Relevance judgments between TREC and Non-TREC assessors. In Proc. SIGIR. 683­684.",1,TREC,True
144,"[2] O. Alonso and S. Mizzaro. 2012. Using crowdsourcing for TREC relevance assessment. Inf. Proc. & Man. 48, 6 (2012), 1053­1066.",1,TREC,True
145,"[3] P. Bailey, N. Craswell, I. Soboro, P. omas, A.P. de Vries, and E. Yilmaz. 2008. Relevance assessment: are judges exchangeable and does it maer. In Proc. SIGIR. 667­674.",0,,False
146,"[4] B. Carteree, V. Pavlu, H. Fang, and E. Kanoulas. 2009. Million ery Track 2009 Overview.. In Proc. TREC.",1,Track,True
147,"[5] T.T. Damessie, F. Scholer, and J.S. Culpepper. 2016. e Inuence of Topic Diculty, Relevance Level, and Document Ordering on Relevance Judging. In Proc. ADCS. 41­48.",0,,False
148,"[6] T.T. Damessie, F. Scholer, K. Ja¨rvelin, and J.S. Culpepper. 2016. e eect of document order and topic diculty on assessor agreement. In Proc. ICTIR. 73­76.",0,,False
149,"[7] J.L. Fleiss. 1981. Balanced incomplete block designs for inter-rater reliability studies. Applied Psychological Measurement 5, 1 (1981), 105­112.",0,,False
150,"[8] A.F. Hayes and K. Krippendor. 2007. Answering the call for a standard reliability measure for coding data. Comm. Methods and Measures 1, 1 (2007), 77­89.",0,,False
151,"[9] G. Kazai, J. Kamps, and N. Milic-Frayling. 2013. An analysis of human factors and label accuracy in crowdsourcing relevance judgments. Inf. Retr. 16, 2 (2013), 138­178.",0,,False
152,"[10] X. Lu, A. Moat, and J. S. Culpepper. 2016. e eect of pooling and evaluation depth on IR metrics. Inf. Retr. 19, 4 (2016), 416­445.",0,,False
153,"[11] T. Saracevic. 2007. Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. J. Amer. Soc. Inf. Sc. Tech. 58, 13 (2007), 1915­1933.",0,,False
154,[12] F. Scholer and A. Turpin. 2009. Metric and relevance mismatch in retrieval evaluation. In Proc. AIRS. 50­62.,0,,False
155,"[13] F. Scholer, A. Turpin, and M. Sanderson. 2011. antifying test collection quality based on the consistency of relevance judgements. In Proc. SIGIR. 1063­1072.",0,,False
156,[14] E. Sormunen. 2002. Liberal relevance criteria of TREC-: Counting on negligible documents?. In Proc. SIGIR. 324­330.,1,TREC,True
157,"[15] M. Stefano. 1997. Relevance: e whole history. J. Amer. Soc. Inf. Sc. 48, 9 (1997), 810­832.",0,,False
158,"[16] R. Tang and P. Solomon. 1998. Toward an understanding of the dynamics of relevance judgment: An analysis of one person's search behavior. Inf. Proc. & Man. 34, 2 (1998), 237­256.",0,,False
159,"[17] A.J. Viera and J.M. Garre. 2005. Understanding interobserver agreement: the kappa statistic. Fam. Med. 37, 5 (2005), 360­363.",0,,False
160,"[18] E.M. Voorhees. 2000. Variations in relevance judgments and the measurement of retrieval eectiveness. Inf. Proc. & Man. 36, 5 (2000), 697­716.",0,,False
161,"[19] J. Wang. 2011. Accuracy, agreement, speed, and perceived diculty of users' relevance judgments for e-discovery. In Proc. of SIGIR Inf. Ret. for E-Discovery Workshop., Vol. 1.",0,,False
162,1092,0,,False
163,,0,,False

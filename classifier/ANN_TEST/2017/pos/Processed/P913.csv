,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Neural Network based Reinforcement Learning for Real-time Pushing on Text Stream,0,,False
3,Haihui Tan,0,,False
4,The Hong Kong Polytechnic University,0,,False
5,"Kowloon, Hong Kong tanhaihui92@gmail.com",0,,False
6,Ziyu Lu,0,,False
7,The Hong Kong Polytechnic University,0,,False
8,"Kowloon, Hong Kong luziyuhku@gmail.com",0,,False
9,Wenjie Li,0,,False
10,The Hong Kong Polytechnic University,0,,False
11,"Kowloon, Hong Kong cswjli@comp.polyu.edu.hk",0,,False
12,ABSTRACT,0,,False
13,"The massive amount of noisy and redundant information in text streams makes it a challenge for users to acquire timely and relevant information in social media. Real-time noti cation pushing on text stream is of practical importance. In this paper, we formulate the real-time pushing on text stream as a sequential decision making problem and propose a Neural Network based Reinforcement Learning (NNRL) algorithm for real-time decision making, e.g., push or skip the incoming text, with considering both history dependencies and future uncertainty. A novel Q-Network which contains a Long Short Term Memory (LSTM) layer and three fully connected neural network layers is designed to maximize the long-term rewards. Experiment results on the real data from TREC 2016 Real-time Summarization track show that our algorithm signi cantly outperforms state-of-the-art methods.",1,TREC,True
14,CCS CONCEPTS,0,,False
15,· Information systems  Document ltering;,0,,False
16,KEYWORDS,0,,False
17,"Text Stream, Real-Time Pushing, Deep Reinforcement Learning",0,,False
18,1 INTRODUCTION,1,DUC,True
19,"With the development of social media, text streams such as Twitter posts, news articles or user reviews are being generated in fastgrowing volumes. The explosive amount of noisy and redundant streaming texts are overwhelming and makes it di cult to nd useful information. Pushing or Summarizing the real-time streaming text for users is of practical importance. In recent decades, the real-time pushing on text streams has attracted increasing attention at Text Retrieve Conferences (TREC), e.g., the Microblog track in 2015 [4] and the Real-time Summarization track in 2016 [6]. For example, [9] which has achieved the best performance for the real-time ltering task at TREC 2015 Microblog track, explored dynamic emission strategies to maintain appropriate thresholds for pushing relevant tweets. Fan et al. [2] proposed an adaptive evolutionary ltering framework to lter interesting tweets from",1,Twitter,True
20,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00. DOI: http://dx.doi.org/10.1145/3077136.3080677",1,ad,True
21,"the Twitter stream with respect to user interest pro les. However, pushing real-time text streams is not a short-term process. It is a dynamic forward decision process in which the current action will a ect further decisions (dependency) and further streaming texts generate uncertainty on the current decision. [5] treated real-time stream summarization as a sequential decision making problem and adopted a locally optimal learning to search algorithm to learn a policy for selecting or skipping a sentence in the text stream, by imitating a heuristic reference policy. However, the heuristic reference policy is di cult to construct for real-time streaming text and it needs massive human interventions. The real-time decision process needs to maximize the long-term rewards by taking both history dependencies and future uncertainty into consideration.",1,Twitter,True
22,"In this paper, we de ne the real-time pushing on text stream as a long-term optimization problem and propose a neural network based reinforcement learning algorithm as the solution. A novel Q-Network is de ned to learn a long-term policy for sequential decision making. When receiving text streams, the Q-network predicts the values for taking each action under the current state and chooses the action with the higher value. A long short-term memory (LSTM) layer is integrated into the Q-Network to represent the high-level abstraction of streaming text by considering both semantic and temporal dependencies of previously pushed texts. The Q-Network is continuously updated according to the observation of interrelationship between actions and rewards, and a long-term policy is explored and exploited to make real-time decisions on text stream. The main contributions of our paper is as follows:",0,,False
23,"· We formulate real-time pushing on text streams as a longterm optimization problem, by considering both history dependencies and future uncertainty of text stream.",0,,False
24,· A Neural Network based Reinforcement learning (NNRL) algorithm is proposed to maximize long-term rewards and obtain the long-term policy for real-time decision making.,0,,False
25,· Experimental evaluations on the real tweet stream show that our method has superior performance over the stateof-the-art methods.,0,,False
26,2 METHOD,0,,False
27,"Problem De nition: Given a text stream S ,"" {t1, t2, · · · , tn, · · · }, Our model makes real-time decisions from an action set A "","" {ap , as } for each incoming text t. ap is to push t while as is to skip it. As our target is to maximize the long-term rewards, we adopt a reinforcement learning framework to de ne and solve this problem.""",1,ad,True
28,"In reinforcement learning methods, the agent model selects an action from the action set A and passes it to the environment  which changes its inner state s and the reward score r . Given the",0,,False
29,913,0,,False
30,Short Research Paper,0,,False
31,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
32,"state st ,"" [x1, a1, x2, a2, · · · , at -1, xt ] which is a sequence of observations and actions a, the model arrives at an optimal action-value function Q(s, a) which is the maximum expected return with a""",0,,False
33,policy  .,0,,False
34,2.1 Neural Network-based Reinforcement Learning (NNRL),0,,False
35,"In our problem, x is the feature representation of a text t, the action set A ,"" {ap , as }. Similar to Deep Q-Network [8], we propose a""",0,,False
36,neural network-based reinforcement learning (NNRL) algorithm,0,,False
37,"where a neural network approximation function as Q-Network is used to estimate the action-value function Q(s, a,  ) ,"" Q(s, a).  is""",0,,False
38,the weight parameters in Q-Network. The Q-Network is trained by minimizing the loss function L( ) as Equation 1.,0,,False
39,"L( ) ,"" Es,a(s,a)[ - Q(s, a;  )]2""",0,,False
40,(1),0,,False
41,"in which ,"" Es [r + maxa Q(s , a ;  )] is the target and (s, a) is the behavior distribution over s and a.""",0,,False
42,Algorithm 1 outlines our proposed neural network-based rein-,0,,False
43,forcement learning (NNRL) algorithm. N is the number of iterations,0,,False
44,Algorithm 1 N,0,,False
45,N,0,,False
46,R,0,,False
47,L,0,,False
48,1: Initialize  in the action-value function Q,0,,False
49,"2: for n ,"" 1, 2, · · · , N do""",0,,False
50,"3: for i ,"" 1, 2, · · · , M do""",0,,False
51,4:,0,,False
52,"Si ,"" [t1, t2, · · · , tn, · · · ]""",0,,False
53,5:,0,,False
54,"initialize s1 ,"" {X^1 }, X^1 is appended representation for x1""",0,,False
55,6:,0,,False
56,"for j ,"" 1, 2, · · · , |S | do""",0,,False
57,7:,0,,False
58,"Compute: Q (sj, ap ;  ) and Q (sj, as ;  )",0,,False
59,8:,0,,False
60,Action: Randomly select an action for aj with  probability;,0,,False
61,"Otherwise aj ,"" maxa A(Q (sj, a ;  )). Execute aj .""",0,,False
62,9:,0,,False
63,Observe: reward rj and xj+1; generate the appended,0,,False
64,representation X^ j+1,0,,False
65,",",0,,False
66,[X^,0,,False
67,(k j,0,,False
68,"),",0,,False
69,x,0,,False
70,j,0,,False
71,+1,0,,False
72,];,0,,False
73,set,0,,False
74,state,0,,False
75,s j +1,0,,False
76,",",0,,False
77,"[sj, aj, X^ j+1].",0,,False
78,10:,0,,False
79,Update Q-Network by performing a gradient descent step,1,ad,True
80,on the loss function in Equation 1.,0,,False
81,11:,0,,False
82,end for,0,,False
83,12: end for,0,,False
84,13: end for,0,,False
85,and M is the number of text streams (text episodes). Each text stream,0,,False
86,"for one topic, e.g. S ,"" {t1, t2, · · · , tn, · · · }, has one episode training. |S | is the number of texts in each text episode S. Firstly, it initializes the start state s1 and the (high-level) representation X^1. For each text t in the episode, it executes four steps. In the rst step, compute Q function values for two actions in A (push or skip) (Line 7). They are Q(sj , ap ;  ) and Q(sj , as ;  ) respectively. At the action step (line 8): with  probability, a random action is chosen for aj ; otherwise, aj "","" maxa A(Q(sj , a ;  )). Then execute aj . In the third step (line 9), observe the reward rj 1and xj+1 ; generate the representation X^j+1 "","" [X^j(k), xj+1] by appending the text feature vector xj+1 and the last k text feature vector from previously selected texts in X^j ; set the next state sj+1 "","" [sj , aj , X^j+1]. Finally, update the Q-Network by performing a gradient descent step [1]""",1,ad,True
87,"1we use expected gain (EG) as the reward. Only when the current text is the terminal of this episode, r equal to the EG value. Otherwise r is zero.",0,,False
88,on the loss function de ned in Equation 1 (line 10). The target j is set as Equation 2:,0,,False
89,"j,",0,,False
90,"rj rj +  maxa Q(sj+1, aj ; i )",0,,False
91,terminal non - terminal,0,,False
92,(2),0,,False
93,"If it is the terminal of the episode, j equals to the reward r . Oth-",0,,False
94,"erwise, j equals to rj +  maxa Q(sj+1, aj ; i ).  is the discounted",0,,False
95,"factor. After training, the Q-network is a neural network approxi-",0,,False
96,mation function for real-time decision making.,0,,False
97,2.2 Q-Network,0,,False
98,Q value for push Q value for skip fully connected fully connected fully connected,0,,False
99,A sequence of vectors,0,,False
100,LSTM Q-Network,0,,False
101,Tweet Skip,1,Tweet,True
102,Tweet Push,1,Tweet,True
103,Tweet ... ... Skip Timeline,1,Tweet,True
104,Tweet Push,1,Tweet,True
105,Tweet Push,1,Tweet,True
106,Tweet,1,Tweet,True
107,?,0,,False
108,Now,0,,False
109,... ...,0,,False
110,Figure 1: Q-Network,0,,False
111,"In this section, we demonstrate the design of our proposed Q-",0,,False
112,Network. Figure 1 shows the architecture of our Q-Network which,0,,False
113,consists of four layers. The rst layer is a single Long Short Term,0,,False
114,Memory (LSTM) [3] layer which generates the high-level abstrac-,0,,False
115,tion of the input sequence. Both semantic and temporal dependen-,0,,False
116,cies of the text stream are considered into LSTM by taking the last k,0,,False
117,previously selected text feature representation. Therefore the input,0,,False
118,"for LSTM is the representation X^ ,"" [x1, x2, · · · , xT ]. T is the length of X^ . LSTM computes the hidden vector H "","" [h1, h2, · · · , hT ] by iterating the following equations:""",0,,False
119,"it ,  (Wxi xt + Whiht -1 + Wcict -1 + bi ) ft ,  (Wx f xt + Whf ht -1 + Wcf ct -1 + bf ) ct , ft ct -1 + it tanh(Wxc xt + Whcht -1 + bc ) ot ,  (Wxoxt + Whoht -1 + Wco + bo ) ht , ot tanh(ct )",0,,False
120,"in which  is the logistic sigmoid function, and i, f , o and c are",0,,False
121,"respectively the input gate, forget gate, output gate and cell activa-",0,,False
122,"tion vectors, all of which are the same size as the hidden vector h. W terms denote weight matrices, e.g. Wxi is the input-input gate weight matrix and Whi is the hidden-input gate matrix. The b terms denote bias vectors, e.g. bh is the hidden bias vector.",0,,False
123,LSTM is connected with three fully connected neural networks.,0,,False
124,The output of LSTM hT (the last item in the hidden vector H ) acts,0,,False
125,as the input for the later fully connected neural network layers.,0,,False
126,Each fully connected neural network can be formated as follows:,0,,False
127,"Zi , ^iWi ;",0,,False
128,"^i+1 , F (Zi )",0,,False
129,914,0,,False
130,Short Research Paper,0,,False
131,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
132,"where each output of previous (the i layer) layer ^ will be the input of the later layer (the i + 1 layer). Here, i indexes from 1 to 3. ^1 is the output of LSTM hT . W are the weight matrix of each fully connected neural network. F is the activation function and for the previous two fully connected neural networks, the activation function is ReLU and we use a linear activation function for the last fully connected neural network.",0,,False
133,3 EXPERIMENTS 3.1 Dataset,0,,False
134,"We use TREC 2016 Microblog Track 2 as data set to evaluate our proposed approach. The dataset consists of the Twitter's sample tweet streams ( approximately 1% of public tweets) during the o cial evaluation period from August 2, 2016 to August 11, 2016. 56 judged interest pro les (topics) of tweets have been given. Each interest pro le (topic) consists of title, description and narrative description. Each sample tweet has a given label for its corresponding topic: highly relevant, relevant and non-relevant. We pre-process the dataset by removing the non-English tweets and tweets which have fewer than 5 words. Also we lter out very irrelevant tweets, e.g. tweets which have no interest pro le keywords. After preprocessing, there is a total of 57,419 tweets for 56 topics. Each topic has a tweet stream. The average number of tweets per topic is about 96.40. We randomly choose 85% of topics (48 topics) of tweets as training set and the remaining as test set.",1,TREC,True
135,Table 1: Features used for basic representation,0,,False
136,Features Name,0,,False
137,Description,0,,False
138,Statistic Temporal Semantic,0,,False
139,N_{title} N_{Narr} N_{exp} N_{word} N_{hashtag} Time,0,,False
140,Time_interval,0,,False
141,Is_redundant is_link Relevance score Cosine Score Text vector,0,,False
142,"Number of terms in ""Title"" Number of terms in ""Narrative"" Number of terms in ""Description"" Number of words in tweet text Number of hashtag in tweet text the current time in one day (ms) time interval between the incoming tweet and the last selected tweet Flag about whether it is redundant Flag about whether a URL exists. SVM regression score Cosine similarity score Word embedding representation",0,,False
143,3.2 Features,0,,False
144,"Before we input the tweet stream into the Q-Network, we rstly extract some features as the basic presentation x for each tweet. The extracted features are listed in Table 1. There are three groups: respectively statistic features, temporal features and semantic features. For each tweet, we compute some statistics as features, including number of hashtag, number of terms and number of terms appearing in the title, narrative description and description. Also, two temporal features are extracted. They are the time in one day (milliseconds since 00:00 of one day) and the time interval between this tweet and the last selected tweet. As semantic features, we",0,,False
145,2 http://trecrts.github.io/,1,trec,True
146,"generate a text vector (dimension,""300) for a tweet by averaging the word embedding vectors in a tweet text. The word embedding representation is obtained from a pre-trained Google News corpus word vector model [7]. And we compute the relevance score from a SVM regression for each topic and the cosine similarity score. Also we use two boolean ags (is_link and is_redudant) about whether URL exists or it is redundant. We normalize all features, and concatenate them into one single vector as the basic presentation x for each tweet. The dimension of the basic presentation is 311.""",0,,False
147,3.3 Evaluation Metric,0,,False
148,We use two TREC 2016 Real-time Summarization Track o cial evaluation metrics. They are expected gain (EG) and Normalized Cumulative Gain (nCG). The expected gain (EG) is de ned:,1,TREC,True
149,EG,0,,False
150,",",0,,False
151,1 N,0,,False
152,G (t ),0,,False
153,where N is the number of tweets returned and G(t) is the gain of,0,,False
154,each tweet: Not relevant tweets receive a gain of 0; relevant tweets,0,,False
155,receive a gain of 0.5; Highly-relevant tweets receive a gain of 1.0.,0,,False
156,Normalized Cumulative Gain (nCG) is de ned:,0,,False
157,nCG,0,,False
158,",",0,,False
159,1 Z,0,,False
160,G (t ),0,,False
161,"where Z is the maximum possible gain (given the ten tweet per day limit). In order to di erentiate the performances in salient days when there are no relevant tweets, there are two variants for each metric, respectively EG-0, EG-1, nCG-0 and nCG-1. For salient days, the EG-1 and nCG-1 gives a score of one if not pushing any tweets, or zero otherwise. In the EG-0 and nCG-0 variants, for a silent day, the gain is zero [9].",0,,False
162,3.4 Compared Methods,0,,False
163,"We compared the following methods: · Query similarity (QS): it pushes the tweet whose relevance score is higher than a xed threshold . Cosine similarity is used to measure the relevance score between the topic title and the tweet text. · YoGosling [9]: implements simple dynamic emission strategies to maintain appropriate dynamic thresholds for pushing updates. It achieved the best performance at the TREC 2015 Microblog Track Real-time Filtering task [4]. · Features+StaticThreshold (FST) [6]:It develops a relevance estimation model based on both lexical and non-lexical features, and set a static threshold to push tweets with their manual observation. It achieved the best performance at the TREC 2016 Real-time Summarization track [6]. · NNRL: This is our proposed algorithm in Section 2.",1,Query,True
164,3.5 Implementation Setting,0,,False
165,"In our algorithm, the learning rate for gradient descent is 0.001 and the discount factor is 1. The  is annealed linearly from 1 to 0, then x it at 0 until converging to a suboptimal policy. Then  is annealed linearly from 0.1 to 0; x at 0 until converging to another suboptimal. Repeat the  exploration step thereafter. In our Q-network, the hidden state size of LSTM is 512. The size of appended sequence from the previously selected text sequence k is",1,ad,True
166,915,0,,False
167,Short Research Paper,0,,False
168,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
169,up to 10 due to LSTM training e ciency. Both the rst and second fully-connected neural network layer have 256 hidden units. The output layer is a fully-connected linear layer with a single output for each valid action.,0,,False
170,EG-0 EG-1,0,,False
171,0.05 EG-0 performance over different thresholds,0,,False
172,0.04 0.03,0,,False
173,Max: 0.0362,0,,False
174,0.02,0,,False
175,0.01,0,,False
176,0.000.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Threshold (®),0,,False
177,(a) EG-0,0,,False
178,0.30 EG-1 performance over different thresholds,0,,False
179,0.25,0,,False
180,Max: 0.2483,0,,False
181,0.20,0,,False
182,0.15,0,,False
183,0.10,0,,False
184,0.05,0,,False
185,0.000.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Threshold (®),0,,False
186,(b) EG-1,0,,False
187,Figure 2: EG results with varying QS threshold ,0,,False
188,3.6 Experimental Results,0,,False
189,Table 2: Evaluation results for all compared methods,0,,False
190,"Method QS ( , 0.45) YoGosling FST NNRL",0,,False
191,EG-1 0.2483 0.2289 0.2698 0.2816,0,,False
192,EG-0 0.0322 0.0253 0.0483 0.0691,0,,False
193,nCG-1 0.2325 0.0253 0.2909 0.2971,0,,False
194,nCG-0 0.0164 0.0253 0.0695 0.0846,0,,False
195,"Table 2 shows the evaluation results. We can see that our proposed NNRL outperforms the other competitors for all evaluation metrics. Our superior performances lie on that we format this problem as a sequential decision making process and deal with both history dependencies and future uncertainty while the other three compared methods mainly adopt the static or dynamic threshold to lter out the incoming tweet. Figure 2 shows the EG evaluation results (similar results with nCG; we omit it due to space limitation) with varying the threshold  of the QS method. There are big di erences in EG performance when using di erent thresholds. Therefore, static methods like QS and FST are inappropriate for the real-time environment which requires a dynamic and adaptive mechanism to consider future uncertainty. Although YoGosling proposed some strategies for obtaining dynamic threshold, it ignores future uncertainty. We use an exemplary case for topic ""Hiroshima atomic bomb"" to demonstrate the adaptive ability of our method (NNLR) to address the potential future uncertainty. Table 3 shows a snippet of the tweet sequence of 7 tweets for topic ""Hiroshima atomic bomb"" with each tweet's time, raw text and the selected action in our method. At the start of the tweet sequence, our method pushes the second tweet rather than the rst one because the second one is more speci c and relevant to the topic. Our method decides to skip the rst tweet and waits for the potential better tweet (e.g. the second one) in the future. After pushing the second one, it skips the following relevant but redundant tweets. When it has pushed some highly-relevant tweets and time elapses, the pushing condition might change as very few relevant tweets come. Therefore, it",1,ad,True
196,"pushes the relevant one while the same text was skipped previously. For example, the rst tweet was skipped but the fourth tweet with the same content is pushed.",0,,False
197,"Table 3: An exemplary case for ""Hiroshima atomic bomb""",0,,False
198,Time Tweet Text,1,Tweet,True
199,Action,0,,False
200,08-02 04:21:18 08-09 17:10:15 08-09 17:15:11 08-09 21:59:15,0,,False
201,08-10 00:36:05,0,,False
202,08-10 02:14:43 08-11 03:41:12,0,,False
203,"Obama At Hiroshima: A World Without Nuclear Weapons ­ Ours - American Thinker RT @HistoryToLearn: Hiroshima, one year after the atomic bomb blast, 1946. RT @HistoryToLearn: Hiroshima, one year after the atomic bomb blast, 1946. Obama At #Hiroshima: A World Without Nuclear Weapons ­ Ours - American Thinker I dropped the bomb but now I have the proof so now I can drop the atomic bomb. I've never been so excited RT @AJENews: Nagasaki marks 71st anniversary of atomic bombing Nagasaki Marks 71st Anniversary of Atomic Bombing.",0,,False
204,Skip Push Skip Push Skip Skip Push,0,,False
205,4 CONCLUSIONS,0,,False
206,"In this paper, we propose a neural network based reinforcement",0,,False
207,learning algorithm to address real-time pushing on text stream. A,1,ad,True
208,novel Q-Network is designed to approximate the maximum long-,0,,False
209,term rewards. Experiment results on real data from TREC 2016,1,TREC,True
210,Real-time Summarization track demonstrate that our algorithm,0,,False
211,is superior to all compared methods for all the o cial evaluation,0,,False
212,metrics and has the ability to make real-time decisions. In future,0,,False
213,"work, we plan to study the case without speci c query topic.",0,,False
214,5 ACKNOWLEDGMENTS,0,,False
215,The work in this paper was supported by Research Grants Coun-,0,,False
216,"cil of Hong Kong (PolyU 152094/14E), National Natural Science",0,,False
217,Foundation of China (61272291) and The Hong Kong Polytechnic,0,,False
218,University (4-BCB5).,0,,False
219,REFERENCES,0,,False
220,[1] Leemon Baird and Andrew Moore. Gradient Descent for General Reinforcement Learning. In NIPS'98.,1,ad,True
221,"[2] Feifan Fan, Yansong Feng, Lili Yao, and Dongyan Zhao. Adaptive Evolutionary Filtering in Real-Time Twitter Stream. In CIKM '16.",1,Twitter,True
222,"[3] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. 9, 8 (1997), 1735­1780.",0,,False
223,"[4] Y. Wang G. Sherman J. Lin, M. Efron and E. Voorhees. Overview of the TREC-2015 Microblog Track. In TREC 2015.",1,TREC,True
224,"[5] Chris Kedzie, Fernando Diaz, and Kathleen McKeown. Real-Time Web Scale Event Summarization Using Sequential Decision Making. In IJCAI '16.",0,,False
225,"[6] Jimmy Lin, Adam Roegiest, Luchen Tan, Richard McCreadie, Ellen Voorhees, and Fernando Diaz. Overview of the TREC-2016 Microblog Track. In TREC 2016.",1,ad,True
226,"[7] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Je rey Dean. Distributed Representations of Words and Phrases and Their Compositionality. In NIPS'13.",1,ad,True
227,"[8] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. Playing Atari with Deep Reinforcement Learning. CoRR (2013).",0,,False
228,"[9] Luchen Tan, Adam Roegiest, Charles L.A. Clarke, and Jimmy Lin. Simple Dynamic Emission Strategies for Microblog Filtering. In SIGIR '16.",1,blog,True
229,916,0,,False
230,,0,,False

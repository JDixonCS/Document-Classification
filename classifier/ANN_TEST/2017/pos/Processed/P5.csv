,sentence,label,data,regex
0,Session 1A: Evaluation 1,1,Session,True
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Navigating Imprecision in Relevance Assessments on the Road to Total Recall: Roger and Me,1,ad,True
3,Gordon V. Cormack,0,,False
4,University of Waterloo gvcormac@uwaterloo.ca,0,,False
5,ABSTRACT,0,,False
6,"Technology-assisted review (""TAR"") systems seek to achieve ""total recall""; that is, to approach, as nearly as possible, the ideal of 100% recall and 100% precision, while minimizing human review effort. The literature reports that TAR methods using relevance feedback can achieve considerably greater than the 65% recall and 65% precision reported by Voorhees as the ""practical upper bound on retrieval performance . . . since that is the level at which humans agree with one another"" (Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness, 2000). This work argues that in order to build--as well as to, evaluate--TAR systems that approach 100% recall and 100% precision, it is necessary to model human assessment, not as absolute ground truth, but as an indirect indicator of the amorphous property known as ""relevance."" The choice of model impacts both the evaluation of system effectiveness, as well as the simulation of relevance feedback. Models are presented that better fit available data than the infallible ground-truth model. These models suggest ways to improve TAR-system effectiveness so that hybrid human-computer systems can improve on both the accuracy and efficiency of human review alone. This hypothesis is tested by simulating TAR using two datasets: the TREC 4 AdHoc collection, and a dataset consisting of 401,960 email messages that were manually reviewed and classified by a single individual, Roger, in his official capacity as Senior State Records Archivist. The results using the TREC 4 data show that TAR achieves higher recall and higher precision than the assessments by either of two independent NIST assessors, and blind adjudication of the email dataset, conducted by Roger, more than two years after his original review, shows that he could have achieved the same recall and better precision, while reviewing substantially fewer than 401,960 emails, had he employed TAR in place of exhaustive manual review.",1,TREC,True
7,1 INTRODUCTION,1,DUC,True
8,"This study contributes to the body of empirical evidence showing that hybrid human-computer classification systems (known in the legal community as ""technology-assisted review""",0,,False
9,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5022-8/17/08. https://doi.org/10.1145/3077136.3080812",1,ad,True
10,Maura R. Grossman,0,,False
11,University of Waterloo maura.grossman@uwaterloo.ca,0,,False
12,"or ""TAR"") can be more effective and more efficient than exhaustive manual review by experts, where effectiveness is measured with respect to an independent gold standard. The results amplify and extend our 2011 work, TechnologyAssisted Review in E-Discovery Can Be More Effective and More Efficient Than Exhaustive Manual Review [14] in the following ways:",0,,False
13," We rigorously specify and evaluate a semi-automated process for human-in-the-loop classification in which the only human input is an initial query, followed by assessment of the documents selected for review by the system, until the system determines that high recall and precision have been achieved, and that the review process is complete;",0,,False
14," We extend the process with a quality-control (""QC"") mechanism, in which the system suggests a subset of the documents for further adjudication, either by the user or another assessor, to mitigate the fallibility of the user's original assessments;",1,ad,True
15," We present a theory of information retrieval (""IR"") system evaluation that extends the Cranfield method [30] to define the end-to-end effectiveness of an interactive IR process, and to model and control for dependencies between the assessments rendered by the human in the loop, and the assessments used to evaluate the result;",0,,False
16," Using the TREC 4 AdHoc collection and the alternate assessments used by Voorhees in Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness [29], we provide evidence that our proposed TAR method achieves substantially better recall and precision than the the alternate NIST assessors would have achieved, had they reviewed the entire collection, with a small fraction of the effort;",1,TREC,True
17," And finally, using a complete categorization of 401,960 email messages from the administration of Virginia Governor Tim Kaine, which was previously manually reviewed by Senior State Records Archivist Roger Christman (""Roger""), we show, using subsequent assessments rendered by Roger, that Roger could have achieved the same recall and higher precision, for a fraction of the effort, had he employed our TAR method to review the 401,960 email messages.",1,ad,True
18,"The following sections develop the theory of how to measure the end-to-end effectiveness of high-recall and high-precision IR efforts, how to simulate a human in the loop, our experimental design, and our results on the TREC 4 and Kaine email datasets.",1,TREC,True
19,5,0,,False
20,Session 1A: Evaluation 1,1,Session,True
21,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
22,2 THEORY,0,,False
23,"It is well understood that the notion of ""relevance"" is imprecise, and that different assessors--or even the same assessor at different times--may provide inconsistent relevance determinations for the same document, regardless of their knowledge and expertise, or the specificity with which ""relevance"" is defined. Nevertheless, it has been observed that relevance determinations by different assessors, while different, are essentially interchangeable as ground truth for the purposes of measuring the relative effectiveness of ad-hoc retrieval systems [3, 29]. In this work, we consider the problem of measuring the end-to-end effectiveness of ""total-recall"" methods, where the goal is to find substantially all relevant documents, and where the overall accuracy in determining relevance rivals that of the user, or any individual assessor. The model for a user is a ""dedicated searcher, not a novice searcher,"" who is ""willing to look at many documents"" in order to find as much relevant information as possible (from TREC-1 [16]). This objective is shared by many critical applications, including electronic discovery in civil litigation, archiving of business or government records, patent search, and systematic review in evidence-based medicine. In 2015 and 2016, the TREC Total Recall Track [10, 15] addressed the total-recall problem, providing to participants a ""Baseline Model Implementation"" (""BMI""),1 simulating a TAR method known as ""continuous active learning"" (""CAL"") (cf. [9, 11]).",1,ad-hoc,True
24,"The ideal result of a ""total-recall"" IR effort is to identify all and only the relevant documents in a collection; that is, to achieve 100% recall and 100% precision. In practice, the ability to reach this goal is limited by the fallibility of human relevance assessment. Even if it were feasible to assess every document in the collection, a certain number of the resulting assessments would be incorrect, yielding less than 100% recall and less than 100% precision. Relevance assessments generated by a learned classifier would also be fallible, likewise falling short of 100% recall and precision. This article addresses the question: Can hybrid human-computer assessments yield higher recall and precision--with less effort--than human assessments alone?",1,ad,True
25,"To answer this question, it is necessary to estimate recall and precision, or another measure of how nearly all and only the relevant documents have been identified. The traditional Cranfield method for IR evaluation [30] offers limited insight because it relies on comparison with a ""gold standard"" for relevance, which itself relies on fallible assessments. At high levels of recall and precision, the Cranfield method tends to measure the ability of the method under test to reproduce the flaws in the gold standard, which--if the flaws are random--is impossible, and if the flaws are systematic--is possible only for methods with similar flaws.",1,ad,True
26,"Measuring the effectiveness of total recall is further complicated by the fact that most high-recall methods involve a human in the loop, and are influenced by that user's fallible assessments. In the simplest ""ranked-retrieval"" scenario, the system orders all documents in the collection by their",0,,False
27,1 http://cormack.uwaterloo.ca/trecvm/.,1,trec,True
28,"likelihood of relevance, and the user examines them in order, until a sufficient number of relevant documents have been identified. In the ""relevance-feedback"" scenario, the user's assessment is communicated to the system, which uses this information to revise the ranking of the yet-to-be-examined documents. The ""active-learning"" or ""uncertainty-sampling"" scenario departs from relevance-feedback scenario in that the documents presented to the user are in the order most useful for machine learning, as opposed to likelihood of relevance, with the effect that the user is typically directed to the most marginally relevant documents to examine.",0,,False
29,"Regardless of the scenario, it is important to define precisely the circumstance under which a document is considered to be ""identified"" by the method. In the ""system-recall "" scenario, a document is deemed to be identified when it is presented to the user, regardless of the user's ultimate relevance assessment. In the ""end-to-end-recall "" scenario, a document is deemed to be identified only when it is presented to the user and the user judges it to be relevant. Where the user is fallible, system recall will generally be higher than end-to-end recall, while system precision will be lower. Which scenario is more apt depends on whether the role of the user is simply to provide guidance to the system, or to make the ultimate determination of whether a document is relevant or not.",0,,False
30,"Quality-control (""QC"") procedures seek to mitigate the impact of fallible relevance assessments, using one or more supplemental assessments for some or all of the documents. In perhaps the easiest case, a second assessment might be rendered for each document. Where the assessments agree, it would be reasonable to assume that they are likely (but not certainly) both correct; where the assessments disagree, one is likely correct and the other is likely not--but how do we know which is which? One might defer to the second assessment, if it could be ascertained that it was more likely to be correct than the first, perhaps due to the application of additional care, or greater knowledge and skill on the part of the second assessor. One might defer to the ""relevant"" assessment if high recall were particularly important, and to the ""not relevant"" assessment if high precision were important. Alternatively, one might defer to a third assessment, effectively deeming the ""majority vote"" to be correct.",1,ad,True
31,"Majority-vote QC incurs overhead of (1 + ) additional assessments, where  is the number of documents in the collection, and  is the rate of discord between the first and second assessments. This overhead may be reduced by selecting a subset of    documents for supplemental assessment. If the subset is a statistical sample, it is possible to quantify, but not to substantially mitigate, the fallibility of the first assessment. If a subset can be identified that includes many of the documents with discordant assessments, deferring to a third assessment for those particular documents can provide mitigation approaching that of majority vote, with considerably lower overhead.",1,ad,True
32,"This work distinguishes between total-recall methods and search tools. At the outset, TREC sought to measure the ease",1,TREC,True
33,6,0,,False
34,Session 1A: Evaluation 1,1,Session,True
35,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
36,with which search tools might be used within the context of a total-recall effort [16]:,0,,False
37,"It should be assumed that the users need the ability to do both high precision and high recall searches, and are willing to look at many documents and repeatedly modify queries in order to get high recall. Obviously they would like a system that makes this as easy as possible.",0,,False
38,"To this end, relevance-based measures of search-tool effectiveness--notably, rank-based measures such as (mean) average precision (""(M)AP""), precision at a fixed cutoff (""P@k""), and R-precision (""P@R,"" where R is the number of relevant documents in the collection)--were used as proxies for ease of use [18]. With certain exceptions, primarily in the legal, intellectual property, and medical domains, interest within TREC and the IR community has generally shifted to more user-centric contexts, where the goal is to satisfy an ephemeral and user-specific information need, and search-tool effectiveness is quantified by proxy measures for user satisfaction, cf. [1].",1,AP,True
39,"Regardless of the context or proxy measure, evaluation efforts like those characterized by Voorhees [29, 30] have focused on search-tool (i.e., ""system"") effectiveness, not the overall effectiveness of the user at using the tool to identify as nearly as practicable all and only the relevant documents, where ""relevance"" is defined by extrinsic criteria (i.e., ""endto-end"" effectiveness). The method of repeated ad-hoc search envisioned by TREC is commonly used, but is far from the only--or necessarily the most effective--total-recall method. In some domains, such as the curation of government archival records, exhaustive manual review by an expert constitutes the de facto standard of acceptable practice. In many contexts, a single query is used to identify the subset of the collection for manual review. In Boolean retrieval, the query specifies precisely the subset to be reviewed; in ranked retrieval, the query suggests the nature of relevance, the search tool ranks the documents by their likelihood of relevance, and the user assesses some number of the top-ranked documents. Traditionally, relevance feedback has been construed as a method to automate the query-formulation task envisioned by TREC: The user's assessment of the results from an initial query are provided to the search tool, which reformulates the query and presents a new set of results to the user, and so on. More recently, supervised machine-learning methods have been used to harness relevance feedback, with reported effectiveness apparently exceeding Voorhees' ""practical upper bound,"" see e.g., [6, 14, 25].",1,ad-hoc,True
40,3 MODELING ASSESSMENT ERROR,0,,False
41,3.1 Assessment Error in Measurement,0,,False
42,"For the purposes of this study, we assume that every document  is either ""relevant"" or ""not relevant"" in its own right (()  {, } ), but its relevance can be observed only indirectly by an assessment under conditions  yielding a positive or negative judgment ((, )  {+, -}). We use the abbreviations , , +, and - to denote () ,"" ,""",0,,False
43,"() ,"" , (, ) "","" +, and (, ) "","" -, respectively. We""",0,,False
44,"assume that for a random document , a positive judg-",0,,False
45,ment is evidence of relevance: Pr[|+] > Pr[]. It,0,,False
46,follows that a negative judgment is evidence of non-relevance:,0,,False
47,Pr[|-] > Pr[]. One of the principal questions,0,,False
48,to be addressed by a model is: How strong is this evidence?,1,ad,True
49,The Cranfield method generally assumes for the pur-,0,,False
50,pose of evaluation that human assessments are infallible,0,,False
51,"(Pr[|+] ,"" 1, Pr[|-]) "","" 0), where  is chosen""",0,,False
52,"carefully, considering the myriad of factors that influence",1,ad,True
53,relevance assessment.,0,,False
54,Biased sampling and/or statistical sampling may be used,0,,False
55,to to reduce the cost of assessment. The pooling method [18],0,,False
56,is the most prominent of a family of biased sampling methods,0,,False
57,"that identify a subset of documents for human assessment,",0,,False
58,and render automatic judgments for the remaining documents.,0,,False
59,"In the pooling method, each document  is either in the pool",0,,False
60,"or not (()  {, }); documents in the pool",0,,False
61,"are assessed, while documents not in the pool are summarily",0,,False
62,deemed not relevant. The pooling method can be viewed,0,,False
63,as a semi-automated assessment under conditions  where,0,,False
64,{,0,,False
65,"(, ) ,"" (, ) () . Biased sampling methods""",0,,False
66,-,0,,False
67,(),0,,False
68,"place further stress on the Cranfield assumption that (, )",0,,False
69,is infallible.,0,,False
70,Statistical sampling may be used to estimate the pro-,0,,False
71,portion of relevant documents in particular subsets of the,0,,False
72,"collection, as necessary to compute summary measures of",0,,False
73,effectiveness [2].,0,,False
74,Multiple assessments per document may be used in place,0,,False
75,of a single assessment. The majority judgment of a  as-,0,,False
76,sessments under conditions 1 ...  will more closely ap-,0,,False
77,"proximate an infallible assessor, under the assumption that",0,,False
78,there is greater than 50% conditional probability that each,0,,False
79,"judgment will be positive for a relevant document, and nega-",0,,False
80,"tive for a non-relevant document, notwithstanding the other",0,,False
81,judgments.,0,,False
82,"Where multiple fallible assessments are available, latent",0,,False
83,class analysis [21] may be used to infer the true positive rate,0,,False
84,"Pr[+|] and false positive rate Pr[+|] for each of the assessment conditions, as well as prevalence Pr[],",0,,False
85,under the assumption of pairwise conditional independence:,0,,False
86,"Pr[+|()] ,"" Pr[+|(), ( , )] for all  "",  .",0,,False
87,3.2 Assessment Error in Simulation,0,,False
88,"Total-recall methods may require relevance assessment for three purposes: (i) To train the system to rank or classify the remaining documents; (ii) to determine the ultimate disposition of each document presented to the user by the system (e.g., produce or withhold in the context of electronic discovery in civil litigation, include or exclude in the context of systematic review in evidence-based medicine); and (iii) to inform the cost-benefit analysis inherent in determining when to stop the review process. For the purposes of this work, we aspire to emulate a user whose fallible assessments are conditionally independent of those used for evaluation. The",0,,False
89,7,0,,False
90,Session 1A: Evaluation 1,1,Session,True
91,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
92,"fallible assessments used for evaluation, although they may closely emulate those of a user, would confound evaluation were they also to be used to simulate feedback, as they are not conditionally independent. Infallible assessments-- if they existed--would be conditionally independent, but a poor emulation of a real user's feedback. In either case, it is desirable to use a separate set of assessments to emulate user feedback. Even so, it is well known that the order of presentation and the proportion of relevant documents can influence human assessment [24, 27, 28]; such influences are not easily controlled when simulating different total-recall methods. The quest for better models to emulate human assessment is met with a triple challenge: (i) determining the true relevance of a document; (ii) aptly modeling the user's response; and (iii) ensuring the model is conditionally independent of the model used for evaluation.",0,,False
93,3.3 When to Stop?,0,,False
94,"An important but rarely studied issue in achieving total recall is when to stop. Blair and Maron [4] reported that users who terminated their searches when they believed they had achieved at least 75% recall, had in fact achieved 20% recall. Eliciting from the user a reliable judgment of when high recall has been achieved remains a vexing problem. Evaluations styled after the TREC AdHoc task have largely finessed this issue, reporting rank-based measures under the assumption that the user would know when to stop, perhaps after reading a fixed number of documents.",1,ad,True
95,"Automated methods show some promise, but have not previously been evaluated in terms of end-to-end recall with a human in the loop, where user feedback is independent of the assessments used for evaluation. Cormack and Grossman [6] have reported statistical and non-statistical methods for ensuring, with high probability, that their continuous active learning (""CAL"") method achieves very high recall, at the expense of precision. The non-statistical ""knee method"" searches for an inflection point in the recall-effort curve--the ""knee""--and continues well beyond that point. Empirical evidence, based on an assumption of infallible user feedback, suggests that their knee method can achieve system-level recall of over 90%, with more than 95% probability, with precision considerably less than 50%.",0,,False
96,4 EXPERIMENTAL DESIGN,0,,False
97,"We conducted two experiments to test the hypothesis that total-recall systems with human assessors in the loop could achieve comparable--or higher--recall and precision, with a small fraction of the effort, than an expert assessor who examined every document in the collection. For this effect to be observable, it is necessary to depart from a model assuming infallible assessment, at least with respect to human assessors in the loop. For evaluation, it is necessary to have a source of reasonably authoritative assessments separate from those used for relevance feedback.",0,,False
98,"4.1 Datasets, Topics, and Assessments",0,,False
99,"Our first experiment simulated participation in the TREC 4 AdHoc Task, using the TREC 4 test collection consisting of 567,528 documents, 49 topics, and the official NIST gold standard of relevance [17]. For user feedback and QC, we used two alternate sets of judgments obtained by NIST, for the same topics, using assessors distinct from those who created the gold standard--the same set of alternate assessments studied by Voorhees [29].",1,TREC,True
100,"Our second experiment reprised the exhaustive manual review of 401,960 email messages from the administration of former Virginia Governor Tim Kaine, which was undertaken by Senior State Archivist Roger Christman, prior to the publication, in 2014, of those he deemed to be ""open records.""2 To simulate user feedback in our experiment, we used Roger's original assessments (the ""Roger I"" assessments). To simulate QC, Roger re-reviewed blind a stratified sample of 2,798 documents (the ""Roger II"" assessments). As the ultimate arbiter of truth, Roger reviewed blind, for a third time, all 901 cases of disagreement between Roger I and Roger II (the ""Roger III"" assessments).",1,ad,True
101,4.2 Total-Recall Methods,0,,False
102,"Our simulation used the same TREC Total Recall Baseline Model Implementation (""BMI""), referenced above in Section 2, modified to read the dataset, topics, and simulated relevance assessments from local files, instead of a server, and to implement Cormack and Grossman's ""knee-method"" stopping criterion [6]. BMI runs autonomously and has no tunable parameters: Our input consisted of the datasets, topics, relevance assessments, and the knee method. Output from the BMI runs consisted of: a ranked list of documents, in the order presented to the simulated user, ending where the knee-method stopping criterion was met; and the inflection point (""knee"") in the gain curve, determined retrospectively by the knee method.",1,TREC,True
103,"The output from BMI was further manipulated to simulate three different result-selection strategies: (i) SystemDetermined, (ii) User-Determined, and (iii) Adjudicated. The end result of the System-Determined strategy was the entire ranked list returned by BMI. The end result of the UserDetermined strategy was the subset of documents in the ranked list that the user judged to be relevant. The end result of the Adjudicated strategy was the subset of the ranked list consisting of those documents that the user and the knee method agreed were relevant, or, where the user and knee method disagreed, a second, auxiliary assessment deemed to be relevant. For the purposes of this work, we deemed the knee method to judge all documents in the ranked list before the knee to be relevant, and all documents after the knee to be non-relevant.",0,,False
104,2See http://www.virginiamemory.com/collections/kaine/.,0,,False
105,8,0,,False
106,Session 1A: Evaluation 1,1,Session,True
107,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
108,Strategy Manual System,0,,False
109,User Adjudicated Adj. - Man.,0,,False
110,-value,0,,False
111,Recall 0.57 0.94 0.55 0.64 0.07 0.0001,0,,False
112,Precision 0.69 0.06 0.81 0.82 0.13,0,,False
113,0.0002,0,,False
114,1 0.63 0.10 0.62 0.69 0.06 0.0001,0,,False
115,"Effort 567,528 22,911 22,911",0,,False
116,"23,662 543,866",0,,False
117,-,0,,False
118,Table 1: Average Effectiveness Measures Over 98 Combinations of 49 TREC 4 Topics and Two Simulated Users.  was computed using a paired t-test.,1,TREC,True
119,4.3 Evaluation,0,,False
120,"For all strategies, we report recall, precision, and 1 , as well as effort, as measured by the number of documents presented to the user for assessment. As a baseline, we used an Exhaustive Manual Review (""Manual Review"") strategy, for which effort is simply the number of documents in the collection. For the System-Determined strategy, effort depends on the number of relevant documents in the collection, as well as the recall and precision achieved: For a given topic and recall level, effort is inversely proportional to precision. For the User-Determined and Adjudicated strategies, there is no direct relationship between effort and precision.",0,,False
121,"Only the System-Determined strategy returns a ranked list from which we can evaluate the recall-precision tradeoff. We can, however, plot recall and precision as a function of effort throughout the progress of a review. These curves illustrate the result that might have been achieved, had a different stopping criterion been applied. They also illustrate how quickly a substantial fraction of the relevant documents can be discovered and forwarded for further analysis or release, while the total-recall effort is still in progress.",1,ad,True
122,4.4 Prediction and Rationale,0,,False
123,"Previously published results report recall-precision breakeven scores on the order of 80% for BMI and related methods [7, 8, 11, 25]. With one notable exception (discussed below), these results were derived using simulated feedback from an assumed-infallible user, and evaluated with respect to the same assumed-infallible gold standard. On the one hand, the simulated feedback was conditionally dependent on the evaluation standard, and therefore possibly ""too good to be true."" On the other hand, the evaluation standard was assumed to be perfect, offering BMI no opportunity to better it. The experiments, by design, could not show whether or not BMI could achieve better recall and/or better precision than a fallible user.",0,,False
124,"There is no basis to assume that a hybrid human-computer system cannot exceed both the recall and precision of its human operator. The literature reports inter-assessor agreement results that are consistent with the hypothesis that a human assessor can achieve on the order of 70% recall and 70% precision [26, 29]. Are the higher results reported for BMI an artifact of too-perfect training, or is a system involving",0,,False
125,"BMI and human assessment, combined, superior to human assessment alone?",0,,False
126,"Achieving a high recall-precision break-even score is irrelevant to the success of a total-recall effort, if the point at which this score is achieved is unknown to the user. Cormack and Grossman's knee-method stopping criterion sacrifices (System-Determined) precision to achieve very high recall, under the assumption that an infallible assessor would screen the results, and the only consequence of low precision would be increased effort. The User-Determined strategy has the (fallible) user act in this capacity.",0,,False
127,"The Adjudicated strategy has BMI and the user share the role of screening, deferring to a second (fallible) assessor the adjudication of cases of disagreement. We assume the inflection point calculated by the knee method to be a good approximation of the recall-precision break-even point; that documents before the knee are more likely to be relevant than not, and that documents after the knee are less likely to be so. This assumption motivates our choice to defer to a second assessor any document before the knee that is judged non-relevant by the user, and any document after the knee that is judged relevant by the user. This strategy makes no assumption that the second assessor is ""better"" than the user. As long as the second assessor is usually correct (as would certainly be the case for an assessor capable of achieving 70% recall and 70% precision), the Adjudicated strategy should achieve higher recall and higher precision than the User-Determined strategy.",1,ad,True
128,"As noted above, one strand of research has evaluated totalrecall methods in the face of fallible users. The TREC Legal Track Interactive Task (see, e.g., [19, 23]) assigned participating teams the task of finding all and only the relevant documents that were responsive to requests for production in a mock civil litigation. A subject matter expert (the ""Topic Authority"") was made available for consultation while the teams were conducting their reviews; the same Topic Authority adjudicated cases of disagreement, after the fact, between the teams and the human assessors who had created a provisional gold standard for evaluation. Teams were allowed to use any method of their choosing, with no restriction on the nature or quantity of human input. Two TAR methods--one rule-based and one substantially similar to BMI--achieved on the order of 80% recall and 80% precision [19]. In a subsequent analysis, Grossman and Cormack [14] estimated the recall and precision of the human assessments that comprised the provisional gold standard, on average, to have been 59.3% and 31.7%, respectively. While deferring a fuller discussion of these results to Section 5, we note that this work generated some criticism, e.g., [12, 13, 31]; most notably, claims that: (i) the assessors were unskilled, poorly trained, poorly vetted, or poorly supervised; (ii) the assessors had a different ""conception of relevance"" from the Topic Authority; (iii) the participating teams devoted extraordinary skill or extraordinary resources to accomplishing the task; (iv) the gold standard, by virtue of the reconsideration process, was biased in favor of the participants; and (v) the gold standard, by",1,TREC,True
129,9,0,,False
130,Session 1A: Evaluation 1,1,Session,True
131,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
132,System-Determined vs. User-Determined Recall and Precision 1,0,,False
133,System-Determined vs. User-Determined Recall and Precision 1,0,,False
134,0.8,0,,False
135,0.8,0,,False
136,Recall (Precision),0,,False
137,Recall (Precision),0,,False
138,0.6,0,,False
139,0.6,0,,False
140,System recall System prec.,0,,False
141,User recall User prec.,0,,False
142,System recall System prec.,0,,False
143,User recall User prec.,0,,False
144,0.4,0,,False
145,0.4,0,,False
146,0.2,0,,False
147,0.2,0,,False
148,0,0,,False
149,0,0,,False
150,0,0,,False
151,1000 2000 3000 4000 5000 6000 7000 8000 9000 10000,0,,False
152,0,0,,False
153,Review Effort (Documents Reviewed),0,,False
154,User-Determined vs. Adjudicated Recall and Precision,0,,False
155,1,0,,False
156,1,0,,False
157,1000,0,,False
158,2000,0,,False
159,3000,0,,False
160,4000,0,,False
161,5000,0,,False
162,Review Effort (Documents Reviewed),0,,False
163,6000,0,,False
164,User-Determined vs. Adjudicated Recall and Precision,0,,False
165,7000,0,,False
166,8000,0,,False
167,0.8,0,,False
168,0.8,0,,False
169,Recall (Precision),0,,False
170,Recall (Precision),0,,False
171,0.6,0,,False
172,0.6,0,,False
173,User recall User prec. Adjudicated recall Adjudicated prec.,0,,False
174,User recall User prec. Adjudicated recall Adjudicated prec.,0,,False
175,0.4,0,,False
176,0.4,0,,False
177,0.2,0,,False
178,0.2,0,,False
179,0,0,,False
180,0,0,,False
181,0,0,,False
182,2000,0,,False
183,4000,0,,False
184,6000,0,,False
185,8000,0,,False
186,10000,0,,False
187,12000,0,,False
188,0,0,,False
189,1000,0,,False
190,2000,0,,False
191,3000,0,,False
192,4000,0,,False
193,5000,0,,False
194,6000,0,,False
195,7000,0,,False
196,8000,0,,False
197,9000,0,,False
198,Review Effort (Documents Reviewed),0,,False
199,Review Effort (Documents Reviewed),0,,False
200,"Figure 1: TREC 4 Topic 239 ­ Tradeoff Between Recall, Precision, and Effort. The top panels compare the System-Determined vs. User-Determined strategies; the bottom panels compare the User-Determined vs. Adjudicated strategies. For the left panels, the first alternate TREC assessor was the user, and the second alternate assessor was the adjudicator; for the right panels, the roles were reversed.",1,TREC,True
201,"virtue of bias on the part of the Topic Authority and the lack of blinding of his or her review, was biased in favor of the participants.",0,,False
202,"This study controls for the skill, effort, and motivation of both users and assessors. The gold standard for the TREC 4 collection, as well as the alternate assessments, were fixed more than two decades ago. All of the NIST assessors were clearly skilled in their craft; most were former NSA analysts. The alternate assessors had no direct knowledge of the primary assessor's judgments. On the other hand, the simulated assessments for the Kaine email dataset were derived from an assessment of 401,960 documents, by the Virginia Senior State Records Archivist, in his official capacity. In forming the gold standard, the same archivist reviewed and then re-reviewed some of his own previous assessments, after wash-out periods of two years and then two months, respectively.",1,TREC,True
203,Our rationale predicted that: (i) BMI alone (the SystemDetermined strategy) would achieve superior recall to Manual,0,,False
204,"Review, but inferior precision, for substantially less effort; (ii) the User-Determined strategy would achieve inferior recall, but superior precision, to the System-Determined strategy, for the same effort; and, (iii) the Adjudicated strategy would achieve superior recall and precision to all other strategies, for moderately higher effort than the System-Determined and User-Determined strategies, but still substantially less than Manual Review.",0,,False
205,5 RESULTS,0,,False
206,"Figure 1 plots recall and precision for a representative TREC 4 topic as a function of effort for each of the BMI-derived methods, using each of the alternate assessors as the user, and the other assessor, as occasioned, for adjudication.3 In comparison, the recall and precision of the Manual Review by",1,TREC,True
207,3Plots and raw results for the other 48 topics are available on request from the authors.,0,,False
208,10,0,,False
209,Session 1A: Evaluation 1,1,Session,True
210,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
211,Topic Legal Hold,0,,False
212,Archival Restricted,0,,False
213,Manual Review,0,,False
214,Recall Precision 1 0.97 0.91 0.94,0,,False
215,0.89,0,,False
216,0.84 0.86,0,,False
217,0.98 0.75 0.84,0,,False
218,"Effort 401,960 381,819 146,594",0,,False
219,Recall 0.96 0.90 0.95,0,,False
220,Adjudicated Precision 1,0,,False
221,0.96 0.96 0.89 0.89 0.80 0.87,0,,False
222,"Effort 40,522 332,410 38,048",0,,False
223,Table 3: Individual Topic Effectiveness for the Kaine Email Dataset.,0,,False
224,Roger I,0,,False
225,Roger II,0,,False
226,rel,0,,False
227,nrel,0,,False
228,Roger I,0,,False
229,rel,0,,False
230,"16,640",0,,False
231,"PP 1,736 1,76P5 PP",0,,False
232,nrel,0,,False
233,PP 474 227 PPP,0,,False
234,P,0,,False
235,"381,065",0,,False
236,P,0,,False
237,Roger II,0,,False
238,rel,0,,False
239,nrel,0,,False
240,rel nrel,0,,False
241,"33,482115,57717,269 7,193 198,40923,824",0,,False
242,Roger II,0,,False
243,rel,0,,False
244,nrel,0,,False
245,Roger I,0,,False
246,rel,0,,False
247,"23,050",0,,False
248,"PP 3,661 296 PPP",0,,False
249,nrel,0,,False
250,"PP 1,135 7,53P6 PP",0,,False
251,P,0,,False
252,"130,821",0,,False
253,P,0,,False
254,"Table 2: Agreement Among Roger I, Roger II, and Roger III on the Virginia Tech Legal Hold Emails. In split cells, numbers below the diagonal show agreement between Roger I and Roger III; numbers above the diagonal show agreement between Roger II and Roger III. The top panel shows Virginia Tech legal hold identification; the middle panel shows archival record identification; the bottom panel shows restricted record identification.",0,,False
255,Strategy Manual Adjud.,0,,False
256, -value 95% c.i.,0,,False
257,"Recall 0.95 0.93 -0.01 0.4 (-.05, .03)",0,,False
258,Precision 0.83 0.88 +0.05 0.006,0,,False
259,"(.03, .07)",0,,False
260,"1 0.88 0.91 +0.02 0.03 (.004, .04)",0,,False
261,"Effort 310,196 136,993 173,203",0,,False
262,Table 4: Average Effectiveness Measures Over Three Topics for the Kaine Email Dataset.  was computed using a paired t-test.,0,,False
263,Roger I & Roger II System & Roger I System & Roger II,0,,False
264,Legal Hold 80.6% 79.1% 79.9%,0,,False
265,Archival 60.2% 70.2% 62.1%,0,,False
266,Restricted 64.2% 67.9% 55.8%,0,,False
267,"Table 5: Pairwise Overlap (i.e., Jaccard Index) Between the System, Roger I, and Roger II.",0,,False
268,System-Determined vs. User-Determined Recall and Precision 1,0,,False
269,0.8,0,,False
270,Recall (Precision),0,,False
271,0.6,0,,False
272,0.4,0,,False
273,0.2,0,,False
274,0 0,0,,False
275,1,0,,False
276,5000,0,,False
277,10000,0,,False
278,15000,0,,False
279,20000,0,,False
280,25000,0,,False
281,Review Effort (Documents Reviewed),0,,False
282,System recall System prec.,0,,False
283,User recall User prec.,0,,False
284,30000,0,,False
285,35000,0,,False
286,User-Determined vs. Adjudicated Recall and Precision,0,,False
287,40000,0,,False
288,0.8,0,,False
289,Recall (Precision),0,,False
290,0.6,0,,False
291,0.4,0,,False
292,0.2,0,,False
293,0 0,0,,False
294,5000,0,,False
295,10000,0,,False
296,User recall User prec. Adjudicated recall Adjudicated prec.,0,,False
297,15000 20000 25000 30000 Review Effort (Documents Reviewed),0,,False
298,35000,0,,False
299,40000,0,,False
300,45000,0,,False
301,"Figure 2: Identification of Kaine Administration Email Pertaining to the Virginia Tech Shooting for Legal Hold ­ Tradeoff Among Recall, Precision, and Effort. The top panel compares the SystemDetermined vs. User-Determined strategies; the bottom panel compares the User-Determined vs. Adjudicated strategies.",1,ad,True
302,"the two assessors were 0.34 and 0.88, and 0.59 and 0.94, respectively. Although the first assessor has substantially lower recall and precision than the second, the System-Determined recall curves are remarkably similar. The User-Determined recall curves are, as predicted, bounded by the recall of the respective users. On the other hand, the Adjudicated recall curves, like the System-Determined curves, are remarkably",0,,False
303,11,0,,False
304,Session 1A: Evaluation 1,1,Session,True
305,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
306,System-Determined vs. User-Determined Recall and Precision 1,0,,False
307,System-Determined vs. User-Determined Recall and Precision 1,0,,False
308,0.8,0,,False
309,0.8,0,,False
310,Recall (Precision),0,,False
311,Recall (Precision),0,,False
312,0.6,0,,False
313,0.6,0,,False
314,0.4,0,,False
315,0.4,0,,False
316,0.2,0,,False
317,0 0,0,,False
318,1,0,,False
319,50000,0,,False
320,100000,0,,False
321,150000,0,,False
322,200000,0,,False
323,Review Effort (Documents Reviewed),0,,False
324,System recall System prec.,0,,False
325,User recall User prec.,0,,False
326,250000,0,,False
327,User-Determined vs. Adjudicated Recall and Precision,0,,False
328,300000,0,,False
329,0.2,0,,False
330,0 0,0,,False
331,1,0,,False
332,5000,0,,False
333,System recall System prec.,0,,False
334,User recall User prec.,0,,False
335,10000,0,,False
336,15000,0,,False
337,20000,0,,False
338,25000,0,,False
339,Review Effort (Documents Reviewed),0,,False
340,30000,0,,False
341,User-Determined vs. Adjudicated Recall and Precision,0,,False
342,35000,0,,False
343,0.8,0,,False
344,0.8,0,,False
345,Recall (Precision),0,,False
346,Recall (Precision),0,,False
347,0.6,0,,False
348,0.6,0,,False
349,0.4,0,,False
350,0.4,0,,False
351,0.2,0,,False
352,0 0,0,,False
353,50000,0,,False
354,User recall User prec. Adjudicated recall Adjudicated prec.,0,,False
355,100000,0,,False
356,150000,0,,False
357,200000,0,,False
358,250000,0,,False
359,Review Effort (Documents Reviewed),0,,False
360,300000,0,,False
361,350000,0,,False
362,"Figure 3: Identification of Archival Records from the Kaine Administration ­ Tradeoff Among Recall, Precision, and Effort. The top panel compares the System-Determined vs. User-Determined strategies; the bottom panel compares the User-Determined vs. Adjudicated strategies.",1,ad,True
363,0.2,0,,False
364,0 0,0,,False
365,5000,0,,False
366,10000,0,,False
367,15000,0,,False
368,20000,0,,False
369,25000,0,,False
370,Review Effort (Documents Reviewed),0,,False
371,User recall User prec. Adjudicated recall Adjudicated prec.,0,,False
372,30000,0,,False
373,35000,0,,False
374,40000,0,,False
375,"Figure 4: Identification of Restricted Archival Records from the Kaine Administration ­ Tradeoff Among Recall, Precision, and Effort. The top panel compares the System-Determined vs. UserDetermined strategies; the bottom panel compares the User-Determined vs. Adjudicated strategies.",1,ad,True
376,"similar, but are superior to both the User-Determined curves. The System-Determined precision curve initially climbs and then declines with increased effort, as expected, while the User-Determined and Adjudicated precision curves are remarkably flat.",0,,False
377,"Table 1 shows average effectiveness and effort measures over 98 runs, comprising 49 topics and two simulated users. As predicted, the System-Determined strategy achieves very high recall on average, with 4% of the effort of Manual Review. The User-Determined strategy achieves slightly lower recall, but substantially higher precision than Manual Review, also with 4% of the effort. The Adjudicated strategy achieves substantially and significantly higher recall, and precision than Manual Review, with 4.2% of the effort.",0,,False
378,"The agreement between Roger I and Roger II, and Roger III's adjudication of their disagreements, is shown in Table 2.",1,ad,True
379,"Overall, Roger I and Roger II have overlap (i.e., Jaccard index) of 80.6%, 60.2%, and 64.2% on each of three topics-- higher than most reported results for separate assessors, but far from perfect. Roger III generally splits the difference between Roger I and Roger II, with a propensity to agree with the negative assessment. Roger, on completing the Roger III assessments, volunteered that ""this was a challenging review,"" suggesting that the adjudication process had identified many hard-to-classify, as opposed to randomly misclassified, documents.",1,ad,True
380,"Roger I rendered these decisions for each of the three topics seriatim as follows: First, the Virginia Tech documents subject to a legal hold were identified; second, documents not subject to the hold were classified as either archival records or non-records; and finally, documents classified as archival records were categorized as restricted or open records. As a consequence, the document collection diminished for each",0,,False
381,12,0,,False
382,Session 1A: Evaluation 1,1,Session,True
383,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
384,"subsequent topic. Roger II and Roger III employed the same protocol, resulting in a handful of anomalous judgments. For example, Roger I classified some records as records or non-records not subject to legal hold, while Roger II classified them as subject to legal hold. For these documents, we recorded the disagreement with respect to legal hold, and asked Roger II to specify whether the document would be an archival record or a non-record, were it not subject to legal hold. Roger III was asked in advance to consider all six combinations of: subject to legal hold or not, and open record, restricted record, or non-record. The appropriate hypothetical judgments were used as the gold standard for each topic.",1,ad,True
385,"The documents reviewed by Roger II formed a stratified sample of the dataset; measures using Roger II or Roger III were estimated using the Horvitz-Thompson estimator [20]. The strata were selected as follows: For each of the three topics and each of the four possible modes of disagreement, 200 documents were selected independently, at random. Because the documents were selected independently, there was some overlap among these strata, and the total number of unique documents was 2,398. After Roger II had commenced his review, it was discovered that one stratum had been repeated and one had been omitted due to a clerical error, so 200 documents from the omitted stratum were added, along with 200 randomly selected documents from outside the stratum, for a total sample size of 2,798. Inclusion probabilities were adjusted to account for the overlapping strata.",1,ad,True
386,"Figures 2, 3, and 4 show effectiveness versus effort for the Adjudication strategy, while Table 3 shows set-based measures for the three Kaine email topics, and Table 4 shows averages over the topics. The summary measures suggest that the Adjudication strategy achieves significantly better precision and 1 than Roger I ( < 0.05), and no significant difference in recall. The gain in 1 appears to come primarily from balancing recall and precision, which is consistent with the purpose of the Adjudication strategy. None of the differences is larger than 0.5%, suggesting that there is little to choose (in terms of effectiveness) between the Adjudication strategy and Manual Review. On the other hand, the Adjudication strategy offers a huge advantage in terms of efficiency.",1,ad,True
387,"Table 5 shows the pairwise overlap between the system's assessment (that was compared to Roger I's assessment in the Adjudication strategy), and Roger I and Roger II themselves. Collectively, the results indicate that, for all intents and purposes, there is little to choose between the system's judgments and Roger's, and that a second opinion--whether by a human or a bionic assessor--can be helpful.",0,,False
388,"The bootstrap method was used to determine the variance due to sampling in the Roger II and Roger III datasets, which showed all per-topic differences between the Adjudication strategy and Manual Review to be significant (with respect to sampling uncertainty).",0,,False
389,DISCUSSION AND CONCLUSIONS,0,,False
390,"Effectiveness measures for total recall depend on who you ask, when you ask, and how often you ask. Even a small amount of error in gold-standard assessments can substantially depress recall, as evidenced by the provisional versus final recall estimates of the TREC 2009 Legal Track, where the estimated recall of the best submissions for four topics rose from less than 20% in the Notebook Draft, to about 80% in the Final Overview Paper [19, appendix]. While Webber et al. [32] attribute this difference to bias on the part of the assessors, it is equally well explained by a typical true positive rate Pr[+| > 0.7], and a typical false positive rate Pr[+|]  0.01. The difference between the Legal Track assessment and other TREC efforts is that the assessors reviewed a statistical sample of the entire collection, not just the pool of documents identified by the systems. As a consequence, a sample representing more than 700,000 non-relevant documents was reviewed; it is no surprise that examples representing several thousand of these non-relevant documents were incorrectly judged as relevant. Most of those false positives were identified by a process similar to the Adjudication strategy we evaluated, in which disagreements between the participating systems and the first assessor were adjudicated by a second assessor, the Topic Authority. It is not necessary to assume that the first assessor was incompetent, or that the Topic Authority was more competent than the first assessor, to explain the TREC 2009 results.",1,TREC,True
391,"Our Adjudication strategy results on the Kaine email dataset are consistent with the superior results reported by Cormack and Mojdeh at TREC 2009 [5]; they ""[re-]examined documents with high scores that were marked `not relevant' and documents with low scores that were marked `relevant.'"" Our results are also entirely consistent with the results reported in our original 2011 study [14]. It is important to bear in mind that results measuring system recall, rather than end-to-end recall, cannot be compared to the results reported here, to those reported in the TREC 2009 Legal Track Overview [19], or to those reported by Grossman and Cormack [14]. Neither can such results be compared when the user assessments are the same as the evaluation assessments.",1,TREC,True
392,"Our results do not support the mantra of ""garbage in, garbage out,"" or that errors in user feedback are ""amplified"" by the use of a TAR method, as opposed to manual review. To the contrary, our results show that system effectiveness is hardly affected by inferior feedback, and that certain TAR methods can mitigate rather than amplify user error.",0,,False
393,"This study raises a number of questions that may be addressed by future work. It is well known--and reconfirmed by this study--that humans judge the same document differently under different circumstances, including the order of presentation. The effect of using a dynamically learned ranking on user feedback has yet to be studied. On the one hand, studies suggest that when assessors review a higher proportion of relevant documents, they are less likely to judge them relevant (see, e.g., Roegiest [24]). Is this explained by a higher error rate, or by the general observation that higher",1,ad,True
394,13,0,,False
395,Session 1A: Evaluation 1,1,Session,True
396,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
397,"prevalence or more experience with a review set can lead assessors to become more discriminating? Our results show that Roger III was less likely to judge documents relevant than Roger I or Roger II, perhaps because he exercised greater diligence, or maybe because he had become better informed through the course of examining borderline documents.",1,ad,True
398,"Roger II and Roger III were blind to the previous Rogers' assessments. While Roger II commented that he recalled several of the themes in the documents, it had been at least two years since he had previously reviewed them. Roger III, on the other hand, had seen the documents two months prior, and in the interim, had reviewed more than ten thousand emails from a different department of the Kaine administration. It is not apparent what effect Roger's memory may have had on the results. The impact of blind review with a wash-out period has yet to be studied; indeed, it is not clear whether the user should be blind, so as to reduce bias, or informed, so as to aid in deliberation, cf. [22]. At TREC 2009, Cormack and Mojdeh [5] appear to have achieved superior results without blinding and no discernible wash-out period, but more study is necessary to arrive at a definitive answer.",1,ad,True
399,"Overall, our results reconfirm the thesis that hybrid humancomputer classification (i.e., TAR) methods can achieve recall and precision that compare favorably with exhaustive manual review by experts, for much less effort. Where higher recall and precision is desired, additional resources are better spent re-reviewing documents that may have been misjudged by the user, than examining the ranked list to extraordinary depths, or sampling low-ranked documents. When recall and precision values approach 100%, it is essential to consider carefully both the accuracy and independence of the gold standard used for evaluation.",1,ad,True
400,ACKNOWLEDGEMENT,0,,False
401,"We are very grateful for the enthusiasm and support we received from our colleagues at the Library of Virginia, most notably, Roger Christman, Susan Gray Page, Rebecca Morgan, and Kathy Jordan; without them, this work would not have been possible.",0,,False
402,REFERENCES,0,,False
403,"[1] A. Al-Maskari and M. Sanderson. A review of factors influencing user satisfaction in information retrieval. Journal of the American Society for Information Science and Technology, 61(5):859­868, 2010.",0,,False
404,"[2] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR 2006.",0,,False
405,"[3] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. de Vries, and E. Yilmaz. Relevance assessment: are judges exchangeable and does it matter? In SIGIR 2008.",0,,False
406,"[4] D. Blair and M. E. Maron. An evaluation of retrieval effectiveness for a full-text document-retrieval system. Communications of the ACM, 28(3):289­299, 1985.",0,,False
407,"[5] G. Cormack and M. Mojdeh. Machine learning for information retrieval: TREC 2009 Web, Relevance Feedback and Legal Tracks. In TREC 2009.",1,TREC,True
408,[6] G. V. Cormack and M. R. Grossman. Engineering quality and reliability in technology-assisted review. In SIGIR 2016.,0,,False
409,[7] G. V. Cormack and M. R. Grossman. Evaluation of machinelearning protocols for technology-assisted review in electronic discovery. In SIGIR 2014.,0,,False
410,[8] G. V. Cormack and M. R. Grossman. Multi-faceted recall of continuous active learning for technology-assisted review. In SIGIR 2015.,0,,False
411,[9] G. V. Cormack and M. R. Grossman. Scalability of continuous active learning for reliable high-recall text classification. In CIKM 2016.,0,,False
412,[10] G. V. Cormack and M. R. Grossman. Waterloo (Cormack) participation in the TREC 2015 Total Recall Track. In TREC 2015.,1,TREC,True
413,"[11] G. V. Cormack and M. R. Grossman. Autonomy and reliability of continuous active learning for technology-assisted review. arXiv:1504.06868, 2015.",0,,False
414,"[12] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 14(5):441­465, 2011.",0,,False
415,"[13] S. Green and M. Yacano. Computers vs. humans? Putting the TREC 2009 study in perspective. New York Law Journal, (Oct. 1), 2012.",1,TREC,True
416,"[14] M. R. Grossman and G. V. Cormack. Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Richmond Journal of Law & Technology, 17(3), 2011.",0,,False
417,"[15] M. R. Grossman, G. V. Cormack, and A. Roegiest. TREC 2016 Total Recall Track Overview. In TREC 2016.",1,TREC,True
418,"[16] D. Harman. Overview of the First Text REtrieval Conference (TREC-1). In TREC 1, 1992.",1,TREC,True
419,"[17] D. Harman. Overview of the fourth text retrieval conference (trec-4). In TREC 4, 1996.",1,trec,True
420,"[18] D. K. Harman. The TREC ad hoc experiments. In E. M. Voorhees and D. K. Harman, editors, TREC - Experiment and Evaluation in Information Retrieval, chapter 4. MIT Press, 2005.",1,TREC,True
421,"[19] B. Hedin, S. Tomlinson, J. R. Baron, and D. W. Oard. Overview of the TREC 2009 Legal Track. In TREC 2009.",1,TREC,True
422,"[20] D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a finite universe. Journal of the American Statistical Association, 47(260):663­685, 1952.",0,,False
423,"[21] K. Krstovski. Efficient Inference, Search and Evaluation for Latent Variable Models of Text with Applications to Information Retrieval and Machine Translation. University of Massachusetts, 2016.",0,,False
424,"[22] T. McDonnell, M. Lease, T. Elsayad, and M. Kutlu. Why is that relevant? Collecting annotator rationales for relevance judgments. In 4th HCOMP, 2016.",1,ad,True
425,"[23] D. W. Oard, B. Hedin, S. Tomlinson, and J. R. Baron. Overview of the TREC 2008 Legal Track. In TREC 2008.",1,TREC,True
426,[24] A. Roegiest and G. V. Cormack. Impact of review-set selection on human assessment for text classification. In SIGIR 2016.,0,,False
427,"[25] A. Roegiest, G. V. Cormack, M. R. Grossman, and C. L. A. Clarke. TREC 2015 Total Recall Track Overview. In TREC 2015.",1,TREC,True
428,"[26] H. L. Roitblat, A. Kershaw, and P. Oot. Document categorization in legal electronic discovery: Computer classification vs. manual review. Journal of the American Society for Information Science and Technology, 61(1):70­80, 2010.",0,,False
429,"[27] M. Sanderson et al. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4(4):247­375, 2010.",0,,False
430,"[28] L. Schamber. Relevance and information behavior. Annual review of information science and technology (ARIST), 29:3­48, 1994.",0,,False
431,"[29] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5), 2000.",0,,False
432,"[30] E. M. Voorhees. The philosophy of information retrieval evaluation. In Workshop of the Cross-Language Evaluation Forum for European Languages, pages 355­370. Springer, 2001.",0,,False
433,"[31] W. Webber. Re-examining the effectiveness of manual review. In Proc. SIGIR Information Retrieval for E-Discovery Workshop, 2011.",0,,False
434,"[32] W. Webber, D. W. Oard, F. Scholer, and B. Hedin. Assessor error in stratified evaluation. In CIKM 2010.",0,,False
435,14,0,,False
436,,0,,False

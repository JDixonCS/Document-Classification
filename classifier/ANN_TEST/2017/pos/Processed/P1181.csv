,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Building Bridges across Social Platforms: Answering Twi er estions with Yahoo! Answers,1,Yahoo,True
3,Mossaab Bagdouri,0,,False
4,Department of Computer Science University of Maryland,0,,False
5,"College Park, MD 20742, USA mossaab@umd.edu",0,,False
6,ABSTRACT,0,,False
7,"is paper investigates techniques for answering microblog questions by searching in a large community question answering website. Some question transformations are considered, some proprieties of the answering platform are examined, how to select among the various available con gurations in a learning-to-rank framework is studied.",1,blog,True
8,CCS CONCEPTS,0,,False
9,·Information systems  estion answering; Test collections;,0,,False
10,KEYWORDS,0,,False
11,Microblogs; CQA; Cross-platform question answering,1,blog,True
12,1 INTRODUCTION,1,DUC,True
13,"Over 81% of the questions asked on the microblogging service Twitter that are not addressed to a speci c user receive no response [7]. For questions that express a true information need, any useful answer might be highly appreciated. Unanswered questions can be handled by suggesting answers to similar prior questions [9] or by routing the new question to some relevant expert who might be willing to provide an answer [5]. is approach has been extensively investigated using questions previously posted to the same platform where the new question has been posted. Well known techniques leverage features that can be extracted from old questions and answers, as well as the social graph between the users, the questions and the answers.",1,blog,True
14,"Sometimes, however, it might be be er to look elsewhere for the answer. Community estion Answering (CQA) websites such as ora and Yahoo! Answers have became very popular in the last decade, gathering hundreds of millions of questions with their answers. is makes them a suitable place to nd answers for questions that have been posed elsewhere. In this paper we use a large crawl of Yahoo! Answers to search for threads that are potentially useful for a tweet question (Section 3), we compare the importance of di erent elds in which we can search (Section 2),",1,Yahoo,True
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080755",1,ad,True
16,Douglas W. Oard,0,,False
17,"iSchool and UMIACS University of Maryland College Park, MD 20742, USA",0,,False
18,oard@umd.edu,0,,False
19,"and we study some approaches for adapting the language of Twi er questions to that of Yahoo! Answers (Section 2.1). We present our results in Section 4 before concluding with an overview of future directions that can bene t from our release of the annotations (Section 5). To the best of our knowledge, this is the rst work to examine the usefulness of a CQA service for answering questions posted on a microblogging service.",1,ad,True
20,2 METHODS,0,,False
21,"In our search task, we want to retrieve a ""thread"" (i.e., an old question with its answers) from Yahoo! Answers that would be useful for answering the question newly posted to Twi er. A thread has several elds in which we can search. A reasonable baseline is the concatenation of the title and body of the question, together with all of its answers. is approximates a simple search for a web page in a search engine. Alternatively, we can index each eld separately.",1,ad,True
22,"is allows us to study the importance of each eld independently from the others, and to examine di erent combination possibilities. We implement this alternative using BM25 [8].",0,,False
23,"ere are two possibilities for indexing the elds of a thread. In the rst, we index each eld of the question, and the concatenation of all of its answers. We call this indexing setup estion-perDocument (QpD). In the second, the indexed document contains the two question elds and a single answer. at is, we index as many documents for a given thread as there are answers. We call this indexing setup Answer-per-Document (ApD). We refer to the indexed elds as question title (T), question body (B), title and body concatenation (C), and answer(s) (A).",1,ad,True
24,"We experiment with various combinations of these four indexed elds. e weight of each eld is by default set to 1, but we also perform a two dimensional grid search on the weights of the B and A elds in the QpD-TBA con guration (i.e., estion-Per-Document, indexing the Title and Body separately along with a single Answer). For each con guration, we score the the top-1 thread, breaking ties (which is o en needed when searching only the T eld) by selecting the most recent thread.",1,TB,True
25,2.1 estion Rewriting,0,,False
26,"Tweets have characteristics that are less common in some other platforms, some of which we address in this section.",1,Tweet,True
27,"2.1.1 Hashtag Segmentation. Twi er users o en use hashtags to highlight some notion. Since Twi er hashtags don't contain spaces, it is common to concatenate the terms of a multi-words expression. Sometimes a CamelCase convention is used, as in ""i wonder if # eBible is or will be on Net ix?"" In other instances,",0,,False
28,1181,0,,False
29,Short Research Paper,0,,False
30,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
31,"no capitalization cues are present, as in ""Is she Ilona or Elona? #thearchers."" We expect hashtag segmentation to improve retrieval e ectiveness. Our segmentation approach has three stages. In the",0,,False
32,"rst, we remove the # symbol, and use Google's cloud natural language API1 to see if the resulting term is classi ed as an entity (although we ignore entities of type OTHER). is stage aims to avoid segmenting single-word proper names such as ""Washington."" In the second stage, we generate one or more candidate segmentations. If a hashtag follows the CamelCase convention (detected with a regular expression), we segment at capital le ers. Otherwise, we use the vocabulary of our Twi er index (Section 3.3) to extract all possible segmentations (deleting any candidates containing 4 or more words).2 Since some segmentations may be unreasonable (e.g., segmenting #iPhone into ""i phone""), in the third stage, we remove segmentations that appear (in order) less frequently in our Twi er index than the hashtag (without the #). If no segmentation passes this lter, we maintain the hashtag (without the #). Otherwise, we replace it with the segmentation that has the highest frequency (breaking ties arbitrarily).",1,AP,True
33,"2.1.2 Spelling Correction. Twi er is mostly accessed from mobile devices3 on which small keyboards increase the chance of misspellings. Consider for example ""Why did the great awaking happen?"" We have li le hope for nding an answer unless the spelling of awaking is corrected to awakening. is problem is particularly critical when the misspelled word is a key term in the question. Another impact appears when a high frequency word (e.g., a stop word) is misspelled, typically resulting in a rare word with high IDF. For example, because we lowercase everything before performing a search, ""should igo to school tomorrow?"" leads to the undesirable retrieval of threads about Inter-Governmental Organizations (IGO).",1,ad,True
34,"We perform spelling correction in three steps. As with hashtag segmentation, we rst exclude terms that are classi ed as entities (of a type other than OTHER). We then generate a list of (up to) the 1,000 closest words by Levenstein distance, using a model trained on character n-grams from our Twi er index.4 Finally, we keep only alternatives for which both their document frequency and the document frequencies of the terms to their le and right (up to the rst stopword) are greater than those of the original word. If any alternatives pass this lter, we return the alternative with the highest document frequency as the possible correction. To limit the e ect of correction mistakes, we treat the possible correction as a synonym of the original word, computing the BM25 score for each",0,,False
35,eld a er summing the term frequencies of the original term and its possible correction and approximating the combined document frequency with the maximum of the two document frequencies (which is the document frequency of the possible correction).,0,,False
36,"2.1.3 Synonyms. e informal language of tweets encourages the adoption of some writing conventions that are less frequent in other platforms. For example, you and conversations would be synonyms to u and convos in ""Should u read your kids convos on the Internet?"" We nd synonyms in three stages. e rst and",1,ad,True
37,"1h ps://cloud.google.com/natural-language 2We use the WordBreakSpellChecker.suggestWordBreaks() method of Lucene 6.3. 3h p://venturebeat.com/?p,2014007 4We use the SpellChecker.suggestSimilar() method of Lucene 6.3.",0,,False
38,Is duplicate? Softmax Cosine,0,,False
39,Question 1,0,,False
40,Pooling BLSTM Embedding,0,,False
41,Pooling BLSTM Embedding,0,,False
42,Question 2,0,,False
43,Figure 1: An architecture for detecting duplicate questions.,0,,False
44,"the third stages are identical to what we do for spelling correction. For the second stage (suggesting a candidate synonym), we use a word2vec [6] model trained on our Twi er corpus to suggest the nearest word to the original one, but only if the cosine similarity of their vectors exceeds an arbitrary threshold of 0.5.",0,,False
45,2.2 Term Statistics,0,,False
46,"e importance of a term is indicated, in the BM25 scoring function, by its IDF. As a result, the same term might have di erent IDF values in di erent corpora. For the question ""What am I gunna do with this dog for the night?"" we observe that night has a high IDF in Yahoo! Answers compared to dog e opposite (and we think more desirable) relative IDF rank is true for Twi er, however. Some words seem to su er from a ""cost of fame"" in which they are so important that many questions are asked about them in Yahoo! Answers, (where there is an entire subcategory for dogs), thus diminishing their IDF. To mitigate this e ect, we can use the IDF statistics from our Twi er index.",1,corpora,True
47,2.3 estion/ estion Similarity,0,,False
48,"Similar questions might be phrased in di erent ways, so we need some way of measuring the extent to which a Twi er question is similar to a question in Yahoo! Answers. ora has recently released a corpus of 404,351 pairs of questions, among which 149,306 are indicated to be duplicates.5 We use 90% of those pairs to train the neural network depicted in Figure 1, and the remaining validation subset to stop training when the accuracy does not improve over the best prior results in the previous 10 epochs. We return the model that has the best accuracy (85.5%) on that validation set, a er optimizing it with ADAM [4], using mean squared error as a loss function as implemented in Keras.6",1,Yahoo,True
49,2.4 Selecting Con gurations and Answers,0,,False
50,"e approaches we have introduced so far aim to nd con gurations that work the best on average. However, it is possible to use the features of the questions and the answers to select the thread to be retrieved given the votes of di erent con gurations, and our prior knowledge of their average performance. Here we present a three-stage process to select the best thread among those returned by several con gurations.",1,ad,True
51,5h p://qim.ec.quoracdn.net/quora duplicate questions.tsv 6h p://keras.io,0,,False
52,1182,0,,False
53,Short Research Paper,0,,False
54,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
55,2.4.1 Ordering the Configurations. Let N be the number of avail-,0,,False
56,"able con gurations, C1, ...Cn N be a subset of con gurations, and",0,,False
57,"T1, ...Tm be the number of training questions. Every pair (Ci ,Tj ) cor-",0,,False
58,"responds to a retrieved thread with a score Si, j . e maximum av-",1,ad,True
59,"erage score, over the training questions, that can be achieved given",0,,False
60,this,0,,False
61,combination,0,,False
62,(with,0,,False
63,an,0,,False
64,oracle),0,,False
65,would,0,,False
66,be:,0,,False
67,S^,0,,False
68,",",0,,False
69,1 m,0,,False
70,"Ti maxCj Si, j .",0,,False
71,Our goal is to nd the subset of con gurations that maximizes this,0,,False
72,value given n. With as many as 74 con gurations in our experi-,0,,False
73,"ments, a greedy search is considerably more e cient than exhaus-",0,,False
74,tively trying every possible combination. We start by nding the,0,,False
75,best single con guration. We then repeatedly use the best com-,0,,False
76,"bination we obtained at iteration n - 1, which gave us potential",0,,False
77,"average score S^n-1, and iterate over the remaining con gurations to maximize S^n . is process yields an ordered list of con gurations",0,,False
78,"that can be added, one at a time, to form several combinations.",1,ad,True
79,"2.4.2 Learning to Rank Threads. With some combination of congurations that, collectively, retrieves a set of threads, we want to learn to rank those threads. For every pair of a question and a retrieved thread, we extract the following vector of features:",1,ad,True
80,"· e BM25 scores of the title, the body, and the answer(s). · e neural similarity scores between the question, and",0,,False
81,"each of the three elds above. · e number of answers in the thread (log scaled). · e min, max, mean and standard deviation of the scores",1,ad,True
82,"of each answer, both for BM25 and neural similarity. · e number of threads with the same BM25 score as the",1,ad,True
83,"candidate (log scaled) · All the same features, using the rewri en question (with",0,,False
84,the three rewriting operations). · Binary indicators of whether each con guration returned,0,,False
85,that thread.,1,ad,True
86,"Given a training question with several threads, we integerize the ground truth score for each thread (Section 3) using 0.5, 1.5 and 2.5 as cuto points to produce scores of 0 (bad), 1( fair), 2 (good) or 3 (excellent). Finally, we train a learning-to-rank (L2R) model based on those threads using the SVMr ank so ware [3].",1,ad,True
87,"2.4.3 Selecting the Best Combination of Configurations. Given an ordered list of con gurations (Section 2.4.1) and a model for ranking the threads of a particular combination of con gurations (Section 2.4.2), we can select the best combination. To do so, we start with the best single con guration, and record its e ectiveness on the training and validation questions, considering it to be the best combination so far. en, we iterate over the ordered con gurations, one at a time, adding each to the pool of con gurations, and training its L2R model. We record the average score of the predictions on the training and validation questions using the actual ground truth scores (not the integerized versions). If the e ectiveness increases in both sets, we consider the actual combination to be the best one. We stop when we nish our enumeration, and return the most recent best combination.",1,ad,True
88,3 TEST COLLECTION,0,,False
89,"We present a set of Twi er questions, a crawl of Yahoo! Answers, and a collection of tweets used to build a language model.",1,Yahoo,True
90,3.1 estions and Answers,0,,False
91,"Among questions with real information needs, only a small fraction could reasonably be answered by an automated system. Consider, for example, ""@user hey, when u coming back?"" Clearly, the asker would want an answer to this question, but probably only the mentioned user could provide it. It would be advisable for an answering system to skip such questions. We have collected a set of 5,000 questions posted on Twi er in February 2016 and asked annotators on the crowdsourcing platform CrowdFlower to indicate whether some stranger probably exists who could read the question and o er a useful answer. In this paper, we use the resulting set of 362 tweets deemed to be answerable questions, with a 177/85/100 split between training, validation and test.",1,ad,True
92,"With the large base of old questions and answers available in Yahoo! Answers, we hope to successfully nd useful answers to a substantial number of questions asked on Twi er. One option would be to issue the question as a query and rely on the questions and answers retrieved by its""black box"" internal search engine. However, this would prevent us from studying the di erent options for building and using the inverted index. us, we obtain a crawl of 123M questions and 673M answers from [2]. We exclude questions and answers that contain any term from a ""dirty word"" list,7 and index all of the remaining questions and answers posted prior to 2016 (to avoid ""leaking"" future information to new answers).",1,Yahoo,True
93,3.2 Ground Truth,0,,False
94,"We also collect annotations for the results of our experiments using CrowdFlower. Because our task is similar, we adopt the same 4level relevance scale (bad, fair, good, excellent) as the the TREC LiveQA track [1]. We assign the weighted average score over three annotators (where the weight is computed from annotator accuracy on a set of questions with known answers) as the ground-truth relevance score of the thread. Annotators with accuracy scores below 85% were removed and replaced.",1,ad,True
95,"As assessing all answers to a question might be impractical when many answers exist, we present only the question title, body, and a what we expect to be the best few answers. We select these answers in part based on metadata from Yahoo! Answers and in part based on whichever of our systems found the answer. We rst select the best answer, as designated by the asker, if one exists. Otherwise, we select the answer with the highest di erence between thumbsup and thumbs-down votes, breaking ties by the score assigned by the system that found that answer. We also include whatever answer our system scored highest. In both cases, if multiple systems retrieve the same thread but disagree on which is the best answer, we include the best answers from each.",1,ad,True
96,"We obtained the annotations in several batches. In each batch we gathered the annotations for all threads that had not been previously assessed for all 362 questions. is allowed us to use the results of prior annotations to incrementally improve our systems, thus generating a richer test set, akin to the way systems from one year are used to guide the development of test collections in subsequent years at shared-task evaluations such as TREC. We note, however, that di erent annotators assessed di erent batches.",1,ad,True
97,7Downloaded from h ps://gist.github.com/roxlu/1769577,1,ad,True
98,1183,0,,False
99,Short Research Paper,0,,False
100,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
101,3.3 Twitter Language Model,1,Twitter,True
102,"For language modeling, we obtained the Twi er random 1% public sample stream between January 2012 and December 2015 from the Internet Archive.8 We keep only English tweets and index them with Lucene (without stemming or stopword removal), recording the positions of each indexed term in its tweet. We use this positional index as a language model to guide our question transformations (Section 2.1).",0,,False
103,4 RESULTS,0,,False
104,"We experiment with a combinations of 74 con gurations. Table 1 shows the average top-1 accuracy (on a [0-3] scale) for some of the combinations. First, we observe that the single best eld is the Title (line 1). It is signi cantly (p < 0.05, two-sided paired t-test) be er than the Body and the Answer elds (lines 2 and 7) in all sets (i.e., training, validation and test). Searching using this eld is also be er than searching using the entire page as a single eld (line 8), with a signi cance observed in the training and test sets. It also appears that question-per-document indexing may be a bit be er than answer-per-document indexing (compare lines 5, 6 and 7 to 9, 10 and 11), but weak signi cance is observed only in the training set (p < 0.1). Tuning the weights of the elds (line 12) seems to over t to the training set, where it is signi cantly (p < 0.05) be er than all combinations other than QpD-T (i.e., lines 2 through 11). On the validation and test sets, this tuning is not be er than some of those combinations. None of the query rewriting methods, individually or in combination, improve the results signi cantly, and the same is true for using the IDF of the Twi er index. Insigni cant positive di erences, when observed, are restricted to the training set. e statistically signi cant improvements we observe (p < 0.01) with the L2R model over all con gurations appears to be an instance of over ing. In fact, the results over the validation set decrease slightly (from 1.48 of line 5 to 1.45 in line 26), and the gain we get in the test set (from 1.32 to 1.37) is not statistically signi cant.",0,,False
105,5 CONCLUSION,0,,False
106,"We studied the possibility of answering the questions asked on Twi er using Yahoo! Answers, and found that, on average, two thirds of the answerable questions do have an excellent answer there. We found that searching in the title eld of the old questions yields a signi cant improvement over search in the concatenation of all the elds of a CQA thread. Small improvements are sometimes observed using various techniques, such as the tuning the weights of the indexed elds, rewriting the tweet question, and using the IDF of an index of tweets. While none of these techniques is particularly be er than the others, the pool of diverse threads they retrieve suggests that a failure analysis might help to identify techniques that can be employed for speci c question types. We have released our test collection to encourage further investigation.9",1,Yahoo,True
107,6 ACKNOWLEDGMENT,0,,False
108,is work was made possible in part by NPRP grant# NPRP 6-13771-257 from the Qatar National Research Fund (a member of Qatar,1,ad,True
109,8h p://archive.org/details/twi erstream 9h p://cs.umd.edu/mossaab/ les/aqweet-answering.tgz,0,,False
110,Table 1: E ectiveness of con gurations over the scale [0-3].,0,,False
111,# Con guration,0,,False
112,Fields,0,,False
113,Average score Train. Valid. Test,0,,False
114,1 BM25 2 BM25 3 BM25 4 BM25 5 BM25 6 BM25 7 BM25 8 BM25,0,,False
115,QpD-T,0,,False
116,1.22,0,,False
117,QpD-B,0,,False
118,0.80,0,,False
119,QpD-TB 1.10,1,TB,True
120,QpD-C,0,,False
121,1.13,0,,False
122,QpD-TA 1.20,0,,False
123,QpD-TBA 1.11,1,TB,True
124,QpD-A,0,,False
125,0.79,0,,False
126,QpD-P,0,,False
127,0.85,0,,False
128,1.19 1.32 0.89 0.75 1.14 1.05 1.16 1.20 1.48 1.21 1.22 1.14 0.82 0.74 1.09 0.88,0,,False
129,9 BM25 10 BM25 11 BM25,0,,False
130,ApD-TA 1.10,0,,False
131,ApD-TBA 1.02,1,TB,True
132,ApD-A,0,,False
133,0.54,0,,False
134,1.27 1.12 1.15 1.04 0.68 0.41,0,,False
135,12 Weighted BM25,0,,False
136,QpD-TBA 1.32 1.28 1.32,1,TB,True
137,13 BM25 + Hashtag Split 14 BM25 + Hashtag Split,0,,False
138,QpD-T QpD-TA,0,,False
139,1.24 1.19 1.31 1.22 1.46 1.20,0,,False
140,15 BM25 + Spell Correction QpD-T 16 BM25 + Spell Correction QpD-TB 17 BM25 + Spell Correction QpD-TA,1,TB,True
141,1.23 1.19 1.30 1.13 1.17 1.03 1.21 1.48 1.19,0,,False
142,18 BM25 + Synonyms 19 BM25 + Synonyms,0,,False
143,QpD-T QpD-TA,0,,False
144,1.22 1.24 1.29 1.21 1.43 1.22,0,,False
145,20 BM25 + 3 Rewriters 21 BM25 + 3 Rewriters 22 BM25 + 3 Rewriters,0,,False
146,QpD-T QpD-TA QpD-C,0,,False
147,1.25 1.24 1.27 1.23 1.41 1.19 1.12 1.17 1.10,0,,False
148,23 BM25 + Twi er IDF 24 BM25 + Twi er IDF 25 BM25 + Twi er IDF,0,,False
149,QpD-T QpD-TA QpD-P,0,,False
150,1.21 1.08 1.32 1.09 1.38 0.96 0.82 1.15 0.97,0,,False
151,"26 L2R , (12) + (20) + (25) + (22)",0,,False
152,1.43 1.45 1.37,0,,False
153,27 Oracle,0,,False
154,1.90 2.03 1.86,0,,False
155,Foundation) and by an IBM Ph.D. Fellowship. e statements made herein are solely the responsibility of the authors.,1,ad,True
156,REFERENCES,0,,False
157,"[1] Eugene Agichtein, David Carmel, Dan Pelleg, Yuval Pinter, and Donna Harman. 2015. Overview of the TREC 2015 LiveQA Track. In TREC. Gaithersburg, MD.",1,TREC,True
158,"[2] Mossaab Bagdouri and Douglas W. Oard. 2015. CLIP at TREC 2015: Microblog and LiveQA. In TREC. Gaithersburg, MD, USA.",1,TREC,True
159,"[3] orsten Joachims. 2006. Training Linear SVMs in Linear Time. In KDD'06. Philadelphia, PA, USA, 217­226.",1,ad,True
160,"[4] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR'15. San Diego, CA, USA.",0,,False
161,"[5] Baichuan Li and Irwin King. 2010. Routing estions to Appropriate Answerers in Community estion Answering Services. In CIKM'10. Toronto, ON, Canada, 1585­1588.",1,ad,True
162,"[6] Tomas Mikolov, Kai Chen, Greg Corrado, and Je rey Dean. 2013. E cient Estimation of Word Representations in Vector Space. In Workshop at ICLR'13.",1,ad,True
163,"[7] Sharoda A. Paul, Lichan Hong, and Ed H. Chi. 2011. Is Twi er a Good Place for Asking estions? A Characterization Study. In ICWSM'11.",1,CW,True
164,"[8] Stephen Robertson and Hugo Zaragoza. 2009. e Probabilistic Relevance Framework: BM25 and Beyond. FnTIR 3, 4 (April 2009), 333­389.",0,,False
165,"[9] Anna Shtok, Gideon Dror, Yoelle Maarek, and Idan Szpektor. 2012. Learning from the Past: Answering New estions with Past Answers. In WWW'12. Lyon, France, 759­768.",0,,False
166,1184,0,,False
167,,0,,False

,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge,0,,False
3,Arman Cohan,0,,False
4,"Information Retrieval Lab, Dept. of Computer Science Georgetown University",0,,False
5,arman@ir.cs.georgetown.edu,0,,False
6,ABSTRACT,0,,False
7,"Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to re ect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the e ectiveness of our model by signi cantly outperforming the state-of-the-art. We furthermore demonstrate how an e ective contextualization method results in improving citation-based summarization of the scienti c articles.",1,ad,True
8,KEYWORDS,0,,False
9,"Text Summarization, Scienti c Text, Information Retrieval",0,,False
10,1 INTRODUCTION,1,DUC,True
11,"In scienti c literature, related work is often referenced along with a short textual description regarding that work which we call citation text. Citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper. Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16]).",0,,False
12,"While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18]. For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned. Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form. Hence, sometimes the citation text is not su ciently informative or in other cases, even inaccurate [17]. This problem is more serious in life sciences where accurate dissemination of knowledge has direct impact on human lives.",0,,False
13,"We present an approach for addressing such concerns by adding the appropriate context from the reference article to the citation texts. Enriching the citation texts with relevant context from the reference paper helps the reader to better understand the context for the ideas, methods or ndings stated in the citation text.",1,ad,True
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080740",1,ad,True
15,Nazli Goharian,0,,False
16,"Information Retrieval Lab, Dept. of Computer Science Georgetown University",0,,False
17,nazli@ir.cs.georgetown.edu,0,,False
18,"A challenge in citation contextualization is the discourse and terminology variations between the citing and the referenced authors. Hence, traditional IR models that rely on term matching for",1,ad,True
19,nding the relevant information are ine ective. We propose to address this challenge by a model that utilizes,1,ad,True
20,"word embeddings and domain speci c knowledge. Speci cally, our approach is a retrieval model for nding the appropriate context of citations, aimed at capturing terminology variations and paraphrasing between the citation text and its relevant reference context.",0,,False
21,"We perform two sets of experiments to evaluate the performance of our system. First, we evaluate the relevance of extracted contexts intrinsically. Then we evaluate the e ect of citation contextualization on the application of scienti c summarization. Experimental results on TAC 2014 benchmark show that our approach signi cantly outperforms several strong baselines in extracting the relevant contexts. We furthermore, demonstrate that our contextualization models can enhance summarizing scienti c articles.",0,,False
22,2 CONTEXTUALIZING CITATIONS,0,,False
23,"Given a citation text, our goal is to extract the most relevant context",0,,False
24,to it in the reference article. These contexts are essentially certain,0,,False
25,"textual spans within the reference article. Throughout, colloquially,",0,,False
26,we refer to the citation text as query and reference spans in the,0,,False
27,reference article as documents. Our approach extends Language,0,,False
28,Models for IR (LM) by incorporating word embeddings and domain,1,LM,True
29,ontology to address shortcomings of LM for this research purpose.,1,ad,True
30,"The goal in LM is to rank a document d according to the conditional probability p(d |q)  p(q|d ) , qi q p(qi |d ) where qi shows the tokens in the query q. Estimating p(qi |d ) is often achieved by max-",1,LM,True
31,imum likelihood estimate from term frequencies with some sort of,0,,False
32,"smoothing. Using Dirichlet smoothing [21], we have:",0,,False
33,p(qi |d ),0,,False
34,",",0,,False
35,"f (qi , d ) + µ p(qi |C) w V f (w, d ) + µ",0,,False
36,(1),0,,False
37,"where f (qi , d ) shows the frequency of term qi in document d, C",0,,False
38,"is the entire collection, V is the vocabulary and µ the Dirichlet",0,,False
39,"smoothing parameter. In the citation contextualization problem, (i)",0,,False
40,the target reference sentences are short documents and (ii) there,0,,False
41,exist terminology variations between the citing author and the,0,,False
42,"referenced author. Hence, the citation terms usually do not appear",0,,False
43,"in the documents and relying only on the frequencies of citation terms in the documents (f (qi , d )) for estimating p(qi |d ) yields an almost uniform smoothed distribution that is unable to decisively",0,,False
44,distinguish between the documents.,0,,False
45,"Embeddings. Distributed representation (embedding) of a word w in a eld F is a mapping w  Fn where words semantically similar to w will be ideally located close to it. Given a query q, we rank the documents d according to the following scoring function",0,,False
46,1133,0,,False
47,Short Research Paper,0,,False
48,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
49,normalized dot product,0,,False
50,normalized dot product,0,,False
51,1.0,0,,False
52,normalized logit,0,,False
53,1.0,0,,False
54,normalized logit,0,,False
55,0.8,0,,False
56,0.8,0,,False
57,0.6,0,,False
58,0.6,0,,False
59,0.4,0,,False
60,0.4,0,,False
61,0.2,0,,False
62,0.0 0,0,,False
63,500,0,,False
64,1000,0,,False
65,1500,0,,False
66,2000,0,,False
67,0.2,0,,False
68,0.0 0,0,,False
69,200 400 600 800 1000,0,,False
70,Figure 1: Dot product of embeddings and its logit for a sample word and its top most similar words (top 2000 and 1000).,0,,False
71,which leverages this property:,0,,False
72,p(qi |d ),0,,False
73,",",0,,False
74,"fsem (qi , d ) + µ p(qi |C ) w V fsem (w, d ) + µ",0,,False
75,(2),0,,False
76,where fsem is a function that measures semantic relatedness of the,0,,False
77,"query term qi to the document d and is de ned as: fsem (qi , d ) ,",0,,False
78,"dj d s (qi , dj ); where dj 's are document terms and s (qi , dj ) is the",0,,False
79,relatedness between the the query term and document term which,0,,False
80,is calculated by applying a similarity function to the distributed,0,,False
81,representations of qi and dj . We use a transformation () of dot products between the unit vectors e (qi ) and e (dj ) corresponding to the embeddings of the terms qi and dj for the similarity function:,0,,False
82,"s (qi , dj ) ,  (e (qi ).e (dj )); if e (qi ).e (dj ) > ",0,,False
83,0;,0,,False
84,otherwise,0,,False
85,We rst explain the role of  and then the reason for considering,0,,False
86,the function  instead of raw dot product.  is a parameter that,1,ad,True
87,controls the noise introduced by less similar words. Many unrelated,0,,False
88,word vectors have non-zero similarity scores and adding them up introduces noise to the model and reduces the performance.  's function is to set the similarity between unrelated words to zero,1,ad,True
89,"instead of a positive number. To identify an appropriate value for  , we select a random set of words from the embedding model and calculate the average and standard deviation of pointwise absolute",1,ad,True
90,value of similarities between terms from these two samples. We then select the threshold  to be two standard deviations larger than the average to only consider very high similarity values (this,0,,False
91,choice was empirically justi ed).,0,,False
92,Examining term similarity values between words shows that,0,,False
93,there are many terms with high similarities associated with each,0,,False
94,"term and these values are not highly discriminative. We apply a transfer function  to the dot product e (qi ).e (dj ) to dampen the e ect of less similar words. In other words, we only want highly",0,,False
95,related words to have high similarity values and similarity should,0,,False
96,quickly drop as we move to less related words. We use the logit,0,,False
97,function for  to achieve this dampening e ect:,0,,False
98, (x,0,,False
99,),0,,False
100,",",0,,False
101,log(,0,,False
102,1,0,,False
103,x -,0,,False
104,x,0,,False
105,),0,,False
106,Figure 1 shows this e ect. The purple line is the normalized dot,0,,False
107,product of a sample word with the most similar words in the model.,0,,False
108,"As illustrated, the similarity score di erences among top words",0,,False
109,"is not very discriminative. However, applying the logit function",0,,False
110,(green line) causes the less similar words to have lower similarity,0,,False
111,values to the target word. Domain knowledge. Successful word embedding methods have,0,,False
112,previously shown to be e ective in capturing syntactic and semantic,0,,False
113,relatedness between terms. These co-occurrence based models are,0,,False
114,"data driven. On the other hand, domain ontologies and lexicons",0,,False
115,that are built by experts include some information that might not,0,,False
116,"be captured by embedding methods [8]. Therefore, using domain",0,,False
117,knowledge can further help the embedding based retrieval model;,0,,False
118,we incorporate it in our model in the following ways: 1) Retro tting: Faruqui et al. [6] proposed a model that uses the,1,corpora,True
119,constraints on WordNet lexicon to modify the word vectors and,0,,False
120,pull synonymous words closer to each other. To inject the domain,0,,False
121,"knowledge in the embeddings, we apply this model on two domain speci c ontologies, namely, M and Protein Ontologies (PO)1.",0,,False
122,We chose these two biomedical domain ontologies because they,0,,False
123,are in the same domain as the articles in the TAC dataset. M,0,,False
124,is a broad ontology that consists of biomedical terms and PO is a,1,ad,True
125,more focused ontology related to biology of proteins and genes. 2) Interpolating in the LM: We also directly incorporate the do-,1,LM,True
126,main knowledge in the retrieval model; we modify the LM into the,1,LM,True
127,following interpolated LM with parameter :,1,LM,True
128,"p(qi |d ) , p1 (qi |d ) + (1 - )p2 (qi |d ) where p1 is estimated using Eq. 2 and p2 is similar to p1 except that",0,,False
129,we replace fsem with the function font which considers domain,0,,False
130,ontology in calculating similarities:,0,,False
131,"font (qi , d ),",0,,False
132,"1,",0,,False
133,"s2 (qi , dj ); s2 (qi , dj ),"" ,""",0,,False
134,"if qi ,dj if qi dj",0,,False
135,dj d,0,,False
136,"0, o.w. ",0,,False
137,"where   [0, 1] is a parameter and qi  dj shows that there is",0,,False
138,an is-synonym relation in ontology between qi and dj 2.,0,,False
139,3 EXPERIMENTS,0,,False
140,Data. We use the TAC 2014 Biomedical Summarization benchmark3.,0,,False
141,This dataset contains 220 scienti c biomedical journal articles and,0,,False
142,313 total citation texts where the relevant contexts for each citation,0,,False
143,"text are annotated by 4 experts. Baselines. To our knowledge, the only published results on TAC",0,,False
144,"2014 is [4], where the authors utilized query reformulation (QR)",0,,False
145,"based on UMLS ontology. In addition to [4], we also implement sev-",1,ad,True
146,eral other strong baselines to better evaluate the e ectiveness of our,0,,False
147,model: 1) BM25; 2) VSM: Vector Space Model that was used in [4]; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling,1,LM,True
148,with LDA smoothing which is a recent extension of the LMD to,1,LM,True
149,also account for the latent topics [10]. All the baseline parameters,0,,False
150,"are tuned for the best performance, and the same preprocessing is",0,,False
151,applied to all the baselines and our methods. Our methods. We rst report results based on training the em-,0,,False
152,"beddings on Wikipedia (WEWiki). Since TAC dataset is in biomedical domain, many of the biomedical terms might be either out-",1,Wiki,True
153,of-vocabulary or not captured in the correct context using gen-,0,,False
154,"eral embeddings, therefore we also train biomedical embeddings (WEBio)4. In addition, we report results for biomedical embeddings with retro tting (WEBio+rtrft), as well as interpolating domain knowledge (WEBio+dmn)",1,ad,True
155,3.1 Intrinsic Evaluation,0,,False
156,"First, we analyze the e ectiveness of our proposed approach for contextualization intrinsically. That is, we evaluate the quality of the",0,,False
157,1https://www.nlm.nih.gov/mesh/; http://pir.georgetown.edu/pro/ 2The values of the parameters  and  were selected empirically by grid search,0,,False
158,3,0,,False
159,http://www.nist.gov/tac/2014/BiomedSumm/ 4We train biomedical embeddings on TREC Genomics 2004 and 2006 collections (both,1,TREC,True
160,Wikipedia and Genomics embeddings were trained using gensim implementation of,1,Wiki,True
161,"Word2Vec, negative sampling, window size of 5 and 300 dimensions.",0,,False
162,1134,0,,False
163,Short Research Paper,0,,False
164,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
165,"Table 1: Results on TAC 2014 dataset. c-P, c-R, c-F: character o set Precision, Recall and F-1 scores; R : R ; cP@K: character o set precision at K. shows statistical signi cant improvement over the best baseline performance (two-tailed t-test, p<0.05). Values are percentages.",0,,False
166,Method,0,,False
167,c-P c-R c-F nDCG R -1 R -2 R -3 c-P@1 c-P@5,0,,False
168,VSM [4],0,,False
169,20.5 24.7 21.2 48.1 49.5 26.4 20.0 31.9 26.1,0,,False
170,BM25,0,,False
171,19.5 18.6 17.8 38.1 43.6 23.2 16.3 25.5 24.2,0,,False
172,DESM [12],0,,False
173,20.3 23.8 22.3 45.6 50.3 26.2 20.6 32.5 26.5,0,,False
174,LMD-LDA [10] 22.6 24.8 22.3 46.0 48.3 26.4 20.1 31.4 27.7,1,LM,True
175,QR [4],0,,False
176,22.2 29.4 23.8 49.8 50.6 27.2 21.8 37.7 28.1,0,,False
177,WEWiki WEBio WEBio+rtrft WEBio+dmn,1,Wiki,True
178,21.8 28.5 23.2 52.8 50.0 26.9 20.9 36.5 29.9,0,,False
179,23.9 31.2 25.5 57.1 51.9 29.2 23.1 46.2 34.1 24.8 33.6 26.4 58.3 52.4 30.7 24.0 55.5 34.9 25.4 33.0 27.0 59.8 53.0 30.6 24.4 56.1 37.1,0,,False
180,"Table 2: Top relevant words to the word ""expression"" according to embeddings trained on Wikipedia vs. Genomics.",1,Wiki,True
181,General (Wikipedia),1,Wiki,True
182,interpretation sense emotion function intension manifestation expressive,0,,False
183,Biomedical (Genomics),0,,False
184,upregulation mrna,0,,False
185,induction protein,0,,False
186,abundance gene,0,,False
187,downregulation,0,,False
188,extracted citation contexts using our contextualization methods in terms of how accurate they are with respect to human annotations.,0,,False
189,"Evaluation. We consider the following evaluation metrics for assessing the quality of the retrieved contexts for each citation from multiple aspects: (i) Character o set overlaps of the retrieved contexts with human annotations in terms of precision (c-P), recall (c-R) and F-score (c-F). These are the recommended metrics for the task per TAC5. (ii) nDCG: we treat any partial overlaps with the gold standard as a correct context and then calculate the nDCG scores. (iii) R -N scores: To also consider the content similarity of the retrieved contexts with the gold standard, we calculate the R scores between them. (iv) Character precision at K (c-P@K): Since we are usually interested in the top retrieved spans, we consider character o set precision only for the top K spans and we denote it with ""c-P@K"".",0,,False
190,Results. The results of intrinsic evaluation of contextualization are presented in Table 1. Our models (last 4 rows of table 1) achieve signi cant improvements over the baselines consistently across most of the metrics. This shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines. The best baseline performance is the query reformulation (QR) method by [4] which improves over other baselines.,0,,False
191,"We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WEwiki and QR in the Table). However, using the domain speci c embeddings (WEBio ) results in 10% c-F improvement over the best baseline. This is expected since word relations in the biomedical context are better captured with biomedical embeddings. In Table 2 an illustrative word ""expression"" gives better intuition why is that the case. As shown, using general embeddings (left column in the table), the most similar words to ""expression"" are those related to",1,ad,True
192,5 https://tac.nist.gov/2014/BiomedSumm/guidelines.html,0,,False
193,Table 3: Breakdown of our best model's character F-score (cF) by quartiles of human performance measured by c-P.,0,,False
194,Quartiles (c-P),0,,False
195,Q1 Q2 Q3 Q4,0,,False
196,c-F of our model 16.14 25.41 33.72 37.50 (mean ± stdev.) ±20.20 ±7.78 ±5.81 ±5.93,0,,False
197,"the general meaning of it. However, many of these related words are not relevant in the biomedical context. In the biomedical context, ""expression"" refers to ""the appearance in a phenotype attributed to a particular gene"". As shown on the right column, the domain speci c embeddings (Bio) trained on genomics data are able to capture this meaning. This further con rms the inferior performance of the out-of-domain word embeddings in capturing correct word-level semantics [13]. Last two rows in Table 1 show incorporating the domain knowledge in the model which results in signi cant improvement over the best baseline in terms of most metrics (e.g. 14% and 16% c-F improvements). This shows that domain ontologies provide additional information that the domain trained embeddings may not contain. While both our methods of incorporating domain ontologies prove to be e ective, interpolating domain knowledge directly (WEBio +dmn) has the edge over retro tting (WEBio +rtrft). This is likely due to the direct e ect of ontology on the interpolated language model, whereas in retro tting, the ontology rst a ects the embeddings and then the context extraction model.",1,corpora,True
198,"To analyze the performance of our system more closely, we took the context identi ed by 1 annotator as the candidate and the other 3 as gold standard and evaluated the precision to obtain an estimate of human performance on each citation. We then divided the citations based on human performance to 4 groups by quartiles. Table 3 shows our system's performance on each of these groups. We observe that, when human precision is higher (upper quartiles in the table), our system also performs better and with more con dence (lower std). Therefore, the system errors correlate well with human disagreement on the correct context for the citations. Averaged over the 4 annotators for each citation, the mean precision was 56.7% (note that this translates to our c-P@1 metric). In Table 1, we observe that our best method (c-P@1 of 56.1%) is comparable with average human precision score (c-P@1 of 56.7%) which further demonstrates the e ectiveness of our model.",0,,False
199,3.2 External evaluation,0,,False
200,"Citation-based summarization can e ectively capture various contributions and aspects of the paper by utilizing citation texts [15]. However; as argued in section 1, citation texts do not always accurately re ect the original paper. We show how adding context from the original paper can address this concern, while keeping the bene ts of citation-based summarization. Speci cally, we compare how using no contextualization, versus various proposed contextualization approaches a ect the quality of summarization. We apply the following well-known summarization algorithms on the set of citation texts, and the retrieved citation-contexts: LexRank, LSAbased, SumBasic, and KL-Divergence (For space constraints, we will not explain these approaches here; refer to [14] for details). We then compare the e ect of our proposed contextualization methods using the standard R -N summarization evaluation metrics.",1,ad,True
201,"Results. The results of external evaluation are illustrated in Table 4. The rst row (""No context"") shows the performance of each",0,,False
202,1135,0,,False
203,Short Research Paper,0,,False
204,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
205,Table 4: E ect of contextualization on summarization.,0,,False
206,Columns are summarization algorithms and rows show ci-,0,,False
207,tation contextualization approaches. No Context uses only,0,,False
208,citations without any contextualization. Evaluation metrics,0,,False
209,are R,0,,False
210,(R ) scores. () shows statistically signi cant im-,0,,False
211,provement over the best baseline performance (p<0.05).,0,,False
212,KLSUM LexRank,0,,False
213,LSA,0,,False
214,SumBasic,0,,False
215,Method,0,,False
216,R1 R2 R1 R2 R1 R2 R1 R2,0,,False
217,No Context,0,,False
218,36.0 8.3 41.3 10.8 34.7 6.5 38.7 8.7,0,,False
219,VSM [4],0,,False
220,35.3 7.9 40.0 9.9 33.5 6.2 39.5 9.4,0,,False
221,BM25,0,,False
222,35.5 8.0 39.8 9.9 33.7 6.0 38.9 8.6,0,,False
223,DESM [12],0,,False
224,36.3 8.7 40.2 10.4 32.6 6.5 38.3 7.9,0,,False
225,LMD-LDA [10] 38.4 9.1 43.1 11.0 37.8 7.6 40.1 8.9,1,LM,True
226,QR [4],0,,False
227,39.9 10.2 43.8 11.7 38.9 8.0 40.1 8.6,0,,False
228,WEWiki WEBio,1,Wiki,True
229,39.7 10.2 42.7 11.8 38.0 8.0 40.2 9.2 41.7 11.7 45.6 13.8 40.3 9.1 42.4 12.6,0,,False
230,WEBio+rtrft 42.9 12.2 46.2 11.6 40.0 8.9 41.3 9.7 WEBio+dmn 44.0 13.4 47.3 13.6 42.3 10.4 44.0 11.7,0,,False
231,"summarization approach solely on the citations without any contextualization. The next 5 rows show the baselines and last 4 rows are our proposed contextualization methods. As shown, e ective contextualization positively impacts the generated summaries. For example, our best method is ""WEBio + dmn"" which signi cantly improves the quality of generated summaries in terms of R over the ones without any context. We observe that two low-performing baseline methods for contextualization according to Table 1 (""VSM"" and ""BM25"") also do not result in any improvements for summarization. Therefore, the intrinsic quality of citation contextualization has direct impact on the quality of generated summaries. These results further demonstrate that e ective contextualization is helpful for scienti c citation-based summarization.",0,,False
232,4 RELATED WORK,0,,False
233,"Related work has mostly focused on extracting the citation text in the citing article (e.g. [1]). In this work, given the citation texts, we focus on extracting its relevant context from the reference paper. Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20]. Our proposed model utilizes word embeddings and the domain knowledge. Embeddings have been recently used in general information retrieval models. Vuli and Moens [19] proposed an architecture for learning word embeddings in multilingual settings and used them in document and query representation. Mitra et al. [12] proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms. Ganguly et al. [7] used embeddings to transform term weights in a translation model for retrieval. Their model uses embeddings to expand documents and use co-occurrences for estimation. Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model. The most relevant prior work to ours is [4] where the authors approached the problem using a vector space model similarity ranking and query reformulations.",1,corpora,True
234,5 CONCLUSIONS,0,,False
235,Citation texts are textual spans in a citing article that explain certain contributions of a reference paper. We presented an e ective model for contextualizing citation texts (associating them with the,0,,False
236,appropriate context from the reference paper). We obtained statisti-,0,,False
237,cally signi cant improvements in multiple evaluation metrics over,0,,False
238,"several strong baseline, and we matched the human annotators",0,,False
239,precision. We showed that incorporating embeddings and domain,1,corpora,True
240,knowledge in the language modeling based retrieval is e ective for,0,,False
241,situations where there are high terminology variations between,0,,False
242,the source and the target (such as citations and their reference,0,,False
243,context). Citation contextualization not only can help the readers,1,ad,True
244,"to better understand the citation texts but also as we demonstrated,",0,,False
245,they can improve other downstream applications such as scienti c,0,,False
246,"document summarization. Overall, our results show that citation",0,,False
247,contextualization enables us to take advantage of the bene ts of,1,ad,True
248,"citation texts, while ensuring accurate dissemination of the claims,",0,,False
249,ideas and ndings of the original referenced paper.,0,,False
250,ACKNOWLEDGEMENTS,0,,False
251,We thank the three anonymous reviewers for their helpful com-,0,,False
252,ments and suggestions. This work was partially supported by Na-,0,,False
253,tional Science Foundation (NSF) through grant CNS-1204347.,0,,False
254,REFERENCES,0,,False
255,"[1] Amjad Abu-Jbara and Dragomir Radev. 2012. Reference scope identi cation in citing sentences. In NAACL-HLT. ACL, 80­90.",1,ad,True
256,[2] Arman Cohan and Nazli Goharian. 2015. Scienti c Article Summarization Using Citation-Context and Article's Discourse Structure. In EMNLP. 390­400.,0,,False
257,[3] Arman Cohan and Nazli Goharian. 2017. Scienti c document summarization via citation contextualization and scienti c discourse. International Journal on Digital Libraries (2017).,0,,False
258,"[4] Arman Cohan, Luca Soldaini, and Nazli Goharian. 2015. Matching Citation Text and Cited Spans in Biomedical Literature: a Search-Oriented Approach. In NAACL-HLT. 1042­1048.",0,,False
259,"[5] Anita de Waard and Henk Pander Maat. 2012. Epistemic modality and knowledge attribution in scienti c discourse: A taxonomy of types and overview of features. In Workshop on Detecting Structure in Scholarly Discourse. ACL, 47­55.",0,,False
260,"[6] Manaal Faruqui, Jesse Dodge, Kumar Sujay Jauhar, Chris Dyer, Eduard Hovy, and A. Noah Smith. 2015. Retro tting Word Vectors to Semantic Lexicons. In NAACL-HLT. Association for Computational Linguistics, 1606­1615.",0,,False
261,"[7] Debasis Ganguly, Dwaipayan Roy, Mandar Mitra, and Gareth J.F. Jones. 2015. Word Embedding Based Generalized Language Model for Information Retrieval. In SIGIR. ACM, 795­798.",0,,False
262,"[8] Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with similarity estimation. Computational Linguistics (2015).",0,,False
263,"[9] Kokil Jaidka, Muthu Kumar Chandrasekaran, Sajal Rustagi, and Min-Yen Kan. 2016. Overview of the CL-SciSumm 2016 Shared Task.. In BIRNDL@ JCDL.",0,,False
264,"[10] Fanghong Jian, Jimmy Xiangji Huang, Jiashu Zhao, Tingting He, and Po Hu. 2016. A simple enhancement for ad-hoc information retrieval via topic modelling. In SIGIR. ACM, 733­736.",1,ad-hoc,True
265,"[11] Qiaozhu Mei and ChengXiang Zhai. 2008. Generating Impact-Based Summaries for Scienti c Literature.. In ACL, Vol. 8. 816­824.",0,,False
266,"[12] Bhaskar Mitra, Eric Nalisnick, Nick Craswell, and Rich Caruana. 2016. A dual embedding space model for document ranking. CoRR arXiv:1602.01137 (2016).",0,,False
267,"[13] Ramesh Nallapati, Bowen Zhou, and Mingbo Ma. 2016. Classify or Select: Neural Architectures for Extractive Document Summarization. arXiv:1611.04244 (2016).",0,,False
268,"[14] Ani Nenkova and Kathleen McKeown. 2012. A survey of text summarization techniques. In Mining text data. Springer, 43­76.",0,,False
269,[15] Vahed Qazvinian and Dragomir R. Radev. 2008. Scienti c Paper Summarization Using Citation Summary Networks (COLING '08). 689­696.,1,ad,True
270,"[16] Anna Ritchie, Stephen Robertson, and Simone Teufel. 2008. Comparing Citation Contexts for Information Retrieval. In CIKM. ACM, 213­222.",0,,False
271,"[17] Ágnes Sándor and Anita De Waard. 2012. Identifying claimed knowledge updates in biomedical research articles. In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse. ACL, 10­17.",0,,False
272,[18] Simone Teufel and Marc Moens. 2002. Summarizing scienti c articles: experiments with relevance and rhetorical status. Computational linguistics 28 (2002).,0,,False
273,[19] Ivan Vuli and Marie-Francine Moens. 2015. Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings. In SIGIR.,0,,False
274,"[20] Stephen Wan, Cécile Paris, and Robert Dale. 2009. Whetting the appetite of scientists: Producing summaries tailored to the citation context. In JCDL. 59­68.",0,,False
275,"[21] Chengxiang Zhai and John La erty. 2004. A study of smoothing methods for language models applied to information retrieval. TOIS 22, 2 (2004), 179­214.",0,,False
276,1136,0,,False
277,,0,,False

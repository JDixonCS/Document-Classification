,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Open Relation Extraction for Support Passage Retrieval: Merit and Open Issues,0,,False
3,Amina Kadry,1,ad,True
4,"Dymatrix Consulting Stu gart, Germany a.kadry@dymatrix.de",1,ad,True
5,ABSTRACT,0,,False
6,Our goal is to complement an entity ranking with human-readable explanations of how those retrieved entities are connected to the information need. Relation extraction technology should aid in,1,ad,True
7,"nding such support passages, especially in combination with entities and query terms. is work explores how the current state of the art in unsupervised relation extraction (OpenIE) contributes to a solution for the task, assessing potential, limitations, and avenues for further investigation.",0,,False
8,"ACM Reference format: Amina Kadry and Laura Dietz. 2017. Open Relation Extraction for Support Passage Retrieval: Merit and Open Issues. In Proceedings of SIGIR '17, Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080744",1,ad,True
9,1 INTRODUCTION,1,DUC,True
10,"It seems obvious that technology for extracting the meaning of text, such as relation extraction, should lead to be er text retrieval methods. Yet, so far successes have been rare. is paper studies di erent ways of exploiting open relation extraction technology, assesses the potential for merit as well as open issues that inhibit further success for text-centric information retrieval.",1,ad,True
11,"Given sentences as input, open relation extraction (OpenIE) algorithms extract information on how knowledge base entities are related by analyzing the grammatical structure of each sentence.",0,,False
12,"To assess opportunities for future merit, we choose a text ranking task that operates on the sentence level and for which information about entities and relations is clearly pertinent: Retrieving explanations for how/why a knowledge base entity is relevant for an information need. is task is useful whenever entities are displayed along with web search results, such as entity cards [3].",0,,False
13,"Task (support passage ranking): A user enters information need Q; an external system predicts a ranking of relevant entities E. Our task is to, for every relevant entity ei  E, retrieve and rank K passages sik that explain why this entity ei is relevant for Q.",0,,False
14,"We postulate and study the following hypothesis. For a given entity ei , passages sik that explain a relevant relationship involving the",0,,False
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080744",1,ad,True
16,Laura Dietz,0,,False
17,"University of New Hampshire Durham, NH, USA dietz@cs.unh.edu",0,,False
18,"entity ei , are also good human-readable descriptions of why the entity is relevant for the information need Q.",1,ad,True
19,"Of course, conventional OpenIE algorithms have no knowledge of the information need Q. erefore, we study outcomes of relation extraction in superposition with retrieval models such as query likelihood. is paper studies how much OpenIE contributes to accomplishing this task. While there are many suggested approaches to OpenIE, we focus on the ClausIE system, which has been shown to be one of the best OpenIE methods on three established benchmark datasets [5].",0,,False
20,"Contributions. is paper features an in-depth study of the utility of a state-of-the-art OpenIE extraction system. We study how relation extraction can help, what are promising avenues for further research, and what are limitation of current relation extraction approaches that need to be overcome.",0,,False
21,"We demonstrate that OpenIE methods provide signi cantly better indicators for entity-centric passage ranking tasks, in contrast to low-level NLP methods such as part-of-speech tagging, named entity recognition, or dependency parsing. Despite these signi cant improvements, we quantify how limitations of current OpenIE systems are a ecting the quality of downstream information retrieval tasks.",0,,False
22,Outline. e state-of-the-art is summarized in Section 2. A short introduction to the relation extraction system ClausIE is given in Section 3. Section 4 details the feature-based learning-to-rank approach through which we evaluate the merit of OpenIE technology.,0,,False
23,antitative experimental results are provided in Section 5.,0,,False
24,2 RELATED WORK,0,,False
25,"Relations and retrieval. Given a relationship in a knowledge graph, Voskarides et al. [14] study the problem of nding human readable descriptions of that relationship. e relationship is given in the form ei , r , ej , where ei and ej are given entities, i.e., nodes in the knowledge graph and r is a type of a relationship, such as works for. Given this relationship, the task is to rank text passages sijk by how well they describe the relationship in human-readable form. is is the inverse problem to relation extraction [5, 11] where the task is to, given a textual description sijk , extract relational facts in the form ei , r , ej . None of these approaches take a further information need Q into consideration.",1,ad,True
26,"In the context of web queries, Schuhmacher et al. [12] apply supervised relation extraction to documents that are relevant for the information need Q and study how many of the extracted relations ei , r , ej are indeed relevant for Q. ey also analyze sentences, such as sijk , from which the relevant relation were extracted.",0,,False
27,Sentence retrieval. Previous work on retrieving entities and support sentences addresses the sentence retrieval problem. For,1,ad,True
28,1149,0,,False
29,Short Research Paper,0,,False
30,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
31,"example Blanco et al. [4] present a model that ranks entity support sentences with learning-to-rank. eir work focuses on features based on named entity recognition (NER) in combination termbased retrieval models. Many features based on using knowledge graph entities for text retrieval could also be applied here, such as the latent entity space model of Liu et al. [9].",0,,False
32,"Temporal event summarization. Temporal summarization is the task of identifying short and relevant sentences about a developing news event such as disasters, accidents, etc. [1] in a realtime se ing. Each event can be seen as a textual query that describes the event. For example, Kedzie et al. [8] propose to cluster sentences with salience predictions in the context of a named event within a multi-document summarization system. In line with many featurebased approaches, their system exploits term-based retrieval, query expansion, geographical and temporal relevance features.",0,,False
33,"estion answering. Given a question in natural language, estion Answering methods focus on providing correct and precise answers [13]. QA systems rst use IR techniques are used to retrieve passages that contain the answer. Next these are analyzed to extract a concise answer. Whenever the question includes an entity, a solution to our task is also applicable to the rst stage of question answering.",0,,False
34,3 FOUNDATION: CLAUSIE,0,,False
35,"ClausIE [5] is an OpenIE (unsupervised relation extraction) system designed for high-precision extractions. In contrast to previous OpenIE approaches, such as TextRunner [2] and Reverb [7], ClausIE distinguishes between the discovery of useful information from a given sentence and the representation of this information through multiple propositions. e system identi es di erent types of clauses, such as adverbial, complement, indirect object, and direct object. In contrast to many earlier approaches, ClausIE does not require labeled or unlabeled training data or global post-processing, making it applicable to open-domain retrieval tasks.",1,ad,True
36,"Example. Given the following sentence with token indices:1 "" e1 rules2 of3 golf4 are5 a6 standard7 set8 of9 regulations10 and11 procedures12 by13 which14 the15 sport16 of17 golf18 should19 be20 played21 .""",0,,False
37,"Phase 1. Clause types are extracted, representing constituents by their head word with token o set. For example:",1,ad,True
38,"Complementary clause SVC(C:set8, V:are5, S:rules2 , A?:of9 )",0,,False
39,Adverbial clause,0,,False
40,"SVA(V:played21, S:sport16, S:by13)",0,,False
41,Phase 2. Propositions of relation tuples are derived. For example:,0,,False
42,e rules2 of golf e rules2 of golf e rules2 of golf,0,,False
43,are5,0,,False
44,a standard set8 of9 regulations,0,,False
45,are5,0,,False
46,a standard set8 of9 procedures,0,,False
47,are5,0,,False
48,a standard set8,0,,False
49,the sport16 of golf should be played21 by13 a standard set8 of regulations the sport16 of golf should be played21 by13 a standard set8 of procedures,0,,False
50,"Whenever entity and query terms are contained in the same proposition, this sentence is likely to explain the connection between query and entity.",0,,False
51,1See demo at h ps://gate.d5.mpi-inf.mpg.de/ClausIEGate/ClausIEGate.,0,,False
52,4 APPROACH: RANKING SENTENCES FOR EXPLAINING ENTITY RELEVANCE,1,AP,True
53,"To study the utility of ClausIE for the support passage ranking task, we make use of a common two-step approach of 1) extracting candidate sentences and 2) using learning to rank (LTR) with a rich set of features, some of which are based on ClausIE's extractions.",0,,False
54,4.1 Extracting Candidate Sentences,0,,False
55,"In order to create a set of candidate sentences for a given query Q and entity ei , a corpus of documents that is pertinent to the entity is required. Any corpus could be used here, such as the ClueWeb corpus with entity links, as used by Schuhmacher et al. [12]. Assuming that OpenIE works best on grammatically wellformed sentences, we instead follow Voskarides et al. [14] and base this study on sentences from the Wikipedia article of the entity ei .",1,ClueWeb,True
56,4.2 Machine Learning (LTR),0,,False
57,Sentences are ranked with a list-wise learning-to-rank (LTR) approach implemented in RankLib.2 e weight parameter is learned by optimizing for the Mean-Average Precision metric (MAP) using coordinate ascent and 20 restarts. e LTR will learn a weighted feature combination to achieve the best possible ranking on the training set. Features of di erent categories are discussed below. We study feature sets for their merit by applying LTR on hold-out test data using cross-validation.,1,MAP,True
58,4.3 Sentence Ranking Features,0,,False
59,"Table 1 details the features which fall into these categories: Text features and quality features (Text) (1­8) capture the relevance and quality of the sentence at the term level. NLP features (9­16) are derived from part-of-speech (POS) and named entity recognition (NER) tags. ese have been speculated to not help IR. Dependency parse tree (DP) features (17­19) capture the grammatical structure of the sentence. We use the Stanford dependency parser [10] which is also used by the ClausIE system. Earlier works on relation extraction use the direct path between two entities in the dependency parse tree [15]. ClausIE features (20­43) capture the sentence's relation information about entity and query terms. Features are divided by positions of the relation proposition, i.e., subject, verb, and object. Relation quality indicators are included, such as the proposition length measured in tokens or the maximum constituent length (number of tokens in dependency subtree)--both averaged across all propositions extracted from this sentence.",0,,False
60,5 EXPERIMENTAL EVALUATION,0,,False
61,We conduct a series of experiments to determine the utility and issues of an available state-of-the-art OpenIE system. We focus on the task of ranking support sentences by how well they explain the relevance of a given entity ei for a given information need Q.,0,,False
62,"e study is divided according to three questions: 1) Under ideal conditions, could relation extractions help rank relevant passages? 2) What quality is achieved by a fully-automatic learning-to-rank",0,,False
63,2 lemurproject.org/ranklib.php,0,,False
64,1150,0,,False
65,Short Research Paper,0,,False
66,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
67,Table 1: Features used for support passage ranking.,0,,False
68,Feat. Description,0,,False
69,Text 1 sentence length measured in number of words 2 sentence position measured as a fraction of the document 3 fraction words that are stop words 4 fraction of query terms covered by sentence 5 sum of ISF of query terms (ISF is inverse sentence frequency) 6 average of ISF of query terms 7 sum of TF-ISF of query terms 8 number of entities mentioned,0,,False
70,NLP 9-12 13 14-16,0,,False
71,for nouns/verbs/adjectives/adverbs: fraction of words with POS tag whether sentence contains a named entity for NER types PER/LOC/ORG: whether NER of type is contained,1,ad,True
72,DP 17 number of edges on the path between two entities in dependency tree 18 indicator whether path goes through root node 19 indicator whether path goes through query term,0,,False
73,ClausIE 20 whether ClausIE generated an extraction from this sentence,0,,False
74,21-27 for all seven clause types: whether clause of this type is extracted 28 proposition length measured in tokens 29 maximum constituent length (size of dependency tree) in proposition,0,,False
75,30-32 for subject/object/both: if another entity is in subject and/or object position of the proposition,0,,False
76,33-34 for subject/object position: if given entity is in position of proposition 35-36 for subject/object position: if any entity is in position of proposition 37-38 for subject/object position: if an entity link is in position of prop. 39-41 for subject/verb/object position: if a query term (ignoring stopwords),0,,False
77,is in position of proposition 42-43 for subject/object position: if a named entity (NER) is in position of,0,,False
78,proposition,0,,False
79,approach with OpenIE features (cf. Section 4)? 3) Which open issues of OpenIE systems inhibit the application to text ranking tasks?,0,,False
80,5.1 Test collection,0,,False
81,"For this study we build a test collection3 for 95 support passage rankings (one per query and entity). We use a subset of ten 2013/2014 TREC Web track queries and (up to) ten relevant entities E for these topics, which are taken from the REWQ gold standard.4 To focus this study on grammatically sound and well-wri en documents, we use Wikipedia articles of each relevant entity as a basis for candidate sentences. ese are taken from the 2012 Wikipedia Wex dump. To obtain a base set for assessment, these sentences are processed by the ClausIE extraction system.",1,TREC,True
82,"We ask assessors to imagine they were to write a knowledge article on the topic Q, on which they were to include information about the given entity ei . Assessors are asked to mark passages that would be suitable support passages for the article by answering the following question:",0,,False
83,AQ1) Explanation: Does the sentence explain the relevance of entity ei ?,0,,False
84,"is way we obtain candidate sentences for 95 query-entity pairs as input topics. We arrive at a total of 31,397 assessed sentences with 2,906 relevant support passages of entity relevance. O en, the relevant aspects of a relevant entity are not noteworthy enough to be described in the entity's article [6]. is leads to 20 query-entity pairs that don't contain any explanations of entity-relevance. ese",1,ad,True
85,3data set available: www.cs.unh.edu/dietz/appendix/openie4ir 4h p://mschuhma.github.io/rewq/,0,,False
86,Table 2: Performance of AQ1­5 as predictors for explanations and Pearson correlation .  signi cance over Qterm,0,,False
87,Prec() Recall ,0,,False
88,Count,0,,False
89,Relation,0,,False
90,0.46 ±0.05 0.28 ±0.03 0.27,0,,False
91,1767 (8%),0,,False
92,Rel rel,0,,False
93,0.52 ±0.05  0.21 ±0.03 0.52,0,,False
94,935 (4%),0,,False
95,ClausIE,0,,False
96,0.45 ±0.05 0.20 ±0.02 0.33,0,,False
97,1172 (5%),0,,False
98,ClausIE rel,0,,False
99,0.49 ±0.05  0.14 ±0.02 0.49,0,,False
100,636 (3%),0,,False
101,Qterm (),0,,False
102,0.38 ±0.04 0.49 ±0.04 0.47,0,,False
103,4476 (20%),0,,False
104,Name,0,,False
105,0.33 ±0.05 0.43 ±0.04 0.35,0,,False
106,6173 (27%),0,,False
107,Table 3: Results on ranking of sentences explaining entity relevance with LTR.,0,,False
108,Method,0,,False
109,Full Text NLP DP ClausIE,0,,False
110,MAP (),1,MAP,True
111,0.44 ±0.03 0.42* ±0.03 0.31* ±0.03 0.33* ±0.03 0.41* ±0.03,0,,False
112,Hurt,0,,False
113,­ 23 39 43 25,0,,False
114,Helped,0,,False
115,­ 9 11 5 11,0,,False
116,Ablation,0,,False
117,MAP,1,MAP,True
118,Full-TEXT Full-NLP Full-DP Full-ClausIE,0,,False
119,0.41 ±0.03 0.43 ±0.04 0.43 ±0.04 0.43 ±0.03,0,,False
120,"are excluded from this study, leaving 75 query-entity pairs and 22,731 support passage annotations of which 2,906 are marked as relevant according to AQ1.",0,,False
121,"In order to study characteristics of sentences in relation to AQ1, we further ask annotators to assess the following questions for each sentence sik , per query Q and entity ei : AQ2) Relation: Does the sentence mention any relationship involving ei ? AQ3) Rel rel: Is this relationship relevant for the explanation? AQ4) ClausIE: Does ClausIE extract a valid relationship from sentence? AQ5) ClausIE rel: Is ClausIE's extraction relevant for the explanation?",0,,False
122,We study these annotations in combination with two heuristics: Qterm: Does the sentence include query terms (stopwords ignored)? Name: Does the sentence include the entity's name?,0,,False
123,5.2 Experiment 1: Relations and Relevance,0,,False
124,"By casting the result of every annotation question (Relation, Rel rel, ClausIE, ClausIE rel) as well as heuristics (Qterm, Name) as a random variable, we study both the Pearson correlation  of these predictors and the ground truth (Explanation / AQ1) as well as their predictive power as measured by set precision and recall in Table 2.",0,,False
125,"ese demonstrate that good explanations are found in sentences that express a relevant relation of the entity (Rel rel / AQ3), re ected in the highest Pearson correlation of 0.52, as well as the highest precision of 0.52. Using Rel rel as a predictor is signi cantly5 be er in terms of precision than using the Qterm heuristic (which achieves precision of 0.38). However, the Qterm heuristic achieves a much higher recall of 0.49. is suggests that combining query terms and relation extractions is a worthwhile avenue for investigation.",0,,False
126,"Of course, this requires an automatic approach for distinguishing relevant from non-relevant relation expressions. On the pessimistic side, only half of all extracted relations are indeed relevant. On optimistic side, macro-avg precision drops only mildly from 0.52 for relevant relations (Rel rel / AQ3) to 0.46 for any relation (AQ2) and 0.45 for ClausIE extractions (AQ4). We speculate that an OpenIE relation extractor can also serve as a quality indicator for passages as it is sensitive towards well-formed sentences.",0,,False
127,"5Paired-t-test with  , 5%.",0,,False
128,1151,0,,False
129,Short Research Paper,0,,False
130,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
131,MAP,1,MAP,True
132,0.5,0,,False
133,0.4,0,,False
134,0.3,0,,False
135,0.2,0,,False
136,0.1,0,,False
137,0,0,,False
138,Full,0,,False
139,Text,0,,False
140,NLP,0,,False
141,DP,0,,False
142,ClausIE,0,,False
143,Figure 1: Results on ranking of sentences explaining entity relevance: full vs. subsets,0,,False
144,5.3 Experiment 2: Evaluation through LTR,0,,False
145,Next we demonstrate that features derived from ClausIE's extractions can be e ectively used to train a learning-to-rank (LTR) method for ranking support passages as detailed in Section 4.,0,,False
146,"e Full feature set, given in Table 1, is divided into four feature sets by category: Text, NLP, DP, and ClausIE. We compare our approach using all features (Full) versus each feature set individually. Statistically signi cant improvements of the Full using a paired t-test ( ,"" 5%) are marked with *. We additionally perform an ablation study by removing one feature subset at a time (Full-category) from the Full feature set, to study redundancy in the feature space.""",1,ad,True
147,"For learning to rank, approaches are evaluated with 5-fold crossvalidation, where all rankings associated with the same query (but di erent entities) are assigned to the same fold. e ranking performance is measured in mean-average precision (MAP) with respect to the ground truth of a sentence explaining the relevance of the entity for the query (AQ1). Results are presented in Table 3 and Figure 1. Unjudged sentences are considered non-relevant. Results given in Table 3 show that the Full method outperforms all other methods signi cantly with a MAP of 0.36. Individually, the strongest feature subsets are ClausIE and Text, and the ablation study con rms that they provide complementary merit.",1,MAP,True
148,"Despite issues due to precision-orientation of OpenIE systems (more details about this in the next section), we obtain signi cant improvements with respect to the recall-oriented evaluation metric MAP. is demonstrates that there is merit in further investigating high-level NLP extractions based on OpenIE. is is in contrast to other kinds of NLP extractions such as POS tags, NER tags, and dependency parse information which are signi cantly worse indicator for support passages.",1,MAP,True
149,5.4 Experiment 3: Open Issues,0,,False
150,"Many NLP-oriented systems are tuned for high precision at the expense of recall. While this is a desirable property in the context of knowledge base population, it may impose limitations for information retrieval tasks.",0,,False
151,"Among all sentences that express a relation, ClausIE is missing this relation in 32% of the cases. Additionally, only half of the sentences with relation expressions actually actually contain a relation that is relevant for the query-entity pair (con rming ndings of Schuhmacher et al. [12]). Together this results in only 636 sentences with relevant ClausIE extractions (3%) of all 22731 annotated sentences. In contrast, our data set contains 2906 sentences (13%) with explanations of relevance.",0,,False
152,"While there are ClausIE extractions for 9951 sentences, only",0,,False
153,1172 constitute a correct extraction. Comparing this to the 2906,0,,False
154,true relevant sentences demonstrates that a perfect recall is not,0,,False
155,obtainable. Let us consider an optimistic thought experiment where,0,,False
156,all sentences with correct ClausIE extractions are relevant. An ideal,0,,False
157,"ranking, which places all relevant sentences rst, would obtain a",0,,False
158,MAP,1,MAP,True
159,value,0,,False
160,of,0,,False
161,1172 2906,0,,False
162,",",0,,False
163,0.41,0,,False
164,(theoretical,0,,False
165,upper,0,,False
166,bound).,0,,False
167,is upper,0,,False
168,bound happens to coincide with the actual MAP achieved by the,1,MAP,True
169,"ClausIE feature set alone, MAP 0.41, cf. Table 3. We conclude that",1,MAP,True
170,our approach obtains an optimal ranking under limitations imposed,0,,False
171,by the o -the-shelf OpenIE system. Improving coverage of OpenIE,0,,False
172,systems is likely to translate to immediate quality improvements,0,,False
173,for text-ranking tasks.,0,,False
174,6 CONCLUSION,0,,False
175,"We study the utility of OpenIE technology ranking sentences by how well they explain the relevance of a given entity for a query. Based on manual assessments and evaluation through a learning-torank framework, the study demonstrates that signi cant improvements are achieved by combining relation features with query and entity matches. While we demonstrate the merit of an OpenIE extraction system, we also quantify losses through limitations of current OpenIE systems. we hope this study stimulates work on relation extraction systems that are designed of information retrieval tasks.",0,,False
176,Acknowledgements,0,,False
177,"is research was performed as part of a Master's thesis at Mannheim University, Germany. We are grateful to Rainer Gemulla for providing access to the ClausIE system. is work was funded in part by a scholarship of the Eliteprogramm for Postdocs of the Baden-Wu¨r emberg Sti ung.",1,ad,True
178,REFERENCES,0,,False
179,"[1] J. Aslam, F. Diaz, M. Ekstrand-Abueg, R. McCreadie, V. Pavlu, and T. Sakai. Trec 2014 temporal summarization track overview. Technical report, 2015.",1,ad,True
180,"[2] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open information extraction from the web. In Proc. of IJCAI, 2007.",1,ad,True
181,"[3] A. Berntson et al. Providing entity-speci c content in response to a search query, Mar. 8 2012. US Patent App. 12/876,638.",0,,False
182,"[4] R. Blanco and H. Zaragoza. Finding support sentences for entities. In Proc. of SIGIR, 2010.",0,,False
183,"[5] L. Del Corro and R. Gemulla. Clausie: clause-based open information extraction. In Proc. of WWW, 2013.",0,,False
184,"[6] L. Dietz, A. Kotov, and E. Meij. Tutorial on utilizing knowledge graphs in text-centric information retrieval. In Proc. of WSDM, 2017.",0,,False
185,"[7] A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information extraction. In Proc. of EMNLP, 2011.",1,ad,True
186,"[8] C. Kedzie, K. McKeown, and F. Diaz. Predicting salient updates for disaster summarization. In Prof. of ACL, 2015.",0,,False
187,"[9] X. Liu and H. Fang. Latent entity space: a novel retrieval approach for entitybearing queries. Information Retrieval Journal, 18(6):473­503, 2015.",0,,False
188,"[10] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky. e Stanford CoreNLP natural language processing toolkit. In Proc. of ACL, 2014.",0,,False
189,"[11] B. Roth, T. Barth, G. Chrupa la, M. Gropp, and D. Klakow. Relationfactory: A fast, modular and e ective system for knowledge base population. In Proc. of EACL, 2014.",0,,False
190,"[12] M. Schuhmacher, B. Roth, S. P. Ponze o, and L. Dietz. Finding relevant relations in relevant documents. In Proc. of ECIR, 2016.",0,,False
191,"[13] M. Surdeanu, M. Ciaramita, and H. Zaragoza. Learning to rank answers to nonfactoid questions from web collections. Computational Linguistics, 37(2):351­383, 2011.",0,,False
192,"[14] N. Voskarides, E. Meij, M. Tsagkias, M. de Rijke, and W. Weerkamp. Learning to explain entity relationships in knowledge graphs. In Proc. of ACL, 2015.",0,,False
193,"[15] L. Yao, A. Haghighi, S. Riedel, and A. McCallum. Structured relation discovery using generative models. In Proc. of EMNLP, 2011.",0,,False
194,1152,0,,False
195,,0,,False

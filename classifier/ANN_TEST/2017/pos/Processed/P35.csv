,sentence,label,data,regex
0,Session 1A: Evaluation 1,1,Session,True
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Can Deep E ectiveness Metrics Be Evaluated Using Shallow Judgment Pools?,0,,False
3,Xiaolu Lu,0,,False
4,"RMIT University Melbourne, Australia",0,,False
5,Alistair Mo at,0,,False
6,"e University of Melbourne Melbourne, Australia",0,,False
7,J. Shane Culpepper,0,,False
8,"RMIT University Melbourne, Australia",0,,False
9,ABSTRACT,0,,False
10,"Increasing test collection sizes and limited judgment budgets create measurement challenges for IR batch evaluations, challenges that are greater when using deep e ectiveness metrics than when using shallow metrics, because of the increased likelihood that unjudged documents will be encountered. Here we study the problem of metric score adjustment, with the goal of accurately estimating system performance when using deep metrics and limited judgment sets, assuming that dynamic score adjustment is required per topic due to the variability in the number of relevant documents. We seek to induce system orderings that are as close as is possible to the orderings that would arise if full judgments were available.",1,ad,True
11,"Starting with depth-based pooling, and no prior knowledge of sampling probabilities, the rst phase of our two-stage process computes a background gain for each document based on rank-level statistics. e second stage then accounts for the distributional variance of relevant documents. We also exploit the frequency statistics of pooled relevant documents in order to determine a threshold for dynamically determining the set of topics to be adjusted. Taken together, our results show that: (i) be er score estimates can be achieved when compared to previous work; (ii) by se ing a global threshold, we are able to adapt our methods to di erent collections; and (iii) the proposed estimation methods reliably approximate the system orderings achieved when many more relevance judgments are available. We also consider pools generated by a two-strata sampling approach.",1,ad,True
12,KEYWORDS,0,,False
13,Test collection; relevance assessment; pooling; shallow judgments.,0,,False
14,1 INTRODUCTION,1,DUC,True
15,"Batch evaluations are performed by calculating a metric score based on a set of judged documents. Despite ve decades of success, this ""Cran eld/TREC"" paradigm also faces challenges. One of the key issues is that realistic collection sizes now greatly exceed the budget available to perform human judgments. ""Pooling-to-depth-d"" is one widely-used approach [25], in which documents in the union of the top-d lists returned from a set of contributing systems are judged, but other documents are not. e pooling depth d is ideally",1,ad,True
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080793",1,ad,True
17,"determined by the needs of the e ectiveness metric to be used, but in reality is also constrained by the experimental budget. Although pooling has identi ed the majority of relevant documents in earlier collections [30], there is growing evidence that this is not true for the web collections that are now the norm [2, 11].",0,,False
18,"e uncertainty in e ectiveness measurement in large collections is the key emphasis of our work here, focusing on how to estimate evaluation scores when reduced judgment sets are used.",0,,False
19,"is is not a new problem, and a range of prediction mechanisms have been proposed [1, 22, 23, 27, 28], mainly focusing on predicting system orderings. We focus on prevailing pool-based test collection construction methods, as these best match our methodology, and on deep evaluation metrics, noting that pool depth has a lesser impact on shallow evaluation metrics such as ERR [6]. Alternative approaches using direct sampling exploit prior knowledge of the probability of each document being judged, and are applied during pool construction, on the assumption that all systems requiring measurement have been identi ed. But that process makes it di cult to infer scores for any new systems that get added later. On the other hand, pooling selects documents based on the assumption that top-ranked documents are both more likely to be relevant, and hence more in uential in computing e ectiveness scores. In this more general se ing there is no a priori knowledge of the system scores, and while that means that regression cannot be applied, new systems can be considered. We also argue that the decision to apply score adjustment should be done on a per topic basis. Robertson [17] notes that topics vary in terms of the number of potential relevant documents, and that this can have a signi cant impact on evaluation scores. Dynamically identifying when to perform score adjustment is thus a second challenge that must be considered.",1,ad,True
20,"e end objective of an evaluation goes beyond the metric scores, of course; in the end we wish to be able to compare and choose between systems, meaning that it is also important that the score estimations are concordant with the system orderings that would arise if full knowledge were available. Since the la er is measured according to a reference point which may not be known, there is no clear optimization goal, another complication that we address.",1,ad,True
21,"ese various considerations lead to two questions: Research estion 1: For each topic, how can we estimate the evaluation score of a system using a shallow pooling depth? Research estion 2: Can stable system rankings be achieved using the adjusted scores? In considering these two questions, we perform experiments using several di erent ad-hoc test collections and a range of modeled pool depths. Our results show that: (i) a two-stage optimization framework generates more accurate score estimations than previous approaches; (ii) topic-based adjustment thresholds identi ed using early TREC collections allow additional improvements in",1,ad,True
22,35,0,,False
23,Session 1A: Evaluation 1,1,Session,True
24,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
25,"estimation accuracy; and (iii) the adjusted evaluation scores yield be er approximations of the ""true"" system rankings than do the unadjusted scores. In addition to standard pooling methods, we also consider two-strata sampling [24].",1,ad,True
26,2 RELATED WORK,0,,False
27,"Incomplete Judgments and Evaluation Bias. Two types of bias arise in batch evaluations: pooling depth bias [19] and system bias [20]. e rst is caused by the use of shallow pools, and the second by performance underestimation for systems that did not contribute to the pool. Both are a result of documents appearing in the ranking for which judgments are not available. e simplest response to unjudged documents is to stipulate that anything not examined in the pooling process is not relevant. Zobel [30] challenged this notion using a series of leave-one-out experiments, and showed for several early TREC collections that while it was likely there were indeed further relevant documents that had not been identi ed, system bias was nevertheless within acceptable levels. However, on more recent web collections, there is growing evidence that this situation may not be assumed [2, 11].",1,TREC,True
28,"Other responses to the issue of unjudged documents have been proposed. Buckley and Voorhees [3] describe BPref, which balances the rank positions of documents judged as non-relevant and relevant, and ignores unjudged documents. In a related approach, Sakai [18] considers condensed lists, which compute scores using a",0,,False
29,"ltered ranking containing only judged documents, and nds that standard metrics give higher discriminative ratios than achieved by BPref. However, the condensed list methodology has not been shown to be stable when comparing relative system orderings using Kendall's  or discrimination ratios [19, 20]. Score Estimation / Collection Construction. Documents without judgments are not distributed randomly in ranked result lists.",0,,False
30,"erefore, sample-based collection construction approaches have been suggested to support statistical inference [1, 22, 23, 27, 28]. Yilmaz and Aslam [27] present an inferred Average Precision (AP) metric that uses an expectation model, and can be coupled with a sampling process to select documents to be judged. eir InfAP metric uses uniform random sampling during collection construction. When compared with standard TREC-style pooling, the results produced by InfAP were strongly correlated with AP. However, this sampling process is random, and retrieval systems return documents in rank order, meaning that relevant documents are more likely to be returned at the top of the list if the system is e ective.",1,AP,True
31,"e use of non-random sampling has also been explored. Yilmaz et al. [28] extended their previous work, proposing metrics XInfAP and XInfNDCG, based on a strati ed sampling process. In contrast, Aslam et al. [1] consider the use of importance sampling for the same task, proposing statAP, which estimates the expectation of AP. e key di erence between InfAP and statAP is that statAP is designed to generate the optimal distribution estimates using all of the contributing systems. Voorhees [24] further examines the e ect of sampling methods on inferred metrics.",1,AP,True
32,"A recent study by Schnabel et al. [23] also used importance sampling, this time in conjunction with Discounted Cumulative Gain (DCG). e key idea in their approach was to use the probability of",0,,False
33,"relevance with respect to rank information when determining the sample distribution. ey provide an analysis on how to derive the optimal sampling distributions under di erent system comparison se ings [22]. Using the proposed framework, any metric can be reformulated in the form of expectations and be estimated directly from the sampling process. Mo at et al. [14] had earlier examined targeted pooling and document judgment order in conjunction with the Rank-Biased Precision (RBP) metric.",1,ad,True
34,"Score Estimation Based on Pooling Methods. Estimation in traditional pooling techniques has also received considerable a ention [4, 7, 9, 10, 16, 26]. Most existing techniques focus on adjusting the bias which exists between pooled and unpooled systems. Webber and Park [26] proposed two methods to perform score adjustment.",1,ad,True
35,"e rst uses an adjustment factor, which is computed from the contributing systems. Each contributing system has an error value assigned when it is le out of the training process, and the mean of those values is applied to any new system to be measured. e second approach requires a set of common topics with ""complete judgments"". A similar calculation is performed in order to obtain the adjustment factor, but restricted to the subset of common topics. To obtain additional adjustment accuracy, Webber and Park introduced randomization to build an unbiased estimator.",1,ad,True
36,"Recent work by Lipani et al. [9] using a precision metric outperformed the rst method of Webber and Park. eir ""anti-Precision"" measurement is similar in spirit to the residual computed by RBP [15]. Lipani et al. [9] compute adjustment factors using the leaveone-run out methodology, and then improve their previous approach by computing an average distribution [10].",1,ad,True
37,"e closest work to our current approach is that of Ravana and Mo at [16]. ey focus on pooling depth bias, proposing three methods to estimate the e ect of unjudged documents, using the residual that can be computed for weighted-precision metrics [15].",0,,False
38,"eir rst method uses a background estimation based on a static scaling factor; the second assumes that the percentage of relevant but unjudged documents can be derived directly from the known score component; and the third uses a parametric combination of the rst two. Lu et al. [12] subsequently de ne the same problem in terms of the anticipated e ectiveness gain as a function of ranking depth. Based on di erent assumptions derived from the underlying gain distributions, they propose several alternatives, and compare the estimates achieved. ey empirically show that relatively simple models can be used to estimate gain values for unjudged documents.",0,,False
39,"An approach due to Bu¨ cher et al. [4] directly predicts the relevance of unjudged documents, using two types of classi ers trained with the existing pool to predict the relevance of unjudged document in a new system. Although the e ectiveness of the classi er is low, their results show that classi cation does help maintain similar system orderings when measured via Kendall's  . Jayasinghe et al. [7] take a similar approach, and show that reliably predicting document relevance is o en di cult.",0,,False
40,3 PRELIMINARIES AND BASELINES,0,,False
41,"Pools. Figure 1 shows the construction of a pool for one topic, with sj,i (on the le ) corresponding to the j th document in the run for system Si , and with the corresponding documents (on the",0,,False
42,36,0,,False
43,Session 1A: Evaluation 1,1,Session,True
44,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
45,T,0,,False
46,S1 s11,0,,False
47,S2 s12,0,,False
48,s21 s31 s41 s51 ... sd1 ...,0,,False
49,s22 s32 s42 s52 ... sd2 ...,0,,False
50,sk1 sk2,0,,False
51,S3 s13 s23 s33 s43 s53 ... sd3 ... sk3,0,,False
52,...,0,,False
53,... ... ... ... ... ... ... ... ...,0,,False
54,Sn s1n s2n s3n s4n s5n ... sdn ... skn,0,,False
55,Sn+1 s1n+1 s2n+1 s3n+1 s4n+1 s5n+1,0,,False
56,... sdn+1,0,,False
57,... skn+1,0,,False
58,Rank,0,,False
59,1 2 3 4 .5. . .d. .,0,,False
60,k,0,,False
61,S1,0,,False
62,D1 D3 D2 D7 D6 ... D10 ... D49,0,,False
63,Complete Set J,0,,False
64,J,0,,False
65,S2 S3 . . . Sn Sn+1 ,0,,False
66,D2 D1 D6 D5 D3 ... D6 ...,0,,False
67,D3 D7 D2 D8 D5 ... D1 ...,0,,False
68,... ... ... ... ... ... ... ...,0,,False
69,D3 D4 D7 D2 D1 ... D5 ...,0,,False
70,D4 D2 D8 D1 D9 ... D3 ...,0,,False
71,D50 D30 . . . D18 D6,0,,False
72,System Matrix: S,0,,False
73,M@k: M1 M2 M3 . . . Mn Mn+1,0,,False
74,Figure 1: Pooling process for a topic T . e le matrix is a rankbased representation; the right one shows the equivalent document,0,,False
75,"identi ers. e two boxes indicate two possible sets of pooled documents, the larger to depth d, and the smaller to some depth d < d. e metric M is evaluated at some depth k, where k may or may not be less than or equal to d or d .",0,,False
76,"right), each potentially retrieved by multiple systems at di erent rank positions. Hence, a document D can also be represented by its rank-position information, D, (pD,1, pD,2, . . . , pD,n ) , in which pD,i is the rank returned for D by contributing system Si . Metric evaluation to depth d for systems S1 to Sn requires that the documents in the set , {D | minni,""1 pD,i  d } be judged. at is, both matrices can be further mapped to a matrix of relevance Rd×n in which rj,i is a relevance, or gain, value.""",0,,False
77,"If there is insu cient judgment volume available, a shallower pool might be formed, with documents D for which minni,""1 pD,i > d not judged, and elements in Rd×n le without values. Unknown relevance labels may also arise for a new system Sn+1, regardless of the pooling and evaluation depths. In this framework, the rst of""",0,,False
78,the two research questions proposed in Section 1 can be split into,0,,False
79,"two aspects: (1a) for each topic, how do we estimate the scores of",0,,False
80,a system using a set of shallow pooled judgments; and (1b) which,0,,False
81,topics may assume that unjudged documents are not relevant and,0,,False
82,which ones should not.,0,,False
83,One method for dealing with missing data is to compute expected,0,,False
84,"gains as a function of retrieval rank [12]. However, modeling rele-",0,,False
85,vance as a function of rank only considers the LHS representation,0,,False
86,"in Figure 1, and ignores that documents can have multiple ranks.",0,,False
87,Addressing that limitation is a key part of our work here.,0,,False
88,"Metric Residuals. Suppose that for some topic T , a set of docu-",0,,False
89,ments results from pooling to depth d (Figure 1). Consider the,0,,False
90,"ranked list returned by some system Si ,"" (s1,i , s2,i , . . . , sk,i ) and let rj,i represent the gain of the document at rank j, normally (but not necessarily) a value in [0, 1]. e e ectiveness Mi of Si when computed to depth k by a weighted-precision metric M is:""",0,,False
91,k,0,,False
92,"Mi ,"" M@k (Si , ) "",",0,,False
93,"rj,i · WM (j) ,",0,,False
94,(1),0,,False
95,"j ,1",0,,False
96,"sj,i ",0,,False
97,"where WM (j) is the weight assigned by the metric at depth j, with",0,,False
98," j ,1",0,,False
99,WM,0,,False
100,(j,0,,False
101,),0,,False
102,",",0,,False
103,"1 [13, 15]; and where the restriction sj,i",0,,False
104,is,0,,False
105,"required to ensure that only de ned values of rj,i are included.",0,,False
106,"A corresponding residual i can then be computed, quantifying the",0,,False
107,metric weighting associated with the unjudged documents [15]:,0,,False
108,k,0,,False
109,"i ,",0,,False
110,rmax · WM (j ) +,0,,False
111,"rmax · WM (j ) ,",0,,False
112,(2),0,,False
113,"j ,""1 sj,i""",0,,False
114,"j,k +1",0,,False
115,"where rmax is the maximum possible gain. Either term might be zero, depending on whether Si contributed to the pool, on the relationship between the evaluation depth k and the pooling depth d, and on whether WM (j) ,"" 0 when j > k, as occurs with truncated""",0,,False
116,metrics.,0,,False
117,ere is a three-way tension between metric depth (quanti ed as,0,,False
118,the expected point reached in the ranking in the corresponding user,0,,False
119,"model [13]); accuracy of measurement, captured by the residual; and the cost | | of performing the judgments. For example, in RBP",0,,False
120,"the tail residual (the second component in Equation 2) is given by pk , and if p ,"" 0.5, k  10 is su cient. Similar calculations apply""",0,,False
121,"for ERR [6]. But in either case, the rst term of Equation 2 might be",0,,False
122,"non-zero for new runs. Furthermore, even the tail residuals might become large for deeper metrics, for example, RBP with p , 0.95.",0,,False
123,"Truncated (that is, non-in nite) metrics such as Scaled DCG at",0,,False
124,"depth 100, SDCG@100, also require deep pools if the residual is to be moderately bounded. e same requirement must apply by",0,,False
125,implication to other deep metrics such as Average Precision.,0,,False
126,"Problem De nition. Consider a set of n contributing systems {S1, S2, . . . , Sn }. For one topic T , let d be a pooling depth at which",0,,False
127,it is believed that a majority of the relevant documents occurring,0,,False
128,in the runs of those systems have been identi ed. We refer to this,0,,False
129,"set of judgments as the ""complete set"". Let d < d be a shallower pooling depth, with judgments forming an incomplete set  . Given a weighted precision metric M, the e ectiveness score of Si evaluated using M and to depth d is denoted as Mi ,"" M@d (Si , ), with a residual of i . Similarly, an estimated metric score based on judgments to depth d < d, is denoted as M^ i "","" Ed (M@d (Si , )) where Ed (·) is an estimation function for the same metric at depth d. Following Lu et al. [12], the estimation error i is then de ned as:""",0,,False
130,i,0,,False
131,",",0,,False
132,Mi 0,0,,False
133,-,0,,False
134,M^ i,0,,False
135,"if M^ i < Mi ,",0,,False
136,"if Mi  M^ i  Mi + i ,",0,,False
137,(3),0,,False
138,M^ i - (Mi + i ) if M^ i > Mi + i .,0,,False
139,"is de nition respects the residual range, and only gives non-zero",0,,False
140,"values if the estimated e ectiveness falls outside the score range arising from the use of at depth d. e challenge is to develop a method Ed (·) that estimates the depth-d e ectiveness score of a contributing system based on a subset of the judgments, and minimizes the average value of i .",0,,False
141,"In the experiments in Section 6 we report the RMS aggregate of the i values computed, across systems and topics; and, as a ""percentage accurate"", the fraction of those values that are zero.",0,,False
142,"Lower-Bound Estimation. A simple approach is to take Ed (x ) ,"" x, that is M^ i "","" Mi , where Mi is the score for system Si when evaluated using , and assert that documents outside do not""",0,,False
143,alter the score. Taking unjudged documents to be not relevant is the,0,,False
144,"normal default in batch evaluation, and is a valid estimator. But the",0,,False
145,"estimation quality depends on the breadth of the pool, and whether",1,ad,True
146,a majority of relevant documents have been identi ed. When there,0,,False
147,37,0,,False
148,Session 1A: Evaluation 1,1,Session,True
149,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
150,J,0,,False
151,Fitted Gain,0,,False
152,Rank ,0,,False
153,1,0,,False
154,S1 s11,0,,False
155,2 3 4 .5. . .d. .,0,,False
156,s21 s31 s41 s51 ... sd1 ...,0,,False
157,k,0,,False
158,sk1,0,,False
159,S2 s12 s22 s32 s42 s52 ... sd2 ... sk2,0,,False
160,S3 s13 s23 s33 s43 s53 ... sd3 ... sk3,0,,False
161,...,0,,False
162,... ... ... ... ... ... ... ... ...,0,,False
163,Sn s1n s2n s3n s4n s5n ... sdn ... skn,0,,False
164,Sn+1 s1n+1 s2n+1 s3n+1 s4n+1 s5n+1,0,,False
165,... sdn+1,0,,False
166,... skn+1,0,,False
167,Gain Vector:,0,,False
168,"ni,1 r1i/n ni,1 r2i/n ni,1 r3i/n ni,1 r4i/n",0,,False
169,"ni,1 r5i/n",0,,False
170,g,0,,False
171, Fitting,0,,False
172,G1(k),0,,False
173,g011 g021 g031 g041 g051 ... g0d1 ...,0,,False
174,g0k1,0,,False
175,G2(k),0,,False
176,g012 g022 g032 g042 g052 ... g0d2 ... g0k2,0,,False
177,G3(k),0,,False
178,g013 g023 g033 g043 g053 ... g0d3 ... g0k3,0,,False
179,...,0,,False
180,... ... ... ... ... ... ...,0,,False
181,...,0,,False
182,Figure 2: Overview of rank-based estimation for a single topic. e judgments are used to infer an observed gain vector g; each,0,,False
183,of a set of m functions G (k ) is then ed to g.,0,,False
184,"are still many unjudged relevant documents, this estimator results in underestimation of system performance. Reasonably good rank correlation between the estimates and the true score over a set of systems can be obtained, but there is no guarantee that the performance of each of the systems has been accurately measured.",0,,False
185,"Interpolative Estimation. A second baseline is provided by the RM interpolative estimator proposed by Ravana and Mo at [16], who scale the metric score across the residual, assuming that unjudged documents are relevant at the same rate as judged ones:",0,,False
186,"M^ i , Mi /(1 - i ) .",0,,False
187,(4),0,,False
188,"A collection-based background probability is used when i , 1. is estimator assumes that gain is accrued at the same rate across",0,,False
189,"all of the documents retrieved by the system, both judged and unjudged. Although more robust than the LB estimator, it does not allow the likelihood of relevance to decrease as the pool is extended from d to d.",0,,False
190,"Rank-Based Estimators. Lu et al. [12] introduce rank-based estimation, illustrated in Figure 2. e judgments are used to estimate expected gain as a function of rank on a per-topic basis. ose rank-based fractional gain predictions are then used for unjudged documents ­ interpolated at depths up to d , and extrapolated from d to the metric evaluation depth k. Lu et al. explore alternative estimation functions, measuring the prediction error using the mechanism described in Equation 3, and nd that while improvements are possible, no single estimator works consistently well across all collections and topics. Rank-based estimation also has the drawback of ignoring the fact that a document can appear at di erent ranks for di erent systems; and hence potentially assigns di erent gain estimates to the same document in di erent runs, a representational issue that usually leads to a biased estimation [29]. As a further drawback, an entire row in the system matrix S (see Figure 1) must be judged in order to compute the expected gain, limiting construction methods to pooling or sampling by rank, and possibly excluding strati ed sampling processes.",1,ad,True
191,Sampling-Based Estimation. Other sampling approaches can also be used when forming the judgment set. Voorhees [24] de-,0,,False
192,"scribes a two-strata sampling method, which consists of shallow pooled judgments to some depth d , and then 10% random sampling to depth d in a second set s . ese judgments allow computation of inferred recall-based metrics, and also inferred versions",0,,False
193,"of weighted-precision metrics, with M^ i for system i calculated as:",0,,False
194,k,0,,False
195,k,0,,False
196,"M^ i ,",0,,False
197,"rj,i · WM (j) +  ·",0,,False
198,"rj,i · WM (j) ,",0,,False
199,(5),0,,False
200,"j ,""1 sj,i """,0,,False
201,"j ,""1 sj,i  s""",0,,False
202,where,0,,False
203,-1,0,,False
204,k,0,,False
205,k,0,,False
206,",",0,,False
207,WM (j) ·,0,,False
208,WM (j),0,,False
209,"j ,""1 sj,i""",0,,False
210,"j ,""1 sj,i  s""",0,,False
211,and where the second term in Equation 5 estimates the total gain,0,,False
212,"associated with documents contained in the second stratum. Here,  is the interpolation estimator. Note that Equation 5 only adapts the RM method for sample based judgments.",1,ad,True
213,4 TWO-STAGE ESTIMATION,0,,False
214,"Overview of the Framework. To compute score estimates, we propose a two-stage framework, guided by a uni ed optimization goal, and built on a set of m  1 per-topic rank-level estimators.",0,,False
215,e overall structure of this mechanism is described in Algorithm 1.,0,,False
216,"We omit the process of obtaining rank-level estimations, discussed",0,,False
217,"brie y in the previous section, and in detail by Lu et al. [12]. at is, we assume as our starting point here that m di erent rank-based estimators have been generated, each derived from the judged documents D  , and that values for a set of gain functions have been computed, with 0j, the gain associated with an unjudged document that appears in the j th position of any of the n system",0,,False
218,Algorithm 1 Estimation Framework,0,,False
219,Input: System matrix Sk×n ; partial relevance judgments with 2[D] the gain associated with document D for D  and,0,,False
220,unde ned otherwise; and a set of m rank-level background,0,,False
221,"gain estimates, 0j, for 1  j  k and 1   m, with 0,  0j, | 1  j  k and 1[D]  1 [D] | 1   m .",0,,False
222,"Output: Values 2[D], gain estimates for the documents D  ",0,,False
223,1: for D  \ do 2[D]  0,0,,False
224,2:   C,0,,False
225,"CV( , S) // compute coe cient of variance",0,,False
226,3: if  >  then,0,,False
227,// adjust only if  exceeds threshold,1,ad,True
228,4: for  1 to m do,0,,False
229,5:,0,,False
230,for D  do 1 [D]  0,0,,False
231,6:,0,,False
232,"w1opt  arg min L h1 ( 0, , w1) | D ",0,,False
233,"w1 [0, 1]n",0,,False
234,7:,0,,False
235,for D  do,0,,False
236,8:,0,,False
237,"1 [D]  h1 0, , w1opt",0,,False
238,9:,0,,False
239,end for,0,,False
240,10: end for,0,,False
241,"11: w2opt  arg min L h2 ( 1[D], w2) | D  w2 [0, 1]m",0,,False
242,12: // get nal per-document estimation,0,,False
243,13: for D  \ do,0,,False
244,14:,0,,False
245,"2[D]  h2 1[D], w2opt",0,,False
246,15: end for,0,,False
247,16: end if,0,,False
248,17: return 2,0,,False
249,38,0,,False
250,Session 1A: Evaluation 1,1,Session,True
251,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
252,"rankings, as predicted by the th of the m di erent estimators.",0,,False
253,"Prior to forming the new combined estimates, we rst compute the coe cient of covariance  from the judgment set [5], in order to",0,,False
254,"determine whether to use a background ""unjudged are not relevant"" predictor. Estimation is computed by steps 4 to 15, with h1 (·) and h2 (·) two parametric combining functions, in which the parameters are obtained by minimizing a loss function L(·). We discuss the details of Algorithm 1, including the rationale behind the use of  ,",0,,False
255,in the next few paragraphs.,0,,False
256,"First Stage. As noted already, one problem with rank-based estimators is the potential inconsistency across runs of the gain a ached",1,ad,True
257,"to any particular document. As always, we assume that one topic is being addressed; the goal in the rst stage is to aggregate the m × n per-document estimates across the m estimators and n systems into a smaller set of m estimates per document. at is, the m rank-level estimators are treated separately at rst, in the loop at step 4, to obtain a consistent background gain for each document D for each model, denoted 1 [D]. is is done via a combining function h1 (·) that maps a vector to a single value. Several options for h1 (·) are available, with the choice between them depending on assumptions",1,ad,True
258,about system quality and the degree to which the systems are cor-,0,,False
259,"related. For simplicity, we assume that the systems are independent and that they vary in quality. erefore, for each document D, a natural combining function is to compute a weighted average, with h1 (step 6) parameterized by an n-element weighting vector w1 that is speci c to the th estimator:",0,,False
260,n,0,,False
261,"D  , h1 ( 0, , w1 | D) ,",0,,False
262,"0pD,i, · w1i",0,,False
263,"i ,1",0,,False
264,n,0,,False
265,(6),0,,False
266,"with w1i ,"" 1 and w1i  [0, 1] ,""",0,,False
267,"i ,1",0,,False
268,"and where 0pD,i, applies the th estimator to the rank at which document D appears in the i th of the n runs. One practical issue is that a document may not be retrieved by all systems in their top-k ranked lists, where k is the maximum depth of lists returned. In such cases the rank-based background gain of that document for that system is set to the modeled gain at depth k.",0,,False
269,"To compute a value for w1, we consider the aggregation process as an optimization problem, where the goal is to minimize the",0,,False
270,estimation error. e estimation error has two granularities: (i) the,0,,False
271,total error of system e ectiveness score calculated using ; and,0,,False
272,(ii) the total error of estimating the background gain of the labeled,0,,False
273,"documents. From either perspective, we can formalize an objective function L and use it at step 6 of Algorithm 1. Consider the rst case, with the system matrix as shown in Figure 1. We de ne L as:",0,,False
274,2,0,,False
275,nk,0,,False
276,"La (·) ,",0,,False
277,"WM (j) · (h1 (·, w1 | sj,i ) - rj,i ) , (7)",0,,False
278,"i,1 j,""1 sj,i """,0,,False
279,"where WM (j) · h1 (·, w1 | sj,i ) is the estimated background gain for document sj,i  , and rj,i is the known relevance value of that same document. As noted, La minimizes the overall estimation",0,,False
280,error of the evaluation scores for the set of systems.,0,,False
281,"e second alternative uses the document-position representation (pD,1, pD,2, . . . , pD,n ):",0,,False
282,"Lb (·) ,",0,,False
283,D,0,,False
284,n 2,0,,False
285,"WM (pD,i ) · (h1 (·, w1 | D) - rD ) , (8)",0,,False
286,"i ,1",0,,False
287,"in which rD is the relevance value of document D and is included only once per document, rather than once per document-rank.",0,,False
288,"When compared to Equation 7, which considers estimation er-",0,,False
289,"rors at the system level, this loss function is focused at the per-",0,,False
290,"document level, seeking to minimize the overall estimation error",0,,False
291,for the weighted gain of each document. Either Equation 7 or Equa-,0,,False
292,"tion 8 can be used at step 6 of Algorithm 1, with the combination function h1 (·) and constraints de ned in Equation 6. e result is the computation of a sequence of w1opt vectors, one for each of the m di erent rank-level estimators.",0,,False
293,Second Stage. Multiple ing models have been proposed because di erent assumptions about the underlying relevance distributions,0,,False
294,"across all systems are plausible, with a risk that no single model",0,,False
295,"covers the true hypothesis space. Indeed, the limited non-random",0,,False
296,training data means that we may su er from a high variance if only,0,,False
297,"one model is considered. erefore, a ""meta"" optimizer is also used,",0,,False
298,"combining results from the rst stage, as described by steps 11",0,,False
299,"to 15. A weighted average is used in this role too, considering each document D, together with the estimated background gains generated by the m previous computations, 1[D]. at combiner, h2 (·) (step 11), is de ned via the m-vector w2 as:",0,,False
300,m,0,,False
301,"D  , h2 1[D], w2 ,"" 1 [D] · w2 ,""",0,,False
302,",1",0,,False
303,m,0,,False
304,(9),0,,False
305,"with w2 ,"" 1 and w2  [0, 1] .""",0,,False
306,",1",0,,False
307,"Both La and Lb can be used in step 11, but may not necessarily be the same. Note that the m-vector w2opt, computed at step 11 as the minimizing value for Equation 9, provides an indication of",0,,False
308,the importance of individual optimizers from the previous stage.,0,,False
309,Previous work has shown that the expected error of combining loss,0,,False
310,functions is smaller than the average error on results output by,0,,False
311,each optimizer in isolation from the rst stage [29].,0,,False
312,"Computing the Coe cient of Variance. e score adjustment and estimation process has been presented on a per-topic basis, with an underlying assumption that a shallow judgment pool cannot identify a majority of the relevant documents. However, some topics may have only a small number of relevant documents, and a shallow depth may be su cient to identify most of them, with adjustment unnecessary. Only if deeper pooling would identify further relevant documents can score adjustment have an e ect on system e ectiveness scores. Hence a coe cient of variance [5] is computed for the relevant documents in the shallow pool and used as an indicator, as described in step 2.",1,ad,True
313,"Pooling is treated as a sampling with replacement process, with an unknown probability of a relevant document being sampled. Although the nal judgment process considers only the documents in the pool, a document returned by multiple systems has a selection",0,,False
314,39,0,,False
315,Session 1A: Evaluation 1,1,Session,True
316,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
317,frequency. e intuition behind  is to make use of that frequency,0,,False
318,information to describe the sample coverage of relevant documents.,0,,False
319,"Consider the system matrix S in Figure 1 and a pooling depth d . Each document sj,i (1  j  d , 1  i  n) has a multiplicity in Sd ×n ; we then group them by that frequency count. Let fi be the number of relevant documents appearing i times in Sd ×n , R ,"" i fi the number of relevant documents, and C "", i i · fi be",0,,False
320,"the total occurrence count of relevant documents. For example, if",0,,False
321,"only D8 and D1 in Figure 1 are identi ed as relevant documents, then we have f1 ,"" 1, f3 "","" 1, and R "", 2 and C ,"" 4. Based on these elements, the coe cient of variance,  , is estimated via [5]:""",0,,False
322,2,0,,False
323,",",0,,False
324,max,0,,False
325,|R | 1-f1 /C,0,,False
326,C,0,,False
327,i i · (i - 1) · · (C - 1),0,,False
328,fi,0,,False
329,-,0,,False
330,"1,",0,,False
331, 0,0,,False
332,.,0,,False
333,(10),0,,False
334,"When  ,"" 0, the probability of sampling a relevant document follows a uniform distribution; and when  is high, the distribution""",0,,False
335,"is skewed, and it is likely that more relevant documents exist due to",0,,False
336,"the low sampling coverage. Based on this, we have two hypotheses:",0,,False
337,Hypothesis 1:  tends to decrease as pooling depth increases.,0,,False
338,"Hypothesis 2: ere is a threshold  , where if  <  , then the existence of unjudged documents will only negligibly a ect the estimate of the system performance, and they can be ignored.",0,,False
339,"e rst hypothesis is easy to understand, because increasing the pooling depth increases the sample size, and increases the sampling coverage. e second hypothesis assumes that the score can be dynamically adjusted based on a threshold. If this is correct, then a point at which the total estimation error is minimal can be observed. Otherwise, we must conclude that a shallow pool is not su cient for nding relevant documents, and adjustment must be applied to all topics in all evaluations.",1,ad,True
340,"Discussion. We have described two possible realizations of loss functions, and one option for the combining functions h1 (·) and h2 (·). More sophisticated mechanisms are also possible. For example, the relationship between systems might be leveraged to derive a be er h1 (·) and its constraint.",0,,False
341,Note also that although our process targets the problem of es-,0,,False
342,"timating the e ectiveness of runs that contribute to the pool, it is",0,,False
343,possible to apply the same process to estimate the score of a new,0,,False
344,"system, and is demonstrated empirically in Section 6. Section 6",0,,False
345,also shows that the framework can be applied to the judgments,0,,False
346,"constructed using two-strata sampling [24], incorporating the addi-",1,corpora,True
347,tional information provided in the second stratum.,0,,False
348,5 COMPARING SYSTEM RANKINGS,0,,False
349,"Section 3 already de ned i , a score-based evaluation criterion. But we are also interested in comparing system orderings as a measure",1,ad,True
350,of usefulness of an estimation regime.,0,,False
351,"Kendall's Distance. is distance metric is widely used to measure the similarity between ranked lists, and counts the number of inverted pairs between two n-item orderings. Let i, j represent the pairwise relationship between the e ectiveness metric means S¯i and S¯j of systems Si and Sj over a set of topics according to one measurement regime, with i, j  {-1, 0, 1} indicating that S¯i < S¯j ,",0,,False
352,"that S¯i ,"" S¯j , and that S¯i > S¯j , respectively; and let i, j be the corresponding values for a second measurement regime and the system""",0,,False
353,"means that it induces, for example, using pooling to a di erent depth. en Kendall's normalized  distance is the number of pairs 1  i < j  n in which i, j · i, j < 0, divided by n(n - 1)/2 to bring it into the range 0    1, with 0 meaning ""identical"".",0,,False
354,Statistical Weighting. Paired t-tests are o en used to quantify the,0,,False
355,"strength of the relationship between two systems, and the values i, j and i, j might be thought of as being continuous rather than ternary. Kumar and Vassilvitskii [8] describe a weighted  distance",0,,False
356,"that counts the strength of each discordant pair, focusing solely on cases where i, j · i, j < 0. In practice we are not only interested in the discordant pairs, but also in pairs that are deemed to be",0,,False
357,signi cantly di erent according to one of the measurement regimes,0,,False
358,"but not the other, even if their overall relationship is concordant. Suppose that S¯i > S¯j according to the rst measurement, and",0,,False
359,"that a paired one-tail statistical test across topics yields pi, j . Values of pi, j near zero indicate a signi cant superiority of Si over Sj ; values close to 0.5 indicate that it is by chance. If we de ne",0,,False
360,"i, j ,"" 00..05 - pi, j""",0,,False
361,"pj,i - 0.5 ",0,,False
362,"if S¯i > S¯j if S¯i ,"" S¯j if S¯i < S¯j ,""",0,,False
363,"then -0.5  i, j  0.5 is a real-valued quantity that captures",0,,False
364,both the direction and strength of the relationship between the two,0,,False
365,systems according to the rst measurement regime. We compute,0,,False
366,"i, j similarly using a second measurement approach, and then, to compare the alternative rankings of n systems induced by the two",0,,False
367,"measurement techniques, calculate",0,,False
368,"dist ,",0,,False
369," · |i, j - i, j | ,",0,,False
370,(11),0,,False
371,1i <j n,0,,False
372,"where   0 is an additional scaling factor. For example, if  ,"" |i, j | then the strength of the relationship between Si and Sj according to the rst measurement regime also in uences the measured distance. Overall, if dist  0, then the two measurement regimes agree in terms of both the direction of each pairwise relationship Si versus Sj , and also its strength. If dist is substantially greater then zero, then the two measurement regimes give rise to many system""",1,ad,True
373,pairs for which there are non-trivial disagreements (including in,0,,False
374,"both discords and in concords) over the strength of the measured relationships. Compared with Kendall's  distance, Equation 11 operates over continuous values, which makes it both resistant",0,,False
375,"to inconclusive changes in rank position, and also sensitive to di erences in which the direction of the relationship between Si and Sj stays the same, but the statistical strength varies markedly.",0,,False
376,6 EXPERIMENTS,0,,False
377,"e experiments described in this section include: (i) a post-hoc analysis for testing two hypotheses proposed in Section 4, and se ing the threshold  ; (ii) evaluating prediction accuracy using RMSE and Acc% as de ned by Lu et al. [12]; (iii) system ordering stability evaluation using the distance metric de ned in Equation 11 with  ,"" 1, and using normalized  distance; and (iv) a case study covering the ClueWeb 2010 (CW10) task.""",1,hoc,True
378,40,0,,False
379,Session 1A: Evaluation 1,1,Session,True
380,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
381,Dataset d |S |,0,,False
382,Judgments per topic,0,,False
383,2-strata,0,,False
384,"d , 10 d , 20 d , 30 d , d",0,,False
385,TREC5 100 76 272 (13) 512 (10) 747 (8) 2298 (4) ­,1,TREC,True
386,TREC9 100 59 174 (11) 322 (8) 462 (7) 1382 (4) 294 (7),1,TREC,True
387,TREC10 100 54 182 (13) 335 (10) 480 (9) 1402 (5) 303 (9),1,TREC,True
388,Rob04 100 42 75 (25) 139 (18) 206 (15) 710 (7) 134 (15),0,,False
389,TB04 80 33 164 (31) 313 (27) 453 (25) 1121 (19) 270 (25),1,TB,True
390,TB05 100 34 111 (41) 202 (36) 291 (33) 878 (25) 187 (33),1,TB,True
391,TB06 50 39 141 (31) 270 (26) 394 (23) 633 (19) ­,1,TB,True
392,CW10 20 21 98 (30) ­,1,CW,True
393,­ 187 (28) ­,0,,False
394,Table 1: Datasets used: d is the original pooling depth and provides the reference point for metric scores; d is a notional pooling depth used our experimentation; and |S | is the number of contributing,0,,False
395,runs. Only Adhoc Task runs are used. e middle four column pairs,1,hoc,True
396,"show the number of judgments averaged across topics at each pooling depth d , and the percentage of relevant documents. e last",0,,False
397,"column shows the statistics when using two-strata sampling [24],",0,,False
398,averaged over topics and over ten random iterations.,0,,False
399,"Experimental Setup. e collections and con guration parameters used in our experiments are shown in Table 1. We also measured a range of behavior using the TREC7 and TREC8 collections, but do not include them here because those two collections were used as part of the post-hoc analysis and parameter se ing. Scripts are available to reproduce all of the various results given here1.",1,TREC,True
400,"Pooling to di erent depths is simulated using the identi ed contributing systems, and the average number of judgments required per topic at di erent pool depths is also shown in Table 1, together with the corresponding percentage of documents identi ed as being relevant. In the experiments measuring rank stability, we also examine the two-strata sampling method described by Voorhees, and averages over ten runs for this randomized approach are included in the table. For the Robust04 task the last 49 topics are used, and judged to a depth of 100; for other tasks, we use all of the original topic set and judgments. Our goal in collection selection was to capture as much variety as possible. TREC5 and Rob04 use the NewsWire document collection, TREC9 and TREC10 use WT10G, a small web collection, and TB04/05/06 use the GOV2 web collection.",1,Robust,True
401,"e ClueWeb 2010 task (CW10) uses the largest web collection but also has fewer contributing systems and a shallow pool depth. It is representative of newer collections, which are large, and have more uncertainty associated with the judgment coverage ­ the core issue which motivated our investigation. We show results for this dataset as a practical application of our work, noting that a pooling depth of d , 20 cannot provide a ground truth for a deep metric [11].",1,ClueWeb,True
402,"We use RBP with p ,"" 0.95 for training and for all testing, as a representative weighted-precision metric. RBP supports graded relevance (needed to make use of the estimated background gains we generate); allows residuals to be computed; and with p "","" 0.95 gives similar system orderings to AP and NDCG [15]. e estimated background gain of each document generated via training using RBP0.95 can also be used to compute other weighted-precision measures, such as the truncated metric SDCG@k when k > d.""",1,ad,True
403,1h ps://github.com/xiaolul/opt est,0,,False
404,"We consider ve methods for predicting e ectiveness scores,",0,,False
405,"three of which are baselines. e rst baseline is the lower bound, LB, which assumes unjudged documents are not relevant; the second is the interpolative estimator of Ravana and Mo at [16] (Equation 4), denoted RM; and the third is the linear model Lin. that is the best of the rank-based approaches described by Lu et al. [12]. ey",0,,False
406,"are compared to the loss functions de ned in Equations 7 and 8, denoted La and Lb respectively, with the same loss function used in both stages, and aggregation via Equations 6 and 9.",0,,False
407,"We use Linear, Zipf and Discrete Weibull models as initial rankbased estimators [12], and hence have m ,"" 3. Two experiments explore rank stability, categorized by how the judgment set is con-""",0,,False
408,structed: (i) pooling based judgments; and (ii) two-strata sampling,0,,False
409,based judgments. Rank stability is measured using the approaches,0,,False
410,discussed in Section 5. e same baselines are used in the rst rank,0,,False
411,"stability evaluation. However, for the sample-based judgments, we consider the metrics InfRBP (p ,"" 0.95) de ned by Equation 5, and Yilmaz and Aslam's InfAP [27] as baselines. roughout the ex-""",1,AP,True
412,"periments, the system scores (plus residuals) and system orderings",0,,False
413,"computed using the same metric, but evaluated at the full pool depth (that is, at k ,"" d), are taken as the """"gold standard"""". e truncated metric AP@d is computed as described by Sakai [21]. Setting  . We rst test the two hypotheses in Section 4, with  in Equation 10 normalized by the number of systems. Average (over topics)  values are plo ed against pool depth in the le -hand plot in Figure 3, showing that  decreases as the pooling depth increases.""",1,AP,True
414,"is is as expected, since the increasing pooling depth results in",0,,False
415,"a more complete judgment set. Among the plo ed datasets the TB06 collection has the largest average  , and corresponds to a high relevance rate (Table 1). TREC5 is a relatively complete test collection, and hence has the lowest  among the datasets plo ed.",1,TB,True
416,"e center pane in Figure 3 shows the distribution of  across topics for d ,"" 10. Although  is usually low for TREC5, there are still some topics that have high values. e same pa ern is""",1,TREC,True
417,"also observable for Rob04 and TREC10. Based on our hypothesis,",1,TREC,True
418,"this observation indicates that, on earlier TREC collections, not all topics necessarily require score adjustment even at d , 10.",1,TREC,True
419,"To set  we use the earlier datasets TREC5, TREC7 and TREC8 and perform a post-hoc analysis, noting that the majority of relevant",1,TREC,True
420,"documents have been identi ed in these collections, and hence",0,,False
421,that the computed RMSE should be close to the true error. e,0,,False
422,"right-hand pane in Figure 3 shows TREC5 outcomes, with three rank-based models plo ed. Weibull (Wei.) may be an overestimate due to the shaping parameter, and Linear (Lin.) tends to provide low estimates due to the monotonically decreasing nature of the",1,TREC,True
423,"model [12]. At rst, neither of the score estimation methods works be er than the lower bound LB, but as  increases, fewer topics need to be estimated, and when  ,"" 0.018, both estimation methods outperform LB. Similar cross-overs occur for TREC7 and TREC8.""",1,TREC,True
424,"Prediction Accuracy. We then employed  ,"" 0.018 for the other datasets, obtaining the results shown in Table 2. When  "","" 0 the Lb method outperforms all three baselines (LB, RM and Lin.) in terms of RMSE and Acc%, while the La approach has a higher RMSE than Lb and on earlier datasets (TREC9, TREC10) is slightly worse than the LB and Lin. baselines. at is, the loss function La provides poorer coverage of the true hypothesis space than does Lb . e RM""",1,TREC,True
425,41,0,,False
426,Session 1A: Evaluation 1,1,Session,True
427,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
428,Mean  (10-3) Percentage (%) RMSE (10-3),0,,False
429,30,0,,False
430,Dataset,0,,False
431,TREC5 Rob04,1,TREC,True
432,TREC10 TB06,1,TREC,True
433,25,0,,False
434,20,0,,False
435,15,0,,False
436,10 10 20 30 40 50 60 70 80 90 100,0,,False
437,Pooling Depth,0,,False
438,50 Datasets,0,,False
439,Rob04 TREC10,1,TREC,True
440,40,0,,False
441,TB06 TREC5,1,TB,True
442,30,0,,False
443,20,0,,False
444,10,0,,False
445,0,0,,False
446,0 4 8 12 16 20 24 28 32,0,,False
447, (10-3),0,,False
448,50,0,,False
449,Method,0,,False
450,LB Lin Wei,0,,False
451,45,0,,False
452,40,0,,False
453,0,0,,False
454,4,0,,False
455,8 12 16 20 24,0,,False
456, (10-3),0,,False
457,"Figure 3: Le :  relative to pooling depth d . Middle: distribution of  per topic when d ,"" 10. Right: impact of the threshold on the training set TREC5, with d "", 10.",1,TREC,True
458,Dataset d,0,,False
459,LB,0,,False
460,Lin. RM,0,,False
461,La,0,,False
462,Lb,0,,False
463," , 0  , 0.018",0,,False
464," , 0  , 0.018",0,,False
465," , 0  , 0.018",0,,False
466,10 TREC9 20,1,TREC,True
467,30,0,,False
468,0.031 (45) 0.012 (59) 0.006 (67),0,,False
469,0.056 (22) 0.025 (32) 0.012 (45),0,,False
470,0.037 (31) 0.031 (44) 0.013 (51) 0.012 (60) 0.006 (66) 0.006 (68),0,,False
471,0.038 (26) 0.030 (44) 0.013 (42) 0.012 (58) 0.005 (63) 0.006 (68),0,,False
472,0.031 (41) 0.031 (46) 0.010 (62) 0.012 (61) 0.005 (71) 0.006 (68),0,,False
473,10 TREC10 20,1,TREC,True
474,30,0,,False
475,0.038 (39) 0.016 (53) 0.007 (64),0,,False
476,0.064 (16) 0.030 (25) 0.015 (37),0,,False
477,0.034 (25) 0.033 (31) 0.015 (45) 0.014 (51) 0.007 (61) 0.007 (63),0,,False
478,0.036 (13) 0.031 (27) 0.016 (36) 0.014 (50) 0.007 (55) 0.007 (62),0,,False
479,0.027 (34) 0.028 (38) 0.012 (53) 0.012 (55) 0.006 (66) 0.006 (66),0,,False
480,10 Rob04 20,0,,False
481,30,0,,False
482,0.046 (21) 0.020 (34) 0.008 (49),0,,False
483,0.088 (5) 0.040 (9) 0.020 (14),0,,False
484,0.043 (21) 0.039 (20) 0.015 (33) 0.016 (34) 0.007 (49) 0.007 (52),0,,False
485,0.045 (9) 0.039 (17) 0.016 (26) 0.015 (32) 0.007 (47) 0.006 (53),0,,False
486,0.039 (20) 0.035 (20) 0.013 (38) 0.016 (35) 0.005 (59) 0.006 (55),0,,False
487,10 0.117 (14) 0.082 (14) 0.082 (15) 0.087 (14) 0.072 (15) 0.077 (15) 0.073 (16) 0.077 (16) TB04 20 0.053 (21) 0.039 (23) 0.039 (25) 0.041 (25) 0.035 (28) 0.037 (28) 0.033 (32) 0.036 (31),1,TB,True
488,30 0.026 (26) 0.020 (39) 0.018 (39) 0.019 (38) 0.015 (45) 0.016 (44) 0.015 (44) 0.016 (43),0,,False
489,10 0.125 (6),0,,False
490,0.080 (5),0,,False
491,0.085 (7) 0.085 (7),0,,False
492,0.070 (6) 0.070 (8),0,,False
493,0.067 (7) 0.067 (9),0,,False
494,TB05 20 0.056 (10) 0.041 (10) 0.039 (13) 0.039 (13) 0.034 (14) 0.034 (14) 0.033 (18) 0.033 (19),1,TB,True
495,30 0.028 (16) 0.022 (18) 0.021 (24) 0.021 (24) 0.018 (24) 0.018 (24) 0.017 (29) 0.017 (29),0,,False
496,10 0.089 (24) 0.065 (43) 0.059 (43) 0.059 (43) 0.047 (55) 0.047 (55) 0.053 (51) 0.053 (50) TB06 20 0.033 (40) 0.023 (68) 0.021 (66) 0.021 (66) 0.013 (81) 0.013 (81) 0.017 (73) 0.017 (73),1,TB,True
497,30 0.013 (58) 0.007 (87) 0.006 (85) 0.006 (85) 0.003 (94) 0.003 (94) 0.005 (89) 0.005 (89),0,,False
498,"Table 2: RMSE and Acc% scores for RBP0.95 for all estimation methods, with d the depth of the reduced pool, and the reference depth d of",0,,False
499,each dataset as listed in Table 1. Bold numbers are the lowest RMSE and highest Acc% for that collection at that depth.,0,,False
500,"approach performs poorly on all of the earlier datasets, for which the assumption that unjudged documents are equivalent to judged ones is inappropriate. On the larger collections such as TB04/05/06, the gain decreases at a slower rate, making the assumptions in RM more appropriate. e LB approach has similar issues, seen in the TB04/05/06 collections. However, for TB06, smaller RMSE (and larger Acc%) values are achieved when compared to the other collections. is is because the reference depth d ,"" 50 is smaller, resulting in larger residuals. As shown in Figure 3, some of the topics may not necessarily require a score adjustment process, especially in the earlier test collections. is explains why the LB estimator works well on those collections. As expected, applying a threshold  improves the estimation for both La and for the Lin. model, on TREC9, TREC10 and Rob04 test collections. Unsurprisingly, on TB04/05/06, only minor score changes are observed when  "","" 0.018 is used, because the computed  values are larger than""",1,TB,True
501,"the threshold, indicating low coverage of the relevant documents",0,,False
502,identi ed. e only unexpected observation occurs on the TB04,1,TB,True
503,"test collection, where the threshold falsely identi es Topic 734 as",0,,False
504,"having a ""su cient"" sampling of relevant documents, but around",0,,False
505,"48% in the nal judged set are relevant, which increases the RMSE",0,,False
506,"value. Table 3 shows the results for a leave-one-group-out experiment at d , 10 (with  ,"" 0), demonstrating the applicability of the framework in adjusting for both system and pooling depth bias.""",1,ad,True
507,"System Ordering Stability on Pooling-Based Judgments. e system orderings derived from the score estimates when compared against the orderings at the reference depth of k ,"" d are shown in Figure 4. Kendall's  correlation was also computed, but the closely-related  distance is used here since it has a strictly positive value. In the rst row, when normalized  distance is measured, the estimation framework gives orderings close to the reference""",0,,False
508,42,0,,False
509,Session 1A: Evaluation 1,1,Session,True
510,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
511,12,0,,False
512,Method,0,,False
513,12,0,,False
514,Method,0,,False
515,12,0,,False
516,Method,0,,False
517,LB RM Lb,0,,False
518,LB RM Lb,0,,False
519,LB RM Lb,0,,False
520,Lin. La,0,,False
521,Lin. La,0,,False
522,Lin. La,0,,False
523,8,0,,False
524,8,0,,False
525,8,0,,False
526, Distance (10-2),0,,False
527, Distance (10-2),0,,False
528, Distance (10-2),0,,False
529,4,0,,False
530,4,0,,False
531,4,0,,False
532,0,0,,False
533,10,0,,False
534,20,0,,False
535,30,0,,False
536,40,0,,False
537,50,0,,False
538,60,0,,False
539,Pooling Depth,0,,False
540,0,0,,False
541,10,0,,False
542,20,0,,False
543,30,0,,False
544,40,0,,False
545,50,0,,False
546,60,0,,False
547,Pooling Depth,0,,False
548,0,0,,False
549,10,0,,False
550,20,0,,False
551,30,0,,False
552,40,0,,False
553,50,0,,False
554,60,0,,False
555,Pooling Depth,0,,False
556,Distance,0,,False
557,20 16 12,0,,False
558,8 4 0,0,,False
559,10,0,,False
560,Method,0,,False
561,LB RM Lb Lin. La,0,,False
562,20,0,,False
563,30,0,,False
564,40,0,,False
565,50,0,,False
566,60,0,,False
567,Pooling Depth,0,,False
568,Distance,0,,False
569,20 16 12,0,,False
570,8 4 0,0,,False
571,10,0,,False
572,Method,0,,False
573,LB RM Lb Lin. La,0,,False
574,20,0,,False
575,30,0,,False
576,40,0,,False
577,50,0,,False
578,60,0,,False
579,Pooling Depth,0,,False
580,Distance,0,,False
581,20 16 12,0,,False
582,8 4 0,0,,False
583,10,0,,False
584,Method,0,,False
585,LB RM Lb Lin. La,0,,False
586,20,0,,False
587,30,0,,False
588,40,0,,False
589,50,0,,False
590,60,0,,False
591,Pooling Depth,0,,False
592,"Figure 4: System ordering comparisons (RBP0.95) for ve estimators. e rst row uses normalized  distance; the second row uses dist (Equation 11). e columns (from le ) show Rob04, TB04, and TB05, with reference lists using LB at d ,"" 100, d "", 80 and d ,"" 100, respectively.""",1,TB,True
593,Dataset LB,0,,False
594,RM,0,,False
595,Lin.,0,,False
596,La,0,,False
597,Lb,0,,False
598,Rob04 TB04 TB05 TB06,1,TB,True
599,0.060 (19) 0.126 (4) 0.068 (11) 0.060 (9) 0.050 (19) 0.181 (11) 0.202 (11) 0.131 (9) 0.117 (11) 0.119 (11) 0.170 (6) 0.141 (5) 0.125 (4) 0.110 (4) 0.110 (5) 0.125 (22) 0.185 (32) 0.112 (35) 0.090 (48) 0.086 (46),0,,False
600,"Table 3: RMSE and Acc% for leave-out-one-group experiments with d ,"" 10 throughout, averages across groups assuming that""",0,,False
601,each group in turn is omi ed from pool construction (RBP0.95).,0,,False
602,"ordering across a range of nominal pool depths d . e RM approach performs well on TB04/05, agreeing with the results in Table 2. However, as noted above,  is sensitive to swaps that might be inconclusive. e bo om row of Figure 4 shows the dist measure of Equation 11. Overall, there are situations in which LB performs poorly, and situations in which RM performs poorly. e Lin., La , and Lb methods consistently provide the highest agreements.",1,TB,True
603,"We also carried out paired t-tests and calculated the discrimination ratio for a signi cance level p ,"" 0.05, and compared against the original discrimination ratios. e Lin., La , and Lb estimation methods used with all have only a small e ect on discrimination""",0,,False
604,ratio when compared to the use of LB and .,0,,False
605,System Ordering Stability on Sample-Based Judgments. We,0,,False
606,also show the applicability of our methods on the judgment set,0,,False
607,"constructed using a two-strata sampling method [24], which has",0,,False
608,been empirically shown to assist when computing inferred metrics.,0,,False
609,"On this set of judgments we compute InfAP using trec eval, and InfRBP as de ned in Equation 5. Figure 6 shows that La , Lb and InfRBP give rise to stable system orderings, with normalized ",1,AP,True
610,Estimation Methods,0,,False
611,"d',20 CW10",1,CW,True
612,"d',20 TB05",1,TB,True
613,"d',60 TB05",1,TB,True
614,UB,0,,False
615,RM,0,,False
616,dist,0,,False
617,0.125,0,,False
618,Lb,0,,False
619,0.100,0,,False
620,0.075,0,,False
621,La,0,,False
622,0.050,0,,False
623,0.025,0,,False
624,Lin,0,,False
625,0.000,0,,False
626,LB,0,,False
627,LB Lin La Lb RM UB LB Lin La Lb RM UB LB Lin La Lb RM UB,0,,False
628,Estimation Methods,0,,False
629,Figure 5: Normalized  distance between system orderings gen-,0,,False
630,erated by di erent estimation methods based on a pool of depth,0,,False
631,"d ,"" 20, and on TB05 based on pool depths of d "", 20 and d , 60.",1,TB,True
632,"distance scores below 0.05 across all collections. When dist is measured, La outperforms InfRBP on all collections but TREC9, while Lb outperforms InfRBP except on TB05. e slightly worse outcome for La on TREC9 is a consequence of the increase in the number of signi cantly di erent system pairs. Note the more",1,TREC,True
633,variable outcomes generated when InfAP is used as the metric,1,AP,True
634,driving the system orderings.,0,,False
635,"Predictions in ClueWeb. As a nal test of our approach, we examine the CW10 collection. It has a shallow pool depth (d ,"" 20), meaning that validation is not possible, as there is no deep-pool reference ordering. Instead, we compute the normalized  distance between each pair of estimation methods, and simply record how""",1,ClueWeb,True
636,"much the rankings di er, as shown in Figure 5. e UB estimator assumes that all unjudged documents are relevant. As a reference",0,,False
637,43,0,,False
638,Session 1A: Evaluation 1,1,Session,True
639,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
640,Method,0,,False
641,100,0,,False
642,10,0,,False
643,InfAP La,1,AP,True
644,InfRBP Lb,0,,False
645, Distance (10-2) Distance,0,,False
646,40 4,0,,False
647,2,0,,False
648,1,0,,False
649,TREC9,1,TREC,True
650,TREC10,1,TREC,True
651,Rob04,0,,False
652,Collection,0,,False
653,TB04,1,TB,True
654,TB05,1,TB,True
655,20,0,,False
656,Method,0,,False
657,10,0,,False
658,InfAP La,1,AP,True
659,InfRBP Lb,0,,False
660,TREC9,1,TREC,True
661,TREC10,1,TREC,True
662,Rob04,0,,False
663,Collection,0,,False
664,TB04,1,TB,True
665,TB05,1,TB,True
666,"Figure 6: System ordering comparisons on a two-strata sampled judgment set, repeated ten times. Judgments are to depth d ,"" 10, plus a 10% random sample of remaining documents to depth 100 to form the second stratum. Note the logarithmic vertical scales.""",0,,False
667,"point, we also compute the same values for TB05, at two depths, d , 20 and d ,"" 60. At the la er depth all estimation approaches tend to agree with each other. On TB05, all of the estimation results, including UB, tend to agree on the system ordering. However, on CW10, there is clear uncertainty, con rming that d "", 60 is a more robust pool depth for TB05 than is d , 20 on either TB05 or CW10 when seeking to apply RBP0.95 as an evaluation metric. Great caution should be exercised when the d , 20 CW10 judgments are used for anything other than shallow metrics.",1,TB,True
668,7 CONCLUSIONS,0,,False
669,"We have presented new methods to improve system comparisons in batch IR evaluation, with the key idea being to predict a gain value for each unjudged document. We show that estimation is a viable technique to predict scores for deep evaluation metrics when limited judgments are available, including the case when the judgments are obtained using strati ed sampling rather than pooling. One important aspect of our approach is to make decisions on when to adjust topics, instead of treating all topics equally.",1,ad,True
670,"A secondary contribution is the development of a new technique to more precisely compare system orderings. By focusing on swaps that are conclusive, our weighted rank correlation coe cient dist can be used to measure the stability of a variety of estimation techniques. Using dist, we show that estimation improves our ability to score and compare systems using limited judgments.",0,,False
671,"It must be noted, however, that the estimation is built on the m rank-based ed models, each of which requires that when constructing the judgment set, documents up to some rank d be fully judged. is means that for some sampling-based judgment approaches, the proposed method is not applicable. Second, while we show that our estimation methods can also account for system bias to some extent, outcomes might be further improved by introducing more randomization into the optimization framework. Hence, in answer to the question posed in the title, our answer remains a somewhat cautious ""be er than before"", rather than a ""yes"".",0,,False
672,Funding. is work was supported by the Australian Research Council's Discovery Projects Scheme (DP140103256 and DP170102231).,0,,False
673,REFERENCES,0,,False
674,"[1] J. A. Aslam, V. Pavlu, and E. Yilmaz. 2006. A statistical method for system evaluation using incomplete judgments. In Proc. SIGIR. 541­548.",0,,False
675,"[2] C. Buckley, D. Dimmick, I. Soboro , and E. M. Voorhees. 2007. Bias and the limits of pooling for large collections. Inf. Retr. 10, 6 (2007), 491­508.",0,,False
676,[3] C. Buckley and E. M. Voorhees. 2004. Retrieval evaluation with incomplete information. In Proc. SIGIR. 25­32.,0,,False
677,"[4] S. Bu¨ cher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboro . 2007. Reliable information retrieval evaluation with incomplete and biased judgements. In Proc. SIGIR. 63­70.",0,,False
678,"[5] A. Chao and S. Lee. 1992. Estimating the number of classes via sample coverage. J. American Statistical Association 87, 417 (1992), 210­217.",0,,False
679,"[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. 2009. Expected reciprocal rank for graded relevance. In Proc. CIKM. 621­630.",1,ad,True
680,"[7] J. K. Jayasinghe, W. Webber, M. Sanderson, and J. S. Culpepper. 2014. Improving test collection pools with machine learning. In Proc. Aust. Doc. Comp. Symp. 2­9.",0,,False
681,[8] R. Kumar and S. Vassilvitskii. 2010. Generalized distances between rankings. In Proc. WWW. 571­580.,0,,False
682,"[9] A. Lipani, M. Lupu, and A. Hanbury. 2015. Spli ing water: Precision and antiprecision to reduce pool bias. In Proc. SIGIR. 103­112.",0,,False
683,"[10] A. Lipani, M. Lupu, E. Kanoulas, and A. Hanbury. 2016. e solitude of relevant documents in the pool. In Proc. CIKM. 1989­1992.",0,,False
684,"[11] X. Lu, A. Mo at, and J. S. Culpepper. 2016. e e ect of pooling and evaluation depth on IR metrics. Inf. Retr. 19, 4 (2016), 416­445.",0,,False
685,"[12] X. Lu, A. Mo at, and J. S. Culpepper. 2016. Modeling relevance as a function of retrieval rank. In Proc. AIRS. 3­15.",0,,False
686,"[13] A. Mo at, P. omas, and F. Scholer. 2013. Users versus models: What observation tells us about e ectiveness metrics. In Proc. CIKM. 659­668.",0,,False
687,"[14] A. Mo at, W. Webber, and J. Zobel. 2007. Strategic system comparisons via targeted relevance judgments. In Proc. SIGIR. 375­382.",0,,False
688,"[15] A. Mo at and J. Zobel. 2008. Rank-biased precision for measurement of retrieval e ectiveness. ACM Trans. Information Systems 27, 1 (2008), 2:1­2:27.",0,,False
689,"[16] S. D. Ravana and A. Mo at. 2010. Score estimation, incomplete judgments, and signi cance testing in IR evaluation. In Proc. AIRS. 97­109.",0,,False
690,[17] S. E. Robertson. 2007. On document populations and measures of IR e ectiveness. In Proc. ICTIR. 9­22.,0,,False
691,[18] T. Sakai. 2007. Alternatives to BPref. In Proc. SIGIR. 71­78. [19] T. Sakai. 2008. Comparing metrics across TREC and NTCIR: The robustness to,1,TREC,True
692,pool depth bias. In Proc. SIGIR. 691­692. [20] T. Sakai. 2008. Comparing metrics across TREC and NTCIR: The robustness to,1,TREC,True
693,"system bias. In Proc. CIKM. 581­590. [21] T. Sakai. 2014. Metrics, statistics, tests. In Bridging Between Information Retrieval",0,,False
694,"and Databases, N. Ferro (Ed.). Springer, 116­163. [22] T. Schnabel, A. Swaminathan, P. I. Frazier, and T. Joachims. 2016. Unbiased",0,,False
695,"comparative evaluation of ranking functions. In Proc. ICTIR. 109­118. [23] T. Schnabel, A. Swaminathan, and T. Joachims. 2015. Unbiased ranking evaluation",0,,False
696,on a budget. In Proc. WWW. 935­937. [24] E. M. Voorhees. 2014. e e ect of sampling strategy on inferred measures. In,0,,False
697,Proc. SIGIR. 1119­1122. [25] E. M. Voorhees and D. K. Harman. 2005. TREC: Experiment and Evaluation in,1,TREC,True
698,Information Retrieval. e MIT Press. [26] W. Webber and L. A. F. Park. 2009. Score adjustment for correction of pooling,1,ad,True
699,bias. In Proc. SIGIR. 444­451. [27] E. Yilmaz and J. A. Aslam. 2008. Estimating average precision when judgments,0,,False
700,"are incomplete. Knowledge and Information Systems 16, 2 (2008), 173­211. [28] E. Yilmaz, E. Kanoulas, and J. A. Aslam. 2008. A simple and e cient sampling",0,,False
701,method for estimating AP and NDCG. In Proc. SIGIR. 603­610. [29] Z. Zhou. 2012. Ensemble Methods: Foundations and Algorithms. CRC press. [30] J. Zobel. 1998. How reliable are the results of large-scale information retrieval,1,AP,True
702,experiments?. In Proc. SIGIR. 307­314.,0,,False
703,44,0,,False
704,,0,,False

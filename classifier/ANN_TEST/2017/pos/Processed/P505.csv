,sentence,label,data,regex
0,Session 4C: Queries and Query Analysis,1,Session,True
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Relevance-based Word Embedding,0,,False
3,Hamed Zamani,0,,False
4,Center for Intelligent Information Retrieval College of Information and Computer Sciences,0,,False
5,"University of Massachuse s Amherst Amherst, MA 01003 zamani@cs.umass.edu",0,,False
6,ABSTRACT,0,,False
7,"Learning a high-dimensional dense representation for vocabulary terms, also known as a word embedding, has recently a racted much a ention in natural language processing and information retrieval tasks. e embedding vectors are typically learned based on term proximity in a large corpus. is means that the objective in well-known word embedding algorithms, e.g., word2vec, is to accurately predict adjacent word(s) for a given word or context. However, this objective is not necessarily equivalent to the goal of many information retrieval (IR) tasks. e primary objective in various IR tasks is to capture relevance instead of term proximity, syntactic, or even semantic similarity. is is the motivation for developing unsupervised relevance-based word embedding models that learn word representations based on query-document relevance information. In this paper, we propose two learning models with di erent objective functions; one learns a relevance distribution over the vocabulary set for each query, and the other classi es each term as belonging to the relevant or non-relevant class for each query. To train our models, we used over six million unique queries and the top ranked documents retrieved in response to each query, which are assumed to be relevant to the query. We extrinsically evaluate our learned word representation models using two IR tasks: query expansion and query classi cation. Both query expansion experiments on four TREC collections and query classi cation experiments on the KDD Cup 2005 dataset suggest that the relevance-based word embedding models signi cantly outperform state-of-the-art proximity-based embedding models, such as word2vec and GloVe.",1,ad,True
8,KEYWORDS,0,,False
9,"Word representation, neural network, embedding vector, query expansion, query classi cation",0,,False
10,"ACM Reference format: Hamed Zamani and W. Bruce Cro . 2017. Relevance-based Word Embedding. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080831",0,,False
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080831",1,ad,True
12,W. Bruce Cro,0,,False
13,Center for Intelligent Information Retrieval College of Information and Computer Sciences,0,,False
14,"University of Massachuse s Amherst Amherst, MA 01003 cro @cs.umass.edu",0,,False
15,1 INTRODUCTION,1,DUC,True
16,"Representation learning is a long-standing problem in natural language processing (NLP) and information retrieval (IR). e main motivation is to abstract away from the surface forms of a piece of text, e.g., words, sentences, and documents, in order to alleviate sparsity and learn meaningful similarities, e.g., semantic or syntactic similarities, between two di erent pieces of text. Learning representations for words as the atomic components of a language, also known as word embedding, has recently a racted much a ention in the NLP and IR communities.",0,,False
17,"A popular model for learning word representation is neural network-based language models. For instance, the word2vec model proposed by Mikolov et al. [24] is an embedding model that learns word vectors via a neural network with a single hidden layer. Continuous bag of words (CBOW) and skip-gram are two implementations of the word2vec model. Another successful trend in learning semantic word representations is employing global matrix factorization over word-word matrices. GloVe [28] is an example of such methods. A theoretical relation has been discovered between embedding models based on neural network and matrix factorization in [21]. ese models have been demonstrated to be e ective in a number of IR tasks, including query expansion [11, 17, 40], query classi cation [23, 41], short text similarity [15], and document model estimation [2, 31].",0,,False
18,"e aforementioned embedding models are typically trained based on term proximity in a large corpus. For instance, the word2vec model's objective is to predict adjacent word(s) given a word or context, i.e., a context window around the target word. is idea aims to capture semantic and syntactic similarities between terms, since semantically/syntactically similar words o en share similar contexts. However, this objective is not necessarily equivalent to the main objective of many IR tasks. e primary objective in many IR methods is to model the notion of relevance [20, 34, 43]. In this paper, we revisit the underlying assumption of typical word embedding methods, as follows:",1,ad,True
19,e objective is to predict the words observed in the documents relevant to a particular information need.,0,,False
20,"is objective has been previously considered for developing relevance models [20], a state-of-the-art (pseudo-) relevance feedback approach. Relevance models try to optimize this objective given a set of relevant documents for a given query as the indicator of user's information need. In the absence of relevance information, the top ranked documents retrieved in response to the query are assumed to be relevant. erefore, relevance models, and in general all pseudo-relevance feedback models, use an online se ing to obtain training data: retrieving documents for the query and",0,,False
21,505,0,,False
22,Session 4C: Queries and Query Analysis,1,Session,True
23,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
24,"then using the top retrieved documents in order to estimate the relevance distribution. Although relevance models have been proved to be e ective in many IR tasks [19, 20], having a retrieval run for each query to obtain the training data for estimating the relevance distribution is not always practical in real-world search engines. We, in this paper, optimize a similar objective in an o ine se ing, which enables us to predict the relevance distribution without any retrieval runs during the test time. To do so, we consider the top retrieved documents for millions of training queries as a training set and learn embedding vectors for each term in order to predict the words observed in the top retrieved documents for each query. We develop two relevance-based word embedding models. e rst one, the relevance likelihood maximization model (RLM), aims to model the relevance distribution over the vocabulary terms for each query, while the second one, the relevance posterior estimation model (RPE), classi es each term as relevant or non-relevant to each query. We provide e cient learning algorithms to train these models on large amounts of training data. Note that our models are unsupervised and the training data is generated automatically.",1,LM,True
25,"To evaluate our models, we performed two sets of extrinsic evaluations. In the rst set, we focus on the query expansion task for ad-hoc retrieval. In this set of experiments, we consider four TREC collections, including two newswire collections (AP and Robust) and two large-scale web collections (GOV2 and ClueWeb09 - Cat. B). Our results suggest that the relevance-based embedding models outperform state-of-the-art word embedding algorithms. e RLM model shows be er performance compared to RPE in the context of query expansion, since the goal is to estimate the probability of each term given a query and this distribution is not directly learned by the RPE model. In the second set of experiments, we focus on the query classi cation task using the KDD Cup 2005 [22] dataset. In this extrinsic evaluation, the relevance-based embedding models again perform be er than the baselines. Interestingly, the query classi cation results demonstrate that the RPE model outperforms the RLM model, for the reason that in this task, unlike the query expansion task, the goal is to compute the similarity between two query vectors, and RPE can learn more accurate embedding vectors with less training data.",1,ad-hoc,True
26,2 RELATED WORK,0,,False
27,"Learning a semantic representation for text has been studied for many years. Latent semantic indexing (LSI) [8] can be considered as early work in this area that tries to map each text to a semantic space using singular value decomposition (SVD), a well-known matrix factorization algorithm. Subsequently, Clinchant and Perronnin [5] proposed Fisher Vector (FV), a document representation framework based on continuous word embeddings, which aggregates a non-linear mapping of word vectors into a document-level representation. However, a number of popular IR models, such as BM25 and language models, o en signi cantly outperform the models that are based on semantic similarities. Recently, extremely e cient word embedding algorithms have been proposed to model semantic similarly between words.",0,,False
28,"Word embedding, also known as distributed representation of words, refers to a set of machine learning algorithms that learn high-dimensional real-valued dense vector representation w  Rd",0,,False
29,"for each vocabulary term w, where d denotes the embedding dimensionality. GloVe [28] and word2vec [24] are two well-known word embedding algorithms that learn embedding vectors based on the same idea, but using di erent machine learning techniques.",0,,False
30,"e idea is that the words that o en appear in similar contexts are similar to each other. To do so, these algorithms try to accurately predict the adjacent word(s) given a word or a context (i.e., a few words appeared in the same context window). Recently, Rekabsaz et al. [30] proposed to exploit global context in word embeddings in order to avoid topic shi ing.",1,ad,True
31,"Word embedding representations can be also learned as a set of parameters in an end-to-end neural network model. For instance, Zamani et al. [39] trained a context-aware ranking model in which the embedding vectors of frequent n-grams are learned using click data. More recently, Dehghani et al. [9] trained neural ranking models with weak supervision data (i.e., a set of noisy training data automatically generated by an existing unsupervised model) that learn word representations in an end-to-end ranking scenario.",0,,False
32,"Word embedding vectors have been successfully employed in several NLP and IR tasks. Kusner et al. [16] proposed word mover's distance (WMD), a function for calculating semantic distance between two documents, which measures the minimum traveling distance from the embedded vectors of individual words in one document to the other one. Zhou et al. [47] introduced an embeddingbased method for question retrieval in the context of community question answering. Vulic´ and Moens [37] proposed a model to learn bilingual word embedding vectors from document-aligned comparable corpora. Zheng and Callan [46] presented a supervised embedding-based technique to re-weight terms in the existing IR models, e.g., BM25. Based on the well-de ned structure of language modeling framework in information retrieval, a number of methods have been introduced to employ word embedding vectors within this framework in order to improve the performance in IR tasks. For instance, Zamani and Cro [40] presented a set of embedding-based query language models using the query expansion and pseudo-relevance feedback techniques that bene t from the word embedding vectors. ery expansion using word embedding has been also studied in [11, 17, 35]. All of these approaches are based on word embeddings learned based on term proximity information. PhraseFinder [14] is an early work using term proximity information for query expansion. Mapping vocabulary terms to HAL space, a low-dimensional space compared to vocabulary size, has been used in [4] for query modeling.",1,corpora,True
33,"As is widely known in the information retrieval literature [11, 38], there is a big di erence between the unigram distribution of words on sub-topics of a collection and the unigram distribution estimated from the whole collection. Given this phenomenon, Diaz et al. [11] recently proposed to train word embedding vectors on the top retrieved documents for each query. However, this model, called local embedding, is not always practical in real-word applications, since the embedding vectors need to be trained during the query time. Furthermore, the objective function in local embedding is based on term proximity in pseudo-relevant documents.",0,,False
34,"In this paper, we propose two models for learning word embedding vectors, that are speci cally designed for information retrieval needs. All the aforementioned tasks in this section can potentially bene t from the vectors learned by the proposed models.",0,,False
35,506,0,,False
36,Session 4C: Queries and Query Analysis,1,Session,True
37,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
38,3 RELEVANCE-BASED EMBEDDING,0,,False
39,"Typical word embedding algorithms, such as word2vec [24] and GloVe [28], learn high-dimensional real-valued embedding vectors based on the proximity of terms in a training corpus, i.e., cooccurrence of terms in the same context window. Although these approaches could be useful for learning the embedding vectors that can capture semantic and syntactic similarities between vocabulary terms and have shown to be useful in many NLP and IR tasks, there is a large gap between their learning objective (i.e., term proximity) and what is needed in many information retrieval tasks. For example, consider the query expansion task and assume that a user submi ed the query ""dangerous vehicles"". One of the most similar terms to this query based on the typical word embedding algorithms (e.g., word2vec and GloVe) is ""safe"", and thus it would get a high weight in the expanded query model. e reason is that the words ""dangerous"" and ""safe"" o en share similar contexts. However, expanding the query with the word ""safe"" could lead to poor retrieval performance, since it changes the meaning and the intent of the query.",1,ad,True
40,"is example together with many others have motivated us to revisit the objective used in the learning process of word embedding algorithms in order to obtain the word vectors that be er match with the needs in IR tasks. e primary objective in many IR tasks is to model the notion of relevance. Several approaches, such as the relevance models proposed by Lavrenko and Cro [20], have been proposed to model relevance. Given the successes achieved by these models, we propose to learn word embedding vectors based on an objective that ma ers in information retrieval. e objective is to accurately predict the terms that are observed in a set of relevant documents to a particular information need.",0,,False
41,"In the following subsections, we rst describe our neural network architecture, and then explain how to build a training set for learning relevance-based word embeddings. We further introduce two models, relevance likelihood maximization (RLM) and relevance posterior estimation (RPE), with di erent objectives using the described neural network.",1,LM,True
42,3.1 Neural Network Architecture,0,,False
43,We use a simple yet e ective feed-forward neural network with a,0,,False
44,single linear hidden layer. e architecture of our neural network,0,,False
45,is shown in Figure 1. e input of the model is a sparse query,0,,False
46,"vector qs with the length of N , where N denotes the total number of vocabulary terms. is vector can be obtained by a projection",0,,False
47,function given the vectors corresponding to individual query terms.,0,,False
48,"In this paper, we simply consider average as the projection function.",0,,False
49,"Hence, qs",0,,False
50,",",0,,False
51,1 |q |,0,,False
52,"w q ew , where ew and |q| denote the one-hot",0,,False
53,"vector representation of term w and the query length, respectively.",0,,False
54,e hidden layer in this network maps the given query sparse vector,0,,False
55,"to a query embedding vector q, as follows:",0,,False
56,"q , qs × WQ",0,,False
57,(1),0,,False
58,where WQ  RN ×d is a weight matrix for estimating query embedding vectors and d denotes the embedding dimensionality. e,0,,False
59,output layer of the network is a fully-connected layer given by:,0,,False
60, (q × Ww + bw ),0,,False
61,(2),0,,False
62,query sparse vector,0,,False
63,hidden layer,0,,False
64,qs,0,,False
65,output layer,0,,False
66,W1 W2 W3,0,,False
67,......... .........,0,,False
68,d neurons,0,,False
69,WN,0,,False
70,N neurons,0,,False
71,Figure 1: e relevance-based word embedding architecture. e objective is to learn d-dimensional distributed represen-,0,,False
72,"tation for words based on the notion of relevance, instead of term proximity. N denotes the total number of vocabulary terms.",1,ad,True
73,where Ww  Rd×N and bw  R1×N are the weight and the bias matrices for estimating the probability of each term.  is the activation function which is discussed in Sections 3.3 and 3.4.,0,,False
74,"To summarize, our network contains two sets of embedding parameters, WQ and Ww . e former aims to map the query into the ""query embedding space"", while the la er is used to estimate the weights of individual terms.",0,,False
75,3.2 Modeling Relevance for Training,0,,False
76,"Relevance feedback has been shown to be highly e ective in improving retrieval performance [7, 32]. In relevance feedback, a set of relevant documents to a given query is considered for estimating accurate query models. Since explicit relevance signals for a given query are not always available, pseudo-relevance feedback (PRF) assumes that the top retrieved documents in response to the given query are relevant to the query and uses these documents in order to estimate be er query models. e e ectiveness of PRF in various retrieval scenarios indicates that useful information can be captured from the top retrieved documents [19, 20, 44]. In this paper, we make use of this well-known assumption to train our model. It should be noted that there is a signi cant di erence between PRF and the proposed models: In PRF, the feedback model is estimated from the top retrieved documents of the given query in an online se ing. In other words, PRF retrieves the documents for the initial query and then estimates the feedback model using the top retrieved documents. In this paper, we propose to train the model in an o ine se ing. Moving from the online to the o ine se ing would lead to substantial improvements in e ciency, because an extra retrieval run is not needed in the o ine se ing. To learn a model in an o ine se ing, we consider a xed-length dense vector for each vocabulary term and estimate these vectors based on the information extracted from the top retrieved documents for large numbers of training queries. Note that our models are",1,ad,True
77,507,0,,False
78,Session 4C: Queries and Query Analysis,1,Session,True
79,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
80,"unsupervised. However, if explicit relevance data is available, such as click data, without loss of generality, both the explicit or implicit relevant documents can be considered for training our models. We leave studying the vectors learned based on supervised signals for future work.",0,,False
81,"To formally describe our training data, letT ,"" {(q1, R1), (q2, R2), · · · , (qm, Rm )} be a training set with m training queries. e ith element of this set is a pair of query qi and the corresponding pseudo-relevance feedback distribution. ese distributions are estimated based on the top k retrieved documents (in our experiments, we set k to 10) for each query. e distributions can be estimated using any PRF model, such as those proposed in [20, 36, 42, 44]. In this paper, we only focus on the relevance model [20], a state-of-the-art PRF model, that estimates the relevance distribution as:""",0,,False
82,p(w |Ri )  p(w |d ),0,,False
83,p(w |d ),0,,False
84,(3),0,,False
85,d Fi,0,,False
86,w qi,0,,False
87,where Fi denotes a set of top retrieved documents for query qi . Note that the probability of terms that do not appear in the top,0,,False
88,retrieved documents is equal to zero.,0,,False
89,3.3 Relevance Likelihood Maximization Model,0,,False
90,"In this model, the goal is to learn the relevance distribution R.",0,,False
91,"Given a set of training data, we aim to nd a set of parameters  R",0,,False
92,in order to maximize the likelihood of generating relevance model,0,,False
93,probabilities for the whole training set. e likelihood function is,0,,False
94,de ned as follows:,0,,False
95,m,0,,False
96,p (w |qi ;  R )p (w | Ri ),0,,False
97,(4),0,,False
98,"i,1 w Vi",0,,False
99,where p is the relevance distribution that can be obtained given,0,,False
100,the learning parameters  R and p(w |Ri ) denotes the relevance model distribution estimated for the ith query in the training set,0,,False
101,(see Section 3.2 for more detail). Vi denotes a subset of vocabulary terms that appeared in the top ranked documents retrieved for the,0,,False
102,query qi . e reason for iterating over the terms that appeared in this set instead of the whole vocabulary set V is that the probability,1,ad,True
103,"p(w |Ri ) is equal to zero for all terms w  V - Vi . In this method, we model the probability distribution p using the",0,,False
104,"so max function (i.e., the function  in Equation (2)) as follows:1",0,,False
105,"p(w |q;  R ) ,",0,,False
106,exp (wT q) w V exp (w T q),0,,False
107,(5),0,,False
108,"where w denotes the learned embedding vector for term w and q is the query vector came from the output of the hidden layer in our network (see Section 3.1). According to the so max modeling and the log-likelihood function, we have the following objective:",0,,False
109,m,0,,False
110,arg max,0,,False
111,p(w |Ri ) log exp (wT qi ) - log,0,,False
112,exp (w T qi ),0,,False
113,"R i,1 w Vi",0,,False
114,w V,0,,False
115,(6),0,,False
116,Computing this objective function and its derivatives would,0,,False
117,be computationally expensive (due to the presence of the normal-,0,,False
118,ization factor w V exp (w T q) in the objective function). Since all the word embedding vectors as well as the query vector are,0,,False
119,"1For simplicity, we drop the bias term in these equations.",0,,False
120,"changed during the optimization process, we cannot simply omit the normalization term as is done in [41] for estimating query embedding vectors based on pre-trained word embedding vectors. To make the computations more tractable, we consider a hierarchical approximation of the so max function, which was introduced by Morin and Bengio [26] in the context of neural network language models and then successfully employed by Mikolov et al. [24] in the word2vec model.",0,,False
121,"e hierarchical so max approximation uses a binary tree structure to represent the vocabulary terms, where each leaf corresponds to a unique word. ere exists a unique path from the root to each leaf, and this path is used for estimating the probability of the word representing by the leaf. erefore, the complexity of calculating so max probabilities goes down from O (|V |) to O (log(|V |)) which is the height of the tree. is leads to a huge improvement in computational complexity. We refer the reader to [25, 26] for the details of calculating the hierarchical so max approximation.",1,ad,True
122,3.4 Relevance Posterior Estimation Model,0,,False
123,"As an alternative to maximum likelihood estimation, we can estimate the relevance posterior probability. In the context of pseudorelevance feedback, Zhai and La ery [44] assumed that the language model of the top retrieved documents is estimated based on a mixture model. In other words, it is assumed that there are two language models for the feedback set: the relevance language model2 and a background noisy language model. ey used an expectationmaximization algorithm to estimate the relevance language model. In this model, we make use of this assumption in order to cast the problem of estimating the relevance distribution R as a classi cation task: Given a pair of word w and query q, does w come from the relevance distribution of the query q? Instead of p(w |R), this model estimates p(R ,"" 1|w, q;  R ) where R is a Boolean variable and R "","" 1 means that the given term-query pair (w, q) comes from the relevance distribution R.  R is a set of parameters that is going to be learned during the training phase.""",1,ad,True
124,"erefore, the problem is cast as a binary classi cation task that can be modeled by logistic regression (which means the function  in Equation (2) is the sigmoid function):",0,,False
125,1,0,,False
126,"p (R ,"" 1|w, q;  R ) "", 1 + e (-wT q)",0,,False
127,(7),0,,False
128,where w is the relevance-based word embedding vector for term w.,0,,False
129,"Similar to the previous model, q is the output of the hidden layer",0,,False
130,"of the network, representing the query embedding vector.",0,,False
131,"In order to address this binary classi cation problem, we consider",1,ad,True
132,"a cross-entropy loss function. In theory, for each training query,",0,,False
133,our model should learn to model relevance for the terms appearing,0,,False
134,in the corresponding pseudo-relevant set and non-relevance for all,0,,False
135,"the other vocabulary terms, which could be impractical, due to the",0,,False
136,"large number of vocabulary terms. Similar to [24], we propose to",0,,False
137,use the noise contrastive estimation (NCE) [12] which hypothesizes,0,,False
138,that we can achieve a good model by only di erentiating the data,0,,False
139,from noise via a logistic regression model. e main concept in NCE,0,,False
140,is similar to those proposed in the divergence from randomness,0,,False
141,model [3] and the divergence minimization feedback model [44].,0,,False
142,"2 e phrase ""topical language model"" was used in the original work [44]. We call it ""relevance language model"" to have consistent de nitions in our both models.",0,,False
143,508,0,,False
144,Session 4C: Queries and Query Analysis,1,Session,True
145,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
146,"Based on the NCE hypothesis, we de ne the following negative cross-entropy objective function for training our model:",0,,False
147,arg,0,,False
148,max,0,,False
149,R,0,,False
150,"m i ,1",0,,False
151,"+ j ,1",0,,False
152,Ewj p (w,0,,False
153,| Ri,0,,False
154,),0,,False
155,"log p(R ,"" 1|wj , qi ;  R )""",0,,False
156,-,0,,False
157,+ Ewj pn (w ),0,,False
158,"j ,1",0,,False
159,"log p(R ,"" 0|wj , qi ;  R ) """,0,,False
160,(8),0,,False
161,"where pn (w ) denotes a noise distribution and  ,"" (+, -) is a pair of hyper-parameters to control the number of positive and negative instances per query, respectively. We can easily calculate p(R "","" 0|wj , qi ) "", 1 - p(R ,"" 1|wj , qi ). e noise distribution pn (w ) can be estimated using a function of unigram distribution U (w ) in the whole training set. Similar to [24], we use pn (w )  U (w )3/4 which has been empirically shown to work e ectively for negative sampling.""",0,,False
162,"It is notable that although this model learns embedding vectors for both queries and words, it is not obvious how to calculate the probability of each term given a query; because Equation 7 only gives us a classi cation probability and we cannot simply use the Bayes rule here (since, not all probability components are known). is model can perform well when computing the similarity between two terms or two queries, but not a query and a term. However, we can use the model presented in [41] to estimate the query model using the word embedding vectors (not the ones learned for query vectors) and then calculate the similarity between a query and a term.",0,,False
163,4 EXPERIMENTS,0,,False
164,"In this section, we rst describe how we train the relevance-based word embedding models. We further extrinsically evaluate the learned embeddings using two IR tasks: query expansion and query classi cation. Note that the main aim here is to compare the proposed models with the existing word embedding algorithms, not with the state-of-the-art query expansion and query classi cation models.",0,,False
165,4.1 Training,0,,False
166,"In order to train relevance-based word embeddings, we obtained millions of unique queries from the publicly available AOL query logs [27]. is dataset contains a sample of web search queries from real users submi ed to the AOL search engine within a three-month period from March 1, 2006 to May 31, 2006. We only used query strings and no session and click information was obtained from this dataset. We ltered out the navigational queries containing URL substrings, i.e., ""h p"", ""www."", "".com"", "".net"", "".org"", "".edu"". All nonalphanumeric characters were removed from all queries. Applying all these constraints leads to over 6 millions unique queries as our training query set. To estimate the relevance model distributions in the training set, we considered top 10 retrieved documents in a target collection in response to each query using the Galago3 implementation of the query likelihood retrieval model [29] with Dirichlet prior smoothing (µ , 1500) [45].",1,ad,True
167,3h p://www.lemurproject.org/galago.php,0,,False
168,"We implemented and trained our models using TensorFlow4. e networks are trained based on the stochastic gradient descent optimizer using the back-propagation algorithm [33] to compute the gradients. All model hyper-parameters were tuned on the training set (the hyper-parameters with the smallest training loss value were selected). For each model, the learning rate and the batch size were selected from [0.001, 0.01, 0.1, 1] and [64, 128, 256], respectively. For RPE , we also tuned the number of positive and negative instances (i.e., + and -). e value of + was swept between [20, 50, 100, 200] and the parameter - was selected from [5+, 10+, 20+]. As suggested in [40], in all the experiments (unless otherwise stated) the embedding dimensionality was set to 300, for all models including the baselines.",1,ad,True
169,4.2 Evaluation via ery Expansion,0,,False
170,"In this subsection, we evaluate the embedding models in the context of query expansion for the ad-hoc retrieval task. In the following, we rst describe the retrieval collections used in our experiments. We further explain our experimental setup as well as the evaluation metrics. We nally report and discuss the query expansion results.",1,ad-hoc,True
171,"4.2.1 Data. We use four standard test collections in our experiments. e rst two collections (AP and Robust) consist of thousands of news articles and are considered as homogeneous collections. AP and Robust were previously used in TREC 1-3 Ad-Hoc Track and TREC 2004 Robust Track, respectively. e second two collections (GOV2 and ClueWeb) are large-scale web collections containing heterogeneous documents. GOV2 consists of the "".gov"" domain web pages, crawled in 2004. ClueWeb (i.e., ClueWeb09Category B) is a common web crawl collection that only contains English web pages. GOV2 and ClueWeb were previously used in TREC 2004-2006 Terabyte Track and TREC 2009-2012 Web Track, respectively. e statistics of these collections as well as the corresponding TREC topics are reported in Table 1. We only used the title of topics as queries.",1,AP,True
172,4.2.2 Experimental Setup. We cleaned the ClueWeb collection,1,ClueWeb,True
173,by ltering out the spam documents. e spam ltering phase was done using the Waterloo spam scorer5 [6] with the threshold of 60%.,0,,False
174,Stopwords were removed from all collections using the standard,0,,False
175,INQUERY stopword list and no stemming were performed.,0,,False
176,"For the purpose of query expansion, we consider the language",0,,False
177,modeling framework [29] and estimate a query language model,0,,False
178,based on a given set of word embedding vectors. e expanded query language model p(w |q ) is estimated as:,0,,False
179,"p(w |q ) , pML (w |q) + (1 -  )p(w |q)",0,,False
180,(9),0,,False
181,"where pML (w |q) denotes maximum likelihood estimation of the original query and  is a free hyper-parameter that controls the weight of original query model in the expanded model. e probability p(w |q) is calculated based on the trained word embedding vectors. In our rst model, this probability can be estimated using Equation (5); while in the second model, we should simply use the Bayes rule given Equation (7) to estimate this probability. However, since we do not have any information about the probability of each",0,,False
182,4h p://tensor ow.org/ 5h p://plg.uwaterloo.ca/gvcormac/clueweb09spam/,0,,False
183,509,0,,False
184,Session 4C: Queries and Query Analysis,1,Session,True
185,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
186,Table 1: Collections statistics.,0,,False
187,ID AP Robust,1,AP,True
188,GOV2,0,,False
189,ClueWeb,1,ClueWeb,True
190,collection Associated Press 88-89 TREC Disks 4 & 5 minus Congressional Record,1,TREC,True
191,2004 crawl of .gov domains,0,,False
192,ClueWeb 09 - Category B,1,ClueWeb,True
193,"queries (title only) TREC 1-3 Ad-Hoc Track, topics 51-200",1,TREC,True
194,"TREC 2004 Robust Track, topics 301-450 & 601-700 TREC 2004-2006 Terabyte Track,",1,TREC,True
195,topics 701-850 TREC 2009-2012 Web Track,1,TREC,True
196,topics 1-200,0,,False
197,#docs 165k 528k,0,,False
198,25m,0,,False
199,50m,0,,False
200,avg doc length 287 254,0,,False
201,648,0,,False
202,1506,0,,False
203,"#qrels 15,838 17,412",0,,False
204,"26,917",0,,False
205,"18,771",0,,False
206,Table 2: Evaluating relevance-based word embeddings in the context of query expansion. e superscripts 0/1/2/3/4 denote that the MAP improvements over MLE/word2vec-external/word2vec-target/GloVe-external/GloVe-target are statistically signi cant. e highest value in each row is marked in bold.,1,MAP,True
207,Collection AP,1,AP,True
208,Robust GOV2 ClueWeb,1,Robust,True
209,Metric,0,,False
210,MAP P@20 NDCG@20,1,MAP,True
211,MAP P@20 NDCG@20,1,MAP,True
212,MAP P@20 NDCG@20,1,MAP,True
213,MAP P@20 NDCG@20,1,MAP,True
214,MLE,0,,False
215,0.2197 0.3503 0.3924,0,,False
216,0.2149 0.3319 0.3863,0,,False
217,0.2702 0.5132 0.4482,0,,False
218,0.1028 0.3025 0.2237,0,,False
219,word2vec,0,,False
220,external target,0,,False
221,0.2399 0.3688 0.4030,0,,False
222,0.2420 0.3738 0.4181,0,,False
223,0.2218 0.3357 0.3918,0,,False
224,0.2215 0.3337 0.3881,0,,False
225,0.2740 0.5257 0.4571,0,,False
226,0.2723 0.5172 0.4509,0,,False
227,0.1033 0.3040 0.2235,0,,False
228,0.1033 0.3053 0.2252,0,,False
229,GloVe,0,,False
230,external target,0,,False
231,0.2319 0.3581 0.4025,0,,False
232,0.2389 0.3631 0.4098,0,,False
233,0.2209 0.3345 0.3918,0,,False
234,0.2172 0.3281 0.3844,0,,False
235,0.2718 0.5186 0.4539,0,,False
236,0.2709 0.5128 0.4485,0,,False
237,0.1029 0.3033 0.2244,0,,False
238,0.1026 0.3048 0.2244,0,,False
239,Rel.-based Embedding,0,,False
240,RLM,1,LM,True
241,RPE,0,,False
242,0.258001234 0.388601234 0.424201234,0,,False
243,0.245001234 0.347601234 0.398201234,0,,False
244,0.286701234 0.536701234 0.45760234,0,,False
245,0.106601234,0,,False
246,0.3073 0.227301,0,,False
247,0.254301234 0.3812034 0.422601234,0,,False
248,0.237201234 0.3409024 0.39550,0,,False
249,0.285501234 0.535801234 0.4557024,0,,False
250,0.1031,0,,False
251,0.3030,0,,False
252,0.2241,0,,False
253,"term given a query, we use the uniform distribution. For other word embedding models (i.e., word2vec and GloVe), we use the standard method described in [11]. For all the models, we ignore the terms whose embedding vectors are not available.",0,,False
254,"We retrieve the documents for the expanded query language model using the KL-divergence formula [18] with Dirichlet prior smoothing (µ , 1500) [45]. All the retrieval experiments were carried out using the Galago toolkit [7].",0,,False
255,"In all the experiments, the parameters  (the linear interpolation coe cient) and m (the number of expansion terms) were set using 2-fold cross-validation over the queries in each collection. We selected the parameter  from {0.1, . . . , 0.9} and the parameter m from {10, 20, ..., 100}.",0,,False
256,"4.2.3 Evaluation Metrics. To evaluate the e ectiveness of query expansion models, we report three standard evaluation metrics: mean average precision (MAP) of the top ranked 1000 documents, precision of the top 20 retrieved documents (P@20), and normalized discounted cumulative gain [13] calculated for the top 20 retrieved documents (nDCG@20). Statistically signi cant di erences of MAP, P@20, and nDCG@20 values based on the two-tailed paired t-test are computed at a 95% con dence level (i.e., p alue < 0.05).",1,MAP,True
257,"4.2.4 Results and Discussion. To evaluate our models, we consider the following baselines: (i) the standard maximum likelihood estimation (MLE) of the query model without query expansion, (ii) two sets of embedding vectors (one trained on Google News as a",0,,False
258,"large external corpus and one trained on the target retrieval collection) learned by the word2vec model6 [24], and (iii) two sets of embedding vectors (one trained on Wikipedia 2004 plus Gigawords 5 as a large external corpus7 and the other on the target retrieval collection) learned by the GloVe model [28].",1,Wiki,True
259,"Table 2 reports the results achieved by the proposed models and the baselines. According to this table, all the query expansion models outperform the MLE baseline in nearly all cases, which indicates the e ectiveness of employing high-dimensional word representations for query expansion. Similar observations have been made in [11, 17, 40, 41]. According to the results, although word2vec performs slightly be er than GloVe, no signi cant di erences can be observed between their performances. According to Table 2, both relevance-based embedding models outperform all the baselines in all the collections, which shows the importance of taking relevance into account for training embedding vectors. ese improvements are o en statistically signi cant compared to all the baselines. e relevance likelihood maximization model (RLM) performs be er than the relevance posterior estimation model (RPE) in all cases and the reason is related to their objective function. RLM learns the relevance distribution for all terms, while RPE learns the classi cation probability of being relevance for vocabulary terms (see Equations (5) and (7)).",1,ad,True
260,6We use the CBOW implementation of the word2vec model.,0,,False
261,also performs similarly. 7Available at h p://nlp.stanford.edu/projects/glove/.,0,,False
262,e skip-gram model,0,,False
263,510,0,,False
264,Session 4C: Queries and Query Analysis,1,Session,True
265,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
266,"Table 3: Top 10 expansion terms obtained by the word2vec and the relevance-based word embedding models for two sample queries ""indian american museum"" and ""tibet protesters"".",0,,False
267,"query: ""indian american museum""",0,,False
268,word2vec,0,,False
269,Rel.-based Embedding,0,,False
270,external,0,,False
271,target,0,,False
272,RLM,1,LM,True
273,RPE,0,,False
274,history,0,,False
275,powwows,0,,False
276,chumash,0,,False
277,heye,0,,False
278,art,0,,False
279,smithsonian heye,0,,False
280,collection,0,,False
281,culture,0,,False
282,afro,0,,False
283,artifacts,0,,False
284,chumash,0,,False
285,british,0,,False
286,mesoamerica smithsonian smithsonian,0,,False
287,heritage,0,,False
288,smithsonians collection,0,,False
289,york,0,,False
290,society,0,,False
291,native,0,,False
292,washington new,0,,False
293,states,0,,False
294,heye,0,,False
295,institution,0,,False
296,apa,0,,False
297,contemporary hopi,0,,False
298,york,0,,False
299,native,0,,False
300,part,0,,False
301,mayas,0,,False
302,native,0,,False
303,americans,0,,False
304,united,0,,False
305,cimam,0,,False
306,apa,0,,False
307,history,0,,False
308,"To get a sense of what is learned by each of the embedding models8, in Table 3 we report the top 10 expansion terms for two sample queries from the Robust collection. According to this table, the terms added to the query by the word2vec model are syntactically or semantically related to individual query terms, which is expected. For the query ""indian american museum"" as an example, the terms ""history"", ""art"", and ""culture"" are related to the query term ""museum"", while the terms ""united"" and ""states"" are related to the query term ""american"". In contrast, looking at the expansion terms obtained by the relevance-based word embeddings, we can see that some relevant terms to the whole query were selected. For instance, ""chumash"" (a group of native americans)9, ""heye"" (the national museum of the American Indian in New York), ""smithsonian"" (the national museum of the American Indian in Washington DC), and ""apa"" (the American Psychological Association that actively promotes American Indian museums). A similar observation can be made for the other sample query (i.e., ""tibet protesters""). For example, the word ""independence"" is related to the whole query that was only selected by the relevance-based word embedding models, while the terms ""protestors"", ""protests"", ""protest"", and ""protesting"" that are syntactically similar to the query term ""protesters"" were considered by the word2vec model. We believe that these di erences are due to the learning objective of the models. Interestingly, the expansion terms added to each query by the two relevance-based models look very similar, but according to Table 2, their performances are quite di erent. e reason is related to the weights given to each term by the two models. e weights given to the expansion terms by RPE are very close to each other because its objective is to just classify each term and all of these terms are classi ed with a high probability as ""relevant"".",1,Robust,True
309,"In the next set of experiments, we consider the methods that use the top retrieved documents for query expansion: the relevance model (RM3) [1, 20] as a state-of-the-art pseudo-relevance feedback model, and the local embedding approach recently proposed by Diaz et al. [11] with the general idea of training word embedding models on the top ranked documents retrieved in response to a given query. Similar to [11], we use the word2vec model to train",0,,False
310,"8For the sake of space, we only report the expanded terms estimated by the word2vec model and the proposed models. 9see h ps://en.wikipedia.org/wiki/Chumash people",1,wiki,True
311,"query: ""tibet protesters""",0,,False
312,word2vec,0,,False
313,Rel.-based Embedding,0,,False
314,external,0,,False
315,target,0,,False
316,RLM,1,LM,True
317,RPE,0,,False
318,demonstrators tibetan,0,,False
319,tibetan,0,,False
320,tibetan,0,,False
321,protestors,0,,False
322,lhasa,0,,False
323,lama,0,,False
324,tibetans,0,,False
325,tibetan,0,,False
326,demonstrators tibetans,0,,False
327,lama,0,,False
328,protests,0,,False
329,tibetans,0,,False
330,lhasa,0,,False
331,independence,0,,False
332,tibetans,0,,False
333,marchers,0,,False
334,dalai,0,,False
335,lhasa,0,,False
336,protest,0,,False
337,lhasas,0,,False
338,independence dalai,0,,False
339,activists,0,,False
340,jokhang,0,,False
341,protest,0,,False
342,open,0,,False
343,protesting,0,,False
344,demonstrations open,0,,False
345,protest,0,,False
346,lhasa,0,,False
347,dissidents,0,,False
348,zone,0,,False
349,zone,0,,False
350,demonstrations barkhor,0,,False
351,followers,0,,False
352,jokhang,0,,False
353,Table 4: Evaluating relevance-based word embedding in pseudo-relevance feedback scenario. e superscripts 1/2/3 denote that the MAP improvements over RM3/Local Embedding/ERM with Local Embedding are statistically signi cant.,1,MAP,True
354,e highest value in each row is marked in bold.,0,,False
355,Collection AP,1,AP,True
356,Robust GOV2 ClueWeb,1,Robust,True
357,Metric,0,,False
358,MAP P@20 NDCG@20,1,MAP,True
359,MAP P@20 NDCG@20,1,MAP,True
360,MAP P@20 NDCG@20,1,MAP,True
361,MAP P@20 NDCG@20,1,MAP,True
362,RM3,0,,False
363,0.2927 0.4034 0.4368,0,,False
364,0.2593 0.3486 0.4011,0,,False
365,0.2863 0.5318 0.4503,0,,False
366,0.1079 0.3111 0.2309,0,,False
367,Local,0,,False
368,Emb.,0,,False
369,0.2412 0.3742 0.4173,0,,False
370,0.2235 0.3366 0.3868,0,,False
371,0.2748 0.5271 0.4576,0,,False
372,0.1041 0.3062 0.2261,0,,False
373,ERM,0,,False
374,Local RLM,1,LM,True
375,0.3047 0.4105 0.4411,0,,False
376,0.2643 0.3498 0.4080,0,,False
377,0.2924 0.5379 0.4584,0,,False
378,0.311912 0.423312 0.4495123,0,,False
379,0.2761123 0.3605123 0.4173123,0,,False
380,0.2986123 0.541712 0.4603123,0,,False
381,0.1094 0.112112,0,,False
382,0.3145 0.3168 0.2328 0.23602,0,,False
383,"word embedding vectors on top 1000 documents. e results are reported in Table 4. In this table, ERM refers to the embedding-based relevance model recently proposed by Zamani and Cro [40] in order to make use of semantic similarities estimated based on the word embedding vectors in a pseudo-relevance feedback scenario. According to Table 4, the ERM model that uses the relevance-based word embedding (RLM10) outperforms all the other methods. ese improvements are statistically signi cant in most cases. By comparing the results obtained by local embedding and those reported in Table 2, it can be observed that there are no substantial di erences between the results for local embedding and word2vec. is is similar to what is reported by Diaz et al. [11] when the embedding vectors are trained on the top documents in the target collection, similar to our se ing. Note that the relevance-based model was also trained on the target collection.",1,LM,True
384,"10For the sake of space, we only consider RLM which shows be er performance compared to RPE in query expansion.",1,LM,True
385,511,0,,False
386,Session 4C: Queries and Query Analysis,1,Session,True
387,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
388,MAP,1,MAP,True
389,0.06 0.07 0.08 0.09 0.1 0.23 0.24 0.25 0.26,0,,False
390,0.30,0,,False
391,0.25 0.20,0,,False
392,0.15 ,0,,False
393,MAP,1,MAP,True
394, AP Robust GOV2 ClueWeb,1,AP,True
395,5,0,,False
396,10,0,,False
397,15,0,,False
398,20,0,,False
399,25,0,,False
400,# expansion terms,0,,False
401,0.10 0.05 0.00,0,,False
402,0.0,0,,False
403, AP Robust GOV2 ClueWeb,1,AP,True
404,0.2,0,,False
405,0.4,0,,False
406,0.6,0,,False
407,0.8,0,,False
408,1.0,0,,False
409,interpolation coefficient,0,,False
410,(a) # expansion terms,0,,False
411,(b) interpolation coe cient,0,,False
412,"Figure 2: Sensitivity of RLM to the number of expansion terms and the interpolation coe cient (), in terms of MAP.",1,LM,True
413,0.29,0,,False
414,MAP,1,MAP,True
415,0.06 0.08 0.1 0.18 0.2 0.22 0.24 0.26 0.28,0,,False
416,0.27,0,,False
417,0.25,0,,False
418,MAP,1,MAP,True
419,0.09 0.1 0.11,0,,False
420,0.07,0,,False
421, AP Robust GOV2 ClueWeb,1,AP,True
422,100,0,,False
423,200,0,,False
424,300,0,,False
425,400,0,,False
426,500,0,,False
427,embedding dimension,0,,False
428,"Figure 3: Sensitivity of RLM to the dimension of embedding vectors, in terms of MAP.",1,LM,True
429,"An interesting observation from Tables 2 and 4 is that the RLM performance (without using pseudo-relevant documents) in Robust and GOV2 is very close to the RM3 performance, and is slightly be er in the GOV2 collection. Note that RM3 needs two retrieval runs11 and uses top retrieved documents, while RLM only needs one retrieval run. is is an important issue in many real-world applications, since the e ciency constraints do not always allow them to have two retrieval runs per query.",1,LM,True
430,"Parameter Sensitivity. In the next set of experiments, we study the sensitivity of RLM as the best performing word embedding model in Table 2 to the expansion parameters. Figure 2a plots the sensitivity of RLM to the number of expansion terms where the parameter  is set to 0.5. According to this gure, in both newswire collections, the method shows its best performance when the queries are expanded with only 10 words. In the GOV2 collection, 15 words are needed for the method to show its best performance.",1,LM,True
431,"Figure 2b plots the sensitivity of the methods to the interpolation coe cient  (see Equation 9) where the number of expansion terms is set to 10. According to the curves correspond to AP and Robust, the original query language model needs to be interpolated with the model estimated using relevance-based word embeddings",1,AP,True
432,"11Diaz [10] showed that for precision-oriented tasks, the second retrieval run can be restricted to the initial rank list for improving the e ciency of PRF models. However, for recall-oriented metrics, e.g., MAP, the second retrieval helps a lot.",1,MAP,True
433, AP Robust GOV2 ClueWeb,1,AP,True
434,1,0,,False
435,2,0,,False
436,3,0,,False
437,4,0,,False
438,5,0,,False
439,million queries,0,,False
440,"Figure 4: e Performance of RLM with respect to di erent amount of training data (training queries), in terms of MAP.",1,LM,True
441,"with equal weights (i.e.,  ,"" 0.5). is shows the quality of the estimated distribution via the learned embedding vectors. In the GOV2 collection, a higher weight should be given to the original query model, which indicates that the original query plays a key role in achieving good retrieval performance in this collection.""",0,,False
442,"We also study the performance of RLM as the best performing word embedding model for query expansion with respect to the embedding dimensionality. e results are shown in Figure 3, where the query expansion performance generally improves as we increase the embedding dimensionality. e performances become stable when the dimension is larger than 300. is experiment suggests that 400 dimensions would be enough for the relevance-based embedding model.",1,LM,True
443,"Due to the large number of parameters in the neural networks, they can require large amounts of training data to achieve good performance. In the next set of experiments, we study how much training data is needed for training our best model. e results are plo ed in Figure 4. According to this gure, by increasing the number of training queries from one million to four million queries, the performance signi cantly increases, and becomes more stable a er four million queries.",0,,False
444,4.3 Evaluation via ery Classi cation,0,,False
445,"In this subsection, we evaluate the proposed embedding models in the context of query classi cation. In this task, each query is",0,,False
446,512,0,,False
447,Session 4C: Queries and Query Analysis,1,Session,True
448,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
449,Table 5: Evaluating embedding algorithms via query classication. e superscripts 1/2 denote that the improvements,0,,False
450,over word2vec/GloVe are signi cant. e highest value in each column is marked in bold.,0,,False
451,Method word2vec GloVe Rel.-based Embedding - RLM Rel.-based Embedding - RPE,1,LM,True
452,Precision,0,,False
453,0.3712,0,,False
454,0.3643 0.394312 0.396112,0,,False
455,F1-measure,0,,False
456,0.4008,0,,False
457,0.3912 0.426712 0.429412,0,,False
458,assigned to a number of labels (categories) which are pre-de ned and a few training queries are available for each label. is is a supervised multi-label classi cation task with li le training data.,0,,False
459,"4.3.1 Data. We consider the dataset that was introduced in KDD Cup 2005 [22] for the internet user search query categorization task and was previously used in [41] for evaluating query embedding vectors. is dataset contains 800 web queries submi ed by real users randomly collected from the MSN search logs. e queries do not contain ""junk"" text or non-English terms. e queries were labelled by three human editors. 67 categories were pre-de ned and up to 5 labels were selected for each query by each editor.",0,,False
460,"4.3.2 Experimental Setup. In our experiments, we performed 5-fold cross-validation over the queries and the reported results are the average of those obtained over the test folds. In all experiments, the spelling errors in queries were corrected in a pre-processing phase, the stopwords were removed from queries (using the INQUERY stopword list), and no stemming was performed.",0,,False
461,"To classify each query, we consider a very simple kNN-based approach proposed in [41]. We rst compute the probability of each category/label given each query q and then select the top t categories with the highest probabilities. e probability p(Ci |q) is computed as follows:",0,,False
462,"p(Ci |q) ,""  (Ci , q)   (Ci , q)""",0,,False
463,(10),0,,False
464,"j  (Cj , q)",0,,False
465,"where Ci denotes the ith category. Ci is the centroid vector of all query embedding vectors with the label of Ci in the training set. We ignore the query terms whose embedding vectors are not available. e number of labels assigned to each query was tuned on the training set from {1, 2, 3, 4, 5}. In the query classi cation experiments, we trained relevance-based word embedding using Robust as the collection.",1,Robust,True
466,"4.3.3 Evaluation Metrics. We consider two evaluation metrics that were also used in KDD Cup 2005 [22]: precision and F1measure. Since the labels assigned by the three human editors di er in some cases, all the label sets should be taken into account.",0,,False
467,ese metrics are computed in the same way as what is described in [22] for evaluating the KDD Cup 2005 submi ed runs. Statistically signi cant di erences are determined using the two-tailed paired t-test computed at a 95% con dence level (p - alue < 0.05).,0,,False
468,"4.3.4 Results and Discussion. We compare our models against the word2vec and GloVe methods trained on the external collections that are described in the query expansion experiments. e results are reported in Table 5, where the relevance-based embedding",0,,False
469,0.430,0,,False
470,F1-measure,0,,False
471,0.428,0,,False
472,0.426,0,,False
473,0.424,0,,False
474,0.422,0,,False
475,0.420,0,,False
476, RLM RPE,1,LM,True
477,100,0,,False
478,200,0,,False
479,300,0,,False
480,400,0,,False
481,500,0,,False
482,embedding dimension,0,,False
483,"Figure 5: Sensitivity of the relevance-based embedding models to the embedding dimensionality, in terms of F1measure.",0,,False
484,0.42 0.40,0,,False
485,F1-measure,0,,False
486,0.38 ,0,,False
487,0.36,0,,False
488,0.34,0,,False
489,1,0,,False
490, RLM RPE,1,LM,True
491,2,0,,False
492,3,0,,False
493,4,0,,False
494,5,0,,False
495,million queries,0,,False
496,"Figure 6: e Performance of relevance-based embedding models with respect to di erent amount of training data (training queries), in terms of F1-measure.",0,,False
497,"models signi cantly outperform the baselines in terms of both metrics. An interesting observation here is that contrary to the query expansion experiments, RPE performs be er than RLM in query classi cation. e reason is that in query expansion the weight of each term is considered in order to generate the expanded query language model. erefore, in addition to the order of terms, their weights should be also e ective for improving the retrieval performance with query expansion. In query classi cation, we only assign a few categories to each query, and thus as long as the order of categories is correct, the similarity values between the queries and the categories do not ma er.",1,LM,True
498,"In the next set of experiments, we study the performance of our relevance-based word embedding models with respect to the embedding dimensionality. e results are plo ed in Figure 5. According to this gure, the performance is generally improved by increasing the embedding dimensionality, and becomes stable when the dimension is greater than 400. is is similar to our observation in the query expansion experiments. We also study the amount of data needed for training our models in Figure 6. According to this gure, at least 4 million queries are needed in order to learn accurate relevance-based word embeddings. It can be seen from Figure 6 that RLM needs more training data compared to RPE in order to perform well, because by increasing the amount of training data the learning curves of these two models get closer.",1,LM,True
499,513,0,,False
500,Session 4C: Queries and Query Analysis,1,Session,True
501,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
502,5 CONCLUSIONS AND FUTURE WORK,0,,False
503,"In this paper, we revisited the underlying assumption in typical word embedding models, such as word2vec and GloVe. Instead of learning embedding vectors based on term proximity, we proposed learning embeddings based on the notion of relevance, which is the primary objective in many IR tasks. We developed two neural network-based models for learning relevance-based word embeddings. e rst model, the relevance likelihood maximization model, aims to estimate the probability of each word in a relevance distribution for each query, while the second one, the relevance posterior estimation model, classi es each term as belonging to relevant or non-relevant class for each query. We evaluated our models using two sets of extrinsic evaluation: query expansion and query classi cation. e query expansion experiments using four standard TREC collections, two newswire and two large-scale web collections, suggested that the relevance-based word embedding models outperform state-of-the-art word embedding algorithms. We showed that the expansion terms chosen by our models are related to the whole query, while those chosen by typical word embedding models are related to individual query terms. e query classi cation experiments also validated these ndings and investigated the e ectiveness of our models.",1,ad,True
504,"In the future, we intend to evaluate the learned embedding models in other IR tasks, such as query reformulation, query intent prediction, etc. We can also achieve more accurate relevance-based embedding vectors by considering the clicked documents for training query, instead of or in addition to the top retrieved documents.",1,ad,True
505,"Acknowledgements. e authors thank Daniel Cohen, Mostafa Dehghani, and Qingyao Ai for their invaluable comments. is work was supported in part by the Center for Intelligent Information Retrieval. Any opinions,",0,,False
506,ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor.,0,,False
507,REFERENCES,0,,False
508,"[1] Nasreen Abdul-jaleel, James Allan, W. Bruce Cro , Fernando Diaz, Leah Larkey, Xiaoyan Li, Donald Metzler, Mark D. Smucker, Trevor Strohman, Howard Turtle, and Courtney Wade. 2004. UMass at TREC 2004: Novelty and HARD. In TREC '04.",1,ad,True
509,"[2] Qingyao Ai, Liu Yang, Jiafeng Guo, and W. Bruce Cro . 2016. Analysis of the Paragraph Vector Model for Information Retrieval. In ICTIR '16. 133­142.",0,,False
510,"[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on Measuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357­389.",0,,False
511,[4] P. D. Bruza and D. Song. 2002. Inferring ery Models by Computing Information Flow. In CIKM '02. 260­269.,0,,False
512,[5] Stephane Clinchant and Florent Perronnin. 2013. Aggregating Continuous Word Embeddings for Information Retrieval. In CVSC@ACL '13. 100­109.,0,,False
513,"[6] Gordon V. Cormack, Mark D. Smucker, and Charles L. Clarke. 2011. E cient and E ective Spam Filtering and Re-ranking for Large Web Datasets. Inf. Retr. 14, 5 (2011), 441­465.",0,,False
514,"[7] Bruce Cro , Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice (1st ed.). Addison-Wesley Publishing Company.",0,,False
515,"[8] Sco Deerwester, Susan T. Dumais, George W. Furnas, omas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. 41, 6 (1990), 391­407.",0,,False
516,"[9] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Cro . 2017. Neural Ranking Models with Weak Supervision. In SIGIR '17.",0,,False
517,"[10] Fernando Diaz. 2015. Condensed List Relevance Models. In ICTIR '15. 313­316. [11] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. ery Expansion with",0,,False
518,Locally-Trained Word Embeddings. In ACL '16. [12] Michael U. Gutmann and Aapo Hyva¨rinen. 2012. Noise-contrastive Estimation of,0,,False
519,"Unnormalized Statistical Models, with Applications to Natural Image Statistics. J. Mach. Learn. Res. 13, 1 (2012), 307­361. [13] Kalervo Ja¨rvelin and Jaana Keka¨la¨inen. 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 422­446.",0,,False
520,[14] Yufeng Jing and W. Bruce Cro . 1994. An Association esaurus for Information Retrieval. In RIAO '94. 146­160.,0,,False
521,[15] Tom Kenter and Maarten de Rijke. 2015. Short Text Similarity with Word Embeddings. In CIKM '15. 1411­1420.,0,,False
522,"[16] Ma J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From Word Embeddings to Document Distances. In ICML '15. 957­966.",0,,False
523,"[17] Saar Kuzi, Anna Shtok, and Oren Kurland. 2016. ery Expansion Using Word Embeddings. In CIKM '16. 1929­1932.",0,,False
524,"[18] John La erty and Chengxiang Zhai. 2001. Document Language Models, ery Models, and Risk Minimization for Information Retrieval. In SIGIR '01. 111­119.",0,,False
525,"[19] Victor Lavrenko, Martin Choque e, and W. Bruce Cro . 2002. Cross-lingual Relevance Models. In SIGIR '02. 175­182.",0,,False
526,[20] Victor Lavrenko and W. Bruce Cro . 2001. Relevance Based Language Models. In SIGIR '01. 120­127.,0,,False
527,[21] Omer Levy and Yoav Goldberg. 2014. Neural Word Embedding as Implicit Matrix Factorization. In NIPS '14. 2177­2185.,0,,False
528,"[22] Ying Li, Zijian Zheng, and Honghua (Kathy) Dai. 2005. KDD CUP-2005 Report: Facing a Great Challenge. SIGKDD Explor. Newsl. 7, 2 (2005), 91­99.",0,,False
529,"[23] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-yi Wang. 2015. Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classi cation and Information Retrieval. In NAACL '15. 912­921.",0,,False
530,"[24] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS '13. 3111­3119.",1,ad,True
531,[25] Andriy Mnih and Geo rey E Hinton. 2009. A Scalable Hierarchical Distributed Language Model. In NIPS '09. 1081­1088.,0,,False
532,[26] Frederic Morin and Yoshua Bengio. 2005. Hierarchical Probabilistic Neural Network Language Model. In AISTATS '05. 246­252.,0,,False
533,"[27] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search. In InfoScale '06.",0,,False
534,"[28] Je rey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In EMNLP '14. 1532­1543.",0,,False
535,[29] Jay M. Ponte and W. Bruce Cro . 1998. A Language Modeling Approach to Information Retrieval. In SIGIR '98. 275­281.,0,,False
536,"[30] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Hamed Zamani. 2017. Word Embedding Causes Topic Shi ing; Exploit Global Context!. In SIGIR '17.",0,,False
537,"[31] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Guido Zuccon. 2016. Generalizing Translation Models in the Probabilistic Relevance Framework. In CIKM '16. 711­720.",0,,False
538,[32] J. J. Rocchio. 1971. Relevance Feedback in Information Retrieval. In e SMART Retrieval System: Experiments in Automatic Document Processing. 313­323.,0,,False
539,"[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning representations by back-propagating errors. Nature 323 (Oct. 1986), 533­536.",0,,False
540,"[34] T. Saracevic. 2016. e Notion of Relevance in Information Science: Everybody knows what relevance is. But, what is it really? Morgan & Claypool Publishers.",0,,False
541,"[35] Alessandro Sordoni, Yoshua Bengio, and Jian-Yun Nie. 2014. Learning Concept Embeddings for ery Expansion by antum Entropy Minimization. In AAAI '14. 1586­1592.",0,,False
542,[36] Tao Tao and ChengXiang Zhai. 2006. Regularized Estimation of Mixture Models for Robust Pseudo-relevance Feedback. In SIGIR '06. 162­169.,1,Robust,True
543,[37] Ivan Vulic´ and Marie-Francine Moens. 2015. Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings. In SIGIR '15. 363­372.,0,,False
544,[38] Jinxi Xu and W. Bruce Cro . 1996. ery Expansion Using Local and Global Document Analysis. In SIGIR '96. 4­11.,0,,False
545,"[39] Hamed Zamani, Michael Bendersky, Xuanhui Wang, and Mingyang Zhang. 2017. Situational Context for Ranking in Personal Search. In WWW '17. 1531­1540.",0,,False
546,[40] Hamed Zamani and W. Bruce Cro . 2016. Embedding-based ery Language Models. In ICTIR '16. 147­156.,0,,False
547,[41] Hamed Zamani and W. Bruce Cro . 2016. Estimating Embedding Vectors for eries. In ICTIR '16. 123­132.,0,,False
548,"[42] Hamed Zamani, Javid Dadashkarimi, Azadeh Shakery, and W. Bruce Cro . 2016. Pseudo-Relevance Feedback Based on Matrix Factorization. In CIKM '16. 1483­ 1492.",1,ad,True
549,"[43] ChengXiang Zhai, William W. Cohen, and John La erty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In SIGIR '03. 10­17.",0,,False
550,[44] Chengxiang Zhai and John La erty. 2001. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In CIKM '01. 403­410.,0,,False
551,"[45] Chengxiang Zhai and John La erty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Trans. Inf. Syst. 22, 2 (2004), 179­214.",0,,False
552,[46] Guoqing Zheng and Jamie Callan. 2015. Learning to Reweight Terms with Distributed Representations. In SIGIR '15. 575­584.,0,,False
553,"[47] Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu. 2015. Learning Continuous Word Embedding with Metadata for estion Retrieval in Community estion Answering. In ACL '15. 250­259.",1,ad,True
554,514,0,,False
555,,0,,False

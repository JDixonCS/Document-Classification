,sentence,label,data,regex
0,Session 4C: Queries and Query Analysis,1,Session,True
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,E icient & E ective Selective ery Rewriting with E iciency Predictions,0,,False
3,Craig Macdonald,0,,False
4,"University of Glasgow Glasgow, Scotland, UK craig.macdonald@glasgow.ac.uk",0,,False
5,Nicola Tonello o,0,,False
6,"ISTI-CNR Pisa, Italy nicola.tonello o@isti.cnr.it",0,,False
7,Iadh Ounis,1,ad,True
8,"University of Glasgow Glasgow, Scotland, UK iadh.ounis@glasgow.ac.uk",1,ad,True
9,ABSTRACT,0,,False
10,"To enhance e ectiveness, a user's query can be rewri en internally by the search engine in many ways, for example by applying proximity, or by expanding the query with related terms. However, approaches that bene t e ectiveness o en have a negative impact on e ciency, which has impacts upon the user satisfaction, if the query is excessively slow. In this paper, we propose a novel framework for using the predicted execution time of various query rewritings to select between alternatives on a per-query basis, in a manner that ensures both e ectiveness and e ciency. In particular, we propose the prediction of the execution time of ephemeral (e.g., proximity) posting lists generated from uni-gram inverted index posting lists, which are used in establishing the permissible query rewriting alternatives that may execute in the allowed time. Experiments examining both the e ectiveness and e ciency of the proposed approach demonstrate that a 49% decrease in mean response time (and 62% decrease in 95th-percentile response time) can be a ained without signi cantly hindering the e ectiveness of the search engine.",0,,False
11,"ACM Reference format: Craig Macdonald, Nicola Tonello o, and Iadh Ounis. 2017. E cient & E ective Selective ery Rewriting with E ciency Predictions. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080827",1,ad,True
12,1 INTRODUCTION,1,DUC,True
13,"Search engines, such as those for the Web, are required to be e ective at answering users' queries but yet also e cient. In particular, while the relevance of the results are important for users' satisfaction, users are in general not willing to wait long for the results to arrive [36]. While search engines are operated in distributed retrieval se ings that can be scaled horizontally to reduce response times, the rami cation of this is increased cost (in terms of both capital outlay and running, e.g., for power) for the search engine infrastructure. is being the case, the e ciency of the search engine is key to providing e ective results without excessive nancial burden. Typically, the infrastructure is designed to maintain a service level, where high percentile response time (the so-called ""tail latencies"" [16, 19]) should not exceed a given target. Indeed, for the Bing search engine, the target is reported to be that 99% percentile response time should not exceed 100 ms [19].",0,,False
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080827",1,ad,True
15,Table 1: Example rewrites for the query `poker tournament'.,0,,False
16,Original query: poker tournament Stemming: poker #syn(tournaments tournament) Proximity: poker tournament #1(poker tournament)0.1,0,,False
17,#uw8(poker tournament)0.1 Stemming and poker #syn(tournaments tournament),0,,False
18,Proximity: #1(poker #syn(tournaments tournament))0.1 #uw8(poker #syn(tournaments tournament))0.1,0,,False
19,"On the other hand, techniques that bene t the e ectiveness of a search engine may hinder e ciency [41], due to their complex nature. For example, in a modern search engine deploying learningto-rank approaches, the number of features to be computed and the learned models both contribute complexity, and have been the subject of recent studies (e.g., [25]). However, the time to traverse the inverted index's posting lists for the query terms, to identify the top K documents -- which are then re-ranked by the learned approach -- takes signi cant time [14].",0,,False
20,"O en the query submi ed by the user is internally rewri en by the search engine to improve the quality of the search results [20, 33]. For instance, traditional pseudo-relevance feedback approaches typically results in a much larger query, with signi cant negative impact on e ciency. More recently, less aggressive query rewriting approaches such as term proximity [30], query substitutions [20] and query-time stemming [34] have been deployed by search engines. Each query rewriting approaches can lead to a query with additional terms, resulting in prolonged execution times.",1,ad,True
21,"Table 1 shows three possible rewritings of the query `poker tournament', based on application of combinations of stemming [34] and sequential dependence (proximity) [30]. In the rewri en examples using an Indri-like query language, complex query operators [37] denote additional postings lists that must be traversed during retrieval, namely: #syn, which combines the constituent terms into a single posting list; #1 creates a posting list representing an exact occurrence of an n-gram; and #uw creates a posting list that provides the number of times a n-gram appears in an unordered window of size  [37]. While the search engine may have indexed posting lists for some n-grams, not all possible query operators may have existing posting lists, and hence ephemeral posting lists are required, which need to be created on-the- y from the constituent terms. Moreover, statistics such as the total number of postings in an ephemeral posting list are unknown, and hence the number of postings to be processed and the resulting execution time of a query containing complex operators cannot be known in advance.",1,ad,True
22,"Indeed, while recent work in query e ciency prediction has shown the possibility of estimating the execution time of a query prior to its processing [19, 21, 29, 38], none of the existing work",0,,False
23,495,0,,False
24,Session 4C: Queries and Query Analysis,1,Session,True
25,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
26,Query,1,Query,True
27,Query Rewriting,1,Query,True
28,Rewritten Query,1,Query,True
29,Top K Processing,0,,False
30,Features Lookup and Calculation,0,,False
31,Documents,0,,False
32,Learned Ranking Function,0,,False
33,Unigram Inverted,0,,False
34,Index,0,,False
35,Top K Retrieval,0,,False
36,Features Repository,0,,False
37,Feature Extraction,0,,False
38,Learning to Rank Technique,0,,False
39,Learned Model Application,0,,False
40,Training Data,0,,False
41,Figure 1: A pictorial representation of the reference web search engine architecture that we consider in this work.,0,,False
42,"has considered the execution time of queries containing query operators that generate ephemeral posting lists, such as #syn, or #1.",0,,False
43,"is makes it di cult to select among query rewriting strategies that use such operators, as their likely execution time is unknown. Hence, in this work, we study the cost of scoring ephemeral posting lists, and use these observations to de ne accurate query e ciency predictions for advanced query operators. Furthermore, we use these query e ciency predictions to instantiate a novel mechanism that selects the best strategy among alternative query rewritings, to improve e ciency, while minimising impact on e ectiveness.",1,ad,True
44,"Our conducted experiments to measure the e ciency of our proposed selective mechanism upon TREC Web track test collections show that a 49% decrease in mean response time, and 62% decrease in tail (95th-percentile) response time, can be a ained without signi cantly hindering the e ectiveness of the search engine. e contributions of this work are as follows: we show how to make query e ciency predictions for ephemeral posting lists created by complex operators such as #syn and #1; we use these advanced query e ciency predictions to propose a selector mechanism that permits the query to be rewri en in an e ective manner while considering a target response time that the search engine should aim to meet.",1,TREC,True
45,"e remainder of this paper is structured as follows: Section 2 provides an overview of a reference search engine architecture, describing the necessary background; Section 3 positions our contributions with respect to existing work; Section 4 describes our mechanism for selecting among query rewrites; Section 5 proposes new query e ciency predictors suitable for application to complex query operators. Our experimental setup and results follow in Sections 6 & 7. Finally, we provide concluding remarks in Section 8.",0,,False
46,2 PRELIMINARIES,0,,False
47,"In this section, we provide some essential background on index organisation and query processing in search engines. In doing so, we follow the reference architecture for a search engine depicted by Figure 1. e following section summarises and discusses the stateof-the-art query rewriting techniques and approaches addressing e cient but e ective retrieval.",1,ad,True
48,"Index Organisation. Given a collection D of documents, each document is identi ed by a non-negative integer called document identi er, or docid di . A posting list It is associated to each term t appearing in the collection, containing the list of the docids of all the documents in which the term occurs at least once. e collection of the posting lists for all of the terms is called the inverted index of D,",0,,False
49,"while the set of the terms is usually referred to as the lexicon. For each term t, the lexicon stores a pointer to its posting list as well as additional information on the statistics of the term in the collection, such as its document frequency Nt , i.e., the length of its posting list, and the total number of occurrences of the term in the collection Ft . Each posting in a posting list typically contains additional information about the term's occurrences in the document, such as the number of occurrences ft,d , and the set of positions, pt,d , where the term t occurs [13]. is position information facilitates phrasal retrieval without resort to large n-gram index data structures.",1,ad,True
50,"e docids in a posting list can be sorted in increasing order enabling the use of e cient compression algorithms and query processing [31]; or the posting lists can be frequency-sorted [39] or impact-sorted [2], allowing for good compression rates, but also presenting practical disadvantages such as their di culty of use for phrasal queries [22, 37]. As such, in this paper, we focus on the more common search scenario of docid-sorted index layouts [15].",1,ad,True
51,"ery Processing. e top K ranked retrieval stage identi es the K highest scored documents in the collection, where the relevance score is a function of the query-document pair. Multi-stage retrieval systems have become the dominant model for e cient and e ective web search engines [14]. In such systems (see Figure 1), a rst ""top K"" stage retrieves from the inverted index a relatively small set of K possibly-relevant documents matching the user query, focusing on optimising recall. Subsequent stages compute additional query dependent (e.g., elds [28], proximity) and query independent features, before applying a learning-to-rank technique to re-rank the K documents coming from the rst stage, aiming to maximise measures like NDCG [25]. e inverted index posting lists are processed in the rst stage only, to produce a small set of candidate documents, that will be re-ranked in the subsequent stage(s).",1,ad,True
52,"Documents are typically scored in the rst stage retrieval by the (weighted) linear combinations of weighting model (e.g., BM25, language models) functions computed for each term-document pair. Such weighting models are usually monotonically increasing in the number of occurrences of the term in the document ft,d . An obvious way to compute the top K scored documents is to exhaustively apply the weighting model to all the documents that match at least one query term in the inverted index. As such an exhaustive method is very expensive for large collections, several dynamic pruning techniques have been proposed in the last few years. Dynamic pruning makes use of the inverted index, augmented with additional data structures, to skip documents that cannot reach a su cient score to enter the top K. us, the nal result is the same as an exhaustive evaluation, but obtained with signi cantly less work. ese techniques include MaxScore [39], WAND [6], and BMW [17]. In this paper, we focus our a ention on the WAND strategy, since we deal with (rewri en) queries that can have a large number of terms (i.e., long queries). Indeed, several previous studies have con rmed that MaxScore performs be er than WAND for short queries while the opposite happens for long queries [18, 31] while, as we will discuss in Sec. 5, the BMW techniques are not suitable for processing rewri en queries.",1,ad,True
53,"WAND augments the posting list of each term t with an upper bound t on the maximum score of that term among all documents in the list. While processing the query by iterating on the posting lists of its terms, it records the top K scores among the documents",0,,False
54,496,0,,False
55,Session 4C: Queries and Query Analysis,1,Session,True
56,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
57,"evaluated thus far. To enter the top K, a new document needs to have a larger score than the current K-th score, which we call the min score. WAND maintains the posting list iterators sorted by increasing docid; at every step, it sums up the maximum scores of the lists in increasing order, until the min score is reached. It can be seen that the current docid of the rst list that exceeds the min score is the rst docid that can reach a score higher than the min score, so the other iterators can safely skip all the documents up to that docid. e alignment of the posting lists during WAND processing is achieved by means of a nextt (d) method upon the posting list iterators, which returns the smallest docid in the posting list It that is greater than or equal to d. is functionality signi cantly enhances the retrieval speed exhibited by WAND, by skipping docids that would never be retrieved in the top K, and hence avoiding their decompression and scoring. Indeed, smaller values of K allow for more skipping, since the threshold is in general larger for small values of K than for large values, resulting in smaller query processing times, as reported, for example, in [38].",0,,False
58,3 RELATED WORK,0,,False
59,"In the following, we survey existing work in query rewriting and in query e ciency predictions, and position our work accordingly.",0,,False
60,ery Rewriting. ere are a number of related works across the areas of query rewriting and e cient yet e ective retrieval.,0,,False
61,"e internal rewriting of a user's query within an IR system has a long history, including pseudo-relevance feedback in the form of automatic query expansion, rst deployed by the SMART system in TREC-3 [7], while others considered the correction of spelling errors or the application of ontologies to identify related concepts [13, Ch. 6]. Indeed, query expansion, which adds additional terms to the query based upon their appearance in the top ranked documents, has been shown to be e ective for adhoc retrieval tasks in evaluation forums such as TREC [42]. For web search, the signi cantly longer generated queries, as well as the need to conduct two retrieval phases, make pseudo-relevance feedback approaches infeasible for e cient retrieval. A further risk is the possibility that the topic of the expanded query can dri from the intent of the initial query.",1,TREC,True
62,"Instead, some of the techniques widely deployed in web search have focused on rewriting the query based on the large amounts of user interaction data available to a web search engine. For instance Jones et al. [20] describe a way to mine common query reformulation pa erns, based on log likelihood ratio, that can be automatically applied to re ne a new query. Random walks on the query-click graph [12] o er similar possibilities for identifying common paraphrasing queries.",1,ad,True
63,"Rewriting the query to include common variants of the original query terms can have an important e ectiveness bene t in addressing the word mismatch problem. Indeed, a query-side approach to stemming has a marked advantage of index-time stemming, in that the other words within the query can be taken into account to decide if the stemming is appropriate for a given word. For instance, Peng et al. [34] describe a context-sensitive stemming approach where query segments1 are carefully considered for stemming, by comparing the language model generation probability of both the original and the replacement segments. Naturally, adding additional terms",1,ad,True
64,"1 N-gram subsequences of queries that demonstrate the underlying grammatical structure, usually determined by dividing longer sequences to maximise n-gram language model probabilities.",0,,False
65,"to the query can have a marked negative impact on e ciency, hence, as noted by Peng et al., it is not desirable to rewrite queries unnecessarily. In our work in this paper, query rewriting by application of stemming is one of the query rewriting techniques that we consider.",0,,False
66,"One method of query rewriting that has gained signi cant bene ts in e ectiveness is the application of term dependence (proximity) operators, to boost the retrieval of documents where the query terms occur close together [30, 35]. In particular, Metzler & Cro 's Markov Random Field sequential dependence model makes use of the Indri complex query operators #1 and #uw formed from adjacent pairs of query terms, added to the original query terms with low weights (typically [0.05,0.1]). However, such rewri en queries have a negative e ciency impact, in that more posting lists must be traversed, while if the index only has unigram posting lists, ephemeral posting lists must be created to handle the #1 and #uw operators2. Another variant, the full dependence model ­ which adds complex query operators for each pair of query terms ­ is generally considered too ine cient for common retrieval use [4, 30]. Hence, for large-scale environments, there has to be a perceived bene t in deploying such a term dependence model, due to its inherent negative e ciency impact. For this reason, we note various works that extract term dependence proximity features at the re-ranking stage [28, 38] ­ an approach that we deploy within our baseline retrieval system in this paper.",1,ad,True
67,"On the other hand, motivated by the ine ciencies in deploying sequential dependence, Wang et al. [41] proposed an e cient variant where the weights for bi-gram operators were adjusted to jointly optimise combinations of e ectiveness and e ciency, and bi-grams predicted not to be useful were eliminated. In this way, the work of Wang et al. is one of the closest to our work. However, their features for estimating the cost of the #1 and #uw bi-gram complex operators assumed the existence of bi-gram index statistics, something that our approach does not require; Moreover, their experiments did not consider deployment under a dynamic pruning strategy, where the e ciency cost of additional complex proximity operators, ­ which have low weighting in the query (see Table 1) ­ may be markedly reduced by the pruning. Finally, as we consider more than just proximity rewriting, our work is more general than that of Wang et al. [41].",1,ad,True
68,"E ciency Predictions. Using a docid-sorted index layout, the time taken for a query to execute is correlated with the length of the posting lists of the query's constituent terms, as these posting lists must be traversed during execution. Dynamic pruning techniques such as WAND and BMW o er some relief as they o er the potential to safely skip the decompression of postings and the scoring of documents that cannot make the current top K. is makes the exact response time of a query di cult to predict, as not every posting in the postings lists will be decompressed and scored. Nevertheless recent work has considered making accurate predictions on the e ciency of a query, either in terms of absolute response time [29], or in terms of those queries with response times exceeding a threshold [19, 21].",0,,False
69,"E ciency predictions facilitate a number of applications for ensuring e cient yet e ective retrieval - for instance, routing queries",0,,False
70,"2 For instance, according to the recent IR system reproducibility e ort [23], Indri's average response time is decreased by a factor of 6 on deploying sequential dependence on the Gov2 corpus.",1,Gov,True
71,497,0,,False
72,Session 4C: Queries and Query Analysis,1,Session,True
73,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
74,"among busy replicated query shard servers [29]; selectively deploying multiple CPU cores for slow queries [19, 21]; or adjusting the pruning aggressiveness or size of K for di erent queries [5, 14, 38]. Of these, the work of Tonello o et al. [38] is among the most similar to ours, in that they vary the number of documents to be retrieved, K, as well as the pruning aggressiveness, before passing to a learning-to-rank re-ranking phase, based on the predicted execution time of the query. Similarly, in a very recently published work, Culpepper et al. [14] de ned an approach for training the rank cuto in a multi-stage ranking system based on closeness in overlap to a ""reference"" system. However, their approach has a key disadvantage in that they use a simple reference system, and hence would not demonstrate the bene t in going beyond that system, for instance in deploying advanced query rewrites.",1,ad,True
75,"Indeed, di erently from [14] & [38], in this paper, we go further by considering a prediction of the execution time of possible rewritings of the users' original queries. is is made possible by the novel prediction of the execution time of complex query operators such as #syn and #uw. In the following, we rstly de ne the problem and introduce our selection mechanism (Section 4), before de ning how to obtain query e ciency predictions for queries involving complex operators in Section 5.",1,ad,True
76,4 SELECTING AMONG QUERY REWRITINGS,0,,False
77,4.1 Problem Statement,0,,False
78,"is work considers the e cient yet e ective rewriting of a given query q. Indeed, the search engine may consider several possible ways to reformulate the query into a re ned instance q, for instance, by spli ing compound words, adding alternative words identi ed using stemming algorithms or common query reformulations approaches, or adding proximity terms such as #1. Some such query rewritings can result in a longer query formulation, hindering its e ciency compared to the original query [33].",1,ad,True
79,"Reformulating the query in a multi-stage ranking system ­ such as one deploying learning to rank ­ can be seen as aiming to improve the recall of the K documents retrieved in the Top K Retrieval stage. is ensures that when re-ranking the documents by the application of the learned model, the search engine has a high chance of identifying the most relevant documents for promotion to the top of the ranked list for presentation to the user. Naturally, improving the formulation of the query may also increase the high precision of these K documents ­ i.e., by retrieving more relevant documents towards the top of that initial list. is suggests that some rewri en queries only need a smaller K  < K. Moreover, as mentioned above, the e ciency of dynamic pruning techniques like WAND is bene ted by smaller K.",0,,False
80,"Hence, with various di erent rewriting techniques available, a natural question arises: for a given query q, which possible rewritings are appropriate to be applied to the query, q1 . . . qn , such that e ectiveness may be improved, and/or, K reduced to improve e ciency, without signi cantly damaging the overall e ectiveness.",0,,False
81,4.2 Selection Mechanism,0,,False
82,"To achieve this, we make use of query e ciency predictions that estimate the execution time of di erent rewritings of the query, before one is selected and executed. A particular challenge in doing so is making accurate estimations of the execution times of",0,,False
83,"rewri en queries that use operators such as #syn, #1 and #uw. We discuss this further in Section 5.",0,,False
84,"Firstly, we generate all possible rewritings {q1 . . . qn } of the original query q. We note that the size of this set varies for each query q, as not all rewritings are applicable to each query ­ for instance, no term dependence can be applied to a query with only a single term, or no stemming may be applicable for each query term. At the very least, the original query will always be present.",0,,False
85,"We also consider m di erent K values for the number of documents to retrieve in the rst retrieval phase, which will then be re-ranked by application of the learned model, namely K1 . . . Km . In doing so, our intuition is that for some queries, simply identifying K ,"" 20 documents will be su cient to identify enough relevant documents, leading to marked e ciency bene ts, particularly if the rewri en query permits higher recall of relevant documents within the top K. A possible plan for executing a query can be denoted as the tuple qi, Kj . All possible query plans, denoted P (q), for executing the query can be generated from the Cartesian product:""",1,ad,True
86,"P (q) ,"" qi, Kj "", {q1 . . . qn } × {K1 . . . Km }",0,,False
87,(1),0,,False
88,"e aim is then to rank and eliminate plans qi, Kj based on their predicted e ciency and expected e ectiveness. While the number of possible rewrites for each query varies, |P (q)|  m.",0,,False
89,"Consider that the search engine has a service level agreement in place [19], which aims to maximise the number of queries answered in time  . Some plans for queries may exceed  . We use query e ciency predictions, denoted t ( qi, Kj ) to eliminate such plans:",0,,False
90,"EP (q,  ) ,"" qi, Kj  P (q) | t ( qi, Kj )  """,0,,False
91,(2),0,,False
92,"Naturally, plans will vary in e ectiveness. One possible approach to select among the feasible plans EP (q,  ) would be to try to predict the e ectiveness of a given rewriting of a query. However, Tonello o et al. [38] examined the usefulness of query performance (e ectiveness) predictors within their selective pruning mechanism, and found them to have li le correlation with maintaining e ectiveness while enhancing e ciency. Moreover, we are not aware of any existing works that make e ectiveness predictions in the presence of complex operators used by some rewritings.",0,,False
93,"Instead, to determine the likely e ectiveness of a plan qi, Kj , we measure the expectation of the e ectiveness for some measure µ (e.g., NDCG) of that given rewriting upon a set of training queries Qtr. Denoting with Qitr, Kj the set of query plans for the rewri en queries according to the i-th rewriting, we compute the expected e ectiveness of measure µ over all such query plans, Eµ ( Qitr, Kj ). However, due to excessively long posting lists, some queries may not have any plans that can be executed in time  , i.e., |EP (q,  )| ,"" 0. For this reason, for such queries, we resort to a best a empt, by selecting the plan with the fastest predicted execution time. e""",1,ad,True
94,"nal selection mechanism to identify the best plan F P (q,  ) for executing the (possibly rewri en) query q in time  is as follows:",0,,False
95,F,0,,False
96,"P (q, ",0,,False
97,),0,,False
98,",",0,,False
99,argmax Eµ,0,,False
100,"qi, Kj  E P (q, )",0,,False
101,(,0,,False
102,"Qitr, Kj",0,,False
103,argmin t (,0,,False
104,"qi, Kj  E P (q, )",0,,False
105,"qi, Kj",0,,False
106,),0,,False
107,),0,,False
108,"if |EP (q,  )| > 0 otherwise",0,,False
109,(3),0,,False
110,e usefulness of this mechanism is driven by the need to have ac-,0,,False
111,"curate estimation of the time to execute a given query plan, namely t ( qi, Kj ). Moreover, as mentioned above, the estimation of the",0,,False
112,498,0,,False
113,Session 4C: Queries and Query Analysis,1,Session,True
114,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
115,Table 2: Complex operators summary table.,0,,False
116,Complex operator,0,,False
117,#syn #1 #uw,0,,False
118,Complex term,0,,False
119,"#syn(car, cars) #1(new, york) #uw8(divx, codec)",0,,False
120,Ephemeral posting list,0,,False
121,Disjunctive/OR Conjunctive/AND Conjunctive/AND,0,,False
122,"execution time of complex query operators, particularly under dynamic pruning strategies such as WAND, have not previously been addressed. In the next section, we propose a novel method to address this problem, by using machine-learned models to predict the execution times of query plans that use complex operators.",1,ad,True
123,5 EFFICIENCY PREDICTION FOR COMPLEX OPERATORS,0,,False
124,"e complex operators involved in stemming and proximity rewritings are summarised in Table 2. Such complex operators can be applied to two or more uni-gram terms, as well as other complex operators, i.e., complex operators can be nested. Complex operators, such as #1, generate complex terms once instantiated, such as #1(new, york). In the following, we discuss how the posting lists for such complex terms are generated (Section 5.1) and how they can be processed together with the original terms in the WAND strategy (Section 5.2). Section 5.3 presents our approach for predicting the query processing time of queries containing complex terms.",0,,False
125,5.1 Complex terms and ephemeral posting lists,0,,False
126,"While posting lists for simple (e.g., uni-gram) terms are stored in the inverted index, it is not feasible to pre-compute statistics and posting lists for complex terms, since their space occupancy will quickly become unmanageable, particularly if complex operators can be nested. Hence, for a complex term #op(t1, . . . , #op1, . . .), its posting list and statistics must be generated on-the- y, such that it can be processed together with other simple and complex terms. Such ephemeral posting lists are materialised as required, depending on the complex operator and the involved terms. We consider two types of ephemeral posting lists: disjunctive/OR-based lists and conjunctive/AND-based lists (see Table 2). OR-based posting lists are used with #syn operators, where the posting lists of two or more terms are merged into a single posting list containing all docids appearing in at least one of the terms' posting lists. ANDbased posting lists are used with #1 and #uw operators, where two or more terms' posting lists must be intersected, i.e., a posting appears in the ephemeral posting list only if it is present in all of the involved posting lists.",0,,False
127,"e involved complex operator de nes how to merge postings into ephemeral posting lists. In OR-based ephemeral posting lists, when the postings of two di erent terms that refer to the same docid are merged, their term frequencies in a document must be added, and the positions arrays must be merged. Similarly, while the terms' frequencies in the collection (Ft ) must be summed, we cannot know in advance the document frequencies (Nt ) of the resulting ephemeral posting list, as the number of docids in common between the two posting lists is unknown. In AND-based ephemeral posting lists, there is no general way to merge term frequencies in documents, while term positions must be stored separately and processed depending on the semantics of the complex",1,ad,True
128,"operator [24]. Again, the document frequency Nt of the resulting ephemeral posting list, i.e., its length, cannot be known in advance.",1,ad,True
129,5.2 ery Processing,0,,False
130,"Given a user query q composed of simple terms, let us assume it",0,,False
131,"is rewri en into a query composed of simple terms and complex terms, collectively denoted by q. Complex terms are obtained by",0,,False
132,applying complex operators ­ summarised in Table 2 ­ to simple or,0,,False
133,"other complex terms, in a nested fashion. e score of a document d w.r.t. the rewri en query q can be expressed as follows:",0,,False
134,"s (q, d ) ,",0,,False
135,"wt st,d +",0,,False
136,wt,0,,False
137,st,0,,False
138,",",0,,False
139,d,0,,False
140,",",0,,False
141,(4),0,,False
142,t qd,0,,False
143,t  (q\q)d,0,,False
144,where wt (resp. wt) denotes the simple (resp. complex) terms,0,,False
145,weights,0,,False
146,(see,0,,False
147,Table,0,,False
148,"1),",0,,False
149,while,0,,False
150,"st,d",0,,False
151,and,0,,False
152,st,0,,False
153,",",0,,False
154,d,0,,False
155,are,0,,False
156,weighting,0,,False
157,models,0,,False
158,for simple and complex terms respectively. e linearity of Eq. (4),0,,False
159,"makes it easily usable in any exhaustive query processing algorithm,",0,,False
160,by exploiting ephemeral posting lists as normal posting lists.,0,,False
161,"Conversely, dynamic pruning techniques are not directly usable,",0,,False
162,"since they rely on the maximum score t of each query term, cal-",0,,False
163,"culated among all documents in the respective posting lists, i.e.,",0,,False
164,"t ,"" wt · maxd It st,d . ese maximum scores can be computed o ine by taking the score value of the top document of a single""",0,,False
165,"term query stored in the lexicon. However, since ephemeral post-",0,,False
166,"ing lists are materialised on-the- y, such a computation cannot be",0,,False
167,"performed o ine, hence we must resort to a runtime estimation for",0,,False
168,"upper bounds on these maximum scores. In [26], the authors pro-",0,,False
169,posed a general framework to approximate the term upper bounds,0,,False
170,for proximity weighting models that monotonically increase with,0,,False
171,"respect to the frequency variable, called MaxTF. In that framework,",0,,False
172,the upper bound t for a term t is computed by using the maximum,0,,False
173,term frequency fmax (t ) that appears in the term's posting list as,0,,False
174,"input for the scoring model, i.e.,",0,,False
175,"t ,"" wt st,d""",0,,False
176,"fmax (t ) ,",0,,False
177,where,0,,False
178,fmax (t ),0,,False
179,",",0,,False
180,max,0,,False
181,d It,0,,False
182,"ft,d .",0,,False
183,(5),0,,False
184,"In our experiments, we exploit the DLH13 weighting model [1]",0,,False
185,"for #syn operators and simple terms, and the pBiL dependence weighting model [35] for #uw and #1 operators3. Both models",0,,False
186,are a generalisation of the parameter-free hypergeometric DFR,0,,False
187,"model in a binomial case, DLH13 for simple terms while pBiL for",0,,False
188,"n-grams, and both are monotonically increasing with respect to",0,,False
189,"the frequency variable ft,d . Hence, given a complex term, we can compute a term upper bound by using the maximum term/n-gram",0,,False
190,frequency computed from the corresponding ephemeral posting,0,,False
191,"list, as in the MaxTF framework. Unfortunately, even computing",0,,False
192,these maximum frequencies is impossible without a complete view,0,,False
193,"of the posting lists, hence we must resort to a further upper bound",0,,False
194,"on those frequencies, easily computed as follows:",0,,False
195,"fmax #syn(t1, t2) ,"" fmax (t1) + fmax (t2),""",0,,False
196,"fmax #1(t1, t2) ,"" min fmax (t1), fmax (t2) ,""",0,,False
197,(6),0,,False
198,"fmax #uw(t1, t2) ,"" min fmax (t1), fmax (t2) .""",0,,False
199,"3 We use these models as they are parameter free, and their MaxTF upper-bound approximations were proven in [26]; However, the method we describe here is equally applicable for scoring simple and complex terms using Dirichlet language modelling. BM25 cannot be applied, as Nt is not available while scoring complex terms.",0,,False
200,499,0,,False
201,Session 4C: Queries and Query Analysis,1,Session,True
202,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
203,"For instance, consider an AND-based ephemeral posting list (such as #1): no document can have more occurrences of the n-gram than the minimum of the maximum frequencies of the involved terms that has been observed in their posting lists. For the OR-based #syn operator, the worst-case maximum frequency would occur if a single document had the maximum frequencies of the constituent terms. Hence, the maximum frequency within a #syn ephemeral posting list cannot be greater than the sum of the maximum observed frequencies of the respective constituent posting lists.",1,ad,True
204,"Such term upper bounds, computed by substituting Equations (6) into Equation (5) as required, can be used in dynamic pruning strategies such as MaxScore and WAND, which leverage global upper bounds on each term, both simple and complex. Conversely, query processing strategies such as BMW leverage local term upper bounds: each posting list is split into consecutive blocks of constant size, e.g., 128 postings per block, and, for each block, a score upper bound is computed and stored, together with largest docid of each block. is cannot be done with ephemeral posting lists, since their sequences of postings is not known in advance. Hence, no block score upper bounds can be computed or stored.",1,ad,True
205,5.3 Predicting Complex Operator E ciency,0,,False
206,"e time spent processing a query in dynamic pruning strategies depends on several factors, the most important being: (i) the number of terms to be processed, (ii) the total number of postings to be processed and (iii) the relative importance of terms, i.e., their score contribution to the overall relevance of a document [29]. As shown in [19, 21, 29, 38], by using statistics derived from terms and queries it is possible to estimate the query processing time. However, since we must deal with ephemeral posting lists, most of the statistics used in prior research are not applicable. For example, the mean scores of postings (used in [29]) cannot be precomputed for an ephemeral posting list, since the postings and their statistics for all possible n-grams cannot feasibly be computed o ine. Hence, we must resort to ""estimators"" of the quantities of interest, in particular for the total number to be processed, i.e., the simple and ephemeral posting list lengths, and the corresponding term upper bounds.",0,,False
207,"To provide an upper bound approximation to the number of postings in an ephemeral posting list, consider a generic complex term #op(t1, t2), where t1 and t2 can be simple or (nested) complex terms. If #op corresponds to an OR-based posting list such as #syn, then the size of its posting list cannot be greater than the sum of the document frequencies of its constituent terms. If #op corresponds to an AND-based posting list, then the total size of its postings list cannot be greater than the minimum of the document frequencies of its constituent terms, i.e., the smallest constituent posting list.",0,,False
208,"For term scores upper bound approximations, we leverage the MaxTF framework, adapted to complex operators, as summarised in Eq. (6). Upper bound approximations are scaled according to the term-speci c weight resulting from query rewriting (see Table 1).",1,ad,True
209,"We adopt a machine-learned approach to predict the processing times of complex queries, i.e., t ( qi, Kj ), for di erent values of K. As we use the WAND dynamic pruning strategy (as justi ed in Section 2), the response times heavily depend on the K number of documents to be retrieved. Hence, we train a di erent model for each value of K, but all models share the same set of statistics. All previous works on query processing time prediction use of query statistics and aggregations (max, min, mean etc.) of term-based",1,ad,True
210,"Table 3: Prediction statistics, projectors and aggregators.",0,,False
211,Statistics,0,,False
212,1. Number of terms (query-based) 2. Document frequency (term-based) 3. Score upper bound (term-based),0,,False
213,Projectors,0,,False
214,"1. Global 2. Original-only terms 3. #syn-only terms 4. #1-only terms 5-... #uw-only terms, one per di erent ",0,,False
215,Aggregators,0,,False
216,"1­2. Minimum, Maximum 3­5. Arithmetic, Harmonic, Geometric Means",0,,False
217,"statistics across the query terms, as reported in Table 3. However, di erent complex operators acting on the same set of terms can have very di erent impacts on the running time of a query. Hence, we propose an additional, intermediate step between statistics generation and aggregation, namely projection. In this step, all query- and term-based statistics are divided into subsets, whose elements are grouped depending on the nature of the term. us we have several statistics projections, one for every complex operator, one for simple terms and one considering all terms globally. Term-based aggregators are then applied to these subsets of statistics. Note that, due to the di erent nature of AND- and OR-based posting lists, we do not consider the sum and variance operators, while we include the minimum operator, as suggested by [19]. In our experiments, we use #uw8 and #uw12 operators, hence we have 6 query-based features and 2×6×5 term-based features, for a total of 66 features. Section 7.1 provides the details and parameters of the learning algorithm using these feature to predict the processing times of complex queries.",1,ad,True
218,6 EXPERIMENTAL SETUP,0,,False
219,6.1 Research estions,0,,False
220,"In the following, we experiment to address two research questions: RQ1: How accurate are our query e ciency predictions for queries with complex operators? (Section 7.1) RQ2: Can we maintain e ectiveness while reducing response time when selectively rewriting queries? (Section 7.2)",1,ad,True
221,"In the remainder of this section, we de ne the experimental setup under which our experiments are conducted.",0,,False
222,6.2 Datasets & Retrieval System,0,,False
223,"Our experiments address both e ciency and e ectiveness, and hence require diverse setups to ensure accurate conclusions can be drawn for both types of measures. All of our experiments are conducted on the TREC ClueWeb09 category B corpus4, which consists of 50M Web documents. For testing e ectiveness, we use the 197 queries from the TREC Web tracks 2009-2012 that have corresponding relevance assessments on a 4-point scale [9], and denote this as WT. For testing e ciency, we follow best practices in",1,ad,True
224,4 h p://lemurproject.org/clueweb09/,0,,False
225,500,0,,False
226,Session 4C: Queries and Query Analysis,1,Session,True
227,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
228,"sampling a signi cant number of queries from a real search engine, namely 1,956 successive queries from the MSN 2006 query log5 [11].",0,,False
229,"We index all 50M documents of the ClueWeb09 corpus using the Terrier IR platform [32], including also the anchor text of incoming hyperlinks to each document. Position information is recorded for each term. Our index is compressed using Elias-Fano encoding provided in [40], widely considered to be the state-of-the-art in terms of fast decompression.",1,ClueWeb,True
230,"For retrieval, we conduct e ciency timings using a machine equipped with an AMD Opteron Processor 6276 with 6 MB L3 cache and 128 GB RAM. e entire index is loaded in memory. All experiments are performed on a single core. While the resulting retrieval times using a single machine for retrieval are marginally higher than would be expected for interactive retrieval in a deployed Web search engine, following previous work [41], this does not detract from the generality of the ndings, and avoids the complexities of performing experiments in a distributed retrieval environment.",1,ad,True
231,"Finally, in our timing experiments, we do not include the time to rewrite the query, calculate e ciency predictions, nor to apply the learned model. Each of these stages is comparatively cheap: for instance, the rewriting approaches discussed below are commonly deployed in search engines; query e ciency predictions can be calculated quickly using only term statistics from the lexicon [19, 29]; and the application of a learned model is also relatively less expensive than the top-K retrieval [38].",0,,False
232,6.3 ery Plans,0,,False
233,"Besides the None strategy, where the original queries are not rewritten, we deploy the following rewriting approaches:",0,,False
234,"MRF. To encapsulate proximity in rewriting a query, we deploy sequential dependence [30] proximity, by considering #1, #uw8 and #uw12 query operators for adjacent query terms.",1,ad,True
235,"Na¨ive. To address word mismatch, we rewrite the query by adding alternatives to query terms within a #syn query operator6. Inspired by Peng et al. [34], who noted corpus analysis as being a suitable method to determine similar words (based on which words they co-occur with). We select alternative terms for a given term t that each have the same stem as t based on Porter's stemmer, and which are among the M most similar terms to t within a word embedding space. We use Deeplearning4j's word2vec tool and word embeddings vectors trained on Wikipedia and the Gigaword corpus7 and select words with common stems in the top M , 20 for each query term t. is results in a less aggressive stemming than either index-time stemming or the equivalent query-side stemming using all alternatives identi ed by a Porter stemmer.",1,ad,True
236,"Na¨iveMRF. Finally, we mix Na¨ive and MRF rewritings to create a nal strategy of generating time-expensive query plans.",0,,False
237,"Table 4 reports the statistics of each query set, incl. the number of simple and complex terms generated by each rewriting approach. Finally, with regards to the value of K, i.e., the number of documents retrieved by WAND during the top K retrieval, we select 4 values, namely 20, 100, 1000 and 5000. is gives us a total of 16 query",0,,False
238,"5 From a sample of 2000 queries, 44 queries had no matching terms in our collection, so were removed. 6 Initial experiments using the context-sensitive approach of [34] showed no e ectiveness bene t over this simpler Na¨ive stemming. 7 Available from h p://nlp.stanford.edu/projects/glove/",1,ad,True
239,Table 4: ery sets statistics per rewriting.,0,,False
240,Dataset Train Test WT,1,WT,True
241,eries Rewriting simple #syn #1 #uw8 #uw12,0,,False
242,Na¨iveMRF 1458 1125 1133 1133,0,,False
243,0,0,,False
244,978,0,,False
245,MRF Na¨ive,0,,False
246,2556 0 1578 1578 781,0,,False
247,1458 1125 0 0,0,,False
248,0,0,,False
249,None,0,,False
250,2556 0 0 0,0,,False
251,0,0,,False
252,Na¨iveMRF 1452 1028 1185 1185,0,,False
253,0,0,,False
254,978,0,,False
255,MRF Na¨ive,0,,False
256,2468 0 1491 1491 768,0,,False
257,1452 1028 0 0,0,,False
258,0,0,,False
259,None,0,,False
260,2468 0 0 0,0,,False
261,0,0,,False
262,Na¨iveMRF 121 126 113 113,0,,False
263,0,0,,False
264,197,0,,False
265,MRF Na¨ive,0,,False
266,244 0 146 146 82,0,,False
267,121 126 0 0,0,,False
268,0,0,,False
269,None,0,,False
270,244 0 0 0,0,,False
271,0,0,,False
272,Table 5: ery-dependent (QD) & -independent (QI) ranking features within our experiments.,0,,False
273,"QD DLH13, Coordinate Level 2",0,,False
274,QD pBiL (term dependence),0,,False
275,2,0,,False
276,"QI Inlinks, Outlinks, PageRank 3",0,,False
277,QI URL features,0,,False
278,6,0,,False
279,QI Content quality [3],0,,False
280,4,0,,False
281,QI Spam score,0,,False
282,1,0,,False
283,Total,0,,False
284,18,0,,False
285,"plans to be plugged into our selective mechanism, and for which we need to train and test our e ciency predictors.",0,,False
286,6.4 Ranking features,0,,False
287,"As mentioned in Section 5, in the top K retrieval phase, we use the DLH13 term weighting model [1] from the Divergence from Randomness framework for weighting simple and #syn terms; For #1 and #uw terms, we use the DFR pBiL model [35]. Next, Table 5 lists the 18 features used for re-ranking the top K results during the application of the learned model. Note, that regardless of whether term dependency complex operators are deployed as a rewri en query, we include the score for the term dependency operators in the feature set. is allows the learner to consider the term dependence features separately from the score used in the initial ranking phase.",0,,False
288,"For learning and ranking, we use the Jforests implementation8 of LambdaMART [8], a gradient-boosted decision tree learning-torank technique. E ectiveness experiments on the 197 TREC Web track queries (denoted WT in Table 4) use a 5-fold cross validation, where each fold has 60% training, 20% validation and 20% test queries. We report NDCG@20.",1,ad,True
289,7 RESULTS,0,,False
290,"In the following, we report the results and analysis addressing our two research questions concerning the accuracy of our e ciency predictions for queries with complex operators (Section 7.1), and the application of the selective rewriting mechanism based upon those e ciency predictions (Section 7.2).",1,ad,True
291,8 h ps://github.com/yasserg/jforests/,0,,False
292,501,0,,False
293,Session 4C: Queries and Query Analysis,1,Session,True
294,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
295,"Table 6: Pearson correlation on the test query set, per rewriting and K value, of the baseline (Base) and the proposed predictors (Pred). Statistically signi cant improvements over the baseline are denoted in bold.",0,,False
296,Rewriting,0,,False
297,20,0,,False
298,K,0,,False
299,100,0,,False
300,1000,0,,False
301,5000,0,,False
302,Base Pred Base Pred Base Pred Base Pred,0,,False
303,Na¨iveMRF 0.639 0.833 0.722 0.840 0.827 0.881 0.861 0.868,0,,False
304,MRF,0,,False
305,0.612 0.803 0.697 0.813 0.788 0.870 0.790 0.884,0,,False
306,Na¨ive 0.620 0.848 0.735 0.878 0.828 0.915 0.886 0.935,0,,False
307,None,0,,False
308,0.548 0.738 0.691 0.842 0.802 0.907 0.884 0.952,0,,False
309,7.1 RQ1: E ciency Prediction,0,,False
310,"We use the 66 e ciency prediction features derived from statistics, projectors and aggregators summarised in Table 3 to train a machine-learned model on the 978 training queries (see Table 4), for every combination of rewriting and K value. e regression algorithm employed is gradient boosted regression trees, as provided by the scikit-learn toolbox. Denoting the 978 train queries as Qtr, we perform a 5-fold cross validation to train each Qitr, Kj model. Each model was trained with 20 trees, learning rate of 0.1, max depth of 5, and the least square loss function. To evaluate the accuracy of each model, we then used the 978 test queries Qte to compute the Pearson correlation, reported in Table 6. We compare each of our models (denoted Pred) with a linear regressor trained using the sum of number of postings per each simple term in the original terms and complex operators (Base). Our models perform very well, with correlations always higher than 0.8, and almost always signi cantly improving over the Base baseline predictor (according to a Fisher Z-transform, p < 0.05). To further assess the quality of our predictors, in Table 7, we compare the mean actual query times and the mean predicted query times for each model. Our models incur a mean prediction error no greater than 13%, even if they tend to underestimate the actual processing time, in particular for the models using the original queries (None).",1,ad,True
311,"Moreover, since we use the trained model to compare the predicted execution times of query plans versus a given time  , in Table 8 we report the precision/recall measures of our models when classifying queries with a predicted execution time greater than 0.750 seconds. As can be observed from the table, the recall of our models is close or above 0.9 for Na¨iveMRF, MRF and Na¨ive rewritings. e smaller recall value for None is not problematic, since very few queries exhibit an execution time greater than 0.750 seconds.",0,,False
312,"Finally, we use the feature importance metric within gradient boosted trees to assess the contribution of the 66 prediction features. To combine the feature importances across various models, for each model we rank features by decreasing importance, and measure mean reciprocal ranks (Mean RR) across models. Table 9 shows the top features across all trained models (reported features a ained Mean RR greater than 0.1). All proposed statistics and aggregators appear in the top features, with the exception of geometric mean (which appears, but it is not reported, as next in the list). Notably, also the minimum document frequency across all terms is in the list, validating the assumption that the minimum aggregator is useful",1,ad,True
313,"Table 7: Average actual and predicted query processing times on the test query set (in ms), per rewriting and K value, with relative errors (in %).",0,,False
314,Rewriting 20,0,,False
315,K,0,,False
316,100,0,,False
317,1000,0,,False
318,5000,0,,False
319,Actual times,0,,False
320,Na¨iveMRF 2718,0,,False
321,3272,0,,False
322,MRF,0,,False
323,1438,0,,False
324,1757,0,,False
325,Na¨ive,0,,False
326,992,0,,False
327,1373,0,,False
328,None,0,,False
329,374,0,,False
330,551,0,,False
331,4244,0,,False
332,5090,0,,False
333,2618,0,,False
334,3310,0,,False
335,1893,0,,False
336,2381,0,,False
337,802,0,,False
338,1042,0,,False
339,Predicted times,0,,False
340,Na¨iveMRF 2618 (-3.65) 3087 (-5.63) 4109 (-3.18) 4608 (-9.46),0,,False
341,MRF,0,,False
342,1511 (5.12) 1762 (0.29) 2655 (1.44) 3101 (-6.31),0,,False
343,Na¨ive,0,,False
344,953 (-3.92) 1261 (-8.16) 1866 (-1.42) 2339 (-1.78),0,,False
345,None,0,,False
346,369 (-1.19) 485 (-12.13) 829 (-3.38) 1029 (-1.29),0,,False
347,Table 8: Precision/Recall accuracy to classify queries taking more than 750 milliseconds to process.,0,,False
348,Rewriting,0,,False
349,20,0,,False
350,K,0,,False
351,100,0,,False
352,1000,0,,False
353,5000,0,,False
354,PRPRPRPR,0,,False
355,Na¨iveMRF 0.752 0.978 0.790 0.991 0.767 1.000 0.828 1.000,0,,False
356,MRF,0,,False
357,0.831 0.936 0.843 0.984 0.819 0.996 0.858 1.000,0,,False
358,Na¨ive 0.769 0.908 0.813 0.935 0.843 0.979 0.867 0.998,0,,False
359,None,0,,False
360,0.800 0.520 0.839 0.719 0.836 0.881 0.783 0.963,0,,False
361,Table 9: Top features ranked by the mean reciprocal rank of importance across all trained models.,0,,False
362,Aggregator,0,,False
363,Arithmetic mean Maximum Arithmetic mean Maximum Maximum Maximum Harmonic mean ­ Maximum Arithmetic mean Minimum,0,,False
364,Feature,0,,False
365,Document frequency Document frequency Score upper bound Document frequency Document frequency Document frequency Document frequency Number of terms Document frequency Document frequency Document frequency,0,,False
366,Projector,0,,False
367,global #syn global original #uw8 global global global #1 original global,0,,False
368,Mean RR,0,,False
369,0.404 0.350 0.306 0.257 0.243 0.215 0.185 0.171 0.152 0.129 0.109,0,,False
370,"when processing AND-based posting lists. Moreover, all projectors appear in the top features list, with the notable exception of #uw12. We explain this by observing that, according to Table 4, the corresponding complex operator occurs infrequently in the processed queries compared with the frequency of other complex operators.",0,,False
371,"Hence, in addressing RQ1, we have experimentally evaluated the accuracy of our predictions at estimating the execution times of rewri en queries processed using WAND for di erent K. Overall, with correlations > 0.8 for all tested rewritings and values of K, we conclude that our e ciency predictions are indeed accurate.",1,ad,True
372,502,0,,False
373,Session 4C: Queries and Query Analysis,1,Session,True
374,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
375,7.2 RQ2: Selective Rewriting,0,,False
376,"In this section, we experiment to determine the levels of e ciency and e ectiveness obtainable when using the selective mechanism proposed in Section 4, and using the predictors for complex operators evaluated in the preceding section. In terms of setup, our selective mechanism ranks the query plans by measuring µ , NDCG based on the validation set for each fold of the WT queries.",1,WT,True
377,"We consider the uniform application of None, K ,"" 5000 to all queries as the baseline that we compare to in terms of e ectiveness. In particular, the use of K "","" 5000 for retrieving on ClueWeb09 has been used by various previous work on the same test collection [10, 28] and empirically veri ed in [27]. We denote the uniform application of this query plan as """"Default"""". In our experiments, we aim to be more e cient than Default, while not experiencing a signi cant decrease in e ectiveness. In addition to Default, we also report the e ciency and e ectiveness of the uniform application of the Fastest, Slowest and Most E ective plans.""",1,ClueWeb,True
378,"Table 10 provides the main ndings of the e ciency and e ectiveness of the uniform query plans. Note that, as discussed in Section 6, we use di erent query sets for measuring e ciency and e ectiveness; in particular, we report mean and 95th percentile (or ""tail"") response times in milliseconds (MRT & TRT, respectively) on the 978 test queries from the MSN 2006 query log; for e ectiveness, we report NDCG@20 for the 197 TREC Web track queries.",1,TREC,True
379,"e rst group of rows reports e ciency and e ectiveness for the Uniform plans, while the lower group reports the results for the Selection mechanism as threshold  is varied. Finally, for each row, Rw denotes the percentage of queries rewri en from None.",0,,False
380,"On inspection of the uniform plan in the top half of Table 10, we note that the MRF and Na¨iveMRF uniform plans result in very high response times (up to 4.86 times slower than Default). In terms of e ectiveness, deploying MRF to increase the recall in the sample results in a marked (but not statistically signi cant) increase in NDCG e ectiveness of the system (0.18770.2001), however, this comes at the cost of retrieval that is 3.2 times slower than Default.",0,,False
381,"Next, we consider the selective mechanism in the bo om half of Table 10. As expected, as  is varied we note changes in both e ectiveness and e ciency. In particular, for  ,"" 500, we nd that a mean response time of 537 ms can be achieved (49% decrease in mean response time, and 62% decrease in tail response time, compared to Default), without signi cantly degrading e ectiveness (0.1877  0.1751). For this se ing, 8-10% of queries are being rewri en. With higher levels of  , we observe similar increases in e ectiveness and observed response times, and with more queries being rewri en. Finally, we note that the selection mechanism does not strictly observe the  threshold, as can be observed by the tail response times ­ this is expected, as the selection mechanism expressed in Equation (3) will default to the predicted fastest plan available (usually None, K "", 20 ) if no plans can be executed within  .",1,ad,True
382,"To provide a graphical illustration of the e ciency/e ectiveness tradeo , Figure 2 presents mean response times and mean NDCG. Lines are provided for both our selective mechanism, Full, denoted by a solid line, as well as the same selective mechanism where the candidate plans are restricted only to those involving None (i.e., with K ,"" {20, 100, 1000, 5000}), denoted None-only, and indicated by a dashed line. For most thresholds, the Full selective mechanism can be observed to o er the best tradeo , with points closer to the upper le hand corner. For low mean response times (e.g., < 400""",1,ad,True
383,"Table 10: E ciency/e ectiveness results using the selective mechanism. * denotes NDCG values that signi cantly di er from that of None, K ,"" 5000 (paired t-test, p < 0.05). TRT denotes the tail response time (95%-th percentile), Rw denotes the % of queries rewritten from None. Times are in ms.""",0,,False
384,Strategy,0,,False
385,E iciency (Test) MRT TRT Rw,0,,False
386,Uniform Plans,0,,False
387,"Default ( None, K ,"" 5000 ) Fastest ( None, K "","" 20 ) Slowest ( Na¨iveMRF, K "","" 5000 ) Most E ective ( MRF, K "", 5000 )",0,,False
388,1037 5281 0 376 1779 0 5045 17994 100 3281 12099 100,0,,False
389,Selective Mechanism,0,,False
390," , 200  , 300  , 400  , 500  , 600  , 700  , 750  , 800  , 900  , 1000",0,,False
391,449 1986 1 472 1986 1 495 1986 4 537 2004 8 577 2023 13 606 2060 19 617 2060 21 639 2128 22 691 2226 27 733 2256 30,0,,False
392,E ectiveness (WT) NDCG@20 Rw,1,WT,True
393,0.1877,0,,False
394,0,0,,False
395,0.1375*,0,,False
396,0,0,,False
397,0.1755 100,0,,False
398,0.2001 100,0,,False
399,0.1642*,0,,False
400,0,0,,False
401,0.1670*,0,,False
402,1,0,,False
403,0.1711*,0,,False
404,6,0,,False
405,0.1751 10,0,,False
406,0.1762 14,0,,False
407,0.1782 18,0,,False
408,0.1779 19,0,,False
409,0.1807 19,0,,False
410,0.1825 21,0,,False
411,0.1828 21,0,,False
412,0.180 0.175,0,,False
413,NDCG@20,0,,False
414,0.170,0,,False
415,0.165,0,,False
416,500,0,,False
417,600,0,,False
418,700,0,,False
419,MRT,0,,False
420,Sig. Diff. vs. Default  False True,0,,False
421,Selection Candidates Full None-only,0,,False
422,"Figure 2: E ciency/e ectiveness tradeo . e best tradeo occurs for points closest to the upper le corner. Points denoted with  are signi cantly less e ective than None, K ,"" 5000 (paired t-test, p < 0.05).""",1,ad,True
423,"ms), the None-only line rises above Full, as only None plans can be deployed to achieve such low response times. On the other hand, for larger mean response times (e.g., > 650 ms), the Full line is more e ective; this supports a central tenet of our work, i.e., when time allows, appropriate rewriting of queries results in increased e ectiveness. Moreover, as indicated by the  points of the None-only",0,,False
424,503,0,,False
425,Session 4C: Queries and Query Analysis,1,Session,True
426,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
427,"line in Figure 2, to achieve the response times savings without selectively rewriting the query, we would be forced to accept signi cant degradations in e ectiveness compared to the Default baseline.",1,ad,True
428,"Finally, to give a avour of the impact of our selective mechanism, we inspect the queries rewri en for  ,"" 500 and select the query `disneyland hotel'. is query was rewri en to use the plan MRF, K "","" 100 (as per the proximity example in Table 1), which had a predicted execution time of 486 ms, due to the relatively informative term disneyland (which only appears in Nt "","" 78422 documents). Hence, the more e ective MRF rewrite was applied, which resulted in a 14% increase in NDCG@20 compared the Default plan (which had a predicted execution time of 542 ms).""",1,ad,True
429,"Overall, in addressing RQ2 we have determined that our selective mechanism can achieve a 49% decrease in mean response time, and 62% decrease in tail (95th-percentile) response time without signi cant degradations in e ectiveness.",1,ad,True
430,8 CONCLUSIONS,0,,False
431,"is work has considered the selective rewriting of web search queries, with the aim of a aining e cient retrieval without hindering the system's e ectiveness. In particular, we showed that it is possible to accurately measure the response time of a search engine in answering a query with complex operators such as #syn and #1, even when using the WAND dynamic pruning strategy. Our detailed experimental setup involved experiments upon the open TREC ClueWeb09 dataset, using TREC Web track queries for measuring e ectiveness in terms of NDCG@20, and real search engine user queries for measuring e ciency. Moreover, we deployed three strategies to rewrite each query. Our experiments showed not only the accuracy of the newly proposed query e ciency predictions for queries involving complex operators, but also the ability of our proposed selective mechanism to enhance e ciency bene ts (a 49% decrease in mean response time, and 62% decrease in tail, i.e., 95th-percentile, response time) without any signi cant degradation in mean NDCG@20. Overall, our results demonstrate that by selectively rewriting the query (when there is the time to execute the rewri en query), e ectiveness can be at least maintained while markedly bene ting response times. Our proposed selective rewriting mechanism can be further extended, for instance to more query rewriting techniques, such as those based on query reformulation and query-click pa erns [12, 20], and modelling the e ectiveness of rewriting plans through risk rather than mean e ectiveness alone.",1,TREC,True
432,REFERENCES,0,,False
433,"[1] Gianni Amati. 2006. Frequentist and Bayesian Approach to IR. In ECIR. 13­24. [2] Vo Ngoc Anh, Owen de Kretser, and Alistair Mo at. 2001. Vector-space ranking",0,,False
434,"with e ective early termination. In SIGIR. 35­42. [3] Michael Bendersky, W. Bruce Cro , and Yanlei Diao. 2011. ality-biased ranking",0,,False
435,"of web documents. In WSDM. 95­104. [4] Michael Bendersky, Donald Metzler, and W. Bruce Cro . 2010. Learning Concept",0,,False
436,"Importance Using a Weighted Dependence Model. In WSDM. 31­40. [5] Daniele Broccolo, Craig Macdonald, Salvatore Orlando, Iadh Ounis, Ra aele",1,ad,True
437,"Perego, Fabrizio Silvestri, and Nicola Tonello o. 2013. Load-sensitive Selective Pruning for Distributed Search. In CIKM. 379­388. [6] Andrei Z. Broder, David Carmel, Michael Herscovici, Aya So er, and Jason Y. Zien. 2003. E cient query evaluation using a two-level retrieval process. In CIKM. 426­434. [7] Chris Buckley, Gerard Salton, James Allan, and Amit Singhal. 1995. Automatic query expansion using SMART: TREC 3. In TREC 3. 69­80. [8] Christopher J.C. Burges. 2010. From RankNet to LambdaRank to LambdaMART: An Overview. Technical Report MSR-TR-2010-82.",1,ad,True
438,"[9] Charles L. A. Clarke, Nick Craswell, and Ellen M. Voorhees. 2012. Overview of the TREC 2012 Web Track. In TREC.",1,TREC,True
439,"[10] Nick Craswell, Dennis Fe erly, Marc Najork, Stephen Robertson, and Emine Yilmaz. 2010. Microso Research at TREC 2009. In TREC.",1,TREC,True
440,"[11] Nick Craswell, Rosie Jones, Georges Dupret, and Evelyne Viegas (Eds.). 2009. Proceedings of the Web Search Click Data Workshop at WSDM 2009.",0,,False
441,[12] Nick Craswell and Martin Szummer. 2007. Random walks on the click graph. In SIGIR. 239­246.,0,,False
442,"[13] W. Bruce Cro , Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice. Addison-Wesley Publishing Company.",0,,False
443,"[14] J. Shane Culpepper, Charles L. A. Clarke, and Jimmy Lin. 2016. Dynamic Cuto Prediction in Multi-Stage Retrieval Systems. In ADCS. 17­24.",0,,False
444,[15] Je rey Dean. 2009. Challenges in building large-scale information retrieval systems: invited talk. In WSDM.,0,,False
445,"[16] Je rey Dean and Luiz Andr Barroso. 2013. e Tail at Scale. Commun. ACM 56 (2013), 74­80.",0,,False
446,[17] Shuai Ding and Torsten Suel. 2011. Faster top-k document retrieval using blockmax indexes. In SIGIR. 993­1002.,0,,False
447,"[18] Marcus Fontoura, Vanja Josifovski, Jinhui Liu, Srihari Venkatesan, Xiangfei Zhu, and Jason Y. Zien. 2011. Evaluation Strategies for Top-k eries over Memory-Resident Inverted Indexes. PVLDB 4, 12 (2011), 1213­1224.",0,,False
448,"[19] Myeongjae Jeon, Saehoon Kim, Seung-won Hwang, Yuxiong He, Sameh Elnikety, Alan L. Cox, and Sco Rixner. 2014. Predictive Parallelization: Taming Tail Latencies in Web Search. In SIGIR. 253­262.",0,,False
449,"[20] Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating ery Substitutions. In WWW. 387­396.",1,ad,True
450,"[21] Saehoon Kim, Yuxiong He, Seung-won Hwang, Sameh Elnikety, and Seungjin Choi. 2015. Delayed-Dynamic-Selective (DDS) Prediction for Reducing Extreme Tail Latency in Web Search. In WSDM. 7­16.",0,,False
451,"[22] Nicholas Lester, Alistair Mo at, William Webber, and Justin Zobel. 2005. SpaceLimited Ranked ery Evaluation Using Adaptive Pruning. In WISE. 470­477.",0,,False
452,"[23] Jimmy Lin, Ma Crane, Andrew Trotman, Jamie Callan, Ishan Cha opadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, and Sebastiano Vigna. 2016. Toward Reproducible Baselines: e Open-Source IR Reproducibility Challenge. In ECIR. 408­420.",1,ad,True
453,"[24] Xiaolu Lu, Alistair Mo at, and J. Shane Culpepper. 2015. On the Cost of Extracting Proximity Features for Term-Dependency Models. In CIKM. 293­302.",0,,False
454,"[25] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Ra aele Perego, Nicola Tonello o, and Rossano Venturini. 2015. ickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees. In SIGIR. 73­82.",0,,False
455,"[26] Craig Macdonald, Iadh Ounis, and Nicola Tonello o. 2011. Upper-bound Approximations for Dynamic Pruning. ACM Trans. Inf. Syst. 29, 4 (2011), 17:1­17:28.",1,ad,True
456,"[27] Craig Macdonald, Rodrygo LT Santos, and Iadh Ounis. 2013. e whens and hows of learning to rank for web search. Information Retrieval 16, 5 (2013), 584­628.",1,ad,True
457,"[28] Craig Macdonald, Rodrygo L.T. Santos, Iadh Ounis, and Ben He. 2013. About Learning Models with Multiple ery-dependent Features . ACM Trans. Inf. Syst. 31, 3 (2013), 11:1­11:39.",1,ad,True
458,"[29] Craig Macdonald, Nicola Tonello o, and Iadh Ounis. 2012. Learning to predict response times for online query scheduling. In SIGIR. 621­630.",1,ad,True
459,[30] Donald Metzler and W. Bruce Cro . 2005. A Markov Random Field Model for Term Dependencies. In SIGIR. 472­479.,0,,False
460,"[31] Giuseppe O aviano, Nicola Tonello o, and Rossano Venturini. 2015. Optimal Space-time Tradeo s for Inverted Indexes. In WSDM. 47­56.",1,ad,True
461,"[32] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and Christina Lioma. 2006. Terrier: A High Performance & Scalable IR Platform. In OSIR.",1,ad,True
462,"[33] Jan Pederson. 2010. ery Understanding at Bing. In Invited Talk, SIGIR 2010 Industry Day.",0,,False
463,"[34] Fuchun Peng, Nawaaz Ahmed, Xin Li, and Yumao Lu. 2007. Context Sensitive Stemming for Web Search. In SIGIR. 639­646.",0,,False
464,"[35] Jie Peng, Craig Macdonald, Ben He, Vassilis Plachouras, and Iadh Ounis. 2007. Incorporating Term Dependency in the DFR Framework. In SIGIR. 843­844.",1,ad,True
465,[36] Eric Shurman and Jake Brutlag. 2009. Performance related changes and their user impacts. In Velocity: Web Performance and Operations Conference.,0,,False
466,"[37] Trevor Strohman, Howard Turtle, and W. Bruce Cro . 2005. Optimization Strategies for Complex eries. In SIGIR. 219­225.",0,,False
467,"[38] Nicola Tonello o, Craig Macdonald, and Iadh Ounis. 2013. E cient and E ective Retrieval Using Selective Pruning. In WSDM. 63­72.",1,ad,True
468,"[39] Howard Turtle and James Flood. 1995. ery evaluation: Strategies and optimizations. Information Processing & Management 31, 6 (1995), 831 ­ 850.",0,,False
469,"[40] Sebastiano Vigna. 2013. asi-succinct indices. In WSDM. 83­92. [41] Lidan Wang, Jimmy Lin, and Donald Metzler. 2010. Learning to E ciently Rank.",0,,False
470,In SIGIR. 138­145. [42] Jinxi Xu and W. Bruce Cro . 1996. ery Expansion Using Local and Global,0,,False
471,Document Analysis. In SIGIR. 4­11.,0,,False
472,504,0,,False
473,,0,,False

,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Word Embedding Causes Topic Shi ing; Exploit Global Context!,0,,False
3,"Navid Rekabsaz, Mihai Lupu, Allan Hanbury",0,,False
4,Information & So ware Engineering Group TU WIEN,0,,False
5,rekabsaz/lupu/hanbury@ifs.tuwien.ac.at,0,,False
6,ABSTRACT,0,,False
7,"Exploitation of term relatedness provided by word embedding has gained considerable a ention in recent IR literature. However, an emerging question is whether this sort of relatedness ts to the needs of IR with respect to retrieval e ectiveness. While we observe a high potential of word embedding as a resource for related terms, the incidence of several cases of topic shi ing deteriorates the nal performance of the applied retrieval models. To address this issue, we revisit the use of global context (i.e. the term co-occurrence in documents) to measure the term relatedness. We hypothesize that in order to avoid topic shi ing among the terms with high word embedding similarity, they should o en share similar global contexts as well. We therefore study the e ectiveness of post ltering of related terms by various global context relatedness measures. Experimental results show signi cant improvements in two out of three test collections, and support our initial hypothesis regarding the importance of considering global context in retrieval.",1,ad,True
8,KEYWORDS,0,,False
9,Word embedding; term relatedness; global context; word2vec; LSI,0,,False
10,"ACM Reference format: Navid Rekabsaz, Mihai Lupu, Allan Hanbury and Hamed Zamani. 2017. Word Embedding Causes Topic Shi ing; Exploit Global Context!. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080733",0,,False
11,1 INTRODUCTION,1,DUC,True
12,"e e ective choice of related terms to enrich queries has been explored for decades in information retrieval literature and approached using a variety of data resources. Early studies explore the use of collection statistics. ey identify the global context of two terms either by directly measuring term co-occurrence in a context (i.e. document) [9] or a er applying matrix factorization [3]. Later studies show the higher e ectiveness of local approaches (i.e. using pseudo-relevant documents) [15]. More recently, the approaches to exploit the advancement in word embedding for IR",1,ad,True
13,"Funded by: Self-Optimizer (FFG 852624) in the EUROSTARS programme, funded by EUREKA, BMWFW and European Union, and ADMIRE (P 25905-N23) by FWF. anks to Joni Sayeler and Linus Wretblad for their contributions in SelfOptimizer. was supported in part by the Center for Intelligent Information Retrieval.",1,ad,True
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080733",1,ad,True
15,Hamed Zamani,0,,False
16,Center for Intelligent Information Retrieval University of Massachuse s Amherst zamani@cs.umass.edu,0,,False
17,"has shown not only to be competitive to the local approaches but also that combining the approaches brings further improvements in comparison to each of them alone [12, 16, 17].",0,,False
18,"Word embedding methods provide vector representations of terms by capturing the co-occurrence relations between the terms, based on an approximation on the likelihood of their appearances in similar window-contexts. Word embedding is used in various IR tasks e.g. document retrieval [11, 12, 18], neural network-based retrieval models [4, 6, 8], and query expansion [16].",0,,False
19,"In all of these studies, the concept of ""term similarity"" is dened as the geometric proximity between their vector representations. However, since this closeness is still a mathematical approximation of meaning, some related terms might not t to the retrieval needs and eventually deteriorate the results. For instance, antonyms (cheap and expensive) or co-hyponyms (schizophrenia and alzheimer, mathematics and physics, countries, months) share common window-context and are therefore considered as related in the word embedding space, but can potentially bias the query to other topics.",0,,False
20,"Some recent studies aim to be er adapt word embedding methods to the needs of IR. Diaz et al. [5] suggest training separate word embedding models on the top retrieved documents per query, while Rekabsaz et al. [13] explore the similarity space and suggest a general threshold to lter the most e ective related terms. While the mentioned studies rely on the context around the terms, in this work we focus on the e ect of similarity achieved from global context as a complementary to the window-context based similarity.",1,ad,True
21,"In fact, similar to the earlier studies [9, 14], we assume each document to be a coherent information unit and consider the cooccurrence of terms in documents as a means of measuring their topical relatedness. Based on this assumption, we hypothesize that to mitigate the problem of topic shi ing, the terms with high word embedding similarities also need to share similar global contexts. In other words, if two terms appear in many similar window-contexts, but they share li le global contexts (documents), they probably re ect di erent topics and should be removed from the related terms.",0,,False
22,"To examine this hypothesis, we analyze the e ectiveness of each related term, when added to the query. Our approach is similar to that of Cao et al. [2] on pseudo-relevance feedback. Our analysis shows that the set of related terms from word embedding has a high potential to improve state-of-the-art retrieval models. Based on this motivating observation, we explore the e ectiveness of using word embedding's similar term when ltered by global context similarity on two state-of-the-art IR models. Our evaluation on three test collections shows the importance of using global context, as combining both the similarities signi cantly improves the results.",1,ad,True
23,1105,0,,False
24,Short Research Paper,0,,False
25,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
26,2 BACKGROUND,0,,False
27,"To use word embedding in document retrieval, recent studies extend the idea of translation models in IR [1] using word embedding similarities. Zuccon et al. [18] use the similarities in the language modeling framework [10] and Rekabsaz et al. [12] extend the concept of translation models to probabilistic relevance framework. In the following, we brie y explain the translation models when combined with word embedding similarity.",0,,False
28,"In principle, a translation model introduces in the estimation of the relevance of the query term t a translation probability PT , de ned on the set of (extended) terms R(t), always used in its conditional form PT (t |t ) and interpreted as the probability of observing term t, having observed term t . Zuccon et al. [18] integrate word embedding with the translation language modeling by using the set of extended terms from word embedding:",0,,False
29,"LM(q, d) , P(q|Md ) ,",1,LM,True
30,PT (t |t )P(t |Md ) (1),0,,False
31,t q t R(t ),0,,False
32,Rekabsaz et al. [12] extend the idea into four probabilistic relevance,0,,False
33,frameworks. eir approach revisits the idea of computing docu-,0,,False
34,"ment relevance based on the occurrence of concepts. Traditionally,",1,ad,True
35,"concepts are represented by the words appear in the text, quanti ed",0,,False
36,by term frequency (t f ). Rekabsaz et al. posit that we can have a,0,,False
37,"t f value lower than 1 when the term itself is not actually appear,",0,,False
38,"but another, conceptually similar term occurs in the text. Based on",0,,False
39,"it, they de ne the extended t f of a query word t in a document as",0,,False
40,follows:,0,,False
41,"t fd , t fd +",0,,False
42,PT (t |t )t fd (t ),0,,False
43,(2),0,,False
44,t R(t ),0,,False
45,"However, in the probabilistic models, a series of other factors are",0,,False
46,computed based on t f (e.g. document length). ey therefore,0,,False
47,propagate the above changes to all the other statistics and refer to,0,,False
48,the nal scoring formulas as Extended Translation model. Among,0,,False
49,"the extended models, as BM25 is a widely used and established",0,,False
50,"model in IR, we use the extended BM25 translation model (BM25)",0,,False
51,"in our experiments. Similar to the original papers in both models,",0,,False
52,the estimation of PT is based on the Cosine similarity between two embedding vectors.,0,,False
53,3 EXPERIMENT SETUP,0,,False
54,"We conduct our experiments on three test collections, shown in Table 1. For word embedding vectors, we train the word2vec skipgram model [7] with 300 dimensions and the tool's default parameters on the Wikipedia dump le for August 2015. We use the Porter stemmer for the Wikipedia corpus as well as retrieval. As suggested by Rekabsaz et al. [13], the extended terms set R(t) is selected from the terms with similarity values of greater than a speci c threshold. Previous studies suggest the threshold value of around 0.7 as an optimum for retrieval [12, 13]. To explore the e ectiveness of less similar terms, we try the threshold values of {0.60, 0.65..., 0.80}.",1,Wiki,True
55,"Since the parameter µ for Dirichlet prior of the translation language model and also b, k1, and k3 for BM25 are shared between the methods, the choice of these parameters is not explored as part of this study and we use the same set of values as in Rekabsaz et al. [12]. e statistical signi cance tests are done using the two sided paired t-test and signi cance is reported for p < 0.05. e evaluation of retrieval e ectiveness is done with respect to Mean Average Precision (MAP) as a standard measure in ad-hoc IR.",1,MAP,True
56,Table 1: Test collections used in this paper,0,,False
57,Name TREC Adhoc 1&2&3 TREC Adhoc 6&7&8 Robust 2005,1,TREC,True
58,Collection Disc1&2 Disc4&5 AQUAINT,1,AQUAINT,True
59,# eries 150 150 50,0,,False
60,# Documents 740449 556028 1033461,0,,False
61,"Table 2: e percentage of the good, bad and neutral terms.",1,ad,True
62,#Rel averages the number of related terms per query term.,0,,False
63,Collection,0,,False
64,reshold 0.60 #Rel Good Neutral Bad,1,ad,True
65,TREC 123,1,TREC,True
66,8.2 7%,0,,False
67,84%,0,,False
68,9%,0,,False
69,TREC 678,1,TREC,True
70,8.8 9%,0,,False
71,78% 14%,0,,False
72,Robust 2005 10.3 8%,1,Robust,True
73,77% 15%,0,,False
74,ALL,0,,False
75,8.1 8%,0,,False
76,81% 11%,0,,False
77,reshold 0.80,0,,False
78,#Rel Good Neutral Bad,1,ad,True
79,1.3 19%,0,,False
80,68% 13%,0,,False
81,1.2 34%,0,,False
82,48% 18%,0,,False
83,1.1 39%,0,,False
84,44% 17%,0,,False
85,1.2 27%,0,,False
86,58% 15%,0,,False
87,4 PRELIMINARY ANALYSIS,0,,False
88,"We start with an observation on the e ectiveness of each individual related term. To measure it, we use the LM model as it has shown slightly be er results than the BM25 model [12]. Similar to Cao et al. [2], given each query, for all its corresponding related terms, we repeat the evaluation of the IR models where each time R(t) consists of only one of the related terms. For each term, we calculate the di erences between its Average Precision (AP) evaluation result and the result of the original query and refer to this value as the retrieval gain or retrieval loss of the related term.",1,LM,True
89,"Similar to Cao et al. [2], we de ne good/bad groups as the terms with retrieval gain/loss of more than 0.005, and assume the rest with smaller gain or loss values than 0.005 as neutral terms. Table 2 summarizes the percentage of each group. Due to the lack of space, we only show the statistics for the lowest (0.6) and highest (0.8) threshold. e average number of related terms per query term is shown in the #Rel eld. As expected, the percentage of the good terms is higher for the larger threshold, however--similar to the observation on pseudo-relevance feedback [2]--most of the expanded terms (58% to 81%) have no signi cant e ect on performance.",1,ad,True
90,Let us imagine that we had a priori knowledge about the e ectiveness of each related term and were able to lter terms with negative e ect on retrieval. We call this approach Oracle post-,1,ad,True
91,"ltering as it shows us the maximum performance of each retrieval model. Based on the achieved results, we provide an approximation of this approach by ltering the terms with retrieval loss.",0,,False
92,Figures 1a and 1b show the percentage of relative MAP improve-,1,MAP,True
93,"ment of the LM and BM25 models with and without post- ltering with respect to the original LM and BM25 models. In the plot, ignore the Gen and Col results as we return to them in Section 6. e results are aggregated over the three collections. In each threshold the statistical signi cance of the improvement with respect to two baselines are computed: (1) against the basic models (BM25 and LM), shown with the b sign and (2) against the translation models without post ltering, shown with the  sign.",1,LM,True
94,"As reported by Rekabsaz et al. [13], for the thresholds less than 0.7 the retrieval performance of the translation models (without post ltering) decreases as the added terms introduce more noise. However, the models with the Oracle post ltering continue to improve the baselines further for the lower thresholds with high margin. ese demonstrate the high potential of using related terms from word embedding but also show the need to customize the set of terms for IR. We propose an approach to this customization using the global-context of the terms in the following.",1,ad,True
95,1106,0,,False
96,Short Research Paper,0,,False
97,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
98,% Improvement (MAP) % Improvement (MAP) Word2Vec Similarity,1,MAP,True
99,20 ½b,0,,False
100,15,0,,False
101,10,0,,False
102,5½ ½,0,,False
103,0,0,,False
104,-5,0,,False
105,-10 0.6,0,,False
106,½b,0,,False
107,½b,0,,False
108,½b,0,,False
109,b½b,0,,False
110,LM,1,LM,True
111,Ld M Gen,0,,False
112,Ld M,0,,False
113,Ld M Col,0,,False
114,0.65,0,,False
115,0.7,0,,False
116,Threshold,0,,False
117,Ld M Oracle 0.75,0,,False
118,½b,0,,False
119,20,0,,False
120,½b,0,,False
121,15,0,,False
122,½b,0,,False
123,10,0,,False
124,½ 5½,0,,False
125,½b,0,,False
126,½b,0,,False
127,0,0,,False
128,-5,0,,False
129,-10 0.6,0,,False
130,BM25,0,,False
131,d BM25,0,,False
132,d BM25 Gen d BM25 Col,0,,False
133,0.65,0,,False
134,0.7,0,,False
135,Threshold,0,,False
136,d BM25 Oracle,0,,False
137,0.75,0,,False
138,0.90,0,,False
139,0.85,0,,False
140,0.80,0,,False
141,0.75,0,,False
142,0.70,0,,False
143,0.65,0,,False
144,0.60 0.5 0.6 0.7 0.8 0.9 1.0 LSI Similarity,0,,False
145,(a) LM,1,LM,True
146,(b) BM25,0,,False
147,(c) Retrieval Gain/Loss,0,,False
148,"Figure 1: (a,b) e percentage of relative MAP improvement to the basic models, aggregated on all the collections. e b and  signs show the signi cance of the improvement to the basic models and the extended models without post ltering respectively (c) Retrieval gain or loss of the related terms for all the collection. e red (light) color indicate retrieval loss and the green (dark) retrieval gain.",1,MAP,True
149,5 GLOBAL-CONTEXT POST FILTERING,0,,False
150,"Looking at some samples of retrieval loss, we can observe many cases of topic shi ing: e.g. Latvia as query term is expanded with Estonia, Ammoniac with Hydrogen, Boeing with Airbus, and Alzheimer with Parkinson. As mentioned before, our hypothesis is that for the terms with high window-context similarity (i.e. word2vec similarity) when they have high global context similarity (i.e. co-occurrence in common documents), they more probably refer to a similar topic (e.g. USSR and Soviet) and with low global context similarity to di erent topics (e.g. Argentina and Nicaragua).",0,,False
151,"To capture the global context similarities, some older studies use measures like Dice, Tanimoto, and PMI [9]. Cosine similarity has been used as well, considering each term a vector with dimensionality of the number of documents in the collection, with weights given either as simple incidence (i.e. 0/1), or by some variant of TFIDF. Cosine can also be used a er rst applying Singular Value Decomposition on the TFIDF weighted term-document matrix, resulting in the well known Latent Semantic Indexing (LSI) method [3] (300 dimensions in our experiments). To compute these measures, we consider both the collection statistics and Wikipedia statistics, resulting in 12 sets of similarities (Dice, Tanimoto, PMI, Incidence Vectors, TFIDF Vectors, LSI Vectors)×(collection, Wikipedia). We refer to these similarity value lists as global context features.",1,Wiki,True
152,"Let rst observe the relationship between LSI and word2vec similarities of the terms. Figure 1c plots the retrieval gain/loss of the terms of all the collections based on their word2vec similarities as well as LSI (when using test collection statistics). e size of the circles shows their gain/loss as the red color (the lighter one) show retrieval loss and green (the darker one) retrieval gain. For clarity, we only show the terms with the retrieval gain/loss of more than 0.01. e area with high word2vec and LSI similarity (top-right) contains most of the terms with retrieval gain. On the other hand, regardless of the word2vec similarity, the area with lower LSI tend to contain relatively more cases of retrieval loss. is observation encourages the exploration of a set of thresholds for global context features to post lter the terms retrieved by embedding.",0,,False
153,"To nd the thresholds for global context features, we explore the highest amount of total retrieval gain a er ltering the related terms with similarities higher than the thresholds. We formulate it",0,,False
154,by the following optimization problem:,0,,False
155,N,0,,False
156,F,0,,False
157,argmin 1 xj > j i,0,,False
158,(3),0,,False
159," i,1 j,1",0,,False
160,"where 1 is the indicator function, N and F are the number of terms",0,,False
161,"and features respectively,  indicates the set of thresholds j , xj the value of the features, and nally refers to the retrieval gain/loss.",0,,False
162,We consider two approaches to selecting the datasets used to,0,,False
163,"nd the optimum thresholds: per collection, and general. In the",0,,False
164,"per collection scenario (Col), for each collection we nd di erent",0,,False
165,thresholds for the features. We apply 5-fold cross validation by rst,0,,False
166,using the terms of the training topics to nd the thresholds (solving,0,,False
167,Eq. 3) and then applying the thresholds to post lter the terms of,0,,False
168,"the test topics. To avoid over ing, we use the bagging method by",0,,False
169,40 times bootstrap sampling (random sampling with replacement),0,,False
170,and aggregate the achieved thresholds.,0,,False
171,"In the general approach (Gen), we are interested in nding a",0,,False
172,"`global' threshold for each feature, which is fairly independent of",0,,False
173,the collections. As in this approach the thresholds are not speci c,0,,False
174,"for each individual collection, we use all the topics of all the test",0,,False
175,collections to solve the optimization problem.,0,,False
176,6 RESULTS AND DISCUSSION,0,,False
177,"To nd the most e ective set of features, we test all combinations of features using the per collection (Col) post- ltering approach. Given the post- ltered terms with each feature set, we evaluate the",0,,False
178,"LM and BM25 models. Our results show the superior e ectiveness of the LSI feature when using the test collections as resource in comparison with the other features as well as the ones based on Wikipedia. e results with the LSI feature can be further improved by combining it with the TFIDF feature. However, adding any of the other features does not bring any improvement and therefore, in the following, we only use the combination of LSI and TFIDF features with both using the test collections statistics.",1,LM,True
179,e evaluation results of the original LM and LM with post ltering with the general (Gen) and per collection (Col) approaches are,1,LM,True
180,"shown in Figure 2. e general behavior of BM25 is very similar and therefore no longer shown here. As before, statistical significance against the basic models is indicated by b and against the translation models without post ltering, by .",0,,False
181,1107,0,,False
182,Short Research Paper,0,,False
183,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
184,0.33 b½,0,,False
185,0.32,0,,False
186,b½,0,,False
187,0.31,0,,False
188,0.30,0,,False
189,b½,0,,False
190,b,0,,False
191,b½,0,,False
192,0.29 ½½,0,,False
193,0.29 b½,0,,False
194,0.28 0.27,0,,False
195,b½,0,,False
196,b½,0,,False
197,0.26,0,,False
198,½ 0.25 ½,0,,False
199,0.24,0,,False
200,0.23 b½,0,,False
201,0.22,0,,False
202,b½,0,,False
203,b½,0,,False
204,0.21,0,,False
205,½,0,,False
206,½,0,,False
207,½,0,,False
208,0.20,0,,False
209,MAP MAP MAP,1,MAP,True
210,0.28,0,,False
211,0.24,0,,False
212,0.19,0,,False
213,0.27,0,,False
214,0.23,0,,False
215,0.18,0,,False
216,0.26,0,,False
217,0.22,0,,False
218,0.17,0,,False
219,0.6,0,,False
220,0.65,0,,False
221,0.7,0,,False
222,0.75,0,,False
223,0.6,0,,False
224,0.65,0,,False
225,0.7,0,,False
226,0.75,0,,False
227,0.6,0,,False
228,0.65,0,,False
229,0.7,0,,False
230,0.75,0,,False
231,TREC 123,1,TREC,True
232,TREC 678,1,TREC,True
233,Robust 2005,1,Robust,True
234,LM,1,LM,True
235,Ld M,0,,False
236,Ld M Gen,0,,False
237,Ld M Col,0,,False
238,Ld M Oracle,0,,False
239,Figure 2: Evaluation results of LM with/without post ltering. LM and LM without post ltering respectively.,1,LM,True
240,"e results show the improvement of the LM models with postltering in comparison with the original LM. e models with postltering approaches speci cally improve in lower word embedding thresholds, however similar to the original translation models, the best performance is achieved on word embedding threshold of 0.7.",1,LM,True
241,e results for both LM and BM25 models with word embedding threshold of 0.7 are summarized in Table 3. Comparing the post-,1,LM,True
242,"ltering approaches, Col shows be er performance than Gen as with the optimum word embedding threshold, it achieves signi cant improvements over both the baselines in two of the collections.",0,,False
243,"Let us look back to the percentage of relative improvements, aggregated over the collections in Figures 1a and 1b. In both IR models, while the Col approach has be er results than Gen, their results are very similar to the optimum word embedding threshold (0.7). is result suggests to use the Gen approach as a more straightforward and general approach for post ltering. In our experiments, the optimum threshold value for the LSI similarities (as the main feature) is around 0.62 (vertical line in Figure 1c).",0,,False
244,"As a nal point, comparing the two IR models shows that despite the generally be er performance of the LM models, the BM25 models gain more. We speculate that it is due to the additional modi cation of other statistics (i.e. document length and IDF) in the BM25 model and therefore it is more sensitive to the quality of the related terms. However an in-depth comparison between the models is le for future work.",1,LM,True
245,7 CONCLUSION,0,,False
246,"Word embedding methods use (small) window-context of the terms to provide dense vector representations, used to approximate term relatedness. In this paper, we study the e ectiveness of related terms, identi ed by both window-based and global contexts, in document retrieval. We use two state-of-the-art translation models to integrate word embedding information for retrieval. Our analysis shows a great potential to improve retrieval performance, damaged however by topic shi ing. To address it, we propose the use of global context similarity, i.e. the co-occurrence of terms in larger contexts such as entire documents. Among various methods to measure global context, we identify LSI and TFIDF as the most e ective in eliminating related terms that lead to topic shi ing. Evaluating the IR models using the post- ltered set shows a signi cant improvement in comparison with the basic models as well as the translation models with no post- ltering. e results",1,ad,True
247,e b and  signs show the signi cance of the improvement to,0,,False
248,Table 3: MAP of the translation models when terms ltered,1,MAP,True
249,with word embedding threshold of 0.7 and post ltered with,0,,False
250,the Gen and Col approach.,0,,False
251,Collection Model Basic Tran. Tran.+Gen Tran.+Col,0,,False
252,TREC 123,1,TREC,True
253,LM 0.275 0.283 BM25 0.273 0.285,1,LM,True
254,0.290 0.288,0,,False
255,0.295 b 0.290 b,0,,False
256,TREC 678,1,TREC,True
257,LM 0.252 0.259 BM25 0.243 0.255,1,LM,True
258,0.262 0.257,0,,False
259,0.261 0.256 b,0,,False
260,Robust 2005,1,Robust,True
261,LM BM25,1,LM,True
262,0.183 0.181,0,,False
263,0.204 0.203,0,,False
264,0.208 0.207,0,,False
265,0.209 b 0.209 b,0,,False
266,demonstrate the importance of global context as a complementary to the window-context similarities.,0,,False
267,REFERENCES,0,,False
268,[1] Adam Berger and John La erty. 1999. Information Retrieval As Statistical Translation. In Proc. of SIGIR.,0,,False
269,"[2] Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. 2008. Selecting good expansion terms for pseudo-relevance feedback. In Proc. of SIGIR.",0,,False
270,"[3] Sco Deerwester, Susan T Dumais, George W Furnas, omas K Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American society for information science (1990).",0,,False
271,"[4] Mostafa Dehghani, Hamed Zamani, A. Severyn, J. Kamps, and W Bruce Cro . 2017. Neural Ranking Models with Weak Supervision. In Proc. of SIGIR.",0,,False
272,"[5] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. ery expansion with locally-trained word embeddings. Proc. of ACL (2016).",0,,False
273,"[6] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro . 2016. A deep relevance matching model for ad-hoc retrieval. In Proc. of CIKM.",1,ad-hoc,True
274,"[7] Tomas Mikolov, Kai Chen, G. Corrado, and J. Dean. 2013. E cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).",1,ad,True
275,"[8] Bhaskar Mitra, F. Diaz, and N. Craswell. 2017. Learning to Match Using Local and Distributed Representations of Text for Web Search. In Proc. of WWW.",0,,False
276,[9] Helen J Peat and Peter Wille . 1991. e limitations of term co-occurrence data for query expansion in document retrieval systems. Journal of the American society for information science (1991).,0,,False
277,[10] Jay M. Ponte and W. Bruce Cro . 1998. A Language Modeling Approach to Information Retrieval. In Proc. of SIGIR.,0,,False
278,"[11] Navid Rekabsaz, Ralf Bierig, Bogdan Ionescu, Allan Hanbury, and Mihai Lupu. 2015. On the use of statistical semantics for metadata-based social image retrieval. In Proc. of CBMI Conference.",1,ad,True
279,"[12] Navid Rekabsaz, Mihai Lupu, and Allan Hanbury. 2016. Generalizing Translation Models in the Probabilistic Relevance Framework. In Proc. of CIKM.",0,,False
280,"[13] Navid Rekabsaz, Mihai Lupu, and Allan Hanbury. 2017. Exploration of a threshold for similarity based on uncertainty in word embedding. In Proc. of ECIR.",0,,False
281,[14] G Salton and MJ MacGill. 1983. Introduction to modern information retrieval. McGraw-Hill (1983).,0,,False
282,[15] Jinxi Xu and W Bruce Cro . 1996. ery expansion using local and global document analysis. In Proc. of SIGIR.,0,,False
283,[16] Hamed Zamani and W Bruce Cro . 2016. Embedding-based query language models. In Proc. of ICTIR.,0,,False
284,[17] Hamed Zamani and W Bruce Cro . 2017. Relevance-based Word Embedding. In Proc. of SIGIR.,0,,False
285,"[18] Guido Zuccon, Bevan Koopman, Peter Bruza, and Leif Azzopardi. 2015. Integrating and evaluating neural word embeddings in information retrieval. In Proc. of Australasian Document Computing Symposium.",0,,False
286,1108,0,,False
287,,0,,False

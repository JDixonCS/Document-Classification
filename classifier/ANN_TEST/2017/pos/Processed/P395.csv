,sentence,label,data,regex
0,Session 4A: Evaluation 2,1,Session,True
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Retrieval Consistency in the Presence of ery Variations,0,,False
3,Peter Bailey,0,,False
4,"Microso Canberra, Australia",0,,False
5,Falk Scholer,0,,False
6,"RMIT University Melbourne, Australia",0,,False
7,ABSTRACT,0,,False
8,"A search engine that can return the ideal results for a person's information need, independent of the speci c query that is used to express that need, would be preferable to one that is overly swayed by the individual terms used; search engines should be consistent in the presence of syntactic query variations responding to the same information need. In this paper we examine the retrieval consistency of a set of ve systems responding to syntactic query variations over one hundred topics, working with the UQV100 test collection, and using Rank-Biased Overlap (RBO) relative to a centroid ranking over the query variations per topic as a measure of consistency. We also introduce a new data fusion algorithm, Rank-Biased Centroid (RBC), for constructing a centroid ranking over a set of rankings from query variations for a topic. RBC is compared with alternative data fusion algorithms.",1,UQV,True
9,"Our results indicate that consistency is positively correlated to a moderate degree with ""deep"" relevance measures. However, it is only weakly correlated with ""shallow"" relevance measures, as well as measures of topic complexity and variety in query expression.",0,,False
10,ese ndings support the notion that consistency is an independent property of a search engine's retrieval e ectiveness.,0,,False
11,CCS CONCEPTS,0,,False
12,·Information systems Retrieval e ectiveness; Test collections;,0,,False
13,KEYWORDS,0,,False
14,"Test collections, retrieval consistency, semantic e ectiveness",0,,False
15,1 INTRODUCTION,1,DUC,True
16,"Evaluating search e ectiveness has several aspects. One aspect that has been especially popular is calculating average scores according to some relevance measure (such as NDCG or AP) over a set of topics and associated queries for some common corpus of information. In the batch evaluation methodology, di erent systems are compared using the same measure, and statistical tests are applied",1,AP,True
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080839",1,ad,True
18,Alistair Mo at,0,,False
19,"e University of Melbourne Melbourne, Australia",0,,False
20,Paul omas,0,,False
21,"Microso Canberra, Australia",0,,False
22,"to determine whether the di erence in performance is likely due to factors other than chance. Alternative methods for determining relevance include user studies, online interleaving, and A/B testing. What is important to note is that retrieval e ectiveness encompasses more than just relevance.",0,,False
23,"Batch evaluation has typically used only a single query per topic, although a number of researchers working on early test collections advocated and explored the e ect of using multiple queries per topic (see work by Spa¨rck Jones and van Rijsbergen [35], Belkin et al. [5] and Buckley and Walz [7] among others). Recent work by Bailey et al. [3] and Koopman and Zuccon [22] has returned to this theme, resulting in new test collections with large numbers of query variations responding to each topic's information need.",1,ad,True
24,"e availability of such test collections allows us to consider a new dimension in assessing the retrieval e ectiveness of search engines ­ namely, how consistent they are when returning results in response to query variations that address the same information need. e importance of consistency can be understood when we consider simple examples like mis-spellings. For example ""facebook"", ""facebok"", and ""faecbook"" pre y clearly all want to nd the Facebook home page. Consistency also applies to more complex examples involving synonyms (for example, ""health bene ts of vitamin c"" and ""health bene ts of ascorbic acid"") or entirely rephrased needs (for example, ""how much does a raspberry pi cost"" and ""price of raspberry pi computer""). In each of these cases, we can contemplate that there exists an ideal ranked set of relevant results drawn from the corpus. Test collections are premised on this principle, where the ideal set for a topic is discovered through judging a document pool formed from di erent rankings. An ideal search engine would return (only) this set of results given a query for a topic, and the di erence in relevance from what is actually returned and this ideal ranking is captured by a relevance measure. Equally, given a set of syntactic query variations, an ideal search engine would return this ideal ranking of results, independent of the query variation.",1,ad,True
25,"Indeed, much research in information retrieval seeks to tackle exactly this problem of nding an ideal set of results without relying solely on the original query's syntactic expression. For instance, query re-writing techniques such as spelling correction [11], term stemming [24], query expansion [33], and query substitutions [19] are used to manipulate the user-entered query and thereby extract a be er set of documents from the index. Stemming and stopword removal [23, 25] may also be used in indexing processes or within the matching algorithms at query execution time.",0,,False
26,"Two strands of research have investigated how to combine rankings to improve relevance e ectiveness: data fusion (for example,",0,,False
27,395,0,,False
28,Session 4A: Evaluation 2,1,Session,True
29,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
30,"Belkin et al. [6]), which merges rankings from di erent query representations; and distributed information retrieval or meta-search (for example, Callan [8]), which merges rankings from di erent underlying search engines or indexes. ese techniques have been assessed principally from the standpoint of improving relevance overall, as measured by the relevance score of the resulting ranking.",0,,False
31,"People use many di erent expressions to describe the same information need (see, for example, Furnas et al. [15]). Even when re nding a single information resource, the same person may use di erent queries [37]. Bailey et al. [2] give evidence that the e ect of query variation on relevance scores dwarfs that of system and topic e ects. We believe that these ndings make it important to consider new approaches to characterizing e ectiveness, including ones that address query variation for a single information need.",1,ad,True
32,"We propose that the consistency in rankings of a system when faced with many di erent query variations for a single topic can be one such measure. e Rank-Biased Overlap measure developed by Webber et al. [38] is adopted to characterize consistency, and used with the UQV100 test collection [3] to investigate consistency across a set of ve systems. Due to the scale of variations within UQV100 (19­101 unique query variations per topic, for 100 topics), each individual topic has a similar number of queries as might be found in an entire query-only processed test collection like the TREC 2014 Web track [10].",1,ad,True
33,"To determine relative consistency for a single system and topic combination, RBO requires us to declare some reference ranking against which the individual rankings for each query variation can be compared. We develop a new fusion algorithm, the RankBiased Centroid (RBC), drawing inspiration from both RBO and Rank-Biased Precision [27], to determine this reference ranking.",0,,False
34,We consider these research questions in regard to RBC fusion:,0,,False
35,RQ-F1 Do RBC rankings outperform the initial query rankings for a system?,0,,False
36,RQ-F2 How does RBC compare to other data fusion algorithms in relevance e ectiveness?,0,,False
37,RQ-F3 Does combining both query variations and systems for RBC outperform query variations-only RBC?,0,,False
38,"en, in connection with consistency, we ask:",0,,False
39,RQ-C1 Do topics vary in consistency? RQ-C2 Does consistency vary with the number of query variations,0,,False
40,or with changes in topic complexity? RQ-C3 Do systems vary in consistency? RQ-C4 Are increases in per-topic consistency for a system inde-,0,,False
41,pendent of increases in relevance for a system?,0,,False
42,2 RELATED WORK,0,,False
43,2.1 Data fusion,0,,False
44,"Data fusion ­ combining evidence from di erent sources ­ is a widely-studied problem. In IR fusion is typically applied when evidence from multiple ranked answer lists needs to be combined into a single ranked list, for example in meta-search, where results from multiple independent search systems are combined into a single ranking [1], and in multi-lingual retrieval, where results from searches across collections in di erent languages are combined into a single answer list [16].",0,,False
45,"Data fusion approaches can be broadly grouped into those that use the ranker's score (that is, the value assigned by a ranking function) of each document in a results list, and those that make use only of the rank position of each document in the answer list. Perhaps the most well-known approaches in the former category are by Fox and Shaw [13], including CombMAX, where the nal score of a document is the maximum of the ranker scores that it received in any input ranked list; CombSUM, where the nal document score is the total of the ranker's scores that it received in the input lists; and CombMNZ, where the nal document score is calculated as in CombSUM but further multiplied by the number of input lists in which the document appears, thereby promoting those documents that were retrieved in multiple lists. e document ranker scores in the input ranked lists may also be normalized in di erent ways, including linear re-scaling into a chosen range [39], or by controlling an upper bound based on the sum or variance of the input scores [28].",1,ad,True
46,"For fusion based only on rank information, techniques from social choice theory such as the Borda count and Condorcet criterion have been applied. In the Borda count [1], each candidate (document) receives a score determined by how many other candidates were ranked lower, with these scores summed across all ballots (input lists). e Condorcet criterion instead determines an outcome based on which candidate achieves the highest number of wins based on pairwise comparisons with all other candidates [12, 29]",1,ad,True
47,"e impact of data fusion techniques on e ectiveness can vary from case to case, leading Wu and McClean [40] to investigate approaches for predicting the performance impact of applying fusion.",1,ad,True
48,"eir results showed that the selective application of fusion, based on features such as the number of component result lists and the overlap of items in these lists, can further enhance the positive impact on nal retrieval e ectiveness.",0,,False
49,"Prior work that has speci cally considered data fusion in the context of multiple queries for the same underlying information need was carried out by Belkin et al. [5], who investigated the e ects of combining ve independent Boolean query formulations for ten TREC topics, and demonstrated that fusing results can substantially boost performance. Subsequent work using the TREC-2 collection further demonstrated that good methods for fusing the results of multiple queries can lead to results that are be er than those of the best single query [6]. Pickens et al. [30] also con rmed that combining multiple queries for the same intent boosts e ectiveness.",1,TREC,True
50,2.2 Measuring consistency,0,,False
51,"In IR, as in many other domains, it can be important to compare the similarity, or consistency, of groups of things. ese groups may be conjoint (consisting of the same items) or disjoint (one group may include items that do not occur in the other group), and may be setbased (where there is no known or inferred ordering of the items) or ordered (where the sequence in which the items occur ma ers). Typical examples where one might wish to compare groups include measuring the similarity of the answer lists returned by two search engines in response to the same query; or the similarity of the e ectiveness ranking of a set of several di erent retrieval systems, when evaluated over two di erent test collections. A wide range of list similarity measures have been proposed and applied.",0,,False
52,396,0,,False
53,Session 4A: Evaluation 2,1,Session,True
54,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
55,"One of the similarity measures most commonly used in IR is Kendall's  , a rank correlation coe cient that calculates the normalized number of concordant pairs (items that are ordered the same in two rankings) minus discordant pairs (items that are ordered di erently in the two rankings), resulting in a score between 1 (perfect agreement between the two rankings) and -1 (perfect disagreement) [9]. Kendall's  is a measure that assumes conjoint ranked lists, that is, the lists are permutations of the same set of items; it is also an unweighted measure, where each pair contributes equally to the outcome, wherever it occurs in the ranking. However, when comparing ranked answer lists, items that are higher in the ranking are more important (users pay more a ention to top-ranked results); similarly, when comparing rankings of system e ectiveness scores, di erences between the top systems are generally of greater interest than di erences between systems that perform less well. TauAP [41] addresses a number of these weaknesses, while Rank-Biased Overlap (RBO) [38] addresses even more. RBO is a generalized measure of similarity between rankings based on a probabilistic user model, and readily handles non-conjoint lists. It applies a geometric sequence of weights to items in the lists; with the emphasis of the weighting adjustable via a user persistence parameter , which also determines the probability that the user will reach a certain rank. Inspired by RBO, Tan and Clarke [36] de ne a family of Maximized E ectiveness Di erence (MED) measures, each based on an IR e ectiveness metric (and hence a di erent underlying user model).",1,AP,True
56,"Jiang et al. [18] examine ranking consistency in web search from the basis of nding similar classes of queries (based on sharing common entity types in a knowledge base), and preserving the relative ordering of URL domains in the rankings for queries belonging to the same class, for example, people who are professional basketballers but also appear in movies. Large scale web log click data is used to derive their models of class similarity, based on URL pa erns. Jiang et al. develop a consistency measure based on Kendall's  across all pairs of queries belonging to the same class.",0,,False
57,"Finally, Zuccon et al. [43] present an evaluation framework using mean variance analysis over retrieval e ectiveness, for both intertopic and intra-topic sources of variation. Systems are preferred, all other things being equal, when one system is more stable than another in the presence of such variation.",0,,False
58,3 RANK-BIASED CENTROIDS,0,,False
59,"As noted in the previous section, a range of methods have been proposed for constructing fused rankings, given an initial set of same-basis source rankings. In this section we introduce a further approach: the rank-biased centroid, or RBC.",0,,False
60,3.1 User model for Borda fusion,0,,False
61,"To motivate the discussion, consider the four alternative rankings R1, R2, R3, and R4 shown in the le side of Figure 1, with each of the elements denoted by a le er of the alphabet. One run has ordered all of the seven di erent elements, while the other three are truncated and omit one or more of the items ­ a typical situation. Moreover, note that even if they are all of the same length, the runs might contain di erent subsets of a larger group of elements ­ they need not be permutations of each other. Finally, note that in the",0,,False
62,"Rank R1 R2 R3 R4  , 0.6  , 0.8  , 0.9",0,,False
63,1 A B A G A (0.89) D (0.61) D (0.35) 2 D D B D D (0.86) A (0.50) C (0.28) 3 B E D E B (0.78) B (0.49) A (0.27) 4 C C C A G (0.50) C (0.37) B (0.27) 5 G ­ G F E (0.31) G (0.37) G (0.23) 6 F ­ F C C (0.29) E (0.31) E (0.22) 7 ­ ­ E ­ F (0.11) F (0.21) F (0.18),0,,False
64,"Figure 1: Example of RBC fusion: four example rankings (le ); and three di erent fused orderings (right). Note that the RBC weights are shown to two decimal places only, and there are no score ties.",0,,False
65,"most general scenario, there are situations in which the provided rankings are pre xes of longer lists, themselves of unknown (and perhaps even in nite) length.",0,,False
66,"e Borda scoring process assigns a weight to item A of 7+7+4 ,"" 18 (note that A does not appear in ranking R2), tying it with B, and placing it behind item D, which gets 6 + 6 + 6 + 5 "","" 23 points. e overall Borda ordering is D, A"",""B, C, G, E, F. In the Borda regime, swapping the two adjacent items at any pair of consecutive ranks gives one of the items a +1 score change, and the other item a -1 change. is occurs regardless of whether the swap takes place at rank 1, at rank 10, or at rank 100. at is, all binary item preferences as expressed in the visible input rankings are regarded as being of equal merit; and any preferences that may not have been surfaced (in the case that the provided rankings are pre xes) are ignored.""",1,ad,True
67,"To create a user model that captures this behavior we can imagine a universe of agents, each of whom acts independently of the others, but follows the same simple rule: they randomly pick a depth d according to some probability distribution, they examine all of the input rankings to depth d (at most ­ but less if the rankings are shorter), and they sort the pool of items according to decreasing order of the number of times they saw each item in their set of length-d pre xes. e nal fused ranking is then a probabilistic expectation over all agents of the orderings that were constructed.",0,,False
68,"Given this overall probabilistic structure, the Borda ordering is derived when the probability distribution used by the agents is taken to be P(d , x) ,"" 1/n; that is, each agent is equally likely to select a pre x of any length between 1 and n, where n is the number of items. e Borda score for an item is then proportional to the expected value of the total number of times it was observed in the individual top-d sets of the probabilistic universe of agents.""",0,,False
69,3.2 An alternative weighting regime,0,,False
70,"Because there are many situations in which the supplied rankings are assumed to be pre xes of arbitrary-length ones, we contend that swaps near the heads of each of the rankings are somehow more indicative of preference than swaps deeper in the rankings. In the example shown in the le side of Figure 1, swapping A and D in ranking R1 has the same net e ect on A's Borda score as does swapping A and F in ranking R4, but the la er swap might seem to be somewhat less damaging to A, since in R4 it has already been deprecated by the person or system that generated that ordering.",1,ad,True
71,397,0,,False
72,Session 4A: Evaluation 2,1,Session,True
73,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
74,"Instead of assigning a Borda weight of (n - x + 1)/n (in a normalized sense) to each item at rank 1  x  n when the rankings are over n items, we suggest that a geometrically decaying weight function be employed, with the distribution of d over depths x given not by 1/n, but instead by (1 - )x-1 for some value 0    1 determined by considering the purpose for which the fused ranking is being constructed. e parameter  is the persistence, or patience of the imagined universe of probabilistic agents; and use of a geometric sequence models the same behavior as is embedded in the e ectiveness metric RBP [27] and in the rank correlation coe cient RBO [38] ­ namely, that the person examining the rankings always examines the rst item in each, and therea er proceeds from the i th to the i + 1 st with conditional probability , and ends their search at the i th with conditional probability (1 - ). In an implementation the fused ranking is determined by assigning a weight of (1 - )i-1 to each item at depth i in any of the rankings, and then summing over items and sorting by total weight.",1,ad,True
75,ere are a number of bene ts of this proposed approach:,0,,False
76,"· as already motivated, greater emphasis is placed on the earlier preferences than on deeper ones in each ranking;",1,ad,True
77,"· an upper bound on the lengths of the rankings is not required, nor are the rankings required to be the same length (the Borda method shares this exibility, albeit somewhat awkwardly);",0,,False
78,"· if further items are added at the tail of any of the rankings, the resultant item scores converge smoothly.",1,ad,True
79,"As extreme values, consider  , 0 and  , 1. When  ,"" 0, the agents only ever examine the rst item in each of the input rankings, and the fused output is by decreasing score of rst preference; this is somewhat akin to a rst-past-the-post election regime. When  "","" 1, each agent examines the whole of every list, and the fused ordering is determined by the number of lists that contain each item ­ a kind of """"popularity count"""" of each item across the input sets. In between these extremes, the expected depth reached by the agents viewing the rankings is given by 1/(1 - ). For example, when  "","" 0.9, on average the rst 10 items in each ranking are being used to contribute to the fused ordering; of course, in aggregate, across the whole universe of agents, all of the items in every ranking contribute to the overall outcome.""",0,,False
80,"In practice, what this means is that di erent values of  between 0 and 1 give rise to di erent fused orderings, balancing topweightedness and exhaustivity. e right side of Figure 1 shows the orderings generated for the rankings R1, R2, R3, and R4, discussed earlier, for three di erent values of . e total weight associated with each item (to two decimals) is also shown. Note how item A is top-ranked when the ranking agents are relatively impatient, and (on average) abandon the ranking early ( ,"" 0.6), but that if the fused ranking is assembled on a more patient basis ( "", 0.8 and  ,"" 0.9), items D and C become preferred, and A is demoted.""",0,,False
81,"As with Borda fusion, unanimous preferences are respected: in the example, because C is below D in all of the four input rankings, it must also fall below D in the fused ranking, regardless of the value of . e di erences that arise as  varies are limited only to the elements where there is disagreement in the input rankings as to their respective ordering. ese are, arguably, exactly the elements that we might be interested in focusing on.",0,,False
82,3.3 Discussion,0,,False
83,"We have de ned RBC in terms of a one-state user model [27]. Another way of looking at it is as an estimation of the normalized document scores used in CombMNZ and CombSUM. By assigning decreasingly small weights to documents further down the ranking, RBC can be viewed as seeking to approximate the long tail of document-query similarity scores generated by disjunctive ranked retrieval systems. Functions other than the geometric sequence might also be suitable for use, for example, Zip an weightings.",0,,False
84,4 FUSION OVER QUERY VARIATIONS,0,,False
85,"is section explores the practical bene t of fusing over query variations, and also shows that fusion over systems retains some of its power even a er query variations have been incorporated.",1,corpora,True
86,4.1 e UQV100 collection,1,UQV,True
87,"e UQV100 test collection is made up of 100 topics and associated information need statements, with approximately 100 individual query variations per topic; 10,835 in total [3]. When spelling correction and normalization are applied there are between 19 and 101 unique query variations per topic; 5,765 in total. ere are also 55,587 relevance judgments available in regard to those 100 topics, covering ClueWeb12-CatB documents pooled from ve systems, spanning three separate search engine code bases, and ve di erent ranking algorithms [26]. We again employ the runs for those ve contributing systems, anonymized here as Systems 1, 2, 3, 4, and 5. Due to some processing anomalies we observed in the run data, the overlapping set of unique query variations processed by all ve systems contains 5,736 queries. Each system run contains a ranking of length 200 for each of those distinct queries. For de niteness we ordered the set of queries for each topic by decreasing frequency according to crowd-based process used to originally collect them [2], with ties broken randomly.",1,UQV,True
88,4.2 Fusion over query variations,0,,False
89,"Table 1 provides a detailed evaluation of approaches for fusion as applied to query variations. Each pane of the table gives results for one e ectiveness metric, and within each pane the columns represent increasing numbers of query variations (note that for",0,,False
90," 20, the legend , x indicates that as many as x query variations were used ­ some topics had fewer than the listed number of variations). Note that , 1 makes use of the most frequentlysuggested query for each of the UQV100 topics; ,"" 2 adds the second most frequently suggested one; and so on. Stepping across each row thus involves more and more input runs being used to form each output run, and as can be seen, e ectiveness scores (with a few exceptions) increase. Many of the fusion methods give very similar e ectiveness. Even so, there are some notable pa erns:""",1,ad,True
91,"· using as few as , 2 query variations gives improved e ectiveness (relative to the , 1 baseline) for all metrics and all fusion methods;",0,,False
92,"· for all of the metrics, RBC with high values of p provides good fused outcomes, comparable with or be er than those achieved by CombMNZ and Borda;",0,,False
93,"· for the two recall-based metrics, RBC-based fusion provides markedly be er outcomes than Borda and CombMNZ;",0,,False
94,398,0,,False
95,Session 4A: Evaluation 2,1,Session,True
96,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
97,Fusion,0,,False
98,"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",0,,False
99,"RBC,  ,"" 0.9 0.491 0.490 RBC,  "","" 0.95 0.497 0.504 RBC,  "","" 0.98 0.505 0.511 RBC,  "", 0.99 0.511 0.520",0,,False
100,0.510 0.516 0.522 0.525,0,,False
101,0.516 0.526 0.535 0.534,0,,False
102,0.524 0.529 0.533 0.533,0,,False
103,Borda,0,,False
104,0.511 0.522 0.527 0.534 0.532,0,,False
105,CombMNZ 0.513 0.521 0.527 0.534 0.532,0,,False
106,"(a) RBP0.85, common baseline 0.474",0,,False
107,0.522 0.526 0.532 0.534,0,,False
108,0.535,0,,False
109,0.531,0,,False
110,Fusion,0,,False
111,"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",0,,False
112,"RBC,  ,"" 0.9 0.490 0.490 RBC,  "","" 0.95 0.496 0.504 RBC,  "","" 0.98 0.503 0.510 RBC,  "", 0.99 0.508 0.519",0,,False
113,0.506 0.514 0.519 0.523,0,,False
114,0.516 0.526 0.533 0.532,0,,False
115,0.523 0.528 0.533 0.531,0,,False
116,Borda,0,,False
117,0.507 0.520 0.525 0.530 0.528,0,,False
118,CombMNZ 0.509 0.517 0.524 0.531 0.528,0,,False
119,"(b) INST, common baseline 0.471",0,,False
120,0.522 0.527 0.532 0.532,0,,False
121,0.532,0,,False
122,0.528,0,,False
123,Fusion,0,,False
124,"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",0,,False
125,Fusion,0,,False
126,"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",0,,False
127,"RBC,  ,"" 0.9 0.222 0.230 0.248 0.265 0.288 0.299 RBC,  "","" 0.95 0.223 0.234 0.256 0.280 0.297 0.303 RBC,  "","" 0.98 0.225 0.239 0.260 0.275 0.281 0.284 RBC,  "", 0.99 0.226 0.241 0.254 0.264 0.266 0.270",0,,False
128,"RBC,  ,"" 0.9 0.437 0.454 RBC,  "","" 0.95 0.438 0.458 RBC,  "","" 0.98 0.440 0.462 RBC,  "", 0.99 0.442 0.464",0,,False
129,0.484 0.505 0.539 0.553 0.489 0.519 0.545 0.554 0.492 0.510 0.521 0.525 0.481 0.494 0.499 0.505,0,,False
130,Borda,0,,False
131,0.226 0.239 0.251 0.260 0.262 0.267 Borda,0,,False
132,0.442 0.464 0.478 0.489 0.493 0.502,0,,False
133,CombMNZ 0.227 0.240 0.252 0.260 0.273 0.266 CombMNZ 0.442 0.463 0.479 0.490 0.494 0.500,0,,False
134,"(c) AP, common baseline 0.204",1,AP,True
135,"(d) NDCG, common baseline 0.409",0,,False
136,"Table 1: Fusion over query variations, average e ectiveness across 100 UQV topics for runs generated from di erent numbers of query variations and according to di erent fusion approaches for System 1: (a) RBP0.85 scores; (b) INST scores; (c) AP scores; (d) NDCG scores.",1,UQV,True
137,"ery variations are sorted in decreasing order of occurrence frequency in the UQV100 collection. e baseline scores for ,"" 1 (that is, executing the single most popular query variation) are shown under each table. So that pa erns of behavior can be seen, the two largest""",1,UQV,True
138,"values in each column are highlighted in bold. Daggers indicate arrangements in which the RBC-based system was signi cantly be er than the corresponding Borda run (one-sided paired t-tests with p < 0.05). No signi cant di erences were detected for CombMNZ fusion, or using RBP0.85 or INST. Similar behavior was observed for other combinations of system and metric (not shown here).",0,,False
139,Fusion,0,,False
140,RBP0.85,0,,False
141,Metric,0,,False
142,INST,0,,False
143,AP,1,AP,True
144,NDCG,0,,False
145,"RBC,  ,"" 0.9 RBC,  "","" 0.95 RBC,  "","" 0.98 RBC,  "", 0.99",0,,False
146,0.503 0.508 0.506 0.505,0,,False
147,0.501 0.506 0.504 0.502,0,,False
148,0.217 0.219 0.220 0.217,0,,False
149,0.441 0.442 0.443 0.440,0,,False
150,Borda,0,,False
151,0.503 0.500 0.215 0.440,0,,False
152,CombMNZ,0,,False
153,0.506 0.505 0.219 0.442,0,,False
154,"Table 2: Fusion over ve di erent retrieval systems, based on one query variation ( , 1). All numbers are average e ectiveness scores over the 100 topics in the UQV100 collection. Single-system scores for the four metrics are shown in the rst two columns of Table 3. e largest two entries in each column are shown in bold. Daggers represent signi cance relative to Borda fusion (p < 0.05).",1,UQV,True
155,"· moreover, the greater the number of query variations being fused, the smaller the value of  needed to obtain those outcomes.",0,,False
156,We also explored round-robin fusion and CombSUM fusion; the former was never competitive (and e ectiveness decreased as query variations were added); and CombSUM typically gave performance slightly inferior to CombMNZ.,1,ad,True
157,"Metric Initial, , 1 mean max",0,,False
158,"Fused, , all mean max",0,,False
159,"Fused2, s , 5 mean gain",0,,False
160,RBP0.85 INST AP NDCG,1,AP,True
161,0.474 0.470 0.190 0.400,0,,False
162,0.487 0.481 0.204 0.411,0,,False
163,0.530 0.532 0.268 0.517,0,,False
164,0.557 0.558 0.303 0.554,0,,False
165,0.559 0.563 0.303 0.561,0,,False
166,+18% +20% +59% +41%,0,,False
167,"Table 3: Summary of e ectiveness gains achieved by fusing rst over query variations, and then second over systems. e rst four data columns are mean and maximum average scores over ve systems; the ""gain"" is relative to the initial system average in the",0,,False
168,rst column. All fusion is carried out using RBC0.95.,0,,False
169,4.3 Fusion over systems,0,,False
170,"Table 2 shows the outcome of applying fusion across the ve systems used in our experiments. A single query is used in each input run ( ,"" 1), and fusion applied to the ve rankings for each topic. In this se ing, all methods give comparable improvements in e ectiveness, with Borda fusion marginally the worst of them.""",0,,False
171,4.4 Double fusion,0,,False
172,"Table 3 provides an overall summary of the e ectiveness gains that can be achieved by fusing over query variations and then over systems, with RBC0.95 used at all fusing steps. Starting on the le ,",0,,False
173,"ve systems each execute one query variation ( , 1) for each of the",0,,False
174,399,0,,False
175,Session 4A: Evaluation 2,1,Session,True
176,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
177,Description,0,,False
178,RBP0.85 INST AP NDCG,1,AP,True
179,"All queries ( , all) First queries ( , 1) Best query per topic",0,,False
180,0.405 0.474 0.712,0,,False
181,0.394 0.151 0.336 0.471 0.204 0.409 0.718 0.271 0.503,0,,False
182,"Table 4: Average metric scores for all queries per topic; for the most popular query per topic; and for per-metric per-topic ""omniscient"" query selections. System 1 is used throughout. ese scores can be directly compared with those shown in the four panes of Table 1.",0,,False
183,UQV100 topics; this can be regarded as being the starting baseline condition (no fusion performed) for both this table and Table 2. e,1,UQV,True
184,"ve-way mean ""average over 100 topics"" and ve-way maximum ""average over 100 topics"" values show typical behavior for ve good retrieval systems when measured using a single query per topic. If each of those ve systems is given more query variations, and generates a single fused run for each of the 100 topics as its output, the values in the middle pair of columns arise. Substantial performance improvements can be observed, and the means of the",0,,False
185,"ve ""fusion over query variations"" systems handsomely exceeds the best average score of the ve original systems.",0,,False
186,"e third pair of columns in Table 3 then shows the outcome of fusing the ve system runs generated a er the query variations have been folded in. e mean scores shown (now with just a single ranking for each of the 100 topics) exceed the previous maximum scores in three of four cases, and exceed the middle-column mean scores in all four cases. e nal column shows the end-to-end gain in e ectiveness that has been achieved by the compound fusing (""initial mean"" to ""fused2 mean""). at is, fusing rst over query variations, and then over systems, gives rise to average e ectiveness gains of 18% and higher. In terms of statistical signi cance, and looking at the various relativities summarized in Table 3:",0,,False
187,"· across the ve systems and four metrics (twenty paired runs in total), the largest p-value computed by a two-tailed paired Student's t-test comparing the corresponding , 1 and , all conditions was less than 0.005;",0,,False
188,"· when the ve ,"" all fused runs are compared with the nal """"fused2"""" run, each metric yields one relatively large p-value, arising when the system that happens to be the """"max"""" is compared with the nal fused run (p  0.8, 0.7, 0.9, and 0.3 respectively across the four metrics, with two di erent systems represented twice each as the """"max"""" one), and a range of other smaller p-values, the largest of which was 0.059 (INST, comparing System 2 with "","" all against the nal fused run), and the remainder of which were 0.01 or smaller.""",0,,False
189,"at is, we are highly con dent that fusion over queries helps retrieval e ectiveness regardless of system and regardless of metric; and also con dent that additional fusion across systems is also bene cial, helping ensure that the outcomes are as good as, or be er than, what would have been a ained if by chance we were already working with the best system for that metric.",1,ad,True
190,4.5 An unrealistic target?,0,,False
191,"Hindsight is a wonderful guide, a fact that is also true in IR. Table 4 shows the result of a post-hoc evaluation of the runs generated",1,hoc,True
192,"for System 1. e rst row shows the (unweighted) average metric scores across all of the UQV100 topics, and for each topic across the (approximately, on average) 55 distinct query variations. e second row shows the baseline e ectiveness scores used in Table 1, arrived at by selecting the most popular of the query variations for each topic. e third row then shows results for four ""oracle"" query subsets, one for each metric, each incorporating (based post hoc on the relevance judgments and the computed metric scores) the ""best"" query variation for each of the UQV100 topics.",1,UQV,True
193,"Comparing the rst and second rows, the most frequently posed query generated by the crowd-workers for each topic obtains notably be er e ectiveness than the average of the variations. at di erence is why we ordered the query variations as we did (Section 4.1). Comparing the second and third rows reveals a substantial further gap ­ for each of the topics and each of the metrics there are highly e ective queries available within the sets created by the crowd-workers. For two of the metrics the single-query oracle runs are outperformed by the best of the fused approaches (Table 1), but for two metrics they are considerably be er. Also worth noting is that the oracle runs have non-trivial di erences, with di erent best queries arising for di erent metrics in many cases. Across the four metrics, a total of 193 best queries were identi ed.",0,,False
194,5 CONSISTENCY DEFINED,0,,False
195,5.1 De nition,0,,False
196,"As discussed in Section 2, Rank-Biased Overlap (RBO) [38] measures the top-weighted rank similarity between two non-conjoint inde nite rankings. As the rankings increase in similarity, especially towards the start of the rankings, the value of RBO trends towards 1.0. Due to the geometric sum of weights, governed by parameter , the total overlap score is bounded, ranging from 0.0 (no overlap) to 1.0 (total overlap).",0,,False
197,"To use RBO to measure consistency across query variations for a topic, we rst select a common reference or objective ranking (the centroid) for each system-topic pair, making use of the RBC algorithm described in the previous sections. e persistence factor  for RBC is set to 0.90, to mirror a user whose expected depth of examination into a ranked list is 10, a reasonably deep level of examination relative to standard web search; also, the UQV100 pooling ensured at least depth-10 judging for each query from each system. Di erent persistence factors could be selected, which would emphasize shallower or deeper probabilities of inspection of the lists forming the centroid or the depth of overlap. e centroid for all-systems might also have been considered; however we sought to measure a system's self-consistency, rather than with respect to a centroid that requires knowledge of other systems.",1,UQV,True
198,"Given a centroid, we compute the RBO score for every query variation, using the same value of  ,"" 0.90. Computation of RBO provides both a point-estimate, and a minimum, residual, and maximum value. Since system runs typically report 200 or more documents, when  "","" 0.90 the residual is less than 10-10, and the point value is essentially equivalent to the minimum and maximum.""",0,,False
199,"Formally, given a set of query variations Vi ,"" { i .1, i .2, . . . , i .k } for a topic ti  T "","" {t1, t2, ..., tn }, given a system S and its rankings for this set of variations Di "","" {d i .1 , d i .2 , ..., d i .k }, and given the corresponding RBC ranking for S and topic ti , denoted di .rbc, we""",0,,False
200,400,0,,False
201,Session 4A: Evaluation 2,1,Session,True
202,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
203,measure topic consistency Cti with respect to S as,0,,False
204,"Cti ,",0,,False
205,"k q,1",0,,False
206,RBO(d,0,,False
207,"i .q , di .rbc) ,",0,,False
208,k,0,,False
209,(1),0,,False
210,"and the collection consistency CT as the average topic consistency over the set of topics T , again with respect to S, as",0,,False
211,"CT ,",0,,False
212,"n i ,1",0,,False
213,Cti,0,,False
214,.,0,,False
215,n,0,,False
216,(2),0,,False
217,"at is, consistency is the average RBO score relative to the pertopic centroids generated for the system, expressed either as a set of per-topic scores, or aggregated over topics for a per-collection score, but always with regard to a system S. We can also speak of a system's consistency, which is simply topic (or collection) consistency for a particular system. Note that the near-zero RBO residual means that issues of averaging over RBO scores with di erent residuals can be ignored. We choose the average of averages for CT because the number of variations per topic may vary, and topics with large numbers of unique variations should not unduly bias nal scores.",0,,False
218,"Unlike for RBC, where we explored the consequence of adding more variations and thus needed an ordering, for consistency we use all unique query expressions without repeats (unweighted). In UQV100, each topic typically has a small number of commonly chosen variations which occur multiple times, and a large number of variations that occur only once. We wished to avoid biasing the consistency measure unduly by counting the contribution of the more popular variations multiple times. e choice of unweighted query variations also reinforces the decision to compute average of averages for CT ; in UQV100 the number of unique query variations per topic ranges from 19 to 101, so double averaging helps avoid undue in uence from the topics with more diverse variations.",1,ad,True
219,5.2 Why this de nition of consistency?,0,,False
220,"Test collection-based evaluation reduces many sources of variance that occur in information seeking in the wild to a level that is tractable from the standpoint of statistical analysis. Our de nition of consistency is predicated on having test collections that embody some plausible set of query variations per information need. We do not claim that it can address all possible sources of, or needs for, desirable consistency (or inconsistency) in information seeking.",1,ad,True
221,"Our de nition of consistency might be brought into question by queries that exhibit extrinsic diversity [31] or intrinsic diversity [32] or searching as learning [14]. Extrinsic diversity occurs when a query has many di erent information needs that might be associated with it, while intrinsic diversity addresses cases where there are multiple sub-tasks associated in satisfying the information need. Searching as learning involves evolving query expression throughout a session. For cases involving extrinsic diversity, test collections without query variations typically declare one information need and judge relevance with respect to that information need; other plausible information needs are ignored. From the standpoint of assessing consistency, our approach is the same and has the same aw of ignoring other information needs. For cases involving intrinsic diversity and searching as learning, approaches have been developed that target aspects of such complex evaluation situations, including TREC's Web track's Diversity task [10] and",1,ad,True
222,"the Session [20] and Tasks [42] tracks. For assessing just consistency, we suggest that test collections should involve more narrow information needs, with an emphasis on developing speci c unambiguous topic statements. Despite this, we believe that consistency is also important for systems that are able to accurately detect and respond to intrinsic diversity queries, and just as with narrower information needs, there will be a wide range of query variations for an information need that is intrinsically diverse.",1,Session,True
223,"One more question the reader might have is why measure consistency at all, and why not just go straight to average relevance. Two issues arise: rst, relevance judgments are a substantially more expensive resource to accumulate, particularly when dealing with test collections with thousands of query variations. Second, although two rankings may have identical relevance scores, they may be completely di erent. Consider an information need such as ""You are worried about the prevalence of fake news, and decide to nd authoritative newspapers to read instead, just like people did last century."" Now consider two rankings in response to two query variations, one of which lists [theguardian.com, zeit.de, .com, washingtonpost.com] and one of which lists [wsj.com, lemonde.fr, nytimes.com, theglobeandmail.com]. From a relevance standpoint, these rankings are e ectively identical, but from a consistency perspective, they share nothing. If we accept that a searcher cares about re-",1,ad,True
224,"nding the same information for an information need, even if they forget the precise query variation they used previously [37], then it is clear that being able to quantify consistency in rankings is not captured by relevance equivalence alone.",0,,False
225,6 ANALYSIS OF CONSISTENCY,0,,False
226,6.1 Consistency and topics,0,,False
227,"We address RQ-C1 by assessing Cti over the ve contributed runs described in Section 4.1. RQ-C1 asks whether topics vary in consistency, and to do this we plot the average and standard deviation of Cti against all 100 topics of UQV100, sorted by increasing Cti . We characterize this in two ways: Figure 2 shows the results for System 1 (where standard deviation is of the RBO scores per variation for the topic), while Figure 3 shows the results when aggregating over the Cti scores obtained from the ve systems. Even with the large standard deviations that can be observed in both plots (while being clearly smaller in the second), we can conclude that di erent topics have di erent consistency. For persistence  ,"" 0.9, approximately 25% of topics have consistency under 0.25, while around 12% have consistency greater than 0.5. We also observe that some topics have great variation in their consistency scores, and others much less; and, overall, that consistency does indeed vary across topics.""",1,ad,True
228,6.2 Consistency and topic attributes,0,,False
229,"In the rst part of RQ-C2 we ask whether there is a relationship between the number of query variations per topic ti and corresponding consistency scores Cti . Intuitively, we might expect that the more unique query variations there are for a topic, the lower the consistency scores. We address this aspect through a correlation analysis, using Spearman's , a non-parametric rank correlation statistic. Unlike Pearson's product-moment coe cient statistic, this does not rely on the data having equal variance or having few to no outliers. A sca er plot, not shown, demonstrated that these",1,ad,True
230,401,0,,False
231,Session 4A: Evaluation 2,1,Session,True
232,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
233,1.00,0,,False
234,Avg. RBO,0,,False
235,0.75 0.50 0.25 0.00,0,,False
236,0,0,,False
237,25,0,,False
238,50,0,,False
239,75,0,,False
240,100,0,,False
241,Topics,0,,False
242,"Figure 2: Consistency scores Cti for System 1, ordered by score, for 100 topics. Bars are ±1 s.d. of the underlying RBO values.",0,,False
243,1.00,0,,False
244,System,0,,False
245,(a) Num. variations,0,,False
246,(b) Est. docs,0,,False
247,(c) Est. queries,0,,False
248,1,0,,False
249,-0.35,0,,False
250,-0.30,0,,False
251,-0.43,0,,False
252,2,0,,False
253,-0.43,0,,False
254,-0.27,0,,False
255,-0.43,0,,False
256,3,0,,False
257,-0.42,0,,False
258,-0.35,0,,False
259,-0.45,0,,False
260,4,0,,False
261,-0.36,0,,False
262,-0.25,0,,False
263,-0.39,0,,False
264,5,0,,False
265,-0.37,0,,False
266,-0.36,0,,False
267,-0.43,0,,False
268,"Table 5: Correlation measured using Spearman's  between Cti and: (a) number of unique query variations per topic; (b) average estimated useful documents per topic; and (c) average estimated useful queries per topic, for each (system, topic) pair. In all cases, p < 0.01().",0,,False
269,1.00,0,,False
270,0.75,0,,False
271,Consistency,0,,False
272,Avg. average RBO,0,,False
273,0.75,0,,False
274,0.50 0.25,0,,False
275,0.00 0,0,,False
276,25,0,,False
277,50,0,,False
278,75,0,,False
279,100,0,,False
280,Topics,0,,False
281,"Figure 3: Average consistency scores Cti from ve systems, ordered by increasing score, for 100 topics. Bars are ±1 s.d. Note that",0,,False
282,the topics may not be in the same order as in Figure 2.,0,,False
283,"requirements might not hold. In Table 5(a), we show the results for all systems. As expected, the direction of the association is negative (Cti goes down when the number of query variations goes up). However, the correlations are relatively weak weak magnitude (0.3 <  < 0.5) so although there is an association, it is not something we can reliably anticipate. e corresponding sca er plot of values is not shown for space reasons, but con rms that there is a broad range of consistency scores as the number of query variations per topic changes. ese outcomes are a li le surprising, suggesting that the causes of increased consistency are complex.",1,ad,True
284,In the second part of RQ-C2 we ask whether there is a relationship between the estimated topic complexity and corresponding consistency scores. Estimated topic complexity is available as the average of the estimates of the number of useful documents (and the number of queries) expected by each person providing a query,0,,False
285,0.50,0,,False
286,0.25,0,,False
287,0.00,0,,False
288,1,0,,False
289,2,0,,False
290,3,0,,False
291,4,0,,False
292,5,0,,False
293,Systems,0,,False
294,"Figure 4: Consistency scores Cti for ve systems and 100 topics. e diamond marks the median, and the horizontal line marks CT .",0,,False
295,"variation for a topic description in UQV100. More complex topics have higher values for these two averaged estimates. Again, intuitively we might expect that the more complex a topic is, the lower the consistency score. As above, we assess the relationship using Spearman's , for both the average estimated documents and average estimated queries per topic ti and corresponding consistency scores Cti . Results are shown in Table 5(b) and (c), and just as before indicate a negative association, as surmised. However, once again the correlations are weak at best, at best of weak magnitude, meaning that increases in estimated topic complexity are only loosely associated with decreases in consistency. Interestingly, the estimates of the required number of queries all have a stronger correlation than the corresponding estimates of the required number of useful documents. is outcome might in part arise because the complexity estimate providers may be be er at estimating changes in small numbers than in larger ones, but there are other possible explanations.",1,UQV,True
296,402,0,,False
297,Session 4A: Evaluation 2,1,Session,True
298,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
299,System,0,,False
300,2,0,,False
301,3,0,,False
302,4,0,,False
303,5,0,,False
304,1,0,,False
305,-9.756 -10.497 -14.362 -1.030,0,,False
306,2,0,,False
307,-0.930 -6.821 9.107,0,,False
308,3,0,,False
309,-8.035 8.490,0,,False
310,4,0,,False
311,12.330,0,,False
312,Table 6: System di erences measured using a paired Student's,0,,False
313,"t-test between all pairs of systems over the corresponding Cti per topic. Values reported are the t statistic, df ,"" 99 in all cases, and""",0,,False
314,signi cant di erences at p < 0.05 () and p < 0.01 ().,0,,False
315,6.3 Consistency and systems,0,,False
316,"If consistency was identical across di erent retrieval systems, then it would be uninteresting when selecting an e ective system. In RQC3, we examine the relationship between collection consistency CT and the ve systems which contributed to UQV100. In Figure 4 we report on the consistency scores for each system, using boxplots to show the spread of values. From this we can observe that Systems 1 and 5 are very similar to each other (CT 0.32); Systems 2 and 3 are very similar to each other and more consistent (CT 0.40), and System 4 is di erent and yet more consistent (CT ,"" 0.44). Using a two-tailed paired Student's t-test, we also assessed each pair of systems; Table 6 con rms our observations.""",1,UQV,True
317,"Another way of understanding the relationship between consistency and systems is to treat each topic as an ""assessor"" and each system as the ""subject"" being assessed with regards to its degree of consistency. Since topics in UQV100 contain a similar number of query variations (average of 55 per topic) as many existing test collections have queries, and past practice has been to examine rank order correlation of systems by comparing sets of systems across two or more collections, we will adopt a similar method here. We assess how similar the relative ordering of systems is using one-way Intraclass Correlation [4], Kendall's Coe cient of Concordance [21] (commonly wri en as Kendall's W ), and Krippendor 's  [17]. All of these measures address inter-assessor agreement, for three or more assessors, and can accommodate ordinal data (and for ICC and , interval or ratio data). In Table 7, we report the results for these measures of rank agreement, where the score is the topic consistency Cti . We use these measures rather than pair-wise comparisons of topics using Kendall's  , since we have 100 topic ""assessors"" involved, and the measures allow us to consider multiple ""assessors"" with a single test statistic, while Kendall's  only supports two ""assessors"". Due to the di erences between ICC and Kendall's W , it is expected that ICC scores may be lower than Kendall's W scores over the same data, since it considers not just relative rank order but also the magnitude of di erences, as discussed by Sheskin [34]. In all three measures, 1 indicates perfect agreement among the assessors, and 0 indicates no agreement beyond what would be expected by chance. Both ICC and  can report small negative values, which also signify no agreement.",1,UQV,True
318,"e values from ICC and Krippendor 's  both indicate there is a very low degree of inter-assessor agreement in rank ordering the systems by consistency; and although Kendall's W is 0.491 for Cti , this is still a relatively low degree of agreement.",0,,False
319,"From these two analyses, we conclude that although these systems do have di erent overall collection consistency CT , they are",0,,False
320,Agreement,0,,False
321,(a) ICC,0,,False
322,(b),0,,False
323,(c),0,,False
324,Kendall's W Krippendor 's ,0,,False
325,Cti,0,,False
326,NDCG INST,0,,False
327,0.111,0,,False
328,-0.002 -0.005,0,,False
329,0.491,0,,False
330,0.169 0.033,0,,False
331,0.090,0,,False
332,-0.003 -0.006,0,,False
333,"Table 7: Rank agreement over all ve systems measured using (a) Intraclass Correlation; (b) Kendall'sW ; and (c) Krippendor 's . For ICC and W , signi cance is reported as p < 0.05(), and p < 0.01(); for  it is not reported. e top row uses Cti as the ranking score for each system; the bo om two rows use NDCG and INST (averaged by topic, as for Cti ).",0,,False
334,System AP NDCG Q RBP INST,1,AP,True
335,1,0,,False
336,0.61 0.68 0.56 0.32 0.35,0,,False
337,2,0,,False
338,0.62 0.70 0.58 0.33 0.37,0,,False
339,3,0,,False
340,0.55 0.62 0.53 0.32 0.36,0,,False
341,4,0,,False
342,0.59 0.65 0.55 0.39 0.40,0,,False
343,5,0,,False
344,0.59 0.65 0.53 0.25 0.30,0,,False
345,"Table 8: Correlation measured using Spearman's  between topic consistency Cti and corresponding relevance measures (AP, NDCG, Q measure, RBP0.85, and INST), for each (system, topic) pair. In all cases, there is a signi cant correlation, with p < 0.01 (), except for System 5 and RBP, signi cant only at p < 0.05 ().",1,AP,True
346,"not systematically ordered on the basis of topic consistency Cti . at is, for one topic a particular system may have high consistency,",0,,False
347,"while for another topic a completely di erent system may have high consistency, and the earlier system may have low consistency.",0,,False
348,6.4 Consistency and relevance,0,,False
349,"Our last investigation, addressing RQ-C4, concerns the relationship between consistency and relevance. e construction of UQV100 guaranteed relevance judgments to at least depth 10 for all query variations for the ve systems being analyzed. us we are able to explore the degree of correlation between consistency and relevance, across a range of relevance measures, including AP, NDCG, Q measure, RBP0.85, and INST. e results, calculated using Spearman's , are displayed in Table 8. While there is moderate correlation for the ""deep"" relevance measures (AP, NDCG, and Q), there is only weak correlation for the ""shallow"" relevance measures (RBP0.85 and INST). Sca er plots of the data, not shown, indicate considerable spread of scores for all measures as Cti increases.",1,ad,True
350,"As a comparison with consistency, we repeat the topics-as""assessors"" inter-assessor agreement analysis, with results reported for average-by-topic NDCG and INST scores in rows two and three of Table 7. While any degree of agreement was only just observable for consistency, with these relevance measures, any agreement on the ordering of systems by relevance is e ectively random.",0,,False
351,7 CONCLUSIONS,0,,False
352,Consistency ­ the ability to give similar results for a topic even when presented with di erent queries ­ is desirable for search engines in a variety of circumstances. We have de ned a consistency,0,,False
353,403,0,,False
354,Session 4A: Evaluation 2,1,Session,True
355,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
356,"measure and explored it across a set of 5,736 query variations across 100 topics, using a novel relevance-based centroid algorithm.",0,,False
357,"e RBC algorithm for fusing rankings has the bene t of incorporating a persistence parameter that allows modeling of di erent depths of a ention into rankings, and adds another strand to the ""RB-"" family. RBC is competitive or be er than existing algorithms, and like Borda count has no reliance on system scores. We con-",1,corpora,True
358,"rmed previous ndings that data fusion over queries and over systems is bene cial, and fusion over both is even be er. With the oracle runs we have also demonstrated that substantially be er e ectiveness performance is possible, at least hypothetically.",0,,False
359,"Based on the various analyses, we can also state that the consistency measure informs us about something di erent to existing measures. Consistency varies by topic and by system, tends to decrease as topic complexity and the number of query variations increases, and has weak-to-moderate correlations with several relevance measures. However, in no circumstance is consistency strongly correlated with any of these existing test collection dimensions, con rming that it measures a di erent property altogether. Neither the measures of consistency nor relevance reliably order the",0,,False
360,"ve systems over the UQV100 topics, indicating there are no clear system winners or losers for this test collection on these dimensions of e ectiveness when examined topic by topic.",1,UQV,True
361,"More investigation is required into the nature of consistency and its e ect on perceptions of the retrieval e ectiveness of search systems. Such work might include user studies, low-level analysis of the root causes of variable ranking within one or more systems, and broadening the current analysis to similar test collections with multiple query variations per topic.",1,ad,True
362,"Acknowledgment is work was supported by the Australian Research Council's Discovery Projects Scheme (projects DP110101934 and DP140102655). Ma Crane, Xiaolu Lu, David Maxwell, and Andrew Trotman assisted greatly, providing the system runs that were analyzed. e UQV100 judgments were generated using resources provided by Microso .",1,UQV,True
363,REFERENCES,0,,False
364,"[1] J. A. Aslam and M. Montague. 2001. Models for metasearch. In Proc. SIGIR. ACM, 276­284.",0,,False
365,"[2] P. Bailey, A. Mo at, F. Scholer, and P. omas. 2015. User variability and IR system evaluation. In Proc. SIGIR. 625­634.",0,,False
366,"[3] P. Bailey, A. Mo at, F. Scholer, and P. omas. 2016. UQV100: A test collection with query variability. In Proc. SIGIR. 725­728. Public data: h p://dx.doi.org/10. 4225/49/5726E597B8376.",1,UQV,True
367,"[4] J. J. Bartko. 1966. e intraclass correlation coe cient as a measure of reliability. Psychological Reports 19, 1 (1966), 3­11.",0,,False
368,"[5] N. J. Belkin, C. Cool, W. B. Cro , and J. P. Callan. 1993. e e ect of multiple query representations on information retrieval system performance. In Proc. SIGIR. 339­346.",0,,False
369,"[6] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. 1995. Combining the evidence of multiple query representations for information retrieval. Inf. Proc. & Man. 31, 3 (1995), 431­448.",0,,False
370,[7] C. Buckley and J. Walz. 1999. e TREC-8 query track. In Proc. TREC.,1,TREC,True
371,"[8] J. Callan. 2002. Distributed information retrieval. In Advances in Information Retrieval. Springer, 127­150.",0,,False
372,[9] B. Cartere e. 2009. On rank correlation and the distance between rankings. In Proc. SIGIR. 436­443.,0,,False
373,"[10] K. Collins- ompson, C. Macdonald, P. N. Benne , F. Diaz, and E. M. Voorhees. 2014. TREC 2014 web track overview. In Proc. TREC.",1,TREC,True
374,[11] S. Cucerzan and E. Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proc. EMNLP. 293­300.,0,,False
375,"[12] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. 2001. Rank aggregation methods for the web. In Proc. WWW. 613­622.",0,,False
376,[13] E. A. Fox and J. Shaw. 1993. Combination of multiple searches. In Proc. TREC. 243­252.,1,TREC,True
377,"[14] L. Freund, H. O'Brien, and R. Kopak. 2014. Ge ing the big picture: Supporting comprehension and learning in search. In Proc. Searching As Learning (SAL) Workshop.",0,,False
378,"[15] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. 1987. e vocabulary problem in human-system communication. Comm. ACM 30, 11 (1987), 964­971.",0,,False
379,"[16] F. C. Gey, N. Kando, and C. Peters. 2005. Cross-language information retrieval: The way ahead. Inf. Proc. & Man. 41, 3 (2005), 415­431.",1,ad,True
380,"[17] A. F. Hayes and K. Krippendor . 2007. Answering the call for a standard reliability measure for coding data. Commun. Methods and Measures 1, 1 (2007), 77­89.",0,,False
381,"[18] J.-Y. Jiang, J. Liu, C.-Y. Lin, and P.-J. Cheng. 2015. Improving ranking consistency for web search by leveraging a knowledge base and search logs. In Proc. CIKM. 1441­1450.",0,,False
382,"[19] R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Generating query substitutions. In Proc. WWW. 387­396.",1,ad,True
383,"[20] E. Kanoulas, B. Cartere e, M. Hall, P. Clough, and M. Sanderson. 2011. Overview of the TREC 2011 session track. In Proc. TREC.",1,TREC,True
384,"[21] M. G. Kendall and B. B. Smith. 1939. e problem of m rankings. Annals of Mathematical Statistics 10, 3 (1939), 275­287.",0,,False
385,[22] B. Koopman and G. Zuccon. 2016. A test collection for matching patients to clinical trials. In Proc. SIGIR. 669­672.,0,,False
386,"[23] R. T.-W. Lo, B. He, and I. Ounis. 2005. Automatically building a stopword list for an information retrieval system. J. Dig. Inf. Man. 3, 1 (2005), 3­8.",0,,False
387,"[24] J. B. Lovins. 1968. Development of a stemming algorithm. MIT Information Processing Group, Electronic Systems Laboratory Cambridge.",0,,False
388,"[25] H. P. Luhn. 1957. A statistical approach to mechanized encoding and searching of literary information. IBM J. Res. Dev. 1, 4 (1957), 309­317.",0,,False
389,[26] A. Mo at. 2016. Judgment pool e ects caused by query variations. In Proc. Aust. Doc. Comp. Symp. 65­68.,0,,False
390,"[27] A. Mo at and J. Zobel. 2008. Rank-biased precision for measurement of retrieval e ectiveness. ACM Trans. Inf. Sys. 27, 1 (2008), 2.1­2.27.",0,,False
391,[28] M. Montague and J. A. Aslam. 2001. Relevance score normalization for metasearch. In Proc. CIKM. 427­433.,0,,False
392,[29] M. Montague and J. A. Aslam. 2002. Condorcet fusion for improved retrieval. In Proc. CIKM. 538­548.,0,,False
393,"[30] J. Pickens, G. Golovchinsky, C. Shah, P. Qvarfordt, and M. Back. 2008. Algorithmic mediation for collaborative exploratory search. In Proc. SIGIR. 315­322.",0,,False
394,"[31] F. Radlinski, P. N. Benne , B. Cartere e, and T. Joachims. 2009. Redundancy, diversity and interdependent document relevance. SIGIR Forum 43, 2 (2009), 46­52.",1,ad,True
395,"[32] K. Raman, P. N. Benne , and K. Collins- ompson. 2013. Toward whole-session relevance: Exploring intrinsic diversity in web search. In Proc. SIGIR. 463­472.",0,,False
396,"[33] S. E. Robertson. 1990. On term selection for query expansion. J. Documentation 46, 4 (1990), 359­364.",0,,False
397,[34] D. J. Sheskin. 2003. Handbook of Parametric and Nonparametric Statistical Procedures. CRC Press.,0,,False
398,"[35] K. Spa¨rck Jones and C. J. van Rijsbergen. 1975. Report on the need for and the provision of an ""ideal"" information retrieval test collection. Technical Report 5266. Computer Laboratory, University of Cambridge. British Library Research and Development Report.",0,,False
399,"[36] L. Tan and C. L. A. Clarke. 2015. A family of rank similarity measures based on maximized e ectiveness di erence. IEEE Trans. Know. Data Eng. 27, 11 (2015), 2865­2877.",0,,False
400,"[37] J. Teevan, E. Adar, R. Jones, and M. A. S. Po s. 2007. Information re-retrieval: Repeat queries in Yahoo's logs. In Proc. SIGIR. 151­158.",1,Yahoo,True
401,"[38] W. Webber, A. Mo at, and J. Zobel. 2010. A similarity measure for inde nite rankings. ACM Trans. Inf. Sys. 28, 4 (2010), 20.1­20.38.",0,,False
402,"[39] M. Wu, D. Hawking, A. Turpin, and F. Scholer. 2012. Using anchor text for homepage and topic distillation search tasks. JASIST 63, 6 (2012), 1235­1255.",0,,False
403,"[40] S. Wu and S. McClean. 2006. Performance prediction of data fusion for information retrieval. Inf. Proc. & Man. 42 (2006), 899­915.",0,,False
404,"[41] E. Yilmaz, J. A. Aslam, and S. Robertson. 2008. A new rank correlation coe cient for information retrieval. In Proc. SIGIR. 587­594.",0,,False
405,"[42] E. Yilmaz, M. Verma, R. Mehrotra, E. Kanoulas, B. Cartere e, and N. Craswell. 2015. Overview of the TREC 2015 tasks track. In Proc. TREC.",1,TREC,True
406,"[43] G. Zuccon, J. Palo i, and A. Hanbury. 2016. ery variations and their e ect on comparing information retrieval systems. In Proc. CIKM. 691­700.",0,,False
407,404,0,,False
408,,0,,False

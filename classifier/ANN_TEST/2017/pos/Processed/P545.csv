,sentence,label,data,regex
0,Session 5A: Retrieval Models and Ranking 3,1,Session,True
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Learning to Diversify Search Results via Subtopic Attention,0,,False
3,"Zhengbao Jiang1,2, Ji-Rong Wen1,2,3, Zhicheng Dou1,2, Wayne Xin Zhao1,2, Jian-Yun Nie4, Ming Yue1,2",0,,False
4,"1School of Information, Renmin University of China 2Beijing Key Laboratory of Big Data Management and Analysis Methods, China 3Key Laboratory of Data Engineering and Knowledge Engineering, MOE, China",0,,False
5,"4DIRO, Universite´ de Montre´al, Que´bec",0,,False
6,"rucjzb@163.com, jirong.wen@gmail.com, dou@ruc.edu.cn,",0,,False
7,"batmanfly@gmail.com, nie@iro.umontreal.ca, yomin@ruc.edu.cn",0,,False
8,ABSTRACT,0,,False
9,"Search result diversification aims to retrieve diverse results to satisfy as many different information needs as possible. Supervised methods have been proposed recently to learn ranking functions and they have been shown to produce superior results to unsupervised methods. However, these methods use implicit approaches based on the principle of Maximal Marginal Relevance (MMR). In this paper, we propose a learning framework for explicit result diversification where subtopics are explicitly modeled. Based on the information contained in the sequence of selected documents, we use attention mechanism to capture the subtopics to be focused on while selecting the next document, which naturally fits our task of document selection for diversification. The framework is implemented using recurrent neural networks and max-pooling which combine distributed representations and traditional relevance features. Our experiments show that the proposed method significantly outperforms all the existing methods.",1,ad,True
10,KEYWORDS,0,,False
11,search result diversification; subtopics; attention,0,,False
12,1 INTRODUCTION,1,DUC,True
13,"In real search scenario, queries issued by users are usually ambiguous or multi-faceted. In addition to being relevant to the query, the retrieved documents are expected to be as diverse as possible in order to cover different information needs. For example, when users issue ""apple"", the underlying intents could be the IT company or the fruit. The retrieved documents should cover both topics to increase the chance to satisfy users with different information needs.",1,ad,True
14,"Traditional approaches to search result diversification are usually unsupervised and adopt manually defined functions with empirically tuned parameters. Depending on whether the underlying intents (or subtopics) are explicitly modeled, they can be categorized into implicit and explicit approaches [28]. Implicit approaches [6] do not model intents explicitly. They emphasize novelty,",1,ad,True
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080805",1,ad,True
16,"i.e. the following document should be ""different"" from the former ones based on some similarity measures. Instead, explicit approaches [1, 12, 13, 16, 27, 35] model intents (or subtopics) explicitly. They aim to improve intent coverage, i.e. the following document should cover the intents not satisfied by previous ones. Intents or subtopics can be determined by techniques such as query reformulation [2, 14, 34, 38] and query clustering based on query logs and other types of information. Existing studies showed that explicit approaches have better performance [12, 13, 16, 27, 35] than implicit approaches due to several reasons: on the one hand, they provide a more natural way to handle subtopics than implicit approaches; on the other hand, their ranking functions are closer to the diversity evaluation metrics which are mostly based on explicit subtopics. Furthermore, most similarity measures used in the implicit approaches, e.g., those based on language model or vector space model, are determined globally on the whole documents, regardless of possible search intents. This might be problematic for search result diversification: two documents could contain similar words and considered globally similar, but this similar part may be unrelated to underlying search intents.",1,ad,True
17,"To avoid heuristic and handcrafted functions and parameters, a new family of research work using supervised learning is proposed. They try to learn a ranking function automatically. Their major focus lies in the modeling of diversity, including structural prediction [36], rewarding functions for novel contents [39], measurebased direct optimization [32], and neural network based method [33]. Regardless of diversity modeling and optimization methods, all these solutions inherit the spirit of MMR which is an implicit approach and do not take intents into consideration. Although the learning methods may result in a better similarity measure, they are hindered by the gap between reducing document redundancy and improving intent coverage. They suffer from similar problems with implicit unsupervised approaches. Without modeling subtopics explicitly, they can't directly improve intent coverage. Hence, there is a need to incorporate explicit subtopic modeling into supervised diversification methods.",1,corpora,True
18,"To address the above issue, we propose to model subtopics in a general supervised learning framework. Our framework combines the strengths of both explicit unsupervised approaches and (implicit) supervised approaches. First, subtopics are explicitly modeled, allowing us to improve intent coverage in a proactive way. Second, it automatically learns the diversification ranking function, and is able to capture complex interaction among documents and",1,ad,True
19,545,0,,False
20,Session 5A: Retrieval Models and Ranking 3,1,Session,True
21,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
22,Table 1: Subtopic relevance example.,0,,False
23,doc\subtopic i1 i2 i3,0,,False
24,d1 d2 d3,0,,False
25,× × × ×,0,,False
26,d4,0,,False
27,×,0,,False
28,×,0,,False
29,"subtopics. We call this framework Document Sequence with Subtopic Attention (DSSA). More specifically, to select the next document, we first model the sequence of selected documents in order to capture their contents as well as their relationship with the subtopics. Then based on the information contained by previous documents, attention mechanism is used to determine the undercovered subtopics to which we have to pay attention in selecting the next document. Attention mechanism has been successfully used to deal with various problems in image understanding [24] and NLP [3, 21]. This mechanism corresponds well to the document selection problem in search result diversification: attention on subtopics changes along with the addition of a document in the result list. For example. Assume that we have 3 subtopics and 4 documents whose relevance judgments are shown in Table 1. Given that we have selected d1 and d2, which cover subtopics i1 and i2, the attention for next choice should incline to i3 which is not covered, thus d3 is a better choice than d4 at this position. We will show that the DSSA framework is general enough to cover the ideas of previous unsupervised explicit methods.",1,ad,True
30,"We then propose a specific implementation of DSSA using recurrent neural networks (RNN) and max-pooling to leverage both distributed representations and traditional relevance features, which we call DSSA-RNNMP. Experimental results on TREC Web Track data show that our method outperforms the existing methods significantly. To our knowledge, this is the first time that a supervised learning framework with attention mechanism is used to model subtopics explicitly for search result diversification.",1,ad,True
31,2 RELATED WORK,0,,False
32,2.1 Implicit Diversification Approaches,0,,False
33,The basic assumption of implicit diversification approaches is that dissimilar documents are more likely to satisfy different information needs. The most representative approach is MMR [6]:,0,,False
34,"SMMR(q, d,",0,,False
35,C),0,,False
36,",",0,,False
37,(1,0,,False
38,-,0,,False
39,")S rel (d ,",0,,False
40,q),0,,False
41,-,0,,False
42,max,0,,False
43,dj C,0,,False
44,"S div (d ,",0,,False
45,dj,0,,False
46,"),",0,,False
47,(1),0,,False
48,"where Srel and Sdiv model document d's relevance to the query q and its similarity to a selected documents dj respectively. To gain high ranking score, a document should not only be relevant, but also be dissimilar from the selected documents. The definition of measures for relevance and document similarity is crucial, which is done manually in this approach.",0,,False
49,"Recently, machine learning methods have been leveraged to learn score functions. Yue and Joachims [36] proposed SVM-DIV which uses structural SVM to learn to identify a document subset with maximum word coverage. However, word coverage may be different from intent coverage. Optimizing the former may not necessarily lead to optimizing the latter. Similar to MMR, Zhu et",1,ad,True
50,Table 2: Categorization of diversification approaches.,0,,False
51,unsupervised,0,,False
52,supervised,0,,False
53,implicit MMR,0,,False
54,"SVM-DIV, R-LTR, PAMM, NTN",0,,False
55,explicit,0,,False
56,"IA-Select, xQuAD, PM2, TxQuAD, TPM2, HxQuAD, HPM2, 0-1 MSKP",1,HP,True
57,DSSA (our approach),0,,False
58,"al. [39] proposed relational learning-to-rank model (R-LTR) which learns to score a document based on both relevance and novelty automatically, in order to maximize the probability of optimal rankings. Based on R-LTR score function, Xia et al. [32] proposed a perceptron algorithm using measures as margins (PAMM) to directly optimize evaluation metrics by enlarging the score margin of positive and negative rankings. They further proposed to use a neural tensor network (NTN) [33] to measure document similarity automatically from document representations, which avoids the burden to define handcrafted diversity features.",0,,False
59,"The above supervised approaches are shown to outperform the unsupervised counterparts. However, they are all implicit approaches without using subtopics. In this paper, we propose a learningbased explicit approach which models subtopics explicitly.",0,,False
60,2.2 Explicit Diversification Approaches,0,,False
61,"Explicit approaches model subtopics underlying a query, aiming at returning documents covering as many subtopics as possible. These approaches leverage external resources to explicitly represent information needs in subtopics. IA-Select [1] uses classified topical categories based on ODP taxonomy. xQuAD [27] is a probabilistic framework that uses query reformulations as intent representations. PM2 [13] tackles search result diversification problem from the perspective of proportionality. TxQuAD and TPM2 [12] represent intents by terms and transform intent coverage to term coverage. Hu et al. [16] proposed to use a hierarchical structure for subtopics instead of a flat list, which copes with the inherent interaction among subtopics. Two specific models, namely HxQuAD and HPM2, were proposed using hierarchical structure. Yu et al. [35] formulated diversification task as a 0-1 multiple subtopic knapsacks (0-1 MSKP) problem where documents are chosen like filling up multiple subtopic knapsacks. To tackle this NP-hard problem, max-sum belief propagation is used.",1,ODP,True
62,"As summarized in Table 2, all existing explicit approaches are unsupervised and the functions and parameters are defined heuristically. In this paper, we use supervised learning to model the interaction among documents and subtopics simultaneously.",0,,False
63,2.3 RNN with Attention Mechanism,0,,False
64,"RNN can capture the interdependency between elements in a sequence. Attention mechanism, which is usually built on RNN, mimics human attention behavior focusing on different local region of the object (an image, a sentence, etc) at different times. In computer vision, Google DeepMind [24] used RNN with attention to extract information from an image by adaptively selecting a sequence of the most informative regions instead of the whole image. In NLP, attention mechanism is typically used in neural machine translation (NMT). Traditional encoder-decoder models encode the source",1,ad,True
65,546,0,,False
66,Session 5A: Retrieval Models and Ranking 3,1,Session,True
67,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
68,"sentence into a fixed-length vector from which the target sentence is decoded. Such fixed-length vector may not be powerful enough to reflect all the information of the source sentence. An attentionbased model [3] was proposed to automatically pay unequal and varied attention to source words during decoding process. In particular, to decide the next target word, not only the fixed-length vector, but also the hidden states corresponding to source words relevant to the target word are used. Luong et al. [21] generalized the idea and proposed two classes of attention mechanism, namely global and local approaches. In this paper, attention mechanism is used on subtopics, which guides the model to emphasize different intents at different positions.",0,,False
69,"In the following section, we will first propose a general framework, then instantiate it with a specific implementation.",0,,False
70,3 DOCUMENT SEQUENCE WITH SUBTOPIC ATTENTION FRAMEWORK,0,,False
71,"Given a query set Q, a document set Dq and a subtopic set Iq for",0,,False
72,"each query q  Q, the goal of explicit methods is to learn a ranking",0,,False
73,"function f (q, Dq, Iq ) which is expected to output a ranking of do-",0,,False
74,cuments in Dq that is both relevant and diverse. The loss function,0,,False
75,could be written in the following general form:,0,,False
76,"L(f (q, Dq , Iq ), Yq ),",0,,False
77,(2),0,,False
78,qQ,0,,False
79,"where L measures the quality gap between the ranking outputted by f and the best ranking Yq . Different from traditional retrieval tasks, diversity has to be considered in the ranking and evaluation",1,ad,True
80,"process. Theoretically, diversity ranking is NP-hard [1, 7]. Hence,",1,NP,True
81,"a common strategy is to make greedy selections [6, 27]: at the t-th position, we assume that t - 1 documents have been selected and formed a document sequence Ct-1. The task is to select a locally optimal document dt from the remaining candidate documents based on a score function S(q, dt , Ct-1, Iq ). Note that implicit supervised methods correspond to the case where Iq is an empty set.",0,,False
82,"To motivate our approach, we start with the ideas of the unsu-",0,,False
83,"pervised explicit approaches, which can be formulated as the fol-",0,,False
84,lowing general form:,0,,False
85,"Sunsupervised(q, dt , Ct -1, Iq ) ,",0,,False
86,(1,0,,False
87,- )S ,0,,False
88,rel(dt,0,,False
89,",",0,,False
90,q)+,0,,False
91, relevance,0,,False
92,"Sdiv(dt , ik ) A(Ct -1, Iq )k ,  diversity",0,,False
93,(3),0,,False
94,ik Iq,0,,False
95,subtopic weights,0,,False
96,where ik  Iq is the k-th subtopic of q and Srel and Sdiv calculate document dt 's relevance to a query and to a subtopic respectively.,0,,False
97,The essence of diversity lies in the function A which calculates the,0,,False
98,"weights for subtopics Iq based Ct -1. For xQuAD, A(Ct -1, Iq )k",0,,False
99,",onPp(irkev|qi)ousddj oCcut-m1 (e1nt-sPe(qduje|inkc)e)",0,,False
100,"where P(ik |q) is the initial importance of subtopic ik , P(dj |ik ) is",0,,False
101,the probability that dj is relevant to ik . The weight of a subtopic,0,,False
102,is determined by the likelihood that previous documents are not,0,,False
103,relevant to this subtopic. PM2 mimics seats allocation of compe-,0,,False
104,"ting political parties to adjust subtopic weights after each selection,",1,ad,True
105,"i.e. A(Ct-1, Iq ) is estimated according to the difference between the subtopic's distributions in Ct-1 and in Iq . All these methods",0,,False
106,sequence of selected,0,,False
107,documents d1 d2 d3,0,,False
108,candidate documents d4 d5 d6,0,,False
109,hidden state,0,,False
110,document sequence representation,0,,False
111,subtopic attention,0,,False
112,subtopic attention distribution,0,,False
113,diversity scoring,0,,False
114,score,0,,False
115,relevance scoring,0,,False
116,subtopics i1 i2,0,,False
117,query q,0,,False
118,Figure 1: Illustration of DSSA framework.,0,,False
119,Table 3: Notations in DSSA.,0,,False
120,Notation Definition,0,,False
121,"r , dt a ranking, the t-th document.",0,,False
122,"q, ik",0,,False
123,"the query, the k-th subtopic.",0,,False
124,vdt,0,,False
125,representation of the document at the t-th position.,0,,False
126,vq,0,,False
127,representation of the query.,0,,False
128,vik,0,,False
129,representation of the k-th subtopic.,0,,False
130,ht,0,,False
131,hidden state of previous t documents.,0,,False
132,"at,k",0,,False
133,"attkKe,""n1tiaotn,k""",0,,False
134,"on ,",0,,False
135,"the k-th 1, at,k ",0,,False
136,"subtopic at the t-th position. [0, 1] where K is the number",0,,False
137,of subtopics. A large value means that this subtopic,0,,False
138,is less satisfied by previous t - 1 documents and thus,0,,False
139,needs more attention at the t-th position.,0,,False
140,sdt,0,,False
141,the final score of the document at the t-th position.,0,,False
142,"don't model the selected documents as a sequence. In addition, the functions and parameters are heuristically defined, which may not best fit the final goal.",1,ad,True
143,"To tackle the above problems, we extend Equation (3) to the following general learning framework:",0,,False
144,"SDSSA(q, dt , Ct -1, Iq ) , sdt ,",0,,False
145,(1,0,,False
146,-,0,,False
147,)Srel (,0,,False
148,(vdt,0,,False
149,",",0,,False
150,vq,0,,False
151,)+ (,0,,False
152,)),0,,False
153,"Sdiv vdt , vi(·) , A H ([vd1 , ..., vdt-1 ]), vi(·) ,",0,,False
154, relevance  diversity,0,,False
155,subtopic attention,0,,False
156,"(4) where documents, queries, and subtopics are denoted by their representations, as explained in Table 3. In this paper, we focus on learning a ranking function only and assume that these representations are given and will not be modified. There are three main components, namely (1) document sequence representation component H , (2) subtopic attention component A, and (3) scoring component Srel and Sdiv, which are also illustrated in Figure 1. This framework is inspired from the attention models",0,,False
157,547,0,,False
158,Session 5A: Retrieval Models and Ranking 3,1,Session,True
159,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
160,"used in image understanding [24] and neural machine translation [3, 21], however adapted to our diversification task.",1,ad,True
161,"Next, we briefly describe the three components. The document sequence representation component H encodes the information contained in document sequence Ct-1 into a fixed-length hidden state ht-1, which could consider the interaction and dependency among these documents. ht-1 could be viewed as a comprehensive and high-level representation of Ct-1. The subtopic attention at,(·) is calculated by the subtopic attention component A using ht-1 and subtopic representations vi(·) . The attention evolves from the first to the last ranking position, driving the model to emphasize different subtopics based on previous document sequence. Finally, the scoring components Srel and Sdiv calculate relevance and diversity scores respectively. Notice that Sdiv is not limited to be a weighted sum over all subtopics as Equation (3). It can incorporate more complex interaction among subtopics.",1,corpora,True
162,"The essence of this framework can be summarized as follows. Along with the selection of more documents, we encode the information of previous document sequence, and the attention mechanism will monitor the degree of satisfaction for each subtopic. High scores are assigned to the documents relevant to less covered subtopics. Finally, multiple subtopics would be well covered by adaptively learning the attention. In this way, our framework builds an intuitive approach to explicitly model subtopics. We name the framework Document Sequence with Subtopic Attention (DSSA). DSSA is a unified architecture that takes both relevance and diversity into consideration, and diversity is achieved by modeling the interaction among documents and subtopics.",1,ad,True
163,4 RESULT DIVERSIFICATION USING DSSA,0,,False
164,"In this section, we instantiate DSSA to a concrete form and articulate the training and prediction algorithms. The main idea of DSSA is to dynamically capture accumulative relevance information of previous document sequence, so as to calculate subtopic attention. Inspired by the recent progress on sequence data modeling, we adapt RNN to capture the information of previous document sequence based on distributed representations of documents. However, the effectiveness of distributed representation heavily depends on a large amount of training data. Typically, the representation is built automatically using the data to optimize an objective function [17]. We do not have such large data and we can only use unsupervised methods (e.g. doc2vec) to create representation, of which the effectiveness could be suboptimal. Indeed, our preliminary experiments using only the distributed representation created by unsupervised methods yield low effectiveness. To compensate this weakness, we also use traditional relevance features such as BM25 score, which are proven useful, to calculate subtopic attention and final score. Such a combination of distributed representations and features has been used in several previous works [29, 33]. In addition to RNN, we also adopt the way using max-pooling [33], which has been shown effective, to implement subtopic attention mechanism. We call this model DSSA-RNNMP (DSSA model using RNN and Max-Pooling), as illustrated in Figure 2. In addition, we also propose a list-pairwise approach for optimization, which is different from the existing studies.",1,ad,True
165,query/subtopic representation,0,,False
166,document representation,0,,False
167,max-pooling signals,0,,False
168,hidden state,0,,False
169,RNN,0,,False
170,h1,0,,False
171,ht -1,0,,False
172,ed1,0,,False
173,edt-1,0,,False
174,max-pooling,0,,False
175,"at ,(·)",0,,False
176,subtopic attention,0,,False
177,edt,0,,False
178,s div dt,0,,False
179,e i1 ei2 eq,0,,False
180,s rel dt,0,,False
181,diversity score,0,,False
182,relevance score,0,,False
183,"xd1 ,i1 xd1 ,i2",0,,False
184,"xd1 , q",0,,False
185,"xdt-1 ,i1 xdt-1 ,i2",0,,False
186,"xdt-1 ,q",0,,False
187,"xdt ,i1 xdt ,i2 xdt ,q",0,,False
188,Figure 2: Architecture of DSSA-RNNMP. Previous t - 1 do-,0,,False
189,"cuments are encoded into ht-1 from distributed representations ed1 , ..., edt-1 . Attention on the k-th subtopic at,k is then calculated based on (1) hidden state ht-1 and subto-",0,,False
190,"pic representation eik (2) max-pooling on relevance features xd1,ik , ..., xdt-1,ik .",0,,False
191,Table 4: Parameters in DSSA-RNNMP.,0,,False
192,"Notation W n, bn W a, wp W s, wr",0,,False
193,Definition parameters of RNN with vanilla cell. parameters used in subtopic attention. parameters used in scoring.,0,,False
194,4.1 A Neural Network Implementation,0,,False
195,"We first describe the constitution of representations, namely vdt , vq , and vik , then elaborate how we implement document sequence representation, subtopic attention, and scoring com-",0,,False
196,ponents. The parameters to be learned are listed in Table 4. vdt : the representation of a document is composed of two parts:,0,,False
197,distributed representations and relevance features. Distributed re-,0,,False
198,"presentation can be constructed in different ways. In this paper,",0,,False
199,"we consider three methods: SVD, LDA [4], and doc2vec [18]. Rele-",0,,False
200,"vance features are those used in traditional IR, such as BM25 score",1,ad,True
201,"etc. Suppose that we have a distributed representation of size Ed , K subtopics, and R relevance features, the total size of vdt would be Ed + R + KR. We use edt  REd , xdt,q and xdt,ik  RR to denote distributed representation, relevance features for a query and",0,,False
202,"a subtopic respectively. vq , vik : we first retrieve top Z documents using some basic re-",0,,False
203,trieval model (such as BM25). These documents are concatenated,0,,False
204,"as a pseudo document, then similar to edt , a distributed representation of size Eq is generated. For consistency, we also use eq and eik  REq to represent these representations.",0,,False
205,548,0,,False
206,Session 5A: Retrieval Models and Ranking 3,1,Session,True
207,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
208,"4.1.1 Document Sequence Representation. H is instantiated using RNN to encode the information of previous document sequence. Several types of RNN cell can be used, ranging from the simple vanilla cell, GRU cell [9], to LSTM cell [15]. For simplicity, we only show the vanilla cell here. At the t-th position, we derive the (accumulative) document sequence representation as follows:",0,,False
209,"ht ,"" tanh(W n [ht -1; edt ] + bn ),""",0,,False
210,(5),0,,False
211,"where W n  RU ×(U +Ed ) (U is the size of the hidden state), bn  RU and [; ] is a concatenation. The cell transforms previous hidden",0,,False
212,"layer ht -1 and another space,",0,,False
213,current where a,0,,False
214,document distributed bias bn is added and a,1,ad,True
215,representation edt to non-linear activation,0,,False
216,"(i.e. tanh) then happens, producing the next hidden layer ht . h0",0,,False
217,is initialized as a vector of zeros. The vanilla cell can be easily,0,,False
218,"replaced by GRU and LSTM cells, whose results will be report in",0,,False
219,Section 6.2.,0,,False
220,"4.1.2 Subtopic Attention. By looking at ht-1 which stores the information of previous t - 1 documents and ei(·) which represents the meaning of each subtopic, we are capable of discovering which",0,,False
221,"intents are not satisfied and thus need to be emphasized at the tth position. To capture this idea, we use A(ht -1, eik ) to measure the (unnormalized) importance of the k-th subtopic at the t-th po-",0,,False
222,"sition, which could be implemented in many ways. We consider",0,,False
223,the following two ways similar to [21]:,0,,False
224,{,0,,False
225,"A (ht -1, eik ) ,",0,,False
226,"ht -1W aeik , -ht -1 · eik ,",0,,False
227,(general) (dot),0,,False
228,(6),0,,False
229,"where W a  RU ×Eq . The ""general"" operation uses bilinear tensor product to relate two vectors multiplicatively through its nonlinearity [30]. The ""dot"" product requires both vectors to be in the same space. Similar ht-1 and eik mean that previous documents are likely to satisfy this subtopic, and thus a lower attention score will be attributed to it. The above way mainly relies on distributed representations, which may not always be effective, especially under limited data.",0,,False
230,"Hence, we further leverage relevance features to enhance the subtopic attention. xdt,ik directly reflects the degree of satisfaction for a subtopic-document pair and is combined linearly using wp to form an explicit signal. To derive the accumulative information of the document sequence, we adopt commonly used max-pooling to select the most significant signal from previous documents:",1,ad,True
231,"A (xd1,ik , ..., xdt-1,ik ) ,"" max([xd1,ik · wp , ..., xdt-1,ik · wp ]), (7)""",0,,False
232,"where A(xd1,ik , ..., xdt-1,ik ) measures the degree of satisfaction of the k-th subtopic based on relevance features through max-pooling. Lower value indicates that the previous documents are more likely to be relevant to this subtopic. Note that if we view the signals produced by max-pooling (i.e. the vectors in ""max-pooling"" section of Figure 2) as a part of the general hidden states, our concrete implementation fit in DSSA framework.",0,,False
233,We adopt an addictive way to integrate both parts and then use softmax to produce (normalized) attention distribution:,1,ad,True
234,"at,k",0,,False
235,",",0,,False
236,"A (ht -1, eik ) +",0,,False
237,"A (xd1,ik , ..., xdt-1,ik ),",0,,False
238,"at,k , Kjw,""i1kweixj pe(xapt(,kat), j )""",0,,False
239,"(wij  0, j).",0,,False
240,(8),0,,False
241,"softmax is modified to include the initial subtopic importance wik , which encodes our intuition that an important subtopic is more",0,,False
242,likely to gain attention than unimportant ones.,0,,False
243,"4.1.3 Scoring. The final score consists of relevance score sdretl and diversity score sddtiv, which are combined by a coefficient :",0,,False
244,"sdt , (1 - )sdretl + sddtiv (0    1).",0,,False
245,(9),0,,False
246,The relevance score and diversity score are calculated as follows:,0,,False
247,"sdretl ,"" S (edt , eq ) + xdt ,q · wr ,""",0,,False
248,sddtiv,0,,False
249,",",0,,False
250,"at , ( ·)",0,,False
251,·,0,,False
252,"SS((eeddtt,,eeiiK1",0,,False
253,) ),0,,False
254,+ ... +,0,,False
255,"xdt,i1 · wr xdt,iK · wr",0,,False
256,",",0,,False
257,(10),0,,False
258,"where wr  RR and at,(·) is the attention derived from subtopic attention component. The diversity score is calculated as a weighted",0,,False
259,combination of the document's relevance to each subtopic by atten-,0,,False
260,tion distribution. We use the same way to calculate document's re-,0,,False
261,levance to a query and to its subtopics using both distributional re-,0,,False
262,"presentations and relevance features, although different ways can",0,,False
263,"be used. Specifically, dt 's relevance to a query q (or a subtopic ik ) is calculated based on both the similarity between two distributed xrtwedpto,rqerse(eponrretxastdeitno,tniakst)iS.oSn (seidantnt,deenwqd)rs(otliornSpear o(reldydutcc,oeemiakbm)i)naaetnscdhfeirnaeglteuvsrcaeonsr.ceSeibfmeeatiwltauerreetnos A, S could also be implemented as:",0,,False
264,{,0,,False
265,S (edt,0,,False
266,",",0,,False
267,eik,0,,False
268,),0,,False
269,",",0,,False
270,"edt W seik , edt · eik ,",0,,False
271,(general) (dot),0,,False
272,(11),0,,False
273,where W s  REd ×Eq . Then the score of a ranking r is calculated by summing up all the |r | documents' scores:,0,,False
274, |r |,0,,False
275,"sr , sdt .",0,,False
276,(12),0,,False
277,"t ,1",0,,False
278,"Vector interaction operations A and S could be implemented using more complex models, such as multilayer perceptron (MLP), to model the interaction between two vectors more accurately. We could also use convolutional neural network (CNN) instead of RNN to model the interaction among a sequence of documents and encode their information. We deliberately choose to use simple mechanisms in this implementation in order to show that the general framework is capable of capturing the essence of diversification even without complex operations. More complex implementations will be examined in future work.",1,ad,True
279,549,0,,False
280,Session 5A: Retrieval Models and Ranking 3,1,Session,True
281,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
282,Algorithm 1 A List-pairwise Approach For Optimization,0,,False
283,d 3,0,,False
284,d 1,0,,False
285,d 3,0,,False
286,d 2,0,,False
287,d 4,0,,False
288,rank1,0,,False
289,1: procedure List-pairwise Training,0,,False
290,"input: loss function L, learning rate r , epochs V , query set Q,",0,,False
291,dd,0,,False
292,1,0,,False
293,2,0,,False
294,d 4,0,,False
295,d 1,0,,False
296,d 2,0,,False
297,d 4,0,,False
298,d rank2 3,0,,False
299,"document set D, evaluation metric M, random permutation count N output: DSSA with trained parameters  2: initialize ",0,,False
300,(a) list-pairwise,0,,False
301,(b) PAMM,0,,False
302,3: for i from 1 to V do,0,,False
303,4:,0,,False
304,"for batch b  GetSamples(Q, D, M, N ) do",0,,False
305,Figure 3: Pair sample examples of (a) list-pairwise and (b),0,,False
306,5:,0,,False
307,"  GetGradient(L(b,  ))",1,ad,True
308,PAMM. Both samples are positive.,0,,False
309,6:,0,,False
310,   - r,0,,False
311,return DSSA,0,,False
312,4.2 A List-pairwise Approach for Optimization,0,,False
313,"Liu [19] classifies LTR approaches into three categories: pointwise, pairwise, and listwise. Search result diversification is naturally a listwise problem because the score of a document depends on the previous documents. Take Table 1 as an example, under no previous documents, d2 is better than d3 because d2 covers one more subtopic (subtopics are of equal weight). However, given that we have selected d1, which is similar to d2 while dissimilar to d3, d3 becomes superior because it provides additional information.",1,ad,True
314,4.2.1 List-pairwise Training. We propose a list-pairwise training approach. We call it list-pairwise because a sample in our algo-,0,,False
315,7: procedure GetSamples,0,,False
316,"input: query set Q, document set Dq for each query q, evaluation metric M, random permutation count N",0,,False
317,"output: a set of ranking pairs with weight and preference {(q(1), C(1), d1(1), d2(1), w(1), y(1)), (q(2), C(2), d1(2), d2(2), w(2), y(2)), ...} include: GetPerms(Dq, l, N , M) return a best ranking (under metric M) and N random permutations of length l. GetPairs(q, Dq, C, M) samples pairs of documents (d1, d2) from Dq \ C under context C if and only if they lead to different metric scores. Let r1  [C, d1], r2  [C, d2], w , |M(r1) - M(r2)| and y , M(r1) > M(r2) . 8: R  ",1,ad,True
318,"rithm consists of a pair of rankings (r1, r2): r1 and r2 are totally identical except the last document. The sample can be written as (C, d1, d2), where C is the shared previous document sequence.",0,,False
319,"The pairwise preference ground-truth is generated based on an evaluation metric M, such as -nDCG. If M(r1) > M(r2), it is positive,",0,,False
320,9: for query q in Q do,0,,False
321,10:,0,,False
322,for l from 0 to |Dq | - 1 do,0,,False
323,11:,0,,False
324,"for perm C in GetPerms(Dq, l, N , M) do",0,,False
325,12:,0,,False
326,"R  R  GetPairs(q, Dq, C, M)",0,,False
327,return R,0,,False
328,"otherwise it is negative. Our approach is similar to pairwise approaches because it aims to compare a pair of documents, but this is done within some context. Similarly to pairwise, the loss function",0,,False
329,"LPAMM ,",0,,False
330,"P (rq+) - P (rq-)  M(rq+) - M(rq-) ,",0,,False
331,"q  Q rq+,rq-",0,,False
332,(16),0,,False
333,can be defined as binary classification logarithmic loss:,0,,False
334,"Llist-pairwise , ",0,,False
335,w (o ),0,,False
336,q  Q o  Oq,0,,False
337,( y(o),0,,False
338,log,0,,False
339,"( P (r1(o),",0,,False
340,) r2(o)),0,,False
341,+,0,,False
342,(1,0,,False
343,-,0,,False
344,y(o)),0,,False
345,log,0,,False
346,( 1,0,,False
347,-,0,,False
348,")) P (r1(o), r2(o)) ,",0,,False
349,(13),0,,False
350,"where condition is 1 if the condition is satisfied, 0 otherwise, MLE maximizes the probability of positive rankings, and PAMM enlarges the probability margin between positive and negative rankings according to an evaluation metric. For MLE, the number of best rankings is usually small if we only have hundreds of que-",0,,False
351,"where Oq is all the pair samples of query q, y(o) ,"" 1 indicates positive and 0 for negative, and P(r1(o), r2(o)) is the probability of being""",0,,False
352,"ries, which may not be enough to train adequately the parameters. PAMM uses preferences between very different rankings that are not comparable (see Figure 3(b)). In contrast, list-pairwise method",1,ad,True
353,positive,0,,False
354,calculated,0,,False
355,by,0,,False
356,1 1+exp(sr2(o),0,,False
357,-sr1(o,0,,False
358,),0,,False
359,),0,,False
360,.,0,,False
361,To,0,,False
362,enhance,0,,False
363,"effectiveness,",0,,False
364,we,0,,False
365,weight,0,,False
366,pairs,0,,False
367,with,0,,False
368,w (o ),0,,False
369,",",0,,False
370,|M,0,,False
371,(r,0,,False
372,(o) 1,0,,False
373,),0,,False
374,-,0,,False
375,M,0,,False
376,"(r2(o))|,",0,,False
377,which,0,,False
378,means,0,,False
379,that,0,,False
380,only allows the last document to be different (Figure 3(a)). This corresponds better to the decision-making situation in which we have to choose a document under a given context. It is expected that,0,,False
381,"the bigger the metric score gap, the more important the pair.",0,,False
382,such a pair sample allows us to better train the ranking function.,0,,False
383,Because DSSA calculates document d's score sdC based on previ-,0,,False
384,Experiments will show that our approach works better.,0,,False
385,"ous document C, we could also use Maximum Likelihood Estima-",0,,False
386,"As shown in Figure 2, our architecture is a unified neural net-",0,,False
387,tion (MLE) or PAMM to optimize our model. We use Plackett-Luce,0,,False
388,"work and the attention function is continuous, so the gradient of",1,ad,True
389,model [22] to estimate the probability of a ranking r :,0,,False
390,the loss function can be backpropagated directly to train the model.,0,,False
391,P(r ),0,,False
392,",",0,,False
393, |r |,0,,False
394,"i ,1",0,,False
395,exp(sdr i[:i-1]),0,,False
396,|r |,0,,False
397,"j ,i",0,,False
398,exp(sdr [j:i-1],0,,False
399,),0,,False
400,",",0,,False
401,(14),0,,False
402,where r [: i - 1] means the top i - 1 documents of ranking r . Then,0,,False
403,the loss functions could be w ritten as:,0,,False
404,"LMLE ,"" - log(P (rq+)),""",1,LM,True
405,(15),0,,False
406,qQ,0,,False
407,"We use mini-batch gradient descent to facilitate training process. Unfortunately, it is impossible to acquire all the list-pairwise",1,ad,True
408,"samples, which has in total |Dq |! (|Dq | is the number of candidate documents) different permutations. So we develop a sampling strategy similar to negative sampling [23] as described in Algorithm 1: for each query q, we sample a large number of pairs of rankings, whose length ranges from 1 to |Dq |. We first obtain some contexts C from both best rankings and randomly sampled negative",0,,False
409,550,0,,False
410,Session 5A: Retrieval Models and Ranking 3,1,Session,True
411,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
412,"rankings (rankings that are not optimal). Then under each C, a pair of documents (d1, d2) are sampled from the remaining documents Dq \ C if and only if they lead to different metric scores.",1,ad,True
413,"4.2.2 Prediction. In prediction stage, for each query, we sequentially and greedily choose the document with the highest score and append it to the ranking list. Specifically, the first document is selected under initial subtopic importance from the whole candidate set Dq . Once the top t - 1 documents have been selected (i.e. |C| ,"" t - 1), we feed each document in Dq \ C into DSSA at the t-th position one by one and choose the one with the highest sdt . This process continues until all the documents in Dq are ranked.""",0,,False
414,"4.2.3 Time Complexities. The training time complexity with vanilla cell and ""general"" operation is O(V · |Q| ·  · |Dq | · ) where V is the number of iterations, |Q| is the number of training queries,  ,"" N · |Dq |2 is the number of sampled pairs where N is the number of random permutations, |Dq | is the number of candidate documents, and  is the complexity for each position:""",0,,False
415," ,"" U (U + Ed ) + KU Eq + KR + KEd Eq + KR, (17)""",0,,False
416,document sequence representation,0,,False
417,subtopic attention,0,,False
418,scoring,0,,False
419,where the dominating terms are KU Eq and KEd Eq which are proportional to the number of subtopics K. How to efficiently handle,0,,False
420,"a large number of subtopics is our future work. The prediction complexity is O(|Dq |2) for each query. We can limit |Dq | to a small number (say 50), so the prediction time can be reasonable.",0,,False
421,5 EXPERIMENTAL SETTINGS,0,,False
422,5.1 Data Collections,0,,False
423,"We use the same dataset as [16] which consists of Web Track dataset from TREC 2009 to 2012. There are 198 queries (query #95 and #100 are dropped because no diversity judgments are made for them), each of which includes 3 to 8 subtopics identified by TREC assessors. The relevance rating is given in a binary form at subtopic level. All experiments are conducted on ClueWeb09 [5] collection.",1,Track,True
424,"We use query suggestions of Google search engine as subtopics, which are released by Hu et al. [16] on their website1. For DSSA, we only use the first level subtopics and leave the exploration of hierarchical subtopics to future work. Following the existing work [16], we simply use uniform weights for these subtopics.",0,,False
425,5.2 Evaluation Metrics,0,,False
426,"We use ERR-IA [8], -nDCG [10], and NRBP [11], which are official diversity evaluation metrics used in Web Track. They measure the diversity by explicitly rewarding novelty and penalizing redundancy. D-measures [26], the primary metric used in NTCIR Intent [25] and IMine task [20], is also included. In addition, we also use traditional diversity measures Precision-IA (denoted as Pre-IA) [1] and Subtopic Recall (denoted as S-rec) [37]. Consistent with existing works [32, 33, 39] and TREC Web Track, all these metrics are computed on top 20 results of a ranking. We use two-tailed paired t-test to conduct significance testing with p-value < 0.05.",1,Track,True
427,1hierarchical search result diversification: http://www.playbigdata.com/dou/hdiv,0,,False
428,"Table 5: Relevance features. Each of the first 3 features is applied to body, anchor, title, URL, and the whole documents.",0,,False
429,Name,0,,False
430,TF-IDF BM25 LMIR,1,LM,True
431,PageRank #inlinks #outlinks,0,,False
432,Description,0,,False
433,the TF-IDF model BM25 with default parameters LMIR with Dirichlet smoothing,1,LM,True
434,PageRank score number of inlinks number of outlinks,0,,False
435,#Features,0,,False
436,5 5 5,0,,False
437,1 1 1,0,,False
438,Table 6: Diversity features. Each feature is extracted over a pair of documents.,0,,False
439,Name,0,,False
440,subtopic diversity text diversity title diversity anchor text diversity link-based diversity URL-based diversity,0,,False
441,Description,0,,False
442,euclidean distance based on SVD cosine-based distance on term vector text diversity on title text diversity on anchor link similarity of document pair URL similarity of document pair,0,,False
443,5.3 Baseline Models,0,,False
444,"We compare DSSA2 to various unsupervised and supervised diversification methods. The non-diversified baseline is denoted as Lemur. We use xQuAD [27], PM2 [13], TxQuAD, TPM2 [12], HxQuAD, and HPM2 [16] as our unsupervised baselines. We use ListMLE [31], R-LTR [39], PAMM [32], and NTN [33] as our supervised baselines. Top 20 results of Lemur are used to train supervised methods. Top 50 (i.e. |Dq |) results of Lemur are used for diversity re-ranking. To construct the representation of a query or a subtopic, we use the top 20 (Z ) documents. We use 5-fold cross validation to tune the parameters in all experiments based on nDCG@20, which is one of the most widely used metrics. A brief introduction to these baselines is as follows:",1,HP,True
445,Lemur. We use the same non-diversified results as [16] for fair comparison. They are produced by language model and retrieved using the Lemur service3 of which the spams are filtered. These results are released by Hu et al. [16] on the website1.,0,,False
446,ListMLE. ListMLE is a representative listwise LTR method without considering diversity.,0,,False
447,"xQuAD, PM2, TxQuAD, TPM2, HxQuAD, and HPM2. These are competitive unsupervised explicit diversification methods, as introduced in Section 2.2. All these methods use  to control the importance of relevance and diversity. HxQuAD and HPM2 use an additional parameter  to control the weight of each layer of the hierarchical structure. Both  and  are tuned using cross validation. They all require a prior relevance function to fulfill diversification re-ranking. Following [39], we use ListMLE.",1,HP,True
448,"R-LTR, PAMM, and NTN. For PAMM, we use -nDCG@20 as the optimization metric. We optimize NTN based on both R-LTR and PAMM, denoted as R-LTR-NTN and PAMM-NTN respectively.",0,,False
449,2data and code available at: http://www.playbigdata.com/dou/DSSA/ 3Lemur service: http://boston.lti.cs.cmu.edu/Services/clueweb09 batch/,0,,False
450,551,0,,False
451,Session 5A: Retrieval Models and Ranking 3,1,Session,True
452,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
453,"To achieve optimal results, for R-LTR and PAMM, we tune the relational function hS (R) from minimal, maximal, and average. For PAMM, we tune the number of positive rankings  + and negative rankings  - per query. For NTN, the number of tensor slices is tuned from 1 to 10. LDA is used to generate distributed representations of size 100 for NTN and DSSA. For all these supervised methods, the learning rate r is tuned from 10-7 to 10-1. For DSSA, we have different settings possible. In our first set of results, we will use ""general"" as the implementation of vector interaction operations A and S, LSTM with hidden size of 50 as the cell of RNN. We set random permutation count as 10 in list-pairwise sampling. Similarly,  of DSSA is tuned by cross validation. We also test the impact of different model settings and permutation counts on performance in Section 6.2 and Section 6.3 respectively.",0,,False
454,"Similar to [39], we implement 18 relevance features and 6 diversity features, as listed in Table 5 and 6 respectively. We collect the candidate and retrieved documents of all queries and subtopics to generate the distributed representations.",0,,False
455,6 EXPERIMENTAL RESULTS,0,,False
456,6.1 Overall Results,0,,False
457,"The overall results are shown in Table 7. We find that DSSA significantly outperforms all implicit and explicit baselines, including both unsupervised and supervised. The improvements are statistically significant (two-tailed paired t-test) for all metrics, except S-rec. The results clearly show the superiority of DSSA.",0,,False
458,"(1) DSSA vs. unsupervised explicit methods. DSSA outperforms unsupervised explicit methods (xQuAD, PM2, TxQuAD, TPM2, HxQuAD, and HPM2) on all the measures. The relative improvement over HxQuAD and HPM2, the best unsupervised explicit approaches, is up to 8.3% and 8.6% respectively in terms of -nDCG. This comparison shows the great advantage of using supervised method for learning the ranking function.",1,HP,True
459,"(2) DSSA vs. supervised implicit methods. DSSA also outperforms supervised implicit methods (R-LTR, PAMM, R-LTRNTN, and PAMM-NTN) by quite large margins. The improvement over R-LTR-NTN and PAMM-NTN, the best supervised implicit approaches is up to 9.9% and 9.4% respectively on -nDCG. This result demonstrates the utility of taking into account subtopics explicitly in supervised approaches. The improvements are similar to those observed between explicit approaches and implicit approaches in unsupervised framework [12, 13, 16, 27]. The combination of the two observations suggests that explicit modeling of subtopics can improve result diversification, whether it is in a supervised or unsupervised framework.",0,,False
460,6.2 Effects of Different Settings,0,,False
461,"We conduct experiments with different settings of DSSA to investigate whether the performance is sensitive to these settings. Different aspects of settings are listed follow. For simplicity, when investigating the impact of each aspect, we keep other aspects the same as the settings specified in Section 5.3.",0,,False
462,"(1) Representation generation methods: SVD, LDA, and doc2vec with window size of 5.",0,,False
463,"(2) Implementation of vector interaction operations A and S: ""general"" and ""dot"".",0,,False
464,Table 7: Performance comparison of all methods. The best result is in bold. Statistically significant differences between DSSA and baselines are marked with various symbols. # indicates significant improvement over all baselines.,0,,False
465,Methods,0,,False
466,ERR-IA -nDCG NRBP D-nDCG Pre-IA S-rec,0,,False
467,Lemur ListMLE,0,,False
468,.271 .369 .287 .387,0,,False
469,.232 .424 .249 .430,0,,False
470,.153 .621 .157 .619,0,,False
471,xQuAD .317 .413,0,,False
472,TxQuAD .308 .410,0,,False
473,HxQuAD .326 .421,0,,False
474,PM2,0,,False
475,.306 .411,0,,False
476,TPM2,0,,False
477,.291 .399,0,,False
478,HPM2,1,HP,True
479,.317 .420,0,,False
480,.284 .437 .272 .441 .294 .441 .267 .450 .250 .443 .279 .455,0,,False
481,.161 .622 .155 .634 .158 .629 .169 .643 .161 .639 .172 .645,0,,False
482,R-LTR,0,,False
483,.303,0,,False
484,PAMM,0,,False
485,.309,0,,False
486,R-LTR-NTN .312,0,,False
487,PAMM-NTN .311,0,,False
488,DSSA,0,,False
489,.356#,0,,False
490,.403 .411 .415 .417 .456#,0,,False
491,.267 .441 .271 .450 .275 .451 .272 .457 .326# .473#,0,,False
492,.164 .631 .168 .643 .166 .644 .170 .648 .185# .649,0,,False
493,Table 8: Effects of different settings.,0,,False
494,Methods,0,,False
495,ERR-IA -nDCG NRBP D-nDCG Pre-IA S-rec,0,,False
496,SVD LDA doc2vec,0,,False
497,.348 .450 .315 .356 .456 .326 .351 .452 .318,0,,False
498,.470 .184 .646 .473 .185 .649 .471 .184 .646,0,,False
499,general dot,0,,False
500,.356 .456 .326 .347 .450 .314,0,,False
501,.473 .185 .649 .470 .184 .647,0,,False
502,vanilla GRU LSTM,0,,False
503,.354 .454 .322 .357 .457 .326 .356 .456 .326,0,,False
504,.471 .184 .649 .473 .185 .649 .473 .185 .649,0,,False
505,DSSA-RNN,0,,False
506,.342,0,,False
507,DSSA-RNNMP .356,0,,False
508,.445 .306 .456 .326,0,,False
509,.466 .172 .657 .473 .185 .649,0,,False
510,"(3) RNN cell: vanilla, GRU, and LSTM cell. (4) Dimensionality: we test several representative settings on",0,,False
511,"the size of distributed representations Ed and Eq , the size of hidden state U as (25, 10), (50, 25), (100, 50), (200, 100). (5) Max-pooling: we experiment without using max-pooling (denoted as DSSA-RNN) in subtopic attention component.",0,,False
512,"The results are reported in Table 8. We can observe that DSSA does not heavily rely on specific settings. As for different representation generation methods, LDA has slightly better results. doc2vec could have been more appropriate if we had large datasets with more queries. The ""general"" operation yields slightly better results. A possible reason is that it is bilinear and thus is more powerful than ""dot"" to model the interaction. GRU and LSTM cells yield slightly better results than vanilla cell because of their ability of modeling long-term dependency. The difference is however small. This may be due to that with a limited number of training data, a model is unable to take advantage of its higher complexity to capture the fine-grained subtlety. Results with different size of distributed representation and hidden state shown in Figure 4(a) also",1,ad,True
513,552,0,,False
514,Session 5A: Retrieval Models and Ranking 3,1,Session,True
515,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
516,-nDCG -nDCG,0,,False
517,0.47,0,,False
518,0.47,0,,False
519,0.46,0,,False
520,0.46,0,,False
521,0.45,0,,False
522,0.45,0,,False
523,0.44,0,,False
524,0.44,0,,False
525,"0.43 (25,10) (50,25) (100,50) (200,100)",0,,False
526,dimensionality,0,,False
527,0.43 0,0,,False
528,5,0,,False
529,10 15 20,0,,False
530,#random permutation,0,,False
531,(a)  -nDCG w.r.t. different size (b)  -nDCG w.r.t. different random permutation count,0,,False
532,Figure 4: Performance tendency of different settings.,0,,False
533,DSSA i1 i2 i3 i4 i5 d1 d2 d3 d4 d5,0,,False
534,PAMM-NTN i1 i2 i3 i4 i5,0,,False
535,DSSA,0,,False
536,PAMM-NTN,0,,False
537,i1 i2 i3 i4 i1 i2 i3 i4,0,,False
538,(a) ranking of query #58,0,,False
539,(b) ranking of query #182,0,,False
540,Figure 5: Case study for DSSA and PAMM-NTN. White means relevant and black means irrelevant.,0,,False
541,Table 9: Effects of different optimization methods.,0,,False
542,Methods ERR-IA -nDCG NRBP D-nDCG Pre-IA S-rec,0,,False
543,MLE,0,,False
544,.349,0,,False
545,PAMM,0,,False
546,.348,0,,False
547,list-pairwise .356,0,,False
548,.446 .315 .445 .315 .456 .326,0,,False
549,.462 .176 .644 .463 .175 .644 .473 .185 .649,0,,False
550,"indicate no strong correlation between performance and settings. -nDCG remains above 0.45 using different sizes. The best performance is achieved using 100-dimensional representation and 50dimensional hidden state. This suggests that high dimensionality may result in overfitting. Without using max-pooling, -nDCG drops to 0.445, which demonstrates the usefulness of using maxpooling to enhance subtopic attention. The small differences between different settings suggest that DSSA is a stable and robust framework. Note that we use both distributed representations and relevance features, which are complementary to each other. This may be one of the reasons of the stability.",0,,False
551,6.3 Effects of Different Optimization Methods,0,,False
552,"Results in Table 9 shows that list-pairwise is more effective than MLE and PAMM. This confirms our earlier intuition that list-pairwise optimization corresponds better to the situation of diversification ranking than the two other methods. Note that even using MLE or PAMM as optimization methods, DSSA could also achieve stateof-the-art performances, which confirms the effectiveness of our explicit learning framework from another perspective.",0,,False
553,"We vary the number of random permutations used in list-pairwise sampling from 0 to 20 to investigate its effect. As depicted in Figure 4(b), the performance does not heavily rely it. The best performance is achieved around 10. More permutations lead to lower effectiveness, which could be explained by model overfitting.",1,ad,True
554,6.4 Visualization and Discussion,0,,False
555,We visualize the ranking results of DSSA and the variation of subtopic attention to better understand why DSSA performs well.,0,,False
556,"We show the top 5 ranking results of query #58 and #182 in Figure 5 to illustrate why DSSA outperforms implicit learning methods. We choose PAMM-NTN as comparison method, which is the best existing learning method. In Figure 5, white means relevant and black means irrelevant. For query #58, DSSA ranks a document relevant to subtopics i3 and i4 first and a document relevant to i1 and i2 at the second position, while the first two documents of PAMM-NTN cover the same subtopics. Note that there is no",0,,False
557,d1 d2 d3 d4 d5,0,,False
558,z1. quit smoking tips (i1),0,,False
559,z2. quit smoking app (i1),0,,False
560,z3. quit smoking calculator (i1) subtopics from Google z4. quit smoking help (i1),0,,False
561,z5. quit smoking benefits (i2),0,,False
562,z6. quit smoking cold turkey (i3),0,,False
563,official subtopics,0,,False
564,z7. quit smoking hypnosis (i4) i1. What are the ways you can quit smoking? i2. What are the benefits of quitting smoking? i3. Can you quit smoking using the cold turkey method? i4. How can hypnosis help someone quit smoking?,0,,False
565,Figure 6: Subtopic attention variation of query #182. The top part is attention and the bottom part is relevance judgment.,0,,False
566,"document covering i5 in the candidate set. For query #182, DSSA successively chooses documents that cover i1, i3, i2, and i4. One additional intent is satisfied at every position. PAMM-NTN, however, just covers i1 and i2 by top 5 documents, which is obviously not optimal. We see that the unequal and varied subtopic attention is capable of discovering unsatisfied subtopics at different positions, which eventually leads to more subtopic coverage.",1,ad,True
567,"To study attention mechanism, we further visualize the variation of subtopic attention of top 5 documents of query #182, namely ""quit smoking"", which has 4 official subtopics (i1 to i4), as shown in Figure 6. The top part is subtopic attention variation and the bottom part is relevance judgment. For attention part, the darker the cell is, the lower the attention (weight) on this subtopic is. Note that we actually leverage query suggestions of Google (z1 to z7) to serve as subtopics, which do not match official ones exactly. We manually align subtopics mined from Google to official ones. At the beginning, all the subtopics have equal attention. The first selected document d1 is relevant to i1, i.e. to the Goggle subtopics z1, z2, z3 and z4. We see that the attention to these latter decreases at second position. Then the document d2 is selected, which is relevant to uncovered i3. We see that the attention to the corresponding z6 begins to diminish from the third position. d3 and d4 satisfy additional i2 and i4 respectively, which leads to the reduction",1,ad,True
568,553,0,,False
569,Session 5A: Retrieval Models and Ranking 3,1,Session,True
570,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
571,"of attention on z5 and z7 at the following position. The subtopic attention, initialized as uniform distribution, ends up with more emphasis on z4, z6, and z7. This example illustrates how the unequal and varied attention drives the model to emphasize different subtopics at different positions, which is crucial in explicit diversification. This example also shows a potential problem inherent for any method using automatically discovered subtopics: those topics may be different from the ones defined by human assessors. Equal distribution is assumed on all the subtopics zi . However, this implies an unequal distribution among the manually defined subtopics (more emphasis is put on i1). Assuming an equal distribution at the beginning may not necessarily be the best approach. We will deal with this problem in our future work.",0,,False
572,7 CONCLUSIONS,0,,False
573,"In this paper, we propose a general learning framework DSSA to model subtopics explicitly for search result diversification. Based on the sequence of selected documents, unequal and varied subtopic attention is calculated, driving the model to emphasize different subtopics at different positions. This is the first time that attention mechanism is used to model the process. We further instantiate DSSA using RNN and max-pooling to handle both distributed representations and relevance features, which outperforms significantly the existing approaches. The results confirm that modeling subtopics explicitly in a learning framework is beneficial and effective and this also avoids heuristically defined functions and parameters. However, accurately modeling the interaction among documents and subtopics is still challenging. There are many other more complex implementations besides our particular way, which will be investigated in future work. The proposed model contains a number of parameters to be learned. This requires a large number of training data. Collecting more training data to fully unlock the potential of the model is another direction. Finally, this work only deals with the learning of a ranking function, assuming that document and query representations have already been created. In practice, learning these representation is another interesting aspect, which could be incorporated into our framework, provided with sufficient training data.",1,ad,True
574,ACKNOWLEDGMENTS,0,,False
575,"Zhicheng Dou is the corresponding author. This work was funded by the National Natural Science Foundation of China under Grant No. 61502501 and 61502502, the National Key Basic Research Program (973 Program) of China under Grant No. 2014CB340403, and the Beijing Natural Science Foundation under Grant No. 4162032.",0,,False
576,REFERENCES,0,,False
577,"[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying Search Results. In WSDM.",0,,False
578,"[2] Ricardo A. Baeza-Yates, Carlos A. Hurtado, and Marcelo Mendoza. 2004. Query Recommendation Using Query Logs in Search Engines. In Current Trends in Database Technology EDBT 2004 Workshops.",1,ad,True
579,"[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. CoRR abs/1409.0473 (2014).",0,,False
580,"[4] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research 3 (2003).",0,,False
581,"[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set. http://boston.lti.cs.cmu.edu/Data/clueweb09/. (2009).",1,Clue,True
582,"[6] Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries. In SIGIR.",1,ad,True
583,[7] Ben Carterette. 2009. An Analysis of NP-Completeness in Novelty and Diversity Ranking. In ICTIR.,1,NP,True
584,"[8] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In CIKM.",1,ad,True
585,"[9] Junyoung Chung, C¸aglar Gu¨l¸cehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. CoRR abs/1412.3555 (2014).",0,,False
586,"[10] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ttcher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In SIGIR.",1,Novelty,True
587,"[11] Charles L. A. Clarke, Maheedhar Kolla, and Olga Vechtomova. 2009. An Effectiveness Measure for Ambiguous and Underspecified Queries. In ICTIR.",0,,False
588,[12] Van Dang and Bruce W. Croft. 2013. Term Level Search Result Diversification. In SIGIR.,0,,False
589,[13] Van Dang and W. Bruce Croft. 2012. Diversity by Proportionality: An Electionbased Approach to Search Result Diversification. In SIGIR.,0,,False
590,"[14] Amac Herdagdelen, Massimiliano Ciaramita, Daniel Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler, and Enrique Alfonseca. 2010. Generalized Syntactic and Semantic Models of Query Reformulation. In SIGIR.",1,Query,True
591,"[15] Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997).",0,,False
592,"[16] Sha Hu, Zhicheng Dou, Xiaojie Wang, Tetsuya Sakai, and Ji-Rong Wen. 2015. Search Result Diversification Based on Hierarchical Intents. In CIKM.",0,,False
593,"[17] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry P. Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In CIKM.",0,,False
594,[18] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In ICML.,0,,False
595,"[19] Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval 3, 3 (2009).",0,,False
596,"[20] Yiqun Liu, Ruihua Song, Min Zhang, Zhicheng Dou, Takehiro Yamamoto, Makoto P Kato, Hiroaki Ohshima, and Ke Zhou. 2014. Overview of the NTCIR-11 IMine Task.. In NTCIR. Citeseer.",0,,False
597,"[21] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention-based Neural Machine Translation. In EMNLP.",0,,False
598,"[22] John I Marden. 1996. Analyzing and modeling rank data. CRC Press. [23] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.",1,ad,True
599,"2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS. [24] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. 2014. Recurrent Models of Visual Attention. In NIPS. [25] Tetsuya Sakai, Zhicheng Dou, Takehiro Yamamoto, Yiqun Liu, Min Zhang, Ruihua Song, MP Kato, and M Iwata. 2013. Overview of the NTCIR-10 INTENT-2 Task.. In NTCIR. [26] Tetsuya Sakai and Ruihua Song. 2011. Evaluating Diversified Search Results Using Per-intent Graded Relevance. In SIGIR. [27] Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting Query Reformulations for Web Search Result Diversification. In WWW. [28] Rodrygo L.T. Santos, Craig Macdonald, Iadh Ounis, and others. 2015. Search result diversification. Foundations and Trends® in Information Retrieval (2015). [29] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In SIGIR. [30] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning With Neural Tensor Networks for Knowledge Base Completion. In NIPS. [31] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise Approach to Learning to Rank: Theory and Algorithm. In ICML. [32] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In SIGIR. [33] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling Document Novelty with Neural Tensor Network for Search Result Diversification. In SIGIR. [34] Jeonghee Yi and Farzin Maghoul. 2009. Query Clustering Using Click-through Graph. In WWW. [35] Hai-Tao Yu and Fuji Ren. 2014. Search Result Diversification via Filling Up Multiple Knapsacks. In CIKM. [36] Yisong Yue and Thorsten Joachims. 2008. Predicting Diverse Subsets Using Structural SVMs. In ICML. [37] Cheng Xiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In SIGIR. [38] Zhiyong Zhang and Olfa Nasraoui. 2006. Mining Search Engine Query Logs for Query Recommendation. In WWW. [39] Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Shuzi Niu. 2014. Learning for Search Result Diversification. In SIGIR.",1,INTENT,True
600,554,0,,False
601,,0,,False

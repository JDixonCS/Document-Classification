,sentence,label,data,regex
0,Demonstration Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,Visual Pool: A Tool to Visualize and Interact with the Pooling Method,0,,False
3,Aldo Lipani,0,,False
4,"TU Wien Institute of So ware Technology & Interactive Systems, Vienna, Austria",0,,False
5,aldo.lipani@tuwien.ac.at,0,,False
6,Mihai Lupu,0,,False
7,"TU Wien Institute of So ware Technology & Interactive Systems, Vienna, Austria",0,,False
8,mihai.lupu@tuwien.ac.at,0,,False
9,Allan Hanbury,0,,False
10,"TU Wien Institute of So ware Technology & Interactive Systems, Vienna, Austria",0,,False
11,allan.hanbury@tuwien.ac.at,0,,False
12,ABSTRACT,0,,False
13,"Every year more than 25 test collections are built among the main Information Retrieval (IR) evaluation campaigns. ey are extremely important in IR because they become the evaluation praxis for the forthcoming years. Test collections are built mostly using the pooling method. e main advantage of this method is that it drastically reduces the number of documents to be judged. It does so at the cost of introducing biases, which are sometimes aggravated by non optimal con guration. In this paper we develop a novel visualization technique for the pooling method, and integrate it in a demo application named Visual Pool. is demo application enables the user to interact with the pooling method with ease, and develops visual hints in order to analyze existing test collections, and build be er ones.",1,ad,True
14,CCS CONCEPTS,0,,False
15,·Information systems  Test collections; Relevance assessment; ·Human-centered computing  Visualization techniques;,0,,False
16,KEYWORDS,0,,False
17,Visualization; Test Collections; Pooling Method; Pooling Strategies,0,,False
18,1 INTRODUCTION,1,DUC,True
19,"Test collection based evaluation in IR is a cornerstone of the IR experimentation. Most o en, test collections are built using the pooling method. is method refers to a sampling procedure, according to a given strategy, of documents to be judged. is demo aims to visualize this procedure, allowing the user deeper insights.",0,,False
20,"A test collection is composed of a collection of documents, a set of topics, and a set of relevance judgements. A relevance judgment (or qrel) expresses the relevance of a document for a given topic. Due to the size of the modern collection of documents, to produce a complete set of relevance judgements is impossible. For example, if we examine what today would be considered a small test collection, with 500,000 documents and 50 topics (approximately the size of the TREC Ad Hoc 8 test collection [16]), the total relevance judgments",1,TREC,True
21,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3084146",1,ad,True
22,"to be made would be 25 × 106. At an optimistic rate of 120 seconds per judgment, this represents the equivalent of around 400 years of work for one person [4]. To solve this problem, early in the modern IR history, a sampling method was developed, the pooling method [14].",1,ad,True
23,"e pooling method consists in building a test collection by using the results provided by a set of search engines. ese are usually systems designed by participants of challenges organized by IR evaluation campaigns such as: TREC, CLEF, NTCIR, or FIRE. In these challenges, every participant is provided a collection of documents and a set of topics. eir task is to develop a search engine to produce a result that maximizes the goal de ned by the challenge. is result is then sent to the organizers, who now have everything they need to implement a pooling strategy.",1,TREC,True
24,"e most common pooling strategy is the Depth@K strategy. is consists of creating a pool by selecting the top K documents from the results submi ed by each system of each participant. e pool is given to the relevance assessors, who will produce a set of relevance assessments, which are then used in combination with an IR evaluation measure to rank the performance of the systems of the participants. ese test collections are then used later by researchers to evaluate their systems. However, when comparing a new system with the search engines that participated in the challenge, the pooled systems have an advantage given by the guarantee that at least their top K documents have been judged, while for the new system this guarantee does not exist. is e ect goes under the name of pool bias, which manifests itself when the evaluated system retrieves documents that will never be considered relevant [5] because they had never been seen by the human assessors.",1,ad,True
25,"is bias can be mitigated by increasing: 1) the depth of the pool, which decreases the probability of retrieving a non-judged document; 2) the number of topics, which reduces the variability of the bias making it easier to correct; and, 3) the number (assumed to be proportional to the variety) of the submi ed results by the participants, which leads to a be er exploration of the information space. However, all of these solutions result in a mere increase of the number of documents to be judged and therefore in an increase of the cost of the test collection. e research in the IR community to reduce the pool bias has branched out into two directions: (a) identifying a pooling strategy and a set of parameters that manifests a lower bias, and (b) estimating the bias to correct the score obtained by the search engine. e former direction has lead to the development of new pooling strategies [7, 10, 11], the la er instead to the development of new pool bias estimators [6, 8, 9]. Moreover, a hybrid approach has been also explored developing",1,ad,True
26,1321,0,,False
27,Demonstration Paper,0,,False
28,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
29,"IR evaluation measures in combination with pooling strategies in order to minimize the pool bias [15, 17].",0,,False
30,"In this paper we present a demo that enables the user to visualize and interact with the pooling method. is demo addresses the needs of four classes of users: test collection builders, researchers, lecturers, and students. is solution aims to, by exploiting the users' sight, develop visual cues to guide the development of more sophisticated analyses. is solution is open source (MIT licensed) and is available on the website of the rst author.",1,ad,True
31,"e reminder of the paper goes as follows. We rst present our solution in Section 2. en we present the three use cases in Section 3. In Section 4 we present the technology used. Finally, we discuss and conclude in Section 5.",0,,False
32,2 VISUAL POOL,0,,False
33,"Visual Pool gives its users a new perspective over the pooling method, integrating a novel information visualization technique.",0,,False
34,"is section is divided into three parts: we rst present our pooling visualization technique, then we explain how this is integrated into the demo application, and we conclude listing the features of the demo. e authors have not found any solution that addresses a similar issue, which makes this solution unique in its kind.",1,ad,True
35,"In Figure 1 we see an example of the pool visualization technique. In this case we have applied a Depth@K pooling strategy. On the le , the run view highlights how the documents are distributed among the runs. On the center, the unique documents runs view",0,,False
36,r,0,,False
37,ab cde,0,,False
38,112345 263789 3 10 11 12 1 13 4 3 14 4 15 16 5 14 17 8 18 11  6 19 20 14 6 21 7 7 22 23 24 3 8 25 8 26 20 14 9 11 27 15 28 2 10 29 6 30 21 4,0,,False
39,r,0,,False
40,ab cde,0,,False
41,12345,0,,False
42,6,0,,False
43,789,0,,False
44,10 11 12,0,,False
45,13,0,,False
46,14,0,,False
47,15 16,0,,False
48,17,0,,False
49,18,0,,False
50,19 20,0,,False
51,21,0,,False
52,22 23 24,0,,False
53,25,0,,False
54,26,0,,False
55,27,0,,False
56,28,0,,False
57,29,0,,False
58,30,0,,False
59,f 12 345,0,,False
60,12345 6789 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30,0,,False
61,"Figure 1: Example of three types of visualization of the same pool a er the application of the pooling strategy Depth@5. Every square represents a document identi ed by its unique id. e documents' color means, if gray that is not judged, if green that is relevant, and if red that is not relevant. e y-axis is always the rank  at which a document has been retrieved. On the le the run view, where on the x-axis we nd the list of runs. On the center the unique documents runs view, where, w.r.t. to the previous view, all the duplicate documents have been removed starting from the top le corner. On the right the pool view, where, w.r.t. to the previous view, the documents have been pushed to the le , and the x-axis is now instead the frequency of unique documents at rank .",1,ad,True
62,"where all the duplicated documents retrieved at a lower rank are removed. On the right, the pool view shows the distribution of unique documents at varying of the rank.",0,,False
63,In Figure 2 we show a screen-shot of the Visual Pool application. In this user interface we identify the following sections (the numbers correspond to those in the gure):,0,,False
64,"(1) Pooling Strategy Selection and Con guration. We can select the pooling strategy among the 22 implemented. Every pooling strategy is con gurable, if needed.",0,,False
65,"(2) Visualization Control. We can select which topic to visualize, and we can control which pool visualization view to display: run, unique documents runs, or pool.",0,,False
66,"(3) Pool Strategy Control. We can control the progress of the pooling strategy. We can here decide if to step the pooling strategy forward by one document or till the end, for the current topic, or for all the topics.",0,,False
67,(4) Visualization. We visualize the pool using the previously described visualization technique.,0,,False
68,(5) Analytics. We have a set of analytics that show statistics about the pool and display the current status of the pooling strategy.,0,,False
69,"(6) Log. e log of the pooling strategy is displayed, where we show the status of the processed documents.",0,,False
70,"(7) Run/QRels upload. We can upload the set of runs to be analyzed. It is possible also to indicate at which size to cut the runs. When an existing test collection is to be analyzed, we can also upload the set of relevance assessments, which will be used to visualize the process of assessment.",1,ad,True
71,"(8) QRels download. We can download the current qrels le, e.g. the current set of relevance assessments as generated by the pooling strategy.",1,ad,True
72,"In summary, here we list all the features implemented in the version of the demo presented at SIGIR:",0,,False
73,"· Load runs les in TREC format with a given size; · Load a qrels le in TREC format; · Select a pooling strategy and con gure its parameters; · Select which topic to visualize; · Control the progress of the pooling strategy; · Visualize the pool in three views: runs, unique documents",1,ad,True
74,"runs, or pool; · Visualize the log of the progress of the pooling strategy; · Visualize the statistics about the pool and the status of the",0,,False
75,pooling strategy. · Save the progress of the pooling strategy as a qrels le in,0,,False
76,TREC format; · If required by the pooling strategy ask the user to judge a,1,TREC,True
77,document; · O er API for controlling the pooling strategy in order to,1,AP,True
78,perform the judgment with an external application.,0,,False
79,In Table 1 are listed all the pooling strategies already implemented in the demo.,1,ad,True
80,3 USE CASES,0,,False
81,In this section we present three use cases that cover the main user needs expressed by the four classes of users we aim to address. e,1,ad,True
82,rst use case is about the visualization of an existing test collection.,0,,False
83,1322,0,,False
84,Demonstration Paper,0,,False
85,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
86,"Figure 2: Screen-shot of the Visual Pool application taken a er having: uploaded the runs with run size 100, uploaded the qrels, and executed the evaluation procedure as dictated by the selected pooling strategy Depth@10. In addition to the colors' meanings presented in Fig. 1, the color black indicates a document that has been pooled but it is not contained in the provided qrels.",1,ad,True
87,Pooling Strategy,0,,False
88,Depth@K,0,,False
89,[14] RRFTake@N,0,,False
90,[2],0,,False
91,Take@N,0,,False
92,[7] RBPTake@N,0,,False
93,[13],0,,False
94,BordaTake@N,0,,False
95,[1] RBPAdaptiveTake@N [13],0,,False
96,CondorcetTake@N [9] RBPAdaptive*Take@N [13],0,,False
97,CombMAXTake@N [12] MTFTake@N,0,,False
98,[3],0,,False
99,CombMINTake@N [12] HedgeTake@N,0,,False
100,[11],0,,False
101,CombMEDTake@N [12] MABRandomTake@N [11],0,,False
102,CombSUMTake@N [12] MABGreedyTake@N [11],0,,False
103,CombANZTake@N [12] MABUCBTake@N,0,,False
104,[11],0,,False
105,CombMNZTake@N [12] MABBetaTake@N,0,,False
106,[11],0,,False
107,DCGTake@N,0,,False
108,[10] MABMaxMeanTake@N [11],0,,False
109,Table 1: List of the implemented pooling strategies and their respective references.,0,,False
110,"e second use case is about the analysis of a pooling strategy. Finally, the third use case is about building a test collection.",0,,False
111,3.1 Visualizing a Test Collection,0,,False
112,"is use case addresses the needs of researchers when (a) interested in checking the properties of a test collection, e.g. visualize the",1,ad,True
113,"pooled runs, assess the behavior of each topic, bias of the nonpooled or new systems, or (b) interested in juxtaposing two or more test collections to compare their properties.",0,,False
114,"For this use case, it is required from the user to provide as input both the runs les and the qrels le. en, select the pooling strategy used to build the test collection, select the appropriate parameters, and execute the pooling strategy. Now, the application will display a visualization similar to Figure 1, where the user can select dynamically which view, and topic to visualize. When multiple test collections are to be compared, the user can repeat the process with a new instance of the application for each test collection.",0,,False
115,3.2 Analyzing of a Pooling Strategy,0,,False
116,"is use case addresses the needs of lecturers to help them explain the pooling method to students, and to address the needs of students to be er understand the algorithm. However, also researchers bene t from this use case, e.g. when interested in juxtaposing the results obtained with di erent pooling strategies.",1,ad,True
117,"For this use case, it is required from the user to provide as input both the runs les and the qrels le. en, the user can select a pooling strategy to be analyzed, and con gure it. Now, the application of the pooling strategy can be controlled by the controllers in the pooling controller section that allows the user to follow the pooling strategy at her/his own pace. To compare di erent pooling",0,,False
118,1323,0,,False
119,Demonstration Paper,0,,False
120,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
121,Name topic_id iteration_id document_id,0,,False
122,rank score run_name,0,,False
123,Type String+ String+ String+ Integer Float32 String,0,,False
124,Ignored? No Yes No Yes No No,0,,False
125,Table 2: Space separated elds of a runs le and their type. e column `Ignored?' indicates if the eld is ignored or not,0,,False
126,by the application.,0,,False
127,Name topic_id iteration_id document_id,0,,False
128,score,0,,False
129,Type String+ String+ String+ Integer,0,,False
130,Ignored? No Yes No No,0,,False
131,Table 3: Space separated elds of a qrels le and their type. e column `Ignored?' indicates if the eld is ignored or not,0,,False
132,by the application.,0,,False
133,"strategies, the user can repeat the process with a new instance of the application for each pooling strategy.",0,,False
134,3.3 Building a Test Collection,0,,False
135,"is use case addresses the needs of a test collection builder to help them control the assessments of the selected documents using the application as a dashboard. is is achieved by making use of the API o ered by the application, which allows a third party application to query the application about which document should be judged, and send a response back with the label.",1,ad,True
136,"For this use case, it is required from the user to provide as input the runs les, select a pooling strategy to be used, and con gure it. en, generate a unique key that will be used by the third party application to communicate with the application. At this point the user is able to follow the judgment process on-line. e application allows the user to change strategy if required, by downloading the current qrels and giving them as input to a new instance of the application.",1,ad,True
137,4 TECHNOLOGY,0,,False
138,"is demo has been developed as a modern web application in JavaScript for the front-end and Scala for the back-end. e frontend is based on the web framework Ember.js1, and on the visualization library p5.js2, which is based on the Processing3 language.",0,,False
139,"e back-end is based on the Play Framework4 and for in-memory storage on Redis5, which is required only to support the API module.",1,AP,True
140,1h ps://emberjs.com 2h ps://p5js.org 3h ps://processing.org 4h ps://www.playframework.com 5h ps://redis.io,0,,False
141,"e input les to be provided to the application are based on the de facto standard format of trec eval6. e format is a nonbreakable space separated le. In Table 2 we show the elds in the correct order as they should be contained by a runs le, and in Table 3 we show the same but for a qrels le. As indicated in the tables, some of the elds are ignored because they are redundant.",1,trec,True
142,e type String+ is a String type that does not contain spaces.,0,,False
143,5 DISCUSSION & CONCLUSION,0,,False
144,"In this demo paper we have presented Visual Pool, an application to help test collection builders, researchers, lecturers, and students to visualize the pooling method. We believe that this technology will have a commercial impact because it allows the building of more e cient test collections but at the same cost, through the application of more e cient pooling strategies. We also believe it will have a research impact because it enables the analysis of new pooling strategies. Finally, it will have an educational impact because it supports lecturers in explaining and students in understanding the pooling method.",0,,False
145,6 ACKNOWLEDGMENTS,0,,False
146,"is research was partly supported by the Austrian Science Fund (FWF) project number P25905-N23 (ADmIRE). is work has been supported by the Self-Optimizer project (FFG 852624) in the EUROSTARS programme, funded by EUREKA, the BMWFW and the European Union.",0,,False
147,REFERENCES,0,,False
148,[1] Javed A. Aslam and Mark Montague. 2001. Models for Metasearch. In Proc. of SIGIR.,0,,False
149,"[2] Gordon V. Cormack, Charles L A Clarke, and Stefan Bue cher. 2009. Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods. In Proc. of SIGIR.",0,,False
150,"[3] Gordon V. Cormack, Christopher R. Palmer, and Charles L. A. Clarke. 1998. E cient Construction of Large Test Collections. In Proc. of SIGIR.",0,,False
151,[4] Bevan Koopman and Guido Zuccon. 2014. Why assessing relevance in medical IR is demanding. In Medical Information Retrieval (MedIR) Workshop.,0,,False
152,"[5] Aldo Lipani. 2016. Fairness in Information Retrieval. In Proc. of SIGIR. [6] Aldo Lipani, Mihai Lupu, and Allan Hanbury. 2015. Spli ing Water: Precision",0,,False
153,"and Anti-Precision to Reduce Pool Bias. In Proc. of SIGIR. [7] Aldo Lipani, Mihai Lupu, and Allan Hanbury. 2016. e Curious Incidence of",0,,False
154,"Bias Corrections in the Pool. In Proc. of ECIR. [8] Aldo Lipani, Mihai Lupu, Evangelos Kanoulas, and Allan Hanbury. 2016. e",0,,False
155,"Solitude of Relevant Documents in the Pool. In Proc. of CIKM. [9] Aldo Lipani, Mihai Lupu, Joao Palo i, Guido Zuccon, and Allan Hanbury. 2017.",0,,False
156,"Fixed Budget Pooling Strategies Based on Fusion Methods. In Proc. of SAC. [10] Aldo Lipani, Joao Palo i, Mihai Lupu, Florina Piroi, Guido Zuccon, and Allan",0,,False
157,"Hanbury. 2017. Fixed-Cost Pooling Strategies Based on IR Evaluation Measures. [11] David E. Losada, Javier Parapar, and Alvaro Barreiro. 2017. Multi-armed bandits",1,ad,True
158,"for adjudicating documents in pooling-based evaluation of information retrieval systems. Information Processing & Management 53, 5 (2017). [12] Craig Macdonald and Iadh Ounis. 2006. Voting for Candidates: Adapting Data Fusion Techniques for an Expert Search Task. In Proc. of CIKM. [13] Alistair Mo at, William Webber, and Justin Zobel. 2007. Strategic System Comparisons via Targeted Relevance Judgments. In Proc. of SIGIR. [14] K. Spa¨rck Jones and C. J. van Rijsbergen. 1975. Report on the need for and provision of an `ideal' information retrieval test collection. British Library Research and Development Report No. 5266 (1975). [15] Ellen M. Voorhees. 2014. e E ect of Sampling Strategy on Inferred Measures. In Proc. of SIGIR. [16] E Voorhes and Donna Harman. 1999. Overview of the eighth text retrieval conference. In Proc. of TREC. [17] Emine Yilmaz and Javed A. Aslam. 2006. Estimating Average Precision with Incomplete and Imperfect Judgments. In Proc. of CIKM.",1,ad,True
159,6h ps://github.com/usnistgov/trec eval,1,trec,True
160,1324,0,,False
161,,0,,False

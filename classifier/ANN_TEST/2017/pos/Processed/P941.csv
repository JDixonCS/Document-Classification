,sentence,label,data,regex
0,Short Research Paper,0,,False
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2,The Impact of Linkage Methods in Hierarchical Clustering for Active Learning to Rank,0,,False
3,Ziming Li,0,,False
4,"University of Amsterdam Amsterdam, e Netherlands",0,,False
5,z.li@uva.nl,0,,False
6,ABSTRACT,0,,False
7,"Document ranking is a central problem in many areas, including information retrieval and recommendation. e goal of learning to rank is to automatically create ranking models from training data. e performance of ranking models is strongly a ected by the quality and quantity of training data. Collecting large scale training samples with relevance labels involves human labor which is timeconsuming and expensive. Selective sampling and active learning techniques have been developed and proven e ective in addressing this problem. However, most active methods do not scale well and need to rebuild the model a er selected samples are added to the previous training set. We propose a sampling method which selects a set of instances and labels the full set only once before training the ranking model. Our method is based on hierarchical agglomerative clustering (average linkage) and we also report the performance of other linkage criteria that measure the distance between two clusters of query-document pairs. Another di erence from previous hierarchical clustering is that we cluster the instances belonging to the same query, which usually outperforms the baselines.",1,ad,True
8,CCS CONCEPTS,0,,False
9,·Information systems Learning to rank;,0,,False
10,1 INTRODUCTION,1,DUC,True
11,"Document ranking is an essential feature in many information retrieval applications. How to sort the returned results according to their degree of relevance has given birth to the area of learning to rank [6], which aims to automatically create ranking models from a training dataset; the learned models are then used to rank the results of new queries. Many learning to rank algorithms have been proposed; they can be categorized into three types of approach: pointwise [2], pairwise [5] and listwise [1].",0,,False
12,"Like other supervised machine learning methods, the performance of a ranking model highly depends on the quantity and quality of training datasets. To create training datasets, experts are hired to manually provide relevance labels for training data, which is expensive and time-consuming. Human labeling, whether by",0,,False
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080684",1,ad,True
14,Maarten de Rijke,0,,False
15,"University of Amsterdam Amsterdam, e Netherlands",0,,False
16,derijke@uva.nl,0,,False
17,"experts or crowds, can be noisy and biased [11]. Active learning is a paradigm to reduce the labeling e ort [12]; it has mostly been studied in the context of classi cation tasks [9, 10, 14].",1,ad,True
18,"In this paper, we present an active learning method to select the most informative query-document pairs to be labeled for learning to rank. Our method relies on hierarchical clustering. Unlike traditional active learning methods, our method is unsupervised and the selected training sets can be used to train di erent learning to rank models. We build on the hypothesis that the information contained in an instance is highly correlated to the instance position in the feature space. Hierarchical clustering has the ability to group instances with similar information into the same cluster and each cluster can be represented by its centroid. While most active learning methods need to rebuild the training models each time new labeled documents are added to the training set, our method labels the instances only once before the training process. We rst evaluate our method on three datasets from Letor 3.0 and nd that the performance of our method is similar or superior to the baselines while we can achieve full training performance with fewer instances. We also analyze the limitations of our method and nd that the e ectiveness of our sampling method is closely related to the features and structures each dataset has.",1,ad,True
19,2 RELATED WORK,0,,False
20,"In order to address the lack of labeled data, active learning has proven to be a promising direction that aims to achieve high accuracy using as few labeled instances as possible [12]. A number of active learning methods have been proposed for classi cation. Most methods start with only a small set of labeled instances and sequentially select the most informative instances to be labeled by an oracle. e trained model will be updated when new labeled instances are added to the training set. Di erent strategies are proposed to choose the most informative instances that can maximize the information value to the current model [9, 10, 14].",1,ad,True
21,"It is not straightforward to extend these methods to ranking problems. On the one hand, these methods try to minimize the classi cation error and do not take into account the rank order while position-based measures are usually non-continuous and non-di erentiable. On the other, each instance in most supervised classi cation tasks can be treated as independent of each other while the instances in learning to rank are conditionally independent. Compared with traditional classi cation problems, there is limited work about active learning in ranking. Long et al. [8] adopt a two-stage active learning strategy schema to integrate query level and document level data selection. ey select samples minimizing the expected loss as the most informative ones and achieves good results in the case of web search ranking. But this method requires",1,ad,True
22,941,0,,False
23,Short Research Paper,0,,False
24,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
25,"a relative big seed set and the ranking models are restricted to pointwise models. Donmez and Carbonell [3] select the documents that woluld impart the greatest change to the current model. While all these methods sequentially select instances to be labeled, Silva et al. [13] adopt hierarchical clustering to ""compress"" the original training set, which is the state-of-the-art selection method in overall performance and e ciency. e method we propose can be viewed as an extension of [13]. e di erence are that we cluster the query-document pairs belonging to the same query separately and average linkage is used, which achieves be er performance.",1,ad,True
26,3 METHOD,0,,False
27,3.1 Hierarchical agglomerative clustering,0,,False
28,"In hierarchical agglomerative clustering, each instance is regarded as a singleton cluster and then merged based on the distance or similarity between clusters until there is only one cluster that contains all instances. e structure of the nal cluster is a tree or dendrogram and each level of the resulting tree is a segmentation of the data. For two given clusters, C1 and C2, and a non-negative real value , if distance f (C1, C2) < , then C1 and C2 will be merged.",0,,False
29,"According to the merging rule, the number of clusters is associated with the value of , which is the indistinguishability threshold [13]. Di erent linkage criterions have been proposed to measure the distance or dissimilarity between two clusters. We use average as our linkage criterion and we also report the performance of minimum linkage, maximum linkage and ward linkage.",0,,False
30,"3.1.1 Minimum linkage clustering. In minimum linkage clustering (also called single linkage clustering), one of the simplest agglomerative hierarchical clustering methods, the value of the shortest link from any member of one cluster to the member of another cluster denotes the distance of these two clusters. For two clusters C1 and C2, the distance between C1 and C2 is:",0,,False
31,f,0,,False
32,"(C1, C2)",0,,False
33,",",0,,False
34,u,0,,False
35,min,0,,False
36,"C1, C2",0,,False
37,"d (u,",0,,False
38,"),",0,,False
39,"In this paper, d is the Euclidean distance. is method is also known as the nearest technique and used in [13]. In this case, minimum linkage clustering can group the instances in ""stringy"" clusters and be converted to nd the Minimum Spanning Tree (MST) of querydocument pairs [4]. By deleting all edges longer than a speci ed indistinguishability threshold in the MST, the remaining connected instances form a hierarchical cluster.",0,,False
40,3.1.2 Average linkage clustering. Average linkage clustering uses,0,,False
41,"the average of distances between all pairs of instances, where each",0,,False
42,"pair consists of two points from two di erent clusters, as the dis-",0,,False
43,tance between two clusters:,0,,False
44,f,0,,False
45,"(C1, C2)",0,,False
46,",",0,,False
47,N1,0,,False
48,1  N2,0,,False
49,"u C1,",0,,False
50,"d (u,",0,,False
51,C2,0,,False
52,"),",0,,False
53,"where N1 and N2 are the sizes of clusters C1 and C2, respectively. We use average linkage as our linkage criterion to measure the",0,,False
54,cluster distance and we perform clustering on each subset which,0,,False
55,contains all the query-document pairs belonging to the same query.,0,,False
56,"3.1.3 Maximum linkage clustering. Di erent from single linkage clustering, the distance of two clusters in maximum linkage clustering is the value of the largest link from one cluster to another",0,,False
57,"cluster. For two clusters C1 and C2, the distance is computed as follows:",0,,False
58,f,0,,False
59,"(C1, C2)",0,,False
60,",",0,,False
61,u,0,,False
62,max,0,,False
63,"C1, C2",0,,False
64,"d (u,",0,,False
65,"),",0,,False
66,where d is the Euclidean distance.,0,,False
67,"3.1.4 Ward linkage clustering. In Ward's linkage method, the",0,,False
68,distance between two clusters is the sum of the squares of the,0,,False
69,distances between all objects in the cluster and the centroid of the,0,,False
70,"cluster. For two clusters C1 and C2, the distance is computed as",0,,False
71,follows:,0,,False
72,"f (C1, C2) ,",0,,False
73,"d (x, µC1C2 )2,",0,,False
74,x C1C2,0,,False
75,where µ is the centroid of the new cluster merged from C1 and C2.,0,,False
76,3.2 Hierarchical clustering for learning to rank,0,,False
77,"In this paper, we apply di erent linkage criteria to hierarchical clustering. As shown in Algorithm 1, we rst cut all the querydocuments pairs into di erent subsets which have di erent query ids and then adopt hierarchical clustering on each subset. A er the last two steps, we can get the clusters on each subset. In our sampling strategy, we use the instance closest to the geometric centroid of each cluster to represent all the query-document pairs in this cluster. In fact, the clustering distribution shows that there is only one single point in most clusters. e nal dataset to be labeled is made up of all the selected instances.",1,ad,True
78,Algorithm 1 Hierarchical Clustering on Each ery (HCEQ),0,,False
79,"Require: Unlabeled dataset D with m queries, desired sampling",0,,False
80,"size n, linkage criterion linka e",0,,False
81,Ensure: e subset S to be labeled,0,,False
82,"1: D1, D2, . . . , Di , . . . , Dm  Di idin Dataset (D). 2: S  ",0,,False
83,"3: for all i  {1, 2, . . . , m} do",0,,False
84,4:,0,,False
85,ni,0,,False
86,n,0,,False
87,|Di | |D |,0,,False
88,"5: C1, C2, . . . , Cj , . . . , Cni  A Clusterin (Di , ni , linka e )",0,,False
89,"6: for all j  {1, 2, . . . , ni } do",0,,False
90,7:,0,,False
91,j  GeometricCentroid (Cj ),0,,False
92,8:,0,,False
93,"pj  N earest N ei hbour (Cj , j )",0,,False
94,9:,0,,False
95,S  S  {pj },0,,False
96,10: end for,0,,False
97,11: end for,0,,False
98,12: return S,0,,False
99,4 EXPERIMENTAL SETUP,0,,False
100,4.1 Data Sets and Evaluation Measure,0,,False
101,"To compare the performance of di erent linkage criteria, we apply hierarchical clustering to a well-known L2R benchmarking collection, Letor 3.0 [7]. In Letor 3.0, there are 7 datasets, based on two document collections: Gov and OHSUMED. We focus on topic distillation (TD2003, TD2004) from GOV and OHSUMED from OHSUMED. In the sets from GOV, each instance is represented by 64 features extracted from the corresponding query-document pairs and has a label indicating its relevance level. e datasets from GOV have binary relevance labels (relevant, not relevant) while",1,Gov,True
102,942,0,,False
103,Short Research Paper,0,,False
104,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
105,"OHSUMED has 3 levels (de nitely relevant, possibly relevant, not relevant) and each instance is represented by a 45-dimensional feature vector. Di erent datasets have di erent numbers of instances; there are 50, 75 and 106 queries, each with 50K, 74K and 16K instances in TD2003, TD2004 and OHSUMED, respectively.",1,TD,True
106,e metric we use is Normalized Discounted Cumulative Gain (NDCG). We run 5-fold cross-validation on all datasets which are query-level normalized and report the average NDCG@10 on 5 folds as nal performance.,0,,False
107,4.2 Baselines,0,,False
108,"We compare the results obtained using our methods with the approach in [13], which is also based on hierarchical clustering; the results of random sampling and the full training set are also reported for reference:",0,,False
109,"Cover. e method proposed in [13] is an unsupervised and compression-based selection mechanism that tries to create a small and highly informative set to represent the original training dataset. Hierarchical clustering (single linkage) is employed to group querydocument pairs into di erent clusters with required number of clusters. e authors also use the instance closest to the geometric centroid of each cluster to represent all the query-document pairs in this cluster and form the nal training set to be labeled. Unlike our method, they perform clustering globally and instances belonging to di erent queries could be grouped into the same cluster.",0,,False
110,"Random. e instances to be labeled are selected randomly and no active learning methods are used here. For the same dataset, we run random sampling 10 times and report the average performance.",0,,False
111,Full Training. We use the labeled original training datasets to train the learning to rank models.,0,,False
112,"To be able to show the di erence between our methods and [13], we select SVMRank and Random Forests as our ranking models which are also used in [13]. e parameters of SVMRank and Random Forests are tuned using a small validation set.",0,,False
113,We run all sampling methods until the fraction of selected instances reaches 50% of the original set.,0,,False
114,5 RESULTS AND ANALYSIS,0,,False
115,5.1 Experimental Results,0,,False
116,"Fig. 1 shows the NDCG@10 of di erent linkage criteria and baselines (denoted by Average, Max, Min, Ward, Cover, Random and Full respectively) on the TD2003, TD2004 and OHSUMED datasets. As we use the instance closest to the centroid to represent the corresponding cluster, the instance selected before may not be selected in later sampling rounds; accuracy is not monotonically increasing.",1,TD,True
117,"On TD2003, in terms of SVMRank (Fig. 1(a)), the accuracy of Average rst exceeds the accuracy of Full at size 2%. Before 24% of the original training set have been selected, all the curves uctuate around 0.35 except Random and Ward. When more and more instances are added to the training set, the performance of Cover goes down and becomes worse than Full. For Random Forests (Fig. 1(d)), Average achieves the highest accuracy before 4% of the original training set has been selected and is the rst one to reach the same accuracy as Full. e accuracies of Ward, Max and Cover start from relatively low points. All methods achieve the performance of Full",1,TD,True
118,"at around 12% except Random. A er 18%, Cover stays below Full (close to 0.36) and uctuates around 0.35.",0,,False
119,"Fig. 1(b) and 1(e) describe the performance of di erent methods on the TD2004 dataset. In Fig. 1(b) we see that Average has the highest starting point when 2% of the original training set have been selected. However, a er one more percent has been added to the training set, all methods have a very similar performance at around 0.295. Average and Cover reach the performance of Full with about 5% and continue to rise before they reach their peaks with 13% and 8%, respectively. Except Random, all methods reach the performance of Full with about 11% selected; their accuracy stays higher than that of Full. In terms of Random Forests (Fig. 1(e)), Average still has the highest starting accuracy and a er the uctuations before 11% of the training set has been selected, Average always performs be er than Full and peaks around 19% of the training set, which is also the highest accuracy in all methods. Cover has a relatively high accuracy when the selected size is 7­ 11%.",1,TD,True
120,"e OHSUMED dataset is based on another document collection, di erent from TD2003 and TD2004. e performance of SVMRank and Random Forests on OHSUMED are shown in Fig. 1(c) and 1(f), respectively. In terms of SVMRank, an interesting thing is that Random has similar performance as hierarchical clustering, which means that hierarchical clustering plays a small role when selecting informative instances in this case. With respect to Random Forests, Average, Min, Ward and Max have very similar performance a er 16% of the training data has been selected and they reach the accuracy of Full at around 30%. Although Cover is the rst method to achieve full training set performance with around 7% and reaches the peak at the same time, its accuracy is not stable and dramatically decreases until 17% of the training set has been selected. A er 30% of the original training set has been selected, all methods achieve the same performance as Full.",1,TD,True
121,5.2 Analysis,0,,False
122,"As we can see from the results presented above, most of the time, Average outperforms other methods on the TD2003 and TD2004 datasets. Although Min and Cover both use the shortest link to measure the distance between two clusters, they have di erent performance and Min performs be er. One possible reason is that our selection mechanism can guarantee that the proportions of selected instances from each query are same and every query contributes to the nal performance. Another reason might be Cover clusters query-document pairs globally, which causes that query-document pairs from di erent queries will be represented by the instances from one speci c query. And this will limit the number of applicable instance pairs for pairwise ranking models.",1,TD,True
123,"On OHSUMED, Average, Min and Max have similar and stable performance while Cover uctuates dramatically when relatively few instances are selected, especially with the Random Forests learner. e di erence between the proposed methods and Random is not signi cant. How datasets are constructed and what structures datasets have will in uence the performance of clusteringbased active learning methods. For example, an instance from the OHSUMED dataset is represented by 45 features which is fewer than for TD2003 and 2004, and some speci c features could have greater impact on the clustering results. In addition, a query has",1,TD,True
124,943,0,,False
125,Short Research Paper,0,,False
126,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
127,NDCG@10,0,,False
128,0.37 0.35,0,,False
129,0.34 0.32,0,,False
130,0.45 0.43 0.41,0,,False
131,0.33 0.31,0,,False
132,0.30,0,,False
133,0.39 0.37 0.35,0,,False
134,NDCG@10,0,,False
135,NDCG@10,0,,False
136,0.29,0,,False
137,Average,0,,False
138,0.27,0,,False
139,Cover Max,0,,False
140,Min,0,,False
141,0.25,0,,False
142,Ward Random,0,,False
143,Full,0,,False
144,0.23 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,0,,False
145,(a) SVMRank on TD2003,1,TD,True
146,% selected,0,,False
147,0.28,0,,False
148,Average,0,,False
149,Cover,0,,False
150,Max,0,,False
151,0.26,0,,False
152,Min,0,,False
153,Ward,0,,False
154,Random,0,,False
155,Full,0,,False
156,0.24 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,0,,False
157,(b) SVMRank on TD2004,1,TD,True
158,% selected,0,,False
159,0.33,0,,False
160,0.31,0,,False
161,Average,0,,False
162,0.29,0,,False
163,Cover Max,0,,False
164,0.27,0,,False
165,Min Ward,0,,False
166,0.25,0,,False
167,Random Full,0,,False
168,0.23 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,0,,False
169,(c) SVMRank on OHSUMED,0,,False
170,% selected,0,,False
171,NDCG@10,0,,False
172,0.38,0,,False
173,0.36,0,,False
174,0.34,0,,False
175,0.32,0,,False
176,0.30,0,,False
177,Average,0,,False
178,0.28,0,,False
179,Cover Max,0,,False
180,Min,0,,False
181,0.26,0,,False
182,Ward Random,0,,False
183,Full,0,,False
184,0.24 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,0,,False
185,%selected,0,,False
186,(d) Random Forests on TD2003,1,TD,True
187,NDCG@10,0,,False
188,0.35,0,,False
189,0.33,0,,False
190,0.31,0,,False
191,0.29,0,,False
192,Average Cover,0,,False
193,Max,0,,False
194,0.27,0,,False
195,Min Ward,0,,False
196,Random,0,,False
197,Full,0,,False
198,0.25 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,0,,False
199,%selected,0,,False
200,(e) Random Forests on TD2004,1,TD,True
201,NDCG@10,0,,False
202,0.45,0,,False
203,0.43,0,,False
204,0.41,0,,False
205,0.39,0,,False
206,0.37,0,,False
207,0.35,0,,False
208,0.33,0,,False
209,0.31,0,,False
210,0.29,0,,False
211,Average Cover,0,,False
212,0.27 0.25,0,,False
213,Max Min Ward,0,,False
214,0.23,0,,False
215,Random Full,0,,False
216,0.21 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,0,,False
217,%selected,0,,False
218,(f) Random Forests on OHSUMED,0,,False
219,"Figure 1: Performance on TD2003, TD2004, and OHSUMED. e x-axis displays the percentage of selected instances.",1,TD,True
220,"only about 152 associated documents in OHSUMED. When we cluster query-document pairs with respect to each query, the number of selected instances from each query is small and every individual query will have very few associated documents which can in uence the performance of ranking models.",0,,False
221,6 CONCLUSIONS AND FUTURE WORK,0,,False
222,"In this paper, we adopt hierarchical clustering to select the most informative instances for learning to rank and report the performance of di erent linkage criteria and baselines. On the Letor 3.0 dataset, the performance of average linkage is similar or superior to the baselines while fewer instances are needed. In the future, we will investigate how to make our method more stronger and robust on di erent datasets. One possible direction is to detect correlations between speci c features and clustering performance. How to choose an optimal fraction of instances for each query while the total number of selected instances is xed is another future direction worth exploring.",1,ad,True
223,"Acknowledgments. is research was supported by Ahold Delhaize, Amsterdam Data Science, the Bloomberg Research Grant program, the Dutch national program COMMIT, Elsevier, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the Microso Research Ph.D. program, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scienti c Research (NWO) under project nrs 612.001.116, HOR-11-10, CI-14-25, 652.002.001, 612.001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.",0,,False
224,REFERENCES,0,,False
225,"[1] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In ICML '07, pages 129­136, 2007.",0,,False
226,"[2] K. Crammer and Y. Singer. Pranking with ranking. In NIPS '01, pages 641­647, 2001.",0,,False
227,"[3] P. Donmez and J. G. Carbonell. Optimizing estimated loss reduction for active sampling in rank learning. In ICML '08, pages 248­255, 2008.",0,,False
228,"[4] J. C. Gower and G. Ross. Minimum spanning trees and single linkage cluster analysis. Applied statistics, pages 54­64, 1969.",0,,False
229,"[5] R. Herbrich, T. Graepel, and K. Obermayer. Support vector learning for ordinal regression. In ICANN '99, pages 97­102, 1999.",0,,False
230,"[6] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225­331, 2009.",0,,False
231,"[7] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR 2007 Workshop on Learning to Rank for Information Retrieval, pages 3­10, 2007.",0,,False
232,"[8] B. Long, J. Bian, O. Chapelle, Y. Zhang, Y. Inagaki, and Y. Chang. Active learning for ranking through expected loss optimization. IEEE Transactions on Knowledge and Data Engineering, 27(5):1180­1191, 2015.",0,,False
233,"[9] A. K. McCallumzy and K. Nigamy. Employing em and pool-based active learning for text classi cation. In ICML '98, pages 359­367, 1998.",0,,False
234,"[10] H. T. Nguyen and A. Smeulders. Active learning using pre-clustering. In ICML '04, page 79, 2004.",0,,False
235,"[11] F. Radlinski and T. Joachims. Active exploration for learning rankings from clickthrough data. In KDD '07, pages 570­579, 2007.",1,ad,True
236,"[12] B. Se les. Active learning literature survey. Technical report, University of Wisconsin­Madison, 2009.",1,ad,True
237,"[13] R. M. Silva, G. Gomes, M. S. Alvim, and M. A. Gon¸calves. Compression-based selective sampling for learning to rank. In CIKM '16, pages 247­256, 2016.",0,,False
238,"[14] S. Tong and D. Koller. Support vector machine active learning with applications to text classi cation. Journal of Machine Learning Research, 2(Nov):45­66, 2001.",0,,False
239,944,0,,False
240,,0,,False

,sentence,label,data,regex
0,Session 7A: Crowdsourcing & Assessment,1,Session,True
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement?,0,,False
3,"Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease",0,,False
4,Qatar University,0,,False
5,University of Texas at Austin,0,,False
6,"{mucahidkutlu,yassmine.barkallah,telsayed}@qu.edu.qa",0,,False
7,"{tmcdonnell,ml}@utexas.edu",0,,False
8,ABSTRACT,0,,False
9,"While crowdsourcing offers a low-cost, scalable way to collect relevance judgments, lack of transparency with remote crowd work has limited understanding about the quality of collected judgments. In prior work, we showed a variety of benefits from asking crowd workers to provide rationales for each relevance judgment [21]. In this work, we scale up our rationale-based judging design to assess its reliability on the 2014 TREC Web Track, collecting roughly 25K crowd judgments for 5K document-topic pairs. We also study having crowd judges perform topic-focused judging, rather than across topics, finding this improves quality. Overall, we show that crowd judgments can be used to reliably rank IR systems for evaluation.",1,TREC,True
10,"We further explore the potential of rationales to shed new light on reasons for judging disagreement between experts and crowd workers. Our qualitative and quantitative analysis distinguishes subjective vs. objective forms of disagreement, as well as the relative importance of each disagreement cause, and we present a new taxonomy for organizing the different types of disagreement we observe. We show that many crowd disagreements seem valid and plausible, with disagreement in many cases due to judging errors by the original TREC assessors. We also share our WebCrowd25k dataset, including: (1) crowd judgments with rationales, and (2) taxonomy category labels for each judging disagreement analyzed.",1,TREC,True
11,KEYWORDS,0,,False
12,"Crowdsourcing, Relevance Assessment, Evaluation, Disagreement",0,,False
13,"ACM Reference Format: Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease. 2018. Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement?. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210033",0,,False
14,1 INTRODUCTION,1,DUC,True
15,"Crowdsourcing platforms such as Amazon's Mechanical Turk provide a low-cost and scalable way of collecting relevance judgments [2, 14]. While crowdsourcing is most often motivated by improved",0,,False
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210033",1,ad,True
17,"scalability, it offers other potential benefits as well. Instead of relying on a single expert judgment for each document, a set of crowd judgments can be collected and aggregated to guard against human error (even trusted judges are fallible), or again human bias, by reflecting average opinion in what is an inherently subjective judging task. It may even be easier to find a crowd judge with relevant expertise than personnel available in one's local area [7, 22].",1,ad,True
18,"While crowdsourced judgments have been used in developing several test collections [5, 16], crowd task designs require special attention to ensure the quality of the collected judgments. Therefore, understanding reasons for crowd disagreements with trusted assessors is important for designing better crowd tasks.",0,,False
19,"Despite many studies reporting high disagreement in relevance judging between trusted assessors [26, 27], disagreements with crowd workers are sometimes attributed to workers being lazy, stupid, or deceitful. While much prior work has sought to improve the quality of collected crowd data, relatively less work has sought to better understand and characterize the types of judging disagreement the crowd tends to exhibit. Moreover, most work has assumed crowd disagreement constitutes error rather than trying to distinguish valid disagreement from actual error.",0,,False
20,"Understanding the reasons behind judging disagreement is difficult without having insights into the judges' thought-processes. Consequently, prior work studying relevance judgments of primary vs. secondary assessors has sometimes relied on research methods involving interaction with participant judges, such as think-aloud [1] and interviewing [26]. However, it can be challenging to apply these methods on the current crowdsourcing platforms.",0,,False
21,"Our earlier work [20, 21] proposed a Rationale Task (RT) design for collecting crowdsourced relevance judgments. In particular, simply asking judges to provide short excerpts from each document to explain their judgment for it was shown to yield a multitude of benefits. In this work, we investigate how we can further exploit these rationales to gain new insights into reasons for judging disagreement, especially with remote crowd work. Largely following our original RT design, we collect roughly 25K crowd judgments for 5K document-topic pairs sampled from the 2014 TREC Web Track [11]. As a refinement, we show that having judges focus on judging within a topic, rather than across topics, improves label quality. Overall, we find that crowd judgments are good enough for ranking information retrieval (IR) systems reliably.",1,TREC,True
22,"Next, we conduct a qualitative analysis using rationales to understand the disagreements and present a novel taxonomy of types of disagreement. In our analysis, we manually inspect 1K crowd judgments for 200 documents (5 judgments per document) in which the aggregated crowd judgment differs from the original TREC judgment, and we assess the relative importance of each disagreement",1,TREC,True
23,805,0,,False
24,"Session 7A: Crowdsourcing & Assessment SIGIR '18, Ann Arbor, MI, USA",1,Session,True
25,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease",0,,False
26,"cause. Our analysis distinguishes between valid disagreement due to subjective considerations (e.g., relevance thresholds) from consistently recognizable human error (e.g., clearly missed evidence of relevance). Among the 200 documents we inspected, we agreed with TREC assessors in only 51.5% of the cases, disagreeing otherwise due to perceived human error (in 21.5%), subjective considerations (20%), and other miscellaneous reasons (7%). Similarly, we agree with 51% of the 1K crowd judgments while disagreeing due to perceived human error (37% of cases), subjective considerations (6%), and other miscellaneous reasons (6%).",1,TREC,True
27,Contributions of our work are as follows:,0,,False
28,· We show that topic-focused crowd judging improves quality vs. our earlier design judging across topics [21].,0,,False
29,"· We show that crowd judgments we collect are largely valid and plausible, and that they enable reliable ranking of participant IR systems in the 2014 TREC Web Track.",1,TREC,True
30,· We demonstrate the value of assessor rationales for helping to explain disagreements in relevance judging.,0,,False
31,"· We present a novel taxonomy over types of disagreement, qualitatively and quantitatively distinguish objective vs. subjective disagreements, and estimate the extent of human error in trusted judgments.",0,,False
32,"· We share our WebCrowd25k dataset1, including: (1) crowd judgments with rationales, and (2) taxonomy category labels for each judging disagreement analyzed. In separate work [13], we also describe and share (3) crowd judging behavioral data.",0,,False
33,"The remainder of this paper is organized as follows. We first present related work in Section 2. We then describe our prior Rationale Task design [21] in Section 3. Section 4 explains our crowd task design and collection, and evaluates the quality of the judgments. In Section 5, we discuss our qualitative analysis and outline our novel taxonomy of disagreement types, presenting the results of the analysis in Section 6. Finally, we conclude in Section 7.",0,,False
34,2 RELATED WORK,0,,False
35,2.1 Reasons behind Disagreements,0,,False
36,"Al-Harbi and Smucker [1] categorized the reasons for disagreement into four groups: difficulty in applying the search topic, difficulty in processing the document, secondary assessor mistakes, and primary assessor mistakes. Sormunen [26] found that disagreement between primary and secondary assessors become more likely with ambiguous topic descriptions. Through examining documents from the TREC 2009 Legal Track, Grossman and Cormak [15] showed that disagreement is mainly caused by human error. Chandar et al. [9] found that longer, less coherent and easily comprehended documents provoke more disagreements. Our analysis includes most of these disagreement reasons and adds subjective considerations and technical judging issues, including four types of crowd errors.",1,TREC,True
37,1 http://qufaculty.qu.edu.qa/telsayed/datasets/webcrowd25k/,0,,False
38,2.2 Understanding Disagreements,0,,False
39,"Al-Harbi and Smucker [1] conducted a think-a-loud study to better understand secondary assessors' thought processes during relevance judging. Sormunen [26] interviewed secondary assessors to learn about reasons for disagreement. Wakeling et al. [28] recorded the behavioral data of assessors (i.e., mouse movements, screenshots and others) and then conducted interview in order to compare primary and secondary relevance judgments. While these methods are effective to understand the disagreement reasons, it is challenging to implement them with current crowdsourcing platforms.",0,,False
40,"Alonso and Mizzaro [2, 3] asked crowd workers to provide optional justifications for their judgments in free text format. In contrast, our RT design [21] requires rationales be provided in the form of document excerpts. While free text gives workers more flexibility, there is no easy quality check. On the other hand, requiring excerpts can be used to detect spammers, as shown in our analysis.",0,,False
41,2.3 Assessing NIST Judgments,0,,False
42,"Scholer et al. [25] showed that NIST assessors judge similar documents differently, suggesting that their judgments can be inconsistent. Kazai et al. [18] found that there is a strong bias to Wikipedia pages (i.e., overrating them) in NIST judgments. Various studies also report that NIST assessors made mistakes in relevance judging [1­3, 8]. In our analysis, we also found that NIST assessors made mistakes but we also distinguish subjective vs. objective forms of our disagreement with NIST judgments.",1,Wiki,True
43,2.4 Crowd Judgment Quality,0,,False
44,"Prior work has a mixture of findings about the quality of crowd judgments. Alonso and Mizzaro [2, 3] claim that crowd judgments can be a reliable alternative for relevance assessment. Blanco et al. [6] show that the ranking of IR systems do not change significantly when crowd judgments are replaced with NIST judgments. Kazai et al. [18] found that crowd workers perform as well as professional judges untrained in web search judging. On the other hand, there are also studies opposing using judgments of non-trained secondary assessors. Bailey et al. [4] show that judgments of primary or trained secondary assessors' judgments cannot be replaced by non-trained secondary assessors' judgments. Kinney et al. [19] report that judgments of non-expert assessors for domain-specific queries cause significant errors affecting system evaluation. Clough et al. [10] compare crowd judgments with expert judgments for domain-specific search tasks. They report that while crowd workers are able to rank systems correctly, they are less capable of differentiating levels of high accurate results than expert assessors. In our work, we show that crowd judgments do not yield significant changes in system ranking for general knowledge topics, establishing that they are reliable enough for practical use.",0,,False
45,3 ORIGINAL RATIONALE TASK DESIGN,0,,False
46,"In this section, we describe the Rationale Task (RT) design from our prior work [21], which we adopt and modify in this work.",1,ad,True
47,"Instructions: We found in [21] that providing overly-specific instructions, examples, or corner-case notes ultimately left workers frustrated, both because of the length of instructions and because it made them feel more unsure about their final answer. As a result,",1,ad,True
48,806,0,,False
49,Session 7A: Crowdsourcing & Assessment,1,Session,True
50,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
51,"Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement? SIGIR '18, Ann Arbor, MI, USA",0,,False
52,"we omitted all such specific instructions and provided a minimal task description, relying on a simple and intuitive judging scale to guide crowd workers (described below).",0,,False
53,"Judging Scale: Our RT design adopts a simple 4-point relevance scale: {Definitely Not Relevant, Probably Not Relevant, Probably Relevant, Definitely Relevant}. There are several proposed benefits of this scale: the relevance categories are evenly spaced across the spectrum of relevance, which makes conversion to binary judgments straightforward and offers flexibility to judges without overcomplicating the scale; each of the relevance categories features colloquial language, rather than jargon common in the IR space; and the categories are symmetric with regard to adjectival descriptors (e.g., Probably Relevant vs. Probably Not Relevant).",1,ad,True
54,Rationales: We asked workers in [21] to provide a rationale to support their labeling decision by selecting 2-3 sentences from documents. These excerpts enable new avenues of automated analysis and also provide expressive freedom for workers to argue in favor of the quality of their work and a new level of transparency which judgments alone do not provide.,0,,False
55,"Serving Tasks To Workers: In our earlier work [21], judging tasks were distributed to crowd workers uniformly at random from among the pool of all available judging tasks. As a result, workers might jump back and forth between different topics during judging.",0,,False
56,"4 MODIFIED DESIGN, DATA, & ANALYSIS",0,,False
57,"In this section, we present and evaluate our modified rationale task design at a much larger scale than in our earlier work [21]. We summarize the main differences in our experimental design vs. our prior study [21] as follows. Firstly, whereas our prior study used 700 NIST-judged documents from the 2009 Web Track, here we use 5K documents from the 2014 Web Track (Section 4.1). Secondly, we ask assessors to judge crawled webpages in the test collection, since this is what NIST does, rather than live versions of pages we used before. Thirdly, whereas our prior study judged a convenient, balanced, and largely random sample of documents, here we strategically sample documents to support ranking of participant IR systems. Fourthly, we employ the topic-focused judging order vs. judging across topics (Section 4.2). Fifthly, we decrease the price of relevance judging from $0.10 to $0.05 as a means of further exploring the stability of RT. Finally, in separate work [13], we describe, analyze, and share additional behavioral data we also collected during crowd judging.",1,Track,True
58,"In Section 4.3, we measure accuracy of crowd judges wrt. TREC assessors under varying aggregation methods. Section 4.4 reports the impact of crowd judging disagreement on the ranking of IR systems.",1,TREC,True
59,4.1 Test Collection & Document Sampling,0,,False
60,"We focus on the 2014 TREC WebTrack [11] (WT2014), which is the most recent TREC Web Track. WT2014 uses ClueWeb122 as document collection and contains 50 topics. The topics are a mixture of broad and specific queries, including navigational, informational and multi-faceted topics. For each topic, only a title and a one-sentence description were recorded. We focused on the ad-hoc search task, for which there are 14,432 relevance judgments. In comparison with our 4-point scale (Section 3), NIST's six-point judging",1,TREC,True
61,2 http://www.lemurproject.org/clueweb12.php/,0,,False
62,"scale slightly differs: 1) spam or junk; not useful for any reasonable purpose, 2) does not provide useful information, 3) provides some information, 4) provides substantial information, 5) dedicated to the topic, and 6) the home page of an entity named in the topic. To induce binary judgments, we map the first two categories to ""nonrelevant"" and the remaining four to ""relevant"". While this six-point scale may valuably support evaluation metrics using graded judgments, it may have side effects when collecting crowd judgments because of its complexity, as mentioned in Section 3. Additionally, we focus on binary disagreements (Section 5) and use mean average precision (MAP) in our evaluation (Section 4.4).",1,ad,True
63,"We sampled 100 documents to re-judge for each topic (i.e., 50 × 100 ,"" 5, 000 documents in total) using statAP [23] weighted sampling. According to the original TREC judgments, 45.4% of these documents are relevant (Table 3). Next, we collected 5 relevance judgments for each document using our NIST-Style Rationale Task on Mechanical Turk. In post-analysis, we found 9 documents having only 4 crowd judgments, so we removed them for consistency, leaving 24,955 judgments for 4,991 documents across 50 topics in our shared WebCrowd25k dataset.""",1,AP,True
64,4.2 Topic-Focused Judging with Rationales,0,,False
65,"Our prior study [21] reported 92% agreement with NIST judgments in Web Track 2009 using the RT design. We adopt this design with a slight modification in the order of documents presented to the crowd workers, intended to better reflect the traditional judging approach employed by NIST. In the original RT design, a crowd worker might first judge a document for Topic X, followed by a document for Topic Y, followed by a Topic Z, before finally returning to Topic X. This varies significantly from the traditional TREC paradigm in which each topic is judged by a single (primary) assessor. To more closely mirror the TREC style of collecting relevance judgments, we propose a topic-focused, NIST-Style Rationale Task Design in which workers continue to judge documents from the same topic until it is exhausted, and only then move on to a fresh topic. This allows assessors to calibrate their internal topic definitions and relevance thresholds [24]. We will show that this yields higher judgment agreement than the original RT design.",1,Track,True
66,"To investigate the effect of topic-focused judging, we randomly selected 370 documents (out of the 4,991 sample above) across 27 topics and collected 5 relevance judgments per document (1850 judgments in total) using both the original RT design (random ordering) and the topic-focused judging described above. Overall, the topic-focused design produced 10.8% higher absolute accuracy (78.1% vs. 67.3% accuracy) over the original randomized ordering (aggregating crowd judgments by majority voting.). In analyzing the randomized ordering judgments, we identified several individual cases where workers were over-rating the relevance of documents, suggesting that they were unable to build an effective relevance threshold [24] while constantly oscillating between topics.",0,,False
67,4.3 Accuracy of Crowd Judgments,0,,False
68,"We next discuss the quality of the relevance judgments we collected with respect to NIST judgments using the large sample of 4,991 documents. We used varying aggregation methods such as Majority Voting (MV) and Dawid-Skene (DS) [12]. We also considered a",0,,False
69,807,0,,False
70,"Session 7A: Crowdsourcing & Assessment SIGIR '18, Ann Arbor, MI, USA",1,Session,True
71,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease",0,,False
72,threshold filtering (TF) method we proposed earlier [21] which filters crowd judgments based on overlap between rationales and then applies a given aggregation method to remaining judgments.,0,,False
73,Table 1: NIST-Style RT Agreement Results wrt. NIST.,0,,False
74,Aggregation Method Majority Voting (MV) Dawid-Skene (DS) Threshold Filtering & MV Threshold Filtering & DS,0,,False
75,Accuracy 0.799 0.798 0.779 0.749,0,,False
76,"Results in Table 1 report simple accuracy of aggregated crowd judgments vs. TREC judgments. MV and DS appear effectively indistinguishable, while filtering performs slightly worse. Most notable, however, is that all of the accuracies are far lower than what reported on WT2009 in our prior study [21]: 0.92. One can imagine a variety of reasons for this; the start of Section 4 discusses a number of differences in experimental design and setup. We analyze the impact on ranking IR systems in Section 4.4, and further analyze disagreements in Section 5.",1,TREC,True
77,4.4 Effect on Ranking IR Systems,0,,False
78,"Next, we assess the impact of judging disagreement on ranking of IR systems participating in WT2014. Following typical TREC evaluation, we induce the ground-truth ranking of systems by using all (14,432) NIST judgments and evaluating systems by mean average precision (MAP). We refer to this ranking as MAP-NIST. In addition to this ground-truth ranking, we also rank the systems based only on the reduced set of 100 documents per topic sampled via statAP (Section 4.1), using either NIST judgments (StatAP-NIST) or crowd judgments (StatAP-Crowd). We calculate the correlation between these three rankings using Kendall's  and AP [30], a variant of Kendall's  giving higher weight to swaps at higher ranks.",1,WT,True
79,Table 2: Correlation of IR system rankings on WT2014.,1,WT,True
80,Correlation Measures,0,,False
81,STATAP-NIST STATAP-Crowd,1,AP,True
82,MAP-NIST,1,MAP,True
83,AP,1,AP,True
84,0.905 0.876,0,,False
85,0.937 0.921,0,,False
86,STATAP-NIST,1,AP,True
87,AP,1,AP,True
88,1.0 1.0,0,,False
89,0.947 0.939,0,,False
90,"Results are shown in Table 2. Remarkably, we see a higher ranking correlation score wrt. ground-truth ranking using crowd judgments than using NIST judgments (0.905 vs. 0.937 for  and 0.876 vs. 0.921 for AP ). While this does not mean that crowd workers provide better judgments than NIST, it does indicate that disagreements between NIST and crowd judgments are not hurting IR system rankings according to either rank correlation metric.",1,AP,True
91,"In order to further explore this, we conduct an additional experiment. We simulate crowd error on the subset of statAP-sampled documents by randomly introducing errors on 20% of the documents (i.e., 1 - the 0.799 accuracy of MV-aggregated crowd judgments vs. NIST judgments). Next, we rank the systems using this judgment set and calculate Kendall's  and AP wrt. using the real NIST judgments for the subset. We repeat this process 100 times",1,ad,True
92,"and calculate average  and AP across trials. Results of this simulation are lower ( ,"" 0.882, with  "","" 0.032, and AP "","" 0.861, with  "", 0.039) than when using the real crowd disagreements (Table 2). This suggests that crowd workers tend to disagree with NIST on documents that do not greatly impact the ranking (something Voorhees [27] reported earlier in analyzing disagreements among NIST judges).",1,AP,True
93,"Thus, despite the lower accuracy of crowd judgments seen in Section 4.3 vs. our prior study [21], we still see that using crowd judgments easily surpasses the traditionally established  , 0.9 threshold for reliable ranking of IR systems [27].",1,ad,True
94,5 UNDERSTANDING DISAGREEMENT,0,,False
95,"In this section, we explain our qualitative analysis into reasons for disagreement using the rationales provided by the crowd judges. We first present the methodology of our analysis (Section 5.1), followed by the reasons for disagreement we identified (Section 5.2).",0,,False
96,5.1 Methodology,0,,False
97,"To investigate the reasons for judging disagreements, we manually inspected a sample of topic-document pairs (See Section 6.1) in which the aggregated judgment of the crowd disagrees with the NIST judgment. Two authors of this paper judged the relevance of sampled documents independently and with no prior knowledge of other judgments by NIST or the crowd. Next, each of the two authors examined the respective NIST and crowd judgments and assigned one of four stance labels: 1) Strongly Agree with NIST; 2) Slightly Agree with NIST; 3) Slightly Disagree with NIST; or 4) Strongly Disagree with NIST. In ""strong"" cases, we perceive clear evidence in the document for our own judgment. With ""slight"" cases, we agree with one of the judgments but believe that the other is also reasonable, given a different interpretation of the topic or perception of relevance (e.g., relevance threshold). Finally, the two authors met in person to compare their labels and discuss their reasoning, ultimately arriving at a single, reconciled label for each case. Note that our stance labels (individual or reconciled) can be easily converted to graded relevance judgments.",1,ad,True
98,"Next, we sought to understand why there is disagreement by performing the following analysis for each document we judged:",0,,False
99,"· Understanding disagreements with NIST: For each case where we disagree with NIST, we carefully inspect the document and NIST judgment.",0,,False
100,"· Understanding disagreements with the crowd: We follow a similar process for each case in which we disagree with a crowd judgment. We consider each individual crowd judgments and rationale. Even when our judgment matches a crowd judgment, we still consult the rationale provided to verify it is reasonable.",0,,False
101,"· Additional Categories: Independent of our stance labels, we also annotated each document-topic pair to note if (1) the document is very long; (2) the document has low readability (e.g., poor web design or writing); or (3) expert knowledge appears necessary to judge the document-topic pair (e.g., knowledge of the American Revolution needed to judge a topic about the war). Note that these labels are not mutually exclusive; i.e., a document may have more than one.",1,ad,True
102,808,0,,False
103,Session 7A: Crowdsourcing & Assessment,1,Session,True
104,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
105,"Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement? SIGIR '18, Ann Arbor, MI, USA",0,,False
106,"In some cases, we identified more than one possible cause for disagreement: ambiguity in the topic description, and/or a crowd worker may have misunderstood the topic or have deemed the relevant content insufficient. In such cases, we determined the reason we believe to be most likely, yielding a single best reason for every case. We repeated this reasoning process over two full passes in order to further increase the consistency of our analysis. The entire process took around 30 hours.",0,,False
107,"Despite careful inspection, our method has certain limitations:",0,,False
108,"(1) Our understanding of each topic is limited to what the original NIST assessor recorded, via topic definition and judgments. Some more nuanced understanding of an intended topic may have eluded us as secondary assessors.",0,,False
109,"(2) While we carefully analyzed a small set of disagreements, we are certainly fallible and susceptible to human error.",0,,False
110,"(3) To further understand the thought process of crowd workers, we rely on (our interpretation of) their provided rationales, which are limited to text excerpts from the documents. Rationales appear most useful to show why a document is relevant and less helpful for showing why a document is not relevant. In the latter case, we do our best to guess the reason for disagreement using all evidence we have at our disposal.",0,,False
111,5.2 Reasons for Disagreement,0,,False
112,"In this section, we discuss in detail the reasons we observed for judging disagreements. We further induce a novel taxonomy over those reasons, presented in Figure 1.",0,,False
113,"5.2.1 Human Error. We sometimes observed seemingly unambiguous evidence for document relevance. For instance, in judging the document clueweb12-0310wb-50-29927 for topic 273 (""Find Wilson's Disease Association website""), the NIST assessor judged it as relevant, but it is neither the website nor has the URL for it. Therefore, in cases where we strongly disagree (with either NIST or crowd), we designate the reason for the disagreement as human error. Due to lack of any insight into NIST judgments beyond topic descriptions and judgments, we are unable to further understand this NIST judgment. However, inspecting crowd rationales led us to categorize 4 types of crowd errors:",0,,False
114,"(1) Topic Misunderstanding. In this category of mistakes, we believe the crowd worker made a good faith effort to judge the document but misunderstood the topic. Ideally, the rationale for a document judged relevant should indicate a part of the document making it relevant to a particular topic. Judging a document as relevant but providing a rationale that is not closely matched to the topic suggests that the crowd worker misunderstood the topic. For instance, in judging document clueweb12-0204wb-61-01007 for topic 273 (""Find Wilson's Disease Association website""), one crowd worker judged it ""Definitely Relevant"" with the following rationale:",1,ad,True
115,"Wilson's disease is an inherited condition which causes copper to build up in the body. This excess copper tends to collect in the brain and liver but can also be found in the corneas (in the eyes) and the kidneys. If . . . not treated properly, it can cause very serious symptoms.",0,,False
116,"The chosen excerpt from the document explains Wilson's disease, which indicates an over-interpretation of the topic definitions, such",0,,False
117,that the crowd worker might have thought that this information about Wilson's disease would be useful for a person who searches for WDA's website. This is also similar to Wakeling et al. [28]'s finding: Secondary assessors can sometimes judge documents as relevant because of thinking that the information on that page can be also useful for a person who makes that search.,0,,False
118,"(2) Missing Relevant Content. Another reason for disagreement could be the lack of concentration or other unknown human error that causes missing a relevant content in a document. We used this label when crowd workers judged a document as ""definitely not relevant"" or ""probably not relevant"" with a rationale not relevant to the topic while there is a clear evidence in the document to be relevant.",0,,False
119,"(3) Spammers. Rationales are also useful in detecting workers who clearly do not follow task instructions, acting as ""spammers"". We observed 4 types of behavior in this category:",1,Spammer,True
120,· Providing rationales from other documents · Providing off-topic rationales for documents judged relevant · Reporting a page load error3 yet providing a rationale · Judging a document with only textual content as relevant yet,1,ad,True
121,using our pre-defined rationale text for non-textual pages4,0,,False
122,"While we typically assume that crowd workers' labels are correct when they match our own, when the label space is small (e.g., binary), we must also account for accidental agreement. We found rationales to be useful in identifying such cases. For example, a judge providing a rationale from the wrong document indicates that the judgment is likely spam, even if the label seems correct.",0,,False
123,"(4) Relevant Rationale for Not-Relevant Judgment. During our analysis, we also observed that some crowd workers judged a document as ""Definitely Not Relevant"" but provided a rationale which is a definitely relevant statement to the topic, i.e., a conflicting rationale. For instance, in judging the document clueweb12-1611wb-41-22823 for topic 270 (i.e., ""Find quotes from Sun Tzu""), a worker provided the following rationale:",0,,False
124,"sun tzu said, ""the good fighters of old first put themselves beyond the possibility of defeat, and then waited for an opportunity of defeating the enemy.""",0,,False
125,"Though this rationale is a perfect example to judge the document as relevant, the worker judged the document as ""Definitely Not Relevant"". Our earlier study [21] noted such behavior as well, which seems to be most likely due to clicking too quickly or misunderstanding the topic or the judging scale.",0,,False
126,"5.2.2 Ambiguous Topic Definition. While a primary assessor formulates a clear definition of the information need in their head, secondary assessors (including ourselves) are reliant upon the primary assessor's written topic description to understand the information need. Overly terse, incomplete topic descriptions may introduce ambiguity in the information need and relevance judging.",1,ad,True
127,"In such cases, while we disagree with the NIST or crowd judgments, we think that his/her judgment is also reasonable based on",0,,False
128,"3In addition to our 4-point judging scale, workers were also provided a fifth option to indicate that a given web page did not load and so could not be judged. 4For cases in which a relevance judgment depends on non-textual content, we asked workers to enter a particular pre-defined text string.",1,ad,True
129,809,0,,False
130,"Session 7A: Crowdsourcing & Assessment SIGIR '18, Ann Arbor, MI, USA",1,Session,True
131,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease",0,,False
132,Reasons for Disagreement,0,,False
133,NIST Error,0,,False
134,Crowd Error,0,,False
135,Misunderstanding,0,,False
136,Missing Relevant Content,0,,False
137,Spammer Relevant Rationale for Not-Relevant,1,Spammer,True
138,Judgment,0,,False
139,Different Perception of Relevance,0,,False
140,Relevance Threshold,0,,False
141,Indirect Relevance,0,,False
142,Ambiguous Topic Definition,0,,False
143,Figure 1: Reasons for Disagreement in Relevance Judging.,0,,False
144,Technical Issues,0,,False
145,Page Load Error,1,ad,True
146,Redirecting Page Missing Multimedia,0,,False
147,"the topic description (given another interpretation of the topic). For example, in judging the documents for topic 261 (i.e., ""What folk remedies are there for soothing a sore throat""), it seems that the NIST assessor did not consider home-remedies nor herbal-remedies as folk remedies and thus did not judge such documents as relevant.",0,,False
148,"5.2.3 Different Perception of Relevance. Assessors may disagree also due to the inherently subjective nature of relevance judging. Many such factors impact relevance judgments, such as novelty of the document [29], reliability of the source [31] and others. We may have disagreed with crowd workers or NIST assessors due to such perception. We identified two types of cases:",0,,False
149,"(1) Relevance Threshold. The amount of relevant content in a document has a large impact on our relevance judgment, as expected. We see this impact more obvious by checking the rationales. For instance, in judging the document clueweb12-1601wb-52-10051 for topic 290 (i.e., ""How do you identify a Norway Spruce?""), one of the crowd workers provided the following rationale:",0,,False
150,"The Norway Spruce (Picea abies) is a large evergreen coniferous tree, growing to 35-55 m tall and with a trunk diameter of up to 1-1.5 m. The shoots are orange-brown and hairless.",0,,False
151,"While showing that the document provides some relevant information for the topic, the crowd worker still judged it as ""Probably Not Relevant"", indicating that although s/he found some relevant content, s/he did not find that the relevant material was sufficient to satisfy the information need. Interestingly, three crowd workers also provided rationales covering these sentences but judged differently (i.e., two as ""Definitely Relevant,"" and one as ""Probably Relevant""), further suggesting different thresholds for relevance.",0,,False
152,"We also found cases in which we disagreed with NIST likely due to different relevance thresholds. For example, for topic 278 (i.e., ""What are the lyrics to the theme song for 'Mister Rogers' Neighborhood""), the NIST assessor judged document clueweb121006wb-74-08027 as ""Not Relevant"", likely due to it only containing the first four lines of the lyrics, rather than the full lyrics. We believe this should be sufficient to judge the document as relevant.",0,,False
153,"(2) Indirect Relevance. According to track judging instructions [11], a page should be judged as ""Not Relevant"" if it does not contain a relevant content but only provides a link or mentions a resource name (e.g., a book or a course given by a university) directly related to the topic of interest. However, we found that NIST assessors and crowd workers consider the links or resource names as relevant information in many cases we inspected.",0,,False
154,"For example, the NIST assessor judged document clueweb120700wb-28-32790 as relevant to topic 267 (i.e., ""What are the lyrics to the song Feliz Navidad"") though the page does not contain the lyrics, only a link named ""View Feliz Navidad Lyrics"". Also in judging the document clueweb12-1509wb-34-18722 for topic 276 (i.e., ""How has African American music influenced history, including cultural history""), we observed that the NIST assessor judged the document as relevant even though it only lists courses offered by a university. While some courses seem relevant to the topic, there is no relevant information for the topic in the page. Similarly, we also observed that some crowd workers give the link as their rationale, suggesting that they consider links in their judging process.",1,ad,True
155,"5.2.4 Technical Issues. Correctly rendering crawled web pages is challenging due to the complex structure of web pages (e.g., containing multimedia files from the host server and also from other web addresses). Therefore, relevance judging of crawled web pages imposes many technical challenges which may have an impact on the relevance judgments. We identified the following issues:",1,ad,True
156,(1) Page Loading Error. Crowd workers explicitly indicated some pages did not load to be judged.,1,ad,True
157,"(2) Missing Multimedia. Rendering web pages using crawled pages is challenging because many pages use varying multimedia files from other web addresses (e.g., YouTube) which may not be available at the time of collecting relevance judgments. Even if the multimedia files are captured during the crawling, it can be challenging to modify the web page source to get the images to display correctly. Missing multimedia becomes a big problem especially with topics such as topic 258 (i.e., ""Find pictures of a hip roof"").",1,ad,True
158,810,0,,False
159,Session 7A: Crowdsourcing & Assessment,1,Session,True
160,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
161,"Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement? SIGIR '18, Ann Arbor, MI, USA",0,,False
162,"(3) Redirecting. In our analysis, we noticed that some pages are redirecting to other pages, causing judging of different documents than we believe were viewed by NIST assessors. This problem was potentially more severe in our earlier study using live pages [21].",0,,False
163,6 RESULTS & DISCUSSION,0,,False
164,"In this section, we present and discuss the results of our analysis. We first explain how we sampled the documents to be manually inspected (Section 6.1) and then show our own judgments for the sampled documents (Section 6.2). Finally, we present the distribution of disagreement reasons (Section 6.3).",0,,False
165,6.1 Sampling Documents to be Inspected,0,,False
166,"Table 3 shows the distribution of documents at varying agreement levels (AL) (i.e., the percentage of crowd workers agreeing with the NIST assessor in judging a particular document when graded judgments are collapsed to binary judgments). To select documents to be analyzed, we opt for stratified sampling. In our sampling method, we consider 3 different agreement levels of crowd workers that cause disagreement with NIST judgments (i.e., 0%, 20%. 40%) and two possible NIST-judged binary relevance judgments (i.e., relevant and not relevant) separately, yielding 6 (,""3x2) different combinations. We sample 25 documents from each case. In order to have a better representation of the disagreements, we also randomly sample 50 more documents from the remaining disagreement cases, resulting in 200 documents in total. The distribution of the 6 cases in our sample is given in Table 4.""",1,ad,True
167,Table 3: Distribution of Documents at Varying Agreement Levels between NIST and Crowd Workers. The shaded rows represent disagreement between crowd and NIST based on majority voting.,1,ad,True
168,Agreement Level 0% 20% 40% 60% 80% 100% Total,0,,False
169,NIST-Judged Not Relevant 1.4% 4.7% 8% 11.7% 14% 14.8% 54.6%,0,,False
170,NIST-Judged Relevant 0.6% 1.6% 3.7% 7.8% 10.9% 20.6% 45.4%,0,,False
171,Total,0,,False
172,2% 6.3% 11.7% 19.6% 24.9 35.5% 100%,0,,False
173,Table 4: Document Distribution of the Manually Inspected Sample.,0,,False
174,Agreement Level 0% 20% 40% Total,0,,False
175,NIST-Judged Not Relevant 28 39 48 115,0,,False
176,NIST-Judged Relevant 25 29 31 85,0,,False
177,Total,0,,False
178,53 68 79 200,0,,False
179,6.2 Our Own Relevance Judgments,0,,False
180,"In this section, we discuss our own relevance judgments for the sampled documents. We discuss the results in two different ways: 1)",0,,False
181,results within the stratified sample of documents (Section 6.1); and,0,,False
182,2) results projected to each of the six cases NIST and aggregated,0,,False
183,"crowd judgment differ, based on the frequency of each case. See",0,,False
184,the first three rows of Table 3 to do this projection.,0,,False
185,The summary of our judgments is given in Table 5. In 23% and,0,,False
186,"25.5% of the cases, we slightly and strongly disagree with NIST,",0,,False
187,respectively. This means that we disagree with NIST assessors in,0,,False
188,"48.5% (,""23% + 25.5%) of the cases we inspected, projected to 47% of""",0,,False
189,all disagreement cases. This suggests that the quality of the crowd,0,,False
190,judgments is much higher than we earlier calculated (Section 4.3).,0,,False
191,"Considering only NIST-judged relevant documents, as expected,",0,,False
192,we agree with NIST in many more cases than we disagree: 51,0,,False
193,"(,7+10+12+4+6+12) vs. 34 (,""7+5+3+7+8+4). However, we see the""",0,,False
194,opposite pattern over the NIST-judged non-relevant documents: 52,0,,False
195,"(,10+9+21+1+6+5) vs. 63 (,8+11+12+9+13+10). This suggests that",0,,False
196,our judgments are even more liberal than NIST assessors [26].,0,,False
197,"We also observe that as fewer crowd workers agree with NIST,",0,,False
198,"our own agreement with NIST similarly decreases, as expected.",0,,False
199,"Specifically, among documents with crowd AL of 40% (i.e., both",0,,False
200,"NIST-judged relevant and non-relevant documents), we disagree",0,,False
201,with,0,,False
202,NIST,0,,False
203,assessors,0,,False
204,in,0,,False
205,"36.7% (,",0,,False
206,12+10+3+4 48+31,0,,False
207,),0,,False
208,of the,0,,False
209,cases.,0,,False
210,Our,0,,False
211,dis-,0,,False
212,agreement,0,,False
213,"(,",0,,False
214,8+9+7+7 28+25,0,,False
215,with NIST ) when the,0,,False
216,increases crowd AL,0,,False
217,to is,0,,False
218,"54.4% (,",0,,False
219,11+13+5+8 39+29,0,,False
220,),0,,False
221,and,0,,False
222,58.5%,0,,False
223,"20% and 0%, respectively.",0,,False
224,6.3 Distribution of Disagreement Reasons,0,,False
225,"In this section, we first discuss the results of our qualitative analysis in the all sampled documents (Section 6.3.1). Next, we focus on only hard-to-process documents (Section 6.3.2) and the problematic topics where there is a high disagreement between NIST and crowd workers (Section 6.3.3).",0,,False
226,"6.3.1 Distribution across the whole sample. In our analysis, we assessed 200 NIST judgments and 1000 (, 200x5) crowd judgments. The distribution of each category of agreement/disagreement is shown in Figure 2.",0,,False
227,"There are several observations we can make from these results. Firstly, our agreement ratios with NIST and crowd workers are similar (51.5% vs. 51.2%). However, among the agreements with crowd judgments, 27.1% (, 1% + 13.6% + 12.5%) of them appear to be accidental agreement (Section 5.2).",0,,False
228,"Secondly, the human error ratio of NIST assessors is lower than human error ratio of crowd workers, as expected (21.5% vs. 36.7%). Among crowd errors, missing relevant content and misunderstanding are the main problems, contributing to about 80% of all crowd errors. Judgments from spammers are just 16.3% of the crowd errors, that is, 6% (,"" 36.7% x 16.3%) of all cases we inspected. We also noticed that among spam judgments, 78% of them use rationales from other documents. Therefore, most of such spam can be easily detected by checking whether each rationale actually exists in the document being judged. We automatically identified that 2,878 crowd judgments in the entire collection (i.e., 11.5% of all crowd judgments) use rationales from other documents.""",0,,False
229,"Misunderstanding and conflicting judgments might be resolved by providing better topic descriptions and task instructions, suggesting that 42.8% (,"" 39.5% + 3.3%) of crowd errors (i.e., 15.7% ("","" 42.8% x 36.7%) of the sample and 18.6% (projected) of the all disagreements) could be eliminated by the """"perfect"""" task design. However, missing""",0,,False
230,811,0,,False
231,"Session 7A: Crowdsourcing & Assessment SIGIR '18, Ann Arbor, MI, USA",1,Session,True
232,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease",0,,False
233,Table 5: Our Relevance Judgments. The ratio of each case with respect to the total sample size is given in parentheses.,0,,False
234,NIST Binary Judgment Crowd Agreement Level,0,,False
235,Strongly agree with NIST Slightly agree with NIST Slightly disagree with NIST Strongly disagree with NIST,0,,False
236,Total,0,,False
237,Not Relevant,0,,False
238,0%,0,,False
239,20%,0,,False
240,40%,0,,False
241,10 (5%) 1 (0.5%) 8 (4%) 9 (4.5%),0,,False
242,9 (4.5%) 6 (3%) 11 (5.5%) 13 (6.5%),0,,False
243,21 (10.5%) 5 (2.5%) 12 (6%) 10 (5%),0,,False
244,28 (14%) 39 (19.5%) 48 (24%),0,,False
245,0%,0,,False
246,7 (3.5%) 4 (2%) 7 (3.5%) 7 (2.5%),0,,False
247,25 (12.5%),0,,False
248,Relevant 20%,0,,False
249,10 (5%) 6 (3%) 5 (2.5%) 8 (4%),0,,False
250,29 (14.5%),0,,False
251,40%,0,,False
252,12 (6%) 12 (6%) 3 (1.5%) 4 (2%),0,,False
253,31 (15.5%),0,,False
254,Total,0,,False
255,69 (34.5%) 34 (17%) 46 (23%) 51 (25.5%),0,,False
256,200 (100%),0,,False
257,Figure 2: Distribution of Agreement & Disagreement Reasons.,0,,False
258,"relevant content, which contributes to 15% of the sample and 12% (projected) of the all disagreements, may be a harder problem to fix. On the other hand, simply paying more could incentivize higher quality work given a more complex task [17].",0,,False
259,"Thirdly, the ratio of different perceptions of relevance for NIST judgments is much higher than its ratio for crowd workers (20% vs. 5.6%). Relevance threshold constitutes to 65% and 80% of those cases for NIST and crowd workers, respectively.",0,,False
260,"Interestingly, we also found that even expert NIST assessors did not always follow track judging instructions [11] wrt. indirect relevance, sometimes judging a document as relevant even when it only pointed to a resource (a web page or a course/book name) that is potentially relevant (7% of all cases we inspected).",0,,False
261,"6.3.2 Distribution across Hard-to-Process Documents. As noted earlier, during our inspection, we labeled the documents that are hard to process using three different labels. Figure 3 shows the distribution of reasons for the documents with these labels. In all three cases, the human error ratio for crowd workers increases compared to its ratio among all sampled documents (36.7% vs. 45%, 45.2%, and 45.6%). On the other hand, human error ratio for NIST is lower than its ratio on all sampled documents (21.5% vs. 12.5%, 16.1%, and 5.6%) suggesting that NIST assessors are not affected by the difficulties in processing the documents. Among crowd errors on document-topic pairs requiring expert knowledge, we found that 55% of the disagreement reasons is misunderstanding. On the other hand, we found that 69% and 83% of the crowd errors are due to missing relevant content for long documents and documents with low readability, respectively.",1,ad,True
262,"(a) Document-Topic Pairs needing Expert Knowledge (16). Top Bar: NIST, Lower Bar: Crowd.",0,,False
263,"(b) Long Documents (21 cases). Top Bar: NIST, Lower Bar: Crowd.",0,,False
264,"(c) Documents having Low Readability (31). Top Bar: NIST, Lower Bar: Crowd. Figure 3: Distribution of Agreement & Disagreement Reasons for Hard-To-Process Documents. The number of documents for each case is given in parentheses.",1,ad,True
265,812,0,,False
266,Session 7A: Crowdsourcing & Assessment,1,Session,True
267,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
268,"Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement? SIGIR '18, Ann Arbor, MI, USA",0,,False
269,Figure 4: Agreement between NIST and Crowd Workers. The horizontal line represents the average agreement among all topics.,0,,False
270,Table 6: Topics with the Lowest Agreement between NIST and Crowd Workers.,0,,False
271,Topic ID,0,,False
272,261 273 260 288 265,0,,False
273,Topic Description,0,,False
274,What folk remedies are there for soothing a sore throat Find Wilson (Wilson's) Disease Association web site Find a list of the major battles of the American Revolution Find quotes from Fidel Castro What were the ten worst tornadoes in the USA,1,ad,True
275,Sample Size,0,,False
276,9 16 8 13 7,0,,False
277,Our Judgment,0,,False
278,Agree w/ NIST Disagree w/ NIST,0,,False
279,1,0,,False
280,8,0,,False
281,12,0,,False
282,4,0,,False
283,7,0,,False
284,1,0,,False
285,9,0,,False
286,4,0,,False
287,5,0,,False
288,2,0,,False
289,"6.3.3 Topic Specific Disagreement Reasons. In our analysis, we noticed very low agreement between NIST and crowd workers for a few particular topics. Figure 4 shows the agreement between NIST and aggregated crowd judgment for each topic. We investigate 5 topics where the agreement is lower than 50% to better understand such topic-specific problems. Descriptions for these topics and our stance labels (collapsed to binary) are shown in Table 6.",0,,False
290,"Topic 261. We realized that the NIST assessor did not think any remedies named as home, herbal or even grandma's remedy (in the document with id clueweb12-1911wb-01-10721) as a folk remedy. We disagree with NIST (i.e., agree with the aggregated crowd judgment) in 8 cases out of 9 in our inspected sample, where 7 of them are due to topic ambiguity. We also determined that, for this topic, 19 crowd judgments, out of 45 (,"" 9 x 5), in our inspected sample and 184 judgments (among 100 x 5 "", 500) in the whole collection are actually spam.",0,,False
291,"Topic 273. We mostly agree with NIST assessors (in 12 cases out of 16). In 46 crowd judgments, out of 80 (,"" 16 x 5), we noticed that the crowd workers misunderstood the topic and usually provided a text that describes Wilson's disease.""",0,,False
292,Topic 260. We determined that 14 judgments in our inspected sample and 194 judgments in the whole collection for this topic are actually spam. Misunderstanding is the second most important disagreement reason we found (25% of the judgments we inspected). We observed that the crowd workers were very liberal in judging documents as relevant if the document is somehow related to American Revolution even though it does not mention any battle name. This appears consistent with past work [19] finding that laymen often fall back on simple query term matching in assessing relevance for topics which exceed their level of topical expertise.,0,,False
293,"Topic 288. In our inspected sample of this topic, 40% of our disagreements with crowd workers appear to be due to workers missing relevant content. We labeled 5 out of 13 documents as either ""too long"" or ""low readability"" and found that 60% of the disagreement reasons for these 5 documents are missing relevant content, suggesting these factors may have negatively impacted judging. We also found that 25% of the judgments suffered from technical issues (20% page load error and 5% redirecting).",1,ad,True
294,Topic 265. The main problem appears to be the high ratio of spam (51% of the crowd judgments we inspected and 46% of all crowd judgments over the entire collection for this topic).,0,,False
295,"We automatically detected spammers who provide rationales that do not exist in the documents to be judged and noticed that automatically-detected spam ratios are particularly high for a few specific topics, and that the agreement ratios for these topics were generally lower than others. A likely explanation is that our topicfocused judging design (Section 4.2) had the unintended side-effect of also concentrating spam within a topic instead of distributing it evenly across topics.",1,ad,True
296,7 CONCLUSION & FUTURE WORK,0,,False
297,"While crowdsourcing offers a low-cost, scalable way to collect relevance judgments [2, 14], lack of transparency with remote workers has limited understanding about the quality of collected judgments. In prior work [20, 21], we investigated the value of asking crowd workers to provide rationales explaining each relevance judgment. In this work, we scaled up this rationale-based judging design to assess its reliability in practice to support a real TREC track evaluation: the 2014 WebTrack [11]. We investigated having crowd",1,TREC,True
298,813,0,,False
299,"Session 7A: Crowdsourcing & Assessment SIGIR '18, Ann Arbor, MI, USA",1,Session,True
300,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease",0,,False
301,"judges focus on judging within a topic, rather than across topics and showed this improved the quality of collected judgments. Overall, we showed that we were able to reliably rank IR systems using crowd judgments.",0,,False
302,"To investigate the potential of rationales to provide new insight into judging disagreements between expert and crowd assessors, we then analyzed 200 disagreements between TREC and crowd judges. We found that rationales for judging relevant do provide useful insights into crowd workers' thought process and can be used to better understand disagreement reasons. However, negative rationales (for judging a document non-relevant) were usually not helpful for disagreement analysis. In total, we disagreed with NIST assessors in 48.5% of the cases we inspected, finding that many crowd disagreements appear valid and plausible. We presented a novel taxonomy over reasons for disagreement, and we share our WebCrowd25k dataset, including: (1) crowd judgments with rationales, and (2) taxonomy category labels for each judging disagreement analyzed. In a separate work [13], we also describe, analyze, and share (3) behavioral data collected during crowd judging.",1,TREC,True
303,"Overall, we believe that forming a rationale is critical to forming a coherent relevance judgment, whether or not judging instructions explicitly require it. Our earlier results [21] showed that requiring annotators to provide rationales incurs almost no additional time, suggesting that annotators might be already doing so implicitly. While we have investigated collecting judgment rationales for crowd work, as we have earlier argued [20, 21], we believe that asking (traditional) expert judges to also provide rationales could provide a myriad of benefits, enriching both the quality and value of collected relevance judgments.",1,ad,True
304,ACKNOWLEDGMENTS,0,,False
305,"We thank the many talented crowd contributors and NIST relevance assessors who provided the data for our study, and the reviewers for their valuable feedback. This work was made possible by NPRP grant# NPRP 7-1313-1-245 from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibility of the authors.",1,ad,True
306,REFERENCES,0,,False
307,"[1] Aiman L Al-Harbi and Mark D Smucker. 2014. A qualitative exploration of secondary assessor relevance judging behavior. In Proceedings of the 5th Information Interaction in Context Symposium. ACM, 195­204.",0,,False
308,"[2] Omar Alonso and Stefano Mizzaro. 2009. Can we get rid of TREC assessors? Using Mechanical Turk for relevance assessment. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation, Vol. 15. 16.",1,TREC,True
309,"[3] Omar Alonso and Stefano Mizzaro. 2012. Using crowdsourcing for TREC relevance assessment. Information Processing & Management 48, 6 (2012), 1053­1066.",1,TREC,True
310,"[4] Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas, Arjen P de Vries, and Emine Yilmaz. 2008. Relevance assessment: are judges exchangeable and does it matter. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 667­674.",0,,False
311,"[5] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2016. UQV100: A test collection with query variability. In Proceedings of the 39th ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 725­728.",1,UQV,True
312,"[6] Roi Blanco, Harry Halpin, Daniel M Herzig, Peter Mika, Jeffrey Pound, Henry S Thompson, and Thanh Tran Duc. 2011. Repeatable and reliable search system evaluation using crowdsourcing. In Proceedings of the 34th ACM SIGIR conference on Research and development in Information Retrieval. ACM, 923­932.",0,,False
313,"[7] Muhammed Fatih Bulut, Yavuz Selim Yilmaz, and Murat Demirbas. 2011. Crowdsourcing location-based queries. In Pervasive Computing and Communications Workshops (PERCOM Workshops), 2011 IEEE International Conference on. IEEE, 513­518.",0,,False
314,"[8] Ben Carterette and Ian Soboroff. 2010. The effect of assessor error on IR system evaluation. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM, 539­546.",0,,False
315,"[9] Praveen Chandar, William Webber, and Ben Carterette. 2013. Document features predicting assessor disagreement. In Proceedings of the 36th ACM SIGIR conference on Research and development in information retrieval. ACM, 745­748.",0,,False
316,"[10] Paul Clough, Mark Sanderson, Jiayu Tang, Tim Gollins, and Amy Warner. 2013. Examining the limits of crowdsourcing for relevance assessment. IEEE Internet Computing 17, 4 (2013), 32­38.",0,,False
317,"[11] Kevyn Collins-Thompson, Craig Macdonald, Paul Bennett, Fernando Diaz, and Ellen M Voorhees. 2015. TREC 2014 Web Track Overview. In Proceedings of the Twenty-Third NIST Text REtrieval Conference (TREC).",1,TREC,True
318,"[12] Alexander Dawid and Allan Skene. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied statistics (1979), 20­28.",0,,False
319,"[13] Tanya Goyal, Tyler McDonnell, Mucahid Kutlu, Tamer Elsayed, and Matthew Lease. 2018. Your Behavior Signals Your Reliability: Modeling Crowd Behavioral Traces to Ensure Quality Relevance Annotations. In 6th AAAI Conference on Human Computation and Crowdsourcing (HCOMP). 10 pages.",0,,False
320,[14] Catherine Grady and Matthew Lease. 2010. Crowdsourcing Document Relevance Assessment with Mechanical Turk. In Proc. of the NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk. 172­179.,1,ad,True
321,"[15] Maura R Grossman and Gordon V Cormak. 2012. Inconsistent responsiveness determination in document review: Difference of opinion or human error. Pace L. Rev. 32 (2012), 267.",0,,False
322,"[16] Maram Hasanain, Reem Suwaileh, Tamer Elsayed, Mucahid Kutlu, and Hind Almerekhi. 2017. EveTAR: building a large-scale multi-task test collection over Arabic tweets. Information Retrieval Journal (21 Dec 2017).",0,,False
323,"[17] Chien-Ju Ho, Aleksandrs Slivkins, Siddharth Suri, and Jennifer Wortman Vaughan. 2015. Incentivizing high quality crowdwork. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 419­429.",0,,False
324,"[18] Gabriella Kazai, Nick Craswell, Emine Yilmaz, and Seyed MM Tahaghoghi. 2012. An analysis of systematic judging errors in information retrieval. In Proceedings of the 21st ACM conference on Information and knowledge management. 105­114.",0,,False
325,"[19] Kenneth A. Kinney, Scott B. Huffman, and Juting Zhai. 2008. How Evaluator Domain Expertise Affects Search Result Relevance Judgments. In Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM '08). ACM, New York, NY, USA, 591­598. https://doi.org/10.1145/1458082.1458160",0,,False
326,"[20] Tyler McDonnell, Mucahid Kutlu, Tamer Elsayed, and Matthew Lease. 2017. The Many Benefits of Annotator Rationales for Relevance Judgments. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17). AAAI, 4909­4913.",0,,False
327,"[21] Tyler McDonnell, Matthew Lease, Mucahid Kutlu, and Tamer Elsayed. 2016. Why is That Relevant? Collecting Annotator Rationales for Relevance Judgments. In Proceedings of the 4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP). AAAI, 139­148. Best Paper Award.",0,,False
328,"[22] Jean-Francois Paiement, James G Shanahan, and Remi Zajac. 2010. Crowdsourcing local search relevance. In Proceedings of CrowdConf.",0,,False
329,"[23] V Pavlu and J Aslam. 2007. A practical sampling strategy for efficient retrieval evaluation. Technical Report. Technical report, Northeastern University.",0,,False
330,"[24] Falk Scholer, Diane Kelly, Wan-Ching Wu, Hanseul S Lee, and William Webber. 2013. The effect of threshold priming and need for cognition on relevance calibration and assessment. In Proceedings of the 36th ACM SIGIR conference on Research and development in information retrieval. 623­632.",0,,False
331,"[25] Falk Scholer, Andrew Turpin, and Mark Sanderson. 2011. Quantifying test collection quality based on the consistency of relevance judgements. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM, 1063­1072.",0,,False
332,"[26] Eero Sormunen. 2002. Liberal relevance criteria of TREC-: Counting on negligible documents?. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 324­330.",1,TREC,True
333,"[27] Ellen M Voorhees. 2000. Variations in relevance judgments and the measurement of retrieval effectiveness. Information processing & mgmt. 36, 5 (2000), 697­716.",0,,False
334,"[28] Simon Wakeling, Martin Halvey, Robert Villa, and Laura Hasler. 2016. A comparison of primary and secondary relevance judgements for real-life topics. In Proc. of the ACM on Conf. on Human Information Interaction and Retrieval. 173­182.",0,,False
335,"[29] Yunjie Xu and Zhiwei Chen. 2006. Relevance Judgment: What Do Information Users Consider Beyond Topicality? Journal of the American Society for Information Science and Technology (JASIS&T) 57, 7 (May 2006), 961­973.",0,,False
336,"[30] Emine Yilmaz, Javed A Aslam, and Stephen Robertson. 2008. A new rank correlation coefficient for information retrieval. In Proceedings of the 31st annual ACM SIGIR conference on Research & development in information retrieval. 587­594.",0,,False
337,"[31] Yinglong Zhang, Jin Zhang, Matthew Lease, and Jacek Gwizdka. 2014. Multidimensional relevance modeling via psychometrics and crowdsourcing. In Proc. of the 37th ACM SIGIR conference on R&D in information retrieval. 435­444.",0,,False
338,814,0,,False
339,,0,,False

,sentence,label,data,regex
0,Short Research Papers II,0,,False
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Query Performance Prediction Focused on Summarized Letor Features,1,Query,True
3,Adrian-Gabriel Chifu,0,,False
4,"Aix Marseille Univ, Université de Toulon, CNRS, LIS Marseille, France",0,,False
5,adrian.chifu@univ-amu.fr,1,ad,True
6,Léa Laporte,0,,False
7,"INSA Lyon, LIRIS, UMR 5205 CNRS Lyon, France",0,,False
8,lea.laporte@insa-lyon.fr,0,,False
9,Josiane Mothe,0,,False
10,"ESPE, Université de Toulouse, IRIT, UMR5505 CNRS Toulouse, France",0,,False
11,Josiane.Mothe@irit.fr,0,,False
12,ABSTRACT,0,,False
13,"Query performance prediction (QPP) aims at automatically estimating the information retrieval system effectiveness for any user's query. Previous work has investigated several types of pre- and post-retrieval query performance predictors; the latter has been shown to be more effective. In this paper we investigate the use of features that were initially defined for learning to rank in the task of QPP. While these features have been shown to be useful for learning to rank documents, they have never been studied as query performance predictors. We developed more than 350 variants of them based on summary functions. Conducting experiments on four TREC standard collections, we found that Letor-based features appear to be better QPP than predictors from the literature. Moreover, we show that combining the best Letor features outperforms the state of the art query performance predictors. This is the first study that considers such an amount and variety of Letor features for QPP and that demonstrates they are appropriate for this task.",1,Query,True
14,CCS CONCEPTS,0,,False
15,· Information systems  Retrieval effectiveness; Information retrieval query processing;,0,,False
16,KEYWORDS,0,,False
17,"Query performance prediction, Query difficulty prediction, Query features, Post retrieval features, Letor features",1,Query,True
18,"ACM Reference Format: Adrian-Gabriel Chifu, Léa Laporte, Josiane Mothe, and Md Zia Ullah. 2018. Query Performance Prediction Focused on Summarized Letor Features . In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210121",1,Query,True
19,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210121",1,ad,True
20,Md Zia Ullah,0,,False
21,"UPS, Université de Toulouse, IRIT, UMR5505 CNRS Toulouse, France mdzia.ullah@irit.fr",0,,False
22,1 INTRODUCTION,1,DUC,True
23,"While Information Retrieval system effectiveness is usually measured as an average effectiveness over queries, it is well-known that a system performs differently on each individual queries. Query performance prediction (QPP) aims at automatically estimating the information retrieval system effectiveness for any user's query difficulty without relevance judgement [7]. Predicting query performance is a first mandatory step to be able to adapt query processing when the query is detected as difficult. System adaptation can consist in selective query expansion [2], or selecting the best system configurations depending on the query features [9], for instance. Thus, having accurate predictors is a hot and very important topic.",1,Query,True
24,"The literature of the field has investigated the use of various types of predictors. Pre-retrieval features can be calculated from the query itself. IDF is an example of a pre-retrieval feature which is calculated by extracting the IDF of each query term from the inverted file and then aggregating the values over the query terms [11]. Some pre-retrieval features need external resources to be calculated as for example the number of query term senses based on WordNet [15]. On the other hand, calculating post-retrieval features requires evaluating the query over the whole collection of documents. They are usually based on functions applied to the document scores or retrieved document lists. For example, Query Feedback (QF) measures the overlap between two retrieved document lists for the original query and the expanded query [22]. Although postretrieval features are based on retrieved document scores, they are not Letor features strictly speaking. Indeed, Letor features have been first used in learning to rank applications where they have been shown to be effective [4, 6, 17, 19]. Letor features have also been used in other applications such as learning to rank system configurations [9] or reducing verbose textual queries [3], but have never been used as QPP. A systematic review does not yet exist in the context of QPP; this is the purpose of this short paper.",1,Query,True
25,"Recent work has focused on ways of combining predictors rather than finding better predictors [10, 18, 22]. Although combining features has been shown to be effective, we believe that the better the features, the better the QPP. Our paper revisits this perspective by evaluating more than 350 post-retrieval features (based on variants of Letor-based features) as QPP features. The paper focuses on the post-retrieval features since they have been shown to be more effective than pre-retrieval features for QPP [21].",0,,False
26,1177,0,,False
27,Short Research Papers II,0,,False
28,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
29,2 SUMMARIZED LETOR-BASED FEATURES,0,,False
30,"Letor features have been widely used in the context of learning to rank [17]. The main goal of learning to rank is to learn a function to better rank retrieved documents given a query; which is a different problem than QPP. Terrier's FAT component [14] implements most of the LETOR feature that can be found in [17]1 as well as a few others2. Most of the features are calculated from a matching score between the query and the document, either considering the document title or the full content, for a total of 39 features.",0,,False
31,"All the features are associated with query-document pairs. To make the Letor features (LF) usable as query features, we have used different summary functions over the retrieved documents, for a given query. More formally, let q be a query in the set of all queries Q and Dq,n be the set of the top-ranked n retrieved documents for the query q. We compute the i-th summarized Letor feature, SLFi (S, q) as follows:",0,,False
32,"SLFi (S, q) ,"" S({LFi (d, q)}, q, Dq,n )""",0,,False
33,"where d  Dq,n is a document, {LFi (d, q)} is the set of values for the Letor feature i for each couple (d, q) and S is a summary function with S  {Min, Max, Mean, Q1, Median, Q3, Std, V ar , and Sum}.",0,,False
34,"We used 9 summary functions for each LF: - Min , Max , Mean , Sum : the minimum (maximum, average, and sum) value of the Letor feature over the retrieved documents; - Q1, meadian , Q3: after calculating the Letor feature for each retrieved document, we sorted the values in increasing order; then the set is divided into quartiles. Q1 (respectively median, Q3) is the value that makes at least one quarter (2 quarters, 3 quarters) of the values having a score lower than Q1 (respectively median, Q3); - Std , V ar : the standard deviation and the variance of the Letor feature values. Each of the summary functions leads to a variant of the Letor feature. Thus, we have a group of summarized Letor features (SLF) for each LF. As opposed to previous research that analyses a few features, in this study, we analysed more than 350 SLF.",1,ad,True
35,3 EVALUATION OF THE FEATURES,0,,False
36,"The accuracy of a query performance predictor is usually measured in the literature by evaluating the link (e.g. correlation value) between the predictor values (considered as the predicted effectiveness) and the actual system effectiveness values [11, 18]. Pearson correlation is usually considered; it assumes a linear correlation between the two analyzed variables. Since correlation may exist without being linear, we complete the study using Kendall correlation as well.",0,,False
37,"The results can be biased by the choice of the system used as a reference. What is now a common reference in QPP is Language Modeling with Dirichlet smoothing and µ,""1000 [8, 18, 20, 22]. We also focus on this system in this paper and keep the analysis of system dependency for future work. With regard to system effectiveness, we considered AP and NDCG, which is also common practice in IR evaluation. Finally, we use four reference collections from TREC: Robust (528,155 newspaper documents, 250 topics), WT10G (1.6 million web/blog documents, 100 topics), GOV2 (25""",1,AP,True
38,"1Features name are the ones used in Terrier; they correspond to different implementations of features number 3, 5, 13, 15, 20, 23, 25, 33, and 35 in Table 2 from [17]. 2All the features are detailed at http://www.terrier.org/docs/v4.0/javadoc/index.html? org/terrier/matching/models/WeightingModel.html",1,ad,True
39,"million web documents, 150 topics), and ClueWeb12B (52 million web documents, 100 topics).",1,ClueWeb,True
40,3.1 Individual effectiveness,0,,False
41,"We studied the individual effectiveness for QPP of any single summarized Letor feature. We also considered the state of the art QPP features, namely: Clarity [8], QF [22], WIG [22], UQC [21], and NQC [21].",1,WIG,True
42,"Since we have more than 350 SLF, it was not possible to present all the correlations; we selected the most correlated features. For each collection, we selected the top 4 features and reported their correlation for the 4 collections in Table 1. In the first part of the table, we present AP while the second part reports NDCG (both @1000). The features are ordered according to decreasing Pearson correlation on GOV2.",1,AP,True
43,"In Table 1, we can see that many of the SLF are more correlated than state of the art QPP features, at least on one of the collections, but many times across several collections. The most correlated feature (AP and NDCG) is WMODEL.DFIZ_std which represents a weighting model based on the standardized distance from independence in term frequency [16]. WMODEL.ML2_std (second feature for AP) represents a weighting model based on multinomial randomness model, with Laplace after-effect model and normalisation 2 [12]. WMODEL.In_expC2_max (second feature for NDCG) is the Inverse Expected Document Frequency weighting model with Bernoulli after-effect and normalization [1]. When considering the 4 different collections, the 2 correlation measures, the 2 evaluation measures, and all the SLFs, we found out that several of them consistently correlate significantly across collections and measures. This is the case for the first 5 SLFs in Table 1. None of the state of the art features are as robust across collections and measures. The state of the art feature that has a similar robustness is QF, but it is not significantly correlated for AP on ClueWeb12B. We also observed that the highest correlations are with SLF calculated on document content rather than on part of it such as the title only.",1,AP,True
44,3.2 The impact of n,0,,False
45,"In the previous section, the SLF have been calculated when considering the n (1000) top-ranked retrieved documents. However, the value of n may have an important impact on the correlation [21]. It was not possible to report the effect of n on all the SLF. Since the idea was to observe the influence of n if any, we chose the first SLF from Table 1, namely WMODEL:DFIZ_std. It corresponds to the feature from Terrier calculated using Divergence From Independence model based on Standardization [13], to which we applied standard deviation when summarizing the values across documents and we showed it is robust across collections for QPP.",0,,False
46,Table 2 reports the Pearson correlation that we obtained on the four collections. We can see that the best value of n is not consistent across collections while it is consistent for AP and NDCG. GOV2 requires fewer documents (top 500) to get the highest correlation than the other collections. This result holds also for other SLFs we analysed. An interesting track for future work is to analyse whether the optimal number of documents is the same across features or not. The robustness of the number of documents across queries is also an interesting problem to study.,1,AP,True
47,1178,0,,False
48,Short Research Papers II,0,,False
49,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
50,"Table 1: Pearson (P.) and Kendall (K.) correlations between the features and the actual system effectiveness (AP@1000 and NDCG@1000). The top four correlated features are selected from each collection separately and then all are ordered by decreasing order based on Pearson correlation on GOV2. We also included the features from the literature that are not Letor-based. ""*"" stands for p-value < 0.05, while ""+"" stands for p-value < 0.001. Values in bold are higher than QF baseline.",1,AP,True
51,AP WMODEL.DFIZ_std WMODEL.ML2_std WMODEL.In_expC2_max WMODEL.PL2_std WMODEL.In_expB2_max WMODEL.IFB2_max WMODEL.BB2_max WMODEL.IFB2_std WMODEL.DFReeKLIM_max WMODEL.DPH_max WMODEL.Js_KLs_max WMODEL.DFRee_max WMODEL.SFM.DirichletLM_max WMODEL.LemurTF_IDF_max WMODEL.LemurTF_IDF_std QF [22] Clarity [8] WIG [22] UQC [21] NQC [21] NDCG WMODEL.DFIZ_std WMODEL.In_expC2_max WMODEL.DFIC_std WMODEL.ML2_std WMODEL.IFB2_max WMODEL.PL2_max WMODEL.DPH_std WMODEL.Js_KLs_max WMODEL.BM25_std WMODEL.DFR_BM25_std WMODEL.XSqrA_M_Q3 WMODEL.XSqrA_M_mean WMODEL.SFM.DirichletLM.._std WMODEL.LemurTF_IDF_std QF [22] WIG [22] UQC [21] Clarity [8] NQC [21],1,AP,True
52,Robust P. K. .191* .200+ .251+ .220+ .320+ .359+ .334+ .296+ .316+ .353+ .318+ .353+ .298+ .344+ .393+ .337+ .343+ .287+ .347+ .292+ .305+ .260+ .295+ .251+ .392+ .324+ .414+ .348+ .457+ .356+ .432+ .343+ .435+ .308+ .315+ .253+ .496+ .358+ .125* .203+ P. K. .306+ .235+ .338+ .359+ .291+ .221+ .326+ .248+ .334+ .351+ .434+ .329+ .380+ .296+ .364+ .271+ .428+ .338+ .429+ .339+ -.055 -.040 -.066 -.047 .354+ .280+ .440+ .340+ .504+ .349+ .364+ .262+ .436+ .343+ .452+ .308+ .117 .191+,1,Robust,True
53,WT10G P. K. .217* .235+ .216* .290+ .403+ .298+ .211* .297+ .389+ .296+ .399+ .304+ .374+ .277+ .318* .310+ .313* .202* .323* .223* .266* .164* .252* .152* .235* .210* .342+ .282+ .207* .279+ .343+ .169* .239* .189* .181 .122 .311* .192* .010 .066 P. K. .321* .261+ .429+ .279+ .294* .221* .290* .294+ .420+ .282+ .389+ .258+ .432+ .342+ .280* .161* .320* .295+ .324* .298+ -.030 -.032 -.059 -.046 .430+ .253+ .206* .267+ .350+ .223* .228* .149* .304* .226+ .255* .170* .065 .106,1,WT,True
54,GOV2 P. K. .453+ .305+ .453+ .328+ .449+ .306+ .449+ .319+ .438+ .297+ .437+ .293+ .436+ .300+ .394+ .260+ .385+ .277+ .385+ .276+ .367+ .262+ .359+ .255+ .350+ .270+ .298+ .251+ .258* .230+ .407+ .285+ .112 .086 .302+ .239+ .286+ .191+ -.037 .086 P. K. .447+ .334+ .444+ .326+ .441+ .328+ .438+ .359+ .429+ .312+ .412+ .322+ .397+ .308+ .382+ .289+ .344+ .289+ .341+ .290+ .335+ .217+ .308+ .202+ .302+ .215+ .243* .247+ .398+ .297+ .329+ .273+ .285+ .213+ .111 .100 -.004 .091,0,,False
55,ClueWeb12B P. K.,1,ClueWeb,True
56,.298* .255+ .265* .256+ .303* .225+ .267* .255+ .304* .228+ .298* .219* .308* .228+ .231* .232+ .376+ .259+ .374+ .253+ .379+ .253+ .377+ .249+ .263* .163* .223* .186* .170 .207* .163 .114 -.148 -.108 .366+ .201* .152 .064 -.095 -.047,0,,False
57,P. K. .301* .239+ .264* .200* .303* .241+ .264* .230+ .257* .195* .324+ .235+ .238* .195* .361+ .243+ .162 .174* .160 .173* .363+ .245+ .359+ .235+ .200* .172* .114 .151* .210* .152* .342+ .186* .083 .020 -.132 -.099 -.143 -.069,0,,False
58,3.3 Combining features for QPP,0,,False
59,"Assuming that different QPP features capture diverse information about query performance, whether a combination of features is likely to be a better predictor than a single feature. Following this assumption, a few studies have investigated the linear combination of features [5, 10, 20, 22]. These studies combine less than 10 features.",0,,False
60,"Considering that we have investigated more than 350 SLFs in the individual effectiveness analysis, using all these features in",0,,False
61,"a single model such as linear regression would not be optimum. In this paper, we choose to analyse the linear combination of the features that correlate the most with the effectiveness measure to predict, for each collection. Thus, for each collection, we plotted the correlations values for the best 20 features in decreasing order, then empirically searched for a gap in the correlation values, by looking at the differences between consecutive correlation values, two by two. We thus selected all the features before this gap. Note that the features may be different depending on whether we consider",0,,False
62,1179,0,,False
63,Short Research Papers II,0,,False
64,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
65,"Table 2: Pearson correlation of the WMODEL:DFIZ_std [13] SLF predictor according to n, the number of top-ranked retrieved documents considered when calculating the feature.",0,,False
66,n,0,,False
67,5 10 50 100 500 1000,0,,False
68,Robust,1,Robust,True
69,.056 .065 .089 .081 .146* .191*,0,,False
70,AP,1,AP,True
71,WT10G GOV2,1,WT,True
72,.027 -.084 .163 .142 .175 .217* .261* .385+ .409+ .407+ .453+ .453+,0,,False
73,ClueWeb12B .320* .255* .266* .243* .269* .298*,1,ClueWeb,True
74,Robust,1,Robust,True
75,.081 .119 .183* .191* .252+ .306+,0,,False
76,NDCG,0,,False
77,WT10G GOV2,1,WT,True
78,.069 -.032 .224* .217* .294* .321* .285+ .397+ .426+ .418+ .453+ .447+,0,,False
79,ClueWeb12B .246* .234* .253* .224* .265* .301*,1,ClueWeb,True
80,"correlation on AP or NDCG. For AP, this process selected 5 features per collection, except for ROBUST (6 features). For NDCG, using the same process we selected 6 features for WT10G and ClueWeb12B, 8 features for Robust, and 10 features for GOV2. We leave the investigation of other feature selection methods such as LASSO (more sparse) and other machine learning methods such as random forest for future work since our main topic here is to investigate the effectiveness of new predictors.",1,AP,True
81,"Similarly to [22] and [10], we used multiple linear regression to combine the features into a single performance prediction model. We considered five different sets of predictors to be combined. A first set S1 contains the two state of the art predictors WIG and QF. It corresponds to our baseline. The second set S2 is composed of the two best Letor predictors on a given collection, according to the simple linear regression experiment. This set can be easily compared to the baseline since it is also composed of two features (thus the same level of complexity and costs to be calculated). The third set S3 contains all the best LETOR features per collection we selected as explained previously. The fourth set S4 is the union of sets S1 and S2, thus it combines the state of the art and the two best SLF features per collection. Finally, we considered all the best predictors selected per collection together with the two baselines WIG and QF in the fifth set S5. We considered the same collections, system, effectiveness, and evaluation measures as in the individual performance prediction analysis. We performed the leave-one-out cross-validation to predict query performance. As shown in Table 3,",1,WIG,True
82,Table 3: Performance of linear regression of combined postretrieval predictors according to Pearson correlation.,0,,False
83,Combination,0,,False
84,S1: WIG + QF S2: 2 Best SLF,1,WIG,True
85,AP S3: Best SLF,1,AP,True
86,S4: S1  S2,0,,False
87,S1  S3: All,0,,False
88,S1: WIG + QF,1,WIG,True
89,S2: 2 Best SLF,0,,False
90,NDCG,0,,False
91,S3: S4:,0,,False
92,Best SLF S1  S2,0,,False
93,S5: S1  S3,0,,False
94,Robust WT10G GOV2 ClueWeb .459+ .274* .399+ .287* .382+ .404+ .438+ .237* .402+ .339+ .420+ .302* .478+ .420+ .465+ .260* .454+ .427+ .509+ .208* .537+ .303* .405+ .286* .430+ .449+ .469+ .211* .458+ .457+ .464+ .312* .556+ .468+ .514+ .293* .526+ .446+ .487+ .188,1,Robust,True
95,"for all the collections except ClueWeb12B and NDCG, the best correlation values are obtained when considering a combination of the two best Letor features and the two state of the art features. Considering ClueWeb12B and NDCG, the best correlation value is",1,ClueWeb,True
96,obtained when considering a linear combination of the best Letor,0,,False
97,"features. If we take a fine-grained look at the results, we observe",0,,False
98,that the best results are obtained when considering either the Letor,0,,False
99,"features alone or combined with WIG and QF, whatever the ef-",1,WIG,True
100,"fectiveness measure is. As an additional analysis, we computed",1,ad,True
101,the correlations between the features from the literature and the,0,,False
102,"best Letor predictors. The correlations values were not significant,",0,,False
103,supporting our finding that Letor features are complementary to,0,,False
104,"state-of-the-art ones. Finally, with regard to AP, results are not di-",1,AP,True
105,"rectly comparable to Raiber's [18] (they found Pearson correlation of .557 for Robust, .346 for WT10G, .570 for GOV2) since we could",1,Robust,True
106,"not reproduce their results; for example, the Pearson correlation",0,,False
107,values they reported for individual state of the art QPPs are higher,0,,False
108,than the ones we re-calculated using the same collections and the,0,,False
109,same reference system.,0,,False
110,4 CONCLUSION,0,,False
111,We showed that the Summarized Letor features we defined are,0,,False
112,good query performance predictors and that some of them are,0,,False
113,more robust than the ones from the literature across collections.,0,,False
114,"Moreover, we found that they are complementary to the existing",0,,False
115,features as when combined we achieved better QPP.,0,,False
116,REFERENCES,0,,False
117,"[1] Giambattista Amati. 2003. Probability Models for IR based on Divergence from Randomness. Department of CS PhD (2003), 182.",0,,False
118,"[2] G. Amati, C. Carpineto, and G. Romano. 2004. Query difficulty, robustness, and selective application of query expansion. In ECIR. 127­137.",1,Query,True
119,"[3] Jaime Arguello, Sandeep Avula, and Fernando Diaz. 2017. Using Query Performance Predictors to Reduce Spoken Queries. In ECIR. Springer, 27­39.",1,Query,True
120,[4] Niranjan Balasubramanian and James Allan. 2010. Learning to select rankers. In ACM SIGIR. 855­856.,0,,False
121,"[5] Shariq Bashir. 2014. Combining pre-retrieval query quality predictors using genetic programming. Applied Intelligence 40, 3 (2014), 525­535.",0,,False
122,"[6] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In ICML. 129­136.",0,,False
123,"[7] David Carmel and Elad Yom-Tov. 2010. Estimating the query difficulty for IR. Syn. Lect. on Inf. Concepts, Retrieval, and Services (2010), 1­89.",1,ad,True
124,[8] Steve Cronen-Townsend and W. Bruce Croft. 2002. Quantifying Query Ambiguity. In Conference on Human Language Technology Research. 104­109.,1,Query,True
125,"[9] Romain Deveaud, Josiane Mothe, and Jian-Yun Nie. 2016. Learning to rank system configurations. In CIKM. ACM, 2001­2004.",0,,False
126,"[10] Claudia Hauff. 2010. Predicting the effectiveness of queries and retrieval systems. SIGIR Forum 44, 1 (2010), 88. https://doi.org/10.1145/1842890.1842906",0,,False
127,"[11] Claudia Hauff, Djoerd Hiemstra, and Franciska de Jong. 2008. A survey of preretrieval query performance predictors. In ACM CIKM. 1419­1420.",0,,False
128,"[12] lker Kocaba, Bekir Taner Dinçer, and Bahar Karaolan. 2014. A Nonparametric Term Weighting Method for Information Retrieval Based on Measuring the Divergence from Independence. Inf. Retr. 17, 2 (April 2014), 153­176.",0,,False
129,"[13] lker Kocaba, Bekir Taner Dinçer, and Bahar Karaolan. 2014. A nonparametric term weighting method for information retrieval based on measuring the divergence from independence. Information retrieval 17, 2 (2014), 153­176.",0,,False
130,"[14] Craig Macdonald, Rodrygo LT Santos, Iadh Ounis, and Ben He. 2013. About learning models with multiple query-dependent features. TOIS 31, 3 (2013).",1,ad,True
131,"[15] Josiane Mothe and Ludovic Tanguy. 2005. Linguistic features to predict query difficulty. In Predicting query difficulty, ACM SIGIR workshop. 7­10.",0,,False
132,[16] Vassilis Plachouras and Iadh Ounis. 2007. Multinomial Randomness Models for Retrieval with Document Fields. In ECIR. 28­39.,1,ad,True
133,"[17] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. LETOR: A benchmark collection for research on learning to rank for IR. Information Retrieval 13, 4 (2010), 346­374.",0,,False
134,"[18] Fiana Raiber and Oren Kurland. 2014. Query-performance prediction: setting the expectations straight. In ACM SIGIR. ACM, 13­22.",1,Query,True
135,"[19] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. 2011. LambdaMerge: merging the results of query reformulations. In ACM WSDM. 795­804.",0,,False
136,"[20] Anna Shtok, Oren Kurland, and David Carmel. 2010. Using statistical decision theory and relevance models for QPP. In ACM SIGIR. 259­266.",0,,False
137,"[21] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits. 2012. Predicting query performance by query-drift estimation. ACM TOIS 30, 2 (2012).",1,ad,True
138,[22] Yun Zhou and W Bruce Croft. 2007. Query performance prediction in web search environments. In ACM SIGIR. 543­550.,1,Query,True
139,1180,0,,False
140,,0,,False

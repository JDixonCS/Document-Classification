,sentence,label,data,regex
0,Demonstration Papers II,0,,False
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,A System for Efficient High-Recall Retrieval,0,,False
3,"Mustafa Abualsaud, Nimesh Ghelani, Haotian Zhang, Mark D. Smucker, Gordon V. Cormack, and Maura R. Grossman",0,,False
4,"University of Waterloo, Ontario, Canada {m2abuals,nghelani,haotian.zhang,mark.smucker,gvcormac,maura.grossman}@uwaterloo.ca",1,ad,True
5,ABSTRACT,0,,False
6,"The goal of high-recall information retrieval (HRIR) is to find all or nearly all relevant documents for a search topic. In this paper, we present the design of our system that affords efficient highrecall retrieval. HRIR systems commonly rely on iterative relevance feedback. Our system uses a state-of-the-art implementation of continuous active learning (CAL), and is designed to allow other feedback systems to be attached with little work. Our system allows users to judge documents as fast as possible with no perceptible interface lag. We also support the integration of a search engine for users who would like to interactively search and judge documents. In addition to detailing the design of our system, we report on user feedback collected as part of a 50 participants user study. While we have found that users find the most relevant documents when we restrict user interaction, a majority of participants prefer having flexibility in user interaction. Our work has implications on how to build effective assessment systems and what features of the system are believed to be useful by users.",1,ad,True
7,CCS CONCEPTS,0,,False
8,· Information systems  Information retrieval;,0,,False
9,KEYWORDS,0,,False
10,High-Recall; Electronic Discovery; Systematic Review,0,,False
11,"ACM Reference Format: Mustafa Abualsaud, Nimesh Ghelani, Haotian Zhang, Mark D. Smucker, Gordon V. Cormack, and Maura R. Grossman. 2018. A System for Efficient High-Recall Retrieval. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­ 12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https: //doi.org/10.1145/3209978.3210176",0,,False
12,1 INTRODUCTION,1,DUC,True
13,"The objective of high-recall information retrieval (HRIR) is to find all or nearly all relevant documents. Example applications of HRIR include electronic discovery (eDiscovery), systematic review and construction of information retrieval test collections.",0,,False
14,"Baseline Model Implementation (BMI) ­ a version of CAL (AutoTAR) ­ is the state-of-the-art approach to high-recall retrieval [2, 3, 5]. CAL uses a machine learning model with an iterative feedback process to retrieve documents. Our work builds on BMI.",0,,False
15,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210176",1,ad,True
16,Figure 1: A high-level view of our platform architecture.,0,,False
17,We demonstrate a platform for retrieving and assessing relevant documents that provides high data processing performance and a user-friendly document assessment interface. The platform is designed to increase assessors' assessment performance and thus reduce their review effort.,0,,False
18,"The performance of the platform was evaluated by 50 participants in a user experiment. In another work [10], we report on the design of user study and show preliminary results from the first 10 participants. In this paper, we show the data-processing performance of our platform, how it works, and how it is used and experienced by users.",0,,False
19,2 SYSTEM ARCHITECTURE,0,,False
20,"Figure 1 shows the architecture of our platform. Our platform is composed of two different retrieval methods, CAL and Search. CAL automatically presents the user with documents based on a machine learning model, whereas Search allows users to enter queries and retrieve lists of documents. The platform server, as shown in the Figure 1, is responsible for displaying the document assessment interfaces to the user, configuring the system, and communicating requests and responses to and from each component. The types of user interactions that the server allows are:",0,,False
21,· Set or change the topic of interest; · Set seed query for building CAL's initial machine learning,0,,False
22,model; · Search for specific documents using Search; · Assess documents retrieved by either Search or CAL; and,0,,False
23,1317,0,,False
24,Demonstration Papers II,0,,False
25,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
26,(a) Topic Description,0,,False
27,Clicking on the text will display a pop-up of the topic statement of what is considered relevant. Users can also create their own topics and their statements.,0,,False
28,(b) Navigation Buttons,0,,False
29,"Users can click on the magnifying icon to switch to the Search component or the light bulb icon to move to the CAL component. The archive icon takes the user to a page that contains a list of all judgments made, where users can export their judgments to different formats.",1,ad,True
30,(c) Document Title,0,,False
31,The title of the document from the data collection,0,,False
32,(d) Paragraph Excerpt,0,,False
33,A selected paragraph by the modified CAL model. The model selects the most-likely relevant paragraph from the document as a summary of the document.,0,,False
34,(e) Show full document,0,,False
35,Users can view the full document content if they wish. The content will be displayed below the paragraph.,0,,False
36,(f) Highlight keywords,0,,False
37,Users can highlight keywords by entering them in the search bar.,0,,False
38,(g) Judging buttons,0,,False
39,"Three buttons corresponding to 3-level relevance scale, not relevant, relevant and highly relevant",0,,False
40,(h) Keyboard shortcuts,0,,False
41,Users can make their judgments using any of the pre-defined keyboard shortcuts.,0,,False
42,(i) Undo Judgments,0,,False
43,"A list of all previously made judgments from the CAL interface. Users can modify their existing judgments by clicking on any of document titles. Once clicked, the document will be displayed again to the user.",1,ad,True
44,Figure 2: The continuous active learning (CAL) user interface.,0,,False
45,· Export judgments sets.,0,,False
46,"All components in the architecture are stand-alone and interact with each other via HTTP API. For example, any search engine can be added to our system with minimal effort. This design also adds flexibility to these components and permits their use in different applications. For instance, the platform server was also used in the TREC 2017 RTS track evaluation for collecting judgments of tweets from the assessors [6]. The CAL component also has a command line interface through which various simulation experiments can be conducted.",1,AP,True
47,"The platform server is built using Django, a Python web framework. CAL and Search are written in C++. The source code is publicly available1.",0,,False
48,3 PLATFORM COMPONENTS & FEATURES,0,,False
49,"Our implementation of continuous active learning (CAL) is based on the Baseline Model Implementation (BMI)2, which served as the baseline method in the TREC Total Recall Track 2015 and 2016 [4, 7]. BMI provides an iterative feedback process and uses a logistic regression classifier trained on relevance judgments from assessors. BMI uses this classifier to present the assessor with top scoring unjudged documents in the collection. After each iteration of judging and re-training, the learning model improves and returns the next most likely relevant documents to the user. We modified BMI as follows:",1,TREC,True
50,1 https://cs.uwaterloo.ca/~m2abuals/code/SIGIR2018/ 2 http://cormack.uwaterloo.ca/trecvm/,1,trec,True
51,"· Paragraph-based: A single paragraph is usually short and can contain enough material for a document to be assessed as relevant. Assessing the relevance of a document using a single paragraph can increase the total number of relevant documents found within a limited time and reduce review effort [10, 11]. Our CAL implementation retrieves the next likely relevant paragraph and presents it to the user for assessment, along with the option to view the full document content.",0,,False
52,"· Frequency of model retraining: The original algorithm processes relevance feedback in batches of size k, where k increases exponentially. Although having an increasing batch size reduces computation cost, it can delay the classifier from exploiting the potential benefits of newer relevance judgments. We made several improvements to the original algorithm to allow relevance feedback to be efficiently processed after each single judgment.",1,ad,True
53,"To meet the performance requirements of our modified algorithm, we implemented BMI in C++. In addition to using efficient data structures and parallel operations, we store all document and paragraph representations in memory. This enables quick training and scoring of documents/paragraphs. For the training of our logistic regression classifier, we used Sofia-ML3. We modified the Sofia-ML library to remove unnecessary features and the performance costs associated with them.",1,ad,True
54,"We compared our implementation by simulating the original BMI algorithm on the TREC 2017 Common Core test collection [1]. With around 1.8 million documents in the collection, the original",1,TREC,True
55,3 https://code.google.com/archive/p/sofia-ml/,0,,False
56,1318,0,,False
57,Demonstration Papers II,0,,False
58,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
59,(a) Topic Description,0,,False
60,Clicking on the text will display a pop-up of the topic statement of what is considered relevant.,0,,False
61,(b) Navigation buttons,0,,False
62,"To navigate between Search, CAL, or archive to export all judgments.",0,,False
63,(c) Search bar,0,,False
64,"The title of the document from the data collection. Users are also able to specify phrases (""new york"") or require words (+france) in their search result.",0,,False
65,(d) Relevance Indicator,0,,False
66,An indicator of the relevance judgment made by the user for the document. Any document that has been judged by either Search or CAL will have this vertical bar indicator.,1,ad,True
67,(e) Number of Results,0,,False
68,"The number of search results returned by the search engine. Users can choose to return 10, 20, 50, or 100 documents per query.",0,,False
69,(f) Judging buttons,0,,False
70,"Three buttons corresponding to 3-level relevance scale, not relevant, relevant and highly relevant.",0,,False
71,(g) Show full document,0,,False
72,Users can view the full document content by clicking on the search result. The full document content will be displayed in a popup.,0,,False
73,Figure 3: The search engine user interface.,0,,False
74,(h) Highlight keywords,0,,False
75,Users can highlight keywords by entering them in the search bar.,0,,False
76,(i) Keyboard shortcuts,0,,False
77,Users can make their judgments using any of the pre-defined keyboard shortcuts.,0,,False
78,"implementation took an average of 294.3 seconds to train the model and score all the documents. In contrast, our implementation took an average of 3.5 seconds (1 thread) and 1.6 seconds (8 threads). It should be noted that the original implementation loads all the document features from disk every time the classifier is trained, while our implementation loads it once and keeps it in memory for subsequent use. This one-time loading cost in our implementation was 31.1 seconds, which is still significantly faster than the original implementation.",1,ad,True
79,"We also measured the performance of our implementation by simulating the modified algorithm on the same collection. The training and scoring of around 30 million paragraphs took an average of 2.1 seconds after every relevance judgment. Since this process causes a noticeable delay for users, we immediately present the next paragraph based on the previous scores while the new scores are being generated by the model. To reduce latency for the user, the browser front-end caches the top 10 highest-scoring paragraphs received from the CAL component. After a user makes a judgment, we immediately present the user with the next paragraph in the cache. The cache is flushed and updated every time it receives a new batch of paragraphs from the CAL component.",0,,False
80,The modifications and the improvements we made increased the responsiveness of our system and removed any perceptible interface lag.,1,ad,True
81,"Figure 3 shows the interfaces of the Search component. We implemented the search component using Indri [9], but our platform is designed such that any search engine can be easily integrated. Our",0,,False
82,"platform interacts with the search engine via HTTP API. Provided a search query, the platform expects the search engine to respond with a ranked list of documents with their summaries.",1,AP,True
83,"In order to help assessors find relevant documents easily, we included features that we found helpful as part of our own usage with a prototype version of our system. The features and their description are shown in Table 1. Screenshots of the CAL interface and features are shown in Figures 2 and 3.",0,,False
84,4 USER STUDY DETAILS,0,,False
85,"We conducted a controlled user study to measure the performance our system. After receiving ethics approval from our university, we recruited 50 participants. The primary purpose of the study was to measure user performance with four variants of the system. A secondary purpose was to collect user feedback regarding the system and its features.",0,,False
86,4.1 Corpus and Topics,0,,False
87,"We used the 50 NIST topics and the corpus provided by the TREC 2017 Common Core Track [1]. The corpus is The New York Times Annotated Corpus [8], which contains over 1.8 million news articles.",1,TREC,True
88,4.2 System Variants,0,,False
89,"The participants experienced all variations of our system during the study. For each variation, the participant spent 1 hour using",0,,False
90,1319,0,,False
91,Demonstration Papers II,0,,False
92,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
93,Table 1: A list of features participants rated.,0,,False
94,Feature Keyword Highlighting Judgment Shortcuts Search Interface Topic Description Undo Judgments Full Document Content Advance Search,0,,False
95,"Description Keyword search within a document or paragraph Keyboard shortcuts for submitting relevance judgments Ability to use a search engine to find documents in addition to the learning interface Display of topic statement of what is considered relevant Ability to review recent judgments and change judgment Ability to view a full document rather than merely a paragraph summary For the search engine, the ability to specify phrases (""new york"") or require words (+france)",1,ad,True
96,"Example Figure 2f, 3h Figure 2h, 3i Figure 3 Figure 2a, 3a Figure 2i Figure 2e Figure 3c",0,,False
97,Percentage,0,,False
98,70,0,,False
99,Very Useless,0,,False
100,Somewhat Useless,0,,False
101,60,0,,False
102,Neutral,0,,False
103,Somewhat Useful,0,,False
104,50,0,,False
105,Very Useful,0,,False
106,40,0,,False
107,30,0,,False
108,20,0,,False
109,10,0,,False
110,0 Keyword HighJluigdhgtminegnt ShorStceuatrsch InterTfaocpeic DescripUtinodno JudgmFenutllsDoc ContAednvtance Search,0,,False
111,Figure 4: Percentage of user preference for different system features.,0,,False
112,Table 2: Percentage of participants preferring a given system variant.,0,,False
113,Treatments,0,,False
114,CAL-P CAL-D CAL-P&Search CAL-D&Search,0,,False
115,Percentage of participants,0,,False
116,16% 26% 10% 48%,0,,False
117,our system to find as many relevant documents as they could for a given topic. The system variations were:,0,,False
118,· CAL-P: CAL component with just paragraph summary. Search component is not enabled.,0,,False
119,· CAL-D: CAL component with paragraph summary and option to view the full document. Search component is not enabled.,0,,False
120,· CAL-P&Search: CAL-P with Search component enabled. · CAL-D&Search: CAL-D with Search component enabled.,0,,False
121,5 DISCUSSION,0,,False
122,"In this section, we compare the user performance and user feedback on different variants of our systems. At the conclusion of the study, we asked participants which system variant they preferred the most. 48% of study participants preferred CAL-D&Search over the more restrictive variants. Our participants want full control of a highly interactive system, but we found that performance is highest when their interactions are limited to producing relevance judgments on paragraph length excerpts.",0,,False
123,"Finally, we asked participants for their feedback on each of the system features in Table 1. We used a 5-point scale to rate each feature. The results are shown in Figure 4. The keyword highlighting feature is the most popular among all features, with 86% of users indicating that it was somewhat useful or very useful.",0,,False
124,6 CONCLUSION,0,,False
125,"In this paper, we described the design of an efficient high-recall information retrieval system. The system allows for both iterative relevance feedback and search, and these components can be easily replaced with different implementations. We found that while participants preferred a system that gives them the flexibility to view full documents and interactively search, actual user performance was maximized when we limited interaction to submitting relevance judgments on paragraph length excerpts.",0,,False
126,ACKNOWLEDGMENTS,0,,False
127,"This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (Grants CRDPJ 468812-14, RGPIN-2017-04239, and RGPIN-2014-03642), in part by a Google Founders' Award, and in part by the University of Waterloo.",1,ad,True
128,REFERENCES,0,,False
129,"[1] James Allan, Evangelos Kanoulas, Dan Li, Christophe Van Gysel, Donna Harman, and Ellen Voorhees. 2017. TREC 2017 Common Core Track Overview. In TREC.",1,TREC,True
130,[2] Gordon V Cormack and Maura R Grossman. 2014. Evaluation of machine-learning protocols for technology-assisted review in electronic discovery. In SIGIR.,0,,False
131,[3] Gordon V. Cormack and Maura R. Grossman. 2015. Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review. CoRR abs/1504.06868 (2015).,0,,False
132,"[4] Maura R Grossman, Gordon V Cormack, and Adam Roegiest. 2016. TREC 2016 Total Recall Track Overview. In TREC.",1,TREC,True
133,"[5] Maura R Grossman, Gordon V Cormack, and Adam Roegiest. 2017. Automatic and Semi-Automatic Document Selection for Technology-Assisted Review. In SIGIR.",0,,False
134,"[6] Jimmy Lin, Salman Mohammed, Royal Sequiera, Luchen Tan, Nimesh Ghelani, Mustafa Abualsaud, Richard McCreadie, Dmitrijs Milajevs, and Ellen Voorhees. 2017. Overview of the TREC 2017 Real-Time Summarization Track. In TREC.",1,ad,True
135,"[7] Adam Roegiest, Gordon V Cormack, Maura R Grossman, and C. L. A. Clarke. 2015. TREC 2015 Total Recall Track Overview. In TREC.",1,TREC,True
136,"[8] Evan Sandhaus. 2008. The New York Times Annotated Corpus. (October 2008). LDC Catalog No.: LDC2008T19, https://catalog.ldc.upenn.edu/ldc2008t19.",0,,False
137,"[9] Trevor Strohman, Donald Metzler, Howard Turtle, and W Bruce Croft. 2005. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, Vol. 2. Amherst, MA, USA, 2­6.",0,,False
138,"[10] Haotian Zhang, Mustafa Abualsaud, Nimesh Ghelani, Angshuman Ghosh, Mark D. Smucker, Gordon V. Cormack, and Maura R. Grossman. 2017. UWaterlooMDS at the TREC 2017 Common Core Track. In TREC.",1,TREC,True
139,"[11] Haotian Zhang, Gordon V Cormack, Maura R Grossman, and Mark D Smucker. 2018. Evaluating Sentence-Level Relevance Feedback for High-Recall Information Retrieval. arXiv (2018).",0,,False
140,1320,0,,False
141,,0,,False

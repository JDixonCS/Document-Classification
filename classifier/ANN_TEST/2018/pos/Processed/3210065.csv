,sentence,label,data,regex
0,Short Research Papers I,0,,False
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Pytrec_eval: An Extremely Fast Python Interface to trec_eval,1,trec,True
3,Christophe Van Gysel,0,,False
4,"University of Amsterdam Amsterdam, The Netherlands",0,,False
5,chris@stophr.be,0,,False
6,ABSTRACT,0,,False
7,"We introduce pytrec_eval, a Python interface to the trec_eval information retrieval evaluation toolkit. pytrec_eval exposes the reference implementations of trec_eval within Python as a native extension. We show that pytrec_eval is around one order of magnitude faster than invoking trec_eval as a sub process from within Python. Compared to a native Python implementation of NDCG, pytrec_eval is twice as fast for practically-sized rankings. Finally, we demonstrate its effectiveness in an application where pytrec_eval is combined with Pyndri and the OpenAI Gym where query expansion is learned using Q-learning.",1,trec,True
8,CCS CONCEPTS,0,,False
9,· Information systems  Evaluation of retrieval results;,0,,False
10,KEYWORDS,0,,False
11,"IR evaluation, toolkits",0,,False
12,"ACM Reference Format: Christophe Van Gysel and Maarten de Rijke. 2018. Pytrec_eval: An Extremely Fast Python Interface to trec_eval. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210065",1,trec,True
13,1 INTRODUCTION,1,DUC,True
14,"Evaluation is a crucial component of any information retrieval (IR) system [2]. Reusable test collections and off-line evaluation measures [7] have been the dominating paradigm for experimentally validating IR research for the last 30 years. The popularity and ubiquity of off-line IR evaluation measures is partly due to the Text REtrieval Conference (TREC) [5]. TREC led to the development of the trec_eval1 software package that is the standard tool for evaluating a collection of rankings. The trec_eval tool allows IR researchers to easily compute a large number of evaluation measures using standardized input and output formats. For a document collection, a test collection of queries with query/document relevance information (i.e., qrel) and a set of rankings generated by a particular IR system (i.e., a system run) for the test collection queries,",1,ad,True
15,Open-source implementation is available at https://github.com/cvangysel/pytrec_eval. 1 https://github.com/usnistgov/trec_eval,1,trec,True
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210065",1,ad,True
17,Maarten de Rijke,0,,False
18,"University of Amsterdam Amsterdam, The Netherlands",0,,False
19,derijke@uva.nl,0,,False
20,"trec_eval outputs a standardized output format containing evaluation measure values. The adoption of trec_eval as an integral part of IR research has led to the following benefits: (a) standardized formats for system rankings and query relevance information such that different research groups can exchange experimental results with minimal communication, and (b) open-source reference implementations of evaluation measures--provided by a third party (i.e., NIST)--that promotes transparent and consistent evaluation.",1,trec,True
21,"While the availability of trec_eval has brought many benefits to the IR community, it has the downside that it is available only as a standalone executable that is interfaced by passing files with rankings and ground truth information. In recent years, the Python programming language has risen in popularity due to its feature richness (i.e., scientific libraries and data structures) and holistic language design [3]. Research progresses at a rate proportional to the time it takes to implement an idea, and consequently, scripting languages (e.g., Python) are preferred over conventional programming languages [6]. Within IR research, retrieval systems are often implemented and optimized using Python (e.g., [4, 9]) and for their evaluation trec_eval is used. However, invoking trec_eval from Python is expensive as it involves (1) serializing the internal ranking structures to disk files, (2) invoking trec_eval through the operating system, and (3) parsing the trec_eval evaluation output from the standard output stream. This workflow is unnecessarily inefficient as it incurs (a) a double I/O cost when the ranking is first serialized by the Python script and subsequently parsed by trec_eval, and (b) a context-switching overhead as the invocation of trec_eval needs to be processed by the operating system.",1,trec,True
22,"We introduce pytrec_eval to counter these excessive efficiency costs and avoid a wild growth of ad-hoc Python-based evaluation measure implementations. pytrec_eval builds upon the trec_eval source code and exposes a Python-first interface to the trec_eval evaluation toolkit as a native Python extension. Rankings constructed in Python can directly be passed to the evaluation procedure, without incurring disk I/O costs; evaluation is performed using the original trec_eval implementation. Due to pytrec_eval's implementation as a native Python extension, context-switching overheads are avoided as the evaluation procedure and its invocation reside within the same process. Next to improved efficiency, pytrec_eval brings the following benefits: (a) current and future reference trec_eval implementations of IR evaluation measures are available within Python, and (b) as the evaluation measures are implemented in C, their execution are typically faster than native Python-based alternatives. The main purpose of this paper is to describe pytrec_eval, provide empirical evidence of the speedup that pytrec_eval delivers, and showcase the use of pytrec_eval in a reinforcement learning application. We ask the following questions: (RQ1) What speedup do we obtain when using pytrec_eval over trec_eval (serialize-invoke-parse workflow)? (RQ2) How fast is pytrec_eval compared to native Python implementations of IR evaluation measures? We also present a demo application that combines Pyndri [9] and pytrec_eval in a query formulation",1,trec,True
23,873,0,,False
24,Short Research Papers I,0,,False
25,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
26,"qrel ,"" {'q1': {'d1': 1, 'd2': 0}, 'q2': {'d2': 1}}""",0,,False
27,"run ,"" {'q1': {'d1': 0.5, 'd2': 2.0}, 'q2': {'d1': 0.5, 'd2': 0.6}}""",0,,False
28,"evaluator ,"" pytrec_eval.RelevanceEvaluator( qrel, {'map', 'ndcg'})""",1,trec,True
29,"result , evaluator.evaluate(run)",0,,False
30,"# result equals # {'q1': {'map': 0.5, 'ndcg': 0.6309297535714575}, # 'q2': {'map': 1.0, 'ndcg': 1.0}}",0,,False
31,"Code snippet 1: Minimal example of how pytrec_eval can be used to compute IR evaluation measures. Evaluation measures (NDCG, MAP) are computed for two queries--q1 and q2--and two documents--d1 and d2--where for q2 we only have partial relevance (d1 is assumed to be non-relevant).",1,trec,True
32,"reinforcement learning setting and provide the environment and the reward signal, integrated within the OpenAI Gym [1].",0,,False
33,2 EVALUATING USING PYTREC_EVAL,1,TREC,True
34,"The pytrec_eval library has a minimalistic design. Its main interface is the RelevanceEvaluator class. The RelevanceEvaluator class takes as arguments (1) query relevance ground truth, a dictionary of query identifiers to a dictionary of document identifiers and their integral relevance level, and (2) a set of evaluation measures to compute (e.g., ndcg, map). Code snippet 1 shows a minimal example on how pytrec_eval can be used to evaluate a ranking. Rankings are encoded by a mapping from document identifiers to their retrieval scores. Internally, pytrec_eval sorts the documents in decreasing order of retrieval score. This behavior mimics the implementation of trec_eval, which ignores the order of documents within the user-provided file, and only considers the document scores. Similar to trec_eval, document ties, which occur when two documents are assigned the same score, are broken by secondarily sorting on document identifier. Query relevance ground truth is passed to pytrec_eval in a similar way to document scores, where relevance is encoded as an integer rather than a floating point value.",1,trec,True
35,"Beyond measures computed over the full ranking of documents, pytrec_eval also supports measures computed up to a particular rank k. The values of k are the same as the ones used by trec_eval. For example, measures ndcg_cut and P correspond to NDCG@k and precision@k, respectively, with k ,"" 5, 10, 15, 20, 30, 100, 200, 500, 1000. The set of supported evaluation measures is stored in the pytrec_eval.supported_measures property and the identifiers are the same as used by trec_eval (i.e., running trec_eval with arguments -m ndcg_cut --help will show documentation for the NDCG@k measure). To mimic the behavior of trec_eval to compute all known evaluation measures (i.e., passing argument -m all_trec to trec_eval), just instantiate RelevanceEvaluator with pytrec_eval.supported_measures as the second argument.""",1,trec,True
36,3 BENCHMARK RESULTS,0,,False
37,"As demonstrated above, pytrec_eval conveniently exposes popular IR evaluation measures within Python. However, the same functionality could be exposed by invoking trec_eval in a serializeinvoke-parse workflow--or--by implementing the evaluation measure natively in Python. In this section we provide empirical benchmark results that show that pytrec_eval, beyond its convenience,",1,trec,True
38,"is also faster at computing evaluation measures than these two alternatives (i.e., invoking trec_eval or native Python).",1,trec,True
39,"Experimental setup. For every hyperparameter configuration, the runtime measurement was repeated 20 times and the average runtime is reported. Speedup denotes the ratio of the runtime of the alternative method (i.e., trec_eval or native Python) over the runtime of pytrec_eval and consequently, a speedup of 1.0 means that both methods are equally fast. When invoking trec_eval using the serialize-invoke-parse workflow, rankings are written from Python to storage without sorting, as trec_eval itself sorts the rankings internally. The resulting evaluation output is read from stdout to a Python string and we do not extract the measure values, as different parsing strategies can lead to large variance in runtime. For the native Python implementation, we experimented with different open-source implementations of the NDCG measure and adapted the fastest implementation as our baseline. The implementation does not make use of NumPy or other scientific Python libraries as (a) we wish to compare to native Python directly and (b) the NumPy-based implementations we experimented with were less efficient than the native implementation we settled with, as NumPy-based implementations require that the rankings are encoded in dense arrays before computing evaluation measures. The evaluated rankings and ground-truth were synthesized by assigning every document a distinct ranking score in N and a relevance level of 1. This allows us to evaluate different evaluation measure implementations with rankings and query sets of different sizes. Experiments were run using a single Intel Xeon CPU (E5-2630 v3) clocked at 2.4GHz, DDR4 RAM clocked at 2.4GHz, an Intel SSD (DC S3610) with sequential read/write speeds of 550MB/s and 450MB/s, respectively, and a hard disk drive (Seagate ST2000NX0253) with a rotational speed of 7200 rpm. All code used to run our experiments is available under the MIT open-source license.2",1,trec,True
40,Results. We now answer our research questions by comparing the runtime performance of pytrec_eval to trec_eval (RQ1) and a native Python implementation (RQ2).,1,trec,True
41,RQ1 What speedup do we obtain when using pytrec_eval over trec_eval (serialize-invoke-parse workflow)?,1,trec,True
42,"Fig. 1 shows matrices of speedups of pytrec_eval over trec_eval obtained using different storage types (increasing order of throughput capacity): a regular hard disk drive (HDD), a solid state drive (SSD) and a memory-mapped file system (tmpfs). For the degenerate case where we have a single query and a single returned document, we observe that there is a clear difference between the different storages. In particular, we can see that tmpfs is faster than SSD, and in turn, SSD is faster than the HDD. However, for larger configurations (upper right box in every grid; 10,000 queries with 1,000 documents) we see that the difference between the storage types fades away and that pytrec_eval always achieves a speedup of at least 17 over trec_eval. This is because (a) starting the serialization (e.g., disk seek time) is expensive (as can be seen in the left-lower box of every grid), but that cost is quickly overshadowed by (b) the cost of context switching between processes. In the case of pytrec_eval, however, context switching is avoided as all logic runs as part of the same process. Consequently, we can conclude that pytrec_eval is at least one order of magnitude faster than invoking trec_eval using a serialize-invoke-parse workflow.",1,trec,True
43,2The benchmark code can be found in the benchmarks sub-directory of the pytrec_eval repository; see the footnote on the first page.,1,trec,True
44,874,0,,False
45,Short Research Papers I,0,,False
46,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
47,Queries Queries,0,,False
48,10000 34 34 30 25 24 22 18 18,0,,False
49,5000 25 20 19 21 21 22 19 17,0,,False
50,1000 14 21 23 19 19 21 18 18,0,,False
51,250 16 24 28 29 27 22 18 18,0,,False
52,50 54 50 49 41 33 25 19 17,0,,False
53,10 229 194 140 96 57 40 22 19,0,,False
54,1 951 780 627 443 229 125 38 28,0,,False
55,1,0,,False
56,5,0,,False
57,10 20 50 100 500 1000,0,,False
58,Documents per query,0,,False
59,10000 49 36 31 26 23 23 18 18,0,,False
60,5000 31 30 28 23 21 22 19 18,0,,False
61,1000 13 21 26 24 24 23 18 18,0,,False
62,250 15 22 26 29 27 28 18 17,0,,False
63,50 45 44 42 37 30 29 21 18,0,,False
64,10 190 142 106 81 46 35 20 19,0,,False
65,1 781 653 565 389 196 113 32 24,0,,False
66,1,0,,False
67,5,0,,False
68,10 20 50 100 500 1000,0,,False
69,Documents per query,0,,False
70,Queries,0,,False
71,10000 55 41 31 28 23 23 18 17,0,,False
72,5000 34 29 26 23 22 22 19 18,0,,False
73,1000 12 21 25 25 24 21 19 17,0,,False
74,250 15 22 26 29 29 27 18 17,0,,False
75,50 45 43 41 37 31 28 20 18,0,,False
76,10 185 144 109 76 47 34 21 19,0,,False
77,1 771 511 525 333 192 102 32 24,0,,False
78,1,0,,False
79,5,0,,False
80,10 20 50 100 500 1000,0,,False
81,Documents per query,0,,False
82,(a) HDD,0,,False
83,(b) SSD,0,,False
84,(c) Memory-mapped (tmpfs),0,,False
85,Figure 1: Speedup of pytrec_eval (down-rounded speedup in each box; runtime measured as average over 20 repetitions) com-,1,trec,True
86,"pared to invoking trec_eval using a serialize-invoke-parse workflow (§1) for different numbers of queries, different numbers",1,trec,True
87,"of ranked documents per query, and using different types of storage (hard disk drive, solid state drive and random access",0,,False
88,memory) for serializing the rankings and query relevance ground truth.,0,,False
89,Speedup,0,,False
90,3,0,,False
91,2,0,,False
92,1,0,,False
93,0 1 3 5 10 20 30 40 50 102 103 104 105,0,,False
94,Number of documents,0,,False
95,"Figure 2: Speedup of pytrec_eval over a native Python implementation of the NDCG evaluation measure (we report average speedup and its standard deviation over 20 repetitions). For practically-sized rankings, pytrec_eval is consistently faster than the native Python implementation.",1,trec,True
96,RQ2 How fast is pytrec_eval compared to native Python implementations of IR evaluation measures?,1,trec,True
97,"Fig. 2 shows the speedup of pytrec_eval over a Python-native implementation of NDCG for a single query and a varying number of documents. Here we see that for extremely short rankings (1­3 documents), the native implementation outperforms pytrec_eval. However, for rankings consisting of 5 documents or more, we can see that pytrec_eval provides a consistent performance boost over the native implementation. The reason for the sub-native performance of pytrec_eval for very short rankings is because--before pytrec_eval computes evaluation measures--rankings need to be converted into the internal C format used by trec_eval. The Python-native implementation does not require this transformation, and consequently, can thus be slightly faster when rankings are very short. However, it is important to note that short rankings are uncommon in IR and that the average ranking consists of around 100 to 1,000 documents. We conclude that pytrec_eval is faster than native Python implementations for practically-size rankings.",1,trec,True
98,4 EXAMPLE: Q-LEARNING,0,,False
99,"We showcase the integration of the Pyndri indexing library [9] and pytrec_eval within the OpenAI Gym [1], a reinforcement learning library, for the task of query expansion. In particular, we use Pyndri to rank documents according to a textual query and",1,trec,True
100,subsequently evaluate the obtained ranking using pytrec_eval. The reinforcement learning agent navigates an environment where,1,trec,True
101,actions correspond to adding a term to the query. Rewards are given,1,ad,True
102,"by an increase or decrease in evaluation measure (i.e., NDCG). The goal is for the agent to learn a policy   that optimizes the expected",0,,False
103,value of the total reward. For the purpose of this demonstration of,0,,False
104,"software interoperability, we synthesize a test collection in order to",0,,False
105,(1) limit the computational complexity that arises from real-world,0,,False
106,"collections, and (2) to give us the ability to create an unlimited",0,,False
107,number of training queries and relevance judgments.,0,,False
108,"Document collection. We construct a synthetic document collection D, of a given size |D| ,"" 100, following the principles laid out by Tague et al. [8]. For a given vocabulary size |V | "","" 10,000, we construct vocabulary V consisting of symbolic tokens. We sample collection-wide unigram (|V | parameters) and bigram (|V | 2 parameters) pseudo counts from an exponential distribution ( "","" 1.0). This incorporates term specificity within our synthetic collection, as""",1,corpora,True
109,only few term uni- and bigrams will be frequent and most will be in-,0,,False
110,frequent. These pseudo counts will then serve as the concentration,0,,False
111,parameters of Dirichlet distributions from which we will sample,0,,False
112,"a uni- and bigram language model for every document. We create |D| documents as follows. For every document d, given the average document length µd ,"" 200, we sample its document size, |d |, from a Poisson with mean µd . We then sample two language models--one for unigrams P (w | d ) and another for bigrams P ((x, y) | d )--from""",0,,False
113,a Dirichlet distribution where the concentration parameters we,0,,False
114,"defined earlier for the whole collection. The document is then constructed as follows. Until we have reached |d | tokens, we repeat the following: (a) sample an n-gram size from a predefined probability distribution (P (n , 1) ,"" 0.9, P (n "", 2) ,"" 0.1), and subsequently, (b) sample an n-gram from the corresponding language model. We truncate a document if it exceeds its pre-defined length |d |.""",0,,False
115,"Query collection. Once we obtained our synthetic document collection D, we proceed by constructing our query set Q, of a given size |Q | ,"" 100,000, as follows. For every query q to be constructed, we select r "", 5 documents uniformly at random from D and denote these as the set of relevant documents Rq  D for query q. Given the average query length µq ,"" 3, the length of query q, q , is then sampled from a Poisson distribution with mean µq . We write""",1,Query,True
116,P w | Rq and P (w | D) to denote the empirical language models,0,,False
117,875,0,,False
118,Short Research Papers I,0,,False
119,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
120,Avg. reward (NDCG),0,,False
121,0.10,0,,False
122,0.05,0,,False
123,0.00,0,,False
124,00h 00m 00s,0,,False
125,02h 46m 40s,0,,False
126,05h 33m 20s,0,,False
127,08h 20m 00s 11h 06m 40s Wall-clock time,0,,False
128,13h 53m 20s,0,,False
129,16h 40m 00s,0,,False
130,Figure 3: Average reward (NDCG) obtained by the Q-learning algorithm over time while training the reinforcement learning,0,,False
131,agent. The agent learns to select vocabulary terms that improve retrieval effectiveness for the set of 100k training queries.,0,,False
132,"estimated from concatenating the relevant documents for query q and from concatenating all documents in the collection D (i.e., the collection language model), respectively. The q terms of query",0,,False
133,"q are sampled with replacement from P w | Rq (1.0 - P (w | D)), such that terms specific to Rq and uncommon in D are selected. Environment. For each query q, the environment is initialized to the state where only the query terms are present. At any given state, the agent can then choose to expand the query terms with any unigram term from the vocabulary V in addition to a null operation action. Rankings are obtained by querying the Indri search engine using Pyndri, using a Dirichlet language model (µ ,"" 2,500), and obtaining a ranking of the top-10 documents. The reward of choosing an action is the NDCG that is obtained by expanding the query with the chosen term. As observation, the agent receives a binary vector indicating which terms of the vocabulary V occur at least once in the current expanded query. After 5 actions--or a perfect NDCG (i.e., 1.0) is achieved--the episode terminates.""",1,ad,True
134,"Reinforcement learning agent. We learn an optimal policy tabular   using Q-learning where the initial values of the Q (·) are initialized to zero. We set the learning rate  , 0.1 and the discount factor  ,"" 0.95. During learning, we maintain an -greedy strategy with  "","" 0.05. Fig. 3 shows the average reward obtained while training an agent on the reinforcement learning problem defined above. The average reward obtained by the agent increases over time. In particular, this example showcases that different IR libraries (Pyndri, pytrec_eval) can easily be integrated with machine learning libraries (OpenAI Gym) to quickly prototype ideas. An essential part here is that expensive operations (i.e., ranking and evaluation) are performed in efficient low-level languages, whereas prototyping occurs in the high-level Python scripting language. All code used in this example is available under the MIT open-source license.3""",1,trec,True
135,5 CONCLUSIONS,0,,False
136,"In this paper we introduced pytrec_eval, a Python interface to trec_eval. pytrec_eval builds upon the trec_eval source code and exposes a Python-first interface to the trec_eval evaluation toolkit as a native Python extension. This allows for convenient and fast invocation of IR evaluation measures directly from Python. We showed that pytrec_eval is around one order of magnitude faster than invoking trec_eval in a serialize-invoke-parse workflow as it avoids the costs associated with (1) the serialization of the rankings to storage, and (2) operation system context switching. Compared to a native Python implementation of NDCG, pytrec_eval is approximately twice as fast for practically-sized rankings (100 to 1,000",1,trec,True
137,3The reinforcement learning code can be found in the examples sub-directory of the pytrec_eval repository; see the footnote on the first page.,1,trec,True
138,"documents). In addition, we showcased the integration of Pyndri [9] and pytrec_eval within the OpenAI Gym [1] and showed that all three modules can be combined to quickly prototype ideas.",1,ad,True
139,"In this paper, we used a tabular function during Q-learning; other functional forms--such as a deep neural network--can also be used. Pyndri and pytrec_eval expose common IR operations through a convenient Python interface. Beyond the convenience that both modules provide, an important design principle is that expensive operations (e.g., indexing, ranking) are performed using efficient low-level languages (e.g., C), while Python takes on the role of an instructor that links the expensive operations. Future work consists of exposing more IR operations as Python libraries and allowing more interoperability amongst modules. For example, currently Pyndri converts its internal Indri structures to Python structures, which are then again converted back to internal trec_eval structures by pytrec_eval. A closer integration of Pyndri and pytrec_eval could result in even faster execution times as both can communicate directly--in cases where one is only interested in the evaluation measures and not the rankings--rather than through Python.",1,trec,True
140,"Acknowledgements. This research was supported by Ahold Delhaize, Amsterdam Data Science, the Bloomberg Research Grant program, the China Scholarship Council, the Criteo Faculty Research Award program, Elsevier, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the Google Faculty Research Awards program, the Microsoft Research Ph.D. program, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scientific Research (NWO) under project nrs CI-14-25, 652.002.001, 612.001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.",0,,False
141,REFERENCES,0,,False
142,"[1] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI gym, 2016.",0,,False
143,"[2] D. Harman. Information retrieval evaluation. Synthesis Lectures on Information Concepts, Retrieval, and Services, 3(2):1­119, 2011.",0,,False
144,"[3] H. Koepke. Why python rocks for research. https://www.stat.washington.edu/ ~hoytak/_static/papers/why-python.pdf, 2010. Accessed February 12, 2018.",0,,False
145,"[4] D. Li and E. Kanoulas. Bayesian optimization for optimizing retrieval systems. In WSDM. ACM, February 2018.",0,,False
146,"[5] NIST. Text retrieval conference, 1992­2017. [6] L. Prechelt. An empirical comparison of seven programming languages. Computer,",0,,False
147,"33(10):23­29, Oct. 2000. [7] M. Sanderson. Test collection based evaluation of information retrieval systems.",0,,False
148,"Foundations and Trends in Information Retrieval, 4(4):247­375, 2010. [8] J. Tague, M. Nelson, and H. Wu. Problems in the simulation of bibliographic",0,,False
149,"retrieval systems. In SIGIR, pages 236­255. ACM, June 1980. [9] C. Van Gysel, E. Kanoulas, and M. de Rijke. Pyndri: a python interface to the indri",0,,False
150,"search engine. In ECIR, pages 744­748. Springer, April 2017.",0,,False
151,876,0,,False
152,,0,,False

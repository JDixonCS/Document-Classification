,sentence,label,data,regex
0,Session 5C: New Metrics,1,Session,True
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,An Axiomatic Analysis of Diversity Evaluation Metrics: Introducing the Rank-Biased Utility Metric,0,,False
3,Enrique Amigó,0,,False
4,"NLP & IR Group at UNED Madrid, Spain",1,ad,True
5,enrique@lsi.uned.es,0,,False
6,Damiano Spina,0,,False
7,"RMIT University Melbourne, Australia damiano.spina@rmit.edu.au",0,,False
8,Jorge Carrillo-de-Albornoz,0,,False
9,"NLP & IR Group at UNED Madrid, Spain",1,ad,True
10,jcalbornoz@lsi.uned.es,0,,False
11,ABSTRACT,0,,False
12,"Many evaluation metrics have been defined to evaluate the effectiveness ad-hoc retrieval and search result diversification systems. However, it is often unclear which evaluation metric should be used to analyze the performance of retrieval systems given a specific task. Axiomatic analysis is an informative mechanism to understand the fundamentals of metrics and their suitability for particular scenarios. In this paper, we define a constraint-based axiomatic framework to study the suitability of existing metrics in search result diversification scenarios. The analysis informed the definition of Rank-Biased Utility (RBU) ­ an adaptation of the well-known Rank-Biased Precision metric ­ that takes into account redundancy and the user effort associated to the inspection of documents in the ranking. Our experiments over standard diversity evaluation campaigns show that the proposed metric captures quality criteria reflected by different metrics, being suitable in the absence of knowledge about particular features of the scenario under study.",1,ad-hoc,True
13,CCS CONCEPTS,0,,False
14,· Information systems  Retrieval effectiveness;,0,,False
15,KEYWORDS,0,,False
16,"Evaluation, Search result diversification, Axiomatic analysis",0,,False
17,"ACM Reference Format: Enrique Amigó, Damiano Spina, and Jorge Carrillo-de-Albornoz. 2018. An Axiomatic Analysis of Diversity Evaluation Metrics: Introducing the Rank-Biased Utility Metric. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3209978.3210024",0,,False
18,1 INTRODUCTION,1,DUC,True
19,The development of better information retrieval systems is driven by how improvements are measured. The design of test collections and evaluation metrics that started with the Cranfield paradigm in the early 1960s allowed researchers to analyze the quality of different retrieval models in an automated and cost-effective way.,1,ad,True
20,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210024",1,ad,True
21,"Since then, many evaluation metrics have been proposed to measure the effectiveness of information retrieval systems [20, 22, 27].",0,,False
22,"Selecting a suitable set of metrics for a specific task is challenging. Comparing metrics empirically against user satisfaction or search effectiveness requires data that is often unavailable. Moreover, findings may be biased to the subjects, retrieval systems or other experimental factors.",0,,False
23,"An alternative consists of modeling theoretically the desirable properties of retrieval systems, as well as the abstraction of the expected users' behavior when performing a specific task. For instance, a metric that looks at how early the relevant document is retrieved in the ranking ­ such as Reciprocal Rank [26] ­ would be an appropriate metric to analyze the performance of systems on a single-item navigational task. However, is often challenging to come up with the proper evaluation tools for more complex search scenarios, as is the case of search result diversification [19]. In this context, the ranking of retrieved documents must be optimized in such a way that diverse query aspects are captured in the first positions. The challenge is that the evaluation of system outputs is affected by multiple variables such as: the deepness of ranking positions, the amount of documents in the ranking related to the same query aspect, relevance grades, the diversity of query aspects captured by single documents or the user's effort when inspecting the ranking.",1,ad,True
24,"Axiomatic analysis has been shown to be an effective methodology to better understand the foundamentals of evaluation metrics [3, 4, 10, 25]. In the context of evaluation, axiomatic approaches consist of a verifiable set of formal constraints that reflect which quality factors are captured by metrics, facilitating the metric selection in specific scenarios. To our knowledge, there is no comprehensive axiomatic analysis of the behavior of diversity metrics in the literature. This paper provides a set of ten formal constraints that focus on both retrieval and diversity quality dimensions.",0,,False
25,"We found that every constraint is satisfied at least by one metric. However, none of the existing diversity metrics satisfy all the proposed constraints simultaneously. In order to solve this gap, we define the metric Rank-Biased Utility (RBU) by integrating components from different metrics in order to capture every formal constraints. RBU is an adaptation of the well-known Rank-Biased Precision metric [16] that incorporates redundancy and the user's effort associated to the inspection of documents in the ranking. Our experiments using standard diversity test collections validate our axiomatic analysis. Results show that, satisfying every constraint with a single metric leads to unanimous evaluation decisions when compared against other existing metrics, i.e., RBU captures quality criteria which are reflected by different metrics. Therefore, this metric offers a solution in the absence of knowledge about the specific",1,ad,True
26,625,0,,False
27,Session 5C: New Metrics,1,Session,True
28,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
29,"characteristic of a diversity-oriented retrieval scenario. Moreover, the theoretical framework presented in this paper helps to decide which metric should be used.",0,,False
30,"The paper is organized as follows. Section 2 describes related work on evaluation of evaluation metrics. Section 3 introduces the formal constraints that we propose to analyze relevance and diversity properties of metrics. Section 4 provides a comprehensive analysis of existing diversity metrics according to these constraints and Section 5 defines the proposed RBU metric. Section 6 details the results of our experiments. Finally, Section 7 concludes the work.",0,,False
31,2 RELATED WORK,0,,False
32,"There is no consensus of meta-evaluation criteria for search result diversification. Some works inherit meta-evaluation criteria from ad-hoc metrics such as sensitivity to system differences [11, 14, 17, 18]. This methodology however does not give information about to what extent metrics capture diversity properties. Smucker and Clarke [21] studied the correspondence between metric scores and user effort when exploring document rankings. This methodology has the advantage of being realistic ­ effort is calibrated from historical log data ­ but only focuses on partial quality aspects.",1,ad-hoc,True
33,"Most of works on diversity metrics are supported by descriptive analysis. In 2008, Clarke et al. [7] meta-evaluated -nDCG by analyzing the effect of modifying the diversity parameter  under different datasets. One year later, Agrawal et al. [1] checked the intent-aware scheme for diversification by studying the evaluation results of three search engines. Clarke et al. [8] proposed Noveltyand Rank-Biased Precision (NRBP), an extension of RBP [16] for diversification, joining properties of the original RBP metric, nDCG and intent-aware metrics. In 2010, Sakai et al. [17] compared their proposed approach to -NDCG and NRBP, in terms of metric agreement under different parameters. The authors considered some meta-evaluation criteria such as interpretability, computability or capability to accommodate graded relevance and score ranges. Three years later, Chandar and Carterette [5] evaluated their approach by studying correlation with previous metrics while reflecting other ranking quality issues. Luo et al. [14] proposed the Cube Test metric. They studied the effect of the metric parameters under synthetic system outputs, in the same manner than Clarke et al. [7]. Tangsomboon and Leelanupab [23] in 2014 and also Yu et al. [31] in 2017, supported their proposed metrics in terms of agreement and disagreement with previous metrics.",1,Novelty,True
34,"Not many works define a way of quantifying the suitability of metrics to capture diversity. An exception is the work by Golbus et al. [11] who defined Document Selection Sensitivity. This metameasure reports to what extent metrics are sensitive to document rankings containing relevant documents but different grades of diversity. Within this line, we define in this work Metric Unanimity (MU), which quantifies to what extent a metric is sensitive to quality aspects captured by other existing metrics.",1,ad,True
35,"On the other hand, metrics have been successfully analyzed in terms of formal constraints in ad-hoc retrieval scenarios [3, 10, 15]. The axiomatic methodology consists of identifying theoretical situations in which metrics should behave in a particular manner. This methodology has several strengths: it is objective, independent from datasets and it facilitates the interpretation of metrics. We",1,ad-hoc,True
36,"found only a few initial works in the context of formal constraints for search result diversification. For instance, Leelanupab et al. [13] reviewed the appropriateness of intent-aware, stating an extreme particular situation in which ERR-IA does not behave as expected. In our work, we meta-evaluate existing metrics on the basis of ten constraints that formalize desirable properties for ranking and diversity effectiveness.",0,,False
37,3 AXIOMATIC CONSTRAINTS,0,,False
38,3.1 Problem Formalization,0,,False
39,We formalize the output of a document retrieval system as an or-,0,,False
40,"dered list of documents d ,"" (d1, . . . , dn ) of length n, extracted from a collection of documents D. In order to express formal constraints,""",0,,False
41,"we use dij to denote the result of swapping documents between positions i and j. Likewise, ddd denotes the result of replacing the document d with the document d  in the ranking d.",0,,False
42,"For search result diversification, we consider a set of query as-",0,,False
43,"pects T ,"" {t1, . . . , tm }. For instance, users searching for a restau-""",0,,False
44,"rant may be interested in the menu, the offers, opening times, etc.",0,,False
45,Each aspect has an associated weight w (tj ) and the sum of all aspect,0,,False
46,weights adds up to 1:,1,ad,True
47,"m j ,1",0,,False
48,w (tj,0,,False
49,),0,,False
50,",",0,,False
51,1.,0,,False
52,"On the other hand, r (di , tj )  [0 . . . 1] represents the graded",1,ad,True
53,relevance of document di to the aspect tj . We assume the user's be-,0,,False
54,"havior follows the cascade model, i.e., the user inspects the ranking",1,ad,True
55,"sequentially from the top to the bottom, until either (i) the user's",0,,False
56,"information needs get satisfied or (ii) the user stops looking (i.e.,",0,,False
57,user's patience is exhausted). Following the same user model than,0,,False
58,"the one used by Expected Reciprocal Rank [6], we consider rele-",0,,False
59,"vance as the suitability of the document to satisfy the user needs,",0,,False
60,"which has a negative correspondence with the probability of exploring more documents. Finally, we use Q (d) to denote the ranking",0,,False
61,"quality score, i.e., the score given by applying an evaluation metric Q to a given ranking d.",0,,False
62,Our axiomatic approach consists of a set of ten formal constraints,0,,False
63,that evaluation metrics may satisfy. These constraints are grouped,0,,False
64,"into two sets: relevance-oriented and diversity-oriented, that we",0,,False
65,describe below.,0,,False
66,"In the definition of the constraints, we may refer to the follow-",0,,False
67,"ing conditions: single aspect (|T | , 1); balanced aspects (t  T (w (t ) ,"" 1/|T |)); binary relevance (t, d (r (d, t )  {0, rc })); no aspect overlap (r (d, t ) > 0  t  t (r (d, t ) "","" 0)); and relevance contribution (r (d, t )  1). The last condition means that finding""",0,,False
68,new relevant documents about the same topic is always effective.,0,,False
69,"In other words, there is always room for new documents to fully",0,,False
70,satisfy the user needs.,0,,False
71,3.2 Relevance-Oriented Constraints,0,,False
72,"In order to isolate relevance from diversity and redundancy, for",0,,False
73,these constraints we will assume single aspect and relevance contri-,0,,False
74,"bution. For the sake of legibility, we use the notation: r (d ) ,"" r (d, t ). We""",0,,False
75,"also denote drel and d¬rel as relevant and non-relevant documents, respectively. That is: i  1..n. r (di¬r el ) , 0 and r (dir el ) ,"" rc . Under these assumptions, we import the five constraints proposed""",0,,False
76,626,0,,False
77,Session 5C: New Metrics,1,Session,True
78,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
79,"by Amigó et al. [3] which capture previous axiomatic properties [10, 15].",0,,False
80,"Constraint 1 (Priority, Pri). Swapping items in concordance with their relevance increases the ranking quality score. Being k > 0:",0,,False
81,"r (di+k ) > r (di ) , Q dii+k > Q d",0,,False
82,(1),0,,False
83,The next constraint is based on the intuition that the effect of relevance depends on the document ranking position. This constraint is also referred as top-heaviness:,0,,False
84,"Constraint 2 (Deepness, Deep). Correctly swapping contiguous items has more effect in early ranking positions:",0,,False
85,"r (di ) , r (dj ) < r (di+1) , r (dj+1) , Q dii+1 > Q djj+1 (2) where i < j.",0,,False
86,"The next constraint reflects that the effort spent by the user to inspect a long (deep) list of search results is limited. In other words, there is an area of the ranking that may never get explored by the user:",0,,False
87,"Constraint 3 (Deepness Threshold, DeepTh). Assuming binary relevance, there exists a value n large enough such that, retrieving only one relevant document at the top of the ranking is better than retrieving n relevant documents after n non-relevant documents:",0,,False
88,"n  N+. Q d1rel, . . . > Q d1¬rel, . . . , dn¬rel, d1rel, . . . , dnrel (3)",0,,False
89,"On the other hand, we can assume that there exists a (short) ranking area which is always explored by the user. In other words, at least a few documents are inspected by the user with a minimum effort. This means that, at the top of the ranking, the amount of captured relevant documents is more important than their relative rank positions.",0,,False
90,"Constraint 4 (Closeness Threshold, CloseTh). Assuming binary relevance, there exists a value m small enough such that retrieving one relevant document in the first position is worse than m relevant documents after m non-relevant documents:",0,,False
91,"m  N+. Q d1rel, . . . < Q d1¬rel, . . . , dm¬rel, d1rel, . . . , dmrel (4)",0,,False
92,"In some particular scenarios, however, this may not hold. For instance, in audio-only search scenarios, search results may be delivered sequentially one-at-a-time.",0,,False
93,"Finally, the amount of documents returned is also an aspect of the system quality. In the same manner that capturing diversity in the first positions is desirable, adding non-relevant documents to the end of the ranking should be penalized by metrics. In other words, the cutoff used by the system to stop returning search results has also an impact on users. Therefore, adding noise at the bottom of the ranking should decrease its effectiveness.",1,ad,True
94,"Constraint 5 (Confidence, Conf). Adding non-relevant documents decreases the score:",0,,False
95,"Q d > Q d, d¬rel",0,,False
96,(5),0,,False
97,3.3 Diversity-Oriented Constraints,0,,False
98,The first diversity-oriented constraint is related to the fact that the metric should be sensitive to the novelty of aspects covered by a single document:,0,,False
99,"Constraint 6 (Query Aspect Diversity, AspDiv). Covering",1,Query,True
100,"more aspects in the same document (i.e., without additional effort of",1,ad,True
101,"inspecting more documents) increases the score. Assuming relevance contribution (d, t : r (d, t )  1):",0,,False
102,"t  T . r (di, t ) > r (di , t ) , Q ddi di > Q d",0,,False
103,(6),0,,False
104,To calculate the gain obtained by observing a new relevant doc-,0,,False
105,"ument in the ranking, most of the existing diversity metrics take",0,,False
106,into account the number of previously observed documents that are,0,,False
107,related with the same aspect. The more an aspect has been covered,0,,False
108,"earlier in the ranking, the less a new document relevant to this",0,,False
109,aspect contributes to the gain. Formally:,0,,False
110,"Constraint 7 (Redundancy, Red). Assuming binary relevance, balanced aspects and no aspect overlap, and being d and d  documents relevant to different aspects r (d, t ) ,"" r (d , t ) "","" rc , then:""",0,,False
111,"|{di  d. r (di , t ) ,"" rc }| > |{di  d. r (di , t ) "", rc }| , (7)",0,,False
112,"Q d, d  > Q d, d",0,,False
113,"The Red constraint assumes binary relevance, by counting rele-",0,,False
114,vant documents for each query aspect. In order to consider graded,1,ad,True
115,"relevance in previously observed documents, we can apply the",0,,False
116,"monotonicity principle. That is, if an aspect t is captured to a greater extent than a second aspect t  in every previously observed document, then the ranking is more redundant w.r.t. t than t . Formally:",0,,False
117,"Constraint 8 (Monotonic Redundancy, MRed). Assuming",0,,False
118,"two balanced aspects (T ,"" {t, t }), relevance contribution, and being d and d  documents exclusively relevant to each aspect, 0 < r (d, t ) "","" r (d , t )  1 and r (d, t ) "","" r (d , t ) "", 0:",0,,False
119,"di  d. r (di , t ) > r (di , t ) ,"" Q d, d  > Q d, d (8)""",0,,False
120,"Intuitively, as well as the exploration capacity or patience of the user is limited, the user's information need is also finite. This means that there should exists a certain point on which a new single piece of information completely satisfies user's information needs, in such a way that retrieving any other documents addressing the same query aspect is not beneficial. Formally:",1,ad,True
121,"Constraint 9 (Aspect Relevance Saturation, Sat). Assum-",0,,False
122,"ing no aspect overlap, there exists a finite relevance value rmax large",0,,False
123,enough such that:,0,,False
124,"(r (dn, t ) ,"" rmax )  (r (dn+1, t ) > 0) "",",0,,False
125,"Q d  Q d, dn+1",0,,False
126,(9),0,,False
127,"Finally, the following constraint captures the relative weight of aspects w (t ) w.r.t. the user's information need:",0,,False
128,"Constraint 10 (Aspect Relevance, AspRel). Aspects with higher",0,,False
129,"weights have more effect in score of the ranking quality. Formally, assuming no aspect overlap, and being di and di documents that are relevant to different aspects that have not been observed before, j < i. r (dj , t ) ,"" r (dj , t ) "","" 0, and r (di , t ) "","" r (di, t ) > 0 then:""",0,,False
130,"w (t ) > w (t ) , Q ddi di > Q d",0,,False
131,(10),0,,False
132,627,0,,False
133,Session 5C: New Metrics,1,Session,True
134,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
135,"In summary, we have defined a total of ten constraints: five relevance-oriented constraints (Pri, Deep, DeepTh, CloseTh and Conf), and five constraints for search result diversification (AspDiv, Red, MRed, Sat, and AspRel). The next section provides an axiomatic analysis of the most popular retrieval and diversity metrics using these constraints.",0,,False
136,4 METRIC ANALYSIS,0,,False
137,"In this section, we firstly analyze standard metrics designed to evaluate retrieval systems in non-diversified scenarios (i.e., singleaspect). Then we analyze the intent-aware family of metrics, as well as a number of popular diversity metrics.",0,,False
138,4.1 Standard Metrics for Ad-hoc Retrieval,1,hoc,True
139,"We analyze here metrics that do not consider multiple aspects of a query or topic, including: Precision at a cutoff k (P@k), Reciprocal Rank (RR) [26], Average Precision (AP), Rank-Biased Precision (RBP) [16], Expected Reciprocal Rank (ERR@k) [6] and Normalized Discounted Cumulative Gain (nDCG@k) [12].",1,AP,True
140,"RBP uses a parameter p that defines user's patience, modeled as the probability of the user to inspect the next document in the ranking. P@k, ERR and nDCG include a cutoff k that limits the rank positions considered in the evaluation measurement.1 The upper part of Table 1 summarizes the properties for the retrieval effectiveness metrics.",0,,False
141,"The constraints defined by Amigó et al. [3] assume that relevance judgments are binary. However, our axiomatic framework defines the constraints Pri and Deep over graded relevance (Eq. 1 and 2, respectively). Therefore, RR, AP and P@k become undefined.2",1,ad,True
142,"The rest of the analysis is inline with the one presented by Amigó et al. [3]: The other metrics (nDCG@k,ERR@k and RBP) satisfy Pri and Deep constraints by applying a relevance discounting factor depending on the depth of the ranking position. With regards to DeepTh (Eq. 3) and CloseTh (Eq. 4) constraints, metrics that rewards relevance in deep ranking positions such as AP or nDCG@k satisfy CloseTh but not DeepTh, while metrics that focus on the top of the ranking (P@k, RR and ERR@k) satisfy DeepTh but not CloseTh. RBP satisfies both CloseTh and DeepTh. The reason is that RBP is supported by a probabilistic user behavior model that takes into account the limitations of the ranking exploration process (i.e., user's patience). None of these metrics satisfy Conf.",1,AP,True
143,"This family of metrics are not applicable in the context of multiple query aspects. Therefore, they do not satisfy the diversityoriented constraints.",0,,False
144,4.2 Intent-Aware Metrics,0,,False
145,"The intent-aware scheme [1] extends standard metrics such as AP or ERR to make them applicable to diversification scenarios. Firstly, each query aspect is evaluated independently and then a weighted average considering query aspect weights is computed. Being Mt (d) the score of d according to the metric M when only the relevance",1,AP,True
146,"1 Due to lack of space, here we focus on the formal properties of the metrics and we provide references to the definition and explanation of the metrics. 2Amigó et al. [3]'s analysis shows that P@k does not satisfy the Pri and Deep constraints, given that it does not consider the order of documents before position k .",0,,False
147,to aspect t is considered:,0,,False
148,"M-IA(d) , w (t )Mt (d)",0,,False
149,t T,0,,False
150,"The central part of Table 1 includes the properties for the intentaware version of the metrics discussed before. Intent-aware metrics converge to the corresponding standard effectiveness metric when the query has only one aspect. Consequently, they inherit the properties of the original metric over the relevance-oriented constraints Pri, Deep, DeepTh and CloseTh.",0,,False
151,"Let us now analyze the diversification-oriented constraints. Besides AP-IA@k, RR-IA and P-IA@k, which are undefined in the context of graded relevance judgments, the intent-aware metrics nDCG-IA@k, ERR-IA@k and RBP-IA satisfy the AspDiv constraint. If a document is relevant for several aspects, then the averaged score across query aspects increases.",1,AP,True
152,"Most of metrics do not satisfy Red and MRed. In the case of P-IA@k, the precision averaged across aspects in a certain cutoff k is independent from to which particular aspect the documents are relevant to.3 RR-IA@k neither satisfies Red given that is sensitive only to the first relevant document for each query aspect. In the case of AP-IA@k, the relevance contribution of a document to the aspect t is higher if relevant documents for t have been observed earlier in the ranking.4 nDCG-IA@k and RBP-IA also fail to satisfy the Red constraint. These two metrics are not sensitive to the relevance of previously observed documents. The contribution of documents depends on the rank position and the amount of relevant documents in the collection.",1,AP,True
153,"On the other hand, the metric ERR-IA@k satisfies both Red and MRed, due to the component j <i (1 - r (dj , t )) which estimates the probability of the user to be satisfied by previously observed documents according to graded relevance levels.",1,ad,True
154,"The Sat constraint is not satisfied by P-IA@k, AP-IA@k, nDCGIA@k nor RBP-IA. The reason is that all these metrics reward new relevant documents regardless the the gain obtained by previous observed documents. However, the saturation relevance for RRIA@k and ERR-IA@k is 1. Finally, the AspRel constraint by all the intent-aware metrics analyzed in this work, given that they all consider the first relevant document for each aspect in the ranking and all of them consider aspect weights w (t ).",1,AP,True
155,4.3 Other Diversity Metrics,0,,False
156,"Besides the intent-aware metrics (M-IA), other metrics have been proposed to evaluate the effectiveness of search result diversification systems [19]. Zhai et al. [32] proposed Subtopic Recall (SRecall@k), which measures the number of aspects captured in the first k positions. Given that the metric only measures the coverage of aspects, does not satisfy Pri, Deep, CloseTh and Conf relevanceoriented constraints. The only diversity oriented constraint that",0,,False
157,3,0,,False
158,For,0,,False
159,"instance,",0,,False
160,being,0,,False
161,ni,0,,False
162,the,0,,False
163,amount,0,,False
164,of,0,,False
165,relevant,0,,False
166,documents,0,,False
167,for,0,,False
168,the,0,,False
169,aspect,0,,False
170,ti,0,,False
171,",",0,,False
172,the,0,,False
173,average,0,,False
174,P@k,0,,False
175,across aspects is:,0,,False
176,1 |T |,0,,False
177,ti T,0,,False
178,ni k,0,,False
179,ti T ni .,0,,False
180,4The contribution of a relevant document in AP is proportional to the precision,1,AP,True
181,"achieved at the document's position, which is higher when relevant documents",0,,False
182,"appear in the previous positions. For instance, being Nr the fixed amount of rel-",0,,False
183,"evant documents for every aspect in the collection, and being dt , dt two doc-",0,,False
184,"uments related with aspect t , and dt a document related with aspect t  then:",0,,False
185,"AP-IA@2(dt ,",1,AP,True
186,dt,0,,False
187,),0,,False
188,",",0,,False
189,1,0,,False
190,1 Nr,0,,False
191,+,0,,False
192,1,0,,False
193,2 Nr,0,,False
194,>,0,,False
195,1,0,,False
196,1 Nr,0,,False
197,+,0,,False
198,1 2,0,,False
199,1 Nr,0,,False
200,","" AP-IA@2(dt , dt )""",1,AP,True
201,628,0,,False
202,Session 5C: New Metrics,1,Session,True
203,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
204,"Table 1: Properties ( ,"" constraint satisfied, "", constraint not satisfied) of existing retrieval and diversity effectiveness metrics.",0,,False
205,Metric,0,,False
206,Relevance-Oriented Constraints Pri Deep DeepTh CloseTh Conf,0,,False
207,P@k RR,0,,False
208,AP nDCG@k ERR@k RBP,1,AP,True
209,P-IA@k RR-IA@k AP-IA nDCG-IA@k ERR-IA@k RBP-IA,1,AP,True
210,S-Recall@k S-RR@100%,0,,False
211,NRBP D#-Measure@k  -nDCG@k EU CT@k,0,,False
212,RBU@k,0,,False
213,Diversity-Oriented Constraints AspDiv Red MRed Sat AspRel,0,,False
214,"satisfies is Sat, given that S-Recall@k considers only the first relevant document for each query aspect and it does not consider aspect weights. Likewise, the metric S-RR@100% ­ an extension to RR also proposed by Zhai et al. [32], defined as the inverse of the rank position on which a complete coverage of aspects is obtained ­ satisfies the same properties as S-Recall@k.",0,,False
215,Clarke et al. [7] proposed Novelty-Biased Discounted Cumulative Gain (-nDCG@k).5 This metric is defined as:,1,Novelty,True
216,k,0,,False
217,"-nDCG@k (d) ,",0,,False
218,"i ,1",0,,False
219,"t T r (di , t )(1 -  )c (i,t ) log(i + 1)",0,,False
220,"where c (i, t ) represents the amount of documents previously ob-",0,,False
221,"served that capture the aspect t. Similarly to the original nDCG,",0,,False
222,"it satisfies Pri, Deep and CloseTh constraints. However, unlike",0,,False
223,"nDCG, DeepTh is also satisfied due to the redundancy factor (1 -  )c (i,t ), which also allows to satisfy Red. AspDiv is satisfied due to the additive relevance across aspects. In contrast, -nDCG@k",1,ad,True
224,"does not satisfy the constraints MRed and Sat. The reason is that the redundancy component (1 -  )c (i,t ) does not consider the rele-",0,,False
225,"vance grade of previously observed documents. Finally, this metric",1,ad,True
226,does not consider the weight of aspects and therefore AspRel is,0,,False
227,not satisfied.,0,,False
228,Clarke et al. [8] proposed Novelty- and Rank-Biased Precision,1,Novelty,True
229,"(NRBP), and adaptation of RBP for search result diversification,",1,ad,True
230,defined as:,0,,False
231,"NRBP(d) , pi-1",0,,False
232,"r (di , t )(1 -  )c (i,t )",0,,False
233,"i ,1",0,,False
234,t T,0,,False
235,"Similarly to the original RBP, NRBP satisfies all relevance-oriented",0,,False
236,"constraints except Conf, given that only relevant documents affect",0,,False
237,"the score. In terms of diversity-oriented constraints, NRBP behaves",0,,False
238,5Note that given that the proposed formal constraints and experiments in this work,0,,False
239,"compare metrics at topic (or query) level, the normalization factor in metrics such as  -nDCG@k can be ignored.",0,,False
240,"similarly to -nDCG@k given that diversification is modeled in a similar manner. Sakai and Song [18] proposed the D#-Measure which combines a D-Measure (e.g., D-nDCG [17]) with the ratio of aspects captured in the first k positions (modeled by S-Recall@k):",0,,False
241,"D#-Measure@k (d) ,  · S-Recall@k (d) + (1-) · D-Measure@k (d)",0,,False
242,"NRBP inherits the properties from nDCG-IA@k, which already satisfies DeepTh and AspRel. Therefore, the S-Recall@k component does not contribute with any additional constraint satisfaction.",1,ad,True
243,"None of previous metrics satisfy Conf. However, there exist in the literature utility-oriented metrics that penalyze non-relevant documents at the end of the ranking. Two examples are the Normalized Discounted Cumulated Utility (nDCU) [30], and the generalized version Expected Utility (EU) [29]. EU is very similar to -nDCG@k (d) but includes a cost factor. Being e the estimated effort for accessing one document, EU can be expressed as:",0,,False
244,|d|,0,,False
245,EU(d),0,,False
246,",",0,,False
247,"i ,1",0,,False
248,1,0,,False
249,+,0,,False
250,1 log(i ),0,,False
251,"r (t )r (di , t )(1 -  )c (i,t ) - e",0,,False
252,t T,0,,False
253,"EU inherits the -nDCG@k (d) properties, but capturing AspRel",0,,False
254,and Conf. However EU does still not satisfy MRed and Sat.,0,,False
255,The Cube Test metric (CT@k) [14] satisfies Sat by adding a,1,ad,True
256,saturation factor. Assuming a linear time effort w.r.t. the amount of,0,,False
257,"inspected documents, CT@k can be expressed as:",0,,False
258,|d|,0,,False
259,CT@k (d),0,,False
260,",",0,,False
261,"i ,1",0,,False
262,1 i,0,,False
263,t T,0,,False
264,"r (t )r (di , t )(1",0,,False
265,"-  )c (i,t ) fSat",0,,False
266,where fSat is 0 or 1 depending if the sum of relevance of documents for the aspect exceeds a certain saturation level. The reciprocal rank,0,,False
267,1,0,,False
268,"discounting factor i affects the constraint CloseTh, rewarding the positions of documents over the amount of relevant documents",0,,False
269,"in top area. In addition, Conf is neither satisfied. There is no con-",1,ad,True
270,tribution or penalty for documents with zero relevance.,0,,False
271,629,0,,False
272,Session 5C: New Metrics,1,Session,True
273,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
274,"Table 1 also includes the proposed metric Rank-Biased Utility (RBU), which we describe below.",0,,False
275,5 RANK-BIASED UTILITY,0,,False
276,The quality of a diversified ranking depends (at least) on the follow-,0,,False
277,ing factors: (i) the position of relevant documents in the ranking;,0,,False
278,(ii) the redundancy regarding each of the aspects covered by pre-,0,,False
279,viously observed documents; (iii) the weights of the aspects seen,0,,False
280,in the ranking and (iv) the effort ­ in terms of user cost or time ­,0,,False
281,derived from inspecting relevant or non-relevant documents. The,0,,False
282,analysis described in Section 4 shows that none of the existing met-,0,,False
283,"rics take into account all these factors. To fill this gap, we propose",0,,False
284,"Ranking-Biased Utility (RBU), which satisfies all the retrieval and",0,,False
285,diversity-oriented formal constraints (see proofs in the appendix).,0,,False
286,The analysis shows that RBP [16] is the only metric that satisfies,0,,False
287,"the four first relevance constraints, while ERR-IA@k [1, 6] is the",0,,False
288,only metric that satisfies all the five diversity-oriented constraints.,0,,False
289,"Expected Utility (EU) is the only that satisfies Conf, capturing the",0,,False
290,suitability of the ranking cutoff.,0,,False
291,"In order to satisfy every constraint, RBU combines the user ex-",0,,False
292,"ploration deepness model from RBP with the redundancy modeled in ERR-IA@k, and also adds the user effort component e in EU to",1,ad,True
293,satisfy the Conf constraint. The metrics RBP and ERR-IA@k can be combined together un-,0,,False
294,der the following user behavior assumptions: (i) The user has a,0,,False
295,probability p to explore the next document and (ii) the user has a,0,,False
296,"probability r (dj , t ) to get gain from document dj for the topic t. Similarly to the ERR-IA@k, the probability of being satisfied by",0,,False
297,document di after observing the documents that occur earlier in,0,,False
298,the ranking is:,0,,False
299,i -1,0,,False
300,"r (di , t ) (1 - r (dj , t ))",0,,False
301,"j ,1",0,,False
302,"Analogously to the user model followed by RBP, the resulting contribution of a document di in the position i must be weighted according to pi :",0,,False
303,i -1,0,,False
304,"pir (di , t ) (1 - r (dj , t ))",0,,False
305,"j ,1",0,,False
306,"In order to satisfy AspRel, the weighted sum of contributions across aspects in T is:",0,,False
307,i -1,0,,False
308,"pi w (t )r (di , t ) (1 - r (dj , t ))",0,,False
309,t T,0,,False
310,"j ,1",0,,False
311,And the cumulative gain across rank positions until k is:,0,,False
312,k,0,,False
313,i -1,0,,False
314,"pi w (t )r (di , t ) (1 - r (dj , t ))",0,,False
315,"i,1 t  T",0,,False
316,"j ,1",0,,False
317,"Similarly to EU, we define RBP in utility terms in order to capture",0,,False
318,"Conf. Being e the effort of observing a document, the rank biased",0,,False
319,"accumulated effort is weighted according to pi , that is:",0,,False
320,"k i ,1",0,,False
321,pi,0,,False
322,e,0,,False
323,.,0,,False
324,"Finally, combining the relevance contribution with the cumula-",0,,False
325,"tive effort, we obtain:",0,,False
326,k,0,,False
327,"RBU@k(d) , pi",0,,False
328,i -1,0,,False
329,"w (t )r (di , t ) 1 - r (dj , t ) - e (11)",0,,False
330,"i,1 t  T",0,,False
331,"j ,1",0,,False
332,"RBU@k matches with the RBP-IA metric when assuming a zero effort (e ,"" 0), and a small contribution of documents in terms of gain for query aspects,""",0,,False
333,i -1,0,,False
334,"r (di , t )  1 ,"" 1 - r (dj , t )  1 "",",0,,False
335,"j ,1",0,,False
336,"RBU@k(d) ,"" w (t ) pi-1r (di , t )1 - 0) "", w (t ) RBPt (d)",0,,False
337,t T,0,,False
338,j i,0,,False
339,t T,0,,False
340,"On the other hand, RBU@k is equivalent to the metric ERR-IA@k when the effort component is zero (e ,"" 0), and the probability of exploring the next document is maximal (p "", 1):",0,,False
341,k,0,,False
342,i -1,0,,False
343,1i,0,,False
344,"w (t )r (di , t ) (1 - r (dj , t )) - 0 , w (t ) ERRt @k (d)",0,,False
345,"i,1 t  T",0,,False
346,"j ,1",0,,False
347,t T,0,,False
348,"We now discuss the role of the effort component e, which repre-",0,,False
349,"sents the cost inherently associated to inspect a new document in the ranking.6 For instance, if e ,"" 0.1 and the inspected document di has a relevance of 0.1 to aspect ti , then the actual gain is zero:""",0,,False
350,"r (di , t ) 1 - r (dj , t ) - e , 0.1 (1 - 0) - 0.1 , 0",0,,False
351,j <i,0,,False
352,j <i,0,,False
353,We have introduced RBU@k and shown that the proposed metric satisfies all the relevance- and diversity-oriented formal con-,0,,False
354,straints. The experiments described in the following sections compare RBU@k to other metrics in the context of standard evaluation campaigns for search result diversification.,0,,False
355,6 EXPERIMENTS,0,,False
356,"We start defining our meta-evaluation metric. Then we evaluate the metrics in different scenarios based on the TREC Web Track 2014 adhoc retrieval task [9], which includes search result diversification. Finally, we corroborate our results under the context of the TREC Dynamic Domain task [28].7",1,TREC,True
357,6.1 Meta-evaluation: Metric Unanimity,0,,False
358,"We aim to quantify the ability of metrics to capture diversity in addition to traditional ranking quality aspects. For this purpose, we define the Metric Unanimity (MU). MU quantifies to what extent a metric is sensitive to quality aspects captured by other existing metrics. It follows a similar concept used by Strictness,8 proposed by Amigó et al. [3] for the ad-hoc retrieval scenario.",1,ad,True
359,"Our intuition is that, if a system improves another system for every quality criteria, this should be unanimously reflected by every metric. A metric that captures all quality criteria should reflect these improvements.",0,,False
360,"Considering the space of system output pair comparisons (i.e., Qd) > Q (d)), MU can be formalized as the Point-wise Mutual Information (PMI) between metric decisions and improvements",0,,False
361,"6In this work, the effort of inspecting or judging a relevant or non-relevant document is the same. We leave for future work the definition of formal constraints that consider these differences [21, 24]. 7Releasable data and scripts used in these experiments are available at https://github. com/jCarrilloDeAlbornoz/RBU. Diversity metrics and RBU are also included in the EvALL evaluation framework [2] http://evall.uned.es/. 8Strictness checks to what extent a metric can outscore metrics that achieve a low score according to other metrics.",0,,False
362,630,0,,False
363,Session 5C: New Metrics,1,Session,True
364,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
365,"Table 2: Metric Unanimity scores (MU) for the TREC Web Track 2014 ad-hoc retrieval task: official (Section 6.2) and simulated scenarios (Section 6.3). Given that normalization has not effect in terms of formal constraints and MU, which work at topic (query) level, normalized version of metrics behave similarly to the metric without normalization (e.g., MU(nDCG) , MU(DCG)) and therefore are not included.",1,TREC,True
366,Official,0,,False
367,"r (d ) ,"" rand(0, r (d )) r (t ) "","" rand(0, r (t ))""",0,,False
368,"|d| ,"" rand(0, |d|)""",0,,False
369,Simulated Scenarios,0,,False
370,"r (d ) ,"" rand(0, r (d )) r (t ) "","" rand(0, r (t ))""",0,,False
371,"|d| ,"" rand(0, 50)""",0,,False
372,"RBUe,"" {0.001, 0.05, 0.1, 0.5 }, p"",0.99  -DCG-IA@1000 ,""{0.1,0.25,0.5}""",0,,False
373,DCG@1000,0,,False
374,0.8024 0.7956 0.7956,0,,False
375,DCG-IA@1000,0,,False
376,0.7956,0,,False
377,"EU ,""{0.1,0.25,0.5},e"",""{0,0.05,0.1,0.5} 0.7956""",0,,False
378,ERR-IA@1000,0,,False
379,0.7956,0,,False
380,ERR@1000,0,,False
381,0.7956,0,,False
382,"NRBPp,""{0.8,0.9,0.99}, "",""{0.1,0.25,0.5} 0.7956""",0,,False
383,AP,1,AP,True
384,0.7926,0,,False
385,AP-IA,1,AP,True
386,0.7926,0,,False
387,"RBPp ,"" {0.8, 0.9, 0.99 } P-IA@20""",0,,False
388,0.7911 0.7272,0,,False
389,P@20,0,,False
390,0.7192,0,,False
391,RR-IA,0,,False
392,0.6835,0,,False
393,RR,0,,False
394,0.6486,0,,False
395,S-Recall@10,0,,False
396,0.3965,0,,False
397,S-Recall@20,0,,False
398,0.3538,0,,False
399,S-Recall@50,0,,False
400,0.3065,0,,False
401,S-Recall@100,0,,False
402,0.2478,0,,False
403,"RBUe,"" {0.001, 0.05, 0.1, 0.5 }, p"",0.99  -DCG-IA@1000 ,""{0.1,0.25,0.5,0.75}""",0,,False
404,DCG@1000,0,,False
405,0.8568 0.7734 0.7734,0,,False
406,DCG-IA@1000,0,,False
407,0.7734,0,,False
408,"EU ,""{0.1,0.25,0.5,0.75},e"",""{0,0.001,0.05,0.5} 0.7734""",0,,False
409,ERR-IA@1000,0,,False
410,0.7734,0,,False
411,ERR@1000,0,,False
412,0.7734,0,,False
413,AP,1,AP,True
414,0.7734,0,,False
415,AP-IA,1,AP,True
416,0.7734,0,,False
417,"NRBPp,""{0.8,0.9,0.99}, "",""{0.1,0.25,0.5,0.75} 0.7734""",0,,False
418,"RBPp,0.99",0,,False
419,0.7717,0,,False
420,"P@{20,50}",0,,False
421,0.7103,0,,False
422,"P-IA@{20,50}",0,,False
423,0.7103,0,,False
424,RR-IA,0,,False
425,0.6704,0,,False
426,RR,0,,False
427,0.6082,0,,False
428,S-Recall@10,0,,False
429,0.4238,0,,False
430,S-Recall@20,0,,False
431,0.4084,0,,False
432,S-Recall@50,0,,False
433,0.3658,0,,False
434,S-Recall@100,0,,False
435,0.3007,0,,False
436,"RBUe,"" {0.001, 0.05, 0.1, 0.5 }, p"","" {0.8, 0.9, 0.99 }""",0,,False
437,0.9808,0,,False
438," -DCG-IA@{50,100,1000} ,""{0.1,0.25,0.5,0.75} 0.7709""",0,,False
439,"DCG-IA@{50,100,1000}",0,,False
440,0.7709,0,,False
441,"EU ,""{0.1,0.25,0.5,0.75},e"",""{0,0.001,0.05,0.5} ERR-IA@{50,100,1000}""",0,,False
442,0.7709 0.7709,0,,False
443,"NRBPp,""{0.8,0.9,0.99}, "",""{0.1,0.25,0.5,0.75} DCG@{50,100,1000}""",0,,False
444,0.7709 0.7687,0,,False
445,"ERR@{50,100,1000}",0,,False
446,0.7679,0,,False
447,AP-IA,1,AP,True
448,0.7642,0,,False
449,AP,1,AP,True
450,0.7627,0,,False
451,"RBPp ,"" {0.8, 0.9, 0.99 } P-IA@20""",0,,False
452,0.7597 0.7077,0,,False
453,P-IA@10,0,,False
454,0.6888,0,,False
455,RR-IA,0,,False
456,0.6841,0,,False
457,RR,0,,False
458,0.6561,0,,False
459,S-Recall@10,0,,False
460,0.5137,0,,False
461,S-Recall@20,0,,False
462,0.4994,0,,False
463,S-Recall@100,0,,False
464,0.4831,0,,False
465,S-Recall@50,0,,False
466,0.4831,0,,False
467,"reported simultaneously by the rest of metrics. Formally, let be m a metric, M the rest of metrics, and a set of system outputs S. Being mi, j and Mi, j are statistical variables over system pairs (di , dj )  S2, indicating a system improvement according to the metric and to the rest of metrics, respectively: 9",0,,False
468,"mi, j  m(di ) > m(dj ) Mi, j  m  M. m(di )  m(dj )",0,,False
469,Then MU is formalized as:,0,,False
470,"MUM, S (m) ,"" PMI mi, j , Mi, j""",0,,False
471,", log",0,,False
472,"P (mi, j, Mi, j ) P (mi, j ) ·P (Mi, j )",0,,False
473,Let us consider the following example illustrated by the Table below:,0,,False
474,m1 m2 m3,0,,False
475,S1 1 0.8 1 S2 0.5 0.3 0.2 S3 0.2 0.4 0.5,0,,False
476,The example consists of three metrics and three system outputs.,0,,False
477,We now compute the MU of the metric m1 regarding the rest,0,,False
478,"of metrics M ,"" {m2, m3}. Here, there are 6 sorted pairs of sys-""",0,,False
479,"tem outputs: (S1, S2),(S2, S1), (S1, S3), etc. The improvements re-",0,,False
480,ported,0,,False
481,by,0,,False
482,m1,0,,False
483,are:,0,,False
484,"m11, 2 ,",0,,False
485,"m11,",0,,False
486,",",0,,False
487,3,0,,False
488,and,0,,False
489,"m12,",0,,False
490,.,0,,False
491,3,0,,False
492,The,0,,False
493,improvement,0,,False
494,re-,0,,False
495,"ported simultaneously by the other metrics are: M1,2, M1,3, and",0,,False
496,"9The a priori probability of a system improvement for every metric is fixed P (mi, j ) ,",0,,False
497,1 2,0,,False
498,.,0,,False
499,That,0,,False
500,"is,",0,,False
501,for,0,,False
502,the,0,,False
503,cases,0,,False
504,on,0,,False
505,which,0,,False
506,two,0,,False
507,system,0,,False
508,outputs,0,,False
509,obtain,0,,False
510,the,0,,False
511,same,0,,False
512,score,0,,False
513,m (di,0,,False
514,),0,,False
515,",",0,,False
516,"m (dj ), we add 0.5 to the statistical count.",1,ad,True
517,"M3,2. m1 agrees with M in two cases. Therefore MUM (m1) ,",0,,False
518,log,0,,False
519,2/6 3/6·3/6,0,,False
520,", 0.415.",0,,False
521,MU has four properties that we describe below.,0,,False
522,Property 1. Capturing every unanimous improvement maximizes MU regardless the other decisions:,0,,False
523,"MUM, S (m) , log",0,,False
524,"P (mi, j , Mi, j ) 1 ·k",0,,False
525," P (mi, j , Mi, j )",0,,False
526,2,0,,False
527,"Property 2. A metric mrand which assigns random or constant scores to every system outputs achieves a zero MU, capturing",0,,False
528,the sensitivity of metrics:,0,,False
529,"MUM, S (mrand) , log",0,,False
530,1 2,0,,False
531,·,0,,False
532,"P (Mi, j )",0,,False
533,1 2,0,,False
534,·,0,,False
535,"P (Mi, j )",0,,False
536,", log(1) , 0",0,,False
537,Property 3. MU is asymmetric. A metric m can be unanimous,0,,False
538,"regarding the rest of metrics, while the rest of metrics are",0,,False
539,not.,0,,False
540,"MU{m2,m3 } (m1) MU{m1,m3 } (m2) MU{m1,m2 } (m3)",0,,False
541,Property 4. MU is not affected by the predominance of a certain family of metrics in the set M:,0,,False
542,"MUM{m }, S (m) ,"" MUM{m,m, ...,m }, S (m)""",0,,False
543,6.2 Experiment 1: TREC Web Track 2014,1,TREC,True
544,"This first experiments aims to measure MU in a standard diversification evaluation campaign: the TREC Web Track 2014 ad-hoc retrieval task [9]. In this benchmark, systems need to perform adhoc retrieval from the ClueWeb-12 collection, for a total of 50 test topics and return the top 10,000 documents. Some of the topics have",1,TREC,True
545,631,0,,False
546,Session 5C: New Metrics,1,Session,True
547,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
548,"multiple aspects ­therefore, diversified rankings may be more effective. We use the 30 official runs submitted to the ad-hoc retrieval task and available at TREC's website.",1,ad-hoc,True
549,"Using our own implementation of the metrics, we execute over the official runs the following metrics: AP, RR, AP-IA and RR-IA which do not require any parameter; P@k, ERR@k, NDCG@k and their corresponding intent-aware variants, using k  {10, 20, 50, 100, 1000}; S-Recall@k, RBP, NRBP and -nDCG@k; with p  {0.8, 0.9, 0.99} and   {0.1, 0.25, 0.5, 0.75}; EU and our proposed metric RBU with the effort parameter e  {0.001, 0.05, 0.1, 0.5}.",1,AP,True
550,"For metrics that do not accept multiple query aspects, we consider the maximum relevance across aspects: r (d ) ,"" maxt T (r (d, t )).""",0,,False
551,"The first column in Table 2 shows the metrics ranked by MU. For the sake of clarity, the table includes for each metric the variant with highest MU. Results show that metrics that satisfy only a few constraints such as P@k or S-Recall@k are substantially less unanimous than the rest of metrics. This means that metrics with higher scores cover the same quality criteria captured by P@k or S-Recall@k, but these two metrics do not capture other criteria captured by the rest of metrics.",0,,False
552,"Our second observation is that a metric with a shallow cutoff (e.g., ERR@50) ­ i.e., it takes into account a few documents in the ranking ­ has lower MU score than its deep counterpart (e.g., ERR@1000). This behavior is consistent for every metric and variants. Likewise, higher values for the patience parameter p in RBP obtains higher MU scores. Intuitively, the shallower the metric is, the less probable is to capture improvements in deep ranking positions.",0,,False
553,"RBU obtains the highest scores, when p ,"" 0.99 (i.e., the metric considers deep positions in the ranking) and all the tested values for the effort component e.""",0,,False
554,6.3 Experiment 2: Simulating Alternative Scenarios,0,,False
555,"In order to study the behavior of metrics under different situations and to corroborate our findings, we repeat the experiment described before after artificially modifying some parameters of the official TREC Web Track experimental setup.",1,TREC,True
556,The second column in Table 2 shows the results when:,0,,False
557,"(1) Enforcing all relevance judgments to be graded: we replace each discrete relevance value r by a random value between zero and r : r (d ) , rand(0..r (d )). This is related to the MRed constraint.",1,ad,True
558,"(2) Randomly assigning a certain weight to each aspect t in such a way that the sum of the weights for each topic (or query) adds up to 1: w (t ) , rand(0..1) and t T w (t ) , 1. This is related to the AspRel constraint.",1,ad,True
559,"(3) The ranking of documents returned by each system is manipulated by reducing randomly its length: |d| ,"" rand(0, . . . , |d|). This variation simulates the situation in which systems should cut their output rankings according to their confidence of retrieving (or not) more relevant documents. This tuning is related to the Conf constraint, which is only satisfied by EU and the proposed metric.""",0,,False
560,"As a result, the difference in terms of MU scores between RBU and the other metrics is larger in this simulated scenario. The experiment suggests that this effect is not due to the fact of satisfying any",0,,False
561,"single constraint, but satisfying several constraints simultaneously. Although EU satisfies Conf and ERR-IA@k satisfies MRed and Sat, RBU outperforms both metrics in terms of MU.",0,,False
562,"In all the previous experiments, we have seen that MU rewards the fact of considering deeper positions in the ranking. In order to isolate this variable, the next simulation (Table 2, third column) reduces the length of rankings substantially, by defining a random cutoff between 0 and 50: |d| ,"" rand(0..50). Consequently, metrics that use a cutoff equal or greater than k "","" 50 will not be rewarded by MU. Remarkably, all the RBU variants with an effort parameter e higher than zero obtain the highest MU scores ­ RBU with e "", 0 (omitted in the table) achieves a 0.7709 MU score.",0,,False
563,This suggests that the effort component e plays an important role when evaluating rankings with different lengths.,0,,False
564,6.4 Experiment 3: Considering Metrics and Default Parameters used in Official Evaluation,0,,False
565,"MU scores depend on the set of metrics in consideration. Therefore, the results could be biased by the selected metric set M and variants. In order to avoid this bias, we consider the official metrics and parameters used by the TREC Web Track organizers. In addition, to avoid the effect of implementation variations or bugs, we compare RBU (implemented by ourselves) against the official evaluation scores released by TREC (first column in Table 3).",1,TREC,True
566,"In this case, AP-IA gets the highest MU score. In terms of RBU, we can see that p values and MU scores are correlated. This shows again than MU is biased by the the amount of documents in the ranking that are visible to the metric. Note that most of metrics proposed by the organizers use a cutoff no greater than k ,"" 20. That is, most of metrics receive less information than AP-IA or NRBP, which take into account all the documents in the ranking.""",1,AP,True
567,"In order to avoid this effect, we focus on metrics that apply the the cutoff k ,"" 20, and we apply the same cutoff to RBU: RBU@2010 Maintaining the amount of documents visible to metrics constant, RBU achieves the same MU score (0.9556) for all the tested variants, obtaining the highest MU score among the metrics. This suggests that the RBU performance in terms of MU is not due to differences in the length of the observed ranking.""",0,,False
568,"The high MU scores of RBU could be possibly due to the fact of having an explicit component for the user effort (e parameter), rather than the ability to capture other quality aspects such as diversity and redundancy. In order to isolate this variable, we consider only three RBU variants with zero value in the effort parameter (e ,"" 0, p "","" {0.8, 0.9, 0.99}). Results at the bottom of second column in Table 3 show that RBU also outperforms the rest of metrics when e "", 0.",0,,False
569,6.5 Experiment 4: Validation using TREC Dynamic Domain Track,1,TREC,True
570,"In order to check the robustness of our empirical conclusions, we repeat the same experiment over TREC Dynamic Domain 2015 [28], which includes 23 official runs. This track consists of an interactive",1,TREC,True
571,"10In this experiment we use the official evaluation scores. Therefore, we cannot adapt AP-IA nor NRBP to this cutoff.",1,ad,True
572,632,0,,False
573,Session 5C: New Metrics,1,Session,True
574,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
575,Table 3: MU scores over official metrics in TREC Web Track 2014 and TREC Dynamic Domain Track 2015.,1,TREC,True
576,TREC Web Track 2014 (Official Metrics),1,TREC,True
577,TREC Dynamic Domain 2015 (Official Metrics),1,TREC,True
578,Official,0,,False
579,"k , 20",0,,False
580,Official,0,,False
581,AP-IA,1,AP,True
582,"RBUe,"" {0, 0.001, 0.05, 0.1, 0.5 }, p"",0.99 RBUe,"" {0, 0.001, 0.05, 0.1, 0.5 }, p"",0.9 RBUe,"" {0, 0.001, 0.05, 0.1, 0.5 }, p"",""0.8 { -DCG, -nDCG }@20""",0,,False
583,0.9771 0.9770..0.9766 0.9763..0.9760 0.9760..0.9750,0,,False
584,0.9540,0,,False
585,"ERR-IA@20, nERR-IA@20",0,,False
586,0.9539,0,,False
587,"NRBP, nNRBP",0,,False
588,0.9509,0,,False
589,"{ ERR-IA, nERR-IA, -DCG, -nDCG }@10 0.9373",0,,False
590,P-IA@20,0,,False
591,0.9310,0,,False
592,"RBUe,"", p"",",0,,False
593,0.9556,0,,False
594,"{ -nDCG, -nDCG }@20 0.9427",0,,False
595,"{ ERR-IA, nERR-IA }@20 0.9425",0,,False
596,P-IA@20,0,,False
597,0.9080,0,,False
598,S-Recall@20,0,,False
599,0.4141,0,,False
600,"k ,"" 20, e "", 0",0,,False
601,"RBUe,"" {0.001, 0.05, 0.1, 0.5 }, p"",0.99 RBUe,""0.001, p"",0.9 RBUe,"" {0.05, 0.1 }, p"",0.9 RBUe,""0.5, p"",0.9 RBUe,""0.001, p"",0.8 RBUe,"" {0.05, 0.1, 0.5 }, p"",0.8 ACT@10",0,,False
602,ERR (Arith. Mean),0,,False
603,CT@10,0,,False
604,0.8488 0.8453 0.8441 0.8440 0.8406 0.8396 0.6276 0.5955 0.5938,0,,False
605,"P-IA@10 { -DCG, -nDCG }@5 { ERR-IA, nERR-IA }@5 P-IA@5 S-Recall@5 S-Recall@10 S-Recall@20",0,,False
606,0.9071 0.9001 0.8999 0.8720 0.5573 0.5001 0.4515,0,,False
607,"RBUe,""0, p"","" {0.8, 0.9, 0.99 } { -DCG, -nDCG }@20 { ERR-IA, nERR-IA }@20""",0,,False
608,P-IA@20,0,,False
609,S-Recall@20,0,,False
610,0.9556 0.9428 0.9425 0.9081 0.4146,0,,False
611,"RBUe,""0, p"","" {0.8, 0.9, 0.99 } ERR (Harm. Mean) P@Recall P@Recall (modified) RR@10""",0,,False
612,0.5937 0.5912 0.1162 0.1044 0.1031,0,,False
613,"search scenario. Systems receive aspect-level feedback iteratively and need to dynamically retrieve as many relevant documents for aspects as possible, using as few iterations as possible. An important particularity of this task is that the system must predict the optimal ranking cutoff which is closely related with the Conf constraint. The official metrics used in this track are Cube Test (CT@k) and Averaged Cube Test (ACT@k) [14], which are included in our experiments.",0,,False
614,"The rightmost column in Table 3 shows that we obtain similar results: all the RBU variants are at the top of the metrics ranking. In this case, the user effort parameter e is important, given that it is necessary to outperform other metrics such as CT@k or ACT@k. In addition, we achieved again the same result when considering only one RBU variant, appearing at the top in terms of MU scores.",1,ad,True
615,7 CONCLUSIONS,0,,False
616,"We defined an axiomatic framework to analyze diversity metrics and found that none of the existing metrics satisfy all the constraints. Inspired by this analysis, we proposed Rank-Biased Utility (RBU, Equation 11), which satisfies all the formal constraints. Our experiments over standard diversity evaluation campaigns show that the proposed metric has more unanimity than the official metrics used in the campaigns, i.e., RBU captures more quality criteria than the ones captured by other metrics. We believe our contributions would help researchers and analysts to define their evaluation framework (e.g., which evaluation metric should be used?) in order to analyze the effectiveness of systems in the context of scenarios involving search result diversification. Future work includes a further parameter sensitivity analysis of metrics, as well as the study of other meta-evaluation criteria such as sensitivity or robustness against noise.",0,,False
617,Acknowledgments. This research was partially supported by the Spanish Government (project Vemodalen TIN2015-71785-R) and the Australian Research Council (project LP150100252). The authors wish to thank the reviewers for their valuable feedback.,1,Gov,True
618,REFERENCES,0,,False
619,"[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying Search Results. In Proc. WSDM. 5­14.",0,,False
620,"[2] Enrique Amigó, Jorge Carrillo-de Albornoz, Mario Almagro-Cádiz, Julio Gonzalo, Javier Rodríguez-Vidal, and Felisa Verdejo. 2017. EvALL: Open Access Evaluation for Information Access Systems. In Proc. SIGIR. 1301­1304.",0,,False
621,"[3] Enrique Amigó, Julio Gonzalo, and Felisa Verdejo. 2013. A General Evaluation Measure for Document Organization Tasks. In Proc. SIGIR. 643­652.",0,,False
622,[4] Luca Busin and Stefano Mizzaro. 2013. Axiometrics: An Axiomatic Approach to Information Retrieval Effectiveness Metrics. In Proc. ICTIR. 8.,0,,False
623,[5] Praveen Chandar and Ben Carterette. 2013. Preference Based Evaluation Measures for Novelty and Diversity. In Proc. SIGIR. 413­422.,1,Novelty,True
624,"[6] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In Proc. CIKM. 621­630.",1,ad,True
625,"[7] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Büttcher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In Proc. SIGIR. 659­666.",1,Novelty,True
626,"[8] Charles L. Clarke, Maheedhar Kolla, and Olga Vechtomova. 2009. An Effectiveness Measure for Ambiguous and Underspecified Queries. In Proc. ICTIR. 188­199.",0,,False
627,"[9] Kevyn Collins-Thompson, Craig Macdonald, Paul Bennett, Fernando Diaz, and Ellen M Voorhees. 2015. TREC 2014 Web Track Overview. In Proc. TREC.",1,TREC,True
628,"[10] Marco Ferrante, Nicola Ferro, and Maria Maistro. 2015. Towards a Formal Framework for Utility-oriented Measurements of Retrieval Effectiveness. In Proc. ICTIR. 21­30.",0,,False
629,"[11] Peter B. Golbus, Javed A. Aslam, and Charles L. A. Clarke. 2013. Increasing Evaluation Sensitivity to Diversity. Inf. Retr. 16, 4 (2013), 530­555.",0,,False
630,"[12] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Sys. 20 (2002), 422­446.",0,,False
631,"[13] Teerapong Leelanupab, Guido Zuccon, and Joemon M. Jose. 2013. Is IntentAware Expected Reciprocal Rank Sufficient to Evaluate Diversity?. In Proc. ECIR. 738­742.",0,,False
632,"[14] Jiyun Luo, Christopher Wing, Hui Yang, and Marti Hearst. 2013. The Water Filling Model and the Cube Test: Multi-dimensional Evaluation for Professional Search. In Proc. CIKM. 709­714.",0,,False
633,[15] Alistair Moffat. 2013. Seven Numeric Properties of Effectiveness Metrics. In Proc. Asia Info. Retri. Soc. Conf. 1­12.,0,,False
634,"[16] Alistair Moffat and Justin Zobel. 2008. Rank-Biased Precision for Measurement of Retrieval Effectiveness. ACM Trans. Inf. Sys. 27, 1 (2008), 2:1­2:27.",0,,False
635,"[17] Tetsuya Sakai, Nick Craswell, Ruihua Song, Stephen Robertson, Zhicheng Dou, and Chin yew Lin. 2010. Simple Evaluation Metrics for Diversified Search Results. In Proc. EVIA. 42­50.",0,,False
636,[18] Tetsuya Sakai and Ruihua Song. 2011. Evaluating Diversified Search Results Using Per-intent Graded Relevance. In Proc. SIGIR. 1043­1052.,1,ad,True
637,"[19] Rodrygo L. T. Santos, Craig Macdonald, and Iadh Ounis. 2015. Search Result Diversification. Found. & Trends in IR 9, 1 (2015), 1­90.",1,ad,True
638,"[20] Falk Scholer, Diane Kelly, and Ben Carterette. 2016. Information Retrieval Evaluation Using Test Collections. Inf. Retr. 19, 3 (2016), 225­229.",0,,False
639,[21] Mark D. Smucker and Charles L.A. Clarke. 2012. Time-based Calibration of Effectiveness Measures. In Proc. SIGIR. 95­104.,0,,False
640,633,0,,False
641,Session 5C: New Metrics,1,Session,True
642,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
643,"[22] Karen Sparck Jones and Cornelis J. van Rijsbergen. 1976. Information Retrieval Test Collections. J. Documentation 32, 1 (1976), 59­75.",0,,False
644,[23] Ake Tangsomboon and Teerapong Leelanupab. 2014. Evaluating Diversity and Redundancy-Based Search Metrics Independently. In Proc. Aust. Doc. Comp. Symp. 42­49.,0,,False
645,"[24] Andrew Turpin, Falk Scholer, Kalvero Jarvelin, Mingfang Wu, and J. Shane Culpepper. 2009. Including Summaries in System Evaluation. In Proc. SIGIR. 508­515.",0,,False
646,"[25] Cornelis J. van Rijsbergen. 1974. Foundation of Evaluation. J. Documentation 30, 4 (1974), 365­373.",0,,False
647,[26] Ellen M. Voorhees. 1999. The TREC-8 Question Answering Track Report. In Proc. TREC. 77­82.,1,TREC,True
648,[27] Ellen M. Voorhees and Donna K. Harman. 2005. TREC: Experiment and Evaluation in Information Retrieval. Vol. 1. MIT Press Cambridge.,1,TREC,True
649,"[28] Hui Yang, John Frank, and Ian Soboroff. 2015. Overview of the TREC 2015 Dynamic Domain Track. In Proc. TREC.",1,TREC,True
650,[29] Yiming Yang and Abhimanyu Lad. 2009. Modeling Expected Utility of Multisession Information Distillation. In Proc. ICTIR. 164­175.,1,ad,True
651,"[30] Yiming Yang, Abhimanyu Lad, Ni Lao, Abhay Harpale, Bryan Kisiel, and Monica Rogati. 2007. Utility-based Information Distillation over Temporally Sequenced Documents. In Proc. SIGIR. 31­38.",1,ad,True
652,"[31] Haitao Yu, Adam Jatowt, Roi Blanco, Hideo Joho, and Joemon M. Jose. 2017. An In-depth Study on Diversity Evaluation: The Importance of Intrinsic Diversity. Inf. Proc. & Man. 53 (2017), 799­813.",0,,False
653,"[32] Cheng Xiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In Proc. SIGIR. 10­17.",0,,False
654,APPENDIX: FORMAL PROOFS,1,AP,True
655,"Proof. Rank-Biased Utility (RBU, Eq. 11) satisfies the constraints: Pri (Eq. 1), Deep (Eq. 2), DeepTh (Eq. 3) and CloseTh (Eq. 4). RBU",0,,False
656,is defined as:,0,,False
657,k,0,,False
658,i -1,0,,False
659,"RBU@k(d) , pi",0,,False
660,"w (t )r (di ) (1 - r (dj, t )) - e",0,,False
661,"i ,1 t T",0,,False
662,"j ,1",0,,False
663,"In the context of these constraints, it is assumed that there is only a single aspect t for a given query or topic. Therefore, RBU can be expressed as:",0,,False
664,k,0,,False
665,i -1,0,,False
666,"RBU@k(d) ,"" pi r (di ) (1 - r (dj, t )) - e""",0,,False
667,"i ,1",0,,False
668,"j ,1",0,,False
669,"In addition, the condition relevance contribution is assumed, i.e., the rele-",1,ad,True
670,"vance of single documents does not completely cover the user information needs r (d )  1. Therefore, we can assume that",0,,False
671,i -1,0,,False
672,i -1,0,,False
673,"(1 - r (dj, t ))  1 , 1",0,,False
674,"j ,1",0,,False
675,"j ,1",0,,False
676,"Finally, the four constraints compare rankings with the same length. This",0,,False
677,"means that we can eliminate the user cost component e, which is e",0,,False
678,"k i ,1",0,,False
679,p,0,,False
680,i,0,,False
681,"for every ranking in comparison. Under all these assumptions, RBU is",0,,False
682,equivalent to the traditional RBP metric [16]:,1,ad,True
683,k,0,,False
684,"RBU@k(d)  pi r (di ) , RBP@k(d)",0,,False
685,"i ,1",0,,False
686,"According to the study by Amigó et al. [3], RBP satisfies the four constraints",0,,False
687,enumerated above.,0,,False
688,Proof. RBU satisfies the Conf constraint (Eq. 5). RBU can be expressed as:,0,,False
689,k,0,,False
690,i -1,0,,False
691,k,0,,False
692,"RBU@k(d) , pi",0,,False
693,"w (t )r (di ) (1 - r (dj , t )) - e pi",0,,False
694,"i ,1 t T",0,,False
695,"j ,1",0,,False
696,"i ,1",0,,False
697,then,0,,False
698,"RBU@k d > RBU@k d, d ¬r el ",0,,False
699,RBU@k(d) > RBU@k(d) - pn+1e  0 > -pn+1e,0,,False
700,Proof. RBU satisfies the AspDiv constraint (Eq. 6). Under the,0,,False
701,constraint conditions: RBU ddi di > RBU d is equivalent to:,0,,False
702,i -1,0,,False
703,i -1,0,,False
704,pi,0,,False
705,"w (t )r (di, t ) (1 - r (dj , t )) > pi",0,,False
706,"w (t )r (di , t ) (i - r (dj , t )) ",0,,False
707,t T,0,,False
708,"j ,1",0,,False
709,t T,0,,False
710,"j ,1",0,,False
711,"w (t )r (di, t ) >",0,,False
712,"(w (t )r (di , t )) ",0,,False
713,"r (di, t ) >",0,,False
714,"(r (di , t )) ",0,,False
715,t T,0,,False
716,t T,0,,False
717,t T,0,,False
718,t T,0,,False
719,"t  T r (di, t ) > r (di , t )",0,,False
720,Proof. RBU satisfies the Red constraint (Eq. 7). Under the constraint conditions:,0,,False
721,"RBU d, d > RBU d, d ",0,,False
722,|d|,0,,False
723,|d|,0,,False
724,"w (t )r (d, t ) (1 - r (dj , t )) > w (t )r (d, t ) (1 - r (dj , t )) ",0,,False
725,"j ,1",0,,False
726,"j ,1",0,,False
727,|d|,0,,False
728,|d|,0,,False
729,"(1 - r (dj , t )) > (1 - r (dj , t )) ",0,,False
730,"j ,1",0,,False
731,"j ,1",0,,False
732,"(1 - rc ) di d|r (di , t ),""rc > (1 - rc ) d d|r (d, t )"",rc ",0,,False
733,"di  d|r (di , t ) ,"" rc > d  d|r (d, t ) "", rc",0,,False
734,Proof. RBU satisfies the MRed constraint (Eq. 8). Under the constraint conditions:,0,,False
735,"RBU d, d > RBU d, d ",0,,False
736,|d|,0,,False
737,|d|,0,,False
738,"w (t )r (d, t ) (1 - r (dj , t )) > w (t )r (d, t ) (1 - r (dj , t )) ",0,,False
739,"j ,1",0,,False
740,"j ,1",0,,False
741,|d|,0,,False
742,|d|,0,,False
743,"(1 - r (dj , t )) > (1 - r (dj , t ))  di  d. (r (di , t ) > r (di , t ))",0,,False
744,"j ,1",0,,False
745,"j ,1",0,,False
746,"Proof. RBU satisfies the Sat constraint (Eq. 9). There exists a relevance value r (dn, t ) , rmax , 1 large enough such that:",0,,False
747,n,0,,False
748,i -1,0,,False
749,n,0,,False
750,"RBU d, dn+1 , pi",0,,False
751,"w (t )r (di ) (1 - r (dj , t )) - e pi +",0,,False
752,"i ,1 t T",0,,False
753,"j ,1",0,,False
754,"i ,1",0,,False
755,n-1,0,,False
756,"w (t )r (dn+1 )(1 - r (dn, t )) (1 - r (dj , t )) - epn+1",0,,False
757,tT,0,,False
758,"j ,1",0,,False
759,"Given that t  t (r (dn+1, t ) ,"" 0), it is equivalent to:""",0,,False
760,n,0,,False
761,i -1,0,,False
762,n,0,,False
763,"RBU d, dn+1 , pi",0,,False
764,"w (t )r (di ) (1 - r (dj , t )) - e pi +",0,,False
765,"i ,1 t T",0,,False
766,"j ,1",0,,False
767,"i ,1",0,,False
768,"n-1 w (t )r (dn+1 )(1 - r (dn, t )) (1 - r (dj , t )) - epn+1",0,,False
769,"j ,1",0,,False
770,"Given that 1 - r (dn, t ) ,"" 0, we obtain:""",0,,False
771,n,0,,False
772,i -1,0,,False
773,n,0,,False
774,"RBU d, dn+1 , pi",0,,False
775,"w (t )r (di ) (1 - r (dj , t )) - e pi + 0 , RBU d",0,,False
776,"i ,1 t T",0,,False
777,"j ,1",0,,False
778,"i ,1",0,,False
779,Proof. RBU satisfies the AspRel constraint (Eq. 10).,0,,False
780,Under the constraint conditions:,0,,False
781,RBU ddi di > RBU d ,0,,False
782,i -1,0,,False
783,i -1,0,,False
784,"w (t )r (d, t ) (1 - r (dj , t )) > w (t )r (d, t ) (1 - r (dj , t )) ",0,,False
785,"j ,1",0,,False
786,"j ,1",0,,False
787,i -1,0,,False
788,i -1,0,,False
789,"w (t )r (d, t ) 1 > w (t )r (d, t ) 1  w (t ) > w (t )",0,,False
790,"j ,1",0,,False
791,"j ,1",0,,False
792,634,0,,False
793,,0,,False

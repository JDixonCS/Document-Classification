,sentence,label,data,regex
0,Session 3C: Question Answering,1,Session,True
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Multihop Attention Networks for Question Answer Matching,0,,False
3,"Nam Khanh Tran, Claudia Niederée",0,,False
4,"L3S Research Center, Leibniz Universität Hannover Hannover, Germany",0,,False
5,"{ntran,niederee}@L3S.de",0,,False
6,ABSTRACT,0,,False
7,"Attention based neural network models have been successfully applied in answer selection, which is an important subtask of question answering (QA). These models often represent a question by a single vector and find its corresponding matches by attending to candidate answers. However, questions and answers might be related to each other in complicated ways which cannot be captured by single-vector representations. In this paper, we propose Multihop Attention Networks (MAN) which aim to uncover these complex relations for ranking question and answer pairs. Unlike previous models, we do not collapse the question into a single vector, instead we use multiple vectors which focus on different parts of the question for its overall semantic representation and apply multiple steps of attention to learn representations for the candidate answers. For each attention step, in addition to common attention mechanisms, we adopt sequential attention which utilizes context information for computing context-aware attention weights. Via extensive experiments, we show that MAN outperforms state-ofthe-art approaches on popular benchmark QA datasets. Empirical studies confirm the effectiveness of sequential attention over other attention mechanisms.",1,ad,True
8,KEYWORDS,0,,False
9,"Answer selection, non-factoid QA, representation learning, attention mechanism",0,,False
10,"ACM Reference Format: Nam Khanh Tran, Claudia Niederée. 2018. Multihop Attention Networks for Question Answer Matching. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210009",0,,False
11,1 INTRODUCTION,1,DUC,True
12,Answer selection (AS) is an important subtask of question answering (QA) that enables selecting the most suitable answer from a list of candidate answers in regard to the input question. One main challenge of this task lies in the complex and versatile semantic relations that can be observed between questions and answers. While for factoid QA the task of answer selection may be largely cast as a,0,,False
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210009",1,ad,True
14,Table 1: An example of a question with an answer from the FiQA dataset. The segments in the answer are related to the segments in the question by the same color.,0,,False
15,Question: Are companies in California obliged to provide invoices?,0,,False
16,"Ground-truth answer: We run into this all the time with our EU clients. As far as I can tell, the only requirements when it comes to invoicing have to do with sales tax, which is determined at the state level, and only in the case that items are taxable. It seems that the service provided to you is not taxable and so there is no obligation under Californian law to provide what you need.",0,,False
17,"textual entailment problem, for non-factoid QA what makes an answer better than another often depends on many factors. Different from many other matching tasks, the linguistic similarities between questions and answers may or may not be indicative for the good answers; depending on what the question is looking for, a good answer may come in different forms. Sometimes a correct answer completes the question precisely with the missing information, and in other scenarios, good answers need to elaborate part of the question to rationalize it, and so on. In other cases, the best answers can also be noisy and include extraneous information irrelevant to the question. In addition, while a good answer must relate to the question, they might not share common lexical units. For example, in the question in Table 1, ""companies"" is not directly mentioned in the answer. This issue may confuse simple word-matching systems.",1,ad,True
18,"These challenges consequently make the traditional models which are commonly based on lexical features [29, 30, 33] less effective compared to deep learning based methods [8, 28]. The neural models often follow the two step procedure: Firstly, representations of questions and answers are learned via a neural encoder such as long short-term memory (LSTM) networks or convolutional neural networks (CNN); Secondly, these representations of questions and answers are composed by an interaction function to produce an overall matching score. In the first step, each word in a question or an answer sequence is first represented with a hidden vector and then all the hidden vectors are aggregated for sequence representations. These models have shown very successful results in the AS task, however they still suffer from an important issue. The answers can be very long and contain lots of words that are not related to the question at hand, especially in non-factoid QA; consequently, the resulting representation might be distracted by non-useful information.",1,ad,True
19,"Recent years, attention-based models are proposed to deal with this challenge and have shown great success in many tasks such",0,,False
20,325,0,,False
21,Session 3C: Question Answering,1,Session,True
22,"SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
23,"as machine translation [2, 23], machine reading comprehension [11] and textual entailments [19]. In the AS task, attention-based approaches aim to focus on segments within the candidate answer that are most related to the question [24, 27]. The segments with a stronger focus are treated as more important and have more influence on the resulting representations. For example, in attentionbased LSTM models [24] as shown in Figure 2, a weight is automatically generated for each word in the answer via an attention model, and the answer is represented as the weighted sum of the hidden vectors. Various attention mechanisms have been proposed in previous studies in which additive attention [2] and multiplicative attention [20] are the two most commonly used. While additive attention is associated with a multi-layer perceptron for computing attention weights, multiplicative attention uses inner product for the weight estimations. Though these attention mechanisms have shown promising results in answer selection, they do not make use of surrounding word context when calculating attention weights, which has been proved to enhance the performance of LSTM based QA [5]. To address this issue, we adopt another attention mechanism, i.e. sequential attention [3] in which an additional LSTM is added to compute a context-aware weight for each hidden vector. This mechanism helps generate more accurate answer representation regarding to the question.",1,ad,True
24,"A common characteristic of the previous attention-based approaches is that the question is represented by one feature vector and a round of attention is applied for learning the representation of the answer. However, in many cases different segments of the answer can be related to different parts of the question. For example, in Table 1 the segment ""the only requirements when it comes to invoicing have to do with sales tax"" is relevant to ""provide invoices"" mentioned in the question, while ""there is no obligation under Californian law"" answers ""California obliged"" stated in the question. Consequently, using one feature vector pair for question answer matching may be not capable of capturing the complex semantic relations between questions and answers. This can lead to suboptimal results. Clearly, it is expected that the more aspects an answer covers the better the answer is. A good system should reflect this expectation.",1,ad,True
25,"In this paper, we propose Multihop Attention Networks (MANs) to deal with this problem. MANs locate, via multiple steps of attention, answer segments that are relevant to different aspects of the question. As illustrated in Figure 1(b), the MAN first uses the question vector to deduce the answer vector in the first attention layer, then the question vector is refined in the next step to learn the answer vector in the second attention layer. Each attention step gives a different attention distribution focusing on the segments that are relevant to one aspect of the question. Finally, we sum up the matching score in each step for scoring the answer. We perform experiments on both factoid QA and non-factoid QA datasets. Experimental results show that our proposed models obtain highly competitive results and outperform state-of-the-art approaches.",0,,False
26,The main contributions of our paper can be summarized as follows:,0,,False
27,· We are the first to investigate the effectiveness of sequential attention mechanism for answers' attentive representations in answer selection.,0,,False
28,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
29,"Nam Khanh Tran, Claudia Niederée",0,,False
30,"· We propose Multihop Attention Networks which represent questions by multiple vectors and use multiple steps of attention for learning the representation of answers. By doing this, MANs can capture different semantic relations between questions and answers.",0,,False
31,· We provide extensive experimental evidence of the effectiveness of our model on both factoid question answering and community-based question answering on different domains. Our proposed approach outperforms many other neural architectures on these datasets.,0,,False
32,2 RELATED WORK,0,,False
33,"Our work is concerned with ranking question and answer pairs to select the most relevant answer for each question. Previous work on this task have primarily used feature engineering, linguistic tools, or external resources [29, 30, 33]. In [33], Yih et al. constructed semantic features based on WordNet and paired semantically related words based on word semantic relations. In [29, 30], the answer selection problem was transformed to a syntactical matching between the question and answer parse trees. Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees [9, 21, 32]. However, apart from relying on the availability of additional resources, the effort of feature engineering and the systematic complexity introduced by the linguistic tools, such models have limited performance and are outperformed by modern deep learning approaches [22, 35].",1,ad,True
34,"Yu et al. [35] employed a convolutional neural network (CNN) for feature learning of QA pairs and subsequently applied logistic regression for prediction. Despite its simplicity, the approach outperforms all traditional approaches [21, 32, 33]. Another attractive quality of deep learning architectures is that features can be learned in an end-to-end fashion. Severyn et al. [22] presented a unified architecture that trains a convolutional neural network together with a multi-layer perceptron, in which features are learned while the parameters of the network are optimized for the task at hand. In addition to CNN, recurrent neural networks such as the long short-term memory (LSTM) networks are also very popular for learning sequence representation and have been widely adopted in QA [24, 25, 28]. In [28], Wang and Nyberg incorporate stacked LSTMs to learn a joint feature vector of question and answer for classification. In [25], Tan et al. combined CNN and LSTM into a hybrid architecture which utilizes the advantages of both architectures. However, these approaches are outperformed by models with attention mechanism.",1,ad,True
35,"Recently, attention-based systems have shown very promising results on a variety of NLP tasks, such as machine translation [2, 23], machine reading comprehension [11], text summarization [20] and textual entailment [19]. Such models learn to focus their attention to specific parts of their input and most of them are based on a one-way attention, in which the attention is basically applied over one type of input based on another input (e.g. over target languages based on the source languages for machine translation, or over documents according to queries for reading comprehension). Most recently, several two-way attention mechanisms are proposed [7, 19, 34], where the information from two input items can influence the computation of each others representations. Both types of attention",1,ad,True
36,326,0,,False
37,Session 3C: Question Answering Multihop Attention Networks for Question Answer Matching,1,Session,True
38,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
39,"show similar performances on AS [7, 25], thus in this paper we only use the one-way attention. However, unlike the previous work, our proposed models use multiple steps of attention instead of one attention step only.",1,ad,True
40,"Additive attention [2] and multiplicative attention [20] are the two most commonly used attention mechanisms. Self-attention or intra-attention is a special case of the additive attention mechanism. It relates elements at different positions from a single sequence by computing attention between each pair of tokens. In recent works, it has been shown effective in natural language inference [16], reading comprehension [13] and neural machine translation [26]. Sequential attention [3] is another type of attention mechanisms which uses an additional bi-directional RNN layer. This additional layer allows local alignment information to be used when computing the attentional score for each token. It has shown promissing results on reading comprehension [3]. In this paper, we show that sequential attention can be well adopted for the AS task and obtain highly competitive results.",1,ad,True
41,"Our proposed MANs are also related to Dynamic Memory Networks (DMNs) [14] in the sense that we both use an iterative attention process instead of only one round of attention. However, each memory in DMNs depends on the memory in the previous step, aiming to narrow down their focus on individual facts or sentences in regard to the single question representation, while in MANs each attention step is applied independently for selecting the informative parts of answers relating to different aspects of the question.",1,ad,True
42,3 APPROACHES,1,AP,True
43,"Although CNNs can be used for representation learning in the AS task, LSTMs have been shown to obtain better performances [7, 24]. Hence, in this work we base our attention-based models on a variation of the LSTM model. We first describe the basic framework for answer selection based on LSTMs, called QA-LSTM [24]. Next we describe in detail different attention-based models that build on top of the QA-LSTM framework. After that, we present our Multihop Attention Networks.",0,,False
44,3.1 Long Short-Term Memory (LSTM),0,,False
45,Long Short-Term Memory (LSTM) [12] networks are a type of,0,,False
46,"recurrent neural network that are capable of learning long term dependencies across sequences. Given an input sequence X ,"" (x1, x2, ..., xn ), where each xt is an E-dimension word vector (xt  RE ), the LSTM returns a sequence embedding or hidden vector ht with a size d (ht  Rd ) at every time step t defined as follows:""",0,,False
47,"it ,  (Wi xt + Uiht -1 + bi )",0,,False
48,"ot ,  (Woxt + Uoht -1 + bo )",0,,False
49,"ft ,  (Wf xt + Uf ht -1 + bf )",0,,False
50,"ut , tanh (Wu xt + Uuht -1 + bu )",0,,False
51,(1),0,,False
52,"Ct , it  ut + ft  Ct -1",0,,False
53,"ht , ot  tanh(Ct )",0,,False
54,"where W, b, U are the parameters of the LSTM network (W  Rd×E , U  Rd×d , b  Rd ) and  ,"" {i, o, f , u} in which the input""",0,,False
55,"i, forget f and output o are three gates, and Ct is the cell state.  is the sigmoid function and  denotes element-wise multiplication.",0,,False
56,"For the sake of brevity, we omit the technical details of LSTM which can be found in many related works.",0,,False
57,"Single-direction LSTMs suffer from the weakness of not making use of the contextual information provided by future tokens. Bidirectional LSTMs (biLSTMs) use both the previous and future context by processing the sequence in two directions, and generate two sequences of output vectors. The output for each token is the concatenation of the two vectors from both directions, i.e.",0,,False
58,"- - ht , ht  ht . The output of biLSTM layer is a sequence of hidden vectors H  RL×2d where L is the maximum sequence length and d is the dimensional size of LSTM.",0,,False
59,3.2 LSTM for Answer Selection,0,,False
60,The basic LSTM-based framework for answer selection (QA-LSTM),0,,False
61,[24] is shown as in Figure 1(c) (without the attention layer). Given an,0,,False
62,"input pair (q, a), where q ,"" (q1, ..., ql ) is a sequence of word indices for question and a "","" (a1, ..., am ) is a sequence of word indices for""",0,,False
63,"candidate answer, the word embeddings (WEs) of both q and a are",0,,False
64,first retrieved by passing the sequences of word indices through a look-up layer. The parameters of this layer are W  RV ×E where,0,,False
65,V is the size of vocabulary and E is the dimensionality of the word,0,,False
66,embeddings. We initialize W with pretrained word embeddings,0,,False
67,"which is inline with previous works [24, 35].",0,,False
68,A biLSTM is then applied separately over the two sequences,0,,False
69,"of WEs creating hidden vectors for the question and answer, i.e.",0,,False
70,-,0,,False
71,-,0,,False
72,"hq (t) ,"" LSTM(hq (t - 1), qt )  LSTM(hq (t + 1), qt ) and ha (t) "",",0,,False
73,-,0,,False
74,-,0,,False
75,"LSTM(ha (t - 1), at )  LSTM(ha (t + 1), at ). Subsequently, the final",0,,False
76,"representations oq and oa for question and answer, respectively,",0,,False
77,can be taken by max or mean pooling over all the hidden vectors or,0,,False
78,"the last hidden vector. As discussed in [8, 25], sharing the same net-",0,,False
79,work parameters is significantly better than using separate question,0,,False
80,"and answer parameters, and converges much faster. Therefore, we",0,,False
81,follow the same procedure by using the same network parameters,0,,False
82,for processing questions and candidate answers.,0,,False
83,"Finally, a cosine similarity sim(q, a) is defined to score the input",0,,False
84,"pair (q, a) and the hinge loss function is used as training objective.",0,,False
85,"L ,"" max{0, M - sim(q, a+) + sim(q, a-)}""",0,,False
86,(2),0,,False
87,"where a+ is a ground truth answer, a- is an incorrect answer randomly chosen from the entire answer space, and M is the margin which controls the extent of discrimination between positive QA pairs and corrupted QA pairs. We treat any question with more than one ground truth as multiple training examples. During training, for each question we randomly sample N negative answers, but only use the one with the highest L to update the model similar to [18, 24].",0,,False
88,3.3 Attention Mechanisms,0,,False
89,"The aforementioned QA-LSTM is basically a siamese network [6] which might fail to notice a potential issue. The answers might be extremely long and contain lots of words that are not related to the question at hand, especially in non-factoid question answering. For example, in Table 1 the first sentence ""we run into this all the time with our EU clients"" does not relate directly to the question. Even if advanced neural networks are exploited, the resulting representation might still be distracted by non-useful information. Thus,",1,ad,True
90,327,0,,False
91,"Session 3C: Question Answering SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",1,Session,True
92,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Nam Khanh Tran, Claudia Niederée",0,,False
93,Question,0,,False
94,Question,0,,False
95,Question,0,,False
96,"

",0,,False
97,P,0,,False
98,sim,0,,False
99,Answer,0,,False
100,A #,0,,False
101,(a),0,,False
102,"(%)

A

 (%)

+

A #(%)

(')",0,,False
103,A,0,,False
104, (') +,0,,False
105,A #('),0,,False
106,"(()  (() sim
#(()

Answer Question

(b)
Answer Question

Question

 P",0,,False
107,sim,0,,False
108,Answer,0,,False
109,A #,0,,False
110,(c),0,,False
111,"(%)

A

 (%) +

A #(%)

(')",0,,False
112,A,0,,False
113, (') +,0,,False
114,A #('),0,,False
115,"(()  (() sim
#(()

Answer

(d)
Answer

Figure 1: Traditional attention-based networks: (a) Interactive attention network; (c) Self-attention network; and our proposed MANs: (b) Multihop interactive attention network; (d) Multihop self-attention network. P: pooling layer, A: attention layer

a number of attention-based models for the answer vector generation have been proposed in order to alleviate this weakness by dynamically aligning the more informative parts of answers to the question. Conceptually, attention mechanisms give more weight to certain words which have more influence on the resulting representation. In the AS task the expectation is that words in the candidate answer that are more important with regard to the input question should receive larger weights. Most previous works such as [18, 24] proceed as follows: the input question is represented by a vector oq using last, max or average pooling and an attention model is used over a sequence of hidden vectors to learn the representation of the answer oa as shown in Figure 2. An attention mechanism which takes into account the question vector for computing the attention weights in learning the answer representation, is called as an interactive attention mechanism. In contrast, an attention mechanism which is employed only on the candidate answer, is considered as a self-attention or intra-attention mechanism. One of the advantages of the intra-attention mechanism is that questions and answers can be embedded into a joint vector space without being paired, so that arbitrary question and answer vectors in that space are directly comparable.
Let Ha = {ha (1), ha (2), ..., ha (m)} denote the hidden vectors of the answer after passing through the biLSTM layer. To produce the final representation of the answer, instead of using the last hidden

(a) (b)



)

*
* (1)

attention

 # $",1,ad,True
116,* (),0,,False
117,...,0,,False
118,...,0,,False
119,)(1) )(2) )(3),0,,False
120,%,0,,False
121,&,0,,False
122,...,0,,False
123,...,0,,False
124,) (),0,,False
125,...,0,,False
126,) ( ),0,,False
127,...,0,,False
128,...,0,,False
129,...,0,,False
130,Question,0,,False
131,Answer,0,,False
132,Figure 2: (a) The question vector representation and (b) The attention mechanism for answer vector generation,0,,False
133,"vector or average or max pooling, an additional attention layer is used as follows:",1,ad,True
134,"t  fattention o~f , ha (t )",0,,False
135,"oa , t ha (t )",0,,False
136,(3),0,,False
137,t,0,,False
138,where ha (t) is the hidden vector of the answer at time t. When,0,,False
139,"the interactive attention mechanism is used, o~f is often equal to the question representation oq as shown in Figure 1(a). When the",0,,False
140,328,0,,False
141,Session 3C: Question Answering Multihop Attention Networks for Question Answer Matching,1,Session,True
142,"intra-attention mechanism is used, fattention only depends on ha (t) as in Figure 1(c). Next, we describe in detail the different implementations of fattention function.",0,,False
143,MLP Attention: Additive attention (or multi-layer perceptron,0,,False
144,attention) [2] is one of the most commonly used attention mech-,0,,False
145,"anisms. It is first used for answer selection by Tan et al. [24]. In [24], the attention function fattention is computed by a multi-layer perceptron network as follows:",0,,False
146,"m(t) , tanh Waha (t) + Wqoq",0,,False
147,"fattention o~f , ha (t ) , softmax wmT m(t )",0,,False
148,(4),0,,False
149,"where Wa,Wq are attentive weight matrices and wm are attentive weight vector. The size of the matrices and vector are often equal to the size of input vectors ha (t) and oq .",0,,False
150,"Bilinear Attention: This is another commonly used attention mechanism. For example, Chen et al. [4] found it effective in machine reading comprehension, Rush et al. [20] used it in abstractive summarization. Santos et al. [7] used this attention mechanism for AS task. In contrast to the additive attention, this attention mechanism makes use of a bilinear term instead of using a tanh layer to estimate the attention function fattention :",1,ad,True
151,"fattention o~f , ha (t ) , softmaxt oTq Wsha (t )",0,,False
152,(5),0,,False
153,where Ws is a network parameter. Sequential Attention: The previous approaches to attention,0,,False
154,select words with only very indirect consideration of their con-,0,,False
155,"text, Brarda et al. [3] address this issue by taking into account",1,ad,True
156,"explicit context sensitivity for computing the attention scoring function fattention . Specifically, instead of producing a single value of fattention for each word in the answer by using a bilinear term as the bilinear attention, a vector t is defined as",1,ad,True
157,"t , oq  ha (t )",0,,False
158,(6),0,,False
159,where  is element-wise multiplication. The vector t is then fed,0,,False
160,"into a new biLSTM layer to get the representation: t ,"" LSTM(- t -1, t""",0,,False
161,")hidLdSeTnMa(tt-ent +ti1o,nt).t",0,,False
162,"vector Finally,",0,,False
163,the attention function fattention is computed as,0,,False
164,"fattention o~f , ha (t ) , softmaxt 1T t",0,,False
165,(7),0,,False
166,"Chen et al. [5] showed that utilizing context information can enhance the performance of LSTM based QA. Therefore, in this paper we aim to investigate the effectiveness of sequential attention in answer selection. Experimental results show that sequential attention can be well adapted for the AS task and outperform other attention mechanisms on different QA datasets.",1,ad,True
167,"Self-Attention: In contrast to aforementioned attention mechanisms where the question vector oq is used to learn the representation of the answer, in this attention mechanism the answer is autonomously embedded into the embedding space without being paired with the question, so that arbitrary question and answer vectors in the space are directly comparable. Generally, self-attention relates different positions of a single sequence in order to compute the final representation of the sequence. This has been successfully employed in a variety of tasks including reading comprehension, abstractive summarization and textual entailment [15]. In the con-",1,ad,True
168,"text of answer selection, fattention o~f , ha (t) can be estimated",0,,False
169,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
170,merely based on ha (t) as follows:,0,,False
171,"s(t) , tanh (Wsha (t) + bs )",0,,False
172,"fattention o~f , ha (t ) , softmax wsT s(t )",0,,False
173,(8),0,,False
174,"where Ws , bs and ws are attention parameters.",0,,False
175,3.4 Multihop Attention Networks,0,,False
176,"In many cases, the semantic relations between a question and an answer can be very complex. Different parts of the answer can relate to different aspects addressed by the question. For example, in Table 1, the question ""are companies in California obliged to provide invoices"" refers to two aspects invoices and California law and each aspect is covered by different parts of the answer. Consequently, using single vectors for the question and answer representations might not be able to uncover their complex semantic relations. This can lead to suboptimal results.",1,ad,True
177,"In this paper, we propose Multihop Attention Networks (MANs) to tackle this problem. Our models are represented in Figure 1(b) and 1(d). Unlike existing models [7, 24], we do not compress the question to a single representation, but instead use multiple vectors for the question representation. Each question vector is then used to match with the answer representation which is learned via an attention layer. Specifically, given a question and a candidate answer, the model first reads the question and the answer using a biLSTM layer. Then, it deploys an iterative matching process to uncover the semantic relations between the question and the answer. In this phase, it first attends to some parts of the question, then finds their corresponding matches by attending to the answer. In the next step, it gives more attention to other parts of the question and searches for their matching parts in the answer. After a fixed number of iterations, the model uses the sum of the matching scores of each step to rank the question-answer pair.",1,ad,True
178,"Let Hq ,"" {hq (1), ..., hq (l)} denote the hidden vectors of the question after passing through the biLSTM layer. To obtain the representation of the question, one of three following mechanisms is usually used: last, mean, or max pooling. Last pooling takes the last vector hq (l), mean pooling averages all vectors and max pooling takes the element-wise maximum of Hq . In [15], Lin et al. proposed self-attention mechanism to replace the max pooling or averaging step. In this paper, we adapt this mechanism with some modifications for creating different question vector representations. Specifically, in each step k, the question representation oq(k) is computed as follows:""",1,ad,True
179,"st(k) , tanh Wq(k)hq (t )  tanh Wm(k)mq(k-1)",0,,False
180,"t(k) , softmax ws(k)T st(k)",0,,False
181,(9),0,,False
182,"oq(k) , t(k)hq (t )",0,,False
183,t,0,,False
184,"where Wq(k),Wm(k) and ws(k) are network parameters, mq(k) is a separate memory vector for guiding the next attention step. It is recur-",0,,False
185,sively updated by,0,,False
186,"mq(k) , mq(k-1) + oq(k)",0,,False
187,(10),0,,False
188,329,0,,False
189,"Session 3C: Question Answering SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",1,Session,True
190,The initial memory vector mq(0) is defined based on the context,0,,False
191,vector oq(0) where,0,,False
192,oq(0),0,,False
193,",",0,,False
194,1 l,0,,False
195,t,0,,False
196,hq (t),0,,False
197,"In each step, the question is represented by a vector oq(k) which focuses specifically on some aspects of the question. The vector oq(k) is then used as input for the attention models described in the previous section to extract the answer representation oa(k). After that, we compute the similarity between question and answer vec-",0,,False
198,tors by their cosine similarity. This similarity score reflects on how,0,,False
199,the answer relates to the corresponding parts of the question. After,0,,False
200,"performing K matching steps, the final similarity between the given",0,,False
201,question and answer becomes,0,,False
202,K,0,,False
203,"sim(q, a) ,"" cos(oq(k), oa(k))""",0,,False
204,(11),0,,False
205,k,0,,False
206,"The overall architecture of this model when K , 2 is shown in",0,,False
207,Figure 1(b). Figure 1(d) presents MAN with using self-attention,0,,False
208,"mechanism for the answer vector generation. In this case, we use",0,,False
209,"two separate attention models described in Equation 9, one model",0,,False
210,"for questions and another for answers. After each step, the same",0,,False
211,procedure is applied as implied by Equation 11. It is important to,0,,False
212,note that separate attention models are applied to questions and,0,,False
213,answers but the same attention parameters are used for questions,0,,False
214,"(answers) in different steps. Therefore, our network parameters are",0,,False
215,"comparable to the models with a single attention layer [7, 24] but",0,,False
216,we outperform the latter models on the datasets tested.,0,,False
217,4 EXPERIMENTAL SETUP,0,,False
218,4.1 Datasets,0,,False
219,"To evaluate the proposed approaches, we conduct an empirical evaluation based on three popular and well-studied benchmark datasets for both factoid and non-factoid question answering. In addition, we use another newly released dataset for the financial domain. These four datasets cover different domains and exhibit different characteristics:",1,ad,True
220,"· TREC-QA - This is a benchmark dataset created by Wang et al. [30] based on Text REtrieval Conference (TREC) QA track (8-13) data. The dataset contains a set of factoid questions, where candidate answers are limited to a single sentence. To enable direct comparison with the previous work, we follow the approach of train/dev/test questions selection from [28], in which all questions with only positive or negative answers are removed. In total, we have 1162 training questions, 65 development questions and 68 test questions. The maximum number of tokens for questions and answers are set to 11 and 60, respectively, the length of the vocabulary |V |,55060 and for each question there are 38 candidate answers on average.",1,TREC,True
221,"· WikiQA - This is a recent popular benchmark dataset for open-domain question answering, based on factual questions from Wikipedia and Bing search logs. For each question, Yang et al. [31] selected Wikipedia pages and used sentences in the summary paragraph as candidates, which are then annotated on a crowdsourcing platform. We follow the same",1,Wiki,True
222,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
223,"Nam Khanh Tran, Claudia Niederée",0,,False
224,"preprocessing steps as Yang et al., where questions with no correct candidate answers are excluded and answer sentences are truncated to 40 tokens. In total, we end up with 873 training questions, 126 development questions and 243 test questions. Since there are only few negative answers for each question in WikiQA, we extend it by randomly selecting a bunch of negative candidates from the answer pool. · InsuranceQA - This is a recently released large-scale nonfactoid QA dataset from the insurance domain created by Feng et al. [8]. In this work we use the first version of the dataset. The dataset is already divided into a training set, a validation set, and two test sets, in which a question may have multiple correct answers and normally the questions are much shorter than the answers. The average length of questions and answers in tokens are 7 and 95, respectively. Such difference imposes additional challenges for the AS task. For each question in the development and test sets, there is a set of 500 candidate answers, which include the ground-truth answers and randomly selected negative answers. · FiQA - This is a new non-factoid QA dataset from the financial domain which has been recently released for WWW 2018 Challenges.1 The dataset is built by crawling Stackexchange, Reddit and StockTwits in which part of the questions are opinionated, targeting mined opinions and their respective entities, aspects, sentiment polarity and opinion holder. We minimally preprocess the data only performing tokenization and lowercasing all words. To reduce the size of resulting vocabulary, we remove all rare words which occur less than 5 times. In this dataset questions and answers are longer than in other datasets, which will consequently bring extra challenges. The maximum number of tokens for questions and answers are set to 20 and 150 respectively. Following the setup for other datasets, we split this dataset into training, development and test sets as shown in Table 2. For each question in the development and test sets, we construct the answer pools by including the correct answer(s) and randomly selected candidates from the complete set of unique answers, as similar to InsuranceQA.",1,Wiki,True
225,"Table 2 presents some statistics about the datasets, including the number of questions in each set, average length of questions and answers as well as average number of candidate answers in the development and test sets.",0,,False
226,4.2 Employed Baselines,0,,False
227,"In this section, we introduce the baselines used for comparison. For all datasets, we report performances of the basic matching model QA-LSTM and the models with a single attention layer described in Section 3.3 including MLP-LSTM, Bilinear-LSTM, Self-LSTM and Sequential-LSTM. Furthermore, we also introduce other baselines for each dataset separately.",0,,False
228,"· TREC-QA - The key competitors of this dataset are the CNN model of Severyn and Moschitti [22], the Attention-based Neural Matching model [7, 24] and the RNN with Positional Attention proposed by Chen et al. [5]. In addition, due to the",1,TREC,True
229,1 https://sites.google.com/view/fiqa/home,0,,False
230,330,0,,False
231,Session 3C: Question Answering Multihop Attention Networks for Question Answer Matching,1,Session,True
232,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
233,Table 2: The statistics of the four employed answer selection datasets. For WikiQA and TREC-QA we remove all questions that have no right or wrong answers.,1,Wiki,True
234,Dataset,0,,False
235,TREC-QA WikiQA,1,TREC,True
236,InsuranceQA,0,,False
237,FiQA,0,,False
238,# of questions (train/dev/test) Avg length of questions Avg length of answers Avg # of candidate answers,0,,False
239,1162/65/68 8 28 38,0,,False
240,873/126/243 6 25 9,0,,False
241,12887/1000/1800x2 7 95 500,0,,False
242,5999/323/324 11 135 500,0,,False
243,"long standing nature of this dataset, we also report works based on traditional feature engineering approaches [10, 30]. · WikiQA - The competitors of this dataset include the Paragraph Vector (PV) and PV + Cnt models [33], CNN + Cnt model [35] which are reported in the original WikiQA paper [31]. Furthermore, we report additional strong baselines including AP-CNN and AP-LSTM [7], ABCNN [34] and RNNPOA [5]. We also report the Pairwise Ranking MP-CNN model [18]. · InsuranceQA - The key competitors of this dataset are the CNN-based ARC-I/II architecture by Feng et al. [8], QALSTM from [24] along with AP-LSTM which are attentive pooling improvements of the former and Inner attentionbased RNN [27]. · FiQA - For this dataset, we reimplemented QA-LSTM [24] and different attention mechanisms on top of QA-LSTM for comparison.",1,ad,True
244,"We denote our proposed models as Multihop-MLP-LSTM, MultihopBilinear-LSTM, Multihop-Sequential-LSTM and Multihop-Self-LSTM which are MANs based on additive attention, bilinear attention, sequential attention and self-attention, respectively, for learning answers' attentive representations.",1,ad,True
245,4.3 Hyperparameters and Training,0,,False
246,This section describes the key evaluation protocol and metrics as well as implementation details of our experiments.,0,,False
247,"4.3.1 Evaluation Metrics. For the evaluation protocols we follow the prior work. Specifically, in TREC-QA and WikiQA we use the Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) metrics which are commonplace in IR and QA research. On the other hand, InsuranceQA and FiQA evaluate on Precision@1 (P@1) which is determined based on whether the top predicted answer is the ground truth. For all competitor methods, we report the performance results from the original paper.",1,TREC,True
248,"4.3.2 Implementation Details and Hyperparameters. The models are implemented in Pytorch. The model parameters are optimized using Adam [1] optimizer with a learning rate of 0.001. A batch size of 100 are used for all datasets. The parameters are regularized with a per-minibatch L2 regularization strength of 10-5 and a dropout of d ,"" 0.3 is also applied to prevent overfitting. The hidden layer size of LSTM models are the same as in previous works for a fair comparison. Specifically, for TREC-QA and InsuranceQA the sizes are set to 300 and 141 respectively as in [24]; the number for WikiQA is 141 as in [7]. For FiQA, we tried different numbers and found out that the size of 512 yields the best results. We tried different""",1,TREC,True
249,"margins M in the hinge loss function and finally fixed the margin to M , 0.2. A number of negative answers N ,"" 50 was used during training. The number of attention steps K is tuned amongst {1, 2, 3} and we also experimented with the set of three vectors by using last, max and average pooling as different representations for the questions. We initialized the word embeddings with 300-dimensional Glove vectors [17] trained on 840 billion words. Embeddings for words not present in the Glove vectors are randomly initialized with each component sampled from the uniform distribution over [-0.25, 0.25]. The word embeddings are also part of the parameters and are optimized during training. Since sequences within a mini-batch have different lengths, we use a mask matrix to indicate the real length of each sequence. We trained all models for a maximum of 40 epochs. We take MAP scores for TREC-QA and WikiQA and P@1 scores for InsuranceQA and FiQA on the development set at every epoch and save the parameters of the network for the top three models. We report the best test score from the saved models. All experiments were conducted on a Linux machine with Nvidia GTX Ti 1080 GPU (12GB RAM). The code to reproduce the reported results and FiQA splits are publicly available at https://github.com/namkhanhtran/nn4nqa.""",1,MAP,True
250,5 EXPERIMENTAL RESULTS,0,,False
251,"In this section, we present our empirical results on all datasets. For all reported results the best result is in boldface.",0,,False
252,5.1 TREC-QA,1,TREC,True
253,"Our results on TREC-QA dataset is summarized in Table 3. Firstly, we observe that all attention-based models outperform the basic matching model QA-LSTM by large margins. Second, the model based on sequential attention mechanism obtains a clear performance gain of around 3% on MAP/MRR against the model with additive or multiplicative attention mechanism. Compared to these models, Self-LSTM achieves comparable results though it does not use any query information for extracting answer representation. It also performs better than QA-LSTM, which indicates that selfattention mechanism can give better representations than simply max or average pooling method.",1,TREC,True
254,"Furthermore, Table 3 shows that MANs outperform all models with only one attention layer. When using the same attention mechanism, the averages increase over the baselines with a single attention layer are 2% ­ 3% in terms of MAP/MRR. Specifically, Multihop-Sequential-LSTM gains an improvement of 1.6% on MAP and 2.8% on MRR compared to Sequential-LSTM. Similarly, Multihop-Bilinear-LSTM obtains 3.3% and 3.2% improvements in terms of MAP and MRR respectively. Multihop-MLP-LSTM also",1,MAP,True
255,331,0,,False
256,"Session 3C: Question Answering SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",1,Session,True
257,Table 3: Experimental results on TREC-QA. Baselines for TREC-QA are reported in the first group. The second group shows the performance of models with a single attention layer. We report the performance of MANs in the last group.,1,TREC,True
258,Model,0,,False
259,MAP MRR,1,MAP,True
260,Wang et al. [30] Heilman & Smith [10] Wang & Nyberg [28] CNN (Severyn & Moschitti) [22] AP-LSTM (Tan et al.) [24] AP-CNN (Santos et al.) [7] RNN-POA (Chen et al. [5]),1,AP,True
261,0.603 0.609 0.713 0.746 0.753 0.753 0.781,0,,False
262,0.685 0.692 0.791 0.808 0.830 0.851 0.851,0,,False
263,QA-LSTM MLP-LSTM Bilinear-LSTM Self-LSTM Sequential-LSTM,0,,False
264,0.737 0.764 0.755 0.759 0.797,0,,False
265,0.810 0.839 0.832 0.830 0.865,0,,False
266,Multihop-MLP-LSTM Multihop-Bilinear-LSTM Multihop-Self-LSTM Multihop-Sequential-LSTM,0,,False
267,0.768 0.788 0.771 0.813,0,,False
268,0.849 0.864 0.864 0.893,0,,False
269,"shows some degree of improvement compared to MLP-LSTM. Based on self-attention mechanism, MAN outperforms the model with one attention layer by 1.2% on MAP and 3.4% on MRR. Overall, Multihop-Sequential-LSTM obtains the best results on TREC-QA dataset and surpasses the strong baseline RNN-POA [5] by 3.2% on MAP and 4.2% on MRR.",1,MAP,True
270,5.2 WikiQA,1,Wiki,True
271,"Table 4 reports the experimental results on WikiQA. First, we observe that MAN-based models outperform the models with a single attention layer. Multihop-Sequential-LSTM outperforms SequentialLSTM by 2% in terms of MAP and 2.3% in terms of MRR. MultihopBilinear-LSTM shows improvements of 3.8% on MAP and 3.9% on MRR compared to Bilinear-LSTM. Multihop-MLP-LSTM and Multihop-Self-LSTM also perform slightly better than MLP-LSTM and Self-LSTM, respectively. Overall, Multihop-Sequential-LSTM achieves the best results on WikiQA and shows some degree of improvement compared to the strongest baseline RNN-POA [5].",1,Wiki,True
272,5.3 InsuranceQA,0,,False
273,"Table 5 reports the experimental results on InsuranceQA. Our proposed approaches achieve highly competitive performances on this dataset, where Multihop-Sequential-LSTM obtains the best P@1 performance overall. Our best model surpasses the strong baseline IARNN-Gate on both test sets. Although most MAN-based models show some degree of improvement compared to the models with a single attention layer, applying one step of attention seems to be sufficient on this dataset. Interestingly, Self-LSTM performs quite well on InsuranceQA dataset even outperforms some interactive attention-based models. Multihop-Self-LSTM does not show any improvement against Self-LSTM. In addition, Sequential-LSTM",1,ad,True
274,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Nam Khanh Tran, Claudia Niederée",0,,False
275,Table 4: Experimental results on WikiQA. Baselines for WikiQA are reported in the first group. The second group shows the performance of models with a single attention layer. We report the performance of MANs in the last group.,1,Wiki,True
276,Model,0,,False
277,MAP MRR,1,MAP,True
278,PV PV + Cnt CNN + Cnt AP-LSTM (Santos et al. [7]) AP-CNN (Santos et al. [7]) ABCNN (Yin et al. [34]) Rank MP-CNN (Rao et al. [18]) RNN-POA (Chen et al. [5]),1,AP,True
279,0.511 0.599 0.652 0.670 0.689 0.692 0.701 0.721,0,,False
280,0.516 0.609 0.665 0.684 0.696 0.710 0.718 0.731,0,,False
281,QA-LSTM MLP-LSTM Bilinear-LSTM Self-LSTM Sequential-LSTM,0,,False
282,0.654 0.686 0.677 0.693 0.702,0,,False
283,0.665 0.695 0.686 0.704 0.715,0,,False
284,Multihop-MLP-LSTM Multihop-Bilinear-LSTM Multihop-Self-LSTM Multihop-Sequential-LSTM,0,,False
285,0.703 0.715 0.702 0.722,0,,False
286,0.712 0.725 0.710 0.738,0,,False
287,Table 5: Experimental results on InsuranceQA. Baselines for InsuranceQA are reported in the first group. The second group shows the performance of models with a single attention layer. We report the performance of MANs in the last group.,0,,False
288,Model,0,,False
289,Test1 Test2,0,,False
290,CNN (Feng et al. [8]) CNN with GESD (Feng et al. [8]) AP-LSTM (Tan et al. [24]) IARNN-Gate (Wang et al [27]),1,AP,True
291,0.628 0.653 0.690 0.701,0,,False
292,0.592 0.610 0.648 0.628,0,,False
293,QA-LSTM MLP-LSTM Bilinear-LSTM Self-LSTM Sequential-LSTM,0,,False
294,0.643 0.693 0.689 0.699 0.702,0,,False
295,0.617 0.648 0.658 0.653 0.665,0,,False
296,Multihop-MLP-LSTM Multihop-Bilinear-LSTM Multihop-Self-LSTM Multihop-Sequential-LSTM,0,,False
297,0.695 0.694 0.682 0.705,0,,False
298,0.655 0.662 0.648 0.669,0,,False
299,again shows better results than MLP-LSTM and Bilinear-LSTM on this dataset.,0,,False
300,5.4 FiQA,0,,False
301,"The results of the proposed models are shown in Table 6. On this new dataset we observe similar behaviours to other datasets. Firstly,",0,,False
302,332,0,,False
303,Session 3C: Question Answering Multihop Attention Networks for Question Answer Matching,1,Session,True
304,Table 6: Experimental results on FiQA. The first group shows the performance of models with a single attention layer. We report the performance of MANs in the second group.,0,,False
305,Model,0,,False
306,MAP MRR P@1,1,MAP,True
307,QA-LSTM MLP-LSTM Bilinear-LSTM Self-LSTM Sequential-LSTM,0,,False
308,0.433 0.497 0.492 0.493 0.504,0,,False
309,0.566 0.616 0.606 0.608 0.621,0,,False
310,0.469 0.509 0.506 0.509 0.522,0,,False
311,Multihop-MLP-LSTM Multihop-Bilinear-LSTM Multihop-Self-LSTM Multihop-Sequential-LSTM,0,,False
312,0.498 0.507 0.488 0.529,0,,False
313,0.613 0.631 0.619 0.655,0,,False
314,0.519 0.534 0.522 0.567,0,,False
315,Table 7: Effect of different number of attention steps on FiQA,0,,False
316,Model,0,,False
317,MAP MRR P@1,1,MAP,True
318,Sequential-LSTM,0,,False
319,0.504 0.621 0.522,0,,False
320,"Multihop-Sequential-LSTM* Multihop-Sequential-LSTM (K,1) Multihop-Sequential-LSTM (K,2) Multihop-Sequential-LSTM (K,3)",0,,False
321,0.514 0.527 0.529 0.523,0,,False
322,0.636 0.649 0.655 0.644,0,,False
323,0.543 0.561 0.567 0.546,0,,False
324,"attention-based models outperform the basic matching model QALSTM. MAN-based models perform better than the models with a single attention layer, in which Multihop-Sequential-LSTM obtains the best performance overall. More specifically, MultihopSequential-LSTM improves Sequential-LSTM by 4.5% in terms of P@1 while Multihop-Bilinear-LSTM shows an improvement of 2.8% on P@1 against Bilinear-LSTM. Multihop-MLP-LSTM indicates 1% enhancement on P@1 compared to MLP-LSTM whereas Multihop-Self-LSTM also increases over Self-LSTM by 1.3% in terms of P@1. Amongst interactive attention-based models SequentialLSTM outperforms MLP-LSTM and Bilinear-LSTM. Compared to these models, Self-LSTM shows highly competitive results.",0,,False
325,"Overall, we summarize the key findings of our experiments.",0,,False
326,"· Similar to previous work, we observe that attention-based models perform significantly better than basic matching models.",0,,False
327,· Multihop Attention Networks are better in capturing the complex semantic relations between questions and answers and outperform the models with only one attention layer.,0,,False
328,· Sequential attention can be well adopted for the AS task and gains considerably improvements compared to traditional attention mechanisms.,1,ad,True
329,· Self-attention can produce better representations than simply max or average pooling method and obtain competitive results on the AS task.,0,,False
330,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
331,"SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
332,5.5 Effect of Attention Steps,0,,False
333,"Table 7 shows the influence of the number of steps on performance on FiQA dataset. Multihop-Sequential-LSTM* is the model where we consider the vectors returned by using max, mean and last pooling as different representations for the questions. Overall, MultihopSequential-LSTM performs better than Sequential-LSTM in which with K , 2 Multihop-Sequential-LSTM obtains the best performance. This might be due to the fact that the questions are rather short and often express two different aspects at most.",0,,False
334,5.6 A Case Study,0,,False
335,"Figure 3 depicts the heat map of a test question from FiQA that was correctly answered by Multihop-Sequential-LSTM. The stronger the color of a word in the question (answer), the larger the attention weight of that word. As can be seen in the figure, in the first step Multihop-Sequential-LSTM puts more focus on some segments of the question and the parts of the answer that have some interactions with the question segments. In the second step, the MAN gives more attention to other segments of the question and consequently some other parts of the answer get more attention.",0,,False
336,6 CONCLUSIONS,0,,False
337,"In this paper, we have presented Multihop Attention Networks for question answer selection. Our proposed MANs use multiple vectors which focus on different parts of a question to represent the overall semantics of the question and then apply multiple steps of attention to learn representations for the candidate answers. In addition, we also showed that sequential attention mechanism can be well adapted for this task. The mechanism allows local alignment information to be used when computing attention weight for each token in a sequence. Experimental results showed that MANs outperform state-of-the-art approaches on popular benchmark QA datasets. Empirical studies also confirm the effectiveness of sequential attention over other attention mechanisms. For future work, we want to employ this architecture on other tasks such as natural language inference, reading comprehension. In addition, combining multihop attention with convolutional neural networks is another direction to explore.",1,ad,True
338,ACKNOWLEDGMENTS,0,,False
339,"We would like to thank Tuan-Anh Hoang and anonymous reviewers for their helpful comments. This work was partially funded by the BMBF project eLabour (01UG1512C), and the DFG project Managed Forgetting (NI-1760/1-1).",0,,False
340,REFERENCES,0,,False
341,[1] Jimmy Ba and Diederik Kingma. 2015. Adam: A Method for Stochastic Optimization. In Proceedings of International Conference of Learning Representations.,0,,False
342,"[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of International Conference of Learning Representations.",0,,False
343,"[3] Sebastian Brarda, Philip Yeres, and Samuel R. Bowman. 2017. Sequential Attention: A Context-Aware Alignment Function for Machine Reading. In Proceedings of the 2nd Workshop on Representation Learning for NLP. 75­80.",1,ad,True
344,"[4] Danqi Chen, Jason Bolton, and Christopher D. Manning. 2016. A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2358­2367.",1,ad,True
345,333,0,,False
346,"Session 3C: Question Answering SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",1,Session,True
347,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Nam Khanh Tran, Claudia Niederée",0,,False
348,"Figure 3: Attention heat map from Multihop-Sequential-LSTM (K,2) for a correctly selected answer.",0,,False
349,"[5] Qin Chen, Qinmin Hu, Jimmy Xiangji Huang, Liang He, and Weijie An. 2017. Enhancing Recurrent Neural Networks with Positional Attention for Question Answering. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. 993­996.",0,,False
350,"[6] Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a Similarity Metric Discriminatively, with Application to Face Verification. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Volume 1 - Volume 01. 539­546.",1,ad,True
351,"[7] Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016. Attentive pooling networks. In CoRR, abs/1602.03609.",0,,False
352,"[8] Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, and Bowen Zhou. 2015. Applying deep learning to answer selection: A study and an open task. In Workshop on Automatic Speech Recognition and Understanding. 813­820.",0,,False
353,"[9] Michael Heilman and Noah A. Smith. 2010. Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. 1011­1019.",0,,False
354,"[10] Michael Heilman and Noah A. Smith. 2010. Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. 1011­1019.",0,,False
355,"[11] Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching Machines to Read and Comprehend. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1. 1693­1701.",1,ad,True
356,"[12] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. (1997), 1735­1780.",0,,False
357,"[13] Minghao Hu, Yuxing Peng, and Xipeng Qiu. 2017. Reinforced Mnemonic Reader for Machine Comprehension. arXiv preprint arXiv:1705.02798 (2017).",1,ad,True
358,"[14] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask Me Anything: Dynamic Memory Networks for Natural Language Processing. In Proceedings of The 33rd International Conference on Machine Learning. 1378­1387.",1,ad,True
359,"[15] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A Structured Self-Attentive Sentence Embedding. In International Conference on Learning Representations 2017 (Conference Track).",1,Track,True
360,"[16] Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. 2016. Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention. CoRR (2016).",0,,False
361,"[17] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Natural Language Processing (EMNLP). 1532­1543.",0,,False
362,"[18] Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. 1913­ 1916.",0,,False
363,"[19] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomás Kociský, and Phil Blunsom. 2015. Reasoning about Entailment with Neural Attention. In arXiv preprint arXiv:1509.06664.",0,,False
364,"[20] Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A Neural Attention Model for Abstractive Sentence Summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 379­389.",0,,False
365,[21] Aliaksei Severyn and Alessandro Moschitti. 2013. Automatic Feature Engineering for Answer Selection and Extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. 458­467.,0,,False
366,[22] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. 373­382.,0,,False
367,"[23] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2. 3104­3112.",0,,False
368,"[24] Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2016. Improved Representation Learning for Question Answer Matching. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 464­473.",0,,False
369,"[25] Ming Tan, Bing Xiang, and Bowen Zhou. 2015. LSTM-based Deep Learning Models for non-factoid answer selection. CoRR abs/1511.04108 (2015).",0,,False
370,"[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30. 6000­6010.",0,,False
371,"[27] Bingning Wang, Kang Liu, and Jun Zhao. 2016. Inner Attention based Recurrent Neural Networks for Answer Selection. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1288­ 1297.",0,,False
372,[28] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 707­ 712.,0,,False
373,[29] Mengqiu Wang and Christopher Manning. 2010. Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and Question Answering. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010). 1164­1172.,0,,False
374,"[30] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). 22­32.",0,,False
375,"[31] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain Question Answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2013­2018.",1,Wiki,True
376,"[32] Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 858­867.",0,,False
377,"[33] Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question Answering Using Enhanced Lexical Semantic Models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1744­1753.",0,,False
378,"[34] Wenpeng Yin, Hinrich Schutze, Bing Xiang, and Bowen Zhou. 2016. ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs. Transactions of the Association for Computational Linguistics (2016), 259­272.",0,,False
379,"[35] Lei Yu, Karl M. Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep learning for answer sentence selection. In NIPS Deep Learning Workshop.",0,,False
380,334,0,,False
381,,0,,False

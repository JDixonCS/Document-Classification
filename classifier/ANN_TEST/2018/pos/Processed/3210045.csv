,sentence,label,data,regex
0,Session 1D: Learning to Rank I,1,Session,True
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Efficient Exploration of Gradient Space for Online Learning to Rank,1,ad,True
3,"Huazheng Wang, Ramsey Langley, Sonwoo Kim, Eric McCord-Snook, Hongning Wang",0,,False
4,Department of Computer Science University of Virginia,0,,False
5,"Charlottesville, VA 22903, USA {hw7ww,rml5tu,sak2m,esm7ky,hw5x}@virginia.edu",0,,False
6,ABSTRACT,0,,False
7,"Online learning to rank (OL2R) optimizes the utility of returned search results based on implicit feedback gathered directly from users. To improve the estimates, OL2R algorithms examine one or more exploratory gradient directions and update the current ranker if a proposed one is preferred by users via an interleaved test.",1,ad,True
8,"In this paper, we accelerate the online learning process by efficient exploration in the gradient space. Our algorithm, named as Null Space Gradient Descent, reduces the exploration space to only the null space of recent poorly performing gradients. This prevents the algorithm from repeatedly exploring directions that have been discouraged by the most recent interactions with users. To improve sensitivity of the resulting interleaved test, we selectively construct candidate rankers to maximize the chance that they can be differentiated by candidate ranking documents in the current query; and we use historically difficult queries to identify the best ranker when tie occurs in comparing the rankers. Extensive experimental comparisons with the state-of-the-art OL2R algorithms on several public benchmarks confirmed the effectiveness of our proposal algorithm, especially in its fast learning convergence and promising ranking quality at an early stage.",1,ad,True
9,CCS CONCEPTS,0,,False
10,· Information systems  Learning to rank; · Theory of computation  Online learning algorithms;,0,,False
11,KEYWORDS,0,,False
12,Online learning to rank; Dueling bandit; Null space exploration,0,,False
13,"ACM Reference Format: Huazheng Wang, Ramsey Langley, Sonwoo Kim, Eric McCord-Snook, Hongning Wang. 2018. Efficient Exploration of Gradient Space for Online Learning to Rank. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3209978. 3210045",1,ad,True
14,1 INTRODUCTION,1,DUC,True
15,The goal of learning to rank is to optimize a parameterized ranking function such that documents that are more relevant to a user's,0,,False
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210045",1,ad,True
17,"query are ranked at higher positions [16]. A trained ranker combines hundreds of ranking features to recognize the relevance quality of a document to a query, and shows several advantages over the manually crafted ranking algorithms [4]. Traditionally, such a ranker is optimized in an offline manner over a manually curated search corpus. This learning scheme, however, becomes a main obstacle hampering the application of learning to rank algorithms for a few reasons: 1) it is expensive and time-consuming to obtain reliable annotations in large-scale retrieval systems; 2) editors' annotations do not necessarily align with actual users' preferences [20]; and 3) it is difficult for an offline-trained model to reflect or capture ever-changing users' information needs in an online environment [21].",1,ad,True
18,"To overcome these limitations, recent research has focused on learning the rankers on the fly, by directly exploiting implicit feedback from users via their interactions with the system [5, 10, 27]. Fundamentally, online learning to rank (OL2R) algorithms operate in an iterative manner: in every iteration, the algorithm examines one or more exploratory directions, and updates the ranker if a proposed one is preferred by the users via an interleaved test [9, 23, 29, 30]. The essence of this type of OL2R algorithms is to estimate the gradient of an unknown objective function with low bias, such that online gradient descent can be used for optimization with low regret [6]. For example, one eventually finds a close to optimal ranker and seldom shows clearly bad results in the process. In the web search scenario, the objective function is usually considered to be the utility of search results, which can be depicted by ordinal comparisons in user feedback, such as clicks [20]. However, to maintain an unbiased estimation of the gradient, uniform sampling of random vectors in the entire parameter space is performed in these algorithms. As a result, the newly proposed exploratory rankers are independent from not only the past interactions with users, but also the current query being served. This inevitably leads to slow convergence and large variance of ranking quality during the online learning process.",1,ad,True
19,"Several lines of works have been proposed to improve the algorithms' online learning efficiency. Hofmann et al. [9] suggested to reduce the step size in gradient descent for better empirical performance. In their follow-up work [8], historical interactions were collected to supplement the interleaved test in the current query and pre-select the candidate rankers. Schuth et al. [23] proposed to explore multiple gradient directions in one multi-interleaved test [24] so as to reduce the number of comparisons needed to evaluate the rankers. Zhao et al. [30] introduced the idea of using two uniformly sampled random vectors with opposite directions as the exploratory directions, with the hope that when they are not orthogonal to the optimal gradient, one of them should be a more effective direction than a simplely uniformly sampled direction. They also developed a contextual interleaving method, which considers",1,ad,True
20,145,0,,False
21,Session 1D: Learning to Rank I,1,Session,True
22,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
23,"historical explorations when interleaving the proposed rankers for comparison, to reduce the noise from multi-interleaving.",0,,False
24,"Nevertheless, all aforementioned solutions still uniformly sample from the entire parameter space for gradient exploration. This results in independent and isolated rankers for comparison. Therefore, less promising directions might be repeatedly tested, as historical interactions are largely ignored when proposing the new rankers. More seriously, as the exploratory rankers are independently proposed for the current query, they might give the same ranking order of the candidate documents for interleaving (this happens when the difference in the feature weight vectors between two rankers are orthogonal to the feature vectors in those candidate documents). In this scenario, no click feedback can differentiate the ranking quality of those rankers in this query. When the interleaved test cannot recognize the best ranker from ordinal comparison in a query, tie will be arbitrarily broken [23, 29]. This again leads to large variance and slow convergence of ranking quality in these types of algorithms.",1,ad,True
25,"We propose improving the learning convergence of OL2R algorithms by carefully exploring the gradient space. First, instead of uniformly sampling from the entire parameter space for gradient estimation, we maintain a collection of recently explored gradients that performed relatively poorly in their interleaved tests. We sample proposal directions from the null space of these gradients to avoid repeatedly exploring poorly performing directions. Second, we use the candidate ranking documents associated with the current query to preselect the proposed rankers, with a focus on those that give different ranking orders over the documents. This ensures that the resulting interleaved test will have a better chance of recognizing the difference between those rankers. Third, when an interleaved test fails to recognize the best ranker for a query, e.g., two or more rankers tie, we compare the tied rankers on the most recent worst performing queries (i.e., the difficult queries) with the recorded clicks to differentiate their ranking quality. We name the resulting algorithm Null Space Gradient Descent, or NSGD for short, and extensively compare it with four state-of-the-art algorithms on five public benchmarks. The results confirm greatly improved learning efficiency in NSGD, with a remarkably fast and stable convergence rate at the early stage of the interactive learning process. This means systems equipped with NSGD can provide users with better search results much earlier, which is crucial for any interactive system.",1,ad,True
26,2 RELATED WORK,0,,False
27,"Online learning to rank has recently attracted increasing attention in the information retrieval community, as it eliminates the heavy dependency on manual relevance judgments for model training and directly estimates the utility of search results from user feedback on the fly. Various algorithms have been proposed, and they can be categorized into two main branches, depending on whether they estimate the utility of individual documents directly [19] or via a parameterized function over the ranking features [29].",0,,False
28,"The first branch learns the best ranked list for each individual query by modeling user clicks using multi-armed bandit algorithms [1, 2]. Ranked bandits are studied in [19], where a k-armed bandit model is placed on each ranking position of a fixed input query to estimate the utility of candidate documents being in that position. The system's learning is accelerated by assuming similar documents have similar utility for the same query [25]. By assuming",0,,False
29,"that skipped documents are less attractive than later clicked ones, Kveton et al. [13] develop a cascading bandit model to learn from both positive and negative feedback. To enable learning from multiple clicks in the same result ranking list, they adopt the dependent click model [7] to infer user satisfaction after a sequence of clicks [12], and later further extend to broader types of click models [31]. However, such algorithms estimate the utility of ranked documents on a per-query basis, and no estimation is shared across queries. This causes them to suffer from slow convergence, making them less practical.",1,ad,True
30,"Another branch of study leverages ranking features and look for the best ranker in the entire parametric space. Our work falls into this category. The most representative work in this line is dueling bandit gradient descent (DBGD) [28, 29], where the algorithm proposes an exploratory direction in each iteration of interaction and uses an interleaved test to validate the exploration for model updating. As only one exploratory direction is compared in each iteration of DBGD, its learning efficiency is limited. Different solutions have been proposed to address this limitation. Schuth et al. [23] propose the Multileave Gradient Descent algorithm to explore multiple directions in each iteration. To evaluate multiple candidate rankers at once, multi-interleaving comparison [24] is used. Zhao et al. [30] propose the Dual-Point Dueling Bandit Gradient Descent algorithm to sample two stochastic vectors with opposite directions as the candidate gradients. When they are not orthogonal to the optimal gradient, one of the two should be a more effective gradient than a single proposal. However, all of the aforementioned algorithms uniformly sample the exploratory directions from the entire parameter space, which is usually very high-dimensional. More importantly, the uniform sampling makes the proposed rankers independent from past interactions, and thus they cannot avoid repeatedly exploring less promising directions.",1,ad,True
31,"Some works have recognized this deficiency and proposed different solutions. Hofmann et al. [8] record historical interactions during the learning process to supplement the interleaved test when comparing the rankers. They also suggest using historical data to preselect the proposed rankers before interleaved test. However, only the most recent interactions are collected in these two solutions, so that they are not necessarily effective in recognizing the quality of different rankers. Oosterhuis et al. [18] create the exploratory directions via a weighted combination over a set of preselected reference documents from an offline training corpus. The reference documents are either uniformly sampled or are the clustering centroids of the corpus. However, the reference documents are fixed beforehand; this limits the quality of learnt rankers, if the offline corpus has a different feature distribution than the incoming online documents. More importantly, none of these solutions consider the feature distribution in the candidate ranking documents of a particular query when proposing exploratory rankers. It is possible that the proposed rankers are not differentiable by any click pattern for a given query, e.g., they rank the documents in the same order. When the best rankers are tied, the winner is arbitrarily chosen. This further slows down the online learning process. In our solution, we preselect the rankers that tend to provide different ranking lists in the current query, so that the resulting interleaved test will have a better chance to tell the difference among those rankers. When a tie occurs, we use the most recent difficult queries to further evaluate the rankers, as those queries are expected to be more discriminative.",0,,False
32,146,0,,False
33,Session 1D: Learning to Rank I,1,Session,True
34,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
35,3 METHOD,0,,False
36,"We improve the learning convergence of OL2R algorithms by carefully exploring the gradient space. In particular, we aim to avoid repeatedly exploring recent poorly performing directions and focus on the rankers that can be best differentiated by the candidate ranking documents associated with the current query. We first give an overview of a basic online learning to rank algorithm, Dueling Bandit Gradient Descent [29], based on which we describe our proposed solution, Null Space Gradient Descent, in details.",1,ad,True
37,3.1 Overview of Dueling Bandit Gradient Descent,1,ad,True
38,"Dueling bandit gradient descent (DBGD) [29] is an OL2R algorithm that learns from interleaved comparisons between one exploratory ranker and one current ranker. Each ranker is represented as a feature weight vector w  Rd , and it ranks documents by taking the inner product with their associated ranking features, i.e., a linear ranking model. As shown in Algorithm 1, at the beginning of iteration t, the algorithm receives a a query and its associated candidate ranking documents, represented as a set of query-document pairs Xt ,"" {x1, x2, ..., xs }. We denote wt0 as the weight vector of the current ranker. DBGD proposes an exploratory direction ut uniformly from the unit sphere, and generates a candidate ranker wt1 "","" wt0 + ut , where  is the step size of exploration. Two ranked lists generated by these two rankers, i.e., l(Xt , wt0) and l(Xt , wt1), are then combined via an interleaving method, such as Team Draft Interleaving [20]. The resultant list is returned to the user for feedback. Based on the feedback and specific interleaving method, a better ranker is determined. If the exploratory ranker wins, the current ranker gets updated with wt0+1 "","" wt0 + ut , where  is the learning rate; otherwise the current ranker stays intact. Such exploration and comparison lead to a low bias estimation of gradient in terms of expectation [6], i.e., f^(w) "","" E[f (w + u)u]d/ , in which f (w) is the target utility function to be estimated. This estimation does not require the function f (w) to be differentiable nor even to be explicitly defined; and thus it is the theoretical basis of this family of OL2R algorithms.""",1,ad,True
39,"However, only one exploration direction ut is proposed for comparison in each iteration, which limits the learning rate of DBGD. To address this limitation, Schuth et al. [24] proposed the Multileaving Gradient Descent algorithm that uniformly explores m directions at the same time, i.e., wt0 + uti where i  {1..m}. Zhao et al. [30] proposed the Dual-Point Dueling Bandit Gradient Descent algorithm to explore two opposite directions each time, i.e., wt0 + ut and wt0 -ut . Although exploring multiple candidates generally improves the learning rate, the expected improvement is still marginal, as the ranking features usually reside in a high dimensional space and uniform sampling is very inefficient. More importantly, the uniform sampling makes the proposed rankers independent from historical interactions and the current query context. Algorithms with history-independent exploration cannot avoid repeatedly exploring less promising directions that have been discouraged by the most recent user feedback. Additionally, context-independent exploration cannot avoid the issue of multiple rankers generating indifferentiable ranking results, such as by ranking the documents in the same order. Both of them further hamper the convergence rate of aforementioned OL2R algorithms.",1,ad,True
40,Algorithm 1 Dueling Bandit Gradient Descent (DBGD) [29],1,ad,True
41,"1: Inputs:  ,  2: Initiate w10 , sample_unit_vector() 3: for t , 1 to T do",0,,False
42,"4: Receive query Xt ,"" {x1, x2, ..., xs }""",0,,False
43,"5: ut , sample_unit_vector()",0,,False
44,"6: wt1 , wt0 + ut",0,,False
45,"7: Generate ranked lists l(Xt , wt0) and l(Xt , wt1) 8: Set L ,""Interleave l(Xt , wt0), l(Xt , wt1) and present L to user""",0,,False
46,"9: Receive click positions Ct on L, and infer click credit ct0 and ct1 for wt0 and wt1 accordingly",0,,False
47,10: if ct1 > ct0 then,0,,False
48,11:,0,,False
49,"wt0+1 , wt0 + ut",0,,False
50,12: else,0,,False
51,13:,0,,False
52,"wt0+1 , wt0",0,,False
53,14: end if,0,,False
54,15: end for,0,,False
55,3.2 Null Space Gradient Descent,1,ad,True
56,Our proposed Null Space Gradient Descent (NSGD) algorithm im-,1,ad,True
57,proves over DBGD-type OL2R algorithms by a suite of carefully,0,,False
58,designed exploration strategies in the gradient space.,1,ad,True
59,"We illustrate the procedure of NSGD in Figure 1. First, to avoid",0,,False
60,uniformly testing exploratory directions in the entire parameter,0,,False
61,"space, we maintain a collection of most recently explored gradients",1,ad,True
62,"that performed poorly in their interleaved tests, and sample new",0,,False
63,proposal directions from the null space of these gradients. As a,1,ad,True
64,"result, we only search in a subspace that is orthogonal to those",0,,False
65,less promising directions. This can be intuitively understood from,0,,False
66,Figure 1 part 1: since the interleaved tests in iteration t - 2 and t - 1,0,,False
67,"unveil the ineffectiveness of the directions marked in red, NSGD",0,,False
68,prevents the current ranker wt from exploring these less promising,0,,False
69,directions again by exploring the null space of them at iteration,0,,False
70,"t. Second, we prefer the proposed rankers that tend to generate",0,,False
71,the most distinct ranking orders from the current ranker for the,0,,False
72,"current query, so that the resulting interleaved test will have a better",0,,False
73,chance of recognizing the best ranker among those being compared.,0,,False
74,"We show such an example in Figure 1 part 2, where the current",0,,False
75,"ranker wt0 and a randomly sampled candidate ranker wt1 rank the candidate documents in the same order. As a result, no interleaved",0,,False
76,test can differentiate their ranking quality in this query. NSGD,0,,False
77,"avoids proposing wt1, and favors wt2 as it ranks the documents in a reverse order and would therefore give the interleaved test a better",0,,False
78,"chance of recognizing the difference between wt0 and wt2. Third, if an interleaved test fails to recognize the best ranker in a query,",0,,False
79,"e.g., a tie is encountered as shown in Figure 1 part 3, we compare",0,,False
80,"the tied rankers on the most recent worst performing queries (i.e.,",0,,False
81,"the difficult queries) with the recorded clicks to differentiate the rankers. Eventually, NSGD aims to reach w with a minimal number",0,,False
82,"of interactions as shown in Figure 1, i.e., faster convergence. The",0,,False
83,"detailed procedures in NSGD are shown in Algorithm 2, and we",0,,False
84,will discuss the key steps of it in the following.,0,,False
85,· Null Space Gradient Exploration. NSGD maintains a fixed size,1,ad,True
86,queue Q of recently explored directions and their corresponding,0,,False
87,"quality, i.e., Q ,",0,,False
88,"(i , qi )",0,,False
89,"T i ,1",0,,False
90,",",0,,False
91,which,0,,False
92,is,0,,False
93,constructed,0,,False
94,in,0,,False
95,line,0,,False
96,32,0,,False
97,to 37 in Algorithm 2. We denote the quality qi of an explored,0,,False
98,147,0,,False
99,Session 1D: Learning to Rank I,1,Session,True
100,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
101,Null space exploration to avoid repeatedly,0,,False
102,w t-1,0,,False
103,1,0,,False
104,exploring less promising directions.,0,,False
105,w,0,,False
106,w,0,,False
107,w*,0,,False
108,t-2,0,,False
109,t,0,,False
110,w w,0,,False
111,w2 d t,0,,False
112,d d,0,,False
113,w0 t,0,,False
114,d,0,,False
115,d,0,,False
116,w1 d,0,,False
117,td,0,,False
118,d,0,,False
119,"c 2 ,c 0 tt",0,,False
120,X X X ... t-1 t-3 t-5,0,,False
121,Preselect differentiable candidate by,0,,False
122,2,0,,False
123,current documents with associated query.,0,,False
124,w 2 >w 0,0,,False
125,t,0,,False
126,t,0,,False
127,Tie breaking function using historical difficult queries,0,,False
128,3,0,,False
129,when multiple winners occur.,0,,False
130,Figure 1: Illustration of model update procedure for the Null Space Gradient Descent algorithm.,1,ad,True
131,direction i as the received click credit difference between the,0,,False
132,"corresponding exploratory ranker and the default ranker by then (i.e., line 33). Intuitively, qi measures the improvement in ranking quality contributed by the update direction i ; when qi is negative, it suggests the direction i cannot improve the current ranker, and",0,,False
133,"therefore should be discouraged in future. To realize this, after receiving a user query, NSGD first constructs G ,"" [1, ..., k ] by selecting the top k worst performing historical directions from Q (i.e., line 8), and then solves for the null space of G denoted as G "","" NullSpace(G) (i.e., line 9). The new exploratory directions are sampled from G (i.e., line 15). Because every vector in the space of G is orthogonal to all k selected historical directions, those ineffective directions (and any linear combination of them)""",0,,False
134,will not be tested in this query.,0,,False
135,Our null space exploration strategy is based on two mild assump-,0,,False
136,"tions: queries are independent and identically distributed (i.i.d.),",0,,False
137,"and the gradient of the target (unknown) utility function satisfies Lipschitz continuity, i.e., ||f^(w1)-f^(w2)||   ||w1 -w2||, where  is a Lipschitz constant for the target utility function f (w). The",1,ad,True
138,assumption that queries are i.i.d. is studied and widely adopted in,1,ad,True
139,"existing learning to rank research [14, 15]. This assumption allows",0,,False
140,NSGD to compare gradient performance across queries and select k worst performing gradients from previous queries. Lipschitz continuity assumption suggests similar rankers would share similar,1,ad,True
141,gradient fields for the same query. This assumption is mild and,1,ad,True
142,"consistent with most of existing learning to rank algorithms [3, 16].",0,,False
143,"However, this assumption requires us to construct the null space",0,,False
144,from all historically explored directions whose associated rankers have a similar weight vector w as the current ranker's. This is,0,,False
145,"clearly infeasible in an online learning setting, as we would have to",0,,False
146,"store the entire updating history and examine it in every iteration. In NSGD, because the learning rate  is set to be small, rankers with",0,,False
147,"close temporal proximity will have similar feature weight vectors,",0,,False
148,"and therefore share a similar gradient field. Hence, NSGD only maintains the most recently tested directions in Q, which approximates the Lipschitz continuity. In our empirical evaluation, we",1,ad,True
149,"also tested the exhaustive solution, but aside from the significantly",0,,False
150,"increased storage and time complexity, little ranking performance",0,,False
151,improvement was observed. This supports our construction of the,0,,False
152,null space in NSGD.,0,,False
153,"Another benefit of sampling from null space is that the search takes place in a reduced problem space. DBGD-type algorithms have to sample in the whole d-dimensional space, while NSGD only samples from a subspace of it, whose rank is at most d - k, when the top k worst performing historical gradients are orthogonal to each other. This advantage is especially appealing when the dimension of ranking features is high, which is usually the case in practical learning to rank applications [4, 16].",1,ad,True
154,"There are two ways to sample from the null space G in NSGD (i.e., line 11): uniformly selecting the basis vectors of the null space or sampling random unit vectors inside the null space. Randomly selecting the basis vectors can maximize the coverage of sampled directions in the null space, as the basis vectors are linearly independent from each other. It improves the exploration efficiency in an early stage. Zhao et al. tested a similar idea in [30], but they performed it over the entire parameter space. However, in the later stage of model update, the true gradients are usually concentrating in a specific region; continuing to select those independent basis vectors becomes less effective. Exploring linear combinations of those basis vectors, i.e., uniformly sampling inside the space, emerges as a better choice then. But directly sampling from the null space at the beginning might be less effective, as it tends to introduce smaller variance in proposing different directions.",1,ad,True
155,"To take advantage of these two sampling schemes, we propose a hybrid sampling method in the null space: comparing with the winning ranker wt0-k created in iteration t - k, if ||wt0 - wt0-k || < 1 - , we switch to sample random unit vectors in G; otherwise uniformly select the basis vectors of G. The intuition behind this switching control is that when the consecutive rankers become similar, it indicates the gradients have converged to a local optimal region, and a refined search is needed to identify the true gradient. Otherwise, the gradient direction has not been identified, and larger diversity is needed to accelerate the exploration of null space. Oosterhuis and Rijke [18] also proposed a similar idea to detect model convergence and convert to more complex models when a simpler model has converged. But their conversion might not always be feasible, e.g., when no linear mapping between models exists; we only switch the sampling schemes for exploratory directions, which has no additional assumption about the model space. · Context-Dependent Ranker Preselection. NSGD selectively constructs the candidate rankers to maximize the chance that they",1,ad,True
156,148,0,,False
157,Session 1D: Learning to Rank I,1,Session,True
158,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
159,Algorithm 2 Null Space Gradient Descent (NSGD),1,ad,True
160,"1: Inputs:  , , n, m, k, kh,T,Th",0,,False
161,"2: Initiate w10 , sample_unit_vector() 3: Set Q , queue(T) and Qh , queue(Th ) as fixed size queues 4: for t , 1 to T do",0,,False
162,"5: Receive query Xt ,"" {x1, x2, ..., xs }""",0,,False
163,6: 7:,0,,False
164,Generate ranked,0,,False
165,"x¯t ,",0,,False
166,"s i ,1",0,,False
167,xi,0,,False
168,list,0,,False
169,l,0,,False
170,(Xt,0,,False
171,",",0,,False
172,wt0),0,,False
173,"8: Construct G ,"" [1, ..., k ] by directions selected from Q""",0,,False
174,with the worst recorded quality q,0,,False
175,"9: G , NullSpace(G)",0,,False
176,"10: for i , 1 to n do",0,,False
177,11:,0,,False
178,"ti , sample_unit_vector(G)",0,,False
179,12: end for,0,,False
180,"13: Select top m gradients that maximize x¯tTti from {ti }in,1",1,ad,True
181,"14: for i , 1 to m do",0,,False
182,15:,0,,False
183,"wti , wt0 + ti",0,,False
184,16:,0,,False
185,"Generate ranked list l(Xt , wti )",0,,False
186,17: end for,0,,False
187,"18: Set Lt ,"" Multileave {l(Xt , wti )}im"",""0 , and present Lt to user""",0,,False
188,"19: Receive click positions Ct on Lt , and infer click credits",0,,False
189,"{cti }im,0 for all rankers 20: Infer winner set Bt from {cti }im,0",0,,False
190,21: if |Bt | > 1 then,0,,False
191,22:,0,,False
192,Select kh worst performing queries,0,,False
193,"(Xi , Li , Ci )",0,,False
194,"kh i ,1",0,,False
195,"from Qh by Eval(Li , Ci ).",0,,False
196,23:,0,,False
197,"j , arg maxo Bt",0,,False
198,"kh i ,1",0,,False
199,Eval(l,0,,False
200,(X,0,,False
201,i,0,,False
202,",",0,,False
203,wo,0,,False
204,"),",0,,False
205,Ci,0,,False
206,),0,,False
207,24: else,0,,False
208,25:,0,,False
209,Set j to the sole winner in Bt,0,,False
210,26: end if,0,,False
211,"27: if j , 0 then",0,,False
212,28:,0,,False
213,"wt0+1 , wt0",0,,False
214,29: else,0,,False
215,30:,0,,False
216,"wt0+1 , wt0 + tj",0,,False
217,31: end if,0,,False
218,"32: for i , 1 to m do",0,,False
219,33:,0,,False
220,"qti , cti - ct0",0,,False
221,34:,0,,False
222,if qti < 0 then,0,,False
223,35:,0,,False
224,"Append (ti , qti ) to Q",0,,False
225,36:,0,,False
226,end if,0,,False
227,37: end for,0,,False
228,"38: Append (Xt , Lt , Ct ) to Qh",0,,False
229,39: end for,0,,False
230,"can be differentiated from the current best ranker wt in the interleaved tests. A straightforward solution is to select the rankers which give totally distinct ranking orders to that from wt0. But this clearly emphasizes too much the exploration of new directions, but ignores the exploitation of current best ranker. Especially in the later stage of model update when the current ranker can already provide satisfactory ranking results, a very distinct ranking indicates a higher risk of providing worse result quality.",1,ad,True
231,"To balance the needs for exploration and exploitation, we propose a Context-Dependent Preselection (CDP) criterion as shown in line 13 of Algorithm 2: after randomly sampling n vectors from G, we select the top m of them that maximize the inner product with the aggregate document feature vector x¯ for query Xt .",0,,False
232,"This can be understood as a necessary condition for having a proposed ranker that generates a different ranked list in Xt than that from wt0. More specifically, as we are learning a linear ranker, the ranking score of each document is computed by the inner product between document feature vector xi and the feature weight vector wt0; and the ranking scores lead to the ranked list l(Xt , wt0). To generate a different ranked list, there has to be at least one document that has different ranking scores under these two rankers, i.e., j, |xjT(wti - wt0)| > 0. This can be simplified as j, |xjTti | > 0; and by the triangle inequality (i.e., |a| + |b |  |a + b |), we require a differentiable ranker to satisfy | j xjTti | > 0. To choose the candidate rankers that can best satisfy this condition, we select the top m proposal directions that maximize this inner product. · History-Dependent Tie Breaking. NSGD is flexible in selecting the number of rankers for comparison: the hyper-parameter m in line 13 is an input to the algorithm. If multiple rankers are selected for comparison, multi-interleaving [24] can be performed to compare the quality of the proposed rankers, i.e., infer the click credit cti for each ranker wti and determine the winning ranker (i.e., line 19 and 20). However, because of position bias in user clicks [11], very few result documents will be clicked each time. The sparsity in result clicks directly reduces the resolution of interleaved test in recognizing the winning ranker, e.g., multiple rankers might share the same aggregate click credit. The situation becomes even worse when multiple rankers are compared. Existing solutions break the tie arbitrarily [29, 30] or heuristically take the mean vector of rankers in the winner set [23]. No solutions consider the ranking problem at hand, and they are not effective in general.",1,ad,True
233,"We propose the idea of leveraging historical queries, especially the most difficult ones, to choose the winner whenever a tie happens. First, in line 38, the 3-tuple comprised of the historical query, its displayed ranking list, and its corresponding click positions are stored in a fixed size queue. In future iterations, they are selected in line 22 to identify the best ranker, whenever a tie happens. Because only click feedback is available in online learning, we use click position Ct in the evaluation function Eval(Lt , Ct ), such as MAP or NDCG by treating clicked documents as relevant, to measure the ranking quality of Lt in query Xt (i.e., line 22 and 23). More importantly, because the ranker is improving on the fly, a poorly served query might be caused by a badly performing ranker, rather than its intrinsic difficulty. Therefore, in NSDG we only collect recent click results to select the most discriminative queries.",1,MAP,True
234,4 EXPERIMENTS,0,,False
235,"In this section, we perform extensive empricial comparisons between our proposed Null Space Gradient Descent (NSGD) algorithm with four state-of-the-art OL2R algorithms on five public learning to rank benchmarks. Both quantitative and qualitative evaluations are performed to examine our proposed gradient space exploration strategies, especially their advantages over the existing solutions in improving online learning efficiency.",1,ad,True
236,4.1 Experiment Setup,0,,False
237,"· Datasets. We used five benchmark datasets which are part of the LETOR 3.0 and LETOR 4.0 collections [17]: MQ2007, MQ2008, TD2003, NP2003 and HP2003. Among them, NP2003 and HP2003 implement navigational tasks, such as homepage finding and namedpage finding; TD2003 implements topic distillation, which is an informational task; MQ2007 and MQ2008 mix both types of tasks.",1,MQ,True
238,149,0,,False
239,Session 1D: Learning to Rank I,1,Session,True
240,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
241,(a) Perfect,0,,False
242,(b) Navigational,0,,False
243,(c) Informational,0,,False
244,Figure 2: Offline NDCG@10 on MQ2007 dataset under three click models.,1,MQ,True
245,(a) Perfect,0,,False
246,(b) Navigational,0,,False
247,(c) Informational,0,,False
248,Figure 3: Standard deviation of offline NDCG@10 on MQ2007 dataset under three click models.,1,MQ,True
249,(a) Perfect,0,,False
250,(b) Navigational,0,,False
251,(c) Informational,0,,False
252,Figure 4: Discounted cumulative NDCG@10 on MQ2007 dataset under three click models.,1,MQ,True
253,"Documents in TD2003, NP2003 and HP2003 datasets are collected from the .GOV collection, which is crawled from the .gov domain; while the MQ2007 and MQ2008 datasets are collected from 2007 and 2008 Million Query track at TREC [26]. In these datasets, each query-document pair is encoded as a vector of ranking features, including PageRank, TF.IDF, BM25, and language model on different parts of a document. The number of features is 46 in MQ2007 and MQ2008, and 64 for the other three datasets. In the MQ2007 and MQ2008 datasets, every document is marked with a relevance label between 0 and 2, while the other datasets only have binary labels. The MQ2007 and MQ2008 datasets contain 1,700 and 1,800 queries respectively, but with fewer assessments per query; while each of the other three datasets only contain fewer than 150 queries but with 1,000 assessments per query. All of the datasets are split into 5 folds for cross validation. We take the training set for online experiments gathering cumulative performance, and use testing set for offline evaluation. · Simulating User Clicks. To make our reported results comparable to existing literature, we follow the standard offline evaluation scheme proposed in Lerot [22], which simulates user interactions",1,TD,True
254,"with an OL2R algorithm. We make use of the Cascade Click Model [7] to simulate user click behavior. The click model simulates user interaction with the system by assuming that as a user scans through the list he/she makes a decision about whether or not to click on a returned document. The probability of a user clicks on a document is conditioned on the relevance label. Likewise, after clicking, the user makes a decision about continuing to look through the documents or to stop. The probability of this decision is also conditioned on the current document's relevance label. Adjusting these probabilities allows us to simulate different types of users.",1,ad,True
255,"We use three click model configurations as shown in Table 1, including: 1) perfect user who clicks on all relevant documents and does not stop browsing, which contributes the least noise; 2) navigational user who would stop early once they've found a relevant document; and 3) informational user who sometimes clicks on irrelevant documents in their search for information, which contributes the most noise. The length of resulting list evaluated by the click models is set to 10 as a standard setting in [23, 30]. · Evaluation Metrics. To evaluate an OL2R algorithm, cumulative Normalized Discounted Cumulative Gain (NDCG) and offline",0,,False
256,150,0,,False
257,Session 1D: Learning to Rank I,1,Session,True
258,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
259,Table 1: Configurations of simulation click models.,0,,False
260,Relevance grade Perfect,1,ad,True
261,Navigational Informational,0,,False
262,Click Probability 01 2 0.0 0.5 1.0,0,,False
263,0.05 0.5 0.95 0.4 0.7 0.9,0,,False
264,Stop Probability 01 2 0.0 0.0 0.0 0.2 0.5 0.9 0.1 0.3 0.5,0,,False
265,"NDCG are commonly used to assess the learning rate and ranking quality of the algorithm [22]. Cumulative NDCG is calculated with a discount factor of  set to 0.995 for each iteration. To assess model estimation convergence, in each iteration we measure cosine similarity between the weight vector updated by an OL2R algorithm and a reference weight vector, which is estimated by an offline learning to rank algorithm trained on the manual relevance judgments. In our experiment, we used LambdaRank [3] with no hidden layer to obtain such a reference ranker in each dataset, because of its superior empirical performance. For all experiments, we fix the total number of iterations T to 1,000 and randomly sample query Xt from the dataset with replacement accordingly. · Evaluation Questions. We intend to answer the following questions through empirical evaluations, to better understand the advantages of our proposed algorithm.",1,ad,True
266,Q1: How does our proposed NSGD algorithm perform in comparison to various baseline OL2R methods?,0,,False
267,Q2: Do candidate directions generated by NSGD explore the gradient space more efficiently than uniform sampling from the entire parameter space?,1,ad,True
268,Q3: How do the different components in NSGD contribute to its final performance?,0,,False
269,Q4: How do different settings of hyper-parameters alter the performance of NSGD?,0,,False
270,· Baselines. We chose the following four state-of-the-art OL2R algorithms as our baselines for comparison:,0,,False
271,- DBGD [29]: A single direction uniformly sampled from the entire parameter space is explored. Team Draft is used to interleave the results of the two rankers for comparison.,0,,False
272,- CPS [9]: It proposes a candidate preselection strategy that uses historical data to preselect the proposed rankers before the interleaved test in DBGD.,0,,False
273,"- DP-DBGD [30]: Two opposite uniformly sampled directions are explored in DBGD. Both Contextual Interleave, which favors the winning direction from the previous iteration, and Team Draft are used in it in our experiment.",0,,False
274,"- MGD [23]: Multiple uniformly sampled directions are explored in single iteration. Multileave is used to interleave the results. If there is a tie, the model updates towards the mean of all winners.",0,,False
275,4.2 Online and offline performance of NSGD,0,,False
276,"We start with our first evaluation question: how does NSGD perform in comparison with baseline OL2R methods? We run all OL2R algorithms over all 5 datasets and all 3 click models. According to the standard hyper-parameter settings of DBGD [29] and other baselines, we set  to 1 and  to 0.1. For algorithms that can explore multiple candidates, including MGD and NSGD, we set number of candidates explored in one iteration to 4, (i.e., m ,"" 4 in NSGD). For NSGD, we set k "","" 25, T "","" 15, kh "","" 10, and Th "", 50. We will discuss the effect of these different hyper-parameters on NSGD in",0,,False
277,"Section ??. All experiments are repeated 15 times for each fold, and we report the average performance.",0,,False
278,"Figures 2 and 4 report the offline and online performance of all OL2R methods on MQ2007 dataset under perfect, navigational and informational click models. We also report the standard deviation of offline NDCG in every iteration of model update on this dataset in Figure 3. Due to the space limit, we cannot report the detailed performance over other datasets, but we summarize the final performance in Table 2 and 3 respectively. From Figure 4, we observe that CPS and NSGD, which both apply candidate preselection, perform better than other methods in terms of cumulative NDCG. This confirms that exploring carefully selected candidate directions generally improves the learning speed in the early iterations compared with the uniform sampling strategy used in other baselines. Our proposed NSGD further improves online learning efficiency over CPS by exploring inside the null space rather than the entire parameter space. From Table 2, we can observe the consistent improvement of NSGD for most of the datasets and click models, which proves the accelerated learning speed by performing more efficient gradient exploration during its online learning process.",1,MQ,True
279,"In Figure 2 we first observe that NSGD improves offline NDCG significantly faster than other baselines, which generally require much more interactions with users to reach the same performance. This further explains our above analysis of the improved learning speed of NSGD shown in Figure 4. For informational users, MGD requires more than 800 iterations to reach performance comparable to NSGD at less than 200 iterations. From Table 3 we observe that algorithms that explore multiple candidate directions in one iteration, including MGD and NSGD, consistently achieve better offline performance than other methods on all 5 datasets and 3 click models. Compared with MGD, NSGD further improves the final offline NDCG on MQ2007, MQ2008 and NP2003 datasets, especially for the informational users. We have discussed in Section 4.1 that MQ2007 and MQ2008 contain more queries with fewer assessments per query. This improvement suggests that NSGD can better identify the most effective exploration directions even under a noisy environment. We have also tested MGD with 9 candidates explored in one iteration (i.e., m ,"" 9) which has achieves best performance according to [23], and observed same consistent improvement of NSGD over MGD with 9 candidates in online performance. Due to space limit we did not report the performance of MGD with 9 candidates in Table2 and 3.""",1,MQ,True
280,"Figure 3 shows the standard deviation of offline NDCG at each iteration. We observe that both NSGD and MGD enjoy a much smaller standard deviation in the perfect and navigational users, suggesting that exploring multiple directions reduce the variance introduced by random exploration. Another reason for the reduced variance in NSGD is the hybrid sampling method mentioned in Section 3.2: the result confirms that first sampling from the basis vectors of null space and then sampling inside the null space provides a more effective exploration, which not only improves learning efficiency but also effectively reduces variance in an early stage. For informational users, who have a lower stop probability and are likely to generate more clicks, they typically contribute nosier clicks and more ties in the comparison. In this case, NSGD reaches a much smaller standard deviation compared with MGD and all other baselines. The reason is that NSGD applies context-dependent candidate preselection to propose the most differentiable directions and use most difficult queries to discern tied candidates. Although",0,,False
281,151,0,,False
282,Session 1D: Learning to Rank I,1,Session,True
283,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
284,Table 2: Online score (discounted cumulative NDCG@10) and standard deviation of each algorithm after 1000 queries under each of the three click models. Statistically significant improvements over MGD baseline are indicated by  (p<0.05).,0,,False
285,Click Model Perfect,0,,False
286,Navigational Informational,0,,False
287,Dataset MQ2007 MQ2008 HP2003 NP2003 TD2003 MQ2007 MQ2008 HP2003 NP2003 TD2003 MQ2007 MQ2008 HP2003 NP2003 TD2003,1,MQ,True
288,DBGD 61.931 (5.535) 81.327 (6.224) 110.012 (8.627) 101.004 (8.702) 39.856 (7.770) 57.989 (4.657) 76.411 (5.983) 95.775 (14.394) 84.699 (12.275) 33.954 (8.368) 55.427 (5.639) 73.941 (6.101) 59.376 (23.637) 56.996 (20.547) 23.021 (8.675),0,,False
289,CPS 59.936 (4.875) 77.694 (6.137) 109.279 (8.565) 98.774 (8.884) 38.054 (6.999) 59.669 (4.911) 75.603 (7.230) 95.925 (12.628) 88.240 (13.039) 35.857 (8.729) 57.094 (5.689) 74.825 (5.419) 56.004 (22.101) 54.615 (19.354) 23.826 (7.964),0,,False
290,DP-DBGD 58.995 (4.926) 76.192 (6.452) 92.422 (11.358) 79.636 (13.338) 34.289 (7.703) 57.301 (4.816) 74.984 (5.959) 88.773 (11.518) 74.521 (14.810) 31.468 (7.322) 55.619 (5.066) 72.392 (6.259) 66.295 (16.782) 62.067 (17.667) 24.948 (6.848),0,,False
291,MGD 59.765 (3.015) 77.543 (4.827) 101.675 (4.943) 104.677 (5.399) 38.380 (5.383) 57.884 (3.266) 75.001 (5.085) 82.244 (26.944) 100.581 (8.962) 36.092 (5.616) 55.338 (3.395) 72.757 (4.690) 75.314 (11.281) 74.497 (13.249) 28.482 (5.299),0,,False
292,NSGD 68.639 (3.311)  88.811 (6.022)  113.890 (8.276)  115.145 (6.287)  42.402 (7.654) 66.635 (2.832)  84.091 (4.553)  109.783 (5.634)  109.433 (5.649)  41.274 (7.318)  67.312 (3.438)  84.053 (4.980)  108.592 (5.503)  108.624 (5.831)  39.386 (7.148) ,0,,False
293,Table 3: Offline score (NDCG@10) and standard deviation of each algorithm after 1000 queries under each of the three click models. Statistically significant improvements over MGD baseline are indicated by  (p<0.05).,0,,False
294,Click Model Perfect,0,,False
295,Navigational Informational,0,,False
296,Dataset MQ2007 MQ2008 HP2003 NP2003 TD2003 MQ2007 MQ2008 HP2003 NP2003 TD2003 MQ2007 MQ2008 HP2003 NP2003 TD2003,1,MQ,True
297,DBGD 0.369 (0.030) 0.465 (0.042) 0.760 (0.067) 0.704 (0.052) 0.267 (0.082) 0.359 (0.034) 0.459 (0.038) 0.728 (0.063) 0.709 (0.035) 0.276 (0.095) 0.319 (0.047) 0.425 (0.050) 0.500 (0.196) 0.526 (0.190) 0.174 (0.099),0,,False
298,CPS 0.383 (0.026) 0.474 (0.042) 0.764 (0.068) 0.702 (0.050) 0.296 (0.094) 0.365 (0.037) 0.456 (0.037) 0.734 (0.072) 0.661 (0.066) 0.285 (0.093) 0.325 (0.049) 0.434 (0.047) 0.463 (0.191) 0.443 (0.179) 0.178 (0.092),0,,False
299,DP-DBGD 0.361 (0.032) 0.461 (0.041) 0.762 (0.062) 0.682 (0.062) 0.286 (0.091) 0.339 (0.031) 0.445 (0.045) 0.752 (0.061) 0.675 (0.061) 0.269 (0.087) 0.325 (0.037) 0.422 (0.054) 0.669 (0.103) 0.657 (0.118) 0.219 (0.094),0,,False
300,MGD 0.408 (0.018) 0.487 (0.037) 0.771 (0.062) 0.712 (0.048) 0.308 (0.096) 0.393 (0.024) 0.477 (0.036) 0.707 (0.156) 0.707 (0.052) 0.303 (0.098) 0.355 (0.036) 0.450 (0.041) 0.736 (0.063) 0.660 (0.059) 0.271 (0.090),0,,False
301,NSGD 0.411 (0.019) 0.488 (0.043) 0.752 (0.752) 0.714 (0.049) 0.289 (0.092) 0.398 (0.022) 0.478 (0.037) 0.744 (0.073) 0.710 (0.039) 0.274 (0.094) 0.383 (0.020)  0.472 (0.036) 0.713 (0.069) 0.707 (0.044)  0.251 (0.085),0,,False
302,"CPS also uses historical interactions to preselect the rankers, it uniformly selects historical interactions, which are not necessarily informative. As a result, the ranking quality of CPS oscillates when the fidelity of user feedback is low.",0,,False
303,4.3 Zooming into NSGD,0,,False
304,"To answer the second and third evaluation questions, we design detailed ablation studies to carefully study NSGD. All the experiments in this section were conducted on MQ2007 under the informational click model, as the dataset has the largest amount of queries and the click model makes the retrieval task the most challenging.",1,MQ,True
305,"In the first experiment, we trained an offline LambdaRank model [3] without any hidden layer using manual relevance labels. The model obtained the best offline NDCG performance in this dataset (around 0.437 in average). Its model parameter is denoted as w. We compare cosine similarity between the weight vector estimated by NSGD and w, to that between the weight vectors generated by MGD and DBGD and w in each iteration. In Figure 5 (a) we can observe that NSGD moves towards w much faster than both MGD and DBGD, which suggests the update directions explored by NSGD are more effective in recognizing the important ranking features.",0,,False
306,"However, note that the final converged model in NSGD is not identical to w, and the final offline NDCG of all OL2R algorithms is worse than LambdaRank's. This is expected: LambdaRank is directly trained by manual labels. To improve an online trained model, one possible solution is to pre-train its weight vector with some offline data, and continue training it with online user feedback. This will take advantage of both training schemes.",1,ad,True
307,"The second experiment serves to study the utility of gradients proposed by NSGD. We mix uniform exploratory directions from the entire parameter space with directions proposed from null space in the same algorithm. Specifically, we have in total 4 candidate rankers for multileaving, in which we vary the number of candidates created by null space gradients from 4 to 0, and we report the selection ratio, i.e., the frequency of selecting null space proposed rankers over the current ranker, versus the frequency of selecting uniformly proposed rankers over the current ranker. This ratio is also normalized by the number of proposed rankers in each type, to make the results comparable. We also report the online performance of each combination to understand the consequence of selecting different types of rankers. The result is shown in Figure 5 (b). We can clearly observe that comparing with the uniform exploratory",1,ad,True
308,152,0,,False
309,Session 1D: Learning to Rank I,1,Session,True
310,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
311,(a) Cosine similarity between online learnt (b) Selection ratio comparing null space and,0,,False
312,model and offline best model w,0,,False
313,uniform gradients,1,ad,True
314,(c) Ablation analysis of NSGD,0,,False
315,Figure 5: Detailed experimental analysis of NSGD on MQ2007 dataset.,1,MQ,True
316,"rankers, rankers proposed by NSGD are always more likely to be selected as a better ranker in all combinations. We see that with more candidates proposed by NSGD, the online performance also increases. These results clearly show the superior quality of directions explored by NSGD, and explains the performance improvement observed comparing with other baselines that uniformly sample directions to explore.",0,,False
317,"To better understand the contribution of different components in NSGD, we disable them in turn and experiment on the resulting variants of NSGD. Specifically, we compare the following four models: 1) NSGD; 2) NSGD without tie breaking, denoted as NSGD w/o (TB); 3) NSGD without tie breaking and context-dependent preselection, denoted as NSGD w/o (CDP & TB); and 4) MGD. The result is reported in Figure 5(c). Comparing NSGD w/o (CDP & TB) with MGD, where the difference is exploring in the null space or entire parameter space, confirms the utility of null space gradient exploration, which avoids repeatedly exploring recent and less promising directions. We want to mention that NSGD w/o (CDP & TB) also significantly improves the learning speed and quickly reaches close to its high offline NDCG in less than 200 iterations, but it took MGD more than 800 iterations to achieve its highest performance. Comparing NSGD w/o (CDP & TB) against NSGD w/o (TB), we observe that our context-dependent candidate preselection further improves the performance by selecting candidates that can be best differentiated by the current query in interleaved tests as compared with uniformly exploring inside the null space. Comparing NSGD w/o (TB) with NSGD, we observe that using difficult queries for tie breaking further improves the performance, rather than arbitrarily breaking the tie or taking the average of winners as suggested by [23], which often introduces unexpected variance in online learning.",1,TB,True
318,"To answer the fourth evaluation question, we study the effect of different hyper-parameter settings of NSGD and their corresponding online performance in the MQ2007 dataset with the three click models mentioned above. · Number of candidates. We vary the number of proposed candidate rankers m from 1 to 10, from which the best ranker set is chosen through team-draft multileaving [23]. The result is reported in Figure 6 (a). Although each click model has different best-performing candidate size, with more candidate directions proposed the performance generally first increases and then slightly decreases. As more candidates are proposed, even though more directions can be explored, it is also easier to have multiple winners in the interleaved test, which introduces unnecessary complexity in recognizing the",1,MQ,True
319,"best ranker. For example, we can clearly observe a trend of decreasing performance across all click models when m is larger than 5. Specifically, since the result list length is set to 10, each ranker will on average only receive 2 clicks when 4 new rankers are proposed. This makes it common to have tied winners. This serves as further motivation for having an effective tie-breaking function in NSGD. We do not present results for m larger than 10 as each candidate ranker can only expect less than one click and the feedback from interleaved test is non informative. · Learning rate. In all DBGD-type OL2R algorithms, the exploration step size is decided by  and the learning rate for updating current ranker is decided by the choice of . Here we study the effect of different learning rate , by fixing  to 1. Figure 6 (b) shows the result of varying  from 0 to 0.5. We notice that in most cases  around 0.1 gives the best performance. This suggests that even though we are exploring with a large step size  , we should use relatively small learning rate  to avoid over-exploration. · Number of historical gradients to construct null space. As mentioned in Section 3.2, when using k historical gradients to construct the null space, NSGD only samples from a subspace whose rank is at most d - k. We vary the choice of k from 5 to 40 (as in MQ2007 the feature dimension d is 46). The result is showed in Figure 6 (c). We observe that when we increase k to 20, the performance keeps relatively stable; but when k goes beyond it, the performance decreases significantly. The key reason is the null space overly reduces the search space when k is too large, such that it prevents NSGD from finding a good direction to explore, and forces it to converge to a suboptimal model quickly. · Number of historical queries for tie breaking. When the algorithm receives multiple winning rankers from an interleaved test, we use the most recent difficult queries to identify the best ranker. In this experiment, we vary the number of historical queries kh for tie breaking from 0 (which means disable our tie breaking function) to 40. The result is showed in Figure 6 (d). Evidently, using more historical queries for tie-breaking leads to an increase the algorithm's performance. However, evaluating candidates over a large number of historical queries also increases the time and storage complexity. To balance computational efficiency and final performance, we set kh , 10 for NSGD in all previous experiments.",1,ad,True
320,5 CONCLUSIONS,0,,False
321,"In this paper, we propose Null Space Gradient Descent (NSGD) to accelerate and improve online learning to rank. To avoid repeatedly exploring less promising directions, NSGD reduces its exploration space to the null space of recently poorly performing directions.",1,ad,True
322,153,0,,False
323,Session 1D: Learning to Rank I,1,Session,True
324,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
325,(a) Number of candidates,0,,False
326,(b) Learning rate ,0,,False
327,(c) Number of historical gradients (d) Number of historical queries for,1,ad,True
328,to construct null space,0,,False
329,tie breaker,0,,False
330,Figure 6: Performance of NSGD under different hyperparameter settings on MQ2007 dataset.,1,MQ,True
331,"To identify the most effective exploratory rankers, NSGD uses a context-dependent preselection strategy to select candidate rankers that maximize the chance of being differentiated by an interleaved test for the current query. When two or more rankers tie, NSGD uses historically difficult queries to evaluate and identify the most effective ranker. We performed thorough experiments over multiple datasets and show that NSGD outperforms both the standard DBGD algorithm as well as several state-of-the-art OL2R algorithms.",0,,False
332,"As our future work, it is important to study the theoretical properties of NSGD, including whether the directions proposed guarantee a low-bias estimation of the true gradients. As we observed in our empirical evaluations, the online trained models are generally worse than the offline trained ones, which benefit most from manual annotations. It is meaningful to combine these two types of learning schemes to maximize the utility of learnt models. Lastly, all OL2R algorithms consider consecutive interactions with users as independent; but this is not always true, particularly when users undergo complex search tasks. In this situation, balancing exploration and exploitation with respect to the search context becomes more important. We plan to explore this direction with our NSGD algorithm, as it already incorporates both long-term and short-term interaction history into gradient exploration.",1,ad,True
333,6 ACKNOWLEDGMENTS,0,,False
334,We thank the anonymous reviewers for their insightful comments. This work was supported in part by National Science Foundation Grant IIS-1553568 and IIS-1618948.,0,,False
335,REFERENCES,0,,False
336,"[1] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. 2002. Finite-time analysis of the multiarmed bandit problem. Machine learning 47, 2-3 (2002), 235­256.",0,,False
337,"[2] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. 1995. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on. IEEE, 322­331.",1,ad,True
338,"[3] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81.",0,,False
339,[4] Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview. In Proceedings of the Learning to Rank Challenge. 1­24.,1,Yahoo,True
340,"[5] Olivier Chapelle and Ya Zhang. 2009. A dynamic bayesian network click model for web search ranking. In Proceedings of the 18th international conference on World wide web. ACM, 1­10.",0,,False
341,"[6] Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. 2005. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 385­394.",1,ad,True
342,"[7] Fan Guo, Chao Liu, and Yi Min Wang. 2009. Efficient multiple-click models in web search. In Proceedings of the Second ACM International Conference on WSDM. ACM, 124­131.",0,,False
343,"[8] Katja Hofmann, Anne Schuth, Shimon Whiteson, and Maarten de Rijke. 2013. Reusing historical interaction data for faster online learning to rank for IR. In Proceedings of the sixth ACM international conference on WSDM. ACM, 183­192.",0,,False
344,"[9] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2013. Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval. Information Retrieval 16, 1 (2013), 63­90.",0,,False
345,"[10] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD. ACM, 133­142.",0,,False
346,"[11] Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. 2017. Accurately interpreting clickthrough data as implicit feedback. In ACM SIGIR Forum, Vol. 51. Acm, 4­11.",0,,False
347,"[12] Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, and Zheng Wen. 2016. DCM bandits: Learning to rank with multiple clicks. In International Conference on Machine Learning. 1215­1224.",0,,False
348,"[13] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. 2015. Cascading bandits: Learning to rank in the cascade model. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15). 767­776.",1,ad,True
349,"[14] Yanyan Lan, Tie-Yan Liu, Zhiming Ma, and Hang Li. 2009. Generalization analysis of listwise learning-to-rank algorithms. In Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 577­584.",0,,False
350,"[15] Yanyan Lan, Tie-Yan Liu, Tao Qin, Zhiming Ma, and Hang Li. 2008. Querylevel stability and generalization in learning to rank. In Proceedings of the 25th international conference on Machine learning. ACM, 512­519.",1,Query,True
351,"[16] Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval 3, 3 (2009), 225­331.",0,,False
352,"[17] Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and Hang Li. 2007. Letor: Benchmark dataset for research on learning to rank for information retrieval. In Proceedings of SIGIR 2007 workshop on learning to rank for information retrieval,",0,,False
353,Vol. 310.,0,,False
354,"[18] Harrie Oosterhuis and Maarten de Rijke. 2017. Balancing Speed and Quality in Online Learning to Rank for Information Retrieval. In Proceedings of the 2017 ACM CIKM. ACM, 277­286.",0,,False
355,"[19] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. 2008. Learning diverse rankings with multi-armed bandits. In Proceedings of the 25th international conference on Machine learning. ACM, 784­791.",1,ad,True
356,"[20] Filip Radlinski, Madhu Kurup, and Thorsten Joachims. 2008. How does clickthrough data reflect retrieval quality?. In Proceedings of the 17th ACM CIKM.",1,ad,True
357,"ACM, 43­52.",0,,False
358,"[21] Mark Sanderson et al. 2010. Test collection based evaluation of information retrieval systems. Foundations and Trends® in Information Retrieval 4, 4 (2010),",0,,False
359,247­375.,0,,False
360,"[22] Anne Schuth, Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2013. Lerot: An online learning to rank framework. In Proceedings of the 2013 workshop on Living labs for information retrieval evaluation. ACM, 23­26.",0,,False
361,"[23] Anne Schuth, Harrie Oosterhuis, Shimon Whiteson, and Maarten de Rijke. 2016. Multileave gradient descent for fast online learning to rank. In Proceedings of the Ninth ACM International Conference on WSDM. ACM, 457­466.",1,ad,True
362,"[24] Anne Schuth, Floor Sietsma, Shimon Whiteson, Damien Lefortier, and Maarten de Rijke. 2014. Multileaved comparisons for fast online evaluation. In Proceedings of the 23rd ACM CIKM. ACM, 71­80.",0,,False
363,"[25] Aleksandrs Slivkins, Filip Radlinski, and Sreenivas Gollapudi. 2013. Ranked ban-",1,ad,True
364,"dits in metric spaces: learning diverse rankings over large document collections. Journal of Machine Learning Research 14, Feb (2013), 399­436. [26] Ellen M Voorhees, Donna K Harman, et al. 2005. TREC: Experiment and evaluation in information retrieval. Vol. 1. MIT press Cambridge.",1,TREC,True
365,"[27] Hongning Wang, Yang Song, Ming-Wei Chang, Xiaodong He, Ahmed Hassan,",0,,False
366,and Ryen W White. 2014. Modeling action-level satisfaction for search task satisfaction prediction. In Proceedings of the 37th international ACM SIGIR conference.,0,,False
367,"ACM, 123­132.",0,,False
368,"[28] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. 2012. The k-armed dueling bandits problem. J. Comput. System Sci. 78, 5 (2012), 1538­1556.",0,,False
369,"[29] Yisong Yue and Thorsten Joachims. 2009. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 1201­1208.",0,,False
370,"[30] Tong Zhao and Irwin King. 2016. Constructing reliable gradient exploration for online learning to rank. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. ACM, 1643­1652.",1,ad,True
371,"[31] Masrour Zoghi, Tomas Tunys, Mohammad Ghavamzadeh, Branislav Kveton,",1,ad,True
372,"Csaba Szepesvari, and Zheng Wen. 2017. Online Learning to Rank in Stochastic Click Models. In International Conference on Machine Learning. 4199­4208.",0,,False
373,154,0,,False
374,,0,,False

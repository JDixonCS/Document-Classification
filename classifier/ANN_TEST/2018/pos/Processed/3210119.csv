,sentence,label,data,regex
0,Short Research Papers II,0,,False
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Beyond Pooling,0,,False
3,Gordon V. Cormack,0,,False
4,University of Waterloo gvcormac@uwaterloo.ca,0,,False
5,ABSTRACT,0,,False
6,"Dynamic Sampling is a novel, non-uniform, statistical sampling strategy in which documents are selected for relevance assessment based on the results of prior assessments. Unlike static and dynamic pooling methods that are commonly used to compile relevance assessments for the creation of information retrieval test collections, Dynamic Sampling yields a statistical sample from which substantially unbiased estimates of effectiveness measures may be derived. In contrast to static sampling strategies, which make no use of relevance assessments, Dynamic Sampling is able to select documents from a much larger universe, yielding superior test collections for a given budget of relevance assessments. These assertions are supported by simulation studies using secondary data from the TREC 2017 Common Core Track.",1,TREC,True
7,"ACM Reference Format: Gordon V. Cormack and Maura R. Grossman. 2018. Beyond Pooling. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210119",0,,False
8,1 INTRODUCTION,1,DUC,True
9,"This paper argues that the pooling method should be replaced by ""Dynamic Sampling"" (""DS""), in which active learning is used to select a stratified sample of documents for assessment to form the ""gold standard"" of relevance in an information retrieval (""IR"") test collection. The essential idea is to adapt Scalable Continuous Active Learning (""S-CAL"") [4]--a technology-assisted review method that repeatedly draws samples for assessment from strata of exponentially increasing size--for this task.",1,ad,True
10,"Each stratum consists of the documents deemed next-most-likely to be relevant by a learning algorithm, based on prior assessments. The feature representation employed by the learning algorithm may be derived from the relevance rankings afforded by the systems to be evaluated, relevance rankings afforded by reference systems, document content, or any combination of these approaches.",0,,False
11,"Together, the samples comprise a statistical sample of an arbitrarily large portion of the entire corpus--not just the top-ranked documents submitted for evaluation--whose size is bounded by a fixed assessment budget, regardless of the number of relevant documents in the corpus (R). An unbiased Horvitz-Thompson estimate of recall-independent measures, such as precision at rank (P@k),",0,,False
12,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210119",1,ad,True
13,Maura R. Grossman,0,,False
14,University of Waterloo maura.grossman@uwaterloo.ca,0,,False
15,"or R, can be derived from this sample [11]. Estimators for recalldependent measures, such as average precision, R-precison (P@R), or NDCG, which employ a non-linear combination of these elementary estimators, entail bias, which can be mitigated by arranging a low-variance estimate of R.",0,,False
16,"Dynamic Sampling is particularly suitable for creating test collections that may be used to evaluate the effectiveness of methods that are not available at the time of construction. Provided enough relevant documents are found by DS to provide a low-variance estimate of R for each topic, future methods have no systematic advantage or disadvantage relative to those whose rankings were used as input to the DS process.",1,ad,True
17,"Using data from the TREC 2017 Common Core Track1 [1], we simulate the use of DS to create test collections using assessment budgets of 100, 200, 300, and 600 documents per topic, which compare favorably to the official Common Core test collection, which was derived using the ""max mean"" dynamic-pooling strategy [9].",1,TREC,True
18,2 THE TREC 2017 COMMON CORE TRACK,1,TREC,True
19,"The TREC 2017 Common Core Track (""Core Track"") offered a strong baseline depth-100 subset pool consisting of 30,030 relevance assessments over 50 topics [9]. The authors employed the AutoTAR active learning method [5] to assess 11,825 documents for the same 50 topics. These two efforts yielded 9,002 and 8,986 positive assessments, respectively, but only 3,715 in common between them. For the purpose of this exposition, we consider ground truth to be the union of these two sets.",1,TREC,True
20,"The Core Track baseline is atypical because of two factors that enured to its advantage. First, the topics had been previously used for the TREC 2004 Robust Track [16], yielding 76,783 relevance assessments, albeit for a different set of documents. Thirty-three of the topics had also been used for the TREC 2005 Robust Track [17], yielding 23,911 assessments for yet another set of documents. Of the 55 system rankings used to form the Core Track depth-100 pool, 14 were influenced by these 100,694 prior assessments.",1,Track,True
21,"A second factor enuring to the advantage of the baseline was the influence of manual assessments conducted by participating teams such as ourselves. At least one other team also employed active learning and assessed many thousands of documents in the course of their participation; a total of 12 system rankings (including three of ours) were influenced by manual effort. The remaining 29 rankings used to form the pool were derived using fully automatic methods, without influence from historical or current relevance assessments.",1,ad,True
22,"While 75 runs, each consisting of 10,000 documents per run, were submitted by participating teams, the Core Track assessment effort considered only the top-ranked 100 documents from 55 runs deemed ""highest priority"" by each of the Track participants.",1,Track,True
23,1See https://trec-core.github.io/2017/.,1,trec,True
24,1169,0,,False
25,Short Research Papers II,0,,False
26,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
27,"The experiments detailed below evaluate Dynamic Sampling methods that consider all submitted documents; a pool consisting of one-third more runs and one-hundred times the pool depth. We also consider the effect of restricting the pool to the 29 fully automatic methods. Finally, we consider DS methods that use no pool at all, selecting documents for assessment from the universe of documents, based on their tf-idf word representation.",0,,False
28,3 DYNAMIC POOLING,0,,False
29,"Move-to-front pooling (""MTF"") [6] is arguably the first application of active learning to the selection of documents for assessment and inclusion in an IR test collection [13]. Unlike the depth-k pooling method, which selects the top-ranked k documents from each of a set of participating runs for assessment, active learning methods repeatedly select documents from among those not yet assessed, based on the relevance assessments rendered for previously selected documents. A number of studies [9, 13] indicate that for k < 100, MTF and other active learning methods yield better test collections than depth-k pooling, for the same number of assessments, relative to a gold-standard test collection constructed using depth-1002 pooling, which has been the de facto standard for nearly three decades.",1,ad,True
30,"The assumption that depth-100 pooling constitutes ground truth has influenced current practice, which typically applies active learning methods to select documents only from the depth-100 pool. This ""dynamic pooling"" approach requires fewer relevance assessments than the depth-100 pool, but yields an inferior test collection that can, at best, approach the quality of the depth-100 pool. Hereafter, we use the term ""subset pooling"" to refer to any strategy that selects a subset of a fixed-depth pool for assessment, and ""dynamic pooling"" to refer to a subset-pooling strategy that employs active learning. Thus, depth-k, meta-ranking, and statistical sampling are subset-pooling strategies, while MTF, hedge, and ""bandit"" methods [9] are dynamic-pooling strategies.",0,,False
31,4 SUBSET SAMPLING,0,,False
32,"The literature describes several methods to employ statistical sampling as a subset-pooling strategy [3, 11, 19]. The general approach is to draw a uniform or stratified random sample from the pool for assessment, and then to measure the effectiveness of a run by applying a statistical estimator to approximate the value of some effectiveness measure (e.g., P@k or average precision), were the entire pool to be judged.",0,,False
33,"The ""infAP"" family of methods [19] (generically, ""infAP"") use a separate statistical estimator for each stratum, and combine the results to form an overall estimate. The ""statAP"" family of methods [11] (generically, ""statAP""), on the other hand, employ a HorvitzThompson estimator [10] to form an overall estimate based on the inclusion probabilities of the relevant documents in the strata, without regard to the strata from which the documents came. The minimal test collections family of methods [3] (generically, ""MTC"") are not statistical sampling methods, as they provide intentionally biased estimates for the purpose of distinguishing among runs.",1,AP,True
34,"2The arguments here apply to depth-K pooling where K > k , but depth-100 is nearly universal, and should be considered synecdoche for depth-K .",0,,False
35,"Dynamic Sampling differs from subset sampling in that it is not constrained to selecting documents from a pool, and it identifies strata sequentially in response to the relevance assessments for previous strata. Of the methods used for subset sampling, only statAP is amenable for DS, because it provides a reasonably unbiased estimate, even with many sparsely sampled strata. In our empirical work, we use Pavlu's reference implementation of statAP.3",1,AP,True
36,5 TOTAL RECALL,0,,False
37,"Interactive Search and Judging (""ISJ""), which involves a sustained search effort to identify and label a substantial fraction of all relevant documents in a collection, has been shown to yield test collections of comparable quality to those yielded by depth-100 pooling, with less effort [6, 14]. More recently, a particular active learning approach has been shown to compare favorably to ISJ, while obviating the need for repeated query reformulation by a search expert [5]. The TREC 2002 Filtering Track [15] coordinators used a similar method to construct a substantially complete set of relevance assessments prior to the conduct of the Track, so that the relevance assessments could be used to simulate real-time feedback, as well as to evaluate the effectiveness of the submitted runs. Subsequent application of depth-100 pooling uncovered a number of relevant documents that were not discovered by the prior effort, but those additional documents were found to have an insubstantial impact on the resulting effectiveness estimates.",1,TREC,True
38,"The current state of the art in active learning for this purpose is AutoTAR [5], as realized by the TREC Total Recall Track Baseline Model Implementation (""BMI"").4 AutoTAR was used to label test collections prior to the TREC 2015 and 2016 Total Recall Tracks, and also was provided to Track partcicipants as a baseline method. No submitted run consistently bettered BMI, whether the effectiveness was measured using the official AutoTAR-selected assessments, statistically sampled assessments, or post-hoc assessments by one of the participating teams [7].",1,TREC,True
39,"The authors used a modified version of BMI to prepare their submission to the Core Track [2]. Although at the time of this writing, only 50 of the Core Track topics had been assessed, 250 topics were provided to participants. For 250 topics, the authors spent 64.1 hours assessing 42,587 documents (on average, 15.4 mins/topic; 5.4 secs/doc), judging 30,124 of them to be relevant (70.7%). Of these judgments, 11,825 pertained to the 50 topics that have, to date, been officially assessed. Given the Core Track's tight timeline, our efforts were necessarily incomplete. Nonetheless, we judged relevant roughly the same number of documents as the Core Track's official assessors. Of these, more than one-third were not in the Core Track's assessment pool. A statistical sample embedded in one of our submissions indicates that the majority of these unassessed documents would have been judged relevant, had they been assessed [2].",1,Track,True
40,"The TREC 2015 and 2016 Total Recall results show that AutoTAR can achieve near-perfect recall given 2R +100 assessments per topic, where R is the number of relevant documents to be found [8, 12]. According to this rule of thumb, we would have had to expend",1,TREC,True
41,3See http://trec.nist.gov/data/million.query/07/statAP_MQ_eval_v3.pl. This implementation corrects a serious error in an unpublished but commonly cited manuscript [10]. The description in Pavlu's dissertation [11] is correct. 4See http://cormack.uwaterloo.ca/trecvm/.,1,trec,True
42,1170,0,,False
43,Short Research Papers II,0,,False
44,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
45,about,0,,False
46,2,0,,False
47,1 2,0,,False
48,times,0,,False
49,as,0,,False
50,much,0,,False
51,assessment,0,,False
52,effort,0,,False
53,to,0,,False
54,achieve,0,,False
55,near-perfect,0,,False
56,"recall, if we assume that there were 14,273 relevant documents, in",0,,False
57,accordance with our ground-truth assumption.,0,,False
58,Over and above the absolute cost of obtaining relevance assess-,0,,False
59,ments is the problem of budgeting and resource allocation. It is,0,,False
60,not known in advance how many documents will be relevant to,1,ad,True
61,"each topic, and therefore how assessment resources should be ap-",0,,False
62,portioned or scheduled over topics. The variance of R in the Core,0,,False
63,"Track topics is huge, ranging from 9 to 1,377, with an average of",1,Track,True
64,285 and a mean of 191. Topics with large R consume an inordinate,0,,False
65,"fraction of the budget, as may topics with very small R, where it",0,,False
66,could be a challenge to find any relevant documents.,0,,False
67,6 DYNAMIC SAMPLING,0,,False
68,"Dynamic Sampling adapts AutoTAR to identify a sequence of strata with exponentially increasing size, which are sampled with diminishing frequency, so that the total number of documents assessed per topic is equal to a sampling budget that is fixed in advance. A sampling budget of precisely 600 assessments per topic, or 30,000 assessments in total, nearly equals the 30,030 assessments of the Core Track depth-100 subset pool.",1,ad,True
69,"The exponentially increasing strata are precisely the exponentially growing batches of documents identified by AutoTAR; however, a uniform random sample of each batch is selected for assessment, and used to train the learning method, which then identifies the next batch. The main idea is derived from Cormack and Grossman's ""S-CAL"" [4], which is itself derived from AutoTAR. While the purpose of S-CAL is to achieve the best possible classifier over an infinite population, with statistical estimation playing a supporting role, the purpose of Dynamic Sampling is to achieve the best possible statistical estimator, with classification playing a supporting role. In response to this difference in emphasis, we amended the procedure by which the sampling rate decays.",0,,False
70,Algorithm 1 Dynamic Sampling Algorithm (from S-CAL [4]).,0,,False
71,1: Construct a relevant pseudo-document from topic description.,0,,False
72,2: The initial training set contains only the pseudo-document.,0,,False
73,3: Set the initial batch size B to 1.,0,,False
74,4: Set the initial decay threshold T to hyper-parameter N .,0,,False
75,5: Temporarily augment the training set by adding 100 random,1,ad,True
76,"documents from the collection, labeled ""not relevant.""",0,,False
77,6: Score all documents using a model induced from training set.,0,,False
78,7: Remove the random documents added in step 5.,1,ad,True
79,8: Select the highest-scoring B documents not previously selected.,0,,False
80,"9: Draw n ,",0,,False
81,B ·N T,0,,False
82, B random documents from step 8.,0,,False
83,10: Render relevance assessments for the n documents.,0,,False
84,11: Add the assessed documents to the training set.,0,,False
85,12: Increase B by,0,,False
86,B 10,0,,False
87,.,0,,False
88,"13: If the number of assessed relevant documents R  T , double T .",0,,False
89,14: Repeat 5 through 13 until assessment budget A is reached.,0,,False
90,"The Dynamic Sampling method is outlined in Algorithm 1. The only input to the method, other than the document collection, topic statement, and assessment budget A, is a hyper-parameter N controlling the decay of the sampling rate. At the outset, and until at least N relevant documents are discovered, every document in",0,,False
91,"every batch presented by AutoTAR is presented for assessment, and used to train the model. In the event that N documents are never found, documents are examined exhaustively until the assessment budget is met. Once N relevant documents are found, the sampling rate is halved until N more relevant documents are found, and so on, until the assessment budget is reached.",0,,False
92,"N quantifies a tradeoff between the objectives of selecting the largest possible number of relevant documents for assessment, and sampling a universe containing the largest possible number of relevant documents. In the extreme case where N ,"" A, and also in the extreme case where R  N , DS is exactly AutoTAR. In the opposite extreme where N  R, the sampling frequency will nearly vanish, and the universe will approach the entire document population. We evaluated all twenty combinations of A  {100, 200, 300, 600} and N  {12, 25, 50, 100, 200}.""",1,ad,True
93,7 FEATURE ENGINEERING,0,,False
94,"In one version of DS, we represented each document as a tf-idf",0,,False
95,"word vector, exactly as calculated by BMI. In a second version,",0,,False
96,we represented each document as a reciprocal-rank vector with d,0,,False
97,"dimensions, corresponding to d runs used to form a depth-10000",0,,False
98,"pool. For the full pool consisting of all Core Track submissions,",1,Track,True
99,"d , 75; for the automatic pool consisting exclusively of automatic",0,,False
100,"runs, d",0,,False
101,",",0,,False
102,29.,0,,False
103,The value of each feature is set,0,,False
104,to,0,,False
105,1 d,0,,False
106,·,0,,False
107,1 50+,0,,False
108,",",0,,False
109,where,0,,False
110,is the rank of the document according to the corresponding run. In,0,,False
111,"Table 1, rows labeled ""DN "" denote results using tf-idf and hyper-",0,,False
112,"parameter N ; rows labeled ""RN "" denote results using rank features",0,,False
113,"from the full pool; and rows labeled ""AN "" denote results using the",0,,False
114,"automatic pool. Rows labeled ""RDN "" and ""ADN "" denote results",0,,False
115,"that average the scores from two separate models in Algorithm 1,",0,,False
116,step 6: one using tf-idf features; the other using rank features from,0,,False
117,"the full pool or automatic pool, respectively.",0,,False
118,8 RESULTS,0,,False
119,"The quality of a test collection may be characterized by how well it computes effectiveness measures for particular runs, or by how well it ranks the relative effectiveness of the various runs that it may be called upon to evaluate. For this short paper, we focus on how well a test collection ranks the 75 Core Track runs according to MAP, using Kendall's  to compare rankings. We have also computed AP [18], variance, and bias, which show the same effect. Table 1 shows  for three subsets of the Core Track pool, and the variants of our Dynamic Sampling method, sorted by the column labeled A , 600.",1,Track,True
120,"Core denotes the 30, 030-document pool identified by the Core Track. ""Core-Auto"" denotes a subset of Core containing assessments only for documents that are among depth-100 pool formed using only the 29 automatic rankings. Core-Auto contains 15,024 assessments--about 300 per topic--and is therefore included under A ,"" 300 in Table 1. """"Core-Sample"""" denotes a two-stratum sample, drawn by the Core Track organizers, of the depth-75 pool of the 55 runs used to form the Core Track pool. The first stratum consists of all documents in the top-10 pool, while the second stratum consists of a 20% sample of all documents in the top-75 pool, excluding the documents in the first stratum. Core-Sample contains 15,024 assessments--also about 300 per topic--and is included under A "", 300.",1,Track,True
121,1171,0,,False
122,Short Research Papers II,0,,False
123,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
124,Table 1: Kendall  correlation between ground truth and test collections built using pooling and dynamic sampling.,0,,False
125,Method,0,,False
126,AD50 RD25 R25 R50 R100 RD100 RD50 AD200 A25 RD200 AD100 Core R12 D50 RD12 D12 AD12 D100 AD25 Core-Sample D200 A100 D25 R200 A50 A12 A200 Core-Auto,0,,False
127,Assessment Budget Per Topic*,0,,False
128,"A,100",0,,False
129,"A,200",0,,False
130,"A,300",0,,False
131,"A,600",0,,False
132,.835,0,,False
133,.932,0,,False
134,.971,0,,False
135,.986,0,,False
136,.881,0,,False
137,.956,0,,False
138,.969,0,,False
139,.982,0,,False
140,.734,0,,False
141,.936,0,,False
142,.970,0,,False
143,.982,0,,False
144,.531,0,,False
145,.887,0,,False
146,.941,0,,False
147,.981,0,,False
148,.431,0,,False
149,.843,0,,False
150,.914,0,,False
151,.980,0,,False
152,.813,0,,False
153,.908,0,,False
154,.953,0,,False
155,.978,0,,False
156,.837,0,,False
157,.933,0,,False
158,.963,0,,False
159,.977,0,,False
160,.814,0,,False
161,.895,0,,False
162,.940,0,,False
163,.977,0,,False
164,.735,0,,False
165,.912,0,,False
166,.951,0,,False
167,.975,0,,False
168,.813,0,,False
169,.897,0,,False
170,.940,0,,False
171,.974,0,,False
172,.815,0,,False
173,.918,0,,False
174,.945,0,,False
175,.972,0,,False
176,.971,0,,False
177,.873,0,,False
178,.947,0,,False
179,.962,0,,False
180,.967,0,,False
181,.794,0,,False
182,.918,0,,False
183,.956,0,,False
184,.967,0,,False
185,.930,0,,False
186,.939,0,,False
187,.956,0,,False
188,.966,0,,False
189,.873,0,,False
190,.943,0,,False
191,.966,0,,False
192,.965,0,,False
193,.892,0,,False
194,.954,0,,False
195,.962,0,,False
196,.964,0,,False
197,.802,0,,False
198,.890,0,,False
199,.931,0,,False
200,.962,0,,False
201,.882,0,,False
202,.946,0,,False
203,.949,0,,False
204,.961,0,,False
205,.944,0,,False
206,.796,0,,False
207,.886,0,,False
208,.926,0,,False
209,.960,0,,False
210,.458,0,,False
211,.807,0,,False
212,.888,0,,False
213,.960,0,,False
214,.838,0,,False
215,.925,0,,False
216,.958,0,,False
217,.959,0,,False
218,.436,0,,False
219,.794,0,,False
220,.881,0,,False
221,.958,0,,False
222,.550,0,,False
223,.853,0,,False
224,.918,0,,False
225,.957,0,,False
226,.839,0,,False
227,.921,0,,False
228,.942,0,,False
229,.955,0,,False
230,.450,0,,False
231,.775,0,,False
232,.850,0,,False
233,.941,0,,False
234,.420,0,,False
235,Core Core-Sample Core-Auto RN AN DN RDN ADN N * ,0,,False
236,"Key Core Track assessment pool Core Track depth-75 stratified sample Core Track pool, automatic only Dynamic, all 75 rankings Dynamic, 29 automatic rankings only Dynamic, content Dynamic, all 75 rankings + content Dynamic, 29 automatic rankings + content denotes hyper-parameter N (see text) Fixed budget per topic unless indicated by  Variable budget per topic",1,Track,True
237,9 CONCLUSIONS,0,,False
238,"For assessment budget A ,"" 600, the Core Track collection is near the median of the various Dynamic Sampling methods. Remarkably, the top-scoring system for this budget is AD50, which uses only automatic rankings and document content--along with relevance feedback--to select documents for assessment. While we cannot say that the difference between AD50 and RD50 results is substantial, it does appear that RDN offers no material advantage over ADN .""",1,Track,True
239,"It also appears that ADN and RDN compare favorably with Core,",0,,False
240,"except for the smallest N ,"" 12. Of the single-model methods, RN""",0,,False
241,"fares the best, and offers an apparently superior plug-in replacement",0,,False
242,to the dynamic-pooling method used by the Core Track. The results,1,Track,True
243,"for DN appear to be slightly inferior to those for Core; however,",0,,False
244,"it should be noted that DN uses no pooling whatsoever, and is",0,,False
245,the only technique presented here that could be used to construct,0,,False
246,a test collection prior to the conduct of an evaluation task. AN,0,,False
247,"is remarkably good, considering the narrow set of rankings from",0,,False
248,"which it is derived, but inferior to the other methods.",0,,False
249,"For A ,"" 300, the results for ADN , RDN , and RN are nearly as""",0,,False
250,"good as for A ,"" 600, and better than Core-Sample. Perhaps un-""",0,,False
251,"surprisingly, the results for Core-Auto are abysmal, confirming",0,,False
252,received wisdom that the pooling method depends on the pres-,0,,False
253,ence of manual runs. The extreme difference between between,0,,False
254,Core-Auto and AD50 is noteworthy because both methods rely on,0,,False
255,"information from the same set of automatic runs, and use the same",0,,False
256,number of assessments.,0,,False
257,"The results for A , 100 and A , 200 show an overall degradation",1,ad,True
258,"and an increase in variability, but may still reflect useful methods,",0,,False
259,as they require one-sixth to one-third as many assessments as the,0,,False
260,Core Track effort. An open question remains: Would assessing all,1,Track,True
261,"250 topics with DS and a budget of A , 120 have yielded a better",0,,False
262,"test collection, for the same total effort of 30,000 assessments? We",0,,False
263,believe so.,0,,False
264,REFERENCES,0,,False
265,"[1] James Allan, Evangelos Kanoulas, Donna Harman, and Ellen Voorhees. TREC 2017 Common Core Track overview. In TREC 2017.",1,TREC,True
266,"[2] Anonymized. Participation in the TREC 2017 Core Track. In TREC 2017. [3] Ben Carterette, James Allan, and Ramesh Sitaraman. Minimal test collections for",1,TREC,True
267,retrieval evaluation. In SIGIR 2006. [4] Gordon V Cormack and Maura R Grossman. Scalability of continuous active,0,,False
268,learning for reliable high-recall text classification. In CIKM 2016. [5] Gordon V Cormack and Maura R Grossman. Autonomy and reliability of contin-,0,,False
269,"uous active learning for technology-assisted review. arXiv:1504.06868, 2015. [6] Gordon V. Cormack, Christopher R. Palmer, and Charles L. A. Clarke. Efficient",0,,False
270,"construction of large test collections. In SIGIR 1998. [7] Maura R Grossman, Gordon V Cormack, and Adam Roegiest. Automatic and",0,,False
271,"semi-automatic document selection for technology-assisted review. In SIGIR 2017. [8] Maura R Grossman, Gordon V Cormack, and Adam Roegiest. TREC 2016 Total Recall Track Overview. In TREC 2016. [9] David E Losada, Javier Parapar, and Álvaro Barreiro. Feeling lucky?: Multi-armed bandits for ordering judgements in pooling-based evaluation. In Proceedings of the 31st Annual ACM Symposium on Applied Computing, pages 1027­1034. ACM, 2016. [10] V Pavlu and J Aslam. A practical sampling strategy for efficient retrieval evaluation. College of Computer and Information Science, Northeastern University, 2007. [11] Virgil Pavlu. Large Scale IR Evaluation. Northeastern University, 2008. [12] Adam Roegiest, Gordon V Cormack, Maura R Grossman, and Charles L A Clarke. TREC 2015 Total Recall Track Overview. In TREC 2015. [13] Mark Sanderson et al. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4(4):247­375, 2010. [14] Mark Sanderson and Hideo Joho. Forming test collections with no system pooling. In SIGIR 2004. [15] Ian Soboroff and Stephen Robertson. Building a filtering test collection for TREC 2002. In SIGIR 2003. [16] Ellen M. Voorhees. Overview of the TREC 2004 Robust Track. In Proceedings of the 13th Text REtrieval Conference, Gaithersburg, Maryland, 2004. [17] Ellen M Voorhees. The TREC 2005 Robust Track. In ACM SIGIR Forum, volume 40. ACM, 2006. [18] Emine Yilmaz, Javed A Aslam, and Stephen Robertson. A new rank correlation coefficient for information retrieval. In SIGIR 2008. [19] Emine Yilmaz, Evangelos Kanoulas, and Javed A. Aslam. A simple and efficient sampling method for estimating AP and NDCG. In SIGIR 2008.",1,TREC,True
272,1172,0,,False
273,,0,,False

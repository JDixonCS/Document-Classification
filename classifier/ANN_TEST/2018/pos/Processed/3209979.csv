,sentence,label,data,regex
0,Session 1D: Learning to Rank I,1,Session,True
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks,0,,False
3,"Yue Feng, Jun Xu, Yanyan Lan, Jiafeng Guo, Wei Zeng, Xueqi Cheng",0,,False
4,"1 University of Chinese Academy of Sciences, Beijing, China 2 CAS Key Lab of Network Data Science and Technology,",1,ad,True
5,"Institute of Computing Technology, Chinese Academy of Sciences",1,ad,True
6,"{fengyue,zengwei}@software.ict.ac.cn,{junxu,lanyanyan,guojiafeng,cxq}@ict.ac.cn",0,,False
7,ABSTRACT,0,,False
8,"The goal of search result diversi cation is to select a subset of documents from the candidate set to satisfy as many di erent subtopics as possible. In general, it is a problem of subset selection and selecting an optimal subset of documents is NP-hard. Existing methods usually formalize the problem as ranking the documents with greedy sequential document selection. At each of the ranking position the document that can provide the largest amount of additional information is selected. It is obvious that the greedy selections inevitably produce suboptimal rankings. In this paper we propose to partially alleviate the problem with a Monte Carlo tree search (MCTS) enhanced Markov decision process (MDP), referred to as M2Div. In M2Div, the construction of diverse ranking is formalized as an MDP process where each action corresponds to selecting a document for one ranking position. Given an MDP state which consists of the query, selected documents, and candidates, a recurrent neural network is utilized to produce the policy function for guiding the document selection and the value function for predicting the whole ranking quality. The produced raw policy and value are then strengthened with MCTS through exploring the possible rankings at the subsequent positions, achieving a better search policy for decision-making. Experimental results based on the TREC benchmarks showed that M2Div can signi cantly outperform the state-of-the-art baselines based on greedy sequential document selection, indicating the e ectiveness of the exploratory decision-making mechanism in M2Div.",1,NP,True
9,KEYWORDS,0,,False
10,Diverse ranking; Markov decision process; Monte Carlo tree search,0,,False
11,"ACM Reference Format: Yue Feng, Jun Xu, Yanyan Lan, Jiafeng Guo, Wei Zeng, Xueqi Cheng . 2018. From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks. In SIGIR '18: The 41st International ACM SIGIR Conference on Research Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3209979",0,,False
12, Corresponding author: Jun Xu,0,,False
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3209979",1,ad,True
14,1 INTRODUCTION,1,DUC,True
15,"One important goal in many information retrieval tasks involves providing search results that covers a wide range of topics for a search query, called search result diversi cation [1]. The goal of search result diversi cation can be formalized as selecting a minimal subset of documents from the candidate set to cover as many di erent subtopics as possible. Since the novelty of a document depends on the other selected documents, selecting an optimal subset of documents amounts to the problem subset selection and its complexity is in general NP-hard.",1,NP,True
16,"Typical approaches treat search result diversi cation as ranking the documents based on their relevance as well as the novelty. Greedy sequential document selection has been widely adopted to construct the diverse document ranking, that is, the document ranking is constructed step by step. At each step the ranking model selects one document from the candidate set for the current ranking position. Usually, the document with the maximal amount of additional utility, a.k.a. marginal relevance, is selected.",1,ad,True
17,"A number of diverse ranking algorithms have been developed under the greedy document selection framework. Di erent algorithms utilize di erent criteria to estimate the additional utility a candidate document can provide. For example, the representative approach of maximal marginal relevance (MMR) [3] uses the sum of the query-document relevance and the maximal document distance (referred to as marginal relevance) as the utility. xQuAD [25] de nes the utility so as to explicitly account for the relationship between documents retrieved for the original query and the possible subqueries. In recent years, machine learning based methods have been proposed for conducting diverse ranking [23, 31, 32, 34, 36, 39]. The relational learning to rank (R-LTR) [39] and its variations [31, 32, 34] de ne the utilities based on the relevance features and the novelty features. MDP-DIV adapted the Markov decision process (MDP) to model the document ranking process. The utility of a document is estimated based on the MDP state, which consists of the query, the preceding documents, and the remaining candidates [33].",1,ad,True
18,"The greedy sequential document selection simpli es the ranking process and can accelerate the online ranking. However, the rankings produced by greedy document selection are inevitably suboptimal. At each ranking position, the greedy selection mechanism only considers the possibilities at the current ranking position (i.e., estimates the utility of each candidate document if it were selected). Thus, greedy document selection will select the locally optimal document at each ranking position. However, a sequence of the locally optimal documents cannot lead to the globally optimal diverse ranking, because the utilities of the documents are not independent. The selection of a document at one position will change",1,ad,True
19,125,0,,False
20,Session 1D: Learning to Rank I,1,Session,True
21,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
22,"the utilities of the remaining candidate documents and thereafter a ects the subsequent decisions. In general the ranking algorithm need to explore the whole ranking space if the optimal ranking is mandatory. However, this is usually infeasible in real applications because of the huge space size: there exist N ! di erent rankings for N documents.",0,,False
23,"Inspired by the success and methodology of the AlphaGo [27] and AlphaGo Zero [28] for the Game of Go, in this paper we propose to enhance the MDP model for diverse ranking [33] with the Monte Carlo tree search (MCTS), for alleviating the suboptimal ranking problem. The new ranking model, referred to as M2Div (stands for MCTS enhanced MDP for Diverse ranking), makes use of an MDP to model the sequential document selection process of diverse ranking. At each time step (corresponding to a ranking position), based on the user query and the preceding document ranking, a recurrent neural network (RNN) is used to produce the policy (a distribution over the candidate documents) for guiding the document selection and the value for estimating the whole document ranking quality (e.g., in terms of -NDCG@M). To alleviate the problem of suboptimal diverse ranking, in stead of greedily selecting a document with the predicted raw policy, M2Div conducts an exploratory decision making: an MCTS is conducted to explore the possible document rankings at the subsequent positions, resulting a strengthened search policy for conducting the real document selection at current position. Since it has explored more future possible document rankings, the search policy has higher probability to select a globally optimal document than the predicted raw policy. Moving to the next iteration, the above process is continued until the candidate set is empty.",1,ad,True
24,"Reinforcement learning is used to train the model parameters. In the training phase, at each training iteration and for each training query, an MCTS guided by the current policy function and value function is conducted at each ranking position. The MCTS produces a search policy for the document selection. Then the model parameters are adjusted to minimize the loss function. The loss function consists of two terms: 1) the squared error between the predicted value and the nal quality of the whole document ranking in terms of -NDCG@M; and 2) the cross entropy of the predicted raw policy and the search policy for document selection. Stochastic gradient descent is utilized for conducting the optimization.",1,ad,True
25,"To evaluate the e ectiveness of M2Div, we conducted experiments on the basis of TREC benchmark datasets. The experimental results showed that M2Div can signi cantly outperform the stateof-the-art diverse ranking approaches that using greedy sequential decision making, including the heuristic based diverse ranking methods of MMR and xQuAD, and the machine learning based diverse ranking methods of PAMM and MDP-DIV. We analyzed the results and showed that the exploratory decision-making mechanism in M2Div does help to improve the ranking performances.",1,TREC,True
26,2 RELATED WORK,0,,False
27,2.1 Search result diversi cation,0,,False
28,It is a common practice to formalize the construction of a diverse ranking list in search as a process of greedy sequential decision making. Existing research focus on designing e ective criteria to,0,,False
29,"estimate the utility a document can provide. Carbonell and Goldstein [3] proposed the maximal marginal relevance criterion, which is a linear combination of the query-document relevance and the document novelty, to select the document. xQuAD [24] directly models di erent aspects of a query and estimates the utility as the relevance of the retrieved documents to each identi ed aspects. Hu et al. [13] proposed a utility function that explicitly leverages the hierarchical intents of queries and selects the documents that maximize diversity in the hierarchical structure. See also [2, 4, 7, 9­ 11, 22, 29]",0,,False
30,"Machine learning techniques have been applied to construct diverse ranking, also adopting the greedy sequential decision making as the basic framework. The key problem becomes how to automatically learn the utility function on the basis of training queries. Some researchers de ne the utility as a linear combination of the handcrafted relevance features and novelty features [23, 31, 34, 39]. The novelty term in the utility function can be modeled with the deep learning model of neural tensor networks [32]. Jiang et al. used recurrent neural networks and max-pooling to model subtopic information explicitly with the attention mechanism [14, 15]. Xia et al. [33] proposed to model the dynamics of the document utility with MDP and learning the model parameters with policy gradient. Other learning approaches please refer to [18, 21, 23, 34, 35, 37].",1,ad,True
31,2.2 Reinforcement learning for IR,0,,False
32,"The reinforcement learning has been widely used in variant IR applications. For example, in [20], a win-win search framework based on partially observed Markov decision process (POMDP) is proposed to model session search as a dual-agent stochastic game. In the model, the state of the search users are encoded as a four hidden decision making states. In [38], the log-based document re-ranking is modeled as a POMDP. [33] and [30] propose to model the process of constructing a document ranking with MDP, for the ranking tasks of search result diversi cation and relevance ranking, respectively. Muti-armed bandit, another type of reinforcement learning model, is also widely applied to rank learning. For example, [23] proposes two online learning bandit algorithms to learn a diverse ranking of documents based on users clicking behaviors. [37] formalizes the interactively optimizing of information retrieval systems as a dueling bandit problem and [16] proposes cascading bandits to identify K most attractive document for users. See also [12]",1,ad,True
33,"Reinforcement learning models are also used for building recommender systems. For example, [26] designs an MDP-based recommendation model for taking both the long-term e ects of each recommendation and the expected value of each recommendation into account. Lu and Yang [19] proposes POMDP-Rec, a neuraloptimized POMDP algorithm, for building a collaborative ltering recommender system.",0,,False
34,"In this paper, we also adopt the reinforcement learning model of MDP to formalize the diverse ranking process in search result diversi cation.",1,ad,True
35,3 MDP AND MCTS,0,,False
36,We employ Markov decision process and Monte Carlo tree search to model diverse ranking process.,0,,False
37,126,0,,False
38,Session 1D: Learning to Rank I,1,Session,True
39,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
40,3.1 Markov decision process,0,,False
41,MDP provides a mathematical framework for modeling the sequential decision making process with the agent-environment interface. The key components of an MDP include:,0,,False
42,"States S is a set of states. For instance, in this paper we de ne the state as a tuple consists of the query, the preceding document ranking, and the candidate documents.",0,,False
43,"Actions A is a discrete set of actions that an agent can take. The actions available may depend on the state s, denoted as A(s).",0,,False
44,"Policy p describes the behaviors of an agent, which is a probabilistic distribution over the possible actions. p is usually optimized to maximize the long term return.",0,,False
45,"Transition T is the state transition function st+1 ,"" T (st , at ) which speci es a function that maps a state st into a new state st+1 in response to the action selected at .""",0,,False
46,"Value: state value function V : S  R is a function that predicts the long term return of the whole episode, on the basis of the current state s under the policy p.",0,,False
47,"An MDP model is running as follows: the agent and environment interact at each of a sequence of discrete time steps, t ,"" 0, 1, 2, · · · . At each time step t the agent receives some representation of the environment's state, st  S, and on that basis selects an action at  A(st ). One time step later, in part as a consequence of its action, the agent nds itself in a new state st+1 "","" T (st , at ). Usually the goal of reinforcement learning is to achieve maximum long term return, that is, to maximize the value V (s0).""",0,,False
48,3.2 Monte Carlo tree search,0,,False
49,"The decisions made by an MDP (e.g., selecting the most con dent actions according to the policy) may get suboptimal results. Theoretically the system has to explore the whole space of decision sequences to get a global optimal result. However, this is usually not feasible. MCTS is a tool to conduct heuristic search inside the whole space, more likely to produce a better decision sequence than that of produced by the greedy decisions.",1,ad,True
50,"Given the time step t, the policy function p, and the state value function V , MCTS aims at searching a strengthened policy for making better decisions. The MCTS consists of four phases: selection, expansion, simulation, and back-propagation:",0,,False
51,"Selection: Starting at the root node R, MCTS recursively selects the optimal child nodes until a leaf node L is reached.",0,,False
52,Expansion: If L is not a terminal node (i.e. it does not end the episode) then create one or more child nodes for L (each corresponds a possible action) and select one child node C according to the predicted policy.,0,,False
53,"Simulation/Evaluation: MCTS runs a simulation from C until a result is achieved. In the AlphaGo Zero [28] the simulation is replaced with the value function for accelerating the tree search. That is, the result of simulation is estimated by the value function.",0,,False
54,Back-propagation: Update the statistics stored in the current move sequence with the simulation or estimated result.,0,,False
55,"The MCTS outputs a search policy  over the actions, which is utilized to choose the action at time step t. The iteration is continued until the whole episode is generated.",0,,False
56,Policy-Value network LSTM,0,,False
57,state '%,0,,False
58,"*($|'%) ""('%)",0,,False
59,raw policy !,0,,False
60,"value function """,0,,False
61,MCTS search policy #,0,,False
62,'%() Environment,0,,False
63,action $%~#,0,,False
64,Figure 1: The agent-environment interaction of M2Div.,0,,False
65,4 DIVERSE RANKING WITH POLICY-VALUE,0,,False
66,NETWORKS,0,,False
67,"In this section, we introduce the proposed M2Div model, which makes use of MDP and MCTS for modeling the diverse ranking process and for strengthening the policy for selecting documents at each of the MDP iteration, respectively. The agent-environment interaction of M2Div is illustrated in Figure 1. Each of the MDP time step corresponds to a ranking position. At time step t (t ,"" 0, 1, · · · ), the policy-value network receives the environment state st and makes use of an LSTM to produce the representation of the state st . After that, guided by the current policy function p and value function V , an MCTS search is executed. The output of MCTS is a strengthened new search policy  . The action at is then selected according to the strengthened policy  , which chooses a document from the candidate set and places it to the ranking position t + 1. Moving to the next time step t + 1, the system nds itself in a new state st+1 and the process is repeated until all of the documents are ranked.""",0,,False
68,4.1 MDP formulation of diverse ranking,0,,False
69,"Suppose we are given a query q, which is associated with a set of retrieved documents X ,"" {x1, · · · , xM }  X, where both the query q and the documents xi are represented with their preliminary representations, i.e., the vectors learned by the doc2vec model [17], and X is the set of all possible documents. The goal of diverse ranking is to construct a model that can rank the documents so that the top ranked documents cover a wide range of subtopics of a search query.""",0,,False
70,"The construction of a diverse ranking can be considered as a process of sequential decision making with an MDP in which each time step corresponds to a ranking position. The states, actions, transition function, value function, and policy function of the MDP are set as:",0,,False
71,"States S: We design the state at time step t as a triple st ,"" [q, Zt , Xt ], where q is the preliminary representation of the user issued query; Zt "", {x(n)}nt ,""1 is the sequence of t preceding documents, where x(n) is the document ranked at position n; Xt is the set of candidate documents. At the beginning (t "","" 0), the state is initialized as s0 "","" [q, , X ], where  is the empty sequence and X contains all of the M retrieved candidate documents.""",0,,False
72,127,0,,False
73,Session 1D: Learning to Rank I,1,Session,True
74,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
75,"Actions A: At each time step t, the A(st ) is the set of actions the agent can choose, each corresponds to a document from Xt . That is, the action at  A(st ) at the time step t selects a document xm(at )  Xt for the ranking position t + 1, where m(at ) is the index of the document selected by at .",0,,False
76,Transition T : The transition function T : S × A  S is de ned as follows:,0,,False
77,"st +1 ,"" T (st , at ) "","" T ([q, Zt , Xt ], at ) (1)""",0,,False
78,","" q, Zt  {xm(at )}, Xt \ {xm(at )} ,""",0,,False
79,"where  appends xm(at ) to Zt and \ removes xm(at ) from Xt . At each time step t, based on state st the system chooses an action at . Then, the system moves to time step t + 1 and the system transits to a new state st+1: rst, the query q is kept unchanged; second, the selected document is appended to the end of Zt , generating a new document sequence; nally, the selected document at step t is removed from the candidate set: Xt +1 ,"" Xt \ {xm(at )}. Thus, the number of actions the agent can choose at t + 1 is reduced by one.""",0,,False
80,"Value function V : The state value function V : S  R is a scalar evaluation, estimating the quality of the whole document ranking (an episode) based on the input state. The value function is learned so as to approximate a prede ned evaluation measure (e.g., -NDCG@M).",0,,False
81,"In this paper, we make use of the long short term memory (LSTM) to summarize the input state s as a real vector, and then de ne the value function as nonlinear transformation of the weighted sum of the LSTM outputs:",0,,False
82,"V (s) ,""  ( w, LSTM(s) + b ) ,""",0,,False
83,(2),0,,False
84,where w and b are the weight vector and the bias to be learned,0,,False
85,"during training, and  (x) ,",0,,False
86,1 1+e -x,0,,False
87,is the nonlinear sigmoid func-,0,,False
88,tion. The deep neural network model LSTM: S  RL maps a,0,,False
89,state to a real vector where L is the number of dimensions. Given,0,,False
90,"s ,"" [q, Z "","" {x1, x2, · · · , xt }, Xt ], where xk (k "","" 1, · · · , t) is the document ranked at k-th position and represented with its doc2vec""",0,,False
91,embedding. LSTM outputs a representation hk for position k as:,0,,False
92,"fk ,"" (Wf xk + Uf hk-1 + bf ), ik "","" (Wi xk + Ui hk-1 + bi ), ok "","" (Wo xk + Uo hk-1 + bo ), ck "",""fk  ck-1 + ik  tanh(Wc xk + Uc hk-1 + bc ), hk "",""ok  tanh(ck ),""",0,,False
93,where h and c are initialized with the query q:,0,,False
94,"[h0, c0] ,"" [ (Vh q),  (Vc q)],""",0,,False
95,"operator """" denotes the element-wise product and "" "" is applied to each of the entries; the variables fk , ik , ok , ck and hk denote the forget gate's activation vector, input gate's activation vector, output gate's activation vector, cell state vector, and output vector of the LSTM block, respectively. Wf , Wi , Wo, Uf , Ui , Uo, Vh, Vc , bf , bi , bo are weight matrices and bias vectors need to be learned during training. The output vector and cell state vector at the t-th cell are concatenated as the output of LSTM:",0,,False
96,"LSTM(s) ,",0,,False
97,"ht T , ct T",0,,False
98,T,0,,False
99,.,0,,False
100,(3),0,,False
101,"Policy function p: The policy p(s) de nes a function that takes the state as input and output a distribution over all of the possible actions a  A(s). Speci cally, each probability in the distribution is a normalized soft-max function whose input is the bilinear product of the LSTM de ned in Equation (3) and the selected document:",0,,False
102,"p(a|s) ,",0,,False
103,exp xTm(a)Up LSTM(s),0,,False
104,",",0,,False
105,a A(s) exp xTm(a )Up LSTM(s),0,,False
106,"where Up is the parameter in the bilinear product. Thus, the policy function p(s) is:",0,,False
107,"p(s) ,"" p(a1|s), · · · , p(a |A(s)| |s) .""",0,,False
108,(4),0,,False
109,4.2 Strengthening policy with MCTS,0,,False
110,Selecting documents greedily with the predicted raw policy p in,0,,False
111,Equation (4) may lead to suboptimal rankings because the policy,1,ad,True
112,only summarizes the historical information and has no idea on,0,,False
113,how the action at will in uent the future decisions. Let's use an example to show the limitation of greedy selection. Suppose that the,0,,False
114,"query q has 6 subtopics, and the 3 retrieved candidate documents d1, d2, and d3 cover the subtopics of {1, 2, 3, 6}, {1, 2, 5}, and {1, 2, 6}, respectively. The greedy selection with raw policy prefers d1 for the rst position, as it covers the most number of subtopics. Thus,",0,,False
115,the document ranking constructed with the greedy policy could be,0,,False
116,"[d1, d2, d3]",0,,False
117,and,0,,False
118,S-recall@2,0,,False
119,",",0,,False
120,5 6,0,,False
121,.,0,,False
122,"However,",0,,False
123,if,0,,False
124,the,0,,False
125,ranking,0,,False
126,algorithm,0,,False
127,"could explore (part of) the whole ranking space, it will found a",0,,False
128,"better document ranking in terms of S-recall@2: [d2, d3, d1] with S-recall@2 , 1. The example clearly indicate that greedy selection",0,,False
129,could lead to suboptimal rankings.,1,ad,True
130,"To alleviate the issue, following the practices in AlphaGo [27]",0,,False
131,"and AlphaGo Zero [28], we propose to conduct lookahead searches",1,ad,True
132,"in the ranking space with MCTS. Speci cally, at each ranking posi-",0,,False
133,"tion t, an MCTS search is executed, guided by the current policy",0,,False
134,"function p and value function V , and output a strengthened new",0,,False
135,search policy  . It is believed that the search policy  will select a,0,,False
136,much better action (document) for the ranking position than that,0,,False
137,of selected by the raw policy p in Equation (4). This is because the,0,,False
138,lookahead MCTS tries to explore the whole ranking space and can,1,ad,True
139,partially alleviate the suboptimal ranking problem.,0,,False
140,Figure 2 illustrates the MCTS process and Algorithm 1 shows,0,,False
141,the details. Each node of the tree corresponds to an MDP state. The,0,,False
142,"tree search algorithm takes a root node sR , number of search times K, value function V , policy function p, human labels , and the",0,,False
143,evaluation measure R as inputs. Note that and R are only used,0,,False
144,at the training time. The algorithm iterate K times and outputs,0,,False
145,a strengthened search policy  for selecting a document for root,0,,False
146,"node sR . Suppose that each edge e(s, a) (the edge from state s to the state T (s, a)) of the MCTS tree stores an action value Q(s, a), visit",0,,False
147,"count N (s, a), and prior probability P(s, a). The raw policy p(sR ) at the root state sR is strengthened with the following steps:",0,,False
148,Selection (line 3 to line 7 of Algorithm 1): Each of the K it-,0,,False
149,"erations starts from the root state sR and iteratively selects the documents that maximize an upper con dence bound. Speci cally,",0,,False
150,"at each time step t of each simulation, an action at is selected from",0,,False
151,128,0,,False
152,Session 1D: Learning to Rank I,1,Session,True
153,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
154,Selection,0,,False
155,Repeat K times,0,,False
156,Evaluation,0,,False
157,Expansion,0,,False
158,Back-propagation,0,,False
159,"!, # , & ' ,"" {)*, )+, ), ... }""",0,,False
160,"!, # , & ' ,"" {)*, )+, ), ... }""",0,,False
161,/+1,0,,False
162,2+3 max,0,,False
163,"!, # , {)*} ' ,"" {)+, ), ... }""",0,,False
164,"!, # , {)+} ' ,"" {)*, ),, ... }""",0,,False
165,"!, # , {)*} ' ,"" {)+, ), ... }""",0,,False
166,"/+1 !, #, '",0,,False
167,/+1 2+3,0,,False
168,/+1,0,,False
169,max,0,,False
170,"!, #, ' !, #, ' !, #, '",0,,False
171,"!, #, ' !, #, '",0,,False
172,"!, # , {)+} ' ,"" {)*, ),, ... } !, #, ' !, #, '""",0,,False
173,"4 , 5(7)",0,,False
174,"!, # , & ' ,"" {)*, )+, ), ... }""",0,,False
175,"!, # , & ' ,"" {)*, )+, ), ... }""",0,,False
176,4,0,,False
177,"2,",0,,False
178,2 :+4 :+>,0,,False
179,"!, # , {)*} ' ,"" {)+, ), ... }""",0,,False
180,"!, # , {)+} ' ,"" {)*, ),, ... }""",0,,False
181,"!, # , {)*} ' ,"" {)+, ), ... }""",0,,False
182,"!, # , {)+} ' ,"" {)*, ),, ... }""",0,,False
183,"!, #, ' !, #, ' !, #, ' !, #, ' !, #, '",0,,False
184,"2,9 :,9 ; , <(,>|7)",0,,False
185,"2,9 :,9 ; , <(,@|7)",0,,False
186,"!, #, ' !, #, '",0,,False
187,2 :+4,0,,False
188,"2, :+>",0,,False
189,4,0,,False
190,"!, #, ' !, #, '",0,,False
191,"!, #, '",0,,False
192,"!, #, ' !, #, '",0,,False
193,Figure 2: The MCTS process for calculating the strengthened search policy  .,0,,False
194,state st so as to maximize action value plus a bonus:,0,,False
195,at,0,,False
196,",",0,,False
197,arg,0,,False
198,max(Q,0,,False
199,a,0,,False
200,(st,0,,False
201,",",0,,False
202,a),0,,False
203,+,0,,False
204,U,0,,False
205,(st,0,,False
206,",",0,,False
207,"a)),",0,,False
208,(5),0,,False
209,"where   0 is the tradeo coe cient, and the bonus U (st , a) is de ned as",1,ad,True
210,"U (st , a) , p(a|st )",0,,False
211,"a A(st ) N (st , a ) ,",0,,False
212,"1 + N (st , a)",0,,False
213,"where p(a|st ) is the predicted probability by the policy function p(st ), A(st ) is the set of available actions (documents) at state st . U (st , a) is proportional to the prior probability but decays with repeated visits to encourage exploration.",0,,False
214,Evaluation and expansion (line 8 to line 19 of Algorithm 1):,0,,False
215,"When the traversal reaches a leaf node sL, the node is evaluated either with the value function V (sL) or with the prede ned performance measure if the node is the end of an episode and the",0,,False
216,"human labels are available. Speci cally, at the test phase or online",0,,False
217,"ranking phase, there is no human label information available. sL will be evaluated with the value function V (sL). At the training phase, if sL is not the end of the episode, V (sL) is used to conduct the evaluation. If sL is the last node of the episode (cannot be expanded), it is evaluated with the true performance measure (e.g.,",0,,False
218,-NDCG@M) at the position. Line 10 and line 18 of Algorithm 1,0,,False
219,shows the evaluation details. Please note that following the practice,0,,False
220,"in AlphaGo Zero program, we use the value function instead of",1,ad,True
221,rollouts for evaluating a node.,0,,False
222,"Then, the leaf node sL may be expanded (line 11 to line 16 of Algorithm 1). Each edge from the leaf position sL (corresponds to each action a  A(sL)) is initialized as: P(sL, a) ,"" p(a|sL) (Equation (4)), Q(sL, a) "","" 0, and N (sL, a) "", 0. In this paper all of the available actions of sL are expanded.",0,,False
223,Back-propagation and update (line 20 to line 28 of Algo-,0,,False
224,"rithm 1): At the end of evaluation, the action values and visit counts",0,,False
225,"of all traversed edges are updated. For each edge e(s, a), the prior",0,,False
226,"probability P(s, a) is kept unchanged, and Q(s, a) and N (s, a) are",0,,False
227,updated:,0,,False
228,"Q(s, a) ",0,,False
229,Q,0,,False
230,"(s,",0,,False
231,a,0,,False
232,")×N (s,a)+V N (s,a)+1",0,,False
233,(s,0,,False
234,L,0,,False
235,),0,,False
236,",",0,,False
237,(6),0,,False
238,"N (s, a)  N (s, a) + 1.",0,,False
239,"Calculate the strengthened search policy (line 29 to line 32 of Algorithm 1): After iterating K times, the strengthened search policy  for the root node sR can be calculated according to the visit counts N (sR , a) of the edges starting from sR :",0,,False
240," (a|sR ) ,",0,,False
241,"N (sR , a)",0,,False
242,",",0,,False
243,"a A(sR ) N (sR , a )",0,,False
244,(7),0,,False
245,for all a  A(sR ).,0,,False
246,4.3 Training with reinforcement learning,0,,False
247,"The model has parameters  ,"" {Wf , Wi , Wo, Uf , Ui , Uo, Up , bf , bi , bo, Vh, Vc , w} to learn. In the training phase, suppose we are given N labeled training queries {(q(n), X (n), (n))}nN"",""1, where (n) denotes the human labels on the documents, in the form of a binary matrix. (n)(i, j) "", 1 if document xi(n) contains the j-th subtopic of q(n) and 0 otherwise.",0,,False
248,"Algorithm (2) shows the training procedure. First, the parameters  is initialized to random weights in [-1, 1]. At each subsequent iteration, for each query, a ranking of documents is generated with current parameter setting. At each ranking position t, an MCTS search is executed, using previous iteration of value function and policy function, and a document xm(at ) is selected according to the search probabilities t .",0,,False
249,"The ranking terminates when the candidate is empty or the ranking exceeds the maximum length de ned by the prede ned evaluation measure R. These sampled documents consist a permutation of documents  , which is then evaluated with the evaluation measure to give a ground-truth return:",0,,False
250,"r ,"" R( , ).""",0,,False
251,129,0,,False
252,Session 1D: Learning to Rank I,1,Session,True
253,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
254,Algorithm 1 TreeSearch,0,,False
255,"Input: root state sR , value function V , and policy function p, number of search times K, trade-o parameter , human labels ,",1,ad,True
256,and performance evaluation function R,0,,False
257,Output: Tree search probabilities ,0,,False
258,"1: for k , 0 to K - 1 do",0,,False
259,2: sL  sR 3: {Selection},0,,False
260,4: while sL is not a leaf node do,0,,False
261,5:,0,,False
262,"a  arg maxa A(sL) Q(sL, a) +  · U (sL, a){Equation (5),",0,,False
263,"using the P, Q, and N stored in the corresponding edges}",0,,False
264,6:,0,,False
265,"sL  child node pointed by edge (sL, a)",0,,False
266,7: end while,0,,False
267,8: {Evaluation and expansion},0,,False
268,9: if sL can be expanded then,0,,False
269,10:,0,,False
270, V (sL) {simulate with value function V },0,,False
271,11:,0,,False
272,for all a  A(sL) do,0,,False
273,12:,0,,False
274,Expand a new edge e which connects to a new node,0,,False
275,"s ,"" [q, sL .Z  {xm(a)}, sL .X \ {xm(a)}]""",0,,False
276,13:,0,,False
277,e.P  p(a|sL) {initialize the prior probability},0,,False
278,14:,0,,False
279,e.Q  0,0,,False
280,15:,0,,False
281,e.N  0,0,,False
282,16:,0,,False
283,end for,0,,False
284,17: else,0,,False
285,18:,0,,False
286,"V (sL) R(sL .Z, )",0,,False
287,"R , NULL or otherwise",0,,False
288,",  {Case 1: predict",0,,False
289,"with V at test phase (R ,NULL or , ); Case 2: directly",0,,False
290,"set to the ground truth (e.g., -NDCG@M) at the training",0,,False
291,phase},0,,False
292,19: end if,0,,False
293,20: {Back-propagation},0,,False
294,21: while sL sR do,0,,False
295,22:,0,,False
296,s  parent of sL,0,,False
297,23:,0,,False
298,e  edge from s to sL,0,,False
299,24:,0,,False
300,e .Q,0,,False
301,e .Q ×e .N + e .N +1,0,,False
302,{Equation (6)},0,,False
303,25:,0,,False
304,e.N  e.N + 1,0,,False
305,26:,0,,False
306,sL  s,0,,False
307,27: end while,0,,False
308,28: end for,0,,False
309,29: {calculate tree search probabilities},0,,False
310,30: for all a  A(sR ) do,0,,False
311,31:,0,,False
312, (a|sR ) ,0,,False
313,a,0,,False
314,"e (sR , a ). N A(sR ) e (sR, a",0,,False
315,).N,0,,False
316,"{e(s, a)",0,,False
317,is,0,,False
318,the,0,,False
319,edge,0,,False
320,from,0,,False
321,s,0,,False
322,to,0,,False
323,the state by taking action a},0,,False
324,32: end for,0,,False
325,33: return ,0,,False
326,"Here R can be any diverse ranking evaluation measure such as NDCG@M etc. The data generated at each time step E ,"" {(st , t )}Tt"",1 and the nal evaluation r are utilized as the ground-truth signals",0,,False
327,in training for adjusting the value function. The model parameters,1,ad,True
328,are adjusted to minimize the error between the predicted value,1,ad,True
329,"V (st ) and the evaluation of the ranking in terms of the chosen evaluation measure, and to maximize the similarity of the raw policy",0,,False
330,"p(st ) to the search policy t . Speci cally, the parameters  are adjusted by gradient descent on a loss function that sums over",1,ad,True
331,Algorithm 2 Train diverse ranking model,0,,False
332,"Input: Labeled training set D ,"" {(q(n), X (n), (n))}nN"",""1, learning rate , number of search steps K, MCTS trade-o parameter ,""",1,ad,True
333,and evaluation measure R,0,,False
334,Output: ,0,,False
335,"1: Initialize   random values in [-1, 1]",0,,False
336,2: repeat,0,,False
337,"3: for all (q, X , )  D do",0,,False
338,4:,0,,False
339,"s  [q, , X ]",0,,False
340,5:,0,,False
341,M  |X |,0,,False
342,6:,0,,False
343,"E , (){empty episode}",0,,False
344,7:,0,,False
345,"for t , 0 to M - 1 do",0,,False
346,8:,0,,False
347,"  TreeSearch(s, V , p, K, , , R) {Algorithm (1): tree",0,,False
348,"search using s as root, with current }",0,,False
349,9:,0,,False
350,"a , arg maxa A(s)  (a|s) {select the best document}",0,,False
351,10:,0,,False
352, (t + 1)  m(a){document xm(a) is ranked at t + 1},0,,False
353,11:,0,,False
354,"E  E  {(s,  )}",0,,False
355,12:,0,,False
356,"s  [q, s.Z  {xm(a)}, s.X \ {xm(a)}]",0,,False
357,13:,0,,False
358,end for,0,,False
359,"14: r  R( , ){evaluating the generalized ranking}",0,,False
360,15:,0,,False
361, -,0,,False
362,"(E,r ) ",0,,False
363,{Update,0,,False
364,parameters.,0,,False
365,is de ned in,0,,False
366,Equation (8)},0,,False
367,16: end for,0,,False
368,17: until converge,0,,False
369,18: return ,0,,False
370,"the mean-squared error and cross-entropy losses, respectively:",0,,False
371,|E |,0,,False
372,"(E, r ) ,",0,,False
373,"t ,1",0,,False
374,(V,0,,False
375,(st,0,,False
376,),0,,False
377,-,0,,False
378,r,0,,False
379,)2,0,,False
380,+,0,,False
381,a,0,,False
382,A(st,0,,False
383,),0,,False
384,t,0,,False
385,(a,0,,False
386,|st,0,,False
387,),0,,False
388,log,0,,False
389,1 p (a |st,0,,False
390,),0,,False
391,.,0,,False
392,(8),0,,False
393,Figure 3 illustrates the construction of the loss given a training,0,,False
394,query. The model parameters are trained by back propagation and,0,,False
395,"stochastic gradient descent. Speci cally, we use AdaGrad [8] on all",1,ad,True
396,parameters in the training process.,0,,False
397,"Please note that following the practices in [28], the search tree",0,,False
398,constructed at the t-th iteration (line 8 to line 12 of Algorithm 2),0,,False
399,is reused at subsequent steps: the child node corresponding to the,0,,False
400,selected document (chosen action) becomes the new root node; the,0,,False
401,"subtree below this child is retained along with all its statistics, while",0,,False
402,the remainder of the tree is discarded.,0,,False
403,4.4 Online ranking,0,,False
404,"The construction of a diverse ranking for an online query is shown in Algorithm 3. Given a user query q, a set of M retrieved documents X , the system state is initialized as s0 ,"" [q, Z0 "","" , X0 "","" X ]. Then, at each of the time steps t "","" 0, · · · , M - 1, the agent receives the state st "","" [q, Zt , Xt ] and searches the policy  with MCTS, on the basis of the value function V and policy function p. Then, it chooses an action a according  , selects the document xm(a) from the candidate set, and places it to the rank t + 1. Moving to the next step t + 1, the state becomes st +1 "","" [q, Zt +1, Xt +1]. The process is repeated until the candidate set becomes empty.""",0,,False
405,"Considering that the MCTS is time consuming and may be infeasible in some online ranking tasks, the online ranking algorithm can also skip the tree search and directly use the raw policy for",0,,False
406,130,0,,False
407,Session 1D: Learning to Rank I,1,Session,True
408,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
409,"/, 1* , 3",0,,False
410,"5""~!""",0,,False
411,"/, 1"" ,"" {5""""}""",0,,False
412,"5#~!# /, 1# ,"" {5"""", 5#} 5?~!? /, 1? "","" {5"""", 5# ... }""",0,,False
413,"@* ,"" {5"""", 5#, 5$ ... }""",0,,False
414,"@"" ,"" {5#, 5$, 5B ... }""",0,,False
415,"@# ,"" {5$, 5B, 5C ... }""",0,,False
416,"@? , {... }",0,,False
417,"!"" /, 1* , 3",0,,False
418,LSTM,0,,False
419,%('|)*) V()*),0,,False
420,"%""",0,,False
421,"-""",0,,False
422,"!""",0,,False
423,7,0,,False
424,"!# /, 1"" ,"" {5""""}""",0,,False
425,LSTM,0,,False
426,"%('|)"") V()"")",0,,False
427,%#,0,,False
428,-#,0,,False
429,!#,0,,False
430,7,0,,False
431,"!$ /, 1# ,"" {5"""", 5#}""",0,,False
432,LSTM,0,,False
433,%('|)#) V()#),0,,False
434,%$,0,,False
435,-$,0,,False
436,!$,0,,False
437,7,0,,False
438,"7 , 9NDCG",0,,False
439,Figure 3: Construction of loss function for a training query.,0,,False
440,Algorithm 3 Diverse ranking with Monte Carlo tree search,0,,False
441,"Input: query q, documents X ,"" {x1, · · · , xM }, number of search""",0,,False
442,"steps K, MCTS trade-o parameter , value function V , and",1,ad,True
443,policy function p,0,,False
444,Output: Permutation of documents ,0,,False
445,"1: s  [q, , X ]",0,,False
446,2: M  |X |,0,,False
447,"3: for t , 0 to M - 1 do",0,,False
448,4:,0,,False
449,"TreeSearch(s, V , p, K, , , NULL) with MCTS,",0,,False
450,p(s),0,,False
451,without MCTS,0,,False
452,{Case 1: ranking with search policy. No human labels and,0,,False
453,evaluation function are available at the test/online ranking,0,,False
454,phase; Case 2: ranking with raw policy for accelerating,0,,False
455,online ranking speeds.},0,,False
456,"5: a  arg maxa A(s)  (a|s) 6:  (t + 1)  m(a){document xm(a) is ranked at t + 1} 7: [q, Z, X ]  s",0,,False
457,"8: s  [q, Z  {xm(a)}, X \ {xm(a)}]",0,,False
458,9: end for,0,,False
459,10: return ,0,,False
460,"ranking, denoted with ""without MCTS"" in the line 4 of Algorithm 3. In the experiments, we observed that the M2Div without MCTS can still outperform the baselines. This is because the training process of M2Div can generate high quality episodes to train the model parameters with the help of MCTS, which leads to more accurate policy function p.",1,ad,True
461,4.5 Di erence with AlphaGo Zero,0,,False
462,"M2Div is inspired by the AlphaGo Zero program. It enjoys a number of merits from AlphaGo Zero, including the shared neural network for estimating policies and values, lookahead MCTS for strengthening the raw policy, and the compounded loss that simultaneously optimizes the value function and the policy function etc. However, M2Div has made a number of fundamental modi cations for search result diversi cation.",1,ad,True
463,"First, the formalizations of the tasks are di erent. AlphaGo Zero formalizes the playing of Go as an alternating Markov game where",0,,False
464,"each action corresponds a move, the states are the raw board positions, the value function approximates the probability of winning, and the next state depends not only on the chosen action but also on the move by the opponent. M2Div, on the other hand, formalizes the ranking of documents as planning with an MDP where each action corresponds to a document selection. The MDP states consists of the query, the preceding document ranking, and the remaining candidates. The value function approximates a diverse ranking evaluation measure (e.g., -NDCG@M). The state transition is fully determined by the current state and the chosen action.",0,,False
465,"Second, the supervision signals for learning the model parameters are di erent. AlphaGo Zero uses the results of self-play (-1 for loss, 0 for draw, and 1 for win) as the supervision signals and the value function is tted to the results. The task of diverse ranking, however, is not a Markov game. It is di cult (also unnecessary) to execute the self-play. In the training phase, M2Div resorts to the human labels and the prede ned evaluation measure to generate supervision information. Speci cally, the -NDCG@M of each generated episode is calculated and used as the ground-truth to t the value function (the rst part of Equation (8)). In this way, M2Div drives the training process so as to directly optimize the evaluation measure of -NDCG@M. Note that -NDCG@M can be replaced with any other diverse ranking evaluation measures.",0,,False
466,"Third, the shared deep neural networks for calculating the policies and values are di erent. AlphaGo Zero makes use of a residual network which takes the raw board position as its inputs and outputs the a probability distribution over moves, and a probability of the current player winning in position. The raw boards can be considered as some xed sized images. In M2Div, the MDP states consist of the queries and sequences of documents etc. Residual network cannot handle the state data as the queries/documents are raw texts, and the lengths of the document sequences vary in di erent time steps. To address the issue, M2Div rst represents the query and document sequence as the state vector of an LSTM (Equation 3). The policy and the value are then calculated on the basis of the representations outputted by the LSTM.",1,ad,True
467,5 EXPERIMENTS,0,,False
468,"We conducted experiments to test the performances of M2Div using a combination of four TREC benchmark datasets: TREC 2009  2012 Web Track datasets (WT2009, WT2010, WT2011, and WT2012).",1,TREC,True
469,5.1 Experimental settings,0,,False
470,"In our experiments, for e ective training of the model parameters and following the practices in [33], we combined four TREC datasets and constructed a new dataset with 200 queries and in total about 45,000 labeled documents. Each query includes several subtopics identi ed by the TREC assessors. The document relevance labels are made at the subtopic level and the labels are binary1.",1,TREC,True
471,"All the experiments were carried out on the ClueWeb09 Category B data collection2, which is comprised of 50 million English web documents. Porter stemming, tokenization, and stop-words removal (using the INQUERY list) were applied to the documents as preprocessing. We conducted 5-fold cross-validation experiments.",1,ClueWeb,True
472,1WT2011 has graded judgements and we treat them as binary in the experiments. 2 http://boston.lti.cs.cmu.edu/data/clueweb09,1,WT,True
473,131,0,,False
474,Session 1D: Learning to Rank I,1,Session,True
475,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
476,"We randomly split the queries into ve even subsets. At each fold, three subsets were used for training, one was used for validation, and one was used for testing. The results reported were the average over the ve trials.",0,,False
477,"The TREC o cial diversity evaluation metrics of -NDCG [6] and ERR-IA [5] were used in the experiments, including. They measure the diversity of a result list by explicitly rewarding diversity and penalizing redundancy observed at every rank. Following the default settings in o cial TREC evaluation program, the parameter  in these evaluation measures are set to 0.5.",1,TREC,True
478,We compared M2Div with several state-of-the-art baselines in search result diversi cation:,0,,False
479,MMR [3]: a heuristic approach in which the document is selected according to maximal marginal relevance.,0,,False
480,xQuAD [24]: a representative method which explicitly models di erent aspects underlying original query in form of subqueries.,0,,False
481,PM-2 [7]: a method of optimizing proportionality for search result diversi cation.,0,,False
482,"We also compared MDP-DIV with the learning methods: SVM-DIV [36]: a learning approach which utilizes structural SVMs to optimize the subtopic coverage. R-LTR [39]: a learning approach developed in the relational learning to rank framework. PAMM [31]: a learning approach that directly optimizes diversity evaluation measure using structured Perceptron. NTN-DIV [32]: a learning approach which automatically learns novelty features based on neural tensor networks. MDP-DIV [33]: a state-of-the-art learning approach which uses an MDP for modeling the diverse ranking process. Following the practice in [33], we con gured the reward function in MDP-DIV as -DCG and the discounting parameter  ,"" 1. M2Div, and the baselines of MDP-DIV and NTN-DIV need preliminary representations of the queries and the documents as their inputs. Following the practices in [33], in the experiments we used the query vector and document vector generated by the doc2vec [17] to represent the document. Doc2vec model was trained on all of the documents in Web Track dataset and the number of vector dimensions were set to 100. For training the model, we used the distributed bag of words model3. The learning rate was set to 0.025 and the window size was set to 8. The M2Div also has some parameters to tune: the training evaluation measure function R was set to -NDCG@5, making the value function V to approximate -NDCG@5. In the training phrase, the MCTS also was set to control the maximal length of the search depth less than 5. That is, the condition at line 9 of Algorithm 1 is true if the length of the tree depth is less than 5 and the candidate set is not empty. In all of the experiments, the learning rate , the number of search times K, the tree search trade-o parameter , and the number of LSTM hidden units h were tuned based on the validation set and set to  "","" 0.01, K "","" 5000,  "","" 3.0, and h "","" 5. In online ranking phase, M2Div has two versions: selecting the documents with the raw policy p or with the MCTS strengthened policy  . These two versions are respectively denoted as """"M2Div(without MCTS)"""" and """"M2Div(with MCTS)"""". The source code of M2Div is available at https://github.com/sweetalyssum/M2DIV.""",1,Track,True
483,3 http://radimrehurek.com/gensim/tutorial.html,1,ad,True
484,Table 1: Performance comparison of all methods on TREC web track datasets.,1,TREC,True
485,Method,0,,False
486,-NDCG@5 -NDCG@10 ERR-IA@5 ERR-IA@10,0,,False
487,MMR xQuAD PM-2,0,,False
488,SVM-DIV R-LTR PAMM( -NDCG) NTN-DIV( -NDCG) MDP-DIV( -DCG) M2Div(without MCTS) M2Div(with MCTS),0,,False
489,0.2753 0.3165 0.3047 0.3030 0.3498 0.3712 0.3962 0.4189,0,,False
490,0.4386 0.4424,0,,False
491,0.2979 0.3941 0.3730 0.3699 0.4132 0.4327 0.4577 0.4762,0,,False
492,0.4835 0.4852,0,,False
493,0.2005 0.2314 0.2298 0.2268 0.2521 0.2619 0.2773 0.2988,0,,False
494,0.3435 0.3459,0,,False
495,0.2309 0.2890 0.2814 0.2726 0.3011 0.3029 0.3285 0.3494,0,,False
496,0.3668 0.3686,0,,False
497,5.2 Experimental results,0,,False
498,"Table 1 reports the performances of our approach and all of the baseline methods in terms of diversity performance metrics including -NDCG@5, -NDCG@10, ERR-IA@5, and ERR-IA@10. Boldface indicates the highest scores among all runs. From the results we can see that, in terms of the four diversity evaluation metrics, ""M2Div (with MCTS)"" and ""M2Div (without MCTS)"" outperformed all of the baseline methods, including the heuristic method of MMR, xQuAD, PM-2 and learning methods of SVM-DIV, R-LTR, PAMM(-NDCG), NTN-DIV(-NDCG), and MDP-DIV(-DCG). We conducted significant testing (t-test) on the improvements of our approaches over the best baseline MDP-DIV(-DCG). The results indicate that most of the improvements are signi cant (p-value < 0.05 and denoted with `*' in Table 1).",0,,False
499,"From the results we can see that ""M2Div (without MCTS)"", which did not conduct MCTS at the online time, still outperformed all of the baselines including ""MDP-DIV(-DCG)"", indicating that the MCTS conducted at the training time can generate better episodes to estimate the model parameters, achieving better raw policy p for ranking. Note that ""M2Div(with MCTS)"", which conducted MCTS at the online time, further improved the ranking accuracies and performed the best among all of the methods. The results indicate that the MCTS can improve the raw policies p at both the training and the online ranking.",0,,False
500,5.3 Discussion,0,,False
501,"In this section, we conducted experiments to investigate how M2Div works and why it can outperform the baselines, using the experimental results on the rst fold of the data as example.",0,,False
502,5.3.1 E ects of Monte Carlo tree search. One key step in M2Div is the MCTS which outputs the search policy  . It is very likely that the search policy  (s) are better than raw policy p(s) in terms of choosing optimal documents.,0,,False
503,"We conducted experiments to show the e ectiveness of the MCTS in online ranking. Speci cally, based on the trained M2Div model, we tracked the online ranking process for query number 148 (""martha stewart and imclone""). The red real curve in Figure 4 shows the -NDCG values of the document ranking generated by  . The blue dashed lines in Figure 4 show the -NDCG values at",0,,False
504,132,0,,False
505,Session 1D: Learning to Rank I,1,Session,True
506,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
507,1.0,0,,False
508,0.8,0,,False
509,®-NDCG,0,,False
510,0.6,0,,False
511,0.4,0,,False
512,0.2,0,,False
513,search policy,0,,False
514,raw policy,0,,False
515,0.0,0,,False
516,0,0,,False
517,1,0,,False
518,2,0,,False
519,3,0,,False
520,4,0,,False
521,5,0,,False
522,position,0,,False
523,Figure 4: -NDCG at the top 5 positions for the ranking generated by the search policy  (red real curve) for query number 148. Blue dashed lines show the -NDCG values if the documents were selected by the raw policy p.,0,,False
524,®-NDCG@5,0,,False
525,0.7,0,,False
526,0.6,0,,False
527,0.5,0,,False
528,0.4,0,,False
529,0.3,0,,False
530,0.2,0,,False
531,search policy,0,,False
532,raw policy,0,,False
533,0.1,0,,False
534,0,0,,False
535,20,0,,False
536,40,0,,False
537,60,0,,False
538,80,0,,False
539,100,0,,False
540,120,0,,False
541,140,0,,False
542,iteration,0,,False
543,"Figure 5: Performance curves of the document rankings generated with the raw policy p and with the search policy  , w.r.t. the training iterations. The curves illustrate the averaged performances over all of the training queries.",0,,False
544,"these 5 positions if the corresponding document was chosen by the raw policy p (note the preceding documents are still selected by  ). From the results, we can see that  improved p at the positions of 1, 3, and 5, showing the e ectiveness of MCTS at the online ranking.",0,,False
545,"We also conducted experiments to show the e ectiveness of the MCTS in o ine training. Speci cally, at the end of each training iteration, based on current ranking model, we tested the performances of the document ranking constructed with the raw policy p and with the MCTS search policy  , in term of -NDCG@5 on the training queries. Figure 5 shows the averaged -NDCG@5 scores among all of the training queries at each training iterations. It is not surprise that the document rankings generated by the search policy  are superior to that of generated by the raw policy p, at all of the training iterations. The results indicate that the lookahead MCTS can generate better diverse document rankings for training the model parameters, at all of the training iterations. The results also partially explained why ""M2Div(without MCTS)"", which did not conduct MCTS at the online time, can still outperform the baselines. The MCTS conducted at the training time produced better diverse document rankings for training the model parameters.",1,ad,True
546,"5.3.2 Ranking with policy or with value? In ""M2Div(without MCTS)"", we rely on the raw policy p for ranking the documents. In principle, the value function V can also be used for ranking, that is, at each state s a document (action) a^ is selected if a^ ,"" arg maxa A(s) V (T (s, a)). We conducted experiments to show the performances of these two approaches and Figure 6 shows the test performance curves of the document rankings with the policy function and with the value function. From the results, we can see that the document rankings generated by the raw policy p are more stable and better than that of generated by the value function V . One possible reason for the phenomenon is that the ranking performance measure -NDCG@5 are not easy to estimate accurately, especially at the early stages of the ranking procedure. This is why p in stead of V is adopted in """"M2Div(without MCTS)"""".""",1,ad,True
547,®-NDCG@5,0,,False
548,0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15,0,,False
549,0,0,,False
550,policy value,0,,False
551,20,0,,False
552,40,0,,False
553,60,0,,False
554,80,0,,False
555,100,0,,False
556,120,0,,False
557,140,0,,False
558,iteration,0,,False
559,"Figure 6: Performance curves of the document rankings generated with the policy function and with the value function, w.r.t. training iterations. The curves illustrates the averaged performances over all of the test queries.",0,,False
560,6 CONCLUSION,0,,False
561,"In this paper we have proposed a novel approach to learning diverse ranking model for search result diversi cation, referred to as M2Div. In contrast to existing methods that greedily select a locally optimal document at each of the ranking positions, M2Div conducts an exploratory decision making with the lookahead MCTS and thus can select a better document. MDP is used to model the ranking process and reinforcement learning is utilized to train the model parameters. M2Div o ers several advantages: ranking with both the shared policy function and the value function, high accuracy in ranking, and exibility of trading-o between accuracy and online ranking speed. Experimental results based on the TREC benchmark",1,ad,True
562,133,0,,False
563,Session 1D: Learning to Rank I,1,Session,True
564,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
565,"datasets show that M2Div can signi cantly outperform the state-ofthe-art baselines including the heuristic methods of MMR, xQuAD and learning methods of R-LTR, PAMM, and MDP-DIV.",0,,False
566,7 ACKNOWLEDGMENTS,0,,False
567,"This work was funded by the 973 Program of China under Grant No. 2014CB340401, the National Key R&D Program of China under Grants No. 2016QY02D0405, the National Natural Science Foundation of China (NSFC) under Grants No. 61773362, 61425016, 61472401, 61722211, and 20180290, and the Youth Innovation Promotion Association CAS under Grants No. 20144310, and 2016102.",0,,False
568,REFERENCES,0,,False
569,"[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying Search Results. In Proceedings of the Second ACM International Conference on Web Search and Data Mining (WSDM '09). 5­14.",0,,False
570,"[2] Sumit Bhatia. 2011. Multidimensional Search Result Diversi cation: Diverse Search Results for Diverse Users. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '11). ACM, New York, NY, USA, 1331­1332.",0,,False
571,"[3] Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '98). 335­336.",1,ad,True
572,[4] Ben Carterette and Praveen Chandar. 2009. Probabilistic Models of Ranking Novel Documents for Faceted Topic Retrieval. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM '09). 1287­1296.,0,,False
573,"[5] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM '09). 621­630.",1,ad,True
574,"[6] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Büttcher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '08). ACM, New York, NY, USA, 659­666.",1,Novelty,True
575,"[7] Van Dang and W. Bruce Croft. 2012. Diversity by Proportionality: An Electionbased Approach to Search Result Diversi cation. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '12). ACM, New York, NY, USA, 65­74.",0,,False
576,"[8] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. J. Mach. Learn. Res. 12 (July 2011), 2121­2159.",1,ad,True
577,[9] Sreenivas Gollapudi and Aneesh Sharma. 2009. An Axiomatic Approach for Result Diversi cation. In Proceedings of the 18th International Conference on World Wide Web (WWW '09). 381­390.,0,,False
578,"[10] Shengbo Guo and Scott Sanner. 2010. Probabilistic Latent Maximal Marginal Relevance. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '10). ACM, 833­834.",0,,False
579,"[11] Jiyin He, Vera Hollink, and Arjen de Vries. 2012. Combining Implicit and Explicit Topic Representations for Result Diversi cation. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '12). ACM, New York, NY, USA, 851­860.",0,,False
580,"[12] Katja Hofmann, Shimon Whiteson, and Maarten Rijke. 2013. Balancing Exploration and Exploitation in Listwise and Pairwise Online Learning to Rank for Information Retrieval. Inf. Retr. 16, 1 (Feb. 2013), 63­90.",0,,False
581,"[13] Sha Hu, Zhicheng Dou, Xiaojie Wang, Tetsuya Sakai, and Ji-Rong Wen. 2015. Search Result Diversi cation Based on Hierarchical Intents. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM '15). ACM, New York, NY, USA, 63­72.",0,,False
582,"[14] Zhengbao Jiang, Zhicheng Dou, Xin Zhao, Jian-Yun Nie, Ming Yue, and Ji-Rong Wen. 2018. Supervised Search Result Diversi cation via Subtopic Attention. IEEE Transactions on Knowledge and Data Engineering (2018).",0,,False
583,"[15] Zhengbao Jiang, Ji-Rong Wen, Zhicheng Dou, Wayne Xin Zhao, Jian-Yun Nie, and Ming Yue. 2017. Learning to Diversify Search Results via Subtopic Attention. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 545­554.",0,,False
584,"[16] Branislav Kveton, Csaba Szepesvári, Zheng Wen, and Azin Ashkan. 2015. Cascading Bandits: Learning to Rank in the Cascade Model. CoRR abs/1502.02763 (2015).",1,ad,True
585,"[17] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014. 1188­1196.",0,,False
586,"[18] Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha, and Yong Yu. 2009. Enhancing Diversity, Coverage and Balance for Summarization Through Structure Learning. In Proceedings of the 18th International Conference on World Wide Web (WWW '09). ACM, New York, NY, USA, 71­80.",0,,False
587,[19] Zhongqi Lu and Qiang Yang. 2016. Partially Observable Markov Decision Process for Recommender Systems. CoRR abs/1608.07793 (2016).,0,,False
588,"[20] Jiyun Luo, Sicong Zhang, and Hui Yang. 2014. Win-win Search: Dual-agent Stochastic Game in Session Search. In Proceedings of the 37th International ACM",1,Session,True
589,"SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 587­596. [21] Lilyana Mihalkova and Raymond Mooney. 2009. Learning to Disambiguate Search Queries from Short Sessions. In Machine Learning and Knowledge Discovery in Databases. Lecture Notes in Computer Science, Vol. 5782. Springer. [22] Filip Radlinski and Susan Dumais. 2006. Improving Personalized Web Search Using Result Diversi cation. In Proceedings of the 29th Annual International ACM",1,Session,True
590,"SIGIR Conference on Research and Development in Information Retrieval (SIGIR '06). ACM, New York, NY, USA, 691­692. [23] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. 2008. Learning Diverse Rankings with Multi-armed Bandits. In Proceedings of the 25th International Conference on Machine Learning (ICML '08). ACM, New York, NY, USA, 784­791. [24] Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting Query Reformulations for Web Search Result Diversi cation. In Proceedings of the 19th International Conference on World Wide Web (WWW '10). 881­890. [25] Rodrygo L. T. Santos, Jie Peng, Craig Macdonald, and Iadh Ounis. 2010. Explicit Search Result Diversi cation through Sub-queries. [26] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based Recommender System. J. Mach. Learn. Res. 6 (Dec. 2005), 1265­1295. [27] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural networks and tree search. Nature 529, 7587 (2016), 484­489. [28] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. 2017. Mastering the game of go without human knowledge. Nature 550, 7676 (2017), 354. [29] Xiaojie Wang, Zhicheng Dou, Tetsuya Sakai, and Ji-Rong Wen. 2016. Evaluating Search Result Diversity Using Intent Hierarchies. In Proceedings of the 39th",1,ad,True
591,"International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '16). ACM, New York, NY, USA, 415­424. [30] Zeng Wei, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2017. Reinforcement Learning to Rank with Markov Decision Process. In Proceedings of the 40th",0,,False
592,"International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '17). 945­948. [31] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). 113­122. [32] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling Document Novelty with Neural Tensor Network for Search Result Diversi cation. In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '16). 395­404. [33] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, Wei Zeng, and Xueqi Cheng. 2017. Adapting Markov Decision Process for Search Result Diversi cation. In Proceed-",1,Novelty,True
593,"ings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '17). 535­544. [34] Jun Xu, Long Xia, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2017. Directly Optimize Diversity Evaluation Measures: A New Approach to Search Result Diversi cation. ACM Trans. Intell. Syst. Technol. 8, 3, Article 41 (Jan. 2017), 26 pages. [35] Hai-Tao Yu, Adam Jatowt, Roi Blanco, Hideo Joho, Joemon Jose, Long Chen, and Fajie Yuan. 2017. A concise integer linear programming formulation for implicit search result diversi cation. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. ACM, 191­200. [36] Yisong Yue and Thorsten Joachims. 2008. Predicting Diverse Subsets Using Structural SVMs. In Proceedings of the 25th International Conference on Machine Learning (ICML '08). ACM, New York, NY, USA, 1224­1231. [37] Yisong Yue and Thorsten Joachims. 2009. Interactively Optimizing Information Retrieval Systems As a Dueling Bandits Problem. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML '09). 1201­1208. [38] Sicong Zhang, Jiyun Luo, and Hui Yang. 2014. A POMDP Model for Contentfree Document Re-ranking. In Proceedings of the 37th International ACM SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 1139­1142. [39] Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Shuzi Niu. 2014. Learning for Search Result Diversi cation. In Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 293­302.",1,ad,True
594,134,0,,False
595,,0,,False

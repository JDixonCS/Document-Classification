,sentence,label,data,regex
0,Session 3C: Question Answering,1,Session,True
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Ranking Documents by Answer-Passage Quality,0,,False
3,Evi Yulianti,0,,False
4,"RMIT University Melbourne, Australia",0,,False
5,Ruey-Cheng Chen,0,,False
6,"SEEK Ltd. Melbourne, Australia",0,,False
7,Falk Scholer,0,,False
8,"RMIT University Melbourne, Australia",0,,False
9,W. Bruce Croft,0,,False
10,"RMIT University Melbourne, Australia",0,,False
11,ABSTRACT,0,,False
12,"Evidence derived from passages that closely represent likely answers to a posed query can be useful input to the ranking process. Based on a novel use of Community Question Answering data, we present an approach for the creation of such passages. A general framework for extracting answer passages and estimating their quality is proposed, and this evidence is integrated into ranking models. Our experiments on two web collections show that such quality estimates from answer passages provide a strong indication of document relevance and compare favorably to previous passage-based methods. Combining such evidence can significantly improve over a set of state-of-the-art ranking models, including Quality-Biased Ranking, External Expansion, and a combination of both. A final ranking model that incorporates all quality estimates achieves further improvements on both collections.",1,corpora,True
13,Mark Sanderson,0,,False
14,"RMIT University Melbourne, Australia",0,,False
15,KEYWORDS,0,,False
16,Document ranking; quality estimation; answer passages,0,,False
17,"ACM Reference Format: Evi Yulianti, Ruey-Cheng Chen, Falk Scholer, W. Bruce Croft, and Mark Sanderson. 2018. Ranking Documents by Answer-Passage Quality. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210028",0,,False
18,1 INTRODUCTION,1,DUC,True
19,"It has long been thought that combining document-level and passagelevel evidence is an effective retrieval approach [8, 46]. Bendersky and Kurland [4], for example, showed that combining evidence from the best-matching passage in retrieved documents leads to increased retrieval effectiveness.",1,ad,True
20,Different types of passages have been examined. Tombros and Sanderson [43] proposed so-called query biased summaries for use,0,,False
21,This author is also affiliated with Universitas Indonesia. This work was conducted during her graduate studies at RMIT University The research work was primarily done at RMIT University,1,ad,True
22,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210028",1,ad,True
23,"Figure 1: An example of questions from the CQA site, Yahoo! Answers, that are related to the given query ""dinosaurs""",1,Yahoo,True
24,"in search result pages. Later work provided evidence supporting the use of summaries as a passage representation to improve ad hoc retrieval [14, 22, 40]. Such summaries are created based on the degree of query-term matching, rather than document relevance. It remains to be seen if more effective passages can be found.",1,ad,True
25,"We investigate whether passages can be biased towards selecting text fragments that are more likely to bear answers to the query, and whether this new approach would give a better indication of underlying document relevance. The induced representation would tend to cover a richer set of text evidence rather than just the given query terms. We call these fragments answer passages.",0,,False
26,"We create answer passages by exploiting content in a specialized resource where high quality, human-curated question-answer structures are abundant: Community Question Answering (CQA) services. The text content on such services is utilized in a specific way: not to reuse or synthesize answers, but to provide an indication as to which text fragments in a document are likely to be part of an accepted answer. This ""answer-bearingness"" property can serve as a valuable document ranking signal.",0,,False
27,"While exploiting information from external resources to improve ranking is common [5, 12], to the best of our knowledge, no past work has studied using an external resource to improve the relevance estimate of document passages for ad hoc retrieval.",1,ad,True
28,Our main contributions are: (1) We develop a new approach for representing passage-level evidence for ad hoc retrieval via a novel,1,ad,True
29,335,0,,False
30,Session 3C: Question Answering,1,Session,True
31,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
32,"use of CQA data; (2) The new approach provides a strong indication of document relevance, and is able to outperform many previous passage-based methods; combining text quality with evidence derived from the new representation leads to further improvements. Our experiments show that incorporating the new evidence significantly improves over state-of-the-art ranking models, including Quality-Biased Ranking (QSDM), External Expansion (EE), and a combination of both.",1,ad,True
33,"The remainder of this paper is organized as follows. Section 2 presents related work, followed by the motivation of this work in Section 3. Section 4 details our framework of passage extraction using external resources and document re-ranking using quality features derived from the answer passages. Sections 5 and 6 describe the experiment and the results. Discussion and concluding remarks are given in Sections 7 and 8.",0,,False
34,2 BACKGROUND,0,,False
35,"Table 1 shows our categorization of approaches to document ranking: considering the object being scored (i.e. document or passage/summary) and the location of information that is exploited (i.e. local or external).1 Work listed in the top-left cell focuses on attempts to improve relevance estimation of a document using the local collection. The top-right cell lists work exploring the use of more focused text representations such as passages or summaries. The bottom-left cell lists work exploiting external resources for improving relevance estimation. A considerable amount of effort was invested in both directions, but the intersection, the bottom-right, has had less exploration. We now examine each cell in turn.",1,ad,True
36,"Document-Based Scoring Using Local Collection. Common retrieval models such as BM25 [38], language models [34], and DfR [2] are in this cell. Among these widely used models, Sequential Dependency Model (SDM) has consistently demonstrated strong effectiveness [29]. Lavrenko and Croft [23] implemented pseudorelevance feedback (PRF) within a language modeling framework. The basic idea of PRF [39] is assuming the initially retrieved topranked documents are relevant and then extracting the most frequent terms from them to improve retrieval effectiveness.",0,,False
37,"Kurland and Lee [20] leveraged link-based methods using interdocument similarities. Bendersky et al. [3] integrated document quality features in a quality-biased SDM (QSDM) framework, and showed the effectiveness of their approach over text- and link-based techniques. To the best of our knowledge, no previous work has reported superior performance to QSDM ranking.",0,,False
38,"Document-Based Scoring Using External Resources. Use of external resources to improve the relevance estimation of documents has also been tried [5, 12]. Diaz and Metzler [12] incorporated information from external corpora, such as the web collections, using a language modeling technique for PRF. External expansion was shown to be effective and was extended by Weerkamp et al. to the task of blog retrieval [45]. Bendersky et al. [5] also explored the use of term/concept statistics derived from external corpora, such as MSN query logs and Wikipedia, into the SDM method.",1,corpora,True
39,"1Note, such a categorization excludes methods that either address more specific retrieval problems (e.g. clustering [36] or learning to rank [27]) or that exploit other data (e.g. link analysis [20] or user signals [1]).",1,ad,True
40,"Passage-Based Scoring Using Local Collection. Combining evidence from passages to improve ad hoc retrieval has been explored by Bendersky and Kurland [4], who showed that incorporating the best-matching passage into the original document language model [34] can significantly improve retrieval effectiveness. Krikon and Kurland [19] further explored the integration of documentbased, cluster-based, and passage-based information to improve document ranking. Relatively little work has considered using document summaries, as another passage representation, to improve retrieval effectiveness. More recently, He et al. [14] showed that combining summaries and documents improves the retrieval effectiveness of a document language model baseline.",1,ad,True
41,"However, as will be shown in our experiments, this approach does not improve over stronger retrieval models such as SDM, suggesting that the passage scoring is not effective in the presence of term proximity information. While the advantage of using passage representations in ad hoc retrieval appears evident, it is still an open question if further improvements can be made.",1,ad,True
42,"Passage-Based Scoring Using External Resources. To the best of our knowledge, using an external collection to better estimate the relevance of retrieved passages for ad hoc retrieval has not been explored. Much of the past work has focused on using external collections for the relevance estimation of documents, including Wikipedia [5, 28] and web collections [12].",1,ad,True
43,"CQA sites allow people to ask questions that are answered by other users in the community. The popularity of CQA, such as Yahoo! Answers, has grown rapidly; in 2016, over 3.0 million people in U.S. accessed Yahoo! Answers per month.2 Previous work has exploited CQA data for many purposes, such as: answering factoid and tips questions [6, 44], answering non-factoid queries [51], predicting information asker and web searcher satisfaction [25, 26], and evaluating answer quality [41]. We are not aware of previous work that has used CQA for improving document ranking.",1,Yahoo,True
44,3 HYPOTHESIS,0,,False
45,Our work tests the following answer-bearingness hypothesis:,0,,False
46,Documents that are likely to bear focused answers to the posed query should be ranked highly.,0,,False
47,"To test the hypothesis, CQA resources are exploited as proxies of an oracle ""answer source"", which is unattainable otherwise. A scoring rule is developed and used in a subsequent passage generation step to score any given passage according to how well its text content approximates the answer source data. Following Bendersky and Kurland [4], we assume that the best-scoring passage under this scoring rule can represent the full document in a quality-biased ranking framework, and therefore quality features derived from the best-scoring passage can directly benefit retrieval. A set of similar strategies was recently reviewed in passage retrieval [17], with an aim of improving the presentation of search results in general.",0,,False
48,We now formally define the research questions:,0,,False
49,RQ1 Can answer passages be exploited to improve document ranking compared to existing methods?,0,,False
50,RQ2 Can incorporating quality features from answer passages improve ad hoc retrieval?,1,corpora,True
51,2 https://www.quantcast.com/answers.yahoo.com,0,,False
52,336,0,,False
53,Session 3C: Question Answering,1,Session,True
54,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
55,"Table 1: Ad hoc retrieval methodologies broken down in two axes, based on the object being scored (columns) and the resource used in relevance estimation (rows). Shaded methods are our addition to this work.",1,hoc,True
56,Local Collection External Resources,0,,False
57,Document,0,,False
58,"Retrieval models: BM25, SDM, or DfR Pseudo relevance feedback [23, 39] Quality-biased ranking (QSDM) [3]",0,,False
59,"External expansion (MoRM) [12, 45] Weighted dependence model (WSD) [5]",0,,False
60,"Passage Passage-based LM [4, 14, 19]",1,LM,True
61,Answer-passage quality,0,,False
62,"Our methodology allows for the creation of multiple passage representations for improving document ranking, which leads to a third research question:",1,ad,True
63,RQ3 Does combining quality features from multiple passage representations make a stronger ranking model?,0,,False
64,4 AN ANSWER-PASSAGE APPROACH,1,AP,True
65,"In passage retrieval, a two-phase approach is used to avoid needing to generate passage representations for all documents. We assume that an initial set of documents DQ with respect to query Q is first retrieved using a standard retrieval function such as BM25 or SDM, to serve as input to the passage retrieval module. Following this step, our answer-passage approach will exploit information from CQA data to induce passages that are likely to bear answers to query Q, and use this passage representation to re-rank documents.",0,,False
66,"We present two different methodologies in the coming sections for extracting and scoring answer passages. Section 4.1 presents a general probabilistic framework that involves external resources in the process of extracting answer passages. An alternative method, described in Section 4.2, leverages open-domain question answering to directly retrieve answer-reporting passages. On either type of representation, a final re-ranking step is performed based on the passage quality, which is described in Section 4.3.",0,,False
67,4.1 A Probabilistic Framework,0,,False
68,"Our approach requires one basic functionality from the CQA resource: the ability to perform question retrieval so that the user can submit a query Q to retrieve a set of related questions and gain access to the respective answers AQ (see Section 5.1). The answers, AQ , are used to improve the estimation of term relevance [12]. In a standard language modeling framework [23], this relevance estimate p(t |Q) is written as:",0,,False
69,p(t |Q) ,0,,False
70,"p(t |A) p(Q |A),",0,,False
71,(1),0,,False
72,A  AQ,0,,False
73,"where p(t |A) is the relevance estimate of term t derived from answer A, and p(Q |A) is the retrieval score of answer A with respect to Q.",0,,False
74,"Improving Relevance Estimation of Terms. For term relevance p(t |A), we consider estimates that are in proportion to a given term weighting function. The following functions are discussed:",0,,False
75,"· Query Likelihood (QL) [34, 53]:",1,Query,True
76,"f (t, A) + µ p(t |C) . |A| + µ",0,,False
77,(2),0,,False
78,· BM25 [38]:,0,,False
79,"f (t, A) (k1 + 1)",0,,False
80,id f (t).,0,,False
81,(3),0,,False
82,"f (t, A) + k1",0,,False
83,1,0,,False
84,-b,0,,False
85,+b,0,,False
86,|A | avgA |A |,0,,False
87,· Embedding Language Model (EMB):,0,,False
88,t   T~,0,,False
89,tA,0,,False
90,A tA,0,,False
91,p(t,0,,False
92,A,0,,False
93,", tA) p(t ,",0,,False
94,1/,0,,False
95,tA,0,,False
96,|A |,0,,False
97,) 1/,0,,False
98,|A,0,,False
99,|,0,,False
100,.,0,,False
101,(4),0,,False
102,The first two functions are based on commonly used retrieval,0,,False
103,"models, Query Likelihood (QL) [34, 53] and BM25 [38]. For QL, µ",1,Query,True
104,controls the degree of Dirichlet smoothing and p(t |C) is the back-,0,,False
105,"ground (collection) language model. For BM25, k1 and b are parameters and avgA |A| is the average answer size. In these equations, f (t, A) denotes the frequency of term t within answer A.",0,,False
106,The third term relevance estimate is based on word embed-,0,,False
107,"dings [31], which can serve as an alternative to more conventional",0,,False
108,"score functions. Our formulation differs from prior work [21, 52]",0,,False
109,in the way the probability of jointly observing term t and answer,0,,False
110,A is defined:,0,,False
111,1/ |A |,0,,False
112,"p(t, A) ",0,,False
113,"tA A p(t, tA)",0,,False
114,.,0,,False
115,(5),0,,False
116,We postulate that the likelihood of jointly observing two terms t and t  in the same document context is proportional to a sigmoidal,0,,False
117,transformation (with scale/location parameters  and x0) of the cosine similarity between the respective word vectors vt and vt:,0,,False
118,"p(t, t ) ",0,,False
119,1,0,,False
120,.,0,,False
121,"1 + exp (-(cos(vt , vt) - x0))",0,,False
122,(6),0,,False
123,It can be shown that the relevance estimate p(t |A) as in (4) follows,0,,False
124,"this derivation. Practically, it suffices to compute the normalization factor in (4) over a smaller subset of terms T~  T .",0,,False
125,"p(Q |A) informs the degree of relevance of answer A with respect to query Q, so that in (1) more relevant answers have stronger influence over the inferred model p(t |Q). As CQA sites do not usually",0,,False
126,"reveal such scores or even the scoring rules, some distributional",0,,False
127,"assumptions are made for computing this estimate. One can assume that the query likelihood of answer A retrieved at the k-th position (within the set AQ ) is distributed logarithmically, in accordance with the Discounted Cumulative Gain (DCG) [16], or geometrically,",1,ad,True
128,in accordance with the Rank-Biased Precision (RBP) metric [32]. For,0,,False
129,"simplicity, in this paper we focus on only the DCG variant, defined",0,,False
130,"as follows, as both variants showed comparable performance in our",0,,False
131,preliminary experiments:,0,,False
132,p(Q |A)  (log k + 1)-1 .,0,,False
133,(7),0,,False
134,337,0,,False
135,Session 3C: Question Answering,1,Session,True
136,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
137,"Extracting Answer Passages. The next step is to incorporate the estimated term relevance into a passage algorithm to extract subdocument representations G that best approximate the retrieved answer-bearing content. Two approaches are taken: extracting fixed-length passages (PSG) and extracting summaries using integer linear programming (ILP). Note that, depending on the approach in use, G can either be a contiguous block of text or a set of sentences put together by using document summarization.",1,corpora,True
138,"The first approach, PSG, is based on the use of fixed-length passages that are common in retrieval [8, 33]. Such representations do not (usually) stick to predefined sentence/paragraph boundaries and can be easily generated using a sliding window algorithm. From all passages in document D, prior work [4] suggests scoring them with a language modeling approach to choose one passage G with the maximum score. Our first approach follows this practice but uses the improved relevance estimates to evaluate passages:",0,,False
139,GPSG,0,,False
140,",",0,,False
141,arg,0,,False
142,max,0,,False
143,G D,0,,False
144,t G p(t |Q).,0,,False
145,(8),0,,False
146,"However, the answer-bearing content may not necessarily form a contiguous text block so that fixed-length passages will catch them. Redundant terms in a passage can also fill up the space easily without providing additional information, rendering the relevance estimate unreliable.",1,ad,True
147,"Our second approach, ILP, draws on document summarization to tackle these issues. It leverages integer linear programming to extract document summaries [13, 42, 47], with the core algorithm extended to incorporate term relevance estimates derived from CQA resources. This particular approach is taken in our framework for both the efficacy and the ease to incorporate external knowledge about topical relevance.3 The algorithm is optimized to select a set of sentences that maximize the coverage of answer-bearing terms in the generated summary G:",1,corpora,True
148,GILP,0,,False
149,",",0,,False
150,arg max,0,,False
151,G D,0,,False
152,L(G),0,,False
153,+,0,,False
154," R(G),",0,,False
155,(9),0,,False
156,where:,0,,False
157,"L(G) ,"" p(t |Q), R(G) "","" p(t |Q) s f (t, G) (10)""",0,,False
158,t G,0,,False
159,t G,0,,False
160,"and |G| is less than or equal to some predefined K and s f (t, G) de-",0,,False
161,"notes the ""sentence frequency"" of term t in summary G. Both objec-",0,,False
162,tive functions L(G) and R(G) are combined using a hyperparameter 0    1. The first objective will try to maximize summary-level,0,,False
163,term coverage and reduce term repetition. The other sentence-level,0,,False
164,objective will include more sentences with highly relevant terms.,0,,False
165,4.2 Open-Domain Question Answering,0,,False
166,"We also implement an alternative answer-passage scoring framework based on a recent open-domain question answering model, called Document Reader (DR) [9]. The goal of open-domain question answering is to automatically extract text fragments (""answers"") from a set of unstructured or free-format documents to address users' questions.",1,ad,True
167,"3More advanced approaches, such as submodular optimization [24], use sentenceto-sentence similarities rather than concept relevance to perform document summarization. It is not clear yet how CQA resources can be incorporated in this regard to improve the extraction of answer passages.",1,ad,True
168,Table 2: List of passage quality features.,0,,False
169,Feature PassageScore PassageOverlap NumSentences QueryOverlap AvgWordWeight AvgTermLen Entropy FracStops StopCover,1,Query,True
170,Definition Objective value to score the passage Bigram overlap with respect to answers Number of sentences Number of query term occurrences Average passage term weight Average passage term length Shannon entropy of the term distribution Fraction of passage terms that are stopwords Fraction of stopwords appear in the passage,0,,False
171,"The DR model takes query Q and document D as input and returns a best-matching passage G ,"" 1, 2, . . . , m  of m terms""",0,,False
172,"that maximizes an answer span score, defined as follows:",0,,False
173,GD R,0,,False
174,",",0,,False
175,arg,0,,False
176,max,0,,False
177,G D,0,,False
178,max,0,,False
179,1i j m,0,,False
180,log pS,0,,False
181,(i,0,,False
182,"|G, Q)",0,,False
183,+,0,,False
184,log pE (j,0,,False
185,"|G, Q).",0,,False
186,(11),0,,False
187,"In this formulation, the score being optimized indicates the loglikelihood of a passage G reporting an answer. The core idea behind",0,,False
188,"DR is to use recurrent neural networks to aggregate term-level evidence (i.e. features), and then for each passage term i estimate if the term starts or ends an answer span with respect to Q using attentive modeling [15]. The best scoring pair i , j  in a passage is identified to compute the final answer span score. Specifically, the two likelihood models pS and pE , for starting and ending an answer span, are defined as:",0,,False
189,"pS (i |G, Q)  exp(vTi WS vQ ), pE (j |G, Q)  exp(vTj WE vQ ),",0,,False
190,(12),0,,False
191,"where vi and vj are passage-term vectors, vQ denotes the query vector, andWS andWE indicate the bilinear mappings. Both passage-",0,,False
192,term vectors and query-term vectors are derived from the hidden,0,,False
193,"states of two separate recurrent neural networks, and the query",0,,False
194,vector is a weighted combination of the derived query-term vectors.,0,,False
195,These definitions are given as follows:,0,,False
196,"v :   G ,"" BiLSTMG (f :   G),""",0,,False
197,"vq : q  Q ,"" BiLSTMQ (eq : q  Q), (13)""",1,MQ,True
198,"vQ ,"" softmax(WQ vq ) vq ,""",0,,False
199,q Q,0,,False
200,"where WQ is a linear mapping, f denotes the feature vector for passage term , and eq denotes the word embeddings for term q.",0,,False
201,4.3 Passage Quality Based Ranking,0,,False
202,"A mix of novel and existing features are employed to estimate the quality of the produced passage, see Table 2. PassageScore denotes the score assigned to the best matching passage in the retrieved document. The score is combined with PassageOverlap to estimate the answer-bearingness level of a passage relative to a given query. PassageOverlap measures the term overlap between a document passage and its related CQA answers. NumSentences is employed as a quality feature based on the idea that a summary with too many short sentences is less likely to be relevant or informative. QueryOverlap has been used in previous studies on web",1,Query,True
203,338,0,,False
204,Session 3C: Question Answering,1,Session,True
205,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
206,Table 3: Test collections used in our experiments.,0,,False
207,Collection Topics,0,,False
208,# Docs,0,,False
209,GOV2,0,,False
210,"TREC Topics 701­850 25,205,179",1,TREC,True
211,"ClueWeb09B TREC Web Topics 1­200 50,066,642",1,ClueWeb,True
212,"search ranking [1] and in query-biased summarization [30]. Other prior work [50] leveraged AvgWordWeight as a sentence feature to generate document summaries. Motivated by the effectiveness of document quality features used by Bendersky et al. [3], we adopt four non-HTML specific quality features to work at the passage level: AvgTermLen, Entropy, FracStops, and StopCover.",1,ad,True
213,"The proposed quality estimates are combined by using a featurebased linear ranking model (see (14)). Previous work has used a similar approach [3, 29], and in most cases combining evidence from different representations and different retrieval functions has been shown to be beneficial [11]. As was done in the QSDM framework [3], the SDM retrieval score is also included in the model:",0,,False
214,"D fSDM(q, D) + j j fj (q, G)",0,,False
215,(14),0,,False
216,"where the weights D + j j ,"" 1, fj represents the j-th feature, and G represents the answer passage. The weights are learned using""",0,,False
217,a learning-to-rank algorithm described in Section 5.3.,0,,False
218,5 EXPERIMENTS,0,,False
219,A series of experiments was conducted to evaluate the effectiveness of the proposed ranking model using quality features extracted from the answer passages. Section 5.1 describes the data and evaluation metrics used in our experiments. Section 5.2 covers the details about baselines and Section 5.3 covers the parameter estimation.,0,,False
220,5.1 Setup,0,,False
221,The code and data used in this paper are made publicly available for interested readers to reproduce our results.4,1,ad,True
222,"Test Collections. Ranking experiments were conducted on two web test collections, GOV2 and CW09B (i.e. ClueWeb09B), using TREC Terabyte 2004­2006 and Web Track 2009­2012 ""title"" topics respectively. An overview of these data sets is provided in Table 3. Both web collections were indexed using the Indri search engine using Krovetz stemming without removing stopwords. The spam filter by Cormack et al. [10] was applied to CW09B, removing spam webpages with a score less than 70. Repeating the same experiments on un-filtered CW09B data leads to the same conclusions, with some slight decreases in absolute early precision (@10) but increases in recall-oriented metrics.",1,CW,True
223,"Retrieval Settings. Initially, a ranked list of 100 documents was retrieved using the SDM, following the configuration parameters suggested in the original paper (T , O , U ) ,"" (0.85, 0.10, 0.05) [29]. This step is performed using the Indri search engine.5 The raw HTML content for each retrieved document was parsed by using BeautifulSoup6 and sentences extracted using the Stanford""",0,,False
224,4 https://github.com/rmit- ir/AnswerPassageQuality 5http://www.lemurproject.org/indri.php (version 5.9) 6https://www.crummy.com/software/BeautifulSoup/ (version 4.0),0,,False
225,CoreNLP toolkit.7 Stopwords were removed from the sentences (using the INQUERY list) and Krovetz stemming was performed.,0,,False
226,"External CQA Resources. The external CQA data were obtained from Yahoo! Answers (Y!A), by submitting our queries to the Y!A search engine and taking the best answer for each of the top ten matching questions. In Y!A, the best answer for each question is chosen by the person who posts the question.8 Our decision to use only the best answer for each question is to ensure good quality information [51]. There are three GOV2 queries (QID 703, 769, 816) and five CW09B queries (QID 95, 100, 138, 143, 146), however, that do not have any matching questions. Since the purpose of this research is to investigate how external evidence can be used to enhance summaries and document ranking, we remove these eight queries from this experiment (we return to the issue of the availability of suitable CQA answers in Section 6.5). The average number of related CQA answers per query in GOV2 and CW09B data are 9.52 and 9.74 (maximum of 10), respectively. The choice of ten as the number of related CQA answers per query is justified based on the result of an initial experiment, where we tried using 1, 5, 10, 20, 50, and 100, and found that according to several metrics, using a single answer is the least effective, while using ten answers gave the most effective results in most of the cases.",1,Yahoo,True
227,"Word Embeddings. Two sets of word embeddings are used, both based on the fasttext package [7]. The first is a pre-trained set of one million word vectors based on the English Wikipedia data in 2017 of 16 billion tokens (denoted as EmbWiki), and the second is a set of five million word vectors trained on our custom crawl of Yahoo! Answers data of five billion tokens (denoted as EmbYA) using the skip-gram algorithm.9 Both sets of vectors are of 300 and 100 dimensions respectively.",1,Wiki,True
228,"Evaluation Metrics. To get a broader understanding to the effectiveness of the proposed method, six evaluation metrics are reported in this study. Top-k effectiveness as the focus of web search is represented by NDCG@10, NDCG@20, P@10, and P@20. The metric MRR, which is widely used in web question answering, is also included. Additionally, we report MAP@100, as our ranking experiment is limited to the top 100 initially retrieved documents. The two-tailed t-test is used for significance testing.",1,ad,True
229,5.2 Baselines,0,,False
230,The following baselines were selected and implemented:,0,,False
231,· Sequential Dependence Model (SDM); · Passage-Based Language Model (MSP and SUM); · Quality-Biased Ranking (QSDM); · External Expansion (EE).,0,,False
232,"Passage-Based Language Models. A passage-based language model is a mixture of three models of the passage pG , the document pD , and the collection pC . The combined model usually takes the following form:",0,,False
233,"p(Q) ,"" t Q [G pG (t ) + D pD (t ) + C pC (t)] ,""",0,,False
234,7https://stanfordnlp.github.io/CoreNLP/ (version 3.8.0) 8 http://yahooanswers.tumblr.com/post/80173794953/important-changes-to-answers 9The custom Yahoo! Answers crawl contains roughly 17 million question-answer pairs submitted to Yahoo! Answers between 2013 and 2016.,1,Yahoo,True
235,339,0,,False
236,Session 3C: Question Answering,1,Session,True
237,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
238,under the constraint that the mixture weights sum to one. The passage model pG and the mixture weights might be implemented slightly differently across methods.,0,,False
239,"Two variants, MSP and SUM, are implemented in this paper. The first model is based on a top-performing variant MSP[length] from Bendersky and Kurland [4]. It locates the best-matching passage G in the document by maximizing the maximum-likelihood estimate pG across a set of candidates, with one key parameter D set by using the document homogeneity estimate h[lenth]. A second approach, called SUM, based on query-biased summarization [14] was shown to be competitive to gradient boosting regression trees. Following the proposed setting [14], the MEAD package [35] is used to implement this method, combining four features: Centroid, Position, Length, and QueryCosine with the default weights.",1,ad,True
240,"Quality-Biased Ranking (QSDM). The quality-biased ranking method [3] is commonly referred to as the state of the art in web document ranking with TREC collections. The method is a linear model that combines the SDM score and ten web document quality features, which are: NumVisTerms, NumTitleTerms, AvgTermLen, FracAnchorText, FracVisText, Entropy (Entropy of the document content), FracStops, StopCover, UrlDepth (depth of the URL path), and FracTableText.",1,TREC,True
241,External Expansion (EE). External Expansion [12] is a standard PRF approach for expanding queries using external corpora based on the Relevance Model [23]. It is generally considered as a strong and effective expansion method when external resources are available.,1,corpora,True
242,5.3 Parameter Estimation,0,,False
243,Parameters for individual baseline methods are tuned as follows:,0,,False
244,"· For passage-based language models (MSP and SUM), the mixture weights are optimized via a grid search over the range {0.00, 0.05, 0.10, . . . , 0.95, 1.00} using cross validation.",0,,False
245,"· For external expansion (EE) the procedure followed closely to the original paper Diaz and Metzler [12]. The number of feedback documents (i.e., CQA answers) was set to ten to align with the data. The number of feedback terms nT , collection model weight C , and the mixture ratio Q with respect to the original query were all learned on the target test collections via 100 rounds of randomized search over randomly re-sampled train/test (50%­50%) query splits.10 In our experiments, (nT , C , Q ) were set to (60, 0.3, 0.2) on GOV2 and to (50, 0.2, 0.2) on CW09B.",1,CW,True
246,Parameters for experimental runs are tuned as follows:,0,,False
247,"· The passage size K is set to fifty words, to be made consistent with the common setting for query-biased summarization [35]. We set  , 0.1 in the extraction of the ILP representation.",1,ad,True
248,"· For QL, we set µ ,"" 100 and for BM25, we set b1 "", 1.2 and k1 ,"" 0.75, based on common settings in adhoc retrieval. Both p(t |C) and id f (t) are estimated on the target collection.""",1,adhoc,True
249,"· For both embedding based estimates EmbWiki and EmbYA, we set  , 10 and x0 , 0 based on cross validation.",1,Wiki,True
250,10This optimization procedure allows the resulting retrieval scores to be included as a feature in our ranking model. The resulting scores are comparable to the procedure proposed by Diaz and Metzler [12].,0,,False
251,"· Our implementation of the DR framework follows the original paper [9]: we encode query and passage vectors using 128 hidden units in three-layer bidirectional LSTMs. The model is trained on the SQuAD dataset [37] using AdaMax [18]. The dropout rate is tuned empirically to 0.5. We use the same set of word embeddings learned from the Y!A data (as with EmbYA), but the effectiveness is roughly comparable to a pre-trained model learned on the Common Crawl data [9].",0,,False
252,"For all methods tested in our experiments, a Coordinate Ascent learning-to-rank algorithm is employed to learn the model weights using ten-fold cross validation, as is commonly practiced in past work [3, 29].11 We used RankLib12 to estimate parameters, which are essentially the weight of each feature. We chose to optimize NDCG@20 throughout the experiments as it gives the best performance in terms of both precision- and recall-oriented metrics.",0,,False
253,6 RESULTS,0,,False
254,"We describe and analyze the effectiveness of ranking using different answer-passage representations: PSG and ILP, as well as passages derived by using open-domain question answering model (DR).",0,,False
255,6.1 Comparisons with Previous Work,0,,False
256,"Our approach is first compared with prior techniques for both test collections, see Table 4. Ten experiments are reported: two representations PSG and ILP with four relevance estimates EmbWiki, EmbYA, QL, and BM25, and the DR framework are tested using title and description queries.",1,Wiki,True
257,"It can be seen that combining SDM with answer-passage quality using all three representations PSG, ILP, and DR, significantly outperforms SDM and the passage-based baselines SDM+MSP and SDM+SUM. While incorporating MSP and SUM shows only marginal benefits over SDM, combining answer-passage quality has seen the biggest effect across all the other methods involved. This provides an answer to RQ1: answer passages can be used to improve ad hoc retrieval in the presence of a strong retrieval baseline SDM, and they can work better than existing passage-based methods.",1,corpora,True
258,"The fact that DR does not provide strong indication of document relevance is unexpected. Among all representations ILP is found to be the most effective, while PSG and DR are roughly comparable. For both SDM+PSG and SDM+ILP, BM25 gives the best results and QL the second, followed by embedding based estimates EmbWiki and EmbYA. The SDM+DR framework works the best on description queries, suggesting that further tuning might be needed for such models to handle non-verbose queries. On both test collections, the best effectiveness is achieved by using SDM+ILP with BM25.",1,Wiki,True
259,6.2 Ad Hoc Retrieval with Passage Quality,0,,False
260,The previous experiment shows that SDM+ILP paired with retrieval function BM25 and QL may have some advantages over strong retrieval model QSDM and SDM+EE. This leads to a further investigation,1,ad,True
261,"11Note that the comparison between ranking algorithms is beyond the scope of this paper. In preliminary experiments, non-linear ranking models such as Gradient Boosted Decision Trees (GBDT) and LambdaMART were also tested, but were found to consistently perform less effectively than Coordinate Ascent on all metrics by a wide margin, suggesting that the ideal response surface is close to a hyperplane, as non-linear models can struggle with this type of ranking problem. 12https://www.lemurproject.org/ranklib.php (version 2.7)",1,ad,True
262,340,0,,False
263,Session 3C: Question Answering,1,Session,True
264,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
265,Table 4: Comparisons with previous methods. Significant differences with respect to SDM/QSDM/SDM+EE are indicated using // for p < 0.05 (or // for p < 0.01). All differences between SDM+PSG/ILP runs and SDM+MSP are significant for p < 0.05.,0,,False
266,GOV2 CW09B,1,CW,True
267,Baseline / Passage Baseline SDM() QSDM() SDM+EE() SDM+MSP SDM+SUM,0,,False
268,Answer-Passage Approach,0,,False
269,SDM+PSG (EmbWiki) SDM+PSG (EmbYA) SDM+PSG (QL) SDM+PSG (BM25) SDM+ILP (EmbWiki) SDM+ILP (EmbYA) SDM+ILP (QL) SDM+ILP (BM25) SDM+DR (Title) SDM+DR (Desc),1,Wiki,True
270,Baseline / Passage Baseline SDM() QSDM() SDM+EE() SDM+MSP SDM+SUM,0,,False
271,Answer-Passage Approach,0,,False
272,SDM+PSG (EmbWiki) SDM+PSG (EmbYA) SDM+PSG (QL) SDM+PSG (BM25) SDM+ILP (EmbWiki) SDM+ILP (EmbYA) SDM+ILP (QL) SDM+ILP (BM25) SDM+DR (Title) SDM+DR (Desc),1,Wiki,True
273,NDCG@10,0,,False
274,0.4769 0.5127 0.5189 0.4826 0.4741,0,,False
275,0.4999 0.5010 0.5085 0.5174 0.5081 0.4983 0.5131 0.5293 0.4821 0.4999,0,,False
276,0.2542 0.2735 0.2880 0.2535 0.2499,0,,False
277,0.2693 0.2752 0.2613 0.2811 0.2843 0.2818 0.3090  0.3115  0.2584 0.2833,0,,False
278,NDCG@20,0,,False
279,0.4751 0.5022 0.5057 0.4745 0.4749,0,,False
280,0.4975 0.4957 0.5068 0.5116 0.4967 0.4951 0.5052 0.5171 0.4786 0.4894,0,,False
281,0.2462 0.2639 0.2736 0.2469 0.2409,0,,False
282,0.2588 0.2644 0.2569 0.2687 0.2652 0.2665 0.2901  0.2955  0.2505 0.2681,0,,False
283,P@10,0,,False
284,0.5694 0.6197 0.6129 0.5782 0.5680,0,,False
285,0.6041 0.6007 0.6102 0.6184 0.6204 0.6075 0.6238 0.6367 0.5735 0.6014,0,,False
286,0.3682 0.3938 0.4021 0.3656 0.3631,0,,False
287,0.3831 0.3856 0.3805 0.3938 0.3954 0.3990 0.4313  0.4379  0.3662 0.3949,0,,False
288,P@20,0,,False
289,0.5469 0.5759 0.5738 0.5422 0.5500,0,,False
290,0.5745 0.5724 0.5823 0.5847 0.5752 0.5779 0.5844 0.5946 0.5480 0.5612,0,,False
291,0.3321 0.3467 0.3590 0.3328 0.3267,0,,False
292,0.3421 0.3479 0.3490 0.3521 0.3392 0.3485 0.3736  0.3787  0.3295 0.3441,0,,False
293,MRR,0,,False
294,0.7763 0.8174 0.8220 0.7696 0.7729,0,,False
295,0.8063 0.8024 0.7991 0.8271 0.8098 0.7900 0.7878 0.8234* 0.7817 0.8038,0,,False
296,0.5010 0.5224 0.5619 0.4989 0.4952,0,,False
297,0.5325 0.5299 0.5222 0.5499 0.5803 0.5579 0.5786  0.5902  0.5298 0.5613,0,,False
298,MAP@100,1,MAP,True
299,0.1802 0.1919 0.1879 0.1805 0.1805,0,,False
300,0.1888 0.1888 0.1929 0.1946 0.1892 0.1876 0.1964 0.2009  0.1811 0.1842 ,0,,False
301,0.1053 0.1094 0.1136 0.1054 0.1047,0,,False
302,0.1058 0.1103 0.1087 0.1113 0.1070 0.1092 0.1164  0.1209  0.1050 0.1094,0,,False
303,"regarding improvements over strong retrieval models. We next incorporate passage quality features into an expanded set of retrieval models, using the ILP representation together with BM25 and EmbYA relevance estimates. For the choice of base models, we used SDM, QSDM, and QSDM+EE, with the latter being a novel and strong combination of quality-biased ranking and external expansion.",1,corpora,True
304,"The results (Table 5) show three rows in each collection for each base model. Incorporating ILP significantly improves SDM for all metrics, across collections. On GOV2, BM25 improves over QSDM for NDCG@10, NDCG@20 and MAP@100. On CW09B, using BM25 leads to significant increases over QSDM for all metrics. For QSDM+EE, significant increases were observed on P@10, P@20, and MAP@100 on the GOV2 data using BM25, and CW09B runs show a similar trend but with a more pronounced effect. We conclude that RQ2",1,corpora,True
305,"is answered: incorporating answer-passage quality can significantly improve ad hoc retrieval in general, but as the base system improves, further gains are likely to get smaller.",1,corpora,True
306,6.3 Combining Multiple Representations,0,,False
307,"Next, two answer-passage representations are involved in the ranking process. Denoted as Combined, this new experimental run effectively leverages passage-level evidence from two representations learned by using different methodologies. The aim is to understand whether quality estimates derived from different representations provide similar effects to document ranking.",0,,False
308,"For this experiment, we incorporate answer-passage quality estimates from both representations ILP (BM25) and DR (Desc) into the",1,corpora,True
309,341,0,,False
310,Session 3C: Question Answering,1,Session,True
311,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
312,Table 5: Retrieval effectiveness of ranking models using quality estimates of answer-biased summaries. Significant differences with respect to baselines SDM/QSDM/QSDM+EE are indicated using // for p < 0.05 (or // for p < 0.01).,0,,False
313,GOV2 CW09B,1,CW,True
314,SDM() SDM+ILP (EmbYA) SDM+ILP (BM25),0,,False
315,QSDM() QSDM+ILP (EmbYA) QSDM+ILP (BM25),0,,False
316,QSDM+EE() QSDM+EE+ILP (EmbYA) QSDM+EE+ILP (BM25),0,,False
317,SDM() SDM+ILP (EmbYA) SDM+ILP (BM25),0,,False
318,QSDM() QSDM+ILP (EmbYA) QSDM+ILP (BM25),0,,False
319,QSDM+EE() QSDM+EE+ILP (EmbYA) QSDM+EE+ILP (BM25),0,,False
320,NDCG@10,0,,False
321,0.4769 0.4983 0.5293,0,,False
322,0.5127 0.5197 0.5412 ,0,,False
323,0.5339  0.5329  0.5442 ,0,,False
324,0.2542 0.2818 0.3115 ,0,,False
325,0.2735 0.2853 0.3107 ,0,,False
326,0.2985  0.3042  0.3194 ,0,,False
327,NDCG@20,0,,False
328,0.4751 0.4951 0.5171,0,,False
329,0.5022 0.5126 0.5245 ,0,,False
330,0.5213  0.5208  0.5311 ,0,,False
331,0.2462 0.2665 0.2955 ,0,,False
332,0.2639 0.2691 0.2959 ,0,,False
333,0.2819+ 0.2864  0.3015 ,0,,False
334,P@10,0,,False
335,0.5694 0.6075 0.6367,0,,False
336,0.6197 0.6238 0.6463,0,,False
337,0.6374 0.6429 0.6605 ,0,,False
338,0.3682 0.3990 0.4379 ,0,,False
339,0.3938 0.3923 0.4333 ,0,,False
340,0.4056 0.4174 0.4338 ,0,,False
341,P@20,0,,False
342,0.5469 0.5779 0.5946,0,,False
343,0.5759 0.5874 0.5939,0,,False
344,0.5901 0.5959  0.6082 ,0,,False
345,0.3321 0.3485 0.3787 ,0,,False
346,0.3467 0.3485 0.3774 ,0,,False
347,0.3610 0.3679  0.3826 ,0,,False
348,MRR,0,,False
349,0.7763 0.7900 0.8234,0,,False
350,0.8174 0.8258 0.8338,0,,False
351,0.8416 0.8044 0.8407,0,,False
352,0.5010 0.5579 0.5902 ,0,,False
353,0.5224 0.5566 0.6002 ,0,,False
354,0.5799  0.5881  0.6138 ,0,,False
355,MAP@100,1,MAP,True
356,0.1802 0.1876  0.2009 ,0,,False
357,0.1919 0.1891 0.2007 ,0,,False
358,0.1948 0.1947 0.1996 ,0,,False
359,0.1053 0.1092 0.1209 ,0,,False
360,0.1094 0.1109 0.1190 ,0,,False
361,0.1148  0.1169  0.1210 ,0,,False
362,Table 6: Combining ILP and DR significantly improves QSDM (significant differences are indicated using  for p < 0.05 or  for p < 0.01).,0,,False
363,GOV2 CW09B,1,CW,True
364,QSDM() QSDM+Combined,0,,False
365,QSDM() QSDM+Combined,0,,False
366,N@20,0,,False
367,0.5022 0.5280,0,,False
368,0.2639 0.2896,0,,False
369,P@20,0,,False
370,0.5759 0.6007,0,,False
371,0.3467 0.3656,0,,False
372,MAP@100,1,MAP,True
373,0.1919 0.1972,0,,False
374,0.1094 0.1166,0,,False
375,"QSDM run. The results of these experiments are shown in Table 6. The Combined method produces strong retrieval runs, but not significantly better than just incorporating ILP. However, QSDM+Combined significantly outperforms QSDM on both collections, and across all metrics except MRR and MAP@100 on GOV2. Regarding the answer to RQ3, we conclude that there is some evidence to support the claim that the use of multiple representations will lead to a stronger ranking model.",1,corpora,True
376,6.4 Feature Importance,0,,False
377,"An ablation analysis was conducted on the run QSDM+EE+ILP (BM25) with twenty one features in total, to examine the relative feature importance. The top seven features for each collection are shown in Table 8, ordered by decreasing difference of NDCG@20 score after removing a feature. NDCG@20 is used as an ordering criterion following our optimization metric in the main experiment. The letter P in square brackets indicates passage-level quality features.",0,,False
378,"SDM remains the most important feature across collections. Some differences between collections can be seen based on the relative importance of features. Our passage quality features AvgWordWeight, QueryOverlap, and FracStop[P] appear more effective on GOV2. On CW09B, PassageScore, StopCover[P], and AvgTermLen are among the top-ranked features. Note that PassageScore and EE also show high importance on CW09B, indicating the usefulness of external CQA resources in ad hoc retrieval.",1,Query,True
379,6.5 Lack of CQA Resources,0,,False
380,"To investigate to what extent the ranking effectiveness changes when the coverage of good related CQA answers is not guaranteed, we conduct an experiment using the related answers obtained from the offline Yahoo! Webscope L6 collection13 using a mixture approach [49].14 Note that this dataset was collected prior to the creation of CW09B, so the respective TREC query topics are less likely to have direct answers in the data.",1,Yahoo,True
381,"Our results (Table 9) suggest a decrease of up to 2.6% on GOV2 and 3% on CW09B compared to the result of using the related CQA answers obtained from Yahoo! Answers (Y!A) search engine (see Table 5). The implication is that a good coverage of related answers is crucial in the generation of answer passages, that are primarily supported by the size of collection and the human efforts involved in curating the best answers.",1,CW,True
382,It is worth noting that using an offline resource with limited coverage of related CQA answers still leads to a significant improvement in NDCG@20 and P@20 on the CW09B collection. This,1,ad,True
383,"13 http://webscope.sandbox.yahoo.com/ 14The weight of SDM retrieval score for question title, question body, and best answer fields are respectively set to 0.5, 0.2, and 0.3 based [49].",0,,False
384,342,0,,False
385,Session 3C: Question Answering,1,Session,True
386,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
387,"Table 7: Answer passages extracted from a top-ranked relevant document clueweb09-enwp01-16-17964 for TREC Web Topic 65, "" Find information and resources on the Korean language."" (query: korean language)",1,TREC,True
388,ILP (EmbYA),0,,False
389,"For example, different endings are used based on whether the subjects and listeners are friends, parents, or honoured persons. in a similar way European languages borrow from Latin and Greek. Its use limited some cases and the aristocracy prefers Classical Chinese for its writing. ""Mortal enemy"" and ""head of state"" are homophones in the South. Learn to read, write and pronounce Korean",1,ad,True
390,ILP (BM25),0,,False
391,"Yanbian (People's Republic of China) Given this, it is sometimes hard to tell which actual phonemes are present in a certain word. Unlike most of the European languages, Korean does not conjugate verbs using agreement with the subject, and nouns have no gender. The Korean language used in the North and the South exhibits differences in pronunciation, spelling, grammar and vocabulary.",0,,False
392,DR (Desc),0,,False
393,"Korean is similar to Altaic languages in that they both lack certain grammatical elements, including number, gender, articles, fusional morphology, voice, and relative pronouns (Kim Namkil). Korean especially bears some morphological resemblance to some languages of the Northern Turkic group, namely Sakha (Yakut).",0,,False
394,Table 8: Results of ablation study to determine feature importance for both test collections.,0,,False
395,GOV2,0,,False
396,CW09B,1,CW,True
397,Feature,0,,False
398,Diff. Feature,0,,False
399,Diff.,0,,False
400,SDM FracStop AvgWordWeight UrlDepth QueryOverlap FracAnchorText FracStop[P],1,Query,True
401,0.0306 0.0101 0.0076 0.0063 0.0052 0.0049 0.0048,0,,False
402,SDM StopCover PassageScore FracVisText EE StopCover[P] AvgTermLen,0,,False
403,0.0223 0.0092 0.0086 0.0077 0.0076 0.0046 0.0038,0,,False
404,Table 9: An investigation of using external CQA resources from offline collection. Significant differences with respect to QSDM are indicated using  for p < 0.05 (or  for p < 0.01).,0,,False
405,GOV2 CW09B,1,CW,True
406,QSDM() QSDM+ILP (BM25),0,,False
407,QSDM() QSDM+ILP (BM25),0,,False
408,N@20,0,,False
409,0.5022 0.5083,0,,False
410,0.2639 0.2804,0,,False
411,P@20,0,,False
412,0.5759 0.5759,0,,False
413,0.3467 0.3679,0,,False
414,MAP@100,1,MAP,True
415,0.1919 0.1926,0,,False
416,0.1094 0.1136,0,,False
417,"is in line with one previous study [51] on exploiting CQA resources for non-factoid question answering, which shows modest improvements in the quality of produced answers even when no CQA answer exactly matches the queries. We note that, in the case where CQA answers are not available for a particular query, a live system can simply back off to a retrieval mode that does not incorporate such evidence, as the lack of appropriate CQA resources is clearly indicated through an empty results list.",1,corpora,True
418,7 DISCUSSION,0,,False
419,"The results in the previous sections provide strong empirical evidence to support the validity of the answer-bearingness hypothesis, and also directly support the recurring argument in previous work [4, 8, 14, 22, 40, 46] that passage-level evidence can benefit retrieval",0,,False
420,"effectiveness. It is however surprising that, the open-domain question answering model shows little benefit in extracting answerbearing passages for document ranking. This may be due to task mismatch (i.e. model trained to detect factoids) or the lack of appropriate training instances. Word embeddings learned on the CQA data are arguably useful for the task, but simpler methodologies appear to win on the overall efficacy.",0,,False
421,"The ILP representation benefits the most from the inclusion of passage quality estimates, which we suspect is due to the fact that summaries are more likely to cover broken sentences on nonrelevant documents. The compressive nature forces summaries to include all textual evidence that seems relevant, but when such evidence is scarce the quality can be poor.",0,,False
422,"For illustrative purposes, some example answer passages produced by using ILP and DR are also given in Table 7. The answer passages are extracted from a top-ranked relevant document for a randomly sampled query topic. We note that the extraction algorithms tend to capture a broader range of answers when the underlying document is relevant, as is shown in the example. While the captured evidence may differ in terms of efficacy for document ranking, it remains an open question how this difference correlates with users' perception towards the answer-passage quality.",1,ad,True
423,8 CONCLUSIONS AND FUTURE WORK,0,,False
424,"In this paper, we proposed quality-biased ranking that incorporates signals from passages that are likely to bear answers. A new approach that exploits external resources in the creation of such passages is developed to induce high-quality sub-document representations, called answer passages, from the retrieved documents. We developed a set of methodologies to improve term relevance estimates and extract answer passages. A range of quality features is extracted from the generated passages, and blended into the ranking model, which leads to improved effectiveness: our experiments on two web collections showed that this approach is more effective than passage-based methods and external expansion, and can significantly improve on state-of-the-art ranking models SDM and QSDM. Signals from multiple representations can also be combined to improve ranking effectiveness. A final ranking model that combines all these quality estimates achieved significant effectiveness improvements on GOV2 and ClueWeb09B.",1,corpora,True
425,343,0,,False
426,Session 3C: Question Answering,1,Session,True
427,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
428,In future work we plan to conduct a user study to gain an under-,0,,False
429,"standing of human perceptions of the quality of answer passages,",0,,False
430,and of the improvement in document ranking. A promising new,0,,False
431,"approach to entity representation has recently been published [48],",0,,False
432,which approaches the problem of ranking from an angle orthogonal,0,,False
433,to our work. We plan to explore a combination of such approaches,0,,False
434,in future. We will look to examine possible improvements to our,0,,False
435,"ranking model, such as using link-based features [20], user behavior",0,,False
436,"signals [1], and filtering CQA answers based on their quality [41].",0,,False
437,9 ACKNOWLEDGMENTS,0,,False
438,This research is supported in part by the Australian Research Coun-,0,,False
439,cil (DP140102655 and DP170102726) and the Indonesia Endowment,0,,False
440,Fund for Education (LPDP). The authors want to thank Marwan,0,,False
441,Torki for kindly sharing his CQA research data.,0,,False
442,REFERENCES,0,,False
443,"[1] Eugene Agichtein, Eric Brill, and Susan Dumais. 2006. Improving web search ranking by incorporating user behavior information. In Proc. of SIGIR. ACM, 19­26.",1,corpora,True
444,"[2] Gianni Amati and Cornelis Joost van Rijsbergen. 2002. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357­389.",0,,False
445,"[3] Michael Bendersky, W. Bruce Croft, and Yanlei Diao. 2011. Quality-biased Ranking of Web Documents. In Proc. of WSDM. ACM, 95­104.",0,,False
446,"[4] Michael Bendersky and Oren Kurland. 2008. Utilizing passage-based language models for document retrieval. In Proc. of ECIR. Springer, 162­174.",0,,False
447,"[5] Michael Bendersky, Donald Metzler, and W. Bruce Croft. 2010. Learning Concept Importance Using a Weighted Dependence Model. In Proc. of WSDM. ACM, 31­40.",0,,False
448,"[6] Jiang Bian, Yandong Liu, Eugene Agichtein, and Hongyuan Zha. 2008. Finding the right facts in the crowd: factoid question answering over social media. In Proc. of WWW. ACM, 467­476.",0,,False
449,"[7] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606 (2016).",0,,False
450,"[8] James P. Callan. 1994. Passage-level Evidence in Document Retrieval. In Proc. of SIGIR. Springer-Verlag New York, Inc., 302­310.",0,,False
451,"[9] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to Answer Open-Domain Questions. In Proc. of ACL. Association for Computational Linguistics, 1870­1879.",1,ad,True
452,"[10] Gordon V. Cormack, Mark D. Smucker, and Charles L. Clarke. 2011. Efficient and Effective Spam Filtering and Re-ranking for Large Web Datasets. Inf. Retr. 14, 5 (Oct. 2011), 441­465.",0,,False
453,"[11] W Bruce Croft. 2002. Combining approaches to information retrieval. In Proc. of ECIR. Springer, 1­36.",0,,False
454,"[12] Fernando Diaz and Donald Metzler. 2006. Improving the Estimation of Relevance Models Using Large External Corpora. In Proc. of SIGIR. ACM, 154­161.",0,,False
455,"[13] Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing. Association for Computational Linguistics, 10­18.",0,,False
456,"[14] Jing He, Pablo Duboue, and Jian-Yun Nie. 2012. Bridging the Gap between Intrinsic and Perceived Relevance in Snippet Generation. In Proc. of COLING. 1129­1146.",0,,False
457,"[15] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems. 1693­1701.",1,ad,True
458,"[16] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. 20, 4 (2002), 422­446.",0,,False
459,"[17] Mostafa Keikha, Jae Hyun Park, and W Bruce Croft. 2014. Evaluating answer passages using summarization measures. In Proc. of SIGIR. ACM, 963­966.",0,,False
460,[18] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).,0,,False
461,"[19] Eyal Krikon and Oren Kurland. 2011. A study of the integration of passage-, document-, and cluster-based information for re-ranking search results. Information Retrieval 14, 6 (2011), 593­616.",0,,False
462,"[20] Oren Kurland and Lillian Lee. 2010. PageRank without hyperlinks: Structural reranking using links induced by language models. ACM TOIS 28, 4 (2010), 18.",0,,False
463,"[21] Saar Kuzi, Anna Shtok, and Oren Kurland. 2016. Query expansion using word embeddings. In Proc. of CIKM. ACM, 1929­1932.",1,Query,True
464,"[22] Adenike M. Lam-Adesina and Gareth J. F. Jones. 2001. Applying Summarization Techniques for Term Selection in Relevance Feedback. In Proc. of SIGIR. ACM, 1­9.",0,,False
465,"[23] Victor Lavrenko and W Bruce Croft. 2001. Relevance based language models. In Proc. of SIGIR. ACM, 120­127.",0,,False
466,"[24] Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Proc. of HLT/NAACL. Association for Computational Linguistics, 912­920.",0,,False
467,"[25] Qiaoling Liu, Eugene Agichtein, Gideon Dror, Evgeniy Gabrilovich, Yoelle Maarek, Dan Pelleg, and Idan Szpektor. 2011. Predicting web searcher satisfaction with existing community-based answers. In Proc. of SIGIR. ACM, 415­424.",0,,False
468,"[26] Yandong Liu, Jiang Bian, and Eugene Agichtein. 2008. Predicting information seeker satisfaction in community question answering. In Proc. of SIGIR. ACM, 483­490.",0,,False
469,"[27] Craig Macdonald, Rodrygo L.T. Santos, and Iadh Ounis. 2012. On the Usefulness of Query Features for Learning to Rank. In Proc. of CIKM. ACM, 2559­2562.",1,ad,True
470,"[28] Edgar Meij and Maarten de Rijke. 2010. Supervised query modeling using wikipedia. In Proc. of SIGIR. ACM, 875­876.",1,wiki,True
471,"[29] Donald Metzler and W. Bruce Croft. 2005. A Markov Random Field Model for Term Dependencies. In Proc. of SIGIR. ACM, 472­479.",0,,False
472,[30] Donald Metzler and Tapas Kanungo. 2008. Machine Learned Sentence Selection Strategies for Query-Biased Summarization. In SIGIR Learning to Rank Workshop.,1,Query,True
473,[31] Bhaskar Mitra and Nick Craswell. 2017. Neural Models for Information Retrieval. arXiv preprint arXiv:1705.01509 (2017).,0,,False
474,"[32] Alistair Moffat and Justin Zobel. 2008. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst. 27, 1 (2008), 2.",0,,False
475,"[33] John O'Connor. 1980. Answer-passage retrieval by text searching. Journal of the Association for Information Science and Technology 31, 4 (1980), 227­239.",0,,False
476,"[34] Jay M Ponte and W Bruce Croft. 1998. A language modeling approach to information retrieval. In Proc. of SIGIR. ACM, 275­281.",0,,False
477,"[35] Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda Çelebi, Stanko Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio Saggion, Simone Teufel, Michael Topper, Adam Winkel, and Zhu Zhang. 2004. MEAD -- A platform for multidocument multilingual text summarization. In Proc. of LREC.",1,ad,True
478,"[36] Fiana Raiber and Oren Kurland. 2013. Ranking document clusters using markov random fields. In Proc. of SIGIR. ACM, 333­342.",0,,False
479,"[37] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016).",1,ad,True
480,"[38] Stephen E Robertson. 1997. Overview of the okapi projects. Journal of Documentation 53, 1 (1997), 3­7.",0,,False
481,"[39] Joseph John Rocchio. 1971. Relevance feedback in information retrieval. The SMART retrieval system: experiments in automatic document processing (1971), 313­323.",0,,False
482,"[40] Tetsuya Sakai and Karen Sparck-Jones. 2001. Generic Summaries for Indexing in Information Retrieval. In Proc. of SIGIR. ACM, 190­198.",0,,False
483,"[41] Chirag Shah and Jefferey Pomerantz. 2010. Evaluating and predicting answer quality in community QA. In Proc. of SIGIR. ACM, 411­418.",0,,False
484,"[42] Hiroya Takamura and Manabu Okumura. 2009. Text summarization model based on maximum coverage problem and its variant. In Proc. of EACL. Association for Computational Linguistics, 781­789.",0,,False
485,"[43] Anastasios Tombros and Mark Sanderson. 1998. Advantages of Query Biased Summaries in Information Retrieval. In Proc. of SIGIR. ACM, 2­10.",1,Query,True
486,"[44] Ingmar Weber, Antti Ukkonen, and Aris Gionis. 2012. Answers, not links: extracting tips from yahoo! answers to address how-to web queries. In Proc. of WSDM. ACM, 613­622.",1,ad,True
487,"[45] Wouter Weerkamp, Krisztian Balog, and Maarten de Rijke. 2012. Exploiting External Collections for Query Expansion. ACM Trans. Web 6, 4 (2012), 1­29.",1,Query,True
488,"[46] Ross Wilkinson. 1994. Effective Retrieval of Structured Documents. In Proc. of SIGIR. Springer-Verlag New York, Inc., 311­317.",0,,False
489,"[47] Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proc. of EMNLP. Association for Computational Linguistics, 233­243.",0,,False
490,"[48] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2017. Word-Entity Duet Representations for Document Ranking. In Proc. of SIGIR. ACM, 763­772.",0,,False
491,"[49] Xiaobing Xue, Jiwoon Jeon, and W Bruce Croft. 2008. Retrieval models for question and answer archives. In Proc. of SIGIR. ACM, 475­482.",0,,False
492,"[50] Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and Juanzi Li. 2011. Social context summarization. In Proc. of SIGIR. ACM, 255­264.",0,,False
493,"[51] Evi Yulianti, Ruey-Cheng Chen, Falk Scholer, W. Bruce Croft, and Mark Sanderson. 2018. Document summarization for answering non-factoid queries. IEEE Trans. Knowl. Data Eng. 30, 1 (2018), 15­28.",0,,False
494,"[52] Hamed Zamani and W Bruce Croft. 2016. Embedding-based query language models. In Proc. of ICTIR. ACM, 147­156.",0,,False
495,"[53] Chengxiang Zhai and John Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Trans. Inf. Syst. 22, 2 (2004), 179­214.",0,,False
496,344,0,,False
497,,0,,False

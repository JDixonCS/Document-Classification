,sentence,label,data,regex
0,Short Research Papers II,0,,False
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Testing the Cluster Hypothesis with Focused and Graded Relevance Judgments,1,ad,True
3,Eilon Sheetrit,0,,False
4,Technion -- Israel Institute of Technology seilon@campus.technion.ac.il,0,,False
5,Oren Kurland,0,,False
6,Technion -- Israel Institute of Technology kurland@ie.technion.ac.il,0,,False
7,ABSTRACT,0,,False
8,"The cluster hypothesis is a fundamental concept in ad hoc retrieval. Heretofore, cluster hypothesis tests were applied to documents using binary relevance judgments. We present novel tests that utilize graded and focused relevance judgments; the latter are markups of relevant text in relevant documents. Empirical exploration reveals that the cluster hypothesis holds not only for documents, but also for passages, as measured by the proposed tests. Furthermore, the hypothesis holds to a higher extent for highly relevant documents and for those that contain a high fraction of relevant text.",1,ad,True
9,"ACM Reference Format: Eilon Sheetrit, Anna Shtok, Oren Kurland, and Igal Shprincis. 2018. Testing the Cluster Hypothesis with Focused and Graded Relevance Judgments. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210120",1,ad,True
10,1 INTRODUCTION,1,DUC,True
11,"The cluster hypothesis is: ""closely associated documents tend to be relevant to the same requests"" [9, 22]. Several cluster hypothesis tests were proposed [5, 9, 20, 23]. Most of these measure similarities between relevant documents with respect to their similarities with non-relevant documents using various similarity estimates [18]1.",0,,False
12,"Cluster hypothesis tests operate on documents and utilize binary relevance judgments2. A relevant document can contain (much) non-relevant information which could affect inter-document similarity estimates, and hence, cluster hypothesis tests3. Furthermore, to address the drawbacks of retrieving relevant documents with much non-relevant information, passages (instead of documents)",1,ad,True
13,"1One of these measures [18] is based on a ""reversed"" cluster hypothesis: documents should be deemed similar if they are co-relevant to the same queries [7]. 2An exception is a cluster hypothesis test applied to entities [19]. 3Some work uses passage-based information to induce inter-document similarity measures so as to address this issue [12, 18].",1,ad,True
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210120",1,ad,True
15,Anna Shtok,0,,False
16,Technion -- Israel Institute of Technology annabel@technion.ac.il,0,,False
17,Igal Shprincis,0,,False
18,"Microsoft, Herzliya, Israel igals@microsoft.com",0,,False
19,"can be retrieved (e.g., [1­3, 6, 8, 10, 13]). However, cluster hypothesis tests were not reported for passage (focused) retrieval.",0,,False
20,"We present novel cluster hypothesis tests that utilize graded and focused relevance judgments [1, 8]; the latter are markups of relevant text in relevant documents. We apply the tests for both documents and passages. We set out to perform a finer analysis than that enabled by using binary relevance judgments. Our tests account for the extent to which a text item (passage or document) is deemed relevant as a whole (i.e., relevance grade), or the amount of relevant text it contains (i.e., focused relevance judgments).",1,ad,True
21,"The proposed tests consider the nearest neighbors of an item (document or passage) in the similarity space. The motivation to use a nearest-neighbor-based test, following Voorhees' cluster hypothesis test which utilizes binary relevance judgments for documents [23], is two-fold. First, the results of this nearest-neighbor-based test [23] were shown to be correlated to some extent, for certain retrieval settings, with the relative effectiveness of using inter-document similarities for retrieval [14, 17]. Second, using nearest-neighbor-based clusters for document retrieval was shown to be highly effective with respect to using other clustering techniques [11, 16].",0,,False
22,"Applying the proposed cluster hypothesis tests over INEX and TREC datasets reveals the following. The cluster hypothesis holds, as measured using the tests, not only for documents but also for passages. Furthermore, the higher the fraction of relevant text an item (passage or document) contains, the more similar it is to other relevant items; specifically, those with a high fraction of relevant text. Finally, documents marked as highly relevant are more similar to other relevant documents (specifically, highly relevant documents) than relevant documents not marked as highly relevant.",1,INEX,True
23,"In summary, our main contributions are cluster hypothesis tests that utilize graded and focused relevance judgments, and showing that the cluster hypothesis holds also for passages.",1,ad,True
24,2 TESTING THE CLUSTER HYPOTHESIS,0,,False
25,"Our goal is to test the cluster hypothesis for documents and passages, henceforth referred to as items, using, when available, graded or focused relevance judgments. As in past work [15, 17, 21], we perform the test on a list of items retrieved for a query. We use language-model-based retrieval and inter-item similarity estimates.",1,ad,True
26,2.1 Document and Passage Retrieval,0,,False
27,"Let q, , d, and D denote a query, a passage, a document and a corpus of documents, respectively. We measure textual similarities using",0,,False
28,1173,0,,False
29,Short Research Papers II,0,,False
30,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
31,"cross entropy (CE) [24]: Sim(x, y) d,ef exp(-CE(pxMLE (·) || pyDir (·)); pxMLE (·) is the maximum likelihood estimate induced from x; pyDir (·) is the Dirichlet smoothed language model induced from y.",0,,False
32,We use L to denote a list of n items (documents or passages),0,,False
33,"most highly ranked with respect to q. The document list, Ldoc , is",0,,False
34,retrieved using a standard language-model-based approach [24]:,0,,False
35,"document d in the corpus is scored by Sim(q, d). The passage list,",0,,False
36,"Lps, is created by ranking all passages of documents in Ldoc .",0,,False
37,"Specifically, passage  in document d ( Ldoc ) is scored by [2, 3]:",0,,False
38,(1 - ),0,,False
39,Sim,0,,False
40, Lp s ,0,,False
41,"(q,  ) S i m (q,",0,,False
42,),0,,False
43,+,0,,False
44,"Sim(q,d ) dLdoc S im(q,d)",0,,False
45,;,0,,False
46,is,0,,False
47,a,0,,False
48,free,0,,False
49,parameter.,0,,False
50,We apply cluster hypothesis tests either to Ldoc or to Lps. The,0,,False
51,tests rely on analyzing the nearest-neighbors of an item x in the,0,,False
52,"list L it is part of: N N (x, L; k) is the set of k items y in L (y x)",0,,False
53,"that yield the highest Sim(x, y).",0,,False
54,2.2 Nearest-Neighbor-Based Tests,0,,False
55,"Inspired by Voorhees' nearest-neighbor cluster hypothesis test [23], we present a suite of novel tests. The tests account for the relevance degree of items: their relevance grade (binary or graded relevance judgments), or the fraction of query-pertaining text they contain (focused relevance judgments [1, 8]). Thus, the tests help to explore how items of low and high relevance degrees are situated in the similarity space with respect to each other.",1,ad,True
56,"A test is performed for query q with respect to a retrieved item list L. As a first step, we define the seed set S ( L); seed items are those whose relevance degree is at least  which is a free parameter. For each seed x in S we compute the average relevance degree of its nearest neighbors in N N (x, L; k). The values attained for the seeds in S are averaged to yield a test value for query q. We report the average over queries of a per-query test value.",0,,False
57,"We use A-B to name a cluster hypothesis test where A is the type of relevance degree used for selecting seeds and B is the type used to quantify relevance degrees of the seeds' neighbors. A and B, which can differ, can be: (i) Binary. The relevance degree of an item is 0 (not relevant) or 1 (relevant) based on binary relevance judgments; for seed selection:  , 1. (ii) Focused. The relevance degree of an item is the fraction of relevant text it contains as measured using focused relevance judgments; for seed selection:  ,"" 0+ (i.e., a seed is an item with relevance degree > 0). (iii) Graded. The relevance degree of an item is its graded relevance judgment: 0 (not relevant), 1 (relevant), 2 (highly relevant); for seed selection:  "", 1 or  , 2.",1,ad,True
58,"The Binary-Binary cluster hypothesis test, where the test value is the average number of relevant items among the nearest neighbors of a relevant item, is Voorhees' original test which was applied to documents [23]. All other tests are novel to this study.",0,,False
59,3 EXPERIMENTAL SETUP,0,,False
60,"The datasets used for experiments are specified in Table 1. The INEX dataset contains English Wikipedia articles which were flattened by removing all XML markups. It was used for the focused retrieval tracks in 2009 and 2010 [1, 8] and includes binary document-level, and focused, relevance judgments. That is, annotators marked every piece of relevant text in a relevant document. The fraction of text, measured in characters, in an item (document or passage) that was marked relevant is the item's focused relevance degree.",1,INEX,True
61,"For the TREC datasets (GOV2 and ClueWeb) there are graded relevance judgments at the document level, but no passage-level nor focused relevance judgments. The 0/1/2 graded relevance degrees we use (see Section 2.2) were set as follows. For GOV2 queries, and for ClueWeb queries from TREC 2009, we used the given relevance grades 0/1/2 as relevance degrees. For ClueWeb queries from TREC 2010, 2011 and 2012, additional relevance grades are available: ""key"" and ""nav"". We assign a relevance degree 1 to documents marked as ""nav"" (navigation) as the relevance description of ""nav"" does not directly translate to ""highly relevant"". Documents marked as ""key"" are assigned a relevance degree 2 following TREC's description (https://plg.uwaterloo.ca/~trecweb/2011.html). The motivation for ""folding"" the relevance grades of the years 2010­2012 to 0/1/2 relevance degrees is two-fold. There are relatively very few documents whose relevance labels are ""nav"" and ""key""; hence, using them as is will bias the cluster hypothesis tests. Second, given the differences of labeling schemes for ClueWeb across the years, using the original relevance grades as relevance degrees would have required addressing the query set of each year separately. This would have resulted in sparsifying the data used to perform the cluster hypothesis tests.",1,TREC,True
62,"Titles of topics were used as (short) queries. (Queries with no relevant documents in the qrels files were removed.) Krovetz stemming was applied to queries and documents. Stopwords on the INQUERY list were removed only from queries. The two-tailed paired permutation test with p  0.05 was used to determine statistically significant differences of cluster hypothesis test results. We apply Bonferroni correction for multiple comparisons where needed. We used non-overlapping fixed-length windows of 50, 150 and 300 terms for passages; using non-overlapping passages was a requirement in INEX's focused retrieval tracks [1, 8]. The Indri toolkit was used for experiments (www.lemurproject.org).",1,INEX,True
63,"Document and passage retrieval were performed as described in Section 2.1. For passage retrieval, the passages in the 1000 most highly ranked documents were ranked. For ClueWeb we removed from the document ranking documents with a Waterloo's spam classifier score below 50 [4]. The Dirichlet smoothing parameter, µ, was set to 1000 [24] for document retrieval and for measuring inter-item similarities (see Section 2.1); µ was set to values in {500, 1500, 2500} for passage retrieval. The value of  for passage retrieval was in {0.1, 0.2, . . . , 0.9}. To set the values of µ and  which affect passage retrieval, we used leave-one-out cross validation; MAiP@1500, the mean (over 101 standard recall points) interpolated average precision computed for the top-1500 passages, was used to optimize passage retrieval performance over the train folds following the suggested practice in INEX's focused retrieval tracks [1, 8].",1,ClueWeb,True
64,"We applied the cluster hypothesis tests to the document (Ldoc ) and passage (Lps) lists with the number of nearest neighbors, k, in {4, 9}, and the number of items in a list, n, in {50, 100, 150, 200}.",0,,False
65,4 EXPERIMENTAL RESULTS,0,,False
66,"Binary-Binary tests. We first study the cluster hypothesis for passages. For reference, we report the test results for documents.",0,,False
67,"We use the INEX dataset, which in contrast to the TREC datasets includes focused relevance judgments, and therefore allows to assign relevance degrees to passages. For both seed selection and quantification of the relevance-degree of nearest neighbors, we use",1,INEX,True
68,1174,0,,False
69,Short Research Papers II,0,,False
70,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
71,Table 1: Datasets used for experiments.,0,,False
72,Corpus # of docs,0,,False
73,Data,0,,False
74,Avg doc. length,0,,False
75,Queries,0,,False
76,"INEX 2,666,190",1,INEX,True
77,2009&2010,0,,False
78,552,0,,False
79,"2009001-2009115, 2010001-2010107",0,,False
80,"GOV2 25,205,179",0,,False
81,GOV2,0,,False
82,930,0,,False
83,701-850,0,,False
84,"ClueWeb 50,220,423 ClueWeb09 (Category B)",1,ClueWeb,True
85,807,0,,False
86,1-200,0,,False
87,"Table 2: Binary-Binary test (""bb"") over INEX for passages of different lengths and documents. ""rand"": the expected test value if neighbors are randomly selected rather than via inter-item similarities as in ""bb"". ""ratio"": the (average over queries) ratio between ""bb"" and ""rand"". Underline: the best result in a row. Boldface: the highest ratio per n (list size).",1,INEX,True
88,"n , 50",0,,False
89,"n , 100",0,,False
90,"n , 150",0,,False
91,"n , 200",0,,False
92,item k bb rand ratio bb rand ratio bb rand ratio bb rand ratio,0,,False
93,Psg50,0,,False
94,4 .680 .520 1.95 .690 .457 2.61 .689 .402 2.74 .699 .377 2.78 9 .646 .520 1.67 .656 .457 2.14 .648 .402 2.39 .659 .377 2.49,0,,False
95,Psg150,0,,False
96,4 9,0,,False
97,.698 .642,0,,False
98,.433 .433,0,,False
99,2.26 1.85,0,,False
100,.710 .664,0,,False
101,.377 .377,0,,False
102,2.57 2.31,0,,False
103,.705 .660,0,,False
104,.339 .339,0,,False
105,2.97 2.67,0,,False
106,.696 .651,0,,False
107,.310 .310,0,,False
108,3.27 2.95,0,,False
109,Psg300,0,,False
110,4 9,0,,False
111,.699 .652,0,,False
112,.408 .408,0,,False
113,2.10 1.92,0,,False
114,.680 .637,0,,False
115,.340 .340,0,,False
116,2.65 2.43,0,,False
117,.672 .629,0,,False
118,.301 .301,0,,False
119,3.19 2.93,0,,False
120,.659 .612,0,,False
121,.270 .270,0,,False
122,3.69 3.34,0,,False
123,Doc,0,,False
124,4 .662 .387 2.28 .625 .306 3.27 .604 .260 4.20 .584 .227 5.07 9 .591 .387 1.90 .563 .306 2.63 .539 .260 3.28 .520 .227 3.93,0,,False
125,"the Binary regime so as to follow previous work on testing the cluster hypothesis for documents [15, 17, 23]. That is, we use the Binary-Binary test. A passage is considered relevant if it contains at least one character marked as relevant (cf., [1, 8]). For consistency across experimental settings, only queries (95 out of 120) for which all item lists contain at least two relevant items were used.",0,,False
126,"As a reference comparison to the test result we present the mean, over queries and seeds per query, precision of the n - 1 items in a list excluding the seed at hand. This baseline, denoted rand, is an estimate of the probability of randomly drawing a relevant item as a neighbor of a seed; hence, this is the expected test result if all neighbors of a seed were randomly selected. (Thus, the result is the same for k , 4 and k ,"" 9 neighbors.) Naturally, the higher n, the lower the rand value. We also report the average ratio over queries between the Binary-Binary test value and the random baseline: the higher the ratio, the cluster hypothesis holds to a larger extent. Using the ratio serves to normalize the test result with respect to the number of relevant items in the list being analyzed which can considerably affect test results.""",0,,False
127,"We see in Table 2 that the ratio, for all passage lengths (50, 150, 300), number of nearest-neighbors considered (k) and sizes of the passage list (n), is substantially larger than 1. The ratio is also substantially larger than 1 for documents which is aligned with previous findings [15, 17, 21]. Hence, we conclude that the cluster hypothesis, as measured using a nearest-neighbor test, holds not only for documents, but also for passages.",0,,False
128,"Table 2 also shows that the ratio for passages and documents is higher for k , 4 neighbors than for k ,"" 9 neighbors. This finding, which is aligned with those in reports for documents [15, 17], can be attributed to the fact that the more distant the neighbors are from a relevant seed, the less likely they are to be relevant.""",0,,False
129,"We also see in Table 2 that for a given number of neighbors, k, it is often the case that the longer the passage, the higher the ratio value. Indeed, the likelihood of vocabulary mismatch between two long relevant passages is lower than that for two short relevant",0,,False
130,Table 3: The effect of  for seed selection on the Focused-,0,,False
131,"Binary and Focused-Focused tests for INEX (n ,"" 50). '1', '2', '3' and '4': statistically significant differences with """" "","" 0+"""",""",1,INEX,True
132," = .10, "" ,"" .25"""" and """" "","" .50"""", respectively.""",0,,False
133,Focused-Binary,0,,False
134,Focused-Focused,0,,False
135,"item k  , 0+  , .10  , .25  , .50  , .75  , 0+  , .10  , .25  , .50  , .75",0,,False
136,Psg50 4 9,0,,False
137,.714 .678,0,,False
138,.7191 .72812 .73512 .74012 .6821 .68712 .69112 .697132,0,,False
139,.652 .621,0,,False
140,.6561 .66212 .66712 .67112 .6241 .62712 .62912 .63412,0,,False
141,Psg150 4 9,0,,False
142,.733 .670,0,,False
143,.740 .6761,0,,False
144,.75512 .68812,0,,False
145,..766975131322,0,,False
146,..777040131322,0,,False
147,.624 .569,0,,False
148,.6331 .5761,0,,False
149,.65012 .58712,0,,False
150,..656955131322,0,,False
151,..6579691313242,0,,False
152,Psg3004 .706 9 .653,0,,False
153,.7241 .6671,0,,False
154,.73612 .6741,0,,False
155,..765856131322,0,,False
156,..7679461313242,0,,False
157,.549 .502,0,,False
158,.5681 .5151,0,,False
159,.58712 .52412,0,,False
160,..650356131322,0,,False
161,..65245413132424,0,,False
162,Doc,0,,False
163,4 .662 9 .595,0,,False
164,.6951 .7061 .7101 .6161 .62612 .6311,0,,False
165,.7171 .6321,0,,False
166,.369 .328,0,,False
167,.3941 .3411,0,,False
168,.3971 .3441,0,,False
169,..430478131,0,,False
170,.4131 .3501,0,,False
171,"passages. Accordingly, the highest ratio values per list size (n) are attained for documents. (Refer to the boldfaced numbers.)",0,,False
172,"Focused-Binary and Focused-Focused tests. Table 3 presents the results of the Focused-Binary and Focused-Focused tests over INEX. In both tests, an item is selected as a seed if its fraction of relevant text  . In the Focused-Binary test, we report the average number of relevant neighbors of the seed, while in the FocusedFocused test we report the average fraction of relevant text that the neighbors contain. We set n ,"" 50, and below we study the effect of varying n. Note that posting the constraint  "","" 0+ for the Focused-Binary test results in the Binary-Binary test reported in Table 2. However, the numbers in Table 3 differ from those in Table 2 as we perform the test only for queries that have in their retrieved item lists at least one seed with respect to  "","" .75 and an additional relevant item. We do not use the ratio values as in Table 2 since we compare test values only for the same item list; i.e., the type of items and n are fixed. Hence, the test results are comparable.""",1,INEX,True
173,"Table 3 shows that increasing the seed's relevance degree (i.e., increasing ) increases both the average number of its neighbors which are relevant (Focused-Binary) and the average fraction of relevant text they contain (Focused-Focused). Many of the improvements in tests' results when increasing  are statistically significant. (Bonferroni correction was applied for multiple comparisons.) Thus, we see that relevant items with a high fraction of relevant text are more likely to be similar to other relevant items, specifically those which also contain a high fraction, than to non-relevant items.",0,,False
174,"In Figure 1 we present the effect of varying the list size, n, on the results of the Focused-Focused test for k , 4. (The results for k ,"" 9 are not presented as they do not convey additional insight.) As can be seen, for all item types, and all values of n, the higher , the higher the test result. This further supports the conclusion based on Table 3 about the connection between the fraction of relevant text items contain and inter-item similarities.""",1,ad,True
175,"For each graph in Figure 1, curves that correspond to a lower  depict milder increase or steeper decline in test values as a function of n, compared to curves that correspond to a higher . Indeed, larger lists (higher n) and/or those containing longer items have lower precision (see the ""rand"" column in Table 2). For these lists, the higher the fraction of non-relevant text in an item, the higher the likelihood it will be more similar to non-relevant items.",0,,False
176,Graded-Binary and Graded-Graded tests. Table 4 presents the results of Graded-Binary and Graded-Graded tests performed over,1,ad,True
177,1175,0,,False
178,Short Research Papers II,0,,False
179,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
180,0.71 0.70 0.69 0.68 0.67 0.66 0.65,0,,False
181,50,0,,False
182,0.64 0.62 0.60 0.58 0.56 0.54 0.52 0.50,0,,False
183,50,0,,False
184," , 0+",0,,False
185," , .10",0,,False
186," , .25",0,,False
187," , .50",0,,False
188," , .75",0,,False
189,Psg50,0,,False
190,100,0,,False
191,150,0,,False
192,n,0,,False
193,0.69 0.68 0.67 0.66 0.65 0.64 0.63 0.62 0.61 0.60 200 50,0,,False
194,Psg150,0,,False
195,100,0,,False
196,150,0,,False
197,200,0,,False
198,n,0,,False
199,Psg300,0,,False
200,100,0,,False
201,150,0,,False
202,n,0,,False
203,0.42 0.41 0.40 0.39 0.38 0.37 0.36 0.35 0.34 0.33 0.32 200 50,0,,False
204,Doc,0,,False
205,100,0,,False
206,150,0,,False
207,200,0,,False
208,n,0,,False
209,"Figure 1: The effect of n on the Focused-Focused test for INEX with k , 4. Note: graphs are not to the same scale.",1,INEX,True
210,"Table 4: Graded-Binary and Graded-Graded tests performed for documents. '1' marks statistically significant difference with respect to "" ,"" 1"""".""",1,ad,True
211,Graded-Binary,1,ad,True
212,GOV2,0,,False
213,ClueWeb,1,ClueWeb,True
214,"n k  ,1  ,2  ,1  ,2",0,,False
215,50,0,,False
216,4 9,0,,False
217,.737 .673,0,,False
218,.7711 .7031,0,,False
219,.641 .581,0,,False
220,.656 .598,0,,False
221,100 4 9,0,,False
222,.710 .657,0,,False
223,.7621 .6911,0,,False
224,.606 .540,0,,False
225,.631 .561,0,,False
226,150 4 9,0,,False
227,.709 .650,0,,False
228,.7551 .6871,0,,False
229,.580 .506,0,,False
230,.597 .525,0,,False
231,200 4 9,0,,False
232,.699 .644,0,,False
233,.7521 .6831,0,,False
234,.558 .487,0,,False
235,.585 .512,0,,False
236,Graded-Graded,1,ad,True
237,GOV2,0,,False
238,ClueWeb,1,ClueWeb,True
239," ,1  ,2  ,1  ,2",0,,False
240,1.001 1.1231 .952 1.0171,0,,False
241,.908 .9751 .859 .8901,0,,False
242,.950 1.1351 .881 .9971,0,,False
243,.875 .9871 .784 .8581,0,,False
244,.934 1.1301 .829 .9381,0,,False
245,.854 .9861 .723 .8001,0,,False
246,.911 1.1211 .798 .9301,0,,False
247,.837 .9781 .697 .7891,0,,False
248,"document lists for the TREC datasets. These datasets have no focused relevance judgments; therefore, the tests are performed only for documents. Furthermore, the INEX dataset is not used as it has no graded relevance judgments. The queries used are those whose retrieved list contains at least one relevant seed with  ,"" 2 and an additional relevant document. Since the comparisons we discuss are for the same list (determined by n), the test results are comparable and there is no need to use the ratio values as in Table 2.""",1,TREC,True
249,"Table 4 shows that in all cases, increasing the relevance degree (grade) of the seed document results in its neighborhood containing more relevant documents (Graded-Binary) with higher relevance grades (Graded-Graded). These findings are conceptually reminiscent of those presented above for the Focused-Binary and FocusedFocused tests. We also see that the increase in the tests' values when moving from  , 1 to  ,"" 2 is always statistically significant for GOV2; for ClueWeb, statistically significant increase is observed for the Graded-Graded test.""",1,ad,True
250,5 CONCLUSIONS AND FUTURE WORK,0,,False
251,We presented novel cluster hypothesis tests that utilize graded and focused relevance judgments. We found that (i) the cluster hypothesis holds for passages; (ii) relevant items (documents or passages),1,ad,True
252,that contain a high fraction of relevant text are more similar to,0,,False
253,"other relevant items (specifically, those with a high fraction of",0,,False
254,"relevant text) than relevant items with low fraction; and, (iii) docu-",0,,False
255,ments marked as highly relevant are more similar to other relevant,0,,False
256,"documents (specifically, highly relevant) than relevant documents",0,,False
257,not marked as such. These findings motivate the development of,0,,False
258,passage retrieval methods that utilize inter-passage similarities.,0,,False
259,Acknowledgments. We thank the reviewers for their comments.,0,,False
260,This paper is based upon work supported in part by the German,0,,False
261,Research Foundation (DFG) via the German-Israeli Project Cooper-,0,,False
262,"ation (DIP, grant DA 1600/1-1).",0,,False
263,REFERENCES,0,,False
264,"[1] Paavo Arvola, Shlomo Geva, Jaap Kamps, Ralf Schenkel, Andrew Trotman, and Johanna Vainio. 2011. Overview of the INEX 2010 ad hoc track. In Comparative Evaluation of Focused Retrieval. Springer, 1­32.",1,INEX,True
265,[2] James P. Callan. 1994. Passage-Level Evidence in Document Retrieval. In Proc. of SIGIR. 302­301.,0,,False
266,"[3] David Carmel, Anna Shtok, and Oren Kurland. 2013. Position-based contextualization for passage retrieval. In Proc. of CIKM. 1241­1244.",0,,False
267,"[4] Gordon V Cormack, Mark D Smucker, and Charles LA Clarke. 2011. Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval 14, 5 (2011), 441­465.",0,,False
268,"[5] Abdelmoula El-Hamdouchi and Peter Willett. 1987. Techniques for the measurement of clustering tendency in document retrieval systems. Journal of Information Science 13, 6 (1987), 361­365.",0,,False
269,"[6] Ronald T. Fernández, David E. Losada, and Leif Azzopardi. 2011. Extending the language modeling framework for sentence retrieval to include local context. Information Retrieval 14, 4 (2011), 355­389.",1,ad,True
270,"[7] Norbert Fuhr, Marc Lechtenfeld, Benno Stein, and Tim Gollub. 2012. The optimum clustering framework: implementing the cluster hypothesis. Information Retrieval 15, 2 (2012), 93­115.",0,,False
271,"[8] Shlomo Geva, Jaap Kamps, Miro Lethonen, Ralf Schenkel, James A Thom, and Andrew Trotman. 2010. Overview of the INEX 2009 ad hoc track. In Focused retrieval and evaluation. Springer, 4­25.",1,INEX,True
272,"[9] Nick Jardine and C. J. van Rijsbergen. 1971. The use of hierarchic clustering in information retrieval. Information storage and retrieval 7, 5 (1971), 217­240.",0,,False
273,"[10] Mostafa Keikha, Jae Hyun Park, W Bruce Croft, and Mark Sanderson. 2014. Retrieving passages and finding answers. In Proc. of ADCS. 81.",0,,False
274,"[11] Oren Kurland. 2009. Re-ranking search results using language models of queryspecific clusters. Information Retrieval 12, 4 (2009), 437­460.",0,,False
275,"[12] Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, and Frédéric Saubion. 2008. Using text segmentation to enhance the cluster hypothesis. In Proc. of AIMSA. 69­82.",1,ad,True
276,[13] Vanessa Murdock and W Bruce Croft. 2005. A translation model for sentence retrieval. In Proc. of HLT-EMNLP. 684­691.,0,,False
277,"[14] S.-H. Na, I.-S. Kang, and J.-H. Lee. 2008. Revisit of nearest neighbor test for direct evaluation of inter-document similarities. In Proc. of ECIR. 674­678.",0,,False
278,"[15] Fiana Raiber and Oren Kurland. 2012. Exploring the cluster hypothesis, and cluster-based retrieval, over the Web. In Proc. of CIKM. 2507­2510.",0,,False
279,[16] Fiana Raiber and Oren Kurland. 2013. Ranking document clusters using markov random fields. In Proc. of SIGIR. 333­342.,0,,False
280,[17] Fiana Raiber and Oren Kurland. 2014. The correlation between cluster hypothesis tests and the effectiveness of cluster-based retrieval. In Proc. of SIGIR. 1155­1158.,0,,False
281,"[18] Fiana Raiber, Oren Kurland, Filip Radlinski, and Milad Shokouhi. 2015. Learning asymmetric co-relevance. In Proc. of ICTIR. 281­290.",1,ad,True
282,"[19] Hadas Raviv, Oren Kurland, and David Carmel. 2013. The cluster hypothesis for entity oriented search. In Proc. of SIGIR. 841­844.",1,ad,True
283,[20] Mark D Smucker and James Allan. 2009. A new measure of the cluster hypothesis. In Proc. of ICTIR. 281­288.,0,,False
284,"[21] Anastasios Tombros, Robert Villa, and C. J. Van Rijsbergen. 2002. The effectiveness of query-specific hierarchic clustering in information retrieval. Information processing & management 38, 4 (2002), 559­582.",0,,False
285,[22] C. J. van Rijsbergen. 1979. Information Retrieval (second ed.). Butterworths. [23] Ellen M. Voorhees. 1985. The cluster hypothesis revisited. In Proc. of SIGIR.,0,,False
286,188­196. [24] Chengxiang Zhai and John Lafferty. 2001. A study of smoothing methods for,0,,False
287,language models applied to ad hoc information retrieval. In Proc. of SIGIR. 334­ 342.,1,ad,True
288,1176,0,,False
289,,0,,False

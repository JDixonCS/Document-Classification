,sentence,label,data,regex
0,Short Research Papers I,0,,False
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Modeling Multidimensional User Relevance in IR using Vector Spaces,0,,False
3,Sagar Uprety,0,,False
4,"The Open University Milton Keynes, United Kingdom",0,,False
5,sagar.uprety@open.ac.uk,0,,False
6,Dawei Song,0,,False
7,"The Open University Milton Keynes, United Kingdom",0,,False
8,dawei.song@open.ac.uk,0,,False
9,ABSTRACT,0,,False
10,It has been shown that relevance judgment of documents is influenced by multiple factors beyond topicality. Some multidimensional user relevance models (MURM) proposed in literature have investigated the impact of different dimensions of relevance on user judgment. Our hypothesis is that a user might give more importance to certain relevance dimensions in a session which might change dynamically as the session progresses. This motivates the need to capture the weights of different relevance dimensions using feedback and build a model to rank documents for subsequent queries according to these weights. We propose a geometric model inspired by the mathematical framework of Quantum theory to capture the user's importance given to each dimension of relevance and test our hypothesis on data from a web search engine and TREC Session track.,1,TREC,True
11,CCS CONCEPTS,0,,False
12,· Information systems  Personalization;,0,,False
13,KEYWORDS,0,,False
14,"Information Retrieval, User modeling, Multidimensional Relevance",0,,False
15,"ACM Reference Format: Sagar Uprety, Yi Su, Dawei Song, and Jingfei Li. 2018. Modeling Multidimensional User Relevance in IR using Vector Spaces. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210130",0,,False
16,Also with Beijing Institute of Technology. Correspondence Author. Also with Tianjin University. Correspondence author.,0,,False
17,"ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210130",1,Gov,True
18,Yi Su,0,,False
19,"School of Computer Science and Technology, Tianjin University",0,,False
20,"Tianjin, China suyi2016@tju.edu.cn",0,,False
21,Jingfei Li,0,,False
22,"National Computer Network Emergency Response Technical Team/Coordination Center of China Beijing, China jingfl@foxmail.com",0,,False
23,1 INTRODUCTION,1,DUC,True
24,"There is a growing body of work investigating different factors which affect user's judgment of relevance [1, 2, 6, 7, 9, 11­13]. A multidimensional user relevance model (MURM) was proposed [12, 13] which defined five dimensions of relevance namely ""Novelty"", ""Reliability"", ""Scope"", ""Topicality"" and ""Understandability"". In a recent paper [7] an extended version of the MURM comprising two additional dimensions ""Habit"" and ""Interest"" is proposed. The ""Interest"" dimension refers to the topical preferences of users in the past, while ""Habit"" refers to their behavioral preferences. For example, accessing specific websites for some particular information or task is considered under the ""Habit"" dimension. Experiments on real-world data show that certain dimensions, such as ""Reliability"" and ""Interest"", are more important for the user than ""Topicality"", in judging a document.",1,Novelty,True
25,"Our hypothesis is that in a particular search session or search task, there is a particular relevance dimension or a combination of relevance dimensions which the user has in mind before judging documents. For example, if the user wants to get a visa to a country, he or she would prefer documents which are more reliable (""Reliability"") for this task, but when looking to book flights to that country, the user might go to his or her preferred websites (""Habit""). Therefore, for next few queries of the session, ""Habit"" dimension becomes more important. Thus, the importance given to relevance dimensions might change as the session progresses or tasks switch. By capturing the importance assigned to each dimension for a query, we can model the dimensional importance and use it to improve the ranking for the subsequent queries. The relevance dimensions are modeled using the Hilbert space formalism of Quantum theory which unifies the logical, probabilistic and vector space based approaches to IR [8]. We place the user's cognitive state with respect to a document at the center of the IR process. Such a state is modeled as an abstract vector with multiple representations existing at the same time in different basis corresponding to different relevance dimensions. This cognitive state comes into reality only when it is measured in the context of user interactions.",0,,False
26,993,0,,False
27,"Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
28,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA S. Uprety et al.",0,,False
29,(a) Figure 1.a,0,,False
30,(b) Figure 1.b,0,,False
31,(c) Figure 1.c,0,,False
32,2 GEOMETRIC REPRESENTATION OF MULTIDIMENSIONAL RELEVANCE,0,,False
33,"Consider a real-valued two dimensional Hilbert Space. Relevance with respect to a dimension (e.g. Reliability) is a vector in this Hilbert space. Non-relevance with respect to the same dimension is an orthogonal vector to it. Further, we denote vectors as kets following the Dirac's notation. For example, the vectors for relevance and non-relevance with respect to novelty are denoted as |novelty and novelty . Figure 1.a shows the construction of a two-dimensional Hilbert Space for a relevance dimension.",0,,False
34,"Next, we model the user's perception of a document with respect to a dimension of relevance also as a vector in this Hilbert space. This vector is a superposition of relevance and non-relevance vectors with respect to a dimension, e.g., |d ,""  |novelty +  novelty . The coefficient | |2 is the weight (i.e., probability of relevance) the user assigns to document d in term of novelty, and | |2 + | |2 "", 1. We will talk about how to calculate these coefficients in the next section. Figure 1.b shows the modeling of user's cognitive state for document d with respect to the Novelty dimension.",1,Novelty,True
35,"Depending on a user's preference of relevance dimensions for a particular query, the user will judge the same document differently. A document might be of interest to the user but may not be novel when the user is looking for latest documents about the query. This phenomena can be modeled in the same Hilbert space by having different basis for different dimensions of relevance. The same document d can be written in terms of another set of basis vectors corresponding to another dimension of relevance. For example:",0,,False
36,"|d1 , 11 |novelty + 11 novelty",0,,False
37,", 12 |habit + 12 habit",0,,False
38,", 13 |topic + 13 topic",0,,False
39,(1),0,,False
40,and so on in all seven basis. Figure 1.c shows the construction of such a Hilbert space showing two basis for simplicity.,0,,False
41,"We have represented user's cognitive state with respect to a single document in different basis corresponding to different dimensions of relevance. Similarly, we can do that for all the documents retrieved for a query. Each document will be represented in a separate Hilbert space.",0,,False
42,"The user's cognitive state for a document d is an abstract vector, because the vector has different representations in different basis.",0,,False
43,"It does not have a definite state, and a particular representation comes into picture only when we talk of a particular relevance dimension. This is similar to the concept of a state vector in Quantum theory which contains all the information about a quantum system, yet is an abstract entity and has different representations of the same system. We get to see a particular representation of a system depending on how we measure it. A document may look highly relevant in one basis, if it has a high weight in that basis and the user makes judgment from the perspective of that relevance dimension. However, the relevance can change if the user considers a different basis (a different perspective of looking at the document).",0,,False
44,3 CAPTURING USER'S CHANGING WEIGHTS TO RELEVANCE DIMENSIONS,1,AP,True
45,"Having established the geometric representation of documents, we can make use of it to capture the weights of relevance dimensions for a user in response to a query (Algorithm 1).",0,,False
46,"In Algorithm 1, the input parameters docsALL and docsSAT correspond to the list of all retrieved documents and SAT-clicked [7] documents for a query respectively. We quantitatively define each of the seven relevance dimensions using some features. For each query-document pair, these features are extracted and computed (Step 3). They are integrated into the LambdaMART [4] Learning to Rank(LTR) algorithm to generate seven relevance scores (one for each dimension) for the query-document pair (Step 4). Please refer to [7] for more details about the features defined for each dimension and the hyper-parameters of the LTR algorithm. Thus, for a query and its set of retrieved documents, we have seven different rankings, one for each relevance dimension. The seven scores assigned to a document for a query are then normalized using the min-max normalization technique across all the documents for the query (Step 5). The normalized score for each dimension forms the coefficient of superposition of the relevance vector for the respective dimension. It quantitatively represents user's perception of the document for that dimension. For example, for a query q, let d1, d2, ..., dn be the ranking order corresponding to the ""Interest"" dimension. Let relevance scores be 1, 2, ..., n respectively. We construct the vector for document d1 in the 'Interest' basis as:",0,,False
47,"|d1 , 11 |interest + 11 interest",0,,False
48,(2),0,,False
49,"where 11 ,",0,,False
50,m,0,,False
51,1 -m i n ( ) ax ()-min(,0,,False
52,),0,,False
53,",",0,,False
54,where,0,,False
55,max (),0,,False
56,is,0,,False
57,the,0,,False
58,maximum,0,,False
59,"value among 1, 2, ..., n . Square root is taken in accordance with",0,,False
60,994,0,,False
61,Short Research Papers I Modeling Multidimensional User Relevance in IR using Vector Spaces,0,,False
62,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
63,"the quantum probability framework. Similarly, the second document is represented in its Hilbert space as:",0,,False
64,"|d2 , 21 |interest + 21 interest",0,,False
65,(3),0,,False
66,"with 21 ,",0,,False
67,2 -m i n ( ) max ()-min(),0,,False
68,.,0,,False
69,As,0,,False
70,done,0,,False
71,with,0,,False
72,the,0,,False
73,interest,0,,False
74,"dimension,",0,,False
75,we can represent each document in a Hilbert space in all the differ-,0,,False
76,ent basis corresponding to different dimensions of relevance (Steps,0,,False
77,6 - 8).,0,,False
78,"Now in the original list of documents, suppose the user SAT-",0,,False
79,clicks on document dx . We have already formed the Hilbert space for each document. The coefficients of superposition for |dx  vector in a basis represent the importance of document dx with respect to,1,ad,True
80,the relevance dimension represented by that basis. We re-capture,0,,False
81,it by taking the projection of |dx  onto the relevance vector of,0,,False
82,"each basis by taking square of their inner products (Step 13), for ex-",0,,False
83,"ample, | novelty|dx  |2, | reliability|dx  |2, etc. Here the notation A|B is the inner product of the vectors A and B. It denotes the",0,,False
84,"probability amplitude, which is in general a complex number. The",0,,False
85,"square of probability amplitude is the probability that document B is relevant with respect to dimension A. For real valued vectors, inner product is same as the dot product. Let x1, ..., x7 be the projections obtained. Note that the values of these projections are",0,,False
86,the same normalized scores which we calculated above. If there are,0,,False
87,"more than one SAT clicked documents for a query, we average over",0,,False
88,the projection scores for each dimension (Step 15).,0,,False
89,"Thus, for a given query in a search session, we have quantita-",0,,False
90,tively captured the user's cognitive state. This cognitive state or the,0,,False
91,user preference for each dimension is the average relevance score,0,,False
92,for that dimension over all SAT-clicked documents of the query.,0,,False
93,These weights are used to re-rank documents for the next query in,0,,False
94,"the session, as explained in the next section.",0,,False
95,Algorithm 1 Capturing weights given to relevance dimensions,0,,False
96,"1: procedure captureWeights(rel, docsALL, docsSAT )",0,,False
97,2: for all r in rel do,0,,False
98, rel - list of 7 dimensions,0,,False
99,3:,0,,False
100,"f eatures[r ]  etFeatures(docsALL, r )  Extract",0,,False
101,features from all retrieved docs for a given query,0,,False
102,4:,0,,False
103,"scores[r ][d]  reRank(docsALL, f eatures[r ])",0,,False
104,re-rank based on each dim and get score,0,,False
105,5:,0,,False
106,normScores[r ][d]  normalizeScores(scores[r ][d]),0,,False
107,6:,0,,False
108,for all d in docsALL do,0,,False
109,7:,0,,False
110,[d][r ]  normScores[d][r ]  construct vectors,0,,False
111,8:,0,,False
112,[d][r ]  1 - | [d][r ]|2,0,,False
113,9: for all r in rel do,0,,False
114,10:,0,,False
115,totalW eiht  0,0,,False
116,11:,0,,False
117,avW eiht[r ]  0,0,,False
118,12:,0,,False
119,for all d in docsSAT do,0,,False
120,13:,0,,False
121,wdr  | r |d |2  Take projections(| [d][r ]|2),0,,False
122,14:,0,,False
123,totalW eiht  totalW eiht + wdr,0,,False
124,15:,0,,False
125,avW eiht[r ]  totalW eiht/|docsSAT |  Only SAT,0,,False
126,clicks considered 16: return avW eiht  User's importance to each dimension,0,,False
127,4 EXPERIMENT AND ANALYSIS,0,,False
128,"We use the same datasets as used in [7]. The first one is the query logs of the Bing search engine and the second one is the combined session tracks of TREC 2013 and TREC 2014. While the Bing query logs contain information about each query in a session, the TREC dataset only contains the data about the last query for each session. The relevance criteria for Bing logs is SAT clicks and for TREC data we consider relevance grades of 1 and above to correspond to relevant documents. In Section 3, we captured the user's dimensional preference for a query in the form of weights. We now use these weights for the next query in the session, to take a weighted combination of the relevance scores of all seven dimensions for each document of the next query. Thus, for the new query, a new relevance score for each document is created based on the weighted dimensional preference for the previous query. We re-rank the documents according to these new scores and perform evaluation. We use the NDCG metric for evaluation and compare the values with those obtained in [7].",1,TREC,True
129,"We also performed an initial analysis of the data to support our hypothesis that some combination of relevance dimensions are preferred by the user in a search session. For some randomly sampled 4837 sessions of the Bing query logs, we found that in 3910 or 80.84 percent of the sessions, one of the top three dimensions for the first query of the session remains in the top three for all the queries of the session. Figure 2.a is the snapshot of one such session showing that the ""Reliability"" remains the top dimension throughout. Figure 2.b shows 20 consecutive sessions for TREC data.",1,TREC,True
130,5 RESULTS AND DISCUSSION,0,,False
131,"We summarize the evaluation results for Bing query logs in Table 1. In the paper [7], re-ranking by using the features of ""Reliability"" dimension gives the best performance for Bing data. However we show that the results obtained by using the weighted combination gives slightly better results (Table 1). The improvement is not significant but the fact is that weighted combination is a general method which will work for other datasets too. Similar to the results reported in [7], for TREC data, it is not ""Reliability"" but ""Interest"" which comes out as the best dimension for re-ranking (Table 2). Therefore one cannot use a fixed relevance dimension for ranking the documents. Table 2 shows that our weighted combination approach also gives improved performance for the TREC data. It is to be noted that TREC data contains information about the last query of each session, and not all the queries. Thus our weighted approach uses the captured weights of the last query of a session to re-rank the documents for the last query of the next session. Improvement over the best result (corresponding to Interest) means that the weighted combination method for ranking works across sessions as well. This indicates that dimensional preference is not only dependent upon the task, but user might have an intrinsic preference for some dimensions as well. Also note that the ""Topicality"" scores correspond to a traditional baseline model as we use tf-idf and BM25 as features for the ""Topicality"" dimension.",1,TREC,True
132,995,0,,False
133,"Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
134,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA S. Uprety et al.",0,,False
135,(a) Figure 2.a,0,,False
136,(b) Figure 2.b,0,,False
137,Dimension,0,,False
138,NDCG@1 NDCG@5 NDCG@10 NDCG@ALL,0,,False
139,Habit Interest Novelty Reliability Scope Topicality Understandability Weighted Combination,1,Novelty,True
140,0.3772 0.4574 0.4110 0.6457 0.2501 0.2001 0.2782 0.6552,0,,False
141,0.5958 0.6178 0.6025 0.7687 0.4692 0.4486 0.4968 0.7814,0,,False
142,0.6533 0.6844 0.6688 0.8038 0.56156 0.5352 0.5867 0.8127,0,,False
143,0.6645 0.6955 0.6783 0.8110 0.5726 0.5482 0.5971 0.8189,0,,False
144,Table 1: Bing logs evaluation,0,,False
145,Dimension,0,,False
146,NDCG@1 NDCG@5 NDCG@10 NDCG@ALL,0,,False
147,Habit Interest Novelty Reliability,1,Novelty,True
148,Scope Topicality Understandability Weighted Combination,0,,False
149,0.0989 0.1981 0.0966 0.1120 0.1318 0.1459 0.1653 0.2364,0,,False
150,0.1406 0.2126 0.1180 0.1333 0.1526 0.1520 0.1913 0.2663,0,,False
151,0.1418 0.2242 0.1316 0.1431 0.1647 0.1887 0.1878 0.2729,0,,False
152,0.1592 0.1831 0.1557 0.1614 0.1671 0.1701 0.1764 0.1944,0,,False
153,Table 2: TREC data evaluation,1,TREC,True
154,6 CONCLUSION AND FUTURE WORK,0,,False
155,"We have thus shown that capturing user's weights for relevance dimensions and ranking based on the combination of these weights leads to a better performance than using only one of the dimensions. The need for a Hilbert space framework is inspired by the fact that some relevance dimensions are incompatible for some documents. A document may not have high relevance weights for both ""Novelty"" and ""Habit"" dimensions at the same time. The more relevant it is in the ""Novelty"" dimension, the less relevant it will be in the ""Habit"" dimension. This is similar to the Uncertainty Principle in Quantum Theory. We therefore model each relevance dimension as a different basis. For some documents, the basis might coincide, but in general there is incompatibility between relevance dimensions which leads to interference and order effects [5, 10]. For example, a user may find a document less reliable due to its source, but when the user considers the ""Topicality"" dimension and reads it, it might remove the doubts about the reliability. Thus ""Topicality"" interferes with ""Reliability"" in relevance judgment. Such order effects were investigated in [3] through user studies. We intend to investigate",1,ad,True
156,"such cognitive phenomena in real world data, and the Hilbert space",0,,False
157,representation described in this paper forms a solid basis to carry,0,,False
158,out such experiments in the future.,0,,False
159,ACKNOWLEDGMENTS,0,,False
160,This work is funded by the European Union's Horizon 2020 research,0,,False
161,and innovation programme under the Marie Sklodowska-Curie,0,,False
162,grant agreement No 721321.,0,,False
163,REFERENCES,0,,False
164,"[1] Carol L. Barry. 1998. Document representations and clues to document relevance. Journal of the American Society for Information Science 49, 14 (1998), 1293­ 1303. https://doi.org/10.1002/(SICI)1097-4571(1998)49:14<1293::AID-ASI7>3.0. CO;2- E",0,,False
165,"[2] Ulises Cerviño Beresi, Yunhyong Kim, Dawei Song, and Ian Ruthven. 2011. Why did you pick that? Visualising relevance criteria in exploratory search. International Journal on Digital Libraries 11, 2 (27 Sep 2011), 59. https://doi.org/10.1007/ s00799- 011- 0067- 7",0,,False
166,"[3] Peter Bruza and Vivien Chang. 2014. Perceptions of document relevance. Frontiers in Psychology 5 (2014), 612. https://doi.org/10.3389/fpsyg.2014.00612",0,,False
167,[4] Christopher J. C. Burges. 2010. From RankNet to LambdaRank to LambdaMART: An Overview.,0,,False
168,"[5] Jerome R. Busemeyer and Peter D. Bruza. 2012. Quantum Models of Cognition and Decision (1st ed.). Cambridge University Press, New York, NY, USA.",0,,False
169,"[6] Célia da Costa Pereira, Mauro Dragoni, and Gabriella Pasi. 2009. Multidimensional Relevance: A New Aggregation Criterion. In Advances in Information Retrieval, Mohand Boughanem, Catherine Berrut, Josiane Mothe, and Chantal Soule-Dupuy (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 264­275.",0,,False
170,"[7] Jingfei Li, Peng Zhang, Dawei Song, and Yue Wu. 2017. Understanding an enriched multidimensional user relevance model by analyzing query logs. Journal of the Association for Information Science and Technology 68, 12 (2017), 2743­2754. https://doi.org/10.1002/asi.23868",0,,False
171,"[8] C. J. van Rijsbergen. 2004. The Geometry of Information Retrieval. Cambridge University Press, New York, NY, USA.",0,,False
172,"[9] Anastasios Tombros, Ian Ruthven, and Joemon M. Jose. 2005. How users assess Web pages for information seeking. Journal of the American Society for Information Science and Technology 56, 4 (2005), 327­344. https://doi.org/10.1002/asi.20106",0,,False
173,"[10] Benyou Wang, Peng Zhang, Jingfei Li, Dawei Song, Yuexian Hou, and Zhenguo Shang. 2016. Exploration of Quantum Interference in Document Relevance Judgement Discrepancy. Entropy 18, 12 (Apr 2016), 144. https://doi.org/10.3390/ e18040144",0,,False
174,"[11] Yunjie Xu and Hainan Yin. 2008. Novelty and topicality in interactive information retrieval. Journal of the American Society for Information Science and Technology 59, 2 (2008), 201­215. https://doi.org/10.1002/asi.20709",1,Novelty,True
175,"[12] Yunjie (Calvin) Xu and Zhiwei Chen. 2006. Relevance judgment: What do information users consider beyond topicality? Journal of the American Society for Information Science and Technology 57, 7 (2006), 961­973. https: //doi.org/10.1002/asi.20361",0,,False
176,"[13] Yinglong Zhang, Jin Zhang, Matthew Lease, and Jacek Gwizdka. 2014. Multidimensional Relevance Modeling via Psychometrics and Crowdsourcing. In Proceedings of the 37th International ACM SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 435­444. https://doi.org/10.1145/2600428.2609577",0,,False
177,996,0,,False
178,,0,,False

,sentence,label,data,regex
0,Short Research Papers II,0,,False
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Effectiveness Evaluation with a Subset of Topics: A Practical Approach,0,,False
3,Kevin Roitero,0,,False
4,"University of Udine Udine, Italy",0,,False
5,roitero.kevin@spes.uniud.it,0,,False
6,Michael Soprano,0,,False
7,"University of Udine Udine, Italy",0,,False
8,soprano.michael@spes.uniud.it,0,,False
9,Stefano Mizzaro,0,,False
10,"University of Udine Udine, Italy",0,,False
11,mizzaro@uniud.it,0,,False
12,ABSTRACT,0,,False
13,"Several researchers have proposed to reduce the number of topics used in TREC-like initiatives. One research direction that has been pursued is what is the optimal topic subset of a given cardinality that evaluates the systems/runs in the most accurate way. Such a research direction has been so far mainly theoretical, with almost no indication on how to select the few good topics in practice. We propose such a practical criterion for topic selection: we rely on the methods for automatic system evaluation without relevance judgments, and by running some experiments on several TREC collections we show that the topics selected on the basis of those evaluations are indeed more informative than random topics.",1,TREC,True
14,t1 · · · tn,0,,False
15,MAP,1,MAP,True
16,"s1 AP1,1 · · · AP1,n MAP1",1,AP,True
17,...,0,,False
18,...,0,,False
19,...,0,,False
20,...,0,,False
21,...,0,,False
22,"sm APm,1 · · · APm,n MAPm",1,AP,True
23,Figure 1: AP and MAP representation and correlation curves (both adapted from [6]).,1,AP,True
24,CCS CONCEPTS,0,,False
25,· Information systems  Test collections;,0,,False
26,KEYWORDS,0,,False
27,"Few topics, test collections, TREC, topic selection",1,TREC,True
28,"ACM Reference Format: Kevin Roitero, Michael Soprano, and Stefano Mizzaro. 2018. Effectiveness Evaluation with a Subset of Topics: A Practical Approach. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210108",0,,False
29,1 INTRODUCTION,1,DUC,True
30,"In Information retrieval (IR) test collection based evaluation, the number of topics used is a critical issue. Since it is one of the main parameters to determine the overall cost, it is not surprising that several researchers have studied how to reduce such a number in TREC-like initiatives. One research direction that has been pursued is to identify the optimal topic subset of a given cardinality that evaluates the systems/runs in the most accurate way [2, 6, 11]. The main limitation of such an approach is that it requires the full evaluation to be run, since it needs the effectiveness evaluation for each system/topic pair (usually represented as a system/topic matrix of, e.g., average precision values). So the results have been so far mainly theoretical, with almost no indication on how to select the few good topics in practice. In this paper we propose such a",1,TREC,True
31,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210108",1,ad,True
32,"practical criterion for topic selection. We rely on the methods for automatic effectiveness evaluation without relevance judgments [1, 4, 9, 10, 12­14, 17], and by running some extensive experiments on several TREC collections we show that the topics selected on the basis of those evaluations are indeed more informative than a random selection of topics.",1,TREC,True
33,2 RELATED WORK,0,,False
34,2.1 A Few Good Topics,0,,False
35,We briefly summarize the research on topic subsets and on effectiveness evaluation without human relevance judgments.,0,,False
36,"The table on the left of Figure 1 is a representation of the results of a TREC-like evaluation. Each row is a system / run and each column is a topic; each cell of the matrix APi, j is the effectiveness value (in this paper we focus on Average Precision, AP) of system i on topic j. When averaging across the n columns (topics) one obtains Mean AP (MAP), a measure of the effectiveness of a system.",1,TREC,True
37,"The number n of topics in an evaluation initiative has received attention since the first TREC editions (and even before) [3]. The classical and main approach has been to understand what happens when selecting a random subset of topics of a given cardinality (i.e., a by computing MAP on a random subset of the columns in Figure 1). By doing so, the evaluation of systems / runs is in general different than when using the full set of topics. The effect is generally measured using the (linear, rank) correlation between the ground truth of the m original MAP values and the m predicted MAP values, obtained on the basis of the topic subset only.",1,TREC,True
38,"A different approach, more related to our work, has been to find the best subset of topics of a given cardinality: one does not select a random subset of columns in Figure 1 but the optimal subset, i.e., the one leading to the highest correlation value between the real and the predicted MAP. The first proposal is by Guiver et al. [6], which performed a heuristic search on all possible topic subsets. More in detail, their work focused on the correlation of three series: (i) Best, the subset of topics which has the highest correlation (i.e.,",1,ad,True
39,1145,0,,False
40,"Short Research Papers II SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
41,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Kevin Roitero, Michael Soprano, and Stefano Mizzaro",0,,False
42,"the predicted MAP of systems is the most similar) with the ground truth; (ii) Worst, the subset of topics which has the which has the smallest correlation (i.e., the predicted MAP of systems is the least similar) with the ground truth; and (iii) Average, the correlation that one might expect when choosing topics randomly. Guiver et al. used both Pearson's linear and Kendall's rank correlations; in this paper we focus on the latter. Their analysis shows that subsets of good and bad topic sets exist with a high / low correlation even at low cardinalities; for example (see the chart on the right of Figure 1), on a ground truth of 50 topics, at cardinality 8 one can identify a Best topic subset with   0.85 and a Worst set with   0.1).",1,MAP,True
43,"This work has been continued by Robertson [11], who questioned the generality of Guiver et al. results, as well as attempted a first example of a practical topic selection strategy, that however turned out to be ineffective. Berto et al. [2] confirmed the findings of Guiver et al. and extended their work by looking at the number, the distribution, and the stability of the Best and Worst topic sets. Such a research direction has been so far purely theoretical, with no indication on how to select the few good topics in practice. In this paper we propose such a practical approach. In this respect, our work is an attempt to make the methodology proposed in the above described publications [2, 6, 11] more similar to the more recent work by Kutlu et al. [7] who proposed a learning-to-rank based approach for topic selection, and analyzed in detail the role of deep and shallow pooling in the topic selection process, and its impact on the evaluation.",0,,False
44,2.2 Evaluation without Relevance Judgments,0,,False
45,"The first proposal of evaluating IR systems without relevance judgments is by Soboroff et al. [13]; their proposal is simply to random sample documents from the pool and treat such documents as relevant. The intuition is that the random sample is performed on a biased set of documents: if many different systems retrieve the same document (maybe even in the first rank positions) that document is probably relevant. Results show that the estimated final rank of systems correlates decently (Kendall's   0.5) with the official TREC ranking; the method fails in predicting the rank of best systems, which are somehow ""peculiar"".",1,TREC,True
46,"Wu and Crestani [17] proposed a method based on data fusion techniques, which are used to merge the ranked lists of documents retrieved by the systems, assigning a popularity score to each document, and using such score to provide an estimated final rank of systems. Wu and Crestani propose five variants to assign the popularity score to each document.",0,,False
47,"Another approach, proposed by Aslam and Savell [1], measures the similarity of the ranked lists of each pair of systems, and ranks such systems by the computed similarity index. This methodology is strongly correlated with Soboroff et al.'s one. Aslam and Savell also raise the issue that the predicted ranking of systems is based on document popularity rather than relevance.",0,,False
48,"Nuray and Can [9, 10] proposed a method based on three strategies used in democratic elections to measure popularity of candidates: the ""RankPosition"", the ""Borda"", and the ""Condorcet"" method. Furthermore, such indexes are computed considering either all the system/runs which participated in a given TREC edition (the ""normal"" method), or selecting just the most ""peculiar"" systems (the",1,TREC,True
49,"bias method), that are the systems that have a ranked list which deviates more from the norm.",0,,False
50,"Spoerri [14] proposed an evaluation method which relies on a set of trials between runs; each run is assigned five times to a set with other four runs, thus in this way each system participate in exactly five trials. Then, for each trial, for each system the method computes the percentage of documents retrieved by the system alone (""Single"" index), the percentage of documents retrieved by all the systems in the trial (""AllFive""), and the algebraic difference between the Single and the AllFive indexes (""SingleMinusAllFive""); those three indexes are averaged across the trials and then retrieval systems are ranked according to their SingleMinusAllFive index, the lower the index the better.",0,,False
51,"Sakai and Lin [12] proposed a variation of the Condorcet method proposed in [10], which is more feasible to compute even for deep pools, and it is strongly correlated although statistically different from Nuray and Can's proposal.",0,,False
52,"All the above methods provide an approximate evaluation matrix; that is a system by topic matrix similar to the one in Figure 1, but with predicted APi, j and MAPi values.",1,AP,True
53,3 OUR APPROACH,1,AP,True
54,"The main limitation of the above ""few good topics"" approaches [2, 6, 11] is that they are only theoretical, not practical. For example, they provide an optimum to aim at (the Best correlation curves) but to actually select the Best subset of topics, the whole TREC evaluation exercise has to be performed, since the matrix in Figure 1 is needed. However, each of the methods for effectiveness evaluation without relevance judgments [1, 4, 9, 10, 12­14, 17] indeed provides an approximation of that matrix, which can possibly be used as input to the approaches that find the optimal subset of a few good topics. This is the main idea of this paper.",1,TREC,True
55,"Thus, we run the few topics approach used in [2, 6, 11] on the approximate matrices. Then, for each cardinality, we consider the Best subset of topics found on the approximate matrices and we use such topics on the real matrix to produce a predicted evaluation of systems using the selected few topics. We then measure the correlation obtained by such a reduced topic subset: if the approximate matrices are representative of the real matrix, then the obtained correlation should be higher than the correlation found with a random selection of topics (i.e., the Average correlation curve in Figure 1).",0,,False
56,"Moreover, instead of using the individual methods alone, we combine such methods. The intuition on which we rely is that while a single method can produce a poorly representative matrix on a particular collection and a highly representative matrix on another collection, a combination of all the approximate matrices should provide a more stable and general AP prediction. To combine the methods, we follow the same approach of Mizzaro et al. [8]: we train a machine learning system that, on the basis of the TREC data of the previous years, learns a model that is then applied on a subsequent year TREC test collection. So, previous years test collections are the training set and the new test collection is then the test set; the features are the approximate matrices produced by the individual methods, and the combination function is the learned best combination of them to fit the real AP values. In other terms, the combination function, or the machine learning model, is",1,ad,True
57,1146,0,,False
58,Short Research Papers II Effectiveness Evaluation with a Subset of Topics: A Practical Approach,0,,False
59,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
60,Table 1: The datasets used in this paper.,0,,False
61,Acronym Name,0,,False
62,Year Topics Runs Used Topics,0,,False
63,TREC5,1,TREC,True
64,TREC6,1,TREC,True
65,TREC7,1,TREC,True
66,TREC8,1,TREC,True
67,TREC01,1,TREC,True
68,TB04,1,TB,True
69,R05 W111 W121 W131 W141,0,,False
70,Ad Hoc 1996,0,,False
71,Ad Hoc 1997,0,,False
72,Ad Hoc 1998,0,,False
73,Ad Hoc 1999,0,,False
74,Ad Hoc 2001,0,,False
75,TeraByte 2004,0,,False
76,Robust,1,Robust,True
77,2005,0,,False
78,Web Track 2011,1,Track,True
79,Web Track 2012,1,Track,True
80,Web Track 2013,1,Track,True
81,Web Track 2014,1,Track,True
82,50,0,,False
83,61 251-300,0,,False
84,50,0,,False
85,74 301-350,0,,False
86,50 103 351-400,0,,False
87,50 129 401-450,0,,False
88,50,0,,False
89,97 501-550,0,,False
90,49,0,,False
91,69 701-750 (no 703),0,,False
92,50,0,,False
93,"74 See [15, Figure 1]",0,,False
94,50,0,,False
95,61 101-150,0,,False
96,50,0,,False
97,48 151-200,0,,False
98,50,0,,False
99,55 201-250,0,,False
100,50,0,,False
101,30 251-300,0,,False
102,"1 Binarized: collapsed relevance levels {-2, 0} into 0, and {1, 2, 3} into 1.",0,,False
103,PPI,0,,False
104,0.12 0.10 0.08 0.06 0.04 0.02 0.00 -0.02,0,,False
105,1,0,,False
106,10,0,,False
107,20,0,,False
108,30,0,,False
109,40,0,,False
110,50,0,,False
111,Cardinality,0,,False
112,Figure 3: The distributions of PPI values for each cardinality.,0,,False
113,TREC8 -- RF 1.0,1,TREC,True
114,W12 -- M5P 1.0,0,,False
115,0.9 0.8,0,,False
116,0.8,0,,False
117,0.7,0,,False
118,0.6,0,,False
119,Tau Tau,0,,False
120,0.6,0,,False
121,0.4,0,,False
122,0.5 0.2,0,,False
123,0.4,0,,False
124,0.0 0.3,0,,False
125,0.2,0,,False
126,-0.2,0,,False
127,1,0,,False
128,10,0,,False
129,20,0,,False
130,30,0,,False
131,40,0,,False
132,50,0,,False
133,Cardinality,0,,False
134,1,0,,False
135,10,0,,False
136,20,0,,False
137,30,0,,False
138,40,0,,False
139,50,0,,False
140,Cardinality,0,,False
141,Figure 2: Two examples of the obtained correlation curves.,0,,False
142,"the one that, on the basis of historical data, provides the best prediction of real AP values. We try six machine learning algorithms [16]: Linear Regression (LR), M5P model tree (M5P), Random Forest (RF), Neural Networks (NN), Support Vector Machine with Polykernel (SVM_Poly), and Support Vector Machine with Radial Basis Function Kernel (SVM_RBF).",1,AP,True
143,4 EXPERIMENTS AND RESULTS,0,,False
144,"To run our experiments we use the ten datasets shown in Table 1. When compared to the experiments by Kutlu et al. [7], we attempt a more general and systematic evaluation, as we consider 10 different test collections instead of the 4 used by them. For each of the test collections, we compute the 22 approximate matrices (the 16 individual methods of Section 2.2 plus their 6 machine learning combinations above described).",1,ad,True
145,"Figure 2 shows two examples of the obtained correlation curves, compared to the Best, Average and Worst. The one on the left is obtained by RF on TREC8 (the same data as in Figure 1); the one on the right by M5P on W12. To better understand, as well as objectively quantify, the effectiveness of the methods, we define the following simple index that measures if and how much the obtained correlation curve is above the Average curve. We denote with Mi (k) the Kendall's  correlation of the i-th method at cardinality k, and with A(k) the  correlation of the Average series at cardinality k. If we have C collections, each of them having T topics, then we can define the Predictive Power Index of method i (PPI(i)) as",1,TREC,True
146,PPI(i),0,,False
147,",",0,,False
148,1 C,0,,False
149,CT,0,,False
150,(Mi (k),0,,False
151,"c,1 k ,1",0,,False
152,-,0,,False
153,A(k )),0,,False
154,.,0,,False
155,"PPI describes the behavior of a topic selection strategy with respect to the Average series (i.e., a random topic selection). If PPI > 0",0,,False
156,PPI,0,,False
157,0.3 0.2 0.1 0.0,0,,False
158,TREC5 TREC6 TREC7 TREC8 TREC01 TB04 R05 Cardinality,1,TREC,True
159,W11,0,,False
160,W12,0,,False
161,W13,0,,False
162,W14,0,,False
163,Figure 4: The distributions of PPI values for each collection.,0,,False
164,"the topic selection strategy is effective, if PPI < 0 it is not, and if PPI , 0 the topic selection strategy is equivalent to the expected value of a random topic selection.",0,,False
165,"The box-plots in Figure 3 show the distributions of the PPI values (y-axis) for each cardinality (x-axis). Each box-plot is a representation of the 22 PPI values obtained, one for each method. The PPI index is almost always positive, for both the median values and the lowest quantiles. Also the number of negative outliers is very low. However, this positive result depends in part on a skewed distribution of the PPI values for different collections, as demonstrated in Figure 4, that shows a breakdown of the PPI values for each collection separately: PPI values are still on the y-axis, the different collections are on the x-axis, and each box-plot is still obtained combining the single PPI values for the 22 methods. The W14 collection is a clear outlier, and the positive results obtained might depend just on it. We therefore repeat the same analysis without W14. Figure 5 shows that, although to a smaller extent, the positive result still hold: PPI are positive. This is also confirmed by running a statistical significance test: all series have p-value < 0.01 according to the paired Wilcoxon signed rank test.",0,,False
166,"Thus, when selecting a few good topics using the approximate matrices obtained with the methods for effectiveness evaluation without relevance judgments, one indeed finds topic subsets that are significantly better than a random selection. This is true for most of such methods; however, some methods could be more effective in this respect. In the last part of the paper we address this issue. We start by remarking that in our scenario not all the cardinalities have the same importance, for two main reasons: (i) above a certain cardinality threshold, the Average  correlation is close to 1 (i.e., the higher the cardinality the harder is to beat the Average baseline),",1,ad,True
167,1147,0,,False
168,"Short Research Papers II SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA",0,,False
169,0.125,0,,False
170,0.100,0,,False
171,0.075,0,,False
172,0.050,0,,False
173,PPI,0,,False
174,0.025,0,,False
175,0.000,0,,False
176,-0.025,0,,False
177,-0.050,0,,False
178,1,0,,False
179,10,0,,False
180,20,0,,False
181,30,0,,False
182,40,0,,False
183,50,0,,False
184,Cardinality,0,,False
185,"Figure 5: The distribution of PPI values for each cardinality, having excluded the outlier collection W14.",0,,False
186,all,0,,False
187,1-5,0,,False
188,1-10,0,,False
189,0.20,0,,False
190,1-15,0,,False
191,1-20,0,,False
192,0.15,0,,False
193,PPI,0,,False
194,0.10,0,,False
195,0.05,0,,False
196,0.00,0,,False
197,-0.05,0,,False
198,SMV5MP_RBF,0,,False
199,RNSFuVNrNMauEyr_BaTPyiaONsLoNCYruomrnaadyLlCoNRroocnermdt3oSa0rOlcReBatNO3nu0kRrPaOoyFsBiFtiaiosnB3o0rdaS3I0NNGurLAaWEySNULoSACrIMmBNAaGlSBLIEoCrMNdIauNW3rUa0UySCBAiVaL1sLRFIaVWnEkUPCoVsi3tionA3L0LFIVWEUCSVA2KAI30,1,ad,True
200,Figure 6: PPI of the methods over cardinality ranges.,0,,False
201,0.125 0.100,0,,False
202,feature M5P RF,0,,False
203,0.075,0,,False
204,0.050,0,,False
205,PPI,0,,False
206,0.025,0,,False
207,0.000,0,,False
208,-0.025,0,,False
209,-0.050,0,,False
210,1,0,,False
211,10,0,,False
212,20,0,,False
213,30,0,,False
214,40,0,,False
215,50,0,,False
216,Cardinality,0,,False
217,"Figure 7: The PPI values obtained by M5P and RF, compared",0,,False
218,to the distribution of PPI values of Figure 5.,0,,False
219,"and (ii) higher cardinalities are less interesting, since the higher the cardinality the less useful the topic set reduction is; in other words, since we are interested in reducing as much as possible the topic set size, the lower the cardinality the better.",0,,False
220,"We therefore perform a breakdown of the PPI measure over the first 5 cardinalities (1,5], the first 10, the first 15, the first 20, and all cardinalities. Figure 6 shows the result: whereas when considering all cardinalities M5P, SVM_RBF, and RF are the three most effective methods, when focusing on lower cardinalities clearly M5P and RF show consistently higher PPI. We thus focus on this two methods in Figure 7. The plot shows that both RF and M5P have a PPI score greater than zero, especially for lower cardinalities, i.e., the most",0,,False
221,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
222,"Kevin Roitero, Michael Soprano, and Stefano Mizzaro",0,,False
223,"interesting ones. Also note that the two curves shown in Figure 2 are two examples of those obtained by these two methods: in both cases, using fewer than 10 topic produces ranking of systems with a Kendall's correlation higher than 0.8 with the ranking obtained with all the topics. Overall, the topics selected applying the fewer topics algorithm to the results of the M5P and RF methods are those with the best results.",0,,False
224,5 CONCLUSIONS AND FUTURE WORK,0,,False
225,"We have applied the few topics approach to the outcome of the methods for effectiveness evaluation without relevance judgments. Our experimental results on ten TREC test collections show that such methods (and especially M5P and RF) allow to select a subset of a few topics that evaluate a population of systems / runs in a more accurate way than a random selection of topics of the same cardinality. This practical result is the first successful attempt of showing the practical usefulness of the few topics approaches [2, 6, 11], thus addressing their main limitation. For example, it could be used to select which topics to evaluate when resources are limited. In future work we plan to include TREC collections with a higher number of topics, as well as report on a more detailed comparison with the work by Kutlu et al. [7]. Furthermore, we plan to apply models used in response theory [5] to study the relationship between topic sets.",1,TREC,True
226,REFERENCES,0,,False
227,[1] Javed A. Aslam and Robert Savell. 2003. On the Effectiveness of Evaluating Retrieval Systems in the Absence of Relevance Judgments. In Proceedings of 26th ACM SIGIR. 361­362.,0,,False
228,"[2] Andrea Berto, Stefano Mizzaro, and Stephen Robertson. 2013. On Using Fewer Topics in Information Retrieval Evaluations. In Proc. of ACM ICTIR 2013. 9:30­ 9:37.",0,,False
229,"[3] Chris Buckley and Ellen M. Voorhees. 2000. Evaluating Evaluation Measure Stability. In Proceedings of the 23rd ACM SIGIR. ACM, New York, NY, USA, 33­40.",0,,False
230,[4] Fernando Diaz. 2007. Performance Prediction Using Spatial Autocorrelation. In Proceedings of 30th ACM SIGIR. 583­590.,0,,False
231,[5] Susan E Embretson and Steven P Reise. 2013. Item response theory. Psychology Press.,0,,False
232,"[6] John Guiver, Stefano Mizzaro, and Stephen Robertson. 2009. A Few Good Topics: Experiments in Topic Set Reduction for Retrieval Evaluation. ACM Trans. Inf. Syst. 27, 4, Article 21 (Nov. 2009), 26 pages.",0,,False
233,"[7] Mucahid Kutlu, Tamer Elsayed, and Matthew Lease. 2018. Intelligent topic selection for low-cost information retrieval evaluation: A New perspective on deep vs. shallow judging. Inform. Processing & Management 54, 1 (2018), 37­59.",0,,False
234,"[8] Stefano Mizzaro, Josiane Mothe, Kevin Roitero, and Md Zia Ullah. 2018. Query Performance Prediction and Effectiveness Evaluation Without Relevance Judgments: Two Sides of the Same Coin. In Proc. of the 41st ACM SIGIR. In press.",1,Query,True
235,[9] Rabia Nuray and Fazli Can. 2003. Automatic Ranking of Retrieval Systems in Imperfect Environments. In Proceedings of 26th ACM SIGIR. 379­380.,0,,False
236,"[10] Rabia Nuray and Fazli Can. 2006. Automatic ranking of information retrieval systems using data fusion. Information Processing & Management 42, 3 (May 2006), 595­614.",0,,False
237,[11] Stephen Robertson. 2011. On the Contributions of Topics to System Evaluation. In Proceedings of the 33rd ECIR. 129­140.,0,,False
238,"[12] Tetsuya Sakai and Chin-Yew Lin. 2010. Ranking Retrieval Systems without Relevance Assessments -- Revisited. In Proceeding of 3rd EVIA -- A Satellite Workshop of NTCIR-8. National Institute of Informatics, Tokyo, Japan, 25­33.",0,,False
239,"[13] Ian Soboroff, Charles Nicholas, and Patrick Cahan. 2001. Ranking Retrieval Systems Without Relevance Judgments. In Proc. of 24th ACM SIGIR. 66­73.",0,,False
240,"[14] Anselm Spoerri. 2007. Using the structure of overlap between search results to rank retrieval systems without relevance judgments. Information Processing & Management 43, 4 (2007), 1059 ­ 1070.",0,,False
241,[15] Ellen M Voorhees. 2003. Overview of the TREC 2003 Robust Retrieval Track.. In Trec. 69­77.,1,TREC,True
242,"[16] Ian H. Witten, Eibe Frank, Mark A. Hall, and Christopher J. Pal. 2016. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann.",0,,False
243,[17] Shengli Wu and Fabio Crestani. 2003. Methods for Ranking Information Retrieval Systems Without Relevance Judgments. In Proceedings of the 2003 ACM Symposium on Applied Computing. 811­816.,0,,False
244,1148,0,,False
245,,0,,False

,sentence,label,data,regex
0,Session 1B: Log Analysis,1,Session,True
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Predicting User Knowledge Gain in Informational Search Sessions,1,Session,True
3,Ran Yu,0,,False
4,"L3S Research Center Hannover, Germany",0,,False
5,yu@l3s.de,0,,False
6,Ujwal Gadiraju,1,ad,True
7,"L3S Research Center Hannover, Germany",0,,False
8,gadiraju@l3s.de,1,ad,True
9,Peter Holtz,0,,False
10,"Leibinz Insitut für Wissensmedien Tübingen, Germany",0,,False
11,holtz@iwm-tuebingen.de,0,,False
12,Markus Rokicki,0,,False
13,"L3S Research Center Hannover, Germany",0,,False
14,rokicki@l3s.de,0,,False
15,Philipp Kemkes,0,,False
16,"L3S Research Center Hannover, Germany",0,,False
17,kemkes@l3s.de,0,,False
18,Stefan Dietze,0,,False
19,"L3S Research Center Hannover, Germany",0,,False
20,dietze@l3s.de,0,,False
21,ABSTRACT,0,,False
22,"Web search is frequently used by people to acquire new knowledge and to satisfy learning-related objectives. In this context, informational search missions with an intention to obtain knowledge pertaining to a topic are prominent. The importance of learning as an outcome of web search has been recognized. Yet, there is a lack of understanding of the impact of web search on a user's knowledge state. Predicting the knowledge gain of users can be an important step forward if web search engines that are currently optimized for relevance can be molded to serve learning outcomes. In this paper, we introduce a supervised model to predict a user's knowledge state and knowledge gain from features captured during the search sessions. To measure and predict the knowledge gain of users in informational search sessions, we recruited 468 distinct users using crowdsourcing and orchestrated real-world search sessions spanning 11 different topics and information needs. By using scientifically formulated knowledge tests, we calibrated the knowledge of users before and after their search sessions, quantifying their knowledge gain. Our supervised models utilise and derive a comprehensive set of features from the current state of the art and compare performance of a range of feature sets and feature selection strategies. Through our results, we demonstrate the ability to predict and classify the knowledge state and gain using features obtained during search sessions, exhibiting superior performance to an existing baseline in the knowledge state prediction task.",0,,False
23,CCS CONCEPTS,0,,False
24,· Human-centered computing  User models; · Computing methodologies  Supervised learning; · Applied computing  Interactive learning environments;,0,,False
25,"ACM Reference Format: Ran Yu, Ujwal Gadiraju, Peter Holtz, Markus Rokicki, Philipp Kemkes, and Stefan Dietze. 2018. Predicting User Knowledge Gain in Informational",1,ad,True
26,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210064",1,ad,True
27,"Search Sessions. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/ 3209978.3210064",1,Session,True
28,1 INTRODUCTION,1,DUC,True
29,"Searching the web for information is among the most frequent online activities. Broder categorized web search queries into having either navigational, transactional or informational intents [4]. In informational web search sessions, the intent of a user is to acquire some information assumed to be present on one or more web pages.",0,,False
30,"Recent research in the search as learning (SAL) domain has recognized the importance of learning scopes and focused on observing and detecting learning needs during web search. Eickhoff et al. investigated the correlation between several query and search mission-related metrics and learning progress [8]. Wu et al. predicted the difficulty of search tasks from query and mission-related features [21]. Collins-Thompson et al. investigated the effectiveness of user interaction with respect to certain learning outcomes [7]. In addition, [22] has shown that data obtained during the search process provides valuable indicators about the domain knowledge of a user.",1,ad,True
31,"Although the importance of learning as an implicit element of web search has been established, there is still only a limited understanding of the impact of search behavior on a user's knowledge state and knowledge gain. Prior work has focused on improving the learning experience and efficiency during search sessions, but the measurement of a user's knowledge gain through the course of an informational search session has not yet been addressed. This is in part due to the difficulty in accurately quantifying knowledge gain through the course of a search session. If web search engines that are currently optimized for relevance can be re-molded to serve learning outcomes, the capability to predict knowledge gain will be a crucial step forward.",1,ad,True
32,"In this paper, we aim to address the aforementioned gap. We used crowdsourcing to recruit users who participated in real-world search sessions spanning 11 different topics and information needs. By using scientifically formulated knowledge tests, we calibrated the knowledge of users before and after their search sessions, quantifying their knowledge gain. We introduce a supervised model to predict a user's knowledge state and knowledge gain from features captured during the search sessions.",1,ad,True
33,75,0,,False
34,Session 1B: Log Analysis,1,Session,True
35,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
36,"Original Contributions. Through our work in this paper, we make the following contributions to the current body of literature: · A model for predicting the user's knowledge gain and state during",0,,False
37,real-world informational search sessions. · An analysis of the affect of user interactions (ranging from the,0,,False
38,"queries entered to their browsing behavior) on their knowledge state and knowledge gain. · We release a dataset capturing user behavior and interactions in 468 experimentally orchestrated informational search missions, capturing all features across the aforementioned dimensions as well as knowledge assessments obtained through pre- and posttests. Given the lack of comparable datasets which are both recent and publicly available, we anticipate that this corpus can facilitate SAL research related to different tasks. Implications. The capability to predict a user's knowledge state and gain through the course of an informational search session has the potential to reshape search engines to support learning outcomes as an implicit part of retrieval and ranking. This is of particular importance given that Web search already augments learning processes in a variety of informal as well as formal learning scenarios, such as classrooms, libraries and in work environments. Our contributions advance the current understanding of learning through web search, setting important precedents for further research.",1,ad,True
39,2 RELATED WORK,0,,False
40,"We discuss two main realms of closely related work ­ studies on the relation between (i) a user's search behavior and knowledge gain, and (ii) a user's search behavior and knowledge state.",0,,False
41,2.1 Search Behavior and Knowledge Gain,0,,False
42,"Eickhoff et al. [8] investigated the correlation between a number of features extracted from search session as well as SERP (Search Engine Results Page) documents with learning needs related to either procedural or declarative knowledge. Results obtained from an analysis of large-scale query logs showed the distinct evolution of particular features throughout search sessions and the correlation of document features with the actual learning intent. The influence of distinct query types on knowledge gain was studied by CollinsThompson et al. [7], finding that intrinsically diverse queries lead to increased knowledge gain. Gadiraju et al. [11] described the use of knowledge tests to calibrate the knowledge of users before and after their search sessions, quantifying their knowledge gain. They investigated the impact of information needs on the search behavior and knowledge gain of users.",1,ad,True
43,"Studies on exploratory search have also investigated a similar set of search behaviors that influence the learning outcome. Hagen et al. [14] investigated the relation between the writing behavior and the exploratory search pattern of writers. The authors revealed that query terms can be learned while searching and reading. In addition, Vakkari [19] provided a structured survey of features indicating learning needs as well as user knowledge and knowledge gain throughout the search process. Zhuang et al. [24] investigated the possibility of using 37 user search behavioral features to predict the user engagement with supervised classifiers. As the engagement in the search process usually is correlated with learning outcome, in",1,ad,True
44,our work we have also taken into consideration the set of features have been studied in this work.,0,,False
45,"The aforementioned prior works have either studied a limited set of features or have addressed only specific learning scenarios and learning types. The generalizability of knowledge gain measures in previous works has not been investigated. In this paper, we extend the current understanding of user knowledge gain in informational search sessions. Using real world information needs and search sessions on the Web, we investigate the possibility of using search activity related features to predict knowledge gain.",1,ad,True
46,2.2 Search Behavior and Knowledge State,0,,False
47,"By matching the learning tasks into different learning stages of Anderson and Krathwohl's taxonomy [1], Jansen et al. studied the correlation between search behaviors of 72 participants and their learning stage [15]. They showed that information searching is a learning process with unique searching characteristics corresponding to particular learning levels. Gwizdka et al. [12] proposed to assess learning outcomes in search environments by correlating individual search behaviors with corresponding eye-tracking measures. Syed and Collins-Thompson [18] proposed to optimize the learning outcome of the vocabulary learning task by selecting a set of documents while considering keyword density and domain knowledge of the learner.",0,,False
48,"White et al. [20] investigated the difference between the behavior of domain experts and non-experts in seeking information on the same topic. By analyzing the activity log of experts and non-experts across different domains, the authors found that the distribution of features such as number of queries and query length differed across the levels of expertise. Zhang et al. [22, 23] explored using search behavior as an indicator for the domain knowledge of a user. Through a small study (n ,"" 35), they identified features such as the average query length or the rank of documents consumed from the search results as being predictive. Further, Cole et al. [6], observed that behavioral patterns provide reliable indicators about the domain knowledge of a user, even if the actual content or topics of queries and documents are disregarded entirely.""",0,,False
49,"Other studies have focused on detecting task difficulty in search environments based on user activity data in situations where the subjective assessment of task difficulty is highly correlated to the user's domain knowledge [13, 17]. Gwizdka and Spence [13] showed that a searcher's perception of task difficulty is a subjective factor that depends on the domain knowledge and some other individual traits. Arguello [2] proposed to use logistic regression to predict task difficulty in a search environment. Data was collected through a crowdsourcing platform, and the author used search tasks created by Wu et al. [21], which contain task difficulty assessments on multiple dimensions.",0,,False
50,"The aforementioned studies focused on investigating the relation between search behavior and a user's knowledge state. Our work leverages these results to derive a comprehensive feature set for our supervised models. In contrast to prior works, we aim at predicting the knowledge state of a user ­ avoiding the need for explicit postsearch knowledge assessments.",0,,False
51,2,0,,False
52,76,0,,False
53,Session 1B: Log Analysis,1,Session,True
54,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
55,3 PROBLEM DEFINITION,0,,False
56,"In the context of Web search, Broder [4] classified search queries according to their intent into three classes: 1) navigational, 2) informational, and 3) transactional. Herein, informational queries are defined as those queries where `the intent of a user is to acquire some information assumed to be present on one or more web pages' [4]. Thus, informational queries imply a particular learning intent; intentional learning is generally defined as learning that is motivated by intentions and is goal directed [3], in contrast to latent or incidental learning.",0,,False
57,"Based on the constructs of intentional learning and informational queries, we arrive at the following definition:",0,,False
58,"Definition 3.1. Intentional Learning-Related Search Session. An intentional learning-related search session comprises of the sequence of a user's actions, with respect to satisfying her learning intent in a web search environment through informational queries. A user's sequence of actions begins with querying the web, and includes browsing through the search results, click and scroll activity, navigation via hyperlinks, query reformulations, and so forth.",1,Session,True
59,"For the sake of simplicity, we henceforth refer to informational sessions, i.e. sessions with a particular learning intent, as ""sessions"".",0,,False
60,"In this paper, from the observed user interactions in informational search sessions, we aim to predict (i) the knowledge state and (ii) knowledge gain of a user as follows.",0,,False
61,"Definition 3.2. Predicting a User's Knowledge State and Gain During Search Sessions. Let s be a search session starting at time ti and ending at time tj aimed at satisfying a particular information need, that is, a learning intent  of user u. Based on the user interactions during session s captured in the time period [ti , tj ], we aim to: (1) classify the knowledge state (KS) k(tj ) of u at time point tj with",1,Session,True
62,"respect to a particular information need. For the sake of this work, a user's knowledge state with respect to a particular information need is defined by the user's capability to correctly respond to a set of questions about the corresponding information need. We classify a user's knowledge state into 3 classes according to her capability: low knowledge state, moderate knowledge state and high knowledge state (Section 4.5). (2) classify the knowledge state change, i.e. the knowledge gain (KG) k(ti , tj ) of u during time period [ti , tj ] into different degrees. Similarly, a user's knowledge gain with respect to a particular information need is defined as the improvement of user capability (accuracy) to correctly respond to a set of test questions about the corresponding information need. We classify user knowledge gain into 3 classes according to the improvement of user capability: low knowledge gain, moderate knowledge gain and high knowledge gain (Section 4.5).",0,,False
63,4 OBTAINING SEARCH SESSION DATA,0,,False
64,"We adopted a crowdsourcing approach and orchestrated search sessions with varying information needs. All interactions of the users during the search sessions were logged. We analyzed the data to further the understanding of user knowledge evolution in informational search sessions on the Web. In this section, we describe the study design and experimental setup.",1,ad,True
65,4.1 Study Design and Search Environment,0,,False
66,"We recruited participants from CrowdFlower1, a premier crowdsourcing platform. At the onset, workers were informed that the task entailed `searching the Web for some information'. Workers willing to participate were redirected to our external platform, SearchWell2, a search system built on top of the Bing Web Search API. We logged worker activity on the platform including mouse movements, clicks, and key presses, using PHP/Javascript and the jQuery library. Workers were first asked to respond to a few questions (called `items') corresponding to a particular topic without searching the Web for answers. The questions took the form of statements pertaining to a topic, and workers had to select whether the statement was `TRUE', `FALSE', or `I DON'T KNOW' in case they were not sure. In this way, we calibrated the knowledge of users corresponding to a given topic. To encourage the workers to respond without external consultation, we informed them that their responses to these questions would not affect their pay. We also encouraged workers to avoid guessing. The results of this pre-test were used to calibrate the knowledge of the workers with respect to the topic. We describe the topics and how the knowledge tests were created in the following Section 4.2. On completing the knowledge calibration test, workers were presented with their actual task.",1,AP,True
67,"Workers were presented an information need corresponding to the topic of the calibration test they completed. They were told to use the SearchWell platform to search the Web and satisfy their information need. To incentivize workers towards realistic attempts to learn about the topic, we informed them that they will have to complete a final test on the topic to successfully finish the task. Furthermore, workers were conveyed the message that depending on their accuracy on the final test they could earn a bonus payment. We subsequently logged all the activities of the workers (mouse movements, key presses and clicks) within the SearchWell platform. Workers were allowed to begin the final test anytime after a search session, which is when a link to the final test was made available. Workers were encouraged to proceed to the next stage only once they felt that their information need was satisfied and when they were ready for the post-session test. On completing the post-session test, workers received a unique code that they could enter on CrowdFlower to claim their reward.",1,ad,True
68,"We restricted the participation to workers from English-speaking countries to ensure that they understood the task and instructions adequately [9, 10]. To ensure reliability of the resulting data, we restricted the participation to Level-3 workers3 on CrowdFlower.",1,ad,True
69,4.2 Topics ­ Defining Information Needs,0,,False
70,"We constructed a corpus of topics representing varying scopes of information needs (with some relatively broader than others). Topics were selected from the TREC 2014 Web Track dataset4, and corresponding information needs were defined accordingly. In all cases, the knowledge of users before beginning an informational search session was assessed using pre-tested and evaluated knowledge tests.",1,ad,True
71,"1 http://www.crowdflower.com/ 2 http://searchwell.l3s.uni-hannover.de/?uid,""12345678. 3Level-3 contributors on CrowdFlower comprise workers who completed over 100 test questions across hundreds of different types of tasks, and have a near perfect overall accuracy. They are workers of the highest quality on CrowdFlower. 4 http://www.trec.nist.gov/act_part/tracks/web/web2014.topics.txt""",1,trec,True
72,3,0,,False
73,77,0,,False
74,Session 1B: Log Analysis,1,Session,True
75,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
76,"Table 1: Topics and corresponding information needs presented to participants in the informational search sessions, along with the internal reliability of the corresponding knowledge tests. `1', `2' represent Cronbach's  for the pre-session test and post-session test respectively. `N' is the number of reliable participants after filtering.",0,,False
77,Topic,0,,False
78,Information Need,0,,False
79,1 2 N,0,,False
80,1. Altitude Sickness 2. American Revolutionary War 3. Carpenter Bees,0,,False
81,4. Evolution 5. NASA Interplanetary Missions 6. Orcas Island 7. Sangre de Cristo Mountains 8. Sun Tzu,0,,False
82,9. Tornado 10. USS Cole Bombing,1,ad,True
83,11. HIV,0,,False
84,"In this task you are required to acquire knowledge about the symptoms, causes and prevention of altitude sickness. (20 items) In this task, you are required to acquire knowledge about the `American Revolutionary War'. (10 items) In this task, you are required to acquire knowledge about the biological species `carpenter bees'. How do they look? How do they live? (10 items) In this task, you are required to acquire knowledge about the theory of evolution. (12 items) In this task, you are required to acquire knowledge about the past, present, and possible future of interplanetary missions that are planned by the NASA. (20 items) In this task you are required to acquire knowledge about the Orcas Island. (20 items) In this task, you are required to acquire knowledge about `Sangre de Cristo' mountain range. (10 items) In this task, you are required to acquire knowledge about the Chinese author Sun Tzu - about his life, his writings, and his influence to the present day. (15 items) In this task, you are required to acquire knowledge about the weather phenomenon that is called `tornado' (20 items) In this task, you are required to acquire knowledge about the 2000 terrorist attack that came to be known as the `USS Cole bombing'. (10 items) In this task you are required to acquire knowledge about the transmission, prevention, and consequences of HIV infection. (45 items)",1,ad,True
85,0.59 0.79 44 0.74 0.55 39 0.79 0.58 43,0,,False
86,0.55 0.72 42 0.80 0.75 40,0,,False
87,0.91 0.85 38 0.70 0.52 38 0.81 0.63 35,0,,False
88,0.82 0.62 37 0.83 0.55 38,0,,False
89,0.87 0.84 74,0,,False
90,"Knowledge tests are scientifically formulated tests that measure the knowledge of a participant on a given topic (for example, the HIV knowledge test [5]). Topics and test items are available online5.",0,,False
91,"Knowledge on all given topics was measured using knowledge tests comprising of between 10 and 45 items. The answer options were in all cases `TRUE', `FALSE', and `I DON'T KNOW'. The differences in the number of items reflects our attempt to feature varying scopes of information needs; relatively narrow (e.g., Carpenter Bees­ 10 items) as well as broad (e.g., NASA Interplanetary Missions­20 items). In the construction of all scales, an item pool comprising of more items than finally used was constructed. After a pilot test with 100 distinct participants recruited via CrowdFlower for each of the 11 topics, items that proved to be either too easy (e.g., more than 80% correct answers) or too hard/ambiguous (e.g., more false than true answers) were discarded. Table 1 presents the topics and corresponding information needs considered for orchestrating the informational search sessions. It also shows the internal reliability (using Cronbach's ) of the pre- and post-session knowledge tests corresponding to each topic. We observe moderate to high values of  in the pre- and post session knowledge tests, suggesting a desirable level of internal consistency.",1,ad,True
92,4.3 Data Collection,0,,False
93,"To further ensure the reliability of responses and the behavioral data thus produced in the search sessions, we filtered workers using the following criteria. · Workers who entered no queries in the SearchWell system. Since",0,,False
94,"the aim of our work is to further the understanding of how the knowledge state of a user evolves in informational search sessions, we discard those users who did not enter a search query. · Workers who selected the same option; either `YES' or `NO', for all items in the calibration test or the post-session test. · Workers who did not complete the post-session test. We filtered out 132 workers due to the aforementioned criteria, resulting in 468 workers across the 11 topics. The analysis and results presented hereafter are based on these 468 sessions",0,,False
95,"alone. For the benefit of further research in this community, the filtered data has been thoroughly anonymized and made publicly available5. We henceforth refer to these filtered workers as users in our experimentally orchestrated information search sessions.",1,ad,True
96,4.4 Descriptive Analysis ­ Important Details,0,,False
97,"We measure the knowledge gain of users in search sessions corresponding to an information need as the difference between their knowledge calibration score and the post-session test score6. Table 2 presents the average knowledge calibration scores, post-session test scores, and the resulting knowledge gain of users across search sessions with different information needs. Across all topics and search sessions, we found that users exhibited an average knowledge gain of around 19%. Nearly 70% of all the workers exhibited a knowledge gain, while the remaining workers did not. The standard deviation observed in the knowledge gain of users across all topics is notably high, due to the varying domain knowledge of users. This is evident from the average calibration scores in Table 2.",0,,False
98,"Table 2: The average knowledge gain of users across the different topics. To enhance readability, the rows have been ordered by ascending knowledge gain.",1,ad,True
99,Topic / Information Need,0,,False
100,"HIV (N,74) Evolution (N,42) NASA Interplanetary Missions (N,40) Altitude Sickness (N,44) Sangre de Cristo Mountains (N,38) Tornados (N,37) Sun Tzu (N,35) American Revolutionary War (N,39) Carpenter Bees (N,43) USS Cole Bombing (N,38) Orcas Island (N,38)",1,ad,True
101,"Overall (N,468)",0,,False
102,Avg. Calibration Score (in %) 66.25 ± 14.86 34.07 ± 17.99 38.1 ± 20.53,0,,False
103,55.88 ± 16.31 33.25 ± 22.40,0,,False
104,34.44 ± 21.02 40.54 ± 23.37 34.52 ± 25.65,0,,False
105,45.65 ± 27.08 30.95 ± 25.22 34.74 ± 30.08 40.76 ± 22.23,0,,False
106,Avg. Post Score (in %) 71.68 ± 14.48 48.15 ± 22.49 52.5 ± 17.43,0,,False
107,70.66 ± 19.11 49.75 ± 18.10,0,,False
108,53.47 ± 16.28 60.18 ± 17.15 55.95 ± 20.71,0,,False
109,67.17 ± 20.29 54.37 ± 16.29 65.51 ± 22.04 59.04 ± 18.58,0,,False
110,Knowledge Gain (in %),0,,False
111,5.44 ± 10.02 14.07 ± 18.66 14.40 ± 22.10,0,,False
112,14.78 ± 17.76 16.50 ± 22.31,0,,False
113,19.03 ± 22.010 19.64 ± 21.59 21.43 ± 27.31,0,,False
114,21.52 ± 30.50 23.41 ± 31.30 30.77 ± 30.25 18.27 ± 23.07,0,,False
115,5 https://sites.google.com/view/predicting-user-knowledge,0,,False
116,"6We consider the `I DON'T KNOW' options that were selected, as incorrect responses while computing the knowledge calibration scores and post-session test scores.",0,,False
117,4,0,,False
118,78,0,,False
119,Session 1B: Log Analysis,1,Session,True
120,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
121,"We found that on average, the highest knowledge gain was observed through the search sessions corresponding to the topic, `Orcas Island', while the least knowledge gain was observed through those corresponding to the topic, `HIV '. We found a negative linear relationship between topics that workers were familiar with (indicated by their calibration scores) and their knowledge gain; R,"" -.65, R2 "","" .42, p < .001. This suggests that the more popular a topic is, or the more familiar that users are with a topic, the lesser they tend to learn about the topic in informational search sessions. Thus, we found that 42% of the variance in the knowledge gain of users can be explained by the topic familiarity.""",0,,False
122,"We found that the average session length of users across the different topics was nearly 5 mins long (M,""4.82, SD"",""5.20). During the search sessions, users navigated to over 5 web pages on average (M"",""5.46, SD"",3.41). We note that on average users entered 2 distinct queries in a search session (M,""2.20, SD"",""2.18), with an average query length of just over 4 terms (M"",""4.56, SD"",""2.63). For each query that was entered, users navigated to over 3 web pages on average (M"",""3.27, SD"",1.60). Users spent nearly 2 minutes actively on web pages they navigated to (M,""1.97, SD"",1.52).",0,,False
123,4.5 Knowledge State and Knowledge Gain Classes,0,,False
124,Table 3: User groups created based on aver ae ± 0.5S D.,0,,False
125,Task Mean SD Low,0,,False
126,Moderate High,0,,False
127,KG 0.193 0.231 167,0,,False
128,179,0,,False
129,122,0,,False
130,KS 0.618 0.191 145,0,,False
131,171,0,,False
132,152,0,,False
133,"We used a Standard Deviation Classification approach to obtain three classes of learners with regard to their level of knowledge. Assuming approximately normal distributions of the respective test scores (X) for the different topics, we transformed the test scores into Z-scores with a mean of 0 and a Standard Deviation (SD) of 1 (standardization). We then used statistically defined intervals (X < 0.5 SD , low; -0.5 SD < X < 0.5 SD , moderate; 0.5 SD < X ,"" high) for the classification of the learners into roughly equal groups with low, moderate, or high knowledge. The same procedure was repeated for knowledge gain. Here as well, the empirical knowledge gain for every test was transformed into corresponding Z-scores and three roughly equal groups (low knowledge gain; moderate knowledge gain; high knowledge gain) were defined accordingly. In view of the substantial variety of different topics, we argue that such a tripartite categorization of knowledge states and knowledge gains respectively allows for the construction of robust models, which are themselves based on a large variety of features. Thus, insights from the learning tasks considered can be generalized to other similar intentional learning activities. This procedure weighs all different knowledge tests equally irrespective of the number of items.""",0,,False
134,5 FEATURE EXTRACTION AND ANALYSIS,0,,False
135,"We approach the problem of predicting knowledge state (k(tj )) and knowledge gain (k(ti , tj )) described in Section 3 with supervised models for classification, where details about the applied classification models are given in Section 6.1. To this end, each session s is represented by a feature vector vì ,"" (f1, f2, ..., fn ), where""",0,,False
136,considered features are described in Section 5.1 and analyzed in Section 5.2.,0,,False
137,5.1 Features Considered,0,,False
138,"We extracted features according to multiple dimensions of a search session, structured into five categories, namely features related to the session, queries, SERP, browsing behavior and mouse movements. The SERP category consists of features extracted from direct interactions with SERP items, while the browsing category consists of features extracted from subsequent user navigation beyond simple SERP clicks. The majority of features is motivated by existing literature, yet none of the features have been used on the inferential tasks of this work.",0,,False
139,"All considered features fi are listed in Table 4 together with the Pearson Correlation Coefficient scores Corr (fi , k(ti , tj )), Corr (fi , k(tj )) between the respective feature and the knowledge gain (state).",0,,False
140,Session Features. The relation between feature s_duration and different stages of learning has been discussed by Jansen et al. [15]. It has been shown that there is a difference in the duration of sessions among the classifications in Anderson and Krathwohl's taxonomy [1]. White et al. [20] also found that the sessions conducted by domain experts were generally longer than non-expert sessions.,1,Session,True
141,"Query Features. Several prior works [2, 15, 20] have investigated the correlation between query activities in a search session and learning performance. Based on the study by White et al. [20], the number of queries (q_num) applied by experts and non-experts show big differences across domains: non-expert users usually run significantly more queries than experts. Jansen et al. [15] also found that the number of queries applied on learning tasks classified as applying stage was significantly different from other learning stages.",1,Query,True
142,"The length of queries (q_term_max {min, av, total }) has been found to have a strong correlation with learning outcome by Zhang et al. [22]. Their study shows that the average query length and user domain knowledge is correlated with a Pearson correlation score of 0.344.",0,,False
143,"The complexity of queries (q_complexity_max_di f f ) has been investigated by Eickhoff et al. [8], and has been found to evolve during the learning process. We applied the same query complexity measure as in [8], which is computed based on the dictionary created by Kuperman et al. [16] that contains a listing of more than 30,000 English words along with the age at which native speakers typically learn the term. The maximum age of acquisition across all query terms is used as query complexity.",0,,False
144,"Furthermore, the investigation from Arguello [2] shows that beside the number of total terms, the number of unique terms (q_uniq_term_{max, min, av, total }, q_uniq_term_ratio) in the session is strongly correlated with knowledge level on the task, while the number and ratio of stop words do not have a big difference when comparing between search sessions with different levels of domain knowledge.",0,,False
145,"As we aim at predicting knowledge state change during a session, similarly to the features discussed above, we extract the features q_len_{ f irst, last }, q_uniq_term_{ f irst, last }, which potentially",0,,False
146,5,0,,False
147,79,0,,False
148,Session 1B: Log Analysis,1,Session,True
149,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
150,Table 4: Features for prediction of knowledge gain and knowledge state.,0,,False
151,CategoryNotation,0,,False
152,Session,1,Session,True
153,s_dur at ion s_dur at ion_per _q,0,,False
154,"Cor r (fi, k (ti, tj ))",0,,False
155,-0.020 -0.019,0,,False
156,Query,1,Query,True
157,"q_num q_t erm_{max, min, av, t ot al } q_uniq_t erm_{max, min, av, t ot al }",0,,False
158,"q_uniq_t erm_r at io q_l en_{f ir st, l ast } q_uniq_t erm_{f ir st, l ast } q_compl exity_{max, min, av } q_compl exity_max _dif f",0,,False
159,"0.052 {0.0002,-0.094,-0.042,0.047} {0.016,-0.087,-0.024,0.06}",0,,False
160,"0.083 {-0.049,0.055} {-0.023,-0.040} {0.097,0.086,0.093} {0.092}",0,,False
161,SERP,0,,False
162,S ERP _click,0,,False
163,-0.009,0,,False
164,"S ERP _click _r ank _{hihest, lowest, av } {-0.101,-0.021,-0.017}",0,,False
165,S ERP _click _int erval,0,,False
166,0.036,0,,False
167,S ERP _click _per _query,0,,False
168,-0.007,0,,False
169,"S ERP _no_click _query_{num, pct }",0,,False
170,"{0.041,-0.051}",0,,False
171,"S ERP _t ime_{tot al, av, max }",0,,False
172,"{0.039,0.022,0.049}",0,,False
173,S ERP _av_t ime_t o_f ir st _cl ick,0,,False
174,-0.002,0,,False
175,"b _nu m b _u ni q _nu m b_num_per _q b_uniq_num_per _q b_t ime_tot al b_t ime_av_per _q Browsing b_t ime_{max, av }_per _pae b_r evisit ed _r at io b_{num, pct }_f r om_S ERP b_{num, pct }_f r om_non_S ERP b_dist inct _domain_num b_t tl _l en_{max, min, av, t ot al } b_pae_size_{max, min, av, t ot al } b_t tl _q_over l ap_{max, min, av, t ot al } b_ur l _q_over l ap_{max, min, av, t ot al }",0,,False
176,"-0.018 0.029 -0.017 -0.017 0.243 0.236 {0.306,0.291} -0.058 {-0.017,0.058} {-0.056,0.057} -0.033 {-0.08,-0.058,-0.078,-0.109} {0.109,-0.093,0.122,0.086} {0.15,0.089,0.14,0.091} {0.16,0.064,0.133,0.044}",0,,False
177,Mouse,0,,False
178,m_num m_num_per _q m_r ank _max m_r ank _max _per _q m_scr oll _dist m_scr oll _dist _per _q m_scr oll _max _pos m_scr oll _max _pos_per _q,0,,False
179,0.066 0.094 0.091 0.095 0.120 0.120 0.142 0.127,0,,False
180,"Cor r (fi, k (tj ))",0,,False
181,0.066 0.066,0,,False
182,"0.103 {0.065,0.032,0.051,0.068} {0.104,0.05,0.084,0.089}",0,,False
183,"-0.002 {0.031,0.105} {0.036,0.087} {0.087,0.078,0.049} {0.077}",0,,False
184,"0.063 {-0.063,0.047,0.095} 0.022 -0.012 {0.077,0.029} {0.091,-0.008,0.043} -0.027",0,,False
185,"0.075 0.109 -0.016 -0.016 0.134 0.063 {0.104,0.089} -0.020 {0.074,0.056} {-0.028,0.025} 0.102 {0.146,0.106,0.146,0.082} {-0.055,-0.074,-0.057,-0.01} {0.005,-0.028,-0.018,0.023} {0.041,0.018,0.025,0.028}",0,,False
186,0.113 0.053 0.067 0.039 0.058 0.025 0.052 0.021,0,,False
187,Feature description,0,,False
188,Duration of the search session of a worker on a given topic Session duration per query,1,Session,True
189,Number of queries in session s,0,,False
190,"Maximum, minimum, average, total number of query terms",0,,False
191,"Maximum, minimum, average number of unique terms per query",0,,False
192,Number,0,,False
193,of,0,,False
194,query,0,,False
195,terms,0,,False
196,/,0,,False
197,unique,0,,False
198,query,0,,False
199,terms,0,,False
200,(,0,,False
201,q_uniq_t e r m_t ot q_t e r m_t ot al,0,,False
202,al,0,,False
203,),0,,False
204,"First, last query length",0,,False
205,"Number of unique terms of first, last query",0,,False
206,"Maximum, minimum, average of query complexity",0,,False
207,Difference between the maximum and minimum complexity,0,,False
208,"Total number of click on search result Average, highest, lowest rank of the clicks Average interval between clicks Average number of clicks per query Number, percentage of SERP with no clicks Total, average, maximum time spend on SERPs Time till first click",0,,False
209,"Total number of pages browsed in session Number of unique pages browsed in session Average number of page browsed per query Average number of unique page viewed per query Total active time on the pages Average active time on the browsed pages per query Maximum, average active time on the browsed pages Ratio of revisited pages Number, percentage of pages visited through SERP Number, percentage of pages visited through pages other than SERP Number of distinct domains of the visited pages Maximum, minimum, average, total page title length Maximum, minimum, average, total page size Maximum, minimum, average and total overlap between query and page title Maximum, minimum, average, total term overlap between query and page URL",0,,False
210,total number of mouseovers in the session average number of mouseovers per query max mouseover rank in the session average max mouseover rank per query total scroll distance in session average scroll distance per query max scroll position in session average max scroll position per query,0,,False
211,are indicators of the knowledge level at the beginning and end of the session.,0,,False
212,"SERP Activity Features. Some activities on SERP have also been investigated by previous works. Specifically, CollinsThompson [7] found that the total number of clicks on SERP (SERP_click) is strongly correlated with a user's understanding of the topic. The analysis shows that users tend to click more often when having stronger interest in the topic.",0,,False
213,"The ranking position of the clicked URL on SERP has also been shown to be a strong indicator of user domain knowledge by Zhang et al. [22]. In [2], the authors discovered that the difficult tasks with which a user is less knowledgeable are associated with more clicks (SERP_click), more clicks on lower ranks (SERP_click_rank_{hihest, lowest, av}), more abandoned queries (SERP_no_click_query_{num, pct }), i.e. queries without clicks, longer time till first click (SERP_av_time_to_f irst_click) and longer time till next click (SERP_click_interval).",0,,False
214,"Browsing Features. Browsing features such as number of documents viewed (b_num, b_uniq_num) and average number of documents viewed per query (b_num_per _q, b_uniq_num_per _q) were shown by several previous works [2, 8, 13, 15] to be positively correlated with the knowledge improvement. More detailed features corresponding to the browsing behavior have also been studied,",0,,False
215,"indicating that the more difficult a task is for a user, the higher the ratio of revisited pages (b_revisited_ratio) is.",0,,False
216,"Despite the number of pages visited, the time spent (corresponds to features b_time_total, b_time_{max, av}_per _q etc.) on the accessed pages are found to vary to a large extent between domain expert and non-expert [20]. Feature SERP_time_{total, av, max } was shown to be effective for predicting the user's assessment of task difficulty [2], which is subject to the user's knowledge state.",0,,False
217,"We further distinguish the viewed pages into two sets {pages navigated through SERP, pages navigated through non-SERP}, by parsing its ancestor page. Hence we extract the features b_{num, pct }_f rom_SERP and b_{num, pct }_f rom_non_SERP that are motivated by the features introduced above.",0,,False
218,"The content of the accessed Web documents strongly influence the user's learning outcome. White et al. [20] found that domain experts encountered different and more diverse domains (feature b_distint_domain_num) than domain novices. Several other document content related features: page size (b_pae_size_{max, min, av, total }), title length b_ttl_len_{max, min, av, total } have also been found to evolve during the learning process [8]. Based on the assumption that domain experts and novices have different capabilities of choosing learning resources, for instance, experts are able to recognize",0,,False
219,6,0,,False
220,80,0,,False
221,Session 1B: Log Analysis,1,Session,True
222,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
223,"useful documents without query terms presented in the page title, we computed features based on the overlap between page title and query (b_url_q_overlap_{max, min, av, total }). The page URL (b_ttl_q_overlap_{max, min, av, total }) as a complementary source containing hints about a page's content has also been considered in the feature extraction process.",0,,False
224,Mouse Features. Features in the Mouse category are indicators of quantity and quality of user interactions with a knowledge source and were also shown to be effective for predicting the user's assessment of task difficulty [2].,0,,False
225,5.2 Feature Analysis and Selection,0,,False
226,"As a basis for feature selection, we analyze the features with respect to their relationship to knowledge gain and knowledge state, as well as their redundancy.",0,,False
227,"Correlation between feature and KG (KS). In order to select the most influential features for the prediction task, we compute Corr (fi , k(ti , tj )) and Corr (fi , k(tj )), i.e. the Pearson correlation coefficient between each feature and the knowledge gain (knowledge state). The correlation scores are shown in Table 4. Based on the computed score, we select the features fulfilling the condition Corr (fi , k(ti , tj ))   for the knowledge gain prediction task and Corr (fi , k(tj ))   for the knowledge state prediction task. Performance of the prediction model using features selected based on varied  and  has been evaluated and corresponding results are presented in Section 7.",0,,False
228,"Correlation between features. We compute the Pearson correlation coefficient between each pair of features Corr (fi , fj ). If Corr (fi , fj )   , i.e. features appear to be not independent, we remove the feature from the feature set, that has lower Corr (fi , k(ti , tj )) respectively lower Corr (fi , k(tj )) for knowledge gain (state) prediction. We evaluate the performance of the prediction model for different values of  . The feature selection results are reported in Section 7.",0,,False
229,"·  - threshold for feature selection based on correlation between features. We also experimented with different  in the range of {1.0, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7}. The number of features in the feature set corresponding to given  ,  and  is reported in Table 5.",0,,False
230,Table 5: Number of features of different configurations.,0,,False
231, (KG),0,,False
232, (KS),0,,False
233,0.0 0.05 0.1 0.15 0.2 0.25 0.3 0.0 0.05 0.1 0.15,0,,False
234," , 1.0 70 43 16 6 4 2 1 70 41 11 0  , 0.95 66 42 16 6 4 2 1 66 38 11 0  , 0.9 56 37 13 6 4 2 1 58 34 11 0  , 0.85 43 29 10 6 4 2 1 44 24 9 0  , 0.8 39 26 7 4 2 1 1 38 19 8 0  , 0.75 37 25 7 4 2 1 1 34 17 7 0  , 0.7 33 24 6 3 1 1 1 32 16 7 0",0,,False
235,"6.1.2 Baseline. As discussed in Section 2, the tasks addressed in this paper are comparably novel. To the best of our knowledge, there are no existing baselines for the task of knowledge gain prediction during informational Web search missions. Therefore, we compare our approach for a number of configurations (described above), using multiple standard classification models. For the prediction of knowledge state k(tj ), we compare our approach in addition to one existing baseline [23]. KSZhan refers to the linear regression model fitted by Zhang et al. [23] for domain knowledge prediction as shown in Equation 1.",1,ad,True
236,"K SZ han , -1.466+0.039 · S aved +0.147 ·Ql en +0.130 · Relmean (1)",0,,False
237,"Saved represents the number of documents saved by the user, which is an extremely sparse feature in a real search environment and does not appear in our dataset. Qlen is the mean query length and Relmean is the mean rank of documents opened in SERPs. As the output of the baseline regression model is a real number, we convert the result into 3 classes according to the definition given in Section 4.5.",0,,False
238,6 EVALUATION - EXPERIMENTAL SETUP,0,,False
239,6.1 Approach Configurations & Baselines,0,,False
240,6.1.1 Configurations and Parameters. · Classifier. We apply a range of standard models for the classifi-,0,,False
241,"cation of the knowledge gain and knowledge state, namely, Naive Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forrest (RF), and Multilayer Perceptron (MP). For our experiments, we used the Weka library for Java7. For each of the configurations described below, we perform grid search to tune the hyperparameters of all of the classifiers. In Section 7, we report the result of the best performing hyperparameter configuration for each classifier. ·  ( )- threshold for feature selection based on correlation between feature and KG (KS). We compare prediction performance before and after applying the selection based on featureKG (KS) correlation. We set the threshold  ( ) for selecting the features in the range of {0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3} ({0.0, 0.05, 0.1, 0.15}). We omit results for larger  ( ), as the resulting number of features is insufficient for training a classifier.",0,,False
242,7 https://www.cs.waikato.ac.nz/ml/weka/,0,,False
243,6.2 Evaluation Metrics,0,,False
244,"For both tasks, we run repeated 10-fold cross-validation with 10 repetitions on all the approaches and configurations described in Section 6.1 and evaluate the results according to the following metrics: · Accuracy (Accu) across all classes: percentage of search ses-",0,,False
245,"sions that were classified with the correct class label. · Precision (P), Recall (R), F1 (F 1) score of class i: we compute",0,,False
246,"the standard precision, recall and F1 score on the prediction result of each class i. · Macro average of precision (P), recall (R), and F1 (F 1): the average of the corresponding score across 3 classes. · Runtime: the time consumed for completing the 10-fold crossvalidation on experimental dataset in milliseconds. To analyze the usefulness of individual features, we make use of the Mean Decrease Accuracy (MDA) metric, which is based on the Random Forest model, i.e. a very well performing model for both tasks as shown in Section 7. MDA quantifies the importance of a feature by measuring the change in prediction accuracy of the Random Forest model when the values of the feature are randomly permuted compared to the original observations.",0,,False
247,7,0,,False
248,81,0,,False
249,Session 1B: Log Analysis,1,Session,True
250,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
251,Method,0,,False
252,NB LR SVM RF MP,0,,False
253,0.85 0.25 0.85 0.05 0.90 0.00 0.95 0.00 0.85 0.25,0,,False
254,Table 6: Performance in knowledge gain prediction task.,0,,False
255,#Features Runtime P,0,,False
256,Low,0,,False
257,Moderate,0,,False
258,High,0,,False
259,Macro average All,0,,False
260,R F1 P R F1 P R F1 P R F1 Accu,0,,False
261,2,0,,False
262,19.1,0,,False
263,0.450 0.747 0.562 0.483 0.268 0.344 0.513 0.384 0.439 0.482 0.467 0.448 0.469,0,,False
264,29,0,,False
265,653.9,0,,False
266,0.498 0.537 0.516 0.459 0.382 0.416 0.379 0.431 0.403 0.445 0.450 0.445 0.450,0,,False
267,56,0,,False
268,441.6,0,,False
269,0.488 0.595 0.536 0.487 0.340 0.400 0.410 0.469 0.437 0.462 0.468 0.458 0.465,0,,False
270,66,0,,False
271,3739.3 0.521 0.542 0.531 0.469 0.410 0.437 0.425 0.480 0.450 0.472 0.477 0.473 0.475,0,,False
272,2,0,,False
273,1919.3 0.452 0.556 0.497 0.421 0.312 0.356 0.425 0.450 0.435 0.433 0.439 0.429 0.435,0,,False
274,Figure 1: Feature importance for knowledge gain prediction.,0,,False
275,7 RESULTS: PREDICTION PERFORMANCE AND FEATURE ANALYSIS,0,,False
276,"In this section, we report the evaluation results of the prediction performance as well as an analysis of feature importance.",0,,False
277,7.1 Knowledge Gain Prediction,0,,False
278,"Performance of different Configurations. For each of the 245 distinct configurations described in Section 6, we run repeated cross-validation as described in the previous section.",0,,False
279,"From all the different combinations of  and  as listed in Table 5, we present the result of the configuration that produces the highest accuracy for each classifier in Table 6 due to space constraints. A complete set of the evaluation results are available online8. We observed that in the knowledge gain prediction task, the highest average F1 score across classes and the highest accuracy always appear in the same configuration for all the classifiers except for LR, where there is a minor difference of .001 in F1 between the two.",0,,False
280,"The Random Forest classifier achieves the best performance in terms of accuracy and average F1 score, slightly outperforming Naive Bayes and SVM. As shown, Naive Bayes is the most efficient classifier in terms of computation time for feature sets of comparable size. Comparing classification performance for the different classes, the results for each of the classifiers consistently show higher F1 scores for the `Low' and `High' knowledge gain classes than for the `Moderate' knowledge gain class. Overall, prediction accuracy is above 0.435 and the F1 score is above 0.429 for all of the classifiers, which indicates that the set of features we extracted from search activities can provide meaningful evidence for predicting knowledge gain.",0,,False
281,8 https://sites.google.com/view/predicting-user-knowledge,0,,False
282,"Feature Impact. The MDA results of each feature are shown in Figure 1. Based on the result, the most important features are: b_time_max_per _pae, b_time_av_per _pae and b_time_total. Mostly active time related features which reflect the effort users spend on learning. Similarly, these features are immediately followed by m_scroll_dist_per _q, m_scroll_max_pos, and SERP_click_rank_lowest, three features that are indicators for the amount of information a user has seen during the session.",0,,False
283,"As shown in Table 6, the NB and MP classifier performs best when using 2 features only, namely b_time_max_per _pae and b_time_av_per _pae. These two features are confirmed as the most important features by our feature importance analysis.",0,,False
284,"Regarding feature categories, the 10 most useful features in terms of MDA belong to the browsing, mouse and SERP categories. Surprisingly, although multiple query features had above average correlation to knowledge gain (see Table 4) ­ in particular, the features related to query complexity (q_complexity_{max, min, av} and q_complexity_max_di f f ) had correlations ranging from .086 to .097, compared to the median of .042 ­ q_uniq_term_total is the only query feature among the 25 highest ranked according to MDA. Analogously, both session features s_duration and s_duration_per _q appear among the 25 highest ranked features despite their relatively low correlations of -.02 and -.019.",1,ad,True
285,7.2 Knowledge State Prediction,0,,False
286,"Performance of different Configurations. We have experimented with all different combinations of  and  as listed in Table 5 for all considered classifiers. The result of the configuration that produces the highest accuracy for each classifier is shown in Table 7. We observe that in the knowledge state prediction task, the highest average F1 score across classes and the highest accuracy always",0,,False
287,8,0,,False
288,82,0,,False
289,Session 1B: Log Analysis,1,Session,True
290,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
291,Method  ,0,,False
292,NB LR SVM RF MP,0,,False
293,0.75 0.1 1.00 0.05 0.95 0.05 1.00 0.00 1.00 0.05,0,,False
294,KSZhan -,0,,False
295,-,0,,False
296,Table 7: Performance in knowledge state prediction task.,0,,False
297,#Features Runtime P,0,,False
298,Low,0,,False
299,Moderate,0,,False
300,High,0,,False
301,Macro average All,0,,False
302,R F1 P R F1 P R F1 P R F1 Accu,0,,False
303,7,0,,False
304,23.5,0,,False
305,0.352 0.712 0.470 0.424 0.218 0.287 0.370 0.211 0.268 0.382 0.380 0.342 0.369,0,,False
306,41,0,,False
307,797.7,0,,False
308,0.338 0.383 0.359 0.402 0.368 0.384 0.372 0.359 0.366 0.370 0.370 0.370 0.370,0,,False
309,38,0,,False
310,292.3,0,,False
311,0.359 0.479 0.409 0.395 0.303 0.342 0.409 0.386 0.397 0.388 0.389 0.383 0.385,0,,False
312,70,0,,False
313,4023.4 0.443 0.456 0.449 0.394 0.358 0.374 0.418 0.447 0.432 0.418 0.421 0.418 0.418,0,,False
314,41,0,,False
315,43619 0.380 0.414 0.396 0.398 0.298 0.341 0.385 0.461 0.419 0.388 0.391 0.385 0.387,0,,False
316,2,0,,False
317,23,0,,False
318,0.320 0.428 0.366 0.328 0.240 0.277 0.362 0.355 0.359 0.337 0.341 0.334 0.335,0,,False
319,Figure 2: Feature importance for knowledge state prediction.,0,,False
320,appear in the same configuration for all the classifiers except Naive Bayes (average F1 of the highest accuracy configuration is 0.006 lower than the maximum average F1).,0,,False
321,"Among all evaluated classifiers, Random Forest reaches the highest accuracy and F1 score, outperforming the other classifiers.",0,,False
322,"Comparison to Baseline. We compare the performance of our approach against the baseline method (KSZhan), shown in the last row in Table 7. The result suggests that, the linear regression model fitted in previous work based on data collected through a lab study does not perform well in the knowledge state prediction task and is outperformed by all five classifiers following our approach.",0,,False
323,"Feature Impact. The MDA results of each feature in the knowledge state prediction tasks are shown in Figure 2. The most important features (q_complexity_av, b_ttl_len_min and b_ttl_len_av) reflect the user's capability of constructing a query and choosing relevant resources. In terms of feature categories, all of the highest ranked features for this task belong to the query and browsing categories.",0,,False
324,"Compared to the knowledge gain task, query complexity features (q_complexity_{min, max, av}) are considerably more useful, while features related to time and effort invested, like b_time_max_per _pae and b_time_av_per _pae, are among the lowest ranked. Other query features related to the used vocabulary (e.g. q_uniq_term_min, q_uniq_term_av, and q_term_total) are ranked similarly highly. Apparently, while the time taken by users to take in the discovered documents is predictive of their knowledge gain, their capability of using complex queries and selecting relevant resources reveals more about their knowledge state.",0,,False
325,8 DISCUSSION,0,,False
326,"Based on the experimental results, we conclude that: i) knowledge gain (state) can be predicted during informational search sessions with a certain level of accuracy, ii) performance of the knowledge gain prediction appears to be generally better, suggesting that the task is easier given the nature of our data, and iii) the performance of the prediction approach is better for more extreme classes, i.e. for low and high knowledge gain (state) classes, whereas performance on the moderate classes is lowest in both tasks, presumably due to the moderate classes being the most overlapping ones with respect to their characteristics. In this section we discuss some of the reasons behind these observations.",0,,False
327,"Most of the features we considered were found to correlate rather weakly with knowledge gain (state). Intuitively, this could be due to the limited duration of the search sessions (just over 5 minutes on average). This could potentially reduce the predictive power of certain features, such as the number of queries or the number of accessed documents. This also rendered evolution-oriented features, which would capture the evolution of queries and behavior throughout a session predictively poor. While these would supposedly be highly indicative of the knowledge gain, they require longer sessions than are usually observable in real-world search sessions as well as in our experimental data.",0,,False
328,"For the prediction of knowledge gain, our feature analysis result shows that the most important features are the ones related to the user's active time. As our experimental dataset contains mostly short sessions, it is understandable that the time spent affects the knowledge gain strongly. However, we believe that in longer search sessions, the learning pattern and the initial knowledge state of a",0,,False
329,9,0,,False
330,83,0,,False
331,Session 1B: Log Analysis,1,Session,True
332,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
333,user might be more influential for the knowledge gain than in short sessions. Further experiments are required to establish this.,0,,False
334,"The results suggest that with the presented approach, the knowledge gain prediction is an easier task than the knowledge state prediction. As shown in Figure 2, the most important features for knowledge state prediction are the features related to the content of queries and browsed documents. Intuitively, these features are also central to the knowledge gain prediction task. Yet, we observe that although the topic descriptions that were given to the users typically provided central keywords for the first query, only a very limited set of queries (1-2) are fired by most users. Given the small number of queries in each session, the query features are less distinguishable and hence, less indicative of the knowledge gain. Thus, query evolution is observable only to a very limited extent.",0,,False
335,9 CONCLUSIONS AND FUTURE WORK,0,,False
336,"In this paper, we propose to use classification models to predict user knowledge state and knowledge gain from behavioral data captured during real-world informational search sessions. Given the lack of available datasets and ground truths, we have created an experimental dataset using crowdsourcing, capturing 468 informational search sessions including user interactions and behavioral traces throughout the process, together with calibration and corresponding post-session knowledge test results.",0,,False
337,"Previous work related to the aforementioned prediction tasks is still very limited. However, existing state of the art was considered when identifying a novel set of 70 features, partially motivated from related work as well as from exploring our gathered data. Classification experiments were conducted with 5 classification models in a variety of configurations and in comparison to a baseline in the knowledge state prediction task.",0,,False
338,"The experimental results underline that a user's knowledge gain and knowledge state can be modeled based on a user's online interactions observable throughout the search process. Through feature analysis, we provide evidence for an improved understanding between individual user behavior and the corresponding knowledge state and change. Alongside these results, we also make the gathered dataset available. This dataset captures user interactions throughout diverse informational search sessions and corresponding knowledge assessments, and thereby provides a resource which can facilitate further research in this area.",0,,False
339,"As a part of future work, we aim to reproduce and refine the findings in more varied search sessions, where durations and learning intents are more diverse; involving considerably longer search sessions and, for instance, procedural knowledge rather than intents focused on declarative knowledge only. This would provide the opportunity to observe evolution-oriented features, such as considering the evolution of queries, their length and complexity.",0,,False
340,"Potential applications of this work include the consideration of user knowledge and the expected learning progress of a user as part of Web search engines and information retrieval approaches, or within informal learning-oriented search settings, such as libraries or knowledge- and resource-centric online platforms.",0,,False
341,REFERENCES,0,,False
342,"[1] L. W. Anderson, D. R. Krathwohl, P. Airasian, K. Cruikshank, R. Mayer, P. Pintrich, J. Raths, and M. Wittrock. A taxonomy for learning, teaching and assessing: A revision of bloom's taxonomy. New York. Longman Publishing. Artz, AF, & Armour-Thomas, E.(1992). Development of a cognitive-metacognitive framework for protocol analysis of mathematical problem solving in small groups. Cognition and Instruction, 9(2):137­175, 2001.",0,,False
343,"[2] J. Arguello. Predicting search task difficulty. In ECIR, volume 14, pages 88­99, 2014.",0,,False
344,"[3] P. Blumschein. Intentional learning. In Encyclopedia of the Sciences of Learning, pages 1600­1601. Springer, 2012.",0,,False
345,"[4] A. Broder. A taxonomy of web search. In ACM Sigir forum, volume 36, pages 3­10. ACM, 2002.",0,,False
346,"[5] M. P. Carey, D. Morrison-Beedy, and B. T. Johnson. The hiv-knowledge questionnaire: Development and evaluation of a reliable, valid, and practical selfadministered questionnaire. AIDS and Behavior, 1(1):61­74, 1997.",1,ad,True
347,"[6] M. J. Cole, J. Gwizdka, C. Liu, N. J. Belkin, and X. Zhang. Inferring user knowledge level from eye movement patterns. Information Processing & Management, 49(5):1075­1091, 2013.",0,,False
348,"[7] K. Collins-Thompson, S. Y. Rieh, C. C. Haynes, and R. Syed. Assessing learning outcomes in web search: A comparison of tasks and query strategies. In Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, pages 163­172. ACM, 2016.",0,,False
349,"[8] C. Eickhoff, J. Teevan, R. White, and S. Dumais. Lessons from the journey: a query log analysis of within-session learning. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 223­232. ACM, 2014.",0,,False
350,"[9] U. Gadiraju, R. Kawase, S. Dietze, and G. Demartini. Understanding malicious behavior in crowdsourcing platforms: The case of online surveys. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, pages 1631­1640. ACM, 2015.",1,ad,True
351,"[10] U. Gadiraju, J. Yang, and A. Bozzon. Clarity is a worthwhile quality­on the role of task clarity in microtask crowdsourcing. In Proceedings of the 28th ACM Conference on Hypertext and Social Media, pages 5­14. ACM, 2017.",1,ad,True
352,"[11] U. Gadiraju, R. Yu, S. Dietze, and P. Holtz. Analyzing knowledge gain of users in informational search sessions on the web. In 2018 ACM on Conference on Human Information Interaction and Retrieval (CHIIR). ACM, 2018.",1,ad,True
353,"[12] J. Gwizdka and X. Chen. Towards observable indicators of learning on search. In SAL@ SIGIR, 2016.",0,,False
354,"[13] J. Gwizdka and I. Spence. What can searching behavior tell us about the difficulty of information tasks? a study of web navigation. Proceedings of the Association for Information Science and Technology, 43(1):1­22, 2006.",0,,False
355,"[14] M. Hagen, M. Potthast, M. Völske, J. Gomoll, and B. Stein. How writers search: Analyzing the search and writing logs of non-fictional essays. In Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, pages 193­202. ACM, 2016.",0,,False
356,"[15] B. J. Jansen, D. Booth, and B. Smith. Using the taxonomy of cognitive learning to model online searching. Information Processing & Management, 45(6):643­663, 2009.",0,,False
357,"[16] V. Kuperman, H. Stadthagen-Gonzalez, and M. Brysbaert. Age-of-acquisition ratings for 30,000 english words. Behavior Research Methods, 44(4):978­990, 2012.",1,ad,True
358,"[17] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Information Processing & Management, 44(6):1822­1837, 2008.",0,,False
359,"[18] R. Syed and K. Collins-Thompson. Retrieval algorithms optimized for human learning. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 555­564. ACM, 2017.",0,,False
360,"[19] P. Vakkari. Searching as learning: A systematization based on literature. Journal of Information Science, 42(1):7­18, 2016.",0,,False
361,"[20] R. W. White, S. T. Dumais, and J. Teevan. Characterizing the influence of domain expertise on web search behavior. In Proceedings of the second ACM international conference on web search and data mining, pages 132­141. ACM, 2009.",0,,False
362,"[21] W.-C. Wu, D. Kelly, A. Edwards, and J. Arguello. Grannies, tanning beds, tattoos and nascar: Evaluation of search tasks with varying levels of cognitive complexity. In Proceedings of the 4th Information Interaction in Context Symposium, pages 254­257. ACM, 2012.",0,,False
363,"[22] X. Zhang, M. Cole, and N. Belkin. Predicting users' domain knowledge from search behaviors. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 1225­1226. ACM, 2011.",0,,False
364,"[23] X. Zhang, J. Liu, M. Cole, and N. Belkin. Predicting users' domain knowledge in information retrieval using multiple regression analysis of search behaviors. Journal of the Association for Information Science and Technology, 66(5):980­1000, 2015.",0,,False
365,"[24] M. Zhuang, G. Demartini, and E. G. Toms. Understanding engagement through search behaviour. In International Conference on Information and Knowledge Management, Proceedings, pages 1957­1966. Association for Computing Machinery, 2017.",0,,False
366,10,0,,False
367,84,0,,False
368,,0,,False

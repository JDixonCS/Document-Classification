,sentence,label,data,regex
0,Session 5B: Entities,1,Session,True
1,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
2,Entity Set Search of Scientific Literature: An Unsupervised Ranking Approach,0,,False
3,"Jiaming Shen, Jinfeng Xiao, Xinwei He, Jingbo Shang, Saurabh Sinha, Jiawei Han",0,,False
4,"Department of Computer Science, University of Illinois Urbana-Champaign, IL, USA {js2, jxiao13, xhe17, shang7, sinhas, hanj}@illinois.edu",0,,False
5,ABSTRACT,0,,False
6,"Literature search is critical for any scientific research. Different from Web or general domain search, a large portion of queries in scientific literature search are entity-set queries, that is, multiple entities of possibly different types. Entity-set queries reflect user's need for finding documents that contain multiple entities and reveal inter-entity relationships and thus pose non-trivial challenges to existing search algorithms that model each entity separately. However, entity-set queries are usually sparse (i.e., not so repetitive), which makes ineffective many supervised ranking models that rely heavily on associated click history. To address these challenges, we introduce SetRank, an unsupervised ranking framework that models inter-entity relationships and captures entity type information. Furthermore, we develop a novel unsupervised model selection algorithm, based on the technique of weighted rank aggregation, to automatically choose the parameter settings in SetRank without resorting to a labeled validation set. We evaluate our proposed unsupervised approach using datasets from TREC Genomics Tracks and Semantic Scholar's query log. The experiments demonstrate that SetRank significantly outperforms the baseline unsupervised models, especially on entity-set queries, and our model selection algorithm effectively chooses suitable parameter settings.",1,ad,True
7,KEYWORDS,0,,False
8,Entity-Set Aware Search; Unsupervised Ranking Model; Unsupervised Model Selection; Literature Search,0,,False
9,"ACM Reference Format: Jiaming Shen, Jinfeng Xiao, Xinwei He, Jingbo Shang, Saurabh Sinha, Jiawei Han. 2018. Entity Set Search of Scientific Literature: An Unsupervised Ranking Approach. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­ 12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3209978.3210055",0,,False
10,1 INTRODUCTION,1,DUC,True
11,"Literature search helps a researcher identify relevant papers and summarize essential claims about a topic, forming a critical step in any scientific research. With the fast-growing volume of scientific publications, a good literature search engine is essential to",0,,False
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210055",1,ad,True
13,"Table 1: Ranking performance on 100 benchmark queries of the S2 production system. Entity-set queries (ESQs), marked bold, perform much weaker than non-ESQs do.",0,,False
14,Metrics NDCG@5 NDCG@10 NDCG@15 NDCG@20,0,,False
15,ESQs,0,,False
16,0.3622 0.3653 0.3840 0.4011,0,,False
17,non-ESQs 0.6291 0.6286 0.6221 0.6247,0,,False
18,Overall 0.5223 0.5233 0.5269 0.5353,0,,False
19,"researchers, especially in the domains like computer science and biomedical science where the literature collections are so massive, diverse, and rapidly evolving--few people can master the state-ofthe-art comprehensively and in depth.",0,,False
20,"A large set of literature search queries contain multiple entities which can be either concrete instances (e.g., GABP (a gene)) or abstract concepts (e.g., clustering). We refer these queries as entity-set queries. For example, a computer scientist may want to find out how knowledge base can be used for document retrieval and thus issues a query ""knowledge base for document retrieval"", which is an entity-set query containing two entities. Similarly, a biologist may want to survey how genes GABP, TERT, and CD11b are associated with cancer and submits a query ""GABP TERT CD11b cancer"", another entity-set query with one disease and three gene entities.",0,,False
21,"Compared with typical short keyword queries, a distinctive characteristic of entity-set queries is that they reflect user's need for finding documents containing inter-entity relations. For example, among 50 queries collected from biologists in 2005 as part of TREC Genomics Track [14], 40 of them are explicitly formulated as finding relations among at least two entities. In most cases, a user who submits an entity-set query will expect to get a ranked list of documents that are most relevant to the whole entity set. Therefore, as in the previous examples, returning a paper about only knowledge bases or only one gene GABP is unsatisfactory.",1,TREC,True
22,"Entity-set queries pose non-trivial challenges to existing search platforms. For example, among the 100 queries1 released by Semantic Scholar (S2), 40 of them are entity-set queries and S2's production ranking system performs poorly on these entity-set queries, as shown in Table 1. The difficulties of handling entity-set queries mainly come from two aspects. First, entity relations within entity sets have not been modeled effectively. The association or cooccurrence of multiple entities has not gained adequate attention from existing ranking models. As a result, those models will rank papers where a single distinct entity appears multiple times higher than those containing many distinct entities. Second, entity-set queries are particularly challenging for supervised ranking models. As manual labeling of document relevance in academic search requires domain expertise, it is too expensive to train a ranking model",1,ad,True
23,1 http://data.allenai.org/esr/Queries/,0,,False
24,565,0,,False
25,Session 5B: Entities,1,Session,True
26,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
27,"based purely on manually labeling. Most systems will first apply an off-the-shelf unsupervised ranking model during their cold-start process and then collect user interaction data (e.g., click information). Unfortunately, entity-set queries are usually sparse (i.e., not so repetitive), and have less associated click information. Furthermore, many off-the-shelf unsupervised models cannot return reasonably good candidate documents for entity-set queries within the top-20 positions. Many highly relevant documents will not be presented to users, which further compromises the usefulness of clicking information.",0,,False
28,"This paper tackles the new challenge--improving the search quality of scientific literature on entity-set queries and proposes an unsupervised ranking approach. We introduce SetRank, an unsupervised ranking framework that explicitly models inter-entity relations and captures entity type information. SetRank first links entity mentions in query and documents to an external knowledge-base. Then, each document is represented with both bag-of-words and bagof-entities representations [36, 37] and fits two language models respectively. On the query side, a novel heterogeneous graph representation is proposed to model complex entity information (e.g., entity type) and entity relations within the set. This heterogeneous query graph represents all the information need in that query. Finally, the query-document matching is defined as a graph covering process and each document is ranked based on the information need it covers in the query graph.",0,,False
29,"Although being an unsupervised ranking framework, SetRank still has some parameters that need to be appropriately learned using a labeled validation set. To further automate the process of ranking model development, we develop a novel unsupervised model selection algorithm based on the technique of weighted rank aggregation. Given a set of queries with no labeled documents, and a set of candidate parameter settings, this algorithm automatically learns the most suitable parameter settings for that set of queries.",0,,False
30,"The significance of our proposed unsupervised ranking approach is two-fold. First, SetRank itself, as an unsupervised ranking model, boosts the literature search performance on entity-set queries. Second, SetRank can be adopted during the cold-start process of a search system, which enables the collection of high-quality click data for training subsequent supervised ranking model. Our experiments on two benchmark datasets2 demonstrate the usefulness of our unsupervised model selection algorithm and the effectiveness of SetRank for searching scientific literature, especially on entity-set queries.",1,ad,True
31,"In summary, this work makes the following contributions: (1) A new research problem, effective entity-set search of scientific",0,,False
32,"literature, is studied. (2) SetRank, an unsupervised ranking framework, is proposed,",0,,False
33,"which models inter-entity relations and captures entity type information. (3) A novel unsupervised model selection algorithm is developed, which automatically selects SetRank's parameter settings without resorting to a labeled validation set. (4) Extensive experiments are conducted in two scientific domains, demonstrating the effectiveness of SetRank and our unsupervised model selection algorithm.",0,,False
34,2 Both benchmark datasets and our model implementations are publicly available at: https://github.,0,,False
35,com/mickeystroller/SetRank.,0,,False
36,"The remaining of the paper is organized as follows. Section 2 discusses related work. Section 3 presents our ranking framework SetRank. Section 4 presents the unsupervised model selection algorithm. Section 5 reports and analyzes the experimental results on two benchmark datasets and shows a case study of SetRank for biomedical literature search. Finally, Section 6 concludes this work with discussions on some future directions.",0,,False
37,2 RELATED WORK,0,,False
38,"We examine related work in three aspects: academic search, entityaware ranking model, and automatic ranking model selection.",1,ad,True
39,2.1 Academic Search,1,ad,True
40,"The practical importance of finding highly relevant papers in scientific literature has motivated the development of many academic search systems. Google Scholar is arguably the most widely used system due to its large coverage. However, the ranking result of Google Scholar is still far from satisfactory because of its bias toward highly cited papers [1]. As a result, researchers may choose other academic search platforms, such as CiteSeerX [33], AMiner [30], PubMed [20], Microsoft Academic Search [29] and Semantic Scholar [38]. Research efforts of many such systems focus on the analytical tasks of scholar data such as author name disambiguation [30], paper importance modeling [28], and entity-based distinctive summarization [26]. However, this work focuses on ad-hoc document retrieval and ranking in academic search. The most relevant work to ours is [38] in which entity embeddings are used to obtain ""soft match"" feature of each query, document pair. However, [38] requires training data to combine word-based and entity-based relevance scores and to select parameter settings, which is rather different from our unsupervised approach.",1,ad,True
41,2.2 Entity-aware Ranking Model,0,,False
42,"Entities, such as people, locations, or abstract concepts, are natural units for organizing and retrieving information [10]. Previous studies found that over 70% of Bing's query and more than 50% of traffic in Semantic Scholar are related to entities [12, 38]. The recent availability of large-scale knowledge repositories and accurate entity linking tools have further motivated a growing body of work on entity-aware ranking models. These models can be roughly categorized into three classes: expansion-based, projection-based, and representation-based.",0,,False
43,The expansion-based methods use entity descriptions from knowledge repositories to enhance query representation. Xu et al. [39] use entity descriptions in Wikipedia as pseudo relevance feedback corpus to obtain cleaner expansion terms; Xiong and Callen [35] utilize the description of Freebase entities related to the query for query expansion; Dalton et al. [7] expand a query using the text fields of the attributes of the query-related entities and generate richer learning-to-rank features based on the expanded texts.,1,Wiki,True
44,"The projection-based methods try to project both query and document onto an entity space for comparison. Liu and Fang [19] use entities from a query and its related documents to construct a latent entity space and then connect the query and documents based on the descriptions of the latent entities. Xiong and Callen [34] use the textual features among query, entities, and documents to model the query-entity and entity-document connections. These additional",1,ad,True
45,566,0,,False
46,Session 5B: Entities,1,Session,True
47,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
48,connections between query and document are then utilized in a learning-to-rank model. A fundamental difference of our work from the above methods is that we do not represent query and document using external terms/entities that they do not contain. This is to avoid adding noisy expansion of terms/entities that may not reflect the information need in the original user query.,1,ad,True
49,"The representation-based methods, as a recent trend for utilizing entity information, aim to build entity-enhanced text representation and combine it with traditional word-based representation [37]. Xiong et al. [36] propose a bag-of-entities representation and demonstrated its effectiveness for vector space model. Raviv et al. [25] leverage the surface names of entities to build an entity-based language model. Many supervised ranking models are proposed to apply learning-to-rank methods for combining entity-based signals with word-based signals. For example, ESR [38] uses entity embeddings to compute entity-based query-document matching score and then combines it with word-based score using RankSVM. Following the same spirit, Xiong et al. [37] propose a word-entity duet framework that simultaneously models the entity annotation uncertainty and trains the ranking model. Comparing with the above methods, we also use the bag-of-entity representation but combine it with word-based representation in an unsupervised way. Also, to the best of our knowledge, we are the first to capture entity relation and type information in an unsupervised entity-aware ranking model.",1,ad,True
50,2.3 Automatic Ranking Model Selection,0,,False
51,"Most ranking models need to manually set many parameter values. To automate the process of selecting parameter settings, some AutoML methods [3, 8] are proposed. Nevertheless, these methods still require a validation set which contains queries with labeled documents. In this paper, we develop an unsupervised model selection algorithm, based on rank aggregation, to automatically choose parameter settings without resorting to a labeled validation set. Rank aggregation aims to combine multiple existing rankings into a joint ranking. Fox and Shaw [9] propose some deterministic functions to combine rankings heuristically. Klementiev et al. [17, 18] propose an unsupervised learning algorithm for rank aggregation based on a linear combination of ranking functions. Another related line of work is to model rankings using a statistical model (e.g., Plackett-Luce model) and aggregate them based on statistical inference [11, 21, 41]. Lately, Bhowmik and Ghosh [2] propose to use object attributes to augment some standard rank aggregation framework. Compared with these methods, our proposed algorithm goes beyond just combining multiple rankings and uses aggregated ranking to guide the selection of parameter settings.",0,,False
52,3 RANKING FRAMEWORK,0,,False
53,"This section presents our unsupervised ranking framework for leveraging entity (set) information in search. Our framework provides a principled way to rank a set of documents D for a query q. In this framework, we represent each document using standard bag-ofwords and bag-of-entities representations [36, 37] (Section 3.1) and represent the query using a novel heterogeneous graph (Section 3.2) which naturally model the entity set information. Finally, we model the query-document matching as a ""graph covering"" process, as described in Section 3.3.",0,,False
54,Field,0,,False
55,Raw Text,0,,False
56,Title,0,,False
57,Playing Atari with Deep Reinforcement Learning,0,,False
58,Abstract,0,,False
59,... learn control policies directly from sensory input using reinforcement learning (RL) ... can apply our RL method to 7 Atari video games ...,0,,False
60,/m/0xwj /m/0hjlw BoE in title field,0,,False
61,playing atari with deep,0,,False
62,reinforcement,0,,False
63,learning,0,,False
64,BoW in title field,0,,False
65,learn control policies directly from,0,,False
66,sensory,0,,False
67,input,0,,False
68,using,0,,False
69,reinforcement,0,,False
70,learning rl we apply our rl,0,,False
71,/m/0h3wrl9 /m/0hjlw,0,,False
72,/m/0xwj,0,,False
73,/m/020mfr,0,,False
74,method to atari video games,0,,False
75,BoE in abstract field,0,,False
76,BoW in abstract field,0,,False
77,"p(e|di,j )",0,,False
78,"p(w|di,j )",0,,False
79,(smoothed) Entity Language Model in abstract field (smoothed) Word Language Model in abstract field,0,,False
80,"Figure 1: An illustrative example showing one document comprised of two fields (i.e., title, abstract) with their corresponding bagof-words and bag-of-entities representations.",0,,False
81,3.1 Document Representation,0,,False
82,"We represent each document using both word and entity information. For words, we use standard bag-of-words representation and treat each unigram as a word. For entities, we adopt an entity linking tool (details described in Section 5.2) that utilizes a knowledge base/graph (e.g., Wikidata or Freebase) where entities have unique IDs. Given an input text, this tool will find the entity mentions (i.e., entity surface names) in the text and link each of them to a disambiguated entity in the knowledge base/graph. For example, given the input document title ""Training linear SVMs in linear time"", this tool will link the entity mention ""SVMs""' to the entity ""Support Vector Machine"" with Freebase id `/m/0hc2f'. Previous studies [25, 36] show that when the entity linking error is within a reasonable range, the returned entity annotations, though noisy, can still improve the overall search performance, partially due to the following:",1,ad,True
83,"(1) Polysemy resolution. Different entities with the same surface name will be resolved by the entity linker. For example, the fruit ""Apple"" (with id `/m/014j1m') will be disambiguated with the company ""Apple"" (with id `/m/0k8z').",0,,False
84,"(2) Synonymy resolution. Different entity surface names corresponding to the same entity will be identified and merged. For example, the entity ""United States of America"" (with id `/m/09c7w0') can have different surface names including ""USA"", ""United States"", and ""U.S."" [25]. The entity linker can map all these surface names to the same entity.",0,,False
85,"After linking all the entity mentions in a document to entities in the knowledge base, we can obtain the bag-of-entities representation of this document. Then, we fit two language models (LMs) for this document: one being word-based (i.e., traditional unigram LM) and the other being entity-based. Notice that in the literature search scenario, documents (i.e., papers) usually contain multiple fields, such as title, abstract, and full text. We model each document field using a separate bag-of-words representation and a separate bag-of-entities representation, as shown in Figure 1.",1,LM,True
86,567,0,,False
87,Session 5B: Entities,1,Session,True
88,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
89,"To exploit such intra-document structures, we generally assume",0,,False
90,"a document di has k fields di ,"" {di,1, . . . , di,k } and thus the document collection can be separated into k parts: {D1, . . . , Dk }. Following [23], we assign each field a weight j and formulate the generation process of a token t given the document di as follows:""",0,,False
91,k,0,,False
92,"p (t |di ) ,"" p (t |di, j )p (di, j |di ),""",0,,False
93,"j ,1",0,,False
94,"p (di, j |di ) ,",0,,False
95,j,0,,False
96,"k j  ,1",0,,False
97,j,0,,False
98,.,0,,False
99,(1),0,,False
100,"Notice the a token t can be either a unigram w or an entity e, and the field weight j can be either manually set based on prior knowledge or automatically learned using the mechanism described in Sec-",0,,False
101,"tion 4. The token generation probability under each document field p(t |di, j ) can be obtained from the maximum likelihood estimate with Dirichlet prior smoothing [40] as follows:",0,,False
102,"p (t |di, j )",0,,False
103,",",0,,False
104,"nt,di, j",0,,False
105,+ µj,0,,False
106,"nt , D j LDj",0,,False
107,"Ldi,j + µj",0,,False
108,",",0,,False
109,(2),0,,False
110,"where nt,di,j and Ldi,j represent the number of token t in di, j and the length of di, j . Similarly, we can define nt,Dj and LDj . Finally, µj is a scale parameter of the Dirichlet distribution for field j. A",0,,False
111,concrete example is shown in Figure 1.,0,,False
112,3.2 Query Representation,1,Query,True
113,"Given an input query q, we first apply the same entity linker used for document representation to extract all the entity information",0,,False
114,"in the query. Then, we design a novel heterogeneous graph to represent this query q, denoted as Gq . Such a graph representation captures both word and entity information in the query and models",0,,False
115,"the entity relations. A concrete example is shown in Figure 2. Node representation. In this heterogeneous query graph, each node represents a query token. As a token can be either a word or",0,,False
116,"an entity, there are two different types of nodes in this graph. Edge representation. We use an edge to represent a latent relation between two query tokens. In this work, we consider two types",0,,False
117,of latent relations: word-word relation and entity-entity relation.,0,,False
118,"For word-word relation, we add an edge for each pair of adjacent",1,ad,True
119,"word tokens with equal weight 1. For instance, given an query ""Atari video games"", we will add two edges, one between word pairs Atari, video and the other between video, game. On the entity side, we aim to emphasize all the possible entity-entity relations,",1,ad,True
120,"and thus add an edge between each pair of entity tokens. Modeling entity type. The type information of each query entity can further reveal the user's information need. Therefore, we assign",1,ad,True
121,the weight of each entity-entity relation based on these two entities',0,,False
122,"type information. Intuitively, if the types of two entities are distant",0,,False
123,"from each other in a type hierarchy, then the relation between these",0,,False
124,two entities should have a larger weight. A similar idea is exploited,0,,False
125,"in [10] and found useful for type-aware entity retrieval. Mathematically, we use e to denote the type of entity e; use",0,,False
126,"LCAu,v to denote the Lowest Common Ancestor (LCA) of two nodes u and v in a given tree (i.e., type hierarchy), and use l (u, v) to denote the length of a path between node u and node v. In Figure 2, for example, the entity tokens `/m/0hjlw' and `/m/0xwj', corresponding to ""reinforcement learning"" and ""Atari"", have types `education.field_of_study' and `computer.game', respectively. The Lowest Common Ancestor of these two types in the type hierarchy is `Thing'. Finally, we define the relation strength between entity",0,,False
127,Thing,0,,False
128,Computer,0,,False
129,Business,0,,False
130,Type hierarchy obtained from knowledge base,0,,False
131,Education ...,0,,False
132,Game,0,,False
133,Algorithm,0,,False
134,Industry Field of Study Department,0,,False
135,/m/0xwj,0,,False
136,/m/020mfr,0,,False
137,/m/0hjlw,0,,False
138,/m/01hyh_,0,,False
139,Query,1,Query,True
140,Play Atari video games using reinforcement learning and machine learning.,0,,False
141,Query with linked entity mentions,1,Query,True
142,Word,0,,False
143,Entity,0,,False
144,word-word relation entity-entity relation,0,,False
145,1,0,,False
146,1 atari,0,,False
147,video 1 game,0,,False
148,play,0,,False
149,/m/0xwj,0,,False
150,3 3,0,,False
151,/m/0hjlw 3,0,,False
152,learning 1,0,,False
153,1 /m/01hyh_,0,,False
154,3 /m/020mfr,0,,False
155,3,0,,False
156,1 using,0,,False
157,1,0,,False
158,reinforcement,0,,False
159,machine,0,,False
160,1,0,,False
161,learning,0,,False
162,1,0,,False
163,and,0,,False
164,1,0,,False
165,Heterogeneous graph representation of query,0,,False
166,Figure 2: An illustrative example showing the heterogeneous graph representation of one query. Word-word relations are marked by dash lines and entity-entity relations are marked by solid lines. Different solid line colors represent different relation strengths based on two entities' types.,0,,False
167,e1 and entity e2 as follows:,0,,False
168,"LC Ae1,e2 ,"" LC A(e1, e2 ),""",0,,False
169,(3),0,,False
170,"e1,e2 ,"" 1 + max l (e1, LC Ae1,e2 ), l (e2, LC Ae1,e2 ) . (4)""",0,,False
171,Our proposed heterogeneous query graph representation is general,0,,False
172,"and can be extended. For example, we can apply dependency parsing",0,,False
173,"for verbose queries, and only add an edge between two word tokens",1,ad,True
174,"that have direct dependency relation. Also, if the importance of each",0,,False
175,"entity-entity relation is given, we can then set the edge weights",0,,False
176,correspondingly. We leave these extensions for future works.,0,,False
177,3.3 Document Ranking using Query Graph,1,Query,True
178,Our proposed heterogeneous query graph Gq represents all information need in the user-issued query. Such need can be either to,0,,False
179,find document discussing one particular entity or to identify papers,0,,False
180,"studying an important inter-entity relation. Intuitively, a document",0,,False
181,that can satisfy more information need should be ranked at a higher,0,,False
182,position. To quantify such information need that is explained by a,0,,False
183,"document, we define the following graph covering process.",0,,False
184,"Query graph covering. If a query token t  q exists in a document di , we say di covers the node in Gq that corresponds to this token. Similarly, if a pair of query tokens t1 and t2 exists in di , we say di covers the edge in Gq that corresponds to the relation of this token pair t1, t2. The subgraph of Gq that is covered by the document di , denoted as Gq |di , represents the information need in the query q that is explained by the document di .",1,Query,True
185,"Furthermore, we follow the same spirit of [22] and view the",0,,False
186,"subgraph Gq |di as a Markov Network, based on which we define the joint probability of the document di and the query q as follows:",0,,False
187,"P (di, q)",0,,False
188,"d,ef",0,,False
189,1 Z,0,,False
190," (c ) ra,nk",0,,False
191,"log  (c ) ra,nk",0,,False
192,"f (c ),",0,,False
193,c Gq |di,0,,False
194,c Gq |di,0,,False
195,c Gq |di,0,,False
196,(5),0,,False
197,"where Z is a normalization factor, c indexes the cliques in graph, and",0,,False
198, (c) is the non-negative potential defined on c. The last equation,0,,False
199,"holds as we let  (c) ,"" exp[f (c)]. Notice that if Gq |d1 is a subgraph of Gq |d2 which means document d1 covers less information than document d2 does, we should have P (d1, q) < P (d2, q). Therefore,""",0,,False
200,"we should design f (·) to satisfy the constraint f (c) > 0, c.",0,,False
201,"In this work, we focus on modeling each single entity and pairwise relations between two entities. Therefore, each clique c can",0,,False
202,be either a node or an edge in the graph. Modeling higher-order,0,,False
203,568,0,,False
204,Session 5B: Entities,1,Session,True
205,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
206,"relations among more than two entities (i.e., cliques with size larger",0,,False
207,than 2) is left for future work. We define the potential functions for,0,,False
208,a single node and an edge as follows:,0,,False
209,"Node potential. Node potential quantifies the information need contained in a single node t, which can be either a word token w or an entity token e. To balance the relative weight of a word token and an entity token, we introduce a parameter E  [0, 1], and define the node potential function f (·) as follows:",0,,False
210,"f (t ) ,",0,,False
211,E · a(P (t |di )) (1 - E ) · a(P (t |di )),0,,False
212,if token t is an entity token if token t is a word token,0,,False
213,(6),0,,False
214,"where a(·) is an activation function that transforms a raw probability to a node potential. Here, we set a(x ) , x in order to amplify P (t |di ) which has a relatively small value.",0,,False
215,"Edge potential. Edge potential quantifies the information need contained in an edge t1, t2 that can be either a word-word (W-W)",0,,False
216,relation or and an entity-entity (E-E) relation. In our query graph,0,,False
217,"representation, all word-word relations have an equal weight of 1, and the weight of each entity-entity relation (i.e., e1,e2 ) is defined by Equation (3). Finally, we calculate the edge potential as follows:",0,,False
218,"f (t1, t2) ,"" t1,t2 · a (P (t1, t2 |di )),""",0,,False
219,(7),0,,False
220,"t1, t2 ,",0,,False
221,"E · e1,e2 (1 - E )",0,,False
222,"if t1, t2 is an E-E relation if t1, t2 is a W-W relation",0,,False
223,(8),0,,False
224,"where t1,t2 measures the edge importance, and a(·) is the same",0,,False
225,activation function as defined above. To simplify the calculation of,0,,False
226,"P (t1, t2|di ), we make an assumption that two tokens t1 and t2 are conditionally independent given a document di . Then, we replace P (t1, t2|di ) with P (t1|di )P (t2|di ) and substitute it in Equation (7).",0,,False
227,"Putting all together. After defining the node and edge potentials,",0,,False
228,we can calculate the joint probability of each document di and query q using Equation (5) as follows:,0,,False
229,"P (di, q) , (1 - E )",0,,False
230,1+,0,,False
231,a (P (w |di )) a (P (w |di )),0,,False
232,w Gq |di,0,,False
233,"w,w Gq |di",0,,False
234,+ E,0,,False
235,1+,0,,False
236,"e,e · a(P (e |di )) a(P (e |di )).",0,,False
237,e Gq |di,0,,False
238,"e, eGq |di",0,,False
239,(9),0,,False
240,"As shown in the above equation, SetRank will explicitly reward",0,,False
241,paper capturing inter-entity relations and covering more unique,0,,False
242,"entities. Also, it uses E to balance the word-based relevance with entity-based relevance, and models entity type information in e,e.",0,,False
243,4 UNSUPERVISED MODEL SELECTION,0,,False
244,"Although being an unsupervised ranking framework, SetRank still has some parameters that need to be appropriately set by ranking model designers, including the weight of title/abstract field and the relative importance of entity token E . Previous study [40] shows that these model parameters have significant influences on the ranking performance and thus we need to choose them carefully. Typically, these parameters are chosen to optimize the performance over a validation set that is manually constructed and contains the relevance label of each query-document pair. Though being useful, the validation set is not always available, especially for those applications (e.g., literature search) where labeling document requires domain expertise.",0,,False
245,"To address the above problem, we propose an unsupervised",1,ad,True
246,model selection algorithm which automatically chooses the param-,0,,False
247,eter settings without resorting to a manually labeled validation set.,0,,False
248,The key philosophy is that although people who design the ranking,0,,False
249,"model do not know the exact ""optimal"" parameter settings, they",0,,False
250,do have prior knowledge about the reasonable range for each of,0,,False
251,"them. For example, the title field weight should be set larger than the abstract field weight, and the entity token weight E should be set small if the returned entity linking results are noisy. Our model",0,,False
252,selection algorithm leverages such prior knowledge by letting the,0,,False
253,ranking model designer input the search range of each parameter's,0,,False
254,value. It will then return the best value for each parameter within,0,,False
255,its corresponding search range. We first describe our notations and,0,,False
256,"formulate our problem in Section 4.1. Then, we present our model",0,,False
257,selection algorithm in Section 4.2.,0,,False
258,4.1 Notations and Problem Formulation,0,,False
259,"Notations. We use SK to denote the collection of rankings over a set of K documents: D ,"" {d1, . . . , dk , . . . , dK }, k  [K] "","" {1, . . . , K }. We denote by  : [K]  [K] a complete ranking, where  (k ) denotes the position of document dk in the ranking, and  -1 (j) is the index of the document on position j. For example, given the ranking: d3  d1  d2  d4, we will have  "","" [2, 3, 1, 4] and  -1 "","" (3, 1, 2, 4). Furthermore, we use the symbol  (instead of  ) to denote an incomplete ranking which includes only some of the documents in D. If document dk does not occur in the ranking, we set  (k ) "","" 0, otherwise,  (k ) is the rank of document dk . In the corresponding  -1, those missing documents simply do not occur. For example, given the ranking: d4  d2  d1, we have  "","" [3, 2, 0, 1] and  -1 "","" (4, 2, 1). Finally, we let I ( )to represent the index of documents that appear in the ranking list  . Problem Formulation. Given a parameterized ranking model M where  denotes the set of all parameters (e.g., {k, b} in BM25, {µ} in query likelihood model with dirichlet prior smoothing), we want to find the best parameter settings   such that the ranking model M  achieves the best ranking performance over the space Q of all queries. In practice, however, the space consisting of all possible values of  can be infinite and we cannot access all queries in Q. Therefore, we assume ranking model designers will input p possible sets of parameter values:  "","" {1, . . . , p } and a finite subset of queries Q  Q. Finally, we formulate our problem of unsupervised model selection as follows:""",1,ad,True
260,"Definition 1. (PROBLEM FORMULATION). Given a parameterized ranking model M , p candidate parameter settings , and an unlabeled query subset Q, we aim to find     such that M  achieves the best ranking performance over Q.",0,,False
261,4.2 Model Selection Algorithm,0,,False
262,"Our framework measures the goodness of each parameter settings i   based on its induced ranking model Mi . The key challenge here is how we can evaluate the ranking performance of each Mi over a query q which has no labeled documents. To address this challenge, we first leverage a weighted rank aggregation technique",1,ad,True
263,to obtain an aggregated rank list and then evaluate the quality of each Mi based on the agreement between its generated rank list and the aggregated rank list. The key intuition here is that high-,0,,False
264,quality ranking models will rank documents based on a similar,0,,False
265,569,0,,False
266,Session 5B: Entities,1,Session,True
267,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
268,Algorithm 1: Unsupervised Model Selection.,0,,False
269,"Input: A parameterized ranking model M , p candidate parameter",0,,False
270,"settings  ,"" {1, · · · , p }, and an unlabeled query subset Q . Output: The best ranking model M  with    .""",0,,False
271,"1 set scor e (M1 ) , scor e (M2 ) , · · · , scor e (Mp ) , 0;",0,,False
272,2 for query q  Q do,0,,False
273,3,0,,False
274,set 1,0,,False
275,",",0,,False
276,2,0,,False
277,",",0,,False
278,·,0,,False
279,· · p,0,,False
280,",",0,,False
281,1 p,0,,False
282,;,0,,False
283,4,0,,False
284,"set  pr ev , N one ;",0,,False
285,5 while True do,0,,False
286,6,0,,False
287,// Weighted Rank Aggregation ;,0,,False
288,7,0,,False
289,for document index j from 1 to |D | do,0,,False
290,8,0,,False
291,"scor e (dj ) , 0;",0,,False
292,9,0,,False
293,for rank list index i from 1 to p do,0,,False
294,10,0,,False
295,"if j  I (i ) (i.e., dj appears in i ) then",0,,False
296,11,0,,False
297,"scor e (dj ) , scor e (dj ) + i ( |i | + 1 - i (dj ));",0,,False
298,12,0,,False
299," ,"" argsort(scor e (d1), · · · , scor e (d|D | ));""",0,,False
300,13,0,,False
301,// Confidence Score Adjustment ;,0,,False
302,14,0,,False
303,for rank list index i from 1 to p do,0,,False
304,15,0,,False
305,"i ,",0,,False
306,exp(-d is t (i || )) i exp(-d i s t (i ||,0,,False
307,),0,,False
308,),0,,False
309,;,0,,False
310,16,0,,False
311,// Convergence Check ;,0,,False
312,17,0,,False
313,"if  ,,  pr ev then",0,,False
314,18,0,,False
315,Break;,0,,False
316,19,0,,False
317,else,0,,False
318,20,0,,False
319,prev  ;,0,,False
320,21 for rank list index i from 1 to p do,0,,False
321,22,0,,False
322,"score (Mi ) , score (Mi ) + i ;",0,,False
323,"23 M  , arg max  scor e (M );",0,,False
324,24 Return M  ;,0,,False
325,distribution while low-quality ranking models will rank documents,0,,False
326,"in a uniformly random fashion. Therefore, the agreement between",0,,False
327,each rank list with the aggregated rank list serves as a good signal,0,,False
328,of its quality.,0,,False
329,"At a high level, our model selection method is an iterative algo-",0,,False
330,rithm which repeatedly aggregates multiple rankings (with their,0,,False
331,corresponding weights) and uses the aggregated rank list to esti-,0,,False
332,"mate the quality of each of them. Given a query q, we first con-",0,,False
333,"struct p ranking models Mi , i  [1, . . . , p], one for each parameter settings i   and obtain its returned top-k rank list i over a doc-",0,,False
334,"ument set Di (i.e., |Di | ,"" k). Then, we construct a unified document""",0,,False
335,"pool D ,",0,,False
336,"p i ,1",0,,False
337,Di .,0,,False
338,After,0,,False
339,"that,",0,,False
340,we,0,,False
341,use,0,,False
342,i,0,,False
343,to,0,,False
344,denote,0,,False
345,the,0,,False
346,confidence,0,,False
347,"score of each ranking model Mi , and initialize all of them with",0,,False
348,equal,0,,False
349,value,0,,False
350,1 p,0,,False
351,.,0,,False
352,During,0,,False
353,each,0,,False
354,"iteration,",0,,False
355,we,0,,False
356,first,0,,False
357,aggregate,0,,False
358,"{1,",0,,False
359,.,0,,False
360,.,0,,False
361,.,0,,False
362,",",0,,False
363,p,0,,False
364,"},",0,,False
365,"weighted by {1, . . . , p }, and obtain the aggregated rank list  .",0,,False
366,"Then, we adjust the confidence score of each ranking model Mi (i.e., i ) based on the distance of two rankings: i and  . Here, we",1,ad,True
367,use  to denote the aggregated rank list because it is a complete,0,,False
368,ranking over the document pool D.,0,,False
369,Weighted Rank Aggregation. We aggregate multiple rank lists using a variant of Borda counting method [6] which considers the,0,,False
370,relative weight of each rank list. We calculate the score of each,0,,False
371,document based on its position in each rank list as follows:,0,,False
372,p,0,,False
373,"scor e (dj ) ,"" i |i | + 1 - i (dj ) 1{j  I (i ) },""",0,,False
374,(10),0,,False
375,"i ,1",0,,False
376,"where |i | denotes the length of a rank list i , and 1{x } is an indicator function. When document dj appears in the rank list i , 1{j  I (i )}",0,,False
377,title abs E,0,,False
378,1 10 2 10,0,,False
379,5 0.5 3 0.7,0,,False
380,...,0,,False
381,... ......,0,,False
382,p 15 5 0.7,0,,False
383,M1 1 d3 d1 d2,0,,False
384,"KT (1k) , 1",0,,False
385,1,0,,False
386,M2 2 d1,0,,False
387,.,0,,False
388,d3,0,,False
389,.,0,,False
390,d4,0,,False
391,Aggregated Rank List,0,,False
392,2  d1 d3 d2 d4,0,,False
393,. .,0,,False
394,. .,0,,False
395,p,0,,False
396,d p,0,,False
397,Mp,0,,False
398,1,0,,False
399,d2,0,,False
400,d3,0,,False
401,posKT (pk),0,,False
402,",",0,,False
403,1 log(1 +,0,,False
404,2),0,,False
405,1 log(1 + 3),0,,False
406,"Figure 3: An illustrative example showing the process of weighted rank aggregation and the calculation of two different ranking distances (i.e., KT and pos KT ).",0,,False
407,"equals to 1, otherwise, it equals to 0. The above equation will reward",0,,False
408,"a document ranked at higher position (i.e., small i (dj )) in a highquality rank list (i.e., large i ) a larger score. Finally, we obtain the aggregated rank of these documents based on their corresponding",0,,False
409,scores. A concrete example in shown in Figure 3.,0,,False
410,"Confidence Score Adjustment. After we obtain the aggregated rank list, we will need to adjust the confidence score i of each ranking model Mi based on the distance between i and aggregated rank list  . In order to compare the distance between an incomplete rank list i with a complete rank list  , we extend the classical Kendall Tau distance [16] and define it as follows:",1,ad,True
411,"KT (i | | ) ,",0,,False
412,1{ (a) >  (b ) }.,0,,False
413,(11),0,,False
414,"i (a)<i (b ) a,b I (i )",0,,False
415,"The above distance counts the number of pairwise disagreements between i and  . One limitation of this distance is that it does not differentiate the importance of different ranking positions. Usually,",0,,False
416,switching two documents in the top part of a rank list should be,0,,False
417,"penalized more, compared with switching another two documents",0,,False
418,"in the bottom part of a rank list. To model such intuition, we propose a position-aware Kendall Tau distance and define it as follows:",0,,False
419,"posKT (i | | ) ,"" i (a)<i (b ) a,bI (i )""",0,,False
420,1,0,,False
421,1,0,,False
422,-,0,,False
423,log2 (1 +  (b )) log2 (1 +  (a)),0,,False
424,1{ (a) >  (b ) }. (12),0,,False
425,"With the distance between two rankings defined, we can adjust the confidence score as follows:",1,ad,True
426,"i ,",0,,False
427,exp(-dist (i | | i exp(-dist (i |,0,,False
428,)) |,0,,False
429,)),0,,False
430,",",0,,False
431,(13),0,,False
432,where dist (i || ) can be either KT (i || ) or posKT (i || ) and we will study how different this choice can influence the model selec-,0,,False
433,tion results in Section 5.4. The key idea of the above equation is,0,,False
434,to promote the ranking model which returns a ranked list better,0,,False
435,aligned with the aggregated rank list.,0,,False
436,"Putting all together. Algorithm 1 summarizes our unsupervised model selection process. Given a query q  Q, we can iteratively",0,,False
437,apply weighted rank aggregation and confidence score adjustment,1,ad,True
438,"until the algorithm converges. Then, we collect the converged",0,,False
439,"{^1, . . . , ^p }. Specifically, ^i is the confidence score of ranking model Mi on query q. With a slight abuse of notation, we use score (Mi ) to denote its accumulated confidence score. Given a set of queries Q, we run the former procedure for each query and sum over all converged ^i . Finally, we return the ranking model M  which has the largest accumulated confidence score.",0,,False
440,570,0,,False
441,Session 5B: Entities,1,Session,True
442,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
443,5 EXPERIMENTS,0,,False
444,"In this section, we evaluate our proposed SetRank framework as well as unsupervised model selection algorithm on two datasets from two scientific domains.",0,,False
445,5.1 Datasets,0,,False
446,"We use two benchmark datasets for the experiments: Semantic Scholar [38] in Computer Science (S2-CS) and TREC 2004&2005 Genomics Track in Biomedical science (TREC-BIO). S2-CS contains 100 queries sampled from Semantic Scholar's query log, in which 40 queries are entity-set queries and the maximum number of entities in a query is 5. Candidate documents are generated by pooling from variations of Semantic Scholar's online production system and all of them are manually labeled on a 5-level scale. Entities in both queries and documents are linked to Freebase using CMNS [13]. As the original dataset does not contain the entity type information, we enhance it by retrieving each entity's most notable type in the latest Freebase dump3 based on its Freebase ID. These types are organized by Freebase type hierarchy. TREC-BIO includes 100 queries designed by biologists and the candidate document pool is constructed based on the top results of all submissions at that time. All candidate documents are labeled on a 3-level scale. In these 100 queries, 86 of them are entity-set queries and the maximum number of entities in a query is 11. The original dataset contains no entity information and therefore we apply PubTator [32], the state-of-the-art biomedical entity linking tool, to obtain 5 types of entities (i.e., Gene, Disease, Chemical, Mutation, and Species) in both queries and documents. We build a simple type hierarchy with root node named `Thing' and each first-level node corresponds to one of the above 5 types.",1,TREC,True
447,5.2 Entity Linking Performance,0,,False
448,"We evaluate the query entity linking using precision and recall at the query level. Specifically, an entity annotation is considered correct if it appears in the gold labeled data (i.e., the strict evaluation in [4]). The original S2-CS dataset provides such gold labeled data. For TREC-BIO dataset, we asked two Master-level students with biomedical science background to label all the linked entities as well as the entities that they could identify in the queries. We also report the entity linking performance on the general domain queries (ClueWeb09 and ClueWeb12) for references [36]. As we can see in Table 2, the overall linking performance of academic queries is better than that of general domain queries, probably because academic queries have less ambiguity. Also, recall of entity linking in TREC-BIO dataset is very high. A possible reason is that the biomedical entities have very distinctive tokens (e.g., ""narcolepsy"" is a specific disease related to sleep and is seldom used in other contexts) and thus it is relatively easier to recognize them.",1,TREC,True
449,5.3 Ranking Performance,0,,False
450,"5.3.1 Experimental Setup. Evaluation metrics. Since documents in both datasets have multilevel graded relevance, we use NDCG@{5,10,15,20} as our main evaluation metrics. All evaluation is performed using standard",1,ad,True
451,3 https://developers.google.com/freebase/,0,,False
452,"Table 2: Entity linking performance on scientific domain queries (S2-CS, TREC-BIO) and general domain queries (ClueWeb09, ClueWeb12).",1,TREC,True
453,Precision Recall,0,,False
454,S2-CS TREC-BIO 0.680 0.678 0.680 0.727,1,TREC,True
455,ClueWeb09 ClueWeb12,1,ClueWeb,True
456,0.577,0,,False
457,0.485,0,,False
458,0.596,0,,False
459,0.575,0,,False
460,"pytrec_eval tool [31]. Statistical significances are tested using twotailed t-test with p-value  0.05. Baselines. We compare SetRank with 4 baseline ranking models: Vector Space Model (BM25 [27]), Query Likelihood Model with",1,trec,True
461,Dirichlet Prior smoothing (LM-DIR) or with Jelinek Mercer smooth-,1,LM,True
462,"ing (LM-JM) [40], and the Information-Based model (IB) [5]. All",1,LM,True
463,"models are applied to the paper's title and abstract fields. Here, we do not compete with Semantic Scholar's production system and ESR model [38] because they are supervised models trained over",0,,False
464,user's click information which is not available in our setting.,0,,False
465,"The parameters of all models, including the field weights, are set",0,,False
466,using 5-fold cross validation over the queries in each benchmark,0,,False
467,dataset using the same paradigm in [25] as follows. For each hold-,1,ad,True
468,"out fold, the other four folds are served as a validation set. A grid",0,,False
469,search is applied to choose the optimal parameter settings that,0,,False
470,"maximize NDCG@20 on the validation set. Specifically, the title",0,,False
471,"and abstract field weights are selected from {1,5,10,15,20,50}; the Dirichlet smoothing parameter µ and Jelinek Mercer smoothing parameter  are chosen from {500, 1000, 1500, 2000, 2500, 3000} and {0.1, 0.2, . . . , 0.9}, respectively; the relative weight of entity token E used in SetRank is selected from {0, 0.1, . . . , 1}. The best performing parameter settings are then saved for the hold-out evaluation.",0,,False
472,"5.3.2 Effectiveness of Leveraging Entity Information. As mentioned before, the entity linking process is not perfect and it generates some noisy entity annotations. Therefore, we first study how different ranking models, including our proposed SetRank, can leverage such noisy entity information to improve the ranking performance. We evaluate three variations of each model ­ one using only word information, one using only entity information, and one using both pieces of information. Results are shown in Table 3. We notice that the usefulness of entity information is inconclusive for baseline models. On S2CS dataset, adding entity information can improve the ranking performance, while on TREC-BIO dataset, it will drag down the performance of all baseline methods. This resonates with previous findings in [15] that simply adding entities into queries and posting them to existing ranking models does not work for biomedical literature retrieval. Compared with baseline methods, SetRank successfully combines the word and entity information and effectively leverages such noisy entity information to improve the ranking performance. Furthermore, SetRank can better utilize each single information source, either word or entity, than other baseline models thanks to our proposed query graph representation. Overall, SetRank significantly outperforms all variations of baseline models.",1,ad,True
473,"5.3.3 Ranking Performance on Entity-Set Queries. We further study each model's ranking performance on entity-set queries. There are 40 and 86 entity-set queries in S2-CS and TRECBIO, respectively. We denote these subsets of entity-set queries as S2-CS-ESQ and TREC-BIO-ESQ. As shown in Table 4, SetRank significantly outperforms the best variation of all baseline methods",1,TREC,True
474,571,0,,False
475,Session 5B: Entities,1,Session,True
476,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
477,"Table 3: Effectiveness of leveraging (noisy) entity information for ranking. Each method contains three variations and the best variation is labeled bold. The superscript """" means the model significantly outperforms the best variation of all 4 baseline methods (with p-value  0.05).",0,,False
478,Dataset Method,0,,False
479,S2-CS,0,,False
480,NDCG@5 NDCG@10 NDCG@15 NDCG@20,0,,False
481,TREC-BIO,1,TREC,True
482,NDCG@5 NDCG@10 NDCG@15 NDCG@20,0,,False
483,Word,0,,False
484,0.3476 0.3785 0.4001 0.4126,0,,False
485,0.3189 0.2968 0.2833 0.2739,0,,False
486,BM25 Entity 0.3319 0.3520 0.3616 0.3752,0,,False
487,0.1542 0.1488 0.1424 0.1419,0,,False
488,Both 0.3675 0.4039 0.4160 0.4333,0,,False
489,0.2613 0.2472 0.2395 0.2337,0,,False
490,LM-DIR Word Entity Both,1,LM,True
491,0.3447 0.3623 0.3781 0.4012,0,,False
492,0.3460 0.3579 0.3673 0.3816,0,,False
493,0.3563 0.3901 0.4077 0.4205,0,,False
494,0.3053 0.2958 0.2852 0.2781,0,,False
495,0.1755 0.1601 0.1579 0.1558,0,,False
496,0.2669 0.2571 0.2591 0.2547,0,,False
497,LM-JM Word Entity,1,LM,True
498,0.3626 0.3394 0.3774 0.3519,0,,False
499,0.4051 0.3666,0,,False
500,0.4182 0.3804,0,,False
501,0.2957 0.2742 0.2642 0.2560,0,,False
502,0.1656 0.1588 0.1575 0.1534,0,,False
503,Both,0,,False
504,0.3625 0.3962 0.4174 0.4362,0,,False
505,0.2826 0.2572 0.2437 0.2362,0,,False
506,Word 0.3759 0.3903,0,,False
507,0.4113,0,,False
508,0.4295,0,,False
509,0.3045 0.2918 0.2835 0.2722,0,,False
510,IB Entity 0.3420 0.3557 0.3699 0.3855,0,,False
511,0.1842 0.1715 0.1664 0.1628,0,,False
512,Both,0,,False
513,0.3729 0.4009 0.4272 0.4421,0,,False
514,0.2770 0.2633 0.2541 0.2406,0,,False
515,Word 0.3890 0.4168 0.4411 0.4674,0,,False
516,0.3417 0.3165 0.3017 0.2900,0,,False
517,SetRank,0,,False
518,Entity Both,0,,False
519,0.3761 0.3885 0.4054 0.4229,0,,False
520,0.4207 0.4431 0.4762 0.4950,0,,False
521,0.2111 0.1976 0.1931 0.1885,0,,False
522,0.3744 0.3522 0.3363 0.3246,0,,False
523,"Table 4: Ranking performance on entity-set queries. The best variation of each baseline method is selected. The superscript """" means the model significantly outperforms all 4 baseline methods (with p-value  0.05).",0,,False
524,Dataset Metric,0,,False
525,S2-CS -ESQ,0,,False
526,NDCG@5 NDCG@10 NDCG@15 NDCG@20,0,,False
527,TREC-BIO -ESQ,1,TREC,True
528,NDCG@5 NDCG@10 NDCG@15 NDCG@20,0,,False
529,BM25,0,,False
530,0.3994 0.4364 0.4454 0.4609,0,,False
531,0.3185 0.2968 0.2812 0.2718,0,,False
532,LM-DIR,1,LM,True
533,0.3522 0.3973 0.4160 0.4264,0,,False
534,0.2934 0.2834 0.2711 0.2644,0,,False
535,LM-JM,1,LM,True
536,0.3812 0.4241 0.4431 0.4618,0,,False
537,0.2940 0.2746 0.2636 0.2553,0,,False
538,IB,0,,False
539,0.3956 0.4209 0.4496 0.4664,0,,False
540,0.3011 0.2896 0.2832 0.2708,0,,False
541,SetRank,0,,False
542,0.4983 0.5130 0.5450 0.5629,0,,False
543,0.3639 0.3406 0.3251 0.3132,0,,False
544,"on S2-CS-ESQ and TREC-BIO-ESQ by at least 25% and 14% respectively in terms of NDCG@5. Also, we can see the advantages of SetRank over the baselines on entity-set queries are larger than those on general queries, This further demonstrates SetRank's effectiveness of modeling entity set information.",1,TREC,True
545,"5.3.4 Effectiveness of Modeling Entity Relation and Entity Type. To study how the inter-entity relation and entity type information can contribute to document ranking, we compare SetRank with two of its variants, SetRank-t and SetRank-ts . The first variant models entity relation among the set but ignores the entity type",0,,False
546,"information, and the second variant simply neglects both entity",0,,False
547,"relation and type. Results are shown in Table 5. First, we compare SetRank-t with",0,,False
548,"SetRank-ts and find that modeling the entity relation in entity sets can significantly improve the ranking results. Such improvement is especially obvious on the entity-set query sets S2-CS-ESQ and TREC-BIO-ESQ. Also, by comparing SetRank with SetRank-t , we can see adding entity type information can further improve",1,TREC,True
549,"ranking performance. In addition, we present a concrete case study",1,ad,True
550,"for one entity-set query in Table 6. The top-2 papers returned by SetRank-ts are focusing on video game without discussing its relation with reinforcement learning. In comparison, SetRank considers the entity relations and returns the paper mentioning both entities.",0,,False
551,5.3.5 Analysis of Entity Token Weight E . We introduce the entity token weight E in Eq. (6) to combine the entity-based and word-based relevance scores. In all previous,0,,False
552,"experiments, we choose its value using cross validation. Here, we",0,,False
553,study how this parameter will influence the ranking performance by constructing multiple SetRank models with different E and directly report their performance on all 100 queries.,0,,False
554,"As shown in Figure 4, for S2-CS dataset, SetRank's ranking performance first increases as E increases until it reaches 0.7 and then starts to decrease when we further increase E . However, for",0,,False
555,Table 5: Ranking performance of different variations of SetRank.,0,,False
556,"Best results are marked bold. The superscript """" means the model significantly outperforms SetRank-ts (with p-value  0.05).",0,,False
557,Dataset S2-CS,0,,False
558,Metric,0,,False
559,NDCG@5 NDCG@10 NDCG@15 NDCG@20,0,,False
560,SetRank-t s 0.3847 0.4095 0.4256 0.4443,0,,False
561,SetRank-t 0.4157 0.4423 0.4655 0.4813,0,,False
562,SetRank,0,,False
563,0.4207 0.4431 0.4762 0.4950,0,,False
564,TREC-BIO,1,TREC,True
565,NDCG@5 NDCG@10 NDCG@15 NDCG@20,0,,False
566,S2-CS -ESQ,0,,False
567,NDCG@5 NDCG@10 NDCG@15 NDCG@20,0,,False
568,TREC-BIO -ESQ,1,TREC,True
569,NDCG@5 NDCG@10 NDCG@15 NDCG@20,0,,False
570,0.3414 0.3257 0.3140 0.3058,0,,False
571,0.4059 0.4311 0.4469 0.4683,0,,False
572,0.3257 0.3100 0.2994 0.2903,0,,False
573,0.3705,0,,False
574,0.3500,0,,False
575,0.3335,0,,False
576,0.3217 0.4800 0.5004 0.5266 0.5378,0,,False
577,0.3594 0.3380 0.3219 0.3100,0,,False
578,0.3744 0.3522 0.3363,0,,False
579,0.3246,0,,False
580,0.4983 0.5130 0.5450 0.5629,0,,False
581,0.3639 0.3406 0.3251 0.3132,0,,False
582,0.46 0.44,0,,False
583,S2-CS TREC-BIO,1,TREC,True
584,0.42,0,,False
585,NDCG@10,0,,False
586,0.4,0,,False
587,0.38,0,,False
588,0.36,0,,False
589,0.34,0,,False
590,0.32,0,,False
591,0.3,0,,False
592,0.28,0,,False
593,0,0,,False
594,0.2,0,,False
595,0.4,0,,False
596,0.6,0,,False
597,0.8,0,,False
598,1,0,,False
599,E,0,,False
600,Figure 4: Sensitivity of E in S2-CS and TREC-BIO datasets.,1,TREC,True
601,"TREC-BIO dataset, the optimal value of E is around 0.3, and if we increases E over 0.6, the ranking performance will drop quickly.",1,TREC,True
602,5.4 Effectiveness of Model Selection,0,,False
603,"5.4.1 Experimental Setup. In this experiment, we try to apply our unsupervised model selection algorithm to choose the best parameter settings of SetRank without using a validation set. We select entity token weight E , title field weight title , abstract field weight abs , dirichlet smoothing factors for both fields µtitle & µabs from {0.2, 0.3, . . . , 0.8}, {5, 10, 15, 20}, {1, 3, 5, 10}, and {500, 1000, 1500, 2000}, respectively. This generates totally 7 × 4 × 4 × 4 × 4 ,"" 1, 792 possible parameter settings and for each of them we can construct a ranking model.""",0,,False
604,"We first apply our unsupervised model selection algorithm (with either KT or posKT as the ranking distance) and obtain the most confident parameter settings returned by it. Then, we plug in these parameter settings into SetRank and denote it as AutoSetRank. For reference, we also calculate the average performance of all 1,792",0,,False
605,ranking models.,0,,False
606,572,0,,False
607,Session 5B: Entities,1,Session,True
608,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
609,Table 6: A case study comparing SetRank with SetRank-ts on one entity-set query in S2-CS. Note: Atari is a video game platform.,0,,False
610,Query Method,1,Query,True
611,1 2 3,0,,False
612,SetRank-t s,0,,False
613,reinforcement learning for video game,0,,False
614,SetRank,0,,False
615,"The effects of video game playing on attention, memory, and executive control",0,,False
616,A video game description language for model-based or interactive learning,0,,False
617,Can training in a real-time strategy video game attenuate cognitive decline in older adults?,1,ad,True
618,Playing Atari with Deep Reinforcement Learning,0,,False
619,A video game description language for model-based or interactive learning,0,,False
620,Real-time neuroevolution in the NERO video game,0,,False
621,"Table 7: Effectiveness of ranking model selection. SetRank-V S : parameters are tuned using 5-fold cross validation. AutoSetRank-(KT /posKT ): parameters are obtained based on our unsupervised model selection algorithm, which uses either KT or pos KT as ranking distance. Mean (±",0,,False
622,Std): the averaged performance of all ranking models with standard derivation shown.,0,,False
623,Dataset S2-CS,0,,False
624,Method,0,,False
625,SetRank-V S AutoSetRank-KT AutoSetRank-posKT,0,,False
626,Mean (± Std),0,,False
627,title abs E µtitle µabs,0,,False
628,NDCG@5,0,,False
629,20 5 0.7 1000 1000,0,,False
630,0.4207,0,,False
631,20 7 0.7 1500 2000,0,,False
632,0.4174,0,,False
633,20 5 0.7 1500 1500,0,,False
634,0.4173,0,,False
635,­ ­ ­ ­ ­ 0.3898 (± 0.0112),0,,False
636,NDCG@10,0,,False
637,0.4431 0.4427 0.4436 0.4128 (± 0.0106),0,,False
638,NDCG@15,0,,False
639,0.4762 0.4730 0.4731 0.4411 (± 0.0161 ),0,,False
640,NDCG@20,0,,False
641,0.4950 0.4929 0.4923 0.4543 (± 0.0163),0,,False
642,SetRank-V S,0,,False
643,20 5 0.2 1000 1000,0,,False
644,0.3744,0,,False
645,0.3522,0,,False
646,0.3363,0,,False
647,0.3246,0,,False
648,TREC-BIO,1,TREC,True
649,AutoSetRank-KT AutoSetRank-posKT,0,,False
650,20 20,0,,False
651,5 0.2 1500 1000 7 0.2 1000 1000,0,,False
652,0.3692 0.3748,0,,False
653,0.3472 0.3564,0,,False
654,0.3305 0.3367,0,,False
655,0.3173 0.3253,0,,False
656,Mean (± Std),0,,False
657,­ ­ ­ ­ ­ 0.3479 (± 0.0103) 0.3238 (± 0.0079) 0.3199 (± 0.0079) 0.3036 (± 0.0093),0,,False
658,"5.4.2 Experimental Result and Analysis. Table 7 shows the results, including the SetRank's performance when a labeled validation set is given. First, we notice that for S2CS dataset, although the parameter settings tuned over validation set do perform better than the ones returned by our unsupervised model selection algorithm, the difference is not significant. For TREC-BIO dataset, it is surprising to find that AutoSetRank-posKT can slightly outperforms SetRank tuned on validation set. Furthermore, the performance of AutoSetRank function is higher than the average performances of all possible ranking models by 2 standard deviations, which demonstrates the effectiveness of our unsupervised model selection algorithm.",1,TREC,True
659,5.5 Use Case Study: Bio-Literature Search,0,,False
660,"In this section we demonstrate the effectiveness of SetRank in a biomedical use case. As preparation, we build a biomedical literature search engine based on over 27 million papers retrieved from PubMed. Entities in all papers are extracted and typed using PubTator. This search system is cold-started with our proposed SetRank model and we show how SetRank can help this search system to accommodate a given entity-set query and returns a highquality rank list of papers relevant to the query. Comparison with PubMed, a widely used search engine for biomedical literature, will also be discussed. A biomedical case. Consider the following case of a biomedical information need. Genomics studies often identify sets of genes as having important roles to play in the processes or conditions under investigation, and the investigators seek to understand better what biological insights such a list of genes might provide. Suppose such a study, having examined brain gene expression patterns in old mice, identifies ten genes as being of potential interest. The investigator forms a query with these 10 genes, submits it to a literature search engine, and examines the top ten returned papers to look for an association between this gene set and a disease. The query consists of symbols of the 10 genes: ""APP, APOE, PSEN1, SORL1, PSEN2, ACE, CLU, BDNF, IL1B, MAPT "". Relevance criterion. We choose the above ten genes for our illustration because these are actually top genes associated with Alzheimer's disease according to DisGeNET [24], and it is unlikely",1,AP,True
661,"that there is another completely different (and unknown) commonality among them. Therefore, a retrieved paper is relevant if and only if it discusses at least one of the query genes in the context of Alzheimer's disease. Furthermore, among all relevant papers, we prefer those covering more unique genes. Result analysis. The top-5 papers returned by PubMed4 and our system are shown in Table 8. We see that the ""Alzheimer's disease"" is explicitly mentioned in the title of all the five papers returned by our system, and the top two papers cover 6 unique genes among the total 10 genes. All five papers returned by SetRank are highly relevant, since they all focus on the association between a subset of our query genes and Alzheimer's disease. In contrast, the top-5 papers retrieved by PubMed are dominated by two genes (i.e., APOE4 and BDNF) and contain none of the remaining eight. Only the 1st of the five papers is highly relevant. It focuses on the association between Alzheimer's disease (mentioned explicitly in the title) and our query gene set. Three other papers (ranked 2nd to 4th) are marginally relevant, in the sense that Alzheimer's disease is the context but not the focus of their studies. The paper ranked 5th is irrelevant. Therefore, users will prefer SetRank since it returns papers covering a large-portion of an entity-set query and helps them to find the association between this entity set with Alzheimer's disease.",1,AP,True
662,6 CONCLUSIONS AND FUTURE WORK,0,,False
663,"In this paper, we study the problem of searching scientific literature using entity-set queries. A distinctive characteristic of entity-set queries is that they reflect user's interest in inter-entity relations. To capture such information need, we propose SetRank, an unsupervised ranking framework which explicitly models entity relations among the entity set. Second, we develop a novel unsupervised model selection algorithm based on weighted rank aggregation to select SetRank's parameters without relying on a labeled validation set. Experimental results on two benchmark datasets corroborate the effectiveness of SetRank and the usefulness of our model selection algorithm. We further discuss the power of SetRankwith a real-world use case of biomedical literature search.",0,,False
664,"As a future direction, we would like to explore how we can go beyond pairwise entity relations and integrate higher-order",0,,False
665,"4 Querying PubMed with the exact same query returns 0 document. To get reasonable results,",1,Query,True
666,"PubMed users have to insert an OR logic between every pairs of genes, and change the default ""sorting by most recent"" to ""sorting by best match"".",0,,False
667,573,0,,False
668,Session 5B: Entities,1,Session,True
669,"SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA",0,,False
670,"Table 8: A real-world use case comparing SetRank with PubMed. The input query contains a set of 10 genes and reflects user's information need of finding an association between this gene set and an unknown disease. Entity mentions in returned paper titles are highlighted in brown and the entity mentions of Alzheimer's disease, which are used to judge paper relevance, are marked in red.",0,,False
671,Query Method Rank,1,Query,True
672,APP APOE4 PSEN1 SORL1 PSEN2 ACE CLU BDNF IL1B MAPT Paper Title,1,AP,True
673,1 2 PubMed 3 4 5,0,,False
674,Apathy and APOE4 are associated with reduced BDNF levels in Alzheimer's disease ApoE4 and A Oligomers Reduce BDNF Expression via HDAC Nuclear Translocation Cognitive deficits and disruption of neurogenesis in a mouse model of apolipoprotein E4 domain interaction APOE-epsilon4 and aging of medial temporal lobe gray matter in healthy adults older than 50 years Influence of BDNF Val66Met on the relationship between physical activity and brain volume,1,AP,True
675,1 2 SetRank 3 4 5,0,,False
676,"Investigating the role of rare coding variability in Mendelian dementia genes (APP, PSEN1, PSEN2, GRN, MAPT, and PRNP) in late-onset Alzheimer's disease Rare Genetic Variant in SORL1 May Increase Penetrance of Alzheimer's Disease in a Family with Several Generations of APOE- 4 Homozygosity APP, PSEN1, and PSEN2 mutations in early-onset Alzheimer disease: A genetic screening study of familial and sporadic cases Identification and description of three families with familial Alzheimer disease that segregate variants in the SORL1 gene The PSEN1, p.E318G variant increases the risk of Alzheimer's disease in APOE-4 carriers",1,AP,True
677,"entity relations into the current SetRank framework. Besides, it would be interesting to explore whether SetRank can effectively model domain expert's prior knowledge about the relative impor-",0,,False
678,"tance of entity relations. Furthermore, the incorporation of user interaction and and extension of current SetRank framework to weakly-supervised settings are also interesting research problems.",1,corpora,True
679,ACKNOWLEDGEMENTS,0,,False
680,This research is sponsored in part by U.S. Army Research Lab. under,0,,False
681,"Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), DARPA under",0,,False
682,"Agreement No. W911NF-17-C-0099, National Science Foundation IIS 16-",0,,False
683,"18481, IIS 17-04532, and IIS-17-41317, DTRA HDTRA11810026, and grant",0,,False
684,1U54GM114838 awarded by NIGMS through funds provided by the trans-,0,,False
685,NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov).,0,,False
686,REFERENCES,0,,False
687,[1] Joran Beel and Bela Gipp. 2009. Google Scholar's ranking algorithm: An Introductory Overview. In ISSI.,0,,False
688,[2] Avradeep Bhowmik and Joydeep Ghosh. 2017. LETOR Methods for Unsupervised Rank Aggregation. In WWW.,1,ad,True
689,"[3] Pavel Brazdil and Christophe Giraud-Carrier. 2017. Metalearning and Algorithm Selection: progress, state of the art and introduction to the 2018 Special Issue. Machine Learning (2017).",0,,False
690,"[4] David Carmel, Ming-Wei Chang, Evgeniy Gabrilovich, Bo-June Paul Hsu, and Kuansan Wang. 2014. ERD'14: entity recognition and disambiguation challenge. SIGIR Forum 48 (2014), 63­77.",0,,False
691,[5] Stéphane Clinchant and Éric Gaussier. 2010. Information-based models for ad hoc IR. In SIGIR.,1,ad,True
692,"[6] Don Coppersmith, Lisa Fleischer, and Atri Rudra. 2006. Ordering by weighted number of wins gives a good ranking for weighted tournaments. In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm. Society for Industrial and Applied Mathematics, 776­782.",0,,False
693,"[7] Jeff Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion using knowledge base links. In SIGIR.",0,,False
694,"[8] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, and Frank Hutter. 2015. Efficient and Robust Automated Machine Learning. In NIPS.",1,Robust,True
695,[9] Edward A. Fox and Joseph A. Shaw. 1993. Combination of Multiple Searches. In TREC.,1,TREC,True
696,[10] Darío Garigliotti and Krisztian Balog. 2017. On Type-Aware Entity Retrieval. In ICTIR.,0,,False
697,[11] John Guiver and Edward Snelson. 2009. Bayesian inference for Plackett-Luce ranking models. In ICML.,0,,False
698,"[12] Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009. Named entity recognition in query. In SIGIR.",0,,False
699,"[13] Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsberg. 2015. Entity linking in queries: Tasks and evaluation. In Proceedings of the 2015 International Conference on The Theory of Information Retrieval. ACM, 171­180.",0,,False
700,"[14] William R. Hersh, Aaron Cohen, Jianji Yang, Ravi Teja Bhupatiraju, Phoebe Roberts, and Marti Hearst. 2005. TREC 2005 Genomics Track Overview. In TREC.",1,TREC,True
701,"[15] Sarvnaz Karimi, Justin Zobel, and Falk Scholer. 2012. Quantifying the impact of concept recognition on biomedical information retrieval. Information Processing & Management 48, 1 (2012), 94­106.",0,,False
702,[16] Maurice G Kendall. 1955. Rank correlation methods. (1955).,0,,False
703,"[17] Alexandre Klementiev, Dan Roth, and Kevin Small. 2007. An Unsupervised Learning Algorithm for Rank Aggregation. In ECML.",0,,False
704,"[18] Alexandre Klementiev, Dan Roth, and Kevin Small. 2008. A Framework for Unsupervised Rank Aggregation. In SIGIR LR4IR Workshop.",0,,False
705,"[19] Xitong Liu and Hui Fang. 2015. Latent entity space: a novel retrieval approach for entity-bearing queries. Information Retrieval Journal 18 (2015), 473­503.",0,,False
706,[20] Zhiyong Lu. 2011. PubMed and beyond: a survey of web tools for searching biomedical literature. In Database.,0,,False
707,[21] Lucas Maystre and Matthias Grossglauser. 2015. Fast and Accurate Inference of Plackett-Luce Models. In NIPS.,0,,False
708,[22] Donald Metzler and W Bruce Croft. 2005. A Markov random field model for term dependencies. In SIGIR.,0,,False
709,[23] Paul Ogilvie and James P. Callan. 2003. Combining document representations for known-item search. In SIGIR.,0,,False
710,"[24] Janet Piñero, Àlex Bravo, Núria Queralt-Rosinach, Alba Gutiérrez-Sacristán, Jordi",0,,False
711,"Deu-Pons, Emilio Centeno, Javier García-García, Ferran Sanz, and Laura I Furlong.",0,,False
712,"2016. DisGeNET: a comprehensive platform integrating information on human disease-associated genes and variants. Nucleic acids research (2016). [25] Hadas Raviv, Oren Kurland, and David Carmel. 2016. Document Retrieval Using Entity-Based Language Models. In SIGIR. [26] Xiang Ren, Jiaming Shen, Meng Qu, Xuan Wang, Zeqiu Wu, Qi Zhu, Meng Jiang,",1,ad,True
713,"Fangbo Tao, Saurabh Sinha, David Liem, Peipei Ping, Richard M. Weinshilboum,",0,,False
714,and Jiawei Han. 2017. Life-iNet: A Structured Network-Based Knowledge Exploration and Analytics System for Life Sciences. In ACL. [27] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends in Information Retrieval (2009).,0,,False
715,"[28] Jiaming Shen, Zhenyu Song, Shitao Li, Zhaowei Tan, Yuning Mao, Luoyi Fu, Li",0,,False
716,"Song, and Xinbing Wang. 2016. Modeling Topic-Level Academic Influence in Scientific Literatures. In AAAI Workshop: Scholarly Big Data. [29] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Paul Hsu,",1,ad,True
717,"and Kuansan Wang. 2015. An Overview of Microsoft Academic Service (MAS) and Applications. In WWW. [30] Jie Tang, Jing Zhang, Limin Yao, Juan-Zi Li, Li Zhang, and Zhong Su. 2008. ArnetMiner: extraction and mining of academic social networks. In KDD. [31] Christophe Van Gysel and Maarten de Rijke. 2018. Pytrec_eval: An Extremely Fast Python Interface to trec_eval. In SIGIR. ACM. [32] Chih-Hsuan Wei, Hung-Yu Kao, and Zhiyong Lu. 2013. PubTator: a web-based text mining tool for assisting biocuration. In Nucleic Acids Research. [33] Jian Wu, Kyle Williams, Hung-Hsuan Chen, Madian Khabsa, Cornelia Caragea,",1,ad,True
718,"Alexander Ororbia, Douglas Jordan, and C. Lee Giles. 2014. CiteSeerX: AI in a Digital Library Search Engine. AI Magazine 36 (2014), 35­48. [34] Chenyan Xiong and James P. Callan. 2015. EsdRank: Connecting Query and Documents through External Semi-Structured Data. In CIKM. [35] Chenyan Xiong and James P. Callan. 2015. Query Expansion with Freebase. In ICTIR. [36] Chenyan Xiong, James P. Callan, and Tie-Yan Liu. 2016. Bag-of-Entities Representation for Ranking. In ICTIR. [37] Chenyan Xiong, James P. Callan, and Tie-Yan Liu. 2017. Word-Entity Duet Representations for Document Ranking. In SIGIR. [38] Chenyan Xiong, Russell Power, and James P. Callan. 2017. Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding. In WWW. [39] Yang Xu, Gareth J. F. Jones, and Bin Wang. 2009. Query dependent pseudorelevance feedback based on wikipedia. In SIGIR. [40] ChengXiang Zhai and John D. Lafferty. 2001. A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval. SIGIR Forum 51 (2001), 268­276.",1,Query,True
719,"[41] Zhibing Zhao, Peter Piech, and Lirong Xia. 2016. Learning Mixtures of PlackettLuce Models. In ICML.",0,,False
720,574,0,,False
721,,0,,False

,sentence,label,data,regex
0,Using Term Location Information to Enhance Probabilistic Information Retrieval,0,,False
1,"Baiyan Liu, Xiangdong An, Jimmy Xiangji Huang",0,,False
2,Information Retrieval and Knowledge Management Research Lab School of Information Technology,0,,False
3,"York University, Toronto, ON M3J 1P3, Canada",1,ad,True
4,"{baiyan, xan, jhuang}@yorku.ca",0,,False
5,ABSTRACT,0,,False
6,"Nouns are more important than other parts of speech in information retrieval and are more often found near the beginning or the end of sentences. In this paper, we investigate the effects of rewarding terms based on their location in sentences on information retrieval. Particularly, we propose a novel Term Location (TEL) retrieval model based on BM25 to enhance probabilistic information retrieval, where a kernel-based method is used to capture term placement patterns. Experiments on f ve TREC datasets of varied size and content indicate the proposed model signif cantly outperforms the optimized BM25 and DirichletLM in MAP over all datasets with all kernel functions, and excels the optimized BM25 and DirichletLM over most of the datasets in P@5 and P@20 with different kernel functions.",1,TREC,True
7,Categories and Subject Descriptors,0,,False
8,H.3.3 [Information Systems]: Information Search and Retrieval,0,,False
9,Keywords,0,,False
10,"Term location, probabilistic information retrieval, noun",0,,False
11,1. INTRODUCTION,1,DUC,True
12,"English has 5 basic sentence patterns [1, 10]: (1) Subject + Verb (e.g., Joe swims); (2) Subject + Verb + Object (e.g., Joe plays the guitar); (3) Subject + Verb + Complement (e.g., Joe becomes a doctor); (4) Subject + Verb + Indirect Object + Direct Object (e.g., I give her a gift); and (5) Subject + Verb + Object + Complement (e.g., We elect him president). Most English simple sentences follow these 5 patterns and exceptions are rare (compound and complex sentences can be split into simple sentences) [10], where nouns and noun-phrases are mostly located in the beginning or the end of sentences. On the other hand, past research has indicated that nouns and noun-phrases are more information-bearing than the other parts of speech in information retrieval (IR) [6, 3, 8, 12]. Therefore, integrating term location information into IR may help improve retrieval performances.",0,,False
13,"Two illustrative experiments are conducted to verify the hypothesis. The f rst illustrative experiment on WT2g dataset (247,491 web documents, TREC'99 Web track) indicates nouns do concen-",1,WT,True
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specif c permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",1,ad,True
15,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,0,,False
16,DOI: http://dx.doi.org/10.1145/2766462.2767827.,0,,False
17,"trate on both ends of sentences as shown in Table 1, where AvgDis is the average of the normalized distances of a set T of nouns from the middle of their sentences as def ned by Eq. 1. In Eq. 1, |T | means the cardinality of set T (In this paper, except as explicitly",0,,False
18,"noted, |x| means absolute value of x).",0,,False
19,"AvgDis ,",0,,False
20,tT avg(|M id(t) - P os(t)|/M id(t)) |T |,0,,False
21,(1),0,,False
22,"where M id(t) is the middle position of the sentence that noun t is in, P os(t) is the position of t, and T is the set of nouns. Since a term may appear in a document more than once, average function avg(.) is used. AvgDis has a range of [0, 1], and is closer to 0 if all nouns are gathered in the middle of sentences.",0,,False
23,"Table 1 shows that AvgDis > 0.5 on both halves of the sentences, which means that the nouns are nearer to the beginning or the end of sentences than to the middle of sentences.",0,,False
24,End Left Right,0,,False
25,Table 1: Noun placement in sentences,0,,False
26,AvgDis # Nouns Avg. sent. len. # Sentences,0,,False
27,"0.5901 24,918,926 0.613 26,286,542",0,,False
28,10.5619,0,,False
29,"14,360,676",0,,False
30,Table 2: Relevant noun placement in sentences,0,,False
31,Term,0,,False
32,Score Term Score,0,,False
33,louisa,0,,False
34,43 head,1,ad,True
35,-24,0,,False
36,gradgrind 27 ladi,1,ad,True
37,-17,0,,False
38,tom,0,,False
39,23 hand,0,,False
40,-16,0,,False
41,bounderbi 22 countri -16,0,,False
42,slackbridg 15 time,0,,False
43,-16,0,,False
44,The second illustrative experiment on Hard Times by Charles,0,,False
45,Dickens [2] illustrates that relevant terms are more likely located in,0,,False
46,"the beginning or the end of than in the middle of sentences as shown in Table 2, where Score > 0 if a term is more often found near the beginning or the end of sentences, and Score < 0 otherwise. To obtain Score of a term t in a document D, sentences in D are each partitioned into three parts, {p1 p2 p3}, where |p1| , |p3| and |p2| ,"" |p1| + |p3|, and a score to t for its each occurrence in D is""",0,,False
47,assigned by Eq. 2:,0,,False
48,"Score(t) ,",0,,False
49,1 -1,0,,False
50,if t  p1  p3 if t  p2,0,,False
51,(2),0,,False
52,Then Score of t for all of its occurrences in D is given by Eq.,0,,False
53,3:,0,,False
54,"Score(t, D) , Score(ti)",0,,False
55,(3),0,,False
56,ti D,0,,False
57,"where ti is the ith occurrence of t in D. Table 2 shows that the highest scoring terms ""louisa"", ""grad-",1,ad,True
58,"grind"", ""bounderbi"", ""tom"", and ""slackbridg"" turn out to be the",0,,False
59,883,0,,False
60,"main or minor characters in the book, whereas the lowest scoring terms are not particularly related with the novel.",0,,False
61,The results from the two illustrative experiments above indicate the hypothesis deserves a deeper investigation. The main contributions of this paper are as follows:,0,,False
62,· We extend BM25 naturally with term location information to enhance probabilistic information retrieveal;,0,,False
63,"· In order to reward terms that are more likely to be nouns, we propose a novel kernel-based Term Location (TEL) retrieval model to capture term placement patterns;",0,,False
64,· Experiments on f ve TREC datasets of varied size and content indicate the proposed model is highly promising.,1,TREC,True
65,2. RELATED WORK,0,,False
66,"Jing and Croft [6] proposed PhraseFinder to automatically construct collection-dependent association thesauri, and then used the association thesauri to assist query expansion and found that nouns and noun-phrases were most effective in improving IR. Liu et al. [8] classif ed noun-phrases into four types ­ proper names, dictionary phrases, simple phrases, and complex phrases ­ and ranked documents based on phrase similarity. Zheng et al [17] used nounphrases and semantic relationships to represent documents in order to assist document clustering, where noun-phrases were extracted with the assistance of WordNet. Yang et al [14] used a parse tree to transform the sentences in legal agreements into subject-verbobject (SVO) representations, which are then used in conjunction with cue terms to identify the provisions provided by the sentences. However, they found that provision extraction using the SVO representation resulted in high precision but low recall, which could be due to the specif city of SVO sentence patterns and the diff culty in parsing complex sentences. Hung et al. [4] used syntactic pattern matching to extract syntactically complete sentences that express event-based commonsense knowledge from web documents, and then semantic role labeling was used to tag the semantic roles of the terms in the sentences, such as the subject and the object. Ibekwe-SanJuan et al. [5] built f nite state automaton with syntactic patterns and synonyms from WordNet, which was used to tag the sentences in scientif c documents according to its category. It is diff cult to f nd syntactic patterns that are effective in all the documents of a single corpus, and rule-based part-of-speech taggers are less effective in unseen text [7]. Terms were rewarded based on their locations in a document in [15, 16].",0,,False
67,3. OUR APPROACH 3.1 Term Location,1,AP,True
68,We assume that the most important terms in the documents are,0,,False
69,near the beginning or the end of the sentences. We determine the importance (relevancy) of a term t by examining its distance from the middle of its sentence in document D as def ned by Eq. 4:,0,,False
70,"q(t, D) ,"" |M id(t, D) - P os(t, D)|""",0,,False
71,(4),0,,False
72,"where M id(t, D) ,"" (SL(t, D) - 1)/2,""",0,,False
73,"SL(t, D) is the length of the sentence in D that contains t, and P os(t, D) is the location of t in the sentence. We use the average distance of t from the middle of its sentences in D if t appears more than once in D, which is def ned as r(t, D) by Eq. 5:",0,,False
74,"r(t, D) ,"" ( q(ti, D))/tf (t, D)""",0,,False
75,(5),0,,False
76,ti D,0,,False
77,"where ti is the ith occurrence of t in D and tf (t, D) is the term frequency of t in D. We def ne m(t, D) to be the average length of the sentence(s) that contain t in D as Eq. 6:",0,,False
78,"m(t, D) ,",0,,False
79,"tiD SL(ti, D)   tf (t, D)",0,,False
80,+,0,,False
81,(6),0,,False
82,"where parameter  has a larger effect for longer sentences since it is proportional to the lengths of the sentences, and parameter  has a proportionally smaller effect for longer sentences since it is the same for all sentences. These two parameters are used to balance term weights in particularly short or long sentences.",0,,False
83,3.2 Kernel Functions,0,,False
84,In order to measure the distances of the terms from the middle,0,,False
85,"of their sentences, we f t a kernel function over each sentence. We",0,,False
86,adjust the weight of each term based on its average distance to the,1,ad,True
87,"middle of its sentences. In this paper, we explore the following",0,,False
88,"kernel functions for our location based reward function RN (t, D)",0,,False
89,used in Eq. 9:,0,,False
90,r2,0,,False
91,"Gaussian - Kernel(r, m) , 1 - e -2m2",0,,False
92,Triangle,0,,False
93,-,0,,False
94,"Kernel(r, m)",0,,False
95,",",0,,False
96,r m,0,,False
97,Cosine,0,,False
98,-,0,,False
99,"Kernel(r, m)",0,,False
100,",",0,,False
101,1,0,,False
102,-,0,,False
103,1,0,,False
104,+,0,,False
105,cos 2,0,,False
106,r m,0,,False
107,"Circle - Kernel(r, m) , 1 -",0,,False
108,1-,0,,False
109,r2 m,0,,False
110,"Quartic - Kernel(r, m) , 1 -",0,,False
111,1-,0,,False
112,r2 m,0,,False
113,2,0,,False
114,"Epanechnikov - Kernel(r, m) ,",0,,False
115,r2 m,0,,False
116,"Triweight - Kernel(r, m) , 1 -",0,,False
117,1-,0,,False
118,r 23 m,0,,False
119,"Among them, Gaussian kernel is widely used in statistics and machine learning such as Support Vector Machines, Triangle kernel, Circle Kernel, and Cosine Kernel are applied to estimate the proximitybased density distribution for the positional language model [9]. Since the kernel functions are not maturely applied in IR, we also explore Quartic kernel, Epanechnikov kernel and Triweight kernel in this work. In these kernel functions, r and m are def ned by Eqs. 5 and 6, respectively. With these kernel functions, the number of terms that are given maximum reward decreases as m(t, D) increases.",0,,False
120,3.3 Integration into BM25,0,,False
121,"In this experiment, we use the BM25 weighting model as our base weighting model. BM25 is def ned as follows:",0,,False
122,"Score(t, D) ,"" T F (t, D)  IDF (t)""",0,,False
123,(7),0,,False
124,where,0,,False
125,"T F (t, D)",0,,False
126,",",0,,False
127,(k3,0,,False
128,+ 1) (k3,0,,False
129, +,0,,False
130,"tf (t, D)  qtf qtf (t))  K",0,,False
131,(t),0,,False
132,",",0,,False
133,IDF,0,,False
134,(t),0,,False
135,",",0,,False
136,log2,0,,False
137,N,0,,False
138,- n(t) n(t) +,0,,False
139,+ 0.5 0.5,0,,False
140,",",0,,False
141,"K , k1 ",0,,False
142,1,0,,False
143,-,0,,False
144,b,0,,False
145,+,0,,False
146,b  |D| AvgDL,0,,False
147,"+ tf (t, D),",0,,False
148,"k1, k3, and b are tuning parameters for BM25, qtf (t) is the frequency of t in the query, |D| is the number of terms in D, AvgDL is the average length of the documents in the collection, N is the number of documents in the collection, and n(t) is the number of documents in the collection that contain t. We modify T F (t, D) to",0,,False
149,account for the reward given to the terms based on their locations in,0,,False
150,the sentences. We def ne the Term Location score (TL) as follows:,0,,False
151,"T L(t, D)",0,,False
152,",",0,,False
153,(k3,0,,False
154,+,0,,False
155,1),0,,False
156," RN(t, D)  tf (t, D) (k3 + qtf (t))  KT L",0,,False
157,qtf (t),0,,False
158,(8),0,,False
159,884,0,,False
160,Performance improvement (%),0,,False
161,MAP improvements of TEL over best BM25 with different kernel functions 3.5,1,MAP,True
162,3,0,,False
163,2.5,0,,False
164,2 1.5,0,,False
165,1 0.5,0,,False
166,0 WT2G,1,WT,True
167,DISK4&5,0,,False
168,Gaussian MAP Triangle MAP Circle MAP Cosine MAP Quartic MAP Epanechnikov MAP Triweight MAP,1,MAP,True
169,WT10G Dataset,1,WT,True
170,BLOGS06,0,,False
171,GOV2,0,,False
172,Performance improvement (%),0,,False
173,P@5 improvements of TEL over best BM25 with different kernel functions 7,0,,False
174,6,0,,False
175,Gaussian P@5,0,,False
176,Triangle P@5,0,,False
177,5,0,,False
178,Circle P@5,0,,False
179,Cosine P@5,0,,False
180,4,0,,False
181,Quartic P@5,0,,False
182,Epanechnikov P@5,0,,False
183,3,0,,False
184,Triweight P@5,0,,False
185,2,0,,False
186,1,0,,False
187,0,0,,False
188,-1,0,,False
189,-2 WT2G,1,WT,True
190,DISK4&5,0,,False
191,WT10G Dataset,1,WT,True
192,BLOGS06,0,,False
193,GOV2,0,,False
194,Performance improvement (%),0,,False
195,P@20 improvements of TEL over best BM25 with different kernel functions 2.5,0,,False
196,Gaussian P@20 2,0,,False
197,Triangle P@20,0,,False
198,Circle P@20,0,,False
199,1.5,0,,False
200,Cosine P@20,0,,False
201,Quartic P@20,0,,False
202,1,0,,False
203,Epanechnikov P@20,0,,False
204,Triweight P@20,0,,False
205,0.5,0,,False
206,0,0,,False
207,-0.5,0,,False
208,-1 WT2G,1,WT,True
209,DISK4&5,0,,False
210,WT10G Dataset,1,WT,True
211,BLOGS06,0,,False
212,GOV2,0,,False
213,Figure 1: Performance improvements of TEL over best BM25 with different kernel functions.,0,,False
214,Performance improvement (%),0,,False
215,MAP improvements of TEL over best DirichletLM with different kernel functions 6,1,MAP,True
216,P@5 improvements of TEL over best DirichletLM with different kernel functions 14,1,LM,True
217,P@20 improvements of TEL over best DirichletLM with different kernel functions 5,1,LM,True
218,Gaussian LM MAP,1,LM,True
219,5,0,,False
220,Triangle LM MAP,1,LM,True
221,Circle LM MAP,1,LM,True
222,4,0,,False
223,Cosine LM MAP,1,LM,True
224,Quartic LM MAP,1,LM,True
225,Epanechnikov LM MAP,1,LM,True
226,3,0,,False
227,Triweight LM MAP,1,LM,True
228,2,0,,False
229,1,0,,False
230,Performance improvement (%),0,,False
231,12,0,,False
232,4.5 Gaussian LM P@20,1,LM,True
233,Performance improvement (%),0,,False
234,4,0,,False
235,Triangle LM P@20,1,LM,True
236,10,0,,False
237,Circle LM P@20,1,LM,True
238,3.5,0,,False
239,Cosine LM P@20,1,LM,True
240,8,0,,False
241,3,0,,False
242,Quartic LM P@20,1,LM,True
243,Gaussian LM P@5,1,LM,True
244,Epanechnikov LM P@20,1,LM,True
245,6,0,,False
246,Triangle LM P@5,1,LM,True
247,2.5,0,,False
248,Triweight LM P@20,1,LM,True
249,Circle LM P@5,1,LM,True
250,4,0,,False
251,Cosine LM P@5,1,LM,True
252,2,0,,False
253,Quartic LM P@5,1,LM,True
254,1.5,0,,False
255,2,0,,False
256,Epanechnikov LM P@5,1,LM,True
257,Triweight LM P@5,1,LM,True
258,1,0,,False
259,0,0,,False
260,0.5,0,,False
261,0 WT2G,1,WT,True
262,DISK4&5,0,,False
263,WT10G Dataset,1,WT,True
264,BLOGS06,0,,False
265,GOV2,0,,False
266,-2 WT2G,1,WT,True
267,DISK4&5,0,,False
268,WT10G Dataset,1,WT,True
269,BLOGS06,0,,False
270,GOV2,0,,False
271,0 WT2G,1,WT,True
272,DISK4&5,0,,False
273,WT10G Dataset,1,WT,True
274,BLOGS06,0,,False
275,GOV2,0,,False
276,Figure 2: Performance improvements of TEL over best DirichletLM with different kernel functions.,1,LM,True
277,where,0,,False
278,"KT L , k1 ",0,,False
279,1,0,,False
280,-,0,,False
281,b,0,,False
282,+,0,,False
283,b  |D| AvgDL,0,,False
284,"+ RN (t, D)  tf (t, D) (9)",0,,False
285,"We integrate our model into BM25 to form the Term Location Score (TEL) as follows: T EL(t, D) ,"" ((1 - )  T F (t, D) +   T L(t, D))  IDF (t)""",0,,False
286,(10) where  controls the contribution of our model.,0,,False
287,4. EXPERIMENTAL RESULTS,0,,False
288,"We conduct experiments on f ve standard TREC collections: WT2G, DISK4&5, WT10G, BLOGS06, and GOV2. These datasets vary in both size and content, where WT2g contains 247,491 general Web documents (TREC'99 Web track), DISK4&5 is comprised of 528,155 newswire documents from sources such as the Financial Times and the Federal Register (TREC'97-99 Ad hoc track), WT10G has 1,692,096 general web documents (TREC'00-01 Web track), BLOGS06 consists of 3,215,171 feeds from late 2005 to early 2006 with associated permalink and homepage documents (TREC'06-08 Blog track), and GOV2 holds 25,178,548 documents crawled from .gov sites (TREC'04-06 Terabyte track).",1,TREC,True
289,We compare our model against the following weighting models when they perform best on each dataset with parameters obtained as follows:,0,,False
290,"1. BM25, with k1 , 1.2 and k3 ,"" 8. We adjust b in the range of [0.1, 0.9] in steps of 0.1 for each dataset to f nd the value of b that gives the best MAP for that dataset.""",1,ad,True
291,"2. DirichletLM. We adjust µ in the range of [100, 3000] in steps of 100. We f nd the optimal value of µ for each dataset.",1,LM,True
292,"The proposed model T EL uses the same values as BM25 for k1, k3, and b, and sets  ,"" 0.2,  "","" 3, and  "", 3 for all datasets.",0,,False
293,"In the future, we would study the optimal values of the model parameters and their relations with the characteristics of the datasets. We use the TREC off cial evaluation measures in our experiments, namely the topical MAP on BLOGS06 [11], and Mean Average Precision (MAP) on all the other datasets [13]. To stress the top retrieved documents, we also include P@5 and P@20 as the evaluation measures. All statistical tests are based on Wilcoxon Matchedpairs Signed-rank test.",1,TREC,True
294,"The experimental results are presented in Table 3. To illustrate the performance differences graphically, we plot the results in Figures 1 and 2. As shown by the two f gures, our model TEL outperforms optimized BM25 and DirichletLM in M AP over all datasets with all kernel functions, and outperforms the two optimized baseline models over most of the datasets in P @5 and P @20 with different kernel functions. The performance improvements of our model TEL against DirichletLM are greater than those against BM25. According to the two f gures, each kernel function has its advantage on some datasets. There is no single kernel function that outperforms others on all the datasets.",1,LM,True
295,5. CONCLUSIONS AND FUTURE WORK,0,,False
296,"In this paper, we extend BM25 and reward the terms based on their locations in the sentences with kernel functions. Experimental study shows that the proposed model performs signif cantly better than BM25 and DirichletLM on MAP over all datasets, and significantly better on P@5, p@10, and p@20 over most datasets.",1,LM,True
297,"In the future, more experiments will be conducted to further investigate the proposed model. We would investigate non-symmetric kernel functions and kernel functions with negative values since the placement of the terms at the beginning of the sentences is different from that at the end of the sentences as indicated in the f rst illustra-",0,,False
298,885,0,,False
299,Model BM 25 DirichletLM TEL Gaussian TEL Triangle TEL Circle TEL Cosine TEL Quartic TEL Epanechnikov TEL Triweight,1,LM,True
300,Eval Metric M AP P @5 P @20 M AP P @5 P @20 MAP P@5 P@20 MAP P@5 P@20 MAP P@5 P@20 M AP,1,AP,True
301,P @5,0,,False
302,P @20,0,,False
303,MAP P@5 P@20 MAP P@5 P@20 MAP P@5 P@20,1,MAP,True
304,WT2G,1,WT,True
305,0.3167 0.5120 0.3870,0,,False
306,0.3059 0.5080 0.3870,0,,False
307,"0.3223*+ (+1.77%,+5.36%)",0,,False
308,"0.5200* (+1.56%,+2.36%)",0,,False
309,"0.3960 (+2.33%,+2.33%)",0,,False
310,"0.3179 (+0.38%,+3.92%)",0,,False
311,"0.5040 (-1.56%,-0.79%)",0,,False
312,"0.3890* (+0.52%,+0.52%)",0,,False
313,"0.3235*+ (+2.15%,+5.75%)",0,,False
314,"0.5280* (+3.13%,+3.49%)",0,,False
315,"0.3950 (+2.07%,+2.07%)",0,,False
316,"0.3186 (+0.60%,+4.15%)",0,,False
317,"0.5160* (+0.78%,+1.57%)",0,,False
318,"0.3890* (+0.52%,+0.52%)",0,,False
319,"0.3199*+ (+1.01%,+4.58%)",0,,False
320,"0.5160* (+0.78%,+1.57%)",0,,False
321,"0.3900* (+0.78%,+0.78%)",0,,False
322,"0.3201*+ (+1.07%,4.64%)",0,,False
323,"0.5200* (+1.56%,+2.36%)",0,,False
324,"0.3930 (+1.55%,+1.55%)",0,,False
325,"0.3179 (+0.38%,+3.92%)",0,,False
326,"0.5080 (-0.78%,0.00%)",0,,False
327,"0.3880* (+0.26%,+0.26%)",0,,False
328,DISK4&5,0,,False
329,0.2176 0.4680 0.3613,0,,False
330,0.2190 0.4560 0.3627,0,,False
331,"0.2201 (+1.15%,+0.50%)",0,,False
332,"0.4627+ (-1.13%,+1.47%)",0,,False
333,"0.3653* (+1.11%,+0.72%)",0,,False
334,"0.2209* (+1.52%,+0.87%)",0,,False
335,"0.4613+ (-1.43%,+1.16%)",0,,False
336,"0.3640*+ (+0.75%,+0.36%)",0,,False
337,"0.2201 (+1.15%,+0.50%)",0,,False
338,"0.4627+ (-1.13%,+1.47%)",0,,False
339,"0.3647* (+0.94%,+0.55%)",0,,False
340,"0.2209* (+1.52%,+0.87%)",0,,False
341,"0.4613+ (-1.43%,+1.16%)",0,,False
342,"0.3643*+ (+0.83%,+0.44%)",0,,False
343,"0.2209* (+1.52%,+0.87%)",0,,False
344,"0.4613+ (-1.43%,+1.16%)",0,,False
345,"0.3643*+ (+0.83%,+0.44%)",0,,False
346,"0.2206 (+1.38%,+0.73%)",0,,False
347,"0.4640+ (-0.85%,1.75%)",0,,False
348,"0.3660* (+1.30%,0.91%)",0,,False
349,"0.2205* (+1.33%,+0.68%)",0,,False
350,"0.4613+ (-1.43%,+1.16%)",0,,False
351,"0.3640*+ (+0.75%,+0.36%)",0,,False
352,WT10G,1,WT,True
353,0.2134 0.3918 0.2776,0,,False
354,0.2168 0.3531 0.2745,0,,False
355,"0.2202* (+3.19%,+1.57%)",0,,False
356,"0.3898+ (-0.51%,10.39%)",0,,False
357,"0.2765 (-0.40%,+0.73%)",0,,False
358,"0.2196* (+2.91%,+1.29%)",0,,False
359,"0.3898+ (-0.51%,+10.39%)",0,,False
360,"0.2755+ (-0.76%,+0.36%)",0,,False
361,"0.2202* (+3.19%,+1.57%)",0,,False
362,"0.4000* (+2.09%,+13.28%)",0,,False
363,"0.2770 (-0.22%,+0.91%)",0,,False
364,"0.2197* (+2.95%,+1.43%)",0,,False
365,"0.3898+ (-0.51%,+10.39%)",0,,False
366,"0.2770+ (-0.22%,+0.91%)",0,,False
367,"0.2199* (+3.05%,+1.43%)",0,,False
368,"0.3918+ (+0.00%,+10.96%)",0,,False
369,"0.2755 (-0.76%,+0.36%)",0,,False
370,"0.2206* (+3.37%,+1.75%)",0,,False
371,"0.3918+ (+0.00%,+10.96%)",0,,False
372,"0.2760 (-0.58%,+0.55%)",0,,False
373,"0.2198* (+3.00%,+1.38%)",0,,False
374,"0.3898+ (-0.51%,+10.39%)",0,,False
375,"0.2765+ (-0.40%,+0.73%)",0,,False
376,BLOGS06,0,,False
377,0.3195 0.6380 0.6095,0,,False
378,0.3125 0.6080 0.5935,0,,False
379,"0.3238+ (+1.35%,+3.62%)",0,,False
380,"0.6640* (+4.08%,+9.21%)",0,,False
381,"0.6180 (+1.39%,+4.13%)",0,,False
382,"0.3241*+ (+1.44%,+3.71%)",0,,False
383,"0.6800* (+6.58%,+11.84%)",0,,False
384,"0.6225 (+2.13%,+4.89%)",0,,False
385,"0.3238+ (+1.35%,+3.62%)",0,,False
386,"0.6680* (+4.70%,+9.87%)",0,,False
387,"0.6185 (+1.48%,+4.21%)",0,,False
388,"0.3239*+ (+1.38%,+3.65%)",0,,False
389,"0.6760* (+5.96%,+11.18%)",0,,False
390,"0.6210 (+1.89%,+4.63%)",0,,False
391,"0.3239*+ (+1.38%,+3.65%)",0,,False
392,"0.6760* (+5.96%,+11.18%)",0,,False
393,"0.6220 (+2.05%,+4.80%)",0,,False
394,"0.3239+ (+1.38%,+3.65%)",0,,False
395,"0.6740* (+5.64%,+10.86%)",0,,False
396,"0.6195 (+1.64%,+4.38%)",0,,False
397,"0.3239*+ (+1.38%,+3.65%)",0,,False
398,"0.6760* (+5.96%,+11.18%)",0,,False
399,"0.6220 (+2.05%,+4.80%)",0,,False
400,GOV2,0,,False
401,0.3008 0.6094 0.5406,0,,False
402,0.2983 0.5919 0.5272,0,,False
403,"0.3045*+ (+1.23%,+2.08%)",0,,False
404,"0.6174*+ (+1.31%,+4.31%)",0,,False
405,"0.5383 (-0.43%,+2.11%)",0,,False
406,"0.3041*+ (+1.10%,+1.94%)",0,,False
407,"0.6067+ (-0.44%,,+2.50%)",0,,False
408,"0.5356 (-0.92%,+1.59%)",0,,False
409,"0.3045*+ (+1.23%,+2.08%)",0,,False
410,"0.6148*+ (+0.89%,+8.37%)",0,,False
411,"0.5376 (-0.55%,+1.97%)",0,,False
412,"0.3043*+ (+1.16%,+2.01%)",0,,False
413,"0.6054+ (-0.66%,+2.28%)",0,,False
414,"0.5383 (-0.43%,+2.11%)",0,,False
415,"0.3043*+ (+1.16%,+2.01%)",0,,False
416,"0.6040+ (-0.89,+2.04%)",0,,False
417,"0.5393 (-0.24%,+2.30%)",0,,False
418,"0.3044*+ (+1.20%,+2.04%)",0,,False
419,"0.6107*+ (+0.21%,+3.18%)",0,,False
420,"0.5383 (-0.43%,+2.11%)",0,,False
421,"0.3043*+ (+1.16%,+2.01%)",0,,False
422,"0.6054+ (-0.66%,+2.28%)",0,,False
423,"0.5379 (-0.50%,+2.03%)",0,,False
424,"Table 3: Comparison between TEL and two baselines BM25 and DirichletLM with different kernel functions: Parameter b of BM25 and parameter µ of DirichletLM are obtained and set individually for each dataset for their best performances, and ""*"" and ""+"" denote statistically signif cant improvements over BM25 and DirichletLM (Wilcoxon signed-rank test with p < 0.05), respectively.",1,LM,True
425,The best result obtained on each dataset is in bold. The two percentages below each value are the percentage improvement of TEL,0,,False
426,"over BM25 and DirichletLM, respectively.",1,LM,True
427,tive experiment. It is also worthwhile to analyze the optimal values of the model parameters and their relations with the characteristics of the datasets. Different term proximity measures would also be explored to improve the performance of our model.,0,,False
428,6. ACKNOWLEDGMENTS,0,,False
429,This research is supported by the research grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada and NSERC CREATE Award. We thank anonymous reviewers for their thorough review comments on this paper.,1,ad,True
430,7. REFERENCES,0,,False
431,"[1] H. Ann. The Essentials of English -- a writer's handbook. New York, Pearson Education, 2003.",0,,False
432,"[2] C. Dickens. Hard Times. Bradbury & Evans, 1854. [3] D. A. Evans and C. Zhai. Noun-phrase analysis in unrestricted text for",1,ad,True
433,"information retrieval. In ACL 1996, pages 17­24. [4] S.-H. Hung, C.-H. Lin, and J.-S. Hong. Web mining for event-based",0,,False
434,"commonsense knowledge using lexico-syntactic pattern matching and semantic role labeling. Expert Systems with Applications, 37(1):341­347, 2010. [5] F. Ibekwe-SanJuan and et al. Annotation of scientif c summaries for information retrieval. CoRR, 2011.",0,,False
435,"[6] Y. Jing and W. B. Croft. An association thesaurus for information retrieval. In RIAO'94, pages 146­160.",0,,False
436,"[7] K. Liu and et al. Effectiveness of lexico-syntactic pattern matching for ontology enrichment with clinical documents. Meth. of info. in med., 50(5):397, 2011.",0,,False
437,"[8] S. Liu, F. Liu, C. Yu, and W. Meng. An effective approach to document retrieval via utilizing wordnet and recognizing phrases. In SIGIR'04, pages 266­272.",0,,False
438,"[9] Y. Lv and C. Zhai. Positional language models for information retrieval. In SIGIR'09, pages 299­306.",0,,False
439,"[10] C. F. Meyer. Introducing English Linguistics. Cambridge University Press, 2010.",0,,False
440,[11] I. Ounis and et al. Overview of the TREC-2006 blog track. [12] O. Vechtomova. Noun phrases in interactive query expansion and document,1,TREC,True
441,"ranking. Info. Retrieval, 9:399­420, 2006. [13] E. Voorhees and D. Harman. TREC: Experiment and evaluation in information",1,TREC,True
442,"retrieval. MIT Press, 2005.",0,,False
443,"[14] D. Yang and et al. A natural language processing and semantic-based system for contract analysis. In ICTAI 2013, pages 707­712.",0,,False
444,"[15] J. Zhao, X. Huang, and S. Wu. Rewarding term location information to enhance probabilistic information retrieval. In SIGIR'12.",0,,False
445,"[16] J. Zhao, X. Huang, and Z. Ye. Modeling term associations for probabilistic information retrieval. ACM Trans. Inf. Syst., 32(2), 2014.",0,,False
446,"[17] H.-T. Zheng and et al. Exploiting noun phrases and semantic relationships for text document clustering. Info. Sci., 179(13):2249­2262, 2009.",0,,False
447,886,0,,False
448,,0,,False

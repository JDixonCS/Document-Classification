,sentence,label,data,regex
0,Non-Compositional Term Dependence for Information Retrieval,0,,False
1,Christina Lioma,0,,False
2,"Department of Computer Science University of Copenhagen, Denmark",0,,False
3,c.lioma@di.ku.dk,0,,False
4,Birger Larsen,0,,False
5,"Department of Communication Aalborg University Copenhagen, Denmark",0,,False
6,birger@hum.aau.dk,0,,False
7,ABSTRACT,0,,False
8,"Modelling term dependence in IR aims to identify co-occurring terms that are too heavily dependent on each other to be treated as a bag of words, and to adapt the indexing and ranking accordingly. Dependent terms are predominantly identified using lexical frequency statistics, assuming that (a) if terms co-occur often enough in some corpus, they are semantically dependent; (b) the more often they co-occur, the more semantically dependent they are. This assumption is not always correct: the frequency of co-occurring terms can be separate from the strength of their semantic dependence. E.g. red tape might be overall less frequent than tape measure in some corpus, but this does not mean that red+tape are less dependent than tape+measure. This is especially the case for non-compositional phrases, i.e. phrases whose meaning cannot be composed from the individual meanings of their terms (such as the phrase red tape meaning bureaucracy).",1,ad,True
9,"Motivated by this lack of distinction between the frequency and strength of term dependence in IR, we present a principled approach for handling term dependence in queries, using both lexical frequency and semantic evidence. We focus on non-compositional phrases, extending a recent unsupervised model for their detection [21] to IR. Our approach, integrated into ranking using Markov Random Fields [31], yields effectiveness gains over competitive TREC baselines, showing that there is still room for improvement in the very well-studied area of term dependence in IR.",1,TREC,True
10,Categories and Subject Descriptors,0,,False
11,H.4 [Information Systems Applications]: Miscellaneous; H.3.3 [Information Search and Retrieval],0,,False
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",1,ad,True
13,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,0,,False
14,DOI: http://dx.doi.org/10.1145/2766462.2767717.,0,,False
15,Jakob Grue Simonsen,0,,False
16,"Department of Computer Science University of Copenhagen, Denmark",0,,False
17,simonsen@di.ku.dk,0,,False
18,Niels Dalum Hansen,0,,False
19,"Department of Computer Science University of Copenhagen, Denmark",0,,False
20,nhansen@di.ku.dk,0,,False
21,General Terms,0,,False
22,"Theory, Experimentation",0,,False
23,1. INTRODUCTION,1,DUC,True
24,"Frege's principle of compositionality posits that the meaning of an expression is a function of the meanings of its constituent expressions and the ways they combine [59]. Applied to linguistics by Montague, this principle implies that the meaning of some text is not just the collective meaning of its words, but also a function of how these words are arranged. Whereas this holds most of the times, occasionally language is non-compositional, i.e. the meaning and arrangement of words alone is not enough to convey the overall semantics. E.g. the phrase red tape (meaning bureaucracy) is not a tape of type red. This linguistic phenomenon is known as non-compositionality.",0,,False
25,"The challenges posed by non-compositionality have spurred Natural Language Processing (NLP) research in automatic non-compositionality detection, e.g. in nouns [1, 47], verb-noun [20] and verb-particle [30] combinations, using techniques such as latent semantic analysis [20], compositional translations to multiple languages [44], sense induction [22] and word space models [23, 42]. An active line of research focuses on distributional and vector-based models of word and phrase meaning leading to vector-space models for compositionality [8, 35, 41]. These advances have not penetrated IR research notably (with the exception of [33], discussed in Section 2), despite long and persistent IR interest in term dependence. A resulting risk is that the strength of term dependence may be consistently miscalculated in IR. We explain this next.",1,ad,True
26,"In IR, dependent terms are predominantly identified using lexical frequency statistics: if terms co-occur often enough in some typically large dataset, they are assumed to be dependent, and the strength of their dependence is typically assumed proportional to their frequency of co-occurrence, e.g. see [10, 34]. Simply stated, the more frequently two terms co-occur, the more dependent we assume they are. This assumption is not always correct. In linguistics, the frequency of term co-occurrence can be somewhat separate from the strength of semantic dependence. Even though the former can be indicative to some extent of the latter, their relation is not symmetric. E.g. red tape might be overall less frequent than tape measure in some corpus, but this does not",0,,False
27,595,0,,False
28,"mean that red+tape are semantically less dependent than tape+measure; quite the contrary. Non-compositionality lies at the heart of this because non-compositional terms are maximally dependent, regardless of their frequency of co-occurrence. So, whereas the strength of term dependence within compositional phrases, e.g., tape measure, white horse, can be reasonably approximated by their frequency of co-occurrence in a corpus, this is not true for non-compositional phrases, like red tape, dark horse.",0,,False
29,"Motivated by this lack of distinction in IR between frequency of term co-occurrence and strength of term dependence, we present a principled approach for treating term dependence in queries. This approach extends a recent unsupervised model for detecting non-compositional phrases using lexical frequency and semantic evidence [21]. The main idea consists of (a) substituting a term in a phrase by a synonym (e.g. red tape would become scarlet tape) and (b) measuring the semantic divergence of the replacement phrase from the original phrase. If their meanings diverge, the original phrase is more likely to be non-compositional. If however their meanings do not diverge much (e.g. tax office would become tax bureau), then the original phrase is less likely to be non-compositional. We extend the vector space model proposed for measuring this divergence in [21] with a probabilistic model that measures the KullbackLeibler divergence between the language models of the original and replacement phrase (Sections 3-4). We apply both approaches to detect strongly dependent query terms, which we then treat in a non-bag of words fashion during ranking (Section 6). Experiments with 350 TREC queries show that our approaches consistently outperform competitive baselines, and are particularly effective for 2-, 3-, and 4-term queries in the web search task.",1,TREC,True
30,2. RELATED WORK,0,,False
31,"Broadly speaking, efforts to model term dependence, also known as term co-occurrence, adjacency and lexical affinities1 in IR, typically model phrases found in queries and/or documents, motivated by the intuition to consider as more relevant those documents in which terms appear in the same order and patterns as they appear in the query, and as less relevant those documents in which terms are separated [51]. These efforts were initiated mainly in the 1980s, and intensified in the 1990s, reporting retrieval benefits. Later, efforts decreased: baseline performance improved, and the cost associated with linguistic processing was not worth the small benefits over the already improved baselines [54].",1,ad,True
32,"Generally, term dependence is detected using either statistical or linguistic information. Research began with the early work on statistical term associations [6, 14, 24, 56] and syntax-based approaches [3, 7, 45], continuing with work on probabilistic term dependence models [15, 46, 60, 61, 63], syntactic methods [5, 32, 49, 50] and statistical approaches [10, 25, 26]. From the mid-1990s onwards, research focused on hybrid methods combining syntactic and statistical approaches of phrase processing [9], phrase-based enhancement of the indexed term representations [64], and phrase-based term weighting [37, 39, 57]. More recent research has focused on statistical methods, primarily using language modelling",0,,False
33,"1Dependence, co-occurrence, adjacency and lexical affinities are not synonyms [16], but in IR they are used interchangeably.",1,ad,True
34,"[31, 34, 36, 52, 55] but not exclusively [28, 40], while attention has also been given to term dependence and efficient large-scale indexing [12, 27]. The Markov Random Field (MRF) model of term dependence [31] reported significant improvements in retrieval effectiveness.",0,,False
35,"Several more recent studies address term dependence, for instance using heuristics [58], formalising the term position in the document [29], or extending the MRF model to concepts [4], all reporting positive findings. This continued interest in term dependence may indicate that it is still an open problem. However, to our knowledge, none of these approaches addresses non-compositionality, except Michelbacher et al. [33], who focus primarily on the automatic detection of the head modifier inside non-compositional phrases and use IR as a task illustrating that the information they detect can be useful. They experiment with a small nonTREC dataset and report statistically significant gains in retrieval precision.",1,ad,True
36,3. NON-COMPOSITIONALITY DETECTION (NCD),0,,False
37,"Non-Compositionality Detection (NCD) aims to identify the presence and strength of non-compositional phrases in language. This is typically realised as a measurement, i.e. through some function that outputs a compositionality score for a phrase. Given a scale of such scores, the minimum and maximum reflect the total absence of compositionality (noncompositionality), e.g. red tape, and complete compositionality, e.g. tax office, respectively. Sliding along such a scale corresponds to moving across phrases of various levels of compositionality, practically facilitating the comparison of phrases on the grounds of their term dependence. We reason that such a comparison may be useful to IR, where systems need to process differently queries at different positions of this scale, i.e. keyword-based (,"" compositional) queries such as London transportation, and queries containing heavily dependent terms ("",non-compositional) such as red tape AL register car.",0,,False
38,"This section presents how we use non-compositionality to model term dependence in IR. Among the various NCD approaches outlined in Section 1, we use the recent approach of [21] because it is unsupervised, resource-efficient, and performs competitively on benchmark tests. We extend this NCD approach, which uses vector spaces, by adding a second estimation of non-compositionality, this time probabilistic. In addition, we formally express the methodological description of [21] as a model of query perturbation, and we model non-compositional term dependence specifically for IR queries, not general phrases like [21], with considerations to data constraints in an IR context.",1,ad,True
39,3.1 Non-compositionality in queries,0,,False
40,"Given a query, we aim to detect the presence and strength of non-compositionality in it. Kiela and Clark [21] posit that non-compositional phrases can be identified by substituting each of the original words in them, one at a time, by some other relevant/synonymous term, and comparing the meaning of each phrase resulting from the substitution to the meaning of the original phrase. The more they diverge, the less compositional the original phrase is. E.g. replacing car by vehicle in import car gives import vehicle, which is semantically similar to the original, but replacing red by",0,,False
41,596,0,,False
42,"scarlet in red tape gives scarlet tape, which is semantically different from the original. Hence, red tape is less compositional than import car. The core idea is that such substitutions are likely to have a low impact on the semantics of compositional phrases, but a high impact on the semantics of non-compositional phrases. The resulting semantic divergence is then approximately inversely proportional to compositionality.",0,,False
43,"Conceptually, we see this approach as applying perturbations over some signal in order to study the resulting effects upon the signal. In our case, the signal is the query and the perturbation is the replacement of a query term by another term. We express this perturbation as follows: Let Sq(I; T ) be the semantic space S of query q containing the ordered set of terms T , where I is the information conveyed by T . Let T¯ denote the ordered set of query terms where one of them has been replaced by another term (e.g., a synonym). I.e., T¯ is the perturbation of T . Then, the non-compositionality Nq, and the compositionality Cq of query q, can be expressed as a function of the divergence  of the resulting semantic spaces (Sq(I; T ), Sq(I; T¯)), for all m , |T | divergences resulting from all substitutions:",0,,False
44,"Nq ,"" f {(Sq(I; T ), Sq(I; T¯) : T¯  {T1, . . . , Tm})} (1)""",0,,False
45,"Cq , g(Nq)",0,,False
46,(2),0,,False
47,"where f is typically some summation or averaging function over the set of divergences, and g is some decreasing function, e.g. g(x)  1/x . Thus, non-compositionality increases with semantic divergence, but compositionality decreases with semantic divergence.",0,,False
48,"Unless constrained, such perturbations risk drifting semantically further away than intended, e.g., if two out of all three terms in a query are replaced simultaneously. Kiela & Clarke address this using two constraints, which we also adopt: (a) only one term is replaced at a time, and (b) a term is replaced by its synonym or a closely related term such as a hyper- or hyponym2. We identify a further risk of degrading performance by considering too many synonyms: The set of perturbations for query q consisting of terms t1 · · · tm, where sj is a synonym of tj, is:",1,ad,True
49,s1t2 · · · tm,0,,False
50,t1s2 · · · tm,0,,False
51,"{p1, . . . , pm} ,",0,,False
52,... t1 · · · tj-1sj tj+1 · · · tm,0,,False
53,...,0,,False
54,t1 · · · tm-1sm,0,,False
55,"where pm denotes the mth perturbation. As a term may have more than one synonym, of various grades of synonymity, the set of perturbations can grow to include all synonyms of the query terms. A selection process must control the perturbations so that: (a) ""best possible"" synonyms (as opposed to near-synonyms) are used, and (b) the number of perturbations is minimised, i.e. we perturb the queries no more than necessary for computing their compositionality. We thus use one perturbation per query term and experimentally show (in Section 6) that this suffices for IR. Other tasks, including NCD per se, may require more perturbations per term.",1,ad,True
56,2We henceforth refer to all forms of closely related terms as synonyms.,0,,False
57,3.2 Semantic divergence,0,,False
58,Computing the divergence in Equation 1 requires that both the query and perturbations be represented in some semantic space that is tractable and amenable to measurement. Kiela & Clarke propose vector spaces (Section 3.2.1). We propose probability spaces as a complementary representation (Section 3.2.2). We present and experiment with both.,0,,False
59,3.2.1 Vector Space,0,,False
60,"We re-express the vector space representation of Kiela & Clarke for queries and their perturbations as follows. Let v(q) and v(pj) be the vector of query q and its perturbation pj respectively. The semantic divergence  between the query and its perturbation can be modelled as the distance d between their vectors (  d), where d is some appropriate distance function. This d can be chosen as any vector distance function, e.g. Euclidean, Chebychev, or the better-known Cosine we use here. Then, assuming a summation function f in Equation 1, the non-compositionality of a query containing terms t1 · · · tm of k synonyms is:",0,,False
61,1 mk,0,,False
62,mk,0,,False
63,"d(v(q), v(pij))",0,,False
64,(3),0,,False
65,"j,1 i,1",0,,False
66,where pij is the perturbation t1 · · · tj-1sij tj+1 · · · tm and sij is the ith synonym of term tj. Using one synonym per term,0,,False
67,only (as we do) reduces this to:,0,,False
68,1m,0,,False
69,"m d(v(q), v(pj))",0,,False
70,(4),0,,False
71,"j,1",0,,False
72,"The main idea is to represent a query and its perturbation as vectors, so that we can interpret their distance as semantic divergence. Practically this means mapping  from Equation (1) to d above. Dating back to Salton, the IR and NLP literature abounds with variations of how the above vector representation can be implemented and interpreted, any of which can be used here. We describe how we build the vectors and how we compare their distance in Section 4.",0,,False
73,3.2.2 Kullback-Leibler Divergence,0,,False
74,"We now present our probabilistic representation of queries and their perturbations. The high-level difference from the previous representation is that instead of representing a query as a vector of terms, we represent it as a distribution of events, where the events correspond to terms. Such representations are called probabilistic because they allow computing the probability of an event occurring, i.e. the probability of a term occurring in the query. When these probabilities are interpreted in a frequentist way, they are approximated by relative frequencies (i.e. normalised word counts). In text processing, this is known as language modelling.",1,ad,True
75,"We reason that, if queries and their perturbations are represented as event distributions, then their divergence can be computed using standard methods, one of the better known being their Kullback-Leibler divergence (KLD). Even though KLD is not a distance metric (it is not symmetric), it is widely used in IR to approximate the semantic distance between texts, where higher KLD values indicate more divergence. We apply this to compute the semantic divergence  in Equation 1, by building a language model for the query and each perturbation. Then, their KLD should be proportional to the semantic divergence  in Equation (1), i.e.",0,,False
76,597,0,,False
77,(  KLD). Let LMq and LMp denote the language models of query q and perturbation p respectively. Their KLD is:,1,LM,True
78,K LD(LMq ||LMp ),1,LM,True
79,",",0,,False
80,LMq,1,LM,True
81,log,0,,False
82,LMq LMp,1,LM,True
83,(5),0,,False
84,"We next describe how we build LMq, LMp and how we operationalise Equation (5).",1,LM,True
85,4. MODEL INDUCTION,1,DUC,True
86,"Both vector and probability space representations presented above approximate how different a perturbation is from the original query, albeit in different ways. This section describes their exact mechanics.",0,,False
87,"We start by describing what the above vectors and language models actually consist of. As the approach is the same for both queries and their perturbations, we henceforth refer to their union as Q. For each term t  Q, we build a context window as follows: we extract bags of terms occurring within a window of maximum n terms away from t in some large document corpus, so that the window consists of 2n + 1 terms. E.g., if n,""5, then we consider 11 terms in total: 5 (immediately preceeding t) + t + 5 (immediately succeeding t). The underlying assumption is that all the terms in a document have some relationship to all other terms in the document, modulo window size, outside of which the relationship is not taken into consideration. In statistical NLP this is a standard way of inducing word semantics from """"the company they keep"""", a.k.a. distributional semantics [11]. These context windows provide the ingredients of the vector and probabilistic representation of our queries and perturbations, explained next.""",0,,False
88,4.1 Vector representation,0,,False
89,"After all context windows of a term t  Q are extracted, we compute a term weight vector wt for t with the aim of capturing the salience of term t. Kiela & Clarke show that such weights can function in a discriminative way for the task of NCD. For each query, we generate a term weight vector by combining the term weight vectors of the terms in the query. Next we explain how we compute the weights of the individual query terms and the weight of the whole query or perturbation.",0,,False
90,4.1.1 Individual Term Weights,0,,False
91,"Kiela & Clarke experiment with these five well-known weighting schemes, adapted to the context window scenario, (even though they only report results from LTU), which we also use:",1,ad,True
92,ATC [43]:,0,,False
93,"wit ,",0,,False
94,0.5,0,,False
95,+,0,,False
96,0.5,0,,False
97,×,0,,False
98,fit maxf,0,,False
99,),0,,False
100,log(,0,,False
101,N n(t),0,,False
102,(6),0,,False
103,"N i,1",0,,False
104,0.5 + 0.5,0,,False
105,×,0,,False
106,fit maxf,0,,False
107,log,0,,False
108,N n(t),0,,False
109,2,0,,False
110,LTU [48]:,0,,False
111,wit,0,,False
112,",",0,,False
113,(log(fit) + 1.0) log,0,,False
114,0.8,0,,False
115,+,0,,False
116,0.2,0,,False
117,Mi av.M,0,,False
118,N n(t),0,,False
119,(7),0,,False
120,Mutual Information (MI) [38]:,0,,False
121,fjt,0,,False
122,"wit , log",0,,False
123,N,0,,False
124,× N,0,,False
125,"j,1",0,,False
126,fjt,0,,False
127,N,0,,False
128,"Mi k,1",0,,False
129,fik,0,,False
130,N,0,,False
131,(8),0,,False
132,Okapi [18]:,0,,False
133,"wit ,",0,,False
134,fit,0,,False
135,0.5 + 1.5 ×,0,,False
136,Mi av.M,0,,False
137,+ fit,0,,False
138,× log,0,,False
139,N - n(t) + 0.5 fit + 0.5 (9),0,,False
140,TFxIDF [53]:,0,,False
141,N,0,,False
142,"wit , log(fit) × log n(t)",0,,False
143,(10),0,,False
144,where wit is the weight of term t in context window i; fit is the frequency of t in context window i; N is the total number of context windows; n(t) is the number of context windows containing t; Mi is the number of terms in context window i; av.M is the average number of terms in all context windows; and maxf is the maximum frequency of any term in any context window.,0,,False
145,"To construct a vector v(t) for each t  Q, we extract the context windows for t, which we denote cwt. For each term, t , represented by an entry in v(t), the corresponding weight is computed as the average of wit for i  cwt.",0,,False
146,4.1.2 Query/Perturbation Weights,1,Query,True
147,"Having built such a vector for each t  Q, the vector of the entire query or perturbation can be constructed in several ways, for instance as the element-wise sum of the vectors of its terms, or as their dilation, or as their pointwise multiplication. We choose the latter because it has been shown more effective for semantic vector representations in NLP [21, 35]. The final query vector q for query q consisting of terms t1 · · · tm is:",0,,False
148,"v(q) , v(t1) · · · v(tm)",0,,False
149,(11),0,,False
150,"where is the binary operator on equal-length vectors of real numbers defined by (x1, . . . , xn) (y1, . . . , yn) ,"" (x1 × y1, . . . , xn × yn). The perturbation vectors are built identically to this. Note that as is associative and commutative, the jth component of v(q) is simply the product of all the jth components of the vectors v(t1), . . . , v(tm).""",0,,False
151,"As Kiela & Clarke point out, using pointwise multiplication has a somewhat `reverse' effect on the semantic distance: overlapping components (i.e. terms appearing in common contexts) are stressed; since their vectors have little overlap outside the non-compositional meaning, their perturbations also have little overlap, resulting in a smaller change in distance when perturbed. Another effect of pointwise multiplication is that the frequency of terms occurring in the context windows of a query term will be strengthened: if a term t has a high weight in both v(t) and v(t ), it will have a high weight in v(t) v(t ); however, low weight in either one of v(t) or v(t ) will correspond to low weight in v(t) v(t ). This means that the vectors of the terms of non-compositional queries, which will in general occur in very different contexts, will have entries with fairly low absolute values. In contrast, for compositional queries, substituting a term by its synonym may yield constructions that",0,,False
152,598,0,,False
153,"can be expected to occur in a number of contexts wildly different from the original, hence will have markedly different contextual statistics and thus greater distance d.",0,,False
154,4.2 Language modelling representation,0,,False
155,"The alternative representation we propose for queries and perturbations is to use the set of all context windows of the terms in a query or perturbation to build a respective language model LMq, LMp (introduced in Equation 5). There exist various ways of building language models from term counts, involving some sort of smoothing of the counts; we use two among the best known, Laplace and Simple GoodTuring.",1,LM,True
156,"Laplace (or add-one) estimates the probability of a term t in the language model of query q, PLP (q, t), as:",1,ad,True
157,"PLP (q, t)",0,,False
158,",",0,,False
159,"cq,t + 1 Cq + V",0,,False
160,(12),0,,False
161,"where cq,t is the count of t in q, Cq is the count of all terms in the context windows of q, and V is the number of terms in the language model of q. We compute it identically for perturbations (replacing q by p above).",0,,False
162,"For sparse data over large vocabularies, Laplace tends to make a very big change to the counts and resulting probabilities because it moves too much probability mass to all unseen events (zero counts). We could move a bit less mass by adding a fractional count rather than 1 (e.g. add ""smoothing"" [17]), but that would require choosing  dynamically, risking inappropriate discounting for many counts, and producing overall counts with poor variances [19]. For these reasons, we also apply Simple Good-Turing [13] smoothing, which uses (i) the counts of hapax legomena (events occurring once) to estimate the counts of unseen events, and (ii) double counts, i.e. the frequency of a frequency. Simple Good-Turing estimates the probability of a term t with frequency r in the language model of query q, PGT (q, t), as:",1,ad,True
163,"PGT (q, t)",0,,False
164,",",0,,False
165,(r + 1) · S(f fr+1) Cq · S(f fr),0,,False
166,for,0,,False
167,r>0,0,,False
168,(13),0,,False
169,"where f f is a vector with frequencies for term frequencies, Cq is as defined as in Equation 12, and S is a function fitted through the observed values of f f to get the expected count of these values (see [13] for more). For zero count values the probability is calculated as follows:",0,,False
170,"PGT (q, t)",0,,False
171,",",0,,False
172,f f1 Cq,0,,False
173,"for r , 0",0,,False
174,(14),0,,False
175,"where f f1 is the frequency of frequency of hapax legomena. We normalise the resulting language model to sum to 1. Simple Good-Turing is known to perform well, especially for large numbers of observations drawn from large vocabularies.",0,,False
176,"The above two smoothing methods produce a language model for each term per query or perturbation. To produce one language model for the whole query or perturbation, we sort the language models of their terms and combine them in four different ways: (1) summing their values in quantiles 2 & 33; (2) averaging their values in quantiles 2 & 3; (3) multiplying their values; (4) using the median of their values. Overall, the above 2 smoothing methods × 4 combinations produce 8 language modelling variations of NCD.",0,,False
177,3We use quantiles 2 & 3 to avoid outliers.,0,,False
178,5. DISCUSSION OF OUR NCD APPROACH,1,AP,True
179,"Both representations (vector and probability space) of the NCD approach we present are parameterised over the notion of semantic divergence, which we operationalise with different weightings, each corresponding to some variation of computing this divergence. Our use of semantic divergence, measured typically as a real number in the model, corresponds to the observation that compositionality is not dichotomous: phrases in general are not only compositional or non-compositional; rather, a fine-grained range of compositionality exists, a fact corroborated by human raters asked to score degrees of compositionality [2, 30]. Suitable divergence functions that could mimic the scores of human raters may exist, but we have not attempted to do so.",0,,False
180,"We have also not attempted to estimate the semantic `accuracy' of the phrases resulting from each perturbation, i.e. the extent to which they are non-sensical, even though Kiela & Clarke state that this is possible with their approach [21]. We estimate solely the divergence between the query and a perturbation, and not how much sense the perturbed phrase makes, for two reasons: (a) we reason that the semantic divergence should in principle suffice for indicating compositionality as we intend to use it in IR; (b) to our knowledge, no scalable automatic approach can adequately approximate such a semantic assessment for query logs.",1,ad,True
181,"Another point of departure from Kiela & Clarke is our treatment of query terms as a list, i.e. a set endowed with a strict order. In principle, all computations presented, both by Kiela & Clarke and by us, can be used with ordinary (i.e., unordered) sets of terms too, as has also been done with term dependence models in IR [31]. We use strictly ordered sets because non-compositionality is never manifested in language in any other way, for instance by mixing the order of non-compositional terms, or by interrupting them by another term. E.g., red tape can function non-compositonally (and mean bureaucracy) only when the terms red and tape appear adjacent and in that specific order. Ergo, no variation of red .+ tape or tape .+ red (in RegEx notation) can have the non-compositional meaning of bureaucracy.",1,ad,True
182,"Finally, perturbations are common in science, and the practice of perturbing queries has even been used in IR before, albeit for different reasons. For instance Vinay et al. [62] employ different query (and document) perturbations for query performance prediction: by altering the query term weights, they observe the documents retrieved, and study the relationship between the amount, or sensitivity of perturbation and the quality of the ranking. Our approach, apart from having a different overall scope, namely term dependence as opposed to query performance prediction, also differs from [62] in that it applies a linguistically informed selection process for each perturbation: we replace query terms by their synonyms, not by varying their respective term weights within some range.",0,,False
183,6. EVALUATION,0,,False
184,6.1 Using NCD for selective term dependence,0,,False
185,"This section presents experiments aiming to quantify the effectiveness of processing query term dependence, not as a bag of words, but as a `set phrase' of strict ordered adjacency, i.e. matching documents that contain an identical (ordered & uninterrupted) sequence of terms. The main idea is to use NCD to select which among a batch or stream",1,ad,True
186,599,0,,False
187,"of queries contain dependent terms, and process only those queries as a `set phrase'; the rest of the queries can be processed as a bag of words. For this initial study, we focus on the non-compositionality of the whole query, not of phrases within queries.",0,,False
188,"We use the non-compositionality score of each query (computed with any of the 5 vector space or 8 language modelling variants presented in Section 4) as a proxy of term dependence. This allows to detect queries more likely to be noncompositional, hence more likely to contain highly dependent terms, rather than those queries that are strictly noncompositional. We do this by ranking queries by their noncompositionality and selecting the  least compositional. These  queries are processed with the MRF model of fully dependent query terms; the rest of the queries in the batch are treated as a bag of words.",0,,False
189,6.2 Experimental Setup,0,,False
190,6.2.1 Baselines & Our Methods,0,,False
191,"We use three baselines: (1) bag of words for all queries, which allows for no term dependence; (2) the MRF model of sequentially dependent query terms [31], which treats as a `set phrase' only adjacent query terms; (3) the MRF model of fully dependent query terms [31], which treats as a `set phrase' the whole query. We compare these baselines against our selective term dependence approach that treats as a `set phrase' the whole query iff the NCD score of this query indicates that it is likely to be non-compositional; this is controlled by the threshold  presented above.",1,ad,True
192,"All three baselines and our 13 NCD variants use a unigram, query likelihood, Dirichlet-smoothed language model for ranking. Note that we use `language model' in two different ways in this work, for two entirely different computations: (a) to estimate the semantic divergence between queries and perturbations (in Section 3.2.2), and (b) to rank documents with respect to queries.",0,,False
193,6.2.2 Data & Tuning,0,,False
194,"We use the TREC 6-8 queries (301-450, title only) of the AdHoc track with Disks 4-5 (minus the Congressional Records for TREC7-8), and queries 1-200 of the Web AdHoc tracks of TREC 2009-2012 with ClueWeb09B4 (see Table 1). We extract the distributional semantics of the NCD model (i.e. build the context windows) from Disks4-5 for queries 301-450, and from ClueWeb09B for queries 1-200. We use no stemming and remove stop words from the queries only (as in [31]). We use Indri 5.85 for indexing and retrieval of at most 1000 documents per query. We evaluate retrieval effectiveness using standard measures of early and deep precision (MAP, NDCG@10, P@10).",1,TREC,True
195,"The Dirichlet ranking model includes a parameter µ that we tune as follows: µ  {100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000, 10000}. We also vary the number  of least compositional queries selected each time:   1 . . . 45 per TREC batch of 50 queries. All tuning is done per evaluation measure using 3-fold cross validation. We report the average of the three test folds. For NCD we extract the first synonym suggested by WordNet6 (to be used for perturbing the query). For these initial experiments, we do not vary the",1,TREC,True
196,4http://lemurproject.org/clueweb09.php/ 5http://www.lemurproject.org/ 6http://wordnet.princeton.edu,0,,False
197,# Documents # Queries TREC track,1,TREC,True
198,Table 1: Datasets,0,,False
199,Disks4-5,0,,False
200,ClueWeb09B,1,ClueWeb,True
201,556077,0,,False
202,50220423,0,,False
203,301-450,0,,False
204,1-200,0,,False
205,TREC6-8 AdHoc Web09-12 AdHoc,1,TREC,True
206,Table 2: Query length (without stopwords) #1 #2 #3 #4 #5,1,Query,True
207,DISKS4-5 11 56 76 7 CWEB09B 57 65 63 13 2,1,CW,True
208,"value of the window of co-occurrence described in Section 4: we set n ,"" 5, i.e. the context window size is 11.""",0,,False
209,6.3 Findings,0,,False
210,"Table 3 shows the retrieval precision of our baselines and NCD approaches. Each cell also displays the % of queries that are treated as a `set phrase'. For the MRF models, 100% means that all queries are treated as a `set phrase', including single-term queries, for which this treatment makes no difference over a bag of words treatment. Overall, our NCD approaches outperform all baselines at all times. The improvement over the strongest baseline is modest (up to >+3.5% for MAP with ATC, >+3.3% for NDCG@10 with MI, and >+4.5% for P@10 with MI), however it is consistent for both datasets and for all evaluation measures (deep and early precision). This means that the performance gain spans across the range of relevant documents (those retrieved in the top ranks, but also those retrieved further down). Unlike earlier findings that the use of co-occurrence information tends to reduce retrieval effectiveness [46], possibly due to the fact that the term relationships modelled may have little discriminating power [31], we notice an overall modest but clear gain in effectiveness.",1,MAP,True
211,"Breaking this down to a per-query basis (cf. the two top plots in Fig. 1), the following two findings emerge. (I) The scale of improvement is higher than that of deterioration: between +0.13 and -0.07 for MAP; and between +0.68 and -0.4 for NDCG@10, for our Laplace sum approach (chosen illustritatively) from the strongest baseline (MRF with full dependence). (II) More queries improve than deteriorate by our approach. Hence, the improvements in Table 3 are not artificially inflated by outliers that might affect the means of the evaluation measures, but are rather representative of the whole body of queries.",1,MAP,True
212,"Furthermore, we show examples of queries yielding the highest and lowest precision difference from the strongest baseline in Table 5. The best queries are not strictly noncompositional; however they do have strongly contextualised semantics and term co-dependence. E.g. french lick resort casino does not denote some other meaning than a particular casino, but it is presumably irrelevant to the semantics of the verb to lick and french as a language or nationality. Most of the best queries in Table 5 are web queries, which often tend to include abbreviations and acronyms, e.g. vbart sf. These are not non-compositional either, but rather idiomatic or colloquial phrases of strong term dependence, and are selected by our NCD approach because they are likely to diverge in meaning if perturbed (i.e. it is not possible to express their meaning alternatively, for instance by near-synonyms). Hence, using NCD to approximate strong term dependence is effective in these",0,,False
213,600,0,,False
214,Table 3: Retrieval precision of the 3 baselines (in grey rows) vs. our 13 non-compositionality approaches.,0,,False
215,Bold marks >highest baseline. The star * marks best overall per measure & collection. %DQ is the % of,0,,False
216,queries processed as dependent (the rest of the queries in the batch are processed as bags of words).,0,,False
217,METHOD,0,,False
218,DISKS4-5,0,,False
219,CWEB09B,1,CW,True
220,DISKS4-5,0,,False
221,CWEB09B,1,CW,True
222,DISKS4-5,0,,False
223,CWEB09B,1,CW,True
224,MAP %DQ MAP %DQ NDCG@10 %DQ NDCG10 %DQ P@10 %DQ P@10 %DQ,1,MAP,True
225,Bag of words,0,,False
226,.1905,0,,False
227,­ .1151,0,,False
228,­ .4276,0,,False
229,­ .3502,0,,False
230,­ .3907,0,,False
231,­ .4167,0,,False
232,-,0,,False
233,Sequential Dependence [31] .1814 100% .1077 100% .3983,0,,False
234,100% .3463,0,,False
235,100% .3687 100% .4120 100%,0,,False
236,Full Dependence [31],0,,False
237,.1933 100% .1151 100% .4341,0,,False
238,100% .3514,0,,False
239,100% .4007 100% .4176 100%,0,,False
240,VECTOR LANG. MODEL,0,,False
241,Laplace sum,0,,False
242,.1948 63% .1188 34% .4406,0,,False
243,70% .3596,0,,False
244,18% .4047 67% .4317 30%,0,,False
245,Laplace average,0,,False
246,.1948 63% .1186 34% .4406,0,,False
247,70% .3596,0,,False
248,18% .4047 67% .4307 36%,0,,False
249,Laplace median,0,,False
250,.1947 67% .1176 51% .4390,0,,False
251,48% .3585,0,,False
252,18% .4060 67% .4278 31%,0,,False
253,Laplace multiplication,0,,False
254,.1948 48% .1182 46% .4388,0,,False
255,36% .3617,0,,False
256,22% .4040 59% .4303 22%,0,,False
257,GoodTuring sum,0,,False
258,.1940 81% .1168 50% .4402,0,,False
259,57% .3618,0,,False
260,31% .4040 79% .4288 28%,0,,False
261,GoodTuring average,0,,False
262,.1940 81% .1167 50% .4402,0,,False
263,57% .3618,0,,False
264,29% .4040 79% .4288 26%,0,,False
265,GoodTuring median,0,,False
266,.1949 73% .1168 56% .4422,0,,False
267,51% .3583,0,,False
268,15% .4067* 51% .4283 32%,0,,False
269,GoodTuring multiplication .1943 71% .1171 29% .4390,0,,False
270,59% .3623,0,,False
271,28% .4053 72% .4302 22%,0,,False
272,ATC,0,,False
273,.1950* 77% .1191* 47% .4446*,0,,False
274,55% .3604,0,,False
275,31% .4053 56% .4308 53%,0,,False
276,LTU,0,,False
277,.1948 75% .1184 40% .4444,0,,False
278,51% .3592,0,,False
279,29% .4053 52% .4278 33%,0,,False
280,MI,0,,False
281,.1946 81% .1188 51% .4445,0,,False
282,59% .3631* 32% .4053 52% .4364* 52%,0,,False
283,Okapi,0,,False
284,.1948 73% .1180 48% .4427,0,,False
285,57% .3597,0,,False
286,20% .4040 57% .4293 21%,0,,False
287,TFIDF,0,,False
288,.1941 56% .1175 30% .4422,0,,False
289,61% .3605,0,,False
290,39% .4053 53% .4294 30%,0,,False
291,Table 4: Retrieval precision for 2/3/4-term queries with our three best non-compositionality approaches. (±,0,,False
292,%): difference from the strongest baseline. Rest of notation as in Table 3.,0,,False
293,METHOD,0,,False
294,Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI,0,,False
295,2 terms (56 queries),0,,False
296,MAP,1,MAP,True
297,%DQ,0,,False
298,.1994,0,,False
299,­,0,,False
300,.1953,0,,False
301,100%,0,,False
302,.2022,0,,False
303,100%,0,,False
304,.2115* (+4.6%) 48%,0,,False
305,.2114 (+4.5%) 48%,0,,False
306,.2114 (+4.5%) 48%,0,,False
307,DISKS4-5,0,,False
308,3 terms (76 queries),0,,False
309,MAP,1,MAP,True
310,%DQ,0,,False
311,.1985,0,,False
312,­,0,,False
313,.1722,0,,False
314,100%,0,,False
315,.1976,0,,False
316,100%,0,,False
317,.2046* (+3.1%) 45%,0,,False
318,.2046* (+3.1%) 46%,0,,False
319,.2046* (+3.1%) 46%,0,,False
320,4 terms (7 queries),0,,False
321,MAP,1,MAP,True
322,%DQ,0,,False
323,.1181,0,,False
324,­,0,,False
325,.1120,0,,False
326,100%,0,,False
327,.1143,0,,False
328,100%,0,,False
329,.1245* (+5.4%) 29%,0,,False
330,.1245* (+5.4%) 29%,0,,False
331,.1245* (+5.4%) 29%,0,,False
332,METHOD,0,,False
333,Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI,0,,False
334,2 terms (56 queries),0,,False
335,NDCG@10,0,,False
336,%DQ,0,,False
337,.4331,0,,False
338,­,0,,False
339,.4183,0,,False
340,100%,0,,False
341,.4174,0,,False
342,100%,0,,False
343,.4855* (+12.1%) 32%,0,,False
344,.4855* (+12.1%) 32%,0,,False
345,.4855* (+12.1%) 32%,0,,False
346,DISKS4-5,0,,False
347,3 terms (76 queries),0,,False
348,NDCG@10,0,,False
349,%DQ,0,,False
350,.4699,0,,False
351,­,0,,False
352,.3685,0,,False
353,100%,0,,False
354,.4421,0,,False
355,100%,0,,False
356,.4968* (+5.7%) 33%,0,,False
357,.4968* (+5.7%) 33%,0,,False
358,.4968* (+5.7%) 33%,0,,False
359,4 terms (7 queries),0,,False
360,NDCG@10,0,,False
361,%DQ,0,,False
362,.3549,0,,False
363,­,0,,False
364,.3394,0,,False
365,100%,0,,False
366,.3768,0,,False
367,100%,0,,False
368,.3902* (+3.6%) 29%,0,,False
369,.3902* (+3.6%) 29%,0,,False
370,.3902* (+3.6%) 29%,0,,False
371,METHOD,0,,False
372,Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI,0,,False
373,2 terms (56 queries),0,,False
374,P@10,0,,False
375,%DQ,0,,False
376,.4018,0,,False
377,­,0,,False
378,.3909,0,,False
379,100%,0,,False
380,.3873,0,,False
381,100%,0,,False
382,.4545* (+13.1%) 20%,0,,False
383,.4527 (+12.7%) 20%,0,,False
384,.4527 (+12.7%) 20%,0,,False
385,DISKS4-5,0,,False
386,3 terms (76 queries),0,,False
387,P@10,0,,False
388,%DQ,0,,False
389,.4286,0,,False
390,­,0,,False
391,.3429,0,,False
392,100%,0,,False
393,.4208,0,,False
394,100%,0,,False
395,.4649* (+8.5%) 30%,0,,False
396,.4649* (+8.5%) 30%,0,,False
397,.4649* (+8.5%) 30%,0,,False
398,4 terms (7 queries),0,,False
399,P@10,0,,False
400,%DQ,0,,False
401,.3000,0,,False
402,­,0,,False
403,.3000,0,,False
404,100%,0,,False
405,.3400*,0,,False
406,100%,0,,False
407,.3400* (±0.0%) 29%,0,,False
408,.3400* (±0.0%) 29%,0,,False
409,.3400* (±0.0%) 29%,0,,False
410,METHOD,0,,False
411,Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI,0,,False
412,2 terms (65 queries),0,,False
413,MAP,1,MAP,True
414,%DQ,0,,False
415,.1290,0,,False
416,­,0,,False
417,.1126,0,,False
418,100%,0,,False
419,.1234,0,,False
420,100%,0,,False
421,.1371 (+6.3%) 43%,0,,False
422,.1368 (+6.0%) 48%,0,,False
423,.1368 (+6.0%) 48%,0,,False
424,CWEB09B,1,CW,True
425,3 terms (63 queries),0,,False
426,MAP,1,MAP,True
427,%DQ,0,,False
428,.1391,0,,False
429,­,0,,False
430,.1235,0,,False
431,100%,0,,False
432,.1377,0,,False
433,100%,0,,False
434,.1480 (+6.4%) 43%,0,,False
435,.1519 (+9.2%) 46%,0,,False
436,.1520* (+9.3%) 46%,0,,False
437,4 terms (13 queries),0,,False
438,MAP,1,MAP,True
439,%DQ,0,,False
440,.1046,0,,False
441,­,0,,False
442,.0949,0,,False
443,100%,0,,False
444,.0982,0,,False
445,100%,0,,False
446,.1120 (+7.1%) 15%,0,,False
447,.1128* (+7.8%) 23%,0,,False
448,.1128* (+7.8%) 23%,0,,False
449,METHOD,0,,False
450,Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI,0,,False
451,2 terms (65 queries),0,,False
452,NDCG@10,0,,False
453,%DQ,0,,False
454,.4003,0,,False
455,­,0,,False
456,.3671,0,,False
457,100%,0,,False
458,.3412,0,,False
459,100%,0,,False
460,.4142 (+3.5%) 32%,0,,False
461,.4143 (+3.5%) 34%,0,,False
462,.4142 (+3.5%) 34%,0,,False
463,CWEB09B,1,CW,True
464,3 terms (63 queries),0,,False
465,NDCG@10,0,,False
466,%DQ,0,,False
467,.2907,0,,False
468,­,0,,False
469,.2902,0,,False
470,100%,0,,False
471,.2958,0,,False
472,100%,0,,False
473,.3267 (+10.4%) 33%,0,,False
474,.3291* (+11.3%) 35%,0,,False
475,.3291* (+11.3%) 35%,0,,False
476,4 terms (13 queries),0,,False
477,NDCG@10,0,,False
478,%DQ,0,,False
479,.3552,0,,False
480,­,0,,False
481,.2409,0,,False
482,100%,0,,False
483,.3213,0,,False
484,100%,0,,False
485,.3873* (+9.8%) 31%,0,,False
486,.3838 (+8.1%) 31%,0,,False
487,.3838 (+8.1%) 31%,0,,False
488,METHOD,0,,False
489,Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI,0,,False
490,2 terms (65 queries),0,,False
491,P@10,0,,False
492,%DQ,0,,False
493,.4894,0,,False
494,­,0,,False
495,.4318,0,,False
496,100%,0,,False
497,.4167,0,,False
498,100%,0,,False
499,.5152 (+5.3%) 20%,0,,False
500,.5167* (+5.6%) 25%,0,,False
501,.5167* (+5.6%) 25%,0,,False
502,CWEB09B,1,CW,True
503,3 terms (63 queries),0,,False
504,P@10,0,,False
505,%DQ,0,,False
506,.3600,0,,False
507,­,0,,False
508,.3550,0,,False
509,100%,0,,False
510,.3617,0,,False
511,100%,0,,False
512,.4067 (+12.4%) 21%,0,,False
513,.4100* (+13.4%) 19%,0,,False
514,.4100* (+13.4%) 19%,0,,False
515,4 terms (13 queries),0,,False
516,P@10,0,,False
517,%DQ,0,,False
518,.3538,0,,False
519,-,0,,False
520,.2692,0,,False
521,100%,0,,False
522,.3615,0,,False
523,100%,0,,False
524,.4154* (+14.9%) 38%,0,,False
525,.4154* (+14.9%) 38%,0,,False
526,.4154* (+14.9%) 38%,0,,False
527,601,0,,False
528,"cases. Our worst performing queries consist of phrases for which many more variants that denote the same meaning exist. E.g. tv show, television programme/broadcast, signs/symptoms/indications heart attack/failure, etc. Restricting this type of queries to strict `set phrase' matching limits the retrieval scope significantly with resulting drops in performance.",1,ad,True
529,Next we focus the analysis on two pertinent aspects of our approach: the number of strongly term dependent queries selected and retrieval performance for 2-4 term queries.,0,,False
530,6.3.1 Number of least compositional queries,0,,False
531,"The number of queries treated as a `set phrase' is lower for our approach than for MRF by 1/3 for Disks4-5 and 2/3 for ClueWeb09B, or by 1/4 for Disks4-5 and 1/3 for ClueWeb09B if we ignore 1-term queries (statistics in Table 2). Compositionality and term dependence in general cannot be measured for single terms, hence 1-term queries are ignored.",1,ClueWeb,True
532,"Since we treat the number  of least compositional queries as a tuneable parameter, one may wonder to what extent the gains we report are due to tuning as opposed to the inherent strength of our approach in detecting term dependence. To answer this, Fig. 2 shows the MAP and NDCG@10 of our MI approach across the range of  values for ClueWeb09B (we can confirm similar trends for P@10 and Disks4-5, and our other NCD approaches). We see that our approach outperforms the strongest baseline (marked by a horizontal line) consistently across the range of , peaking when roughly  ,""80 least compositional queries (out of 200, or 143 if one excludes 1-term queries) are treated as strongly term dependent. Practically this means that our approach can be used without necessarily tuning  and is likely not to give large fluctuations in both early and deep precision.""",1,MAP,True
533,6.3.2 Queries of 2-4 terms,0,,False
534,"Finally, we focus on queries of 2, 3 and 4 terms because these are the most likely to include strong term dependence, hence they are ideal for comparing our approaches to the MRF models.",0,,False
535,"Table 4 shows the retrieval precision of our baselines and our three best NCD approaches (marked by * in Table 3) specifically for queries of these lengths. Again all our approaches outperform all baselines at all times. The only exception is for 4-term queries in Disks4-5 and P@10, where our methods perform equally to the strongest baseline (no gain, no loss). Overall, our NCD approaches outperform the strongest baseline by up to >+5% for MAP, >+6% for NDCG@10, and >+8% for P@10, on average. The two middle and lower plots in Fig. 1 show that these improvements are not due to outliers, but are instead spread over the queries. Fig. 1 illustrates this for 2- and 3-term queries w.r.t. MAP and NDCG@10, but we confirm that the same trend applies to 4-term queries and P@10. Hence, for queries of length 2-4, i.e. predominantly phrasal queries, our approaches outperform all baselines notably. This finding, combined with the relative robustness of the threshold  discussed above, mean that our approach could be used as part of the IR pipeline, e.g. for 80% of the incoming queries of length 2-4. Note that these types of queries form the majority of all queries, at least in our TREC data (see Table 2), hence they are not a negligible sample.",1,MAP,True
536,MAP all 350 queries,1,MAP,True
537,NDCG@10 all 350 queries,0,,False
538,0.1 0.05,0,,False
539,0 -0.05,0,,False
540,MAP 121 2-term queries,1,MAP,True
541,0.15,0,,False
542,0.1,0,,False
543,0.05,0,,False
544,0 MAP 139 3-term queries,1,MAP,True
545,0.12 0.1 0.08 0.06 0.04 0.02,0,,False
546,0 -0.02,0,,False
547,0.6 0.4 0.2,0,,False
548,0 -0.2 -0.4,0,,False
549,NDCG@10 121 2-term queries 0.4 0.3 0.2 0.1,0,,False
550,0 -0.1,0,,False
551,NDCG@10 139 3-term queries,0,,False
552,0.3,0,,False
553,0.2,0,,False
554,0.1,0,,False
555,0,0,,False
556,-0.1,0,,False
557,"Figure 1: Sorted per-query difference (y-axis) in MAP/NDCG@10 between the strongest baseline (Full Dependence) and our Laplace sum method, for all, 2-term, & 3-term queries in DISKS4-5 & CWEB09B. The horizontal line marks the baseline (points above are gains). Each point is a query.",1,MAP,True
558,0.1188,0,,False
559,MAP,1,MAP,True
560,NDCG@10 0.36,0,,False
561,0.1151,0,,False
562,0.3514,0,,False
563,0 20 40 60 80 100 120,0,,False
564,0 20 40 60 80 100 120,0,,False
565,Figure 2: MAP & NDCG@10 (y-axis) vs.  most non-compositional queries in CWEB09B according to MI (x-axis). The horizontal line marks the baseline. Each point is a query.,1,MAP,True
566,Table 5: Queries with most gain/loss from NCD.,0,,False
567,Best,0,,False
568,Worst,0,,False
569,bart sf,0,,False
570,tv show,0,,False
571,ct jobs,0,,False
572,industrial espionage,0,,False
573,french lick resort casino export controls cryptography,0,,False
574,civil right movement,0,,False
575,signs heartattack,0,,False
576,602,0,,False
577,7. DISCUSSION,0,,False
578,"The relative gains in retrieval precision reported above should not be considered as indications of accurate noncompositionality detection. The suitability of our proposed probabilistic representation of queries and their perturbations in particular remains to be evaluated for NCD accuracy. Moreover, several of our choices of NCD settings can be further explored, e.g. synonymy selection or smoothing choices. In this initial study we opted for default or popular settings, where possible. For these reasons, we have refrained from making a quantitative comparison between the vector space and probability space NCD variations, other than reporting the retrieval precision they yield. This means that the NCD variations we present are not necessarily calibrated to this domain or task. Calibrating them could potentially improve performance even more, but would incur some computational cost, the major bulk of which would likely lie in the extraction of context windows from some large dataset. In an IR scenario, this can be done offline, and is perhaps not too distant from the query analytics widely used.",0,,False
579,"Regarding our data, the query sets we use are `curated' by TREC, in the sense that those queries that are perhaps not understood by human assessors, or for which no relevant documents are easily found during pooling, may have been omitted. This selection may have affected non- or lowcompositionality queries. This agrees with the finding that the number of IR benchmark queries that contain strongly dependent terms in general is small [65]. Unfiltered query logs may contain more such queries, making our approach potentially even more useful in such a practical setting.",1,TREC,True
580,8. CONCLUSIONS,0,,False
581,"We presented an approach for detecting strongly dependent query terms using the linguistic property of non-compositionality. Non-compositional meaning cannot be induced from the meanings of individual words or their arrangement in a query. E.g., hot dog is not a type of dog that is hot, but rather a type of food. We used unsupervised measurement of non-compositionality to approximate the detection of strongly dependent query terms. Such queries are challenging to IR because they cannot be processed to some reasonable accuracy by bag of words approaches. Motivated by this, we focussed not on how these queries can be treated during ranking (there is a lot of literature in this area generally for term dependence, which can be applied here), but on how these queries can be selected from a batch or stream of incoming queries. This specific question has so far been addressed by assuming that the more frequently terms co-occur in a query, the more dependent they are. This assumption is however not always true, because frequency is not always proportional to the strength of semantic association. The unsupervised method for measuring non-compositionality that we used is recent and uses vector spaces [21]. We extended it by adding a probabilistic representation that uses Kullback-Leibler divergence. We experimentally showed that all variants of our approach were effective in selecting which queries to treat as term dependent and resulted in gains for both early and deep precision (> 5%) with respect to a range of baselines (standard bag of words and competitive MRF with sequential and full dependence [31]).",1,ad,True
582,"In the future we plan to analyse the amount of non- or low-compositionality queries in real-life query logs, as opposed to TREC data. As discussed in Section 7, there may be more low-compositionality queries in those samples. We also intend to investigate optimal ways of measuring noncompositionality within a query, as opposed to considering the non-compositionality of a query as a whole as we did here. Another interesting direction is the direct mapping of the non-compositionality score of a query into the strength of its term dependence used during ranking. In this initial study we treated all queries selected as least-compositional in the same way as fixed phrases processing them identically; in doing so, we ignored their grades of non-compositionality. Modelling this may yield further improvements and is an interesting research question in its own right.",1,TREC,True
583,Acknowledgments.,0,,False
584,Partially funded by the first author's FREJA research excellence fellowship (grant no. 790095).,0,,False
585,9. REFERENCES,0,,False
586,"[1] T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows. An empirical model of multiword expression decomposability. In ACL Multiword Expressions Wksh., pages 89­96. 2003.",0,,False
587,"[2] C. Bannard, T. Baldwin, and A. Lascarides. A statistical approach to the semantics of verb-particles. In ACL Multiword Expressions Wksh., pages 65­72, 2003.",0,,False
588,"[3] P. B. Baxendale. Machine-made index for technical literature. IBM Journal for R&D, 2:354­361, 1958.",1,ad,True
589,"[4] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In WSDM, pages 31­40, 2010.",0,,False
590,"[5] M. Dillon and A. Gray. FASIT - a fully automatic syntactically based indexing system. JASIS, 34:99­108, 1983.",0,,False
591,"[6] L. B. Doyle. Indexing and abstracting by association. Part I. Am. Doc., 13:378­390, 1962.",0,,False
592,"[7] L. L. Earl. The resolution of syntactic ambiguity in automatic language processing. Information Storage and Retrieval, 8:277­308, 1972.",0,,False
593,"[8] K. Erk. Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6(10):635­653, 2012.",0,,False
594,"[9] D. A. Evans and C. Zhai. Noun-phrase analysis in unrestricted text for IR. In ACL, pages 17­24, 1996.",0,,False
595,"[10] J. L. Fagan. The effectiveness of a non syntactic approach to automatic phrase inducing for document retrieval. JASIS, 40:115­132, 1989.",0,,False
596,"[11] J. R. Firth. A synopsis of linguistic theory. Selected papers of J.R. Firth 1952-1959, pages 168­205, 1968.",0,,False
597,"[12] S. Fujita. More reflections on aboutness TREC-2001 evaluation experiments at Justsystem. In TREC, pages 331 ­ 338, 2001.",1,TREC,True
598,"[13] W. Gale and G. Sampson. Good-Turing frequency estimation without tears. J. of Quant. Ling., 2(3):217­237, 1995.",0,,False
599,"[14] V. E. Giuliano and P. E. Jones. Linear associative IR. Vistas in Information Handling: The Augmentation of Man's Intellect by Machine, 1:30­54, 1963.",0,,False
600,"[15] D. J. Harper and C. J. K. van Rijsbergen. An evaluation of feedback in document retrieval using concurrence data. J. of Doc., 34:189­216, 1978.",0,,False
601,"[16] F. Heylighen and J. Dewaele. Variation in the contextuality of language. F. of Sci., 7(3):293­340, 2002.",0,,False
602,"[17] H. Jeffreys. Theory of Probability. Clarendon, 1948. [18] R. Jin, C. Falusos, and A. G. Hauptmann. Meta-scoring:",0,,False
603,"Automatically evaluating term weighting schemes in IR without precision-recall. In SIGIR, pages 83­89, 2001.",0,,False
604,603,0,,False
605,"[19] D. Jurafsky and J. Martin. Speech and Language Processing. Pearson, 2009.",0,,False
606,"[20] G. Katz and E. Giesbrecht. Automatic identification of non-compositional multi-word expressions using LSA. In ACL Multiword Expressions Wksh., pages 12­19. 2006.",0,,False
607,"[21] D. Kiela and S. Clark. Detecting compositionality of multi-word expressions using nearest neighbours in vector space models. In EMNLP, pages 1427­1432, 2013.",0,,False
608,"[22] I. Korkontzelos and S. Manandhar. Detecting compositionality in multi-word expressions. In ACL-IJCNLP, pages 65­68, 2009.",0,,False
609,"[23] L. Krcma´r, K. Jezek, and P. Pecina. Determining compositionality of expresssions using various word space models and methods. In Continuous Vector Space Models and their Compositionality Wksh., pages 64­73, 2013.",0,,False
610,"[24] M. E. Lesk. Word-word associations in document retrieval systems. Am. Doc., 20:27­38, 1969.",0,,False
611,"[25] D. D. Lewis. An evaluation of phrasal and clustered representations on a text categorization task. In SIGIR, pages 37­50, 1992.",0,,False
612,"[26] D. D. Lewis and W. B. Croft. Term clustering of syntactic phrases. In SIGIR, pages 385­404, 1990.",0,,False
613,"[27] J. Lin. Indexing & Retrieving Natural Language Using Ternary Expressions. Master's thesis, U. of Maryland, USA, 2001.",0,,False
614,"[28] R. Losee. Term dependence: Truncating the Bahadur Lazarsfeld expansion. IPM, 30(2):293­303, 1994.",1,ad,True
615,"[29] Y. Lv and C. Zhai. Positional language models for information retrieval. In SIGIR, pages 299­306, 2009.",0,,False
616,"[30] D. McCarthy, B. Keller, and J. Caroll. Detecting a continuum of compositionality in phrasal verbs. In ACL Multiword Expressions Wksh., pages 73­80. 2003.",0,,False
617,"[31] D. Metzler and B. Croft. A MRF model for term dependencies. In SIGIR, pages 472­479, 2005.",0,,False
618,"[32] D. P. Metzler, T. Noreault, L. Richey, and P. B. Heidorn. Dependency parsing for information retrieval. In SIGIR, pages 313­324, 1984.",0,,False
619,"[33] L. Michelbacher, A. Kothari, M. Forst, C. Lioma, and H. Schu¨tze. A cascaded classification approach to semantic head recognition. In EMNLP, pages 793­803, 2011.",1,ad,True
620,"[34] G. Mishne and M. de Rijke. Boosting web retrieval through query operations. In ECIR, pages 502­516, 2005.",0,,False
621,"[35] J. Mitchell and M. Lapata. Composition in distributional models of semantics. Cognitive Science, 34(8):1388­1429, 2010.",0,,False
622,"[36] R. Nallapati and J. Allan. Capturing term dependencies using a language model based on sentence trees. In CIKM, pages 383­390, 2002.",0,,False
623,"[37] M. Narita and Y. Ogawa. The use of phrases from query texts in IR. In SIGIR, pages 318­320, 2000.",0,,False
624,"[38] P. Pantel and D. Lin. Document clustering with committees. In SIGIR, pages 199­206. ACM, 2002.",0,,False
625,"[39] J. Pederson, C. Silverstein, and C. Vogt. Verity at TREC-6: out-of-the-box and beyond. In TREC-6, pages 259 ­ 274, 1997.",1,TREC,True
626,"[40] V. Plachouras and I. Ounis. Multinomial randomness models for retrieval with document fields. In ECIR, pages 28­39, 2007.",0,,False
627,"[41] S. Reddy, I. Klapaftis, D. McCarthy, and S. Manandhar. Dynamic & static prototype vectors for semantic composition. In IJCNLP, pages 705­713, 2011.",0,,False
628,"[42] S. Reddy, D. McCarthy, S. Manandhar, and S. Gella. Exemplar-based word-space model for compositionality detection: shared task system description. In DiSCo, pages 54­60. 2003.",0,,False
629,"[43] J. W. Reed, Y. Jiao, T. E. Potok, B. A. Klump, M. T. Elmore, and A. R. Hurson. TF-ICF: A new term weighting scheme for clustering dynamic data streams. In ICMLA, pages 258­263, 2006.",0,,False
630,"[44] B. Salehi and P. Cook. Predicting the compositionality of multiword expressions using translations in multiple languages. In SEM, pages 266­275. 2013.",0,,False
631,"[45] G. Salton. Automatic phrase matching. Readings in Automatic Language Processing, pages 169­188, 1966.",1,ad,True
632,"[46] G. Salton, C. Buckley, and C. T. Yu. An evaluation of term dependence models in information retrieval. In SIGIR, pages 151­173, 1982.",0,,False
633,"[47] S. Schulte im Walde, S. Mu¨ller, and S. Roller. Exploring vector space models to predict the compositionality of German noun-noun compounds. In SEM, pages 255­265. 2013.",0,,False
634,"[48] A. Singhal. AT&T at TREC-6. In TREC-6, pages 215­225, 1997.",1,TREC,True
635,"[49] A. Smeaton and K. van Rijsbergen. Experiment on incorporation syntactic processing of user queries into a document retrieval strategy. In SIGIR, pages 31­51, 1988.",1,corpora,True
636,"[50] A. F. Smeaton. Incorporating syntactic information into a document retrieval strategy: An investigation. In SIGIR, pages 103­113, 1986.",1,corpora,True
637,"[51] F. J. Smith and K. Devine. Storing and retrieving word phrases. IPM, 21(3):215­224, 1985.",0,,False
638,"[52] F. Song and W. B. Croft. A general language model for IR. In CIKM, pages 316­321, 1999.",0,,False
639,"[53] K. Sp¨arck-Jones. A statistical interpretation of term specificity and its application in retrieval. J. of Doc., 28(1):132­142, 1972.",0,,False
640,"[54] K. Sp¨arck-Jones and J. Tait. Charting a New Course: MLP and IR: Essays in Honour of Karen Sp¨arck Jones. Springer, 2005.",0,,False
641,"[55] M. Srikanth and R. K. Srihari. Incorporating query term dependencies in language models for document retrieval. In SIGIR, pages 405­406. ACM, 2003.",1,corpora,True
642,"[56] H. E. Stiles. The association factor in information retrieval. Journal of the ACM, 8:271­279, 1961.",0,,False
643,"[57] T. Strzalkowski and F. Lin. Natural language IR TREC-6 report. In TREC, pages 347 ­ 366, 1997.",1,TREC,True
644,"[58] T. Tao and C. Zhai. An exploration of proximity measures in ir. SIGIR, pages 295­302. ACM, 2007.",0,,False
645,"[59] R. H. Thomason. Formal Philosophy. Selected Papers of Richard Montague. Yale University Press, 1974.",0,,False
646,"[60] H. Turtle and B. Croft. Evaluation of an inference network-based retrieval model. TOIS, 9(3):187­222, 1991.",0,,False
647,"[61] C. J. K. van Rijsbergen. A theoretical basis for the use of co-occurrence data in information retrieval. J. Doc., 33:106­119, 1977.",0,,False
648,"[62] V. Vinay, I. J. Cox, N. Milic-Frayling, and K. R. Wood. On ranking the effectiveness of searches. In SIGIR, pages 398­404, 2006.",0,,False
649,"[63] C. T. Yu, C. Buckley, K. Lam, and G. Salton. A generalised term dependence model in IR. Information Technology: R&D, 2:129­154, 1983.",0,,False
650,"[64] C. Zhai, X. Tong, N. Milic-Frayling, and D. A. Evans. Evaluation of syntactic phrase indexing - CLARIT NLP track report. In TREC-5, pages 347­358, 1997.",1,TREC,True
651,"[65] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Comput. Surv., 38(2):6, 2006.",0,,False
652,604,0,,False
653,,0,,False

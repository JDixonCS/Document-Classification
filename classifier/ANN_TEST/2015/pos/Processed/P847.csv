,sentence,label,data,regex
0,Features of Disagreement Between Retrieval Effectiveness Measures,0,,False
1,Timothy Jones,0,,False
2,Paul Thomas,0,,False
3,RMIT University,0,,False
4,CSIRO,1,CSIRO,True
5,timothy.jones@rmit.edu.au paul.thomas@csiro.au,0,,False
6,Falk Scholer,0,,False
7,Mark Sanderson,0,,False
8,RMIT University,0,,False
9,RMIT University,0,,False
10,falk.scholer@rmit.edu.au mark.sanderson@rmit.edu.au,0,,False
11,ABSTRACT,0,,False
12,"Many IR effectiveness measures are motivated from intuition, theory, or user studies. In general, most effectiveness measures are well correlated with each other. But, what about where they don't correlate? Which rankings cause measures to disagree? Are these rankings predictable for particular pairs of measures? In this work, we examine how and where metrics disagree, and identify differences that should be considered when selecting metrics for use in evaluating retrieval systems.",0,,False
13,Categories and Subject Descriptors,0,,False
14,H.3.4 [Information Storage and Retrieval]: Systems and software--performance evaluation,0,,False
15,Keywords,0,,False
16,IR evaluation; effectiveness measures; binary relevance,0,,False
17,1. INTRODUCTION,1,DUC,True
18,"Since information retrieval systems were first evaluated, system effectiveness measures have been a topic of discussion, study and controversy. When a new evaluation metric is introduced­or existing measures are criticised or praised­the discussion is usually motivated either by theoretical concerns with earlier metrics [5, 8, 9], or by user studies [1]. Sometimes, both approaches are combined [4].",0,,False
19,Most IR effectiveness measures assume that the quality of results returned by a search engine can be calculated as a function of the gain vector inferred by a ranked list­ sometimes also including additional knowledge such as the total number of relevant documents available [2]. In this paper we use:,1,ad,True
20,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '15, August 09­13, 2015, Santiago, Chile Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 . . . $15.00. http://dx.doi.org/10.1145/2766462.2767824.",1,ad,True
21,· k as the depth to which a particular effectiveness metric has been evaluated.,0,,False
22,· R as the total number of relevant documents available.,0,,False
23,"In the case of binary relevance (where documents are assumed to either be relevant, or not), the gain vector can be represented as a bit string. In simple cases, it is easy to examine these bitstrings by eye and reason about which list should be preferred. For example, where k ,"" 3, {1, 0, 0} is almost always better than {0, 0, 1}.""",0,,False
24,"However, some cases become more contentious. Consider two vectors with k , 10 and R , 10:",0,,False
25,"A , {1100010000} B , {1000101100}",0,,False
26,"If asked which list is better, most researchers would say ""it depends on the task"". However, it's not always immediately clear whether particular effectiveness metrics would agree on which list is better. In this particular example, AP prefers ranking A over B, while DCG prefers ranking B over A. Put another way, it's not always clear which metric prefers which type of task, or whether particular metrics are consistent in their preferences. In this work, we perform an exhaustive search of the possible binary relevance vectors where k ,"" 10, and investigate where the disagreement between metrics lies.""",1,AP,True
27,2. RELATED WORK,0,,False
28,"It's worth noting that many metric descriptions don't completely specify the details of implementation. For example, an implementation of DCG requires a selection of both a gain and a discount function. Similarly, an implementation of RBP requires a selection of the p parameter [9]. Kanoulas and Aslam examine several different possible choices for the gain and discount functions in NDCG [6]. They used a Generalisability Theory approach to find choices that produced a stable ranking of systems. In this environment, they show that the optimal discount function is less steep than previously thought, and also that the optimal gain function gives nearly equal weight to relevant and highly relevant documents.",0,,False
29,"Although producing a single score for a retrieved list is convenient, a single score is not a particularly effective way of capturing system performance [7]. One illustration of this is",0,,False
30,847,0,,False
31,Metric,0,,False
32,DCG SP SN-DCG SN-AP Precision NDCG SDCG RR/ERR Recall AP RBP,1,AP,True
33,Bounced,0,,False
34,No No Yes Yes Yes Yes Yes Yes Yes Yes Yes,0,,False
35,Monoton.,0,,False
36,Yes Yes No No No No No Yes Yes Yes Yes,0,,False
37,Converg.,0,,False
38,Yes Yes No No Yes Yes Yes No Yes Yes Yes,0,,False
39,Top-wgt,0,,False
40,Yes Yes Yes Yes No Yes Yes No No Yes Yes,0,,False
41,Localis.,0,,False
42,Yes Yes Yes Yes Yes No Yes Yes No No Yes,0,,False
43,Complete,0,,False
44,Yes Yes No No Yes No Yes Yes No No Yes,0,,False
45,Realis.,0,,False
46,No No Yes Yes No Yes No Yes No No No,0,,False
47,Table 1: Properties of effectiveness metrics used (table reproduced and slightly modified from Moffat [8]. See original paper for footnotes and full discussion). All metrics are assumed to be an @k version.,0,,False
48,"the idea of 1-equivalence [10], which is the set of binary result vectors that receive the same score as a single document. The authors note that many 1-equivalent sets are contentious or at least counter-intuitive.",0,,False
49,"An interesting approach to measuring the quality of an effectiveness measure is to use the Maximum Entropy Method to infer the probabilities that each rank contains a relevant document, given the effectiveness score [2]. Once this probability vector is computed from the effectiveness score, a predicted precision­recall curve can be produced. The error between this and the actual precision­recall curve can be used to determine the quality of the metric. This approach asks the question ""how good is the effectiveness score at inferring the original ranked list?"".",0,,False
50,"Motivated by the wide variety of effectiveness metrics available, Moffat [8] introduces seven properties for describing and comparing metrics. They are:",0,,False
51,"· Boundedness: Whether the range of scores a metric can produce is bounded or not. For example, NDCG is bounded in the range 0­1, while DCG is unbounded in the upper range.",0,,False
52,"· Monotonicity: If extra documents are appended to the tail end of a ranking, the new score produced by a monotonic ranking is never less than the previous score. For example, Recall-at-k and DCG are monotonic, while AP is not.",1,AP,True
53,"· Convergence: If a document within the top k is swapped with a more relevant document outside the top k, then the score always increases if the metric is convergent. For example, DCG is convergent, while RR is not.",0,,False
54,"· Top-weighted: If a document within the top k is swapped with a more relevant document also within the top k but lower down the ranking, then the score always increases if the metric is top-weighted. For example, AP is top-weighted, while Precision-at-k is not.",1,AP,True
55,"· Localisation: If a score at depth k can be produced using only the information about the documents down to depth k, then a metric is localised. For example, RR is localised, while NDCG and AP both require additional information about the relevant documents that did not make it in to the ranking.",1,AP,True
56,"· Completeness: If a score can be produced when a query returns no relevant documents, then the metric is complete. For example, DCG is complete, while NDCG and AP both produce a division by zero when there are no relevant documents.",1,AP,True
57,"· Realisability: If a collection has any relevant documents, then a metric is realisable if it is possible to achieve the maximum value for that metric. For example, RR is realisable, but RBP is not.",0,,False
58,"Moffat notes that it is impossible for a metric to satisfy all seven properties, as some property combinations exclude others (for example, a metric that is monotonic and convergent cannot also be realisable if k < R).",0,,False
59,3. METRIC DISAGREEMENT,0,,False
60,"Many previous comparisons between effectiveness metrics have looked at correlation between metrics. In this work, we are interested in the specific cases where metrics do not agree. That is, the cases where we have two binary relevance vectors A and B where­for example­AP says that A is the most effective result list, while DCG says that B is the most effective result list. In this work, we only focus on rankings where k ,"" 10, since many of the metrics above include a user model, and it is common for users to only examine the first page of results in a web setting.""",1,AP,True
61,3.1 Method,0,,False
62,"We generate all possible combinations for binary relevance rankings where k , 10 and R ,"" 10. This yields a total of 1,023 possible ranked lists, containing all possible rankings of 10 documents with up to 10 relevant documents (the all 0's case is ignored). There are 1,045,506 pairs of rankings, ignoring the pairs where both rankings are identical. Each ranked list is evaluated for each metric, and for each pair of ranked lists and pair of metrics, agreement or disagreement is recorded. All metrics were computed using 64-bit floating point numbers, and equality was checked using an epsilon of 2-53.""",0,,False
63,3.2 Metrics,0,,False
64,"We use the same metrics as Moffat [8], with the exception of HIT, which would receive the same score for every ranked list described above.",0,,False
65,848,0,,False
66,RBP 0.95 DCG NDCG SDCG RBP 0.85 AP SP MRR ERR SNAP RBP 0.50 SNDCG,1,AP,True
67,P@k/R@k RBP 0.95 DCG/NDCG/SDCG RBP 0.85 AP/SP MRR/ERR SNAP RBP 0.50,1,AP,True
68,17.57,0,,False
69,23.99 8.65,0,,False
70,23.75 6.87 5.32,0,,False
71,23.06 7.02 3.01 2.88,0,,False
72,54.09 50.79 43.02 45.73 45.04,0,,False
73,36.45 20.41 13.25 13.59 13.43 38.31,0,,False
74,39.99 24.95 16.30 18.29 17.99 33.27,0,,False
75,8.05,0,,False
76,38.34 23.00 14.35 16.31 16.05 35.24,0,,False
77,4.25 4.41,0,,False
78,"Table 2: Disagreement between metrics (%, lower is better).",0,,False
79,Property,0,,False
80,Bounded Monoton. Converg. Top-wgt Localis. Complete Realis.,0,,False
81,Has-prop,0,,False
82,25.57 25.63 22.23 21.45 21.97 22.40 22.81,0,,False
83,No-prop,0,,False
84,3.00 19.25 24.70 22.15 21.13 20.95 18.33,0,,False
85,Cross-prop,0,,False
86,16.98 20.60 24.14 25.82 25.26 24.65 24.98,0,,False
87,"Table 3: Disagreement between metrics with particular properties (%, lower is better). Bolded groups show significance.",0,,False
88,"The metrics used are DCG, SP, SN-DCG [8], SN-AP [8], Prec@k, NDCG [6], SDCG [8], RR, Recall@k, AP [3] and RBP [9]. For RBP, 3 different values of p were used: 0.5, 0.85 and 0.95. As recommended by the authors [9], the lower bound of the score was taken to be the RBP score. ERR was also implemented, but it scores identically to RR in binary relevance [4]. Similarly, where metrics are identical except for normalisation (eg DCG and NDCG), metrics completely agree. See Table 1 for the breakdown of properties of each measure. Where choices for implementation were available, we implement the metric as described by Moffat [8].",1,AP,True
89,4. RESULTS,0,,False
90,"The disagreement between each pair of measures can be seen in Table 2. In some cases (such as AP vs RR), the disagreement is not surprising, but in other cases where one would expect metrics to agree, they do not­such as the high disagreement with SNDCG and most other metrics. Also of note is the high disagreement between RBP variants­ although this is not surprising, it is worth mentioning that simply changing the discount parameter dramatically affects the behaviour of the metric.",1,AP,True
91,4.1 Realistic disagreement,0,,False
92,"The disagreement percentages shown in Table 2 are assuming that each possible ranking with k ,"" 10 can be returned by some system. However, retrieval systems aren't random; the results returned depend on collection statistics and other""",0,,False
93,Metric combination,0,,False
94,AP DCG,1,AP,True
95,DCG RBP85,0,,False
96,RBP85 RBP95,0,,False
97,RBP50 RBP95,0,,False
98,Recall,0,,False
99,52.41 22.09,0,,False
100,33.24 37.71,0,,False
101,0.00 89.96,0,,False
102,0.00 89.89,0,,False
103,First-rel,0,,False
104,4.07 71.63,0,,False
105,62.38 11.27,0,,False
106,75.00 1.38,0,,False
107,70.24 0.00,0,,False
108,First-irrel,0,,False
109,65.54 14.34,0,,False
110,11.27 62.38,0,,False
111,1.38 75.00,0,,False
112,0.00 70.24,0,,False
113,"Table 4: Features of rankings where metric pairs disagree, in percentages. Since features are sometimes tied, they do not add to 100.",1,ad,True
114,"features which are not independent of relevance. To investigate whether these vectors are reasonable, we produced the k ,"" 10 bitstrings for all runs submitted to the TREC4-8 adhoc tracks, and the 2005 and 2006 TREC robust tracks. All of the 1024 possible vectors were returned by real runs submitted to the track. This validates this exhaustive method of investigation.""",1,TREC,True
115,4.2 Properties of metrics,0,,False
116,"To investigate whether the properties of effectiveness measures affect the disagreement between metrics, each disagreement score was sorted in to one of three buckets for each property: has-prop, where both metrics have the property; no-prop, where neither metric has the property, and crossprop, where one metric has the property, and one does not. Table 3 shows the mean agreement between metrics in each bucket. Surprisingly, a single property is not enough to determine whether two metrics agree. Of note is the case of metrics which are not bounded, but as Table 1 shows, there are only two measures in the no-prop category for that property. With the exception of monotonicity, boundedness, and realisability, the has-prop category contains the lowest mean disagreement. However, a one way ANOVA test with a Tukey post-hoc analysis shows only the convergent/has-prop group and the realisable/no-prop group to be statistically significant predictors of agreement.",1,hoc,True
117,849,0,,False
118,4.3 Task behaviour,0,,False
119,"To investigate whether metrics disagreed in a predictable way that might be correlated with a task goal, all disagreeing pairs of ranked lists from each pair of metrics were examined to find out which metric preferred: recall, the list with the most relevant documents (ignoring ties); first-rel, the list with the first relevant document (ignoring ties); and firstirrel, the list with the first irrelevant document (ignoring ties). Some selected results are in Table 4. Each pairing is divided by the space in the table. When AP and DCG disagree about which ranked list is better, this table shows that in 52.41% of disagreeing cases, AP will prefer the list with more relevant documents, while DCG prefers the higher recall list only 22.09% of the time. These numbers do not sum to 100%, as sometimes there is a tie in the recall between two ranked lists, and sometimes a disagreement means that one of the two metrics gives the same score to two ranked lists (as is the case with AP when comparing {1, 0, 0, 0} and {0, 1, 0, 1} when R , 10).",1,AP,True
120,"This approach provides an alternative way to think about metric selection. When choosing say between AP and DCG (two metrics with strong agreement), this approach allows us to drill down into the differences between the metrics. If we prefer finding a relevant document quickest, then Table 4 suggests that DCG is the metric to choose, with 71.63% of cases preferring the ranking with the highest ranked initial relevant document. Alternatively, if recall is preferred, then AP is the better metric. Note that although Table 2 shows only a 3.01% disagreement between AP and DCG, this is still over 15,000 pairs of ranked lists.",1,AP,True
121,"The difference between the user model of RBP and DCG can also be seen in this data­DCG prefers the list with the earliest relevant document more often than RBP. And, as the weighting factor in RBP decreases, the metric prefers earlier relevant documents. Conversely, as the weight in RBP increases, the list with higher recall is preferred.",0,,False
122,5. CONCLUSION,0,,False
123,"In this work we have introduced a novel strategy for investigating the disagreement between effectiveness metrics­by counting and examining the pairs of hypothetical rankings where the metrics disagree with each other. We validated our strategy by demonstrating that all possible rankings of 10 binary relevant documents have appeared in search results submitted to two of the TREC tracks, and we performed an initial investigation into whether the properties of effectiveness measures can be used to predict agreement. We found that two of the properties appeared to be weak predictors of agreement between metrics. Then, we used a feature-based approach to investigate whether the disagreement between a pair of metrics could be described in terms of task features. This approach allows statements like ""if I select AP over DCG, I am preferring recall over highly ranked documents"" to be made, allowing fine-grained selection between metrics.",1,TREC,True
124,6. FUTURE WORK,0,,False
125,"As mentioned above, search engines do not produce a random result set. Although all possible rankings for k ,"" 10 did appear in real search results during the TREC ad-hoc and robust tracks, the frequency with which each ranking appears is not uniform. It would be valuable to examine the likelihood of disagreement that each result list has. It is""",1,TREC,True
126,possible that the gain vectors produced by systems could be used to determine how contentious they are­how sensitive evaluation would be to metric selection.,0,,False
127,"An obvious extension to Section 4.2 is to consider multiple groupings of properties. Although it seems that individual properties of effectiveness measures are not enough to predict agreement, perhaps some combination of the properties might be. In future work, we intend to include more variants of metrics, such as alternative discounts and gain functions. This may lead to discovery of further properties.",1,ad,True
128,"In Section 4.3, we only consider three features of ranked lists that map to task goals, but there are many more we could consider (such as longest consecutive run of relevant documents, or lowest ranked relevant document). Additionally, user preference experiments could be constructed using pairs of vectors where metrics disagree.",0,,False
129,"As noted in Section 4, many of the DCG variants completely agree on individual ranked lists, as all that changes is the normalisation. However, if multiple queries (and therefore multiple ranked lists) are used for evaluation­as in the case of TREC tracks­then different normalisation strategies may well cause further disagreement between metrics.",1,TREC,True
130,"Finally, this work has only considered the binary relevance case. Many of these metrics behave differently when there are multiple grades of relevance. An important next step is to repeat this analysis using graded relevance.",1,ad,True
131,Acknowledgements,0,,False
132,"In addition to the anonymous reviewers, we would like to thank Alistair Moffat for helpful comments and a correction.",1,ad,True
133,7. REFERENCES,0,,False
134,"[1] A. Al-Maskari, M. Sanderson, and P. Clough. The relationship between IR effectiveness measures and user satisfaction. In Proc SIGIR, pages 773­774, 2007.",0,,False
135,"[2] J. A. Aslam, E. Yilmaz, and V. Pavlu. The maximum entropy method for analyzing retrieval measures. In Proc SIGIR, pages 27­34, 2005.",0,,False
136,"[3] C. Buckley and E. M. Voorhees. Evaluating evaluation measure stability. In Proc SIGIR, pages 33­40, 2000.",0,,False
137,"[4] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proc CIKM, pages 621­630, 2009.",1,ad,True
138,"[5] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422­446, Oct. 2002.",0,,False
139,"[6] E. Kanoulas and J. A. Aslam. Empirical justification of the gain and discount function for nDCG. In Proc. CIKM, pages 611­620, 2009.",0,,False
140,"[7] S. Mizzaro. The good, the bad, the difficult, and the easy: Something wrong with information retrieval evaluation? In Proc. ECIR, volume 4956, pages 642­646, 2008.",1,ad,True
141,"[8] A. Moffat. Seven numeric properties of effectiveness metrics. In AIRS, volume 8281, pages 1­12, 2013.",0,,False
142,"[9] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):2:1­2:27, Dec. 2008.",0,,False
143,"[10] P. Thomas, D. Hawking, and T. Jones. What deliberately degrading search quality tells us about discount functions. In Proc SIGIR, pages 1107­1108, 2011.",1,ad,True
144,850,0,,False
145,,0,,False

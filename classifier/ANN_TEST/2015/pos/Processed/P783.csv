,sentence,label,data,regex
0,Modelling Term Dependence with Copulas,0,,False
1,Carsten Eickhoff,0,,False
2,"Dept. of Computer Science ETH Zurich, Switzerland",0,,False
3,ecarsten@inf.ethz.ch,0,,False
4,Arjen P. de Vries,0,,False
5,"CWI Amsterdam Amsterdam, The Netherlands",1,CW,True
6,arjen@acm.org,0,,False
7,Thomas Hofmann,0,,False
8,"Dept. of Computer Science ETH Zurich, Switzerland",0,,False
9,thomas.hofmann@inf.ethz.ch,0,,False
10,ABSTRACT,0,,False
11,"Many generative language and relevance models assume conditional independence between the likelihood of observing individual terms. This assumption is obviously na¨ive, but also hard to replace or relax. There are only very few term pairs that actually show significant conditional dependencies while the vast majority of co-located terms has no implications on the document's topical nature or relevance towards a given topic. It is exactly this situation that we capture in a formal framework: A limited number of meaningful dependencies in a system of largely independent observations. Making use of the formal copula framework, we describe the strength of causal dependency in terms of a number of established term co-occurrence metrics. Our experiments based on the well known ClueWeb'12 corpus and TREC 2013 topics indicate significant performance gains in terms of retrieval performance when we formally account for the dependency structure underlying pieces of natural language text.",1,ClueWeb,True
12,Categories and Subject Descriptors,0,,False
13,Information Systems [Information Retrieval]: Retrieval models,0,,False
14,Keywords,0,,False
15,Relevance models; Multivariate relevance; Ranking; Probabilistic framework; Language models.,0,,False
16,1. INTRODUCTION & RELATED WORK,1,DUC,True
17,"Generative n-gram language models are frequently used tools for representing document or collection vocabulary in the form of probability distributions over (spans of) textual tokens. They are popular for a wide array of tasks, including sentiment analysis, machine translation, content based classification and document retrieval. Most state-of-the-art models assume individual terms to be independently drawn from the underlying distribution. While this independence",0,,False
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",1,ad,True
19,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,0,,False
20,DhtOtpI:://hdtxtp.d:/o/id.xo.rdgo/1i.0o.r1g1/1405./12174656/426726.62X76X7X83.X1 XXXXXX.,0,,False
21,"assumption greatly simplifies the computation of conditional probabilities, it is also rather na¨ive. It is easy to find examples of proper names such as ""Barack Obama"" or ""Hong Kong"", but also other fixed expressions (""tax evasion"") as well as non-consecutive constructs (""dollar "" , ""stock ""), that should benefit from an explicit model of term interdependency. It is easy to see that some of these examples go beyond the capabilities of mere higher-order language models. In the past, there have been a number of attempts to formally integrate term dependency structures into generative models.",0,,False
22,"Van Rijsbergen's early work on dependency trees [19] theoretically establishes the use of maximum spanning trees and term co-occurrence statistics in order to establish local term dependency structures. Yu et al. [20] present a comparison of tree and cluster-based methods for dependency modelling in information retrieval. Srikanth and Srihari [18] investigate dependency-aware relevance models by using higher order n-gram models and comparing to the unigram setting. They report consistent improvements for the dependency models. Croft et al. [7] propose the use of inference networks trained on the basis of term proximity information to model the relevance of entire phrases. In a related effort, Lossee [12] confirms the important role of proximity in dependency modelling. The author reports optimal performance using context windows of 3-5 terms surrounding each candidate term. In a comparison study of multiple dependency models, Bruza and Song [4] achieve best results using a matrix representation of co-occurrence contexts to describe terms. Gao et al. [11] explicitly decouple the dependency structure of a sentence from the concrete term generation probabilities in the form of linkages.",0,,False
23,"Nallapati and Allan [14] relax the independence assumption by modelling documents as groups of (still independent) sentences. Within each sentence, however, they condition the probability of observing a term on all previous terms in the same sentence. Their sentence model is based on the maximum spanning tree over the fully connected sentence graph. The individual strength of dependency within term pairs is measured in terms of the Jaccard co¨efficient. They later refine this model by advancing from dedicated sentence trees to entire forests [15] of trees for each connected component in the sentence graph. Cao et al. [5] use Bayesian networks to combine two sources of term dependence: cooccurrence and semantic relatedness. The latter is expressed in terms of proximity in the WordNet graph. While most approaches define document language models and compare their respective likelihoods of having generated the query,",1,ad,True
24,783,0,,False
25,"Bai et al. [1] propose a term-dependency query model in order to account explicitly for query expansion. Metzler and Croft [13] learn Markov Random Fields that account for various forms of term dependence on the basis of arbitrary feature vectors. Bendersky et al. [3, 2] propose a learning to rank approach that accounts for higher order term and concept dependencies using hypergraphs. Shi and Nie [17] investigate different dependence weighting schemes based on the concept's utility in the respective retrieval task.",0,,False
26,"In this paper, we present the use of copulas, a robust statistical model family that is able to explicitly decouple marginal observations (i.e., the individual likelihoods of generating terms) from their underlying dependency structures. Comparing to a number of well-known baseline methods and evaluating on a large-scale standard dataset, we show the competitive performance of this novel approach to dependency modelling.",0,,False
27,2. METHODOLOGY,0,,False
28,"In this section, we will give a brief overview of the formal copula framework, introducing the relevant notation and conventions. For a more comprehensive introduction to the topic, please refer to the previous IR applications by Eickhoff et al. [8, 9], or the surveys by Embrechts [10] and Schmidt [16]. Let X be a k-dimensional random vector of observations that we wish to use as input to our copula model:",0,,False
29,"Xk ,"" (x1, x2, . . . , xk)""",0,,False
30,"The copula allows us to model the likelihood of observing X by offering computationally efficient approximations to the true joint probability distribution in the high-dimensional space of cardinality k. As a first step, copulas require input scores U k to be uniformly distributed in the [0 . . . 1] interval. We can achieve this by defining a set of transformations F (X) between raw marginal observations X and their normalized equivalents on the unit cube U .",0,,False
31,"U k ,"" (u1, u2, . . . , uk) "", F (X) ,"" (f1(u1), f2(u2), . . . , fk(uk))""",0,,False
32,An easy example of such a function is the empirical distribution function f^:,0,,False
33,f^(t),0,,False
34,",",0,,False
35,1 n,0,,False
36,1 {xi  t},0,,False
37,The cumulative distribution function C for all copulas is fully defined in terms of a generator  and its inverse -1:,0,,False
38,"C(u1, u2, . . . , uk) , -1((u1) + (u2) + . . . + (uk))",0,,False
39,"There are many concrete instantiations of such copula functions. Each copula family defines their own generator and inverse. A previous study [8] compared a wide range of copula families for the task of Web retrieval, finding Gumbel copulas to be the most adequate choice in this setting. For reasons of space, this paper builds on the previous findings and concentrates exclusively on Gumbel copulas. Their generators are given in the following form:",1,ad,True
40,-1(t),0,,False
41,",",0,,False
42,exp(-t,0,,False
43,1 ,0,,False
44,),0,,False
45,"(t) , (-log(t))",0,,False
46,"The resulting distribution function for a 2-dimensional Gumbel copula is, for example:",0,,False
47,"C (u1 ,",0,,False
48,u2),0,,False
49,",",0,,False
50,exp(-((-log(u1)),0,,False
51,+,0,,False
52,(-log(u2 )) ),0,,False
53,1 ,0,,False
54,),0,,False
55,"Once the choice of copula family is made, we are left with just a single parameter  that allows us to control the strength of dependency between the individual marginal observations u. If we, for example, set  ,"" 1, our distribution function defaults to the case of conditional independence:""",1,ad,True
56,"C,1 , exp(-(-log(u1)) + (-log(u2))) , u1  u2",0,,False
57,"Any choice of  > 1 results in an increasing degree of conditional dependency between the k dimensions of our observation. At this point, we have introduced all relevant components for our original use case of statistical language modelling. A traditional unigram language model describes the likelihood of observing a string of text T under a given class c as the product of the individual likelihoods of each term:",1,ad,True
58,"P (T |c) , P (t1|c)P (t2|c) . . . P (t|T ||c)",0,,False
59,"The same can be achieved under the copula framework by considering the class-conditional probabilities of observing individual terms t1, t2, . . . as our marginal observations, making the dimensionality of our copula k , |T |:",0,,False
60,"P (T |c) , C,""1(P (t1|c), P (t2|c), . . . P (t|T ||c))""",0,,False
61,"By choosing  ,"" 1, we ensure conditional independence between the marginal term observation likelihoods, giving us the standard unigram language model. As we however increase , the strength of dependency between the individual terms increases. This ability to account for term dependence makes the copula framework a powerful alternative to the standard language modelling scheme. At this point, any setting of  globally describes the relationship between all terms. In practice, however, we much rather want a select few terms to depend on each other, while the majority of terms occur indeed independently.""",0,,False
62,"This is easily achieved by using nested copulas. Instead of combining all dimensions in a single step as described earlier, they allow for a nested hierarchy of multiple copulas that estimate joint distributions for sub sets of the full term space and subsequently combine scores until one global model is obtained. Generally, an example of a fully nested copula with k dimensions is given by:",1,ad,True
63,"C0(u1, C1(u2, C2(. . . , Ck-2(uk-1, uk))))",0,,False
64,"By means of the structure of the nesting ""tree"", nested copulas can explicitly model which dimensions depend on each other directly. Instead of the global  parameter discussed earlier, each of the constituent copulas defines their respective i, determining the strengths of these (per-dimension) dependencies. This mechanism gives nested copulas a theoretical advantage in flexibility over their non-nested counterparts. Effectively, this allows us to describe formally grounded probabilistic models under which select term pairs show dependencies ( > 1) while the majority of terms oc-",1,ad,True
65,784,0,,False
66,"C,1",0,,False
67,C>1,0,,False
68,"P(""is"") P(""in"") P(""greenland"")",0,,False
69,"P(""winter"") P(""harsh"")",0,,False
70,"Figure 1: The copula dependence tree for the sentence ""Winter is harsh in Greenland."" shows significant dependence between the terms ""winter "" and ""harsh"" while the remaining terms occur independently.",0,,False
71,"cur independently of each other ( , 1). Figure 1 shows an example of such a situation.",0,,False
72,"At this point, the final missing component in our language modelling scheme is a way to determine the concrete settings of  for a pair of terms. To this end, we define conditional dependency in terms of frequency of co-occurrence in a document corpus and rely on two widely used co-occurrence metrics. The point-wise mutual information between terms t1 and t2 as well as their Jaccard coefficient measure in which fraction of sentences the terms co-occur.",0,,False
73,PMI,0,,False
74,"(t1,",0,,False
75,t2),0,,False
76,",",0,,False
77,log2,0,,False
78,"P (t1,t2) P (t1)P (t2)",0,,False
79,"J (t1, t2)",0,,False
80,",",0,,False
81,|t1 |t1,0,,False
82,t2 | t2 |,0,,False
83,"Finally, the dependency parameter t1,t2 is defined on the basis of the concrete choice of metric m  {PMI , J} and its collection-wide metric mean (µPMI , µJ ) across all potential term pairs. All those pairs of higher-than-average co-occurrence frequency are assigned values of  proportionally to their relative co-occurrence rate. Since  is defined in the range [1, ) and the resulting scores scale in a nonlinear fashion, there is no need to further address or remove outlier pairs of extremely high frequency.",1,ad,True
84,"m,t1,t2 ,",0,,False
85,"m(t1 ,t2 ) µm",0,,False
86,1,0,,False
87,"if m(t1, t2) > µm else",0,,False
88,3. EXPERIMENTS,0,,False
89,"To empirically test the performance of the previously presented copula-based language modelling scheme, we investigate its performance at the task of adhoc document retrieval. Instead of modelling the likelihood of observing a given document under a topic specific language model, we will now establish one distinct model per document and compare their respective likelihoods of having generated the query q.",1,adhoc,True
90,"P (rel|q, d)  P (q|d)",0,,False
91,|q|,0,,False
92,"P (q|d)indep , P (wi|d)",0,,False
93,"i,1",0,,False
94,"P (q|d)cop ,"" Cd(w1, w2, . . . , wn)|w  q""",0,,False
95,"For our experimental comparison, we rely on the widely used ClueWeb'12 corpus, a collection of 730 million authentic Web documents. Our 50 topics originate from TREC's 2013 Adhoc retrieval task [6]. We contrast our method's",1,ClueWeb,True
96,Table 1: Retrieval performance on ClueWeb'12 and,1,ClueWeb,True
97,TREC 2013 Adhoc topics at a cut-off threshold of,1,TREC,True
98,20 retrieved documents.,0,,False
99,Model,0,,False
100,Precision Recall F1 MAP,1,MAP,True
101,Unigram LM 0.31,1,LM,True
102,0.22 0.26 0.41,0,,False
103,Bigram LM,1,LM,True
104,0.34,0,,False
105,0.26 0.3 0.45,0,,False
106,SenTree,0,,False
107,0.35,0,,False
108,0.28 0.31 0.47,0,,False
109,MRF,0,,False
110,0.38,0,,False
111,0.31 0.34 0.51,0,,False
112,Copula LM 0.41* 0.35* 0.38* 0.52,1,LM,True
113,"performance with a number of established as well as stateof-the-art baselines such as standard unigram and bigram language models, Nallapati's sentence trees [14], as well as the Markov Random Field model [13] and apply Laplace smoothing to all LM variants in order to account for previously unseen query terms. Table 1 details the respective performances obtained by the various methods in terms of precision, recall, F1 and MAP, each computed at a cut-off rank of 20 retrieved documents. Statistically significant improvements over all baseline methods are indicated by the asterisk character. Statistical significance was tested using a Wilcoxon signed-rank test at   0.05-level. We can note that, due to their wider context, the classification performance of bigram language models significantly exceeds that of the lower-order model. SenTrees as well as the MRF model which explicitly capture term dependence show even higher classification performance. Finally, our copula language model yields significant performance improvements across most metrics and baselines. The improvements over the MRF model were only significant for some of the considered metrics.",1,LM,True
114,4. CONCLUSION,0,,False
115,"In this paper, we demonstrated the use of the copula framework, a model family from the field of robust statistics, for representing term dependencies in language models. The main advantages of the proposed model are its formal rigour, the low model complexity in terms of training effort as well as disk space requirements and its high degree of flexibility. As an additional advantage, copulas have been previously shown [9] to be beneficial for qualitative manual inspection of results.",1,ad,True
116,"Our experiments, based on a sizeable document collection (ClueWeb'12) confirm the competitive performance of the proposed model in comparison with a number of state-ofthe-art baselines.",1,ClueWeb,True
117,785,0,,False
118,"The present paper describes early results of an ongoing body of research. Consequently, there are numerous directions for future work that are interesting to explore: (1) In this paper, we investigated ""single-layer"" dependency structures with a nesting depth of 1. The nested copula framework, however, is able to capture arbitrarily complex structures. Given a modified dependency estimation scheme, the model can easily account for cases of fine-grained multilevel dependencies. (2) Similarly, the current model regards only dependencies of degree 2. Since the copula framework is able to account for higher-degree dependencies (i.e., between three or more terms) this is another promising alley for continued research. (3) Previous work has investigated different forms of inter-term dependency, including, for example, semantic proximity. It would be easy to integrate such additional sources of evidence into our  estimation step. (4) We would like to draw from the existing wealth of topic modelling techniques in order to describe not merely the dependency structure between individual terms but also between terms and more high-level (latent) concepts, allowing for exciting new insights. (5) Finally, we would like to explore means of representing local term context into the dependency model. Take for instance the two terms ""new"" and ""york"". In the query [affordable real estate on the US east coast] these terms clearly have some dependency, whereas they probably are less dependent when the query is [affordable real estate in yorkshire, UK]. The current paper highlights the applicability of the context invariant method for general queries. In the future we will additionally investigate the importance of the immediate context.",1,ad,True
119,5. REFERENCES,0,,False
120,"[1] Jing Bai, Dawei Song, Peter Bruza, Jian-Yun Nie, and Guihong Cao. Query expansion using term relationships in language models for information retrieval. In Proceedings of the 14th ACM international conference on Information and knowledge management, pages 688­695. ACM, 2005.",1,Query,True
121,"[2] Michael Bendersky and W Bruce Croft. Modeling higher-order term dependencies in information retrieval using query hypergraphs. In Proceedings of the 35th international ACM SIGIR conference, pages 941­950. ACM, 2012.",0,,False
122,"[3] Michael Bendersky, Donald Metzler, and W Bruce Croft. Learning concept importance using a weighted dependence model. In Proceedings of the third ACM international conference on Web search and data mining, pages 31­40. ACM, 2010.",0,,False
123,"[4] Peter Bruza and Dawei Song. A comparison of various approaches for using probabilistic dependencies in language modeling. In Proceedings of the 26th annual international ACM SIGIR conference, pages 419­420. ACM, 2003.",0,,False
124,"[5] Guihong Cao, Jian-Yun Nie, and Jing Bai. Integrating word relationships into language models. In Proceedings of the 28th annual international ACM SIGIR conference, pages 298­305. ACM, 2005.",0,,False
125,"[6] Kevyn Collins-Thompson, Paul Bennett, Fernando Diaz, Charles LA Clarke, and Ellen M Voorhees. Trec 2013 web track overview. In 22nd Text REtrieval Conference, Gaithersburg, Maryland, 2014.",0,,False
126,"[7] W Bruce Croft, Howard R Turtle, and David D Lewis. The use of phrases and structured queries in",0,,False
127,"information retrieval. In Proceedings of the 14th annual international ACM SIGIR conference, pages 32­45. ACM, 1991.",0,,False
128,"[8] Carsten Eickhoff and Arjen P de Vries. Modelling complex relevance spaces with copulas. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1831­1834. ACM, 2014.",0,,False
129,"[9] Carsten Eickhoff, Arjen P de Vries, and Kevyn Collins-Thompson. Copulas for information retrieval. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, pages 663­672. ACM, 2013.",0,,False
130,"[10] P. Embrechts, F. Lindskog, and A. McNeil. Modelling dependence with copulas and applications to risk management. Handbook of heavy tailed distributions in finance, 8(329-384):1, 2003.",0,,False
131,"[11] Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and Guihong Cao. Dependence language model for information retrieval. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 170­177. ACM, 2004.",0,,False
132,"[12] Robert M Losee. Term dependence: truncating the bahadur lazarsfeld expansion. Information processing & management, 30(2):293­303, 1994.",1,ad,True
133,"[13] Donald Metzler and W Bruce Croft. A markov random field model for term dependencies. In Proceedings of the 28th annual international ACM SIGIR conference, pages 472­479. ACM, 2005.",0,,False
134,"[14] Ramesh Nallapati and James Allan. Capturing term dependencies using a language model based on sentence trees. In Proceedings of the eleventh international conference on Information and knowledge management, pages 383­390. ACM, 2002.",0,,False
135,[15] Ramesh Nallapati and James Allan. An adaptive local dependency language model: Relaxing the naive bayes' assumption. 2003.,1,ad,True
136,"[16] T. Schmidt. Coping with copulas. Risk Books: Copulas from Theory to Applications in Finance, 2007.",0,,False
137,"[17] Lixin Shi and Jian-Yun Nie. Using various term dependencies according to their utilities. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 1493­1496. ACM, 2010.",0,,False
138,"[18] Munirathnam Srikanth and Rohini Srihari. Incorporating query term dependencies in language models for document retrieval. In Proceedings of the 26th annual international ACM SIGIR conference, pages 405­406. ACM, 2003.",1,corpora,True
139,"[19] Cornelis Joost van Rijsbergen. A theoretical basis for the use of co-occurrence data in information retrieval. Journal of documentation, 33(2):106­119, 1977.",0,,False
140,"[20] Clement T Yu, Chris Buckley, K Lam, and Gerard Salton. A generalized term dependence model in information retrieval. Technical report, Cornell University, 1983.",0,,False
141,786,0,,False
142,,0,,False

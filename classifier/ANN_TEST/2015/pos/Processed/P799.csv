,sentence,label,data,regex
0,A Head-Weighted Gap-Sensitive Correlation Coefficient,1,ad,True
1,Ning Gao,0,,False
2,"College of Information Studies/UMIACS University of Maryland, College Park",0,,False
3,ninggao@umd.edu,0,,False
4,Douglas W. Oard,0,,False
5,"College of Information Studies/UMIACS University of Maryland, College Park",0,,False
6,oard@umd.edu,0,,False
7,ABSTRACT,0,,False
8,"Information retrieval systems rank documents, and shared-task evaluations yield results that can be used to rank information retrieval systems. Comparing rankings in ways that can yield useful insights is thus an important capability. When making such comparisons, it is often useful to give greater weight to comparisons near the head of a ranked list than to what happens further down. This is the focus of the widely used AP measure. When scores are available, gap-sensitive measures give greater weight to larger differences than to smaller ones. This is the focus of the widely used Pearson correlation measure (). This paper introduces a new measure, GAP, which combines both features. System comparisons from the TREC 5 Ad Hoc track are used to illustrate the differences in emphasis achieved by AP, , and the proposed GAP.",1,ad,True
9,Categories and Subject Descriptors,0,,False
10,H.3.4 [Systems and Software]: Performance evaluation,0,,False
11,Keywords,0,,False
12,Evaluation Metric; Rank Correlation Coefficient,0,,False
13,1. INTRODUCTION,1,DUC,True
14,"In information retrieval evaluation, we often wish to compare alternative systems based on some single-valued evaluation measures. For example, we might want to know whether comparing systems using relevance judgments created by one user (to whom we have access) can be used to compare systems in ways that are predictive of what we would have seen had some other user made the judgments [9]. Alternatively, we might want to know whether we can better approximate the system comparison results we could compute with very extensive relevance judgments on a large number of topics by reducing the number of topics or by reducing the number of judgments per topic [2]. In such cases, we can formulate our research question as asking about the correlation between two ranked lists of scores, where the scores result from some evaluation metric such as F1, Expected Reciprocal Rank (ERR), Normalized",1,ad,True
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",1,ad,True
16,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,0,,False
17,DOI: http://dx.doi.org/10.1145/2766462.2767793.,0,,False
18,"Discounted Cumulative Gain (NDCG), or Mean Average Precision (MAP).",1,MAP,True
19,"When making such comparisons, we focus on two desiderata. First, we prefer that those differences to be large, since small differences may not reflect any meaningful degree of impact on the user experience [3]. Second, we prefer that those differences be statistically significant, since unreliable measurements of differences could be misleading [7]. When extending our comparison from pairs to large sets of systems, as is common in shared-task evaluations such as TREC, CLEF, NTCIR and FIRE, we often care more about distinguishing systems that are very different from each other, which requires the evaluation metric to be gap-sensitive. We also care more about the comparisons among the best systems than we do about comparisons between, for example, the best and the worst systems, which requires the evaluation metric to be headweighted.",1,ad,True
20,"Perhaps the most widely used measure of rank correlation in information retrieval research is Kendall's  [4] in which swapping systems is penalized. Yilmaz et al. [10] introduced a headweighted variant of  that they call AP that penalizes the misranking of the best systems (i.e., those near the head of the list), and that measure is now also often reported. Buckley and Voorhees have observed, however, that when systems receive very similar scores we should care less about swaps than when those scores are very different [1]. They therefore created a gap-sensitive measure by suppressing the effect of small swaps by treating any swap within a ""fuzziness value"" (e.g., 5% relative to the smaller value) as not being large enough to be counted. In this paper, we propose to generalize that approach to penalize swaps in proportion to the difference in scores, so that large swaps will gave the greatest influence on the measure, but small swaps are not completely ignored. This approach thus potentially offers greater insight, without the need to commit to a specific ""fuzziness"" threshold. Moreover, we combine this more nuanced approach with the head-weighted design of AP to produce a new correlation measure for ranked lists of scores that is both head-weighted and gap-sensitive. We call our new measure GAP (for Gap And Position).",1,ad,True
21,"The remainder of the paper is organized as follows, Section 2 reviews the prior work on the use of correlation measures for system comparison. Section 3 then define GAP and establishes that it has a number of desirable properties. Section 4 complements this analytic perspective with empirical results for system scores from the TREC 5 Ad Hoc task, and then further analyses the focus of different correlation coefficient metrics through the heatmap of system pair weights. Section 5 concludes the paper with some remarks on limitations of the GAP measure that may inspire further work on this important problem.",1,GAP,True
22,799,0,,False
23,2. RELATED WORK,0,,False
24,The Pearson correlation coefficient between two items is defined as the covariance of the two items divided by the product of their standard deviations:,0,,False
25,",",0,,False
26,E [(X,0,,False
27,- µX )(Y X · Y,0,,False
28,- µY )],0,,False
29,(1),0,,False
30,"where X and Y are the vectors of ranked lists; E is the expectation;  is the standard deviation; and µ is the mean [6]. Given two ranked lists of items, the Spearman correlation coefficient [11] is defined as the Pearson correlation coefficient between the ranks (i.e., with the ranks used in place of scores). The most widely used measure in information retrieval research is neither Pearson nor Spearman correlation, however, but rather Kendall's  [4]. Kendall's  evaluates the correlation of two lists of items by counting their concordant and discordant pairs.",0,,False
31,"To fulfill the specific evaluation needs for different tasks, various definitions of correlation coefficients derived from Kendall's  have been proposed. Kendall's tau rank distance [5] measures the disagreements between two ranking lists by counting the swaps that the bubble sort algorithm needed to sort one list in the same order as the other. The AP Correlation coefficient (AP), proposed by Yilmaz et al. [10], focuses and penalizes more on the errors at high rankings. Given a ground-truth list and prediction list, AP of the two lists is defined as:",1,AP,True
32, AP,1,AP,True
33,",",0,,False
34,2 (N - 1),0,,False
35,N,0,,False
36,·,0,,False
37,"i,2",0,,False
38,Ci i-1,0,,False
39,-1,0,,False
40,(2),0,,False
41,"where N is the number of items in the list; Ci is the number of items above rank i in the prediction list and correctly ranked with respect to the item at rank i in the ground-truth list. For each item at rank i, AP only checks the positions of the i - 1 items with ranks above i, and calculates the proportion of the correctly ordered items with respect to the item at rank i. Finally, the value of AP is a linear combination of all ranks i in the prediction list.",1,AP,True
42,3. THE TAU GAP COEFFICIENT,1,GAP,True
43,"GAP is a non-parametric correlation coefficient, particularly sensitive to errors at high rankings and errors for item pairs with large score differences (gaps). In this section, we present the definition of GAP. Since the definition of AP and GAP are both based on swapped item pairs counting, we further explore the mathematical properties of GAP comparing with AP.",1,GAP,True
44, GAP,1,GAP,True
45,",",0,,False
46,(N,0,,False
47,2 - 1),0,,False
48,N,0,,False
49,·,0,,False
50,"i,2",0,,False
51, j<i |CG ji|  j<i |G ji|,0,,False
52,-1,0,,False
53,(3),0,,False
54,"Given two ranked lists, a ground truth list and a prediction list, GAP is defined as in Equation 3, where N is the number of items in the lists; i represents the item at rank i in the prediction list; j is the item ranked higher than i in the prediction list; G ji is the gap between the item at rank j and item at rank i in ground-truth list; CG ji returns the same value as G ji when the item pair ( j, i) in the prediction list are ranked in the same order as in the ground-truth list, otherwise, return 0. For each item at rank i in the prediction list, GAP only considers the rank relation of the i - 1 item pairs ( j, i).  j<i |G ji| is the sum of all the gaps for the i - 1 item pairs, and  j<i |CG ji| is the sum of the gaps for correctly ordered pairs.",1,GAP,True
55,THEOREM 1. The value of GAP is always between -1 and 1.,1,GAP,True
56,"PROOF. For each item at rank i, there will be i - 1 pairs of items ( j, i) taken into the consideration by GAP.  j<i |G ji| is the sum of the gaps of these i - 1 pairs of items.  j<i |CG ji| is the sum of the",1,GAP,True
57,"gaps that the prediction list ranks the two items ( j, i) in the same or-",0,,False
58,der,0,,False
59,as,0,,False
60,the,0,,False
61,ground-truth,0,,False
62,list.,0,,False
63,"Therefore,",0,,False
64, j<i |CG ji|  j<i |G ji|,0,,False
65,is,0,,False
66,always,0,,False
67,between,0,,False
68,0 and 1.,0,,False
69,"Normalized across all ranks i,",0,,False
70,1 (N-1),0,,False
71,·,0,,False
72,"Ni,2",0,,False
73, j<i |CG ji|  j<i |G ji|,0,,False
74,is,0,,False
75,always,0,,False
76,between,0,,False
77,0,0,,False
78,and,0,,False
79,1;,0,,False
80,2 (N-1),0,,False
81,"· Ni,2",0,,False
82, j<i |CG ji|  j<i |G ji|,0,,False
83,is,0,,False
84,always,0,,False
85,between,0,,False
86,0 and 2. Then the value of GAP is always between -1 and 1.,1,GAP,True
87,"THEOREM 2. If the gaps between the items follow the uniform distribution, then GAP is equal to AP.",1,GAP,True
88,"PROOF. Let the uniform gap be g, then  j<i |G ji| ,"" (i - 1) · g, and  j<i |CG ji| "","" Ci · g. Therefore,""",0,,False
89, GAP,1,GAP,True
90,",",0,,False
91,(N,0,,False
92,2 - 1),0,,False
93,N,0,,False
94,·,0,,False
95,"i,2",0,,False
96,Ci · g (i - 1) · g,0,,False
97,-1,0,,False
98," ,",0,,False
99,(N,0,,False
100,2 - 1),0,,False
101,N,0,,False
102,·,0,,False
103,"i,2",0,,False
104,Ci (i - 1),0,,False
105,-1,0,,False
106,",",0,,False
107,AP,1,AP,True
108,(4),0,,False
109,"Moreover, if the errors are uniformly distributed over the all the ranks in prediction list, then GAP, AP and  are equivalent.",1,GAP,True
110,"THEOREM 3. For two prediction lists with same number of items N and same number of errors located at the same ranks, if the errors of one list have large gaps and the errors of the other list have small gaps, then the value of GAP for the list with large error gaps GAP-Large will be smaller than the GAP for the list with small error gaps GAP-Small .",1,GAP,True
111,"PROOF. If we define FG ji as the gap between items at rank i and rank j when the item pair ( j, i) in the prediction list are ranked in the converse order as in the ground-truth list, or otherwise 0, then GAP could be represented as:",1,GAP,True
112, GAP,1,GAP,True
113,",",0,,False
114,(N,0,,False
115,2 - 1),0,,False
116,N,0,,False
117,·,0,,False
118,"i,2",0,,False
119, j<i,0,,False
120, j<i |CG ji| |CG ji| +  j<i |FG ji|,0,,False
121,-1,0,,False
122,(5),0,,False
123,"If two prediction lists have the same number of errors located at the same ranks, then their will have the same N, i, j and CG ji. The only difference between GAP-Large and GAP-Small is that GAP-Large has larger values for FG ji, so that the value of GAP-Large will be always smaller than GAP-Small .",1,GAP,True
124,"THEOREM 4. For a prediction list, if the swapped item pairs always have small gaps, then the value of GAP is larger than AP; if the swapped item pairs always have large gaps, then the value of GAP is smaller than AP.",1,GAP,True
125,PROOF. The expectation of the difference between GAP and AP is:,1,GAP,True
126, E [GAP,1,GAP,True
127,- AP],1,AP,True
128,",",0,,False
129,N,0,,False
130,2 -1,0,,False
131,N,0,,False
132,·,0,,False
133,"i,2",0,,False
134, j<i CG ji  j<i G ji,0,,False
135,-,0,,False
136,Ci i-1,0,,False
137," ,",0,,False
138,N,0,,False
139,2 -1,0,,False
140,N,0,,False
141,·,0,,False
142,"i,2",0,,False
143,(i - 1)  j<i CG ji -Ci  j<i G ji (i - 1)  j<i G ji,0,,False
144,(6),0,,False
145,"Let Pi be the probability that the items before item i in the prediction list are ranked as the same order as in ground-truth list, then we have",0,,False
146,"Ci , Pi · (i - 1)",0,,False
147,(7),0,,False
148,800,0,,False
149,Then,0,,False
150, E [GAP,1,GAP,True
151,- AP],1,AP,True
152,",",0,,False
153,N,0,,False
154,2 -1,0,,False
155,N,0,,False
156,·,0,,False
157,"i,2",0,,False
158,(i - 1)( j<i CG ji - Pi  j<i G ji) (i - 1)  j<i G ji,0,,False
159,   2 N,0,,False
160,",",0,,False
161,N,0,,False
162,-1,0,,False
163,·,0,,False
164,"i,2",0,,False
165,1  j<i G ji ·,0,,False
166,CG ji - Pi G ji,0,,False
167,j<i,0,,False
168,j<i,0,,False
169,(8),0,,False
170,The overall expectation of E[GAP - AP] is a linear combination,1,GAP,True
171,"over all ranks i  [2, N]. However, for each rank position i in the",0,,False
172,"predict list, the expectation of the difference between GAP and AP",1,GAP,True
173,is:,0,,False
174,  Ei[GAP,1,GAP,True
175,- AP],1,AP,True
176,",",0,,False
177,2  j<i G ji,0,,False
178,·,0,,False
179,CG ji - Pi G ji,0,,False
180,j<i,0,,False
181,j<i,0,,False
182,(9),0,,False
183,"Since  j<i G ji is always positive, the relation between GAP and AP at rank i depends on ( j<i CG ji - Pi  j<i G ji). For rank i, there are (i - 1) pairs of items and their corresponding gaps in the prediction list taken into consideration, and there are Pi(i - 1) pairs ordered correctly.  j<i CG ji is the sum of the gaps for these correct ordered pairs, and Pi  j<i G ji is a definite proportion of the total gaps of the (i - 1) pairs. If the Pi(i - 1) correctly ordered pairs with large gaps, then  j<i CG ji > Pi  j<i G ji, and Ei[GAP - AP] > 0; if the Pi(i - 1) correctly ordered pairs focus on the pairs with small gaps, then Ei[GAP - AP] < 0; ideally, if the Pi(i - 1) correctly ordered pairs are distributed randomly across all levels of gaps, then Ei[GAP - AP] ,"" 0. Since the expectation of E[GAP - AP] is a linear combination over all Ei[GAP - AP], we could conclude that if the error gaps of a prediction list are relatively small, then GAP is larger than AP; if the error gaps of a prediction list are relatively large, then GAP is smaller than AP.""",1,GAP,True
184,4. COMPARISON OF THE METRICS,0,,False
185,"In this section, we use 61 participating systems from TREC 5, ranked by MAP, as a case study to compare the different emphases of the correlation coefficients , AP and our proposed GAP.",1,TREC,True
186,4.1 Correlation Scores for Prediction Lists,0,,False
187,"Figure 1 shows the result of a two-sided paired t-test for each pair of systems based on Average Precision (AP) scores for each of the 50 topics as samples, with both axes sorted in the same order. In that figure, system pairs with p < 0.05 are plotted as white (37%) and system pairs with p  0.05 are plotted as black (63%). This illustrates clearly that swaps among many of the systems (near the main diagonal, where score differences are smallest) would offer little insight into whether one evaluation framework yielded system comparisons that we meaningfully different from another.",1,AP,True
188,"Figures 2 and 3 then each illustrate two ways of making ranked lists to compare. Each dot represents a system, with the true (TREC5) rank of that system plotted on the X-axis, and a randomly permuted rank on the Y-axis. We produce the random permutations by randomly selecting five system pairs each time and swapping them. In Figure 2, we randomly select systems pairs that are statistically significantly different from each other (i.e., five of what were black dots in Figure 1). We do these five random draws 50 times (Figure 2 shows only one of the 50 times). The correlation coefficient metrics that result are  , 0.70; AP , 0.68; GAP ,"" 0.65, averaging over 50 such random draws of five pairs to swap. In Figure 3, we randomly select system pairs from among the set of pairs that are not statistically significantly different from each other (i.e., five of what were white dots in Figure 1). We do this 50 times. The correlation coefficient metrics that result are  "", 0.99;",1,TREC,True
189,"AP , 0.92; GAP ,"" 0.97, averaging over 50 such random draws of pairs to swap. In general, we can see that swapping system pairs with large gaps yields lower correlation scores with any measure than swapping system pairs with small gaps. But the key observation to make is that, as proven in theorem 4, when the swapped system pairs have relative large gaps, GAP "", 0.65 is lower than AP ,"" 0.68; whereas when the swapped system pairs have relatively small gaps, GAP "", 0.97 is larger than AP , 0.92.",1,AP,True
190,4.2 Weights for System Pairs,0,,False
191,"Figures 4, 5 and 6 show the weight for each system pair in evaluating the correlation coefficient of two ranked lists under the measurement of , AP and GAP respectively. The systems are also ranked by their ground truth MAP scores. So cell (1, 61) represents the weight of only swapping the systems at rank 1 and rank 61. In detail, taking GAP in Figure 6 as example, the value of cell (1, 61) is calculated by: (1) produce the prediction list by only swapping the systems at rank 1 and 61 in ground truth list; (2) calculate the value of GAP between the ground truth list and the prediction list; (3) fill the value of (1 - GAP) to cell (1, 61); (4) after filling in all the cells, rescale the matrix to (0, 1) and build the heatmap. Therefore, in general, darker cells represent system pairs with higher weight in evaluating the correlation coefficient.",1,AP,True
192,"Figure 4 shows the heatmap of system pair weights by using . Since  is only sensitive to the score difference, not the ranks of items, we can observe that the value of  is dominated by the swapping pairs with large differences, probably composed of a system at very high rank and a system at very low rank. The system pairs along the diagonal with non-significant difference get lower weights as expected. However, the swap of top ranked systems also get lower weights because of their relatively small difference. The heatmap of AP in figure 5 shows a progressively decreasing from the top-left corner of (1, 1) to the bottom-right corner of (61, 61) due to its sensitivity over ranks. However, comparing Figure 5 with Figure 1, we can see that the 37% non-significant system pairs still get non-ignorable weights in calculating AP. Figure 6 shows the heatmap using GAP. Since GAP is sensitive to both top ranked systems and the gap between system pairs, we can also observe a decreasing of weights for the system pairs from top-left corner to the bottom-right corner. At the same time, we can see a expansion of the lower weight area (bright-ish area) along the diagonal line comparing with the heatmap of AP. This is due to GAP's sensitivity to the system pairs with small difference. Overall, the value of GAP is dominated by the top ranked system pairs with large difference.",1,AP,True
193,5. CONCLUSION,0,,False
194,"In this paper, we proposed a new metric GAP for evaluating the correlation coefficient between two ranked lists of scores. We have shown that GAP is sensitive to both top ranked items and to swapped item pairs that exhibit larger differences. Through analysis, we have shown that GAP compares favorably with both  and AP, and using the TREC 5 Ad Hoc track as a case study we have illustrated that the swaps that GAP is most sensitive to the ones we have argued we should care the most about. Although we have introduced GAP in the context of system comparison, it is of course a general correlation coefficient for ranked lists of scores that could be applied in any case in which both a head-weighted and gap-sensitive measure would be useful. There are, however, some characteristics of GAP that might be further improved upon. For example, it might be useful to completely discount differences in scores that are not statistically reliable indicators of real differences in system behavior. As Figure 1 illustrates, such cases can",1,GAP,True
195,801,0,,False
196,Ground Truth Rank,0,,False
197,1,0,,False
198,30,0,,False
199,60 60,0,,False
200,30 Ground Truth Rank,0,,False
201,Prediction Rank,0,,False
202,1 20 40 60,0,,False
203,Prediction Rank,0,,False
204,1 20 40 60,0,,False
205,1,0,,False
206,1,0,,False
207,20,0,,False
208,40,0,,False
209,60,0,,False
210,1,0,,False
211,20,0,,False
212,40,0,,False
213,60,0,,False
214,Ground Truth Rank,0,,False
215,Ground Truth Rank,0,,False
216,Figure 1: Significance of the MAP differ- Figure 2: Significant difference between Figure 3: No significant difference be-,1,MAP,True
217,ence on TREC 5.,1,TREC,True
218,system pairs.,0,,False
219,tween system pairs.,0,,False
220,1,0,,False
221,1,0,,False
222,1,0,,False
223,Ground Truth Rank,0,,False
224,Ground Truth Rank,0,,False
225,Ground Truth Rank,0,,False
226,30,0,,False
227,30,0,,False
228,30,0,,False
229,60 1,0,,False
230,30,0,,False
231,60,0,,False
232,Ground Truth Rank,0,,False
233,Figure 4: Gap-Sensitive: ,0,,False
234,60 1,0,,False
235,30,0,,False
236,60,0,,False
237,Ground Truth Rank,0,,False
238,Figure 5: Head-Weighted: AP,1,ad,True
239,60 1,0,,False
240,30,0,,False
241,60,0,,False
242,Ground Truth Rank,0,,False
243,Figure 6: The proposed GAP,1,GAP,True
244,"be common. As another example, GAP implicitly presumes that the scores are represented on a meaningful interval scale, meaning that (for example) a user would prefer a difference of 0.2 twice as much as they would prefer a difference of 0.1. User studies have shown that some current evaluation measures do not exhibit anywhere near this degree of correlation to extrinsic measures of success such as task completion rates [8]. Future extensions that more closely model extrinsic measures of satisfaction or success might therefore be useful. Nonetheless, we see the progression from  to AP and now to GAP to be a useful one, and one that can perhaps serve as a basis for future extensions of these and other kinds.",1,GAP,True
245,6. ACKNOWLEDGEMENT,0,,False
246,"This work has been supported in part by NSF award 1065250. Opinions, findings, conclusions and recommendations are those of the authors and may not reflect NSF views.",0,,False
247,References,0,,False
248,"[1] C. Buckley and E. M. Voorhees. Evaluating evaluation measure stability. In SIGIR, pages 33­40, 2000.",0,,False
249,"[2] N. Gao et al. Reducing reliance on relevance judgments for system comparison by using expectation-maximization. In ECIR, pages 1­12. 2014.",0,,False
250,"[3] K. S. Jones. Automatic indexing. Journal of Documentation, 30(4):393­432, 1974.",0,,False
251,"[4] M. G. Kendall. A new measure of rank correlation. Biometrika, pages 81­93, 1938.",0,,False
252,"[5] M. G. Kendall. Rank correlation methods. Griffin, 1948.",0,,False
253,"[6] K. Pearson. Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58(347-352):240­242, 1895.",0,,False
254,"[7] M. Smucker et al. A comparison of statistical significance tests for information retrieval evaluation. In CIKM, 2007.",0,,False
255,"[8] A. Turpin and W. R. Hersh. Why batch and user evaluations do not give the same results. In SIGIR, pages 225­231, 2001.",0,,False
256,"[9] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Inf. Process. Manage., 36(5):697­716, 2000.",0,,False
257,"[10] E. Yilmaz et al. A new rank correlation coefficient for information retrieval. In SIGIR, pages 587­594, 2008.",0,,False
258,"[11] G. U. Yule. An introduction to the theory of statistics. C. Griffin, limited, 1919.",0,,False
259,802,0,,False
260,,0,,False

,sentence,label,data,regex
0,Splitting Water: Precision and Anti-Precision to Reduce Pool Bias,0,,False
1,Aldo Lipani,0,,False
2,"Inst. of Software Technology & Interactive Systems Vienna University of Technology Vienna, Austria",0,,False
3,lipani@ifs.tuwien.ac.at,0,,False
4,Mihai Lupu,0,,False
5,"Inst. of Software Technology & Interactive Systems Vienna University of Technology Vienna, Austria",0,,False
6,lupu@ifs.tuwien.ac.at,0,,False
7,Allan Hanbury,0,,False
8,"Inst. of Software Technology & Interactive Systems Vienna University of Technology Vienna, Austria",0,,False
9,hanbury@ifs.tuwien.ac.at,0,,False
10,ABSTRACT,0,,False
11,"For many tasks in evaluation campaigns, especially those modeling narrow domain-specific challenges, lack of participation leads to a potential pooling bias due to the scarce number of pooled runs. It is well known that the reliability of a test collection is proportional to the number of topics and relevance assessments provided for each topic, but also to same extent to the diversity in participation in the challenges. Hence, in this paper we present a new perspective in reducing the pool bias by studying the effect of merging an unpooled run with the pooled runs. We also introduce an indicator used by the bias correction method to decide whether the correction needs to be applied or not. This indicator gives strong clues about the potential of a ""good"" run tested on an ""unfriendly"" test collection (i.e. a collection where the pool was contributed to by runs very different from the one at hand). We demonstrate the correctness of our method on a set of fifteen test collections from the Text REtrieval Conference (TREC). We observe a reduction in system ranking error and absolute score difference error.",1,ad,True
12,Categories and Subject Descriptors,0,,False
13,H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation,0,,False
14,General Terms,0,,False
15,"Experimentation, measurement, performance",0,,False
16,Keywords,0,,False
17,"Evaluation, bias, pool, test collection, TREC",1,TREC,True
18,1. INTRODUCTION,1,DUC,True
19,A test collection is a valuable resource for Information Retrieval (IR) researchers because it gives the IR community,0,,False
20,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767749 .",1,ad,True
21,"a common ground to facilitate the development of search models. Numerous test collections have been developed in the field since the first Cranfield experiments in the 1960s. Since the start of TREC in the 1990s, this creation happens at a rate of approximately 25 test collections per year. A test collection is composed of: a set of documents, a set of topics and a set of relevance assessments for each topic, derived from the collection of documents. The number of documents in the collection generally makes the full judgment of the document set for every topic infeasible. Therefore, the relevance assessment process is generally optimized by pooling the top N documents for each run. The pool is constructed from systems taking part in the challenge for which the collection was made, at a specific point in time, after which the collection is generally frozen in terms of relevance judgments. The pooling technique aims to identify an unbiased sample of relevant documents. Nevertheless, pool bias negatively affects the score of unpooled runs--those of systems not present at the time of test collection creation. This is a drawback that ultimately affects the reliability of the test collection. The variables controlling this reliability are [14]: the number of topics and their representativeness of the information needs of the target user, the number of documents assessed per run, and, last but not least, the diversity of the pooled systems (often however only assessed as the cardinality of the set of runs).",1,TREC,True
22,"In the last decades the IR community has branched out significantly in a variety of domains and applications, with the creation of specific IR test collections focusing on specific problems. At the same time, benchmarking techniques developed in the IR community are being implemented in industry. Information aware companies request measures to quantify the quality of their information access systems in general, and search systems in particular. With a narrower focus however, the effort to successfully solve the challenges facing the creators of test collections takes on new significance. Most notably, it is often difficult to acquire a sufficient number of participants and diverse systems in order to fulfill the required run diversity to guarantee a reliable test collection.",1,ad,True
23,"In this paper, we estimate the pool bias by studying the effect of an unpooled run on the set of pooled runs, when a fixed-depth pooling strategy is used. We do this through the estimation of an average unjudged rate, which we then normalize with its potential growth interval, in order to adjust the pool bias. Additionally, we introduce an indicator that",1,ad,True
24,103,0,,False
25,"provides strong clues about the quality of a new, unpooled run.",0,,False
26,"We do this based for Precision at cut-off P @n. There are two reasons to consider such a ""simple"" metric. First, it is a cornerstone for many other metrics developed for the most popular of user models these days: the web user [12]. Second, it is easy to understand by all users. This ""understandability"" of the IR metrics has drawn moderate attention from our community recently [10]. Our own experience in the industry leads us to believe that, when results are not presented as simply precision and recall, any numbers are just assumed to be precision or recall. Decision makers at lower or higher levels, trying to make sense of MAP, or any other commonly used metric in our community, will most often read 0.12 as 12% and simply assume that either 12% of documents are relevant or 12% of relevant documents have been returned on average. Of course, we do not forget why all the other metrics have been invented to replace, or complement, precision at cut-off: 1) for an ideal run, if the topic has less relevant documents then n, P @n does not reach 1; it is not normalized by the number of relevant documents, therefore it is difficult to average over topics, 2) it partially neglects the position of the documents. Nevertheless, there are many cases where P @n is useful (most often, but not only, for the user modeled as considering blocks of 10 documents at a time on the web). This is also demonstrated by its continued use and reporting throughout a majority of evaluation tracks at TREC, CLEF, or NTCIR.",1,ad,True
27,"We propose a new bias correction method and demonstrate its effectiveness through leave-one-out experiments, at different levels and combinations, organizations and systems, all the pooled runs, or only the 75% of the top runs as done in previous papers [22, 19, 2, 21, 20]. We then evaluate the results using the mean absolute error (MAE) and the system rank error (SRE), comparing it against the results obtained with the reduced pool, and with the method of correcting pool bias introduced by Webber and Park [23]. We do this on fifteen test collections from TREC, five of which are domain specific test collections.",1,TREC,True
28,"In short, the contributions of this study are as follows: 1. a new perspective on P @n, based on the effect of a",0,,False
29,new run on the set of runs which contributed to the creation of the pool;,0,,False
30,2. an indicator to trigger bias correction only when it is indeed necessary;,0,,False
31,"3. a bias correction method for P @n, including extensive experimental results to show that it outperforms existing bias correction methods,",0,,False
32,"The remainder of the paper is structured as follows: in Section 2 we provide a brief summary of the extensive work already done to assess and correct pool bias. Section 3 provides the intuition of our method and introduces the required concepts, followed in Section 4 by the method itself. In Section 5 we present and discuss our experimental results. We conclude in Section 6.",1,ad,True
33,2. RELATED WORK,0,,False
34,"Work related to pool bias can be grouped in three categories: first, that aiming to fundamentally change the way assessment is done, by, instead of pooling, choosing assessment documents in order to maximize some evaluation goal. For instance, Cormack et al. [11] suggest to boost the proportional of relevant documents, Moffat et al. [15] to fo-",1,ad,True
35,"cus on the score accuracy of the best-performing systems, Carterette et al [7, 6] to maximize confidence that one system has or has not a better score then another one, using different models of probability of relevancy. While important, this is not the focus of the current paper, which instead addresses the problem of evaluating against existing test collections built using pooling.",1,ad,True
36,"Second, there are those studies aiming to assess the reliability of a test collection. This reliability (or lack thereof) can be often traced back to the pooling procedure. Most recently Urbano et al. [20] proposed an estimation of reliability of a test collection using Generalization Theory. We shall use this in our study to better understand the observations made in our experiments.",1,ad,True
37,"Finally, and most related to this study, are those works that address the problem of pool bias directly. Here, three different strategies have been studied: removing the bias from the onset, at test collection creation time; creating new metrics to better handle unjudged documents; or estimating the score error to make an adjustment in the metric.",1,ad,True
38,"The first strategy is the most desirable: enforcing a diverse set of runs through the efforts of the test collection creators themselves. Especially early test collections have, for instance, created manual runs to increase the likelihood of relevant documents appearing in the pool. The benefit of such efforts has been demonstrated, among others by Kuriyama et al. [13]. Nevertheless, not all test collections have this advantage, and adding such runs a posteriori, after an initial set of systems have been evaluated, is not done because it breaks the comparability of the runs evaluated across the years.",1,ad,True
39,"A lot more can be done and has been done for the second strategy: new metrics. In 2004, Buckley and Voorhees [4] introduced BPref as a metric specifically designed to handle incomplete information, which, as pointed out by Sakai in 2007 [17], is a restricted form of Average Precision (AP) on a so called `condensed list'. These are condensed versions of the runs where unjudged documents are filtered out. Sakai introduces a new metric (the Q-measure) and shows that it is possible to obtain better performance then BPref even applying already well-known metrics to the condensed list. The concept of condensed list, first denoted as such by Sakai, was however already explored in relation to AP with the measure Induced AP, introduced by Yilmaz and Aslam [24] the year before, in 2006. Induced AP is Average Precision calculated on condensed lists. The methods explored by these three contributions do not simulate the effect of shallow pooling or of comparing unpooled runs against pooled ones, because they remove the effect of bias sampling from the query relevance (qrel) set, ending up with an unrealistic use case. This was later addressed by Sakai [18], who demonstrated that the condensed list approach leads in favor of new systems, the effect of these metrics instead creating incomplete relevance information by playing with the depth of the pool. Also in their 2006 report, Yilmaz and Aslam [24] introduce the Inferred AP, a more complex metric which is a closer approximation of AP but requires knowledge about the documents down to depth 100. Inferred AP adjusts the score sampling uniformly from the pooled documents and then estimates the true mean of the sample to adjust AP.",1,AP,True
40,"Later, Aslam and colleagues address the issue of the uniform sampling used in the 2006 version of Inferred AP [25]. In this later version they use a stratified sampling scheme.",1,ad,True
41,104,0,,False
42,"However, their finding that the method is not subject to pooling bias is not confirmed in practice by Carterette et al. in 2008 [8]. As they write, this is possibly because aggregating probability of inclusion across multiple runs by taking the mean of the per run probabilities may not properly account for reinforcement by similar systems.",0,,False
43,"The problem of incomplete judgments leads to the definition of a completely new metric, defined by Moffat and Zobel [16]--called Rank-Biased Precision--expressed by a single value and a residual. The Residual quantifies the uncertainty introduced by the unjudged documents. Its value is computable thanks to the fact that it is not normalized by the number of relevant documents. This implies that the computation of the metric defines a lower bound for the given run. Moffat and Zobel attempted to make a measure that is naturally convergent, where the contribution of each rank has a fixed weight. This would have both benefits of a normalized metric and those of a metric averageable over topics with different numbers of relevant documents. This attempt was unsuccessful, as pointed out by Sakai [18], who proved this to be inferior with respect to the condensed list.",1,ad,True
44,At this point we have the transition to the third category of approaches to solve the pool bias: metric error estimation and correction.,0,,False
45,"In their presentation of Rank-Biased Precision (RBP), Moffat and Zobel had already introduced the discussion concerning the fact that the residual can be used to estimate and correct pool bias. Webber and Park [23] continue their work on RBP by adding to the score the average residual calculated against the pool proceeding with a leave-one-runout approach. To estimate it they span two dimensions: the topics and the systems. They used Rank-Biased Precision at ten (RBP@10) and Precision at ten (P@10) although the results for this last metric were not reported in the 2009 paper, the authors only mentioned that they were similar to RBP. In the present study we return to precision at cut-off and look not only at coefficients to correct pool bias, but also at whether there is something to correct in the first place.",1,ad,True
46,3. PRECISION AND ANTI-PRECISION,0,,False
47,"The intuition at the base of the proposed method is that we can observe how a new, unpooled run impacts the existing, pooled runs. Given such an existing run, we can imagine reranking it based on the ranks of its documents in the unpooled run. A ""bad"" new run will tend to bring down known relevant documents and push up non-relevant ones. Quantifying these changes we create a measure of the potential quality of the new run.",1,ad,True
48,"In the following we describe theoretically the measures later used to reduce the pool bias. In evaluating IR systems, Precision (P ) is one of the two fundamental measures. We recall its definition: given D a set of documents, Dr a subset of D (the documents in a run r), q a topic, and  a function of relevancy returning the level of relevancy of the document d for the topic q, P is defined as:",0,,False
49,"P ,"" |d  Dr : (d, q) > 0| |Dr |""",0,,False
50,"Precision represents the proportion of relevant and retrieved documents against the retrieved ones. From P we derive the definition of Precision at cut-off n (P @n), used to better handle ranked retrieval systems: given  a function that",0,,False
51,"returns the rank of a document d in a run r, we have:",0,,False
52,"P @n ,"" |d  Dr : (d, q) > 0, (d, r)  n| n""",0,,False
53,"The measure takes into account only the relevant documents because it is supposed to be used when there is a complete knowledge of the relevance function over the documents in the run. When we consider the problem of missing relevance assessments this assumption is not true, ending up considering unjudged documents as non-relevant. To overcome this problem and take into account the missing information about the run, we define the complement of Precision, called Anti-Precision (P ). Anti-Precision measures the proportion of non-relevant and retrieved documents against the retrieved documents. In statistics, a similarly defined quantity is referred to as the False Discovery Rate (FDR) [1]. It is used in quantifying the results of multiple hypothesis testing experiments. However, given the very different use of it here, we continue to refer to it as Anti-Precision in this study, and define it as:",0,,False
54,"P ,"" |d  Dr : (d, q) "", 0| |Dr |",0,,False
55,"As well as for Precision, we define also the cut-off version (P @n):",0,,False
56,"P @n ,"" |d  Dr : (d, q) "","" 0, (d, r)  n| n""",0,,False
57,"Indeed, when a run is fully judged the following equation holds:",0,,False
58,"P +P ,1",0,,False
59,"When it is not, and unjudged documents are present in the run, the sum of P and P is lower than 1, reduced by a quantity that represents the proportion of retrieved and unjudged documents against the retrieved documents. We refer to this as k bar (k¯).",0,,False
60,"P + P , 1 - ¯k",0,,False
61,"This quantity represents the uncertainty of the measurement. Just as P and P , ¯k can be also defined at cut-off (¯k@n).",0,,False
62,3.1 Analysis of a run shuffle,0,,False
63,"Before going on to the details of our proposed method, let us perform an imagination exercise in order to better understand the information content of a partially judged run. We want to analyze which kind of information precision and anti-precision expose if a given run r gets shuffled. As in a deck of cards a shuffling changes the order of the documents of a run and produces a new run that we will indicate as r . This run has the same set of documents as before. We want to observe the variation in score the run obtains in the two states, original and shuffled. If we would use P , since there is no information about the position of the documents in the formula, we would measure a change of 0. Therefore, let us observe P @n. Given a run r and its shuffled version r we define:",0,,False
64,"P @n(r ) , P @n(r ) - P @n(r)",0,,False
65,"P @n has domain [-1, 1] and is the variation in precision of the run after a shuffle. Its increase in value is the result of the combination of the following two related effects: the shuffle",0,,False
66,105,0,,False
67,"moved up relevant documents, placing them in the top n, or moved down non-relevant or unjudged documents with the consequential moving up of potential relevant documents in the run. It decreases if the opposite happens. We also define P @n as following:",0,,False
68,"P @n(r ) , P @n(r ) - P @n(r)",0,,False
69,"P @n has domain [-1, 1] and is the variation in anti-precision of the run after a shuffle. Its increase in value is the result of the combination of the following two related effects: the shuffle moved up non-relevant documents, placing them in the top n, or moved down relevant or unjudged documents with the consequential moving up of potential non-relevant documents in the run. It decreases if the opposite happens.",0,,False
70,"Finally, ¯k@n that is derived as following:",0,,False
71,"¯k@n(r ) , ¯k@n(r ) - ¯k@n(r)",0,,False
72,", 1 - (P @n(r ) + P @n(r ))",0,,False
73,(1),0,,False
74,- [1 - (P @n(r) + P @n(r))],0,,False
75,", -P @n(r ) - P @n(r)",0,,False
76,"¯k@n has domain [-1, 1] and is the variation of unjudged documents on a given run. Its increase in value is the result of the combination of the following effects: the shuffle moved up unjudged documents or moved down relevant and non-relevant documents with the consequential moving up of potential unjudged documents in the run. An interesting property of this function, which is possible to prove, is that if r has been judged to depth d : d  n, then the domain of the function ¯k@n is [0, 1]. This property always holds for pooled runs because they verify the condition (provided of course that no mistakes occurred in the pooling process).",0,,False
77,"In summary, when a run changes the order of its documents, P , P , and ¯k are indicators of the direction of the judged relevant, judged non-relevant, and unjudged documents in the list.",0,,False
78,3.2 Effect of a run on a pooled run,0,,False
79,"Now let us make a step further and consider not the relationship between a run and a random shuffle of itself, but between a run and another run. In the particular case where each run ranks completely the entire collection, this is the same as above. In general however, the systems only provide runs down to a certain limit (say 1000). To study this effect, we need to define a merging function between the two runs. The unpooled run will have an effect on the pooled run, measured by the quantities described above.",0,,False
80,"Such a merging function can simply be based on the rank of the documents in the run. The aim here is not to add or remove documents from a run, so although the word ""merging"" could imply the transfer of documents between the two runs to make a new one, we must keep in mind that all we need to do here is transfer only the information about the rank of the documents. We do this by linearly combining the ranks if the two runs share the same document.",1,ad,True
81,"In the following formula, by ru we denote the new, previously unseen and unpooled run, whose effect on rp an existing run, we want to study. This effect we represent as a new, synthetic run r , which consists exclusively of documents present in rp, potentially re-ordered.",0,,False
82,"r , rp  ru ,"" {d  rp : (d, r ) "","" µ(d, rp, ru)} (2)""",0,,False
83,where,0,,False
84,"µ(d, rp, ru) ,",0,,False
85,"(d, rp)(1 - ) + (d, ru) if d  ru",0,,False
86,"(d, rp)",0,,False
87,if else,0,,False
88,"µ is the weighted arithmetic mean between the rank of the document in rp and the rank of the document in ru, with 0    1. When the same rank is assigned by µ to two different documents, which can happen in some cases for a pair of documents of which one is also in ru and the other one is not, the common document is inserted after the rpexclusive document. In other words, the original run rank has priority.",0,,False
89,"As any functional composition operator, our merging operator  is not commutative and always represents the effect of its right member on its left member.",0,,False
90,"In this context, P @n and P @n can be used to analyze the quality of an unpooled run against the pooled one. An increase in P @n is the result of two forces, one direct and one indirect: 1) direct, if the relevant documents in the top n of ru are the same documents found at the bottom of rp, they will be pushed up; 2) indirect, if the ru has non-relevant or unjudged documents in the bottom that are in the top n documents of rp, they will be pushed down. The contribution decreases if the contrary happens. As well for P @n the contribution is: 1) direct, if the non-relevant documents in the top n of ru are shared with documents in the bottom of rp; 2) indirect, if the ru has relevant or unjudged documents in the bottom that are in the top n documents of rp. If the run rp would be judged in its totality, these two effects would be perfectly correlated and it would be possible to calculate one just knowing the other from the following equation:",0,,False
91,"P @n + P @n , 0",0,,False
92,"However, when rp contains unjudged documents at ranks below n, their sum becomes -¯k@n, as shown in Eq. 1.",0,,False
93,"As explained above, ¯k@n represents the ratio of unjudged documents brought to the top n of the run rp by the run ru. Moreover, it is possible to prove that P , 0 and P ,"" 0 if and only if one of the following two conditions occurs: 1) the two runs rp and ru do not share any documents with each other in their top n documents, or 2) the two runs are identical in the top n. These are the two cases where our method will not say anything about the new run ru just by using the existing run rp (but we might based on other pooled runs).""",0,,False
94,"Let us now take an example to illustrate how this indicator could be useful to understand the behavior of a run and predict its quality. We use the test collection Robust 2005 and in particular we focus our attention on a special run that presents an unusual effect, the routing run sab05ror1. It has the peculiarity of being strongly discounted when it is not in the pool. Buckley et al. [3] studied it at length, pointing out that the reason for its behavior was related to the size of the test collection. For this run let us calculate P @10 and k¯@10. Let us also consider the average of P @10 and P @10, which we denote as follows:",1,Robust,True
95,1,0,,False
96,P @10,0,,False
97,",",0,,False
98,|Rp| 1,0,,False
99,P @10(rp,0,,False
100,rRp,0,,False
101, ru),0,,False
102,P @10,0,,False
103,",",0,,False
104,|Rp|,0,,False
105,P @10(rp,0,,False
106,rRp,0,,False
107, ru),0,,False
108,where Rp is the set of runs used in the creation of the test collection.,0,,False
109,106,0,,False
110,P @10,0,,False
111,-P @10,0,,False
112,0.02 0.05,0,,False
113,0.08 0.01,0,,False
114,0.00,0,,False
115,0.04,0,,False
116,0.00 -0.05,0,,False
117,0.00,0,,False
118,-0.01,0,,False
119,0.00 0.05 0.10 0.15 0.20,0,,False
120,0.00 0.05 0.10 0.15 0.20,0,,False
121,0.00 0.05 0.10 0.15 0.20,0,,False
122,^,0,,False
123,"Figure 1: Plot of P @10, P @10 and  against the residual (^) in a leave-one-organization-out experiment,",0,,False
124,for the Robust 2005 test collection. The run indicated as is the unusual run sab05ror1.,1,Robust,True
125,Table 1: Measures computed for the run sab05ror1 when it is not part of the pool,0,,False
126,P @10 k¯@10 P @10 P @10 0.4220 0.444 0.0065 -0.1053,0,,False
127,"Table 1 shows these values for this particular run. When the run is not part of the pool, P @10 assigns it the 11th position in 18th runs. ¯k@10 says that there are many documents that are unjudged and that therefore there is a high potential to grow. P @10 indicates a low average positive contribution to the pooled runs, and shows that among the relevant documents there is little intersection. P @10 instead is negative which suggests that many non-relevant documents have been ranked lower than before, therefore suggesting a good ability of this special run to discriminate relevant documents from non-relevant ones. In Figure 1 we show the resulted P @10, P @10 against the residual error (^, the difference between the true score and the unpooled score), generated with a leave-one-organization-out approach. Here we can observe that just using P @10 is not enough because it takes into account only one of the two positive contributions of the run, the other one being the reduction in P @10. Let us now return to the general case. When the average negative contribution of the unpooled run to other runs is reduced (i.e. P < 0) and the run has a positive contribution (i.e. P > 0), the run suffers from pool bias and its score should be adjusted. More problematic is the case when P and P have the same sign (i.e. the run has both a negative and a positive contribution, on average). Indeed, if we have P > 0 and P > 0 we would improve the P @n score of the run only if their ratio is greater then the ratio of P to P , because it means that there is a chance to improve the existing score. On the other hand, if we have P < 0 and P < 0 we would improve only if their ratio is lower then the ratio of P to P because it means that the contribution of the run is more able to discriminate the non-relevant documents. From these observations we derive a single value indicator that merges the information of all the indicators defined:",1,ad,True
128," , k¯@n(P @n · P @n - P @n · P @n)",0,,False
129,(3),0,,False
130,For all runs where  > 0 we apply our correction method. The sign of the difference in the brackets is equivalent to,0,,False
131,"the ratios discussion above. The ¯k factor has no impact on the sign (as k¯  0), but removes those special cases where",0,,False
132,Algorithm 1 Adjustment based on pooled runs,0,,False
133,ru  unpooled run,0,,False
134,Rp  set of pooled runs,0,,False
135,T  set of topics,0,,False
136,Q  qrels on T derived from Rp,0,,False
137,sru  P @n(ru) sru  P @n(ru) ¯kru  1 - (sru + sru ) for all rp  Rp do,0,,False
138,rp  rp  ru Prp  (P @n(rp) - P @n(rp)),0,,False
139,P rp  (P @n(rp) - P @n(rp)),0,,False
140,¯krp  (-P rp - P rp),0,,False
141,end for,0,,False
142,Pru,0,,False
143,1 |Rp |,0,,False
144,rpRp Prp,0,,False
145,P ru,0,,False
146,1 |Rp |,0,,False
147,rpRp P rp,0,,False
148,  ¯kru (Pru sru - P ru sru ),0,,False
149,if  > 0 then,0,,False
150,k¯ru,0,,False
151,1 |Rp |,0,,False
152,rpRp ¯krp,0,,False
153,"a  ¯kru max(¯kru , 0)",0,,False
154,else,0,,False
155,a0,0,,False
156,end if,0,,False
157,return sru + a,0,,False
158,"¯k ,"" 0, since in these cases there is no possibility to improve the score of the run (i.e. to get it closer to what we would have obtained if the run had been contributing to the pool).""",1,ad,True
159,"Returning briefly to the example of the sab05ror1 run, we can now see in Figure 1 that  clearly distinguishes this run from the rest.",0,,False
160,4. ADJUSTING SCORE FOR BIAS,0,,False
161,"Now that we have an understanding of which runs are suffering from pool bias, with respect to precision at cut-off, we proceed by presenting our method to adjust the score (Algorithm 1). As hinted at before, the method is to adjust the pool bias suffered by a system that has not been pooled by measuring the effect of the system on the pooled runs. The only information that is needed is the relevance assessments for each topic and the pooled runs, normally available for most existing test collections. As we presented earlier, P @n as calculated with the incomplete pool is a lower bound for the score of ru. To correct the pool bias we want to add a",1,ad,True
162,107,0,,False
163,"quantity that stays within its uncertainty limit ¯kru . In other words, our growth potential in terms of P @n is bounded by ¯kru . We are interested in estimating the missing precision of the unjudged documents in the run ru.",0,,False
164,"The question is then: Where in this interval do we find our correction value? In the absence of any other external information, we will take the average effect of this run ru on the existing runs, in terms of ¯k.",0,,False
165,"We do this by computing the ¯krp produced by ru on a pooled run rp via the run composition function defined in Eq. 2. This measures the aggregated change in precision and anti-precision, as described by Eq. 1. We do this for each run in the pool, and average these values. This average, denoted k¯ru , when positive, acts as a maximum likelihood estimator for our position in [0,¯kru ]. Therefore, the correction quantity is the product between ¯kru and k¯ru . The following section will therefore add ¯kru max(¯kru , 0) to the P @n for those runs with  > 0, as shown in the last seven lines of Algorithm 1.",1,ad,True
166,5. EXPERIMENTS,0,,False
167,"To test the pool bias adjustment developed in the previous section we used 15 test collections sampled from TREC: 7 test collections from the Ad Hoc track, 3 from the Web track, and 5 from more domain specific IR tracks: Genomics, Robust, Legal, Medical and Microblog. We tested the algorithm1 through a leave-one-out approach comparing our method with that of Webber and Park [23]. As the baseline we consider the traditional evaluation against the reduced pool. We call this the reduced pool to distinguish it from the ground truth pool--the one also containing documents exclusively contributed by the removed runs or organizations. We performed the leave-one-out at two different levels: 1) leave-one-run-out: as firstly described by Zobel [26], one run at a time is exited from the pool. This is done by removing all the documents uniquely introduced by it from the relevance assessments; 2) leave-one-organization-out: as introduced by Bu¨ttcher [5], it is similar to the leave-one-runout, with the difference that not only is one run removed from the pool, but also all the runs generated by the same organization. This is done by removing all the documents uniquely introduced by the organization's runs from the relevance assessments. This second approach simulates better the testing of a new run, since in most cases it has been observed that the runs produced by the same organization come from the same system, with only some parameter variation. Therefore, they often bring to the pool the same relevant documents. Finally, as in previous studies [2, 19, 20, 21, 22], to avoid buggy implementations of some of the systems that took part in the challenges, we tested again with only the top 75% of runs of each test collection.",1,ad,True
168,"The results for the leave-one-run-out, in addition to being less realistic as a model of real life, are also more conservative than those for leave-one-organization-out, such that in the following we shall discuss only the latter.",1,ad,True
169,5.1 Correction results,0,,False
170,"In these settings, we explored the different value of the weight of the merging function . In which we observe that as expected, as  goes from 1 to 0, the role of the new run on the runs in the pool decreases, to the point where, for",0,,False
171,1The software is available on the website of the first author.,0,,False
172," ,"" 0, the method no longer makes any change to the existing runs. This degree of change in  also affects the variability of P and P which decreases as well due to the lower variation between the synthetic and the pooled runs. This effect grows linearly with  and in the following we shall report the maximum effects, obtained for  "","" 1. Moreover, we tested the algorithm with different bias indicators (i.e. replacing  with P or P , as discussed in Section 3.2), thus testing the presence and absence of information about relevant or non-relevant documents in the relevance assessment as potential flags to trigger bias correction. We consistently observe lower performance compared with .""",0,,False
173,"Figure 3 shows the comparison, per test collection, of the three different approaches in the leave-one-organizationout experiment as a function of the Mean Absolute Error (MAE). As defined before in [23], the Mean Absolute Error is computed as the absolute difference between the scores of two runs, averaged over the set of topics. In addition to observing the error in the scores, it is also of interest to see how many rank reversals occur. The System Rank Error (SRE) is the sum of all the variation on rank of the system given the true rank. Figure 4 shows the SRE. For the experiments with the 75% top runs, actual values are reported in Table 2. In addition to these two measures, in Table 2, we also reported the SRE*, which only counts the variation on system rank when the difference among them is statistically significant in the ground truth (Tukey's test, p < 0.05 [9]).",1,ad,True
174,"In the table and plots we observe that our method, in a majority of cases outperforms the reduced pool and the Webber method. The last lines of Table 2 show how often each method outperformed both other methods (ties are not counted). It also shows how often it obtained the absolute worst score. It can be observed that, of the 675 tests summarized in Table 2, the proposed method obtains the worst performer mark exactly three times. On the other hand, the competing method is significantly more aggressive in its bias correction. In the majority of the cases it obtains worse system scores and rankings when compared to the simpler method of not doing anything (i.e. the reduced pool). This happens in particular in Ad Hoc 6, 7, 8 and Robust 2005. The proposed method is shown to be stable. Particularly important, it gets worse scores exactly once on SRE*, the metric measuring reversals among systems identified to be statistically significantly different. And, it happens with Medical 2011 and P @100, increasing the SRE* from 0 (for reduced pool) to 10, which reason should be found in the used shallow pool depth of 10.",1,Robust,True
175,5.2 Relation to test collection stability,0,,False
176,"The results observed in Figures 3 and 4, as well as in Table 2 lead us to question whether or not there is a connection between the effect of our method and the quality of the test collection. We therefore compare the percentage MAE change for each test collection and for each n of P @n, with the two coefficients of stability recently adapted by Urbano et al. [20] from Generalizability Theory: the Generalizability Coefficient (E2) and Dependability (). E2 measures the stability based on system variance and the relative differences between systems;  measures the stability based on system variance and the absolute effectiveness scores. To infer that a test collection is reliable, both measures must tend to 1. Figure 2 shows the relation between these two factors and the change in MAE for P @10 and P @100 for",1,ad,True
177,108,0,,False
178,MAE Percentage Change,0,,False
179,"P @10, E2 -60",0,,False
180,-40,0,,False
181,-20,0,,False
182,0,0,,False
183,"20 P @100, E2",0,,False
184,-60,0,,False
185,-40,0,,False
186,-20,0,,False
187,0,0,,False
188,20,0,,False
189,"P @10,  P @100, ",0,,False
190,Ad Hoc 2 Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Genomics 2005 Legal 2006 Medical 2011 Microblog 2011 Robust 2005 Web 2001 Web 2002 Web 9,1,blog,True
191,1.00,0,,False
192,0.75,0,,False
193,0.50 1.00,0,,False
194,0.75,0,,False
195,0.50,0,,False
196,"Figure 2: Plots of the percentage change of Mean Absolute Error for P @10 and P @100 against the coefficients of stability, Generalizability Coefficient (E2) and Dependability (). Spearman's rank correlation for P@10: E2 is 0.48 (p>0.07) and for  is 0.36 (p>0.18). For P@100: E2 is 0.17 (p>0.54) and for  is 0.09 (p>0.74).",0,,False
197,"the 15 test collections studied here. This change in MAE is calculated between our method and the traditional, reduced pool method. In general, we observe a weak correlation with E2 and  (i.e. less error for more unstable test collections). With P @10 our method has a stronger effect with more unstable test collections. An interesting case happens at P @100, where for some test collection the MAE percentage change is positive, that is resulting in a lack of correlation, with which we get more ambiguous results.",1,ad,True
198,"To understand this, it is needed to understand that when the cut-off of P @n is greater then the depth of the pool, we are essentially comparing with an uncertain ground truth, since also the large pool (the one with the runs of the organization we removed for testing) is affected by the presence of unjudged documents. This uncertainty in comparing the result when the depth of the pool is less then the considered P @n needs to be considered when looking at these results, as well as those of all other proposed methods. The depth of each test collection is available for reference in Table 2.",0,,False
199,6. CONCLUSION,0,,False
200,"The primary focus of this paper is an insight that information about the quality of an unpooled run can be obtained by observing its effect on existing, pooled runs. Such an effect is modeled by the creation of a synthetic run, obtained by merging the two runs--the pooled and the unpooled--in a very simple way, by linearly combining the ranks of each document in the old run (i.e. we do not want to add new documents to the old run, just observed how its own documents shift as a function of the information provided by the new run). The effect is measured with essentially three quantities: the change in the position of the judged relevant documents (measured via precision), the change in the position of the judged non-relevant documents (measured via anti-precision), and the change in the position of the unjudged documents (measured via a measure ¯k we define). Observing these changes across the set of pooled runs--the effect of the new run on the existing runs--we identify a",1,ad,True
201,"coefficient  whose sign allows us to decide whether a bias correction should be made or not. We then proceed with the provision of a bias correction procedure based on the above three quantities, which we show to be conservative in the sense that it never damages significant rank orders, and only very rarely affects changes in system rankings. This is opposed to previous methods which are too aggressive in the bias correction, and in so being, add another level of uncertainty to the system rankings.",1,ad,True
202,"The proposed method addresses a significant concern coming from research but also from practice: the necessity to have a reliable, yet understandable metric, which we can communicate to partners outside of our community. This last condition significantly restricts our possible choices. Precision at cut-off is by far the most easily understood quantity to communicate and with this study we have shown that we can correct pool bias when considering a run that has not participated in the creation of the pool.",1,ad,True
203,Acknowledgements,0,,False
204,This research was supported by the Austrian Science Fund (FWF) project number P25905-N23 (ADmIRE).,0,,False
205,7. REFERENCES,0,,False
206,"[1] Y. Benjamini and Y. Hochberg. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society, B57(1), 1995.",0,,False
207,"[2] D. Bodoff and P. Li. Test theory for assessing ir test collections. In Proc. of SIGIR, 2007.",0,,False
208,"[3] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Inf. Ret., 10(6), 2007.",0,,False
209,"[4] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In Proc. of SIGIR, 2004.",0,,False
210,"[5] S. Bu¨ttcher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboroff. Reliable information retrieval evaluation",0,,False
211,109,0,,False
212,"Table 2: Summary of the results per test collection generate trough a leave-one-organization-out using the top 75% of the pooled runs. With: |R| number of runs submitted, |O| number of organizations involved, |Rp| number of pooled runs, d depth of the pool and |T | number of topics.",0,,False
213,Track,1,Track,True
214,P@n,0,,False
215,MAE,0,,False
216,Ours SRE,0,,False
217,SRE*,0,,False
218,Webber MAE SRE SRE*,0,,False
219,Reduced MAE SRE,0,,False
220,Pool SRE*,0,,False
221,|R|: 38,0,,False
222,Ad,0,,False
223,Hoc,0,,False
224,2,0,,False
225,|O|: 22 |Rp|: 36 d : 100,0,,False
226,|T |: 50,0,,False
227,5 0.0063 19 10 0.0082 32 20 0.0132 50 30 0.0149 51 100 0.0216 88,0,,False
228,0 0.0069 18 0 0.0084 27 0 0.0119 48 0 0.0129 42 2 0.0169 61,0,,False
229,0 0.0065 19 0 0 0.0085 32 0 0 0.0138 52 0 0 0.0164 56 0 2 0.0293 122 4,0,,False
230,|R|: 40 5 0.0025 3 0 0.0038 3 0 0.0025 3 0,0,,False
231,Ad,0,,False
232,Hoc,0,,False
233,3,0,,False
234,|O|: 22 |Rp|: 26 d : 200,0,,False
235,10 0.0024 5 20 0.0042 5 30 0.0049 13,0,,False
236,0 0.0029 0 0 0.0041 6 0 0.0051 12,0,,False
237,0 0.0025 5 0 0.0042 5 0 0.0051 13,0,,False
238,0 0 0,0,,False
239,|T |: 50 100 0.0067 21 0 0.0071 28 0 0.0085 25 0,0,,False
240,|R|: 33 5 0.0065 17 0 0.0069 20 0 0.0073 18 0,0,,False
241,Ad,0,,False
242,Hoc,0,,False
243,4,0,,False
244,|O|: 19 |Rp|: 32 d : 100,0,,False
245,10 0.0081 23 20 0.0089 29 30 0.0089 27,0,,False
246,0 0.0081 22 0 0.0096 33 0 0.0098 31,0,,False
247,0 0.0093 25 0 0.0107 33 0 0.0117 32,0,,False
248,0 0 0,0,,False
249,|T |: 50 100 0.0084 26 0 0.0115 35 0 0.0161 52 0,0,,False
250,|R|: 61,0,,False
251,Ad,0,,False
252,Hoc,0,,False
253,5,0,,False
254,|O|: 21 |Rp|: 61 d : 100,0,,False
255,|T |: 50,0,,False
256,5 0.0067 39 10 0.0070 50 20 0.0071 59 30 0.0068 61 100 0.0048 66,0,,False
257,0 0.0073 39 0 0.0069 39 0 0 0.0072 50 0 0.0074 50 0 0 0.0077 61 0 0.0080 66 0 0 0.0080 73 0 0.0085 77 0 0 0.0086 116 0 0.0104 136 0,0,,False
258,|R|: 74,0,,False
259,Ad,0,,False
260,Hoc,0,,False
261,6,0,,False
262,|O|: 17 |Rp|: 19 d : 55,0,,False
263,|T |: 50,0,,False
264,5 0.0179 16 10 0.0224 15 20 0.0253 18 30 0.0263 20 100 0.0136 25,0,,False
265,3 0.0308 28 5 0.0234 20 5 6 0.0354 29 8 0.0283 22 11 6 0.0355 35 12 0.0336 31 12 6 0.0365 38 11 0.0389 31 11 0 0.0186 34 4 0.0288 41 4,0,,False
266,|R|: 103 5 0.0011 4 0 0.0018 4 0 0.0012 4 0,0,,False
267,Ad,0,,False
268,Hoc,0,,False
269,7,0,,False
270,|O|: 42 |Rp|: 79 d : 100,0,,False
271,10 0.0017 8 20 0.0021 13 30 0.0022 24,0,,False
272,0 0.0022 8 0 0.0027 18 0 0.0029 28,0,,False
273,0 0.0017 8 0 0.0022 13 0 0.0025 27,0,,False
274,0 0 0,0,,False
275,|T |: 50 100 0.0025 40 0 0.0038 58 0 0.0038 53 0,0,,False
276,|R|: 129 5 0.0036 11 8 0.0042 11 8 0.0038 11 8,0,,False
277,Ad,0,,False
278,Hoc,0,,False
279,8,0,,False
280,|O|: 41 |Rp|: 80 d : 100,0,,False
281,10 0.0036 7 20 0.0035 6 30 0.0035 7,0,,False
282,5 0.0043 9 1 0.0042 14 1 0.0042 10,0,,False
283,7 0.0038 9 2 0.0038 7 1 0.0038 8,0,,False
284,7 2 2,0,,False
285,|T |: 50 100 0.0030 26 6 0.0049 42 8 0.0043 38 7,0,,False
286,|R|: 104 5 0.0023 15 0 0.0031 15 0 0.0023 15 0,0,,False
287,|O|: 23 10 0.0022 17 0 0.0027 19 0 0.0025 19 0,0,,False
288,Web,0,,False
289,9,0,,False
290,|Rp|: 64 d : 100,0,,False
291,20 0.0027 25 30 0.0027 41,0,,False
292,0 0.0027 21 0 0.0026 34,0,,False
293,0 0.0030 26 0 0.0034 43,0,,False
294,0 0,0,,False
295,|T |: 50 100 0.0043 105 0 0.0036 109 1 0.0052 147 3,0,,False
296,|R|: 97 5 0.0010 5 0 0.0015 5 0 0.0010 5 0,0,,False
297,Web,0,,False
298,2001,0,,False
299,|O|: 29 |Rp|: 61 d : 100,0,,False
300,10 0.0016 9 20 0.0017 14 30 0.0018 26,0,,False
301,0 0.0021 9 0 0.0021 17 0 0.0021 15,0,,False
302,0 0.0016 9 0 0.0019 14 0 0.0022 27,0,,False
303,0 0 0,0,,False
304,|T |: 50 100 0.0040 96 0 0.0026 63 0 0.0041 87 0,0,,False
305,|R|: 69,0,,False
306,Web,0,,False
307,2002,0,,False
308,|O|: 16 |Rp|: 69 d : 50,0,,False
309,|T |: 50,0,,False
310,5 0.0038 54 10 0.0045 76 20 0.0041 78 30 0.0044 106 100 0.0039 138,0,,False
311,0 0.0038 54 0 0.0040 80 0 0.0039 78 0 0.0036 87 0 0.0024 92,0,,False
312,0 0.0038 54 0 0 0.0049 80 0 0 0.0051 95 0 0 0.0052 120 0 0 0.0038 136 0,0,,False
313,|R|: 62 5 0.0052 64 0 0.0054 69 0 0.0057 69 0,0,,False
314,Genomics,0,,False
315,2005,0,,False
316,|O|: 32 |Rp|: 58 d : 60 |T |: 49,0,,False
317,10 0.0063 111 20 0.0077 89 30 0.0084 106 100 0.0055 93,0,,False
318,0 0.0061 110 0 0.0066 80 0 0.0069 81 0 0.0049 93,0,,False
319,0 0.0073 117 0 0.0088 106 0 0.0100 139 0 0.0091 197,0,,False
320,0 0 0 0,0,,False
321,|R|: 74,0,,False
322,Robust,1,Robust,True
323,2005,0,,False
324,|O|: 17 |Rp|: 19 d : 55,0,,False
325,|T |: 50,0,,False
326,5 0.0179 16 10 0.0224 15 20 0.0253 18 30 0.0263 20 100 0.0136 25,0,,False
327,3 0.0308 28 5 0.0234 20 5 6 0.0354 29 8 0.0283 22 11 6 0.0355 35 12 0.0336 31 12 6 0.0365 38 11 0.0389 31 11 0 0.0186 34 4 0.0288 41 4,0,,False
328,|R|: 34,0,,False
329,Legal,0,,False
330,2006,0,,False
331,|O|: 8 |Rp|: 23 d : 10,0,,False
332,|T |: 39,0,,False
333,5 0.0593 80 10 0.0572 94 20 0.0306 64 30 0.0219 63 100 0.0063 54,0,,False
334,0 0.1146 135 11 0.1327 136 11 15 0.1097 138 25 0.1440 139 25 12 0.0813 128 24 0.0984 139 33 1 0.0636 117 17 0.0750 130 30 17 0.0224 101 33 0.0250 107 39,0,,False
335,|R|: 127 5 0.0267 142 0 0.0309 159 0 0.0464 261 0,0,,False
336,Medical,0,,False
337,2011,0,,False
338,|O|: 29 |Rp|: 56 d : 10,0,,False
339,10 0.0265 157 20 0.0224 152 30 0.0201 153,0,,False
340,0 0.0377 219 0 0.0209 142 0 0.0153 121,0,,False
341,1 0.0586 336 0 0.0326 206 0 0.0229 174,0,,False
342,5 2 0,0,,False
343,|T |: 34 100 0.0092 176 8 0.0054 97 0 0.0078 149 0,0,,False
344,|R|: 184 5 0.0053 101 14 0.0056 101 14 0.0053 101 14,0,,False
345,Microblog,1,blog,True
346,2011,0,,False
347,|O|: 58 |Rp|: 119 d : 30,0,,False
348,10 20 30,0,,False
349,0.0060 183 0.0065 217 0.0069 272,0,,False
350,57 0.0063 183 68 0.0065 217 77 0.0070 256,0,,False
351,57 0.0062 183 64 0.0067 217 76 0.0076 294,0,,False
352,57 68 95,0,,False
353,|T |: 49 100 0.0022 156 12 0.0022 143 13 0.0024 159 14,0,,False
354,top performer 46 35 20 18 23 2 worst performer 2 3 1 26 18 1,0,,False
355,000 44 33 12,0,,False
356,"with incomplete and biased judgements. In Proc. of SIGIR, 2007. [6] B. Carterette. Robust test collections for retrieval evaluation. In Proc. of SIGIR, 2007. [7] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In Proc. of SIGIR, 2006. [8] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over thousands of queries. In Proc. of SIGIR, 2008. [9] B. A. Carterette. Multiple testing in statistical analysis of systems-based information retrieval experiments. ACM Trans. Inf. Syst., 30(1), Mar. 2012. [10] C. L. A. Clarke and M. D. Smucker. Time well spent. In Proc. of IIiX, 2014. [11] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In Proc. of SIGIR, 1998. [12] C. Hauff and F. de Jong. Retrieval system evaluation: Automatic evaluation versus incomplete judgments. In Proc. of SIGIR, 2010. [13] K. Kuriyama, N. Kando, T. Nozue, and K. Eguchi. Pooling for a large-scale test collection: An analysis of the search results from the first NTCIR workshop. Inf. Ret., 5(1), 2002. [14] W.-H. Lin and A. Hauptmann. Revisiting the Effect of Topic Set Size on Retrieval Error. In Proc. of SIGIR, 2005. [15] A. Moffat, W. Webber, and J. Zobel. Strategic system comparisons via targeted relevance judgments. In Proc. of SIGIR, 2007. [16] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1), Dec. 2008. [17] T. Sakai. Alternatives to bpref. In Proc. of SIGIR, 2007. [18] T. Sakai and N. Kando. On information retrieval metrics designed for evaluation with incomplete relevance assessments. Inf. Ret., 11(5), 2008. [19] M. Sanderson and J. Zobel. Information retrieval system evaluation: Effort, sensitivity, and reliability. In Proc. of SIGIR, 2005. [20] J. Urbano, M. Marrero, and D. Mart´in. On the measurement of test collection reliability. In Proc. of SIGIR, 2013. [21] E. M. Voorhees. Topic set size redux. In Proc. of SIGIR, 2009. [22] E. M. Voorhees and C. Buckley. The effect of topic set size on retrieval experiment error. In Proc. of SIGIR, 2002. [23] W. Webber and L. A. F. Park. Score adjustment for correction of pooling bias. In Proc. of SIGIR, 2009. [24] E. Yilmaz and J. A. Aslam. Estimating average precision with incomplete and imperfect judgments. In Proc. of SIGIR, 2006. [25] E. Yilmaz, E. Kanoulas, and J. A. Aslam. A simple and efficient sampling method for estimating AP and NDCG. In Proc. of SIGIR, 2008. [26] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In Proc. of SIGIR, 1998.",1,Robust,True
357,110,0,,False
358,0.030 0.025 0.020 0.015 0.010 0.005 0.010,0,,False
359,0.008,0,,False
360,0.006,0,,False
361,Ad Hoc 2 Ours Reduced Pool Webber,0,,False
362,Ad Hoc 5,0,,False
363,0.0050 0.0045 0.0040 0.0035 0.0030 0.0025,0,,False
364,0.005,0,,False
365,0.004,0,,False
366,0.003,0,,False
367,Ad Hoc 8 Web 2002,0,,False
368,Legal 2006 0.15,0,,False
369,0.008 0.006 0.004 0.002 0.012 0.010 0.008 0.006 0.004 0.005 0.004 0.003 0.002,0,,False
370,0.010 0.008 0.006 0.004,0,,False
371,0.06,0,,False
372,Ad Hoc 3 Ad Hoc 6,0,,False
373,Web 9 Genomics 2005 Medical 2011,0,,False
374,0.0150 0.0125 0.0100 0.0075 0.0050,0,,False
375,0.003 0.002 0.001 0.004 0.003 0.002 0.001,0,,False
376,0.04,0,,False
377,0.03,0,,False
378,0.02,0,,False
379,Ad Hoc 4 Ad Hoc 7 Web 2001 Robust 2005 Microblog 2011,1,Robust,True
380,M AE,0,,False
381,0.10,0,,False
382,0.04,0,,False
383,0.006,0,,False
384,0.004,0,,False
385,0.05,0,,False
386,0.02,0,,False
387,0.002 0.00,0,,False
388,5 10 20 30,0,,False
389,100,0,,False
390,5 10 20 30,0,,False
391,100,0,,False
392,P @n,0,,False
393,5 10 20 30,0,,False
394,100,0,,False
395,"Figure 3: Plots per test collection of the Mean Absolute Error against the P @n of the Reduced Pool and the two approaches, Ours and Webber, to correct pool bias. Generated using a leave-one-organization-out,",0,,False
396,using all the runs for the continuous lines and only the top 75% for the dashed lines. Our approach uses as,0,,False
397,"indicator  and  , 1.",0,,False
398,111,0,,False
399,Ad Hoc 2 125,0,,False
400,Ours,0,,False
401,Ad Hoc 3,0,,False
402,Ad Hoc 4,0,,False
403,50,0,,False
404,100,0,,False
405,Reduced Pool,0,,False
406,20,0,,False
407,40,0,,False
408,75,0,,False
409,Webber,0,,False
410,50,0,,False
411,10,0,,False
412,30,0,,False
413,25,0,,False
414,20,0,,False
415,0,0,,False
416,Ad Hoc 5,0,,False
417,Ad Hoc 6,0,,False
418,Ad Hoc 7,0,,False
419,150,0,,False
420,25,0,,False
421,60,0,,False
422,120,0,,False
423,20,0,,False
424,90,0,,False
425,15,0,,False
426,10 60,0,,False
427,5,0,,False
428,Ad Hoc 8,0,,False
429,160 40,0,,False
430,120 30,0,,False
431,80 20,0,,False
432,Web 9,0,,False
433,40,0,,False
434,20,0,,False
435,Web 2001 100,0,,False
436,75 50,0,,False
437,SRE,0,,False
438,10,0,,False
439,40,0,,False
440,25,0,,False
441,Web 2002,0,,False
442,0,0,,False
443,Genomics 2005 200,0,,False
444,40,0,,False
445,Robust 2005,1,Robust,True
446,150,0,,False
447,35,0,,False
448,150,0,,False
449,30,0,,False
450,100,0,,False
451,100,0,,False
452,25,0,,False
453,20,0,,False
454,50 Legal 2006,0,,False
455,15,0,,False
456,Medical 2011,0,,False
457,Microblog 2011,1,blog,True
458,300,0,,False
459,200,0,,False
460,300,0,,False
461,250,0,,False
462,150,0,,False
463,200,0,,False
464,200,0,,False
465,100,0,,False
466,150,0,,False
467,50,0,,False
468,100,0,,False
469,100,0,,False
470,5 10 20 30,0,,False
471,100,0,,False
472,5 10 20 30,0,,False
473,100,0,,False
474,5 10 20 30,0,,False
475,100,0,,False
476,P @n,0,,False
477,"Figure 4: Plots per test collection of the System Rank Error against the P @n of the Reduced Pool and the two approaches, Ours and Webber, to correct pool bias. Generated using a leave-one-organization-out, using all the runs for the continuous lines and only the top 75% for the dashed lines. Our approach uses as indicator  and  , 1.",0,,False
478,112,0,,False
479,,0,,False

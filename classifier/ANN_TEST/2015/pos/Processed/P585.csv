,sentence,label,data,regex
0,A Probabilistic Model for Information Retrieval Based on Maximum Value Distribution,0,,False
1,Jiaul H. Paik,0,,False
2,"University of Maryland, College Park, USA",0,,False
3,jia.paik@gmail.com,0,,False
4,ABSTRACT,0,,False
5,"The main goal of a retrieval model is to measure the degree of relevance of a document with respect to the given query. Probabilistic models are widely used to measure the likelihood of relevance of a document by combining within document term frequency and term specificity in a formal way. Recent research shows that tf normalization that factors in multiple aspects of term salience is an effective scheme. However, existing models do not fully utilize these tf normalization components in a principled way. Moreover, most state of the art models ignore the distribution of a term in the part of the collection that contains the term. In this article, we introduce a new probabilistic model of ranking that addresses the above issues. We argue that, since the relevance of a document increases with the frequency of the query term, this assumption can be used to measure the likelihood that the normalized frequency of a term in a particular document will be maximum with respect to its distribution in the elite set. Thus, the weight of a term in a document is proportional to the probability that the normalized frequency of that term is maximum under the hypothesis that the frequencies are generated randomly. To that end, we introduce a ranking function based on maximum value distribution that uses two aspects of tf normalization. The merit of the proposed model is demonstrated on a number of recent large web collections. Results show that the proposed model outperforms the state of the art models by significantly large margin.",1,ad,True
6,Categories and Subject Descriptors,0,,False
7,H.3.3 [Information Systems]: Information Search and Retrieval: Retrieval Models,0,,False
8,General Terms,0,,False
9,Algorithm; Experimentation; Performance,0,,False
10,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",1,ad,True
11,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,0,,False
12,DOI: http://dx.doi.org/10.1145/2766462.2767762.,0,,False
13,Keywords,0,,False
14,Document ranking; Retrieval model; Extreme value theory,0,,False
15,1. INTRODUCTION,1,DUC,True
16,"To measure the weight of a term in a document, most well known functions combine three major components the term frequency, the inverse document frequency and the document length. The term frequency factor is a key evidence for determining a term's salience in a document, while inverse document frequency is used for attenuating the effect of terms that occur too often in the collection to be meaningful for relevance determination. On the other hand, term frequency is closely related with document length, since long documents tend to use a term repeatedly. Thus, term frequency normalization, in accordance with the document length, is necessary to remove the advantage that the long documents have in retrieval over the short documents.",1,ad,True
17,"Given these three major components, the key question is then how these components can be integrated to produce a composite weight for each query term in each document and that is where one model differs from the other. Most well known weighting functions under the vector space model compute the composite weight by taking the product of the tf factor and the idf factor, where the tf factor is some combination of tf and the document length. Classical probabilistic models (for example BM25 [24]), adopt somewhat the same strategy. Although, they have the same objective, the two models have very different ways of determining the functional form of the tf factor. The nature of the tf functions under the vector space framework are generally constructed empirically, which are primarily guided by the experimental results, while BM25 formula is derived by approximating the logarithm of odds ratio of two Poisson distributions- one for relevant documents and the other for non-relevant documents. On the other hand, language models (LM) [21] differ from the above models in a fundamental way in the sense that the documents are ranked based on the likelihood that the query has been generated from the document in consideration. In addition, unlike tf.idf models, language models do not use explicit length normalization. The length of the document is an integral part of the probability estimation. Non-parametric probabilistic models are also known to be very effective in information retrieval. One of the widely used non-parametric probabilistic model is divergence from randomness (DFR) [1] based approaches, where the term weight is computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution. One major deficiency with",1,ad,True
18,585,0,,False
19,"these models is that they consider only the document length normalized tf and ignore within document relative tf distributions. Recent research [20] shows that integration of within document relative tf into scoring model improves performance significantly. However, it is yet not clear how this variable can be added into the existing models formally.",1,ad,True
20,"This article describes a probabilistic retrieval model that obviates empirical way of determining a ranking function, unlike existing tf-idf models [29, 20]. The model introduces a tf factor based on the distribution of maximum values of normalized tf. The model achieves a number of important goals. First, it integrates the recent multi-aspect tf normalization schemes into a probabilistic framework. Second, the model automatically factors in the distribution of normalized tf in a term specific way, unlike many standard models. Third, it uses a mixture of two maximum value distribution to better model distributions of terms having varying heaviness of tails. To the best of our knowledge, this work is the first to address the ranking problem using the distribution of maximum values.",1,ad,True
21,"The effectiveness of the proposed model is evaluated on a number of recent web test collections containing millions of documents. We compare the performance of the proposed method to the state of the art representative baselines from tf-idf model, classical probabilistic model, language model and divergence from randomness model. Our primary experimental results show that the proposed model almost always outperforms the state of the art baselines by a significantly large margin. We carry out additional set of experiments to compare the performance of the proposed model against a log-logistic (LL) based model that uses multi-aspect tf normalization. Once again, the results suggest that the proposed model is often significantly better than LL model. Moreover, the results demonstrate that our model is more precise than the state of the art models, thereby making it a potential choice for web search.",1,ad,True
22,"We organize the article as follows. Section 2 reviews the state of the art. The proposed approach is described in Section 3. The experimental setup is detailed in Section 4. In Section 5, we present the experimental results. Finally, we conclude in Section 6.",0,,False
23,2. PRIOR WORK,0,,False
24,"Modeling term weight is the central issue in an information retrieval system. Three widely used models in IR are the probabilistic models [23], the vector space model [28, 27], and the inference network based model [31]. Furthermore, probabilistic models can be broadly classified into three groups, namely the classical probabilistic model, language model and a non-parametric divergence from randomness model. A large number of instances of these models exist in the literature. In this section we mainly review the state of the art representatives from each of these categories.",1,ad,True
25,2.1 Classical Probabilistic Model,0,,False
26,"The key part of the probabilistic models is to estimate the probability of relevance of the documents for a query. This is where most probabilistic models differ from one another. Since the introduction of full text search, a large number of weighting formulae have been developed that attempt to measure document relevance probabilistically and BM25 [22] seems to be the most effective weighting function from among them. BM25 model approximates the two Pois-",0,,False
27,"son model of relevance. The approximation is done using a increasing asymptotic tf function. Although, structurally, BM25 and tf-idf functions are very similar (in the sense that they both use tf and idf factor), they differ in many respects. First, BM25 has a well grounded theory, while most of the tf-idf models have an empirical background. Second, anatomically, IDF factor of BM25 discounts the collection size by the document frequency of the term, which is different from the standard IDF factor. Third, BM25 uses a different query term frequency function, unlike tf-idf models where that function is linear. The length normalization factor uses the average document length and a parameter has been introduced to control the relative length effect.",0,,False
28,2.2 Language Model,0,,False
29,"Probabilistic language modeling approaches [21, 15] follow a different principle in estimating the relevance of a document, unlike classical probabilistic models. Typically, language modeling approaches compute the probability of generating a query from a document, assuming that the query terms are chosen independently. Unlike TF-IDF models, language modeling approaches do not explicitly use document length factor and the idf component. It seems that the length of the document is an integral part of this formula and that automatically takes care of the length normalization issue. However, smoothing is crucial and it has very similar effect as the parameter that controls the length normalization factor and term specificity in pivoted normalization or BM25 model. Three major smoothing techniques (Dirichlet, Jelinek-Mercer and Two-stage) are commonly used in this model [32].",0,,False
30,"Although, query likelihood model is reasonably effective, one major deficiency with using a multinomial distribution as a language model is that all term occurrences are treated independently. The term-independence assumption in information retrieval is often adopted in theory and practice, as it renders the retrieval problem tractable. It is well known that once a term occurs in a document, it is more likely to reappear in the same document. This phenomenon is known as word burstiness [18] and is a type of dependency that is not modelled in the multinomial language model. Cummins et al. [8] present a Smoothed Polya Urn Document language model, which incorporates word burstiness only into the document model. They use the Dirichlet compound multinomial (DCM) to model documents in place of the standard multinomial distribution, whereas the standard multinomial is used to model query generation.",1,ad,True
31,2.3 Divergence from Randomness Model,0,,False
32,Amati and Rijsbergen [1] proposed a class of non-parametric probabilistic approaches to term weighting called divergence from randomness (DFR) model. The weight of a term in DFR models is the amount of divergence between a term distribution produced by a random process and the actual term distribution. The anatomy of the weighting function of DFR is defined as follows,0,,False
33,"w(t, d) , -log2(P rob1) · (1 - P rob2).",0,,False
34,(1),0,,False
35,"The left factor measures the information content of the term in a document based on its distribution in the entire collection, while the right factor measures the information gain of the term with respect to its occurrence in the elite set (set of documents that contains the term). P rob1 is com-",0,,False
36,586,0,,False
37,"puted using various well known distributions (such as BoseEinstein statistics, Poisson distributions etc), while P rob2 is measured using Laplace law of succession or the ratio of two Binomial distributions. Like other models, DFR models use the same basic components. However, the integration of various component are derived theoretically. DFR models use explicit length normalization and following standard practice, average document length is considered as the ideal document length.",0,,False
38,2.4 Vector Space Models,0,,False
39,"In vector space model, the search problem is viewed in a different way. Queries and documents are represented as the vectors of terms. To compute a score between a document and a query, the model measures the similarity between the query and document vector using cosine function. The central part of the vector space model is to determine the weight of the terms that are present in the query and the documents. Salton and Buckley [26] summarize a number of term weighting approaches which use various types of normalization. It is evident that document length is an important component in effective term weighting. Singhal et al. [29] identify a number of weaknesses of cosine and maximum tf normalization and they observe that a weighting formula that retrieves documents with chances similar to their probability of relevance performs better. Following this observation, they propose a pivoted normalization scheme that acts as a correction factor of old normalization and is one of the most effective term weighting schemes in the vector space framework. Typically, the term weighting functions in vector space model are constructed empirically. Several work tried to go beyond purely empirical approaches and use the data instead to learn the patterns that satisfy the data. For example, Greiff [12] uses exploratory data analysis to uncover some important relationship between the document frequency and the relevance of a document.",1,ad,True
40,"Most of the earlier work on vector space model normalizes the term frequency in accordance with the length of the documents. Paik [20] argued that the length based normalization alone is not sufficient to capture the different aspects of term salience and that within document distribution of the terms plays an important role. He then proposed a two-aspect normalization scheme. An asymptotic bounded increasing function (much in spirit with BM tf function) is then used to transform the normalized tf values. Two tf components are then combined using query length information. However, the main weakness of the model is its highly empirical nature and that is where the model proposed in this article differs from [20]. The proposed model has a formal probabilistic foundation that directly produces the weighting function.",0,,False
41,2.5 Other Models,0,,False
42,"In inference network, document retrieval is modeled as an inference process [31]. A document instantiates a term with a certain strength and given a query, the credit from multiple terms is accumulated to compute a relevance, which is very much equivalent to the similarity score of vector space model. From an operational angle, the strength of instantiation of a term for a document can be considered as weight of the term in a document. The strength of instantiation of a term can be computed using any reasonable formula.",0,,False
43,"Some models go beyond the use of bag of words features only and incorporates the proximity/phrases of query terms in the documents [6, 9]. Metzler and Croft [19] develop a general formal framework for modeling term dependencies via Markov Random Fields. The model allows arbitrary text features, such as occurrence of single term, ordered phrases and unordered phrases to be incorporated as the potential evidences of relevance. They explore full independence (bag of words) , full dependence (between every pair of query terms) and sequential dependence (between consecutive query terms) in the language modeling framework. Since, the model has to compute the positional information during query processing time, it is more computationally complex than our model.",1,corpora,True
44,"Fang et al. [10] give a comprehensive analysis of four retrieval models by defining a set of constraints that needs to be satisfied for effective retrieval. Using these constraints the strengths and weaknesses of some well known models are analyzed and some of the models are modified. There are also a number of recent works that focus on the constraint based analysis of the retrieval models [4, 7].",0,,False
45,3. PROPOSED WORK,0,,False
46,"In this section we describe the proposed ranking model. We first revisit the key variables used in a typical ranking model and describe the roles they play. We then describe how maximum value can be used for ranking. Finally, we turn on to present the maximum value based models and their parameter estimation.",0,,False
47,3.1 TF-IDF Model: A Probabilistic View,0,,False
48,"Within document Term frequency and inverse document frequency (idf) are the two main building blocks of information retrieval models that measure query-document similarity. These two variables play a complementary role in ranking documents in response to a query. The idf factor of a term t (idf (t)) measures the information gain of randomly picking a document that will fall in the elite set for t (the set of document that contains t and henceforth we denote it as E(t)). On the other hand, tf factor of t for a document d, (tf f (t, d)) measures the relative weights of documents within E(t). Thus, from an operational perspective, idf (t) balances the weight between different E(t), while tf f (t, d) adjusts the relative weights of documents within the same elite set. Term frequency hypothesis suggests that tf f (t, d) is an increasing function of normalized term frequency. Intuitively, this means, if the rank of a document d having ntf (t, d) (normalized tf of t in d) is relatively high in E(t), the contribution made by tf f (t, d) is also high. Hence, given the distribution of normalized tf of a term in E(t), a natural way to measure tf f (t, d) is to take the percentage of documents in E(t) having normalized tf not higher than ntf (t, d). Thus, tf f (t, d) can be defined as follows:",1,ad,True
49,"tf f (t, d)  P (X  ntf (t, d))",0,,False
50,(2),0,,False
51,where X is the random variable on normalized tf values in E(t).,0,,False
52,"Lv and Zhai [17] argued that straightforward non-parametric (plain percentile based) way of estimating this probability does not fully factor in the main objective of tf hypothesis, since it ignores the quantum of differences of normalized tf values. Thus, they advocate the use of parametric probability distribution functions to circumvent this limitation.",1,ad,True
53,587,0,,False
54,"They use log-logistic distribution for computing tf f (t, d) as follows",0,,False
55,"tf f (t, d) ,"" P (X  ntf (t, d)|c, ) "","" F (ntf (t, d)|c, ) ntf (t, d)""",0,,False
56,","" c + ntf (t, d) (3)""",0,,False
57,"where c > 0 and  > 0 are the model parameters which can be estimated from the normalized tf values in E(t). The main issue in this approach is to choose the right distribution function that captures the distribution of normalized values properly. We use maximum value distribution of two aspect normalized tf values in the above framework to measure the tf f (t, d). In the next two sections, we describe multi-aspect tf normalization scheme followed by the maximum value based model.",0,,False
58,3.2 Term Frequency Normalization,0,,False
59,"Raw term frequencies are known to be less effective because of its correlation with the document length. Thus, a long document enjoys preference over a short document if the term frequency is used as is. A document becomes longer if it contains many unrelated contents together. Therefore, although the frequency of a term may not increase in this case, the document uses many distinct terms. Since, the chance of a random match of a term between a query and a document is approximately proportional to the number of distinct terms in the document, long documents get an additional advantage over shorter documents. On the other hand, documents also become longer if they repeat the same content, thereby resulting in higher term frequencies without giving any additional useful information.",1,ad,True
60,"Therefore, to enhance retrieval accuracy, it is imperative to regularize the term frequency in accordance with the document length. A standard and successful approach for doing this is to compare the length of the concerned document to the length of an ideal document (pivotal document). Both, pivoted tf-idf and BM25 effectively use this strategy where the length of the pivotal document is the average document length of the retrieval collection. Thus, the tf of an average length document remain unchanged, while tf of the documents longer (shorter) than average length document are punished (rewarded).",0,,False
61,"Recently, Paik [20] argued that the traditional length based normalization alone is not sufficient to capture the different aspects of term importance and proposed two normalization formulae- one is based on within document average term frequency, while the other makes use of the traditional length based approach. These two normalized tf s are then combined. We use the same normalization schemes as described in [20], since it gives state of the art results. For convenience, the normalization factors are called ritf (t, d) (relative intra-document frequency of term t in the document d) and lrtf (t, d) (length normalized frequency of term t in the document d). The following equations formally define the normalization schemes.",1,ad,True
62,"ritf (t, d) ,"" log(1 + tf (t, d))""",0,,False
63,(4),0,,False
64,log(k + mtf (d)),0,,False
65,adl,1,ad,True
66,"lrtf (t, d) ,"" tf (t, d) log(1 + )""",0,,False
67,(5),0,,False
68,l(d),0,,False
69,"The terms mtf , adl and l(d) denote the mean term frequency of the document that contains t, the average docu-",1,ad,True
70,"ment length of the collection and the length of the document d, and k ( 1) is a smoothing parameter. The proposed model combines these frequency normalizations in a probabilistic framework.",0,,False
71,3.3 Limitations of Existing Models,0,,False
72,In the last section we have described multi-aspect tf nor-,0,,False
73,malization scheme. In this section we discuss the potential,0,,False
74,limitations of existing methods and the major difficulties in,0,,False
75,integrating multi-aspect tf normalization into the state of,0,,False
76,the art probabilistic models.,0,,False
77,We start our discussion with the MATF model. We reiter-,0,,False
78,"ate that, although, idf function does not vary much from one",0,,False
79,"model to the other, it is the tf function that often makes",0,,False
80,the main difference.,0,,False
81,In,0,,False
82,"[20],",0,,False
83,the,0,,False
84,function,0,,False
85,x 1+x,0,,False
86,is,0,,False
87,used,0,,False
88,to,0,,False
89,transform the normalized tf values to enforce term cover-,0,,False
90,"age. However, the function has a number of notable short-",0,,False
91,"comings. First, the choice of the function is purely empirical",0,,False
92,"in nature. Second, the function does not have the knowl-",0,,False
93,"edge of the distribution of tf in the elite set. Third, since",0,,False
94,the function operates on the tf values having incompatible,0,,False
95,"range (range of ritf is much smaller than that of lrtf ), one",0,,False
96,"component overpowers the other component, thereby com-",0,,False
97,promising the ultimate effectiveness.,0,,False
98,BM25 model is a nice bridge between tf.idf and probabilis-,0,,False
99,"tic model. Anatomically, BM25 is clearly separable into tf",0,,False
100,"and idf component, where the tf function is a special case of",0,,False
101,log-logistic model and is guided by 2-Poisson model. BM25,0,,False
102,normalizes tf in accordance with the document length where,0,,False
103,average document length is used as an ideal (or pivotal) doc-,0,,False
104,"ument. However, it is not clear how to integrate relative",0,,False
105,"intra-document tf into this model, since the notion of pivot",0,,False
106,"for relative intra-document tf is hard to define. Moreover,",0,,False
107,BM25's tf function is also distribution independent.,0,,False
108,"Unlike the previous two models, divergence from random-",0,,False
109,ness model (DFR) takes a more principled approach in terms,0,,False
110,"of factoring in the term distribution. Once again, it is yet",0,,False
111,unknown how relative intra document tf (ritf (t)) can be,0,,False
112,added to this model that will be theoretically consistent with,1,ad,True
113,"DFR's basic principle. Moreover, normalized tf values are",0,,False
114,"continuous valued random variable and thus, an attempt to",0,,False
115,"integrate it into DRF will give rise to theoretical anomaly,",0,,False
116,since DFR uses discrete distributions to measure informa-,0,,False
117,tion gain.,0,,False
118,Language model is very different from all the models dis-,0,,False
119,cussed above primarily because it neither uses idf explicitly,0,,False
120,nor it uses length normalization. Thus we confine our dis-,0,,False
121,cussion on the models that have explicit tf and idf factors.,0,,False
122,In the next section we describe the maximum value based,0,,False
123,model and how it can be used to circumvent some of the,0,,False
124,"problems outlined above, followed by the development of a",0,,False
125,model that uses two aspect tf normalization in a probabilis-,0,,False
126,tic framework.,0,,False
127,3.4 Maximum Value Model,0,,False
128,"Unlike existing ranking models, we attempt to measure tf f (t, d) based on the nature of some of the largest values of normalized tf for that term. A natural consequence of using maximum value based ranking is that it makes the weight of a term in a document dependent upon the distribution of normalized tfs in E(t).",0,,False
129,"To that direction, the simplest possible approach could be to take the maximum value of normalized tf for a term t",0,,False
130,588,0,,False
131,"and then measure tf f (t, d) relative to the maximum value. Clearly, this scoring is perfectly consistent with tf hypothesis, where the document having highest normalized tf gets highest weight. We can easily think of two naive approaches to measure tf f (t, d) that are based on maximum values. One potentially feasible approach can be percentile based scoring that we have outlined before, while the other simple approach can be to measure tf f (t, d) as a ratio of ntf (t, d) (or some increasing function of ratio) and the maximum normalized tf for that term in the collection. To understand the limitations of these two approaches, let us consider the following examples.",0,,False
132,"Let x1, x2, . . . , xn-1, xn be the normalized tf values for a term t in ascending order. As our first case, let us assume that (xn - xn-1)  0. The percentile based method may give higher weight for xn compared to xn-1 even if they are nearly the same. This happens because percentile based method does not factor in the magnitude of difference, which consequently violates the tf hypothesis. As a second case, if it happens that xn xn-1, scoring based on ratio gives too much priority on the maximum value alone, which results in sharp discount of scores of other documents. As a consequence, a document even if genuinely relevant, is undesirably punished.",0,,False
133,"These problems are addressed using a sampling based technique which exclusively focuses on maximum values of samples. Rather than relying on a single value, we attempt to measure the distribution of values at the right tail where some of the largest values fall. Hence, our main goal is to model the nature of the right tail of ntf (t, d). We hypothesize that the most potentially relevant documents for a term fall on that part of the distribution. Quite clearly, this hypothesis is consistent with the standard tf hypothesis. Thus, the main challenge is to model the nature of the right most tail as accurately as possible. In other words, this model measures the likelihood that ntf (t, d) will fall on the right most tail. Thus, if the probability is higher, likelihood of d being relevant will also be higher.",1,ad,True
134,"We now focus on the models for maximum values. We reiterate that in order to avoid the influence of a single quantity (maximum value), the following sampling based approach is taken to derive maximum value distributions. Let us assume that N samples, each of size n are drawn from the same population. From each sample we can get the largest value. Thus in nN observations we have N largest values corresponding to each random sample. The distribution of the largest values in nN observations will tend to follow the same asymptotic expression as the distribution of the largest value in samples of size n. Consequently, the asymptote must be such that the largest value of a sample of size n taken from it must have the same asymptotic distribution. Formally, the maximum value distribution is defined as follows. Let X1, X2, . . . , Xn be independent and identically distributed random variable with distribution F . Let Mn ,"" max(X1, X2, . . . Xn). Then,""",0,,False
135,"P r(Mn  x) ,"" P r(X1  x, X2  x . . . Xn  x) (6)""",0,,False
136,", F n(x)",0,,False
137,(7),0,,False
138,"Since a linear transformation does not change the form of the distribution, the probability that the largest value is less than x should be equal to the probability of a linear function",0,,False
139,prob,0,,False
140,0.6 mitchell travel,0,,False
141,0.5,0,,False
142,0.4,0,,False
143,0.3,0,,False
144,0.2,0,,False
145,0.1,0,,False
146,0,0,,False
147,1,0,,False
148,2,0,,False
149,3,0,,False
150,4,0,,False
151,5,0,,False
152,6,0,,False
153,7,0,,False
154,8,0,,False
155,9,0,,False
156,10,0,,False
157,normalized tf,0,,False
158,Figure 1: Distributions of random samples of normalized elite set term frequency of mitchell and travel.,0,,False
159,"of x. Thus, the above equation is equivalent to",0,,False
160,P r( Mn - bn an,0,,False
161," x) , F n(anx + bn).",0,,False
162,(8),0,,False
163,"Fisher-Tippett-Gnedenko theorem [11] states that if a pairs of real numbers (an, bn) (an and bn must be functions of n) exist such that an > 0 and",0,,False
164,lim F n(anx + bn)  D(x),0,,False
165,(9),0,,False
166,n,0,,False
167,"for a distribution F , then D(x) can be Type I or Type II distribution defined below.",0,,False
168,The type I distribution [13] (known as Gumbel distribution) is defined as,0,,False
169,x-µ,0,,False
170,"Fg(x) ,"" exp(- exp(-  )), µ  R;  > 0.""",0,,False
171,(10),0,,False
172,while type II distribution [13] (Frechet distribution) for pos-,0,,False
173,itive random variable is defined as,0,,False
174,µ,0,,False
175,"Ff (x) ,"" exp(- x ), x  0; µ > 0;  > 0.""",0,,False
176,(11),0,,False
177,"Having defined the maximum value distribution, our next major goal is to verify that the maximum value distributions satisfy the mandatory preconditions in order to be applicable in our task. Specifically, the data must be coming from a distribution F that satisfies Fisher-Tippett-Gnedenko theorem. Thus, our primary goal is to fix the underlying distribution function from which the data have been supposedly generated. In order to guess F , we first examine the distributions of normalized frequencies for a few randomly chosen terms. We noticed that the density graphs near the extreme right tail are not monotonically decreasing and it happens primarily because of the presence of random noise or extreme outliers. We empirically (by plotting) identify the points at which the density graphs violate this smoothness for the first time and ignore all the data larger than this particular point. On Clueweb collections, our analysis suggests that normalized tf values between 70-80 seem to be",1,Clue,True
178,589,0,,False
179,"a reasonable cut-off point and thus, in our experiments we set it to 75 empirically (but that value may depend on the nature of the collection). We then plot the distributions of the truncated data. As an example, Figure 1 shows distributions for two selected terms. To better understand the relationship between the pattern of distributions and term's collection level occurrence, we choose two terms (""mitchell"" and ""travel"") of varying specificity. Figure 1 clearly shows that both the terms seem to be following long tail distributions with monotonically decreasing density functions. We consider two such long tail distributions ­ namely, exponential distribution and Pareto distribution. Note that the nature of the tails are different in these two cases.",0,,False
180,Case 1.,0,,False
181,"Suppose the data have been distributed from exponential distribution. Thus, F (x) ,"" 1 - exp(-x/),  > 0. If we choose an "", 1 and bn ,"" ln n, Then""",0,,False
182,"F n(anx + bn) ,",0,,False
183,-x - ln n n,0,,False
184,1 - exp(-,0,,False
185,),0,,False
186,(12),0,,False
187,exp(-x/) n,0,,False
188,", lim 1 -",0,,False
189,(13),0,,False
190,n,0,,False
191,n,0,,False
192,", exp(- exp(-x/)).",0,,False
193,(14),0,,False
194,"Thus, if the data is generated from exponential distribution, for an , 1 and bn ,"" ln n, maximum value distribution converges to Gumbel distribution.""",0,,False
195,Case 2.,0,,False
196,"Suppose now the data have Pareto tail. Thus, 1 - F (x) ,"" cx- as x  , with c > 0 and  > 0. Again if we set""",0,,False
197,1,0,,False
198,"an , n  and bn ,"" 0, then for x > 0 we have""",0,,False
199,"F n(anx) , 1 - c(anx)- n",0,,False
200,(15),0,,False
201,x- n,0,,False
202,", lim 1 - c",0,,False
203,n,0,,False
204,n,0,,False
205,(16),0,,False
206,", exp(-( µ )) (setting c , µ) (17) x",0,,False
207,"which turned out to be Frechet distribution. Hence, the above results provide us the necessary evidence that the maximum value distributions can be applied on our data.",0,,False
208,Mixture Model.,0,,False
209,"Although, Fg and Ff are the asymptotic approximations to maximum value models, the shapes of their distributions are not identical. Frechet distribution has longer right tail (Pareto tail) than Gumbel. This has some interesting corelation with the distribution of term frequencies in a large collection. If a term is more general (but not really stopwords), the frequency distribution for that term likely to have a longer tail than that of more specific term. Figure 1 illustrates this point clearly: the density curve of ""mitchell "" (which is a rare term) touches the x-axis much before that of ""travel "" (which is a more general term). Thus, an attempt to model the distribution of a term using only one of Gumbel and Frechet may lead to lower accuracy. Any real query contains terms having varying collection frequency and this motivates us to use a weighted mixture of the two distributions. Thus, our resulting distribution is defined as",1,ad,True
210,"G(x) ,"" p · Fg(x) + (1 - p) · Ff (x), 0 < p < 1 (18)""",0,,False
211,"where p can be considered as prior of Fg(x). A straightforward way to estimate p is to use a standard method such as gradient ascent method that directly optimizes a target retrieval metric (such as NDCG@20). Indeed, we adopt such an approach, but not directly on p. As we have discussed earlier, Fg (Gumbel) distribution seems better in modeling the distribution of a term having relatively smaller df values (more specific). Thus, instead of optimizing the value of p independently, we make the value of p dependent on df . Specifically, if a term has low df (high idf ) we give higher weight to Fg(x). In other words, p should be higher for high idf terms. We formalize this intuition using the following well known linear model",1,ad,True
212,"p ,  · idf",0,,False
213,(19),0,,False
214,1-p,0,,False
215,which gives the following solution for p,0,,False
216,"p ,  · idf .",0,,False
217,(20),0,,False
218,1 +  · idf,0,,False
219,where  (> 0) is a free parameter.,0,,False
220,3.5 Scoring Function,0,,False
221,"We are now ready to define our final scoring function. Our scoring function uses two aspect tf normalization in maximum value distribution framework. Formally, if X and Y be the random variables corresponding to ritf (t) and lrtf (t) in E(t) respectively, then tf f (t, d) is defined as",1,ad,True
222,"tf f (t, d) ,"" ·P (X  ritf (t, d))+(1-)·P (Y  lrtf (t, d)) "",""  · G(ritf (t, d)) + (1 - ) · G(lrtf (t, d)) (21)""",0,,False
223,"where 0 <  < 1, is the interpolation parameter. Consequently, the final scoring function for a query Q , q1q2 . . . qn and a document d is defined as",0,,False
224,"S(Q, d) ,"" tf f (t, q) · idf (q)""",0,,False
225,(22),0,,False
226,qQ,0,,False
227,"where idf (t) ,"" log(N/df (t)). The parameter   (0, 1) in Equation 21 is set empirically.""",0,,False
228,3.6 Model Parameter Estimation,0,,False
229,"In this section we detail our method for estimating the parameters of the two maximum value distribution models described in the last section. These parameters play important role in determining the actual shape of the distributions which in turn make them term dependent. There are many methods for parameter estimation including maximum likelihood estimation (MLE), which perhaps is an obvious choice. However, in our case, MLE does not seem to be a good choice for the reason we detail next.",0,,False
230,"We explain the difficulty with Gumbel distribution only (similar argument holds for Frechet). The log-likelihood function of Gumbel based on random sample x1, x2, . . . , xn is given by",0,,False
231,"L(, µ) , - n xi - µ - n ln  - n exp(- xi - µ ). (23)",0,,False
232,"i,1",0,,False
233,"i,1",0,,False
234,The system of differential equations (used for MLE),0,,False
235,"L , L , 0",0,,False
236,(24),0,,False
237,µ ,0,,False
238,590,0,,False
239,yields the following estimates for µ and ,0,,False
240,"µ , (ln n - ln n exp(- xi ))",0,,False
241,(25),0,,False
242,"i,1",0,,False
243,and,0,,False
244,n,0,,False
245,xi,0,,False
246,exp(-,0,,False
247,xi ,0,,False
248,),0,,False
249,x¯,0,,False
250,",",0,,False
251,+,0,,False
252,"i,1 n",0,,False
253,.,0,,False
254,exp(-,0,,False
255,xi ,0,,False
256,),0,,False
257,"i,1",0,,False
258,(26),0,,False
259,"Clearly, Equation 26 shows that  does not have closed form expression. Thus, we need to apply iterative numerical methods to find value of . Iterative methods may take substantial amount of time for very large collection such as Clueweb, since it needs to iterate over the set of maximum values from each random sample for each distinct term in the collection. This is precisely the reason we use point estimates (with a somewhat empirical transformation) of central tendencies for these models.",1,Clue,True
260,3.6.1 Parameter Estimation for Gumbel,0,,False
261,The mean of Gumbel distribution is,0,,False
262,µ + 0.57 · ,0,,False
263,(27),0,,False
264,while the standard deviation is,0,,False
265, .,0,,False
266,(28),0,,False
267,6,0,,False
268,To estimate the values of  and µ we equate them with,0,,False
269,"corresponding sample mean and standard deviation, which",0,,False
270,finally gives the following estimates.,0,,False
271,6,0,,False
272,6,0,,False
273," , s and µ , x¯ - 0.58 · s",0,,False
274,(29),0,,False
275,"where x¯ and s are sample mean and standard deviation respectively. Since our data is positive random variable and originates from exponential distribution we use Equation 14 for final ranking. Thus, we do not need to worry about the parameter µ. Our only concern is the parameter . Surprisingly, point estimate of  as is does not perform well in practice. Thus, in practice, we use a linear transformation,  ,"" z1 + z2 · s, where z1 and z2 are set empirically to 2.5 and 0.04 respectively.""",0,,False
276,3.6.2 Parameter Estimation for Frechet,0,,False
277,Mean and variance for Frechet are defined respectively as,0,,False
278,"µ(1 - 1/),  > 1",0,,False
279,(30),0,,False
280,and,0,,False
281,"µ2((1 - 2/) - 2(1 - 1/)),  > 2.",0,,False
282,(31),0,,False
283,"Once again, the above two expressions are not very convenient to use since the improper integral (.) needs to be evaluated in order to compute the parameter. Fortunately, median and mode of Frechet distribution have much manageable expressions. Median is defined as",0,,False
284,µ0.69-1/,0,,False
285,(32),0,,False
286,and mode is defined as,0,,False
287,µ(1 + 1 )-1/.,0,,False
288,(33),0,,False
289,"As in Gumbel, we can equate these two expressions to sample median and mode to estimate the model parameters.",0,,False
290,"However, unlike Gumbel, the parameters do not have closed form solution, which can be achieved by using any standard numerical method. Note that, in this case we do not need to iterate over the sample of maximum values, instead mode and median computed once for a term is enough. It is also important to note that although median for a sample is easy to determine, we need to do a little processing to compute mode from a set of real numbers. To compute mode of a sample, we create non-overlapping bins of numbers having 0.5 as the interval. We then take the median of the bin having highest frequency as our sample mode. We have adopted computationally efficient parameter estimation methods. However, a large number of other methods exist in the literature. Thus, it may be interesting to see whether other estimation strategies can improve the retrieval results without sacrificing efficiency too much.",1,ad,True
291,4. EXPERIMENT SETUP,0,,False
292,"In this section, we describe the experiment setup used to evaluate the proposed model. Our experiments have the following two major objectives.",0,,False
293,1. To compare the performance of the model against the state of the art probabilistic models (Section 5.1).,0,,False
294,2. To compare against a recently proposed multi-aspect tf-idf weighting scheme [20] (Section 5.2).,0,,False
295,Table 1: Summary of the test collections and topics,0,,False
296,used in our experiments. `M' stands for million.,0,,False
297,Collection,0,,False
298,# doc topics,0,,False
299,# topics,0,,False
300,Clueweb.B-09 & 10 Clueweb.B-11 & 12 MQ-2009 Clueweb.A-09 & 10 Clueweb.A-11 & 12,1,Clue,True
301,50M 50M 50M 500M 500M,0,,False
302,1-100,0,,False
303,100,0,,False
304,101-200,0,,False
305,100,0,,False
306,20001-30000 684,0,,False
307,1-100,0,,False
308,100,0,,False
309,101-200,0,,False
310,100,0,,False
311,"We summarize the test collections used in our experiments in Table 4. The test collections are taken from TREC web tasks of recent years (2009-2012) as well as from million query 2009 (MQ-2009). The collections contain web documents and real web queries sampled from a search engine log. The documents are crawled from web and hence they have variety of content quality. Clueweb.B collection contains nearly 50 million documents, while ClueWeb.A collection contains approximately 500 million web pages. In MQ-2009 collection, although many queries available, not all queries have been judged. Thus, we use 684 queries for which judgments are available. All the collections have graded relevance assessment. It is important to note that, MQ-2009 queries have incomplete relevance assessment. Therefore, our evaluation methodology skips the unjudged documents from the ranked lists in order to compute the values of well known metrics following the recommendation made in [25].",1,TREC,True
312,Documents and queries are stemmed via Porter stemmer. Stopwords are removed from documents and queries. Statistically significant performance differences are determined using a paired t-test at 95% confidence level (p < 0.05). All our experiments are done using title field of the topics.,0,,False
313,591,0,,False
314,Table 2: Retrieval effectiveness of the proposed method (MVD) compared to probabilistic models. Statisti-,0,,False
315,cally significant improvements are indicated using the first letter of the less effective method. The highest,0,,False
316,"value per column is boldfaced. The numbers in parenthesis indicate relative improvement over LM, PL2 and",1,LM,True
317,"BM25, respectively.",0,,False
318,Clueweb.B-09 & 10 Clueweb.B-11 & 12 MQ-2009,1,Clue,True
319,Clueweb.A-09 & 10 Clueweb.A-11 & 12,1,Clue,True
320,LM 0.309,1,LM,True
321,0.264,0,,False
322,0.367,0,,False
323,0.254,0,,False
324,0.219,0,,False
325,PL2 0.312,0,,False
326,0.263,0,,False
327,0.373,0,,False
328,0.256,0,,False
329,0.219,0,,False
330,ERR@20,0,,False
331,BM25 0.306 MVD 0.337lpb,0,,False
332,0.253 0.286lpb,0,,False
333,0.372 0.408lpb,0,,False
334,0.248 0.286lpb,0,,False
335,0.221 0.257lpb,0,,False
336,"(8.9, 7.9, 9.9)",0,,False
337,"(8.2, 8.8, 12.9)",0,,False
338,"(11.2, 9.5, 9.8) (12.7, 11.4, 15.0) (17.8, 17.5, 16.4)",0,,False
339,NDCG@10,0,,False
340,LM PL2 BM25 MVD,1,LM,True
341,"0.282 0.285 0.284 0.332lpb (17.9, 16.5, 16.8)",0,,False
342,"0.228 0.231 0.222 0.268lpb (17.3, 16.1, 20.4)",0,,False
343,"0.395 0.393 0.391 0.422lpb (7.0, 7.4, 7.9)",0,,False
344,"0.200 0.205 0.208 0.261lpb (30.7, 27.0, 25.3)",0,,False
345,"0.194 0.196 0.191 0.231lpb (19.0, 17.8, 21.3)",0,,False
346,NDCG@20,0,,False
347,LM PL2 BM25 MVD,1,LM,True
348,"0.275 0.278 0.280 0.325lpb (18.4, 17.0, 16.4)",0,,False
349,"0.228 0.228 0.225 0.265lpb (15.9, 16.2, 17.8)",0,,False
350,"0.459 0.458 0.453 0.479b (4.5, 4.5, 5.8)",0,,False
351,"0.193 0.195 0.208 0.248lpb (28.7, 27.4, 19.0)",0,,False
352,"0.196 0.198 0.186 0.228lpb (16.4, 15.2, 22.7)",0,,False
353,4.1 Baselines,0,,False
354,"The performance of the proposed model is compared to a number of state of the art retrieval models from different families. BM25 [24] is chosen as the representative baseline from the classical probabilistic model. From language model, we choose Dirichlet smooth version [32], since it is known to be the most effective among the language models [10]. From divergence from randomness family, we choose PL2 [1] as the baseline, following recent work [10, 14].",0,,False
355,"Pivoted document length normalization is chosen as a basic TF-IDF baseline. MATF [20] is chosen as another state of the art tf-idf model. Note that, MATF is a highly effective empirical tf-idf model and one of the major objectives of the proposed model is to advance the multi-aspect TF model using a probabilistic foundation. Finally, since our model attempts to capture the distribution of normalized tf, we also compare to multi-aspect TF normalization with a log-logistic distribution which has similar purpose. Thus, our set of baselines contains members from all state of the art families.",1,ad,True
356,4.2 Free parameters and evaluation metrics,0,,False
357,"All the baseline models (except MATF) and the proposed model contain one or more free parameters. It is important to note that the parameters of these models often influence the performance to a statistically significant degree. Hence, for the sake of reliable and competitive comparison, the parameters are optimized using 5-fold cross validation with the corresponding evaluation measure (ERR@20 [2], NDCG@10 [16] or NDCG@20) as the target metric.",0,,False
358,"We choose expected reciprocal rank (ERR), NDCG@10, and NDCG@20 as our evaluation measures. ERR has been the primary evaluation metric for recent TREC web tracks [3]. NDCG@k leverages graded relevance and also has a position wise discounting. Thus, it reflects the overall quality of the documents at top k. On the other hand, ERR@k is a precision bias metric that leverages graded relevance assessment. Thus, ERR is more suitable metric for web search.",1,TREC,True
359,"Clueweb collections contain substantial number of spam documents. Thus, following previous work [5], we have filtered out spam documents from the collections. Specifically, documents assigned by Waterloo's spam classifier [5] with a score below 70 were filtered out from the initial corpus. The score indicates the percentage of all documents in ClueWeb that are presumably ""spammier"" than the document at hand. The models are then run on the residual corpus to produce final ranked lists.",1,Clue,True
360,5. RESULTS,0,,False
361,In this section we summarize retrieval performance of the proposed method and the baseline methods. Throughout the result section MVD denotes the proposed model.,0,,False
362,5.1 Comparison to Probabilistic Models,0,,False
363,"Table 2 compares the performance of MVD to that of the three probabilistic models, namely, language model with Dirichlet prior, BM25 and PL2. First, we compare the performances measured by ERR@20. Table 2 shows that, on two Clueweb.B collections, MVD outperforms LM, PL2 and BM25 by a margin of 8% to 12% and all the differences are statistically significant. On MQ-2009 collection, MVD is once again always statistically significant compared to all the baselines with a margin more than 9%. Similarly, on two Clueweb.A datasets, MVD is unequivocally superior to the baselines and quite clearly the performance differences are even larger than that on Clueweb.B and MQ-2009. The baseline methods seem to be performing nearly equally and in none of the cases, the performance differences among the baselines found to be statistically significant.",1,Clue,True
364,"Our next goal is to analyze the results measured in terms of NDCG@10. Once again, MVD gives consistent performance improvement over LM, BM25 and PL2 on Clueweb.B collections. The performance differences are always statistically significant with more than 15% relative improvements. Results on MQ-2009 collection also show that MVD is significantly more effective than all the baselines, however the relative differences are smaller compared to Clueweb collec-",1,LM,True
365,592,0,,False
366,Table 3: Retrieval effectiveness of the proposed method (MVD) compared to tf-idf models. Statistically,0,,False
367,significant improvements are indicated using the first letter of the less effective method. The highest value,0,,False
368,"per column is boldfaced. The numbers in parenthesis indicate relative improvement over PIVOT, MATF and",0,,False
369,"LL, respectively.",0,,False
370,Clueweb.B-09 & 10 Clueweb.B-11 & 12 MQ-2009,1,Clue,True
371,Clueweb.A-09 & 10 Clueweb.A-11 & 12,1,Clue,True
372,PIVOT 0.263,0,,False
373,0.234,0,,False
374,0.367,0,,False
375,0.169,0,,False
376,0.196,0,,False
377,MATF 0.283,0,,False
378,0.282,0,,False
379,0.388,0,,False
380,0.227,0,,False
381,0.251,0,,False
382,ERR@20,0,,False
383,LL MVD,0,,False
384,0.290 0.337pml,0,,False
385,0.275 0.286pl,0,,False
386,0.391 0.408pml,0,,False
387,0.244 0.286pml,0,,False
388,0.240 0.257pl,0,,False
389,"(27.9, 18.8, 16.0) (22.0, 1.5, 3.9)",0,,False
390,"(11.2, 5.1, 4.4) (69.6, 26.1, 16.9) (31.2, 2.7, 7.1)",0,,False
391,NDCG@10,0,,False
392,PIVOT MATF LL MVD,0,,False
393,"0.219 0.276 0.287 0.332pml (51.6, 20.5, 15.8)",0,,False
394,"0.196 0.240 0.234 0.268pml (36.6, 11.4, 14.5)",0,,False
395,"0.381 0.402 0.418 0.422pm (10.8, 5.0, 1.0)",0,,False
396,"0.177 0.197 0.207 0.261pml (47.5, 32.3, 25.8)",0,,False
397,"0.175 0.213 0.206 0.231pml (32.5, 8.7, 12.5)",0,,False
398,NDCG@20,0,,False
399,PIVOT MATF LL MVD,0,,False
400,"0.212 0.286 0.284 0.325pml (53.5, 13.5, 14.4)",0,,False
401,"0.200 0.243 0.235 0.265pml (32.0, 9.0, 12.7)",0,,False
402,"0.442 0.466 0.477 0.479pm (8.3, 2.8, 0.4)",0,,False
403,"0.181 0.202 0.201 0.248pml (37.2, 22.7, 23.0)",0,,False
404,"0.171 0.209 0.198 0.228pml (33.2, 9.4, 15.0)",0,,False
405,"tions. One reason for smaller difference is that the baseline NDCG@10 numbers are very high, which makes the relative improvements smaller. The effectiveness of MVD on Clueweb.A collections is even more encouraging. MVD surpasses the baselines on Clueweb.A-09 & 10 collection by more than 20% margin which is clearly highly significant. We observe similar trend on the other Clueweb.A collection. As in ERR@20, the baselines seem to be performing with equal effectiveness.",1,Clue,True
406,"We notice very similar (as in ERR@20 and NDCG@20) behavior of MVD on Clueweb.B collections measured by NDCG@20. Once again, MVD is consistently and significantly better than all the baselines with noticeably large margin of relative improvement. The picture is slightly different on MQ-2009 collection. Although, MVD is better than all the baselines, difference against BM25 only found to be significant. We suspect that sparser relevance judgements of MQ-2009 collection is a possible reason behind smaller differences. Finally, MVD beats the baselines by a convincingly large margin thereby maintaining its consistency as in the previous cases.",1,Clue,True
407,"Overall, the results indicate that the proposed model based on distribution of maximum values yields consistent and significant retrieval performance improvement over the three state of the art probabilistic baselines from different categories measured by NDCG measures. We conclude that the proposed model is significantly more precise than the baselines on all the collections, thereby making it a very suitable for web search. The experiments also reveal that the performance of the baselines are very similar to each other, irrespective of the collection, which corroborates earlier findings that if parameters of the models are properly optimized, language model, BM25 and divergence from randomness model are closely comparable.",0,,False
408,5.2 Comparison to TF-IDF Models,0,,False
409,"The experiments in this section are designed to compare the proposed method to a number of tf-idf models. By this set of experiments, we intend to achieve the following major goals.",0,,False
410,1. How does the proposed model perform compare to a basic tf-idf model that uses only pivoted document length normalization?,0,,False
411,"2. Since MVD is based on multi-aspect term frequency normalization in a new probabilistic framework, how does it compare against a recent tf-idf model (MATF) that introduced multi-aspect tf normalization? We reiterate that this is the main issue we sought to address using maximum value based model.",1,ad,True
412,"3. We mentioned before that MATF combines the two normalized tf using an empirical tf function that transforms normalized tf values. Moreover, it does not factor in the distribution of normalized tf in the elite set for the particular term. Thus, in this section we compare the performance of MVD to a method that uses log-logistic probability distribution of two normalized tfs. The method is denoted as LL in the table. The parameters of this model is estimated using the method detailed in [17].",0,,False
413,"Table 5.2 compares the performance of tf-idf methods and MVD. First, it is clear from the table that MVD is highly significantly better than PIVOT. This holds for all collection and measured by all three evaluation measures. The performance differences are unequivocally statistically significant. On Clueweb (both A and B) collections, MVD gives upto 50% relative improvement over PIVOT. Second, MATF, which is based on relative intra-document tf normalization and length based normalization (which we call multi-aspect tf normalization), is always poorer than MVD and the differences are almost always statistically significant. More importantly, the margin of improvement by MVD is often noticeably high.",1,Clue,True
414,"Thus, we conclude that maximum value distribution has large impact on retrieval performance. Finally, we compare the proposed method to log-logistic distribution based method denoted as LL in the table. Note that LL uses distribution of multi-aspect tf normalization for estimating rel-",0,,False
415,593,0,,False
416,evance and thus has probabilistic interpretation. Table 5.2 once again shows that MVD often significantly surpasses LL.,0,,False
417,6. CONCLUSION,0,,False
418,"In this paper we introduce a probabilistic information retrieval model. The proposed model is guided by the principle that given the normalized frequency of a term in a document, the score is proportional to the likelihood that the normalized tf is maximum with respect to its distribution in the elite set for the corresponding term. We use a mixture of two maximum value distribution, that factors in varying specificity of query terms. The proposed model, integrates multi-aspect tf normalization scheme proposed recently in a probabilistic framework. Unlike many existing models, the proposed model takes into account the term specific distribution in the elite set. However, the unique contribution is that the model measures the likelihood of relevance focussing on the maximum values of the distribution, which we believe the first such effort to view ranking problem from this perspective. An empirical evaluation on large web collections containing millions of documents and hundreds of real world web queries demonstrates that the model significantly outperforms the state of the art probabilistic models from different families. As a future work, we plan to incorporate term proximity (ordered and un-ordered bigram) information into our model.",1,corpora,True
419,Acknowledgments,0,,False
420,I thank Doug Oard for useful discussions and suggestions. Without his support and advice this work would not have been possible. This research was supported in part by DARPA contract HR0011-12-C-0015 and NSF award 1065250.,1,ad,True
421,7. REFERENCES,0,,False
422,"[1] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 2002.",0,,False
423,"[2] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In ACM CIKM, 2009.",1,ad,True
424,"[3] C. L. A. Clarke, N. Craswell, I. Soboroff, and E. M. Voorhees. Overview of the trec 2011 web track. In TREC, 2011.",1,trec,True
425,"[4] S. Clinchant and E. Gaussier. Retrieval constraints and word frequency distributions a log-logistic model for ir. Inf. Retr., 2011.",0,,False
426,"[5] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval, 2011.",0,,False
427,"[6] W. B. Croft, H. R. Turtle, and D. D. Lewis. The use of phrases and structured queries in information retrieval. In ACM SIGIR, 1991.",0,,False
428,"[7] R. Cummins and C. O'Riordan. A constraint to automatically regulate document-length normalisation. In ACM CIKM, 2012",0,,False
429,"[8] R. Cummins, J. H. Paik, and Y. Lv. A polya urn document language model for improved information retrieval. ACM Trans. Inf. Syst., 2015.",0,,False
430,"[9] J. Fagan. Automatic phrase indexing for document retrieval. In ACM SIGIR, 1987.",0,,False
431,"[10] H. Fang, T. Tao, and C. Zhai. Diagnostic evaluation of information retrieval models. ACM Trans. Inf. Syst., 2011.",0,,False
432,"[11] R. A. Fisher and L. H. C. Tippett. Limiting forms of the frequency distribution of the largest or smallest member of a sample. In Mathematical Proceedings of the Cambridge Philosophical Society, 1928.",0,,False
433,"[12] W. R. Greiff. A theory of term weighting based on exploratory data analysis. In ACM SIGIR, 1998.",0,,False
434,"[13] E. J. Gumbel. Statistics of extremes. Courier Dover Publications, 2012.",0,,False
435,"[14] B. He and I. Ounis. A study of the dirichlet priors for term frequency normalisation. In ACM SIGIR, 2005.",0,,False
436,"[15] D. Hiemstra, S. Robertson, and H. Zaragoza. Parsimonious language models for information retrieval. In ACM SIGIR, 2004.",0,,False
437,"[16] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4), Oct. 2002.",0,,False
438,"[17] Y. Lv and C. Zhai. A log-logistic model-based interpretation of tf normalization of bm25. In ECIR, 2012",0,,False
439,"[18] R. E. Madsen, D. Kauchak, and C. Elkan. Modeling word burstiness using the dirichlet distribution. In ICML, 2005.",1,ad,True
440,"[19] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In ACM SIGIR, 2005.",0,,False
441,"[20] J. H. Paik. A novel tf-idf weighting scheme for effective ranking. In ACM SIGIR, 2013.",0,,False
442,"[21] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In ACM SIGIR, 1998.",0,,False
443,"[22] S. Robertson and H. Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4), Apr. 2009.",0,,False
444,[23] S. E. Robertson. Readings in information retrieval. chapter The probability ranking principle in IR. 1997.,1,ad,True
445,"[24] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In ACM SIGIR, 1994.",0,,False
446,"[25] T. Sakai. Alternatives to bpref. In ACM SIGIR, 2007. [26] G. Salton and C. Buckley. Term-weighting approaches",0,,False
447,"in automatic text retrieval. Inf. Process. Manage., 24(5), Aug. 1988. [27] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., 1986. [28] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11), Nov. 1975. [29] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In ACM SIGIR, 1996. [30] K. Sparck Jones. Document retrieval systems. chapter A statistical interpretation of term specificity and its application in retrieval. 1988. [31] H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Trans. Inf. Syst., 9(3), July 1991. [32] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2), Apr. 2004.",0,,False
448,594,0,,False
449,,0,,False

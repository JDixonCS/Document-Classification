,sentence,label,data,regex
0,An Entity Class-Dependent Discriminative Mixture Model for Cumulative Citation Recommendation,0,,False
1,Jingang Wang,0,,False
2,School of Computer Science Beijing Institute of Technology,0,,False
3,bitwjg@bit.edu.cn,0,,False
4,Zhiwei Zhang,0,,False
5,Dept. of Computer Science Purdue University,0,,False
6,zhan1187@purdue.edu,0,,False
7,Dandan Song,0,,False
8,School of Computer Science Beijing Institute of Technology,0,,False
9,sdd@bit.edu.cn,0,,False
10,Luo Si,0,,False
11,Dept. of Computer Science Purdue University,0,,False
12,lsi@purdue.edu,0,,False
13,Chin-Yew Lin,0,,False
14,Knowledge Mining Group Microsoft Research,0,,False
15,cyl@microsoft.com,0,,False
16,Qifan Wang,0,,False
17,Dept. of Computer Science Purdue University,0,,False
18,wang868@purdue.edu,0,,False
19,Lejian Liao,0,,False
20,School of Computer Science Beijing Institute of Technology,0,,False
21,liaolj@bit.edu.cn,0,,False
22,ABSTRACT,0,,False
23,"This paper studies Cumulative Citation Recommendation (CCR) for Knowledge Base Acceleration (KBA). The CCR task aims to detect potential citations of a set of target entities with priorities from a volume of temporally-ordered stream corpus. Previous approaches for CCR that build an individual relevance model for each entity fail to handle unseen entities without annotation. A baseline solution is to build a global entity-unspecific model for all entities regardless of the relationship information among entities, which cannot guarantee to achieve satisfactory result for each entity. In this paper, we propose a novel entity class-dependent discriminative mixture model by introducing a latent entity class layer to model the correlations between entities and latent entity classes. The model can better adjust to different types of entities and achieve better performance when dealing with a broad range of entities. An extensive set of experiments has been conducted on TREC-KBA-2013 dataset, and the experimental results demonstrate that the proposed model can achieve the state-of-the-art performance.",1,CCR,True
24,Categories and Subject Descriptors,0,,False
25,H.3.3 [Information Search and Retrieval]: Retrieval Models,0,,False
26,This work was partially done when the first author was visiting Purdue University and Microsoft Research Asia. Corresponding Author,0,,False
27,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09-13, 2015, Santiago, Chile.",1,ad,True
28,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,0,,False
29,DOI: http://dx.doi.org/10.1145/2766462.2767698.,0,,False
30,Keywords,0,,False
31,Cumulative Citation Recommendation; Knowledge Base Acceleration; Mixture Model; Information Filtering,0,,False
32,1. INTRODUCTION,1,DUC,True
33,"In recent years, we have witnessed a proliferation of open domain Knowledge Bases (KBs) such as Freebase1 and Yago2. They have been used in many applications such as query answering, entity search and entity linking and have shown great promises. These KBs are usually organized around entities such as persons, organizations, locations, and so on. Currently, the maintenance of a KB mainly relies on human editors. However, with the explosion of information, largescale KBs are hard to be kept up-to-date solely by human editors. Taking English Wikipedia for example, there are approximately 4.7 million entities but merely 132,938 active editors3. The less popular entities cannot be updated in time because they are not spotlighted. As reported in [14], the median time delay between a cited document's publishing and its citation in Wikipedia is almost one year. An outdated KB severely limits the effectiveness of applications depending on it. This gap could be bridged if relevant documents of KB entries can be automatically detected as soon as they emerge online and then be recommended to the editors with various levels of relevance. This is called the Cumulative Citation Recommendation (CCR). Formally, given a KB entity, CCR is a task to filter highly relevant documents from a chronological stream corpus and evaluate their citation-worthiness to the target entity.",1,Wiki,True
34,"Most previous approaches (e.g., [2, 25]) for CCR are highly supervised and require sufficient training data to build an individual relevance model for each entity. These approaches are infeasible when dealing with a large-scale KB, since the",1,CCR,True
35,1https://www.freebase.com/ 2http://www.mpi-inf.mpg.de/departments/ databases-and-information-systems/research/ yago-naga/yago/ 3http://meta.wikimedia.org/wiki/List_of_ Wikipedias#1_000_000.2B_articles,1,wiki,True
36,635,0,,False
37,"labeling work is labor intensive. One solution is to build a global entity-unspecific discriminative model and optimize it to achieve an overall optimal performance for all entities [29, 36]. However, these models ignore the distinctions between different entities and learn a set of fixed model parameters for all entities, which leads unsatisfactory performance when dealing with a diverse entity set. For instance, it is not intuitve to apply the same discriminative model for Geoffery Hinton and Appleton Museum of Art. The former entity is a computer scientist, while the latter one is a museum. Nevertheless, the global model treats them equally without considering the prior entity class knowledge. We assume that entities from a same class have similar tastes and preferences when citing relevant documents, which means they have similar combination weights in the discriminative model. Therefore, for an entity with little training data, the training data of its similar entities from the same class can be utilized to learn the combination weights. In comparison to the global model, more accurate combination weights are learned for each entity by this manner.",1,ad,True
38,"Based on this observation, we build an adaptive discriminative model for different types of entities by utilizing the underlying entity class information, i.e. entity class dependent discriminative mixture model. We introduce an intermediate latent entity class layer and define a joint distribution over the entity-document pairs and latent classes conditioned on the observations. The aim is to achieve relevance estimation through learning a mixture model which is expected to outperform the global model, while maintaining the capability to reveal the hidden correlations between entities and entity classes. The model can be viewed as a hierarchical combination of a discriminative component and a mixing component, so two types of features are required: entity-document features for the discriminative component and entity-class features for the mixing component.",1,ad,True
39,"For the discriminative component, we develop a set of bursty features as temporal features in addition to semantic features. The bursty features are detected from two independent data sources: the stream corpus (internal) and certain third-party data (external) like Google Trends.",1,ad,True
40,"For the mixing component, we explore two types of entityclass features to model the correlations between entities and hidden classes, including profile-based features and categorybased features. Profile-based features are constructed from the entity's profile in KB, while category-based features rely on the existing category labels for the entity in KB.",0,,False
41,"To the best of our knowledge, this is the first research work that focuses on modeling correlations between entities and hidden entity classes in discriminative model for CCR. Our model is capable of tackling less popular entities with little training data and unseen entities that do not exist in the training set, which is indispensable in a practical CCR system. Empirical studies have been conducted on TREC-KBA-2013 dataset to show the effectiveness and robustness of the proposed mixture model. Experimental results demonstrate that our model achieves the state-ofthe-art performance on TREC-KBA-2013 dataset.",1,CCR,True
42,"The rest of this paper is organized as follows. Section 2 summarizes related works. Section 3 introduces an entity class-dependent discriminative mixture model for CCR. Section 4 describes features required in our model, especially the temporally bursty features and their detection methods. Section 5 presents the detailed experimental results and pro-",1,CCR,True
43,vides some discussion. Section 6 concludes this paper and points out possible future work.,0,,False
44,2. RELATED WORK,0,,False
45,"Although CCR was first proposed in TREC-KBA tracks, the similar research problem has been studied in several topics of information retrieval.",1,CCR,True
46,Topic/Event Detection and Tracking.,1,Track,True
47,"Topic Detection and Tracking (TDT) is a track hosted by TREC from 1997 to 2004 [1]. A similar research topic in recent years is event detection. Both TDT and event detection are concerned with the development of techniques for finding and following events in broadcast news or social media. The techniques adopted for TDT and event detection can be broadly classified into two categories: (1) clustering documents based on the semantic distance between them [34], or (2) grouping the frequent words together to represent events [22]. In [22], a finite automaton model is proposed to detect events in stream by modeling events as state transitions. This method has been validated widely by lots of other studies [18, 17, 35]. We also adopt this model to detect KB entities' bursts in the stream corpus and then extract bursty features for them. Different from above works, we model entities' occurrences to capture bursty activities instead of words' occurrences. Another difference between CCR and TDT is that CCR needs to make fine-grained citation-worthiness distinctions between relevant documents further.",1,Track,True
48,Cumulative Citation Recommendation.,0,,False
49,"TREC has launched the KBA-CCR track since 2012. Participants treat CCR as either a ranking problem [3, 2, 4] or a classification problem [3, 5, 29]. Classification and Learning to Rank methods have been compared and evaluated [2, 15], and both of them can achieve the state-of-art performance with a powerful feature set. Several supervised learning techniques, such as SVM [21], language models [26, 10], Markov Random Fields [7], and Random Forests [4, 5, 29] are utilized. Meanwhile, a variety of relevance scoring methods have been tried, including standard Lucene scoring [6], and custom ranking functions based on entity cooccurrences [25]. A time-aware evaluation paradigm is developed to study time-dependent characteristics of CCR [9].",1,TREC,True
50,"However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. Entity-unspecific methods, regardless of entity distinctions, are employed to address this problem [29, 28]. Nevertheless, characteristics of different entities are lost in the entity-unspecific methods. Some other researchers employ transfer learning techniques to learn across entities by using entity-unspecific meta-features [36], or utilize a semi-supervised approach to profile an entity by leveraging its related entities and weighting them with the training data [23]. These methods have demonstrated that the correlations between entities are useful for CCR. Nevertheless, all these methods are empirically designed and the performance can be improved further.",1,ad,True
51,"What's more, query expansion is often employed because the name of the target entity is too sparse to be a good query. Other name variants and contextual information of terms or related entities from Wikipedia or from the document stream",1,Wiki,True
52,636,0,,False
53,"[7, 6] are used to enrich the semantic features of entities. In addition to semantic features, temporal features have been proved especially helpful in CCR [2, 5, 29].",1,ad,True
54,Mixture Model.,0,,False
55,"Mixture model has been proved effective to address the problem of data insufficiency in several information retrieval tasks, including expert search [11], federated search [19], collaborative filtering [20] and image retrieval [30]. By introducing latent layers to learn flexible combination weights for different feature vectors, mixture model can always outperform simple discriminative models with fixed combination weights. Hence, we propose an entity class-dependent discriminative mixture model to deal with the entities with little training data, which will be described in next section.",1,ad,True
56,3. DISCRIMINATIVE MODEL FOR CCR,1,CCR,True
57,"This section proposes a novel learning framework by modeling each entity's distribution across hidden entity classes and combining it with a logistic regression model to form a final discriminative model. First we provide a formal definition of the research problem and model it as a classification task, and then present two discriminative models: a global model and an entity class-dependent mixture model.",0,,False
58,3.1 Problem Statement,0,,False
59,"We consider CCR as a binary classification problem that treats the relevant entity-document pairs as positive instances and irrelevant ones as negative instances. Many probabilistic classification techniques in the literature generally fall into two categories: generative models and discriminative models. Discriminative models have attractive theoretical properties [24] and generally perform better than their generative counterparts in the field of information retrieval [16, 32]. Therefore, we adopt discriminative probabilistic models in this paper.",1,CCR,True
60,"Given a set of KB entities E , {eu}(u ,"" 1,    , M ) and a document collection D "", {dv}(v ,"" 1,    , N ), our objective is to estimate the relevance of a document d to a given entity e. In other words, we need estimate the conditional probability of relevance P (r|e, d) with respect to an entity-document pair (e, d). Each entity-document pair (e, d) is represented as a feature vector f (e, d) "","" (f1(e, d),    , fK (e, d)), where K indicates the number of entity-document features. Moreover, to model the hidden entity class information, each entity can be represented as an entity-class feature vector g(e) "","" (g1(e),    , gL(e)), where L indicates the number of entity-class features. The entity-document features and entity-class features will be introduced in Section 4 later.""",0,,False
61,3.2 Global Discriminative Model,0,,False
62,"This paper utilizes logistic regression, a traditional discriminative model, to estimate the conditional probability P (r|e, d), in which r(r  {1, -1}) is a binary label to indicate the relevance of the entity-document pair (e, d). The value of r is 1 if the document d is relevant to the entity e, otherwise r ,"" -1. Formally, the parametric form of P (r "","" 1|e, d) can be expressed as follows in terms of logistic functions over a linear combination of features,""",1,ad,True
63,K,0,,False
64,"P (r ,"" 1|e, d) "","" ( ifi(e, d))""",0,,False
65,(1),0,,False
66,"i,1",0,,False
67,"where (x) ,"" 1/(1 + exp(-x)) is the standard logistic function, and i is the combination parameter for the ith entry of the feature vector. For the irrelevant class, we have""",0,,False
68,K,0,,False
69,"P (r ,"" -1|e, d)"",1 - P (r ,"" 1|e, d)"",""(- ifi(e, d)) (2)""",0,,False
70,"i,1",0,,False
71,"It is worth noting that for different values of r, the only",0,,False
72,"difference in P (r|e, d) is the sign within the logistic func-",0,,False
73,"tion. Therefore, we adopt the general representation of",1,ad,True
74,"P (r|e, d) , (r",0,,False
75,"K i,1",0,,False
76,i,0,,False
77,"fi(e,",0,,False
78,d)),0,,False
79,in,0,,False
80,the,0,,False
81,following,0,,False
82,sections.,0,,False
83,"The conditional probability of relevance P (r|e, d) represents",0,,False
84,the extent to which the document d is relevant to the entity,0,,False
85,e. The entity-documents pairs are then classified as positive,0,,False
86,"or negative according to the value of P (r ,"" 1|e, q). Since the""",0,,False
87,learned weights are identical for all entity-document pairs,0,,False
88,"and regardless of specific entities, this model is also denoted",0,,False
89,as global discriminative model (GDM) in this paper.,0,,False
90,"Several other approaches for CCR [5, 29] can be deemed as",1,CCR,True
91,global discriminative models adopting different classification,1,ad,True
92,functions such as decision trees and Support Vector Machine,0,,False
93,(SVM).,0,,False
94,3.3 Entity Class-Dependent Mixture Model,0,,False
95,"In the GDM introduced in Subsection 3.2, a fixed set of combination weights (i.e., ) are learned to optimize the overall performance for all entities. However, the best combination strategy for a given entity is not always the best for others. The entities stored in KBs are extremely diverse, including persons, organizations, locations, events, etc. Different entities have personalized criteria to detect relevant documents.",0,,False
96,"We propose an entity class-dependent discriminative mixture model (ECDMM) by introducing an intermediate latent class layer to capture the entity class information in the learning framework. A latent variable z is utilized to indicate which entity class the combination weights z ,"" (z1,    , zK ) are drawn from. The choice of z depends on the target entity e in the entity-document pair (e, d). The joint probability of relevance r and the latent variable z is represented as""",0,,False
97,"P (r, z|e, d; , ) ,"" P (z|e; )P (r|e, d, z; )""",0,,False
98,(3),0,,False
99,"where P (z|e; ) is the mixing coefficient, representing the probability of choosing hidden entity class z given entity e, and  is the corresponding parameter. P (e, d, z; ) denotes the mixture component which takes a logistic functions for r , 1 (or r , -1).  ,"" {zi} is the set of combination parameters where zi is the weight for the ith feature vector entry for the given training instance (e, d) under the hidden class z. By marginalizing out the latent variable z, the corresponding mixture model can be written as""",0,,False
100,Nz,0,,False
101,K,0,,False
102,"P (r|e, d; , ) ,"" P (z|e; ) r zifi(e, d) (4)""",0,,False
103,z,0,,False
104,"i,1",0,,False
105,where Nz is the number of latent entity classes. If P (z|e; ),0,,False
106,"follows the multinomial distribution, the model cannot eas-",0,,False
107,ily generalize the combination weights to unseen entities,0,,False
108,beyond the training set since each parameter in multino-,0,,False
109,mial distribution specifically corresponding to a training en-,0,,False
110,"tity. To address this problem, we adopt a soft-max function",1,ad,True
111,1 Ze,0,,False
112,exp(,0,,False
113,"Lz j,1",0,,False
114,zj,0,,False
115,gj,0,,False
116,(e)),0,,False
117,to,0,,False
118,model,0,,False
119,P (z|e; ),0,,False
120,instead.,1,ad,True
121,zj is,0,,False
122,the weight parameter associated with the jth entity feature,0,,False
123,637,0,,False
124,"in the latent entity class z and Ze is the normalization factor that scales the exponential function to be a proper probability distribution. In this representation, each entity e is denoted by a bag of entity-class features (g1(e),    , gLz (e)) where Lz is the number of entity features. By plugging the soft-max function into Eq. 4, we can get",0,,False
125,1 Nz,0,,False
126,Lz,0,,False
127,K,0,,False
128,"P (r|e, d; , ),",0,,False
129,exp,0,,False
130,"Ze z,1",0,,False
131,zj gj (e),0,,False
132,"j,1",0,,False
133,"r zifi(e, d)",0,,False
134,"i,1",0,,False
135,(5),0,,False
136,"Because zj is associated with each entity feature instead of each entity, the above model allows the estimated zj to be applied to less popular entities and even unseen entities.",1,ad,True
137,"Suppose entity-document pairs in training set are represented as T ,"" {(eu, dv)}, and R "","" {ruv} denotes the corresponding relevance judgment (i.e., +1 or -1) of (eu, dv), where u "","" 1,    , M and v "","" 1,    , N . Assume training instances in T are independently generated, the conditional likelihood of the training data is written as follows.""",0,,False
138,MN,0,,False
139,"P (R|T ),",0,,False
140,"P (ruv|eu, dv),",0,,False
141,"u,1 v,1",0,,False
142,"MN u,1v,1",0,,False
143,1 Nz,0,,False
144,Lz,0,,False
145,K,0,,False
146,Zeu,0,,False
147,exp(,0,,False
148,"z,1",0,,False
149,"j,1",0,,False
150,zj gj (eu))(ruv,0,,False
151,"zifi(eu, dv))",0,,False
152,"i,1",0,,False
153,(6),0,,False
154,3.4 Parameter Estimation,0,,False
155,"The parameters (i.e.  and ) in Eq. 6 can be estimated by maximizing the following data log-likelihood function,",0,,False
156,MN,0,,False
157,Nz 1,0,,False
158,"L(, ),",0,,False
159,log,0,,False
160,"u,1 v,1",0,,False
161,"( z,1 Zeu",0,,False
162,exp( zj gj (eu))),0,,False
163,"j,1",0,,False
164,(7),0,,False
165,K,0,,False
166,"(ruv zifi(eu, dv))",0,,False
167,"i,1",0,,False
168,"where M is the number of the entities and N is the number of the documents in training set. gj(eu) denotes the jth feature for the uth entity and ruv denotes the relevance judgment for the pair (eu, dv). A typical approach to maximize Eq. 7 is to use Expectation-Maximization (EM) algorithm [8].",0,,False
169,E-Step.,0,,False
170,The E-step can be derived as follows by computing the posterior probability of z given entity eu and document dv.,0,,False
171,"P (z|eu, dv) ,",0,,False
172,exp(,0,,False
173,"Lz j,1",0,,False
174,zj,0,,False
175,gj,0,,False
176,(eu))(ruv,0,,False
177,"K i,1",0,,False
178,"zifi(eu,",0,,False
179,dv )),0,,False
180,(8),0,,False
181,z exp(,0,,False
182,"Lz j,1",0,,False
183,zj,0,,False
184,gj,0,,False
185,(eu))(ruv,0,,False
186,"K i,1",0,,False
187,"zifi(eu,",0,,False
188,dv )),0,,False
189,M-Step.,0,,False
190,"By optimize the auxiliary Q function, we can derive the following parameter update rules.",0,,False
191,"z ,",0,,False
192,K,0,,False
193,(9),0,,False
194,"arg max P (z|eu, dv) log (ruv zifi(eu, dv))",0,,False
195,z uv,0,,False
196,"i,1",0,,False
197,"z , arg max",0,,False
198,z u,0,,False
199,1,0,,False
200,Lz,0,,False
201,"P (z|eu, dv)",0,,False
202,v,0,,False
203,log,0,,False
204,Zeu,0,,False
205,exp(,0,,False
206,"j,1",0,,False
207,zj,0,,False
208,gj,0,,False
209,(eu)),0,,False
210,(10),0,,False
211,"The M-step can be optimized by any gradient descent method. To optimize Eq. 9 and Eq. 10, we employ the minFunc toolkit4, a collection of Matlab functions for solving optimization problems using Quasi-Newton strategy. When the value of L(, ) converges to a local optima, the estimated parameters can be plugged back into the model to compute the probability of relevance for entity-document pairs. Since EM is only guaranteed to converge to local optima given different starting points, we try several starting points and choose the model that leads to the greatest log-likelihood.",1,ad,True
212,3.5 Discussion,0,,False
213,The ECDMM can exploit the following two advantages over the GDM: (1) the combination weights are able to change across entities and hence lead to a gain of flexibility. (2) it offers probabilistic semantics for the latent entity classes and thus each entity can be associated with multiple classes.,1,ad,True
214,Determining the number of latent Variables.,0,,False
215,"The number of hidden entity classes can be determined by some model selection criterion. We choose Akaike Information Criteria (AIC), which has been shown suitable in determining the number of latent classes in mixture models. As a measure of the goodness of fit of an estimated statistical model, AIC is defined as",0,,False
216,"2m - 2L(, )",0,,False
217,(11),0,,False
218,"where m is the total number of parameters in the model. AIC offers a relative estimation of the information loss when a given model is used to represent the process that generates the data. Given a set of models, the preferred model is the one with the minimum AIC value.",0,,False
219,4. FEATURES,0,,False
220,"In this section, we present the two types of features used in the discriminative models. Entity-document features f (e, d) are used in the discriminative components of GDM and ECDMM. In addition, ECDMM requires entity-class features g(e) to learn the mixing coefficients in the mixture component.",1,ad,True
221,4.1 Entity-Document Features,0,,False
222,"Entity-document features (i.e., f (e, d)) are composed of semantic and temporal features.",0,,False
223,4.1.1 Semantic Features,0,,False
224,"We adopt the semantic features listed in Table 1, which have been proved effective in CCR [28, 29]. Semantic features can model semantic characteristics of document-entity pairs.",1,ad,True
225,4.1.2 Temporal Features,0,,False
226,"Entities are evolving in the stream corpus as time goes by, yet semantic features are not capable of portraying the",0,,False
227,4http://www.cs.ubc.ca/~schmidtm/Software/minFunc. html,0,,False
228,638,0,,False
229,Table 1: Semantic features.,0,,False
230,Feature,0,,False
231,Description,0,,False
232,N (erel),0,,False
233,# of entity e's related entities found in its profile page,0,,False
234,"N (d, e)",0,,False
235,# of occurrences of e in document d,0,,False
236,"N (d, erel)",0,,False
237,# of occurrences of the related entities in document d,0,,False
238,"F P OS(d, e)",0,,False
239,First occurrence position of e in d,0,,False
240,"F P OSn(d, e)",0,,False
241,"F P OS(d, e) normalized by the document",0,,False
242,length,0,,False
243,"LP OS(d, e)",0,,False
244,Last occurrence position of e in d,0,,False
245,"LP OSn(d, e)",0,,False
246,"LP OS(d, e) normalized by the document",0,,False
247,length,0,,False
248,"Spread(d, e)",1,ad,True
249,"LP OS(d, e) - F P OS(d, e)",0,,False
250,"Spreadn(d, e) Spread(d, e) normalized by document",1,ad,True
251,length,0,,False
252,"Simcos(d, si(e)) Cosine similarity between d and the ith section of e's profile",0,,False
253,"Simjac(d, si(e)) Jaccard similarity between d and the ith section of e's profile",0,,False
254,"Simcos(d, ci)",0,,False
255,Cosine similarity between d and the ith citation of e in the KB,0,,False
256,"Simjac(d, ci)",0,,False
257,Jaccard similarity between d and the ith citation of e in the KB,0,,False
258,"dynamic characteristics of entities. So we resort temporal features to make up this deficiency. Previous work [2, 5, 29] considering temporal features can be summarized as a straightforward strategy that counts the daily (or hourly) occurrences of target entities in the stream corpus and calculates some statistical indicators as temporal features. To exploit the effectiveness of temporal features, novel bursty features are introduced in this paper. The underlying intuition is that the occurrences of entities in the stream do not distribute uniformly. If the amount of documents referring to an entity increases sharply in a short time period when something important is happening around the entity, this time period is detected as one bursty period of this entity. We make an assumption that documents occur in a bursty period of an entity are more likely to be related to it than those not.",0,,False
259,"The bursty periods of an entity can be detected either from stream corpus or from third-party data sources, denoted as internal bursty periods and external bursty periods respectively. Due to the heterogeneity of data sources, we use different burst detection methods to identify internal bursty periods and external bursty periods for entities.",0,,False
260,Internal Burst Detection.,0,,False
261,Burst detection from a stream of documents have been,0,,False
262,"thoroughly investigated in TDT and event detection [22, 17,",1,TD,True
263,31].,0,,False
264,Since our goal is not to develop a new burst detection algo-,0,,False
265,"rithm, we simply adopt Kleinberg's 2-state finite automaton",1,ad,True
266,model [22] to identify bursty periods of entities. There are,0,,False
267,two states q0 and q1 in the finite automaton A. For every,0,,False
268,"target entity e, when A in state q0, it has low emission rate",0,,False
269,0,0,,False
270,",",0,,False
271,|Rd (e)| T,0,,False
272,",",0,,False
273,where,0,,False
274,Rd(e),0,,False
275,is,0,,False
276,the,0,,False
277,number,0,,False
278,of,0,,False
279,all,0,,False
280,documents,0,,False
281,referring to e over the whole time range T . When A in state,0,,False
282,"q1,",0,,False
283,the,0,,False
284,rate,0,,False
285,is,0,,False
286,increased,0,,False
287,to,0,,False
288,1,0,,False
289,",",0,,False
290,s,0,,False
291,|Rd (e)| T,0,,False
292,",",0,,False
293,where,0,,False
294,1,0,,False
295,>,0,,False
296,0,0,,False
297,be-,0,,False
298,cause s is a scaling factor larger than 1.0 and s is empirically,0,,False
299,set as 2.0 in our work. The larger the number of documents,0,,False
300,"referring to entity e at time t, the higher the likelihood of e",0,,False
301,being identified as a bursty entity at t.,0,,False
302,"After performing the burst detection algorithm, if the au-",0,,False
303,"tomaton of entity e is in the state q1 during a time period [tstart, tend], [tstart, tend] is a burtsy period of e with a bursty weight bw(tstart,tend)(e). The bursty weight is defined as the cost improvement incurred by assigning state q1 over the bursty period instead of q0, and can be found in [22].",1,ad,True
304,External Burst Detection.,0,,False
305,"External resources, such as daily view statistics of entities' profile pages, are utilized as temporal features in previous work [2, 29]. Since some KBs do not provide page view statistics for entities as Wikipedia, we also include Google Trends5 to detect external bursts. Akin to Wikipedia statistics, Google Trends can provide a numeric sequence v ,"" (v1,    , vT ) for each entity e, where vi denotes the normalized search volume of e in the ith day.""",1,Wiki,True
306,"We detect external bursts of entity e from v with a tailored moving average (MA) method [27]. More concretely, for each vi in v,",0,,False
307,1. Calculate a moving average sequence of length w as,0,,False
308,M Aw(i),0,,False
309,",",0,,False
310,vi,0,,False
311,+ vi-1,0,,False
312,+    + vi-w+1 w,0,,False
313,"2. Calculate a cutoff c(i) based on previous MA sequences P reMA ,"" (M Aw(1),    , M Aw(i)) as""",0,,False
314,"c(i) , mean(P reMA) +   std(P reMA)",0,,False
315,"3. Detect bursty day sequence d, where d , {i|M Aw(i)  c(i)}",0,,False
316,"4. Calculate the bursty weight sequence w ,"" (w1,    , wT ) for e as follows.""",0,,False
317," 0, i  d",0,,False
318,wi,0,,False
319,",",0,,False
320,"M Aw(i) , c(i)",0,,False
321,i,0,,False
322,d,0,,False
323,5. Compact each segment of consecutive days in d into a,0,,False
324,"bursty period [tstart, tend] of entity e, and the bursty weight bw(tstart,tend) is calculated as the average weight of all the bursts in this period.",0,,False
325,"The moving average length can be varied to detect long-term or short-term bursts. We set the moving average length as 7 days (i.e., w ,"" 7). The cutoff value is empirically set as 2 times the standard deviation of the M A (i.e.,  "", 2).",0,,False
326,Bursty Feature Representation.,0,,False
327,"Given an entity-document pair (e, d), we define a bursty value b(e, d) to represent the temporal correlation between d and e. Let t be the timestamp of d. If t falls in one of e's bursty periods, say [tstart, tend], then b(d, e) is calculated as Eq. 12 shows. If t is not in any bursty period of e, b(d, e) is set as 0.",0,,False
328,"b(d, e)",0,,False
329,",",0,,False
330,(1,0,,False
331,-,0,,False
332,t - tstart ) tend - tstart,0,,False
333,,0,,False
334,"bw(tstart ,tend ) (e),",0,,False
335,(12),0,,False
336,"t  [tstart, tend]",0,,False
337,In,0,,False
338,"Eq. 12, 1 -",0,,False
339,t-tstart tend -tstart,0,,False
340,is,0,,False
341,a decaying coefficient,0,,False
342,reflecting,0,,False
343,the intuition that the documents appear at the beginning,0,,False
344,of a bursty period are more informative than those appear,0,,False
345,5http://www.google.com/trends/,0,,False
346,639,0,,False
347,Geoffery Hinton,0,,False
348,Labeled Categories,0,,False
349,Canadian Computer Scientists,1,ad,True
350,AI Researchers,0,,False
351,Fellows of AAAI,0,,False
352,Parent Categories,0,,False
353,Computer Scientists,0,,False
354,Researchers,0,,False
355,Fellows of Learned Societies,0,,False
356,Labeled Categories,0,,False
357,American Computer Scientists,0,,False
358,Programming Language Researchers,0,,False
359,Fellows of ACM,0,,False
360,Barbara Liskov,0,,False
361,Figure 1: Two entities without common labeled categories but with shared parent categories.,0,,False
362,"at the end. Please note that b(d, e) can be calculated based on external bursts and internal bursts respectively. To avoid using future information during burst detection, we carefully perform burst detection algorithm (either internal or external) in a daily incremental manner. When dealing with an entity-document pair, the bursty periods are determined by the data before the timestamp of this document.",0,,False
363,4.2 Entity-Class Features,0,,False
364,"In ECDMM, besides entity-document features, entity-class features (i.e., g(e) in Eq. 5) are required to learn the mixing coefficients. Here we consider two types of prior knowledge to design entity-class features.",0,,False
365,4.2.1 Profile-based features,0,,False
366,"Each entity in KBs is uniquely identified by its profile page, which contains the basic information of this entity, such as name, address and experiences. We crawl the profile pages of all the entities as a profile collection. After removing stop words, we represent each entity as a feature vector with the bag-of-words model, where term weights are determined by the TF-IDF scheme.",1,ad,True
367,4.2.2 Category-based features,0,,False
368,"Some KBs like Wikipedia organize entities with hierarchical categories. For example, Geoffrey Hinton in Wikipedia, is labeled with categories such as Canadian computer scientists, Artificial intelligence researchers, and Fellows of AAAI. Besides these labeled categories, we take the parent categories of the labeled categories into consideration to deal with the circumstance in Figure 1. The two alike entities can not be correlated if we only consider labeled categories.",1,Wiki,True
369,"Similar to profile-based feature vector, we leverage a ""bagof-categories"" model to represent each entity as a categorybased feature vector. Given an entity without category information, we manually assign a meta-category for it according to its profile. We supplement three meta-categories: person, facility and organization, which can cover all the entities in our dataset. The category-based feature vector of entity e is denoted as gc(e) ,"" (c1(e),    , cN (e)), where N is the total number of categories. ci(e) equals to 1 if e is labeled with category ci, otherwise ci(e) is 0.""",0,,False
370,"Therefore, given a target entity set E, we can generate two feature vectors for each e  E: profile-based vector gp(e) and category-based vector gc(e) respectively.",0,,False
371,5. EXPERIMENTS,0,,False
372,"In this section, we first introduce the dataset for experiments. After that, we report an extensive set of experimental results of our proposed models and baselines in two scenarios of CCR. At last, analysis and discussion are presented based on the experimental results.",1,CCR,True
373,5.1 Dataset,0,,False
374,"We conduct our experiments on TREC-KBA-2013 dataset6, a standard test bed provided by TREC. The data set is composed of a target entity set and a document collection called stream corpus.",1,TREC,True
375,Entity Set.,0,,False
376,"The target entity set includes 121 Wikipedia entities and 20 Twitter entities, more specifically, 98 people, 19 organizations, and 24 facilities from 14 inter-related communities such as small towns like Danville, KY and academic communities like Turing award winners.",1,Wiki,True
377,Stream Corpus.,0,,False
378,"The temporally-ordered stream corpus, containing approximately 1 billion documents crawled from October 2011 to the end of February 2013. Each document is associated with a timestamp indicating its time of crawling. The corpus have been split with documents from October 2011 to February 2012 as training instances and the remainder for evaluation. We adopt the same training/test range setting in our experiments.",1,ad,True
379,Annotation.,0,,False
380,"The relevance of entity-document pairs are labeled following a four-point scale relevance setting, including vital, useful, neural and garbage. The definitions are listed in Table 2.",0,,False
381,Table 2: Four-point scale relevance estimation in TREC-KBA-2013.,1,TREC,True
382,"Vital timely info about the entity's current state, actions, or situation. This would motivate a change to an already up-to-date KB article.",1,ad,True
383,"Useful possibly citable but not timely, e.g., background biography, secondary source information.",0,,False
384,"Neutral informative but not citable, e.g., tertiary source like Wikipedia article itself.",1,Wiki,True
385,"Garbage no information about the target entity could be learned from the document, e.g., spam.",0,,False
386,The details of the annotations for Wikipedia and Twitter entities are demonstrated in Table 3.,1,Wiki,True
387,5.2 Evaluation Scenarios,0,,False
388,"According to different granularity settings, we evaluate the proposed models in two classification scenarios respectively.",0,,False
389,6http://trec-kba.org/kba-stream-corpus-2013.shtml,1,trec,True
390,640,0,,False
391,Table 3: The number of training and test instances (entity-document pairs) for Wikipedia and Twitter entities,1,Wiki,True
392,respectively.,0,,False
393,Training,0,,False
394,Test,0,,False
395,Vital Useful Neutral Garbage Vital Useful Neutral Garbage,0,,False
396,Wikipedia 2096 2257 1162,1,Wiki,True
397,1756 8639 16053 5649 18694,0,,False
398,Twitter 182 326,1,Twitter,True
399,72,0,,False
400,569 1808 2953 1491,0,,False
401,4103,0,,False
402,Total 2278 2583 1234,0,,False
403,2325 10447 19006 7140 22797,0,,False
404,Vital Only.,0,,False
405,"Only vital entity-document pairs are treated as positive instances, and the others are negative instances. This scenario is the essential task of CCR.",1,CCR,True
406,Vital + Useful.,0,,False
407,"Both vital and useful entity-document pairs are treated as positive instances, and the others are negative ones.",0,,False
408,5.3 Experimental Methodology,0,,False
409,Experiments in this section investigate the effectiveness of our proposed mixture model and baseline methods in the two scenarios. The following methods are compared:,0,,False
410," Global Discriminative Model (GDM). As presented in Subsection 3.2, this approach learns a set of fixed weights for all entity-documents pairs.",0,,False
411," Na쮑ve Entity Class-Dependent Discriminative Mixture Model (Na쮑ve ECDMM). This approach uses entitydocument features instead of entity-class features for the mixing component (i.e., g(e) :,"" f (e, d) in Eq. 5) of ECDMM.""",1,ad,True
412, Profile-based Entity Class-Dependent Discriminative Mixture Model (profile ECDMM). This approach utilizes profile-based features as entity-class features for the mixing component of ECDMM.,0,,False
413, Category-based Entity Class-Dependent Discriminative Mixture Model (category ECDMM). This approach utilizes category-based features as entity-class features for the mixing component of ECDMM.,0,,False
414," Combination Entity Class-Dependent Discriminative Mixture Model (combine ECDMM). This approach utilizes profile-based and category-based features together as entity-class features for the mixing component of ECDMM. In our experimental setting, we simply union the two feature vectors together into an integral feature vector.",0,,False
415,"For reference, we also include three top-ranked approaches in the TREC-KBA-2013 track as baselines.",1,TREC,True
416," Official Baseline [13]. A string matching approach implemented by TREC-KBA organizers. For each target entity, they split the entity's name into different tokens and manually composite them into reliable aliases of the entity. These alias are utilized to filter relevant documents from the stream corpus with the strategy that documents referring to any alias are rated as vital to the target entity. A relevance score is estimated according to the length of matched string.",1,TREC,True
417," BIT-MSRA [29]. An entity-unspecific random forests classification model, which is the first place approach in TREC-KBA-2013 track. This approach can be considered as a variant of GDM utilizing a different kernel.",1,TREC,True
418," UDEL [23]. An entity-centric query expansion approach that achieves the second best performance in TREC-KBA-2013 track. Given a target entity, the approach first detect related entities from the profile page of the entity. Then, these related entities are utilized as expansion terms and combine with the target entity as a new query to detect and rank the relevant documents. The optimal weights of query terms are learned from the training data. The relevance score of a document is estimated according to its position in the ranking list.",1,TREC,True
419,5.4 Hidden Classes Analysis,0,,False
420,"For all mixture models, the number of hidden classes are determined according to AIC value. The optimal numbers of latent classes of all variants of ECDMM are reported in Table 4. The number of optimal classes of category ECDMM is obviously larger than the optimal numbers of the other mixture models, which possibly caused by the hierarchical structures of categories in our category-based feature set. Although the incorporation of parent categories can build the correlation between two similar entities without common labeled categories, it brings some noisy correlations in the meantime. For instance, a politician and a business man both living in Florida share a common parent category ""Living people from Ocala, FLorida"", this correlation will mislead the model and come to an non-optimal fit to the data.",1,corpora,True
421,Table 4: Number of hidden classes determined by,0,,False
422,AIC for each mixture model.,0,,False
423,Model,0,,False
424,Vital Vital + Useful,0,,False
425,na쮑ve ECDMM,0,,False
426,9,0,,False
427,10,0,,False
428,profile ECDMM 7,0,,False
429,6,0,,False
430,category ECDMM 13,0,,False
431,12,0,,False
432,combine ECDMM 9,0,,False
433,8,0,,False
434,5.5 Overall Results,0,,False
435,"This section presents the overall performance of all experimental methods. We adopt F1 (harmonic mean between precision and recall), accuracy and AUC (Area Under Curve) [12] as the evaluation measurements. All the measurements are computed in an entity-insensitive manner. In other words, the measurements are computed based on the test pool of all entity-document pairs regardless of specific entities. The results are reported in Table 5.",1,ad,True
436,641,0,,False
437,Table 5: Overall classification results of evaluated models.,0,,False
438,Methods,0,,False
439,Vital Only,0,,False
440,Vital + Useful,0,,False
441,P,0,,False
442,R F1 Accu AUC P,0,,False
443,R F1 Accu,0,,False
444,Official Baseline .171 .942 .290 .175 .475 .540 .972 .694 .532,0,,False
445,BIT-MSRA,0,,False
446,.214 .790 .337 .445 .580 .589 .974 .734 .615,0,,False
447,UDEL,0,,False
448,.169 .806 .280 .259 .473 .573 .893 .698 .579,0,,False
449,GDM,0,,False
450,.218 .507 .304 .587 .556 .604 .913 .727 .565,0,,False
451,na쮑ve ECDMM .223 .400 .286 .644 .548 .627 .912 .744 .656,0,,False
452,profile ECDMM .332 .376 .353 .754 .606 .669 .866 .755 .692,0,,False
453,category ECDMM .316 .422 .362 ..734 .612 .672 .894 .767 .704,0,,False
454,combine ECDMM .397 .418 .407 .783 .640 .703 .877 .780 .731,0,,False
455,AUC .488 .578 .547 .588 .631 .675 .685 .716,0,,False
456,"We notice that combine ECDMM achieves best on all measurements except recall. The official baseline achieve the best recall of all methods, which is not surprising since the official baseline is a manual method to detect as many relevant documents as possible by manually selecting reliable aliases of an entity in advance.",1,ad,True
457,"Compared with GDM regardless of entity class information, all the mixture models employing entity-class features explicitly (i.e., profile ECDMM, category ECDMM and combine ECDMM) achieve better classification performance in both scenarios. Even Na쮑ve ECDMM which does not employ entity-class features explicitly can outperform GDM and other three baselines. This reveals that the mixture model is an effective strategy to enhance the straightforward discriminative model. Na쮑ve ECDMM is not robust in two scenarios. Although it outperforms GDM in vital + useful scenario, it cannot beat GDM in vital only scenario. This is possibly caused by its implicitly employment of entityclass features. The entity-document features are noisy, because the document-related counterpart contributes nothing to capture hidden entity classes.",0,,False
458,"Both profile ECDMM and category ECDMM outperform na쮑ve ECDMM remarkably, revealing that profile-based features and category-based features are effective in modeling hidden entity classes. Category-based features are more promising than profile-based features, which is reasonable because the category labels in KBs contain prior human knowledge on entity class and taxonomy information. Even though combine ECDMM combines profile-based features and category-based features in a straightforward manner, it achieves the best performance. In comparison to GDM, combine ECDMM improve F1 more than 10 percent and AU C approximately 10 percent. We believe that the performance can be enhanced further with more comprehensive entity class information and combination strategy.",0,,False
459,5.6 Fine-grained Results,0,,False
460,This section compares the methods in a fine-grained level.,0,,False
461,We need guarantee our mixture models not only achieve,0,,False
462,"remarkable overall performance, but also perform well in",0,,False
463,"entity-level. Hence, we recomputed the measurements in an",0,,False
464,entity-sensitive manner.,0,,False
465,"Based on the classification results for each entity ei (i ,",0,,False
466,"1,    , M ) in the test set, precision and recall of each model",0,,False
467,"are first calculated as P (ei) and R(ei). Then, we compute",0,,False
468,"the macro-averaged precision and recall over all entities, de-",0,,False
469,"noted as macro P ,",0,,False
470,M 1,0,,False
471,P (ei),0,,False
472,M,0,,False
473,and,0,,False
474,macro,0,,False
475,R,0,,False
476,",",0,,False
477,M 1,0,,False
478,R(ei ),0,,False
479,M,0,,False
480,"respectively. At last, macro-averaged F1 is computed ac-",0,,False
481,cording to macro P and macro R.,0,,False
482,"The macro-averaged measurements in two scenarios are reported in 2(a) and 2(b) respectively. The three baselines are labeled with red color, and the blue dots represent our proposed methods. The best method is labeled with pentagram in both figures. The parallel solid curves are contour lines of F1 value, which means the dots in the same curve achieve same F1 values. The dots lying in upper right achieve higher F1 than the lower left ones. Obviously, combine ECDMM achieves the best F1 in both scenarios. We also find our mixture models (i.e., blue dots) achieve higher precision in vital only scenario, demonstrating our models can detect vital documents more accurately than the baselines.",0,,False
483,5.7 Performance on Unseen Entities,0,,False
484,"This section evaluates the generalization ability of our proposed models to handle unseen entities in the training set. A robust model is able to handle unseen entities as well as training entities. As listed in Table 6, there are 10 unseen entities in the TREC-KBA-2013 dataset. We evaluate the performance of our models on the unseen entity set composed of these 10 entities. We choose macro-averaged accuracy as the evaluation measurement. Due to the sparse positive instances for some unseen entities, it is improper to adopt precision, recall and F1 for evaluation because they possibly become 0, in which case these measurements cannot reflect the performance suitably. The results are reported in Table 7.",1,TREC,True
485,Table 7: The averages of accuracies over 10 unseen,0,,False
486,entities. Methods,0,,False
487,accu@(vital) accu@(vital + useful),0,,False
488,Official Baseline,0,,False
489,.175,0,,False
490,.532,0,,False
491,BIT-MSRA,0,,False
492,.445,0,,False
493,.614,0,,False
494,UDEL,0,,False
495,.259,0,,False
496,.579,0,,False
497,GDM,0,,False
498,.552,0,,False
499,.565,0,,False
500,na쮑ve ECDMM,0,,False
501,.587,0,,False
502,.608,0,,False
503,profile ECDMM,0,,False
504,.623,0,,False
505,.647,0,,False
506,category ECDMM,0,,False
507,.565,0,,False
508,.431,0,,False
509,combine ECDMM,0,,False
510,.580,0,,False
511,.582,0,,False
512,"In both scenarios, the best classification results are achieved by profile ECDMM, which outperforms category ECDMM and combine ECDMM. A possible explanation for the unsatisfactory performance of category ECDMM is that the category information of unseen entities are not covered well in the training set, especially the Twitter entities. For these entities, there is too little category information to model their hidden classes accurately.",1,Twitter,True
513,642,0,,False
514,Recall Recall,0,,False
515,0.9,0,,False
516,Official Baseline,0,,False
517,BIT-MSRA,0,,False
518,UDEL,0,,False
519,GDM,0,,False
520,0.8,0,,False
521,nave_ECDMM,0,,False
522,profile_ECDMM,0,,False
523,category_ECDMM,0,,False
524,combine_ECDMM,0,,False
525,0.7,0,,False
526,0.6,0,,False
527,0.5,0,,False
528,0.4,0,,False
529,0.3,0,,False
530,0.15,0,,False
531,0.2,0,,False
532,0.25,0,,False
533,0.3,0,,False
534,0.35,0,,False
535,0.4,0,,False
536,0.45,0,,False
537,0.5,0,,False
538,Precision,0,,False
539,(a) Vital Only,0,,False
540,0.8 0.75,0,,False
541,0.7 0.65,0,,False
542,0.6 0.45,0,,False
543,Official Baseline BIT-MSRA UDEL GDM nave_ECDMM profile_ECDMM category_ECDMM combine_ECDMM,0,,False
544,0.5,0,,False
545,0.55,0,,False
546,0.6,0,,False
547,0.65,0,,False
548,0.7,0,,False
549,Precision,0,,False
550,(b) Vital + Useful,0,,False
551,Figure 2: Macro-averaged recall VS. macro-averaged precision over all test entities. The best approach is dotted as pentagram.,0,,False
552,Table 6: The statistics of test instances for 10 unseen entities.,0,,False
553,Entity,0,,False
554,KB vital useful neutral/garbage,0,,False
555,"The Ritz Apartment (Ocala,Florida) Wiki 4",1,Wiki,True
556,1,0,,False
557,5,0,,False
558,Keri Hehn,0,,False
559,Wiki 3,1,Wiki,True
560,0,0,,False
561,0,0,,False
562,Chiara Nappi,0,,False
563,Wiki 2,1,Wiki,True
564,3,0,,False
565,55,0,,False
566,Chuck Pankow,0,,False
567,Wiki 7,1,Wiki,True
568,0,0,,False
569,10,0,,False
570,John H. Lang,0,,False
571,Wiki 2,1,Wiki,True
572,0,0,,False
573,1,0,,False
574,Joshua Boschee,0,,False
575,Wiki 191 23,1,Wiki,True
576,5,0,,False
577,MissMarcel,0,,False
578,Twitter 52 13,1,Twitter,True
579,3,0,,False
580,evvnt,0,,False
581,Twitter 1,1,Twitter,True
582,3,0,,False
583,40,0,,False
584,GandBcoffee,0,,False
585,Twitter 0,1,Twitter,True
586,2,0,,False
587,2,0,,False
588,BartowMcDonald,0,,False
589,Twitter 1,1,Twitter,True
590,18,0,,False
591,9,0,,False
592,total 10 3 60 17 3 219 68 44 4 28,0,,False
593,"In vital only scenario, all the variants of our mixture model can achieve better classification performance than GDM and the other baselines. The results validate the flexibility of our mixture model as expectation, which is essential for a practical CCR system. Our mixture model is not only good at handling existing entities in the training set, but also capable of dealing with unseen entities.",1,CCR,True
594,5.8 Bursty Feature Analysis,0,,False
595,"To further validate the effectiveness of the proposed bursty features, we evaluate them with the help of Information Gain (IG). Table 8 reports the IGs of the proposed features in two scenarios. All the IGs are computed following the method proposed in [33]. The higher of the IG achieved by a feature, the more powerful role it plays in the classification. The maximum, mean and median IGs of semantic features are also presented for reference. Since the bursty features are only used in the discriminative counterpart of ECDMM, we evaluate them with GDM.",0,,False
596,"In Table 8, external bursty features perform best out of all features in both scenarios, conforming that external bursts of an entity are accompanied with occurrences of its relevant documents. However, internal bursts are not so helpful in vital + useful scenario as in vital only scenario. This is possibly caused by the incompleteness of the stream corpus. As we know, the stream corpus are crawled from the web, so it is possibly a biased snapshot of the true web. In addition, we only utilize the occurrences of a target entity itself in the stream corpus to detect its internal bursts currently. We can include more evidences to improve the accuracy of",1,ad,True
597,Table 8: Information gain values of features.,0,,False
598,Information Gain,0,,False
599,Feature,0,,False
600,Vital Only Vital + Useful,0,,False
601,external bursty feature 0.130,0,,False
602,0.286,0,,False
603,internal bursty feature 0.020,0,,False
604,0.008,0,,False
605,max1 mean 2 median3,0,,False
606,0.121 0.046 0.039,0,,False
607,0.175 0.081 0.067,0,,False
608,1 maximum IG of all semantic features 2 mean IG of all semantic features 3 median IG of all semantic features,0,,False
609,"internal burst detection. For instance, contextual related entities can be resorted to enhance the detection accuracy of internal burst.",0,,False
610,6. CONCLUSIONS AND FUTURE WORK,0,,False
611,"The objective of Cumulative Citation Recommendation (CCR) is to detect citation-worthy documents for a set of KB entities from a chronological stream corpus. To address the problem of training data insufficiency for less popular entities, we propose an entity class-dependent discriminative mixture model (ECDMM) by introducing a latent entity class layer to model the hidden entity class information. The model can be adjusted to different types of entities by learning flexible combination parameters according to underlying entity classes. Experimental results demonstrate that ECDMM can improve the performance of CCR. Entity-",1,CCR,True
612,643,0,,False
613,"document features and entity-class features are developed for the discriminative and mixing components of ECDMM respectively. In terms of entity-class features, profile-based and category-based features are validated separately and in a combination strategy. The novel bursty features developed as entity-document features are proved rewarding. Our ECDMM with proposed semantic and temporal features can achieve the state-of-the-art performance on TREC-KBA2013 dataset.",1,TREC,True
614,"For future work, we wish to explore more useful entityclass features and apply more proper combination strategies to improve the entity class-dependent mixture model.",0,,False
615,Acknowledgement,0,,False
616,"The authors would like to thank Jing Liu and Ning Zhang for their valuable suggestions and the anonymous reviewers for their helpful comments. This work is funded by the National Program on Key Basic Research Project (973 Program, Grant No. 2013CB329600), National Natural Science Foundation of China (NSFC, Grant Nos. 61472040 and 60873237), and Beijing Higher Education Young Elite Teacher Project (Grant No. YETP1198).",0,,False
617,7. REFERENCES,0,,False
618,"[1] J. Allan. Introduction to topic detection and tracking. In Topic Detection and Tracking, volume 12 of The Information Retrieval Series, pages 116. Springer US, 2002.",1,Track,True
619,"[2] K. Balog and H. Ramampiaro. Cumulative citation recommendation: classification vs. ranking. In SIGIR, pages 941944. ACM, 2013.",0,,False
620,"[3] K. Balog, H. Ramampiaro, N. Takhirov, and K. Nrv캻g. Multi-step classification approaches to cumulative citation recommendation. In OAIR, pages 121128. ACM, 2013.",0,,False
621,"[4] R. Berendsen, E. Meij, D. Odijk, M. d. Rijke, and W. Weerkamp. The university of amsterdam at trec 2012. In TREC. NIST, 2012.",1,trec,True
622,"[5] L. Bonnefoy, V. Bouvier, and P. Bellot. A weakly-supervised detection of entity central documents in a stream. In SIGIR, pages 769772. ACM, 2013.",0,,False
623,"[6] Z. W. C. Tompkins and S. G. Small. Sawus: Siena's automatic wikipedia update system. In TREC. NIST, 2012.",1,wiki,True
624,"[7] J. Dalton and L. Dietz. Bi-directional linkability from wikipedia to documents and back again: Umass at trec 2012 knowledge base acceleration track. In TREC. NIST, 2012.",1,wiki,True
625,"[8] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), pages 138, 1977.",0,,False
626,"[9] L. Dietz and J. Dalton. Time-aware evaluation of cumulative citation recommendation systems. In SIGIR 2013 Workshop on Time-aware Information Access (TAIA2013), 2013.",0,,False
627,"[10] L. Dietz, J. Dalton, and K. Balog. Umass at trec 2013 knowledge base acceleration track. In TREC. NIST, 2013.",1,trec,True
628,"[11] Y. Fang, L. Si, and A. Mathur. Discriminative probabilistic models for expert search in heterogeneous information sources. Information Retrieval, 14(2):158177, 2011.",0,,False
629,"[12] T. Fawcett. An introduction to roc analysis. Pattern Recogn. Lett., 27(8):861874, June 2006.",0,,False
630,"[13] J. Frank, S. J. Bauer, M. Kleiman-Weiner, D. A. Roberts, N. Triouraneni, C. Zhang, and C. R`e. Evaluating stream filtering for entity profile updates for trec 2013. In TREC. NIST, 2013.",1,trec,True
631,"[14] J. R. Frank, M. Kleiman-Weiner, D. A. Roberts, F. Niu, C. Zhang, C. Re, and I. Soboroff. Building an Entity-Centric Stream Filtering Test Collection for TREC 2012. In TREC. NIST, 2012.",1,TREC,True
632,"[15] G. G. Gebremeskel, J. He, A. P. d. Vries, and J. Lin. Cumulative citation recommendation: A feature-aware comparison of approaches. In Database and Expert Systems Applications (DEXA), pages 193197. IEEE, 2014.",0,,False
633,"[16] A. Genkin, D. D. Lewis, and D. Madigan. Large-scale bayesian logistic regression for text categorization. Technometrics, 2007.",1,ad,True
634,"[17] Q. He, K. Chang, and E.-P. Lim. Using burstiness to improve clustering of topics in news streams. In ICDM, pages 493498. IEEE, 2007.",0,,False
635,"[18] Q. He, K. Chang, E.-P. Lim, and J. Zhang. Bursty feature representation for clustering text streams. In SDM, pages 491496. SIAM, 2007.",0,,False
636,"[19] D. Hong and L. Si. Mixture model with multiple centralized retrieval algorithms for result merging in federated search. In SIGIR, pages 821830. ACM, 2012.",0,,False
637,"[20] R. Jin, L. Si, and C. Zhai. A study of mixture models for collaborative filtering. Information Retrieval, 9(3):357382, 2006.",0,,False
638,"[21] B. Kjersten and P. McNamee. The hltcoe approach to the trec 2012 kba track. In TREC. NIST, 2012.",1,trec,True
639,"[22] J. Kleinberg. Bursty and hierarchical structure in streams. In KDD, pages 91101. ACM, 2002.",0,,False
640,"[23] X. Liu, J. Darko, and H. Fang. A related entity based approach for knowledge base acceleration. In TREC. NIST, 2013.",1,TREC,True
641,"[24] A. Y. Ng and M. I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 841848. MIT Press, 2002.",0,,False
642,"[25] A. D. O. Gross and H. Toivonen. Term association analysis for named entity filtering. In TREC. NIST, 2012.",1,TREC,True
643,"[26] J. H. C. B. S. Araujo, G. Gebremeskel and A. de Vries. Cwi at trec 2012 kba track and session track. In TREC. NIST, 2012.",1,trec,True
644,"[27] M. Vlachos, C. Meek, Z. Vagena, and D. Gunopulos. Identifying similarities, periodicities and bursts for online search queries. In SIGMOD, pages 131142. ACM, 2004.",0,,False
645,"[28] J. Wang, L. Liao, D. Song, L. Ma, C.-Y. Lin, and Y. Rui. Resorting relevance evidences to cumulative citation recommendation for knowledge base acceleration. In WAIM, 2015.",0,,False
646,"[29] J. Wang, D. Song, C.-Y. Lin, and L. Liao. Bit and msra at trec kba ccr track 2013. In TREC. NIST, 2013.",1,trec,True
647,"[30] Q. Wang, L. Si, and D. Zhang. A discriminative data-dependent mixture-model approach for multiple instance learning in image classification. In ECCV, pages 660673. 2012.",0,,False
648,"[31] J. Weng and B.-S. Lee. Event detection in twitter. In ICWSM, volume 11, pages 401408. AAAI, 2011.",1,CW,True
649,"[32] Y. Yang and X. Liu. A re-examination of text categorization methods. In SIGIR, pages 4249. ACM, 1999.",0,,False
650,"[33] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization. In ICML, pages 412420, 1997.",0,,False
651,"[34] Y. Yang, T. Pierce, and J. Carbonell. A study of retrospective and on-line event detection. In SIGIR, pages 2836. ACM, 1998.",0,,False
652,"[35] W. X. Zhao, R. Chen, K. Fan, H. Yan, and X. Li. A novel burst-based text representation model for scalable event detection. In ACL, pages 4347. ACL, 2012.",0,,False
653,"[36] M. Zhou and K. C.-C. Chang. Entity-centric document filtering: boosting feature mapping through meta-features. In CIKM, pages 119128. ACM, 2013.",0,,False
654,644,0,,False
655,,0,,False

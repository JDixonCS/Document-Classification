,sentence,label,data,regex
0,An Initial Investigation into Fixed and Adaptive Stopping Strategies,0,,False
1,David Maxwell and Leif Azzopardi,0,,False
2,"School of Computing Science University of Glasgow Glasgow, Scotland",0,,False
3,d.maxwell.1@research.gla.ac.uk Leif.Azzopardi@Glasgow.ac.uk,0,,False
4,ABSTRACT,0,,False
5,"Most models, measures and simulations often assume that a searcher will stop at a predetermined place in a ranked list of results. However, during the course of a search session, real-world searchers will vary and adapt their interactions with a ranked list. These interactions depend upon a variety of factors, including the content and quality of the results returned, and the searcher's information need. In this paper, we perform a preliminary simulated analysis into the influence of stopping strategies when query quality varies. Placed in the context of ad-hoc topic retrieval during a multi-query search session, we examine the influence of fixed and adaptive stopping strategies on overall performance. Surprisingly, we find that a fixed strategy can perform as well as the examined adaptive strategies, but the fixed depth needs to be adjusted depending on the querying strategy used. Further work is required to explore how well the stopping strategies reflect actual search behaviour, and to determine whether one stopping strategy is dominant.",1,ad,True
6,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval:Search Process H.3.4 [Information Storage and Retrieval]:Systems and Software:Performance Evaluation,0,,False
7,Keywords Search Strategies; Search Behaviour; Stopping Strategies; Evaluation,0,,False
8,1. INTRODUCTION,1,DUC,True
9,"Most models, simulations and measures that examine or evaluate searcher interaction typically rely on the assumption that searchers will reach a fixed depth, with precisionat-n (P @n) being a prime example. In practice, this assumption is unlikely to hold. Indeed, searchers are likely to vary their interaction and the depth to which they inspect snippets and documents, depending on the performance of the system, their information need/task, and the amount of",0,,False
10,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.",1,ad,True
11,"SIGIR'15, August 09 - 13, 2015, Santiago, Chile Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00 DOI: http://dx.doi.org/10.1145/2766462.2767802",0,,False
12,Kalervo Järvelin and Heikki Keskustalo,0,,False
13,"School of Information Sciences University of Tampere Tampere, Finland",0,,False
14,kalervo.jarvelin@uta.fi heikki.keskustalo@uta.fi,0,,False
15,"time they have available [4, 17, 18, 20, 22]. For example, a searcher may issue a query that does not return any relevant documents (i.e. a `dud' query). It is then likely that once the searcher inspects a few snippets and/or documents, (s)he will conclude that the issued query was unsuccessful. In this case, the searcher is then likely to issue a new query rather than continue down the current results list. This intuition has been confirmed by empirical analysis, where research shows that searchers examined significantly fewer documents when the search system failed to retrieve any relevant material in the top ten results, in contrast to when it did [1]. Thus, real searchers are inherently adaptive, and their behaviour is conditioned on the quality of the ranked lists that they interact with. In this paper, we examine the aforementioned fixed depth assumption, and perform a preliminary analysis to determine the impact of assuming a fixed depth when interacting with a search system over the course of a session. To this end, we will propose two adaptive stopping strategies - motivated by various stopping rules and compare them to the fixed depth stopping strategy.",1,ad,True
16,"While we assume that adaptive approaches may perform better, we hypothesise that this depends on the quality of the queries issued. For example, if all queries issued by a searcher are `duds', then the stopping strategy may be irrelevant. However, what if all their queries are successful, or if their queries are of varying quality? What then is the influence of the stopping strategy on overall performance?",1,ad,True
17,2. RELATED WORK,0,,False
18,"Knowing when to stop is considered a fundamental aspect of human thinking [15]. Consequently, IR researchers have examined stopping behaviours in a bid to understand why, and when, searchers stop. Many studies investigating when people stop searching have concluded that the decision was mainly based on intuition, or the subjective notion of ""feeling good enough"" [22] - often termed satisficing [9, 21, 22]. Furthermore, decisions have also been shown to be highly dependent upon the task type being undertaken, time constraints, and various actions that the searcher performs [4, 17, 20, 22]. However, in this work, we are interested in the stopping rules and heuristics that have been proposed [5, 6, 7, 11, 15] and how to operationalise them.",0,,False
19,"When formulating such rules, researchers have considered stopping behaviours with respect to the overall search task (e.g. ceasing the search when enough information has been acquired to meet some threshold, or satisfy some task or goal) [5, 6, 15]. For example, Nickles et al. [15] proposed a",0,,False
20,903,0,,False
21,"number of rules investigating the sufficiency of information: the mental list rule, where searchers construct a mental list of criteria about a given item (such as a car) that must be satisfied before stopping; the representational stability rule, where a searcher continues to examine information until the underlying mental model that they possess of the topic begins to stabilise; the difference threshold rule, where a searcher sets an a priori difference level to gauge when he or she is not learning anything new; and the magnitude threshold rule, where a searcher has a cumulative amount of information that must be reached before he or she stops.",0,,False
22,"Other work focuses on stopping behaviour at the query level (e.g. whether the searcher continues to examine documents, or decides to submit a new query) [7, 11]. For example, Cooper [7] devised two stopping rules for examining a list of ranked results: the frustration point rule, where a searcher stops after a certain number of non-relevant documents are encountered; and the satisfaction stopping rule, where searchers would stop when a certain number of relevant documents were obtained. Later, Cooper [8] developed rules using utility theory, positing that searchers stop examining documents once the effort of examining another document outweighs the benefit of moving to a new results list. Similar rules can be obtained from Search Economic Theory [1] and Information Foraging Theory [16].",0,,False
23,"In this work, we focus on query level stopping rules and implement two variations of the frustration point rule to explore the relationship between stopping strategies and query performance. This rule was also implemented by Lin and Smucker [12]. When navigating similar documents, their simulated searchers stopped after seeing two contiguous nonrelevant documents. Our work however considers different implementations, and explores a range of stopping thresholds for ad-hoc topic retrieval.",1,ad-hoc,True
24,3. EXPERIMENTAL METHOD,0,,False
25,"To explore the influence of the stopping strategy on overall performance, we conducted a number of simulations where we varied the stopping strategy and querying strategy. This was to determine which stopping strategy achieved the best performance when the quality of queries was varied. Our simulations consisted of `searchers' performing ad-hoc topic retrieval over a series of topics on two TREC collections.",1,ad-hoc,True
26,"Corpora, Topics and System: We used two test collections: TREC AQUAINT with the TREC 2005 Robust Track topics, and TREC WT2g with the TREC Ad-Hoc and Small Web topics. Both topic sets were comprised of 50 topics. Each collection was indexed using the Whoosh IR toolkit1, where stopwords2 were removed and Porter stemming applied. The retrieval model used was PL2 (c , 10.0).",1,TREC,True
27,3.1 Simulations,0,,False
28,"While various methods have been proposed to model or simulate session-based retrieval [2, 3, 14, 19], we utilise an adaption of the method proposed by Baskaya et al. [3] as follows. A searcher (1) issues a query to the system, and then (2) proceeds to examine the first/next result snippet, or decides to issue a new query (back to (1)). If a given snippet is considered relevant, (3) the document is examined. If said document is considered relevant, (4) it is marked as",1,ad,True
29,1 https://pypi.python.org/pypi/Whoosh/ 2Fox's classical stopword list was used. Refer to http://git.io/vT3So for the complete list.,0,,False
30,Table 1: Summary of interaction times (in seconds) used for the simulations in this study.,0,,False
31,Time Required to...,0,,False
32,Seconds,0,,False
33,...issue a query ...undertake an initial results page examination ...examine an individual snippet ...examine a document ...mark a document as relevant,0,,False
34,15.1 1.1 1.3 21.45 2.57,0,,False
35,"such, and the searcher returns to (2). If either a snippet or document are considered non-relevant, the searcher returns to (2) - with the document remaining unmarked.",0,,False
36,"Experimental Setup: At (2), Baskaya et al. [3] assumed in one of their baselines a fixed depth of ten. In this paper, we consider a range of fixed depths n. In addition, we also include two adaptive stopping strategies (see Section 3.2). To generate queries of varying quality, we will employ a range of strategies as suggested by Keskustalo et al. [10] (see Section 3.3). To determine the relevance of a document, the corresponding TREC relevance assessments are typically used. Here, the action/decision of clicking on a relevant snippet or marking a relevant document is determined in a probabilistic manner. In this work, we used the probabilities of clicking on a (non)relevant snippet and the probabilities of marking a document as (non)relevant from the study performed by Smucker and Clarke [18]. In previous work, for each run, whether a document is examined or marked relevant is determined on the fly. This means that for the same query (or even a different query), the same snippet/document can be considered relevant and then non-relevant, or vice versa. In this paper, we pre-compute whether a document is considered relevant or not a priori, so that for different thresholds, depths and other factors, the same judgements are made for each run. This means we can perform a pairwise comparison, thus reducing the total number of simulations required.",1,ad,True
37,"Finally, the goal of the search task is to find as many relevant documents in a fixed time period of 1200 seconds (20 minutes). For each action performed during the simulation, the times in Table 1 were used. The estimates for each action were obtained from a user study we performed with 48 subjects over the TREC 2005 Robust Track [13].",1,TREC,True
38,3.2 Stopping Strategies,0,,False
39,"We considered three stopping strategies - the default fixed depth strategy (SS1 ), and two other strategies based on the frustration point rule (SS2 & SS3 ) [7].",0,,False
40,"SS1 (Fixed Depth): This fixed stopping strategy encodes the heuristic that a searcher will stop examining a results list after they have viewed x1 snippets, regardless of their relevance to the given topic.",0,,False
41,"SS2 (Total Non-Relevant): Under this stopping strategy, the searcher will stop once they have observed x2 nonrelevant snippets. If a snippet has been previously seen and was considered non-relevant, it is included in the count.",0,,False
42,"SS3 (Contiguous Non-Relevant): Similar to SS2 above, the searcher will stop using this strategy when they observe x3 non-relevant documents in a row. As above, previously seen non-relevant snippets are included in the count.",0,,False
43,"For this analysis, we set the thresholds (x1, x2 & x3) to be 1-20 in steps of 1, and 25-50 in steps of 5. The final value of 50 was sufficiently deep enough such that if a simulated",0,,False
44,904,0,,False
45,"searcher only issued one query and examined all documents, they would run out of time. Note that for SS1, x1 corresponds to the maximum depth per query, whereas for SS2 and SS3, x2 and x3 represent the minimum depth per query. For example, when x2 ,"" 3, a searcher is willing to tolerate three non-relevant snippets. However, they may see two relevant snippets in the process, and thus stop at a depth of five. In our results, we will report the average depth per query for each xi. This will therefore allow us to compare across the three implemented stopping strategies.""",0,,False
46,3.3 Querying Strategies and Selection,1,Query,True
47,"Keskustalo et al. [10] define and analyse a number of different querying strategies. For the purposes of this paper, we have selected the best performing querying strategy (referred to as QS3 , consisting of two pivot terms and one other term), and the worst performing querying strategy (referred to as QS1 , consisting of a series of single terms) [3]. These querying strategies were used to determine how the different stopping strategies performed when combined with differing querying quality. We also implemented a blended querying strategy QS1+3 , which interleaved the queries generated by QS1 and QS3 . QS1+3 was implemented to determine how robust the different stopping strategies were against poor performing (or `dud') queries.",0,,False
48,"Queries were generated as follows. For each topic, the title and description were used to create a Maximum Likelihood Estimate (MLE) language model, i.e. p(term|topic). For QS1 , we then extracted a list of all single terms, ranking them according to this probability. For QS3 , we took all two term combinations of the title terms, and selected the pair with the highest joint probability as the pivot. A list of three term candidate queries q was then constructed by appending another term from the topic to the pivot. These were then ranked according to p(q|topic).",0,,False
49,4. RESULTS,0,,False
50,"Figure 1 plots the mean depth per query3 versus the rate of gain per second (averaged over all sessions and topics), given each threshold value for the AQUAINT collection4. All nine combinations of the aforementioned querying strategies (QS1 , QS1+3 & QS3 ) and stopping strategies (SS1 , SS2 & SS3 ) are shown. From the plots, we can see that the adaptive stopping strategies (SS2 & SS3 ) generally outperformed the fixed depth strategy (SS1 ), regardless of the querying strategy employed, or the depth attained.",1,AQUAINT,True
51,"Table 2 reports the maximum gain attained across each of the stopping and querying strategies for both AQUAINT and WT2g, together with the thresholds (xi) and mean depth per query (d). To determine whether one stopping strategy outperformed another, we performed a series of paired t-tests comparing each stopping strategy with a given querying strategy (e.g. SS1 & QSx vs. SS2 & QSx ). We found that there were no significant differences at p , 0.05.",1,AQUAINT,True
52,"To examine why this was the case, given that adaptive strategies should be intuitively more successful, we evaluated the retrieval performance of the queries that were issued during the simulation. Table 3 reports the mean retrieval performance metrics (P @5, P @10 & P @20) for",1,ad,True
53,3The depth for a query is the lowest item in the results list for which a simulated searcher examined the associated snippet of. Mean depth is averaged for all simulated searchers over each query issued. 4Similar plots were obtained for the WT2g collection.,1,WT,True
54,x 10-3 AQUAINT: Mean Depth/Query vs. Mean Rate of Gain,1,AQUAINT,True
55,9,0,,False
56,8,0,,False
57,7,0,,False
58,Mean Rate of Gain/Second,0,,False
59,6,0,,False
60,5,0,,False
61,4,0,,False
62,SS1 w/ QS1,0,,False
63,3,0,,False
64,SS2 w/ QS1,0,,False
65,SS3 w/ QS1,0,,False
66,SS1 w/ QS1+3,0,,False
67,2,0,,False
68,SS2 w/ QS1+3,0,,False
69,SS3 w/ QS1+3,0,,False
70,SS1 w/ QS3,0,,False
71,1,0,,False
72,SS2 w/ QS3,0,,False
73,SS3 w/ QS3,0,,False
74,0,0,,False
75,5,0,,False
76,10,0,,False
77,15,0,,False
78,20,0,,False
79,25,0,,False
80,Mean Depth/Query,1,Query,True
81,Figure 1: The mean depth per query versus the,0,,False
82,mean rate of gain per second for the AQUAINT col-,1,AQUAINT,True
83,"lection, for each query and stopping strategy. Each",0,,False
84,"point represents a particular threshold value, i.e. xi.",0,,False
85,"both collections and for each of the three querying strategies used (QS1 , QS1+3 & QS3 ). In line with previous work, the three-term querying strategy (QS3 ) outperformed the single-term strategy (QS1 ) [10], while blended QS1+3 queries performed better than those generated by QS1 , but worse than QS3 queries. Notably, each querying strategy had a high variance. This resulted in 35-40% of queries issued under QS1 achieving P @10 ,"" 0 (`dud' queries), while approximately 25% of queries under QS1+3 and QS3 were `duds'. This suggests that our manipulation provided a mixture of highly performing and underperforming queries, yet the performance of the best cases for the fixed strategy were similar in overall gain to the adaptive strategies. This may be an artefact of the simulation as fixed interaction probabilities were used, whereas searchers may be more or less likely to click depending on the quality of the list (and the information scent [21]). In future work, we will examine how the behaviour of a searcher changes given the quality of the ranked list to provide a more grounded simulation.""",1,ad,True
86,"When we examined the threshold for the fixed depth stopping strategy (SS1 ), we observed that it was quite high, ranging from 25-50. This range is much deeper than inspecting ten results per query, which is typically assumed in simulations [3]. Furthermore, real searchers are unlikely to know in advance the average performance of their queries or adopt only one particular querying strategy, so it is unlikely that a real searcher would subscribe to a fixed depth strategy. Similarly to SS1 , SS2 also requires a range of thresholds in order to achieve maximum gain. In contrast, SS3 is more consistent, where thresholds range from three to five over both collections, depending on the querying strategy used.",1,ad,True
87,905,0,,False
88,Table 2: Maximum cumulative gain values with corresponding thresholds and depths for each stopping and querying strategy for AQUAINT and WT2g. Significance tests indicate that there are no differences between stopping strategies.,1,AQUAINT,True
89,QS1,0,,False
90,QS1+QS3,0,,False
91,QS3,0,,False
92,AQUAINT,1,AQUAINT,True
93,4.5 ± 1.1,0,,False
94,7.6 ± 1.1,0,,False
95,10.6 ± 1.8,0,,False
96,"SS1 x1 ,"" 45, d "", 18.7 x1 ,"" 25, d "", 14.3 x1 ,"" 30, d "", 15.9",0,,False
97,4.7 ± 0.9,0,,False
98,7.8 ± 1.1,0,,False
99,10.9 ± 0.9,0,,False
100,"SS2 x2 ,"" 19, d "", 17.9 x2 ,"" 9, d "", 12.8 x2 ,"" 9, d "", 13.9",0,,False
101,4.7 ± 0.7,0,,False
102,8 ± 1.1,0,,False
103,10.5 ± 0.7,0,,False
104,"SS3 x3 ,"" 5, d "", 18.5 x3 ,"" 3, d "", 12.5 x3 ,"" 3, d "", 13.7",0,,False
105,3.9 ± 1,0,,False
106,4.9 ± 0.9,0,,False
107,6.6 ± 1.4,0,,False
108,"SS1 x1 ,"" 45, d "", 18.2 x1 ,"" 25, d "", 13.6 x1 ,"" 50, d "", 18.9",0,,False
109,3.9 ± 0.7,0,,False
110,5 ± 0.7,0,,False
111,6.7 ± 0.9,0,,False
112,"SS2 x2 ,"" 17, d "", 17 x2 ,"" 9, d "", 12.8 x2 ,"" 30, d "", 19.6",0,,False
113,3.8 ± 0.6,0,,False
114,4.9 ± 0.6,0,,False
115,6.6 ± 0.8,0,,False
116,"SS3 x3 ,"" 5, d "", 19 x3 ,"" 3, d "", 12.2 x3 ,"" 4, d "", 16.9",0,,False
117,WT2g,1,WT,True
118,"Table 3: Means (and standard deviations) of the queries issued during the simulation for each querying strategy, both for AQUAINT (AQ.) and WT2g.",1,AQUAINT,True
119,QS1,0,,False
120,QS1+3,0,,False
121,QS3,0,,False
122,WT2g AQ.,1,WT,True
123,P@5 P@10 P@20,0,,False
124,0.2 ± 0.27 0.21 ± 0.26 0.13 ± 0.18,0,,False
125,0.32 ± 0.31 0.31 ± 0.29,0,,False
126,0.2 ± 0.2,0,,False
127,0.41 ± 0.31 0.41 ± 0.28 0.32 ± 0.25,0,,False
128,P@5 P@10 P@20,0,,False
129,0.19 ± 0.26 0.19 ± 0.22 0.15 ± 0.21,0,,False
130,0.25 ± 0.29 0.26 ± 0.25 0.18 ± 0.21,0,,False
131,0.32 ± 0.33 0.35 ± 0.29 0.29 ± 0.25,0,,False
132,"This strategy is also more in line with intuition. Here, the searcher moves to the next query after encountering three to five contiguous non-relevant documents. This suggests that SS3 is more robust across query performance, but further research is required to confirm this.",0,,False
133,5. SUMMARY AND FUTURE WORK,0,,False
134,"In this paper, we used simulations to examine different stopping strategies. Overall, the adaptive stopping strategies tend to outperform the fixed stopping strategy. However, to our surprise, this was not significantly so. The caveat being that for the fixed stopping strategy to provide similar performance, the right threshold needs to be chosen. In practice, this would require a different type of adaptive behaviour, where the searcher changes the depth they are willing to go to based on their querying strategy. This seems unlikely. The most robust stopping strategy appeared to be SS3 (contiguous non-relevant), with a threshold of around three to five non-relevant documents. This stopping strategy seems to match better with intuition, but whether real searchers adopt such a strategy is an open question. In future work, we will examine a greater variety of querying strategies/selection methods (such as lower and higher precision) to determine whether the adaptive strategies result in greater gains. We shall also explore which strategy, if any, best characterises real searcher stopping behaviour, and explore whether there is a relationship between results list quality and interaction probabilities. Other adaptive stopping strategies will be examined, as well as comparing proposed strategies against observed stopping behaviours.",1,ad,True
135,Acknowledgments: We would like to thank the ESF-funded,0,,False
136,MUMIA COST Action (ref. ECOST-STSM-IC1002-080914-049840).,0,,False
137,"We would also like to thank Hora¸tiu Bota, Sean McKeown, Alas-",0,,False
138,tair Maxwell and Paul Thomas for their input.,0,,False
139,References,0,,False
140,"[1] L. Azzopardi. Modelling interaction with economic models of search. In Proc. 37th ACM SIGIR, pages 3­12, 2014.",0,,False
141,"[2] F. Baskaya, H. Keskustalo, and K. J¨arvelin. Time drives interaction: Simulating sessions in diverse searching environments. In Proc. 35th ACM SIGIR, pages 105­114, 2012.",0,,False
142,"[3] F. Baskaya, H. Keskustalo, and K. J¨arvelin. Modeling behavioral factors in interactive information retrieval. In Proc. 22nd ACM CIKM, pages 2297­2302, 2013.",0,,False
143,"[4] M. J. Bates. The fallacy of the perfect thirty-item online search. RQ, 24(1):pp. 43­50, 1984.",0,,False
144,"[5] G. Browne, M. Pitts, and J. Wetherbe. Stopping rule use during web-based search. In 38th HICSS, page 271, 2005.",0,,False
145,"[6] G. J. Browne, M. G. Pitts, and J. C. Wetherbe. Cognitive stopping rules for terminating information search in online tasks. MIS Quarterly, 31(1):89­104, 2007.",0,,False
146,"[7] W. S. Cooper. On selecting a measure of retrieval effectiveness part ii. implementation of the philosophy. J. of the American Society for Info. Sci., 24(6):413­424, 1973.",0,,False
147,"[8] W. S. Cooper. The paradoxical role of unexamined documents in the evaluation of retrieval effectiveness. Info. Processing and Management, 12(6):367 ­ 375, 1976.",1,ad,True
148,"[9] M. Dostert and D. Kelly. Users' stopping behaviors and estimates of recall. In Proc. 32nd ACM SIGIR 2009, pages 820­821, 2009.",0,,False
149,"[10] H. Keskustalo, K. J¨arvelin, A. Pirkola, T. Sharma, and M. Lykke. Test collection-based ir evaluation needs extension toward sessions -- a case of extremely short queries. In Proc. 5th AIRS, pages 63­74, 2009.",0,,False
150,"[11] D. Kraft and T. Lee. Stopping rules and their effect on expected search length. IPM, 15(1):47 ­ 58, 1979.",0,,False
151,"[12] J. Lin and M. D. Smucker. How do users find things with pubmed?: Towards automatic utility evaluation with user simulations. In Proc. 31st ACM SIGIR, pages 19­26, 2008.",0,,False
152,"[13] D. Maxwell and L. Azzopardi. Stuck in traffic: How temporal delays affect search behaviour. In Proc. 5th IIiX, pages 155­164, 2014.",0,,False
153,"[14] A. Moffat, P. Thomas, and F. Scholer. Users versus models: What observation tells us about effectiveness metrics. In Proc. 22nd ACM CIKM, pages 659­668, 2013.",0,,False
154,"[15] K. R. Nickles, S. P. Curley, and P. G. Benson. Judgment-based and reasoning-based stopping rules in decision making under uncertainty. Technical report, U. of Minnesota, 1995.",0,,False
155,"[16] P. Pirolli and S. Card. Information foraging. Psychological Review, 106:643­675, 1999.",0,,False
156,"[17] C. Prabha, L. S. Connaway, L. Olszewski, and L. R. Jenkins. What is enough? Satisficing information needs. J. of Documentation, 63(1):74­89, 2007.",0,,False
157,"[18] M. D. Smucker and C. L. Clarke. Time-based calibration of effectiveness measures. In Proc. 35th ACM SIGIR, pages 95­104, 2012.",0,,False
158,"[19] P. Thomas, A. Moffat, P. Bailey, and F. Scholer. Modeling decision points in user search behavior. In Proc. 5th IIiX, pages 239­242, 2014.",0,,False
159,"[20] E. G. Toms and L. Freund. Predicting stopping behaviour: A preliminary analysis. In Proc. 32nd ACM SIGIR, pages 750­751, 2009.",0,,False
160,"[21] W. C. Wu, D. Kelly, and A. Sud. Using information scent and need for cognition to understand online search behavior. In Proc. 37th ACM SIGIR, pages 557­566, 2014.",0,,False
161,"[22] L. Zach. When is ""enough"" enough? modeling the information-seeking and stopping behavior of senior arts administrators: Research articles. J. of the American Society for Info. Sci. and Tech., 56(1):23­35, 2005.",1,ad,True
162,906,0,,False
163,,0,,False

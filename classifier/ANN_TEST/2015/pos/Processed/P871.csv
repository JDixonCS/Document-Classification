,sentence,label,data,regex
0,Modeling Multi-query Retrieval Tasks Using Density Matrix Transformation,0,,False
1,"Qiuchi Li1,2, Jingfei Li1, Peng Zhang1, Dawei Song 1,3",0,,False
2,"1School of Computer Science and Technology, Tianjin University, China 2Department of Electronic Engineering, Tsinghua University, China 3The Computing Department, The Open University, United Kingdom",0,,False
3,"liqiuchi2015@gmail.com, jingfl@foxmail.com, {pzhang, dwsong}@tju.edu.cn",0,,False
4,ABSTRACT,0,,False
5,"The quantum probabilistic framework has recently been applied to Information Retrieval (IR). A representative is the Quantum Language Model (QLM), which is developed for the ad-hoc retrieval with single queries and has achieved significant improvements over traditional language models. In QLM, a density matrix, defined on the quantum probabilistic space, is estimated as a representation of user's search intention with respect to a specific query. However, QLM is unable to capture the dynamics of user's information need in query history. This limitation restricts its further application on the dynamic search tasks, e.g., session search. In this paper, we propose a Session-based Quantum Language Model (SQLM) that deals with multi-query session search task. In SQLM, a transformation model of density matrices is proposed to model the evolution of user's information need in response to the user's interaction with search engine, by incorporating features extracted from both positive feedback (clicked documents) and negative feedback (skipped documents). Extensive experiments conducted on TREC 2013 and 2014 session track data demonstrate the effectiveness of SQLM in comparison with the classic QLM.",1,LM,True
6,Categories and Subject Descriptors,0,,False
7,"H.3.3 [Information Search and Retrieval]: Query formulation, Relevance feedback, Retrieval Models",1,Query,True
8,Keywords,0,,False
9,"Quantum Language Model, Session Search, Density Matrix Transformation",1,Session,True
10,1. INTRODUCTION,1,DUC,True
11,"Recently, various quantum theory (QT) based IR models are developed under the inspiration of the pioneering work",0,,False
12,"Corresponding Author: Dawei Song, Email: dwsong@tju.edu.cn",0,,False
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767819 .",1,ad,True
14,"of van Rijsbergen [8], which draws a clear connection between the QT and IR. Piwowarski et al. [5] proposed that queries and documents can be modeled as density operators and subspaces respectively, but the tensor space based representation method has not led to good performance. The advent of Quantum Language Model (QLM) [7] , a representative QT-based IR model, successfully solved this issue. In QLM, both single terms and compound term dependencies are represented as projectors in a vector space, while queries and documents are represented as density matrices defining a quantum probability distribution in the space. An EMbased training method for the estimation of density matrix is then devised [7]. The advantages of QLM over traditional language models have been demonstrated from both theoretical and experimental perspectives.",1,ad,True
15,"Despite its success in the ad-hoc retrieval, QLM (referred to as classical QLM in the rest of the paper) is solely targeted on single ad-hoc queries. It is insufficient to capture the dynamics of users' information need in response to the user's interaction with the search engine. As a result, it is difficult for the classical QLM to be applied in more complex search tasks, such as multi-query session search.",1,ad-hoc,True
16,"To address this challenge, we propose to integrate user' short-term interaction information into the estimation of QLM for the current query, and correspondingly a novel Session-based QLM (SQLM) is proposed. The evolution of the user's information need within a search session is modeled by the density matrix transformation, i.e., transforming the original density matrices (for single queries) by some principled rules based on user interactions (e.g., the click and dwell time). We also put forward the concepts of positive projectors and negative projectors extracted from the positive feedback documents (clicked documents) and negative feedback documents (skipped documents), respectively, to enhance the representation ability of the QLM. Specially, a novel training algorithm for QLM with different projectors is devised. Although there exists a body of related work [3][9] for integrating users' interaction information in IR models, they did not model term dependencies in queries and documents, compared with the SQLM proposed in this paper.",1,ad,True
17,2. QLM PRELIMINARIES,1,LM,True
18,2.1 Quantum Probability,0,,False
19,"In the field of IR, the quantum probability is defined on a real finite space Rn [7] for simplicity (originally, defined on the infinite Hilbert space). In this paper, we use",0,,False
20,871,0,,False
21,"the Dirac's notation to represent a unit column vector u  Rn as |u and its transpose uT as u|, respectively. An",0,,False
22,elementary quantum event can be uniquely represented by a projector onto a 1-dimensional subspace of Rn. For a unit,0,,False
23,"vector |u , the corresponding elementary quantum event, or",0,,False
24,"the projector, is denoted as |u u|. Suppose |e1 ,|e2 ,...,|en forms an orthonormal basis for Rn, then each unit vector",0,,False
25,"|v can be uniquely expressed as the superposition of |ei : |v ,"" i vi|ei , where i vi2"",1.",0,,False
26,A measure µ is introduced to define the quantum proba-,0,,False
27,bility on Rn. It satisfies two conditions: (I) for every pro-,0,,False
28,"jector |v v|, µ(|v v|)  [0, 1] and (II) for any orthonormal",0,,False
29,"basis {|ui } for Rn, we have",0,,False
30,"n i,1",0,,False
31,µ(|ui,0,,False
32,"ui|) , 1.",0,,False
33,The,0,,False
34,Gleason's Theorem [2] can prove the existence of a mapping,0,,False
35,"function µ(|v v|) , tr(|v v|) for any vector v given a density matrix   Sn (Sn is the density matrix space con-",0,,False
36,taining all n-by-n positive semi-definite matrices with trace,0,,False
37,"1, i.e., tr() ,"" 1). Formally, any density matrix  assigns a""",0,,False
38,"quantum probability for each quantum event in vector space Rn , thereby uniquely determining a quantum probability",0,,False
39,distribution over the vector space.,0,,False
40,2.2 Classical Quantum Language Model,0,,False
41,"The classical Quantum Language Model(QLM) aims at modeling term dependencies in the principled quantum theory formulation. Different from traditional language models, QLM extracts term dependencies in each document as projectors in the quantum probabilistic space. The single words correspond to projectors |ei ei|, and the compound terms (with two or more words for each term) correspond to projectors |v v| (refer to Section 2.1). The projectors are used to estimate density matrices q and d for a query and each document by maximizing a likelihood function with the EMbased iterative approach, i.e., RR algorithm [7]. Then, the top retrieved documents in the initial search results returned by the traditional language model are re-ranked according to the negative VN-Divergence between q and d. For details of the classical QLM, please refer to [7].",1,LM,True
42,3. SESSION QUANTUM LANGUAGE MODEL (SQLM),1,LM,True
43,3.1 Framework,0,,False
44,"In QLM, a single query can be represented by a density matrix  over a vector space for a certain vocabulary. The positive definite matrix with unitary trace can be decomposed as follows:",1,LM,True
45,n,0,,False
46," , i(|ui ui|)",0,,False
47,(1),0,,False
48,"i,1",0,,False
49,"where |ui is a eigenvector, and i is the eigenvalue. Corre-",0,,False
50,"spondingly, i , |ui ui| can be interpreted as an elemen-",0,,False
51,"tary quantum event or projector, and i is the corresponding",0,,False
52,quantum probability for the elementary event (,0,,False
53,"n i,1",0,,False
54,"i,1).",0,,False
55,"By obtaining a density matrix, we actually obtain a set of",0,,False
56,mutually orthogonal quantum elementary events along with,0,,False
57,"the corresponding discrete probability distribution, and vice",0,,False
58,"versa. In a real search scenario, a user often interacts with",0,,False
59,the search engine many times before achieving his/her actual,0,,False
60,information need. We propose a density matrix transforma-,0,,False
61,"tion framework to model the interaction process, which is",0,,False
62,mathematically formulated as a mapping function T in the,0,,False
63,density matrix space Sn.,0,,False
64,T : Sn  Sn,0,,False
65,(2),0,,False
66,"In session search, we assume that there exists an ""ideal"" transformation T for density matrices which can model dynamic query intention in the historical interactions. Specifically, T is a transformation that for any two consecutive queries qi-1 and qi, the estimated density matrix ^i,"" T i-1 represents the user's information need for qi, where i-1 is a representation of qi-1. This implies we further make a 1st order Markov assumption that a query is dependent solely on its last previous query. This assumption is reasonable because the dependency can continuously back-propagate in the session.""",0,,False
67,"Theoretically, from Eq.(1), we can easily find that T can be divided into two separate transformation process: T ,"" T1T2. T1 is the transformation operator for quantum events |ui . Since |ui forms an orthogonal basis, T1 can be any standard transition matrix. T2 changes the original probability distribution for the events (namely, change the values of i), and it is be a diagonal matrix. In this sense, the transformation of density matrix is basically a transformation of main quantum events, and a reallocation of quantum probability for each event.""",0,,False
68,"In practice, however, this consideration seems infeasible due to its high degree of freedom. Suppose a vocabulary V with |V| distinct words, T1 will have a freedom of O(|V|2), and T2 will have a freedom of O(|V|). Thus the model is prone to be overfiting and computationally expensive. Moreover, it is hard to draw a clear and reasonable connection from the training of T1 and T2 to the extraction of projectors. Therefore, we propose an iterative training approach to represent the transformation process, inspired by the updating method of the classical QLM. Specially in this paper, we use the density matrix i-1 for query qi-1 as the initial density matrix to train the density matrix i for query qi.",1,LM,True
69,"To facilitate subsequent discussions, we define notations for the session search. In a search session, we have a set of historical interaction units {Qi, Ri, Ci}Ni,""-11, where Qi, Ri and Ci represent the query, returned documents and clicked documents for the ith interaction unit respectively. We need to use the historical interaction information to retrieve documents for the current query QN . To this end, we first obtain the top N retrieved documents returned by the traditional language model (LM), denoted as RN . {i}Ni"",""-11 denote a set of |V|-order density matrices representing user's information need for each historical query, where |V| is the size of the vocabulary containing all distinct terms in the historical queries and the current query.""",1,ad,True
70,3.2 Modeling a Single Query,1,Query,True
71,"For a historical interaction of a search session, the first clicked document of the query is not always the first one in the search results list. In other words, users often skip some irrelevant results before clicking the first assumingly relevant document. Therefore, we assume that the ""skip"" behavior is a strong negative feedback signal of users, since the user would have otherwise clicked them. In our assumption, some extreme cases are neglected. For example, the user may gain the right information only by reading the snippets without detecting any click behaviors. Based on this point, we form a positive documents set Rpos(i) with all clicked documents as well as a negative set Rneg(i) with all skipped documents",1,ad,True
72,872,0,,False
73,"in Ri for each query qi. Note that, Rpos(i) is equivalent to",0,,False
74,"parameter in [0,1], which we will further discuss in Section",0,,False
75,"Ci, and Rneg(i) is null for the queries whose first returned",0,,False
76,4.2. The objective function (3) can therefore be updated as,0,,False
77,document is clicked.,0,,False
78,From,0,,False
79,Rpos(i),0,,False
80,and,0,,False
81,"Rneg (i),",0,,False
82,positive,0,,False
83,projectors,0,,False
84,Ppos,0,,False
85,",",0,,False
86,{i,0,,False
87,}Mpos,0,,False
88,"i,1",0,,False
89,and,0,,False
90,negative,0,,False
91,projectors,0,,False
92,Pneg,0,,False
93,",",0,,False
94,{j,0,,False
95,}Mneg,0,,False
96,"j,1",0,,False
97,are,0,,False
98,extracted,0,,False
99,us-,0,,False
100,"ing the method discussed in Section 2.2, where Mpos and",0,,False
101,Mpos,0,,False
102,Mneg,0,,False
103,"LP () ,",0,,False
104,WD(i) log tr(i),0,,False
105,log tr(j) (8),0,,False
106,"i,1",0,,False
107,"j,1",0,,False
108,Mneg is the number of positive and negative projectors re-,0,,False
109,spectively. Note that some details when extracting projec-,0,,False
110,where D(i) is the document containing the projector i.,0,,False
111,"tors: i) only single words, bi-grams and tri-grams are con-",0,,False
112,"The new objective function is similar to Eq.5, and the only",0,,False
113,"sidered as possible compound dependencies, since otherwise",0,,False
114,difference is that the new one multiplies each projector in,0,,False
115,the computational complexity will be exponential to the vocabulary size; ii) we use TFIDF to assign the superposition,0,,False
116,clicked documents by a weight WD(i). Thus the updating methods discussed for Eq.6 can still be applied to the new,0,,False
117,weight vi rather than the IDF or UNIFORM weights intro-,0,,False
118,objective function in Eq.8.,0,,False
119,"duced in the original paper [7], since TFIDF is a document specific measure and has better distinguishability across",0,,False
120,3.3 Density Matrix Transformation,0,,False
121,documents. In order to maximize the probability that all,0,,False
122,"In this paper, we do not train the quantum events trans-",0,,False
123,positive events happen while all negative events not happen,0,,False
124,formation operator T1 and the quantum probability change,0,,False
125,"with respect to the quantum probability distribution (i.e.,",0,,False
126,"operator T2 for density matrix transformation operator T ,",0,,False
127,"the density matrix ), the Maximum Likelihood Estimation",0,,False
128,"because of the high freedom. Instead, we propose an itera-",1,ad,True
129,(MLE) problem can be formulated as,0,,False
130,tive training algorithm to approximate the process of density,0,,False
131,Mpos,0,,False
132,Mneg,0,,False
133,matrix transformation between two subsequent queries:,0,,False
134,"^ , argmax( log tr(i) log(1 - tr(j))) (3)",0,,False
135,"i,1",0,,False
136,"j,1",0,,False
137,where i and j denote a positive projector and a negative projector. Since,0,,False
138,"1 - tr(j) , tr((I - j))",0,,False
139,", (|V| - 1)  tr(j))",0,,False
140,(4),0,,False
141,Algorithm 1 : Density Matrix Transformation.,0,,False
142,1: 0  diag(LM ); // Initiate the density matrix 0 with the traditional unigram language model.,1,LM,True
143,"2: for k , 1; k  N - 1; k +,"" 1 do 3: Extract projectors from Rkpos, Rkneg (Section 3.2); 4: Estimate k with initial density matrix k-1 with""",0,,False
144,"T rainingSteps(k) , Sk-1 iterative steps;",0,,False
145,where,0,,False
146,j,0,,False
147,",",0,,False
148,I -j |V |-1,0,,False
149,is,0,,False
150,also,0,,False
151,a,0,,False
152,legal,0,,False
153,density,0,,False
154,matrix,0,,False
155,and,0,,False
156,|V| - 1,0,,False
157,is a constant. Then Eq.(3) can be rewritten as,0,,False
158,5: end for 6: Return the desired density matrix for interactions N-1.,0,,False
159,Mpos,0,,False
160,Mneg,0,,False
161,"^ , argmax( log tr(i) log tr(j))",0,,False
162,(5),0,,False
163,"i,1",0,,False
164,"j,1",0,,False
165,"Eq.5 is similar to the objective function in classical QLM. Thus we can apply the similar updating method used in [7] to update the density matrix . Since the RR algorithm in [7] dose not guarantee convergence, we revise it by utilizing the updating method in [4]:",1,LM,True
166,~(m+1),0,,False
167,",",0,,False
168,(1,0,,False
169,-,0,,False
170, )^(m),0,,False
171,+,0,,False
172,^(m)R((m^ )),0,,False
173,+ 2,0,,False
174,R((^m))^(m),0,,False
175,(6),0,,False
176,"It can be strictly proved in [4] that for a sufficiently small value of , Eq.(6) guarantees global convergence. Although this updating method guarantees the global convergence theoretically, it requires a sufficiently small value of parameter , resulting in a slow training speed. Therefore, in this paper we do not target on training the density matrix to its convergence, but control an appropriate iterative steps (will be discussed in Section 3.3).",0,,False
177,"In SQLM, we also model the dwelling time and click sequence for each clicked document. The assumption is that a longer dwelling time and an earlier click mean that the document is more likely to be relevant. Specifically, the weight for a clicked document d is calculated as",1,LM,True
178,Wd,0,,False
179,td,0,,False
180,", e tall",0,,False
181, cSeqd-1,0,,False
182,(7),0,,False
183,"where td is the dwelling time for the document d, tall is the lasting time of the whole interaction, Seqd denotes the rank of d in the returned document list, and c is a decaying",0,,False
184,"The training steps are different for different queries, since we believe nearer queries to the current query will have stronger influence on the estimation of current query. The initial training steps S and the discount factor  are free parameters which need to be further discussed. The more steps the density matrix is trained, the closer it moves towards the current query density matrix and away from the initial matrix. Thus, gaining an appropriate training steps can achieve a balance between the current query information and historical interaction information.",0,,False
185,3.4 Ranking,0,,False
186,"We use the top K (we set K , 50 in this paper) retrieved",0,,False
187,documents (pseudo feedback documents) returned by traditional LM to train a pseudo feedback QLM pN for current query. The representation of user's search intention can be formulated as the linear combination of N-1 and pN :,1,ad,True
188,"^ , pN + (1 - )N-1",0,,False
189,(9),0,,False
190,"where  controls the extent to which the history influence on the query representation. After obtaining ^, it can be used to re-rank the retrieved documents following the same method in [7].",0,,False
191,4. EMPIRICAL EVALUATION,0,,False
192,4.1 Experimental Setup,0,,False
193,Empirical evaluations are conducted on the TREC 2013 and 2014 session track data shown in Table 1. The corpus,1,TREC,True
194,873,0,,False
195,Table 1: Statistics For TREC 2013 and 2014,1,TREC,True
196,Datasets (TREC 2014's official ground truth only,1,TREC,True
197,contains the first 100 sessions).,0,,False
198,Items,0,,False
199,TREC 2013,1,TREC,True
200,TREC 2014,1,TREC,True
201,#Sessions,1,Session,True
202,87,0,,False
203,100,0,,False
204,#Queries,0,,False
205,442,0,,False
206,453,0,,False
207,#Avg. session length,0,,False
208,5.08,0,,False
209,4.53,0,,False
210,#Max. session length,0,,False
211,21,0,,False
212,11,0,,False
213,Table 2: Performance on TREC 2013 and 2014.,1,TREC,True
214,TREC2013 NDCG@10 chg%,1,TREC,True
215,MAP,1,MAP,True
216,chg%,0,,False
217,QLM,1,LM,True
218,0.0763,0,,False
219,+0.00 0.01708 +0.00,0,,False
220,SQLM,1,LM,True
221,0.0847,0,,False
222,+11.01 0.01799 +5.32,0,,False
223,SQLM+LM 0.0967,1,LM,True
224,+26.74 0.01994 +16.74,0,,False
225,TREC2014 NDCG@10 chg%,1,TREC,True
226,MAP,1,MAP,True
227,chg%,0,,False
228,QLM,1,LM,True
229,0.0909,0,,False
230,+0.00,0,,False
231,0.0164,0,,False
232,+0.00,0,,False
233,SQLM,1,LM,True
234,0.0950,0,,False
235,+4.51,0,,False
236,0.0170,0,,False
237,+3.66,0,,False
238,SQLM+LM 0.1033,1,LM,True
239,+13.64 0.0180,0,,False
240,+9.76,0,,False
241,"used in our experiments is the ClueWeb12 full corpus1 which consists of 733,019,372 English webpages collected from the Internet. We index the corpus with Indri search engine2. In the indexing process, we filtered out all documents with Waterloo's spam scores [1] less than 70, removed the stop words and stemmed all words with Porter stemmer [6].",1,ClueWeb,True
242,"To verify the effectiveness of the proposed model, we compared the following models: (i) QLM, the classic quantum language model which is regarded as the baseline model; (ii) SQLM, the proposed session-based quantum language model; (iii) SQLM+LM, the combination model of SQLM and traditional language model (LM), which takes the feature of LM into consideration (the linear combination parameter is ). We employ the official evaluation metrics MAP and NDCG@10 to evaluate the models.",1,LM,True
243,"A number of parameters are involved in the proposed models, and they are summarized as follows: c in Eq.7, S and  in Algorithm 1,  in Eq.9, and  in model SQLM +LM . For the global setup, we select  , 0.01 in Eq.6. The selection of best parameters will be discussed in next section.",1,LM,True
244,4.2 Results and Discussion,0,,False
245,"Table 2 reports the experimental results for TREC 2013 and 2014 datasets respectively. In the tables, ""chg%"" means the improvement percentage over the baseline, i.e., QLM.",1,TREC,True
246,"Since the modeling process of SQLM only involves matrix addition and multiplication, the computing complexity is low, allowing us to conduct a grid search to find the best parameter configuration. For TREC 2013, the best parameter configuration is {c ,"" 0.95, S "","" 10,  "","" 1.05,  "", 0.7} for SQLM; and {c ,"" 0.95, S "","" 30,  "","" 1.05,  "","" 0.6 ,  "","" 0.9} for SQLM+LM. For TREC 2014, the best parameter configuration is {c "","" 0.95, S "","" 30,  "","" 1.0,  "","" 0.7} for SQLM, and {c "","" 0.95, S "","" 30,  "","" 1.15,  "","" 0.9,  "", 0.9} for SQLM+LM.",1,LM,True
247,"The results indicate that the proposed SQLM achieves improvements over the classical QLM, on both TREC 2013 (11.01% improvement for NDCG and 5.33% for MAP), and TREC 2014 Session data (4.51% relative improvements for NDCG and 3.66% for MAP). Moreover, a linear combination",1,LM,True
248,1http://www.lemurproject.org/clueweb12/index.php 2http://www.lemurproject.org,0,,False
249,"of SQLM and LM can further enhance the performance of SQLM, suggesting that SQLM is adaptive to other features such as the traditional LM. It also indicates that SQLM has a large potential for further improvements.",1,LM,True
250,5. CONCLUSION AND FUTURE WORK,0,,False
251,"In this paper, we present a novel quantum theory based probabilistic framework for multi-query retrieval task, i.e., session search. By extending the classical Quantum Language Model (QLM), our proposed Session-based Quantum Language Model (SQLM) incorporates the sound mechanism of the density matrix transformation to approximate the dynamics of information need entailed in historical interactions, for re-ranking the initial results generated by the search engine. At the operational level, we utilise the information from both clicked documents and top unclicked documents, and devise a new training algorithm. Extensive experiments on both TREC 2013 and 2014 Session track datasets demonstrate that SQLM does perform better than classical QLM for multi-query retrieval systems, and also show its potential of being further improved for session search.",1,LM,True
252,"Therefore, it is safe and reasonable to conclude that the proposed Session-based Quantum Language Model(SQLM) is a feasible expansion of classical Quantum Language Model(QLM) on the multi-query session search tasks. As for future work, we believe that a better retrieval result could be achieved if one can find a better realization of density matrix transformation based on the quantum inference, and incorporate more features into the framework. We will also apply the model to data closer to real-time retrieval systems.",1,Session,True
253,6. ACKNOWLEDGMENTS,0,,False
254,"This work is supported in part by them Chinese National Program on Key Basic Research Project (973 Program, grant No.2013CB329304, 2014CB744604), the Chinese 863 Program (grant No. 2015AA015403), the Natural Science Foundation of China (grant No. 61402324, 61272265), and the Research Fund for the Doctoral Program of Higher Education of China (grant no. 20130032120044). Any comments from anonymous reviewers are appreciated.",0,,False
255,7. REFERENCES,0,,False
256,"[1] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5):441­465, 2011.",0,,False
257,"[2] A. M. Gleason. Measures on the closed subspaces of a hilbert space. J. Math. Mech, 6(6):885­893, 1957.",0,,False
258,"[3] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In SIGIR, pages 453­462. ACM, 2013.",0,,False
259,"[4] M. G. A. Paris and J. Reh´l´ccek. Quantum State Estimation. 649, 2004.",0,,False
260,"[5] B. Piwowarski, I. Frommholz, M. Lalmas, and K. Van Rijsbergen. What can quantum theory bring to information retrieval. In CIKM, pages 59­68. ACM, 2010.",0,,False
261,"[6] M. F. Porter. An algorithm for suffix stripping. Program: electronic library and information systems, 14(3):130­137, 1980.",0,,False
262,"[7] A. Sordoni, J.-Y. Nie, and Y. Bengio. Modeling term dependencies with quantum language models for ir. In SIGIR, pages 653­662. ACM, 2013.",0,,False
263,"[8] C. J. Van Rijsbergen. The geometry of information retrieval. Cambridge University Press, 2004.",0,,False
264,"[9] S. Zhang, D. Guan, and H. Yang. Query change as relevance feedback in session search. In SIGIR, pages 821­824. ACM, 2013.",1,Query,True
265,874,0,,False
266,,0,,False

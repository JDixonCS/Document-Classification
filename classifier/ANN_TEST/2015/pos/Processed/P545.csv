,sentence,label,data,regex
0,Representative & Informative Query Selection for Learning to Rank using Submodular Functions,1,Query,True
1,Rishabh Mehrotra,0,,False
2,"Dept of Computer Science University College London, UK",0,,False
3,r.mehrotra@cs.ucl.ac.uk,0,,False
4,Emine Yilmaz,0,,False
5,"Dept of Computer Science University College London, UK",0,,False
6,emine.yilmaz@ucl.ac.uk,0,,False
7,ABSTRACT,0,,False
8,"The performance of Learning to Rank algorithms strongly depend on the number of labelled queries in the training set, while the cost incurred in annotating a large number of queries with relevance judgements is prohibitively high. As a result, constructing such a training dataset involves selecting a set of candidate queries for labelling. In this work, we investigate query selection strategies for learning to rank aimed at actively selecting unlabelled queries to be labelled so as to minimize the data annotation cost. In particular, we characterize query selection based on two aspects of informativeness and representativeness and propose two novel query selection strategies (i) Permutation Probability based query selection and (ii) Topic Model based query selection which capture the two aspects, respectively. We further argue that an ideal query selection strategy should take into account both these aspects and as our final contribution, we present a submodular objective that couples both these aspects while selecting query subsets. We evaluate the quality of the proposed strategies on three real world learning to rank datasets and show that the proposed query selection methods results in significant performance gains compared to the existing state-of-the-art approaches.",0,,False
9,Categories and Subject Descriptors,0,,False
10,H.3.3 [Information Storage And Retrieval]: Information Search and Retrieval--Learning to Rank,0,,False
11,Keywords,0,,False
12,"Learning to Rank, Query Selection, Active Learning, Submodularity",1,Query,True
13,1. INTRODUCTION,1,DUC,True
14,"Most modern search technologies are based on machine learning algorithms that learn to rank documents given a query, an approach that is commonly referred to as ""learning to rank"". Learning to Rank algorithms aim to learn ranking functions that achieve good ranking objectives on test data.",0,,False
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",1,ad,True
16,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,0,,False
17,DOI: http://dx.doi.org/10.1145/2766462.2767753.,0,,False
18,"Such learning methods require labelled data for training. As is the case with many supervised learning algorithms, the performance of Learning to Rank algorithms are often highly correlated with the amount of labelled training data available[1][17][7].",0,,False
19,"Constructing such labelled training data for learning-torank tasks incurs prohibitive costs since it requires selecting candidate queries, extracting features from query-document pairs and annotating documents in terms of their relevance to these queries (annotations are used as labels for training). The major bottleneck in constructing learning-to-rank collections is annotating documents with query specific relevance grades. It is essential therefore, both for the efficiency of the construction methodology and for the efficiency of the training algorithm, that only a small subset of queries be selected. The query selection, though, should be done in a way that does not harm the effectiveness of learning.",1,ad,True
20,"Active Learning algorithms help reduce the annotation costs by selecting a subset of informative instances to be labelled. Unlike traditional algorithms, active learning strategies for ranking algorithms are more complex because of the inherent query-document pair structure embodied in ranking datasets, non-smooth cost functions, etc., hence these cannot be applied directly in ranking setting.",1,ad,True
21,"Existing approaches for active learning for ranking have focused on selecting documents [1], selecting queries [17] or balancing number of queries with depth of documents judged using random query selection [27].",0,,False
22,"In this work, we focus on selecting subset of queries to be labelled so as to minimize the data annotation cost. Prior work on selecting queries made use of expected loss optimization [17] to estimate which queries should be selected but their approach is limited to rankers that predict absolute graded relevance which is not the case with modern Learning to Rank algorithms since many of them induce a ranking and not absolute labels [4]. Apart from the learning to rank setting, query selection has also received significant attention for evaluation setting [13] wherein the goal was to find a subset of queries that most closely approximates the system evaluation results that would be obtained if instead documents for the full set of queries was judged instead. However, it was shown by Aslam et a.l [1] that learning to rank and evaluation of retrieval systems are quite different from each other and that datasets constructed for evaluating quality of retrieval systems are not necessarily good for training and vice versa. Therefore, query selection strategies that are directly devised for learning to rank purposes are needed.",1,ad,True
23,545,0,,False
24,"Intuitively, an optimal subset of queries constructed for learning to rank should have two characteristics: (i) informativeness, which measures the ability of an instance (query) in reducing the uncertainty of a statistical model (ranking model) and (ii) representativeness, which measures if an instance (query) well represents the possible input patterns of unlabelled data (unlabelled queries) [22]. Most existing active learning for ranking algorithms solely focus on the informativeness aspect of queries without considering the representativeness aspect which can lead to possible selection of noisy queries, not quite representative of the whole population of queries; thus, significantly limiting the performance of query selection.",1,ad,True
25,"In this work, we focus on query selection strategies for learning to rank and propose novel query selection algorithms aimed at finding an optimal subset of queries to be labelled. Since problems associated with subset selection are generally NP-Hard or NP-Complete[12], we approximate the solution by an iterative query selection process so as to minimize the data annotation cost without severely degrading the performance of the ranking model.",1,NP,True
26,"We describe two paradigms of query selection strategies based on the aspects of informativeness and representativeness described above and propose novel query selection techniques: Permutation Probability based query selection and query selection based on topic models which capture these two aspects, respectively. We further present a new algorithm based on defining a submodular objective that combines the powers of the two paradigms. Submodular functions have the characteristic of diminishing returns [19], which is an important attribute of any query-subset selection technique since the value-addition from individual queries should ideally decrease as more and more queries are selected. Thus, not only are submodular functions natural for query subset selection, they can also be optimized efficiently and scalably such that the result has mathematical performance guarantees.",1,ad,True
27,We show that our proposed algorithms result in significant improvements compared to state-of-the-art query selection algorithms thereby helping in reducing data annotation costs.,0,,False
28,2. RELATED WORK,0,,False
29,"Active Learning for Labelling Cost Reduction: A number of active learning strategies have been proposed for the traditional supervised learning setting, a common one being uncertainty sampling which selects the unlabelled example about which the model is most uncertain how to label. Some of the others adopt the idea of reducing the generalization error and select the unlabelled example that has the highest effect on the test error, i.e. points in the maximally uncertain and highly dense regions of the underlying data distribution[9]. A comprehensive active learning survey can be found in [22].",1,ad,True
30,"Reducing judgment effort for learning to rank has received significant amount of attention from the research community. Learning to rank methods are quite different than approaches used for classification as they require optimizing nonsmooth cost functions such as NDCG and AP [24]. Moreover, owing to the unique query-document structure which inherent to the learning to rank setting, it is not straightforward to extend the models devised for traditional supervised learning settings to ranking problems. In recent",1,AP,True
31,"years, active learning has been actively extended to rank learning and can be classified into two classes of approaches: document level and query level active learning.",0,,False
32,"Document Selection for Learning to Rank: Based on uncertainty sampling, Yu et al [28] selected the most ambiguous document pairs, in which two documents received close scores predicted by the current model, as informative examples. Donmez et al.[8] chose those document pairs, which if labelled could change the current model parameters significantly. Silva et al [23] proposed a novel document level active sampling algorithm based on association rules, which does not rely on any initial training seed.",0,,False
33,"Query Selection for Learning to Rank: For query level active learning, Yilmaz et al. [27] empirically showed that having more queries but shallow documents performed better than having less queries but deep documents. They balance number of queries with depth of documents judged using random query selection. Cai et al. [5] propose the use of Query-By-Committee (QBC) based method to select queries for ranking adaptation but omit the evaluation of the query selection part and focussed on the ranking adaptation results instead. Long et al. [17] introduced an expected loss optimization (ELO) framework for ranking, where the selection of query and documents were integrated in a principled 2 staged active learning framework and most informative queries selected by optimizing the expected DCG loss but the proposed approach is limited to rankers that predict absolute graded relevance and hence not generalizable to all rankers. Authors in [2] adapt ELO to work with any ranker by introducing a calibration phase where a classification model is trained over in the validation data. Moreover, they show that estimating expected loss in DCG is more robust than NDCG even when the final performance measure is NDCG.",1,Query,True
34,"Thus, QBC attempts to capture the informativeness aspect of queries by selecting queries which minimize the disagreement among a committee of rankers while the Expected loss optimization based approach formulates informativeness in terms of expected DCG loss; both these approaches fail to capture the representativeness aspect of queries which we show outperforms both these approaches.",0,,False
35,"Submodular Maximization: Submodularity is a property of set functions with deep theoretical and practical consequences. Submodular maximization generalizes to many well-known problems, e.g., maximum weighted matching, max coverage, and finds numerous applications in machine learning and social networks. In Information Retrieval, submodular objectives have been majorly employed for diversified retrieval[29] & learning from implicit feedback[21]. A seminal result of Nemhauser et al. [19] states that a simple greedy algorithm, based on a submodular objective, produces solutions competitive with the optimal (intractable) solution. In fact, if assuming nothing but submodularity, no efficient algorithm produces better solutions in general [10].",0,,False
36,3. QUERY SELECTION STRATEGIES,0,,False
37,"Our aim is to actively select the optimal subset of unlabelled queries for obtaining relevance judgements so as to reduce data annotation costs. Intuitively, the selected queries",0,,False
38,546,0,,False
39,should have two major properties: informativeness & representativeness. We describe both these properties below and provide intuitions motivating each.,0,,False
40,3.1 Informativeness,0,,False
41,"Informativeness measures the ability of an instance in reducing the uncertainty of a statistical model[22]. Ideally, the selected queries should be maximally informative to the ranking model. In learning to rank setting, Informativeness based query selection focusses on greedily selecting queries which are most informative to the current version of the ranking model.",0,,False
42,Different notions of informativeness can be encapsulated by different techniques depending on how query-level informativeness is quantified. Two possible measures of capturing a query's informativeness include: (i) Uncertainty based informativeness & (ii) Disagreement based informativeness.,0,,False
43,"Uncertainty based informativeness quantifies the querylevel information as the uncertainty associated with the optimal document ranking order for that query. Query selection strategies focusing on uncertainty reduction would greedily select the query instance about which the current ranking model is most uncertain about, thereby trying to reduce the overall uncertainty associated with the ranking model.",1,Query,True
44,"Disagreement based informativeness, on the other hand, quantifies the query-level informativeness as the disagreement in this query's document rankings among a committee of ranking models. The key idea here is that the maximally informative query is one about whose document rankings, the committee of ranking models maximally disagree; hence obtaining relevance labels for such a query would provide the maximum information. Among the existing approaches for query selection for ranking models, the Queryby-Committee [5] attempts to capture the Informativeness aspect of queries based on a disagreement measure.",1,Query,True
45,3.2 Representativeness,0,,False
46,"Representativeness measures if an instance well represents the overall input patterns of unlabelled data [22]. Web search queries can span a multitude of topics and information needs, with even a small dataset containing a broad set of queries ranging from simple navigational queries to very specific domain-dependent queries. In learning to rank settings, this implies that selected queries should have strong correlation with the remaining queries, as without this correlation there is no generalizability and predictive capability. Different notions of representativeness can be defined covering different characteristics of individual queries. Improving the representativeness of the selected query subset improves the coverage aspect of the query collection - the more representative selected queries are, the more they cover the entire query collection.",1,ad,True
47,3.3 Informativeness vs Representativeness,0,,False
48,"Selecting queries solely based on their informativeness aspects could possibly lead to selection of noisy queries. In line with the Meta-Search Hypothesis [14][15], rankers tend to agree on relevant documents and disagree about nonrelevant docs. Hence, the queries that a ranker is unsure about or there is big disagreements across rankers are likely to be the ones that contain a lot of nonrelevant documents. Such noisy, outlier queries which majorly have non-relevant documents would lead to maximal disagreement and uncertainty",1,ad,True
49,"among ranking models, and thus would be wrongly labelled maximally informative. Also, the set of informative queries might not necessarily represent the set of all possible queries, which lead to less coverage of the unlabelled query set.",1,ad,True
50,"On the other hand, selecting queries based on representativeness aspects could lead to the selection of a query that is very similar to the a query already in the labelled set and hence, does not provide much information to the ranking model. Despite being representative, such queries possibly offer redundant information to the ranking models. Ideally, a query selection algorithm should take into account both these aspects while selecting queries. Existing work has majorly looked into selecting queries by considering informativeness based on disagreement among rankers (Query-byCommittee) or informativeness in terms of expected DCG loss (Expected loss optimization). Both these approaches fail to capture the representativeness aspect of queries. In addition to a novel informativeness approach based on uncertainty reduction, we present a representativeness based approach and finally couple both these aspects for query selection via a joint submodular objective which jointly incorporates informativeness & representativeness.",1,ad,True
51,"As our first contribution, we present a novel informativeness based query selection scheme ( 4) based on permutation probabilities of document rankings which tries to reduce uncertainty among rankers. While no existing query selection scheme for learning to rank incorporates the representativeness aspect of queries, we propose a LDA topic model based query selection scheme ( 5) which captures the representative aspect of queries while constructing the query subset. An ideal query subset would have both informative & representative queries. As our third contribution, we combine the two paradigms of representativeness & informativeness by proposing a coupled model based on submodular functions( 6).",1,corpora,True
52,4. CAPTURING INFORMATIVENESS VIA PERMUTATION PROBABILITIES,1,AP,True
53,"Our first novel query selection scheme is aimed at capturing the informative-aspect of queries. We maintain a committee of ranking models C ,"" {1, 2, ..., C } which are trained on a randomly selected subset from the current labelled set, and thus contain different aspects of the training data depending on the queries in their subset. It is to be noted that these ranking models could be generated using any learning to rank algorithm. Given the set of currently labelled query instances, our goal is to pick the next query (q) from the set of unlabelled queries by selecting the maximally informative query instance. The query-level informativeness is defined in terms of the uncertainty associated with the optimal document ranking orders among the |C| ranking models. We follow a similar approach as outlined by Cai et al.[5] to maintain a committee of rankers. However, unlike Query-By-Committee [5] which encapsulates informativeness via ranker disagreements, our approach presents an alternate view of informativeness based on uncertainty reduction wherein a ranking model's uncertainty for the query's document ranking order is defined based on the concept of permutation probabilities.""",1,Query,True
54,"More specifically, each committee ranking model is allowed to score the documents associated with each query following which a permutation probability is calculated on the",0,,False
55,547,0,,False
56,"ranking obtained on sorting these document scores. Thus, each query gets a permutation probability score by each committee member. The most informative query is considered to be the query instance which minimizes the maximum permutation probability of document scores given by each ranking model committee member.",0,,False
57,"We postulate that a query which has the minimum permutation probability score from among the maximum scores assigned between the different ranking models is maximally informative in the sense that even the best ranker among the committee is highly uncertain about its document rankings and hence this query obtained the least permutation probability score among the set of unlabelled candidate queries. We select a query for which the probability with respect to the most certain (maximum permutation probability) model is minimal, i.e., a query for which even the most certain committee member has minimum confidence.",0,,False
58,"To define permutation probabilities, we make use of the Plackett-Luce model [20]. The Plackett-Luce (P-L) model is a distribution over rankings of items (documents) which is described in terms of the associated ordering of these items (documents). We define P (|) as the probability of obtaining the ranking order () based on the score (k) assigned to each document (k) by the ranking model learnt thus far. For each query, we rank the documents based on the scores assigned by model learnt so far and calculate the probability of the ranking order obtained () using the permutation probability defined as follows:",0,,False
59,"P (|) ,",0,,False
60,i,0,,False
61,"i,""1,...,K i +    + K""",0,,False
62,(1),0,,False
63,"where each ranking  has an associated ordering of document scores  ,"" (1,    , K ) and an ordering is defined as a permutation the K document indices with i representing the score assigned to document i (at rank i) by the ranking model. We make use of a committee of ranking models and select the maximally informative query based on a greedy min-max algorithm described next.""",0,,False
64,4.1 Min-Max PL Probability Algorithm,0,,False
65,Building a Min-Max PL Probability based selection system involves two components: (i) building a committee of ranking models that are well diversified and compatible with the currently labelled data and (ii) computing permutation probabilities by each committee member for each query in the unlabelled set of queries & selecting maximally informative query as per the min-max score.,0,,False
66,"Committee Construction: Following the work of [5], we use query-by-bagging approach to construct the members. Given the set of currently labelled instances, bagging generates C partitions of sub samples by sampling uniformly with replacement, and then the committee can be constructed by training each of its members on one portion of the sub-sample partitions. We randomly initialize the initial set of labelled queries with a small base set of queries and their labelled documents. We sample with replacement for C times in the set of labelled queries and train a ranking model on each subset of queries. Such a sampling procedure allows us to create various different training datasets that each represent a subset of the data possibly having very different characteristics than each other. These C models represents our C committee mem-",0,,False
67,"bers. We set the size of each subset to be 50 % of the current labelled subset size at each step. The maximally informative query q is selected for annotation which obtains the lowest min-max score, the calculation of which is described below.",0,,False
68,"Calculating min-max score: For each query q in the candidate set of unlabelled queries, the C committee members return C ranked lists. Following the construction of |C| ranking models, for each ranking model per query, we sort the documents based on the scores given by the ranking model and compute the permutation probability of obtaining this ranking order.",0,,False
69,"Thus, each query has |C| permutation probability scores. In order to minimize the overall uncertainty associated with the ranking models, we select the maximally informative query q, i.e., the query that has the minimum value of the permutation probability assigned by its most certain committee member, i.e., the committee member that has the highest permutation probability score associated with the query's document ranking order. Thus,",0,,False
70,"q , argminqDu maxcC P (qc|qc)",0,,False
71,"c k k,""1,...,K c k +    + c K""",0,,False
72,(2),0,,False
73,"where each ranking qc has an associated ordering  ,"" (1c,    , Kc ) and an ordering is defined as a permutation the K document indices with c k representing the score assigned to document k by the ranking model c.""",0,,False
74,5. CAPTURING REPRESENTATIVENESS VIA LDA TOPICS,1,AP,True
75,"A major drawback associated with pure-Informativeness based models is that often they tend to select outlier queries. As is confirmed by the Meta-Search Hypothesis [14][15], rankers tend to agree on relevant documents but disagree on non-relevant documents. In such a scenario, an outlier query which majorly has non-relevant documents would lead to maximal disagreement and uncertainty in the ranking model, and thus will be wrongly labelled maximally informative. This motivates the need for considering the representativeness aspect of queries.",1,ad,True
76,"The information-seeking behaviour of users tend to vary based on the search task at hand [25] which suggests that the importance of feature weights for queries belonging to different tasks or topics are likely to be very different. The relative importance of different features are likely to be very different for different tasks. For example, queries belonging to a topic such as news would warrant high authority websites to be ranked higher (i.e., larger weight on the pagerank score) while queries belonging to (say) educational informational content would prefer the documents better matched with their query terms be ranked higher (i.e., larger weight on the relevance features such as BM25). To capture these diverse variations in the feature weights, the training set should ideally be composed of representative queries from different tasks. This makes it necessary that the labelled set of queries have representative queries spanning the entire",0,,False
77,548,0,,False
78,array of different topics. We propose a Latent Dirichlet Allocation (LDA) [3] topic model based query selection scheme which tries to capture this insight by selecting representative queries which are most topically similar to the set of unlabelled queries.,0,,False
79,"Based on this intuition, we conjecture that representative queries would be those that are most similar to the set of unlabelled queries in terms of their topical distribution. To capture the heterogeneity among all queries in the search logs, we make use of the concept of latent topics. We learn these latent topics from the collection of queries and represent each query as a probability distribution over these latent topics. We train an LDA model, a generative model which posits that each document (query in our case) is a mixture of a small number of topics and that each word's (query term's) creation is attributable to one of the document's (query's) topics. Each query is represented as a feature vector corresponding to its distribution over the LDA topics. To find representative queries, we select the query with the maximum average similarity from among the unlabelled set of queries, i.e.,",0,,False
80,q,0,,False
81,",",0,,False
82,argmaxq,0,,False
83,1 |Du|,0,,False
84,qi Du,0,,False
85,"sim(Tq ,",0,,False
86,Tqi ),0,,False
87,(3),0,,False
88,"where |Du| represents the number of queries in the unlabelled set Du; Tq represents the query q's feature vector in the LDA topic space and sim(Tq, Tqi ) can be any similarity score between queries; we use the cosine similarity between",0,,False
89,the topic-space representations of queries q and qi.,0,,False
90,6. COMBINING REPRESENTATIVENESS & INFORMATIVENESS,0,,False
91,"The approaches discussed so far have looked at either the informativeness of queries and selected queries which are most informative in terms of their ability reduce the uncertainty of the ranking model or they have focussed on representativeness of queries and selected representative queries spanning the entire array of different topics. As we discussed earlier in subsection 3.3, optimizing for only one of the two criteria for query selection could significantly limit the performance of query selection by selecting suboptimal query subsets. In this section we present a way of combining the two objectives by means of submodular functions and propose a submodular objective which jointly captures the notions of representativeness and informativeness.",0,,False
92,6.1 Submodular Functions,0,,False
93,"Submodular functions are discrete functions that model laws of diminishing returns and can be defined as follows:[19]: Given a finite set of objects (samples) Q ,"" {q1, ..., qn} and a function f : 2S  + that returns a real value for any subset S  Q, f is submodular if given S  S , and q / S""",0,,False
94,f (S + q) - f (S)  f (S + q) - f (S ),0,,False
95,(4),0,,False
96,"That is, the incremental ""value"" of q decreases when the set in which q is considered grows from S to S . A function is monotone submodular if S  S , f (S)  f (S ). Powerful guarantees exist for such subtypes of monotone submodular function maximization. Though NP-hard, the problem of maximizing a monotone submodular function subject to a cardinality constraint can be approximately solved by a simple greedy algorithm [19] with a worst-case approximation",1,NP,True
97,"factor (1 - e-1). This is also the best solution obtainable in polynomial time unless P,NP [10].",1,NP,True
98,6.2 Problem Formulation,0,,False
99,"Submodularity is a natural model for query subset selection in Learning to Rank setting. Indeed, an important characteristic of any query-subset selection technique would be to decrease the value-addition of a query q  Q based on how much of that query has in common with the subset of queries already selected (S). The value f (q|S) of a query in the context of previously selected subset of queries S further diminishes as the subset grows S  S. In our setting, each q  Q is a distinct query, Q corresponds to the entire collection of queries and S corresponds to the subset of queries already selected from Q.",1,ad,True
100,"Mathematically, the query subset selection problem can be formulated as selecting the subset of queries S which maximizes the value of f (S) where f (S) captures both the representativeness aspect as well as the informativeness aspects of queries. We next describe in detail the construction of such a monotone submodular function and later present a greedy algorithm to approximately solve the problem of query subset selection.",0,,False
101,6.3 Submodular Query Selection,1,Query,True
102,"We model the quality of the query subset in terms of both the representativeness & informativeness. To capture both these traits, we model the quality of the query subset as:",0,,False
103,"F (S) , (S) + (1 - )(S)",0,,False
104,(5),0,,False
105,"where (S) captures the representativeness aspect of the query subset (S) with respect to the entire query set Q while (S) rewards selecting informative queries. The parameter  controls the trade-off between the importance of representativeness & informativeness while selecting queries. A single weighting scheme would not be suitable for all problems since depending on the constituent queries, size of the overall dataset and the size of the subset that needs to be selected, different weighting schemes would produce different results. The function F (S) will be monotone submodular if each of (S) and (S) are individually monotone submodular. We defer an in-depth analysis of the trade-off between representativeness & informativeness aspects to subsection 8.1 and next describe the details of both these functions.",1,ad,True
106,6.3.1 Representativeness: (S),0,,False
107,"(S) can be interpreted either as a set function that measures the similarity of query subset S to the overall query set Q, or as a function representing some form of ""representation"" of Q by S. Most naturally, (S) should be monotone, as representativeness improves with a larger subset. (S) should also be submodular: consider adding a new query to two query subsets, one a subset of the other. Intuitively, the increment when adding a new query to the small subset should be larger than the increment when adding it to the larger subset, as the information carried by the new query might have already been covered by those queries that are in the larger subset but not in the smaller subset. Indeed, this is the property of diminishing returns.",1,ad,True
108,"We employ the same functional form of (S) as was adopted by Lin et al.[16]. Specifically, a saturated coverage function",1,ad,True
109,549,0,,False
110,is defined as follows:,0,,False
111,"(S) ,"" min {Cq(S), Cq(Q)}""",0,,False
112,(6),0,,False
113,qQ,0,,False
114,"where Cq(S) is a set based function defined as Cq(S) : 2S  and 0    1 is a threshold co-efficient. Intuitively,",0,,False
115,"Cq(S) measures how topically similar S is to query q or how much of the query q is covered by the subset S. Building on top of the earlier proposed LDA topic model based query selection, we define the coverage function Cq(S) in terms of the topical coverage of queries. More specifically,",0,,False
116,"Cq(S) ,",0,,False
117,"wq,q",0,,False
118,(7),0,,False
119,q S,0,,False
120,"where wq,q  0 measures the topical similarity between queries q and q . Since Cq(S) measures how topically similar S is to query q, summing Cq(S) q  Q would measure how similar the current subset S is to the overall set of queries Q. It is important to note that Cq(Q) is just the largest value Cq(S) can ever obtain because Q is the set of all the queries we have and it maximally represents all the information we have. We call a query q saturated by the subset of queries S when min {Cq(S), Cq(Q)} ,"" Cq(Q). When q is saturated in this way, any new query cannot further improve the coverage even if it is very similar to the query q. Thus, this gives other queries which are not yet saturated a higher chance of being better covered and hence the resulting subset tends to better cover the entire set of queries Q.""",0,,False
121,6.3.2 Informativeness: (S),0,,False
122,"The (S) function described above intuitively captures the notion of coverage or representativeness by selecting subset of queries S which are topically most representative of the entire set of queries Q. While representativeness is an important trait, we also wish to capture the informativeness aspect of queries and select queries which are most informative to the current version of the ranking model. We formulate the functional form of (S) based on top of the earlier proposed ways of encapsulating query-level informativeness in terms of either ranker disagreements or model uncertainity, or both. As a precursor, it is worth mentioning that to define the function (S) we make use of LDA topic model which gives us k-topics and we associate each query to one of these k-topics. Formally, we define the (S) function as follows:",0,,False
123,K,0,,False
124,"(S) ,",0,,False
125,q,0,,False
126,(8),0,,False
127,"i,1 qPiS",0,,False
128,"where Pi, i ,"" 1, ..., K is the topical-partition of the set of queries Q into K-topics and q captures the informativeness carried by the query q based on the current ranking model. The function (S) rewards topical-diversity along with valuing informativeness since there is usually more benefit to selecting a query from a topic not yet having one of its query already chosen. As soon as a query is selected from a topic, other queries from the same topic starthaving diminishing gain owing to the square root function ( 2+ 1 > 3+ 0). It is easy to show that (S) is submodular by the composition rule. The square root is non-decreasing concave function. Inside each square root lies a modular function with non-negative weights (and thus is monotone). Applying the square root to such a monotone submodular function yields""",1,ad,True
129,"a submodular function, and summing them all together retains submodularity.",0,,False
130,"The informativeness of a query q can be defined based on the metrics proposed earlier. To incorporate the informativeness aspects of queries, we experiment with various different formulations of the singleton-query rewards (q) include the following::",1,corpora,True
131, Disagreement Score for a query - this allows us to capture information about the disagreement about the document rankings for a query among a committee of ranking models [5],0,,False
132, Uncertainty associated with the query - this allows us to capture the ranking model's uncertainty about the query's document rankings 4,0,,False
133, Combination of uncertainty & disagreement.,0,,False
134,"Based on empirical analysis, we find that the disagreement based reward functions perform better than the rest of the formulations across all datasets, so we skip the performance comparisons among these.",0,,False
135,6.4 Greedy Optimization,0,,False
136,"Having defined the individual functions based on the different paradigms, we formulate the overall query subset selection problem as the selection of the subset S of queries which maximizes the following function:",1,ad,True
137,"F (S) ,  min",0,,False
138,"wq,q , ",0,,False
139," wq,q",0,,False
140,qQ,0,,False
141,q S,0,,False
142,q Q K,0,,False
143, (9),0,,False
144,+ (1 - ),0,,False
145,q,0,,False
146,"i,1 qPiS",0,,False
147,"Modelling the query selection problem in such an objective provides many advantages. Firstly, the submodular formulation provides a natural way of coupling the different aspects of query selection. Secondly, the above formulation can be optimized efficiently and scalably given the monotone submodular form of the function F (S). Assuming we wish to select a subset of N queries from the total unlabelled set of Q queries, the problem reduces to solving the following optimization problem:",1,ad,True
148,"S , argmax F (S)",0,,False
149,"SQ,|S|N",0,,False
150,(10),0,,False
151,"While solving this problem exactly is NP-complete [10], tech-",1,NP,True
152,niques like ILP [18] can be used but scaling it to bigger,0,,False
153,datasets becomes prohibitive. Since the function F (S) is,0,,False
154,"submodular, it can be shown that a simple greedy algorithm",0,,False
155,will have a worst-case guarantee of f (S)  0.63F (Sopt) where Sopt is the optimal and,0,,False
156,(S1-is1e,0,,False
157,)F (Sopt)  the greedy,0,,False
158,solution [10]. This constant factor guarantee has practical,0,,False
159,"importance. First, a constant factor guarantee stays the",0,,False
160,"same as N grows, so the relative worst-case quality of the",0,,False
161,solution is the same for small and for big problem instances.,0,,False
162,"Second, the worst-case result is achieved only by very con-",0,,False
163,trived and unrealistic function instances - the typical case is,0,,False
164,almost always much better. The greedy solution works by,0,,False
165,starting with an empty set and repeatedly augmenting the,0,,False
166,set as,0,,False
167,S  S  argmax F (q|S),0,,False
168,(11),0,,False
169,qQ\S,0,,False
170,550,0,,False
171,nQueries 30 50 100 150 250 350 400 500,0,,False
172,SF 0.496& 0.502&,0,,False
173,0.509 0.518& 0.528&,0,,False
174,0.527 0.531& 0.535&,0,,False
175,MQ2007 Dataset,1,MQ,True
176,LDA,0,,False
177,PL ELO,0,,False
178,0.495,0,,False
179,0.493 0.493,0,,False
180,0.501 0.504,0,,False
181,0.496 0.494 0.510& 0.506,0,,False
182,0.517,0,,False
183,0.510 0.511,0,,False
184,0.527 0.528 0.531&,0,,False
185,0.519 0.523 0.526,0,,False
186,0.517 0.520 0.523,0,,False
187,0.531,0,,False
188,0.530 0.528,0,,False
189,QBC 0.493 0.490 0.500 0.506 0.513 0.525 0.526 0.527,0,,False
190,RDM 0.482 0.485 0.501 0.507 0.516 0.523 0.524 0.526,0,,False
191,nQueries 30 50 100 150 250 350 400,0,,False
192,SF 0.730 0.735 0.741& 0.743,0,,False
193,0.745 0.751& 0.753&,0,,False
194,MQ2008 Dataset,1,MQ,True
195,LDA PL ELO 0.728 0.722 0.716 0.731 0.731 0.720 0.740 0.739 0.724 0.742 0.740 0.729 0.745 0.748& 0.735 0.749 0.745 0.749 0.750 0.748 0.746,0,,False
196,QBC 0.728 0.734 0.735 0.742 0.746 0.747 0.747,0,,False
197,RDM 0.714 0.721 0.733 0.734 0.740 0.744 0.745,0,,False
198,nQueries 25 30 35 40 45 50,0,,False
199,SF 0.466,0,,False
200,0.464,0,,False
201,0.463 0.478& 0.481& 0.484&,0,,False
202,OHSUMED Dataset,0,,False
203,LDA,0,,False
204,PL ELO QBC,0,,False
205,0.459,0,,False
206,0.466 0.478&,0,,False
207,0.463 0.473&,0,,False
208,0.476,0,,False
209,0.463 0.454 0.467,0,,False
210,0.465 0.455 0.458,0,,False
211,0.460,0,,False
212,0.468 0.455 0.469,0,,False
213,0.473,0,,False
214,0.456 0.455 0.464,0,,False
215,0.466,0,,False
216,0.467 0.467 0.472,0,,False
217,RDM 0.432 0.462 0.463 0.460 0.473 0.464,0,,False
218,Figure 1: Performance evaluation based on NDCG@10 scores for,0,,False
219,"the different algorithms; SF: Submodular function based query selection, LDA: LDA Topic Model based query selection, PL: Permutation Probability Based Query Selection, ELO: expected loss minimization baseline, QBC: Query-By-Committee baseline, RDM: Random query selection baseline. nQueries is the number of queries in the labelled set ,"" base set + actively labelled queries. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.""",1,Query,True
220,until we select the N number of queries in the subset we intended.,0,,False
221,"Overall, we select query subsets based on the aforementioned formulations; we next describe in detail the experimental evaluation performed to compare the performances of the three proposed approaches against state-of-the-art baselines.",0,,False
222,7. EXPERIMENTAL EVALUATION,0,,False
223,"We evaluate the proposed query selection strategies on web search ranking and show that the proposed techniques can result in good performance with much fewer labelled queries. We next describe our experimental settings along with the baselines, dataset and evaluation metrics used.",0,,False
224,7.1 Compared Approaches,0,,False
225,We compare the performance of the proposed query selection strategies against existing state-of-the-art approaches. The compared approaches include:,0,,False
226, Query-By-Committee (QBC): The Query-ByCommittee (QBC) approach involves maintaining a committee of models wherein each member is then allowed to vote on the labellings of query candidates. The most informative query is considered to be the instance about which the committee members most disagree. QBC based query selection strategy was used in [5] for ranking adaptation.,1,Query,True
227," Expected Loss Optimization (ELO): Based on the ELO framework described by Long et al [17], we im-",0,,False
228,"plemented the query-selection phase of the originally proposed 2-phase active learning framework to select queries wherein the most informative queries are selected by optimizing the expected DCG loss. As is mentioned in the original paper, we use score-range normalization to calculate the gain function. For details, please refer to [17].",0,,False
229, Random Query Selection (RDM): Queries are selected randomly for labelling from among the set of unlabelled queries. It is to be noted that random query selection is the primary method used in most settings [6].,1,Query,True
230, Permutation Probability Model (PL): Our first proposed approach ( 4) based on capturing informativeness of queries via the uncertainty reduction principle.,0,,False
231, Topic Model (LDA): Our second proposed approach ( 5) based on selecting representative queries which are most topically similar to the set of unlabelled queries.,0,,False
232, Submodular Model (SF): Our final proposed approach ( 6) based on the coupled submodular objective which incorporates both the aspects of query informativeness & representativeness.,1,corpora,True
233,7.2 Dataset,0,,False
234,"We use three commonlyused real-world learning to rank datasets: (i) MQ2007; (ii) MQ2008 from LETOR 4.0 which uses query sets from Million Query track of TREC 2007, TREC 2008 and (iii) the OHSUMED test collection, a subset of the MEDLINE database, which is a bibliographic database of important, peer-reviewed medical literature maintained by the National Library of Medicine. It is worth mentioning that the proposed approaches make use of query term information which is not available in many other ranking datasets, hence we restrict our evaluation to these three datasets having query term information. There are 1700 queries in MQ2007, 800 queries in MQ2008 and 100 queries in the OHSUMED dataset. The MQ2007 & MQ2008 datasets are of notable size and query selection indeed makes sense in the such datasets; the OHSUMED dataset, on the other hand, has too few queries to select from which isn't ideal for a query selection scenario. Nevertheless, we compare performances across all datasets.",1,MQ,True
235,"We adopt a 5-fold cross validation scheme with each fold divided into three parts, one each for training, validation and testing in the ratio 3:1:1. Each query-document pair is represented using 46 features [45 in case of the OHSUMED dataset) along with the relevance score from among {0,1,2}. The test set is used to evaluate the different query selection strategies while active learning is performed on queries from the training set.",1,ad,True
236,7.3 Experimental Setting,0,,False
237,"We start with a base set of 40 labelled queries randomly sampled from the entire query set; the rest of the queries form the candidate set. We make use of C ,"" 4 committee members (where applicable) each of which is constructed based on the procedure described earlier (subsection 4.1). To learn the initial ranking models for each of the committee members, we randomly select a sample of 20 queries from the base set of 40 queries and build a ranking model based""",0,,False
238,551,0,,False
239,% Queries 5% 10% 25% 50%,0,,False
240,SF,0,,False
241,0.726 0.735 0.738& 0.745&,0,,False
242,MQ2008 Dataset,1,MQ,True
243,LDA PL ELO 0.731 0.721 0.729 0.737& 0.733 0.734 0.732 0.733 0.731 0.731 0.734 0.735,0,,False
244,QBC 0.730 0.734 0.730 0.734,0,,False
245,RDM 0.728 0.726 0.727 0.728,0,,False
246,SF,0,,False
247,0.514& 0.508 0.513& 0.516&,0,,False
248,MQ2007 Dataset LDA PL ELO QBC 0.505 0.486 0.501 0.498 0.498 0.501 0.503 0.507 0.509 0.511 0.507 0.511 0.510 0.505 0.509 0.514,1,MQ,True
249,RDM 0.498 0.496 0.504 0.505,0,,False
250,Table 1: Generalizability across different Learning to Rank algorithm: NDCG performance based on ADARANK algorithm. Performance,0,,False
251,"evaluation based on NDCG@10 scores for the different algorithms; SF: Submodular function based query selection, LDA: LDA Topic Model based query selection, PL: min-max Plackett-Luce Based Query Selection, ELO: expected loss minimization baseline, QBC: Query-By-Committee baseline, RDM: Random query selection baseline. % Queries is the % of queries in the labelled set ,"" base set + actively labelled queries. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.""",1,Query,True
252,"on these queries as training data. We first focus on LambdaMART [11], (a state-of-the-art learning to rank algorithm that was the winner of the Yahoo! Learning to Rank challenge [4]) to build ranking models used in the initial part of our experiments. We later show (subsection 8.2) that the queries selected by this method could also be used by other Learning to Rank algorithms.",1,Yahoo,True
253,"The entire experiment is repeated multiple times over the 5 folds on each dataset. We perform batch mode Active Learning for queries by selecting a batch of top 10 queries from the candidate set of queries based on the query selection criterion at each round and iteratively add them to our base set. Queries having no relevant documents were ignored while calculating the different metrics. Based on empirical estimation, the threshold parameter in equation 6 was initialized as  ,"" 0.8. For our initial results, we evaluate the performance of the proposed query selection strategies based on their NDCG@10 values in the test set. We later analyse the generalizability of our approach on a different metric (MAP).""",1,ad,True
254,8. RESULTS,0,,False
255,"We compare the NDCG@10 performance of the test set against the number of queries in training set (base queries plus actively selected) in Figure 1 for the different datasets and compare the performance of the proposed query selection schemes against the QBC, ELO and Random baselines (statistically significant results are highlighted in the respective tables). For all the methods, the NDCG@10 values tends to increase with the number of iterations which is in line with the intuition that the quality of the ranking model is positively correlated with the number of examples in the training set.",0,,False
256,"While min-max PL based query selection stems from the same class of approaches (informativeness based) like the two baselines ELO & QBC, it performs better than both these baselines in most cases; this is in line with our initial claim of capturing informative queries from an alternate view of informativeness based on uncertainty reduction. We observe that LDA Topic Model based query selection performs better than existing baselines as well as the PL model which suggests that the quality of the queries selected by this scheme is better than those selected by other strategies which are mostly based on the informativeness aspect. Perhaps selecting queries based on the informativeness results in some noisy outlier queries getting selected, a case which LDA topic model based query selection avoids by selecting representative queries. The minor fluctuations and occasional dip in the NDCG values on adding more queries",1,ad,True
257,0.54,0,,False
258,0.535,0,,False
259,0.53,0,,False
260,0.525,0,,False
261,NDCG 10,0,,False
262,0.52,0,,False
263,0.515,0,,False
264,0.51,0,,False
265,0.505,0,,False
266,0.5,0,,False
267,0.495,0,,False
268,0.49 0,0,,False
269,",0.1 ,0.2 ,0.3 ,0.4 ,0.5",0,,False
270,50,0,,False
271,100,0,,False
272,150,0,,False
273,200,0,,False
274,250,0,,False
275,300,0,,False
276,350,0,,False
277,400,0,,False
278,450,0,,False
279,500,0,,False
280,no of queries,0,,False
281,Figure 2: Tradeoff analysis between Informativeness & Representativeness for the MQ2007 datasets. The  coefficient in equation 5 controls the relative importance of the two aspects.,1,ad,True
282,"to the labelled set could be explained by the fact that some queries are indeed noisy and selecting such queries induces noise in the ranking models, which results in a slightly worse ranker performance.",0,,False
283,"Finally, we observe from the results that the submodular objective (SF) outperforms the baselines as well as (in most cases) our own proposed purely informativeness & purely representativeness based query selection schemes across the different datasets. While purely informativeness based methods tend to select noisy queries, purely representativeness based methods might possibly select queries which are representative but add redundant information. Hence, selecting queries based on the coupled aspects selects queries which are not only representative of other unselected queries, but are also informative to the ranking model.",1,ad,True
284,8.1 Trade-Off between Informativeness & Representativeness,1,ad,True
285,"Our main motivation behind introducing the submodular objective was to couple the notions of informativeness and representativeness in a joint coherent manner. Indeed, an ideal subset of queries would be a fine blend of queries which convey the maximal amount of information to the ranking model while at the same time, be characteristic of the unselected set of queries. In Figure 2, we present a example analysis on one of the datasets of the relative importance of the two aspects and how they contribute to the overall ranking performance. As can be seen in the figure, a relative weight-",0,,False
286,552,0,,False
287,% Queries 5% 10% 25% 50%,0,,False
288,SF 0.354& 0.371&,0,,False
289,0.362 0.360,0,,False
290,MQ2008 Dataset,1,MQ,True
291,LDA PL ELO,0,,False
292,0.354& 0.344 0.341,0,,False
293,0.328 0.369& 0.371&,0,,False
294,0.354 0.357 0.367,0,,False
295,0.362 0.357 0.369,0,,False
296,QBC 0.340 0.361 0.362 0.346,0,,False
297,RDM 0.342 0.352 0.361 0.365,0,,False
298,SF,0,,False
299,0.164 0.164& 0.165& 0.166&,0,,False
300,MQ2007 Dataset LDA PL ELO,1,MQ,True
301,0.147 0.147 0.154 0.146 0.155 0.148 0.158 0.161 0.157 0.166& 0.160 0.153,0,,False
302,QBC 0.163 0.160 0.161 0.135,0,,False
303,RDM 0.149 0.159 0.160 0.159,0,,False
304,Table 2: Generalizability across different Learning to Rank algorithm: AP performance based on Adarank algorithm. Performance evaluation,1,AP,True
305,"based on NDCG@10 scores for the different algorithms; SF: Submodular function based query selection, LDA: LDA Topic Model based query selection, PL: min-max Plackett-Luce Based Query Selection, ELO: expected loss minimization baseline, QBC: Query-By-Committee baseline, RDM: Random query selection baseline. % Queries is the % of queries in the labelled set ,"" base set + actively labelled queries. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.""",1,Query,True
306,"ing scheme of  ,"" 0.3 (which weighs representativenessvs-informativeness in 3:7 proportions) works best for query selection. This highlights that while representativeness is important, selecting informative queries from the different topics indeed helps. Also, it must be noted that the informativeness term in Equation 9 not only contains contributions from query's singleton informativeness reward, but also has contributions from the topical segregation of queries into partitions. Overall, we chose the coefficient  "","" 0.3 to weigh the contributions from the two aspects while reporting results. It is to be noted that domain knowledge about the dataset in consideration can be used to vary  accordingly, depending on the desired proportion of representativeness & informativeness.""",0,,False
307,"For a milder sized dataset (MQ2008), putting more weight on informativeness helps initially while the relative contributions tend toequal out once a certain threshold of queries have been selected. Overall, the general weighting factor or  , 0.3 works well consistently across different datasets.",1,MQ,True
308,8.2 Generalizability Across Learning Algorithms & Metrics,0,,False
309,"For initial results shown before, the query selection method uses LambdaMART as the learning to rank algorithms optimized for the NDCG metric. Since the labelled learning to rank dataset generated as a result of the query selection process could potentially be used in any future ranking systems, the selected queries should ideally be usable by any learning to rank algorithm, optimized for any metric. We analyze such generalization performance in these sets of experiments. While the initial set of results presented above were NDCG values based on LambdaMART ranking algorithm optimizing for NDCG metric, we divert from our original setting and present results on a different ranker: AdaRank [26] in table 1. Similar results for the OHSUMED dataset can be seen in Fig 3. Additionally, we demonstrate the performance of the proposed query selection strategies on a different metric (MAP) and report results in Table 2. Overall, we can see that the proposed query selection methodologies consistently perform better than the baselines across different ranking algorithms and metrics.",1,MAP,True
310,8.3 Labelling Cost Reduction,0,,False
311,We next analyse the reduction in labelling cost achieved as compared to the case where the entire set of unlabelled queries were labelled. The performance of the ranking function trained with the whole labelled data set is referred to as the optimal performance. When the performance of the active learning model obtained with the proposed algorithms,0,,False
312,Algorithm SF LDA PL ELO,0,,False
313,QBC RDM,0,,False
314,MQ2007,1,MQ,True
315,SS LCR  370 63%  390 61%  490 51%  560 44%  620 39%  720 29%,0,,False
316,MQ2008,1,MQ,True
317,SS LCR  330 57%  400 48%  510 35%  520 34%  540 31%  570 27%,0,,False
318,OHSUMED,0,,False
319,SS LCR  45 29%  55 14%  55 14%  60 6%  60 6%  60 6%,0,,False
320,Table 3: The performance in terms of the Labelling Cost Reduction (LCR) and the Saturated Size (SS) for the various compared approaches.,0,,False
321,nQueries 30 40 50,0,,False
322,SF 0.473 0.478 0.478,0,,False
323,OHSUMED Dataset,0,,False
324,LDA PL ELO QBC 0.469 0.478 0.477 0.478 0.478 0.475 0.472 0.477 0.466 0.469 0.477 0.477,0,,False
325,RDM 0.466 0.467 0.473,0,,False
326,Figure 3: Results on the OHSUMED dataset with LamdaMART,0,,False
327,"Learning to Rank algorithm. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.",0,,False
328,"is comparable to the optimal performance, we call the size of training data as the saturated size (SS). Table 3 highlights the approximate labelling cost reduction (LCR) results obtained via the proposed query selection techniques. The %ages were calculated based on the average number of queries in the training set. The corresponding values were calculated using the LambdaMART implementation with NDCG metric. Experimental evaluation shows the proposed query selection algorithms indeed require less number of queries to be labelled than baseline methods to achieve comparable ranking performance. It is worth mentioning that at some point, adding more queries to the labelled training set doesn't help improve ranking performance, as can be seen by the results of the RDM algorithm in the table: with about 720 labelled queries out of 1015 queries, the algorithm is able to demonstrate comparable ranking performance.",1,ad,True
329,9. CONCLUSION & FUTURE WORK,0,,False
330,"We formulated approaches to the query selection problem into two classes: informativeness based and representativeness based strategies and proposed two novel query selection strategies, one from each class respectively: permutation probability based and LDA Topic Model based query selection. Additionally, we argued that an ideal query selection scheme should incorporate insights from both the aspects and presented a principled way of coupling information from the two aspects. Based on rigorous experiments",1,corpora,True
331,553,0,,False
332,we demonstrated the efficacy of the proposed query selection schemes. A possible line of future work could look at enriching the representativeness aspect by adding document level information to the topic model.,1,ad,True
333,10. REFERENCES,0,,False
334,"[1] J. A. Aslam, E. Kanoulas, V. Pavlu, S. Savev, and E. Yilmaz. Document selection methodologies for efficient and effective learning-to-rank. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 468475. ACM, 2009.",0,,False
335,"[2] M. Bilgic and P. N. Bennett. Active query selection for learning rankers. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 10331034. ACM, 2012.",0,,False
336,"[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:9931022, 2003.",0,,False
337,"[4] C. J. Burges, K. M. Svore, P. N. Bennett, A. Pastusiak, and Q. Wu. Learning to rank using an ensemble of lambda-gradient models. In Yahoo! Learning to Rank Challenge, 2011.",1,ad,True
338,"[5] P. Cai, W. Gao, A. Zhou, and K.-F. Wong. Relevant knowledge helps in choosing right teacher: active query selection for ranking adaptation. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 115124. ACM, 2011.",1,ad,True
339,"[6] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge overview. In Yahoo! Learning to Rank Challenge, 2011.",1,Yahoo,True
340,"[7] O. Chapelle, Y. Chang, and T.-Y. Liu. Future directions in learning to rank. In Yahoo! Learning to Rank Challenge, 2011.",1,Yahoo,True
341,"[8] P. Donmez and J. G. Carbonell. Optimizing estimated loss reduction for active sampling in rank learning. In Proceedings of the 25th international conference on Machine learning, pages 248255. ACM, 2008.",0,,False
342,"[9] P. Donmez, J. G. Carbonell, and P. N. Bennett. Dual strategy active learning. In Machine Learning: ECML 2007, pages 116127. Springer, 2007.",0,,False
343,"[10] U. Feige. A threshold of ln n for approximating set cover. Journal of the ACM, 1998.",0,,False
344,"[11] Y. Ganjisaffar, R. Caruana, and C. V. Lopes. Bagging gradient-boosted trees for high precision, low variance ranking models. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 8594. ACM, 2011.",1,ad,True
345,"[12] Y. Hamo and S. Markovitch. The compset algorithm for subset selection. In IJCAI, pages 728733, 2005.",0,,False
346,"[13] M. Hosseini, I. J. Cox, N. Milic-Frayling, M. Shokouhi, and E. Yilmaz. An uncertainty-aware query selection model for evaluation of ir systems. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 901910. ACM, 2012.",0,,False
347,[14] J. H. Lee. Combining multiple evidence from different properties of weighting schemes. In Proceedings of the 18th annual international ACM SIGIR conference on,0,,False
348,"Research and development in information retrieval, pages 180188. ACM, 1995.",0,,False
349,"[15] J. H. Lee. Analyses of multiple evidence combination. In ACM SIGIR Forum. ACM, 1997.",0,,False
350,"[16] H. Lin and J. Bilmes. Multi-document summarization via budgeted maximization of submodular functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010.",0,,False
351,"[17] B. Long, O. Chapelle, Y. Zhang, Y. Chang, Z. Zheng, and B. Tseng. Active learning for ranking through expected loss optimization. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 267274. ACM, 2010.",0,,False
352,"[18] G. L. Nemhauser and L. A. Wolsey. Integer and combinatorial optimization, volume 18. Wiley New York, 1988.",0,,False
353,"[19] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functions-i. Mathematical Programming, 1978.",0,,False
354,"[20] R. L. Plackett. The analysis of permutations. Applied Statistics, pages 193202, 1975.",0,,False
355,"[21] K. Raman, P. Shivaswamy, and T. Joachims. Online learning to diversify from implicit feedback. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2012.",0,,False
356,"[22] B. Settles. Active learning literature survey. University of Wisconsin, Madison, 52:5566, 2010.",1,ad,True
357,"[23] R. Silva, M. A. Gonc퇫lves, and A. Veloso. Rule-based active sampling for learning to rank. In Machine Learning and Knowledge Discovery in Databases, pages 240255. Springer, 2011.",0,,False
358,"[24] M. Taylor, J. Guiver, S. Robertson, and T. Minka. Softrank: optimizing non-smooth rank metrics. In Proceedings of the 2008 International Conference on Web Search and Data Mining. ACM, 2008.",0,,False
359,"[25] R. W. White and D. Kelly. A study on the effects of personalization and task information on implicit feedback performance. In Proceedings of the 15th ACM international conference on Information and knowledge management, pages 297306. ACM, 2006.",0,,False
360,"[26] J. Xu and H. Li. Adarank: a boosting algorithm for information retrieval. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007.",0,,False
361,"[27] E. Yilmaz and S. Robertson. Deep versus shallow judgments in learning to rank. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 662663. ACM, 2009.",0,,False
362,"[28] H. Yu. Svm selective sampling for ranking with application to data retrieval. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 354363. ACM, 2005.",0,,False
363,"[29] Y. Yue and C. Guestrin. Linear submodular bandits and their application to diversified retrieval. In Advances in Neural Information Processing Systems, 2011.",0,,False
364,554,0,,False
365,,0,,False

,sentence,label,data,regex
0,Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures,0,,False
1,Long Xia Jun Xu Yanyan Lan Jiafeng Guo Xueqi Cheng,0,,False
2,"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",1,ad,True
3,"xialong@software.ict.ac.cn, {junxu, lanyanyan, guojiafeng, cxq}@ict.ac.cn",0,,False
4,ABSTRACT,0,,False
5,"In this paper we address the issue of learning a ranking model for search result diversification. In the task, a model concerns with both query-document relevance and document diversity is automatically created with training data. Ideally a diverse ranking model would be designed to meet the criterion of maximal marginal relevance, for selecting documents that have the least similarity to previously selected documents. Also, an ideal learning algorithm for diverse ranking would train a ranking model that could directly optimize the diversity evaluation measures with respect to the training data. Existing methods, however, either fail to model the marginal relevance, or train ranking models by minimizing loss functions that loosely related to the evaluation measures. To deal with the problem, we propose a novel learning algorithm under the framework of Perceptron, which adopts the ranking model that maximizes marginal relevance at ranking and can optimize any diversity evaluation measure in training. The algorithm, referred to as PAMM (Perceptron Algorithm using Measures as Margins), first constructs positive and negative diverse rankings for each training query, and then repeatedly adjusts the model parameters so that the margins between the positive and negative rankings are maximized. Experimental results on three benchmark datasets show that PAMM significantly outperforms the state-of-the-art baseline methods.",1,ad,True
6,Categories and Subject Descriptors,0,,False
7,H.3.3 [Information Search and Retrieval]: Information Search and Retrieval ­ Retrieval Models,0,,False
8,General Terms,0,,False
9,Algorithms,0,,False
10,Keywords,0,,False
11,search result diversification; maximal marginal relevance; directly optimizing evaluation measures,0,,False
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",1,ad,True
13,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,0,,False
14,DOI: http://dx.doi.org/10.1145/2766462.2767710.,0,,False
15,1. INTRODUCTION,1,DUC,True
16,"It has been widely observed that users' information needs, described by keyword based queries, are often ambiguous or multi-faceted. It is important for commercial search engines to provide search results which balance query-document relevance and document diversity, called search result diversification [1, 30]. One of the key problems in search result diversification is ranking, specifically, how to develop a ranking model that can sort documents based on their relevance to the given query as well as the novelty of the information in the documents.",0,,False
17,"Methods for search result diversification can be categorized into heuristic approaches and learning approaches. The heuristic approaches construct diverse rankings with handcrafted ranking rules. As a representative method in the category, Carbonell and Goldstein [2] propose the maximal marginal relevance (MMR) criterion for guiding the construction ranking models. In MMR, constructing of a diverse ranking is formulated as a process of sequential document selection. At each iteration, the document with the highest marginal relevance is selected. The marginal relevance can be defined as, for example, a linear combination of the querydocument relevance and the maximum distance of the document to the selected document set. A number of approaches have been proposed [8, 23, 24, 25] on the basis of the criterion and promising results have been achieved. User studies also shows that the user browsing behavior matches very well with the maximal marginal relevance criterion: usually users browse the web search results in a top-down manner, and perceive diverse information from each individual document based on what they have obtained in the preceding results [5]. Therefore, in a certain sense, we can say that maximal marginal relevance has been widely accepted as a criterion for guiding the construction of diverse ranking models.",0,,False
18,"Recently, machine learning approaches have been proposed for the task of search result diversification [14, 20, 22, 29, 31], especially the methods that can directly optimize evaluation measures on training data [16, 28]. Yue and Joachims [28] propose SVM-DIV which formulates the task as a problem of structured output prediction. In the model, the measure of subtopic diversity is directly optimized under the structural SVM framework. Liang et al. [16] propose to conduct personalized search result diversification via directly optimizing the measure of -NDCG, also under the structural SVM framework. All of these methods try to resolve the mismatch between the objective function used in training and the final evaluation measure used in testing. Experimen-",0,,False
19,113,0,,False
20,"tal results also showed that directly optimizing the diversity evaluation measures can indeed improve the diverse ranking performances [16, 28]. One problem with the direct optimization approaches is that it is hard, if not impossible, to define a ranking model that can meet the maximal marginal relevance criterion under the direct optimization framework.",0,,False
21,"In this paper, we aim to develop a new learning algorithm that utilizes the maximal marginal relevance model for ranking as well as can directly optimize any diversity evaluation measure in training. Inspired by the work of R-LTR [31] and Perceptron variations [7, 15], we propose a new algorithm for search result diversification, referred to as PAMM (Perceptron Algorithm using Measures as Margins). PAMM utilizes a sequential document selection process as its ranking model. In learning, it first generates positive rankings (ground truth rankings) and negative rankings for the training queries. It then repeats the process of estimating the probabilities for the rankings, calculating the margins between the positive rankings and negative rankings in terms of the ranking probabilities, and updating the model parameters so that the margins are maximized. We show that PAMM algorithm minimizes an upper bound of the loss function that directly defined over the diversity evaluation measures.",0,,False
22,PAMM offers several advantages: 1) adopting the ranking model that meets the maximal marginal relevance criterion; 2) ability to directly optimize any diversity evaluation measure in training; 3) ability to use both positive rankings and negative rankings in training.,1,ad,True
23,"To evaluate the effectiveness of PAMM, we conducted extensive experiments on three public TREC benchmark datasets. The experimental results showed that our methods significantly outperform the state-of-the-art diverse ranking approaches including MMR, SVM-DIV, and R-LTR. We analyzed the results and showed that PAMM makes a good balance between the relevance and diversity via maximizing marginal relevance in ranking. We also showed that by directly optimizing a measure in training, PAMM can indeed enhance the ranking performances in terms of the measure.",1,TREC,True
24,"The rest of the paper is organized as follows. After a summary of related work in Section 2, we describe the general framework of learning maximal marginal relevance model in Section 3. In Section 4 we discuss the proposed PAMM algorithm. Experimental results and discussions are given in Section 5. Section 6 concludes this paper and gives future work.",0,,False
25,2. RELATED WORK,0,,False
26,Methods of search result diversification can be categorized into heuristic approaches and learning approaches.,0,,False
27,2.1 Heuristic approaches,0,,False
28,"It is a common practice to use heuristic rules to construct a diverse ranking list in search. Usually, the rules are created based on the observation that in diverse ranking a document's novelty depends on not only the document itself but also the documents ranked in previous positions. Carbonell and Goldstein [2] propose the maximal marginal relevance criterion to guide the design of diverse ranking models. The criterion is implemented with a process of iteratively selecting the documents from the candidate document set. At each iteration, the document with the highest marginal relevance score is selected, where the score is a linear combination of the query-document relevance and the maximum",0,,False
29,"distance of the document to the documents in current result set. The marginal relevance score is then updated in the next iteration as the number of documents in the result set increases by one. More methods have been developed under the criterion. PM-2 [8] treats the problem of finding a diverse search result as finding a proportional representation for the document ranking. xQuAD [25] directly models different aspects underlying the original query in the form of sub-queries, and estimates the relevance of the retrieved documents to each identified sub-query. See also [3, 9, 10, 11, 21]",0,,False
30,"Heuristic approaches rely on the utility functions that can only use a limited number of ranking signals. Also, the parameter tuning cost is high, especially in complex search settings. In this paper, we propose a learning approach to construct diverse ranking models that can meet the maximal marginal relevance criterion.",0,,False
31,2.2 Learning approaches,0,,False
32,"Methods of machine learning have been applied to search result diversification. In the approaches, rich features can be utilized and the parameters are automatically estimated from the training data. Some promising results have been obtained. For example, Zhu et al. [31] proposed the relational learning to rank model (R-LTR) in which the diverse ranking is constructed with a process of sequential document selection. The training of R-LTR amounts to optimizing the likelihood of ground truth rankings. More work please refer to [14, 20, 22, 29]. All these methods, however, formulate the learning problem as optimizing loss function that loosely related to diversity evaluation measures.",0,,False
33,"Recently methods that can directly optimize evaluation measures have been proposed and applied to search result diversification. Yue and Joachims [28] formulate the task of constructing a diverse ranking as a problem of predicting diverse subsets. Structural SVM framework is adopted to perform the training. Liang et al. [16] propose to conduct personalized search result diversification, also under the structural SVM framework. In the model, the loss function is defined based on the diversity evaluation measure of -NDCG. Thus, the algorithm can be considered as directly optimizing -NDCG in training. One issue with the approach is that it is hard to learn a maximal marginal relevance model under the structural SVM framework.",1,ad,True
34,"In this paper, we propose a Perceptron algorithm that can learn a maximal marginal relevance model, at the same time directly optimizing diversity evaluation measures.",0,,False
35,3. LEARNING MAXIMAL MARGINAL RELEVANCE MODEL,0,,False
36,We first describe the general framework of learning maximal marginal relevance model for search result diversification.,0,,False
37,3.1 Maximal marginal relevance model,0,,False
38,"Suppose that we are given a query q, which is associated with a set of retrieved documents X ,"" {x1, · · · , xM }, where each document xi is represented as a D-dimensional relevance feature vector. Let R "","" RM×M×K denotes a 3-way tensor representing relationship among the M documents, where Rijk stands for the k-th relationship feature of document xi and document xj.""",0,,False
39,114,0,,False
40,Algorithm 1 Ranking via maximizing marginal relevance,0,,False
41,"Input: documents X, document relation R, and ranking model parameters r and d",0,,False
42,"Output: ranking y 1: S0   2: for r ,"" 1, · · · , M do 3: y(r)  arg maxj:xj X\Sr-1 fSr-1 (xj , Rj ) 4: Sr  Sr-1  {xy(r)} 5: end for 6: return y""",0,,False
43,"The maximal marginal relevance model creates a diverse ranking over X with a process of sequential document selection. At each step, the document with the highest marginal relevance is selected and added to the tail of the list [31]. Specifically, let S  X be the set of documents have been selected for query q at one of the document selection step. Given S, the marginal relevance score of each document xi  X\S at current step is defined as a linear combination of the query-document relevance and diversity of the document to the documents in S:",1,ad,True
44,"fS (xi, Ri) ,"" rT xi + dT hS (Ri),""",0,,False
45,(1),0,,False
46,"where xi denotes the relevance feature vector of the document, Ri  RM×K is the matrix representation of the relationship between document xi and the other documents (note that Rij  RK denotes the relationship feature vector of document pair (xi, xj)), and r and d are the weights for the relevance features and diversity features, respectively.",0,,False
47,The first term in Equation (1) represents the relevance of,0,,False
48,"document xi to the query and the second term represents the diversity of xi w.r.t. documents in S. Following the practice in [31], the relational function hS(Ri) is defined as the minimal distance:",0,,False
49,"hS (Ri) ,"" min Rij1, · · · , min RijK .""",0,,False
50,xj S,0,,False
51,xj S,0,,False
52,"According to the maximal marginal relevance criterion, sequential document selection process can be used to create a diverse ranking, as shown in Algorithm 1. Specifically, given a query q, the retrieved documents X, and document relationship R, the algorithm initializes S0 as an empty set. It then iteratively selects the documents from the candidate set. At iteration r (r ,"" 1, · · · , M ), the document with the maximal marginal relevance score fSr-1 is selected and ranked at position r. At the same time, the selected document is inserted to Sr-1.""",0,,False
53,3.2 Learning the ranking model,0,,False
54,Machine learning approaches can be used to learn the,0,,False
55,"maximal marginal relevance model. Suppose we are given N labeled training queries {(X(n), R(n), J (n))}Nn,""1, where J (n) denotes the human labels on the documents, in the form of a binary matrix. J(n)(i, s) "","" 1 if document x(in) contains the s-th subtopic of qn and 0 otherwise1. The learning process, thus, amounts to minimize the loss over all of the training""",0,,False
56,queries:,0,,False
57,N,0,,False
58,min,0,,False
59,"L y^(n), J (n) ,",0,,False
60,(2),0,,False
61,"r ,d n,1",0,,False
62,"1Some datasets also use graded judgements. In this paper,",1,ad,True
63,we assume that all labels are binary.,0,,False
64,Table 1: Summary of notations.,0,,False
65,Notations,0,,False
66,Explanations,0,,False
67,q,0,,False
68,"X ,"" {x1, · · · , xM } xi  RD R  RM×M×K""",0,,False
69,Y,0,,False
70,yY,0,,False
71,"y(t)  {1, · · · , M }",0,,False
72,"Sr  X fS (xi, Ri) hS (Ri) d r J",0,,False
73,"E(X, y, J)  [0, 1]",0,,False
74,query list of documents for q,0,,False
75,document relevant feature vector,0,,False
76,relationship tensor among M documents set of rankings over documents the ranking of documents index of the document ranked at t selected documents before iteration r the scoring function at each step the relational function on Ri weights for relevance features weights for diversity features human labels on document subtopics diversity evaluation measure,0,,False
77,"where y^(n) is the ranking constructed by the maximal marginal relevance model (Algorithm 1) for documents X(n), and L(y^(n), J(n)) is the function for judging the `loss' of the predicted ranking y(n) compared with the human labels J(n).",0,,False
78,3.3 Diversity evaluation measures,0,,False
79,"In search result diversification, query level evaluation measures are used to evaluate the `goodness' of a ranking model. These measures include -NDCG [5], ERR-IA [4], and NRBP [6] etc. We utilize a general function E(X, y, J)  [0, 1] to represent the evaluation measures. The first argument of E is the set of candidate documents, the second argument is a ranking y over documents in X, and the third argument is the human judgements. E measures the agreement between y and J.",0,,False
80,"As an example of diversity evaluation measures, -NDCG [5] is a variation of NDCG [13] in which the newly found subtopics are rewarded and redundant subtopics are penalized. The -NDCG score at rank k can be defined by replacing the raw gain values in standard NDCG@k with novelty-baised gains:",0,,False
81,"-NDCG@k ,",0,,False
82,"k r,1",0,,False
83,N,0,,False
84,G(r)/,0,,False
85,log(r,0,,False
86,+,0,,False
87,1),0,,False
88,"k r,1",0,,False
89,N,0,,False
90,G,0,,False
91,(r)/,0,,False
92,log(r,0,,False
93,+,0,,False
94,1),0,,False
95,",",0,,False
96,(3),0,,False
97,"where N G(r) ,"" s J (y(r), s)(1 - )Cs(r-1) is the novelty-""",0,,False
98,"biased gain at rank r in ranking y, Cs(r-1) ,",0,,False
99,"r-1 k,1",0,,False
100,J,0,,False
101,"(y(k),",0,,False
102,s),0,,False
103,denotes the number of documents observed within top r - 1,0,,False
104,"that contain the s-th subtopic, N G(r) is the novelty-biased",0,,False
105,"gain at rank r in a positive ranking, and y(k) denotes the",0,,False
106,index of the document ranked at k. Usually the parameter,0,,False
107, is set to 0.5.,0,,False
108,ERR-IA [4] is another popular used diversity evaluation,0,,False
109,measure. Given a query with several different subtopics,0,,False
110,"s, the probability of each intent Pr(s|q) can be estimated,",0,,False
111,"where s Pr(s|q) , 1. The intent-aware ERR at rank k can be computed as:",0,,False
112,"ERR-IA@k ,"" Pr(s|q)ERR@k(s),""",0,,False
113,(4),0,,False
114,s,0,,False
115,where ERR@k(s) is the expected reciprocal rank score at k in terms of subtopic s.,0,,False
116,Table 1 gives a summary of the notations described above.,0,,False
117,115,0,,False
118,4. OUR APPROACH: PAMM,1,AP,True
119,4.1 Evaluation measure as loss function,0,,False
120,"We aim to maximize the diverse ranking accuracy in terms of a diversity evaluation measure on the training data. Thus, the loss function in Equation (2) becomes",0,,False
121,N,0,,False
122,"1 - E(X(n), y^(n), J (n)) .",0,,False
123,(5),0,,False
124,"n,1",0,,False
125,It is difficult to directly optimize the loss as E is a nonconvex function.,0,,False
126,"We resort to optimize the upper bound of the loss function under the framework of structured output prediction. According to Theorem (2) in [27], we know that the loss function defined in Equation (5) can be upper bounded by the function defined over the ranking pairs:",0,,False
127,N,0,,False
128,"max E(X(n),y+,J (n))-E(X(n),y-,J (n)) ·",0,,False
129,"n,1",0,,False
130,y+ Y+(n) ; y- Y-(n),0,,False
131,"F (y+, X(n), R(n))  F (y-, X(n), R(n)) , (6)",0,,False
132,"where Y+(n) is the set of all possible `positive' rankings (rankings whose -NDCG/ERR-IA equals to one) for the n-th query, Y-(n) is the set of all possible `negative' rankings (rankings whose -NDCG/ERR-IA is less than one) for the n-the query, · is one if the condition is satisfied otherwise zero, and F (X, R, y) is the query level ranking model. F takes the document set X, document relationship R, and ranking over the document y as inputs. The output of F is the confidence score of the ranking y. The predicted y^(n) in Equation (5) can be considered as the ranking that maximizes F :",0,,False
133,"y^(n) ,"" arg max F (X(n), R(n), y),""",0,,False
134,(7),0,,False
135,yY (n),0,,False
136,"where Y(n) is the set of all possible rankings over X(n). Here F is defined as the probability of generating the ranking list y with a process of iteratively selecting the top ranked documents from the remaining documents, and using the marginal relevance function fS in Equation (1) as the selection criterion:",0,,False
137,"F (X, R, y) ,""Pr(y|X, R)""",0,,False
138,",""Pr(xy(1) · · · xy(M)|X, R)""",0,,False
139,M -1,0,,False
140,",",0,,False
141,"Pr(xy(r)|X, Sr-1, R)",0,,False
142,(8),0,,False
143,"r,1",0,,False
144,M -1,0,,False
145,",",0,,False
146,"r,1",0,,False
147,"exp{fSr-1 (xi, Ry(r))}",0,,False
148,"M k,r",0,,False
149,exp{fSr-1,0,,False
150,"(xi,",0,,False
151,Ry(k))},0,,False
152,"where y(r) denotes the index of the document ranked at the r-th position in y, Sr-1 , {xy(k)}rk-,""11 is the documents ranked at the top r - 1 positions in y, fSr-1 (xi, Ri) is the marginal relevance score of document xi w.r.t. the selected documents in Sr-1, and S0 "",""  is an empty set. With the definition of F , it is obvious that the maximal marginal""",0,,False
153,relevance process of Algorithm 1 actually greedily searches,0,,False
154,the solution for optimizing the problem of Equation (7).,0,,False
155,To conduct the optimization under the Perceptron frame-,0,,False
156,"work, the upper bound of Equation (6) is further relaxed,",0,,False
157,by replacing the max with sum and moving the term,0,,False
158,"(E(X(n), y+, J (n)) - E(X(n), y-, J (n))) into · as margin. The upper bound of Equation (6) becomes:",0,,False
159,N,0,,False
160,"F (X(n), R(n), y+) - F (X(n), R(n), y-) ",0,,False
161,"n,1y+ ;y-",0,,False
162,"E(X(n),y+,J (n))-E(X(n),y-,J (n)) . (9)",0,,False
163,"This is because i xi  maxi xi if xi  0 holds for all i, and x - y  z  z · x  y holds if z  [0, 1]. Please note that we assume E(X, y+, J)  [0, 1] and thus we have (E(X(n), y+, J (n)) - E(X(n), y-, J (n)))  [0, 1] because E(X(n), y+, J (n)) > E(X(n), y-, J (n)).",0,,False
164,4.2 Direct optimization with Perceptron,0,,False
165,"The loss function in Equation (9) can be optimized under the framework of Perceptron. In this paper, inspired by the work of structured Perceptron [7] and Perceptron algorithm with uneven margins [15], we have developed a novel learning algorithm to optimize the loss function in Equation (9). The algorithm is referred to as PAMM and is shown in Algorithm 2.",0,,False
166,"PAMM takes a training set {(X(n), R(n), J (n))}Nn,""1 as input and takes the diversity evaluation measure E, learning rate , number of positive rankings per query  +, and number of negative rankings per query  - as parameters. For each query qn, PAMM first generates  + positive rankings P R(n) and  - negative rankings N R(n) (line (2) and line (3)). P R(n) and N R(n) play as the random samples of Y+(n) and Y-(n), respectively. PAMM then optimizes the model parameters r and d iteratively in a stochastic manner over the ranking pairs: at each round, for each pair between a positive ranking and a negative ranking (y+, y-), the gap of these two rankings in terms of the query level ranking model F "","" F (X, R, y+) - F (X, R, y-) is calculated based on current parameters r and d (line (9)). If F is smaller than the margin in terms of evaluation measure E "","" E(X, y+, J)-E(X, y-, J) (line (10)), the model parameters will be updated so that F will be enlarged (line (11) and line (12)). The iteration continues until convergence. Finally, PAMM outputs the optimized model parameters (r, d).""",0,,False
167,"Next, we will explain the key steps of PAMM in detail.",0,,False
168,4.2.1 Generating positive and negative rankings,0,,False
169,"In PAMM, it is hard to directly conduct the optimization over the sets of positive rankings Y+(n) and negative rankings Y-(n), because in total these two sets have M ! rankings if the candidate set contains M documents. Thus, PAMM samples the rankings to reduce the training time.",0,,False
170,"For each training query, PAMM first samples a set of positive rankings. Algorithm 3 illustrates the procedure. Similar to the online ranking algorithm shown in Algorithm 1, the positive rankings are generated with a sequential document selection process and the selection criteria is the diversity evaluation measure E. After generating the first positive ranking y(1), the algorithm constructs other positive rankings based on y(1), by randomly swapping the positions of two documents whose subtopic coverage are identical.",0,,False
171,"For each training query, PAMM also samples a set of negative rankings. Algorithm 4 shows the procedure. The algorithm simply generates random rankings iteratively. If the generated ranking is not a positive ranking and satisfies the",0,,False
172,116,0,,False
173,Algorithm 2 The PAMM Algorithm,0,,False
174,"Input: training data {(X(n), R(n), J (n))}Nn,""1, learning rate""",0,,False
175,", diversity evaluation measure E, number of positive rankings per query  +, number of negative rankings per query  -.",0,,False
176,"Output: model parameters (r, d)",0,,False
177,"1: for n ,"" 1 to N do 2: P R(n) PositiveRankings(X(n), J (n), E,  +) {Algo-""",0,,False
178,rithm 3},0,,False
179,"3: N R(n) NegativeRankings(X(n), J (n), E,  -) {Algo-",0,,False
180,rithm 4},0,,False
181,4: end for,0,,False
182,"5: initialize {r, d}  random values in [0, 1]",0,,False
183,6: repeat,0,,False
184,"7: for n , 1 to N do",0,,False
185,8:,0,,False
186,"for all {y+, y-}  P R(n) × N R(n) do",0,,False
187,9:,0,,False
188,"F  F (X(n), R(n), y+) - F (X(n), R(n), y-)",0,,False
189,"{F (X, R, y) is defined in Equation (8)}",0,,False
190,10:,0,,False
191,"if F  E(X(n), y+, J (n)) - E(X(n), y-, J (n))",0,,False
192,then,0,,False
193,11:,0,,False
194,calculate r(n) and d(n) {Equation (10),0,,False
195,and Equation (11)},0,,False
196,12:,0,,False
197,"(r, d)  (r, d) +  × (r(n), d(n))",0,,False
198,13:,0,,False
199,end if,0,,False
200,14: end for,0,,False
201,15: end for,0,,False
202,16: until convergence,0,,False
203,"17: return (r, d)",0,,False
204,"user predefined constraints (e.g, -NDCG@20  0.8), the ranking will be added into the ranking set N R.",1,ad,True
205,"Please note that in some extreme cases Algorithm 3 and Algorithm 4 cannot create enough rankings. In our implementations, the algorithms are forced to return after running enough iterations.",0,,False
206,4.2.2 Updating r and d,0,,False
207,"Given a ranking pair (y+, y-)  P R(n) × N R(n), PAMM updates r and d as",0,,False
208,"r  r +  × r and d  d +  × d,",0,,False
209,"if F (X, R, y+) - F (X, R, y-)  E(X, y+, J) - E(X, y-, J). The goal of the update is to enlarge the margin between y+ and y- in terms of query level model: F ,"" F (X, R, y+) - F (X, R, y-). For convenience of calculation, we resort to the problem of""",0,,False
210,"F (X, R, y+)",0,,False
211,max log,0,,False
212,",",0,,False
213,"r,d F (X, R, y-)",0,,False
214,"because F (X, R, y) > 0 and log(·) is a monotonous increasing function. Thus, r can be calculated as the gradient:",1,ad,True
215,r,0,,False
216," ,",0,,False
217,log,0,,False
218,"F (X,R,y+) F (X,R,y-)",0,,False
219,r,0,,False
220," log F (X, R, y+)  log F (X, R, y-)",0,,False
221,",",0,,False
222,-,0,,False
223,",",0,,False
224,r,0,,False
225,r,0,,False
226,(10),0,,False
227,Algorithm 3 PositiveRankings,0,,False
228,"Input: documents X, diversity labels J, evaluation measure E, and the number of positive rankings  +",0,,False
229,"Output: positive rankings P R 1: for r , 1 to |X| do 2: y(1)(r)  arg maxj:xj X\Sr-1",0,,False
230,"E Sr-1  {xj }, y(1)(1), · · · , y(1)(r - 1), j , J",0,,False
231,"3: Sr  Sr-1  {xy(1)(r)} 4: end for 5: P R  {y(1)} 6: while |P R| <  + do 7: y  y(1) 8: (k, l)  randomly choose two documents whose hu-",0,,False
232,"man labels are identical, i.e., J(y(k)) , J(y(1)(l)) 9: y(k)  y(l) {swap documents at rank k and l} 10: if y / P R then 11: P R  P R  {y} 12: end if 13: end while 14: return P R",0,,False
233,Algorithm 4 NegativeRankings,0,,False
234,"Input: documents X, diversity labels J, evaluation measure E, and number of negative rankings  -",0,,False
235,"Output: N R 1: N R ,""  2: while |N R| <  - do 3: y  random shuffle (1, · · · , |X|) 4: if y / N R and E(X, y, J) is as expected then 5: N R  N R  {y} 6: end if 7: end while 8: return N R""",0,,False
236,where,0,,False
237," log F (X, R, y)  ,",0,,False
238,"|X |-1 j,1",0,,False
239,log,0,,False
240,Pr(xy(j),0,,False
241,|X,0,,False
242,\Sj-1,0,,False
243,",",0,,False
244,R),0,,False
245,r,0,,False
246,r,0,,False
247,|X |-1,0,,False
248,",",0,,False
249,"j,1",0,,False
250,xy(j) -,0,,False
251,"|X | k,j",0,,False
252,xy(k),0,,False
253,"exp{fSj-1 (xy(k),",0,,False
254,Ry(k))},0,,False
255,.,0,,False
256,"|X | k,j",0,,False
257,"exp{fSj-1 (xy(k),",0,,False
258,Ry(k))},0,,False
259,"Similarly, d can be calculated as",0,,False
260," log F (X, R, y+)  log F (X, R, y-)",0,,False
261,"d ,",0,,False
262,d,0,,False
263,-,0,,False
264,", d",0,,False
265,(11),0,,False
266,where,0,,False
267,|X |-1,0,,False
268," log F (X, R, y)",0,,False
269,",",0,,False
270,d,0,,False
271,"j,1",0,,False
272,hSj-1 (Ry(j))-,0,,False
273,"|X | k,j",0,,False
274,hSj-1 (Ry(k)),0,,False
275,"exp{fSj-1 (xy(k),",0,,False
276,Ry(k))},0,,False
277,.,0,,False
278,"|X | k,j",0,,False
279,"exp{fSj-1 (xy(k),",0,,False
280,Ry(k))},0,,False
281,"Intuitively, the gradients r and d are calculated so that the line 12 of Algorithm 2 will increase F (X, R, y+)",1,ad,True
282,"and decrease F (X, R, y-).",0,,False
283,4.3 Analysis,0,,False
284,We analyzed time complexity of PAMM. The learning process of PAMM (Algorithm 2) is of order O(T · N ·  + ·  - ·,0,,False
285,117,0,,False
286,"Table 2: Statistics on WT2009, WT2010 and",1,WT,True
287,WT2011. Dataset #queries #labeled docs #subtopics per query,1,WT,True
288,WT2009,1,WT,True
289,50,0,,False
290,WT2010,1,WT,True
291,48,0,,False
292,WT2011,1,WT,True
293,50,0,,False
294,5149 6554 5000,0,,False
295,38 37 26,0,,False
296,"M 2 · (D + K)), where T denotes the number of iterations, N the number of queries in training data,  + the number of positive rankings per query,  - the number of negative rankings per query, M the maximum number of documents for queries in training data, D the number of relevance features, and K the number of diversity features. The time complexity of online ranking prediction (Algorithm 1) is of order O(M 2(D + K)).",0,,False
297,"PAMM is a simple yet powerful learning algorithm for search result diversification. It has several advantages compared with the existing learning methods such as R-LTR [31], SVM-DIV [28], and structural SVM [26].",1,ad,True
298,"First, PAMM employs a more reasonable ranking model. The model follows the maximal marginal relevance criterion and can be implemented with a process of sequential document selection. In contrast, structural SVM approaches [26] calculate all of the ranking scores within a single step, as that of in relevance ranking. The marginal relevance of each document cannot be taken into consideration at ranking time.",0,,False
299,"Second, PAMM can incorporate any diversity evaluation measure in training, which makes the algorithm focus on the specified measure when updating the model parameters. In contrast, R-LTR only minimizes loss function that is loosely related to diversity evaluation measures and SVM-DIV is trained to optimize the subtopic coverage.",1,corpora,True
300,"Third, PAMM utilizes the pairs between the positive rankings and the negative rankings in training, which makes it possible to leverage more information in training. Specifically, it enables PAMM algorithm to enlarge the margins between the positive rankings and negative rankings when updating the parameters. In contrast, R-LTR only uses the information in the positive rankings and the training is aimed to maximizing the likelihood.",0,,False
301,5. EXPERIMENTAL RESULTS,0,,False
302,5.1 Experiment setting,0,,False
303,"We conducted experiments to test the performances of PAMM using three TREC benchmark datasets for diversity tasks: TREC 2009 Web Track (WT2009), TREC 2010 Web Track (WT2010), and TREC 2011 Web Track (WT2011). Each dataset consists of queries, corresponding retrieved documents, and human judged labels. Each query includes several subtopics identified by TREC assessors. The document relevance labels were made at the subtopic level and the labels are binary2. Statistics on the datasets are given in Table 2.",1,TREC,True
304,"All the experiments were carried out on the ClueWeb09 Category B data collection3, which comprises of 50 million English web documents. Porter stemming, tokenization, and stop-words removal (using the INQUERY list) were applied",1,ClueWeb,True
305,2WT2011 has graded judgements. In this paper we treat them as binary. 3http://boston.lti.cs.cmu.edu/data/clueweb09,1,WT,True
306,"to the documents as preprocessing. We conducted 5-fold cross-validation experiments on the three datasets. For each dataset, we randomly split the queries into five even subsets. At each fold three subsets were used for training, one was used for validation, and one was used for testing. The results reported were the average over the five trials.",0,,False
307,"As for evaluation measures, -NDCG@k (Equation (3)) with  , 0.5 and k , 20 is used. We also used ERR-IA@k (Equation (4)) with k , 20 to evaluate the performances.",0,,False
308,We compared PAMM with several types of baselines. The baselines include the conventional relevance ranking models in which document diversity is not taken into consideration. Query likelihood (QL) [18] language models for informa-,1,Query,True
309,tion retrieval. ListMLE [17] a representative learning-to-rank model for,0,,False
310,information retrieval. We also compared PAMM with three heuristic approaches to search result diversification in the experiments. MMR [2] a heuristic approach to search result diversifica-,0,,False
311,"tion in which the document ranking is constructed via iteratively selecting the document with the maximal marginal relevance. xQuAD [25] a representative heuristic approach to search result diversification. PM-2 [8] another widely used heuristic approach to search result diversification. Please note that these three baselines require a prior relevance function to implement their diversification steps. In our experiments, ListMLE was chosen as the relevance function. Learning approaches to search result diversification are also used as baselines in the experiments. SVM-DIV [28] a representative learning approach to search result diversification. It utilizes structural SVMs to optimize the subtopic coverage. SVM-DIV does not consider relevance. For fair performance comparison, in the baseline, we first apply ListMLE to capture relevance, and then apply SVM-DIV to re-rank the top-K retrieved documents. Structural SVM [26] Structural SVM can be configured to directly optimize diversity evaluation measures, as shown in [16]. In the paper, we used structural SVM to optimize -NDCG@20 and ERR-IA@20, denoted as StructSVM(-NDCG) and StructSVM(ERR-IA), respectively. R-LTR [31] a state-of-the-art learning approach to search result diversification. The ranking function is a linear combination of the relevance score and diversity score between the current document and those previously selected. Following the practice in [31], in our experiments we used the results of R-LTRmin which defines the relation function hS(R) as the minimal distance.",0,,False
312,5.2 Features,0,,False
313,"As for features, we adopted the features used in the work of R-LTR [31]. There are two types of features: the relevance features which capture the relevance information of a query with respect to a document, and the diversity features which represent the relation information among documents. Table 3 and Table 4 list the relevance features and diversity features used in the experiments, respectively.",1,ad,True
314,4http://www.dmoz.org,0,,False
315,118,0,,False
316,Table 3: Relevance features used in the experiments.,0,,False
317,"The first 4 lines are query-document matching features, each applied to the fields of body, anchor, title, URL, and the whole documents. The latter 3",0,,False
318,lines are document quality features. [31],0,,False
319,Name Description,0,,False
320,# Features,0,,False
321,TF-IDF The tf-idf model,0,,False
322,5,0,,False
323,BM25 BM25 with default parameters,0,,False
324,5,0,,False
325,LMIR LMIR with Dirichlet smoothing,1,LM,True
326,5,0,,False
327,MRF[19] MRF with ordered/unordered phrase,0,,False
328,10,0,,False
329,PageRank PageRank score,0,,False
330,1,0,,False
331,#inlinks number of inlinks,0,,False
332,1,0,,False
333,#outlinks number of outlinks,0,,False
334,1,0,,False
335,Table 4: The seven diversity features used in the experiments. Each feature is extracted over two documents. [31],0,,False
336,Name,0,,False
337,Description,0,,False
338,Subtopic Diversity Text Diversity Title Diversity Anchor Text Diversity ODP-Based Diversity Link-Based Diversity URL-Based Diversity,1,ODP,True
339,Euclidean distance based on PLSA[12] Cosine-based distance on term vectors Text diversity on title Text diversity on anchor ODP4 taxonomy-based distance Link similarity of document pair URL similarity of document pair,1,ODP,True
340,5.3 Experiments with TREC datasets,1,TREC,True
341,"In the experiments, we made use of the benchmark datasets of WT2009, WT2010, and WT2011 from the TREC Web Track, to test the performances of PAMM.",1,ad,True
342,"PAMM has to tune some parameters. The learning rate parameter  was tuned based on the validation set during each experiment. In all of the experiments in this subsection, we set the number of positive rankings per query  + ,"" 5, and number of negative rankings per query  - "","" 20. As for the parameter E of PAMM, -NDCG@20 and ERR-IA@20 were utilized. The results for PAMM using -NDCG@20 in training are denoted as PAMM(-NDCG). The PAMM results using ERR-IA@20 as measures are denoted as PAMM(ERR-IA).""",0,,False
343,"The experimental results on WT2009, WT2010, and WT2011 are reported in Table 5, Table 6, and Table 7, respectively. Numbers in parentheses are the relative improvements compared with the baseline method of query likelihood (QL). Boldface indicates the highest score among all runs. From the results, we can see that PAMM(-NDCG) and PAMM(ERRIA) outperform all of the baselines on all of the three datasets in terms of both -NDCG@20 and ERR-IA@20. We conducted significant testing (t-test) on the improvements of PAMM(-NDCG) over the baselines in terms of -NDCG@20 and ERR-IA@20. The results indicate that all of the improvements are statistically significant (p-value < 0.05). We also conducted t-test on the improvements of PAMM(ERRIA) over the baselines in terms of -NDCG@20 and ERRIA@20. The improvements are also statistically significant. All of the results show that PAMM is effective for the task of search result diversification.",1,WT,True
344,"We observed that on all of the three datasets, PAMM(NDCG) trained with -NDCG@20 performed best in terms of -NDCG@20 while PAMM(ERR-IA) trained with ERRIA@20 performed best in terms of ERR-IA@20. The results indicate that PAMM can enhance diverse ranking perfor-",0,,False
345,mances in terms of a measure by using the measure in training. We will further discuss the phenomenon in next section.,0,,False
346,Table 5: Performance comparison of all methods in,0,,False
347,official TREC diversity measures for WT2009.,1,TREC,True
348,Method,0,,False
349,ERR-IA@20,0,,False
350,-NDCG@20,0,,False
351,QL ListMLE,0,,False
352,MMR xQuAD,0,,False
353,PM-2 SVM-DIV StructSVM(-NDCG) StructSVM(ERR-IA),0,,False
354,R-LTR PAMM(-NDCG) PAMM(ERR-IA),0,,False
355,0.164 0.191(+16.46%) 0.202(+23.17%) 0.232(+41.46%) 0.229(+39.63%) 0.241(+46.95%) 0.260(+58.54%) 0.261(+59.15%) 0.271(+65.24%) 0.284(+73.17%) 0.294(+79.26%),0,,False
356,0.269 0.307(+14.13%) 0.308(+14.50%) 0.344(+27.88%) 0.337(+25.28%) 0.353(+31.23%) 0.377(+40.15%) 0.373(+38.66%) 0.396(+47.21%) 0.427(+58.74%) 0.422(+56.88%),0,,False
357,Table 6: Performance comparison of all methods in,0,,False
358,official TREC diversity measures for WT2010.,1,TREC,True
359,Method,0,,False
360,ERR-IA@20,0,,False
361,-NDCG@20,0,,False
362,QL ListMLE,0,,False
363,MMR xQuAD,0,,False
364,PM-2 SVM-DIV StructSVM(-NDCG) StructSVM(ERR-IA),0,,False
365,R-LTR PAMM(-NDCG) PAMM(ERR-IA),0,,False
366,0.198 0.244(+23.23%) 0.274(+38.38%) 0.328(+65.66%) 0.330(+66.67%) 0.333(+68.18%) 0.352(+77.78%) 0.355(+79.29%) 0.365(+84.34%) 0.380(+91.92%) 0.387(+95.45%),0,,False
367,0.302 0.376(+24.50%) 0.404(+33.77%) 0.445(+47.35%) 0.448(+48.34%) 0.459(+51.99%) 0.476(+57.62%) 0.472(+56.29%) 0.492(+62.91%) 0.524(+73.51%) 0.511(+69.21%),0,,False
368,Table 7: Performance comparison of all methods in,0,,False
369,official TREC diversity measures for WT2011.,1,TREC,True
370,Method,0,,False
371,ERR-IA@20,0,,False
372,-NDCG@20,0,,False
373,QL ListMLE,0,,False
374,MMR xQuAD,0,,False
375,PM-2 SVM-DIV StructSVM(-NDCG) StructSVM(ERR-IA),0,,False
376,R-LTR PAMM(-NDCG) PAMM(ERR-IA),0,,False
377,0.352 0.417(+18.47%) 0.428(+21.59%) 0.475(+34.94%) 0.487(+38.35%) 0.490(+39.20%) 0.512(+45.45%) 0.513(+45.74%) 0.539(+53.13%) 0.541(+53.70%) 0.548(+55.68%),0,,False
378,0.453 0.517(+14.13%) 0.530(+17.00%) 0.565(+24.72%) 0.579(+27.81%) 0.591(+30.46%) 0.617(+36.20%) 0.613(+35.32%) 0.630(+39.07%) 0.643(+41.94%) 0.637(+40.62%),0,,False
379,5.4 Discussions,0,,False
380,"We conducted experiments to show the reasons that PAMM outperforms the baselines, using the results of the WT2009 dataset as examples.",1,WT,True
381,5.4.1 Effect of maximizing marginal relevance,0,,False
382,"We found that PAMM makes a good tradeoff between the query-document relevance and document diversity via maximizing marginal relevance. Here we use the result with regard to query number 24 (""diversity"" which contains 4 subtopics), to illustrate why our method is superior to the baseline method of Structural SVM trained with -NDCG@20 (denoted as StructSVM(-NDCG)). Note that structural",1,ad,True
383,119,0,,False
384,PAMM intermediate rankings,0,,False
385,1,0,,False
386,"StructSVM 2, 4",0,,False
387,"2, 4",0,,False
388,"2, 4",0,,False
389,"2, 4",0,,False
390,"2, 4",0,,False
391,ranking positions,0,,False
392,2,0,,False
393,3,0,,False
394,4,0,,False
395,"1, 4",0,,False
396,2,0,,False
397,"1, 3",0,,False
398,2,0,,False
399,4,0,,False
400,"1, 3",0,,False
401,"1, 3",0,,False
402,2,0,,False
403,4,0,,False
404,"1, 3",0,,False
405,"1, 4",0,,False
406,4,0,,False
407,"1, 3",0,,False
408,"1, 4",0,,False
409,2,0,,False
410,"5 -NDCG@5 4 0.788 1, 4 0.744 1, 4 0.803 2 0.812 4 0.815",0,,False
411,Figure 1: Example rankings from WT2009. Each shaded block represents a document and the number(s) in the block represent the subtopic(s) covered by the document.,1,WT,True
412,"SVM cannot leverage the marginal relevance in its ranking model. Figure 1 shows the top five ranked documents by StructSVM(-NDCG), as well as four intermediate rankings generated by PAMM(-NDCG) (denoted as fS0 , fS1 , fS2 , and fS3 ). The ranking denoted as fSr is generated as: first sequentially selecting the documents for ranking positions of 1, 2, · · · , r - 1 with models fS0 , fS1 , · · · , fSr-2 , respectively; then ranking the remaining documents with fSr-1 . For example, the intermediate ranking denoted as fS2 is generated as: selecting one document with fS0 and setting it to rank 1, then selecting one document with fS1 and set it to rank 2, and finally ranking the remaining documents with fS2 and putting them to the tail of the list. Each of the shaded block indicates a document and the number(s) in the block indicates the subtopic(s) assigned to the document by the human annotators. The performances in terms of -NDCG@5 are also shown in the last column. Here we used -NDCG@5 because only the top 5 documents are shown.",1,ad,True
413,"The results in Figure 1 indicate the effectiveness of the maximal marginal relevance criterion. We can see that the -NDCG@5 increases steadily with the increasing rounds of document selection iterations. In the first iteration, fS0 selects the most relevant document and puts it to the first position, without considering the diversity. Thus, the NDCG@5 of the ranking generated by fS0 is lower than that of by StructSVM(-NDCG). In the second iteration, the ranking function fS1 selects the document associated with subtopics 1 and 3 and ranks it to the second position, according to the maximal marginal relevance criterion. From the view point of diverse ranking, this is obviously a better choice than StructSVM(-NDCG) made, which selects the document with subtopics 1 and 4. (Note that both Structural SVM and PAMM select the document with subtopics 2 and 4 for the first position.) In the following steps, fS2 and fS3 select documents for ranking positions of 3 and 4, also following the maximal marginal relevance criterion. As a result, fS1 , fS2 , and fS3 outperforms StructSVM(-NDCG).",1,ad,True
414,5.4.2 Ability to improve the evaluation measures,0,,False
415,"We conducted experiments to see whether PAMM has the ability to improve the diverse ranking quality in terms of a measure by using the measure in training. Specifically, we trained models using -NDCG@20 and ERR-IA@20 and",0,,False
416,ERR-IA@20,0,,False
417,-NDCG@20,0,,False
418,0.43,0,,False
419,0.42,0,,False
420,0.41,0,,False
421,0.4,0,,False
422,0.39,0,,False
423,fold1 fold2 fold3 fold4 fold5 PAMM(-NDCG) PAMM(ERR-IA),0,,False
424,Figure 2: Performance in terms of -NDCG@20 when model is trained with -NDCG@20 or ERRIA@20.,0,,False
425,0.3,0,,False
426,0.28,0,,False
427,0.26,0,,False
428,0.24,0,,False
429,fold1 fold2 fold3 fold4 fold5 PAMM(-NDCG) PAMM(ERR-IA),0,,False
430,Figure 3: Performance in terms of ERR-IA@20 when model is trained with -NDCG@20 or ERRIA@20.,0,,False
431,"evaluated their accuracies on the test dataset in terms of both -NDCG@20 and ERR-IA@20. The experiments were conducted for each fold of the cross validation and performances on each fold are reported. Figure 2 and Figure 3 show the results in terms of -NDCG@20 and ERR-IA@20, respectively. From Figure 2, we can see that on all of the 5 folds (except fold 1), PAMM(-NDCG) trained with NDCG@20 performs better in terms of -NDCG@20. Similarly, from Figure 3, we can see that on all of the 5 folds (except fold 4), PAMM(ERR-IA) trained with ERR-IA@20 performs better in terms of ERR-IA@20. Similar results have also been observed in experiments on other datasets (see the results in Table 5, Table 6, and Table 7). All of the results indicate that PAMM can indeed enhance the diverse ranking quality in terms of a measure by using the measure in training.",0,,False
432,5.4.3 Effects of positive and negative rankings,0,,False
433,"We examined the effects of the number of positive rankings generated per query (parameter  +). Specifically, we compared the performances of PAMM(-NDCG) w.r.t. different  + values. Figure 4 shows the performance curve in terms of -NDCG@20. The performance of R-LTR base-",0,,False
434,120,0,,False
435,-NDCG@20,0,,False
436,0.44 12,0,,False
437,10 0.42,0,,False
438,8,0,,False
439,6,0,,False
440,0.4,0,,False
441,4,0,,False
442,PAMM(-NDCG),0,,False
443,PAMMR-(LT-NRDCG) 2,0,,False
444,0.38,0,,False
445,traiRn-inLTg Rtime,0,,False
446,0 1 2 3 4 5 6 7 8 9 10,0,,False
447,parameter  +,0,,False
448,Figure 4: Ranking accuracies and training time w.r.t.  +.,0,,False
449,0.44 8,0,,False
450,0.42,0,,False
451,6,0,,False
452,-NDCG@20,0,,False
453,0.4,0,,False
454,0.38 5,0,,False
455,4,0,,False
456,PAMM(-NDCG) 2 PAMMR-(LT-NRDCG),0,,False
457,traiRn-inLTg Rtime 0,0,,False
458,10 15 20 25 30 parameter  -,0,,False
459,Figure 5: Ranking accuracies and training time w.r.t -.,0,,False
460,time (hours) performance,0,,False
461,time (hours) -NDCG@20,0,,False
462,0.43,0,,False
463,0.42,0,,False
464,0.41,0,,False
465,0.4,0,,False
466,0.39,0,,False
467,0.38 0.5,0,,False
468,PAMM(-NDCG) R-LTR,0,,False
469,0.6,0,,False
470,0.7,0,,False
471,0.8,0,,False
472,0.9,0,,False
473,-NDCG@20,0,,False
474,Figure 6: Ranking accuracies w.r.t. different NDCG@20 values of the negative rankings.,0,,False
475,0.4,0,,False
476,0.3,0,,False
477,0.2 -NDCG@20 ERR-IA@20,0,,False
478,0,0,,False
479,10 20 30 40 50 60,0,,False
480,number of rounds,0,,False
481,Figure 7: Learning curve of PAMM(-NDCG).,0,,False
482,"line is also shown for reference. From the result, we can see that the curve does not change much with different  + val-",0,,False
483,"ues, which indicates the robustness of PAMM. Figure 4 also shows training time (in hours) w.r.t. different  + values. The training time increased dramatically with large  +, be-",0,,False
484,cause more ranking pairs are generated for training. In our experiments  + was set to 5.,0,,False
485,"We further examined the effect of the number of negative rankings per query (parameter  -). Specifically, we",0,,False
486,compared the performances of PAMM(-NDCG) w.r.t. different  - and the results are shown in Figure 5. From the,0,,False
487,"results, we can see that the performance of PAMM increasing steadily with the increasing  - values until  - ,"" 20,""",1,ad,True
488,which indicates that PAMM can achieve better ranking per-,0,,False
489,formance with more information from the negative rankings.,0,,False
490,"As the cost, the training time increased dramatically, be-",0,,False
491,"cause more training instances are involved in training. In our experiments,  - was set to 20.",0,,False
492,We also conducted experiments to show the effect of sam-,0,,False
493,pling the negative rankings with different -NDCG values.,0,,False
494,"Specifically, in each of the experiment, we configured the Al-",0,,False
495,gorithm 4 to choose the negative rankings whose -NDCG@20,0,,False
496,"values are 0.5, 0.6, 0.7, 0.8, and 0.9, respectively. Figure 6",0,,False
497,shows the performances of PAMM(-NDCG) w.r.t. different,0,,False
498,-NDCG@20 values of the sampled negative rankings. From,0,,False
499,"the results, we can see that PAMM performs best when the",0,,False
500,-NDCG@20 of the sampled negative rankings ranges from 0.6 to 0.9. The results also indicate that PAMM is robust and not very sensitive to different methods of sampling the negative rankings.,0,,False
501,5.4.4 Convergence,0,,False
502,"Finally we conducted experiments to show whether PAMM can converge in terms of the diversity evaluation measures. Specifically, we showed the learning curve of PAMM(-NDCG) in terms of -NDCG@20 and ERR-IA@20 during the training phase. At each training iteration the model parameters are outputted and evaluated on the test data. Figure 7 shows the performance curves w.r.t. the number of training iterations. From the results, we can see that the ranking accuracy of that PAMM(-NDCG) steadily improves in terms of both -NDCG@20 and ERR-IA@20, as the training goes on. PAMM converges and returns after running about 60 iterations. We also observed that in all of our experiments, PAMM usually converges and returns after running 50100 iterations. Similar phenomenon was also observed from the learning curve of PAMM(ERR-IA). The results indicates that PAMM converges fast and conducts the training efficiently.",1,ad,True
503,121,0,,False
504,6. CONCLUSION AND FUTURE WORK,0,,False
505,"In this paper we have proposed a novel algorithm for learning ranking models in search result diversification, referred to as PAMM. PAMM makes use of the maximal marginal relevance model for constructing the diverse rankings. In training, PAMM directly optimizes the diversity evaluation measures on training queries under the framework of Perceptron. PAMM offers several advantages: employs a ranking model that follows the maximal marginal relevance criterion, ability to directly optimize any diversity evaluation measure, and ability to utilize both positive rankings and negative rankings in training. Experimental results based on three benchmark datasets show that PAMM significantly outperforms the state-of-the-art baseline methods including SVM-DIV, structural SVM, and R-LTR.",1,ad,True
506,"Future work includes theoretical analysis on the convergence, generalization error, and other properties of the PAMM algorithm, and improving the efficiency of PAMM in both offline training and online prediction.",0,,False
507,7. ACKNOWLEDGMENTS,0,,False
508,"This research work was funded by the 973 Program of China under Grants No. 2014CB340401, No. 2012CB316303, the 863 Program of China under Grants No. 2014AA015204, the National Natural Science Foundation of China under Grant No. 61232010, No. 61425016, No. 61173064, No. 61472401, No. 61203298, and No. 61202214.",0,,False
509,We would like to express our gratitude to Prof. Chengxiang Zhai who has offered us valuable suggestions in the academic studies.,1,ad,True
510,8. REFERENCES,0,,False
511,"[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of the 2th ACM WSDM, pages 5­14, 2009.",0,,False
512,"[2] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st ACM SIGIR, pages 335­336, 1998.",0,,False
513,"[3] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In Proceedings of the 18th ACM CIKM, pages 1287­1296, 2009.",0,,False
514,"[4] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM CIKM, pages 621­630, 2009.",1,ad,True
515,"[5] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of the 31st ACM SIGIR, pages 659­666, 2008.",1,Novelty,True
516,"[6] C. L. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. In Proceedings of the 2nd ICTIR, pages 188­199, 2009.",0,,False
517,"[7] M. Collins. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In EMNLP '02, pages 1­8, 2002.",0,,False
518,"[8] V. Dang and W. B. Croft. Diversity by proportionality: an election-based approach to search result diversification. In Proceedings of the 35th ACM SIGIR, pages 65­74, 2012.",0,,False
519,"[9] S. Gollapudi and A. Sharma. An axiomatic approach for result diversification. In Proceedings of the 18th WWW, pages 381­390, 2009.",0,,False
520,"[10] S. Guo and S. Sanner. Probabilistic latent maximal marginal relevance. In Proceedings of the 33rd ACM SIGIR, pages 833­834, 2010.",0,,False
521,"[11] J. He, V. Hollink, and A. de Vries. Combining implicit and explicit topic representations for result diversification. In Proceedings of the 35th ACM SIGIR, pages 851­860, 2012.",0,,False
522,"[12] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd ACM SIGIR, pages 50­57, 1999.",0,,False
523,"[13] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422­446, Oct. 2002.",0,,False
524,"[14] L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. Enhancing diversity, coverage and balance for summarization through structure learning. In Proceedings of the 18th WWW, pages 71­80, 2009.",0,,False
525,"[15] Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and J. Kandola. The perceptron algorithm with uneven margins. In Proceedings of the 19th ICML, pages 379­386, 2002.",0,,False
526,"[16] S.-S. Liang, Z.-C. Ren, and M. de Rijke. Personalized search result diversification via structured learning. In Proceedings of the 20th ACM SIGKDD, pages 751­760, 2014.",0,,False
527,"[17] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.",0,,False
528,"[18] C. D. Manning, P. Raghavan, and H. SchU¨ tze. An Introduction to Information Retrieval. Cambridge University Press, 2009.",0,,False
529,"[19] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proceedings of the 28th ACM SIGIR, pages 472­479, 2005.",0,,False
530,"[20] L. Mihalkova and R. Mooney. Learning to disambiguate search queries from short sessions. In Machine Learning and Knowledge Discovery in Databases, volume 5782, pages 111­127, 2009.",0,,False
531,"[21] F. Radlinski and S. Dumais. Improving personalized web search using result diversification. In Proceedings of the 29th ACM SIGIR, pages 691­692, 2006.",1,ad,True
532,"[22] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse rankings with multi-armed bandits. In Proceedings of the 25th ICML, pages 784­791, 2008.",1,ad,True
533,"[23] D. Rafiei, K. Bharat, and A. Shukla. Diversifying web search results. In Proceedings of the 19th WWW, pages 781­790, 2010.",0,,False
534,"[24] K. Raman, P. Shivaswamy, and T. Joachims. Online learning to diversify from implicit feedback. In Proceedings of the 18th ACM SIGKDD, pages 705­713, 2012.",0,,False
535,"[25] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proceedings of the 19th WWW, pages 881­890, 2010.",0,,False
536,"[26] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. J. Mach. Learn. Res., 6:1453­1484, Dec. 2005.",0,,False
537,"[27] J. Xu, T.-Y. Liu, M. Lu, H. Li, and M. Wei-Ying. Directly optimizing evaluation measures in learning to rank. In Proceedings of the 31th ACM SIGIR, pages 107­114, 2008.",0,,False
538,"[28] Y. Yue and T. Joachims. Predicting diverse subsets using structural svms. In Proceedings of the 25th ICML, pages 1224­1231, 2008.",0,,False
539,"[29] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th ICML, pages 1201­1208, 2009.",0,,False
540,"[30] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proceedings of the 26th ACM SIGIR, pages 10­17, 2003.",0,,False
541,"[31] Y. Zhu, Y. Lan, J. Guo, X. Cheng, and S. Niu. Learning for search result diversification. In Proceedings of the 37th ACM SIGIR, pages 293­302, 2014.",0,,False
542,122,0,,False
543,,0,,False

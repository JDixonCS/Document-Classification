,sentence,label,data,regex
0,The Effect of Expanding Relevance Judgements with Duplicates,0,,False
1,Gaurav Baruah,0,,False
2,David R. Cheriton School of Computer Science,0,,False
3,"University of Waterloo, Canada",1,ad,True
4,gbaruah@uwaterloo.ca,0,,False
5,Adam Roegiest,0,,False
6,David R. Cheriton School of Computer Science,0,,False
7,"University of Waterloo, Canada",1,ad,True
8,aroegies@uwaterloo.ca,0,,False
9,Mark D. Smucker,0,,False
10,Department of Management Sciences,0,,False
11,"University of Waterloo, Canada",1,ad,True
12,mark.smucker@uwaterloo.ca,0,,False
13,ABSTRACT,0,,False
14,"We examine the effects of expanding a judged set of sentences with their duplicates from a corpus. Including new sentences that are exact duplicates of the previously judged sentences may allow for better estimation of performance metrics and enhance the reusability of a test collection. We perform experiments in context of the Temporal Summarization Track at TREC 2013. We find that adding duplicate sentences to the judged set does not significantly affect relative system performance. However, we do find statistically significant changes in the performance of nearly half the systems that participated in the Track. We recommend adding exact duplicate sentences to the set of relevance judgements in order to obtain a more accurate estimate of system performance.",1,Track,True
15,Categories and Subject Descriptors: H.3.4 [Systems and Software Performance evaluation]: Efficiency and Effectiveness,0,,False
16,Keywords: Duplicate Detection; Evaluation; Pooling,0,,False
17,1. INTRODUCTION,1,DUC,True
18,"The Temporal Summarization Track (TST) at TREC 2013 [2], required returning information relevant to topics, from a web-scale time-ordered document stream (the TREC KBA 2013 Stream Corpus [1]). The Sequential Update Summarization (SUS) task in the TST, called for participating systems (runs), to return updates (sentences) about events (topics), with the goal that new updates should contain information that is new to the user.",1,Track,True
19,"For the SUS task, the Stream corpus is effectively a collection of documents (along with their timestamps), spanning the time period between October 2011 to January 2013, with an average of 93,037 documents per hour. Each participating system was tasked to return updates from a duration spanning 240 hours for each topic. In all, the TST received 28 runs from the participants of the SUS task. The number of updates returned in the runs varied from 110 to 2,815,808.",0,,False
20,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609534.",1,ad,True
21,"For evaluation of runs, the pool of updates judged by track assessors contains 9,113 sentences for 9 task topics.",0,,False
22,"We found that there are a very large number of duplicate sentences in the corpus. Indeed, the track organizers found duplicates amongst the sentences sampled from runs, while constructing the pool for assessment. Accordingly, they created an evaluation framework designed to accommodate for duplicates within the judged set of sentences. In effect, the identifier of a duplicate sentence (within the judged set of sentences) is mapped to the identifier of the designated original sentence (prototype). However, the duplicates of judged sentences from the corpus, were not mapped to the prototypes. The track's evaluation is also designed to omit sentences that are not in the judged set. Such omission is consistent with similar approaches [7, 3], and works very well for evaluating relative system performance (Section 3.1).",0,,False
23,"Including exact duplicates of judged sentences from the corpus may have ramifications for a system's evaluation as well as for the re-usability of the TST test collection. For example, if sentence s is an exact duplicate of a sentence p in the judged set, a system would neither be rewarded nor penalized for returning s, since s is not in the judged set of sentences. This may lead to an unfair evaluation for the system.",1,ad,True
24,"In this work we investigate the effect of expanding the set of judged sentences with their exact duplicates from the corpus, where, an exact duplicate is a sentence that exactly matches the text of the judged sentence for each character in the sentence string. We find that:",0,,False
25,· There exist an extremely large number of exact duplicate sentences in the corpus (Section 2.2).,0,,False
26,"· Adding duplicates to the judged set of sentences, does not change relative ranking of systems with respect to Kendall's  (Section 3.1).",0,,False
27,"· System performance is affected when the judged set of sentences is expanded with duplicates (Section 3.2), with 13 of 28 submitted runs showing statistically significant changes for a paired t-test with p-value  0.05.",0,,False
28,2. OBSERVATIONS ON DUPLICATES,0,,False
29,We briefly describe the original set of judged sentences of the SUS task and set a context for subsequent sections.,0,,False
30,2.1 Original Judged Sentences,0,,False
31,"Each topic in the SUS task corresponds to an event (of type earthquake, storm, accident, bombing or shooting) that",0,,False
32,1159,0,,False
33,"occurred within the corpus duration. The query for the topic included query terms, and query start and end times. The query was considered active for the time interval between query start and end times (the query duration, typically 240 hours).",0,,False
34,"The assessors initially identified a set of nuggets (short snippets of text containing information relevant to the topic), from the edit history of the Wikipedia1 article for an event. A pool of sentences for evaluation was created by sampling 60 updates per topic per submitted run with highest confidence scores (a confidence score was required for each update in a run). Some close duplicates were identified from this initial sample which allowed more updates to be included in the pool [2]. This created a pool of sentences totalling 9,113 for 9 topics. A pooled sentence was matched against nuggets and was considered relevant if it contained a nugget.",1,Wiki,True
35,2.2 Expanded Set of Judged Sentences,0,,False
36,"Given the original set of judged sentences, for each topic, for each sentence in the judged set, we found the exact duplicates within the query duration of the topic, from the Stream corpus. For each duplicate found, we added it to the original judged set and mapped its identifier to the identifier of its prototype. Table 1 lists the number of judged sentences, the number of duplicates known to exist within the original judged set, the number of sentences for which the duplicates were found within the query duration. Also shown are the number of relevant sentences in the original judged set for each topic, and the number of relevant sentences for which duplicates were found in the query duration from the corpus.",1,ad,True
37,"We see that the original judged set expands nearly a 1000 times from 9,113 to 9,034,179, when duplicates are added. However, the number of relevant duplicates found is extremely less at 97,256 (about 10 times the size of the original set). Table 2 shows the top occurring duplicate sentences. In fact, the top 3 most occurring duplicate sentences account for 67% of the total duplicates found. We observed that most of the duplicates that occur with high frequency are duplicates of non relevant sentences. They appear to be boilerplate sentences found in web-site navigation menus or at the end of news articles. In contrast the most frequent relevant duplicate, ""National Hurricane Center in Miami said Isaac became a Category 1 hurricane Tuesday with winds of 75 mph."", was found to occur just 5,403 times, in the query duration for topic 5 (""Hurricane Isaac"").",1,ad,True
38,"We feel that the high number of duplicates found overall could be because of news/web syndication services. News wire documents form the second largest component of documents in the Stream corpus [1], with social documents (blogs and forums) forming the bulk of the corpus, and documents sourced from links submitted to bitly.com forming the remainder. One would expect a high number of news/web articles to be generated after the occurrence of a catastrophic event.",1,blog,True
39,3. EFFECT OF ADDING DUPLICATES TO THE ORIGINAL SET OF JUDGEMENTS,0,,False
40,The TST introduces new measures to evaluate temporal summarization. The measures are analogous to precision/gain and recall and they are also designed to account,0,,False
41,1http://www.wikipedia.org/,1,wiki,True
42,"for latency and verbosity of updates (sentences). Participant systems are tasked to return a timestamp along with each update about an event. Latency discounts are applied to sentences that are returned later than the first known occurrence of the nugget of information that they contain. The nuggets were identified, and their time of first occurrence noted, by the track's assessors, using the edit histories of the Wikipedia articles for the topics (Section 2.1). Verbosity discounts are applied based on the length of the returned sentences. Longer sentences are penalized more than shorter sentences by the verbosity discount, which essentially forms an aspect of ""user friendliness"" for a system.",1,Wiki,True
43,"The track introduces two precision-like measures, Expected Latency Gain (E[LG]) and Expected Gain (E[G]), as well as two recall-like measures, Latency Comprehensiveness (LC) and Comprehensiveness (C). The E[LG] and E[G] measures use the relevance score (0/1 for binary relevance) as a measure of gain and potentially discount the gain for updatelatency. The C and LC measures attempt to capture how well a system performed at returning all identified nuggets and how quickly it emitted updates containing these nuggets from the canonical time of first occurrence for the nuggets.",0,,False
44,"The track coordinators consider E[LG] and LC to be the official metrics of the track and we report our analyses for these metrics only. Detailed descriptions of all metrics can be found in the 2013 Temporal Summarization Track Overiew [2]. We note here that in the track's evaluation framework, a system returning sentences not present in the judged set of sentences is neither penalized nor rewarded. On the other hand, returning duplicate sentences present in the judged set will result in the verbosity discount being applied for each.",1,Track,True
45,3.1 Effect on Systems' Ranking,0,,False
46,"Table 3 provides Kendall's  correlation between the rankings given by the original judged and expanded set for each of the four task measures. We can see that the correlation tends to be high indicating that relative performance is typically maintained regardless of assessment pool. This is consistent with other works conducting similar research [7, 10, 11, 3, 9]. Accordingly, we can see that not including all exact duplicates in the judged sentences was a reasonable and effective method of relative system performance evaluation. However, there may still be benefit to expanding the judged set because there are changes to the absolute performance scores of the systems (Section 3.2) which may be indicative of a change in the user experience.",0,,False
47,3.2 Effect on System Performance,0,,False
48,"Overall, 13 submitted runs showed a statistically significant change in E[LG] and 12 submitted runs showed a statistically significant change in LC, for a paired t-test with p-value  0.05. The average difference across topics (and standard deviation) between the original judged set and the expanded set are presented in Tables 4 and 5 for E[LG] and LC, respectively. Runs for which there was no difference in score are not listed in the tables. Runs are listed in sorted order based upon the run names.",0,,False
49,"There exist several runs with no E[LG] change which is primarily due to the fact that they do not contain any newly identified duplicates and so would not be penalized for them. The majority of the the other runs, do experience a general decrease in the performance with respect to the expanded",0,,False
50,1160,0,,False
51,Topic,0,,False
52,1 2 3 4 5 6 8 9 10 Total,0,,False
53,#sentences,0,,False
54,779 912 762 1463 1069 1517 1128 873 610 9113,0,,False
55,Original Judged Set of Sentences,0,,False
56,#known #with duplicates #relevant,0,,False
57,duplicates found in corpus sentences,0,,False
58,100,0,,False
59,309,0,,False
60,431,0,,False
61,180,0,,False
62,474,0,,False
63,381,0,,False
64,112,0,,False
65,494,0,,False
66,211,0,,False
67,276,0,,False
68,946,0,,False
69,410,0,,False
70,0,0,,False
71,689,0,,False
72,82,0,,False
73,187,0,,False
74,905,0,,False
75,493,0,,False
76,205,0,,False
77,609,0,,False
78,172,0,,False
79,172,0,,False
80,423,0,,False
81,168,0,,False
82,89,0,,False
83,338,0,,False
84,287,0,,False
85,1321,0,,False
86,5187,0,,False
87,2635,0,,False
88,#relevant with dup.s in corpus,0,,False
89,146 202 154 260 63 270 102 97 143 1437,0,,False
90,Expanded Judged Set,0,,False
91,#sentences #relevant,0,,False
92,sentences,0,,False
93,833794,0,,False
94,1445,0,,False
95,2241589,0,,False
96,6301,0,,False
97,552145,0,,False
98,25199,0,,False
99,264474,0,,False
100,22587,0,,False
101,821897,0,,False
102,17043,0,,False
103,730296,0,,False
104,18661,0,,False
105,1057643,0,,False
106,1741,0,,False
107,2430455,0,,False
108,2384,0,,False
109,101886,0,,False
110,1895,0,,False
111,9034179,0,,False
112,97256,0,,False
113,Table 1: The number of sentences in the Original judged set vs. the Expanded set,0,,False
114,Frequency 3376809 2013684 673876 529085 294662,0,,False
115,... 166557 111503,0,,False
116,... 81985,0,,False
117,"Topics 2,9,1 2,9,8",0,,False
118,3 5 8,0,,False
119,"6,9 8,9",0,,False
120,6,0,,False
121,"Duplicate Sentence All rights reserved./All rights reserved Yahoo! New User ? 3. This material may not be published, broadcast, rewritten or redistributed.",1,Yahoo,True
122,U.S. Register Sign In Help New Firefox,0,,False
123,16 Optimised for Yahoo! Notifications Help Mail My Y!,1,Yahoo,True
124,Join Here .,0,,False
125,Table 2: Examples of Duplicate Sentences with high number of occurrences across all topics,0,,False
126,Judged Sentences expanded with exact duplicates lowercase duplicates,0,,False
127,Kendall's  for Ranking Metric,0,,False
128,E[LG] E[G] LC,0,,False
129,C,0,,False
130,0.899 0.894 0.942 0.937,0,,False
131,0.899 0.894 0.942 0.937,0,,False
132,"Table 3: Rank correlation between Original Judged sentences vs Expanded set, for TST measures",0,,False
133,"set; though, the affect on such runs is not consistent across topics. Of particular note is run 8 in Table 4, which has a general increase in performance indicating that run 8 did return relevant sentences which were not in the original judged set, but were duplicates of relevant sentences from the original set. This increase was found not to be statistically significant with a p-value > 0.05 for a paired t-test. However, with more topics, we may find more statistically significant positive improvements [8].",0,,False
134,"Furthermore, we can see that the duplicate detection in the expanded set does not hurt but improves LC performance on average. This makes sense since systems may have returned relevant sentences duplicate to those in the original set of judgements. By expanding the original judged set with exact duplicates, we argue that a more accurate assessment of absolute performance is being achieved since runs are now being rewarded or penalized for new sentences which were not present in the original set.",0,,False
135,3.3 Variations on Duplicate Detection,0,,False
136,"We also tried duplicate detection with simple transformations (to ensure minimal information loss) like lowercase-ing, whitespace-collapsing (reducing sequences of whitespace to a single space) and whitelower (lowercase + whitespace). The",0,,False
137,Run,0,,False
138,µ () Run,0,,False
139,µ (),0,,False
140,1 0.0112 (0.0134) 2 0.0097 (0.0100),0,,False
141,4 0.0107 (0.0138) 8 -0.0037 (0.0089),0,,False
142,9,0,,False
143,0.0391 (0.0559) 10 0.0363 (0.0388),0,,False
144,11 0.0006 (0.0030) 12 0.0001 (0.0028),0,,False
145,13 0.0014 (0.0019) 14 0.0013 (0.0016),0,,False
146,15 0.0007 (0.0020) 16 0.0014 (0.0019),0,,False
147,18 0.0151 (0.0163) 19 0.0160 (0.0181),0,,False
148,20 0.0162 (0.0152) 21 0.0171 (0.0192),0,,False
149,22 0.0008 (0.0016) 23 0.0008 (0.0016),0,,False
150,24 0.0054 (0.0067) 25 0.0054 (0.0061),0,,False
151,26 0.0052 (0.0048) 27 0.0036 (0.0030),0,,False
152,28 0.0007 (0.0010),0,,False
153, denotes a p-value  0.05 for a paired t-test,0,,False
154,Table 4: Average Difference and Standard Deviation of E[LG] between the Original Set and the Expanded Set of judged sentences,0,,False
155,"lowercase transformation is a common normalization technique employed in search engine indexing, and may introduce errors (e.g. US, the country, vs us, the pronoun). It did increase the total number of duplicates found to 10,872,223 but found only 44 new duplicates for relevant updates. Both whitespace transformations did not produce different sets of sentences than their basis transformations.",0,,False
156,"The Kendall's  between the original judged set and the expanded set, using exact duplicates and lowercase duplicates, are identical (Table 3), due to the fact they both produced identical rankings, and in fact scores, for all systems. The lowercase transformation found additional duplicates overall but very few relevant duplicates and hence there are insignificant changes in the scores when averaged across topics and runs. With more topics and more participant systems, we might expect to see the effect of such transformations to be more pronounced.",1,ad,True
157,4. DISCUSSION,0,,False
158,"We believe that much of the negative effect of the original judged set expansion (on the gain-based measures) would be subsumed, in the majority of cases, if the verbosity penalization were applied to all sentences retrieved by a system. Currently, such penalization only occurs if a retrieved sentence is also present in the judged set. While beneficial for evaluating system performance for finding relevant sentences, ignoring unjudged sentences does not accurately reflect the user experience. One would expect a large difference in performance scores between submitting 1,000 sentences and 1",0,,False
159,1161,0,,False
160,Run,0,,False
161,µ () Run,0,,False
162,µ (),0,,False
163,1 -0.0594 (0.0446) 2 -0.0594 (0.0446),0,,False
164,4 -0.0809 (0.0571) 8,0,,False
165,-0.0138 (0.0183),0,,False
166,9 -0.0325 (0.0402) 10 -0.0408 (0.0611),0,,False
167,11 -0.1247 (0.0550) 12 -0.1324 (0.0557),0,,False
168,18 -0.0915 (0.0604) 19 -0.1045 (0.0586),0,,False
169,20 -0.0669 (0.0551) 21 -0.0734 (0.0526),0,,False
170,24 -0.0192 (0.0247) 25 -0.0297 (0.0328),0,,False
171,26 -0.0038 (0.0061),0,,False
172, denotes a p-value  0.05 for a paired t-test,0,,False
173,Table 5: Average Difference and Standard Deviation of LC between the Original Set and the Expanded Set of judged sentences,0,,False
174,"million sentences; a difference which has the potential to overwhelm the user. The official set of judgements averages around 1,000 sentences per topic which results in an upper limit on verbosity penalization.",0,,False
175,"However, the current evaluation methology is consistent with that of [7], who found that removing unjudged documents from evaluation of ranked lists works well and does not affect the relative system performance. Indeed, the new TST metrics are stable for measuring relative system performance, even after processing an expanded set of judgements that is 1000 times the size of the original.",0,,False
176,"Nugget-based evaluation [4, 6] - where identified relevant material is representative of relevance - is aimed towards automatic identification of nuggets in the whole collection. However, tracking duplicates of the retrievable unit (e.g. documents, sentences) may be useful depending on the evaluation metrics for the specific task at hand (such as Temporal Summarization).",0,,False
177,5. FUTURE WORKS,0,,False
178,"As an immediate future work, we plan to investigate the effect of including unjudged updates for verbosity discounts. Accounting for unjudged sentences in runs may not be a straightforward task due to the potential quantity of them. A simple but potentially inefficient mechanism would be to store information (e.g. length, timestamp, duplicates) about every sentence in the corpus which would facilitate applying the necessary discount. Alternatively, it may be possible to produce an estimate of unjudged sentence lengths. This may require the use of some form of sampling and the use of inclusion probabilities (e.g. [5]). Determining a reasonable method for applying verbosity penalization to unjudged sentences is an area of research that we intend to pursue.",0,,False
179,"The TST at TREC 2013 had only 9 topics. As per [8, 12], even though there are statistically significant changes in systems' performance, we cannot currently presume that the effects would be reproducible for a different/larger set of topics. We definitely need to test for the effect of duplicates on more number of topics.",1,TREC,True
180,6. CONCLUSIONS,0,,False
181,"We experimented with expanding the set of judged sentences with exact duplicates from the corpus and investigated its effects on the evaluation of temporal summarization. We found that adding exact duplicate sentences to the set of relevance judgements, does not affect relative ordering of temporal summarization systems. It does however,",1,ad,True
182,"induce a change in the performance scores of the systems. 13 out of 28 systems that participated in the Temporal Summarization Track at TREC 2013 experienced a statistically significant change in performance scores with respect to the track's metrics. With more topics from the TREC 2014 version of the track, we expect to get a more accurate estimate of changes in performance when evaluating with a large number of duplicates. Expansion of relevance judgements with exact duplicates is simple and not only does it help produce more accurate performance scores but also potentially aids in reusability of the test collection for the development of new temporal summarization systems.",1,Track,True
183,7. ACKNOWLEDGMENTS,0,,False
184,"We thank Rakesh Guttikonda for helpful discussions and ideas regarding this work. This work was made possible by the facilities of SHARCNET (www.sharcnet.ca) and Compute/Calcul Canada, and was supported in part by NSERC, and in part by the Google Founders Grant, and in part by the University of Waterloo.",1,ad,True
185,8. REFERENCES,0,,False
186,[1] KBA Stream Corpus 2013. http: //trec-kba.org/kba-stream-corpus-2013.shtml.,1,trec,True
187,"[2] J. Aslam, F. Diaz, M. Ekstrand-Abueg, V. Pavlu, and T. Sakai. TREC 2013 Temporal Summarization. In TREC, 2013.",1,TREC,True
188,"[3] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In SIGIR, pages 25­32, 2004.",0,,False
189,"[4] G. Marton and A. Radul. Nuggeteer: Automatic nugget-based evaluation using descriptions and judgements. In HLT-NAACL, pages 375­382, 2006.",1,ad,True
190,"[5] V. Pavlu and J. Aslam. A practical sampling strategy for efficient retrieval evaluation, Technical Report, College of Computer and Information Science, Northeastern University. 2007.",0,,False
191,"[6] V. Pavlu, S. Rajput, P. B. Golbus, and J. A. Aslam. IR system evaluation using nugget-based test collections. In WSDM, pages 393­402, 2012.",0,,False
192,"[7] T. Sakai and N. Kando. On information retrieval metrics designed for evaluation with incomplete relevance assessments. Information Retrieval, 11(5):447­470, 2008.",0,,False
193,"[8] M. Sanderson and J. Zobel. Information retrieval system evaluation: effort, sensitivity, and reliability. In SIGIR, pages 162­169, 2005.",0,,False
194,"[9] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In SIGIR, pages 66­73, 2001.",0,,False
195,"[10] A. Trotman and D. Jenkinson. IR evaluation using multiple assessors per topic. ADCS, 2007.",0,,False
196,"[11] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In SIGIR, pages 315­323, 1998.",0,,False
197,"[12] E. M. Voorhees and C. Buckley. The effect of topic set size on retrieval experiment error. In SIGIR, pages 316­323, 2002.",0,,False
198,1162,0,,False
199,,0,,False

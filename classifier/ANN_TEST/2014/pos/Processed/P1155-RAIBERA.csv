,sentence,label,data,regex
0,The Correlation between Cluster Hypothesis Tests and the Effectiveness of Cluster-Based Retrieval,0,,False
1,Fiana Raiber fiana@tx.technion.ac.il,0,,False
2,Oren Kurland kurland@ie.technion.ac.il,0,,False
3,"Faculty of Industrial Engineering and Management, Technion Haifa 32000, Israel",0,,False
4,ABSTRACT,0,,False
5,"We present a study of the correlation between the extent to which the cluster hypothesis holds, as measured by various tests, and the relative effectiveness of cluster-based retrieval with respect to document-based retrieval. We show that the correlation can be affected by several factors, such as the size of the result list of the most highly ranked documents that is analyzed. We further show that some cluster hypothesis tests are often negatively correlated with one another. Moreover, in several settings, some of the tests are also negatively correlated with the relative effectiveness of cluster-based retrieval.",0,,False
6,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,0,,False
7,"Keywords: cluster hypothesis, cluster-based retrieval",0,,False
8,1. INTRODUCTION,1,DUC,True
9,"The cluster hypothesis states that ""closely associated documents tend to be relevant to the same requests"" [19]. The hypothesis plays a central role in information retrieval. Various tests were devised for estimating the extent to which the hypothesis holds [5, 20, 3, 17]. Furthermore, inspired by the hypothesis, document retrieval methods that utilize document clusters were proposed (e.g., [10, 11, 6, 7, 15]).",0,,False
10,"There are, however, only a few reports regarding the correlation between the cluster hypothesis tests and the relative effectiveness of cluster-based retrieval with respect to document-based retrieval [20, 3, 13]. Some of these are contradictory: while it was initially argued that Voorhees' nearest neighbor cluster hypothesis test is not correlated with retrieval effectiveness [20], it was later shown that this test is actually a good indicator for the effectiveness of a specific cluster-based retrieval method [13].",1,ad,True
11,"The aforementioned reports focused on a single cluster hypothesis test (the nearest neighbor test), used a specific retrieval method which is not state-of-the-art and were evaluated using small documents collections which were mostly composed of news articles. Here, we analyze the correla-",0,,False
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609533.",1,ad,True
13,"tion between cluster hypothesis tests and the relative effectiveness of cluster-based retrieval with respect to documentbased retrieval using a variety of tests, state-of-the-art retrieval methods and collections.",0,,False
14,"We found that (i) in contrast to some previously reported results [3], cluster hypothesis tests are in many cases either negatively correlated with one another or not correlated at all; (ii) cluster hypothesis tests are often negatively correlated or not correlated at all with the relative effectiveness of cluster-based retrieval methods; (iii) the correlation between the tests and the relative effectiveness of the retrieval methods is affected by the number of documents in the result list of top-retrieved documents that is analyzed; and, (iv) the type of the collection (i.e., Web vs. newswire) is a strong indicator for the effectiveness of cluster-based retrieval when applied over short retrieved document lists.",0,,False
15,2. RELATED WORK,0,,False
16,"The correlation between cluster hypothesis tests was studied using small document collections, most of which were composed of news articles [3]. We, on the other hand, use a variety of both (small scale) newswire and (large scale) Web collections. The correlation between cluster hypothesis tests and the effectiveness of cluster-based retrieval methods was studied using only a single test -- Voorhees' nearest neighbor test [20, 13]. Each study also focused on a different cluster-based retrieval method. This resulted in contradictory findings. In contrast, we use several cluster hypothesis tests and retrieval methods.",1,ad,True
17,"Document clusters can be created either in a query dependent manner, i.e., from the list of documents most highly ranked in response to a query [21] or in a query independent fashion from all the documents in a collection [5, 10]. In this paper we study the correlation between cluster hypothesis tests and the effectiveness of retrieval methods that utilize query dependent clusters [6, 7, 15]. The reason is threefold. First, these retrieval methods were shown to be highly effective. Second, we use for experiments large-scale document collections; clustering all the documents in these collections is computationally difficult. Third, the cluster hypothesis was shown to hold to a (much) larger extent when applied to relatively short retrieved lists than to longer ones or even to the entire corpus [18].",0,,False
18,3. CLUSTER HYPOTHESIS TESTS AND,0,,False
19,CLUSTER-BASED RETRIEVAL,0,,False
20,To study the correlation between tests measuring the extent to which the cluster hypothesis holds and the effective-,0,,False
21,1155,0,,False
22,"ness of cluster-based retrieval methods, we use several tests and (state-of-the-art) retrieval methods.",0,,False
23,"Let Dinit be an initial list of n documents retrieved in response to query q using some retrieval method. The retrieval method scores document d by score(q, d). (Details of the scoring function used in our experiments are provided in Section 4.) All the cluster hypothesis tests and the retrieval methods that we consider operate on the documents in Dinit. In what follows we provide a short description of these tests and methods.",0,,False
24,Cluster hypothesis tests. The first test that we study con-,0,,False
25,ceptually represents the Overlap test [5]. The test is based,0,,False
26,"on the premise that, on average, the similarity between two",0,,False
27,relevant documents should be higher than the similarity be-,0,,False
28,"tween a relevant and a non-relevant document. Formally,",0,,False
29,let R(Dinit) be the set of relevant documents in Dinit and,0,,False
30,N (Dinit) the set of non-relevant documents; nR and nN de-,0,,False
31,"note the number of documents in R(Dinit) and N (Dinit), re-",0,,False
32,spectively. The score assigned by the Overlap test to Dinit is,0,,False
33,1,0,,False
34,P,0,,False
35,"( ) nR(nR-1) di,dj R(Dinit),di,dj",0,,False
36,"sim(di,dj )+sim(dj ,di)",0,,False
37,"; sim(·, ·) is 1 ( ) nRnN",0,,False
38,"P di R(Dinit),dj N (Dinit)",0,,False
39,"sim(di,dj )+sim(dj ,di)",0,,False
40,an inter-text similarity measure described in Section 4.1 This,0,,False
41,score is averaged over all the tested queries for which nR and,0,,False
42,nN are greater than 1 to produce the final test score.,0,,False
43,We next consider Voorhees' Nearest Neighbor test (NN),0,,False
44,[20]. For each relevant document di ( R(Dinit)) we count the number of relevant documents among di's k - 1 nearest neighbors in Dinit; k is a free parameter. These counts are,0,,False
45,averaged over all the relevant documents retrieved for all the,0,,False
46,tested queries. The nearest neighbors of di are determined,0,,False
47,"based on sim(di, dj).",0,,False
48,The Density test [3] is defined here as the ratio between,0,,False
49,the average number of unique terms in the documents in,0,,False
50,Dinit and the number of terms in the vocabulary. The un-,0,,False
51,"derlying assumption is, as for the tests from above, that rel-",0,,False
52,evant documents are more similar to each other than they,0,,False
53,"are to non-relevant documents. Now, if the number of terms",0,,False
54,"that are shared by documents in the initial list is high, then",0,,False
55,presumably relevant documents could be more easily distin-,0,,False
56,guished from non-relevant ones.,0,,False
57,We also explore the Normalized Mean Reciprocal Distance,0,,False
58,test (nMRD) [17]. The test is based on using a complete,0,,False
59,relevant documents graph. Each vertex in the graph rep-,0,,False
60,resents a different document in R(Dinit); each pair of ver-,0,,False
61,tices is connected with an edge. The edge weight repre-,0,,False
62,sents the distance between the documents. The distance,0,,False
63,"between documents di and dj is defined as the rank of dj in a ranking of all the documents d  Dinit (d ,"" di) that is created using sim(di, d); the rank of the highest""",0,,False
64,ranked document is 1. The score assigned by the nMRD test,0,,False
65,to D is P ; init,0,,False
66,1,0,,False
67,nR,0,,False
68,"PnR i,1",0,,False
69,1 log2 i+1,0,,False
70,"1 di,dj R(Dinit),di,""dj spd(di,dj )""",0,,False
71,"spd(di, dj) is the shortest path distance between di and dj",0,,False
72,in the graph. This score is averaged over all tested queries,0,,False
73,for which nR > 1 to produce the final nMRD score.,0,,False
74,Cluster-based document retrieval methods. Let C l(Dinit),0,,False
75,be the set of clusters created from the documents in Dinit using some clustering algorithm. All the cluster-based re-,0,,False
76,"1We use both sim(di, dj) and sim(dj, di) as the similarity measure that was used for experiments is asymmetric. Further details are provided in Section 4.",0,,False
77,trieval methods that we consider re-rank the documents in,0,,False
78,Dinit using information induced from clusters in C l(Dinit).,0,,False
79,The interpolation-f method (Interpf in short) [6] directly,0,,False
80,ranks the documents in Dinit. The score assigned to docu-,0,,False
81,ment,0,,False
82,d,0,,False
83,(,0,,False
84,Dinit ),0,,False
85,is,0,,False
86," score(q,d)",0,,False
87,P di Dinit,0,,False
88,"score(q,di )",0,,False
89,+,0,,False
90,(1,0,,False
91,-,0,,False
92,),0,,False
93,; P cC,0,,False
94,l(Dinit ),0,,False
95,"sim(q,c)sim(c,d)",0,,False
96,P di Dinit,0,,False
97,P cC l(Dinit),0,,False
98,"sim(q,c)sim(c,di )",0,,False
99,is,0,,False
100,a,0,,False
101,free,0,,False
102,parameter.,0,,False
103,The cluster-based retrieval methods that we consider next,0,,False
104,"are based on a two steps procedure. First, the clusters in",0,,False
105,C l(Dinit) are ranked based on their presumed relevance to,0,,False
106,"the query. Then, the ranking of clusters is transformed to a",0,,False
107,ranking over the documents in Dinit by replacing each cluster,0,,False
108,with its constituent documents (and omitting repeats).,0,,False
109,"The AMean and GMean methods [12, 16] rank the clus-",0,,False
110,ters based on the arithmetic and geometric mean of the orig-,0,,False
111,"inal retrieval scores of the documents in a cluster, respec-",0,,False
112,"tively. Specifically, AMean assigns cluster c with the score",0,,False
113,1 |c|,0,,False
114,P,0,,False
115,dc,0,,False
116,"score(q,",0,,False
117,d),0,,False
118,where,0,,False
119,|c|,0,,False
120,is,0,,False
121,the,0,,False
122,number,0,,False
123,of,0,,False
124,documents,0,,False
125,in,0,,False
126,1,0,,False
127,c.,0,,False
128,The,0,,False
129,score,0,,False
130,assigned,0,,False
131,to,0,,False
132,c,0,,False
133,by,0,,False
134,GMean,0,,False
135,is,0,,False
136,Q,0,,False
137,dc,0,,False
138,"score(q,",0,,False
139,d),0,,False
140,|c|,0,,False
141,.,0,,False
142,Another cluster ranking method that we use is Clus-,0,,False
143,tRanker [7]. ClustRanker assigns cluster c with the score,0,,False
144," + cent(c)sim(q,c)",0,,False
145,P ci C,0,,False
146,l(Dinit ),0,,False
147,"cent(ci )sim(q,ci )",0,,False
148,(1-),0,,False
149,P dc,0,,False
150,"score(q,d)sim(c,d)cent(d)",0,,False
151,P ci C l(Dinit),0,,False
152,P dci,0,,False
153,"score(q,d)sim(ci ,d)cent(d)",0,,False
154,;,0,,False
155,cent(d),0,,False
156,and,0,,False
157,cent(c) are estimates of the centrality of a document d in,0,,False
158,"Dinit and that of a cluster c in C l(Dinit), respectively. These",0,,False
159,estimates are computed using a PageRank algorithm that,0,,False
160,"utilizes inter-document and inter-cluster similarities [9, 7].",0,,False
161,We also use the recently proposed state-of-the-art,0,,False
162,ClustMRF cluster ranking method [15]. ClustMRF uses,0,,False
163,Markov Random Fields which enable to integrate various,0,,False
164,types of cluster-relevance evidence.,0,,False
165,4. EXPERIMENTAL SETUP,0,,False
166,"Experiments were conducted using the datasets specified in Table 1. WSJ, AP and ROBUST are small (mainly) newswire collections. WT10G is a small Web collection and GOV2 is a crawl of the .gov domain. CW09B is the Category B of the ClueWeb09 collection and CW09A is its Category A English part. We use two additional settings, CW09BF and CW09AF, for categories B and A [2], respectively. These settings are created by filtering out from the initial ranking documents that were assigned with a score below 50 and 70 by Waterloo's spam classifier for CW09B and CW09A, respectively. Thus, the initial lists, Dinit, used for these two settings presumably contain fewer spam documents.",1,AP,True
167,corpus,0,,False
168,# of docs # of unique terms data,0,,False
169,queries,0,,False
170,WSJ AP,1,AP,True
171,"173,252 242,918",0,,False
172,"ROBUST 528,155",0,,False
173,WT10G GOV2 CW09B CW09BF CW09A CW09AF,1,WT,True
174,"1,692,096 25,205,179 50,220,423",0,,False
175,"503,903,810",0,,False
176,"186,689 259,501 663,700 4,999,228 39,251,404 87,262,413",0,,False
177,"507,500,897",0,,False
178,Disks 1-2 Disks 1-3,0,,False
179,Disks 4-5 (-CR),0,,False
180,WT10g GOV2,1,WT,True
181,"151-200 51-150 301-450, 600-700 451-550 701-850",0,,False
182,ClueWeb09 Cat. B 1-200,1,ClueWeb,True
183,ClueWeb09 Cat. A 1-200,1,ClueWeb,True
184,Table 1: TREC data used for experiments.,1,TREC,True
185,1156,0,,False
186,The Indri toolkit was used for experiments2. Titles of,0,,False
187,topics served for queries. We applied Krovetz stemming to,0,,False
188,documents and queries. Stopwords were removed only from,0,,False
189,queries using INQUERY's list [1].,0,,False
190,We use the nearest neighbor clustering algorithm to cre-,0,,False
191,"ate the set of clusters C l(Dinit) [4]. A cluster is created from each document di  Dinit. The cluster contains di and the k - 1 documents dj  Dinit (dj , di) with the high-",0,,False
192,"est sim(di, dj). We set k , 5. Recall that k is also the",0,,False
193,number of nearest neighbors in the NN cluster hypothesis,0,,False
194,test. Using such small overlapping clusters was shown to be,0,,False
195,"highly effective, with respect to other clustering schemes, for",0,,False
196,"cluster-based retrieval [4, 11, 7, 14, 15].",0,,False
197,"The similarity between texts x and y, sim(x, y), is defined",0,,False
198,as,0,,False
199," exp -CE

pDx ir[0](·)",0,,False
200,pDy ir[µ],0,,False
201, (·) ;,0,,False
202,CE,0,,False
203,is,0,,False
204,the,0,,False
205,cross,0,,False
206,en-,0,,False
207,tropy measure and pDz ir[µ](·) is the Dirichlet-smoothed (with,0,,False
208,the smoothing parameter µ) unigram language model in-,0,,False
209,"duced from text z [8]. We set µ , 1000 in our experi-",0,,False
210,ments [22]. This similarity measure was found to be highly,0,,False
211,"effective, specifically for measuring inter-document similar-",0,,False
212,"ities, with respect to other measures [9]. The measure is",0,,False
213,"used to create Dinit -- i.e., score(q, d) d,""ef sim(q, d) --3 and""",0,,False
214,"to compute similarities between the query, documents and",0,,False
215,clusters. We represent a cluster by the concatenation of its,0,,False
216,"constituent documents [10, 6, 8, 7]. Since we use unigram",0,,False
217,language models the similarity measure is not affected by,0,,False
218,the concatenation order.,0,,False
219,To study the correlation between two cluster hypothesis,0,,False
220,"tests, we rank the nine experimental settings (WSJ, AP, RO-",1,AP,True
221,"BUST, WT10G, GOV2 and the ClueWeb09 settings) based",1,WT,True
222,on the score assigned to them by each of the tests. Kendall's-,0,,False
223, correlation between the rankings of experimental settings,0,,False
224,is the estimate for the correlation between the tests. We note,0,,False
225,that Kendall's- is a rank correlation measure that does not,0,,False
226,depend on the actual scores assigned to the settings by the,0,,False
227,tests. Kendall's- ranges from -1 to +1 where -1 represents,0,,False
228,"perfect negative correlation, +1 represents perfect positive",0,,False
229,"correlation, and 0 means no correlation.",0,,False
230,The correlation between a cluster hypothesis test and the,0,,False
231,relative effectiveness of a cluster-based retrieval method is,0,,False
232,also measured using Kendall's- . The experimental settings,0,,False
233,are ranked with respect to a cluster-based retrieval method,0,,False
234,by the performance improvement it posts over the original,0,,False
235,"document-based ranking. Specifically, the ratio between the",0,,False
236,Mean Average Precision at cutoff n (MAP@n) of the rank-,1,MAP,True
237,ing induced by the method and the MAP@n of the initial,1,MAP,True
238,ranking is used; n is the number of documents in Dinit.,0,,False
239,"The free-parameter values of Interpf, ClustRanker and",0,,False
240,ClustMRF were set using 10-fold cross validation. Query,1,Query,True
241,IDs were used to create the folds; MAP@n served for the,1,MAP,True
242,optimization criterion in the learning phase. The value of,0,,False
243, which is used in Interpf and ClustRanker is selected from,0,,False
244,"{0, 0.1, . . . , 1}. To compute the document and cluster cen-",0,,False
245,"trality estimates in ClustRanker, the dumping factor and the",0,,False
246,number of nearest neighbors that are used in the PageRank,0,,False
247,"algorithm were selected from {0.1, 0.2, . . . , 0.9} and {5, 10, 20,",0,,False
248,"30, 40, 50}, respectively. The implementation of ClustMRF",0,,False
249,follows that in [15].,0,,False
250,"2www.lemurproject.org/indri 3Thus, the initial ranking is induced by a standard languagemodel-based approach.",0,,False
251,"n , 50 n , 100 n , 250 n , 500",0,,False
252,Overlap NN Density nMRD,0,,False
253,Overlap NN Density nMRD,0,,False
254,Overlap NN Density nMRD,0,,False
255,Overlap NN Density nMRD,0,,False
256,Overlap,0,,False
257,1.000 -0.171 -0.778,0,,False
258,0.278,0,,False
259,1.000 -0.354 -0.833 -0.222,0,,False
260,1.000 -0.329 -0.611 -0.556,0,,False
261,1.000 -0.588 -0.778 -0.611,0,,False
262,NN,0,,False
263,-0.171 1.000 0.229,0,,False
264,-0.229,0,,False
265,-0.354 1.000 0.412 0.000,0,,False
266,-0.329 1.000 0.569 0.329,0,,False
267,-0.588 1.000 0.650 0.402,0,,False
268,Density,0,,False
269,-0.778 0.229 1.000,0,,False
270,-0.056,0,,False
271,-0.833 0.412 1.000 0.389,0,,False
272,-0.611 0.569 1.000 0.722,0,,False
273,-0.778 0.650 1.000 0.722,0,,False
274,nMRD,0,,False
275,0.278 -0.229 -0.056,0,,False
276,1.000,0,,False
277,-0.222 0.000 0.389 1.000,0,,False
278,-0.556 0.329 0.722 1.000,0,,False
279,-0.611 0.402 0.722 1.000,0,,False
280,Table 2: The correlation between cluster hypothesis tests (measured in terms of Kendall's- ). n is the number of documents in Dinit.,0,,False
281,5. EXPERIMENTAL RESULTS,0,,False
282,"The correlations between the cluster hypothesis tests are presented in Table 2 for different values of n. With the exception of the Overlap test, we can see that the correlation between all other pairs of tests increases with increasing values of n, but can be negative or zero for low values of n. The Overlap test is negatively correlated with all the other tests across almost all values of n.",0,,False
283,"A decent positive correlation is attained between Density and NN for n  100. For n  250 a decent positive correlation is also attained between nMRD and NN. While nMRD is a global test that considers the relations between all the documents in Dinit, NN is a more local test that only considers the relations between a document and its nearest neighbors.",0,,False
284,"For n  {250, 500}, nMRD and Density are the most correlated tests. This finding is surprising since these tests are based on completely different properties of the initial list. While nMRD is based on directly measuring interdocument similarities, the Density test is based on the number of unique terms in the documents which presumably attests to the ability to differentiate between relevant and non-relevant documents.",0,,False
285,Cluster-based document retrieval methods. We next,0,,False
286,"study the correlation between the cluster hypothesis tests and the relative effectiveness of cluster-based retrieval methods. For reference, we report the correlation numbers with respect to a ranking of the experimental settings induced by the size of the corresponding collections (Size). The results are presented in Table 3.",0,,False
287,"We observe a negative correlation between the Density test and all five cluster-based retrieval methods for n ,"" 50. This finding can be explained as follows. First, the size of the collection is positively correlated with the number of terms in the vocabulary. (Refer back to Table 1.) Now, by definition, this number is negatively correlated with Density. Second, the size of the collection is positively correlated with the effectiveness of cluster-based retrieval methods as observed in Table 3 for the Size correlations for n "","" 50. We note that here, the Web collections are larger than the newswire collections and are in general noisier. Thus, we conclude that the type of the collection, i.e., Web vs. newswire, can have""",0,,False
288,1157,0,,False
289,"n , 50 n , 100 n , 250 n , 500",0,,False
290,Overlap NN,0,,False
291,Interpf AMean,0,,False
292,0.761 -0.029 0.111 0.000,0,,False
293,GMean,0,,False
294,0.333 -0.229,0,,False
295,ClustRanker 0.778 -0.057,0,,False
296,ClustMRF,0,,False
297,0.889 -0.171,0,,False
298,Interpf,0,,False
299,0.500 0.000,0,,False
300,AMean,0,,False
301,-0.222 -0.118,0,,False
302,GMean,0,,False
303,-0.333 -0.059,0,,False
304,ClustRanker 0.611 0.059,0,,False
305,ClustMRF,0,,False
306,0.722 -0.295,0,,False
307,Interpf,0,,False
308,-0.254 0.486,0,,False
309,AMean GMean ClustRanker ClustMRF,0,,False
310,-0.611 -0.333 -0.056,0,,False
311,0.500,0,,False
312,0.150 0.210 0.150 -0.210,0,,False
313,Interpf AMean GMean ClustRanker ClustMRF,0,,False
314,-0.111 -0.444 -0.444 -0.222,0,,False
315,0.389,0,,False
316,0.155 0.340 0.279 0.402 -0.155,0,,False
317,Density nMRD Size,0,,False
318,-0.648 -0.333 -0.444 -0.667 -0.778,0,,False
319,0.028 0.278 -0.056 0.167 0.167,0,,False
320,0.725 0.286 0.457 0.743 0.857,0,,False
321,-0.333 0.056 0.167,0,,False
322,-0.444 -0.556,0,,False
323,-0.167 0.556 0.333,0,,False
324,-0.278 -0.056,0,,False
325,0.400 -0.114 -0.229,0,,False
326,0.514 0.629,0,,False
327,0.254 0.333 0.389 0.000 -0.333,0,,False
328,0.111 0.444 0.444 0.333 -0.167,0,,False
329,0.028 0.500 0.556 -0.056 -0.278,0,,False
330,0.167 0.500 0.611 0.278 0.000,0,,False
331,-0.203 -0.400 -0.400,0,,False
332,0.057 0.400,0,,False
333,-0.057 -0.457 -0.457 -0.286,0,,False
334,0.229,0,,False
335,"Table 3: The correlation between cluster hypothesis tests and the relative effectiveness of cluster-based retrieval methods. The Size ""test"" ranks experimental settings by the number of documents in the collections. n is the number of documents in Dinit.",0,,False
336,an influence on the effectiveness of cluster-based retrieval methods for short retrieved lists.,0,,False
337,"Another observation that we make based on Table 3 is that for n , 50 the correlation attained for the nMRD and NN tests is often lower than that attained for Overlap and Size. The relatively high positive correlation attained for the Overlap and the Size tests for n ,"" 50 suggests that these tests are very strong indicators for the relative effectiveness of cluster-based retrieval with respect to document-based retrieval when applied to short retrieved lists. For larger values of n, NN and nMRD, as well as Density, start to post more positive correlations while the reverse holds for Overlap and Size.""",0,,False
338,"We can also see that none of the retrieval methods is correlated only positively or only negatively with all the tests for any fixed value of n. In addition, only in a few cases a test is either only positively or only negatively correlated with all the retrieval methods for a fixed value of n. Thus, we conclude that the correlation between the effectiveness of a retrieval method and a cluster hypothesis test can substantially vary (both positively and negatively) across retrieval methods and tests.",1,ad,True
339,6. CONCLUSIONS,0,,False
340,"We studied the correlation between cluster hypothesis tests and cluster-based retrieval effectiveness. We showed that the correlation between the two depends on the specific tests and methods that are used, and on the number of documents in the result list that is analyzed. We also showed that the type of the collection, i.e., Web or newswire, can be a stronger indicator for the relative effectiveness of cluster-based retrieval with respect to document-based retrieval, for short retrieved lists, than tests designed for estimating the extent to which the cluster hypothesis holds.",0,,False
341,Acknowledgments We thank the reviewers for their comments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center. This work has also been supported in part by Microsoft Research through its Ph.D. Scholarship Program.,0,,False
342,7. REFERENCES,0,,False
343,"[1] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proceedings of TREC-9, pages 551­562, 2000.",1,TREC,True
344,"[2] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Informaltiom Retrieval Journal, 14(5):441­465, 2011.",0,,False
345,"[3] A. El-Hamdouchi and P. Willett. Techniques for the measurement of clustering tendency in document retrieval systems. Journal of Information Science, 13:361­365, 1987.",0,,False
346,"[4] A. Griffiths, H. C. Luckhurst, and P. Willett. Using interdocument similarity information in document retrieval systems. Journal of the American Society for Information Science, 37(1):3­11, 1986.",0,,False
347,"[5] N. Jardine and C. J. van Rijsbergen. The use of hierarchic clustering in information retrieval. Information Storage and Retrieval, 7(5):217­240, 1971.",0,,False
348,"[6] O. Kurland. Re-ranking search results using language models of query-specific clusters. Journal of Information Retrieval, 12(4):437­460, August 2009.",0,,False
349,"[7] O. Kurland and E. Krikon. The opposite of smoothing: A language model approach to ranking query-specific document clusters. Journal of Artificial Intelligence Research, 41:367­395, 2011.",0,,False
350,"[8] O. Kurland and L. Lee. Clusters, language models, and ad hoc information retrieval. ACM Transactions on information systems, 27(3), 2009.",1,ad,True
351,"[9] O. Kurland and L. Lee. PageRank without hyperlinks: Structural reranking using links induced by language models. ACM Transactions on information systems, 28(4):18, 2010.",0,,False
352,"[10] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proceedings of SIGIR, pages 186­193, 2004.",0,,False
353,"[11] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.",0,,False
354,"[12] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proceedings of ECIR, pages 454­462, 2008.",0,,False
355,"[13] S.-H. Na, I.-S. Kang, and J.-H. Lee. Revisit of nearest neighbor test for direct evaluation of inter-document similarities. In Proceedings of ECIR, pages 674­678, 2008.",0,,False
356,"[14] F. Raiber and O. Kurland. Exploring the cluster hypothesis, and cluster-based retrieval, over the web. In Proceedings of CIKM, pages 2507­2510, 2012.",0,,False
357,"[15] F. Raiber and O. Kurland. Ranking document clusters using markov random fields. In Proceedings of SIGIR, pages 333­342, 2013.",0,,False
358,"[16] J. Seo and W. B. Croft. Geometric representations for multiple documents. In Proceedings of SIGIR, pages 251­258, 2010.",0,,False
359,"[17] M. D. Smucker and J. Allan. A new measure of the cluster hypothesis. In Proceedings of ICTIR, pages 281­288, 2009.",0,,False
360,"[18] A. Tombros, R. Villa, and C. van Rijsbergen. The effectiveness of query-specific hierarchic clustering in information retrieval. Information Processing and Management, 38(4):559­582, 2002.",0,,False
361,"[19] C. J. van Rijsbergen. Information Retrieval. Butterworths, second edition, 1979.",0,,False
362,"[20] E. M. Voorhees. The cluster hypothesis revisited. In Proceedings of SIGIR, pages 188­196, 1985.",0,,False
363,"[21] P. Willett. Query specific automatic document classification. International Forum on Information and Documentation, 10(2):28­32, 1985.",1,Query,True
364,"[22] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334­342, 2001.",1,ad,True
365,1158,0,,False
366,,0,,False

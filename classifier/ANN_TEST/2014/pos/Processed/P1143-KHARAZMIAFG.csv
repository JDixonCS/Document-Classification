,sentence,label,data,regex
0,Using Score Differences for Search Result Diversification,0,,False
1,Sadegh Kharazmi Mark Sanderson Falk Scholer,1,ad,True
2,"RMIT University & NICTA, Melbourne, Australia",0,,False
3,"{sadegh.kharazmi, mark.sanderson, falk.scholer}@rmit.edu.au",1,ad,True
4,David Vallet,0,,False
5,"NICTA, Sydney, Australia",0,,False
6,david.vallet@nicta.com.au,0,,False
7,ABSTRACT,0,,False
8,"We investigate the application of a light-weight approach to result list clustering for the purposes of diversifying search results. We introduce a novel post-retrieval approach, which is independent of external information or even the full-text content of retrieved documents; only the retrieval score of a document is used. Our experiments show that this novel approach is beneficial to effectiveness, albeit only on certain baseline systems. The fact that the method works indicates that the retrieval score is potentially exploitable in diversity.",0,,False
9,Categories and Subject Descriptors,0,,False
10,"H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval­retrieval models, search process",0,,False
11,General Terms,0,,False
12,"Algorithm, Theory, Experimentation",0,,False
13,Keywords,0,,False
14,Diversity; Score Difference; Clustering,0,,False
15,1. INTRODUCTION,1,DUC,True
16,"User queries submitted to an Information Retrieval (IR) system are often ambiguous at different levels [7]. To address such ambiguity, IR systems attempt to diversify search results, so that they cover a wide range of possible interpretations (aspects, intents or subtopics) of a query. Consequently, the number of redundant items in a search result list should be decreased, while the likelihood that a user will be satisfied with any of the displayed results should become higher.",1,ad,True
17,"In traditional IR, the estimated relevance of a document, which is used to determine the ranking of search results, depends primarily on query-document similarity. In diversified retrieval, search result rankings are based not only on querydocument similarity, but also on the other documents that",1,ad,True
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609530.",1,ad,True
19,"have been retrieved prior to the current document under consideration (i.e., document-document similarity).",0,,False
20,"Many of the proposed diversification techniques take a greedy approach, comparing a document to all previously retrieved documents, or the subtopics of a query. Also, they may use additional information, such as past user interactions, to identify which of the possible subtopics of a query are more likely to be interesting to the user. Most effective diversification approaches in the literature use techniques that focus on coverage, favoring documents that cover as many novel subtopics of a query as possible. This is in contrast to earlier techniques that focus on novelty, estimating the newness of a document with respect to those already retrieved. Novelty-based techniques usually exploit implicit information, such as differences in document content.",1,ad,True
21,"One source of implicit information derived from search results that appears to have never been investigated are differences in retrieval scores: score differences. Retrieval systems will usually, in response to a query, return a list of documents sorted by a relevance score, indicating the degree to which a query and document match. When analyzing a retrieved document list, the differences between the scores of adjacent retrieved documents differ, and this variation might be exploitable. Two documents that receive similar relevance scores are likely to share similar features; they might therefore address the same subtopics of a user's query. Conversely, two adjacent documents that have a large score difference are likely to have fewer features in common, which suggests that the documents might cover different query subtopics.",1,ad,True
22,Our research question is to ask if we can exploit score differences to help with search result diversification.,0,,False
23,"We develop a simple non-greedy diversification approach that uses differences between the scores of the initially retrieved documents. The approach is experimentally investigated using the TREC framework, comparing to baselines and state-of-the-art diversification approaches.",1,TREC,True
24,2. RELATED WORK,0,,False
25,"There are two key approaches to diversifying search results, based on explicit or implicit evidence [9]. The explicit approaches [1, 10] match retrieved documents to the subtopics of a query, which are ""pre-derived"" from external sources such as a query log or taxonomies. Implicit approaches attempt to diversify based on a representation of already-retrieved documents.",1,ad,True
26,"Maximal Marginal Relevance (MMR) [2] is perhaps the most widely-studied implicit approach. Here, a diverse set of",0,,False
27,1143,0,,False
28,"results (S) is built incrementally from an initial retrieved list (R). The results are picked from R using a greedy approach where, in each iteration, the document that is most novel is selected. Novelty in this case is defined as the mean contentbased dissimilarity between the candidate document and the already selected documents in S. A tuning parameter  defines the trade-off between relevance and diversity.",1,Novelty,True
29,"Inspired by Modern Portfolio Theory (MPT) in finance, Wang et al. introduced a new implicit approach that analyses the expected mean and variance of the return of a portfolio [11]. Facility Location Analysis (FLA) was introduced by Zuccon et.al [12] to improve the MPT approach.",0,,False
30,"Explicit approaches are focused on query subtopics, which can be derived from a pre-defined taxonomy such as the Open Directory Project1 (ODP), internal document features, query logs, or online resources [1, 8, 9]. The two most effective explicit diversification approaches are xQuAD [10] and IASelect [1].",1,ODP,True
31,All of these approaches use an iterative greedy selection approach to rank the most diverse documents.,0,,False
32,3. SCORE DIFFERENCES,0,,False
33,"We hypothesize that documents with similar features (e.g., content, aspects covered, length) will be allocated (by ranking functions) similarity scores that are close together. Conversely, documents with different features are likely to be allocated similarity scores that are further apart.",0,,False
34,"Let D1, D2, . . . , DN be an initial ranking of documents which are ordered by a ranking function s(D), and  be a difference threshold parameter. Then if",0,,False
35,|s(Di) - s(Di+1)| <  s(Di+1),0,,False
36,"we assume that the documents cover the same query subtopic. If the value is  , it indicates that the two documents belong to different subtopics.",0,,False
37,"To test our hypothesis, we set up an experiment to measure the score differences between pairs of adjacent ranked documents that either covered the same or different subtopics of a query. We used the documents, queries, and diversity relevance judgments from the TREC 2009­2011 Web Tracks [3, 4]. Documents were ranked using the Dirichletsmoothed language model from the Indri IR system2 with default parameter settings. The query subtopics covered by individual answer documents are defined in the TREC relevance judgments. All non-relevant documents were assigned to an extra ""non-relevant"" subtopic. In total, 148 topics (TREC Web Track 2009, 2010 and 2011 queries) were used and for each topic, and pairs of documents in the top 100 positions in a ranked list were examined.",1,ad,True
38,"We tested the Language Modeling (LM) and Okapi BM25 ranking functions, which are widely used in retrieval research, and have been shown to be effective ranking functions [6]. First, using language modelling as the ranking function to score documents, our analysis shows that for pairs of documents where there is no change in subtopic, the mean measured score difference is 0.065. Conversely, when the subtopic changes, the mean difference in scores is 0.073. Although the differences are small, a pairwise permutation test indicates that they are statistically significant (p",1,LM,True
39,"1http://www.dmoz.org/ 2Version 5.2, http://lemurproject.org/indri.php",0,,False
40,< 0.01). This analysis suggests that score differences have the potential to be used as a technique to help with result diversification.,0,,False
41,"We repeated the same experiment using BM25. The mean measured score difference when there is no change in subtopic is 0.069, versus 0.071 when the subtopic changes. A pairwise permutation test indicates that these differences are not statistically significant. We therefore hypothesise that score differences are less likely to work well when the BM25 ranking function is used as a base run.",0,,False
42,4. DIVERSIFYING RESULTS USING SCORE DIFFERENCES,0,,False
43,"To apply the score differences technique for result diversification, first, the score difference between each pair of documents, starting at rank position 1, was calculated. The top 100 documents were then re-ranked by decreasing size of the score difference between each document and the document above it. The documents with the biggest difference between the paired documents would now be top ranked, and they should be documents covering different subtopics.",0,,False
44,"After some experimentation with this simple approach, we found that it was better to re-rank documents based on a linear combination of the rank positions in the initial ranking and score differences, as shown in Algorithm 1. This approach to diversification, RankScoreDiff, does not use any information apart from the similarity scores from an initial retrieval run. It is therefore an implicit diversification approach.",0,,False
45,Algorithm 1 RankScoreDiff(L),0,,False
46,L  ScoreDiff(L),0,,False
47,for 1 < i  |L| do,0,,False
48,Score(L[i]),0,,False
49,1 Rank(L[i]),0,,False
50,+,0,,False
51,1 Rank(L,0,,False
52,[i]),0,,False
53,end for,0,,False
54,Sort L on Score(L[i]),0,,False
55,5. EXPERIMENTAL SETUP,0,,False
56,"We investigated the effectiveness of RankScoreDiff as a diversification approach using the diversity task framework of the TREC Web Track from 2009­2011, which comprises 148 queries. Results are reported over the Clueweb category B collection. The Clueweb Online Services3 were used to retrieve the top 100 documents for each query, using the same ranking functions and version of Indri described in Section 3. The top 100 documents were diversified using the methods under test.",1,TREC,True
57,"Effectiveness was measured with -nDCG, a widely-used metric that incorporates both relevance and diversity into a single score. The parameter , which sets the relative importance of these two evaluation considerations, was set to 0.5, as recommended by the creators of the measure [5]. In the subsequent presentation of results, two-tailed paired t-tests are used to evaluate statistical significance.",1,corpora,True
58,5.1 Implementation and Tuning,0,,False
59,"For diversity approaches that require explicitly defined subtopics as an input (xQuAD and IASelect), two sources of",0,,False
60,3http://boston.lti.cs.cmu.edu/Services/,0,,False
61,1144,0,,False
62,Runs Initial Run (LM) MMR MPT FLA-MPT RankScoreDiff xQuAD RankScoreDiff + xQuAD IASelect RankScoreDiff + IASelect,1,LM,True
63,-nDCG@5,0,,False
64,0.235 0.233 0.235 0.240 0.246 * 0.246  0.258 * ,0,,False
65,0.266 0.283 *  ,0,,False
66,ODP Subtopics -nDCG@10,1,ODP,True
67,0.276 0.274 0.277 0.280,0,,False
68,0.274 0.286  0.291 ,0,,False
69,0.298 0.315 *  ,0,,False
70,-nDCG@20 0.315 0.317 0.316 0.320 0.324 * 0.326  0.332 0.337 0.347 *,0,,False
71,-nDCG@5,0,,False
72,0.235 0.233 0.232 0.240 0.246 * 0.318 **  0.318 **  0.321 **  0.323 ** ,0,,False
73,TREC Subtopics,1,TREC,True
74,-nDCG@10 -nDCG@20,0,,False
75,0.276,0,,False
76,0.315,0,,False
77,0.274,0,,False
78,0.317,0,,False
79,0.277,0,,False
80,0.319,0,,False
81,0.280,0,,False
82,0.320,0,,False
83,0.274 0.357 **  0.358 **  0.365 **  0.367 ** ,0,,False
84,0.324 * 0.396 ** 0.397 **  0.400 ** 0.405 **,0,,False
85,"Table 2: Effectiveness of diversification approach using language modeling as baseline. For approaches that need explicit representation of subtopics, TREC official subtopics and ODP subtopics were used.",1,TREC,True
86,Implicit Explicit,0,,False
87,Method,0,,False
88,MMR MPT F LA - M P T,0,,False
89,xQuADODP I AS electODP xQuADT rec IASelectT rec,1,ODP,True
90,Spearman's  0.92 0.86 0.83 0.78 0.75 0.67 0.71,0,,False
91,Table 1: The Spearman correlation of RankScoreDiff with other diversification approaches in terms of effectiveness measured by -nDCG@20.,0,,False
92,"subtopic definitions were used: first, the TREC Web Track official subtopics; and second, subtopics derived from the ODP using TextWise4 services, with three levels of categorization to generate subtopics. These subtopics represent an upper-bound on effectiveness (perfect knowledge from the relevance judgments), and a reasonable but imperfect approach, respectively.",1,TREC,True
93,"All approaches used in the experiments were trained to provide the best possible uniform diversification, to ensure that comparisons between the methods are fair. For approaches that required the tuning of parameters, this was carried out using 10-fold cross-validation to determine the best value on each collection. Parameters were tuned at increments of 0.1, and the best  value obtained as 0.8 for the ODP subtopics and 0.9 for the official TREC subtopics.",1,ODP,True
94,5.2 Experimental Results,0,,False
95,"We investigated the impact on effectiveness of the proposed approach as a diversification feature and compared it to existing approaches in the literature [1, 2, 10, 11, 12].",0,,False
96,"Table 1 shows the Spearman correlation between our proposed approach and other diversification approaches. The results show that, in general, RankScoreDiff is more strongly correlated with implicit diversification approaches than with explicit approaches; the difference with the latter becomes more pronounced when the TREC (perfect) subtopics are available.",1,TREC,True
97,"The results of our effectiveness experiments are shown in Tables 2 and 3, for the LM and BM25 initial retrieval runs, respectively. To carry out a detailed analysis, different baselines were considered. For this reason the following comparisons were made:",1,LM,True
98,· A significant difference between the measured technique and the initial run is shown using * (p < 0.05) and ** (p < 0.01).,0,,False
99,4http://www.textwise.com/,0,,False
100,"· A significant difference between the measured technique and implicit approaches (MMR, MPT, FLAMPT and RankScoreDiff), which are independent of external knowledge such as subtopics, is shown using  (p < 0.05) and  (p < 0.01). (The symbol indicates that a technique is significantly better than all four of the implicit approaches at the specified level.)",0,,False
101,"· A significant difference between a state-of-the-art explicit diversification method (xQuAD or IASelect), compared to RankScoreDiff combined with that method, is shown using  (p < 0.05) and  (p < 0.01).",0,,False
102,Language model as a baseline,0,,False
103,"Table 2 shows the results when using LM as an initial retrieval run. It can be seen that RankScoreDiff (row 5) significantly improves over the base run (row 1) for -nDCG@5 and -nDCG@20. Although there is some marginal improvement in comparison with other implicit approaches (rows 2-4), this improvement is only significant for -nDCG@5. The results suggest that RankScoreDiff is competitive in comparison with implicit approaches, but it is not as good as the explicit approaches (rows 6 and 8). However, RankScoreDiff can also be used in combination with the explicit approaches (rows 7 and 9 of the table).",1,LM,True
104,"Using ODP subtopics, the combination of RankScoreDiff with an explicit approach improves over using the explicit approach on its own in most cases. The improvement is significant for -nDCG@5 and -nDCG@10 when IASelect and RankScoreDiff are combined. Using the TREC (perfect) subtopics, marginal improvements are obtained when combining RankScoreDiff with the explicit approaches, however the combined approach is not significantly different compared with the original explicit approach. In addition, the combined approaches are always significantly better than the base run, and are usually significantly better than the implicit approaches.",1,ODP,True
105,OKAPI BM25 as a baseline,1,AP,True
106,"Table 3 shows results when using BM25 as a baseline run. The improvements in effectiveness over the base run are marginal for all implicit approaches, including RankScoreDiff.",0,,False
107,"Furthermore, Table 3 shows that for the ODP subtopics, even the explicit approaches do not lead to significant improvements over the base run. Similarly, combining RankScoreDiff with xQuAD and IASelect for the ODP subtopics does not improve significantly on the baseline, and in some cases reduces effectiveness. When using TREC (perfect) subtopics, all explicit approaches (on their own, or combined",1,ODP,True
108,1145,0,,False
109,Runs Initial Run (BM25) MMR MPT FLA-MPT RankScoreDiff xQuAD RankScoreDiff + xQuAD IASelect RankScoreDiff + IASelect,0,,False
110,-nDCG@5 0.268 0.266 0.270 0.275 0.270 0.273 0.275 0.263 0.256,0,,False
111,ODP Subtopics -nDCG@10 0.300 0.300 0.301 0.305 0.298 0.309 0.309 0.294 0.288,1,ODP,True
112,-nDCG@20 0.336 0.337 0.336 0.340 0.335 0.344 0.339 0.331 0.323,0,,False
113,-nDCG@5,0,,False
114,0.268 0.266 0.272 0.275 0.270 0.335 **  0.341 **  0.348 ** 0.343 ** ,0,,False
115,TREC Subtopics,1,TREC,True
116,-nDCG@10 -nDCG@20,0,,False
117,0.300,0,,False
118,0.336,0,,False
119,0.300,0,,False
120,0.337,0,,False
121,0.302,0,,False
122,0.337,0,,False
123,0.305,0,,False
124,0.340,0,,False
125,0.298,0,,False
126,0.335,0,,False
127,0.377 ** ,0,,False
128,0.407 **,0,,False
129,0.378 ** ,0,,False
130,0.409 **,0,,False
131,0.389 ** ,0,,False
132,0.420 **,0,,False
133,0.384 ** ,0,,False
134,0.413 **,0,,False
135,"Table 3: Effectiveness of diversification approach using Okapi (BM25) as baseline. For approaches that need explicit representation of subtopics, TREC official subtopics and ODP subtopics were used.",1,TREC,True
136,"with RankScoreDiff) improve significantly over the base run and over the implicit approaches. The combination of RankScoreDiff and xQuAD could marginally improve over xQuAD on its own for -nDCG@5, while this is not the case for IASelect.",0,,False
137,"Overall, the results in Tables 2 and 3 show that RankScoreDiff is equivalent in effectiveness with other implicit approaches, although with no significant improvement over strong base runs. However, in the absence of perfect subtopics, RankScoreDiff can potentially be used in combination with explicit approaches to provide a boost in effectiveness. We note that a particular feature of RankScoreDiff is that it is computationally much less intensive than all other diversification approaches, being based only on information that is already available with a base run.",1,ad,True
138,6. CONCLUSIONS,0,,False
139,"This paper examined a novel approach that uses the differences in original retrieval scores as evidence of diversity, based on the assumption that similar documents will receive similar retrieval scores with respect to a given query, and that similar documents could represent a similar subtopic. We experimentally evaluated the use of a score difference technique to diversify search results. In contrast with existing diversification techniques, which need additional document representations or external subtopics, our proposed approach only needs the relevance score provided by a ranking function. From the results, diversifying using score differences is competitive with other implicit diversification approaches. However, none of these approaches regularly lead to significant improvements over a base run. When perfect subtopic knowledge is not available, the RankScoreDiff approach can potentially boost the effectiveness of state-ofthe-art explicit diversification techniques.",1,ad,True
140,Our analysis of the distribution of score differences showed that the approach is directly affected by the ranking function that generates the initial retrieval scores.,0,,False
141,"In future work, we plan to investigate how particular features of ranking functions interact with the score differences approach. For example, a parameterised ranking function such as BM25 allows individual effects such as length normalisation, or the relative emphasis of TF and IDF effects, to be isolated and explored.",0,,False
142,8. REFERENCES,0,,False
143,"[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proc. WSDM, pages 5­14. ACM, 2009.",0,,False
144,"[2] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. SIGIR, pages 335­336. ACM, 1998.",0,,False
145,"[3] C. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 web track. In Proc. TREC, 2009.",1,TREC,True
146,"[4] C. Clarke, N. Craswell, I. Soboroff, and G. Cormack. Preliminary overview of the trec 2010 web track. In Proc. TREC, volume 10, 2010.",1,trec,True
147,"[5] C. Clarke, M. Kolla, G. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proc. SIGIR, pages 659­666. ACM, 2008.",1,Novelty,True
148,"[6] W. B. Croft, D. Metzler, and T. Strohman. Search engines: Information retrieval in practice. Addison-Wesley Reading, 2010.",1,ad,True
149,"[7] F. Radlinski, P. Bennett, B. Carterette, and T. Joachims. Redundancy, diversity and interdependent document relevance. In Proc. SIGIR, volume 43, pages 46­52. ACM, 2009.",1,ad,True
150,"[8] F. Radlinski and S. Dumais. Improving personalized web search using result diversification. In Proc. SIGIR, pages 691­692. ACM, 2006.",1,ad,True
151,"[9] R. L. T. Santos, C. Macdonald, and I. Ounis. On the role of novelty for search result diversification. Information Retrieval, 2012.",0,,False
152,"[10] R. L. T. Santos, J. Peng, C. Macdonald, and I. Ounis. Explicit search result diversification through sub-queries. In Proc. ECIR, pages 87­99, Milton Keynes, UK, 2010. Springer.",0,,False
153,"[11] J. Wang and J. Zhu. Portfolio theory of information retrieval. In Proc. SIGIR, pages 115­122. ACM, 2009.",0,,False
154,"[12] G. Zuccon, L. Azzopardi, D. Zhang, and J. Wang. Top-k retrieval using facility location analysis. In Proc. ECIR, pages 305­316. Springer, 2012.",0,,False
155,7. ACKNOWLEDGMENTS,0,,False
156,"This work was supported in part by the Australian Research Council (DP130104007), as well as NICTA Victoria which is funded by both the Federal and State governments.",0,,False
157,1146,0,,False
158,,0,,False

,sentence,label,data,regex
0,The Search Duel: A Response to a Strong Ranker,0,,False
1,Peter Izsak,0,,False
2,"Technion, Israel",0,,False
3,peteriz@tx.technion.ac.il,0,,False
4,Fiana Raiber,0,,False
5,"Technion, Israel",0,,False
6,fiana@tx.technion.ac.il,0,,False
7,Oren Kurland,0,,False
8,Moshe Tennenholtz,0,,False
9,"Technion, Israel",0,,False
10,Microsoft Research and,0,,False
11,kurland@ie.technion.ac.il,0,,False
12,"Technion, Israel",0,,False
13,moshet@ie.technion.ac.il,0,,False
14,ABSTRACT,0,,False
15,"How can a search engine with a relatively weak relevance ranking function compete with a search engine that has a much stronger ranking function? This dual challenge, which to the best of our knowledge has not been addressed in previous work, entails an interesting bi-modal utility function for the weak search engine. That is, the goal is to produce in response to a query a document result list whose effectiveness does not fall much behind that of the strong search engine; and, which is quite different than that of the strong engine. We present a per-query algorithmic approach that leverages fundamental retrieval principles such as pseudofeedback-based relevance modeling. We demonstrate the merits of our approach using TREC data.",1,ad,True
16,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,0,,False
17,"Keywords: search engine competition, dueling algorithms",0,,False
18,1. INTRODUCTION,1,DUC,True
19,"We revisit the classic ad hoc relevance ranking problem from a competition perspective. Rather than addressing a single ranking system, we consider a duel in which a search problem -- i.e., a query representing an information need -- is presented to two players. One of the players has a relevance ranking function that is considerably ""weaker"" (i.e., less effective) than that of the other player. Yet, his goal is to produce a ranking that is competitive with that created by the player with the stronger ranking function.",1,ad,True
20,"On the theory side, this type of interaction is modeled in the context of the recently introduced dueling algorithms [9]. Our goal is to introduce a pragmatic manifestation in the context of an adversarial retrieval setting (e.g., the Web).",1,ad,True
21,"The ranking functions employed by leading Web search engines are remarkably effective. However, there is still much room for improving retrieval effectiveness due to various reasons. Many of these are related to the adversarial",1,ad,True
22,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609474.",1,ad,True
23,"nature of the retrieval setting (e.g., search engine optimization efforts). Furthermore, it is impossible for a search engine to index all possible documents in a large-scale and dynamically changing collection such as the Web. This reality provides some hope for a search engine with a relatively weak ranking function to compete with a search engine that has a much stronger ranking function.",0,,False
24,"A potential approach to addressing the duel challenge is to try to explicitly learn the ranking function of the strong search engine. However, this approach falls short in our competitive setting. That is, some of the most important information types utilized by the strong ranker may not be available to the weak ranker; e.g., those based on user engagement information such as clickthrough data.",1,ad,True
25,"We propose a per-query competitive approach. We let the weak search engine observe the output (i.e., the result list of the most highly ranked documents) of the strong search engine for a query. Using this list, which is treated as a pseudo feedback set, we induce a relevance model [10]. The model is used to modify the ranking of the weak search engine. The modification is based on a bi-modal criteria: retrieval effectiveness (in terms of relevance) and diversification with respect to the results presented by the strong search engine. The motivation for diversification is based on the following realization. Users of the strong search engine would have no incentive to switch to (or consider) the weak engine if they are presented with the same results.",0,,False
26,"Empirical evaluation performed with TREC data attests to the effectiveness of our approach. For example, we show that the approach can be used to boost the retrieval effectiveness of weak rankers to a level competitive with that of strong rankers, while maintaining relatively low overlap with the strong rankers' result lists. The approach also substantially outperforms a highly effective fusion method that merges the results of the strong and weak search engines.",1,TREC,True
27,"This paper describes a preliminary, and the first (to the best of our knowledge), attempt to address the interesting and practical challenge of a search engine duel. Naturally, an abundance of research challenges, in addition to those we address here, arise. We discuss some of these in Section 5.",1,ad,True
28,2. RELATED WORK,0,,False
29,"As mentioned, from a theory perspective, the work on dueling algorithms [9] deals with a setting similar to ours. Although the emphasis is on computing minimax strategies, the basic building block is computing the response of one",0,,False
30,919,0,,False
31,"agent to another. However, the model is stylized and does not refer to the realistic search duel we discuss here.",0,,False
32,"There is a large body of work on merging document lists that were retrieved in response to a query from the same corpus [4] or from different corpora [1]. Our use of a relevance model induced from one list to re-rank another list is conceptually reminiscent of work on using inter-document similarities between two lists for re-ranking [11]. However, in contrast to all these results merging approaches which aim to maximize only relevance, our methods are designed to also minimize overlap with the strong engine's result list.",1,corpora,True
33,"There is work on training a ranker for one domain (language) and applying it to another [7, 8]. In contrast, we do not assume different domains or languages and we do not train a ranker but rather use a pseudo-feedback-based relevance model.",0,,False
34,"We note that existing methods for diversifying search results (e.g., [2, 12]) focus on a single retrieved list and on a notion of diversification -- i.e., coverage of query aspects -- which is different than that we address here; that is, the overlap with another result list.",1,ad,True
35,3. SEARCH ENGINE RESPONSES,0,,False
36,FRAMEWORK,0,,False
37,"Let q be a query which is fixed here and after. Let C be a corpus of documents upon which two search engines perform a search in response to q. One of the search engines, henceforth referred to as strong, is assumed to have a more effective relevance ranking function than that of the other -- the so called weak engine. Specifically, the ranking induced by the strong engine in response to q is assumed to be of higher effectiveness than that induced by the weak engine.",0,,False
38,"We use L[sntr] ong and L[wne]ak to refer to the lists of the n documents that are the most highly ranked by the strong and weak engines, respectively. The goal we pursue is devising an effective response strategy for the weak engine, given that it has access to the list L[sntr] ong of the strong engine.1 By response we mean producing a new result list, L[wne]ak;response, composed of n documents, that will replace the original list, L[wne]ak; yet, L[wne]ak can be used to produce the response.",0,,False
39,"A key question is what makes a response ""effective"". Obviously, L[wne]ak;response should be of the highest possible effectiveness, preferably, not significantly lower than that of L[sntr] ong. Supposedly, then, a highly effective response is setting L[wne]ak;response d,""ef L[sntr] ong; that is, having the weak engine replicate the result list produced by the strong engine. However, assuming that the strong search engine already has a well established, and wide, base of users, such a response is not likely to result in any incentive for users to switch engines. Therefore, the second criterion we set for an effective response is that the overlap, in terms of shared documents, between L[wne]ak;response and L[sntr] ong will be minimal; i.e., the goal is to differentiate the weak engine from the strong engine.""",1,ad,True
40,"In summary, the weak engine should produce a result list that is as diverse as possible, and competitive in terms of effectiveness, with respect to the result list produced by the",0,,False
41,"1In practical settings, the weak engine can potentially record, from time to time, the results produced by the strong search engine, specifically, in response to common queries.",0,,False
42,"strong engine. In doing so, the weak engine can use its original list L[wne]ak and that of the strong engine, L[sntr] ong.",0,,False
43,3.1 Relevance Modeling as a Basis for Response Strategies,0,,False
44,"The basic assumption underlying the strategies that we employ below is that there are relevant documents in L[wne]ak that are not in L[sntr] ong. The reason could be, for example, a different coverage of the indexes of the two engines.",0,,False
45,"Yet, it is not necessarily the case that these documents are",0,,False
46,ranked high enough due to the relatively weak relevance,0,,False
47,"ranking function of the weak engine. Thus, we use a relevance language model R [10] induced from L[skt]rong -- the k ( n) documents that are the highest ranked in L[sntr] ong -- to re-rank L[wne]ak; L[wne]ak;re-rank denotes the resultant ranked list. As R reflects a model of relevance of the strong engine",0,,False
48,"with respect to the query q, we assume that the ranking of L[wne]ak;re-rank is of higher effectiveness than that of L[wne]ak. We provide empirical support to this assumption in Section 4. The re-ranking of L[wne]ak using R is based on the cross entropy between R and the language models induced from the",0,,False
49,documents. Details regarding (relevance) language model,0,,False
50,induction are provided in Section 4.1.,0,,False
51,"In what follows we present several strategies of producing the final result list of the weak engine, L[wne]ak;response, using the lists L[wne]ak;re-rank and L[sntr] ong.",0,,False
52,3.2 Response Strategies,0,,False
53,"The first response strategy, WeakReRank, is simply using L[wne]ak;re-rank for L[wne]ak;response. The source for diversity with the strong list L[sntr] ong is the assumed existence of documents in L[wne]ak that are not in L[sntr] ong. The presumably improved effectiveness of L[wne]ak;re-rank is due to the way it was created; that is, re-ranking L[wne]ak using a relevance model induced from L[sntr] ong.",0,,False
54,The second response strategy is a probabilistic round robin,0,,False
55,"procedure, henceforth referred to as ProbRR. We create L[wne]ak;response top down by scanning the lists L[wne]ak;re-rank and L[sntr] ong also top down. The next document selected for L[wne]ak;response is taken from L[wne]ak;re-rank with probability p and from L[sntr] ong with probability 1-p. (Documents that were already selected are skipped.) Smaller values of p result in more documents taken from L[sntr] ong. Accordingly, the overlap of the final result list L[wne]ak;response with L[sntr] ong can increase and the effectiveness is potentially maintained at the same level as that of L[sntr] ong. Hence, ProbRR enables a certain degree of control over the effectiveness and diversification of L[wne]ak;re-rank with respect to L[sntr] ong using different values of p. However, L[wne]ak;re-rank can contain documents that are also in L[sntr] ong. Thus, higher values of p do not necessarily directly translate to increased diversity with respect to L[sntr] ong.",1,ad,True
56,"To directly control the level of diversity of L[wne]ak;response with respect to L[sntr] ong, we examine a variant of ProbRR termed ProbResRR -- the third response strategy we propose. Instead of using L[wne]ak;re-rank and L[sntr] ong we use",1,ad,True
57,920,0,,False
58,"L[wne]ak;re-rank \ L[sntr] ong and L[sntr] ong; i.e., we remove from L[wne]ak;re-rank documents that are in L[sntr] ong and maintain the ranking of the residual documents. We then apply the same procedure described above for ProbRR to create the final result list L[wne]ak;response. Using larger values of p results in increased diversity with respect to L[sntr] ong.",0,,False
59,4. EVALUATION,0,,False
60,4.1 Experimental Setup,0,,False
61,"We used the Web tracks of TREC 2009­2011, henceforth TREC-2009, TREC-2010, and TREC-2011, to create the strong vs. weak search engine setting. We focused on runs submitted for the ClueWeb09 category B collection which is composed of around 50 million documents.",1,TREC,True
62,"We randomly selected 30 pairs of runs from all those submitted and which contain at least 1000 documents as results for each query in the track. In each pair of runs, the result lists of the run whose MAP (@1000) is higher serve for the lists of the strong engine, L[sntr] ong, and those of the run with the lower MAP serve for the lists of the weak engine, L[wne]ak. Each result list contains n ,"" 1000 documents. We report average performance over the 30 samples. Thus, here, the strong and weak engines are represented by """"averages"""" over runs of which one is on average more effective than the other.""",1,MAP,True
63,Titles of TREC topics serve for queries. Stopwords on the INQUERY list were removed from queries but not from documents. The Indri toolkit (www.lemurproject.org/indri) was used for experiments.,1,TREC,True
64,"To evaluate retrieval performance, we use MAP (@1000) and NDCG (@20). Statistically significant differences of performance, computed over the 30 pairs of runs, were determined using the two-tailed paired t-test at a 95% confidence level. To measure the diversity of the result list of the weak engine with respect to that of the strong engine, we use the overlap (i.e., number of shared documents) at the top ten (OV@10) and twenty (OV@20) ranks of the two lists.",1,MAP,True
65,"We use Dirichlet-smoothed document language models with the smoothing parameter set to 1000. For the relevance model R, we use the rank-based RM3 model [5] which is constructed from the top k (,"" 10) documents in the strong engine's result list. (We use ranks rather than retrieval scores as the latter are not assumed to be known.) The number of terms used by RM3, and its query anchoring parameter, are set to the default values of 50 and 0.5, respectively. Using this (under optimized) default parameter setting allows to demonstrate the potential of using the relevance modeling idea as a basis for producing responses.""",0,,False
66,"As a reference comparison response strategy we use the highly effective CombMNZ fusion method [6] to merge the result lists of the weak and strong engines. Document scores induced from ranks, as suggested in [3], are used in CombMNZ. As all other fusion methods, CombMNZ addresses (explicitly) only one of the two criteria for effective response strategy -- i.e., retrieval effectiveness.",1,ad,True
67,4.2 Experimental Results,0,,False
68,The performance numbers of all methods are presented in Table 1. We use ProbRR(p) and ProbResRR(p) to indicate that the ProbRR and ProbResRR response strategies were used with the probability parameter p.,0,,False
69,"Table 1 shows that Strong is much more effective than Weak for both MAP and NDCG; the differences are substantial and statistically significant for all experimental settings. Furthermore, the overlap between Strong and Weak, as measured by OV@10 and OV@20, is low. Thus, the experimental setting we used adheres to the problem definition: (i) the strong search engine is (much) stronger in terms of retrieval effectiveness, and (ii) the overlap between the result lists of the strong and weak search engines is not high.",1,MAP,True
70,"We also see in Table 1 that WeakReRank is quite an effective response strategy; specifically, in comparison to a highly effective fusion method (CombMNZ) in terms of both retrieval effectiveness and overlap at top ranks with Strong. WeakReRank's retrieval effectiveness can be statistically significantly worse than that of Strong. However, WeakReRank outperforms Weak for MAP and NDCG, with all the improvements being statistically significant. Although the overlap at top ranks of WeakReRank with Strong is larger than that of Weak, it is still quite low with respect to that of the other strategies considered. These findings attest to the effectiveness of using relevance modeling based on the result list of the strong search engine so as to re-rank the result list of the weak search engine.",1,MAP,True
71,"Table 1 also shows that ProbRR is a highly effective response strategy in many cases. Evidently, the balance between retrieval effectiveness and overlap with Strong can be effectively controlled via the parameter p. Although in terms of overlap with Strong, ProbRR is somewhat less effective than WeakReRank and CombMNZ, in terms of retrieval effectiveness it is substantially better than the two and often better than Strong.",0,,False
72,"It is also evident in Table 1 that ProbResRR is less effective than ProbRR for MAP and NDCG. However, the overlap numbers for ProbResRR are lower than those for ProbRR. This finding is not surprising because ProbRR uses the re-ranked list of the weak engine while ProbResRR uses the residual documents in the same list that remain after the removal of documents that also appear in the result list of the strong engine.",1,MAP,True
73,The effectiveness-diversity tradeoff. We next study the,1,ad,True
74,"effect of the parameter p used in ProbRR and ProbResRR on the tradeoff between the retrieval effectiveness of the response list of the weak search engine, as measured using MAP, and its overlap with the result list produced by the strong search engine, measured using OV@10.",1,ad,True
75,"Figure 1 presents the results of setting p to values in {0, 0.1, . . . , 1}; p ,"" 0 amounts to using only the result list of the strong search engine in ProbRR and ProbResRR, while p "", 1 amounts to using the result list L[wne]ak;re-rank.",0,,False
76,"We can see that for ProbResRR, MAP decreases with increasing values of p. The reason is that fewer documents that appear in the result list of the strong engine are used. Furthermore, ProbRR outperforms ProbResRR for all p > 0. In addition, we see that ProbRR can attain its optimal performance for 0 < p < 1 (i.e., outperform both Strong and WeakReRank) which echoes findings in work on fusion [4].",1,MAP,True
77,"For both ProbRR and ProbResRR, OV@10 decreases with increasing values of p, as fewer documents are selected from the result list of the strong engine. As expected, for the same value of p (> 0), the OV@10 of ProbRR is higher than that of ProbResRR. Yet, for the same value of overlap, the MAP of ProbRR is higher than that of ProbResRR.",1,MAP,True
78,921,0,,False
79,Strong,0,,False
80,Weak WeakReRank,0,,False
81,CombMNZ,0,,False
82,ProbRR(0.2) ProbRR(0.5) ProbRR(0.7) ProbResRR(0.2) ProbResRR(0.5) ProbResRR(0.7),0,,False
83,MAP,1,MAP,True
84,17.1w,0,,False
85,11.4s 16.7w,0,,False
86,13.4sw,0,,False
87,18.3sw 18 7s,0,,False
88,.w 18.6sw 15.4sw 11.2s 8.2sw,0,,False
89,TREC-2009 NDCG OV@10 OV@20,1,TREC,True
90,26.5w -,0,,False
91,-,0,,False
92,19.5s 30 3s,0,,False
93,.w,0,,False
94,21.6sw,0,,False
95,27.8sw 29.2sw 29.7sw 23.7sw 18.4s 13.9sw,0,,False
96,17 4 .,0,,False
97,35.0,0,,False
98,40.9,0,,False
99,87.2 67.5 55.4 81.0 52.2 32.6,0,,False
100,19 4 .,0,,False
101,34.5,0,,False
102,43.6,0,,False
103,87.1 67.6 54.8 80.5 52.4 33.2,0,,False
104,MAP,1,MAP,True
105,19.9w,0,,False
106,13.6s 16.5sw,0,,False
107,16.4sw,0,,False
108,20 6s .w,0,,False
109,20.2w 19.4w 17.9sw 12.8s 9.0sw,0,,False
110,TREC-2010 NDCG OV@10 OV@20,1,TREC,True
111,25 2 .w,0,,False
112,-,0,,False
113,-,0,,False
114,16.9s 19.8sw,0,,False
115,18.9sw,0,,False
116,24.1sw 22.6sw 21.5sw 21.8sw 16.0s 11.7sw,0,,False
117,19 7 .,0,,False
118,29.3,0,,False
119,38.3,0,,False
120,85.2 63.3 49.7 79.4 50.0 29.9,0,,False
121,23 3 .,0,,False
122,30.5,0,,False
123,42.1,0,,False
124,85.0 64.4 50.6 79.8 50.5 29.8,0,,False
125,MAP,1,MAP,True
126,19.3w,0,,False
127,13.1s 17.6sw,0,,False
128,17.2sw,0,,False
129,21.7sw 22 3s,0,,False
130,.w 22.0sw 19.2w 15.4sw 12.0s,0,,False
131,TREC-2011 NDCG OV@10 OV@20,1,TREC,True
132,28.0w -,0,,False
133,-,0,,False
134,21.6s 27.9w,0,,False
135,24.0sw,0,,False
136,28.9sw 29.6sw 29 7s,0,,False
137,.w 26.2sw 21.7s 17.6sw,0,,False
138,18 6 .,0,,False
139,30.3,0,,False
140,45.1,0,,False
141,86.0 65.6 51.4 80.3 50.4 30.4,0,,False
142,20 7 .,0,,False
143,31.1,0,,False
144,47.5,0,,False
145,86.3 66.1 52.8 80.0 50.2 30.6,0,,False
146,"Table 1: Main result table. The numbers in the parentheses for the ProbRR and ProbResRR strategies are the value of the p parameter. The highest result in a column for MAP and NDCG, and the lowest for OV@10 and OV@20, is boldfaced. `s' and `w' mark statistically significant differences, for MAP and NDCG, with Strong and Weak, respectively.",1,MAP,True
147,MAP OV@10,1,MAP,True
148,MAP OV@10,1,MAP,True
149,MAP OV@10,1,MAP,True
150,TREC-2009 20,1,TREC,True
151,18,0,,False
152,16,0,,False
153,14,0,,False
154,12,0,,False
155,10,0,,False
156,8,0,,False
157,6,0,,False
158,4,0,,False
159,ProbRR (MAP),1,MAP,True
160,ProbRR (OV@10),0,,False
161,2,0,,False
162,ProbResRR (MAP),1,MAP,True
163,ProbResRR (OV@10) 0,0,,False
164,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,0,,False
165,p,0,,False
166,100 80 60 40 20 0 1,0,,False
167,TREC-2010 22,1,TREC,True
168,20,0,,False
169,18,0,,False
170,16,0,,False
171,14,0,,False
172,12,0,,False
173,10,0,,False
174,8,0,,False
175,6,0,,False
176,4,0,,False
177,ProbRR (MAP) ProbRR (OV@10),1,MAP,True
178,2,0,,False
179,ProbResRR (MAP),1,MAP,True
180,ProbResRR (OV@10) 0,0,,False
181,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,0,,False
182,p,0,,False
183,100 80 60 40 20 0 1,0,,False
184,TREC-2011 24,1,TREC,True
185,22,0,,False
186,20,0,,False
187,18,0,,False
188,16,0,,False
189,14,0,,False
190,12,0,,False
191,10,0,,False
192,8,0,,False
193,ProbRR (MAP),1,MAP,True
194,ProbRR (OV@10),0,,False
195,6,0,,False
196,ProbResRR (MAP),1,MAP,True
197,ProbResRR (OV@10) 4,0,,False
198,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,0,,False
199,p,0,,False
200,100 80 60 40 20 0 1,0,,False
201,Figure 1: The effect of the parameter p on MAP and OV@10. The y-axis on the right of a figure is the range of OV@10. The y-axis on the left of a figure is the range of MAP.,1,MAP,True
202,"Thus, we arrive to the conclusion that tuning the parameter p in ProbRR and ProbResRR helps to effectively control the balance between retrieval effectiveness and overlap (diversity) with the strong engine. Furthermore, ProbRR is a more effective response strategy than ProbResRR as it allows to attain improved level of retrieval effectiveness for the same level of overlap with the strong search engine.",0,,False
203,5. CONCLUSIONS AND FUTURE WORK,0,,False
204,"We presented the first (preliminary) attempt to address the search engine duel problem; namely, how can a search engine with a relatively weak relevance ranking function compete with a search engine with a much stronger relevance ranking function? We devised an algorithmic response framework which consists of several strategies that can be used by the weak search engine. We consider a response to be effective if it results in improved search effectiveness and the search results are different than those presented by the strong engine. Empirical evaluation demonstrated the merits of our response strategies and shed some light on the (relevance) effectiveness-diversity tradeoff embodied in our bi-modal criteria for response effectiveness.",1,ad,True
205,"Devising additional criteria for response effectiveness, along with developing additional corresponding response strategies, is the first future venue we intend to explore. We also plan on devising response strategies for the strong engine.",1,ad,True
206,Acknowledgments We thank the reviewers for their comments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center. This work has also been supported in part by Microsoft Research through its Ph.D. Scholarship Program.,0,,False
207,6. REFERENCES,0,,False
208,"[1] J. Callan. Distributed information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 5, pages 127­150. Kluwer Academic Publishers, 2000.",1,ad,True
209,"[2] J. G. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR, pages 335­336, 1998.",0,,False
210,"[3] G. V. Cormack, C. L. A. Clarke, and S. Bu¨ttcher. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proc. of SIGIR, pages 758­759, 2009.",0,,False
211,"[4] W. B. Croft. Combining approaches to information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 1, pages 1­36. Kluwer Academic Publishers, 2000.",1,ad,True
212,"[5] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A language modeling framework for selective query expansion. Technical Report IR-338, Center for Intelligent Information Retrieval, University of Massachusetts, 2004.",0,,False
213,"[6] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proc. of TREC-2, 1994.",1,TREC,True
214,"[7] W. Gao, J. Blitzer, and M. Zhou. Using english information in non-english web search. In Proc. of the 2nd ACM workshop on Improving Non English Web Searching, iNEWS, pages 17­24, 2008.",0,,False
215,"[8] W. Gao, P. Cai, K.-F. Wong, and A. Zhou. Learning to rank only using training data from related domain. In Proc. of SIGIR, pages 162­169, 2010.",0,,False
216,"[9] N. Immorlica, A. T. Kalai, B. Lucier, A. Moitra, A. Postlewaite, and M. Tennenholtz. Dueling algorithms. In Proc. of STOC, pages 215­224, 2011.",0,,False
217,"[10] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. of SIGIR, pages 120­127, 2001.",0,,False
218,"[11] L. Meister, O. Kurland, and I. G. Kalmanovich. Re-ranking search results using an additional retrieved list. Information Retrieval, 14(4):413­437, 2011.",1,ad,True
219,"[12] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proc. of WWW, pages 881­890, 2010.",0,,False
220,922,0,,False
221,,0,,False

,sentence,label,data,regex
0,Search Result Diversification via Data Fusion,0,,False
1,Shengli Wu and Chunlan Huang,0,,False
2,"School of Computer Science and Telecommunication Engineering Jiangsu University, Zhenjiang, China 212013",0,,False
3,"swu@ujs.edu.cn, palaceo77@163.com",0,,False
4,ABSTRACT,0,,False
5,"In recent years, researchers have investigated search result diversification through a variety of approaches. In such situations, information retrieval systems need to consider both aspects of relevance and diversity for those retrieved documents. On the other hand, previous research has demonstrated that data fusion is useful for improving performance when we are only concerned with relevance. However, it is not clear if it helps when both relevance and diversity are both taken into consideration. In this short paper, we propose a few data fusion methods to try to improve performance when both relevance and diversity are concerned. Experiments are carried out with 3 groups of top-ranked results submitted to the TREC web diversity task. We find that data fusion is still a useful approach to performance improvement for diversity as for relevance previously.",1,TREC,True
6,Categories and Subject Descriptors,0,,False
7,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ retrieval models,0,,False
8,General Terms,0,,False
9,"Algorithms, Experimentation, Measurement, Performance",0,,False
10,Keywords,0,,False
11,"Search result diversification, data fusion, linear combination, weight assignment",0,,False
12,1. INTRODUCTION,1,DUC,True
13,"In recent years, researchers have taken various approaches to investigate search result diversification [3, 1]. In such situations, information retrieval systems need to consider both relevance and diversity for those retrieved documents. In this short paper, we aim to find out if and how data fusion can help with this.",0,,False
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609451.",1,ad,True
15,"Previous research on data fusion (such as in [2, 6, 8]) demonstrates that it is possible to improve retrieval performance when we only consider relevance. Now with the new dimension of diversification, we need to re-evaluate the technology. In particular, some fusion methods need to be modified to accommodate for the new situation.",0,,False
16,"We may divide data fusion methods into two broad categories, according to how they deal with component results: equal-treatment and biased methods. As their names would suggest, the former treats all component results equally, while the latter does not. CombSum, CombMNZ, and the Condorcet method belong to the first category, while the linear combination method is a representative of the second category. Equal-treatment methods can likely be used in the new situation without modification, but the linear combination method needs more consideration.",1,ad,True
17,"In linear combination, weight assignment is a key issue for achieving good fusion performance and a considerable number of weight assignment methods have been proposed. Generally speaking, we need to consider two factors for weight assignment. One is the performance of every component retrieval system involved, and the other is the dissimilarity (or distance) between those component systems/results. For the information retrieval systems involved, well-performing systems should be given greater weights , while systems performing poorly should be assigned smaller weights. On the other hand, smaller weights should be assigned to those results that are similar to the others, while greater weights should be assigned to those results that are more different to the others. When assigning weights, we may take into consideration performance or dissimilarity, or even both together. It is also possible to use some machine learning techniques, known as ""learning to rank"", to train weights using some training data. This is especially popular for combining results at the feature level. These methods are aimed at optimizing a goal that is related to retrieval effectiveness measured by a given metric such as average precision. Because the metrics used for result diversification (e.g., ERR-IA@20) are very different from metrics such as average precision, almost all methods in this category cannot be directly used for result diversification.",0,,False
18,"In this paper, we are going to investigate data fusion methods, especially linear combination, for result diversification. Experiments are carried out to evaluate them with 3 groups of results submitted to the TREC web diversity task between 2009 and 2011. Experiments show that the proposed methods perform well and have the potential to be used for this purpose in practice.",1,TREC,True
19,827,0,,False
20,Table 1: Information of 3 groups of results submitted to the web diversity task in TREC,1,TREC,True
21,TREC 2009,1,TREC,True
22,TREC 2010,1,TREC,True
23,TREC 2011,1,TREC,True
24,Run,0,,False
25,ERR-IA@20 Run,0,,False
26,ERR-IA@20 Run,0,,False
27,ERR-IA@20,0,,False
28,MSRAACSF,0,,False
29,0.2144 qirdcsuog3,0,,False
30,0.2051 ICTNET11DVR3 0.4764,0,,False
31,MSDiv3,0,,False
32,0.2048 THUIR10DvNov,0,,False
33,0.3355 liaQEWikiAnA,1,Wiki,True
34,0.2287,0,,False
35,uogTrDYCcsB,0,,False
36,0.1922 UAMSD10aSRfu,0,,False
37,0.2423 msrsv2011d1,0,,False
38,0.4994,0,,False
39,UamsDancTFb1 0.1774 UCDSIFTDiv,1,TD,True
40,0.2100 UAmsM705tFLS,0,,False
41,0.4378,0,,False
42,mudvimp,0,,False
43,0.1746 UMd10IASF,0,,False
44,0.2546 uogTrA45Nmx2,0,,False
45,0.5284,0,,False
46,UCDSIFTdiv,0,,False
47,0.1733 uogTrB67xS,0,,False
48,0.2981 UWatMDSqltsr,0,,False
49,0.4939,0,,False
50,NeuDiv1,0,,False
51,0.1705 cmuWi10D,0,,False
52,0.2484 uwBA,0,,False
53,0.3986,0,,False
54,THUIR03AbClu 0.1665 ICTNETDV10R2 0.3222 CWIcIA2t5b1,1,TD,True
55,0.3487,0,,False
56,Average,0,,False
57,0.1842 Average,0,,False
58,0.2645 Average,0,,False
59,0.4265,0,,False
60,Variance,0,,False
61,0.0003 Variance,0,,False
62,0.0024 Variance,0,,False
63,0.0098,0,,False
64,2. SEVERAL METHODS FOR RESULT,0,,False
65,DIVERSIFICATION,0,,False
66,"As aforementioned, weight assignment is a key issue for",0,,False
67,"the linear combination method. In this section, we look at",0,,False
68,"different ways of dealing with this issue. Firstly, we may",0,,False
69,consider the performance of the retrieval system in question,0,,False
70,"and its similarity with other retrieval systems separately, so",0,,False
71,that we may obtain two types of weights. Then a combi-,0,,False
72,nation of these two types of weights can be used to fuse,0,,False
73,results. Note that performance and dissimilarity are two in-,0,,False
74,"dependent factors. On the one hand, performance of a result",0,,False
75,concerns the ranking positions of relevant documents and,0,,False
76,how diversified those relevant documents are in the ranked,0,,False
77,"list of documents; on the other hand, the dissimilarity of two",0,,False
78,or more results is concerned with how different the ranking,0,,False
79,positions of the same documents are in two or more indi-,0,,False
80,"vidual results, making no distinction between relevant and",0,,False
81,non-relevant documents.,0,,False
82,Suppose there are a group of information retrieval sys-,0,,False
83,"tems ir1, ir2,...,irt, with some training data available for us",0,,False
84,to measure their performance and the dissimilarity between,0,,False
85,"them. We further assume that their performances are p1,",0,,False
86,"p2,..., pt, respectively, as measured by a given metric (e.g.,",0,,False
87,"ERR-IA@20), so that we may then assign the value of a function of pi (such as pi, pi2, and so on) to wi for (1  i  t),",0,,False
88,as the performance-related weight of iri.,0,,False
89,Different approaches are possible for calculating the dis-,0,,False
90,similarity (or similarity) between two results. One approach,0,,False
91,is to refer to each result as a set of documents and calculate,0,,False
92,the overlap rate between two results (sets). It is also possi-,0,,False
93,ble to calculate the correlation (such as Spearman's ranking,0,,False
94,coefficient or Kendall's tau coefficient) of two ranked lists of,0,,False
95,documents. If all the documents in the two results are as-,0,,False
96,"sociated with proper scoring information, then score-based",0,,False
97,methods such as the Euclidean distance or city block dis-,0,,False
98,tance can be used. In the following we discuss two different,0,,False
99,ways of doing it.,0,,False
100,Let us consider the top-n documents in all component re-,0,,False
101,"sults. Suppose that document dij, in result ri, appears or is referred to in cij of the other t - 1 results, then all n",0,,False
102,top-ranked documents of ri are referred in all other results,0,,False
103,"refi ,",0,,False
104,"n j,1",0,,False
105,cij,0,,False
106,times.,0,,False
107,"For each document dij, the maxi-",0,,False
108,"mum times it can appear in the other t - 1 results is t - 1,",0,,False
109,This fact can be used to define the dissimilarity of ri to other,0,,False
110,results as,0,,False
111,"1 disi , n",0,,False
112,n,0,,False
113,(t - 1 - cij ) t-1,0,,False
114,(1),0,,False
115,"j,1",0,,False
116,disi are always in the range of 0 and 1. We may define disi or a function of them as the dissimilarity-related weights. Methods using this definition are referred to as reference-based methods later in this paper. One advantage of using such methods for dissimilarity is that we can obtain the weights for component systems by considering all the documents of them together.,1,ad,True
117,"An alternative of calculating the dissimilarity between results is to compare documents' ranking difference for each pair of them. Let us consider the n top-ranked documents in both results rA and rB. Suppose that m (m  n) documents appear in both rA and rB, and (n - m) of them appear in only one of them. For those n - m documents that only appear in one of the results, we simply assume that they occupy the places from rank n + 1 to rank 2n - m whilst retaining the same relative orders in the other result. Thus we can calculate the average rank difference of all the documents in both results and use it to measure the dissimilarity of rA and rB. To summarize, we have",0,,False
118,"v(rA, rB)",0,,False
119,",",0,,False
120,1,0,,False
121,di,0,,False
122,{,0,,False
123,rA,0,,False
124,di,0,,False
125,rB,0,,False
126,|pA(di) - pB(di)|,0,,False
127,n,0,,False
128,m,0,,False
129,"i,""1,2,...,m""",0,,False
130,+,0,,False
131,dirAdi/rB |pA(di) - (n + i)| n-m,0,,False
132,"i,""1,2,...,n-m""",0,,False
133,+,0,,False
134,di / rA di rB,0,,False
135,|pB,0,,False
136,(di) n,0,,False
137,- -,0,,False
138,(n m,0,,False
139,+,0,,False
140,i)|,0,,False
141,},0,,False
142,(2),0,,False
143,"i,""1,2,...,n-m""",0,,False
144,Here pA(di) and pB(di) denote the rank position of di in,0,,False
145,"rA and rB, respectively.",0,,False
146,1 n,0,,False
147,is,0,,False
148,the,0,,False
149,normalization,0,,False
150,coefficient,0,,False
151,"which guarantees that v(rA, rB) is in the range of 0 and 1.",0,,False
152,"Based on Equation 2, the dissimilarity weight of ri (1  i ",0,,False
153,t) is defined as,0,,False
154,1,0,,False
155,"j,i",0,,False
156,"disi , t - 1",0,,False
157,"v(ri, rj )",0,,False
158,(3),0,,False
159,"j,""1,2,...,t""",0,,False
160,Methods that use this definition are referred to as ranking difference based methods. No matter how we obtain,0,,False
161,828,0,,False
162,"the weights for dissimilarity, we may combine dissimilarity-",0,,False
163,"related weights with performance-related weights. Different options, such as pi disi, pi2 disi, pi disi2, and so on, might be used to obtain weight wi for information retrieval system iri. At the fusion stage, the linear combination method uses the following equation to calculate scores:",0,,False
164,t,0,,False
165,"g(d) , wi  si(d)",0,,False
166,(4),0,,False
167,"i,1",0,,False
168,where g(d) is the global score that document d obtains dur-,0,,False
169,"ing data fusion, si(d) is the (normalized) score that document d obtains from information retrieval system iri (1  i  t), and wi is the weight assigned to system iri. All the documents can be ranked according to the global scores they",0,,False
170,obtain.,0,,False
171,3. EXPERIMENTS,0,,False
172,"In the 3 successive years from 2009 to 2011, the web track of TREC used the collection of ""ClueWeb09"". The collection consists of roughly 1 billion web pages crawled from the Web.",1,TREC,True
173,"3 groups of results are chosen for the experiment. They are 8 top-ranked results 1 (measured by ERR-IA@20) submitted to the diversity task in the TREC 2009, 2010, and 2011 web track. The information about all the selected results is summarized in Table 1.",1,TREC,True
174,"As we know, it is harder to get improvement over better component results through data fusion. However, the purpose of the experiments is going to see if we can obtain even better results by fusing a number of top-ranked results submitted.",0,,False
175,"In the 3 aforementioned groups of results, the 2009 group has the lowest average effectiveness (.1842), the lowest best effectiveness (.2144), and the smallest variance (.0003); the 2011 group has the highest average effectiveness (.4265), the highest best effectiveness (.5284), and the largest variance (.0098); for all three metrics the 2010 group comes second (average is .2645, best is .3356, variance is .0024). We will see that the effectiveness of the fused results is affected by these factors.",0,,False
176,"In the 2009 group, M SRAACSF [4] is the best performer (ERR IA@20: 0.2144). This run is submitted by Microsoft Research Asia in Beijing. For a given query, sub-topics are mined from different sources including anchor texts, search result clusters, and web sites at which search results are located; and documents are ranked by considering both relevance and diversity of mined sub-topics.",0,,False
177,"In the 2010 group, T HU IR10DvN ov is the best among all 8 runs selected for the experiment. Its performance is 0.3355 when measured by ERR IA@20. Two other runs msrsv3div and uwgym (baseline) are slightly better than T HU IR10DvN ov. The technical details of this run are not known because we cannot find the corresponding report for this. We speculate that this run is submitted by a research group in Tsinghua University.",0,,False
178,"In the 2011 group, uogT rA45N mx2 [7] is the best performer (ERR IA@20: 0.5284). This run is submitted by the",0,,False
179,1msrsv3div and uwgym in 2010 and UDCombine2 in 2011 are not chosen because they include much fewer documents than the others and using them would cause problems in calculating weights for the linear combination method and in the fusion process as well.,0,,False
180,Table 2: Performance (measured by ERR-IA@20) of a group of data fusion methods (p denotes performance-related weight and dis denotes dissimilarity-related weight; dis is calculated using either Equation 1 or Equation 3; the figures in parentheses indicate the improvement rate of each method over the best component result; the figures in bold indicate the highest value in the column),0,,False
181,Group best result p p2,0,,False
182,dis  p(Eq.1) dis  p2(Eq.1) dis2  p(Eq.1),0,,False
183,dis  p(Eq.3) dis  p2(Eq.3) dis2  p(Eq.3),0,,False
184,2009 0.2144 0.2544 (18.66%) 0.2499 (16.56%) 0.2552 (19.03%) 0.2492 (16.23%) 0.2548 (18.84%) 0.2553 (19.08%) 0.2503 (16.74%) 0.2534 (18.19%),0,,False
185,2010 0.3355 0.3567 (6.32%) 0.3684 (9.81%) 0.3548 (5.75%) 0.3705 (10.43%) 0.3533 (5.31%) 0.3531 (5.25%) 0.3658 (9.03%) 0.3562 (6.17%),0,,False
186,2011 0.5284 0.5398 (2.16%) 0.5343 (1.12%) 0.5398 (2.16%) 0.5355 (1.34%) 0.5410 (2.38%) 0.5388 (1.97%) 0.5347 (1.19%) 0.5330 (0.87%),0,,False
187,Ave. 0.3551 0.3836 9.05% 0.3842 9.16% 0.3833 8.60% 0.3851 9.33% 0.3830 8.84% 0.3824 8.77% 0.3836 8.99% 0.3809 8.41%,0,,False
188,IR research group at Glasgow University. It uses Terrier,0,,False
189,with a component xQuAD for search result diversification.,0,,False
190,The primary idea is to find useful information of sub-topics,0,,False
191,by sending the initial query to three commercial web search,0,,False
192,engines.,0,,False
193,"In each year group, 50 queries are divided into 5 groups:",0,,False
194,"1-10, 11-20, 21-30, 31-40, and 41-50. 4 arbitrary groups",0,,False
195,"of them are used as training queries, while the remaining",0,,False
196,one group is used for fusion test. This is referred to as the,0,,False
197,five-fold cross validation method in statistics and machine,0,,False
198,learning [5]. Every result is evaluated using ERR-IA@20,0,,False
199,over training queries to obtain the performance weight pi.,0,,False
200,"On the other hand, either Equation 1 or Equations 2 and 3",0,,False
201,are used with training data to obtain disi for the dissimilar-,0,,False
202,"ity weight. After that, we try 5 different ways of combining the weights: pi, pi2, pi  disi, pi2  disi, and pi  disi2.",0,,False
203,"In order to fuse component results by linear combination,",0,,False
204,reliable scores are required for all the documents required.,0,,False
205,"In this study, we use the reciprocal function [2]. According",0,,False
206,"to [2], the reciprocal function is very good for converting",0,,False
207,"rankings into scores. For any resultant list r ,"" <d1, d2,...,""",0,,False
208,"dn>,",0,,False
209,a,0,,False
210,score,0,,False
211,of,0,,False
212,1 i+60,0,,False
213,is,0,,False
214,assigned,0,,False
215,to,0,,False
216,document,0,,False
217,di,0,,False
218,at,0,,False
219,rank,0,,False
220,i.,0,,False
221,Experimental results are shown in Tables 2 and 3. Two,0,,False
222,"metrics, ERR-IA@20 and -nDCG@20, are used to evaluate",0,,False
223,all the fusion methods. The best component result is used,0,,False
224,as the baseline. When calculating dissimilarity weights by,0,,False
225,"reference based method, or Equation 1, we use the top 100",0,,False
226,documents in all component results. We have also tried,0,,False
227,"some other options, including the top 50 and the top 200,",0,,False
228,though the experimental results are omitted here since they,0,,False
229,are so similar to what we observed for the top 100. When,0,,False
230,"using rank difference based method, or Equations 2 and 3,",0,,False
231,829,0,,False
232,Table 3: Performance (measured by -nDCG@20) of a group of data fusion methods (p denotes performance-related weight and dis denotes dissimilarity-related weight; dis is calculated using either Equation 1 or Equation 3; the figures in parentheses indicate the improvement rate of each method over the best component result; the figures in bold indicate the highest value in the column),0,,False
233,Group best result p p2,0,,False
234,dis  p(Eq.1) dis  p2(Eq.1) dis2  p(Eq.1),0,,False
235,dis  p(Eq.3) dis  p2(Eq.3) dis2  p(Eq.3),0,,False
236,2009 0.3653 0.4130 (13.06%) 0.4108 (12.46%) 0.4141 (13.36%) 0.4100 (12.24%) 0.4150 (13.61%) 0.4138 (13.28%) 0.4108 (12.46%) 0.4126 (12.95%),0,,False
237,2010 0.4745 0.5071 (6.87%) 0.5226 (10.14%) 0.5057 (6.58%) 0.5241 (10.45%) 0.5045 (6.32%) 0.5054 (6.51%) 0.5202 (9.63%) 0.5084 (7.14%),0,,False
238,2011 0.6298 0.6510 (3.37%) 0.6468 (2.70%) 0.6513 (3.41%) 0.6477 (2.84%) 0.6522 (3.56%) 0.6506 (3.30%) 0.6478 (3.00%) 0.6488 (3.17%),0,,False
239,Ave. 0.4869 0.5237 7.77% 0.5267 8.43% 0.5237 7.78% 0.5273 8.51% 0.5239 7.83% 0.5233 7.70% 0.5263 8.36% 0.5233 7.75%,0,,False
240,"to calculate dissimilarity weights, we use all the documents in each component result.",0,,False
241,"From Tables 2 and 3, we can see that all the data fusion methods involved perform better than the best component result. However, improvement rates vary from one year group to another. For all the data fusion methods involved, the largest improvement of over 10% occurs in the 2009 year group, which is followed by the 2010 year group with improvement between 5% and 11%, while the smallest improvement of less than 4% occurs in the 2011 year group. According to [8], the target variable of performance improvement of the fused result over the best component result is affected by a few factors. Among other factors, the variance of performance of all the component results and the performance of the best component result (see Table 1) have negative effect on the target variable. This can partially explain what we observe: all data fusion methods do the best in the 2009 data set, the worst in the 2011 data set, and the medium in the 2010 data set.",0,,False
242,"Intuitively, such a phenomenon is understandable. If a component result is very good and a large percentage of relevant documents in multiple categories are retrieved and top-ranked, then it must be very difficult to make any further improvements over this result; on the other hand, if some of the results are much poorer than the others, then it is very difficult for the fused result to outperform the best component result. Anyway, in all 3 data sets, all of the fused results exhibit improvements over the best component result.",0,,False
243,"If we compare performance-related weights to combined weights, it is not always the case that combined weights can achieve greater improvement. However, if we examine the greatest improvement in each case, it always happens",0,,False
244,"when some form of combined weights is used. On average over three year groups, dis  p2(Eq.1) performs the best no matter if ERR-IA@20 or -nDCG@20 is used for evaluation. This suggests that dis  p2(Eq.1) is a very good option for the combined weight.",0,,False
245,4. CONCLUSIONS,0,,False
246,"In this short paper we have reported our investigation on the search result diversification problem via data fusion. Especially we focus on the linear combination method. Two options of calculating dissimilarity weights and several options of combining performance-related weights and dissimilarity-related weights have been proposed. Experiments with 3 groups of results submitted to the TREC web diversity task show that all the data fusion methods perform well and better than the best component result. Among those methods proposed, a combined weight of square performance and dissimilarity (calculated by comparing ranking difference of pair-wise results) outperforms the others on average.",1,TREC,True
247,"In summary, the experiments demonstrate that data fusion is still a useful technique for performance improvement when addressing search result diversification.",1,ad,True
248,5. REFERENCES,0,,False
249,"[1] E. Aktolga and J. Allan. Sentiment diversification with different biases. In Proceedings of the 36th Annual International ACM SIGIR Conference, pages 593­602, Dublin, Ireland, July 2013.",0,,False
250,"[2] G. V. Cormack, C. L. A. Clarke, and S. Bu¨ttcher. Reciprocal rank fusion outperforms Condorcet and individual rank learning mthods. In Proceedings of the 32nd Annual International ACM SIGIR Conference, pages 758­759, Boston, MA, USA, July 2009.",0,,False
251,"[3] V. Dang and W. B. Croft. Term level search result diversification. In Proceedings of the 36th Annual International ACM SIGIR Conference, pages 603­612, Dublin, Ireland, July 2013.",0,,False
252,"[4] Z. Dou, K. Chen, R. Song, Y. Ma, S. Shi, and J. Wen. Microsoft Research Asia at the web track of TREC 2009. In Proceedings of The Eighteenth Text REtrieval Conference, Gaithersburg, Maryland, USA, November 2009.",1,TREC,True
253,"[5] R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (Volumn 2), pages 1137­1145, Montreal, Canada, August 1995.",1,ad,True
254,"[6] J. H. Lee. Analysis of multiple evidence combination. In Proceedings of the 20th Annual International ACM SIGIR Conference, pages 267­275, Philadelphia, Pennsylvania, USA, July 1997.",1,ad,True
255,"[7] R. McCreadie, C. Macdonald, R. Santos, and I. Ounis. University of Glasgow at TREC 2011: Experiments with terrier in crowdsourcing, microblog, and web tracks. In Proceedings of The Twentieth Text REtrieval Conference, Gaithersburg, Maryland, USA, November 2011.",1,ad,True
256,"[8] S. Wu and S. McClean. Performance prediction of data fusion for information retrieval. Information Processing & Management, 42(4):899­915, July 2006.",0,,False
257,830,0,,False
258,,0,,False

,sentence,label,data,regex
0,Query-Performance Prediction: Setting the Expectations Straight,1,Query,True
1,Fiana Raiber fiana@tx.technion.ac.il,0,,False
2,Oren Kurland kurland@ie.technion.ac.il,0,,False
3,"Faculty of Industrial Engineering and Management, Technion -- Israel Institute of Technology",0,,False
4,ABSTRACT,0,,False
5,"The query-performance prediction task has been described as estimating retrieval effectiveness in the absence of relevance judgments. The expectations throughout the years were that improved prediction techniques would translate to improved retrieval approaches. However, this has not yet happened. Herein we provide an in-depth analysis of why this is the case. To this end, we formalize the prediction task in the most general probabilistic terms. Using this formalism we draw novel connections between tasks -- and methods used to address these tasks -- in federated search, fusion-based retrieval, and query-performance prediction. Furthermore, using formal arguments we show that the ability to estimate the probability of effective retrieval with no relevance judgments (i.e., to predict performance) implies knowledge of how to perform effective retrieval. We also explain why the expectation that using previously proposed query-performance predictors would help to improve retrieval effectiveness was not realized. This is due to a misalignment with the actual goal for which these predictors were devised: ranking queries based on the presumed effectiveness of using them for retrieval over a corpus with a specific retrieval method. Focusing on this specific prediction task, namely query ranking by presumed effectiveness, we present a novel learning-to-rank-based approach that uses Markov Random Fields. The resultant prediction quality substantially transcends that of state-of-the-art predictors.",1,ad,True
6,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,0,,False
7,"Keywords: query-performance prediction, learning-to-rank",0,,False
8,1. INTRODUCTION,1,DUC,True
9,The query-performance prediction task has attracted much research attention [11]. The goal of the task has been stated as estimating the effectiveness of retrieval performed in response to a query with no relevance judgments [11]. An important claim put forth for motivating the development,0,,False
10,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609581.",1,ad,True
11,of query-performance predictors was that using them would help to improve retrieval effectiveness.,0,,False
12,"However, the expectation that improved prediction methods would translate to improved retrieval approaches has not been realized. One of our goals here is to explore why this is the case. To this end, we formalize the prediction challenge in the most general probabilistic terms. Using this formalism we draw novel connections between tasks and methods used to address them in federated search [10], fusion-based retrieval [14] and query-performance prediction [11].",1,ad,True
13,"We formally show that the ability to successfully estimate the probability of effective retrieval with no relevance judgments (i.e., to predict performance) directly implies knowledge of how to perform effective retrieval. Thus, at the conceptual level, it should come as no surprise that improved prediction methods do not lead to improved retrieval methods. On the practical level, we make the observation that previously proposed query-performance prediction methods -- specifically, those analyzing the retrieved list -- were designed as approaches to ranking queries based on presumed retrieval effectiveness rather than as direct estimates for the probability of effective retrieval. Nevertheless, we argue for why even in their capacity as query ranking techniques the most effective among the proposed predictors were not, and cannot, be used to improve retrieval effectiveness. The core issue is that these predictors are by design effective only in ranking queries for specific retrieval methods.",1,ad,True
14,"Given that previously proposed query-performance predictors are essentially query ranking methods, we present a learning-to-rank approach based on Markov Random Fields for query-performance prediction. The resultant prediction quality substantially transcends that of state-of-the-art predictors. We then use our approach in a fusion-based retrieval setting to further support our claim that the expectation that using query-performance predictors would help to improve retrieval effectiveness is somewhat unrealistic. These predictors should be examined and evaluated with respect to the task they were designed for: estimating for which queries a retrieval performed over a given corpus with a given retrieval method would result in better performance.",0,,False
15,2. RELATED WORK,0,,False
16,"There are three main lines of work on query-performance prediction. The first is devising pre-retrieval prediction methods that are based only on the query and corpus-based statistics [15, 25, 42, 37, 23, 53, 41]. The second is that on post-retrieval prediction approaches that analyze also the retrieved list [11]. Specifically, some methods measure the",0,,False
17,13,0,,False
18,"clarity of the list with respect to the corpus [15, 16, 2, 24]. Others quantify different notions of the robustness of the list [52, 51, 54, 4, 40]. There are post-retrieval predictors that rely on analysis of the retrieval scores distribution in the list [50, 9, 19, 55, 6, 38, 17, 18, 46].",0,,False
19,"The third line of work is on prediction frameworks. One framework is based on measuring distances between query representations, relevant documents, and the corpus [12]. In another framework the prediction challenge was casted as a utility estimation task [45]. A probabilistic framework was proposed to explain the common grounds of various (preand post- retrieval) prediction methods [29]. The basic probability used in this work to derive the framework emerges as a specific case of the formalism we present here. In a similar vein, various post-retrieval predictors were presented as instantiations of a simple prediction principle [28]; namely, comparing the retrieved list with a pseudo (in)effective list.",0,,False
20,"In contrast to all this previous work, we formally show that the query-performance prediction task which has been addressed is a specific instance of a much more general prediction challenge. Accordingly, we establish novel connections between various prediction tasks. We also study the reasons to why improved query-performance prediction has not translated to improved novel retrieval methods.",1,ad,True
21,"It was argued using a simulation [21] that the state-ofthe-art prediction quality should much improve to allow for effective selective query expansion [2, 16]. We provide here reasons to the potential difficulty in substantially improving prediction quality by connecting the ability to predict performance with the knowledge to perform effective retrieval.",0,,False
22,"Our learning-to-rank framework for query-performance prediction integrates previously proposed pre- and post- retrieval predictors. Pre-retrieval predictors were integrated using linear regression [22]. In contrast to our work, postretrieval predictors which (as we show) are more effective than pre-retrieval predictors were not integrated. More generally, the prediction task was not treated as a ranking problem. Post- and pre- retrieval predictors were also integrated in a probabilistic framework [29]. In contrast to our learningto-rank framework, only two post-retrieval predictors and one pre-retrieval predictor could be integrated; and, the integration was non parametric. Hence, the relative prediction quality of the predictors, as determined using a train set of queries, could not be utilized. Linear interpolation of only two post-retrieval predictors was also proposed [51, 55, 45, 28], but without applying a learning-to-rank approach.",0,,False
23,3. THE PREDICTION TASK,0,,False
24,"The query performance prediction task was stated as ""estimating the effectiveness of a search performed in response to a query in the absence of relevance judgements"" [15, 11]. To formalize the task in the most general terms, we use the following notation. Q, C and M are random variables that take as values queries, document corpora, and retrieval methods, respectively. A retrieval method gets as input a query and a corpus of documents and produces a ranking over the corpus. The random variable R is assigned with a value 1 if the retrieval was effective and 0 otherwise. Accordingly, predicting retrieval effectiveness amounts to estimating",1,corpora,True
25,"p(R ,"" 1|Q, C, M ).""",0,,False
26,(1),0,,False
27,Different assignments to the random variables in Equation 1 result in different prediction tasks that we discuss below.,0,,False
28,"These tasks were not necessarily referred to in past work as prediction, nor derived from the same basic formalism. Table 1 summarizes these tasks.",0,,False
29,3.1 Prediction over corpora,1,corpora,True
30,"In the federated retrieval setting [10, 44], a set of corpora, {ci}, can be used for retrieval. The resource selection problem is to select a subset of these corpora to perform the retrieval on [10, 44]. This task can be performed by addressing a prediction challenge over corpora; specifically, estimating for each corpus c the probability that using a given query q in some retrieval method upon this corpus will result in effective retrieval. Formally, we fix Q ,"" q, and for each c ( {ci}) estimate p(R "", 1|Q ,"" q, C "","" c, M ), for any assignment of M . We re-write this probability as""",1,corpora,True
31,"pq(R , 1|C ,"" c),""",0,,False
32,(2),0,,False
33,to emphasize the fact that the query is fixed and that the estimation is performed across corpora and independently of a retrieval method.,1,corpora,True
34,"In practice, the probability in Equation 2 is not estimated directly. Rather, the corpora are ranked in response to the query using some form of corpora representation [10, 44].",1,corpora,True
35,3.2 Prediction over retrieved lists,0,,False
36,"The fusion task is merging lists that were retrieved from the same corpus in response to a query [14]. The lists differ due to the retrieval methods employed to produce them; e.g., the document representation, the query representation, and the ranking function can be varied [14].",0,,False
37,"A long standing challenge in work on fusion is estimating the effectiveness of the lists to be fused in lack of relevance judgements [3]. Such estimates could be used, for example, to weigh the lists when linearly fusing them [3, 31, 43]; or, to select a single list as the final result list [2, 16, 34, 5].",0,,False
38,"Thus, estimating list effectiveness in the fusion setting is a prediction task performed over retrieval methods, and consequently, over retrieved lists. That is, the choice of a retrieval method for a given pair of a query and a corpus entails a ranking. Formally, we fix Q , q and C ,"" c as retrieval is performed over a single corpus. Then, for each retrieval method m we follow Equation 1 and estimate p(R "", 1|Q ,"" q, C "","" c, M "", m). Doing that amounts to estimating p(R , 1|Q ,"" q, C "","" c, L "","" l) for each retrieved document list l, where L is a random variable that takes as values such lists. We write this probability as""",0,,False
39,"pq;c(R , 1|L ,"" l),""",0,,False
40,(3),0,,False
41,"to emphasize the fact that both the query and the corpus are fixed, and the estimation is across retrieved lists.",0,,False
42,"In previous work on fusion, the probability in Equation 3 was not directly estimated. Rather, the estimation task was often replaced with a ""ranking retrieved lists with respect to a query"" task for a fixed query and corpus. List ranking was performed based on past performance of the retrieval methods (e.g., [3, 31]), features assumed to be correlated with list effectiveness [34, 5, 43], or the similarity between the list and the ""average list"" among those available [47, 19].",0,,False
43,3.3 Prediction over queries,0,,False
44,"Heretofore, we used the term ""prediction"" to refer to the tasks described above as they are manifestations of the challenge of estimating Equation 1 with no relevance judgments.",0,,False
45,14,0,,False
46,prediction task,0,,False
47,target probability,0,,False
48,fixed,0,,False
49,fundamental,0,,False
50,"p(R ,"" 1|Q, C, M )""",0,,False
51,none,0,,False
52,over corpora,1,corpora,True
53,"pq (R , 1|C , c)",0,,False
54,Q,0,,False
55,over lists,0,,False
56,"pq;c(R , 1|L , l)",0,,False
57,"Q, C",0,,False
58,"(pre-) over queries pc(R , 1|Q , q)",0,,False
59,C,0,,False
60,"(post-) over queries pc(R , 1|Q ,"" q, L "", l) C",0,,False
61,varied no effect,0,,False
62,none none,0,,False
63,C,0,,False
64,L,0,,False
65,L,0,,False
66,none,0,,False
67,Q,0,,False
68,L,0,,False
69,"Q, L none",0,,False
70,"Table 1: Summary of the prediction tasks. ""target probability"": the probability to be estimated using no relevance judgments; ""fixed"", ""varied"", ""no effect"": the random variables whose values are (i) fixed, (ii) varied (and consequently, over which prediction is performed), and (iii) has no effect on the estimation, respectively. A triplet of a query (Q), corpus (C) and retrieval method (M ) entails a retrieved list (L). ""pre-"" and ""post-"" stand for preretrieval and post-retrieval, respectively.",0,,False
71,"However, these tasks were not referred to as ""prediction"" in past literature. In fact, almost all methods that were proposed in past work on query-performance prediction address (either explicitly or implicitly) the challenge of prediction over queries. These methods can be categorized into the two classes that we discuss next [11].",1,ad,True
72,Pre-retrieval prediction. The first class of prediction over,0,,False
73,"queries approaches is that of pre-retrieval prediction. Given a query and a fixed corpus of documents, retrieval effectiveness is estimated before retrieval is employed and independently of a retrieval method [15, 25, 42, 37, 24, 53, 41]. Then, the estimates are compared across queries to predict which queries will result in more effective retrieval. Formally, C is fixed to c, and following Equation 1 the task becomes estimating p(R , 1|Q ,"" q, C "","" c, M ). To emphasize the fact that the corpus is fixed, and that the estimation is across queries and independent of a retrieval method (and therefore of a retrieved list), we write the probability as:""",0,,False
74,"pc(R , 1|Q , q).",0,,False
75,(4),0,,False
76,"As was the case for the prediction tasks described in Sections 3.1 and 3.2, Equation 4 was not directly estimated in past work [15, 25, 42, 37, 24, 53, 41].1 Rather, methods for ranking queries with respect to a corpus have been devised.",0,,False
77,"We note the interesting connection between Equations 2 and 4. Both are specific instantiations of Equation 1. In the prediction over corpora case (Equation 2), the query is fixed and the corpora are varied. In the pre-retrieval prediction over queries case (Equation 4), the corpus is fixed and the queries are varied. In both cases, the estimation (prediction) is for any retrieval method. Perhaps not surprisingly, then, effective methods for ranking corpora and queries (in a pre-retrieval mode) bear much resemblance. For example, the CORI resource selection approach for federated search [10] ranks corpora with respect to the query using a tf.idf-based similarity measure. The SCQ pre-retrieval over queries predictor scores queries with respect to a corpus also using a tf.idf-based similarity measure [53]. This connection between the formal tasks of resource selection and pre-retrieval prediction over queries, and methods used to address them, is novel to this study.",1,corpora,True
78,"1Pre-retrieval predictors were used in a linear regression to predict average precision [22], but the probability of effective retrieval was not directly estimated.",0,,False
79,Post-retrieval prediction. Methods referred to as post-,0,,False
80,"retrieval query-performance predictors estimate retrieval effectiveness based on the query, the corpus, and the retrieved list [15, 2, 16, 50, 9, 52, 12, 51, 54, 4, 19, 55, 24, 6, 45, 17, 18, 46]. The goal is to compare estimates across queries as is the case for pre-retrieval prediction that was discussed above. We can formalize the task as follows. We fix a corpus, C ,"" c. Then, following Equation 1 we estimate for each pair of a query (q) and a retrieval method (m) p(R "", 1|Q ,"" q, C "","" c, M "","" m). Given a corpus, a pair of a query and a retrieval method entails a retrieved list, l. Furthermore, most work on post-retrieval prediction has focused on analyzing the retrieved list, rather than the retrieval method itself.2 Thus, the probability of interest is:""",0,,False
81,"pc(R , 1|Q ,"" q, L "", l).",0,,False
82,(5),0,,False
83,"Equation 5 served as the basis for deriving a probabilistic framework that was used to explain the commonalities between prediction over queries methods [29]. Virtually all previously proposed post-retrieval predictors do not directly estimate Equation 5. Instead, scoring methods are devised for pairs of a query and a retrieved list. The scores are used to rank the queries by presumed retrieval effectiveness.",1,ad,True
84,"We also note the fundamental difference between Equations 5 and 3. In post-retrieval prediction over queries (Equation 5) estimation is performed across pairs of query and retrieved list. In the fusion setting (Equation 3), the query is fixed and estimation is performed across retrieved lists. We come back to this point below.",0,,False
85,3.4 Improved prediction  improved retrieval?,0,,False
86,"Work on query-performance prediction -- i.e., prediction over queries -- based the motivation for devising predictors on the claim that improved prediction would help to improve retrieval effectiveness [11, Chapter 1]. However, there was very little empirical support to this claim.3 We now explore the gap between the claim and the lack of its realization.",0,,False
87,"Consider Equation 1 that was used to derive the prediction tasks. Suppose that we fix the query, Q ,"" q, and that the corpus contains a single document d which is the only one that can be retrieved by any retrieval method. Accordingly, the prediction task becomes estimating p(R "", 1|Q ,"" q, D "","" d); D is a random variable that takes documents as values. That is, the prediction task amounts to the core estimation challenge in probabilistic retrieval: what is the probability that this document is relevant to this query? [49]. Thus, we arrive to the following argument:""",0,,False
88,"The ability to successfully estimate the probability of effective retrieval with no relevance judgments (i.e., predict performance) directly implies knowledge of how to perform effective retrieval.",0,,False
89,"2There is work on predicting retrieval effectiveness based on statistics of features used by the retrieval method [5, 6]. 3Small performance improvements were attained when using (i) post-retrieval predictors in federated search [52] and (ii) pre-retrieval predictors in a learning-to-rank framework [35]. Post-retrieval predictors were also integrated with many (ranker specific) features to select a ranking function [5] (with somewhat inconclusive findings about the resultant effectiveness) and for fusion [43] (with small relative contribution to overall performance). Selective query expansion using predictors was not shown to be of much merit [2, 16].",0,,False
90,15,0,,False
91,"A potential criticism about the argument just posed is that prediction methods are presumably more successful in estimating retrieval effectiveness for a retrieved document set than for a single document, because they exploit properties of such sets. To address this criticism, we can set the corpus in Equation 1 to contain only a cluster of similar documents. Work on ranking document clusters in response to a query [33, 39] showed that effective estimates for cluster relevance translate to improved document ranking. Thus, we get further support to the argument that the ability to predict the effectiveness of a retrieved document set implies knowledge of performing effective document retrieval.4",1,ad,True
92,"Another potential attack on the argument is that the inability to accurately estimate the probability of effective retrieval does not necessarily imply inability to effectively rank the items of concern. Indeed, in Sections 3.1, 3.2 and 3.3 we made the observations that in most cases the probability for effective retrieval was not estimated directly, but rather ranking approaches have been employed.",1,ad,True
93,"The situation just described is reminiscent of that in ad hoc retrieval. In probabilistic retrieval methods [49], the probability of document relevance is rarely directly estimated. Instead, quantities which are presumably rank equivalent to this probability are used to rank documents.",1,ad,True
94,"Hence, we re-visit the above mentioned motivation for devising prediction over queries methods from a ranking point of view. A recurring example for using these predictors to improve retrieval effectiveness -- which was not backed up empirically -- was switching a retrieval method if retrieval was predicted to fail [11]. Presumably, there is a task mismatch: post-retrieval over queries predictors rank pairs of a query and a retrieved list, while the suggested task is ranking retrieved lists for a given query (prediction over retrieved lists).5 However, if we fix the query in post-retrieval over queries prediction methods we should presumably get retrieved-lists ranking methods. More generally, fixing the query in Equation 5 results in the post-retrieval over queries prediction task becoming the prediction over retrieved lists task (Equation 3).",0,,False
95,"Yet, as it turns out, the most effective previously proposed post-retrieval over queries predictors are (either explicitly or implicitly) coupled with, and therefore effective for, a specific retrieval method or family of retrieval methods. We explain why this is the case in Appendix A. Thus, it should not be expected that these prediction methods could help to select a retrieved list for a given query. In other words, these predictors are essentially query ranking functions which exploit information induced from the retrieved list and which are based on a specific retrieval approach. Indeed, the evaluation of the prediction quality of these methods in past work was based on fixing the corpus and the retrieval method -- often, to a standard document ranking approach that uses document-query surface-level similarities -- and measuring the effectiveness of the query ranking [11].",0,,False
96,"We draw the following conclusions. The ability to directly estimate, with no relevance judgments, the probability for effective retrieval implies the knowledge of how to perform effective retrieval. Thus, at this conceptual level, it should",0,,False
97,4The connection between query-performance prediction and cluster ranking was also stated elsewhere [27]. 5Pre-retrieval query-performance predictors are independent of the retrieved list and therefore cannot be used alone to select a retrieved list.,0,,False
98,"not be expected that improved prediction would translate to novel improved retrieval methods. On the practical level, the prediction tasks discussed above have been addressed using ranking functions rather than by directly estimating the probability for effective retrieval. Specifically, the most effective post-retrieval over queries predictors are query ranking methods that are committed to specific retrieval approaches. Hence, it should not be expected that using these predictors would help in selecting a retrieved list for a query so as to improve retrieval performance.",1,ad,True
99,3.5 Learning to rank queries using Markov Random Fields,0,,False
100,"With the realization that previously proposed prediction over queries methods (a.k.a. query-performance predictors [11]) -- both pre-retrieval and post-retrieval -- are essentially query ranking methods, we now turn to tackle the prediction over queries task using a learning-to-rank approach.",0,,False
101,"Inspired by Equation 1, and given that the choice of a query and a retrieval method entails a retrieved list for a given corpus, we set as a goal to estimate",0,,False
102,"p(R ,"" 1, Q, C, L).""",0,,False
103,(6),0,,False
104,"This is the joint probability for a relevance event and a triplet of a query (Q), a corpus (C), and a document list (L) retrieved for the query from the corpus.",0,,False
105,"We estimate Equation 6 using Markov Random Fields (MRFs). An MRF is defined over a graph G with nodes representing random variables and edges representing dependencies between the variables. The nodes of the graph here are the query (Q), the corpus (C), and the retrieved list (L). As is standard practice in using MRFs to rank documents [36], we omit the relevance random variable (R) from the graph to simplify the formulation. This has no effect on the resultant query ranking. Accordingly, we re-write Equation 6 as pR,""1(Q, C, L). This is the joint probability over G's nodes which can be written as:""",0,,False
106,"pR,""1(Q, C, L)""",0,,False
107,",",0,,False
108,1 Z,0,,False
109,Y,0,,False
110,x(x);,0,,False
111,(7),0,,False
112,xX (G),0,,False
113,X(G) are the cliques in G; x(x) is a potential (positive),0,,False
114,function,0,,False
115,defined,0,,False
116,over,0,,False
117,the,0,,False
118,clique,0,,False
119,x;,0,,False
120,"Z ,""PQ,C,L""",0,,False
121,Q,0,,False
122,xX (G),0,,False
123,x(x),0,,False
124,is the normalization factor. Computing this factor is a very,0,,False
125,"hard task. However, since our goal is to rank queries, and",0,,False
126,"the normalization factor does not affect this ranking, we do",0,,False
127,not compute it. This is yet another example for the transi-,0,,False
128,tion mentioned in Section 3.4 from devising direct estimates,0,,False
129,of probabilities to applying rank-equivalence manipulations,0,,False
130,that yield ranking functions.,0,,False
131,We use the standard instantiation of potential functions,0,,False
132,"[36]: x(x) d,""ef exp(xfx(x)), where fx(x) is a feature func-""",0,,False
133,tion defined over the clique x and weighted by the parameter,0,,False
134,"x. Using the feature functions in Equation 7, omitting the",0,,False
135,"normalization factor, and applying a log transformation, we",0,,False
136,arrive to a linear ranking function for queries:,0,,False
137,"Score(Q; C, L) d,ef",0,,False
138,X xfx(x).,0,,False
139,(8),0,,False
140,xX (G),0,,False
141,Most linear learning-to-rank methods are based on a linear combination of feature functions as in Equation 8 [32]. Our motivation to address the query ranking challenge using MRFs is the correspondence between the graph structure,1,ad,True
142,16,0,,False
143,and the types of information used by previously proposed query-performance prediction approaches that serve for feature functions in Section 3.5.1.,0,,False
144,"Each feature function fx(x) that we use in Equation 8 is defined as log(hx(x) + ), where h is one of the feature functions defined below;  , 10-10 is a smoothing factor. This results in a conjunction style integration of feature functions similarly to work on using MRFs for document [36] and cluster [39] ranking. In Section 4.1.1 we use SVMrank [26] to learn the values of the x's in Equation 8.",0,,False
145,3.5.1 Cliques and feature functions,0,,False
146,"We consider four different cliques in the graph G. These are depicted in Figure 1. In what follows, we describe the cliques and the feature functions associated with them. All feature functions are previously proposed query-performance predictors. Using these enables us to (i) demonstrate their correspondence with cliques; and, (ii) study whether a learningto-rank method can effectively integrate them.",0,,False
147,The xQC clique. This clique is composed only of the query,0,,False
148,"and the corpus. Thus, it corresponds to pre-retrieval queryperformance predictors that do not analyze the retrieved list. We use three different measures for a query term that utilize corpus-based information. Then, we apply three functions: Sum, Average and Max, to aggregate the measures across the query terms. The first measure is the tf.idf-based similarity between a term and the corpus computed using the SCQ predictor [53]. The second measure is the variance of the tf.idf values of a term over the documents in the corpus in which it appears [53], referred to as VAR. The third measure is the inverse document frequency, IDF, of the term [16]. Using the SCQ-based measure amounts to assuming that query terms similar to the corpus indicate effective retrieval. The VAR and IDF measures are estimates of the discriminative power of the query terms. All together, we have 9 feature functions of the form ""AB"" for the xQC clique, where A  {Sum,Avg,Max} and B  {SCQ,VAR,IDF}.",0,,False
149,The xL clique. This clique contains only the retrieved list.,0,,False
150,Feature functions associated with the clique reflect queryindependent properties of the list that potentially attest to retrieval effectiveness.,0,,False
151,"The first feature function measures the cohesion of the retrieved list [12, 51]. The premise is that a cohesive list is more likely to be effective than a diverse one. To measure cohesion, we compute for each document d in L its similarity with all documents in L, and average the resultant values over all the documents in L. The inter-document similarity measure is described in Section 4.1.1.",0,,False
152,The next feature functions are adopted from recent work on estimating document-list quality [40]. Three measures are defined for each document in the list. These potentially attest to the breadth of the document content. Content breadth can be thought of as a prior for document relevance [8]. The measures are then averaged over the documents in the retrieved list to yield the feature functions.,1,ad,True
153,High entropy of the term distribution in the document potentially indicates content breadth [8]. The entropy of document d is defined as -Pwd p(w|d) log p(w|d); w is a term and p(w|d) is the probability assigned to w by an unsmoothed unigram language model induced from d.,1,ad,True
154,"Figure 1: The four cliques considered for graph G. G is composed of a query node (Q), corpus node (C) and a retrieved list node (L).",0,,False
155,"Two additional feature functions are: (i) the ratio between the number of stopwords and non-stopwords in the document, sw1; and (ii) the fraction of stopwords in a stopword list (INQUERY's in our case [1]) that appear in the document, henceforth sw2. These functions were shown to be highly effective document quality measures, especially for noisy Web collections [8]. The reason is that the presence of stopwords presumably attests to the use of ""rich"" language and therefore to content breadth [8].",1,ad,True
156,The xLC clique. This clique includes the retrieved list and,0,,False
157,"the corpus but not the query. Thus, it corresponds to queryindependent properties of the retrieved list which are quantified using corpus-based information. One such property is Clarity [15]. This is the KL divergence between a relevance language model [30] induced from the list and that induced from the corpus. The premise is that a retrieved list with a language model different from that of the corpus is focused, and hence reflects effective retrieval. We also use ImpClarity which is a variant of Clarity proposed for Web corpora [24]: only the terms that appear in less than t% of the documents in the corpus are used to induce a relevance language model from the list; t is a free parameter.",1,corpora,True
158,The xQLC clique. This clique contains all the nodes in the,0,,False
159,"graph. The first two feature functions that we consider are the highly effective post-retrieval query-performance predictors WIG [55] and NQC [46]. WIG is the difference between the mean retrieval score in the list and that of the corpus which represents a pseudo non-relevant document.6 The premise is that high retrieval scores with respect to that of the corpus attest to effective retrieval. NQC is defined as the standard deviation of retrieval scores in the list. High deviation was argued to correlate with potentially reduced query drift, and thus with improved effectiveness [46].7",1,WIG,True
160,The highly effective UEF prediction framework [45] is based on re-ranking the retrieved list L using a relevance,0,,False
161,6The difference is normalized by the query length to ensure inter-query compatibility of prediction values. 7The standard deviation is normalized with respect to the corpus retrieval score to ensure inter-query compatibility of prediction values.,0,,False
162,17,0,,False
163,"language model induced from L. We use the exponent of the Pearson correlation between the scores in L and those produced by the re-ranking as a basic prediction measure. The measure is scaled by the value assigned by some basic predictor -- in our case, Clarity, ImpClarity, WIG or NQC-- to produce the final prediction value. We get 4 feature functions: UEF(Clarity), UEF(ImpClarity), UEF(WIG) and UEF(NQC). The core idea, as mentioned in Appendix A, is that the relevance-model-based re-ranking is more effective than the original ranking and can therefore serve as a reference comparison. The basic predictors serve to estimate the presumed quality of the relevance model.",1,WIG,True
164,4. EVALUATION,0,,False
165,"We start in Section 4.1 by evaluating the effectiveness of our learning-to-rank method, henceforth LTRoq, for the prediction over queries task. Then, in Section 4.2 we address the claim made in Section 3.4 that previously-proposed postretrieval over queries predictors are not effective for prediction over retrieved lists.",1,ad,True
166,4.1 Prediction over queries,0,,False
167,4.1.1 Experimental setup,0,,False
168,"The TREC datasets used for the prediction over queries experiments are specified in Table 2. WSJ, AP and ROBUST are small collections, mostly composed of news articles. WT10G is a small Web collection, and GOV2 is a crawl of the .gov domain. For the ClueWeb09 collection we used both the Category B subset, CW09B, and the English part of Category A, CW09A. To study the effect of spam documents on prediction quality, we created an additional experimental setting for each category of ClueWeb09 [13]. Specifically, documents assigned with a score below 50 (70) by Waterloo's spam classifier [13] were filtered out from the initial ranking created over the documents in CW09B (CW09A); the residual corpus ranking was used for experiments. Accordingly, we get the additional CW09BF setting for Category B and the CW09AF setting for Category A.",1,TREC,True
169,corpus,0,,False
170,WSJ AP,1,AP,True
171,ROBUST,0,,False
172,WT10G GOV2 CW09B CW09BF CW09A CW09AF,1,WT,True
173,"# of docs 173,252 242,918",0,,False
174,"528,155",0,,False
175,"1,692,096 25,205,179",0,,False
176,"50,220,423",0,,False
177,data Disks 1-2 Disks 1-3,0,,False
178,Disks 4-5 (-CR),0,,False
179,WT10g GOV2,1,WT,True
180,ClueWeb09 Category B,1,ClueWeb,True
181,queries,0,,False
182,"151-200 51-150 301-450, 600-700 451-550 701-850",0,,False
183,1-200,0,,False
184,"503,903,810 ClueWeb09 Category A 1-200",1,ClueWeb,True
185,Table 2: TREC data used to evaluate the quality of prediction over queries.,1,TREC,True
186,"Titles of TREC topics served as queries. We applied Krovetz stemming via the Indri toolkit8, which was used for experiments. Stopwords on the INQUERY list [1] were removed from queries, but not from documents.",1,TREC,True
187,"We used our approach to predict the effectiveness of the query likelihood (QL) retrieval model [48] which is often used in reports on query-performance prediction [15, 54, 24, 53, 45, 46, 29]. Dirichlet-smoothed unigram language models",0,,False
188,8www.lemurproject.org/indri,0,,False
189,are used with the smoothing parameter set to 1000. Log,0,,False
190,query-likelihood values are used for retrieval scores.,0,,False
191,"The similarity between documents di and dj, used by the",0,,False
192,"cohesion feature function, is defined as the exponent of the",0,,False
193,negative cross entropy between the unsmoothed language,0,,False
194,model of di and the Dirichlet-smoothed (with the smoothing,0,,False
195,parameter set to 1000) language model of dj.,0,,False
196,"Except for the pre-retrieval predictors, all the predictors",0,,False
197,defined as feature functions in LTRoq analyze the n most,0,,False
198,highly ranked documents in the retrieved list. To learn,0,,False
199,which values of n are better than others for each of these pre-,0,,False
200,dictors we do the following. For each feature function fx(x),0,,False
201,"defined over a clique x, instead of using a single value of n we",1,ad,True
202,use,0,,False
203,a,0,,False
204,linear,0,,False
205,combination,0,,False
206,of,0,,False
207,feature,0,,False
208,"functions,",0,,False
209,P,0,,False
210,n,0,,False
211,x;nfx;n(x);,0,,False
212,"fx;n(x) is a feature function defined exactly as fx(x), but an-",0,,False
213,alyzes only the n highest ranked documents in the list. Each,0,,False
214,"linear combination per feature function, as that just defined,",0,,False
215,is weighed by x following Equation 8.,0,,False
216,All the predictors that were used as feature functions in,0,,False
217,LTRoq serve as reference comparisons. As already men-,1,ad,True
218,"tioned, a few of these are state-of-the-art. We also use as",0,,False
219,a baseline the state-of-the-art Query Feedback (QF) post-,1,Query,True
220,retrieval predictor [55]. A relevance language model induced,0,,False
221,from the n most highly ranked documents in the retrieved,0,,False
222,list is used to rank the entire collection. The number of,0,,False
223,documents that are among the top QF documents in both,0,,False
224,the original retrieved list and that retrieved by using the,0,,False
225,relevance model serves as the prediction value; QF is a free,0,,False
226,parameter. QF was not used as a feature function in LTRoq,0,,False
227,"because it has two free-parameters, n and QF , and their",0,,False
228,effective values do not generalize well across queries [46].,0,,False
229,The reference comparison predictors are based on a single,0,,False
230,value of n as is standard. This value is set via cross val-,0,,False
231,"idation as described below. Note that in contrast, LTRoq",0,,False
232,integrates instantiations of the same predictor with various,0,,False
233,values of n as feature functions.,0,,False
234,"Following common practice [11], prediction over queries",0,,False
235,quality is measured by the Pearson correlation between the,0,,False
236,values assigned to queries by a predictor and the actual av-,0,,False
237,erage precision (AP@1000) computed for these queries using,1,AP,True
238,TREC's relevance judgments.,1,TREC,True
239,The weights associated with feature functions in LTRoq,0,,False
240,"are learned in two separate phases. We use SVMrank [26],",0,,False
241,"applied with default free-parameter values, to learn weights",0,,False
242,in each phase using the same query train set. Queries are,0,,False
243,ranked for SVMrank by the AP(@1000) attained using the,1,AP,True
244,QL retrieval method.,0,,False
245,"In the first phase, we learn independently for each feature",0,,False
246,function fx(x) the values of the x;n weights in the linear,0,,False
247,combination it forms:,0,,False
248,P,0,,False
249,n,0,,False
250,x;nfx;n(x).,0,,False
251,As the pre-retrieval,0,,False
252,"predictors are independent of n, we treat them as a sep-",0,,False
253,arate linear combination of feature functions. The weight,0,,False
254,associated with each feature function that corresponds to a,0,,False
255,"pre-retrieval predictor is learned, independently, in the first",0,,False
256,"phase. In the second phase, we learn the values of the x",0,,False
257,"weights of the linear combinations of the feature functions,",0,,False
258,including that of the pre-retrieval predictors.,0,,False
259,To learn and test the weights of the feature functions in,0,,False
260,"LTRoq, and the value of n for the reference comparison pre-",0,,False
261,"dictors, we randomly split the queries per each experimental",0,,False
262,setting into two equal-sized sets and use two-fold cross vali-,0,,False
263,dation. The train set is used to learn the feature-functions',0,,False
264,weights in LTRoq in two phases as described above. The,0,,False
265,18,0,,False
266,SumSCQ AvgSCQ MaxSCQ SumVAR AvgVAR MaxVAR SumIDF AvgIDF MaxIDF,0,,False
267,entropy sw1 sw2 cohesion,0,,False
268,Clarity ImpClarity,0,,False
269,WIG NQC UEF(Clarity) UEF(ImpClarity) UEF(WIG) UEF(NQC),1,WIG,True
270,QF,0,,False
271,LTRoq,0,,False
272,WSJ -.062,0,,False
273,.391 .409 .194 .483 .395 .015 .271 .147 -.160 -.010 -.130 .140 .551 .309 .620 .654 .611 .543 .558 .639 .462,0,,False
274,.695,0,,False
275,AP -.182,1,AP,True
276,.469 .338 .153 .645 .497 -.103 .415 .207 -.185 -.053 -.045 .279 .569 .565 .542 .526 .613 .636 .575 .552 .495,0,,False
277,.692,0,,False
278,ROBUST .072 .238 .341 .279 .407 .424 .267 .394 .429,0,,False
279,-.120 -.049 -.110 -.004,0,,False
280,.371 .453 .493 .496 .537,0,,False
281,.563 .538 .510 .457,0,,False
282,.557,0,,False
283,WT10G .141 .320 .406 .256 .312 .406 .195 .180 .233,1,WT,True
284,-.171 -.076 -.026 -.036,0,,False
285,.153 .256 .399 .405 .327 .421 .413 .435 .378,0,,False
286,.346,0,,False
287,GOV2 .267 .311 .357 .360 .361 .384 .318 .268 .287 .248 .307 .318 .232 .104 .352 .466 .336 .437 .537 .502 .417 .374,0,,False
288,.570,0,,False
289,CW09B .229 .243 .362 .275 .255 .358 .278 .237 .330 .419 .435 .419,1,CW,True
290,-.110 -.239,0,,False
291,.166 .281 .234 -.023 .265 .361 .272 .221,0,,False
292,.512,0,,False
293,CW09BF .230 .220 .331 .267 .224 .322 .268 .211 .293 .492 .409 .439,1,CW,True
294,-.040 -.197,0,,False
295,.091 .319 .406 .133 .382 .466 .458 .273,0,,False
296,.535,0,,False
297,CW09A,1,CW,True
298,.455 .151 .364,0,,False
299,.455 .170 .360 .493 .173 .380 .388 .430 .281 -.140 -.363 .030 .256 .067 -.097 .028 .063 .069 .068,0,,False
300,.457,0,,False
301,CW09AF .314 .198 .317 .313 .190 .262 .345 .207 .306 .401 .360,1,CW,True
302,.403 -.171 -.267 -.078,0,,False
303,.328 .274 -.004 .217 .375 .253 .166,0,,False
304,.422,0,,False
305,Table 3: Prediction (over queries) quality of LTRoq. `' marks a statistically significant difference with LTRoq. The best result per experimental setting is boldfaced.,0,,False
306,"value of n for the reference comparisons is learned by optimizing Pearson's correlation over the train set. The evaluation score for a split of the query set is the average prediction quality over the two test sets defined by the split. We repeat this procedure 30 times, and report the average prediction quality over the 30 splits (i.e., Pearson's correlation). For fairness of comparison, the pre-retrieval predictors which serve as reference comparisons are evaluated using the same splits, although they do not incorporate free parameters. Statistically significant differences of prediction quality are determined using the two-tailed paired t-test with p < 0.05 computed over the query set splits.",1,corpora,True
307,"To construct a relevance language model [30], which is used in Clarity, ImpClarity, UEF and QF, we used unsmoothed document language models and set the number of terms to 100 [46]; for ImpClarity we set t ,"" 1. The value of n in the prediction methods that rely on it was selected from {5, 10, 25, 50, 100, 250, 500, 1000}. For QF, we set QF to a value in {5, 10, 25, 50, 100}.""",0,,False
308,4.1.2 Experimental results,0,,False
309,"The prediction quality of the various predictors is presented in Table 3. Our first observation based on Table 3 is that, in general, the post-retrieval predictors yield prediction quality that surpasses that of the pre-retrieval predictors. A case in point, the most effective pre-retrieval predictor in an experimental setting is outperformed by the most effective post-retrieval predictor for seven out of the nine settings. The inferiority of the pre-retrieval predictors could be attributed to the fact that they are independent of the retrieved list that is evaluated.",0,,False
310,"Another observation that we make based on Table 3 is that in most cases for the ClueWeb09 settings the pre-retrieval predictors are more effective when used to predict the effectiveness of the retrieved list before suspected spam documents were filtered out, i.e., for CW09B and CW09A, rather than after they were removed (CW09BF and CW09AF). On the other hand, post-retrieval predictors are more effective when predicting the effectiveness of the retrieved list after spam documents were filtered out. This finding could poten-",1,ClueWeb,True
311,"tially be attributed to the fact that post-retrieval predictors analyze the retrieved list, while pre-retrieval predictors utilize only corpus-based query-term statistics which does not change by removing spam documents (only) from the list.",0,,False
312,"Most importantly, we see in Table 3 that for six out of the nine experimental settings, the best prediction quality is attained by LTRoq. Furthermore, LTRoq outperforms each of the reference comparisons (often to a statistically significant degree) in a vast majority of the experimental settings. LTRoq can be (sometimes statistically significantly) outperformed by some of the predictors for ROBUST, WT10G and CW09A. However, none of the predictors statistically significantly outperforms LTRoq for more than two settings. As UEF, NQC, WIG and QF are state-of-the-art methods, we conclude that LTRoq is the most effective prediction-overqueries method reported in the literature.",1,WT,True
313,Feature function analysis. We next study the importance,0,,False
314,"of the different feature functions used in LTRoq. To this end, we present in Table 4 for each experimental setting the five feature functions assigned with the highest x values by SVMrank in the second phase of training. Recall that the weights assigned to feature functions in LTRoq are learned in two phases. The weights of the pre-retrieval predictors are set in the first phase of training; the weight of the linear combination of these predictors is set in the second phase. Hence, the pre-retrieval predictors are treated here as a single group, denoted Pre.",0,,False
315,"We can see in Table 4 that, with the exception of WSJ and GOV2, the pre-retrieval predictors are always among the top-5 feature functions. In contrast, when not integrated in LTRoq, pre-retrieval predictors are often outperformed by post-retrieval predictors as shown in Table 3.",0,,False
316,"Another observation that we make based on Table 4 is that for the Web settings (WT10G, GOV2 and the four ClueWeb09 settings) at least one of the feature functions defined over the xL clique is always among the top-5 feature functions; for the newswire collections, these feature functions are never among the top-5 feature functions. This finding attests to the merits of using feature functions that",1,WT,True
317,19,0,,False
318,WSJ,0,,False
319,AP,1,AP,True
320,ROBUST,0,,False
321,WT10G,1,WT,True
322,GOV2,0,,False
323,CW09B,1,CW,True
324,CW09BF CW09A CW09AF,1,CW,True
325,UEF(NQC),0,,False
326,UEF(Clarity) NQC,0,,False
327,Pre,0,,False
328,sw2,0,,False
329,sw1,0,,False
330,sw1,0,,False
331,Pre,0,,False
332,sw1,0,,False
333,NQC,0,,False
334,UEF(ImpClarity) UEF(ImpClarity) sw1,0,,False
335,sw1,0,,False
336,sw2,0,,False
337,sw2,0,,False
338,sw1,0,,False
339,UEF(WIG),1,WIG,True
340,UEF(Clarity) Clarity,0,,False
341,Pre,0,,False
342,UEF(NQC),0,,False
343,UEF(WIG) Pre,1,WIG,True
344,UEF(WIG) cohesion entropy,1,WIG,True
345,UEF(WIG),1,WIG,True
346,UEF(NQC),0,,False
347,UEF(NQC),0,,False
348,UEF(ImpClarity) ImpClarity UEF(ImpClarity) Pre,0,,False
349,UEF(WIG) Pre,1,WIG,True
350,UEF(ImpClarity) Pre,0,,False
351,ImpClarity,0,,False
352,UEF(WIG),1,WIG,True
353,UEF(NQC) cohesion,0,,False
354,NQC,0,,False
355,NQC,0,,False
356,sw2,0,,False
357,"Table 4: The top-5 feature functions used by LTRoq for prediction over queries. ""Pre"": pre-retrieval predictors.",0,,False
358,"analyze query-independent properties of the retrieved list for prediction over queries in Web collections, as was also demonstrated elsewhere [40].",0,,False
359,"We can also see that feature functions associated with all four types of cliques used by LTRoq are represented in the top-5 feature functions. As just noted, the feature functions defined over the xL clique are among the top-5 feature functions for all Web settings. The pre-retrieval predictors which are defined over the xQC clique are almost always among the top-5 feature functions. UEF, which is defined over the xQLC clique, is always among the top-5 feature functions when instantiated with either WIG or ImpClarity. The least represented clique in the top-5 feature functions is xQL which has representatives (Clarity or ImpClarity) for only three out of the nine settings.",1,WIG,True
360,4.2 Prediction over retrieved lists,0,,False
361,"We next study the effectiveness of utilizing the feature functions defined in Section 3.5.1, and which were used by LTRoq to rank queries, for the prediction over retrieved lists task. To this end, we use a learning-to-rank approach, denoted LTRol, for ranking lists. Recall that these feature functions are previously proposed predictors over queries. Our goal in using these feature functions here is to empirically examine our claim from Section 3.4. That is, these predictors, which are essentially query ranking functions, should not be expected to be effective for ranking retrieved lists (i.e., for the prediction over retrieved lists task).",0,,False
362,4.2.1 Experimental setup,0,,False
363,Let {li} be a set of lists retrieved in response to a query q,0,,False
364,"using some retrieval methods. S(d; q, l) is the (sum-normalized)",0,,False
365,"retrieval score of document d in l ( {li}); S(d; q, l) d,ef 0 if d",0,,False
366,is not in l. Linear fusion methods [3] assign d with the score,0,,False
367,P,0,,False
368,l{li,0,,False
369,},0,,False
370,l,0,,False
371,S,0,,False
372,(d;,0,,False
373,"q,",0,,False
374,"l),",0,,False
375,where,0,,False
376,l,0,,False
377,is,0,,False
378,a,0,,False
379,(non-negative),0,,False
380,weight,0,,False
381,as-,0,,False
382,signed to list l. The prediction over retrieved lists task that,0,,False
383,we focus on here is learning the l weights. These weights,0,,False
384,should reflect the effectiveness of the lists with respect to q.,0,,False
385,Runs submitted to TREC serve as the results of different,1,TREC,True
386,retrieval methods. Details of the TREC benchmarks are,1,TREC,True
387,provided in Table 5. We randomly sample 5 runs out of all,0,,False
388,the runs submitted to a track; the runs sampling procedure,0,,False
389,is repeated 30 times. The 100 most highly ranked documents for a query in a run serve as a retrieved list to be fused.9,0,,False
390,"We use the mean average precision at cutoff 100 (MAP),",1,MAP,True
391,"and the precision of the top-5 documents (p@5), averaged",0,,False
392,"over the 30 samples, as evaluation measures. Statistically",0,,False
393,significant differences of retrieval performance are computed,0,,False
394,with respect to the average performance over the 30 samples.,0,,False
395,"The pre-retrieval predictors are not used here, neither",0,,False
396,"when applied alone nor when integrated in LTRol, as they",0,,False
397,"9Fusion methods were shown to be most effective when fusing relatively short lists [47, 7].",0,,False
398,TREC,1,TREC,True
399,TREC7 TREC8 TREC9 TREC18 TREC19 TREC20 TREC21,1,TREC,True
400,track Ad hoc,1,hoc,True
401,Web,0,,False
402,Web,0,,False
403,"# of docs 528,155 1,692,096",0,,False
404,"50,220,423",0,,False
405,data Disks 4-5 (-CR) WT10g,1,WT,True
406,ClueWeb09 Category B,1,ClueWeb,True
407,queries,0,,False
408,351-400 401-450 451-500,0,,False
409,1-50 51-100 101-150 151-200,0,,False
410,Table 5: Data used for the evaluation of fusion-based retrieval effectiveness when using prediction over retrieved lists to weigh lists.,0,,False
411,"are independent of a retrieved list and LTRol uses a linear combination of predictors. The remaining feature functions that are used in LTRol are computed for each retrieved list. While in Section 4.1 these feature functions were used to rank queries, here we use them to weigh the lists.",0,,False
412,"Since our focus here is on ranking lists rather than queries, the query-length and corpus normalization factors used in WIG and NQC to ensure across query compatibility need not be computed [46]. Thus, WIG is the mean retrieval score in a retrieved list; NQC is the standard deviation. Instead of using QL retrieval scores to weigh documents when constructing the relevance model in Clarity, ImpClarity and UEF, document retrieval scores in the lists are used [46].",1,WIG,True
413,"For reference comparison we use several state-of-the-art predictors over queries as list weights, namely WIG, NQC, UEF(WIG) and UEF(NQC). These were described in Section 3.5.1. We also report the performance of using the following three list-weighting schemes: (i) optimal weighting, OPT, where the weight assigned to a list is its actual AP(@100); (ii) UNI, where all the lists are uniformly weighted; this amounts to using the CombSum linear fusion method [20]; and, (iii) cross validation, CV, where the weight assigned to a retrieved list for queries in a test set is the MAP computed based on the queries in the train set for the same retrieval method (i.e., the same run). CV is based on the premise that past performance of a retrieval method (i.e., over train queries) indicates its performance with new queries. In contrast, all other predictors that we consider (except for UNI) weigh a list by directly estimating its effectiveness.",1,WIG,True
414,"The number of the highest ranked documents in a retrieved list (n) considered in the feature functions and reference comparisons, and the weights of the feature functions in LTRol, are set using 5-fold cross validation; query IDs are used to create the folds. The weights in LTRol are learned using SVMrank [26] in two phases as was the case for LTRoq10. AP@100 is used in the learning phase to rank the lists that are fused in the training set. Since each of the lists",1,AP,True
415,10We apply min-max normalization to the l weights which are assigned by SVMrank to the retrieved lists to avoid using negative weights.,0,,False
416,20,0,,False
417,TREC7 TREC8 TREC9 TREC18 TREC19 TREC20 TREC21,1,TREC,True
418,MAP p@5 MAP p@5 MAP p@5 MAP p@5 MAP p@5 MAP p@5 MAP p@5,1,MAP,True
419,WorstRun 318..77ucuc 247..18ucuc 124..65ucuc 215..96ucuc 164..05ucuc 197..72ucuc 309..00ucuc,0,,False
420,MedianRun,0,,False
421,1455..48ucuc 2500..29ucuc 1235..06ucuc 1301..45ucuc 11.2uc 27.3c 1239..89ucuc 1341..66ucuc,0,,False
422,BestRun,0,,False
423,21.2 59.1uc 27.2u 64.9u 19.7u 38.7u,0,,False
424,14.6 36.1 16.9u 38.8u,0,,False
425,20.3 36.7,0,,False
426,20.8 39.1,0,,False
427,OPT 2655..22ucuc 3608..55ucuc 2435..45ucuc 1483..27ucuc 2402..74ucuc 2483..45ucuc 2465..45ucuc,0,,False
428,UNI,0,,False
429,20.2c 54.4c 25.8c 60.9c 17.1c 34.8c 14.3c 36.5,0,,False
430,14.8c 28.6c 21.1c 37.7c 21.6c 37.7c,0,,False
431,CV 21.4u 56.7u 27.6u 63.2u,0,,False
432,19.0u 38.3u,0,,False
433,14.9u,0,,False
434,36.6 16.9u 34.4u 22.0u 38.5u,0,,False
435,22.4u 38.7u,0,,False
436,WIG 20.5uc 54.1c 26.6uc 61.3c,1,WIG,True
437,17.1c 34.5uc 14.2c,0,,False
438,36.3 1248..71ucuc 21.2c,0,,False
439,37.7c,0,,False
440,21.8,0,,False
441,37.2c,0,,False
442,NQC 1580..97ucuc 26.3c,0,,False
443,61.1c 1249..66ucuc 3122..45ucuc 218..55ucuc 1375..52ucuc 1384..27ucuc,0,,False
444,UEF(WIG) 20.6uc 54.1c 2661..64ucuc 17.1c 34.0uc 14.2c 36.9,1,WIG,True
445,14.6c 28.6c,0,,False
446,20.8c 37.5c,0,,False
447,21.7c 37.4c,0,,False
448,UEF(NQC),0,,False
449,1590..08ucuc 26.5uc 61.4c 1340..82ucuc 1322..35ucuc 228..05ucuc 1375..52ucuc 1384..97ucuc,0,,False
450,LTRol,0,,False
451,20.6c 57.3u 2662..55ucu,0,,False
452,17.1c 34.1c 13.9uc 35.5 16.5u 36.4uc 19.9uc 37.2c,0,,False
453,20.8c 42.3uc,0,,False
454,"Table 6: Retrieval effectiveness of using LTRol to weigh lists for linear fusion. WorstRun, MedianRun and BestRun: average performance of the worst, median and best run, among the five fused, respectively. `u' and `c' mark statistically significant differences with UNI and CV, respectively. The best result of a prediction method per experimental setting and evaluation metric is boldfaced.",0,,False
455,"to be fused is composed of 100 documents, the value of n is selected from {5, 10, 25, 50, 100}. All other implementation details are the same as those described in Section 4.1.1.",0,,False
456,4.2.2 Experimental results,0,,False
457,"The results are presented in Table 6. WorstRun, MedianRun and BestRun denote the average performance, over the 30 samples, of the worst, median and best run among the 5 runs in a sample, respectively. The substantial performance differences among the three, along with the very high OPT performance numbers, attest to the substantial merits of assigning effective weights to the lists when fusing them.",0,,False
458,"We can see that LTRol is more effective than NQC and UEF(NQC) in most relevant comparisons (track × evaluation metric). LTRol is also more effective than WIG and UEF(WIG) in about half of the relevant comparisons. Furthermore, LTRol is the only method among those using predictors over queries which in most relevant comparisons outperforms UNI (i.e., uniform weighting of lists). Nonetheless, LTRol is outperformed in most cases by CV. This means that weighing a retrieved list based on the past performance of the retrieved method that produces the list is much more effective than applying prediction-over-queries methods to the list itself to set its weight. Thus, we get empirical support to the claim from Section 3.4 that using predictors over queries to rank retrieved lists is not highly effective.11",1,WIG,True
459,5. SUMMARY,0,,False
460,"We framed the query-performance prediction task in the most general probabilistic terms. This gave rise to novel connections between tasks, and methods used to address them, in federated search, fusion-based retrieval, and previous work on query-performance prediction (i.e., prediction over queries). We formally argued that successful prediction directly implies knowledge of how to perform effective retrieval. We also argued why using previously proposed query-performance predictors was not shown to improve retrieval effectiveness. These are query ranking methods that",1,ad,True
461,"11Learning-to-rank methods were used to fuse lists [43] and to rank them [5]. Both approaches use features specific to, and shared by, the rankers employed. In the TREC setting here, the features used by the rankers are not necessarily known, nor shared. We used predictors over queries that can be applied to any retrieved list.",1,TREC,True
462,"are suited for specific retrieval methods. In addition, we devised a learning-to-rank approach for predicting performance over queries. The resultant prediction quality substantially transcends that of state-of-the-art predictors. We used our approach to show that predictors over queries are not very effective for prediction over retrieved lists.",1,ad,True
463,Acknowledgments We thank the reviewers for their comments and Yun Zhou for a discussion about the connections between prediction tasks. This work was supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center. This work was also supported in part by Microsoft Research through its Ph.D. Scholarship Program.,0,,False
464,6. REFERENCES,0,,False
465,"[1] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proc. of TREC-9, pages 551­562, 2000.",1,TREC,True
466,"[2] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. of ECIR, pages 127­137, 2004.",1,Query,True
467,"[3] C. C. V. ant Garrison W. Cottrell. Fusion via linear combination of scores. Information Retrieval, 1(3):151­173, 1999.",0,,False
468,"[4] J. A. Aslam and V. Pavlu. Query hardness estimation using Jensen-Shannon divergence among multiple scoring functions. In Proc. of ECIR, pages 198­209, 2007.",1,Query,True
469,"[5] N. Balasubramanian and J. Allan. Learning to select rankers. In Proc. of SIGIR, pages 855­856, 2010.",0,,False
470,"[6] N. Balasubramanian, G. Kumaran, and V. R. Carvalho. Predicting query performance on the web. In Proc. of SIGIR, pages 785­786, 2010.",0,,False
471,"[7] S. M. Beitzel, E. C. Jensen, A. Chowdhury, O. Frieder, D. A. Grossman, and N. Goharian. Disproving the fusion hypothesis: An analysis of data fusion via effective information retrieval strategies. In Proc. of SAC, pages 823­827, 2003.",0,,False
472,"[8] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of web documents. In Proc. of WSDM, pages 95­104, 2011.",0,,False
473,"[9] Y. Bernstein, B. Billerbeck, S. Garcia, N. Lester, F. Scholer, and J. Zobel. RMIT university at trec 2005: Terabyte and robust track. In Proc. of TREC-14, 2005.",1,trec,True
474,"[10] J. Callan. Distributed information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 5, pages 127­150. Kluwer Academic Publishers, 2000.",1,ad,True
475,"[11] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool, 2010.",1,Query,True
476,"[12] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In Proc. of SIGIR, pages 390­397, 2006.",0,,False
477,"[13] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Informaltiom Retrieval Journal, 14(5):441­465, 2011.",0,,False
478,21,0,,False
479,"[14] W. B. Croft. Combining approaches to information retrieval. In W. B. Croft, editor, Advances in information retrieval, chapter 1, pages 1­36. Kluwer Academic Publishers, 2000.",1,ad,True
480,"[15] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proc. of SIGIR, pages 299­306, 2002.",0,,False
481,"[16] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A language modeling framework for selective query expansion. Technical Report IR-338, Center for Intelligent Information Retrieval, University of Massachusetts, 2004.",0,,False
482,"[17] R. Cummins. Predicting query performance directly from score distributions. In Proc. of AIRS, pages 315­326, 2011.",0,,False
483,"[18] R. Cummins, J. M. Jose, and C. O'Riordan. Improved query performance prediction using standard deviation. In Proc. of SIGIR, pages 1089­1090, 2011.",0,,False
484,"[19] F. Diaz. Performance prediction using spatial autocorrelation. In Proc. of SIGIR, pages 583­590, 2007.",0,,False
485,"[20] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proc. of TREC-2, 1994.",1,TREC,True
486,"[21] C. Hauff and L. Azzopardi. When is query performance prediction effective? In Proc. of SIGIR, pages 829­830, 2009.",0,,False
487,"[22] C. Hauff, L. Azzopardi, and D. Hiemstra. The combination and evaluation of query performance prediction methods. In Proc. of ECIR, pages 301­312, 2009.",0,,False
488,"[23] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In Proc. of CIKM, pages 1419­1420, 2008.",0,,False
489,"[24] C. Hauff, V. Murdock, and R. A. Baeza-Yates. Improved query difficulty prediction for the web. In Proc. of CIKM, pages 439­448, 2008.",0,,False
490,"[25] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In Proc. of SPIRE, pages 43­54, 2004.",0,,False
491,"[26] T. Joachims. Training linear svms in linear time. In Proc. of KDD, pages 217­226, 2006.",0,,False
492,"[27] O. Kurland, F. Raiber, and A. Shtok. Query-performance prediction and cluster ranking: Two sides of the same coin. In Proc. of CIKM, pages 2459­2462, 2012.",1,Query,True
493,"[28] O. Kurland, A. Shtok, D. Carmel, and S. Hummel. A unified framework for post-retrieval query-performance prediction. In Proc. of ICTIR, pages 15­26, 2011.",0,,False
494,"[29] O. Kurland, A. Shtok, S. Hummel, F. Raiber, D. Carmel, and O. Rom. Back to the roots: a probabilistic framework for query-performance prediction. In Proc. of CIKM, pages 823­832, 2012.",0,,False
495,"[30] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. of SIGIR, pages 120­127, 2001.",0,,False
496,"[31] D. Lillis, F. Toolan, R. W. Collier, and J. Dunnion. Probfuse: a probabilistic approach to data fusion. In Proc. of SIGIR, pages 139­146, 2006.",0,,False
497,"[32] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.",0,,False
498,"[33] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proc. of SIGIR, pages 186­193, 2004.",0,,False
499,"[34] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.",0,,False
500,"[35] C. Macdonald, R. L. T. Santos, and I. Ounis. On the usefulness of query features for learning to rank. In Proc. of CIKM, pages 2559­2562, 2012.",0,,False
501,"[36] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proc. of SIGIR, pages 472­479, 2005.",0,,False
502,"[37] J. Mothe and L. Tanguy. Linguistic features to predict query difficulty. In ACM SIGIR 2005 Workshop on Predicting Query Difficulty - Methods and Applications, 2005.",1,Query,True
503,"[38] J. P´erez-Iglesias and L. Araujo. Standard deviation as a query hardness estimator. In Proc. of SPIRE, pages 207­212, 2010.",0,,False
504,"[39] F. Raiber and O. Kurland. Ranking document clusters using markov random fields. In Proc. of SIGIR, pages 333­342, 2013.",0,,False
505,"[40] F. Raiber and O. Kurland. Using document-quality measures to predict web-search effectiveness. In Proc. of ECIR, pages 134­145, 2013.",0,,False
506,"[41] F. Scholer and S. Garcia. A case for improved evaluation of query difficulty prediction. In Proc. of SIGIR, pages 640­641, 2009.",0,,False
507,"[42] F. Scholer, H. E. Williams, and A. Turpin. Query association surrogates for web search. JASIST, 55(7):637­650, 2004.",1,Query,True
508,"[43] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. Lambdamerge: merging the results of query reformulations. In Proc. of WSDM, pages 795­804, 2011.",0,,False
509,"[44] M. Shokouhi and L. Si. Federated search. Foundations and Trends in Information Retrieval, 5(1):1­102, 2011.",0,,False
510,"[45] A. Shtok, O. Kurland, and D. Carmel. Using statistical decision theory and relevance models for query-performance prediction. In Proc. of SIGIR, 2010.",0,,False
511,"[46] A. Shtok, O. Kurland, D. Carmel, F. Raiber, and G. Markovits. Predicting query performance by query-drift estimation. ACM Transactions on Information Systems, 30(2):11, 2012.",0,,False
512,"[47] I. Soboroff, C. K. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In Proc. of SIGIR, pages 66­73, 2001.",0,,False
513,"[48] F. Song and W. B. Croft. A general language model for information retrieval (poster abstract). In Proc. of SIGIR, pages 279­280, 1999.",0,,False
514,"[49] K. Sparck Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval: development and comparative experiments - part 1. Information Processing and Management, 36(6):779­808, 2000.",0,,False
515,"[50] S. Tomlinson. Robust, Web and Terabyte Retrieval with Hummingbird Search Server at TREC 2004. In Proc. of TREC-13, 2004.",1,Robust,True
516,"[51] V. Vinay, I. J. Cox, N. Milic-Frayling, and K. R. Wood. On ranking the effectiveness of searches. In Proc. of SIGIR, pages 398­404, 2006.",0,,False
517,"[52] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In Proc. of SIGIR, pages 512­519, 2005.",0,,False
518,"[53] Y. Zhao, F. Scholer, and Y. Tsegay. Effective pre-retrieval query performance prediction using similarity and variability evidence. In Proc. of ECIR, pages 52­64, 2008.",0,,False
519,"[54] Y. Zhou and B. Croft. Ranking robustness: a novel framework to predict query performance. In Proc. of CIKM, pages 567­574, 2006.",0,,False
520,"[55] Y. Zhou and B. Croft. Query performance prediction in web search environments. In Proc. of SIGIR, pages 543­550, 2007.",1,Query,True
521,APPENDIX,1,AP,True
522,A. POST-RETRIEVAL PREDICTION OVER,0,,False
523,QUERIES,0,,False
524,"We argue that post-retrieval prediction over queries methods (henceforth prediction methods or predictors) were coupled with, and are hence effective for, specific retrieval methods; that is, for ranking queries by the presumed effectiveness of using these retrieval methods for them.",0,,False
525,There are predictors that explicitly rely on features used by the retrieval method [6]. This reliance is implicit for other prediction methods as we discuss next.,0,,False
526,"As described in Section 2, there are three classes of postretrieval predictors. The Clarity predictor [15] was shown to be ineffective for a variety of retrieval methods [41].",0,,False
527,"Predictors that analyse the retrieval scores distribution [50, 9, 19, 55, 6, 38, 17, 18, 46] are by definition dependent on the retrieval method as noted in some work [55, 46]. In fact, almost all of these predictors are suited for, and were evaluated with, retrieval methods that use only documentquery surface-level similarities to rank documents.",0,,False
528,"Query feedback (QF) [55] and UEF [45] are among the most effective post-retrieval predictors that do not analyze retrieval scores. Both rely on the premise that retrieval using pseudo-feedback-based query expansion is more effective than that at hand. Indeed, in both reports [55, 45], the retrieval method used for evaluation was a language-modelbased approach with no query expansion. Comparison with query-expansion-based retrieval, or some other effective retrieval, need not necessarily indicate the effectiveness of retrieval methods that are much more effective [47].",1,Query,True
529,22,0,,False
530,,0,,False

,sentence,label,data,regex
0,Hypothesis Testing for the Risk-Sensitive Evaluation of Retrieval Systems,0,,False
1,B. Taner Dinçer,0,,False
2,"Dept of Statistics & Computer Engineering Mugla University Mugla, Turkey",0,,False
3,dtaner@mu.edu.tr,0,,False
4,Craig Macdonald and Iadh Ounis,1,ad,True
5,"School of Computing Science University of Glasgow Glasgow, UK",0,,False
6,{firstname.lastname}@glasgow.ac.uk,0,,False
7,ABSTRACT,0,,False
8,"The aim of risk-sensitive evaluation is to measure when a given information retrieval (IR) system does not perform worse than a corresponding baseline system for any topic. This paper argues that risk-sensitive evaluation is akin to the underlying methodology of the Student's t test for matched pairs. Hence, we introduce a risk-reward tradeoff measure TRisk that generalises the existing URisk measure (as used in the TREC 2013 Web track's risk-sensitive task) while being theoretically grounded in statistical hypothesis testing and easily interpretable. In particular, we show that TRisk is a linear transformation of the t statistic, which is the test statistic used in the Student's t test. This inherent relationship between TRisk and the t statistic, turns risk-sensitive evaluation from a descriptive analysis to a fully-fledged inferential analysis. Specifically, we demonstrate using past TREC data, that by using the inferential analysis techniques introduced in this paper, we can (1) decide whether an observed level of risk for an IR system is statistically significant, and thereby infer whether the system exhibits a real risk, and (2) determine the topics that individually lead to a significant level of risk. Indeed, we show that the latter permits a state-of-the-art learning to rank algorithm (LambdaMART) to focus on those topics in order to learn effective yet risk-averse ranking systems.",1,ad,True
9,Categories and Subject Descriptors: H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval; G3.3 [Probability and Statistics]: Experimental design,0,,False
10,"Keywords: Risk-Sensitive Evaluation, Student's t Test",0,,False
11,1. INTRODUCTION,1,DUC,True
12,"Various paradigms for the evaluation of information retrieval (IR) systems rely on many topics to produce reliable estimates of their effectiveness. For instance, in the TREC series of evaluation forums, 50 topics is generally seen as the minimum for producing a reliable test collection [2, 25]. However, in more recent times, the evaluation of systems has increasingly focused upon their robustness - ensuring that a given IR system performs well on difficult topics (as",1,ad,True
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609625 .",1,ad,True
14,"investigated by the TREC Robust track [24]), or at least as well as a baseline system (which is known as risk-sensitive evaluation [26]). Recently, the TREC 2013 Web track introduced a risk-sensitive task, which assessed how systems could perform effectively yet without exhibiting large losses compared to a pre-determined baseline system [10].",1,TREC,True
15,"In such a risk-sensitive evaluation, the risk associated with an IR system is defined as the risk of performing a given particular topic less effectively than a given baseline system [8, 9, 26]. In particular, the URisk risk-sensitive evaluation measure [26] calculates the absolute difference of an effectiveness measure (e.g. NDCG) between a given retrieval system and the baseline system, in a manner that more strongly emphasises decreases with respect to the baseline (known as risk) than gains (reward). A parameter   0 controls the riskreward tradeoff towards losses in effectiveness compared to the baseline, where  , 0 weights risk and rewards equally.",1,ad,True
16,"In this paper, we argue that in the current practice of risk-sensitive evaluation based on URisk, any amount of loss in an IR system's average effectiveness, observed on a particular set of topics, is considered enough in magnitude to infer that the system exhibits a ""real risk"". However, from a statistical viewpoint, such an inferential decision may be said to be valid only if the observed amount of loss cannot be attributed to chance fluctuation. Otherwise, it will be equally likely that the corresponding system may or may not be under a real risk, meaning that it is possible that the system can perform every topic with a score higher than that of the baseline system on another set of topics that could be drawn from the population of topics. On the other hand, it is also possible that the observed amount of loss in a particular system's average effectiveness can be attributed to a chance fluctuation, while the corresponding performance losses for some individual topics are statistically significant in magnitude. In other words, significant performance losses for a few topics may not result in a significant total loss on average, given a relatively large set of topics.",0,,False
17,"Hence, we advocate that risk-sensitive evaluation can actually provide the necessary basis for (i) testing the significance of the observed amount of loss in a given IR system's average effectiveness, called inferential risk analysis in this paper, and (ii) testing the significance of the corresponding losses for individual topics, called exploratory risk analysis.",1,ad,True
18,"Indeed, we show that the URisk risk-reward tradeoff measure is actually a linear transformation of the t statistic, as used in the Student's t test. Therefore, using this statistical interpretation of URisk based upon hypothesis testing, this paper proposes a new risk-reward tradeoff measure, TRisk, which is a linear transformation of the existing URisk measure, yet is theoretically grounded upon the Student's t test",1,ad,True
19,23,0,,False
20,"for testing the significance of the observed amount of loss in a given IR system's average effectiveness. For  ,"" 0, TRisk is equivalent to the standard t statistic used typically in the Student's t test for testing the null hypothesis of equality in the population mean effectiveness for two IR systems. However, for  > 0, the URisk measure emphasises performance losses compared to the baseline effectiveness. This raises challenges in the estimation of the standard error of the calculated URisk scores. For this reason, we propose the use of the Jackknife technique (or leave-one-out) [11], which is a re-sampling technique for estimating the bias and the standard error of any estimate. The Jackknife technique serves two purposes: firstly, to allow the empirical verification of the estimation of the standard error of URisk as valid; and secondly, for testing the significance of the corresponding performance losses for individual topics.""",0,,False
21,"From a practical perspective, a risk-sensitive evaluation serves two objectives: firstly, as a step further than the classical evaluation of IR systems, which takes into account the stability or variance of retrieval results across queries as well as for the average retrieval effectiveness [8, 9]; and secondly, as a technique for jointly optimising the retrieval effectiveness and robustness of retrieval frameworks such as learning to rank [26]. Indeed, compared to the existing URisk measure, this paper contributes to both objectives, by exploiting the theory of statistical hypothesis testing for allowing meaningful interpretation of risk-sensitive evaluation scores, and also by allowing a learning to rank technique, namely LambdaMART, to focus on those topics that lead to a significant level of risk, in order to learn effective yet risk-averse ranking systems. The remainder of this paper is structured as follows: Section 2 provides an overview of risk-sensitive evaluation practices, including URisk; Section 3 relates the URisk measure to the t statistic, and hence proposes the new TRisk risk-sensitive evaluation measure, and discusses the estimation of the standard error. Section 4 and Section 5 describe new forms of analysis, inferential and exploratory respectively, that arise from the TRisk measure, and demonstrate their application upon the TREC 2012 Web track. Next, Section 6 shows how TRisk can improve the robustness of the LambdaMART state-of-the-art learning to rank technique. Finally, we review some related work and provide concluding remarks in Sections 7 & 8, respectively.",1,ad,True
22,2. RISK-SENSITIVE EVALUATION,0,,False
23,"Different approaches in IR such as query expansion [1, 5] and learning to rank [17] behave differently across topics, often improving the effectiveness for some of the topics while degrading performance for others. This results in a high variation in effectiveness across the topics. To address such variation, there has been an increasing focus on the effective tackling of difficult topics in particular (e.g. through the TREC Robust track [23]), or more recently, on the risksensitive evaluation of systems across many topics [8, 9, 26].",1,ad,True
24,"Originally, the aim of risk-sensitive evaluation [9] was to provide new analysis techniques for quantifying and visualising the risk-reward tradeoff of any retrieval strategy that requires a balance between risk and reward. Hence, it facilitates the quest for ranking strategies that are more robust in retrieval effectiveness compared to a baseline retrieval strategy ­ robust in the sense of the stability or variance of the retrieval results across topics, while achieving good average performance over all topics.",1,ad,True
25,The variance with respect to a given baseline system b over a given set of topics Q with c topics can then be measured as,0,,False
26,"a risk function FRisk, which takes into account the downsiderisk of a new system r (i.e. performing a topic worse than the baseline) is defined in [26] as follows:",0,,False
27,FRisk,0,,False
28,",",0,,False
29,1 c,0,,False
30,c,0,,False
31,"max [0, (bi - ri)] ,",0,,False
32,(1),0,,False
33,"i,1",0,,False
34,"where ri and bi are respectively the score of the system r and the score of the baseline system b on topic i, as measured by a retrieval effectiveness measure (e.g. NDCG@20, ERR@20 [6]). Similarly, a reward function FReward, which takes into account the upside-risk (i.e. performing a topic better than the baseline) is defined as:",0,,False
35,FReward,0,,False
36,",",0,,False
37,1 c,0,,False
38,c,0,,False
39,"max [0, (ri - bi)] .",0,,False
40,(2),0,,False
41,"i,1",0,,False
42,"Thereby, the overall gain in the retrieval effectiveness of r with respect to b can be expressed as:",0,,False
43,"UGain , FReward - FRisk.",0,,False
44,(3),0,,False
45,"Next, a single measure, URisk [26], which allows the riskreward tradeoff to be adjusted, was defined:",1,ad,True
46,"URisk , UGain -  · FRisk",0,,False
47,",",0,,False
48,1 c,0,,False
49,q + (1 + ),0,,False
50,"q , (4)",0,,False
51,qQ+,0,,False
52,qQ -,0,,False
53,"where q ,"" rq - bq. The left summand in the square brackets, which is the sum of the score differences q for all q where rq > bq (i.e., q  Q+), gives the total win (or upsiderisk) with respect to the baseline. Orthogonally, the right summand, which is the sum of the score differences q for all q where rq < bq, gives the total loss (or downside-risk). The risk sensitivity parameter   0 controls the tradeoff between reward and risk (or win and loss):  "","" 0 results in a pure gain model, while for higher , the penalty for under-performing with respect to the baseline is increased: typically  "","" 1, 5, 10 [10].""",1,ad,True
54,"In this paper, we extend the original aforementioned aim of risk-sensitive evaluation with the following contributions: 1. A well-established statistical hypothesis testing theory for risk-sensitive evaluations from which arises a new risk measure TRisk (Section 3), to turn risk-sensitive evaluation from a descriptive analysis to a fully-fledged inferential analysis (Section 4). 2. A method for exploratory risk analysis that can identify the topics that commit real levels of risk (Section 5). 3. Adaptations of the proposed TRisk measure that can enhance the robustness of the state-of-the-art LambdaMART learning to rank technique, compared to URisk, without degradations in overall effectiveness, where the learned model adaptively adjusts with respect to the risk level committed by individual topics (Section 6).",1,ad,True
55,3. THE NEW TRISK MEASURE,0,,False
56,"Without loss of generality, at  ,"" 0, the risk-reward tradeoff measure URisk reduces to the UGain formula in Eq. (3), which can be expressed as the average gain over c topics:""",1,ad,True
57,UGain,0,,False
58,",",0,,False
59,1 c,0,,False
60,c,0,,False
61,i,0,,False
62,",",0,,False
63,1 c,0,,False
64,c,0,,False
65,(ri - bi).,0,,False
66,(5),0,,False
67,"i,1",0,,False
68,"i,1",0,,False
69,"In the context of statistics, UGain refers to the sample mean of paired score differences, d¯, for two IR systems (the system under evaluation r and the baseline system b):",0,,False
70,24,0,,False
71,"d¯ ,",0,,False
72,r¯ - ¯b,0,,False
73,",",0,,False
74,1 c,0,,False
75,c,0,,False
76,"(ri - bi) , UGain",0,,False
77,(6),0,,False
78,"i,1",0,,False
79,"and in the context of evaluating IR systems, this refers to the difference in average effectiveness between two IR systems, r¯-¯b, where r¯ and ¯b are respectively the average effectiveness of system r and the average effectiveness of the baseline system b over c topics.",0,,False
80,"On the other hand, the Student's t statistic for matched pairs, as is commonly applied when testing the significance of results between two systems, can be expressed as:",0,,False
81,t,0,,False
82,",",0,,False
83,d¯ SE(d¯),0,,False
84,",",0,,False
85,r¯ - ¯b SE(d¯),0,,False
86,",",0,,False
87,(7),0,,False
88,"Within Eq. (7), the standard error of paired sample mean, SE(d¯), can be estimated as follows:",0,,False
89,SE(d¯),0,,False
90,",",0,,False
91,sd c,0,,False
92,",",0,,False
93,(8),0,,False
94,"where sd , c-1 (i - d¯)2 is the paired sample standard",0,,False
95,"deviation. Hence, we argue that the Student's t statistic of",0,,False
96,Eq. (7) is actually a linear transformation of UGain from,0,,False
97,"Eq. (3), which we call TGain:",0,,False
98,TGain,0,,False
99,",",0,,False
100,UGain S E (UGain ),0,,False
101,",",0,,False
102,c sd,0,,False
103,×,0,,False
104,UGain .,0,,False
105,(9),0,,False
106,"This transformation can be referred to as studentisation (c.f., t-scores) [14], which in fact is a type of standardisation (i.e., z-scores). Standardisation is a monotonic linear transformation, which transforms any given set of data to a set with zero mean and unit variance, while preserving the original data distribution in shape.",0,,False
107,"The t-score of a raw UGain measurement, TGain, differs from the raw measurement in two important aspects. First, given a set of IR systems, a test collection, and a baseline system, the systems' ranking to be obtained on the basis of TGain will not necessarily be concordant with the systems' ranking to be obtained on the basis of UGain, since the t statistic takes into account the inherent variation in the observed paired score differences ri - bi across the topics, i.e., SE(UGain). Second, given a particular baseline system, the two TGain scores to be obtained on two different test collections for the same IR system are comparable with each other in magnitude, at least in theory [7], while the two UGain scores are not, as typical in the case of the two raw effectiveness scores to be yielded from a standard effectiveness measure, such as mean average precision [28].",0,,False
108,"Having shown how TGain can be defined as a linear transformation of UGain, based upon the t statistic, we now examine URisk, which allows the risk-reward tradeoff to be controlled by the  parameter. For   0, the t statistic based on URisk, which we call TRisk, can be expressed as follows:",1,ad,True
109,TRisk,0,,False
110,",",0,,False
111,URisk S E (URisk,0,,False
112,),0,,False
113,.,0,,False
114,(10),0,,False
115,"Although both the TGain formula in Eq. (9) and the TRisk formula in Eq. (10) stem from the classical t statistic in Eq. (7), the estimation of the standard error in URisk, the estimation of SE(URisk) within TRisk, is not as straightforward as in the case of SE(UGain), for the reason that the URisk formula reweighs the score differences i in averaging, proportionally to , for each topic i where ri < bi, as opposed to UGain. Hence, in the remainder of this section, we propose two methods to estimate SE(URisk): A",0,,False
116,"speculative parametric estimator SEx¯ that is an analogy to the paired sample standard deviation sd (Section 3.1); and a nonparametric Jackknife Estimator SEJ , based on the leaveone-out Jackknife technique (Section 3.2). Indeed, later in Section 3.3, we use the Jackknife Estimator SEJ to show the validity of the speculative SEx¯ estimator.",0,,False
117,"On the other hand, TRisk has several advantages over URisk. Firstly, it can be easily interpreted for an inferential analysis of risk. Indeed, we will later show in Section 4 that in order to test the significance of an observed riskreward tradeoff score between a particular IR system and a provided baseline system, one can use TRisk as the test statistic of the Student's t test for matched pairs.",1,ad,True
118,"Secondly, TRisk permits the identification of topics that commit significant risk or not ­ we call this exploratory risk analysis ­ which we present later in Section 5.",0,,False
119,"Finally, this exploratory risk analysis leads to new risksensitive measures that can be directly integrated into the LambdaMART learning to rank technique, to produce learned models that exhibit less risk than those obtained from URisk whilst not degrading effectiveness, as explained in Section 6.",1,ad,True
120,3.1 Parametric Estimator of SE(URisk),0,,False
121,Let the random variable Xi denote the risk-reward tradeoff score between system r and baseline b for topic i:,1,ad,True
122,"Xi ,",0,,False
123,i (1 + )i,0,,False
124,if ri > bi if ri < bi,0,,False
125,(11),0,,False
126,"for i ,"" 1, 2, . . . , c and a predefined value of   0. Then, the""",0,,False
127,"standard error of URisk, SE(URisk) can be approximated by the standard error of the sample mean x¯:",0,,False
128,SEx¯,0,,False
129,",",0,,False
130,sx c,0,,False
131,",",0,,False
132,(12),0,,False
133,"where s2x ,"" c-1 (xi - x¯)2. Here, the sample mean x¯ corresponds to the URisk score considered as the arithmetic mean of the sample of the observed individual topic risk-reward""",0,,False
134,"tradeoff scores x1, x2, . . . , xc at a predefined value of :",1,ad,True
135,x¯,0,,False
136,",",0,,False
137,URisk,0,,False
138,",",0,,False
139,1 c,0,,False
140,c,0,,False
141,xi.,0,,False
142,"i,1",0,,False
143,(13),0,,False
144,"This parametric estimator of SE(URisk), SEx¯, is speculative and hence its validity might be compromised to some extent. Therefore, we empirically verify the validity of SEx¯ in estimating SE(URisk) by means of comparing it with a nonparametric re-sampling technique, called the Jackknife [21], which we present in Section 3.2. Indeed, by comparing the two estimates of SE(URisk) (i.e., the parametric estimate SEx¯ of Eq. (12) and the nonparametric Jackknife estimate of SE(URisk)), one can decide whether an inference to be made on the basis of the TRisk statistic is valid. If the two estimates agree with each other, such an inference may be said to be valid, otherwise its validity is compromised.",1,ad,True
145,3.2 Jackknife Estimate of SE(URisk),0,,False
146,"In this paper, the Jackknife technique is employed for a purpose which serves two different aims: 1) as a mechanism of the empirical verification of the validity of an inference to be made based on the TRisk statistic in Eq. (10), and 2) as a mechanism for exploratory risk analysis.",1,ad,True
147,"Jackknife, which is also known as the Quenouille-Tukey Jackknife or leave-one-out, was first introduced by Quenouille [18] and then developed by Tukey [21]. Tukey used the Jackknife technique to determine how an estimate is affected by the subsets of observations when discordant values",0,,False
148,25,0,,False
149,"(i.e., outlier data) are present. In the presence of discordant values, it is expected that the Jackknife technique could reduce the bias in the estimate. Although the original objective of Jackknife is to detect outliers, in principle it is a re-sampling technique for estimating the bias and the standard error of any estimate [11]. In Jackknife, the same test is repeated by leaving one subject out each time: this explains why this technique is also referred to as leave-one-out.",0,,False
150,"Let the random variables X1, X2, . . . , Xc denote a random sample of size c, such that Xi is drawn identically and independently from a distribution F for i ,"" 1, 2, . . . , c. Suppose that the goal is to estimate an unknown parameter  of F . It can be shown that  can be estimated by a statistic ^, which is derived from an observed sample x1, x2 . . . , xc from F , with a measurable amount of sampling error [15].""",0,,False
151,"An unbiased estimator ^ is a statistic whose expected value E(^) is equal to the true value of the population parameter of interest , i.e., E(^) , . The amount of bias associated with an estimator is therefore given by:",0,,False
152,"bias(^) , E(^ - ) , E(^) - .",0,,False
153,(14),0,,False
154,"We denote as X(i) the sub-sample without the datum Xi. There are in total c sub-samples of size c-1 for i ,"" 1, 2, . . . , c:""",0,,False
155,"X(i) ,"" X1, X2, . . . , Xi-1, Xi+1, . . . , Xc. Next, let the estimate derived from the ith sub-sample X(i)""",0,,False
156,"be denoted as ^(i), and the mean over c sub-samples be:",0,,False
157,^(.),0,,False
158,",",0,,False
159,1 c,0,,False
160,c,0,,False
161,^(i).,0,,False
162,"i,1",0,,False
163,(15),0,,False
164,"The Jackknife estimate of bias, which is actually a nonparametric estimate of E(^ - ), is defined as follows [21]:",0,,False
165,biasJ (^),0,,False
166,",",0,,False
167,(c -,0,,False
168,1)(^(.),0,,False
169,-,0,,False
170,^),0,,False
171,",",0,,False
172,(c - 1) c,0,,False
173,c,0,,False
174,(^(i) - ^).,0,,False
175,"i,1",0,,False
176,"and, in accordance, the bias-reduced Jackknife estimate of  is defined as ~ , ^ - biasJ (^) , c^ - (c - 1)^(.).",0,,False
177,Tukey [21] showed that the Jackknife technique can also,0,,False
178,"be used to estimate the variance of ^ by introducing the so-called pseudo-values, ~(i) ,"" c^ - (c - 1)^(i), such that""",0,,False
179,varJ (^),0,,False
180,",",0,,False
181,1 c(c - 1),0,,False
182,c,0,,False
183,~(i),0,,False
184,- ~,0,,False
185,2,0,,False
186,",",0,,False
187,(c - 1) c,0,,False
188,c,0,,False
189,^(i) - ^(.),0,,False
190,2,0,,False
191,.,0,,False
192,"i,1",0,,False
193,"i,1",0,,False
194,This nonparametric Jackknife estimate of variance gives the empirical estimate of the standard error of ^:,0,,False
195,"SE(^) , varJ (^).",0,,False
196,(16),0,,False
197,"For the TRisk statistic in Eq. (10), the standard error of",0,,False
198,"URisk, SE(URisk), can hence be estimated by substituting URisk into Eq. (16) as ^:",0,,False
199,"SEJ , varJ (URisk).",0,,False
200,(17),0,,False
201,3.3 Empirical Validation of SE(URisk),0,,False
202,"The nonparametric estimator SEJ is an alternative to the parametric estimator SEx¯ (Eq. (12)). In this section, we empirically compare these estimates of SE(URisk) with each other, to assess the validity of the result of a hypothesis test to be performed using TRisk as the test statistic. In general, if the two estimates agree, the test result may be said to be valid, and otherwise its validity will be compromised. As a result, nonparametric methods can help to alleviate doubts about the validity of the analysis performed [14].",0,,False
203,"In the following, we compare the estimates using the submitted runs to the TREC Web track. In particular, the provided baseline run for the TREC 2013 Web track risksensitive task is based on the Indri retrieval platform. However, as the submitted runs and results for the TREC 2013 campaign were not yet publicly available at the time of writing, in the following we perform an empirical study based on runs submitted to the TREC 2012 Web track. Indeed, the 2013 track coordinators have made available a set of Indri runs on the TREC 2012 Web track topics1 that correspond to the TREC 2013 baseline runs - in our results, we use the 2012 equivalent run to the 2013 pre-determined baseline, the so-called indriCASP. We report the URisk values obtained using the official TREC 2012 evaluation measure, ERR@20.",1,TREC,True
204,"Table 1 reports the parametric estimates (SEx¯) and the nonparametric Jackknife estimates (SEJ ) of the standard errors associated with the average risk-reward tradeoff scores (URisk), calculated for each of the TREC 2012 Web track top 8 ad-hoc runs over c ,"" 50 topics, with respect to the indriCASP baseline, applying several risk-sensitivity parameter values of  "","" 0, 1, 5, 10. From the results, it can be observed that the two estimates, SEx¯ and SEJ agree with each other for each of the 8 runs. In fact, over all of the 48 runs submitted to the TREC 2012 Web track, we observe a Root Mean Square Error (RMSE) of 0.000 between SEx¯ and SEJ . Thus, we conclude that it is highly likely that it would be valid to conduct an inferential risk analysis upon those TREC 2012 runs based on the new risk-reward tradeoff measure TRisk (Eq. (10)), regardless of how SE(URisk) is estimated. An example of inferential risk analysis based on TRisk follows in the next section.""",1,ad,True
205,4. INFERENTIAL RISK ANALYSIS,0,,False
206,"The goal of the classical evaluation of IR systems is to decide whether one IR system is better in retrieval effectiveness than another on the population of topics. This goal can be formulated into a (two-sided) null hypothesis, as given by:",0,,False
207,"H0 : µr , µb or H0 : µr - µb ,"" 0,""",0,,False
208,(18),0,,False
209,"against the alternative hypothesis H1 : µr ,"" µb, where µr and µb represent respectively the population mean performance of the system r and the population mean performance of the baseline system b. The test statistic for this null hypothesis is the t statistic (Eq. (7)), since the larger values of t are evidence against the null hypothesis H0 : µr - µb "","" 0. Below, we describe the hypothesis testing of H0 in abstract terms, before explaining how it can be applied to TRisk (Section 4.1) and illustrating its application upon the TREC 2012 Web track runs (Section 4.2).""",1,TREC,True
210,"In order to decide how much difference between the two sample means r¯ and ¯b is assumed to be large enough to reject the null hypothesis, we should first determine how much difference can be attributed to a chance fluctuation. It can be shown that, under the null hypothesis H0, the sampling (or null) distribution of the test statistic t can be approximated by a Student's t distribution with df ,"" c - 1 degrees of freedom for any population distribution with finite mean µ and variance 2 > 0, because of the central limit theorem [12]. Thus, at a predefined significance level of  (typically  "","" 0.05 for 95% confidence), two standard deviations (±t(/2,df) × SE(d¯)) determine the maximum difference that can be attributed to chance fluctuation, where in between the critical values ±t(/2,df) the area under the Student's t""",0,,False
211,1https://github.com/trec-web/trec-web-2013,1,trec,True
212,26,0,,False
213,"Table 1: Calculated risk-reward tradeoff scores, URisk for the TREC 2012 Web track top 8 ad-hoc runs at the risk-sensitivity parameter values of  ,"" 0, 1, 5, 10, along with the parametric estimates SEx¯ and the nonparametric Jackknife estimates SEJ of the associated standard errors SE(URisk). indriCASP is the baseline.""",1,ad,True
214,",0",0,,False
215,",1",0,,False
216,",5",0,,False
217," , 10",0,,False
218,ERR@20 URisk SEx¯ SEJ URisk SEx¯ SEJ URisk SEx¯ SEJ URisk SEx¯ SEJ,0,,False
219,uogTrA44xi,0,,False
220,0.3132 0.1185 0.0528 0.0528 0.0556 0.0739 0.0739 -0.1959 0.1755 0.1755 -0.5104 0.3091 0.3091,0,,False
221,srchvrs12c09,0,,False
222,0.3049 0.1102 0.0479 0.0479 0.0679 0.0644 0.0644 -0.1015 0.1489 0.1489 -0.3133 0.2619 0.2619,0,,False
223,DFalah121A,0,,False
224,0.2920 0.0974 0.0425 0.0425 0.0467 0.0632 0.0632 -0.1558 0.1588 0.1588 -0.4089 0.2827 0.2827,0,,False
225,QUTparaBline,0,,False
226,0.2901 0.0954 0.0448 0.0448 0.0385 0.0672 0.0672 -0.1893 0.1703 0.1703 -0.4740 0.3033 0.3033,0,,False
227,utw2012fc1,0,,False
228,0.2195 0.0248 0.0449 0.0449 -0.0558 0.0705 0.0705 -0.3782 0.1849 0.1849 -0.7813 0.3314 0.3314,0,,False
229,ICTNET12ADR2 0.2149 0.0203 0.0416 0.0416 -0.0495 0.0637 0.0637 -0.3286 0.1648 0.1648 -0.6774 0.2950 0.2950,0,,False
230,indriCASP,0,,False
231,0.1947,0,,False
232,*,0,,False
233,*,0,,False
234,*,0,,False
235,*,0,,False
236,*,0,,False
237,*,0,,False
238,*,0,,False
239,*,0,,False
240,*,0,,False
241,*,0,,False
242,*,0,,False
243,*,0,,False
244,irra12c,0,,False
245,0.1723 -0.0223 0.0410 0.0410 -0.1182 0.0693 0.0693 -0.5014 0.1904 0.1904 -0.9805 0.3437 0.3437,0,,False
246,qutwb,0,,False
247,0.1659 -0.0287 0.0462 0.0462 -0.1342 0.0791 0.0791 -0.5560 0.2194 0.2194 -1.0832 0.3969 0.3969,0,,False
248,"distribution sums up to (1 - ). If an observed t-score is greater than t(/2,df), or less than -t(/2,df), one can reject H0 with 100%(1 - ) confidence, denoted as the p-value.",0,,False
249,4.1 Inference Based on TGain and TRisk,0,,False
250,"The above protocol of hypothesis testing is referred to as the Student's t test for matched pairs, or paired t test for short, in statistics. Hence, in the context of risk-sensitive evaluation, the TGain formula in Eq. (9) stands for the test statistic t. In fact, at  ,"" 0, testing the significance of an observed risk-reward tradeoff score between r and b (i.e. an observed UGain score) is akin to testing the significance of the observed difference between r¯ and ¯b.""",1,ad,True
251,"To test the significance of an observed UGain score, one can therefore compare the corresponding TGain score with the two-sided critical ±t(/2,df) values at a desired level of significance . If -t(/2,df)  TGain  t(/2,c-1), the observed UGain score can be attributed to chance fluctuation, meaning that the observed gain in the performance of the system r with respect to the baseline system b is not statistically significant. In such a case, it is equally likely that the observed UGain score may or may not occur on another topic sample drawn from the population. Otherwise, if TGain  -t(/2,c-1) or TGain  t , (/2,c-1) one can however be sure that a UGain score at least as extreme as the observed score would occur on 100(1 - )% of the topic samples that could be drawn from the population.",0,,False
252,"Both TGain and TRisk stem from the t statistic. Indeed, for  ,"" 0, TGain "","" TRisk, while for  > 0, SE(URisk) was shown to be valid in Section 3.3. Hence, we argue that an equivalent inferential analysis can be conducted upon the TRisk scores that have been calculated based on URisk. In the following, we provide an illustration of such inferential analysis upon runs submitted to the TREC 2012 Web track, but the same inferential analysis methodology could be applied for any risk-sensitive evaluation scenario.""",1,TREC,True
253,4.2 Inferential Analysis of Web Track Runs,1,Track,True
254,"Given a particular IR system, a baseline system, and a set of c topics, one can use the paired t test for testing the significance of the calculated average tradeoff score between risk and reward over the c topics, URisk, by comparing the corresponding t-score, TRisk, with the critical values ±t(/2,df) at a desired level of significance . To illustrate such an analysis, Table 2 reports the URisk risk-reward tradeoff scores based on ERR@20, and the corresponding TRisk scores for the 8 highest performing TREC 2012 ad-hoc runs, given the baseline run indriCASP (we omit other submitted runs for brevity, however the following analysis would be equally applicable to them). As the TREC 2012 Web track has 50",1,ad,True
255,"topics, for a significance level of  ,"" 0.05, the critical values for TRisk are ±t(0.025,49) "", ±2.",0,,False
256,"In Table 2, the URisk scores to which a two-sided paired t test gives significance are those that have a corresponding TRisk score less than -2 or greater than +2. For example, at  ,"" 0, the calculated URisk scores of the top 4 runs are significant with a p-value less than 0.05. This means that, under the null hypothesis H0 : µr "","" µb, given another sample of 50 topics from the population, the probability of observing a risk-reward tradeoff score, between any one of these 4 runs and the baseline run indriCASP, that is as extreme or more extreme than the one that was observed is less than 0.05, i.e. the associated p-values. Since TRisk > 0, for those runs, the declared significance counts in favour of """"reward"""" against """"risk"""". Thus, one can conclude, with 95% confidence, that the expected per topic effectiveness of each of the top 4 runs is, on average, higher than the expected per topic effectiveness of the baseline run indriCASP on the population of topics. In other words, given a topic from the population, it is highly likely that any one of the top 4 runs will not perform worse for that topic than indriCASP. This suggests, as a result, that those top runs do not exhibit a real risk that is generalisable to the population of topics.""",1,ad,True
257,"On the other hand, a run with TRisk < -2 at  ,"" 0 will be under a real risk, though among the shown top 8 TREC 2012 runs there is no such run. For those runs with -2  TRisk < +2, such as utw2012fc1 and qutwb, the risk analysis performed here is inconclusive, since the associated URisk scores can be attributed to chance fluctuation, i.e. it is equally likely that they may or may not be under a real risk.""",1,TREC,True
258,"Next, we observe from Table 2 that as  increases, the observed tradeoffs between risk and reward for each run changes in favour of risk compared to reward, hence the runs exhibiting significant URisk scores change. For example, each of the runs with significant URisk scores at  ,"" 0 (i.e., the top 4 runs) have a URisk score that can be attributed to a chance fluctuation at  "","" 10, while, in contrast, those runs whose URisk scores can be attributed to chance fluctuation at  "","" 0 (i.e., the last 4 runs) have a significant URisk score at  "", 10.",1,ad,True
259,"Figure 1 shows the change in the TRisk scores of the TREC 2012 top 8 ad-hoc runs for several risk-sensitivity  parameter values from 0 to 15. From the figure, we observe that for  > 5 the TRisk scores for all runs are negative in sign, and for the last 4 runs the calculated URisk scores can be considered statistically significant (i.e., TRisk > -2.0 for  > 5). It is also observed that, even for  ,"" 15, the calculated URisk scores of the top 4 TREC runs can still be attributed to chance fluctuation.""",1,TREC,True
260,"As a result, the inferential analysis performed so far suggests that, in general, none of the 8 top TREC 2012 ad-hoc",1,TREC,True
261,27,0,,False
262,"Table 2: URisk and TRisk scores risk-reward tradeoff scores for the top 8 TREC 2012 ad-hoc runs at  ,"" 0, 1, 5, 10,""",1,ad,True
263,where the baseline is indriCASP. The underlined URisk scores are those for which a two-tailed paired t test,0,,False
264,gives significance with p < 0.05 - i.e. exhibit a TRisk score greater than +2 or less than -2.,0,,False
265,",0",0,,False
266,",1",0,,False
267,",5",0,,False
268," , 10",0,,False
269,URisk TRisk p-value URisk TRisk p-value URisk TRisk p-value URisk TRisk p-value,0,,False
270,uogTrA44xi,0,,False
271,0.1185 2.2440 0.029 0.0556 0.7528 0.455 -0.1959 -1.1163 0.270 -0.5104 -1.6512 0.105,0,,False
272,srchvrs12c09,0,,False
273,0.1102 2.3034 0.026 0.0679 1.0541 0.297 -0.1015 -0.6817 0.499 -0.3133 -1.1961 0.237,0,,False
274,DFalah121A,0,,False
275,0.0974 2.2899 0.026 0.0467 0.7401 0.463 -0.1558 -0.9808 0.332 -0.4089 -1.4466 0.154,0,,False
276,QUTparaBline 0.0954 2.1305 0.038 0.0385 0.5723 0.570 -0.1893 -1.1116 0.272 -0.4740 -1.5626 0.125,0,,False
277,utw2012fc1,0,,False
278,0.0248 0.5526 0.583 -0.0558 -0.7914 0.432 -0.3782 -2.0457 0.046 -0.7813 -2.3574 0.022,0,,False
279,ICTNET12ADR2 0.0203 0.4869 0.629 -0.0495 -0.7775 0.441 -0.3286 -1.9942 0.052 -0.6774 -2.2960 0.026,0,,False
280,irra12c,0,,False
281,-0.0223 -0.5446 0.588 -0.1182 -1.7038 0.095 -0.5014 -2.6335 0.011 -0.9805 -2.8525 0.006,0,,False
282,qutwb,0,,False
283,-0.0287 -0.6226 0.536 -0.1342 -1.6956 0.096 -0.5560 -2.5342 0.015 -1.0832 -2.7295 0.009,0,,False
284,Figure 1: The change in standardised TRisk scores for the top TREC 2012 ad-hoc runs for 0    15.,1,TREC,True
285,3 Upper Critical Value,0,,False
286,2,0,,False
287,1,0,,False
288,"Null Hypothesis H0:µr , µb",0,,False
289,uogTrA44xi srchvrs12c09 DFalah121A QUTparaBline utw2012fc1 ICTNET12ADR2 indriCASP irra12c qutwb,0,,False
290,0,0,,False
291,TRisk score,0,,False
292,-1 Lower Critical Value,0,,False
293,-2,0,,False
294,-3 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15,0,,False
295, value,0,,False
296,"runs are under a real risk of performing any given topic from the population worse than the baseline run indriCASP, on average. In particular, there can be no significant reduction in risk that could be attained for the top 4 systems, given a baseline system with the average retrieval effectiveness of indriCASP. On the other hand, a significant reduction in risk could be attained, on average, for the last 4 systems, particularly for  > 5.",0,,False
297,"Lastly, in Table 2, it is notable that the high URisk scores do not necessarily imply high TRisk scores, because of the fact that each system would in general have a different inherent variation in ri -bi across topics (i.e. SE(URisk)) from that of the other systems. For example, consider the runs uogTrA44xi and srchvrs12c09. At  ,"" 0, uogTrA44xi has a URisk score (0.1185) higher than the URisk score (0.1102) of srchvrs12c09, while srchvrs12c09 has a higher TRisk score than uogTrA44xi, i.e. 2.3034 vs. 2.2440. This shows that a ranking of retrieval systems obtained based on TRisk will not necessarily be concordant with the ranking of systems obtained based on URisk.""",0,,False
298,5. EXPLORATORY RISK ANALYSIS,0,,False
299,"In the previous section, the risk analysis that we performed could hide significant performance losses on individual topics. Nevertheless, one can perform an exploratory risk analysis to determine those individual topics on which the observed risk-reward tradeoff score between a given IR system and the baseline system (i.e., xi) is statistically significant. In the following, we provide a definition for exploratory risk analysis (Section 5.1), which we later illustrate upon the TREC 2012 Web track runs (Section 5.2).",1,ad,True
300,5.1 Definition,0,,False
301,"The TRisk measure permits the topic-by-topic analysis of risk-reward tradeoff measurements, which we refer to as ex-",1,ad,True
302,ploratory risk analysis. Such an analysis is implicitly sug-,0,,False
303,gested by the t statistic itself. The t statistic in Eq. (7) can,0,,False
304,be rewritten as follows:,0,,False
305,"t,",0,,False
306,d¯ SE(d¯),0,,False
307,",",0,,False
308,1 c,0,,False
309,"c i,1",0,,False
310,(ri,0,,False
311,-,0,,False
312,bi),0,,False
313,sd/ c,0,,False
314,",",0,,False
315, c c,0,,False
316,"c i,1",0,,False
317,ri,0,,False
318,- sd,0,,False
319,bi,0,,False
320,.,0,,False
321,(19),0,,False
322,"In here,",0,,False
323,each component of the sum ti,0,,False
324,",",0,,False
325,: ri -bi,0,,False
326,sd,0,,False
327,gives the,0,,False
328,standardised score of the observed difference in effectiveness,0,,False
329,"between the system r and the baseline system b on topic i,",0,,False
330,"for i ,"" 1, 2, . . . , c.""",0,,False
331,"In analogy, the TRisk measure, which stems from the t statistic, can be rewritten as:",0,,False
332,"TRisk ,",0,,False
333,URisk SEx¯,0,,False
334,",",0,,False
335,1 c,0,,False
336,c,0,,False
337,"i,1",0,,False
338,xi,0,,False
339,sx/ c,0,,False
340,",",0,,False
341, c c,0,,False
342,"c i,1",0,,False
343,xi sx,0,,False
344,",",0,,False
345,(20),0,,False
346,"where each component of the sum, in this case, gives the",0,,False
347,standardised score of the individual topic risk-reward trade-,1,ad,True
348,"off measurements x1, x2, . . . , xc:",0,,False
349,TRi,0,,False
350,",",0,,False
351,xi sx,0,,False
352,.,0,,False
353,(21),0,,False
354,"In a similar manner that we compare the calculated TRisk score of a given IR system with the two-sided critical values ±t(/2,df) to decide whether the system exhibits a significant level of risk on average (Section 4), to decide whether an observed loss (or gain) on a particular topic i is significant, we can compare the component TRi score with the same critical values ±t(/2,df) , at a desired significance level of . If -t(/2,df)  TRi  t(/2,df), the observed loss (or gain) can be attributed to chance fluctuation, and otherwise it can be considered statistically significant.",0,,False
355,"Indeed, this is one of the typical methods of outlier detection in statistics [14]. Recall that the original objective of Jackknife is to detect outliers [21]. The TRisk measure can also be expressed in terms of the Jackknife estimate of bias, following Wu [29]:",0,,False
356,TRisk,0,,False
357,",",0,,False
358,URisk SEJ,0,,False
359,",",0,,False
360,1 c,0,,False
361,"c i,1",0,,False
362,(c,0,,False
363,-,0,,False
364,1) (^(i) SEJ,0,,False
365,-,0,,False
366,^),0,,False
367,.,0,,False
368,(22),0,,False
369,"Here, each component of the sum:",0,,False
370,"TJi ,",0,,False
371,(c - 1) (^(i) SEJ,0,,False
372,- ^),0,,False
373,",",0,,False
374,"(c - 1) (x¯(i) - x¯) , (23) varJ (x¯)",0,,False
375,gives the standardised Jackknife estimate of bias in URisk due to leaving the topic risk-reward score xi out of the sam-,0,,False
376,"ple x1, x2, . . . , xc, where x¯ , URisk and x¯(i) is the URisk score to be obtained when the ith topic is leaved out of the",0,,False
377,"topic set in use, for i ,"" 1, 2, . . . , c.""",0,,False
378,"In general, both the TRi statistic in Eq. (21) and the TJi statistic in Eq. (23) can be used for the purpose of exploratory risk analysis. However, there is a certain difference",0,,False
379,28,0,,False
380,"between them in theory. Using TRi , we can decide whether an observed performance loss on topic i is significant, by comparing the topic risk-reward score xi with the maximum score that can be attributed to chance fluctuation, but as if the single datum xi is the whole sample. In contrast, using TJi , we can make the same decision by comparing the observed difference between two URisk scores, x¯(i) - x¯, with the maximum difference that can be attributed to chance fluctuation. Since we showed in Section 3.3 that the two estimates of the standard error for each TREC run are in perfect agreement (i.e. SEx¯  SEJ ), we argue that this theoretical difference has no practical consequences. Hence, in the following, we provide an illustration of exploratory risk analysis on the TREC 2012 Web track runs, based on TJi alone. However, initial experiments showed no differences between TRi and TJi .",1,TREC,True
381,5.2 Exploratory Analysis of Web Track Runs,1,Track,True
382,"Figure 2 shows the standardised Jackknife estimate of bias in the URisk scores calculated for two TREC runs, namely uogTrA44xi and qutwb at  ,"" 0, 5, 10, 15 for the 50 TREC 2012 Web track topics, where indriCASP is the baseline. This standardised Jackknife estimate of bias, TJi is estimated by leaving one TREC 2012 Web track topic out of the set of topics {151, 152, . . . , 200} in turn. In the figure, the topics that result in a significant performance loss (gain) for the corresponding systems with respect to indriCASP, at the significance level of  "","" 0.05, are those which have a TJi score less than -2 (greater than 2, respectively). Horizontal lines at -2 and +2 are shown to aid clarity.""",1,TREC,True
383,"From Figure 2, at  ,"" 0 it can be observed that uogTrA44xi has more significant wins in number than qutwb, and less significant losses. This shows in detail why the declared significance for uogTrA44xi in Section 4 counts in favour of reward against risk, while the observed tradeoff between risk and reward can be attributed to chance fluctuation for qutwb, with respect to the baseline indriCASP.""",1,ad,True
384,"In general, both of the runs uogTrA44xi and qutwb exhibit considerable performance losses with respect to indriCASP on the same topics, including 166, 172, 174, 175, and 191, out of which 2 are significant for uogTrA44xi (i.e., 166 and 175) and 4 are significant for qutwb (i.e., 166, 172, 175, and 191), at  ,"" 0. In particular, consider the topic 166, on which the magnitude of the TJi score is nearly the same for both runs. It is notable here that, as  increases, the significance of that topic relatively doubles for uogTrA44xi, while for qutwb it nearly remains the same. The situation is also similar for topic 175, though the TJi score of uogTrA44xi at  "", 0 is small in magnitude compared to that of qutwb.",0,,False
385,"This is one of the important differences between TRisk and URisk in assessing the risk associated with IR systems. Given a particular topic i, the same amount of performance loss with respect to a provided baseline effectiveness can lead to different TJi (and TRi ,"" xi/SEx¯) scores for different IR systems, depending on the variation in the observed riskreward tradeoff across the topics (i.e., different SEx¯ for different systems), while leading to the same topic risk-reward score, xi, for i "","" 1, 2, . . . , c. As  increases, the topic riskreward score xi increases proportionally for both of the runs uogTrA44xi and qutwb. However, the tradeoff counts, on average, significantly in favour of reward against risk for uogTrA44xi, whereas, it counts neither in favour of reward nor against risk for qutwb, as shown in Section 4. Thus, the same margin of increase in topic risk-reward tradeoff score xi in""",1,ad,True
386,favour of risk should lead to a relatively higher level of risk,1,ad,True
387,"for uogTrA44xi than that for qutwb, in a way that TJi did. Assessing the level of risk that a topic commits for a given",0,,False
388,IR system relative to the level of risk associated with the sys-,0,,False
389,"tem on average is a property unique to the measures TJi and TRi . Besides the use of these measures for exploratory risk analysis, this property also enables adaptive risk-sensitive",1,ad,True
390,"optimisation within a learning to rank technique, as we ex-",0,,False
391,plain in the next section.,0,,False
392,6. ADAPTIVE RISK OPTIMISATION,1,AP,True
393,"In this section, we describe how to exploit the new riskreward tradeoff measure TRisk (Eq. (10)) in learning robust ranking models that maximises average retrieval effectiveness while minimising risk-reward ratio, in the context of the state-of-the-art LambdaMART learning to rank technique [30]. As discussed below, Wang et al. [26] proposed to integrate URisk (Eq.(4)) within LambdaMART to achieve risk sensitive optimisation, by using  to penalise risk during the learning process. However, URisk considers topics equally regardless of the level of risk they commit. In contrast, we propose to adaptively change the level of risksensitivity, so that the total risk-sensitivity is distributed across the topics proportionally to the level of risk each topic commits. In the following: Section 6.1 provides an overview of the LambdaMART objective function, while Section 6.3 describes the integration of URisk within LambdaMART; Section 6.3 explains our proposed adaptive risk-sensitive optimisation approaches, with the experimental setup & results following in Sections 6.4 & 6.5, respectively.",1,ad,True
394,6.1 LambdaMART,0,,False
395,"LambdaMART [30] is a state-of-the-art learning to rank technique, which won the 2011 Yahoo! learning to rank challenge. It can be described as a tree-based technique, in that its resulting learned model takes the form of an ensemble of regression trees, which is used to predict the score of each document given the document's feature values. During learning, LambdaMART creates a sequence of gradient boosted regression trees that improve an effectiveness metric. In general, for our purposes2, it is sufficient to state that LambdaMART's objective function is based upon the product of two components: (i) the derivative of a crossentropy that originates from the RankNet learning to rank technique [3] calculated between the scores of two documents a and b, and (ii) the absolute change M in an evaluation measure M due to the swapping of documents a and b [4]. Therefore the final gradient naew of a document a within the objective function is obtained over all pairs of documents that a participates in for query q:",1,Yahoo,True
396,"naew ,",0,,False
397,ab · |Mab|,0,,False
398,"b,a",0,,False
399,"where ab is RankNet's cross-entropy derivative, and Mab is the change in an evaluation measure M by swapping documents a and b. Various IR evaluation measures are suitable for use as M , including NDCG and MAP, as they have been shown to satisfy a consistency property [4]: for a pair of documents a and b where a is ranked higher than b, if the relevance label of a is higher than b, then a ""degrading"" swap of a and b must result in a decrease in M (i.e. M  0), and orthogonally M  0 for ""improving"" swaps.",1,MAP,True
400,"2Further details on LambdaMART can be found in [4, 26].",0,,False
401,29,0,,False
402,Figure 2: and qutwb,0,,False
403,"Bar graph showing the at  ,"" 0, 5, 10, 15, where""",0,,False
404,standardised indriCASP is,0,,False
405,Jackknife estimate the baseline.,0,,False
406,of,0,,False
407,bias,0,,False
408,in,0,,False
409,the,0,,False
410,"URisk ,",0,,False
411,"TJi ,",0,,False
412,for,0,,False
413,uogTrA44xi,0,,False
414,uogTrA44xi,0,,False
415,qutwb,0,,False
416,"TJ at ,0 i",0,,False
417,2 0 -2,0,,False
418,"TJ at ,5 i",0,,False
419,"TJ at ,10 i",0,,False
420,4 2 0 -2 -4,0,,False
421,4 2 0 -2 -4,0,,False
422,4 2 0 -2 -4,0,,False
423,151 156 161 166 171 176 181 186 191 196,0,,False
424,151 156 161 166 171 176 181 186 191 196,0,,False
425,"TJ at ,15 i",0,,False
426,6.2 Risk-Sensitive Optimisation,0,,False
427,"Wang et al. [26] demonstrated that a more robust learned model could be obtained from LambdaMART if the M is replaced by the difference in URisk for a given swap of two documents, denoted T . In doing so, their implementation weights the value of M by  + 1 only for the topics with down-side risk, while for the topics with up-side risk it leaves M as is, T , M . T was shown to exhibit the consistency property iff the underlying evaluation measure M is consistent (e.g. as obtained from NDCG).",0,,False
428,6.3 Adaptive Risk-Sensitive Optimisation,0,,False
429,"Compared to URisk, TRisk is grounded in the theory of hypothesis testing and produces values that are easily interpretable ­ as shown in Section 4. However, as a linear transformation of URisk, the direct application of TRisk as T within LambdaMART to attain risk-sensitive optimisation cannot offer marked improvements on the resulting learned models. On the other hand, the exploratory risk analysis of Section 5 offers a promising direction, as it permits the learning to rank process to adaptively focus on topics depending upon the level of risk that they commit. In this section, we propose two new models of adaptive risk-sensitive optimisation that exploit the standardised topic risk-reward tradeoff scores (TRi , Eq. (21)), but which differ on which individual topics they operate on. In particular, the first model, SemiAdaptive Risk-sensitive Optimisation (SARO), focuses only on the topics with down-side risk and augments only the corresponding M values. In contrast, the Fully Adaptive Risk-sensitive Optimisation (FARO) model operates on all topics and augments every M value. Hence, compared to URisk as used in [26], FARO and SARO both alter the importance of riskier topics within the learning process.",1,ad,True
430,"In URisk, M is multiplied by  + 1 if the topic commits a downside risk3. This amounts to a static level of sensitivity for each topic, irrespective of the level of risk that the topic commits. In contrast, based on the standardised topic",0,,False
431,"3This follows directly from the definition of Eq. (4), however the consistency proof in Section 4.3.2 of [26] defines T for different scenarios.",0,,False
432,"risk-reward tradeoff scores, TRi (Eq. (21)), we propose to adaptively adjust  so that the total level of sensitivity can",1,ad,True
433,be distributed across the topics proportional to the levels,0,,False
434,"of risk that they commit. In order to achieve this, for each",0,,False
435,topic we must estimate the probability of observing a risk-,0,,False
436,"reward score greater than the actual observed TRi score. Technically speaking, we need to estimate the cumulative",0,,False
437,"probability P r (Z  TRi ), where TRi is the observed riskreward tradeoff score and Z is the corresponding standard",1,ad,True
438,"normal variable of TRi for all topics i ,"" 1, 2, .., c. For large sample sizes (generally agreed to be  30), the distribu-""",0,,False
439,tion of the t statistic in Eq. (7) can be approximated by,0,,False
440,"the standard normal probability distribution function, with",0,,False
441,"zero mean and unit variance [15]. Thus, the probability",0,,False
442,"P r (Z  TRi ), which is the probability of a topic risk-reward score greater than TRi , can be estimated by the standard normal cumulative distribution function (·), as follows:",0,,False
443,"P r (Z  TRi )  1 -  (TRi ) ,",0,,False
444,(24),0,,False
445,"for i ,"" 1, 2, . . . , c. (Z) is a monotonically increasing func-""",0,,False
446,"tion of the standard normal random variable Z, where 0 ",0,,False
447,"(Z ,"" z)  1 for -  z  , and at Z "","" 0, (Z) "","" 0.5. Hence, we can replace the original  in T as  as follows:""",0,,False
448," , [1 - (TRi )] · .",0,,False
449,(25),0,,False
450,"where 0    . As the level of risk TRi committed by topic i increases,  also increases. By substituting  into T (as defined by Wang et al. [26]), this augments the M values for every topic with a weight proportional to the level of risk that each topic commits.",0,,False
451,"The application of  differs between the SARO and FARO models. In particular, SARO only addresses the down-side risk, as in the case of URisk. Indeed, under the null hypothesis H0 : µr ,"" µb, the higher the level of down-side risk (i.e. the larger the size of the difference ri - bi < 0), the higher the probability of observing a topic risk-reward tradeoff score greater than the observed score (P r (Z  TRi )). Hence, SARO varies  from 0 to , according to the downside risk of each topic.""",1,ad,True
452,"On the other hand, FARO operates on all topics. Indeed, for the topics with up-side risk, FARO gives lower weights",0,,False
453,30,0,,False
454,to the topics that more strongly outperform the baseline,0,,False
455,"system (i.e. as the difference ri - bi > 0 increases). At the extreme, if topic i exhibits maximal improvements over the baseline (i.e. ri - bi ,"" 1), then (TRi ) "","" 1, and hence topic i has minimal emphasis on the learner. In other words, the learner focuses on improving the riskier topics. FARO operates on all topics, by redefining T as follows:""",0,,False
456,"T  ,"" (1 + ) × M,""",0,,False
457,(26),0,,False
458,"Moreover, for  ,"" 0,  "","" 0, hence T  "","" M , i.e. the gain-only LambdaMART, as for URisk.""",0,,False
459,"Finally, we informally comment on the consistency of SARO and FARO: For both models, we calculate SE(URisk) after the first iteration of boosting within LambdaMART, and",0,,False
460,"not for each considered swap ­ we found this to be sufficient to obtain accurate estimates of SE(URisk); Next, the consistency of SARO follows from URisk, as our replacement of  with , as 0    . For FARO, T  only changes sign with M , again as 0    . Hence, as long as M is consistent, both SARO and FARO are also consistent.",0,,False
461,6.4 Experimental Setup,0,,False
462,"We implement the URisk, SARO and FARO models within the Jforests implementation [13] of LambdaMART4. Experiments are conducted using the large MSLR-Web10k learning to rank dataset5, as used by Wang et al. [26]. This dataset encompasses 9,685 queries with labelled documents obtained from a commercial web search engine. For each ranked document for each query, a range of 136 typical query-independent, query-dependent and query features are provided.",1,MSLR,True
463,"We use identical hyper-parameters for LambdaMART to those described by Wang et al. [26], namely: the minimum number of documents in each leaf m ,"" 500, 1000, the number of leaves l "","" 50, the number of trees in the ensemble nt "", 800 and the learning rate r ,"" 0.075. The best m value is chosen for each of the five folds using the validation topic set, based on the NDCG@10 performance of the original LambdaMART algorithm, and used for all experiments for that fold thereafter. For the calculation of risk measures, like [26], we use the ranking obtained from the BM25.whole.document feature as the baseline system. The NDCG@10 performance of this baseline is 0.309.""",0,,False
464,"The performances obtained for LambdaMART upon the MSLR-Web10k in terms of NDCG@1 and NDCG@10 are similar in magnitude to those reported by Wang et al. [26], however we note some differences in the risk profile. Such differences are expected given the different implementations: Wang et al. [26] used a private implementation of LambdaMART, while we use and adapt an open source machine learning toolkit for URisk, SARO and FARO. Nevertheless, the reported results allow valid conclusions to be drawn, including identical conclusions to [26] on the impact of using URisk within LambdaMART.",1,MSLR,True
465,6.5 Results for SARO and FARO,0,,False
466,"Table 3 reports the effectiveness and robustness results for FARO and SARO along with URisk, for  ,"" 1, 5, 10, 206. In the table, the gain over the baseline effectiveness is ex-""",0,,False
467,"4All of our code has been integrated to Jforests, available at https://code.google.com/p/jforests/ 5http://research.microsoft.com/en-us/projects/mslr/ 6,0 is equivalent to the normal LambdaMART algorithm.",0,,False
468,"Table 3: Results for SARO, FARO and URisk.",0,,False
469," , 0  , 1  , 5  , 10  , 20",0,,False
470,NDCG@1 (URisk) NDCG@1 (SARO),0,,False
471,0.472 0.468 0.458 0.442 0.423 - 0.470 0.463 0.455 0.439,0,,False
472,NDCG@1 (FARO),0,,False
473,- 0.468 0.467 0.470 0.469,0,,False
474,NDCG@10 (URisk) NDCG@10 (SARO),0,,False
475,0.480 0.478 0.470 0.458 0.448 - 0.479 0.474 0.468 0.458,0,,False
476,NDCG@10 (FARO),0,,False
477,- 0.479 0.477 0.479 0.478,0,,False
478,Risk/Reward (URisk) 0.172 0.168 0.164 0.176 0.185 Risk/Reward (SARO) - 0.167 0.164 0.169 0.177,0,,False
479,Risk/Reward (FARO) - 0.170 0.171 0.171 0.172,0,,False
480,Loss/Win (URisk) Loss/Win (SARO),0,,False
481,0.281 0.278 0.267 0.272 0.275 - 0.267 0.266 0.272 0.270,0,,False
482,Loss/Win (FARO),0,,False
483,- 0.272 0.274 0.271 0.277,0,,False
484,Loss (URisk ) Loss (SARO) Loss (FARO),0,,False
485,Win (URisk ) Win (SARO) Win (FARO),0,,False
486,2080 -,0,,False
487,7400 -,0,,False
488,2059 1996,0,,False
489,2025 7417 7470,0,,False
490,7451,0,,False
491,1992,0,,False
492,1992 2040 7468 7476 7452,0,,False
493,2019,0,,False
494,2024 2040 7427 7437 7469,0,,False
495,2040 2010,0,,False
496,2060 7406 7441,0,,False
497,7429,0,,False
498,Loss > 20% (URisk) Loss > 20% (SARO),0,,False
499,Loss > 20% (FARO),0,,False
500,1180 -,0,,False
501,1130 1036 1124 1124 1152 1145,0,,False
502,1036,0,,False
503,1046 1155,0,,False
504,1042 1032 1172,0,,False
505,"pressed as the risk (Eq. (1)) to reward (Eq. (2)) ratio (i.e., the ""Risk/Reward"" rows). Similarly, the number of topics that the risk-sensitive optimisation contributed to reward against risk is expressed as the loss to win ratio (i.e., the ""Loss/Win"" rows). Raw numbers of losses and wins associated with each  value for each model are also shown. Finally the ""Loss > 20%"" rows show, for each model, the number of topics on which the relative loss in performance over the BM25 baseline was higher than 20%7.",0,,False
506,"As expected, since the semi-adaptive risk-sensitive optimisation (SARO) and the risk-sensitive optimisation based on URisk focus on only those topics with down-side risk, there is a steady decrease in average retrieval effectiveness (i.e., NDCG@1 and NDCG@10), as the risk-sensitivity parameter value of  increases. Nevertheless, SARO results in a decrease in average retrieval effectiveness that is less than URisk, for all  values. In contrast, the fully adaptive risk-sensitive optimisation (FARO) maintains the average retrieval effectiveness nearly constant across all  values, as well as the values of the quality and robustness measures, namely the risk-reward ratio and the loss-win ratio.",1,ad,True
507,"For SARO, the observed values of the two quality and robustness metrics (risk-reward ratio and loss-win ratio) are better than for URisk across the  values. For the metric ""Loss > 20%"", they are comparable between SARO and URisk, given a topic sample as large as 9685 in size.",0,,False
508,"Next, for FARO, the observed values of the two quality and robustness metrics are comparable with that of the risksensitive optimisation based on URisk across  values, and for the metric, ""Loss > 20%"" the observed values for FARO are slightly worse than that of both URisk and SARO.",0,,False
509,"To summarise, the empirical evidence in Table 3 suggest that (i) FARO is best suited for retrieval tasks that are not tolerant to any loss in average effectiveness but also require robustness in effectiveness across the topics, and (ii) SARO suits retrieval tasks that require primarily robustness but are tolerant to some loss in the achievable average effectiveness.",0,,False
510,"7Similar measures are reported in [26]. With 9685 topics, all NDCG differences are statistically significant.",0,,False
511,31,0,,False
512,7. RELATED WORK,0,,False
513,"To the best of our knowledge, this paper is the first work examining risk-sensitive evaluation from the perspective of statistical inference. Indeed, while there has been some investigation into measures of robustness in the literature, such as Geometric-Mean Average Precision [24], developed within the context of the TREC 2004 Robust track, this paper advances upon the URisk measure, first proposed in [26] in 2012. The TRisk measure is the test statistic counterpart of URisk, which enables hypothesis testing on the level of risk associated with a given IR system. As a result, it facilitates adaptive risk-sensitive optimisation within learning to rank.",1,TREC,True
514,"Outside of risk-sensitive evaluation, statistical hypothesis testing has a long history within IR. Van Rijsbergen [22] noted that ""there are no known statistical tests applicable to IR"". However, later, Hull [32] recommended various hypothesis tests for the evaluation of retrieval experiments, including the Student's t test for matched pairs. Zobel [31] was the first to apply re-sampling techniques in IR, by using a leaveone-out technique for assessing the effect of pooling on the effectiveness measurements and the significance of hypothesis tests, including the paired t test and the Wilcoxon signed rank test. Later, Smucker et al. [19, 20] and also Urbano et al. [27] investigated nonparametric re-sampling techniques, such as the bootstrapping and permutation tests, for the purposes of the evaluation of retrieval experiments.",0,,False
515,"Finally, much work in developing effective learning to rank techniques has occurred in the last few years, as reviewed by Liu [16]. Macdonald et al. [17] examined how the choice of evaluation measure encoded within their loss functions impacted upon the effectiveness of various learning to rank techniques. In particular, it is notable that the AdaRank technique [16, Ch. 4] focuses on hard queries using boosting. Taking a different approach, Wang et al. [26] proposed a risk-sensitive optimisation for the state-of-the-art LambdaMART technique, based on their URisk measure. We further extend URisk to the new TRisk measure within this paper, which is both theoretically founded, and results in more effective and less risky learning to rank.",0,,False
516,8. CONCLUSIONS,0,,False
517,"This paper proposed the new TRisk measure for risk-sensitive evaluation, which is theoretically grounded within hypothesis testing. It easily allows inferential hypothesis testing of risk, as well as the exploratory identification of topics that commit significant levels of risk. In particular, we showed how TRisk could be integrated within the state-of-the-art LambdaMART learning to rank technique, to permit effective yet risk-averse retrieval. Indeed, compared to the existing URisk measure, we attain higher effectiveness with comparable or better risk/reward tradeoffs. For future work, we believe that there is a huge scope to build further effective and risk-averse adaptations for learning to rank upon TRisk, other than SARO and FARO, and beyond LambdaMART.",1,ad,True
518,9. REFERENCES,0,,False
519,"[1] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. ECIR, 2004.",1,Query,True
520,"[2] C. Buckley and E. M. Voorhees. Evaluating evaluation measure stability. In Proc. SIGIR, 2000.",0,,False
521,"[3] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proc. ICML, 2005.",1,ad,True
522,"[4] C. J. C. Burges. From RankNet to Lambdarank to LambdaMART: An overview. Technical Report MSR-TR-2010-82, 2010.",0,,False
523,"[5] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. Automatic query refinement using lexical affinities with maximal information gain. In Proc. SIGIR, 2002.",0,,False
524,"[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proc. CIKM, 2009.",1,ad,True
525,"[7] J. Cohen. Statistical Power Analysis for the Behavioral Sciences. 2nd edition, 1988.",0,,False
526,[8] K. Collins-Thompson. Accounting for stability of retrieval algorithms using risk-reward curves. In Proc. Future of Evaluation in IR Workshop. SIGIR. 2009.,0,,False
527,"[9] K. Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In Proc. CIKM, 2009.",0,,False
528,"[10] K. Collins-Thompson, P. N. Bennett, F. Diaz, C. Clarke, and E. Voorhees. TREC 2013 Web Track Guidelines.",1,TREC,True
529,"[11] B. Efron. Bootstrap methods: Another look at the jackknife. Annals of Statistics, (7):1­26, 1979.",0,,False
530,"[12] H. Fisher. A History of the Central Limit Theorem: From Classical to Modern Probability Theory. Springer, 2010.",0,,False
531,"[13] Y. Ganjisaffar, R. Caruana and C. Lopes. Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models. In Proc. SIGIR, 2011.",1,ad,True
532,"[14] D. C. Hoaglin, F. Mosteller, and J. W. Tukey, editors. Understanding robust and exploratory data analysis. 1983.",0,,False
533,"[15] R. V. Hogg, A. T. Craig, and J. W. McKean. Introduction to Mathematical Statistics. 6th edition, 2004.",0,,False
534,"[16] T.-Y. Liu. Learning to rank for information retrieval. Foundations & Trends in IR, 3(3):225­331, 2009.",0,,False
535,"[17] C. Macdonald, R. Santos, and I. Ounis. The whens and hows of learning to rank for web search. Information Retrieval, 16(5):584-628, 2013.",0,,False
536,"[18] M. Quenouille. Approximate tests of correlation in time series. J. Royal Statistical Society, (11):18­84, 1949.",0,,False
537,"[19] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proc. CIKM, 2007.",0,,False
538,"[20] M. D. Smucker, J. Allan, and B. Carterette. Agreement among statistical significance tests for information retrieval evaluation at varying sample sizes. In Proc. SIGIR, 2009.",0,,False
539,"[21] J. W. Tukey. Bias and confidence in not quite large samples. Annals of Mathematical Statistics, 29(2):614, 1958.",0,,False
540,"[22] C. van Rijsbergen. Information Retrieval. 2nd edition, 1979.",0,,False
541,"[23] E. M. Voorhees. Overview of the TREC 2003 robust retrieval track. In Proc. TREC, 2003.",1,TREC,True
542,"[24] E. M. Voorhees. The TREC Robust retrieval track. SIGIR Forum, 39(1):11­20, 2005.",1,TREC,True
543,"[25] E. M. Voorhees and C. Buckley. The effect of topic set size on retrieval experiment error. In Proc. SIGIR, 2002.",0,,False
544,"[26] L. Wang, P. N. Bennett, and K. Collins-Thompson. Robust ranking models via risk-sensitive optimization. In Proc. SIGIR, 2012.",1,Robust,True
545,"[27] J. Urbano, M. Marrero, and D. Mart´in. On the measurement of test collection reliability. In Proc. SIGIR, 2013.",0,,False
546,"[28] W. Webber, A. Moffat, and J. Zobel. Score standardization for inter-collection comparison of retrieval systems. In Proc. SIGIR, 2008.",0,,False
547,"[29] C. F. J. Wu. Jackknife, bootstrap and other resampling methods in regression analysis. Annuals of Statistics, 14:1261­1350, 1986.",0,,False
548,"[30] Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. Ranking, boosting, and model adaptation. Technical Report MSR-TR-2008-109, 2008.",1,ad,True
549,"[31] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In Proc. SIGIR, 1998.",0,,False
550,"[32] J. Zobel. Using statistical testing in the evaluation of retrieval experiments. In Proc. SIGIR, 1993.",0,,False
551,32,0,,False
552,,0,,False

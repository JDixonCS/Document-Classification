,sentence,label,data,regex
0,Using the Cross-Entropy Method to Re-Rank Search Results,0,,False
1,"Haggai Roitman, Shay Hummel",0,,False
2,"IBM Research - Haifa Haifa 31905, Israel",0,,False
3,"{haggai,shayh}@il.ibm.com",0,,False
4,Oren Kurland,0,,False
5,"Faculty of Industrial Engineering and Management, Technion Haifa 32000, Israel",0,,False
6,kurland@ie.technion.ac.il,0,,False
7,ABSTRACT,0,,False
8,"We present a novel unsupervised approach to re-ranking an initially retrieved list. The approach is based on the Cross Entropy method applied to permutations of the list, and relies on performance prediction. Using pseudo predictors we establish a lower bound on the prediction quality that is required so as to have our approach significantly outperform the original retrieval. Our experiments serve as a proof of concept demonstrating the considerable potential of the proposed approach. A case in point, only a tiny fraction of the huge space of permutations needs to be explored to attain significant improvements over the original retrieval.",0,,False
9,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval] Retrieval Models,0,,False
10,"General Terms: Algorithms, Experimentation",0,,False
11,"Keywords: Re-ranking, Optimization, Performance Prediction",0,,False
12,1. INTRODUCTION,1,DUC,True
13,"We present a novel unsupervised approach to the challenge of re-ranking a document list that was retrieved in response to a query so as to improve retrieval effectiveness. The approach utilizes a Monte-Carlo-based optimization method, named the Cross Entropy (CE) method [13], which is applied to permutations of the list. The approach relies on a retrieval performance predictor that can be applied to any ranking of the list.",0,,False
14,"Given the reliance on performance prediction, we present a novel pseudo predictor that enables to fully control the prediction quality. We use the pseudo predictor in our approach to set a lower bound on the prediction quality that is needed so as to have our approach significantly outperform the initial ranking. Further empirical evaluation provides a proof of concept for our approach. Specifically, via the exploration of a tiny fraction of the huge space of all possible permutations, our approach finds highly effective permutations. The retrieval effectiveness of these permutations is substantially, and statistically significantly better, than that of the original ranking of the list.",0,,False
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.",1,ad,True
16,http://dx.doi.org/10.1145/2600428.2609454.,0,,False
17,2. RELATED WORK,0,,False
18,"The Cross Entropy (CE) method [13] that is used in our approach is a Monte Carlo framework for rare event estimation and combinatorial optimization. The CE method has been previously applied in many domains such as machine learning, simulation, networks, etc. [13]. To the best of our knowledge, our work is the first to use the CE method in the information retrieval domain.",0,,False
19,"Our approach relies on predicting the retrieval performance of permutations of a document list. Applying performance prediction to select one of two retrieved lists was explored in some work [2, 6, 3, 11]. However, the conclusions regarding the resultant effectiveness of using the proposed predictors were inconclusive. In contrast, we do not present a concrete predictor. Rather, we devise a pseudo predictor that enables to control prediction quality, and accordingly set a lower bound on the prediction quality required so as to have our approach outperform the initial ranking.",0,,False
20,"Using a simulation study, a lower bound on the prediction quality required for effective selective query expansion was set [8]. While this work focused on performance prediction over queries, our approach relies on prediction over rankings for the same query. Furthermore, our approach is not committed to any ranking paradigm. In addition, rather than use a simulation, we propose a novel pseudo predictor that allows to control prediction quality.",1,ad,True
21,"Finally, we note that some list-wise learning to rank approaches [10] are also based on finding effective permutations of the same list, although not with the Cross Entropy method that we employ. Permutations are explored during the training phase and a ranker is induced. In contrast, our approach employs optimization over permutations as an unsupervised re-ranking mechanism.",0,,False
22,3. FRAMEWORK,0,,False
23,3.1 Problem Definition,0,,False
24,Let Dqk denote the list of the k documents in a corpus D that are the most highly ranked by some initial search performed in response to query q. Let Dqk denote the set of all k! possible permutations (rankings) of the documents in Dqk. Let   Dqk denote a single permutation of Dqk and let (d) further denote the position (rank) of document d ( Dqk) in . Let Q() denote the retrieval performance (effectiveness) of the permutation  ( Dqk ).,0,,False
25,"The goal is to find a permutation  ( Dqk ) such that Q() is maximized. Unfortunately, finding an optimal per-",0,,False
26,839,0,,False
27,"mutation is NP-Complete [1]. In addition, with no prior relevance judgement on the documents in Dqk, the true performance Q() for any given permutation   Dqk is unknown. Hence, the performance of any given permutation can only be predicted, and the task becomes even more challenging.",1,NP,True
28,"Let Q() denote the predicted performance of the permutation  ( Dqk ). Potential predictors may utilize any available pre-retrieval features [9] (e.g., induced from the query q and the corpus D), post-retrieval features (e.g., induced from the result list Dqk or the permutation ) or their combination [4].",0,,False
29,3.2 An Optimization Approach,0,,False
30,"We next propose an optimization approach that effectively finds ""promising"" permutations which have the best",0,,False
31,predicted performance according to a given predictor Q(). Since the resultant retrieval effectiveness of the optimiza-,0,,False
32,"tion procedure depends on the prediction quality of the predictor employed, we empirically derive a lower bound for the prediction quality of ""effective"" predictors. That is, if a predictor with a prediction quality higher than the lower bound is used in our approach then the approach is guaranteed to find -- as determined based on the benchmarks we have used -- as a solution a permutation  whose retrieval performance is better than that of the initial ranking.",0,,False
33,3.2.1 Optimization using the Cross Entropy method,0,,False
34,"We propose a Monte-Carlo optimization approach to our permutation-based re-ranking task. The approach, which we term Cross Entropy Re-ranking Optimization (CERO), uses the Cross Entropy (CE) method [13]. Within the CE method, optimal solutions to hard problems (such as that we try to solve) are modeled as rare events whose occurrence probability is effectively estimated [13]. Given such estimates, optimal solutions can then be efficiently generated. Under a certain condition, the CE method is expected to converge to the optimal solution [12].",0,,False
35,We now describe our algorithm and provide its pseudo code in Algorithm 1. The algorithm gets as an input the,0,,False
36,"initial ranked list Dqk , a performance predictor Q () and several tuning parameters (mentioned below) that control the learning rate and convergence of the algorithm. The algorithm iteratively explores permutations in Dqk using random sampling. To this end, the algorithm induces a probability space of ""promising"" permutations over Dqk using the feedback it gets about the relative performance of permutations that were explored in previous iterations. It is easy to show that for a given k, a unique bijection exists between the permutation set Dqk and the set of all k! possible Hamiltonian paths in a complete graph with k nodes. Therefore, random permutations can be efficiently drawn by sampling Hamiltonian paths using a simple constrained random walk method [13]. Let P(ti,j) denote the the probability for a single step transition between node i and node j in the graph, which corresponds to the event (dj) ,"" (di) + 1, i.e., document di is ranked in  one position before document dj. With no prior knowledge on the permutations probability space, the algorithm is initialized with the uniform probability (having the maximum entropy) and Dqk is considered as the current best performing permutation. The algorithm's""",0,,False
37,Algorithm 1 Cross Entropy Re-ranking Optimization,0,,False
38,"1: input: Dqk , Q () , N, , ",0,,False
39,2: initialize:,0,,False
40,"3: P(0i,j) ,",0,,False
41,1 k-1,0,,False
42,",",0,,False
43,"i,j",0,,False
44,"0,",0,,False
45,"i,j",0,,False
46,"4:  , Dqk 5: t , 1 6: loop 7: Randomly draw N permutations l  Dqk using P t-1",0,,False
47,"8: if Q () < maxl,""1,...,N Q (l) then""",0,,False
48,9:,0,,False
49," , arg maxl,""1,...,N Q(l)""",0,,False
50,10: end if,0,,False
51,11: Sort permutations l according to Q (l) 12: Let t be the sample (1-)-quantile of the performances:,0,,False
52,"Q (l);l ,"" 1, .., N""",0,,False
53,"13: for i ,"" 1, . . . , k; j "","" 1, . . . , k do""",0,,False
54,14:,0,,False
55,"P(ti,j ) ,",0,,False
56,"N l,1",0,,False
57,I {l (dj,0,,False
58,"),l (di )+1}I {Q(l )t }",0,,False
59,"N l,1",0,,False
60,I {Q(l )t },0,,False
61,15:,0,,False
62,"P(ti,j ) ,""P(ti-,j1) +(1-)P(ti,j )""",0,,False
63,16: end for,0,,False
64,17: if t converged then,0,,False
65,18:,0,,False
66,stop and return ,0,,False
67,19: else,0,,False
68,20:,0,,False
69,"t,t+1",0,,False
70,21: end if,0,,False
71,22: end loop,0,,False
72,"goal is to converge (via cross entropy minimization) to the unknown probability space of optimal permutations, from which optimal solutions can be generated [13].",0,,False
73,"On each iteration t, N random permutations are sampled based on the last induced promising permutations probability space P t-1. Next, the predicted performance of each sampled permutation is calculated and the performance of the current best performing permutation  is updated accordingly. ""New"" promising permutations are then explored by first sorting the sampled permutations according to their predicted performance and then updating the transition probabilities based on the top-N  performing samples, whose minimum performance is t. Given t, each transition probability P(ti,j) is induced according to the relative number of permutations out of the top-N  permutations (which have predicted performance equal to or higher than t) that also ranked document di one position above document dj. A fixed smoothing scheme, controlled by parameter , further allows to trade between the exploration (given by P(ti,j)) and the exploitation (given by P(ti-,j1)) of the algorithm.",1,ad,True
74,The algorithm continuous until some convergence criteria is met. In this work we follow the convergence criteria suggested in [13] and stop the algorithm if the sample (1 - )-performance quantile t does not change within several consecutive iterations.,0,,False
75,3.2.2 A criterion for effective prediction,0,,False
76,"The CERO algorithm is generic and can employ any performance predictor. Naturally, however, the prediction quality of the predictor has significant impact on the retrieval effectiveness of the ranking produced by the algorithm.",0,,False
77,"Thus, we turn to devise a method for determining the lower bound of prediction quality that will result in the CERO algorithm outperforming the initial ranking of Dqk. The bound is independent of a prediction approach.",0,,False
78,"Herein, we measure retrieval performance using average precision (AP@k); i.e., Q() in our case is the AP of the per-",1,AP,True
79,840,0,,False
80,corpus,0,,False
81,GOV2 WT10G TREC8,1,WT,True
82,# of documents,0,,False
83,"25,205,179 1,692,096 528,155",0,,False
84,queries,0,,False
85,701-850 451-550 401-450,0,,False
86,disks,0,,False
87,GOV2 WT10g 4&5-{CR},1,WT,True
88,Table 1: TREC data used for experiments.,1,TREC,True
89,"mutation . Following standard practice in work on queryperformance prediction [4], prediction quality is measured by the Pearson correlation between the true AP of permu-",1,AP,True
90,"tations (Q()) and their predicted performance (Q()). To derive a lower bound on prediction quality, we next",0,,False
91,"present an approach for generating pseudo AP predictors, whose prediction quality can be controlled. Following previous observations [5], we assume that true AP values follow a normal distribution1.",1,AP,True
92,We first normalize the AP of the permutation  ( Dqk ):,1,AP,True
93,Qnorm(),0,,False
94,",",0,,False
95,Q() - EDqk,0,,False
96,(AP ) ;,1,AP,True
97,(1),0,,False
98,V arDqk (AP ),1,AP,True
99,"EDqk (AP ) and V arDqk (AP ) are the expectation and the variance of the true AP values of the permutations in Dqk , respectively. The two statistics can be estimated using max-",1,AP,True
100,"imum likelihood estimation for normal distribution, by sam-",0,,False
101,pling a large enough random (uniform) sample of permuta-,0,,False
102,"tions in Dqk (e.g., N ,"" 1000). Since AP follows a normal distribution, we get that as k  , for any permutation""",1,AP,True
103,"  Dqk : Qnorm()  N (0, 1) [5]. Proposition 1 defines a -correlated pseudo AP predictor;",1,AP,True
104,"that is, a predictor with a  prediction quality (i.e., Pearson",0,,False
105,correlation with true AP). The proof is quite straightforward,1,AP,True
106,and is ommitted due to space considerations.,0,,False
107,"Proposition 1. Given a query q, initial result list Dqk, and permutation   Dqk , a -correlated pseudo AP predic-",1,AP,True
108,"tor, denoted Q(), is obtained as follows:",0,,False
109,"Q() , Qnorm() + 1 - 2X",0,,False
110,(2),0,,False
111,"where Qnorm() is the normalized true AP value according to Eq. 1 and X  N (0, 1).",1,AP,True
112,4. EVALUATION,0,,False
113,4.1 Setup,0,,False
114,"The TREC corpora and queries used for experiments are specified in Table 1. Titles of TREC topics served for queries. The Apache Lucene2 search library (version 4.3) was used for the experiments. Documents and queries were processed using Lucene's default analysis (i.e., tokenization, stemming, stopwords, etc). For each query, 100 documents were retrieved using Lucene's implementation, employed with default free-parameter values, of each of the following retrieval methods: vector space model (TF-IDF), query-likelihood (QL with Dirichlet smoothing) and Okapi BM25. Thus, we",1,TREC,True
115,1The assumption was further verified in our experiments using the 2 goodness-of-fit test. Details are omitted due to space considerations. 2http://lucene.apache.org,0,,False
116,"obtained three initial lists, Dqk, composed of k ,"" 100 documents, for each query. Mean average precision (MAP@k) serves as the evaluation measure. Statistically significant differences of performance are measured using the paired t-test with a 95% confidence level.""",1,MAP,True
117,"Each initial list was re-ranked using the CERO algorithm employed with pseudo AP predictors of varying prediction quality levels. To control prediction quality, the pseudo predictors were generated according to Eq. 2; the prediction quality was varied from  , 0.05 (worst predictor) to  ,"" 1.0 (best predictor). Following previous recommendations [13], the algorithm's learning parameters were set as follows: N "","" 1000,  "", 0.01 and  , 0.7.",1,AP,True
118,"Eff ciency considerations. To implement CERO, we used",0,,False
119,"a parallelized version of the Cross Entropy method [7]. On average, CERO converged in 21.32 iterations (with a 15.6 standard deviation). CERO explores at each iteration a maximum of N ,"" 1000 permutations. Thus, all together, CERO considered only about 21k - 36k permutations out of the 100! possible permutations of the initial list.""",0,,False
120,4.2 Results,0,,False
121,CERO's effectiveness. We first study the potential of our,0,,False
122,"permutation-based optimization approach; specifically, in finding highly effective permutations in the huge space (100!) of permutations. To this end, we neturilize the effect of prediction quality by applying CERO with a ""predictor"" which reports the true AP of the considered permutations (i.e.,  , 1.0). The resultant MAP performance is presented in Table 2. As reference comparisons we use the initial ranking and an optimal re-ranking where all relevant documents from the initial list are positioned at the highest ranks.",1,AP,True
123,"We can see in Table 2 that, overall, CERO results in very good approximations. Specifically, CERO's MAP is at least as 91% as good as that of the optimal MAP. Recall from above that CERO explores only a tiny fraction of all permutations of the documents in the result list. It is worth noting that an even better approximation may be obtained by finer tuning of the algorithm (e.g., following [12]). We leave this exploration for future work, and view the results presented in Table 2 as a solid proof of concept for the optimization approach we have employed.",1,MAP,True
124,The effect of prediction quality. In Table 3 we present,0,,False
125,"the effect of the prediction quality of the pseudo AP predictors used in CERO on its MAP retrieval performance. Evidently, and as should be expected, the higher the prediction quality (), the better the performance. Furthermore, as from  ,"" 0.3 CERO improves over the initial ranking for all the retrieval methods and across all corpora; for   0.35 the improvements are always statistically significant. With higher prediction quality, the improvements over the initial ranking become very substantial.""",1,AP,True
126,5. CONCLUSIONS AND FUTURE WORK,0,,False
127,We presented a novel approach to re-ranking an initially retrieved list. The approach is based on applying the Cross Entropy optimization method [13] upon permutations of the list. Query-performance predictors are used to evaluate the performance of permutations. Empirical evaluation pro-,1,Query,True
128,841,0,,False
129,"Initial Optimal CERO ( , 1.0)",0,,False
130,BM25,0,,False
131,.151 .274 .251 (92%),0,,False
132,GOV2 QL,0,,False
133,.172 .296 .275 (93%),0,,False
134,TF-IDF,0,,False
135,.137 .253 .232 (91%),0,,False
136,BM25,0,,False
137,.156 .352 .320 (91%),0,,False
138,WT10G QL,1,WT,True
139,.164 .391 .367 (94%),0,,False
140,TF-IDF,0,,False
141,.158 .349 .322 (92%),0,,False
142,BM25,0,,False
143,.198 .400 .367 (92%),0,,False
144,TREC8 QL,1,TREC,True
145,.206 .407 .373 (92%),0,,False
146,TF-IDF,0,,False
147,.190 .388 .356 (92%),0,,False
148,"Table 2: The MAP of the initial retrieval, optimal re-ranking and the CERO algorithm when employed with the true AP as the ""predictor"". The percentages are with respect to Optimal.",1,MAP,True
149,"Initial  , 1.0  , .95  , .90  , .85  , .80  , .75  , .70  , .65  , .60  , .55  , .50  , .45  , .40  , .35  , .30  , .25  , .20  , .15  , .10  , .05",0,,False
150,BM25,0,,False
151,.151 .251 (66%) .249 (65%) .245 (62%) .242 (60%) .237 (57%) .232 (54%) .222 (47%) .222 (47%) .215 (43%) .207 (37%) .199 (32%) .192 (27%) .182 (20%) .170 (12%) .161 (6%),0,,False
152,.154 (2%) .143 (-5%) .134 (-11%) .120 (-20%) .119 (-21%),0,,False
153,GOV2,0,,False
154,QL,0,,False
155,.172 .275 (60%) .272 (58%) .267 (56%) .262 (53%) .255 (49%) .248 (44%) .245 (42%) .231 (35%) .229 (33%) .219 (28%) .214 (24%) .202 (18%) .194 (13%) .180 (5%),0,,False
156,.173 (1%) .165 (-4%) .150 (-13%) .144 (-16%) .141 (-18%) .137 (-20%),0,,False
157,TF-IDF,0,,False
158,.137 .232 (69%) .231 (69%) .225 (64%) .225 (64%) .218 (59%) .212 (55%) .208 (52%) .203 (48%) .198 (45%) .194 (42%) .181 (32%) .173 (27%) .172 (25%) .157 (15%),0,,False
159,.144 (5%) .135 (-2%) .125 (-8%) .120 (-12%) .117 (-14%) .110 (-20%),0,,False
160,BM25,0,,False
161,.156 .320 (105%) .320 (105%) .320 (104%) .319 (104%) .311 (99%) .309 (97%) .301 (93%) .301 (92%) .292 (87%) .290 (85%) .280 (79%) .260 (66%) .257 (64%) .211 (35%) .199 (27%) .172 (10%),0,,False
162,.151 (-3%) .134 (-14%) .110 (-30%) .79 (-50%),0,,False
163,WT10G,1,WT,True
164,QL,0,,False
165,.164 .367 (124%) .362 (121%) .361 (120%) .354 (115%) .355 (116%) .342 (108%) .345 (110%) .333 (103%) .328 (100%) .312 (90%) .303 (85%) .272 (66%) .258 (57%) .246 (50%) .222 (35%),0,,False
166,.177 (8%) .139 (-15%) .124 (-24%) .119 (-28%) .93 (-43%),0,,False
167,TF-IDF,0,,False
168,.158 .322 (104%) .321 (104%) .317 (101%) .317 (101%) .313 (99%) .309 (96%) .303 (93%) .296 (88%) .293 (86%) .285 (81%) .284 (81%) .260 (65%) .249 (58%) .225 (43%) .213 (35%),0,,False
169,.172 (9%) .130 (-18%) .112 (-29%) .105 (-33%) .103 (-35%),0,,False
170,BM25,0,,False
171,.198 .367 (85%) .364 (83%) .361 (82%) .358 (81%) .354 (79%) .348 (76%) .342 (72%) .332 (67%) .325 (64%) .311 (57%) .303 (53%) .285 (44%) .267 (35%) .244 (23%) .221 (11%),0,,False
172,.201 (1%) .167 (-16%) .145 (-27%) .123 (-38%) .115 (-42%),0,,False
173,TREC8,1,TREC,True
174,QL,0,,False
175,.206 .373 (81%) .371 (80%) .370 (80%) .364 (77%) .362 (76%) .358 (74%) .345 (68%) .344 (67%) .332 (62%) .321 (56%) .310 (51%) .293 (42%) .268 (30%) .248 (21%) .227 (10%) .194 (-6%) .159 (-22%) .148 (-28%) .133 (-35%) .122 (-41%),0,,False
176,TF-IDF,0,,False
177,.190 .356 (87%) .356 (87%) .354 (86%) .348 (83%) .342 (80%) .339 (78%) .330 (73%) .322 (70%) .317 (67%) .304 (60%) .294 (54%) .278 (46%) .259 (36%) .237 (24%) .212 (12%) .179 (-6%) .165 (-13%) .139 (-27%) .120 (-37%) .109 (-43%),0,,False
178,Table 3: The MAP of the initial retrieval and of CERO when using different -correlated pseudo AP predictors. marks a statistically significant improvement over the initial ranking. Numbers in italics are lower than those for the initial ranking. The reported percentages are with respect to the initial ranking.,1,MAP,True
179,"vided a proof of concept for our approach. That is, the optimization procedure finds highly effective permutations by exploring only a tiny fraction of the space of all possible permutations. In addition, we devised novel pseudo predictors that allow to carefully control prediction quality and to infer the minimal prediction quality required for our approach to (significantly) outperform the original ranking.",1,ad,True
180,Our main plan for future work is devising query-performance predictors that yield a prediction quality that is higher than that we established as a lower bound for effective application of our approach. We note that almost all previously proposed query-performance predictors [4] are not suited for this task as they operate over different queries rather than over different lists retrieved for the same query.,0,,False
181,Acknowledgment,0,,False
182,We would like to thank David Carmel for discussions on an earlier version of this work. Oren Kurland's work is supported in part by the Israel Science Foundation under grant no. 433/12 and by a Google faculty research award.,0,,False
183,6. REFERENCES,0,,False
184,"[1] N. Alon. Ranking tournaments. SIAM Journal on Discrete Mathematics, 20(1):137­142, 2006.",0,,False
185,"[2] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. of ECIR, pages 127­137, 2004.",1,Query,True
186,"[3] N. Balasubramanian and J. Allan. Learning to select rankers. In Proc. of SIGIR, pages 855­856, 2010.",0,,False
187,"[4] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2010.",1,Query,True
188,"[5] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In Proc. of SIGIR, pages 268­275, 2006.",0,,False
189,"[6] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A language modeling framework for selective query expansion. Technical Report IR-338, Center for Intelligent Information Retrieval, University of Massachusetts, 2004.",0,,False
190,"[7] G. E. Evans, J. M. Keith, and D. P. Kroese. Parallel cross-entropy optimization. In Proc. of WSC, pages 2196­2202, 2007.",0,,False
191,"[8] C. Hauff and L. Azzopardi. When is query performance prediction effective? In Proc. of SIGIR, pages 829­830, 2009.",0,,False
192,"[9] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In Proc. of CIKM, pages 1419­1420, 2008.",0,,False
193,"[10] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.",0,,False
194,"[11] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.",0,,False
195,"[12] L. Margolin. On the convergence of the cross-entropy method. Annals of Operations Research, 134(1):201­214, 2005.",0,,False
196,"[13] R. Y. Rubinstein and D. P. Kroese. The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning. Springer, 2004.",0,,False
197,842,0,,False
198,,0,,False

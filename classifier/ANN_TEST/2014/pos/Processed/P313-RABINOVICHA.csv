,sentence,label,data,regex
0,Utilizing Relevance Feedback in Fusion-Based Retrieval,0,,False
1,Ella Rabinovich,0,,False
2,"IBM Research Labs, Haifa",0,,False
3,Israel,0,,False
4,ellak@il.ibm.com,0,,False
5,Ofri Rom,0,,False
6,Thomson Reuters,1,Reuters,True
7,ofri.rom@ thomsonreuters.com,0,,False
8,Oren Kurland,0,,False
9,"Faculty of IE&M, Technion",0,,False
10,Israel,0,,False
11,kurland@ie.technion.ac.il,0,,False
12,ABSTRACT,0,,False
13,"Work on using relevance feedback for retrieval has focused on the single retrieved list setting. That is, an initial document list is retrieved in response to the query and feedback for the most highly ranked documents is used to perform a second search. We address a setting wherein the list for which feedback is provided results from fusing several intermediate retrieved lists. Accordingly, we devise methods that utilize the feedback while exploiting the special characteristics of the fusion setting. Specifically, the feedback serves two different, yet complementary, purposes. The first is to directly rank the pool of documents in the intermediate lists. The second is to estimate the effectiveness of the intermediate lists for improved re-fusion. In addition, we present a meta fusion method that uses the feedback for these two purposes simultaneously. Empirical evaluation demonstrates the merits of our approach. As a case in point, the retrieval performance is substantially better than that of using the relevance feedback as in the single list setting. The performance also substantially transcends that of a previously proposed approach to utilizing relevance feedback in fusion-based retrieval.",1,ad,True
14,"Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Relevance feedback, Retrieval models",0,,False
15,"Keywords: fusion, relevance feedback",0,,False
16,1. INTRODUCTION,1,DUC,True
17,"It is a well-established fact that using (positive) relevance feedback in ad hoc retrieval helps to substantially improve retrieval effectiveness [29, 30]. Usually, relevance feedback, if available, is provided for the documents most highly ranked by some initial search performed in response to the query. Then, information induced from the feedback documents is used for a second retrieval.",1,ad,True
18,"Here we address the challenge of utilizing relevance feedback in fusion-based retrieval [12]. That is, the document list for which feedback is provided results from merging (fusing) several intermediate lists that were produced using dif-",1,ad,True
19,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609573.",1,ad,True
20,"ferent retrievals from the same corpus in response to the query. Thus, our main goal is to address the questions of whether and how relevance feedback can be effectively utilized while accounting for the special characteristics of the fusion setting. Furthermore, we opt for an approach that is not committed to a specific retrieval framework (e.g., vector space, language modeling) as we operate in a fusion setting.",1,ad,True
21,"We present retrieval methods that use the relevance feedback for two different, yet complementary, purposes. The first is to directly rank the pool of documents in the intermediate lists. The second is to estimate the effectiveness of the intermediate lists so as to re-fuse them. Several listeffectiveness estimates are proposed based on the observation that this is essentially an evaluation task with minimal (incomplete) relevance judgments. To simultaneously leverage both purposes just described, we present a meta fusion method. The method fuses the direct ranking induced over the pool with that created by the re-fusion of the intermediate lists.",0,,False
22,"Empirical evaluation performed with TREC corpora attests to the merits of our approach. Specifically, the retrieval performance is substantially better than that attained by treating the relevance feedback as in the standard single retrieved list setting; i.e., disregarding the fact that the list for which feedback is provided results from fusing intermediate lists. Furthermore, our approach is substantially more effective than the only previously proposed method to utilizing relevance feedback in fusion-based retrieval [4].",1,TREC,True
23,2. RELATED WORK,0,,False
24,"There is a large body of work on using relevance feedback for retrieval [29, 30, 9]. In contrast to our work, the fusionbased retrieval setting has not been specifically addressed, with the exception of some work which is discussed below. As already noted, we show that leveraging the special characteristics of the fusion setting when utilizing the relevance feedback is of much merit.",1,ad,True
25,"There has been much work on fusing document lists that were retrieved from the same corpus in response to a query (e.g., [7, 14, 21, 22, 28, 36, 2, 12, 3, 10, 26, 39, 5, 23, 32, 6, 31, 38]). However, to the best of our knowledge, there has only been a single report on using relevance feedback in a fusion-based retrieval setting [4]. A fusion method served for active feedback, that is, selecting documents to be judged by the user through an iterative process. We do not address the active feedback task. However, as a fusion approach that utilizes a feedback set of documents was proposed, specifically, to estimate list effectiveness for re-fusion [4], we use",1,ad,True
26,313,0,,False
27,"this work for reference comparison. The list-effectiveness estimate proposed in this work [4] is different than those we present here. Furthermore, in contrast to our approach, the feedback was not used to directly rank documents in the intermediate lists. Accordingly, the integration of the two purposes that the feedback can be used for -- direct ranking and list-effectiveness estimation for re-fusion -- is not proposed in contrast to our work. In Section 4.2.4 we empirically show that our approach is substantially more effective in utilizing the relevance feedback.",0,,False
28,"In work on fusion, the intermediate retrieved lists (or segments thereof) were weighted (i) uniformly (which is the most common case), (ii) using unsupervised approaches [39, 38], or (iii) based on past performance of the retrieval method as determined using a train set of queries [7, 2, 23, 32, 6, 31]. In Section 4.2.5 we demonstrate the relative merits of using our proposed list-effectiveness estimates which utilize relevance feedback to weight the lists.",0,,False
29,3. RETRIEVAL FRAMEWORK,0,,False
30,"Let q and D be a query and a corpus of documents, re-",0,,False
31,"spectively. Suppose that the documents lists L1, . . . , Lm,",0,,False
32,"each composed of n documents, were retrieved from D in",0,,False
33,"response to q by m different retrievals. These can be based,",0,,False
34,"for example, on different query representations, document",0,,False
35,"representations, and ranking functions [12]. In what follows",0,,False
36,we use the notation d  L to indicate that document d is in,0,,False
37,the list L; SL(d) is d's normalized (non negative) score in,0,,False
38,"L; if d  L, we set SL(d) d,ef 0. Details regarding the score",0,,False
39,normalization approach are provided in Section 4.1.,0,,False
40,The goal of fusion methods is to merge the lists into one,0,,False
41,"result list, Lfuse. For example, the CombSUM method [14]",0,,False
42,scores,0,,False
43,d,0,,False
44,by,0,,False
45,SCombSUM (d),0,,False
46,"d,ef",0,,False
47,P,0,,False
48,Li :dLi,0,,False
49,SLi (d).,0,,False
50,"Thus,",0,,False
51,docu-,0,,False
52,ments that are ranked high in many of the lists are rewarded.,0,,False
53,"The CombMNZ method [14, 22] further rewards documents",0,,False
54,"that appear in many of the lists: SCombMNZ (d) d,ef |{Li :",0,,False
55,d  Li}|SCombSUM (d).,0,,False
56,3.1 Using Relevance Feedback,0,,False
57,"As in previous work on using relevance feedback [30, 9], we assume that a user scans the list she is presented with, Lfuse in our case, top down until she encounters r documents that are relevant to the information need she expressed using the query q. We use Rq[r](Lfuse) (henceforth Rq) to denote the set of these relevant documents, and F (Lfuse) to denote the set of all documents she scanned and therefore judged; i.e., F (Lfuse) \ Rq are the non-relevant documents the user encountered. We note that the user need not be aware of the fact that the result list she scans (Lfuse) was produced by fusing intermediate lists. Our goal is to devise retrieval methods that use information induced from F (Lfuse).",0,,False
58,"Several of the approaches that we present use some query expansion method. The method takes as input several documents -- the relevant ones (Rq) in our case -- the query q, and some corpus-based term statistics. The method then produces a query model, Mq;Rq , that can be used to rank documents; the score assigned to document d is S(d; Mq;Rq ). For example, in Rocchio's method [29], the query model is a tf.idf-based vector where cosine is often used as the scoring function. In the mixture model [41] and relevance model [20] approaches, the query model is a unigram language model; documents are ranked by the cross entropy between",0,,False
59,the query model and their language models. In Section 4.1 we provide the details of the query expansion method used for experiments. We hasten to point out that our methods are not committed to a specific query expansion approach.,0,,False
60,"Following standard practice in work on utilizing relevance feedback [30], we can use Mq;Rq to rank the entire corpus; CorpusRank denotes this approach. We also study a method, FusedListReRank, which uses Mq;Rq to re-rank Lfuse rather than to rank the entire corpus.",0,,False
61,"However, CorpusRank and FusedListReRank do not account for the special characteristics of the retrieval setting we address here. That is, the fact that the list Lfuse, for which relevance feedback is provided, results from fusing intermediate retrieved lists. The methods we present below do exploit this fact.",1,ad,True
62,3.2 Exploiting the Special Characteristics of the Fusion Setting,0,,False
63,We start with the simple observation that fusion-based re-,0,,False
64,"trieval is a two steps procedure. First, a pool of documents,",0,,False
65,Dpool,0,,False
66,"d,ef",0,,False
67,S,0,,False
68,i,0,,False
69,Li,0,,False
70,",",0,,False
71,is,0,,False
72,created,0,,False
73,by,0,,False
74,the,0,,False
75,different,0,,False
76,retrievals.,0,,False
77,"Then,",0,,False
78,"list-specific properties of documents in the pool (e.g., docu-",0,,False
79,ment scores in the lists) are used to rank the pool. Accord-,0,,False
80,"ingly, we devise methods that rank Dpool using the relevance",0,,False
81,"feedback. Following common practice in work on fusion [12],",0,,False
82,documents not in the pool are assigned with a zero score in,0,,False
83,all methods.,0,,False
84,"Our first method, PoolRank, ranks Dpool using the query",0,,False
85,model Mq;Rq which was induced from the relevant documents and the query:,0,,False
86,"SP oolRank(d) d,ef S(d; Mq;Rq ).",0,,False
87,(1),0,,False
88,"The pool contains documents ""considered"" relevant by retrievals which were based only on the query. Thus, using information induced from relevant documents to re-rank it can potentially improve retrieval effectiveness.",0,,False
89,"Our second method, ReFuse, uses the relevance feedback to weight the intermediate lists Li so as to re-fuse them. The development of ReFuse is guided by probabilistic retrieval principles as described next.",0,,False
90,The goal of probabilistic retrieval methods is to estimate the probability p(d|Iq) that d is relevant to the information need Iq expressed by q. Relevance is determined with respect to the information need rather than with respect to the query which is a signal about the information need.,0,,False
91,"To estimate p(d|Iq) using information induced from the intermediate lists, and inspired by some recent work on predicting query-performance for fusion [25], we can write",0,,False
92,"p(d|Iq) ,"" X p(d|Iq, Li)p(Li|Iq),""",0,,False
93,(2),0,,False
94,Li,0,,False
95,if we assume that p(Li|Iq) is a probability distribution over the intermediate lists.,0,,False
96,"Following common practice in work on aspect and mixture models [16], we first make the assumption that d is independent of Iq given Li. Then, we get the estimate",0,,False
97,"p^(d|Iq) d,""ef X p^(d|Li)p^(Li|Iq),""",0,,False
98,(3),0,,False
99,Li,0,,False
100,"where p^ denotes an estimate for p. That is, the probability that d is relevant to Iq is estimated based on estimates for",0,,False
101,314,0,,False
102,"its association with the intermediate lists (p^(d|Li)); the impact of a list Li depends on its effectiveness (relevance) with respect to Iq (p^(Li|Iq)). Thus, Equation 3 reflects a transition from using q as an explicit signal about Iq to using the intermediate lists that were retrieved in response to q as a pseudo signal about Iq.",0,,False
103,"The mixture model just described is the conceptual basis of linear fusion methods [2].1 For example, the CombSUM method [14], which was mentioned above, is a linear fusion method: normalized document scores in the lists serve for list-association measures (p^(d|Li)); and, uniform list-effectiveness estimates (p^(Li|Iq)) are used.",0,,False
104,"In the absence of relevance feedback, estimating the effectiveness of the intermediate lists is a difficult task [2, 39]. However, here, some relevance feedback is provided, although for the fused list Lfuse and not for the intermediate lists. In Section 3.2.1 we present a few measures that use this relevance feedback to estimate the effectiveness of the intermediate lists. Using these estimates in the linear fusion framework results in our ReFuse method that scores d ( Dpool) by:",0,,False
105,"SReF use(d) d,ef X wIq (Li; F (Lfuse))SLi (d); (4)",0,,False
106,Li :dLi,0,,False
107,wIq (Li; F (Lfuse)) is Li's estimated effectiveness with re-,0,,False
108,"spect to Iq; and, d's score in list association measure as",0,,False
109,Li in,0,,False
110,",CSoLmib(dS)U, sMer.2ves",0,,False
111,as,0,,False
112,the,0,,False
113,document-,0,,False
114,Meta Fusion. The relevance feedback served two different,0,,False
115,"purposes in the PoolRank and ReFuse methods; namely, to directly rank the pool and to estimate the effectiveness of the intermediate lists so as to re-fuse them, respectively. We next integrate these approaches.",0,,False
116,"Instead of making the independence assumption that led from Equation 2 to Equation 3 -- i.e., that d is independent of Iq given Li -- we estimate p(d|Iq, Li) using p^^(d|Iq)+(1- )p^(d|Li); p^^(d|Iq) is some estimate for p(d|Iq);  is a free parameter. Using the estimate for p(d|Iq, Li) in Equation 2, applying some algebra, and using the assumption from above that p(Li|Iq) is a distribution over the intermediate lists, we derive a new estimate for p(d|Iq):",1,ad,True
117,p^^(d|Iq) + (1 - ) X p^(d|Li)p^(Li|Iq).,0,,False
118,(5),0,,False
119,Li,0,,False
120,"This estimate ""backs off"" from some direct estimate (p^^(d|Iq))",0,,False
121,to the mixture-based estimate from Equation 3.,0,,False
122,"For the direct estimate, p^^(d|Iq), we use the normalized",0,,False
123,score assigned by PoolRank to d:,0,,False
124,. SP oolRank (d),0,,False
125,P d Dpool,0,,False
126,SP oolRank (d),0,,False
127,This is a probability distribution over the entire corpus as,0,,False
128,"documents not in Dpool are assigned with a 0 score. The resultant estimate is based on using the query model Mq;Rq , which was induced from the relevant documents and used in",0,,False
129,"PoolRank for ranking, as a representation for Iq.",0,,False
130,"Then, following Equation 5 we interpolate the direct esti-",0,,False
131,mate just described with the normalized score assigned by,0,,False
132,"1We write ""conceptual"" to emphasize the fact that the actual measures that are used in work on linear fusion methods are not necessarily valid probability distributions [2]. 2Document d is associated only with lists in which it ap-",0,,False
133,"pears, because SLi (d) d,ef 0 if d  Li.",0,,False
134,ReFuse to d:,0,,False
135,. SReF use(d),0,,False
136,P d Dpool,0,,False
137,SReF use(d),0,,False
138,This is a distribution,0,,False
139,over the entire corpus which is based on the linear mixture,0,,False
140,"model described in Equation 3. Accordingly, we arrive to our MetaFuse method that scores d by:3",0,,False
141,SMetaF use(d),0,,False
142,"d,ef",0,,False
143,P,0,,False
144,d,0,,False
145,SP oolRank(d) S Dpool P oolRank,0,,False
146,(d,0,,False
147,),0,,False
148,(6),0,,False
149,+,0,,False
150,(1,0,,False
151,-,0,,False
152,),0,,False
153,SReF use(d),0,,False
154,P,0,,False
155,d Dpool,0,,False
156,SReF use(d),0,,False
157,.,0,,False
158,"The name MetaFuse is coined based on the following observation. The PoolRank method induces a ranking over Dpool using Mq;Rq . This ranking is essentially linearly fused, using Equation 6, with a second ranking of Dpool which was created by ReFuse. The ReFuse ranking is by itself the result of linearly fusing the rankings of the intermediate lists L1, . . . , Lm using list-effectiveness estimates in Equation 4.",0,,False
159,"For  , 1 and  ,"" 0, MetaFuse becomes PoolRank and ReFuse, respectively. More generally, the higher the value of , the more weight is put on the ranking produced by using the query model that was induced from the relevant documents. Lower values of  result in more emphasis on the re-fusion of the intermediate lists that are weighted using information induced from the feedback documents.""",0,,False
160,3.2.1 Estimating list effectiveness,0,,False
161,"We now turn to address the task of estimating the effectiveness of an intermediate retrieved list Li with respect to Iq using the feedback document set, F (Lfuse). The estimate, denoted wIq (Li; F (Lfuse)), is used in Equation 4 for the ReFuse method, which is then used in MetaFuse in Equation 6.",1,ad,True
162,"It is important to note that documents in Li, even if are relevant, might not be among those for which relevance feedback is available. Recall that the relevance feedback was provided for the documents most highly ranked in the fused list Lfuse. Thus, the challenge is estimating retrieval effectiveness with incomplete (minimal) relevance judgments.",0,,False
163,"The first list-effectiveness estimate that we consider is the standard average precision measure, AP. AP is computed using the feedback set, F (Lfuse), and treats unjudged documents -- i.e., those not in F (Lfuse) -- as non relevant.",1,AP,True
164,"To address the scarcity of relevance judgments in our setting, we also consider infAP [40]. This is a state-of-the-art retrieval effectiveness measure that was designed as an approximation to average precision (AP); specifically, for cases of incomplete relevance judgments. An important difference between infAP and AP is that the former differentiates between unjudged and non-relevant documents and the latter does not. We compute infAP based on the feedback set F (Lfuse). Documents not in F (Lfuse) are treated as unjudged. For comparison purposes, we also consider a variant of infAP, termed infAPonlyRel, which is computed using only the set of relevant documents, Rq; i.e., all other documents are treated as unjudged.4",1,ad,True
165,"3Experiments -- actual numbers are omitted as they convey no additional insight -- showed that using a weighted geometric mean of the normalized scores of PoolRank and ReFuse yields performance that is very similar to that of using the weighted arithmetic mean from Equation 6. 4We note that in contrast to the case for the original setting in which infAP was introduced [40], here infAP is not",1,ad,True
166,315,0,,False
167,"It is worth noting at this point that we could have potentially used information induced from the non-relevant documents (F (Lfuse) \ Rq) to also improve the query model, Mq;Rq , which is used in PoolRank. However, utilizing negative feedback to improve retrieval performance has long been known as an extremely hard task [17, 29, 37] with the potential merits confined to very difficult queries [37, 18].",0,,False
168,"The development of the third and fourth list-effectiveness estimates, referred to as Kendall- and Pearson, is inspired by work on query-performance prediction [33]. The query model, Mq;Rq , which was induced from the relevant documents, is used to re-rank Li; the re-ranked list is denoted ReRank(Li). As Li was not created using relevance feedback, it is presumably less effective than ReRank(Li). Consequently, the latter can serve as a positive reference comparison to the former for estimating effectiveness [33]; i.e., we assume that the more similar the rankings of Li and ReRank(Li), the higher the effectiveness of Li. We use Kendall's- and Pearson's correlation coefficient to measure the similarity between Li and ReRank(Li). While Kendall's- is based on document ranks, Pearson's correlation coefficient depends on document scores. As the values assigned by the two measures are in [-1, 1] we shift and re-scale them to [0, 1] for consistency with the estimates described above. The resultant values serve as Li's effectiveness estimates.",0,,False
169,4. EVALUATION,0,,False
170,4.1 Experimental Setup,0,,False
171,"The methods we presented in Section 3 utilize relevance feedback. The feedback is provided for the documents at the top ranks of a result list which is produced by fusing several intermediate retrieved lists. Thus, to evaluate the effectiveness of the methods, we use runs submitted to different tracks of TREC as the intermediate lists.",1,TREC,True
172,"Table 1 provides the details of the TREC tracks used for experiments. We used the ad hoc tracks of TREC3, TREC7 and TREC8, and the Web track of TREC9. These were also used in prior work on fusion [3, 26, 4, 5, 23, 32, 19]. We randomly sample 5 runs from all those submitted to a track and which contain at least 100 documents as a result for every query. (We refer to these runs as candidates in Table 1.) We use 30 such random samples; each sample constitutes an experimental setting. The retrieval effectiveness numbers that we report are averages over these 30 samples (settings). The n ,"" 100 most highly ranked documents in a run per query serve for an intermediate retrieved list.5 Retrieval scores in the lists are min-max normalized [22, 27, 24]. Then, the five lists for each query are fused using the CombMNZ method [14, 22], which was described in Section 3. CombMNZ is a highly effective fusion approach that commonly serves as a baseline in work on fusion [3, 26, 23, 32, 19].""",1,TREC,True
173,"To create the set of feedback documents, F (Lfuse), we scan the list Lfuse which was produced by CombMNZ top down until r relevant documents are accumulated or the",0,,False
174,"necessarily a statistical estimate for AP. Yet, the empirical results presented in Section 4.2 attest to the merits of using infAP as a list effectiveness estimate in our setting. 5It was argued, based on the ""skimming effect"" principle [2], and empirically demonstrated [34, 35, 8, 19], that there are clear merits in fusing relatively short lists.",1,AP,True
175,TREC,1,TREC,True
176,TREC3 TREC7 TREC8 TREC9,1,TREC,True
177,Data,0,,False
178,Disks 1&2 Disks 4&5-CR Disks 4&5-CR WT10G,1,WT,True
179,Queries,0,,False
180,151-200 351-400 401-450 451-500,0,,False
181,# of candidate runs,0,,False
182,38 86 113 64,0,,False
183,Table 1: TREC data used for experiments. Candidate runs are those that contain at least 100 results for every query.,1,TREC,True
184,"end of the list is reached.6 F (Lfuse) is the set of documents scanned. Documents in F (Lfuse) are either relevant or non-relevant as determined by using TREC's qrels files. (Documents in F (Lfuse) with no judgement in the qrels file are considered not relevant as is standard.) The set of relevant documents in F (Lfuse) was denoted Rq in Section 3.1. We present results for r  {1, . . . , 5}.",1,TREC,True
185,Titles of TREC topics serve for queries. Tokenization and Porter stemming were applied to queries and documents using the Lemur toolkit7 which was used for experiments.,1,TREC,True
186,"Query expansion method. As described in Section 3, a",1,Query,True
187,few of our approaches use some query expansion method.,0,,False
188,The method produces a query model Mq;Rq that can be used for ranking. For experiments we use the effective rele-,0,,False
189,"vance model number 3 (RM3) [20, 1] as the query expansion",0,,False
190,"method. When using relevant documents to construct RM3,",0,,False
191,"which is a unigram language model, the probability assigned",0,,False
192,to,0,,False
193,term,0,,False
194,w,0,,False
195,is,0,,False
196,"[20,",0,,False
197,1]:,0,,False
198,(1 - )p(w|q) +,0,,False
199, r,0,,False
200,P,0,,False
201,dRq,0,,False
202,p(w|d);,0,,False
203,is,0,,False
204,a free parameter; p(w|q) is the maximum likelihood esti-,0,,False
205,mate of term w with respect to q; p(w|d) is the probability,0,,False
206,assigned to w by a Jelinek-Mercer smoothed unigram lan-,0,,False
207,guage model induced from document d with smoothing pa-,0,,False
208,"rameter  [20];  , 0.1 following previous recommendations",0,,False
209,[20]. It is common practice [1] to clip the relevance model,0,,False
210,by using only the  terms to which it assigns the highest,0,,False
211,probability; these terms' probabilities are sum-normalized,0,,False
212,to yield a valid probability distribution. To rank documents,0,,False
213,"using RM3, we use the cross entropy between the relevance",0,,False
214,model and their (smoothed) unigram language models [20].,0,,False
215,"To this end, we use Dirichlet smoothed document language",0,,False
216,models with the smoothing parameter set to 1000 [41]. We,0,,False
217,note that RM3 interpolates the query language model with,0,,False
218,a linear mixture of the language models of the given relevant,0,,False
219,"documents. Therefore, it is the language-model-based ana-",0,,False
220,"logue of Rochhio's method [29, 20]. The latter interpolates",0,,False
221,"the query vector with the centroid (i.e., linear mixture) of",0,,False
222,the vectors of relevant documents.,0,,False
223,"Evaluation metrics. To evaluate retrieval effectiveness, we",0,,False
224,"use the mean average precision at cutoff n ,"" 100 (MAP@100), which is the size of the intermediate lists that are fused, and the precision of the top 10 document (p@10). Statistically significant differences of performance are determined using the two-tailed paired t-test computed at a 95% confidence level based on the average performance per query over the 30 samples of runs.""",1,MAP,True
225,"6In the very few cases (specifically, 0.8% of the cases for TREC9) that fusing a sample of 5 lists (for a specific query) results in a list with no relevant documents, we omit this sample. 7www.lemurproject.org",1,TREC,True
226,316,0,,False
227,CombMNZ,0,,False
228,CorpusRank FusedListReRank PoolRank ReFuse MetaFuse,0,,False
229,TREC3,1,TREC,True
230,"r,1",0,,False
231,"r,2",0,,False
232,MAP p@10 MAP p@10,1,MAP,True
233,20.3 69.3,0,,False
234,22.6 72.8 22.5 76.1c 24.1cf 75.1c 20.2cpf 68.4cpf 24.9cpfr 76.2cr,0,,False
235,20.3 69.3,0,,False
236,24.9 77.5 23.4c 78.8 25.6cf 78.2 20.8cpf 70.3cpf 26.0cpfr 79.0r,0,,False
237,TREC7,1,TREC,True
238,"r,1",0,,False
239,"r,2",0,,False
240,MAP p@10 MAP p@10,1,MAP,True
241,21.1 53.7,0,,False
242,22.8 56.9 22.8 57.3 23.4c 57.1 22.1 56.5 25.1cpfr 59.3cpfr,0,,False
243,21.1 53.7,0,,False
244,25.1 60.9 24.2 60.6 25.4f 60.8 22.7fp 57.7 27.3cpfr 63.3cpfr,0,,False
245,TREC8,1,TREC,True
246,"r,1",0,,False
247,"r,2",0,,False
248,MAP p@10 MAP p@10,1,MAP,True
249,23.9 54.3,0,,False
250,25.2 58.8 25.2 59.6 25.9c 59.4 25.4 56.9f 27.8cpfr 60.9r,0,,False
251,23.9 54.3,0,,False
252,27.2 61.6 26.5 61.6 27.6f 61.8 26.0p 58.2cpf 29.2cpfr 63.8cpfr,0,,False
253,TREC9,1,TREC,True
254,"r,1",0,,False
255,"r,2",0,,False
256,MAP p@10 MAP p@10,1,MAP,True
257,19.8 35.2,0,,False
258,28.0 42.2 27.3 43.6 28.8f 43.9 22.6cpf 38.1fp 29.3crf 44.8r,0,,False
259,19.8 35.2,0,,False
260,33.2 47.8 31.5c 47.8 33.5f 48.4 23.4cpf 39.5cpf 33.6fr 48.5r,0,,False
261,"Table 2: Main result table. Boldface marks the best result in a column. Italics marks performance that is statistically significantly better than that of CombMNZ. 'c', 'f ', 'p' and 'r' mark statistically significant differences with CorpusRank, FusedListReRank, PoolRank and ReFuse, respectively.",0,,False
262,"An important question in evaluating the retrieval effectiveness of methods that utilize relevance feedback is whether to consider for evaluation the given set of relevant documents [9]. To compare our methods to each other with respect to various aspects (e.g., the number relevant documents), we consider the given relevant documents in the evaluation presented in Sections 4.2.1, 4.2.2 and 4.2.3. We note that our methods do not directly position the given relevant documents at the highest ranks of the final result list. Thus, this evaluation also enables to study their ability to rank high the given relevant documents. When comparing our methods with reference comparisons in Sections 4.2.4 and 4.2.5, we use a residual corpus approach for evaluation [9]: the given relevant documents (and in Section 4.2.4 also the given non-relevant documents) are not considered in the evaluation. For completeness, we re-visit the comparison of our methods using the residual corpus evaluation approach in Section 4.2.6. We note that in all cases the final result list that is evaluated contains n , 100 documents.",0,,False
263,Free-parameter values. Our methods that use RM3 de-,0,,False
264,"pend on its free parameters. The MetaFuse method has an additional free parameter, . The free-parameter values, in all methods, are set using leave-one-out cross validation performed over queries. MAP serves as the optimization criterion for the train set. The parameter  in MetaFuse is set to values in {0, 0.1, . . . , 1}. The values of  and , which are used by RM3, are selected from {0.5, 0.8, 0.9, 1} and {10, 50, 75}, respectively8.",1,ad,True
265,4.2 Experimental results,0,,False
266,4.2.1 Main result,0,,False
267,"We show in Section 4.2.3 that the infAP list-effectiveness estimate is more effective than the others we consider. Hence, unless otherwise stated, the results presented for ReFuse and MetaFuse are based on using infAP. In practical scenarios, very few relevant documents are available as feedback (if at all). Thus, in Table 2 we present results for r  {1, 2}. Below we study the effect of further varying r.",1,AP,True
268,"We see in Table 2 that, as is expected, all methods that use the relevance feedback are more effective -- almost always to a statistically significant degree ­ than CombMNZ which",0,,False
269,"8The optimal value of  as determined over the train sets of queries was in most cases  0.9; that of  was in most cases in {50, 75}. As a case in point for performance sensitivity analysis, setting  , 0.9 and  ,"" 50 in MetaFuse often results in MAP performance very similar to that attained using leave-one-out cross validation, except for TREC9.""",1,MAP,True
270,does not utilize it. In Section 4.2.6 we show that the same finding holds with the residual-corpus evaluation approach.,0,,False
271,"We can also see in Table 2 that CorpusRank, which ranks the entire corpus using the relevance model, is somewhat more effective in terms of MAP than FusedListReRank; FusedListReRank uses the relevance model to re-rank the list produced by CombMNZ (Lfuse). In terms of p@10, the reverse often holds. However, the performance differences are statistically significant in very few cases.",1,MAP,True
272,We now turn to analyze the performance of the methods that leverage the special characteristics of the fusion setting when exploiting the relevance feedback. PoolRank uses the relevance model to rank the pool of documents in the intermediate retrieved lists. Its performance is better in most relevant comparisons (track × evaluation measure) than that of CorpusRank and FusedListReRank; in quite a few cases the performance improvements are statistically significant.,0,,False
273,"The ReFuse method uses the relevance feedback to estimate the effectiveness of the intermediate lists so as to refuse them. Its performance is worse than that of the CorpusRank, FusedListReRank and PoolRank methods; these use the relevance model induced from the relevant documents to directly rank documents. Yet, we see that the performance of ReFuse is superior to that of CombMNZ in a vast majority of the relevant comparisons. CombMNZ uses uniform list-effectiveness estimates, while ReFuse utilizes infAP (here) with the given feedback to estimate list effectiveness.",1,AP,True
274,"The most effective method in Table 2 is MetaFuse. Specifically, MetaFuse always outperforms -- often substantially and to a statistically significant degree -- CorpusRank and FusedListReRank. These two methods use the relevance feedback as in the single retrieved list case; i.e., they do not leverage the fact that feedback is provided for a list which results from fusion. MetaFuse does leverage this fact by integrating PoolRank and ReFuse. Thus, we conclude that there is much merit in exploiting the special characteristics of the fusion setting when using the relevance feedback.",0,,False
275,"We also see in Table 2 that although PoolRank is always more effective than ReFuse, MetaFuse that integrates the two yields performance that transcends that of both; the improvements are statistically significant in most relevant comparisons. This finding attests to the fact that the two purposes for which the relevance feedback is used in MetaFuse -- direct ranking of documents and list-effectiveness estimation for re-fusion -- are complementary.",0,,False
276,Varying the number of relevant documents. Figure 1 ex-,0,,False
277,"hibits the effect of varying r, the number of given relevant documents, on the performance of the various methods.",0,,False
278,317,0,,False
279,MAP MAP MAP MAP,1,MAP,True
280,0.32,0,,False
281,CombMNZ,0,,False
282,CorpusRank,0,,False
283,0.3,0,,False
284,FusedListReRank PoolRank,0,,False
285,ReFuse,0,,False
286,0.28,0,,False
287,MetaFuse,0,,False
288,TREC3,1,TREC,True
289,CombMNZ,0,,False
290,0.34,0,,False
291,CorpusRank,0,,False
292,FusedListReRank,0,,False
293,0.32,0,,False
294,PoolRank ReFuse,0,,False
295,MetaFuse,0,,False
296,0.3,0,,False
297,TREC7,1,TREC,True
298,0.34,0,,False
299,CombMNZ,0,,False
300,CorpusRank,0,,False
301,FusedListReRank,0,,False
302,0.32,0,,False
303,PoolRank ReFuse,0,,False
304,MetaFuse,0,,False
305,0.3,0,,False
306,TREC8,1,TREC,True
307,0.45,0,,False
308,CombMNZ,0,,False
309,CorpusRank,0,,False
310,0.4,0,,False
311,FusedListReRank PoolRank,0,,False
312,ReFuse,0,,False
313,MetaFuse,0,,False
314,0.35,0,,False
315,TREC9,1,TREC,True
316,0.26,0,,False
317,0.28,0,,False
318,0.28,0,,False
319,0.3,0,,False
320,0.24,0,,False
321,0.26,0,,False
322,0.22,0,,False
323,0.24,0,,False
324,0.26,0,,False
325,0.25,0,,False
326,0.2,0,,False
327,0.22,0,,False
328,0.24,0,,False
329,0.2,0,,False
330,1,0,,False
331,2,0,,False
332,3,0,,False
333,4,0,,False
334,5,0,,False
335,1,0,,False
336,2,0,,False
337,3,0,,False
338,4,0,,False
339,5,0,,False
340,1,0,,False
341,2,0,,False
342,3,0,,False
343,4,0,,False
344,5,0,,False
345,1,0,,False
346,2,0,,False
347,3,0,,False
348,4,0,,False
349,5,0,,False
350,r,0,,False
351,r,0,,False
352,r,0,,False
353,r,0,,False
354,Figure 1: The effect of varying the number of relevant documents (r) on MAP performance. Note: figures are not to the same scale.,1,MAP,True
355,"Our first observation is that the performance of all methods increases with increasing values of r. The CorpusRank, FusedListReRank and PoolRank methods directly rank documents using the relevance model which is constructed from the r relevant documents. Thus, these methods benefit from the improved quality of the relevance model when constructed from more relevant documents. The ReFuse method uses the feedback (relevant and non-relevant documents) with infAP to estimate the effectiveness of the intermediate lists so as to re-fuse them. Thus, increasing r, which means more feedback, results in more reliable estimates. Naturally then, the effectiveness of MetaFuse, which integrates PoolRank and ReFuse, improves with increasing values of r.",1,AP,True
356,"We also see in Figure 1 that the relative performance patterns of the methods across the values of r are consistent with those exhibited in Table 2 for r  {1, 2}. For example, PoolRank is in many cases more effective than CorpusRank and FusedListReRank. This provides further support to the relative merits of using the relevance model to rank the pool of documents in the intermediate lists with respect to using it to (re-)rank the entire corpus or the final fused list.",0,,False
357,"We observe in Figure 1 that although PoolRank is consistently more effective than ReFuse, MetaFuse which integrates them is in most cases more effective than both. We note that most of the improvements over ReFuse, and many of the improvements over PoolRank, specifically for TREC7 and TREC8, were found to be statistically significant.",1,TREC,True
358,"With increasing values of r the performance difference between MetaFuse and PoolRank become smaller. (For r > 2 in TREC9 the performance is almost identical.) The reason is that the performance differences between PoolRank and ReFuse become larger when increasing r. Indeed, the relative performance improvements of PoolRank with increasing values of r are larger than those of ReFuse. This finding means that the improvements in the quality of the relevance model used in PoolRank have relatively more impact on the resultant retrieval effectiveness than those of the list effectiveness estimates used in ReFuse for re-fusion.",1,TREC,True
359,4.2.2 Balancing the roles of the relevance feedback,0,,False
360,"The parameter  in MetaFuse controls the balance between the two purposes (roles) that the relevance feedback serves. Higher values of  result in more reliance on using the relevance model in PoolRank to rank the documents in the pool; for  ,"" 1, MetaFuse becomes PoolRank. Lower values of  result in more reliance on using the relevance feedback to estimate list effectiveness for re-fusion in ReFuse; specifi-""",0,,False
361,"cally,  ,"" 0 amounts to ReFuse. In Figure 2 we present the effect of varying  on the performance of MetaFuse. The other free parameters of the methods (i.e., those of the relevance model) are set using leave-one-out cross validation.""",0,,False
362,"We see in Figure 2 that for  > 0 MetaFuse outperforms ReFuse (MetaFuse with  , 0). The reason is that MetaFuse integrates ReFuse with PoolRank (MetaFuse with  ,"" 1) and the latter outperforms the former. Yet, often, the best performance of MetaFuse is attained for  < 1. This finding attests to the merit in using the relevance feedback simultaneously to directly rank documents in the pool (PoolRank) and to estimate list effectiveness for re-fusion (ReFuse).""",0,,False
363,A general trend observed in Figure 2 is that the optimal value of  rises when increasing the number of relevant documents (r). This finding can be explained by the fact that the performance of PoolRank improves to a much larger extent than that of ReFuse when increasing the value of r as discussed above for Figure 1.,0,,False
364,4.2.3 Comparing list-effectiveness estimates,0,,False
365,One of the two purposes for which relevance feedback is,0,,False
366,"used in our methods -- specifically, in ReFuse (Equation 4)",0,,False
367,that is used by MetaFuse -- is to estimate the effectiveness,0,,False
368,"of the intermediate lists. Insofar, infAP was used in the eval-",1,AP,True
369,uation. We now turn to study the performance of ReFuse,0,,False
370,when using all the list-effectiveness estimates proposed in,0,,False
371,Section 3.2.1. Recall that we are provided with relevance,0,,False
372,feedback for the set F (Lfuse) of the documents most highly,0,,False
373,"ranked in the fused list Lfuse. Thus, for many documents",0,,False
374,in the intermediate lists there are no relevance judgments.,0,,False
375,"For comparison, we study two additional list-effectiveness",1,ad,True
376,estimates which are applied in ReFuse and which do not use,0,,False
377,the relevance feedback. The first is uniform that considers,0,,False
378,all intermediate lists to be of the same effectiveness. Us-,0,,False
379,ing ReFuse with uniform amounts to the CombSUM fusion,0,,False
380,method [14] mentioned in Section 3.,0,,False
381,"The second estimate is overlap [38]. For a list Li, the",0,,False
382,overlap,0,,False
383,is,0,,False
384,defined,0,,False
385,as,0,,False
386,P,0,,False
387,"j,i",0,,False
388,2|Li T Lj | |Li|+|Lj |,0,,False
389,--,0,,False
390,"i.e.,",0,,False
391,the,0,,False
392,normalized,0,,False
393,sum of its overlap with all other lists. Conceptually similar,0,,False
394,list-effectiveness estimates were used in other work on fusion,0,,False
395,[39] and in work on evaluating the effectiveness of search,0,,False
396,systems without relevance judgments [34]. The premise is,0,,False
397,that inter-list similarity (in terms of shared documents) in-,0,,False
398,dicates effective retrieval. The performance of ReFuse using,0,,False
399,the list-effectiveness estimates is presented in Figure 3.,0,,False
400,318,0,,False
401,MAP MAP MAP MAP,1,MAP,True
402,MAP MAP MAP MAP,1,MAP,True
403,TREC3 0.28,1,TREC,True
404,0.27,0,,False
405,0.26,0,,False
406,0.25,0,,False
407,0.24,0,,False
408,0.23,0,,False
409,0.22,0,,False
410,"r,1",0,,False
411,"r,2",0,,False
412,0.21,0,,False
413,"r,3 r,4",0,,False
414,0.2,0,,False
415,"r,5",0,,False
416,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,0,,False
417,TREC7,1,TREC,True
418,0.3,0,,False
419,0.28,0,,False
420,0.26,0,,False
421,0.24,0,,False
422,"r,1",0,,False
423,0.22,0,,False
424,"r,2 r,3",0,,False
425,"r,4",0,,False
426,0.2,0,,False
427,"r,5",0,,False
428,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,0,,False
429,TREC8,1,TREC,True
430,0.31,0,,False
431,0.3,0,,False
432,0.29,0,,False
433,0.28,0,,False
434,0.27,0,,False
435,0.26,0,,False
436,0.25,0,,False
437,"r,1",0,,False
438,"r,2",0,,False
439,0.24,0,,False
440,"r,3",0,,False
441,"r,4",0,,False
442,0.23,0,,False
443,"r,5",0,,False
444,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 ,0,,False
445,TREC9,1,TREC,True
446,0.38,0,,False
447,0.36,0,,False
448,0.34,0,,False
449,0.32,0,,False
450,0.3,0,,False
451,0.28,0,,False
452,0.26,0,,False
453,"r,1 r,2",0,,False
454,"r,3",0,,False
455,0.24,0,,False
456,"r,4",0,,False
457,"r,5",0,,False
458,0.22,0,,False
459,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,0,,False
460,"Figure 2: The performance of MetaFuse for different values of the number of relevant documents (r) when varying .  , 1 amounts to PoolRank and  , 0 amounts to ReFuse. Note: figures are not to the same scale.",0,,False
461,TREC3,1,TREC,True
462,TREC7,1,TREC,True
463,TREC8,1,TREC,True
464,TREC9,1,TREC,True
465,0.23,0,,False
466,AP,1,AP,True
467,infAP 0.225 infAPonlyRel,1,AP,True
468,Kendall,0,,False
469,0.22,0,,False
470,Pearson,0,,False
471,uniform,0,,False
472,0.215,0,,False
473,overlap,0,,False
474,0.21,0,,False
475,0.205,0,,False
476,0.2,0,,False
477,AP,1,AP,True
478,0.25,0,,False
479,infAP,1,AP,True
480,infAPonlyRel,1,AP,True
481,Kendall,0,,False
482,0.24,0,,False
483,Pearson,0,,False
484,uniform,0,,False
485,overlap,0,,False
486,0.23,0,,False
487,0.22,0,,False
488,0.29,0,,False
489,AP infAP,1,AP,True
490,infAPonlyRel,1,AP,True
491,0.28,0,,False
492,Kendall,0,,False
493,Pearson,0,,False
494,uniform,0,,False
495,0.27,0,,False
496,overlap,0,,False
497,0.26,0,,False
498,0.25,0,,False
499,0.28,0,,False
500,AP,1,AP,True
501,infAP,1,AP,True
502,infAPonlyRel,1,AP,True
503,0.26,0,,False
504,Kendall,0,,False
505,Pearson,0,,False
506,uniform,0,,False
507,overlap,0,,False
508,0.24,0,,False
509,0.22,0,,False
510,0.195,0,,False
511,0.21,0,,False
512,0.24,0,,False
513,0.2,0,,False
514,0.19,0,,False
515,0.23,0,,False
516,0.2,0,,False
517,1,0,,False
518,2,0,,False
519,3,0,,False
520,4,0,,False
521,5,0,,False
522,1,0,,False
523,2,0,,False
524,3,0,,False
525,4,0,,False
526,5,0,,False
527,1,0,,False
528,2,0,,False
529,3,0,,False
530,4,0,,False
531,5,0,,False
532,1,0,,False
533,2,0,,False
534,3,0,,False
535,4,0,,False
536,5,0,,False
537,r,0,,False
538,r,0,,False
539,r,0,,False
540,r,0,,False
541,Figure 3: The performance of ReFuse with various list-effectiveness estimates. Note: figures are not to the same scale,0,,False
542,"Our first observation based on Figure 3 is that all listeffectiveness estimates are almost always more effective than the uniform estimate. We also see that the overlap measure, which does not use the relevance feedback, is more effective than the Pearson and Kendall- estimates that do use it. However, overlap is often substantially less effective than the other estimates that use the relevance feedback, namely, infAP, infAPonlyRel and AP.",1,AP,True
543,"We see in Figure 3 that, in general, the performance of ReFuse when employed with the list-effectiveness estimates that use the relevance feedback tends to increase when the number of relevant documents (r) increases. The increase for Pearson and Kendall- is, however, extremely small. Recall that these two estimates, in contrast to infAP, infAPonlyRel and AP, do not estimate list effectiveness directly; rather, via the comparison of the list with its re-ranked version attained by using the relevance model. Thus, it may come as no surprise that using Pearson and Kendall- in ReFuse yields performance that is inferior (often substantially) to that of using infAP, infAPonlyRel and AP.",1,AP,True
544,"It is evident in Figure 3 that using in ReFuse the infAP estimate, which was used insofar in the experiments reported above, results in the best overall performance. infAP is the only estimate that directly exploits the non-relevant documents in F (Lfuse) by differentiating them from unjudged documents -- i.e., documents not in F (Lfuse). Using infAPonlyRel, which treats non-relevant documents as unjudged, and AP which treats non-relevant and unjudged documents the same, result in performance that is often somewhat inferior to that of using infAP.",1,AP,True
545,"With increased number of relevant documents (r) the performance of using AP becomes closer to that of using infAP. (For almost all values of r for TREC9 the performance is almost identical.) The reason is that AP becomes more robust when more relevant documents are available. In contrast, the performance of using infAPonlyRel with increased r becomes more inferior to that of using infAP, because infAPonlyRel treats the non-relevant documents as unjudged. Note that increased r is likely to result in increased number of non-relevant documents in F (Lfuse) by the virtue of the experimental setting described in Section 4.1.",1,AP,True
546,4.2.4 Comparison with the Hedge method,0,,False
547,"As noted in Section 2, there is a single previous report on using relevance feedback in the context of fusion-based retrieval [4]. TREC runs were fused using relevance judgments obtained through an iterative process of active relevance feedback based on the Hedge method [15]. Here, we use the approach as a reference comparison that fuses the intermediate lists using the feedback documents (F (Lfuse)).",1,TREC,True
548,The loss of document d in F (Lfuse) with respect to an intermediate list Li in which it appears is:,0,,False
549,l(d; Li),0,,False
550,"d,ef",0,,False
551,1,0,,False
552,(-1)rel(d),0,,False
553,tmax,0,,False
554,X,0,,False
555,2,0,,False
556,1; j,0,,False
557,(7),0,,False
558,"j,tk",0,,False
559,"tk is the rank of d in Li; rel(d) is 1 if d is relevant and 0 if it is not; tmax ,"" |Dpool| is the size of the pool of documents in the intermediate lists. Then, Li's weight is defined as:""",0,,False
560,wIq (Li; F (Lfuse)),0,,False
561,"d,ef",0,,False
562," , P d:dLi F",0,,False
563,(Lf use ),0,,False
564,l(d;Li ),0,,False
565,(8),0,,False
566,319,0,,False
567,MAP MAP MAP MAP,1,MAP,True
568,TREC3,1,TREC,True
569,TREC7,1,TREC,True
570,TREC8,1,TREC,True
571,TREC9,1,TREC,True
572,0.24 0.22,0,,False
573,0.2,0,,False
574,0.23,0,,False
575,0.21,0,,False
576,0.24,0,,False
577,0.18,0,,False
578,0.22,0,,False
579,0.2,0,,False
580,0.22,0,,False
581,0.16,0,,False
582,0.21,0,,False
583,0.19,0,,False
584,0.14,0,,False
585,0.2,0,,False
586,0.18,0,,False
587,0.2,0,,False
588,0.19,0,,False
589,0.17,0,,False
590,0.12,0,,False
591,0.18,0,,False
592,0.17,0,,False
593,Hedge ReFuse(Hedge),0,,False
594,0.16,0,,False
595,ReFuse PoolRank,0,,False
596,0.15,0,,False
597,MetaFuse,0,,False
598,0.16 Hedge,0,,False
599,0.15 ReFuse(Hedge) ReFuse,0,,False
600,0.14,0,,False
601,PoolRank,0,,False
602,MetaFuse,0,,False
603,0.13,0,,False
604,0.18,0,,False
605,Hedge,0,,False
606,0.16,0,,False
607,ReFuse(Hedge) ReFuse,0,,False
608,PoolRank,0,,False
609,MetaFuse,0,,False
610,0.14,0,,False
611,0.1,0,,False
612,0.08,0,,False
613,Hedge ReFuse(Hedge),0,,False
614,ReFuse,0,,False
615,0.06,0,,False
616,PoolRank,0,,False
617,MetaFuse,0,,False
618,0.04,0,,False
619,1,0,,False
620,2,0,,False
621,3,0,,False
622,4,0,,False
623,5,0,,False
624,1,0,,False
625,2,0,,False
626,3,0,,False
627,4,0,,False
628,5,0,,False
629,1,0,,False
630,2,0,,False
631,3,0,,False
632,4,0,,False
633,5,0,,False
634,1,0,,False
635,2,0,,False
636,3,0,,False
637,4,0,,False
638,5,0,,False
639,r,0,,False
640,r,0,,False
641,r,0,,False
642,r,0,,False
643,"Figure 4: Comparison with the Hedge method [4]. In contrast to the case in previous figures, a special residual corpus approach is used for evaluation wherein the given relevant and non-relevant documents are not considered in the evaluation. Note: figures are not to the same scale.",0,,False
644,"where  is a free parameter with a value in {0.1, . . . , 0.9}. If",0,,False
645,"Li  F (Lfuse) ,  then wIq (Li; F (Lfuse)) d,ef 0. The fusion score of d ( Dpool) is its average weighted loss",0,,False
646,"over all lists, computed as if it is non-relevant (rel(d) d,ef 0):",0,,False
647,SHedge(d),0,,False
648,"d,ef",0,,False
649,X l(d; Li)wIq (Li;,0,,False
650,F (Lfuse)).,0,,False
651,(9),0,,False
652,Li,0,,False
653,"If d  Li, then l(d; Li) is set in Equation 9 to the average loss of all the documents in Dpool \ Li, where these are treated as if they are not relevant and positioned below the documents in Li (i.e., at ranks 101 to tmax , |Dpool|).",0,,False
654,"The original implementation of Hedge as an iterative active feedback approach positioned the given feedback documents at the highest ranks of the final result list [4]. Such direct positioning calls for a residual-corpus approach for evaluation [9]. Specifically, here, the documents in F (Lfuse) are removed from all evaluated rankings and the residual rankings serve for evaluation.",0,,False
655,"Our ReFuse and MetaFuse methods use the infAP listeffectiveness estimate which was shown above to be the most effective among those considered. In addition, we also study an instance of our ReFuse method, ReFuse(Hedge), in which the list-effectiveness estimate is that defined in Equation 8 and used by Hedge. The parameter  used by Hedge and ReFuse(Hedge) is set using leave-one-out cross validation performed over the queries in a track. Recall that the free-parameter values of our methods are also set using leaveone-out cross validation. Figure 4 presents the results.",1,AP,True
656,"We first see in Figure 4 that in contrast to Figures 1, 2 and 3 the curves are (almost always) monotonically decreasing with increasing values of r. The reason is that we use here a residual corpus approach for evaluation wherein all feedback documents are not considered for evaluation.",0,,False
657,"Figure 4 shows that ReFuse outperforms ReFuse(Hedge). This means that infAP is a more effective list-effectiveness estimate than that used by Hedge (Equation 8). Furthermore, in all tracks, except for TREC3, ReFuse outperforms Hedge. Both are linear fusion methods that differ in the listweighting function, and in the scores assigned to documents; in ReFuse the retrieval scores of a document in the lists are used, and in Hedge Equation 9 is used.",1,AP,True
658,"We also see in Figure 4 that PoolRank and MetaFuse substantially outperform Hedge. A vast majority of the performance improvements for PoolRank, and all of them for MetaFuse, were found to be statistically significant.",0,,False
659,4.2.5 Comparison with past-performance-based estimation of list effectiveness,0,,False
660,"ReFuse, and therefore MetaFuse, use the relevance feed-",0,,False
661,back to estimate the effectiveness of the intermediate lists,0,,False
662,so as to re-fuse them. We now turn to compare them with,0,,False
663,methods that estimate list effectiveness based on the past,0,,False
664,performance of the retrieval method used to create the list.,0,,False
665,Past performance is determined using a train set of queries.,0,,False
666,"In our experimental setting, the intermediate lists are de-",0,,False
667,rived from TREC runs. Each run contains the results of a,1,TREC,True
668,retrieval method for the queries in a track. We used a leave-,0,,False
669,one-out cross validation procedure across queries throughout,0,,False
670,"the evaluation. Thus, all queries in a run except for the one",0,,False
671,"at hand serve as the train set. Based on this set, the past",0,,False
672,performance of the retrieval method is determined.,0,,False
673,The Learning method [2] is a linear fusion method that,0,,False
674,uses Equation 4 as is the case for ReFuse. For a list effec-,0,,False
675,tiveness estimate it uses the MAP (mean average precision),1,MAP,True
676,of the run computed over the train set of queries.,0,,False
677,ProbFuse is a highly effective fusion method [23]. It uses,0,,False
678,the train query set (henceforth Q) to estimate the effective-,0,,False
679,ness of segments of the intermediate lists retrieved for a test,0,,False
680,query.,0,,False
681,"Specifically, p^(dk|L) d,ef",0,,False
682,P 1,0,,False
683,"|Rk,q |",0,,False
684,is |Q|,0,,False
685,"qQ |Rk,q |+|Nk,q |",0,,False
686,"an estimate for the probability that a document, denoted",0,,False
687,"dk, in the k'th segment of an intermediate list L, will be relevant to some query. |Rk,q | and |Nk,q | are the number",0,,False
688,"of documents marked as relevant and non-relevant, respectively, to a train query q; these documents appear in the",0,,False
689,"k'th segment of an intermediate list in the train set which was retrieved for q in the same run that L belongs to. That is, Rk,q and Nk,q are documents retrieved in response to q",0,,False
690,by the same retrieval method that produced L.,0,,False
691,A document d in the pool (Dpool) of documents retrieved,0,,False
692,for a test,0,,False
693,query,0,,False
694,is,0,,False
695,scored by,0,,False
696,SP robF use(d),0,,False
697,"d,ef",0,,False
698,P,0,,False
699,Li,0,,False
700,1 k,0,,False
701,p^(dk,0,,False
702,|Li,0,,False
703,"),",0,,False
704,where k is the number of the segment of Li in which d ap-,0,,False
705,"pears; if d  Li then p^(dk|Li) d,ef 0 for all k. We use the",0,,False
706,train set of queries (with MAP serving as the optimization,1,MAP,True
707,criterion) to also set the number of segments in the inter-,0,,False
708,"mediate lists to a value in {2, 4, 6, 8, 10, 15, 20, 25, 30, 40, 50}.",0,,False
709,(Recall that the lists are composed of 100 documents.),0,,False
710,For an additional reference comparison we use CombMNZ,1,ad,True
711,"[14, 22] which essentially utilizes uniform list-effectiveness",0,,False
712,estimates. (Refer back to Section 3 for details.) As Learn-,0,,False
713,"ing, ProbFuse and CombMNZ do not use the relevance feed-",0,,False
714,320,0,,False
715,MAP MAP MAP MAP,1,MAP,True
716,TREC3,1,TREC,True
717,TREC7,1,TREC,True
718,TREC8,1,TREC,True
719,TREC9,1,TREC,True
720,0.24,0,,False
721,0.22,0,,False
722,0.24,0,,False
723,0.2,0,,False
724,0.18,0,,False
725,0.22,0,,False
726,0.2,0,,False
727,0.22,0,,False
728,0.16,0,,False
729,0.2,0,,False
730,0.18,0,,False
731,0.2,0,,False
732,0.14,0,,False
733,0.12,0,,False
734,0.16,0,,False
735,0.18,0,,False
736,0.18,0,,False
737,0.1,0,,False
738,CombMNZ 0.16 Learning,0,,False
739,ProbFuse ReFuse,0,,False
740,0.14 MetaFuse,0,,False
741,0.14 CombMNZ Learning ProbFuse,0,,False
742,0.12 ReFuse MetaFuse,0,,False
743,0.16 CombMNZ Learning,0,,False
744,0.14 ProbFuse ReFuse,0,,False
745,0.12 MetaFuse,0,,False
746,0.08 CombMNZ Learning,0,,False
747,0.06 ProbFuse ReFuse,0,,False
748,0.04 MetaFuse,0,,False
749,1,0,,False
750,2,0,,False
751,3,0,,False
752,4,0,,False
753,5,0,,False
754,1,0,,False
755,2,0,,False
756,3,0,,False
757,4,0,,False
758,5,0,,False
759,1,0,,False
760,2,0,,False
761,3,0,,False
762,4,0,,False
763,5,0,,False
764,1,0,,False
765,2,0,,False
766,3,0,,False
767,4,0,,False
768,5,0,,False
769,r,0,,False
770,r,0,,False
771,r,0,,False
772,r,0,,False
773,Figure 5: Comparison with fusion methods that estimate list effectiveness based on past performance of the retrieval method. A residual corpus evaluation approach is used where the given relevant documents are not considered in the evaluation. Note: figures are not to the same scale.,0,,False
774,"back, specifically, the given relevant documents, we use a residual-corpus approach to evaluation [9]: the given relevant documents are removed from all evaluated rankings and the residual rankings serve for evaluation. Figure 5 presents the performance numbers. All curves are monotonically decreasing due to the residual-corpus evaluation approach.",0,,False
775,"We see in Figure 5 that all methods outperform CombMNZ in almost all cases. We also see that in many cases ReFuse outperforms Learning. The main exceptions are for a small number of relevant documents (r) for TREC3 and TREC7. It is important to recall that the feedback is provided for the fused list (Lfuse) and not for the intermediate lists. Thus, the feedback available for the intermediate lists is scarce as was discussed in Section 3.2.1. Thus, we conclude that estimating list effectiveness with minimal relevance feedback can often result in better fusion performance than that of estimating list effectiveness using (much) information about the past performance of the retrieval method.",1,TREC,True
776,"We also see in Figure 5 that ProbFuse outperforms ReFuse and Learning. This comes as no surprise because ProbFuse uses estimates for the effectiveness of segments of the retrieved lists (and higher segments are rewarded) while ReFuse and Learning use estimates for the entire lists. Thus, a future direction is integrating the feedback-based list effectiveness estimates of ReFuse with the segment-based ones of ProbFuse. Yet, as shown in Figure 5, our MetaFuse method that utilizes the relevance feedback, but does not rely on past performance of the retrieval method to estimate list effectiveness, consistently outperforms ProbFuse; many of the improvements are substantial and statistically significant.",0,,False
777,4.2.6 Further evaluation using the residual corpus approach,0,,False
778,The main comparison of our approaches which was presented in Section 4.2.1 was based on considering the given relevant documents for the evaluation. Here we compare the methods' performance with the residual corpus approach [9]: the given relevant documents are removed from any ranking that is evaluated and the residual ranking serves for evaluation. Figure 6 presents the performance curves.,0,,False
779,"We observe in Figure 6 the same relative performance patterns observed in Figure 1; the latter was based on evaluation that considers the given relevant documents. Specifically, (i) all methods that use the relevance feedback perform (almost always) better than CombMNZ which does not use it; we note that almost all of these improvements are sta-",0,,False
780,"tistically significant; (ii) using the relevance model induced from the relevant documents to rank only the the pool of documents in the intermediate retrieved lists (PoolRank) is often more effective than using it to rank the entire corpus (CorpusRank) or to re-rank the final fused list (FusedListReRank); (iii) the methods that use the relevance model to directly rank documents (CorpusRank, FusedListReRank, PoolRank, MetaFuse) are more effective than the ReFuse method that uses the relevance feedback to estimate the effectiveness of the intermediate lists so as to re-fuse them; and, (iv) our MetaFuse method, which uses the relevance feedback to both directly rank documents and re-fuse the intermediate lists, is the most effective. Many of the improvements it posts over the other methods are substantial and were found to be statistically significant.",0,,False
781,5. CONCLUSIONS AND FUTURE WORK,0,,False
782,"We addressed the challenge of using relevance feedback in the fusion-based retrieval setting. That is, the feedback is provided for the documents most highly ranked in a list that results from fusing several intermediate retrieved lists.",1,ad,True
783,"We devised methods that use the relevance feedback for two different, yet complementary, purposes. The first is directly ranking the pool of documents in the intermediate lists. The second is estimating the effectiveness of the intermediate lists so as to re-fuse them. We presented a meta fusion approach that uses the feedback for these two purposes simultaneously.",0,,False
784,"Empirical evaluation demonstrated the merits of our approach. For example, the resultant retrieval performance is much better than that of using the feedback as in the single retrieved list setting; i.e., ignoring the fact that the feedback is provided for a list that results from fusion.",0,,False
785,"We plan to explore additional list-effectiveness estimates to be used in our approach. Adapting our methods to use pseudo feedback, rather than true feedback, is another interesting future venue.",1,ad,True
786,6. ACKNOWLEDGMENTS,0,,False
787,We thank the reviewers for their comments. This paper is based upon work done in part while Ofri Rom was at the Technion. Oren Kurland's work is supported in part by the Israel Science Foundation under grant no. 433/12 and by Google's and Yahoo!'s faculty research awards.,1,Yahoo,True
788,321,0,,False
789,MAP MAP MAP MAP,1,MAP,True
790,0.28 0.26 0.24 0.22 0.2 0.18 0.16,0,,False
791,1,0,,False
792,TREC3,1,TREC,True
793,CombMNZ,0,,False
794,0.26,0,,False
795,CorpusRank,0,,False
796,FusedListReRank,0,,False
797,0.24,0,,False
798,PoolRank,0,,False
799,ReFuse MetaFuse,0,,False
800,0.22,0,,False
801,0.2,0,,False
802,0.18,0,,False
803,0.16,0,,False
804,0.14,0,,False
805,0.12,0,,False
806,2,0,,False
807,3,0,,False
808,4,0,,False
809,5,0,,False
810,1,0,,False
811,r,0,,False
812,TREC7,1,TREC,True
813,CombMNZ,0,,False
814,CorpusRank,0,,False
815,0.26,0,,False
816,FusedListReRank,0,,False
817,PoolRank ReFuse,0,,False
818,0.24,0,,False
819,MetaFuse,0,,False
820,0.22,0,,False
821,0.2,0,,False
822,0.18,0,,False
823,0.16,0,,False
824,0.14,0,,False
825,0.12,0,,False
826,2,0,,False
827,3,0,,False
828,4,0,,False
829,5,0,,False
830,1,0,,False
831,r,0,,False
832,TREC8,1,TREC,True
833,CombMNZ CorpusRank,0,,False
834,0.2,0,,False
835,FusedListReRank,0,,False
836,PoolRank,0,,False
837,0.18,0,,False
838,ReFuse,0,,False
839,MetaFuse,0,,False
840,0.16,0,,False
841,0.14,0,,False
842,0.12,0,,False
843,0.1,0,,False
844,0.08,0,,False
845,0.06,0,,False
846,2,0,,False
847,3,0,,False
848,4,0,,False
849,5,0,,False
850,1,0,,False
851,r,0,,False
852,TREC9,1,TREC,True
853,CombMNZ CorpusRank FusedListReRank,0,,False
854,PoolRank ReFuse,0,,False
855,MetaFuse,0,,False
856,2,0,,False
857,3,0,,False
858,4,0,,False
859,5,0,,False
860,r,0,,False
861,Figure 6: Performance comparison of our methods using the residual corpus evaluation approach where the given relevant documents are not considered for evaluation. Note: figures are not to the same scale.,0,,False
862,7. REFERENCES,0,,False
863,"[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 -- novelty and hard. In Proc. of TREC-13, pages 715­725, 2004.",1,ad,True
864,"[2] C. C. V. ant Garrison W. Cottrell. Fusion via linear combination of scores. Information Retrieval, 1(3):151­173, 1999.",0,,False
865,"[3] J. A. Aslam and M. Montague. Models for metasearch. In Proc. of SIGIR, pages 276­284, 2001.",0,,False
866,"[4] J. A. Aslam, V. Pavlu, and R. Savell. A unified model for metasearch, pooling, and system evaluation. In Proc. of CIKM, pages 484­491, 2003.",0,,False
867,"[5] J. A. Aslam, V. Pavlu, and E. Yilmaz. Measure-based metasearch. In Proc. of SIGIR, pages 571­572, 2005.",0,,False
868,"[6] N. Balasubramanian and J. Allan. Learning to select rankers. In Proc. of SIGIR, pages 855­856, 2010.",0,,False
869,"[7] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked retrieval systems. In Proc. of SIGIR, pages 173­181, 1994.",0,,False
870,"[8] S. M. Beitzel, E. C. Jensen, A. Chowdhury, O. Frieder, D. A. Grossman, and N. Goharian. Disproving the fusion hypothesis: An analysis of data fusion via effective information retrieval strategies. In Proc. of SAC, pages 823­827, 2003.",0,,False
871,"[9] C. Buckley and S. Robertson. Relevance feedback track overview: TREC 2008. In Proc. of TREC-17, 2008.",1,TREC,True
872,"[10] A. Chowdhury, O. Frieder, D. A. Grossman, and M. C. McCabe. Analyses of multiple-evidence combinations for retrieval strategies. In Proc. of SIGIR, pages 394­395, 2001.",0,,False
873,"[11] W. B. Croft, editor. Advances in Information Retrieval: Recent Research from the Center for Intelligent Information Retrieval. Number 7 in The Kluwer International Series on Information Retrieval. Kluwer, 2000.",0,,False
874,"[12] W. B. Croft. Combining approaches to information retrieval. In Croft [11], chapter 1, pages 1­36.",0,,False
875,"[13] W. B. Croft and J. Lafferty, editors. Language Modeling for Information Retrieval. Number 13 in Information Retrieval Book Series. Kluwer, 2003.",0,,False
876,"[14] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proc. of TREC-2, 1994.",1,TREC,True
877,"[15] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computing Systems Science, 55(1):119­139, 1997.",0,,False
878,"[16] T. Hofmann, J. Puzicha, and M. I. Jordan. Learning from dyadic data. In Proc. of NIPS, pages 466­472, 1998.",1,ad,True
879,"[17] E. Ide. New experiments in relevance feedback. In Salton G. (Ed.), The SMART Retrieval System (pp. 337-354). Englewood Cliffs, N. J.: Prentice-Hall, Inc, 1971.",0,,False
880,"[18] M. Karimzadehgan and C. Zhai. Improving retrieval accuracy of difficult queries through generalizing negative document language models. In Proc. of CIKM, pages 27­36, 2011.",1,ad,True
881,"[19] A. K. Kozorovitzky and O. Kurland. Cluster-based fusion of retrieved lists. In Proc. of SIGIR, pages 893­902, 2011.",0,,False
882,"[20] V. Lavrenko and W. B. Croft. Relevance models in information retrieval. In Croft and Lafferty [13], pages 11­56.",0,,False
883,"[21] J. H. Lee. Combining multiple evidence from different properties of weighting schemes. In Proc. of SIGIR, pages 180­188, 1995.",0,,False
884,"[22] J. H. Lee. Analyses of multiple evidence combination. In Proc. of SIGIR, pages 267­276, 1997.",0,,False
885,"[23] D. Lillis, F. Toolan, R. W. Collier, and J. Dunnion. Probfuse: a probabilistic approach to data fusion. In Proc. of SIGIR, pages 139­146, 2006.",0,,False
886,"[24] I. Markov, A. Arampatzis, and F. Crestani. Unsupervised linear score normalization revisited. In Proc. of SIGIR, pages 1161­1162, 2012.",0,,False
887,"[25] G. Markovits, A. Shtok, O. Kurland, and D. Carmel. Predicting query performance for fusion-based retrieval. In Proc. of CIKM, pages 813­822, 2012.",0,,False
888,"[26] M. Montague and J. A. Aslam. Condorcet fusion for improved retrieval. In Proc. of CIKM, pages 538­548, 2002.",0,,False
889,"[27] M. H. Montague and J. A. Aslam. Relevance score normalization for metasearch. In Proc. of CIKM, pages 427­433, 2001.",0,,False
890,"[28] K. B. Ng and P. P. Kantor. An investigation of the preconditions for effective data fusion in information retrieval: A pilot study, 1998.",0,,False
891,"[29] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, editor, The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313­323. Prentice Hall, 1971.",0,,False
892,"[30] I. Ruthven and M. Lalmas. A survey on the use of relevance feedback for information access systems. Knowledge Engineering Review, 18(2):95­145, 2003.",0,,False
893,"[31] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. Lambdamerge: merging the results of query reformulations. In Proc. of WSDM, pages 795­804, 2011.",0,,False
894,"[32] M. Shokouhi. Segmentation of search engine results for effective data-fusion. In Proc. of ECIR, pages 185­197, 2007.",0,,False
895,"[33] A. Shtok, O. Kurland, and D. Carmel. Using statistical decision theory and relevance models for query-performance prediction. In Proc. of SIGIR, 2010.",0,,False
896,"[34] I. Soboroff, C. K. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In Proc. of SIGIR, pages 66­73, 2001.",0,,False
897,"[35] T. Tsikrika and M. Lalmas. Merging techniques for performing data fusion on the web. In Proc. of CIKM, pages 127­134, 2001.",0,,False
898,"[36] C. C. Vogt and G. W. Cottrell. Predicting the performance of linearly combined IR systems. In Proc. of SIGIR, pages 190­196, 1998.",0,,False
899,"[37] X. Wang, H. Fang, and C. Zhai. A study of methods for negative relevance feedback. In Proc. of SIGIR, pages 219­226, 2008.",0,,False
900,"[38] S. Wu. Data fusion in information retrieval. Springer, 2012.",0,,False
901,"[39] S. Wu and F. Crestani. Data fusion with estimated weights. In Proc. of CIKM, pages 648­651, 2002.",0,,False
902,"[40] E. Yilmaz and J. A. Aslam. Estimating average precision with incomplete and imperfect judgments. In Proc. of CIKM, pages 102­111, 2006.",0,,False
903,"[41] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. of SIGIR, pages 334­342, 2001.",1,ad,True
904,322,0,,False
905,,0,,False

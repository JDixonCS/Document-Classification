,sentence,label,data,regex
0,Necessary and Frequent Terms in Queries,0,,False
1,Jiepu Jiang,0,,False
2,Center for Intelligent Information Retrieval School of Computer Science,0,,False
3,University of Massachusetts Amherst,0,,False
4,jpjiang@cs.umass.edu,0,,False
5,James Allan,0,,False
6,Center for Intelligent Information Retrieval School of Computer Science,0,,False
7,University of Massachusetts Amherst,0,,False
8,allan@cs.umass.edu,0,,False
9,ABSTRACT,0,,False
10,"Vocabulary mismatch has long been recognized as one of the major issues affecting search effectiveness. Ineffective queries usually fail to incorporate important terms and/or incorrectly include inappropriate keywords. However, in this paper we show another cause of reduced search performance: sometimes users issue reasonable query terms, but systems cannot identify the correct properties of those terms and take advantages of the properties. Specifically, we study two distinct types of terms that exist in all search queries: (1) necessary terms, for which term occurrence alone is indicative of document relevance; and (2) frequent terms, for which the relative term frequency is indicative of document relevance within the set of documents where the term appears. We evaluate these two properties of query terms in a dataset. Results show that only 1/3 of the terms are both necessary and frequent, while another 1/3 only hold one of the properties and the final third do not hold any of the properties. However, existing retrieval models do not clearly distinguish terms with the two properties and consider them differently. We further show the great potential of improving retrieval models by treating terms with distinct properties differently.",1,corpora,True
11,Categories and Subject Descriptors,0,,False
12,"H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ query formulation, retrieval models.",0,,False
13,General Terms,0,,False
14,"Performance, Experimentation, Human Factors.",0,,False
15,Keywords,0,,False
16,Query; term frequency; term occurrence.,1,Query,True
17,1. INTRODUCTION,1,DUC,True
18,"Term frequency (TF) is widely used as an important heuristic in retrieval models [1­3]. The assumption is that documents with comparatively higher frequencies of query terms are more likely to be relevant. However, we suspect that in many cases this assumption does not hold. Instead, users may adopt some query terms to simply include or exclude documents regardless of the occurrences of the terms ­ that is, in those cases TF does not indicate the relevance of documents as long as the terms appear. In such cases, retrieval models that heavily exploit TF may",1,ad,True
19,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'14, July 06­11, 2014, Gold Coast, QLD, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 $15.00. http://dx.doi.org/10.1145/2600428.2609536",1,ad,True
20,incorrectly rank some non-relevant documents with high frequencies of the terms to the top.,0,,False
21,"We define the following two properties of query terms. We say that a term is necessary to a topic if most relevant documents contain the term. Documents with no occurrences of the necessary term are unlikely to be relevant. In comparison, we say that a term is frequent to a topic if relevant documents usually have relatively more occurrences of the term comparing to the non-relevant ones. Documents in which the frequent term appears many times are more likely relevant compared to those where the term appears less frequently. Note that the two properties do not conflict with each other: a term can be both necessary and frequent.",0,,False
22,"We hypothesize that both necessary and frequent terms exist in user queries, but some query terms may only conform to one of the two properties. We study the following research questions:",0,,False
23,RQ1: Do query terms differ with respect to the two properties? We examine the two properties of query terms in a dataset based on term occurrences in relevant and non-relevant documents.,0,,False
24,RQ2: How do users perceive the two properties of query terms? Do users' opinions agree with those learned from the dataset and do users agree with each other? We ask assessors to annotate query terms regarding the two properties and analyze the results.,0,,False
25,"RQ3: Assuming we know the properties of query terms, can we improve search performance by treating terms differently? We show a simple approach that can achieve 35% improvement in nDCG@10 compared to the query likelihood model, if it knows these properties of query terms. Results suggests great potential for improving search performance by identifying properties of query terms and treating them differently in retrieval models.",0,,False
26,2. EVALUATION OF TERM PROPERTIES,0,,False
27,"In this section, we define indicators and examine query term properties in the TREC Robust 2004 dataset.",1,TREC,True
28,2.1 Indicators of Term Properties,0,,False
29,We denote the degree to which a query term w is necessary to a,0,,False
30,"topic by P(X,""1|R), the probability of observing w in the set of""",0,,False
31,"relevant documents, R. X,1 refers to the occurrence of w in a",0,,False
32,document regardless of its frequency. In a dataset with R being,0,,False
33,"judged, we can estimate P(X,""1|R) by Equation (1), where: N is""",0,,False
34,the total number of documents in R; Nw is the number of documents in R where w appears at least once. The greater the,0,,False
35,"value of P(X,""1|R), the more necessary the term to the topic.""",0,,False
36,P^ ,0,,False
37,X,0,,False
38,1|,0,,False
39,R,0,,False
40,Nw N,0,,False
41,(1),0,,False
42,We evaluate to what degree a query term w is frequent to a,0,,False
43,"topic by comparing P(w|R) and P(w|NR), where NR is the set of",0,,False
44,non-relevant documents. P(w|R) is the probability of the term w in,0,,False
45,"relevant documents, which is estimated by Equation (2), where:",0,,False
46,P(w|d) is the probability of w in the multinomial document,0,,False
47,language model of d; each document d in R has an equal weight,0,,False
48,1/N to contribute to P(w|R). We estimate P(w|d) using maximum,0,,False
49,likelihood estimation with Dirichlet smoothing [4]. The parameter,0,,False
50, is selected to optimize the nDCG@10 of query likelihood model,0,,False
51,1167,0,,False
52,1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0,0,,False
53,0,0,,False
54,"P(X,1|R)",0,,False
55,75 150 225 300 375 450 525 600,0,,False
56,(a),0,,False
57,10000.000 1000.000,0,,False
58,100.000 10.000 1.000 0 0.100 0.010,0,,False
59,0.001,0,,False
60,P(w|R)/P(w|NR),0,,False
61,75 150 225 300 375 450 525 600,0,,False
62,(b),0,,False
63,16.000 8.000,0,,False
64,4.000 2.000 1.000,0,,False
65,0 0.500 0.250,0,,False
66,0.125,0,,False
67,"P(w|R, X,""1)/P(w|NR, X"",1)",0,,False
68,75 150 225 300 375 450 525 600,0,,False
69,(c),0,,False
70,"Figure 1. Distribution of P(X,""1|R), P(X"",1|R)/P(X,""1|NR), P(w|R)/P(w|NR), and P(w|R, X"",""1)/P(w|NR, X"",1) on 663 query terms.",0,,False
71,"in the dataset. We estimate P(w|NR) in a similar form but among the set of non-relevant documents. The greater the value of P(w|R) compared to P(w|NR), the more frequent is the term w.",0,,False
72,P^ w |,0,,False
73,R,0,,False
74,1 N,0,,False
75, dR,0,,False
76,Pw|,0,,False
77,d,0,,False
78,(2),0,,False
79,It should be noted that we can easily observe P(w|R) > P(w|NR),0,,False
80,"when w is necessary for R but rarely appears in NR. Therefore, we",0,,False
81,further examine a stronger form of the frequent term property:,0,,False
82,"within the set of documents where w appears at least once,",0,,False
83,relatively higher frequency of the term indicates greater likelihood,0,,False
84,of relevance. We quantify this stronger property by comparing,0,,False
85,"P(w|X,""1,R) and P(w|X"",""1,NR). The two probabilities are estimated""",0,,False
86,"similar to Equation (2), but within the set of relevant and non-",0,,False
87,relevant documents where w appears at least once.,0,,False
88,2.2 Evaluation,0,,False
89,"We calculate the indicators related to term properties in TREC Robust 2004 dataset. The dataset includes 250 queries and 663 query terms (counting multiple occurrences of the same term in different queries). We remove the Indri standard stopwords and stem using the Krovetz stemmer when processing documents and queries. Figure 1(a), 1(b), and 1(c) show the distribution of P(X,""1|R), P(w|R)/P(w|NR), and P(w|R,X"",""1)/P(w|NR,X"",1) for the 663 query terms.",1,TREC,True
90,"Results show that it is very common to use query terms that do not hold the two properties. As shown in Fig. 1(a), among the 663 query terms, only 18.5% are fully necessary ­ i.e., P(X,1|R),1 ­ and 44.8% roughly hold the necessary property ­ P(X,""1|R)0.8. Moreover, 33% of the query terms do not hold the necessary property (P(X"",""1|R)<0.5), and 50% of the queries have at least one such term. Using query terms with the frequent term property is also very common in the dataset: Figures 1(b) and 1(c) show that 475 out of the 663 query terms (71.6%) hold the basic frequent term property, but only 373 (56.3%) hold the stronger form where P(w|R,X"",""1)/P(w|NR,X"",""1)>1. Among the 250 queries, 57.8% have at least one term that does not hold the frequent term property and 75.1% have at least one term that does not hold the stronger form of the frequent term property.""",0,,False
91,"We further evaluate the relation between search effectiveness and using query terms that do not hold the two properties. Figure 2 shows the average nDCG@10 of queries in which at least one term's value of the three indicators is less than P, where P ranges from 0.1 to 1.0. Results suggest that queries with terms that do not hold either of the two properties are less effective. For example, for the set of queries with at least one term's value of P(X,""1|R) < 0.5, the nDCG@10 of these queries is only 0.356, less effective than those of the 250 queries on average. For queries with terms that do not hold either of the two properties, search performance declined by a greater magnitude. However, we noticed that for queries with terms that have P(w|R)/P(w|NR) < P ranging from 0.2 to 0.6, there are no apparent differences in the queries' search""",0,,False
92,"performance. This indicates that P(w|R)/P(w|NR) is less indicative of term's search effectiveness. In following discussions, we use the stronger form of frequent term property and adopt P(w|R,X,""1)/P(w|NR,X"",1) as the indicator.",1,ad,True
93,0.50,0,,False
94,"nDCG@10 of queries P(X,1|R) < P",0,,False
95,0.45,0,,False
96,nDCG@10 of queries P(w|R)/P(w|NR) < P,0,,False
97,"nDCG@10 of queries P(w|R,X,""1)/P(w|NR,X"",1) < P",0,,False
98,0.40,0,,False
99,0.35,0,,False
100,0.30,0,,False
101,0.25,0,,False
102,0.20,0,,False
103,0.15,0,,False
104,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 ALL P,0,,False
105,"Figure 2. nDCG@10 of queries with at least one term for which the three indicators < P. P ranges from 0.1 to 1.0. ""ALL"" shows the average nDCG@10 of ALL queries.",0,,False
106,"Necessary Terms P(X,1|R)0.8",0,,False
107,"Frequent Terms P(w|R,X,""1)/P(w|NR,X"",1)>1",0,,False
108,74 terms,0,,False
109,224 terms,0,,False
110,149 terms,0,,False
111,216 terms are neither necessary nor frequent.,0,,False
112,Figure 3. Overlap of terms conforming to two properties.,0,,False
113,"We further show the overlap of query terms conforming to the two properties in Figure 3. Among the 663 terms, 224 (33.8%) are both necessary and frequent, but 223 (33.6%) only hold one of the two properties. The remaining 216 terms (32.6%) are neither necessary nor frequent. This suggests different strategies should be adopted to improve ineffective queries. The 216 terms that do not hold either property are not indicative of document relevance and would be better removed. For the 74 terms only having the necessary term property, we should prefer documents where the term appears but do not give further credit to high term frequency. For the 149 terms only having the frequent term property, we should prefer documents where the term appears many times over those where the term appears only once or twice, but it may be risky to filter out documents without any occurrence of the term.",1,ad,True
114,"To summarize, our results show that whether or not a term holds the two properties affects the search effectiveness of queries. In the dataset, only 1/3 of the query terms hold both properties. Another 1/3 hold only one of the two properties. The other 1/3 have neither property. This suggests that we may",0,,False
115,1168,0,,False
116,"improve search systems in two different ways: identify terms with these properties and adopt different ranking criteria; predict terms without any of the two properties and discount the effects of such terms in ranking. In further sections, we explore such potentially by assuming we can correctly identify properties of terms.",1,ad,True
117,3. USER JUDGMENTS OF PROPERTIES,0,,False
118,"In this section, we study whether or not users can make correct judgments of these two term properties. This is meaningful for two reasons. First, if the users make poor judgments on query properties, it provides a new explanation for ineffective queries. Second, it the users can make correct judgments, systems may benefit from providing query languages allowing users to express their sense of the properties.",0,,False
119,"We asked 10 users to annotate 100 TREC queries selected from the TREC Robust 2004 dataset (Topic 301-400). Each user annotated 15 queries, with 10 overlapping with another two users. For example, the first user annotated query 301-315, the second user on query 311-325, ... , and the last user on query 391-400 as well as 301-305. This resulted in 10 users' annotations on the 100 queries. For 50 queries, we have only one user's annotation, and for the other 50, we have two users' annotations, so that we can study users' agreements on the properties of query terms. For each query term, we asked users two yes/no questions as follows. We say that a user annotated a query term as necessary or frequent if the answer on Q1 or Q2 is yes, respectively.",1,TREC,True
120,Q1: I believe most of the relevant results should have this word. Results that do not contain this word are unlikely to be useful.,0,,False
121,Q2: I believe this word should appear many times in relevant results. Results in which the word appears only once or twice are less likely to be useful.,0,,False
122,"We found that pairs of users have some agreement on whether or not a term is necessary, but their opinions are rather independent of each other on the frequent terms. Among the 126 query terms involving two users' annotations, users agreed in 67% of the cases regarding whether or not a term is necessary. However, they agreed only in 48% of the cases on whether a term is frequent.",0,,False
123,Table 1. Correctness of user annotation of term properties.,0,,False
124,Property,0,,False
125,P,0,,False
126,Num Y/N by P,0,,False
127,Num Y/N by Users,0,,False
128,User Acc / Prior,0,,False
129,Class,0,,False
130,Prec,0,,False
131,Rec,0,,False
132,0.8 88/164 Necessary,0,,False
133,0.5 145/107,0,,False
134,201/51,0,,False
135,0.50/0.65,0,,False
136,Y N,0,,False
137,201/51,0,,False
138,0.63/0.57,0,,False
139,Y N,0,,False
140,0.41 0.93 0.88 0.27 0.63 0.87 0.63 0.30,0,,False
141,1.0 124/128 Frequent,0,,False
142,0.8 156/96,0,,False
143,165/87,0,,False
144,0.60/0.51,0,,False
145,Y N,0,,False
146,165/87,0,,False
147,0.61/0.62,0,,False
148,Y N,0,,False
149,0.57 0.76 0.66 0.45 0.67 0.71 0.48 0.44,0,,False
150,"Table 1 shows the accuracy of users' annotations of query term properties comparing to those evaluated by the values of P(X,""1|R) and P(w|R,X"",""1)/P(w|NR,X"",1). Results show that in general it is difficult for users to make correct judgments on the query terms' properties. If we use P(X,""1|R)>0.5 as the criteria for necessary terms, users' judgments are slightly better than a classifier using prior probability of the classes (accuracy 0.63 versus 0.57). When we use P(w|R,X"",""1)/P(w|NR,X"",""1)>1.0 as the threshold for frequent terms, users did also only slightly better than a classifier using prior probabilities (accuracy 0.60 versus 0.51). The accuracy and precision of user judgments look not useful. Moreover, when we adopt different criteria for term properties, e.g. P(X"",""1|R)>0.8, users' judgments may even be worse than a classifier using the prior probability of classes.""",1,ad,True
151,"To conclude, the results of user annotation on query term properties show that it is very difficult for users to select the properties of query terms prior to looking at search results. Users also agree only slightly with others on whether a term property applies. Specifically, users' judgments on frequent terms are completely independent of others.",0,,False
152,4. SYSTEMS USING TERM PROPERTIES,0,,False
153,"In this section, we explore the potential of improving retrieval systems assuming we know the properties of terms correctly. The prediction of term properties is left for future work.",0,,False
154,4.1 Approaches,0,,False
155,"Let q be a query. We assume we know the set of necessary terms qN and the set of frequent terms qF. Note that qN and qF can be empty set, and a term in q may be in neither qN nor qF. We rank a document d by Equation (3), where: we assume qN and qF are independent given d; each term in qN and qF are generated independently of other terms from d by different process PN(w|d) and PF(w|d).",0,,False
156,"P qN , qF | d ",0,,False
157, PqN | d  PqF | d ,0,,False
158,(3),0,,False
159,  PN w | d    PF w | d ,0,,False
160,wqN,0,,False
161,wqF,0,,False
162,"We calculate PN(w|d) and PF(w|d) in Eq(4) and Eq(5). In Eq(4), we calculate PN(w|d) as the probability of selecting a term w from d's vocabulary Vd ignoring the frequency of terms in d. |Vd| is the size of d's vocabulary. PN(w) is the probability that w exists in a the vocabulary of a document in the whole corpus. In a corpus of",0,,False
163,"k documents, we estimate PN(w) as Eq(6). N is a parameter for smoothing. PF(w|d) is simply the probability of a term w from the multinomial document language model of d, estimated using",0,,False
164,maximum likelihood estimation with Dirichlet smoothing. In our,0,,False
165,"experiments, we set F to the value that can maximize nDCG@10 of using all terms as qF and no term as qN for retrieval (equivalent to query likelihood model). In contrast, we set N to the value that can maximize nDCG@10 of using all terms as qN and no term as qF for retrieval.",0,,False
166,P^N,0,,False
167,w,0,,False
168,|,0,,False
169,d,0,,False
170,1,0,,False
171,N Vd,0,,False
172, PN ,0,,False
173, N,0,,False
174,w,0,,False
175,(4),0,,False
176,P^F,0,,False
177,w,0,,False
178,|,0,,False
179,d,0,,False
180,c,0,,False
181,"w,",0,,False
182,d,0,,False
183,  F ,0,,False
184,d  F,0,,False
185,PF,0,,False
186,w,0,,False
187,(5),0,,False
188, P^N w ,0,,False
189,1 k,0,,False
190,d,0,,False
191,1 Vd,0,,False
192,(6),0,,False
193,"For a necessary term w in qN, PN(w|d) totally ignores the frequency of w in d. Its value depends only on whether or not w",0,,False
194,"appears in d. In addition, it favors documents with a small",1,ad,True
195,vocabulary. (This is intuitively correct because observing w in d is,0,,False
196,less informative if d is very long and has a large vocabulary.),0,,False
197,"When we put all the query terms into qF and none into qN, Equation (3) falls back to the query likelihood language model.",0,,False
198,4.2 Search Effectiveness,0,,False
199,"In this section, we evaluate the approaches proposed above by assuming different sets of necessary and frequent terms. Table 2 shows the results. For ""qN"" and ""qF"" in Table 2, ""none"" means do not use any terms, ""all"" means using all query terms, and ""best"" means using the best possible combination of query terms (the set of query terms that leads to the best nDCG@10).",1,ad,True
200,"We first evaluate the effectiveness of PN(w|d) and PF(w|d) on different set of terms individually. Unsurprisingly, using all terms as necessary terms (N++) performs worse than using all terms as frequent terms (F++ and also Query Likelihood). However,",1,Query,True
201,1169,0,,False
202,"simply ignoring term frequencies of all documents still achieved nDCG@10 as high as 0.293. This indicates that solely considering term occurrences is still useful in many cases. However, simply using all terms as both necessary and frequent terms (N++F++) did not result in any improvements.",0,,False
203,"We further examine whether removing inappropriate terms from qN or qF can lead to improved search performance. As shown in Table 2, removing inappropriate terms from qF can potentially improve nDCG@10 from 0.438 (F++) to 0.514 (F+), and from 0.436 (N++F++) to 0.528 (F++F+). Similarly, removing terms from qN can potentially improve nDCG@10 from 0.293 (N++) to 0.329 (N+), and from 0.436 (N++F++) to 0.503 (N+F++). When we remove inappropriate words from both qN and qF (N+F+), we can potentially improve nDCG@10 to 0.590, which is about 35% improvements comparing to QL and N++F++. This suggests that there is great potentiality of improving search performance if we can predict correctly the frequent and necessary words.",1,ad,True
204,"However, it should be noted that the best set of terms for qN and qF are dependent of each other. When we use the best set of qN in N+F++ and the best set of qF in N++F+ for retrieval (N+F+ local), there will be 10% decline of nDCG@10 comparing to N+F+. Besides, we found that a part of the improvement of search performance comes from removing inappropriate terms from both qN and qF. If we restrict that all the query terms should be in at least one of qN and qF (N+F+ (-rmv)), the nDCG@10 declined from 0.590 to 0.552, although still a substantial improvement comparing to F++ (QL).",0,,False
205,"We further examine whether using the indicators of properties in section 2, i.e., P(X,""1|R) and P(w|R,X"",""1)/P(w|NR,X"",""1), can effectively select the appropriate set of terms for qN and qF to enhance search performance. We examined a simple rule-based approach as follows. We start with all query terms in qF and no terms in qN. We remove terms in qF if P(w|R,X"",""1)/P(w|NR,X"",1) < 1.05. If the removed term has P(X,""1|R)>0.2, we add the term into qN. Besides, we add all terms with P(X"",""1|R)>0.95 into qN. This simply rule-based approach (N+F+ P) improves nDCG@10 by 8.7% comparing to F++ (using all terms for qF). This suggests that the two indicators are effective criterion of selecting qN and qF. However, the performance of the selected qN and qF cannot be compared with the best possible qN and qF in N+F+. This indicates that the two indicators are not enough for selecting qF and qN. The exploration of predictors for qF and qN is left for future works.""",1,ad,True
206,"Earlier, we showed that users made poor judgments on the properties of query terms. To further verify the quality of users' judgments, we select terms into qN and qF if users answered yes in Q1 and Q2. As shown in Table 2, this approach reduces search",1,ad,True
207,Table 2. Potential improvements of search performance.,0,,False
208,Label F++ (QL) F+ N++ N+ N++F++ N++F+ N+F++ N+F+ N+F+ local N+F+ (-rmv),0,,False
209,N+F+ P,0,,False
210,N+F+ user,0,,False
211,F+RM,0,,False
212,qN none none all best all all best best best.L best,0,,False
213,P(X|R),0,,False
214,user,0,,False
215,none,0,,False
216,"qF all best none none all best all best best.L best P(w|R,X) / P(w|NR,X)",0,,False
217,user,0,,False
218,RM100,0,,False
219,nDCG@10 0.438 0.514 0.293 0.329 0.436 0.528 0.503 0.590 0.541 0.552,0,,False
220,0.476,0,,False
221,0.416,0,,False
222,0.644,0,,False
223,Change / Baseline -,0,,False
224,+17.4% / F++ -,0,,False
225,+12.3% / N++ -,0,,False
226,+21.1% / N++F++ +15.4% / N++F++ +35.3% / N++F++ +24.1% / N++F++ +26.6% / N++F++,0,,False
227,+8.7% / QL,0,,False
228,QL: nDCG@10 0.443 (100 queries),0,,False
229,-,0,,False
230,* N/F in the run labels refers to qN/qF; ++ means using all terms; + means using selected query terms.,0,,False
231,performance. The nDCG@10 is 0.416 (N+F+ user) versus 0.443 in QL on the same set of 100 queries. This further confirms that it is difficult for users to make useful judgments on term properties.,0,,False
232,"So far we limit the set of query terms among those being issued by the users, and the improvements of search performance mainly comes from correct identification of the necessary terms and the frequent terms. We compare our approach with query expansion on the potential of improving search performance. We estimate the true relevance model based on qrels, and use the top 100 terms (""RM100"") as qF for search. As shown in Table 2, solely working on the set of query terms issued by users, N+F+ is not much worse than F+RM (true relevance model) on nDCG@10, which extensively exploits the representative terms in relevant results.",0,,False
233,5. FUTURE WORK,0,,False
234,"In this preliminary study, we show that retrieval models that exploit term frequency can potentially be improved substantially by separately considering TF for some query terms and counting only occurrence or non-occurrence for some other query terms. This conclusion comes from our findings that query terms hold different properties. Specifically, sometimes the frequencies of terms do not indicate document relevance as long as the terms appear. In such cases, existing retrieval models may incorrectly rank documents with high term frequencies to the top. Queries with terms lacking either property are less effective in general.",0,,False
235,"Future work on this topic mainly focuses on the prediction of an appropriate set of terms in qN and qF. As discussed in section 4, though values of the two indicators can effectively predict qN and qF, it is far from perfect and the two indicators are also computed based on known relevance judgments.",0,,False
236,"Our study is closely related but different from the recent work of term necessity prediction by Zhao and Callan [5, 6]. Zhao et al. focused on predicting P(w|R) and aimed at solving term mismatch by selecting terms with highly predicted P(w|R) values for query expansion. In comparison, we do not expand the query but aim at recognizing the correct properties of query terms that are issued by the users. The two approaches follow different directions but may potentially be combined. As shown in Table 2, our approach may have substantial improvements on search performance that is comparable to those can be achieved by predicting P(w|R).",0,,False
237,ACKNOWLEDGEMENT,0,,False
238,"This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",0,,False
239,6. REFERENCES,0,,False
240,"[1] Lafferty, J. and Zhai, C. 2001. Document language models, query models, and risk minimization for information retrieval. In Proc. SIGIR'01: 111-119.",0,,False
241,"[2] Ponte, J.M. and Croft, W.B. 1998. A language modeling approach to information retrieval. Proc. SIGIR'98: 275-281.",0,,False
242,"[3] Robertson, S.E. et al. 1995. Okapi at TREC-3. NIST Special Publication 500-226: Proceedings of the Third Text REtrieval Conference (TREC-3).",1,TREC,True
243,"[4] Zhai, C. and Lafferty, J. 2001. A study of smoothing methods for language models applied to Ad Hoc information retrieval. In Proc. SIGIR'01: 334­342.",0,,False
244,"[5] Zhao, L. and Callan, J. 2012. Automatic term mismatch diagnosis for selective query expansion. In Proc. SIGIR'12: 515-524.",0,,False
245,"[6] Zhao, L. and Callan, J. 2010. Term necessity prediction. In Proc. CIKM'10: 259­268.",0,,False
246,1170,0,,False
247,,0,,False

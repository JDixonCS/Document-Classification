,sentence,label,data,regex
0,Fusion Helps Diversification,0,,False
1,Shangsong Liang,0,,False
2,"University of Amsterdam Amsterdam, The Netherlands",0,,False
3,s.liang@uva.nl,0,,False
4,Zhaochun Ren,0,,False
5,"University of Amsterdam Amsterdam, The Netherlands",0,,False
6,z.ren@uva.nl,0,,False
7,Maarten de Rijke,0,,False
8,"University of Amsterdam Amsterdam, The Netherlands",0,,False
9,derijke@uva.nl,0,,False
10,ABSTRACT,0,,False
11,"A popular strategy for search result diversification is to first retrieve a set of documents utilizing a standard retrieval method and then rerank the results. We adopt a different perspective on the problem, based on data fusion. Starting from the hypothesis that data fusion can improve performance in terms of diversity metrics, we examine the impact of standard data fusion methods on result diversification. We take the output of a set of rankers, optimized for diversity or not, and find that data fusion can significantly improve state-of-the art diversification methods. We also introduce a new data fusion method, called diversified data fusion, which infers latent topics of a query using topic modeling, without leveraging outside information. Our experiments show that data fusion methods can enhance the performance of diversification and DDF significantly outperforms existing data fusion methods in terms of diversity metrics.",1,ad,True
12,Categories and Subject Descriptors,0,,False
13,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--retrieval models,0,,False
14,Keywords,0,,False
15,Data fusion; rank aggregation; diversification; ad hoc retrieval,1,ad,True
16,1. INTRODUCTION,1,DUC,True
17,"Search result diversification is widely being studied as a way of tackling query ambiguity. Instead of trying to identify the ""correct"" interpretation behind a query, the idea is to make the search results diversified so that users with different backgrounds will find at least one of these results to be relevant to their information need [2]. In contrast to the traditional assumption of independent document relevance, search result diversification approaches typically consider the relevance of a document in light of other retrieved documents [40]. Diversification models try to identify the probable ""aspects"" of the query and return documents for each aspect, thereby making the result list more diverse.",1,ad,True
18,"Data fusion approaches, also called rank aggregation approaches, consist in combining result lists in order to produce a new and hopefully better ranking [16, 42]. Here, results lists can be produced by",0,,False
19,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '14, July 06­11, 2014, Gold Coast, QLD, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ... $15.00. http://dx.doi.org/10.1145/2600428.2609561",1,ad,True
20,"a wide range of ranking approaches, based, e.g., on different query or document representations. Data fusion methods can improve retrieval performance in terms of traditional relevance-oriented metrics like MAP and precision@k over the methods used to generate the individual result lists being fused [17, 26, 27, 49]. One reason is that retrieval approaches often return very different non-relevant documents, but many of the same relevant documents [49].",1,ad,True
21,"We examine the hypothesis that data fusion can improve performance in terms of diversity metrics by promoting aspects that are found in disparate ranked lists to the top of the fused list. Our first step in testing this hypothesis is to examine the impact of existing data fusion methods in terms of diversity scores when fusing ranked lists. We find that they tend to improve over individual component runs on nearly all of the diversity metrics that we consider: Prec-IA, MAP-IA, -NDCG, ERR-IA (all at rank 20).",1,MAP,True
22,"Building on these findings we propose a new data fusion method, called diversified data fusion (DDF). Based on latent Dirichlet allocation (LDA), it operates on documents in the result lists to be fused, whether the result lists have been diversified or not. DDF infers latent topics, their probabilities of being relevant and a multinomial distribution of topics over the documents being fused. Thus, it integrates topic structure and rank information. DDF does not assume the explicit availability of query aspects, but infers these as well as the latent prior for a given query via the documents being fused. Experimental results show that DDF can aggregate result lists--whether produced by diversification or ad hoc retrieval models--and boost the diversity of the final fused list, outperforming state-of-the-art diversification methods and established data fusion methods, especially in terms of intent-aware precision metrics.",1,ad,True
23,Our contributions in this paper can be summarized as follows:,0,,False
24,i. We tackle the challenge of search result diversification in a novel way by using data fusion methods.,0,,False
25,ii. We propose a novel data fusion method that aims at optimizing diversification measures and that proves to be especially effective in terms of intent-aware precision metrics.,0,,False
26,iii. We analyze the effectiveness of data fusion for result diversification and find that our fusion method as well as other fusion methods can significantly outperform state-of-the-art diversification methods.,0,,False
27,§2 discusses related work. §3 describes the fusion models that we use (old and new). §4 describes our experimental setup. §5 is devoted to our experimental results and we conclude in §6.,0,,False
28,2. RELATED WORK,0,,False
29,"We distinguish between three directions of related work: search result diversification, data fusion, and latent topic modeling.",0,,False
30,303,0,,False
31,2.1 Search result diversification,0,,False
32,"Search result diversification is similar to ad hoc search, but differs in its judging criteria and evaluation measures [8, 12]. The basic premise in search result diversification is that the relevance of a set of documents depends not only on the individual relevance of its members, but also on how they relate to one another [2]. Ideally, users can find at least one relevant document to the underlying information need. Most previous work on search result diversification can be classified as either implicit or explicit [39, 41].",1,ad,True
33,"Implicit approaches to result diversification promote diversity by selecting a document that differs from the documents appearing before it in terms of vocabulary, as captured by a notion of document similarity, such as cosine similarity or Kullback-Leibler divergence. Carbonell and Goldstein [6] propose the maximal marginal relevance (MMR) method, which reduces redundancy while maintaining query relevance when selecting a document. Chen and Karger [7] describe a retrieval method incorporating negative feedback in which documents are assumed to be non-relevant once they are included in the result list, with the goal of maximizing diversity. Zhai et al. [51] present a subtopic retrieval model where the utility of a document in a ranking is dependent on other documents in the ranking and documents that cover many different subtopics of a query topic are found. Other implicit work includes, e.g., [1] where set-based recommendation of diverse articles is proposed. We also tackle the problem of search result diversification implicitly, but in a different way, i.e., by data fusion.",1,corpora,True
34,"Explicit approaches to diversification assume that a set of query aspects is available and return documents for each of them. Past work has shown that explicit approaches are usually somewhat superior to implicit diversification techniques. Well-known examples include xQuAD [39], RxQuAD [45], IA-select [2], PM-2 [13], and, more recently, DSPApprox [14]. Instead of modeling a set of aspects implicitly, these algorithms obtain the set of aspects either manually, e.g., from aspect descriptions [8, 12], or they create them directly from, e.g., suggested queries generated by commercial search engines [13, 39] or predefined aspect categories [44]. We propose an implicit fusion-based diversification model where we do not assume that the aspects of the query are available but do assume that we can infer the underlying topics and the prior relevance of each topic for search result diversification.",1,ad,True
35,2.2 Data fusion,0,,False
36,"A core concern in data fusion is how to assign a score to a document that appears in one of the lists to be fused [17, 19, 42, 49]. Most previous work on data fusion focuses on optimizing a traditional evaluation metric, like MAP, p@k and nDCG. Fusion approaches can be categorized into supervised or unsupervised: Supervised data fusion approaches, like -Merge [43], first extract a number of features, either from documents or lists, and then utilize a machine learning algorithm to train the fusion model [15, 17, 49].",1,ad,True
37,"In contrast, unsupervised data fusion methods mainly use either retrieval scores or ranks of documents in the lists to be merged, with the CombSUM family of fusion methods being the oldest and one of the most successful ones in many information retrieval tasks [26, 42]. State-of-the-art data fusion methods ClustFuseCombSUM and ClustFuseCombMNZ (both cluster-based methods) are proposed in [23]. Methods utilizing retrieval scores take score information from the lists to be fused as input, while those utilizing rank information only use order information of the documents appearing in the lists to be fused as input. Data fusion methods utilizing rank information have many uses and applications in information retrieval, including, e.g., expert search [30, 35], query reformulations [43], meta-search [4, 17] and microblog search [31, 32].",1,blog,True
38,We do not make the assumption that labeled data is available but integrate standard unsupervised data fusion information into our diversified fusion model for search result diversification via a latent topic model.,0,,False
39,2.3 Topic modeling,0,,False
40,"Topic models have been proposed for reducing the high dimensionality of words appearing in documents into low-dimensional ""latent topics."" From the first work on topic models [21], the Probablistic LSI model, topic models have received significant attention [5, 18, 22] and have proved to be effective in many information retrieval tasks [24, 47, 50]. Latent dirichlet allocation (LDA) [5] represents each document as a finite mixture over ""latent"" topics where each topic is represented as a finite mixture over words existing in that document. Based on LDA, many extensions have been proposed, e.g., to handle users' connections with particular documents and topics [37], to learn relations among different topics [25, 29], for topic over time [46], for dynamic mixture model [48], or tweet summarization [36]. LDA has also been extended to sentiment analysis [28]. We propose a novel topic model where fusion scores of each document appearing in lists to be fused are used to boost the performance of state-of-the-art diversification methods.",0,,False
41,"Our work adds the following to the work discussed above. We propose a fusion-based approach to the search result diversification task. We find that existing unsupervised fusion methods significantly outperform state-of-the-art diversification methods. In addition, we propose a novel fusion method, diversified data fusion, that uses the output of a fusion step and a topic modeling step as input to a diversification step. To the best of our knowledge, ours is the first attempt to utilize data fusion for diversification.",1,ad,True
42,3. FUSION METHODS,0,,False
43,"We first review our notation and terminology. Then we introduce the task to be addressed, as well as the baseline fusion methods that we use in this paper plus a new fusion method.",1,ad,True
44,3.1 Notation and terminology,0,,False
45,"We summarize the main notation used in this paper in Table 1. In the remainder, we distinguish between queries, aspects and topics. A query is an expression of an information need; in our experimental evaluation below, queries are provided as part of a TREC test collection. An aspect (sometimes called subtopic at the TREC Web track) is an interpretation of an information need. We use topic to refer to latent topics as identified by a topic modeling method, in our case LDA. A component list is a ranked list that serves as input for a data fusion method. A fused list is a list that is the result of applying a fusion method to component lists.",1,TREC,True
46,3.2 The diversified data fusion task,0,,False
47,"The diversified data fusion task that we address is this: given a query, an index of documents, and a set of ranked lists of documents produced in response to a query, aggregate the lists into a final result list where documents should be diversified. The component lists may or may not have been diversified themselves or ranked by relevance only.",1,ad,True
48,The underlying data fusion problem consists of running a ranking function FX that satisfies:,0,,False
49,"L ,"" {L1, L2, . . . , Lm}, q, C -FX Lf ,""",0,,False
50,"where L is a set of components lists, m ,"" |L| is their number, C the document corpus, q a query, and Lf the final fused list.""",0,,False
51,304,0,,False
52,Table 1: Basic notation used in the paper.,0,,False
53,Notation Gloss,0,,False
54,C,0,,False
55,document corpus,0,,False
56,q,0,,False
57,query,0,,False
58,z,0,,False
59,topic,0,,False
60,d,0,,False
61,document,0,,False
62,w,0,,False
63,a token,0,,False
64,Nd,0,,False
65,number of tokens in d,0,,False
66,Li,0,,False
67,i-th ranked list of documents,0,,False
68,L,0,,False
69,set of ranked lists to be fused,0,,False
70,m,0,,False
71,"number of ranked lists to be fused, i.e., m , |L|",0,,False
72,CL,0,,False
73,set of documents that appear in the lists L,0,,False
74,|CL|,0,,False
75,number of documents in CL,0,,False
76,FX,0,,False
77,a data fusion method,0,,False
78,FX(d; q) score of document d for query q according to a data fusion,0,,False
79,method FX,0,,False
80,RLi d,0,,False
81,rank-based score of d in list Li,0,,False
82,"rank(d, Li) rank of d in list Li",0,,False
83,|Li|,0,,False
84,length of list Li,0,,False
85,R,0,,False
86,set of top ranked documents,0,,False
87,qt[z|q],0,,False
88,quotient score for z given q in PM-2 algorithm [13],0,,False
89,vz|q,0,,False
90,probability of z given q,0,,False
91,sz|q,0,,False
92,portion of seat occupied by z given q in PM-2,0,,False
93,a free trade-off parameter in PM-2,1,ad,True
94,the parameter of topic Dirichlet prior,0,,False
95,the parameter of token Dirichlet prior,0,,False
96,T,0,,False
97,number of topics,0,,False
98,V,0,,False
99,number of unique tokens in CL,0,,False
100,d,0,,False
101,multinomial distribution of topics specific to d,0,,False
102,z,0,,False
103,multinomial distribution of tokens specific to topic z,0,,False
104,µz,0,,False
105,mean of Log-normal distribution of fusion scores for topic z,0,,False
106,z,0,,False
107,deviation of Log-normal distribution of fusion scores for z,0,,False
108,zdi,0,,False
109,topic associated with the i-th token in the document d,0,,False
110,wdi,0,,False
111,i-th token in document d,0,,False
112,fdi,0,,False
113,fusion score for token wdi,0,,False
114,3.3 Baseline data fusion methods,0,,False
115,Let RLid denote the score of document d based on the rank,0,,False
116,"of d in list Li; in the literature on data fusion, one often finds",0,,False
117,"RLid , 0 if d / Li (d still in the combined set of documents",0,,False
118,"CL :,",0,,False
119,"m i,1",0,,False
120,Li).,0,,False
121,"In both CombSUM and CombMNZ, RLid is",0,,False
122,often defined as:,0,,False
123,"RLid ,",0,,False
124,"(1+|Li |)-rank(d,Li ) |Li |",0,,False
125,0,0,,False
126,"d  Li d / Li,",0,,False
127,(1),0,,False
128,"where |Li| is the length of Li and rank(d, Li)  {1, . . . , |Li|} is the rank of d in Li. The well-known CombSUM fusion method [17, 49], for instance, scores d by the sum of its rank scores in the lists:",0,,False
129,"FCombSUM(d; q) :,"" Li RLid, while CombMNZ [17, 49] rewards d that ranks high in many lists:""",0,,False
130,"FCombMNZ(d; q) :,"" |{Li : d  Li}| · FCombSUM(d; q),""",0,,False
131,"where |{Li : d  Li}| is the number of lists in which d appears. We consider CombSUM, CombMNZ and two state-of-the-art",0,,False
132,"data fusion methods, ClustFuseCombSUM and ClustFuseCombMNZ [23], that integrate cluster information into CombSUM and CombMNZ, respectively, as baseline fusion methods.",0,,False
133,"In addition, a natural and direct way of diversifying a result list in the setting of data fusion is this: first rank the documents in the component lists by their estimated relevance to the query through a standard data fusion method, such as CombSUM, and then diversify the ranking through effective search result diversification models, such as MMR [6] and PM-2 [13]. In our experiments, we implement two more baselines, called CombSUMMMR and",1,ad,True
134,"CombSUMPM-2. They first use CombSUM to obtain a fused list and then use MMR and PM-2, respectively, to diversify the list.",0,,False
135,3.4 Diversified data fusion,0,,False
136,"We propose a diversified data fusion (DDF) method that not only inherits the merits of traditional data fusion methods, i.e., it can improve the performance on relevance orientated metrics, but also considers a query as a compound rather than a single representation of an underlying information need, and regards documents appearing in the component lists as mixtures of latent topics.",1,ad,True
137,3.4.1 Overview of DDF,0,,False
138,"DDF consists of three main parts: (I) perform standard data fusion; (II) infer latent topics; (III) perform diversification; see Algorithm 1. In the first part (""Part I"" in Algorithm 1), DDF computes the fusion scores of the documents in the component lists based on an existing unsupervised data fusion method (steps 1 and 2 in Algorithm 1); in this paper we use CombSUM, as our experimental results in §5.1 and §5.2 show that CombSUM outperforms other plain fusion methods in most cases. In the second part (""Part II"" in Algorithm 1), DDF integrates fusion scores into an LDA topic model such that latent topics of the documents, their corresponding estimated relevance scores, and the multinomial distribution of the topics specific to each document can be inferred (steps 3­15 in Algorithm 1). In the last part (""Part III"" in Algorithm 1), DDF uses the outputs of Parts I and II as input for an existing diversification method; in this paper, we use PM-2 [13] because it is a the state-ofthe-art search result diversification model. Some concepts in PM-2, such as ""quotient"" and ""seat,"" play important roles in the definition of the diversification step; they will be discussed in §3.4.3.",0,,False
139,"Below we describe how to infer latent topics (""Part II"" in Algorithm 1) in §3.4.2 and how we utilize the information generated from latent topics and fusion scores (""Part III"") in §3.4.3.",0,,False
140,3.4.2 Part II: Inferring latent topics,0,,False
141,"Previous work on search result diversification shows that explicitly computing the probabilities of aspects of a query can improve diversification performance [1, 20, 39]. We do not assume that aspect information is explicitly available; we infer latent topics and their probabilities of being relevant using topic modeling.",0,,False
142,"Topic discovery in DDF is influenced not only by token co-occurrences, but also by the fusion scores of documents in the component lists. To avoid normalization and because fusion scores of the documents theoretically belong to (0, +), we employ a lognormal distribution for fusion scores to infer latent topics of the query via the documents and their relevance probabilities.",0,,False
143,"The latent topic model used in DDF is a generative model of relevance and the tokens in the documents that appear in the component individual lists. The generative process used in Gibbs sampling [34] for parameter estimation, is as follows:",0,,False
144,"i. Draw T multinomials z from a Dirichlet prior , one for each topic z;",0,,False
145,"ii. For each document d  CL, draw a multinomial d from a Dirichlet prior ; then for each token wdi in document d:",0,,False
146,"(a) Draw a topic zdi from multinomial d; (b) Draw a token wdi from multinomial zdi ; (c) Draw a fusion score fdi for wdi from Log-normal N (µzdi ,",0,,False
147,zdi ).,0,,False
148,"Fig. 1 shows a graphical representation of our model. In the generative process, the fusion scores of tokens observed in the same document are the same and computed by a data fusion method, like",0,,False
149,305,0,,False
150,Algorithm 1: Diversified data fusion,0,,False
151,Input : A query q,0,,False
152,"Ranked lists to be fused, L1, L2, . . . , Lm",0,,False
153,"The combined set of documents CL :,",0,,False
154,"m i,1",0,,False
155,Li,0,,False
156,A standard fusion method X,0,,False
157,A tradeoff parameter ,1,ad,True
158,Number of latent topics T,0,,False
159,"Hyperparameters , ",0,,False
160,Output: A final fused diversified list of documents Lf .,0,,False
161,/* Part I: Perform standard data fusion,0,,False
162,*/,0,,False
163,"1 for d ,"" 1, 2, . . . , |CL| do 2 Initialize FX(d|L, q) using a standard fusion method X""",0,,False
164,/* Part II: Infer latent topics,0,,False
165,*/,0,,False
166,3 Randomly initialize topic assignment for all tokens in w,0,,False
167,"4 for z ,"" 1, 2, . . . , T do""",0,,False
168,5 Initialize µz and z randomly for topic z,0,,False
169,"6 for iter ,"" 1, 2, . . . , Niter do""",0,,False
170,"7 for d ,"" 1, 2, . . . , |CL| do""",0,,False
171,8,0,,False
172,"for i ,"" 1, 2, . . . , Nd do""",0,,False
173,9,0,,False
174,"draw zdi from P (zdi|w, r, z-di, , , µ, , L, q)",0,,False
175,10,0,,False
176,update nzdiwdi and mdzdi,0,,False
177,"11 for z ,"" 1, 2, . . . , T do""",0,,False
178,12,0,,False
179,update µz and z,0,,False
180,13 Compute the posterior estimate of ,0,,False
181,"14 for z ,"" 1, 2, . . . , T do""",0,,False
182,15,0,,False
183,vz|q ,0,,False
184,exp{uz,0,,False
185,+,0,,False
186,1 2,0,,False
187,z2,0,,False
188,},0,,False
189,T z,0,,False
190,",1 exp{uz",0,,False
191,+,0,,False
192,1 2,0,,False
193,2 z,0,,False
194,},0,,False
195,/* Part III: Perform diversification,0,,False
196,*/,0,,False
197,16 Lf  ,0,,False
198,17 R  CL,0,,False
199,"18 for z ,"" 1, 2, . . . , T do""",0,,False
200,19,0,,False
201,sz|q  0,0,,False
202,20 for all positions in the ranked list Lf do,0,,False
203,"21 for z ,"" 1, 2, . . . , T do""",0,,False
204,22,0,,False
205,qt[z|q],0,,False
206,",",0,,False
207,vz|q 2sz|q +1,0,,False
208,23,0,,False
209,z  arg maxz qt[z|q],0,,False
210,24,0,,False
211,"d  arg maxdR  × qt[z|q] × P (d|z, q)+",0,,False
212,25,0,,False
213,"(1 - ) z,""z qt[z|q] × P (d|z, q)""",0,,False
214,26,0,,False
215,Lf  Lf  {d},0,,False
216,27,0,,False
217,R  R\{d},0,,False
218,/* append d to Lf */,0,,False
219,"28 for z ,"" 1, 2, . . . , T do""",0,,False
220,29,0,,False
221,sz|q  sz|q +,0,,False
222,"P (d|z,q) z P (d|z ,q)",0,,False
223,"CombSUM, for the document, although a fusion score is generated for each token from the log-normal distribution. We use a fixed number of latent topics, T , although a non-parametric Bayes version of DDF that automatically integrates over the number of topics would certainly be possible. The posterior distribution of topics depends on the information from two modalities--both tokens and the fusion scores of the documents.",0,,False
224,"Inference is intractable in this model. Following [18, 24, 34, 36, 46, 47, 50], we employ Gibbs sampling to perform approximate inference. We adopt a conjugate prior (Dirichlet) for the multinomial distributions, and thus we can easily integrate out  and , analytically capturing the uncertainty associated with them. In this way we facilitate the sampling, i.e., we need not sample  and  at all. Because we use the continuous log-normal distribution rather than discretizing fusion scores, sparsity is not a big concern in fitting the model. For simplicity and speed we estimate these log-normal distributions µ and  by the method of moments, once per iteration of Gibbs sampling (see the Appendix). We find that the sensitivity of the hyper-parameters  and  is limited. Thus, for simplicity,",1,ad,True
225,q,0,,False
226,L,0,,False
227,z,0,,False
228,T,0,,False
229,w,0,,False
230,f,0,,False
231,µ,0,,False
232,T,0,,False
233,Nd,0,,False
234,T,0,,False
235,|CL|,0,,False
236,Figure 1: DDF graphical model for Gibbs sampling.,0,,False
237,"we use fixed symmetric Dirichlet distributions ( , 50/T and  , 0.1) in all our experiments.",0,,False
238,"In the Gibbs sampling procedure above, we need to calculate the conditional distribution P (zdi|w, r, z-di, , , µ, , L, q) (step 9 in Algorithm 1), where z-di represents the topic assignments for all tokens except wdi. We begin with the joint probability of documents to be fused, and using the chain rule, we can obtain the conditional probability conveniently as",0,,False
239,"P (zdi|w, r, z-di, , , µ, , L, q) ",0,,False
240,(mdzdi + zdi - 1) ×,0,,False
241,nzdiwdi + wdi - 1,0,,False
242,"V v,1",0,,False
243,(nzdi,0,,False
244,v,0,,False
245,+,0,,False
246,v ),0,,False
247,-,0,,False
248,1,0,,False
249,×,0,,False
250,"1  exp{- (ln FX(d|L, q) - µzdi )2 },",0,,False
251,"FX(d|L, q)zdi 2",0,,False
252,2z2di,0,,False
253,where nzv is the total number of tokens v that are assigned to topic,0,,False
254,"z, mdz represents the number of tokens in document d that are",0,,False
255,assigned to topic z. An overview of the Gibbs sampling procedure,0,,False
256,we use is shown from step 3 to step 12 in Algorithm 1; details are,0,,False
257,provided in the Appendix.,0,,False
258,One merit of our generative model for DDF is that we can predict,0,,False
259,a fusion score for any document once the tokens in the document,0,,False
260,"have been observed. Given a document, we predict its fusion score",0,,False
261,by choosing the discretized fusion score that maximizes the poste-,0,,False
262,rior which is calculated by multiplying the fusion score probability,0,,False
263,of all tokens from their corresponding topic-wise log-normal dis-,0,,False
264,"tributions, i.e., arg maxf",0,,False
265,"Nd i,1",0,,False
266,p(f,0,,False
267,|µzi,0,,False
268,",",0,,False
269,zi,0,,False
270,).,0,,False
271,"More importantly, after the Gibbs sampling procedure, we can",0,,False
272,easily infer the multinomial distribution of topics specific to each,0,,False
273,document d  CL as (step 13 in Algorithm 1):,0,,False
274,"d,z ,",0,,False
275,"nd,z + z",0,,False
276,"T z,1",0,,False
277,"(nd,z",0,,False
278,+,0,,False
279,z,0,,False
280,),0,,False
281,",",0,,False
282,(2),0,,False
283,"where nd,z is the number of tokens assigned to latent topic z in document d; we can also conveniently estimate the probability of a topic being relevant to the query, denoted as vz|q, by (step 15 in Algorithm 1):",0,,False
284,"vz|q :,",0,,False
285,E[f |z],0,,False
286,T z,0,,False
287,",1",0,,False
288,E[f,0,,False
289,|z,0,,False
290,],0,,False
291,",",0,,False
292,T z,0,,False
293,"exp{uz + ,1 exp{uz",0,,False
294,1 2,0,,False
295,z2},0,,False
296,+,0,,False
297,1 2,0,,False
298,z2,0,,False
299,", }",0,,False
300,(3),0,,False
301,where E denotes the expectation.,0,,False
302,3.4.3 Part III: Diversification,0,,False
303,"In Part III of our DDF model we propose a modification of PM2. Before we discuss the details of this modification, we briefly describe PM-2. PM-2 is a probabilistic adaptation of the SainteLaguë method for assigning seats (positions in the ranked list) to",1,ad,True
304,306,0,,False
305,"members of competing political parties (aspects) such that the number of seats for each party is proportional to the votes (aspect popularity, also called aspect probabilities, i.e., p(z|q)) they receive. PM-2 starts with a ranked list Lf with k empty seats. For each of these seats, it computes the quotient qt[z|q] for each topic z given q following the Sainte-Laguë formula:",0,,False
306,"qt[z|q] ,"" vz|q ,""",0,,False
307,(4),0,,False
308,2sz|q + 1,0,,False
309,"where vz|q is the probability of topic z given q, i.e., the weight of topic z. According to the Sainte-Laguë method, this seat should",0,,False
310,be awarded to the topic with the largest quotient in order to best,0,,False
311,"maintain the proportionality of the list. Therefore, PM-2 assigns the current seat to the topic z with the largest quotient. The document to fill this seat is the one that is not only relevant to z but to other",0,,False
312,topics as well:,0,,False
313,"d ,"" arg max  × qt[z|q] × P (d|z, q) +""",0,,False
314,(5),0,,False
315,dR,0,,False
316,"(1 - ) z,""z qt[z|q] × P (d|z, q) ,""",0,,False
317,"where P (d|z, q) is the probability of d talking about topic z for a given q. After the document d is selected, PM-2 increases the",0,,False
318,portion of seats occupied by each of the topics z by its normalized relevance to d:,0,,False
319,sz|q  sz|q +,0,,False
320,"P (d|z, q)",0,,False
321,z,0,,False
322,P (d|z,0,,False
323,",",0,,False
324,. q),0,,False
325,This process repeats until we get k documents for Lf or we are out of candidate documents. The order in which a document is appended to Lf determines its ranking.,0,,False
326,"We face two challenges in PM-2: it is non-trivial to get the aspect probability vz|q (i.e., p(z|q)), which is often set to be uniform, and it is non-trivial to compute p(d|z, q), which usually requires explicit access to additional information. To address the first challenge, we compute vz|q by (3), such that (4) can be modified as:",1,ad,True
327,"qt[z|q] ,",0,,False
328,"p(z|q) ,",0,,False
329,2sz|q + 1,0,,False
330,exp{uz,0,,False
331,+,0,,False
332,1 2,0,,False
333,z2,0,,False
334,},0,,False
335,(2sz|q + 1),0,,False
336,T z,0,,False
337,",1",0,,False
338,exp{uz,0,,False
339,.,0,,False
340,+,0,,False
341,1 2,0,,False
342,z2,0,,False
343,},0,,False
344,"For the second challenge, instead of computing P (d|z, q) explicitly, we modify P (d|z, q) and apply Bayes' Theorem so that",1,ad,True
345,"p(z|d, q)p(d|q) p(z|d, q)p(d|q)",0,,False
346,"P (d|z, q) ,",0,,False
347,",",0,,False
348,. (6),0,,False
349,p(z|q),0,,False
350,vz|q,0,,False
351,"Then we integrate the fused score generated by CombSUM into our model, i.e., we set",0,,False
352,"p(d|q) ra,nk FCombSUM(d; q)",0,,False
353,"in (6). As a result, after applying (6) to (5), DDF selects a candidate document by:",0,,False
354,"d ,"" arg max  · qt[z|q] · p(z|d, q) · FCombSUM(d; q) +""",0,,False
355,dR,0,,False
356,vz |q,0,,False
357,(7),0,,False
358,(1 - ),0,,False
359,"z,z",0,,False
360,qt[z|q],0,,False
361,·,0,,False
362,", p(z|d,q)·FCombSUM (d;q)",0,,False
363,vz|q,0,,False
364,"where p(z|d; q) is the probability of document d belonging to topic z, which can easily be inferred in our DDF model by (2) (i.e., p(z|d, q) ,"" d,z). Therefore, after applying (2) and (3), (7) can be rewritten as:""",0,,False
365,d,0,,False
366,", arg max",0,,False
367,dR,0,,False
368, · qt[z|q] ·,0,,False
369,"d,z · FCombSUM(d;",0,,False
370,exp{µz,0,,False
371,+,0,,False
372,1 2,0,,False
373,z2,0,,False
374,},0,,False
375,q,0,,False
376,),0,,False
377,+,0,,False
378,(8),0,,False
379,(1 - ),0,,False
380,"z,z",0,,False
381,qt[z|q],0,,False
382,·,0,,False
383,", d,z ·FCombSUM(d;q)",0,,False
384,exp{µz,0,,False
385,+,0,,False
386,1 2,0,,False
387,z2,0,,False
388,},0,,False
389,where it should be noted that we ignore the constant term,0,,False
390,"T z,1",0,,False
391,exp{µz,0,,False
392,+,0,,False
393,1 2,0,,False
394,z2,0,,False
395,"},",0,,False
396,as it has no impact on selecting the candidate document d.,0,,False
397,4. EXPERIMENTAL SETUP,0,,False
398,"In this section, we describe our experimental setup; §4.1 lists our research questions; §4.2 describes our data set; §4.3 lists the metrics and the baselines; §4.4 details the settings of the experiments.",0,,False
399,4.1 Research questions,0,,False
400,The research questions guiding the remainder of the paper are:,0,,False
401,"RQ1 Do fusion methods help improve state-of-the-art search diversification methods? Do they help in terms of intent-aware precision, as our main metric? Does DDF beat standard and state-of-the-art fusion methods? (See §5.1 and §5.2.)",0,,False
402,RQ2 What is the effect on the diversification performance of DDF and fusion methods of the number of component lists? Does the contribution of fusion to diversification performance depend on the quality of the component lists? (See §5.3),0,,False
403,RQ3 Does DDF outperform the best diversification and fusion methods on each query? (See §5.4.),0,,False
404,RQ4 How do the rankings of DDF differ from those produced by other fusion methods? (See §5.5.),0,,False
405,RQ5 What is the effect on the diversification performance of DDF of the number of latent topics used by DDF? (See §5.6.),0,,False
406,4.2 Data set,0,,False
407,"In order to answer our research questions we work with the runs submitted to the TREC 2009, 2010, 2011 and 2012 Web tracks, and the billion-page ClueWeb09 collection.1 There are two tasks in these tracks: an ad hoc search task and a search result diversification task [8, 10­12]. We only focus on the diversification task, where the top-k documents returned should not only be relevant but also cover as many aspects as possible in response to a given query. In total, we have 200 ambiguous queries from the four years, with 2 queries (#95 and #100 in the 2010 edition) not having relevant documents. Typically, each query has 2 to 5 aspects, and some relevant documents are relevant to more than 2 aspects of the query.",1,TREC,True
408,"Many of the runs submitted to these four years of the Web track for the diversification task were generated by state-of-the-art diversification methods. In total, we have 119, 88, 62 and 48 runs from the 2009, 2010, 2011 and 2012 editions, respectively.2",0,,False
409,4.3 Evaluation metrics and baselines,0,,False
410,"We evaluate our component runs and fused runs using several standard metrics that are official evaluation metrics in the diversification tasks at TREC Web tracks [8, 10­12] and are widely used in the literature on search result diversification [2, 3, 13, 14, 38, 40]: Prec-IA@k [2], MAP-IA@k [2], ERR-IA@k [2] and nDCG@k [9]. The former two are set-based and indicate, respectively, the precision and mean average precision across all aspects of the query in the search results, whereas the remaining ones are cascade measures that penalize redundancy at each position in the ranked list based on how much of that information the user has already seen from documents at earlier ranks.",1,TREC,True
411,1Available from http://boston.lti.cs.cmu.edu/ Data/clueweb09.,0,,False
412,2All runs are available from http://trec.nist.gov.,1,trec,True
413,307,0,,False
414,We follow published work on search result diversification and mainly compute the metric scores at depth 20. Statistical significance of observed differences between the performance of two runs is tested using a two-tailed paired t-test and is denoted using (or,0,,False
415,") for significant differences for  ,"" .01, or (and ) for  "","" .05. When assessing a fusion method X we will prefer fusion methods that are safe, where we say that X is safe for metric M if applying X to a set of component runs always yields a fused run that scores at least as high as the highest scoring component run in the set (according to M ). We consider several baselines. Two standard fusion methods [26], CombSUM and CombMNZ; two state-of-the-art fusion methods [23], ClustFuseCombSUM and ClustFuseCombMNZ; each year's best performing runs in the diversification tasks at the TREC Web track [8, 10­12], and state-of-the-art plain diversification methods, xQuAD [39] and PM-2 [13]. As DDF builds on both fusion and diversification methods, we also consider two fusion methods, CombSUMMMR and CombSUMPM-2, that integrate plain diversification methods MMR [6] and PM-2 into CombSUM for diversification, respectively.""",1,TREC,True
416,4.4 Experiments,0,,False
417,"We report on five main experiments aimed at answering the research questions listed in §4.1. In our first experiment, aimed at determining whether fusion methods help diversification, we fuse the five top performing diversification result lists from the TREC Web 2009, 2010, 2011 and 2012 submitted runs (some lists are generated by the implementation of PM-2) by our baselines, viz., CombSUM, CombMNZ, ClustFuseCombSUM, ClustFuseCombMNZ, CombSUMMMR and CombSUMPM-2 (see §4.3). The performance of the baselines is compared against that of DDF.",1,TREC,True
418,"Our second experiment is aimed at understanding the effect on the diversification performance of DDF and fusion methods of the number of component lists; we randomly sample k  {2, 4, . . . , 26} component runs from the submitted runs in the TREC Web 2012 track and fuse them. We repeat the experiments 20 times and report the average results and the standard deviations. We also show one sample's result when fusing 4 runs.",1,TREC,True
419,"Next, in order to understand how DDF outperforms the best component run and the fusion methods per query, our third experiment provides a query-level analysis. Our fourth experiment is aimed at understanding how the runs generated by DDF differ from those produced by other fusion methods; we zoom in on the differences between DDF and the next best performing fusion method, CombSUMPM-2, in terms of the documents (and aspects) retrieved by one, but not the other, or by both.",0,,False
420,"Finally, to understand the influence of the number of latent topics used in DDF, we vary the number of latent topics and assess the performance of DDF. We also use an oracle variant of DDF, called DDF2, where for every test query we consider as many latent topics as there are aspects according to the ground truth. The number of topics used in DDF is set to 10, unless stated otherwise.",0,,False
421,5. RESULTS,0,,False
422,"In §5.1 we examine the performance of baseline fusion methods on the diversification task, which we follow with a section on the performance of DDF in §5.2. §5.3 details the effect of the number of lists; §5.4 provides a query-level analysis; §5.5 zooms in on the effect on ranking of DDF compared to the next best fusion method; §5.6 examines the effect of the number of latent topics on DDF.",0,,False
423,5.1 Performance of baseline fusion methods,0,,False
424,In Table 2 we list the diversity scores of the baseline fusion,0,,False
425,"methods on the diversity task: CombSUM, CombMNZ, ClustFuseCombSUM, ClustFuseCombMNZ, CombSUMMMR, CombSUMPM-2, with the 5 best performing component lists from the TREC Web 2009, 2010, 2011 and 2012 tracks, respectively.3 For all metrics and in all years, almost all baseline fusion methods outperform the state-of-the-art diversification methods, and in many cases significantly so. Note, however, that none of the baseline methods is safe in the sense defined in §4.3. Additionally, Table 3 shows the diversity scores of the baseline fusion methods when we fuse 4 randomly sampled runs from the 2012 data set, which confirms that fusion does help diversification.",1,TREC,True
426,5.2 The performance of DDF,0,,False
427,"Inspired by the success of baseline fusion methods on the diversification task, we now consider our newly proposed fusion method, DDF. Returning to Tables 2 and 3, two types of conclusion emerge. First, DDF outperforms all component runs (note that component runs in Table 2 are the best runs in the tracks), on all metrics, for all years. In other words, it is safe in the sense defined in Section 4.3. The difference between DDF and the best performing component run is always significant. We believe that the strong performance of DDF is due to the fact that DDF not only focuses on improving the relevance score of fused run but also explicitly tries to diversify the fused run.",0,,False
428,"Second, DDF outperforms all baseline fusion methods, on all metrics. In many cases, CombSUMPM-2 and CombSUM yield the second and third best performance, respectively, but DDF outperforms them in every case, and often significantly so. DDF can beat CombSUMPM-2 as it tackles two main challenges in PM-2 (see §3.4.3), although they build on the same framework. CombSUMMMR follows a similar strategy as DDF but its performance is worse than that of DDF. This is due to the fact that MMR models documents as if they are centered around a single topic only. It is clear from Tables 2 and 3 that cluster-based data fusion methods (ClustFuseCombSUM, ClustFuseCombMNZ) sometimes perform a little worse than the standard fusion method they build on (CombSUM, CombMNZ). This is because cluster-based fusion focuses on relevance of the documents rather than on diversification.",0,,False
429,5.3 Effect of the number of component lists,0,,False
430,"Next, we zoom in on DDF. In particular, we explore the effect of varying the number of lists to be fused on its performance. Fig. 2 shows the fusion results of randomly sampling k  {2, 4, . . . , 26} lists from the 48 runs submitted to the TREC Web 2012 track plus the PM-2 runs (due to space limitations, we only report results using the 2012 runs; the findings on other years are qualitatively similar). For each k, we repeat the experiment 20 times and report on the average scores and the corresponding standard deviations indicated by the error bars in the figure. We use CombSUM as a representative example for comparison with DDF, as the results of other baseline fusion methods are worse or have qualitatively similar results to those of CombSUM. As shown in Fig. 2, DDF always outperforms CombSUM in terms of the Prec-IA, -nDCG and ERRIA evaluation metrics and the performance gaps remain almost unchanged, in absolute terms, no matter how many component lists are fused. One reason for this is that as DDF builds on CombSUM, it inherits the merits of the fusion method, and more importantly, at the same time it tries to infer latent topics and rerank the high",1,TREC,True
431,"3The run ""PM-2 (TREC)"" is the run that utilizes aspect information from the ground truth in the PM-2 model and the run ""PM-2 (engine)"" is produced using information from a commercial search engine. The run ""xQuAD (uogTrX)"" is a uogTrX TREC edition run generated using the xQuAD algorithm; see [33].",1,TREC,True
432,308,0,,False
433,"Table 2: Performance obtained using the 2009­2012 editions of the TREC Web tracks. The best performing run per metric per year is in boldface. Statistically significant differences between fusion method and the best component run, between DDF and CombSUM, and between DDF and CombSUMPM-2, are marked in the upper right hand corner of the fusion method score, in the upper left hand corner of DDF's score, and in the lower left hand corner of DDF's score, respectively.",1,TREC,True
434,Prec-IA MAP-IA -nDCG ERR-IA,1,MAP,True
435,2012 DFalah120A DFalah120D xQuAD (uogTrA44xi) xQuAD (uogTrA44xu) xQuAD (uogTrB44xu) ClustFuseCombMNZ ClustFuseCombSUM CombSUMMMR CombSUMPM-2 CombMNZ CombSUM DDF,0,,False
436,.3241 .3241 .3349 .3504 .3389 .3533 .3545 .3558 .3718 .3663 .3592 .3904,0,,False
437,.0990 .0990 .1345 .1360 .1339 .1488 .1495 .1544 .1826 .1785 .1767 .1910,0,,False
438,.5291 .5291 .5917 .6061 .5795 .6010 .5965 .6106 .6228 .6154 .6114 .6334,0,,False
439,.4259 .4259 .4873 .5048 .4785 .5105 .5049 .5115 .5179 .5153 .5126 .5266,0,,False
440,2011 ICTNET11ADR2 umassGQdist xQuAD (uogTrA45Nmx2) xQuAD (uogTrA45Vmx) UWatMDSdm ClustFuseCombMNZ ClustFuseCombSUM CombSUMMMR CombSUMPM-2 CombMNZ CombSUM DDF,0,,False
441,.2993 .3003 .3039 .3030 .3214 .3303 .3296 .3395 .3450 .3413 .3376 .3596,0,,False
442,.1328 .1313 .1365 .1323 .1350 .1757 .1775 .1830 .2024 .1943 .1966 .2102,0,,False
443,.5725 .5513 .6298 .6304 .5979 .6221 .6307 .6341 .6448 .6430 .6423 .6496,0,,False
444,.4658 .4530 .5284 .5238 .4875 .5001 .5110 .5107 .5196 .5209 .5216 .5295,0,,False
445,2010 CSE.pm2.run cmuWi10D xQuAD (uogTrA42x) PM-2 (engine) PM-2 (TREC) ClustFuseCombMNZ ClustFuseCombSUM CombSUMMMR CombSUMPM-2 CombMNZ CombSUM DDF,1,TREC,True
446,.1832 .1879 .1845 .2009 .2026 .2105 .2072 .2115 .2129 .2177 .2159 .2285,0,,False
447,.0351 .0599 .0529 .0414 .0430 .0845 .0825 .0836 .0839 .0899 .0875 .0910,0,,False
448,.4165 .3452 .3558 .3660 .4449 .4313 .4257 .4366 .4379 .4471 .4454 .4627,0,,False
449,.3052 .2484 .2454 .2581 .3320 .3221 .3148 .3189 .3193 .3411 .3350 .3406,0,,False
450,2009 NeuDiv1 NeuDivW75 xQuAD(uogTrDPCQcdB) xQuAD (uogTrDYCcsB) uwgym ClustFuseCombMNZ ClustFuseCombSUM CombSUMMMR CombSUMPM-2 CombMNZ CombSUM DDF,0,,False
451,.1343 .1239 .1302 .1268 .1224 .1381 .1379 .1424 .1588 .1400 .1400 .1631,0,,False
452,.0458 .0397 .0463 .0444 .0456 .0681 .0680 .0682 .0754 .0666 .0664 .0731,0,,False
453,.2781 .2501 .2968 .3081 .2798 .3076 .3223 .3343 .3887 .3343 .3482 .4005,0,,False
454,.1705 .1598 .1848 .1922 .1701 .1937 .2005 .2028 .2674 .2033 .2080 .2713,0,,False
455,"ranked documents in terms of novelty of the documents. For the MAP-IA metric, however, the gaps increase with more component lists being fused. The performance of both DDF and CombSUM increases faster when the number of component lists increases but is  10 than when the number of component lists is > 10, for all the metrics. This seems to be inherent to the underlying CombSUM method and is due to the fact that with smaller numbers of component lists, there is simply more space available at depth 20 to obtain improvements than with larger numbers of component lists.",1,MAP,True
456,Table 3: Performance obtained using the 2012 editions of the TREC Web track. The best performing run per metric is in boldface. Other notational conventions as in Table 2.,1,TREC,True
457,Prec-IA MAP-IA -nDCG ERR-IA,1,MAP,True
458,2012 QUTparaBline xQuAD (uogTrA44xl) utw2012c1 PM-2 (TREC) ClustFuseCombMNZ ClustFuseCombSUM CombSUMMMR CombSUMPM-2 CombMNZ CombSUM DDF,1,TREC,True
459,.2261 .2957 .1637 .2631 .2735 .2752 .2783 .2934 .2864 .2884 .3193,0,,False
460,.0639 .1077 .0439 .0601 .1155 .1172 .1189 .1305 .1267 .1275 .1409,0,,False
461,.5270 .5161 .5075 .5245 .5717 .5726 .5799 .6013 .5851 .5944 .6107,0,,False
462,.4185 .4009 .4046 .4155 .4608 .4674 .4633 .4877 .4708 .4803 .4919,0,,False
463,5.4 Query-level analysis,1,Query,True
464,"We take a closer look at per test query improvements of DDF over the best baseline fusion run when fusing the best 5 runs in 2012, viz., CombSUMPM-2, which outperforms the best component list. Fig. 3 shows the per query performance differences in terms of Prec-IA, MAP-IA, -nDCG and ERR-IA, respectively, of DDF against CombSUMPM-2. DDF achieves performance improvements for many queries when compared against CombSUMPM-2, although the differences are sometimes relatively small.",1,MAP,True
465,"In a very small number of cases, DDF performs poorer than CombSUMPM-2. This appears to be due to the fact that DDF ""over-diversifies"" documents in runs produced by CombSUM that have very few relevant document to start with, so that DDF ends up promoting different but non-relevant documents.",0,,False
466,5.5 Zooming in on Prec-IA@k,0,,False
467,"Next, we zoom in on one of the metrics that shows the biggest relative differences between DDF and the next best performing fusion method, Prec-IA, so as to understand how the runs generated by DDF differ from those by other fusion-based methods. Here, again, we use CombSUMPM-2 as a representative, as it tends to outperform or equal the other fusion methods. Specifically, we report changes in the number of relevant documents for DDF against CombSUMPM-2 when fusing the 2012 runs in Table 2 in 2012; see Fig. 4. Red bars indicate the number of relevant documents that appear in the run of DDF but not the run of CombSUMPM-2, white bars indicate the number of relevant documents in both runs, whereas blue bars indicate the number of relevant documents that appear not in DDF but in CombSUMPM-2; topics are ordered first by the size of the red bar, then the size of the white bar, and finally the size of the blue bar.",0,,False
468,"Clearly, the differences between DDF and CombSUMPM-2 in the top 5 and 10 are more limited than the differences in the top-15 and 20, but in all cases DDF outperforms CombSUMPM-2. E.g., in total there are 45 more relevant documents in the top 20 of the run produced by DDF than those in the CombSUMPM-2 run (49 relevant documents in DDF but not in CombSUMPM-2, 4 relevant documents in CombSUMPM-2 but not in DDF). We examine the matter further by comparing the Prec-AI@5, 10, 15, 20 scores of the DDF and CombSUMPM-2 runs for the 2012 data; see Table 4. The differences at small depths (5, 10) are weakly statistically significant while those at bigger depths are significant, confirming our observations in Fig. 4; we also find that DDF statistically significantly outperforms CombSUMPM-2 in terms of Prec-IA scores at depth 5, 10, 15 and 20, which again confirms the above observations based on Fig. 4.",0,,False
469,309,0,,False
470,Prec-IA,0,,False
471,0.35 0.3,0,,False
472,0.25 0.2 0,0,,False
473,DDF CombSUM,0,,False
474,10,0,,False
475,20,0,,False
476,30,0,,False
477,Number of runs to be fused,0,,False
478,MAP-IA,1,MAP,True
479,0.3 0.25,0,,False
480,0.2 0.15,0,,False
481,0.1 0.05,0,,False
482,0,0,,False
483,0.59,0,,False
484,0.55,0,,False
485,-nDCG,0,,False
486,0.51,0,,False
487,DDF CombSUM,0,,False
488,10,0,,False
489,20,0,,False
490,30,0,,False
491,Number of runs to be fused,0,,False
492,0.47 0.43,0,,False
493,0,0,,False
494,0.5,0,,False
495,0.45,0,,False
496,ERR-IA,0,,False
497,0.4,0,,False
498,DDF CombSUM,0,,False
499,10,0,,False
500,20,0,,False
501,30,0,,False
502,Number of runs to be fused,0,,False
503,0.35 0,0,,False
504,DDF CombSUM,0,,False
505,10,0,,False
506,20,0,,False
507,30,0,,False
508,Number of runs to be fused,0,,False
509,"Figure 2: Effect on performance (in terms of Prec-IA, MAP-IA, -nDCG and ERR-IA) of the number of component lists, using runs sampled from the TREC 2012 Web track. We plot averages and standard deviations. Note: the figures are not to the same scale.",1,MAP,True
510,0.2,0,,False
511,0.2,0,,False
512,0.5,0,,False
513,0.2,0,,False
514,ERR-IA,0,,False
515,-nDCG,0,,False
516,MAP-IA,1,MAP,True
517,Prec-IA,0,,False
518,0,0,,False
519,0,0,,False
520,0,0,,False
521,0,0,,False
522,-0.2 queries,0,,False
523,-0.2 queries,0,,False
524,-0.5 queries,0,,False
525,-0.2 queries,0,,False
526,"Figure 3: Per query performance differences of DDF against CombSUMPM-2 (second row). The figures shown are for fusing the runs in TREC Web 2012 track, for Prec-IA@20, MAP-IA@20, -nDCG@20 and ERR-IA@20 (from left to right). A bar extending above the center of a plot indicates that DDF outperforms CombSUMPM-2, and vice versa for bars below the center.",1,TREC,True
527,"Table 4: Prec-IA@5, 10, 15, 20 performance comparison between CombSUMPM-2 and DDF. A statistically significant difference between DDF and CombSUMPM-2 is marked in the upper left hand corner of the DDF score.",0,,False
528,Prec-IA@,0,,False
529,5,0,,False
530,10 15 20,0,,False
531,CombSUMPM-2 .4367 .4066 .3887 .3718,0,,False
532,DDF,0,,False
533,.4555 .4194 .4060 .3904,0,,False
534,5.6 Effect of the number of topics,0,,False
535,"Finally, we examine the effect on the overall performance of the number of latent topics used in DDF, and contrast the performance of DDF with varying number of latent topics against DDF2, CombSUM and CombSUMPM-2. Here, DDF2 is the same algorithm as DDF except that for every test query it considers as many latent topics as there are aspects according to the ground truth. We use DDF2, DDF, CombSUM and CombSUMPM-2 to fuse the component result runs listed in Table 2 in 2012 as an example. We vary the number of latent topics in DDF from 2 to 16. See Fig. 5.",0,,False
536,"When the number of latent topics used in DDF increases from 2 to 6, the performance of DDF increases dramatically. When only 2 latent topics are used, the performance is worse than that of CombSUM and CombSUMPM-2; e.g., Prec-IA@20 for DDF is 0.3404, while the scores of CombSUM and CombSUMPM-2 are 0.3592 and 0.3718, respectively. In contrast, when the number of latent topics varies between 8 to 16, the performance of DDF seems to level off. This demonstrates another merit of our fusion model, DDF: it is robust and not sensitive to the number of latent topics once the number of latent topics is ""large enough."" Another important finding from Fig. 5 is that DDF2 always enhances the performance of DDF, CombSUM and CombSUMPM-2, for all metrics, which demonstrates the fact that latent topics can enhance the performance. The performance differences between DDF2 and DDF are quite marginal and not statistically significant. We leave it as future work to dynamically estimate the number of aspects (and latent topics) of an incoming query and to use this estimate in DDF.",0,,False
537,6. CONCLUSION,0,,False
538,Most previous work on search result diversification focuses on,0,,False
539,"the content of the documents returned by an ad hoc algorithm to diversify the results implicitly or explicitly, i.e., using implicit or explicit representations of aspects. In this paper we have adopted a different perspective on the search result diversification problem, based on data fusion. We proposed to use traditional unsupervised and state-of-the-art data fusion methods, CombSUM, CombMNZ, ClustFuseCombSUM, ClustFuseCombMNZ, CombSUMMMR and CombSUMPM-2 to diversify result lists. This led to the insight that fusion does aid diversification. We also proposed a fusion-based diversification method, DDF, which infers latent topics from ranked lists of documents produced by a standard fusion method, and combines this with a state-of-the-art result diversification model. We found that data fusion approaches outperform state-of-the-art search result diversification algorithms, with DDF invariably giving rise to the highest scores on all of the metrics that we have considered in this paper. DDF was shown to behave well with different numbers of component lists. We also found that DDF is insensitive to the number of latent topics of a query, once a sufficiently large number was chosen, e.g., 10.",1,ad,True
540,"As to future work, we aim to incorporate into DDF methods for automatically estimating the number of aspects, which will be used to set the number of latent topics. The last and third part of DDF is based on a particular choice of method, viz. PM-2, and we only apply rank-based fusion methods for diversification. In future work we plan to compare these choices with alternative choices, and apply other fusion alternatives, e.g., score-based fusion methods.",1,corpora,True
541,"Acknowledgements. We thank Van Dang for generating the PM-2 runs for us. This research was partially supported by the China Scholarship Council, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreements nrs 288024 and 312827, the Netherlands Organisation for Scientific Research (NWO) under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, the Center for Creation, Content and Technology (CCCT), the QuaMerdes project funded by the CLARIN-nl program, the TROVe project funded by the CLARIAH program, the Dutch national program COMMIT, the ESF Research Network Program ELIAS, the Elite Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), the Netherlands eScience Center under project number 027.012.105, the Yahoo! Faculty Research and Engagement Program, the Microsoft Research PhD program, and the HPC Fund.",1,ad,True
542,7. REFERENCES,0,,False
543,"[1] S. Abbar, S. Amer-Yahia, P. Indyk, and S. Mahabadi.",1,ad,True
544,310,0,,False
545,number,0,,False
546,Top 5 documents 6,0,,False
547,4,0,,False
548,2,0,,False
549,number,0,,False
550,Top 10 documents 10,0,,False
551,5,0,,False
552,number,0,,False
553,Top 15 documents 15,0,,False
554,10,0,,False
555,5,0,,False
556,number,0,,False
557,Top 20 documents 20 15 10,0,,False
558,5,0,,False
559,00,0,,False
560,10 20 30 40 50,0,,False
561,queries,0,,False
562,00,0,,False
563,10 20 30 40 50,0,,False
564,queries,0,,False
565,00,0,,False
566,10 20 30 40 50,0,,False
567,queries,0,,False
568,00,0,,False
569,10 20 30 40 50,0,,False
570,queries,0,,False
571,"Figure 4: How runs produced by DDF and CombSUMPM-2 differ. Red, white, blue bars indicate the number of relevant documents that appear in DDF but not in CombSUMPM-2, in both runs and not in DDF but in CombSUMPM-2, respectively, at corresponding depth k (for k ,"" 5, 10, 15, 20). Figures should be viewed in color.""",0,,False
572,Prec-IA,0,,False
573,0.4,0,,False
574,0.35 0.3 0,0,,False
575,DDF2 DDF CombSUMPM-2 CombSUM,0,,False
576,5,0,,False
577,10,0,,False
578,15,0,,False
579,Number of topics,0,,False
580,MAP-IA,1,MAP,True
581,0.19 0.18 0.17 0.16,0,,False
582,0,0,,False
583,DDF2 DDF CombSUMPM-2 CombSUM,0,,False
584,5,0,,False
585,10,0,,False
586,15,0,,False
587,Number of topics,0,,False
588,-nDCG,0,,False
589,0.64,0,,False
590,0.62 0.6,0,,False
591,0.58 0,0,,False
592,DDF2 DDF CombSUMPM-2 CombSUM,0,,False
593,5,0,,False
594,10 15,0,,False
595,Number of topics,0,,False
596,ERR-IA,0,,False
597,0.54 0.52,0,,False
598,0.5 0.48,0,,False
599,0,0,,False
600,DDF2 DDF CombSUMPM-2 CombSUM,0,,False
601,5,0,,False
602,10,0,,False
603,15,0,,False
604,Number of topics,0,,False
605,"Figure 5: Performance comparison between DDF2, DDF, CombSUMPM-2 and CombSUM when varying the number of latent topics used in DDF. Note: the figures are not to be the same scale.",0,,False
606,"Real-time recommendation of diverse related articles. In WWW, pages 1­12, 2013. [2] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In WSDM, pages 5­14, 2009. [3] E. Aktolga and J. Allan. Sentiment diversification with different biases. In SIGIR, pages 593­600, 2013. [4] J. A. Aslam and M. Montague. Models for metasearch. In SIGIR'01, pages 276­284, 2001. [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993­1022, 2003. [6] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR, pages 335­336, 1998. [7] H. Chen and D. R. Karger. Less is more: probabilistic models for retrieving fewer relevant documents. In SIGIR, pages 429­436, 2006. [8] C. L. A. Clarke and N. Craswell. Overview of the TREC 2011 web track. In TREC, pages 1­9, 2011. [9] C. L. A. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Büttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR, pages 659­666, 2008. [10] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 web track. In TREC, pages 1­9, 2009. [11] C. L. A. Clarke, N. Craswell, I. Soboroff, and G. V. Cormack. Overview of the TREC 2010 web track. In TREC, pages 1­9, 2010. [12] C. L. A. Clarke, N. Craswell, and E. M. Voorhees. Overview of the TREC 2012 web track. In TREC, pages 1­8, 2012. [13] V. Dang and W. B. Croft. Diversity by proportionality: An election-based approach to search result diversification. In SIGIR, pages 65­74, 2012. [14] V. Dang and W. B. Croft. Term level search result diversification. In SIGIR, pages 603­612, 2013. [15] M. Efron. Information search and retrieval in microblogs. J. Am. Soc. for Inform. Sci. and Techn., 62(6):996­1008, 2011. [16] M. Farah and D. Vanderpooten. An outranking approach for rank aggregation in information retrieval. In SIGIR'07, 2007.",1,TREC,True
607,"[17] E. A. Fox and J. A. Shaw. Combination of multiple searches. In TREC-2, 1994.",1,TREC,True
608,"[18] T. L. Griffiths and M. Steyvers. Finding scientific topics. PNAS, 101:5228­5235, 2004.",0,,False
609,"[19] D. He and D. Wu. Toward a robust data fusion for document retrieval. In IEEE NLP-KE'08, 2008.",0,,False
610,"[20] J. He, V. Hollink, and A. de Vries. Combining implicit and explicit topic representations for result diversification. In SIGIR, pages 851­860, 2012.",0,,False
611,"[21] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR, pages 50­57, 1999.",0,,False
612,"[22] O. Jin, N. N. Liu, K. Zhao, Y. Yu, and Q. Yang. Transferring topical knowledge from auxiliary long texts for short text clustering. In CIKM, pages 775­784, 2011.",0,,False
613,"[23] A. K. Kozorovitsky and O. Kurland. Cluster-based fusion of retrieved lists. In SIGIR'11, pages 893­902, 2011.",0,,False
614,"[24] T. Kurashima, T. Iwata, T. Hoshide, N. Takaya, and K. Fujimura. Geo topic model: joint modeling of user's activity area and interests for location recommendation. In WSDM, pages 375­384, 2013.",0,,False
615,"[25] J. D. Lafferty and D. M. Blei. Correlated topic models. In NIPS'05, pages 147­154, 2005.",0,,False
616,"[26] J. H. Lee. Combining multiple evidence from different properties of weighting schemes. In SIGIR'95, pages 180­188, 1995.",0,,False
617,"[27] J. H. Lee. Analyses of multiple evidence combination. In SIGIR, 1997.",0,,False
618,"[28] F. Li, M. Huang, and X. Zhu. Sentiment analysis with global topics and local dependency. In AAAI, 2010.",0,,False
619,"[29] W. Li and A. McCallum. Pachinko allocation: Dag-structured mixture models of topic correlations. In ICML, pages 577­584. ACM, 2006.",0,,False
620,"[30] S. Liang and M. de Rijke. Finding knowledgeable groups in enterprise corpora. In SIGIR'13, pages 1005­1008, 2013.",1,corpora,True
621,"[31] S. Liang, M. de Rijke, and M. Tsagkias. Late data fusion for microblog search. In ECIR'13, pages 743­746, 2013.",1,blog,True
622,"[32] S. Liang, Z. Ren, and M. de Rijke. The impact of semantic document expansion on cluster-based fusion for microblog",1,blog,True
623,311,0,,False
624,"search. In ECIR'14, pages 493­499, 2014. [33] N. Limsopatham, R. McCreadie, and M.-D. Albakour.",1,ad,True
625,"University of Glasgow at TREC 2012: Experiments with Terrier in medical records, microblog, and web tracks. In TREC, 2012. [34] J. S. Liu. The collapsed gibbs sampler in bayesian computations with applications to a gene regulation problem. J. Am. Stat. Assoc., 89(427):958­966, 1994. [35] C. Macdonald and I. Ounis. Voting for candidates: Adapting data fusion techniques for an expert search task. In CIKM, 2006. [36] Z. Ren, S. Liang, E. Meij, and M. de Rijke. Personalized time-aware tweets summarization. In SIGIR, 2013. [37] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. The author-topic model for authors and documents. In UAI, pages 487­494, 2004. [38] T. Sakai, Z. Dou, and C. L. A. Clarke. The impact of intent selection on diversified search result. In SIGIR, 2013. [39] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In WWW, pages 881­890, 2010. [40] R. L. Santos, C. Macdonald, and I. Ounis. Intent-aware search result diversification. In SIGIR, pages 595­604, 2011. [41] R. L. T. Santos, J. Peng, C. Macdonald, and I. Ounis. Explicit search result diversification through sub-queries. In ECIR, 2010. [42] J. A. Shaw and E. A. Fox. Combination of multiple searches. In TREC 1992, pages 243­252. NIST, 1993. [43] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. LambdaMerge: merging the results of query reformulations. In WSDM, pages 795­804, 2011. [44] I. Szpektor, Y. Maarek, and D. Pelleg. When relevance is not enough: promoting diversity and freshness in personalized question recommendation. In WWW '13, 2013. [45] S. Vargas, P. Castells, and D. Vallet. Explicit relevance models in intent-oriented information retrieval diversification. In SIGIR, pages 75­84, 2012. [46] X. Wang and A. McCallum. Topics over time: a non-markov continuous-time model of topical trends. In KDD'06, pages 424­433, 2006. [47] X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In SIGIR, pages 178­185, 2006. [48] X. Wei, J. Sun, and X. Wang. Dynamic mixture models for multiple time-series. In IJCAI, pages 2909­2914, 2007. [49] S. Wu. Data fusion in information retrieval, volume 13 of Adaptation, Learning and Optimization. Springer, 2012. [50] Z. Xu, Y. Zhang, Y. Wu, and Q. Yang. Modeling user posting behavior on social media. In SIGIR, pages 545­554, 2012. [51] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR, pages 10­17, 2003.",1,TREC,True
626,APPENDIX Gibbs sampling derivation for DDF model,1,AP,True
627,"We begin with the joint distribution P (w, f , z|, , µ, , L) and use conjugate priors to simplify the integrals. Notation defined in §3.",0,,False
628,"P (w, f , z|, , µ, , L, q) ,"" P (w|z, )p(f |µ, , z, L)P (z|)""",0,,False
629,","" P (w|, z)p(|)d × p(f |µ, , z, L, q) P (z|)P (|)d""",0,,False
630,|CL| Nd,0,,False
631,T,0,,False
632,",",0,,False
633,P (wdi|zdi ) p(z |)d,0,,False
634,"d,1 i,1",0,,False
635,"z,1",0,,False
636,|CL| Nd,0,,False
637,×,0,,False
638,"p(fdi|µzdi , zdi , L, q)",0,,False
639,"d,1 i,1",0,,False
640,|CL|  Nd,0,,False
641,×,0,,False
642, P (zdi|d)p(d|) d,0,,False
643,"d,1 i,1",0,,False
644,",",0,,False
645,TV,0,,False
646,T,0,,False
647,nzvzv,0,,False
648,"z,1 v,1",0,,False
649,"z,1",0,,False
650,(,0,,False
651,"V v,1",0,,False
652,v,0,,False
653,),0,,False
654,"V v,1",0,,False
655,(v ),0,,False
656,"V v,1",0,,False
657,zvv -1,0,,False
658,d,0,,False
659,|CL| Nd,0,,False
660,×,0,,False
661,"p(fdi|µzdi , zdi , L, q)",0,,False
662,"d,1 i,1",0,,False
663,×,0,,False
664,|CL| T,0,,False
665,|CL |,0,,False
666,dmzdz,0,,False
667,"d,1 z,1",0,,False
668,"d,1",0,,False
669,(,0,,False
670,"T z,1",0,,False
671,z,0,,False
672,),0,,False
673,"T z,1",0,,False
674,(z,0,,False
675,),0,,False
676,"T z,1",0,,False
677,dzz -1,0,,False
678,d,0,,False
679,",",0,,False
680,(,0,,False
681,"V v,1",0,,False
682,v,0,,False
683,),0,,False
684,T,0,,False
685,"V v,1",0,,False
686,(v,0,,False
687,),0,,False
688,(,0,,False
689,"T z,1",0,,False
690,z ),0,,False
691,"T z,1",0,,False
692,(z,0,,False
693,),0,,False
694,|CL |,0,,False
695,|CL| Nd,0,,False
696,×,0,,False
697,"p(fdi|µzdi , zdi , L, q)",0,,False
698,"d,1 i,1",0,,False
699,×,0,,False
700,"T z,1",0,,False
701,(,0,,False
702,"V v,1",0,,False
703,(nzv,0,,False
704,"V v,1",0,,False
705,(nzv,0,,False
706,+ +,0,,False
707,v ) v )),0,,False
708,"|CL | d,1",0,,False
709,(,0,,False
710,"T z,1",0,,False
711,(mdz,0,,False
712,"T z,1",0,,False
713,(mzd,0,,False
714,+ +,0,,False
715,z ) z )),0,,False
716,"Using the chain rule, we can obtain the conditional probability conveniently,",0,,False
717,"P (zdi|w, f , z-di, , , µ, , L, q)",0,,False
718,","" P (zdi, wdi, fdi|w-di, f-di, z-di, , , µ, , L, q) P (wdi, fdi|w-di, f-di, z-di, , , µ, , L, q)""",0,,False
719,"P (w, f , z|, , µ, , L, q) ,",0,,False
720,"P (w, f , z-di|, , µ, , L, q)",0,,False
721,because zdi depends only on wdi and fdi,0,,False
722,"P (w, f , z|, , µ, , L, q) ",0,,False
723,"P (w-di, f-di, z-di, |, , µ, , L, q)",0,,False
724,(mdzdi + zdi - 1),0,,False
725,nzdiwdi + wdi - 1,0,,False
726,"V v,1",0,,False
727,(nzdi,0,,False
728,v,0,,False
729,+,0,,False
730,v ),0,,False
731,-,0,,False
732,1,0,,False
733,×,0,,False
734,1 ,0,,False
735,exp{- (ln fdi - µzdi )2 },0,,False
736,fdizdi 2,0,,False
737,2z2di,0,,False
738,(mdzdi + zdi - 1),0,,False
739,nzdiwdi + wdi - 1,0,,False
740,"V v,1",0,,False
741,(nzdi,0,,False
742,v,0,,False
743,+,0,,False
744,v ),0,,False
745,-,0,,False
746,1,0,,False
747,×,0,,False
748,1,0,,False
749," exp{- (ln FX(d|L, q) - µzdi )2 },",0,,False
750,"FX(d|L, q)zdi 2",0,,False
751,2z2di,0,,False
752,"where FX(d|L, q)  (0, +) is a fusion score generated by a standard fusion method FX for document d  CL given the observation of lists L to be merged and query q. We use FCombSUM(d|L, q).",0,,False
753,Since the data fusion score of a token that appears in d when fusing all,0,,False
754,"the lists in L given a query q and the latent topics of which is zdi, is drawn from log-normal distributions, sparsity is not a big problem for parameter",0,,False
755,"estimation of both µzdi and zdi . For simplicity, we update both µzdi and zdi after each Gibbs sample iteration by maximum likelihood estimation:",0,,False
756,"µ^zdi , ,",0,,False
757,"^z2di , ,",0,,False
758,"|CL | d ,1",0,,False
759,"|CL | d ,1",0,,False
760,"|CL | d ,1",0,,False
761,"|CL | d ,1",0,,False
762,Nd i (zd,0,,False
763,i,0,,False
764,",zdi) ln fd",0,,False
765,i,0,,False
766,nzdi,0,,False
767,Nd i (zd,0,,False
768,i,0,,False
769,",zdi )",0,,False
770,ln FX(d,0,,False
771,"|L, q)",0,,False
772,nzdi,0,,False
773,Nd i (zd,0,,False
774,i,0,,False
775,",zdi)(ln fd",0,,False
776,i,0,,False
777,- µ^)2,0,,False
778,nzdi,0,,False
779,Nd i (zd,0,,False
780,i,0,,False
781,",zdi)(ln FX(d",0,,False
782,"|L, q) - µ^)2",0,,False
783,nzdi,0,,False
784,312,0,,False
785,,0,,False

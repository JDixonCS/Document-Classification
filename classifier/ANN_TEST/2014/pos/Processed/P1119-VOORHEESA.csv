,sentence,label,data,regex
0,The Effect of Sampling Strategy on Inferred Measures,0,,False
1,Ellen M. Voorhees,0,,False
2,National Institute of Standards and Technology,0,,False
3,ellen.voorhees@nist.gov,0,,False
4,ABSTRACT,0,,False
5,"Using the inferred measures framework is a popular choice for constructing test collections when the target document set is too large for pooling to be a viable option. Within the framework, different amounts of assessing effort is placed on different regions of the ranked lists as defined by a sampling strategy. The sampling strategy is critically important to the quality of the resultant collection, but there is little published guidance as to the important factors. This paper addresses this gap by examining the effect on collection quality of different sampling strategies within the inferred measures framework. The quality of a collection is measured by how accurately it distinguishes the set of significantly different system pairs. Top-K pooling is competitive, though not the best strategy because it cannot distinguish topics with large relevant set sizes. Incorporating a deep, very sparsely sampled stratum is a poor choice. Strategies that include a top-10 pool create better collections than those that do not, as well as allow Precision(10) scores to be directly computed.",1,ad,True
6,Categories and Subject Descriptors,0,,False
7,H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation,0,,False
8,Keywords,0,,False
9,Test collections; Incomplete judgments; Sampling,0,,False
10,1. INTRODUCTION,1,DUC,True
11,"Test collections drive much of the research in information retrieval. Obtaining human judgments regarding document relevance is the most expensive direct cost of building a test collection, so recent research has focused on developing evaluation mechanisms to support fair comparison of retrieval results using relatively small amounts of judging effort. One such mechanism is the judgment-set-building process that supports the computation of extended inferred estimates of traditional evaluation measures using many fewer judgments",1,ad,True
12,"This paper is authored by an employee(s) of the United States Government and is in the public domain. Non-exclusive copying or redistribution is allowed, provided that the article citation is given and the authors and agency are clearly identified as its source. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. 2014 ACM 978-1-4503-2257-7/14/07 http://dx.doi.org/10.1145/2600428.2609524 .",1,Gov,True
13,"than would be required to reliably compute the traditional scores directly [4, 5]. This framework has proved popular because it is convenient to use in practice and supports a variety of evaluation measures.",1,ad,True
14,"Empirical evaluation of the inferred measures shows the estimates can have high fidelity to their directly-computed counterparts, but the quality of the estimates depends on the sampling strategy used to select which documents to judge. Despite the importance of the sampling strategy on estimate quality, there is little guidance in the literature on how best to sample to obtain good estimates. This paper thus examines the question of how the sampling strategy used in creating judgment sets affects the quality of the resulting test collection when using extended inferred measures.",0,,False
15,2. METHODOLOGY,0,,False
16,"A test collection is a triple containing a document set, a set of topics, and a set of relevance judgments which we will call a qrels. A run is the output of a retrieval system for each of the topics in the collection. This output is assumed to be a list of documents ranked by decreasing similarity to the topic. The quality of a run is evaluated using the mean over all topics in the collection of some metric that computes a per-topic score based on the ranks at which relevant documents are retrieved. This work considers three measures, mean average precision (MAP), normalized discounted cumulative gain (NDCG), and precision at 10 documents retrieved (P(10)) and their inferred counterparts.",1,MAP,True
17,"Ideally, qrels would be complete meaning that all documents are judged for all topics. Complete judgments are infeasible for all but the tiniest of document sets, however, so instead some subset of documents is judged. In pooling [2], the top K results from each of a set of runs are combined to form the pool and only those documents in the pool are judged. Runs are subsequently evaluated by assuming that all unpooled (and hence unjudged) documents are not relevant. The inferred measures framework uses stratified sampling strategies and estimates the values of the measures based on the judgments obtained on sampled documents.",1,ad,True
18,Both pooling and the inferred measures framework construct a judgment set based on a set of runs. The framework focuses different amounts of assessing effort on different regions of the ranked lists (the strata) by assigning a sampling rate to each stratum. The strata are defined by disjoint sets of contiguous ranks. A document belongs to the stratum defined by the smallest rank at which the document was retrieved across the set of runs. The sampling rate is the probability of selecting a document from the stra-,0,,False
19,1119,0,,False
20,"tum to be judged. The inferred measures use the knowledge of a stratum's size, sampling rate, and number of relevant documents found from among the judged set to estimate the total number of relevant documents in the stratum, and combines the different strata's estimates to compute a final global estimate of the number of relevant documents for a topic. Similar local estimates are made for the number of relevant documents retrieved by an individual run based on the number of judged documents retrieved by that run in each of the strata.",1,ad,True
21,"A sampling strategy is the combination of strata definition and sampling rate per stratum. Different sampling strategies applied to the same run set create different test collections since the judged set of documents depends on the sampling strategy. The quality of a sampling strategy is thus measured by the quality of the test collections it induces. To measure the quality of a test collection, we use the accuracy of determining the set of significantly different run pairs as compared to a gold standard set of runs pairs (similar to the approach used by Bompada, et al. [1]).",1,ad,True
22,"In general, the larger the assessment budget (i.e., maximum number of human judgments that can be obtained), the better a test collection will be. Thus it is important to control for the total number of documents judged when comparing sampling strategies. In practice, this means that stratum size and sampling rate must be traded off against one another: small strata can have large sampling rates but large strata must be sampled more sparsely.",1,ad,True
23,"We consider the following strategies in this work. pool: A single, exhaustively judged (thus shallow) stra-",0,,False
24,"tum. The same documents as in pooling are judged, but subsequent evaluation uses the inferred measures.",0,,False
25,"1stratum: A single stratum drawn to a moderate depth and using a moderate sampling rate. In this study, the 1stratum strategy uses a depth of 100 and a sampling rate of 30%.",0,,False
26,"2strata: An exhaustively judged small initial stratum plus a moderate depth and sampling rate second stratum. This strategy is motivated by the fact that it allows P(depth) to be computed exactly, and it concentrates the most assessment effort at ranks that have a large effect on the measures' scores. In this study, the 2strata strategy consists of all documents in ranks 1­10 plus a 10% sample of documents in ranks 11­100.",0,,False
27,"3strata: An exhaustively judged small initial stratum, a moderate depth, moderate sampling rate second stratum, and a deep, very sparsely sampled final stratum. The addition of a large, sparsely sampled stratum is motivated by the hope that it will provide a better estimate of the total number of relevant documents. In this study, the 3strata strategy consists of all documents from ranks 1­10 in union with a 10% sample of documents from ranks 11­20 and a 1% sample of documents from ranks 21­1000.",1,ad,True
28,"To examine the quality of a sampling strategy, we use an existing test collection and create a set of sampled collections containing a subset of the original collection's judgments. We then compare the average behavior of the sampled collections to that of the original collection, which we treat as gold standard behavior. We use two existing TREC collections as the base collection in this work: the collection built in the the TREC-8 ad hoc retrieval task and the collection built in the TREC 2012 Medical Records track.",1,TREC,True
29,"The TREC-8 ad hoc track collection contains about 528,000 mostly newswire documents and 50 topics. Binary relevance judgments were created through pooling with K , 100. This collection was chosen for this study because it is a high quality test collection that has been used in many similar studies; evaluation scores computed on this collection are as close to Truth as we are likely to ever get. We use traditional evaluation measures computed over the official qrels as the gold standard for this collection.",1,TREC,True
30,"The original motivation for this study was unexpectedly noisy scores that resulted from using inferred measures in the TREC 2011 Medical Records track [3]. The 2012 Medical Records track collection is similar to the 2011 collection in that it uses the same document set and similar topics, but it contains a better set of relevance judgments to use as ground truth. The document set is approximately 17,000 medical visit reports and there are 47 topics. Judgment sets were created using a two strata strategy. The first stratum contains all documents retrieved between ranks 1­15 from 88 runs, and the second stratum is drawn from ranks 16­ 100 with a sampling rate of 25%. Documents were judged on a three-way scale of relevant, partially relevant and not relevant. For this collection, the gold standard scores are the inferred scores computed using the entire qrels created in the track. The computation of infNDCG uses a gain value of 1 for partially relevant documents and 2 for fully relevant documents.",1,TREC,True
31,"Call a run that contributed to the judgment sets for the base collection a judged run1. To test a sampling strategy, we randomly split the set of judged runs in half, and apply the sampling strategy to only the runs in one half. Once a judgment set is produced, we evaluate all runs submitted to the original TREC track using it. The entire process is repeated 50 times, each time called a trial, with each trial using a different random split of judged runs. All sampling strategies use precisely the same split of judged runs in each trial, but necessarily produce different judgment sets from that split.",1,TREC,True
32,3. EXPERIMENTAL RESULTS,0,,False
33,"Using the set of sampled judgment sets, we can compare the sampling strategies along three dimensions: the number of judged documents required, the accuracy of determining significantly different run pairs, and the accuracy of estimates of the total number of relevant documents, R.",0,,False
34,3.1 Judgment set size,0,,False
35,"As mentioned above, comparisons of sampling strategies must control for the total number of judgments a strategy requires, called the judgment set size. The mean judgment size over the 50 trials for each sampling strategy is given in Table 1.",0,,False
36,"Note that it is possible that a document selected to be judged might not actually have a judgment in the base collection's qrels. When this happens, we ignore the selection (i.e., we mark it as not selected). To account for this effect, the sampling rate actually used in constructing the sample judgment sets was greater than the target rate such that the number of judgments obtained approximated the number the target rate would produce (while always sampling",0,,False
37,"1While 129 runs were submitted to the TREC-8 ad hoc task, only 71 runs were judged. All submitted runs were judged for the TREC 2013 Medical Records collection.",1,TREC,True
38,1120,0,,False
39,Table 1: Mean judgment set size per strategy,0,,False
40,TREC-8,1,TREC,True
41,TREC 2012,1,TREC,True
42,Mean,0,,False
43,Mean,0,,False
44,ID,0,,False
45,Size,0,,False
46,ID,0,,False
47,Size,0,,False
48,pool15 179.7 pool15 197.9,0,,False
49,pool25 296.0,0,,False
50,1stratum 323.2 1stratum 255.9,0,,False
51,2strata 221.6 2strata 227.8,0,,False
52,3strata 199.4,0,,False
53,"from the appropriate stratum). The rates given in the definitions of the strata are the target rates, which are also the effective sampling rates. The strata sizes and sampling rates provide only coarse control of the resulting judgment set size, however, and there are sampling strategies the base collection cannot support because there are too many missing judgments in its qrels. The TREC 2012 collection cannot support a pooled strategy with a depth greater than 15, nor does it support the exploration of a 3strata strategy. We use a pool depth of both 15 and 25 for the TREC-8 collection since depth 25 is a better match for the other methods' judgment set sizes on that collection while depth 15 is used for the TREC 2012 collection.",1,TREC,True
54,"The lack of fine control means there is a wider spread in the range of judgment set sizes across the strategies than is ideal. However, the 1stratum strategy is the strategy with the largest judgment set size, and as will be seen below, it is not a very good strategy even with the larger number of judgments.",1,ad,True
55,3.2 Collection quality,0,,False
56,"We use the accuracy of detecting the set of significantly different run pairs as the measure of a sampled collection's quality. For a given measure, we first compute the set of significantly different run pairs in the base collection using a paired t-test with  ,"" 0.05. Using the same test for a sampled collection, we calculate the number of pairs in each of the following five categories: true positive: the run pair is significantly different in both""",0,,False
57,"collections and the collections agree as to which system is better; true negative: the run pair is not significantly different in either collection; miss: the run pair is significantly different in the base collection, but not the sampled collection; false alarm: the run pair is not significantly different in the base collection, but is in the sampled collection; inversion: the run pair is significantly different in both collections, but the collections disagree as to which is the better run. The accuracy of the significant pairs determination is then computed as the number of correct classifications over the sum of correct and incorrect classifications where inversions are counted as both a false alarm and a miss. Accuracy scores are given in Table 2, which shows the mean value as well as the minimum and maximum accuracy values observed across the 50 trials. The 1stratum strategy is consistently the worst strategy for the NDCG and P(10) measures, and is only somewhat more competitive for MAP. This strategy is the only strategy examined that does not judge all of the documents retrieved in the top 10 ranks, but instead samples uniformly from the top 100 ranks. Since all three measures strongly emphasize",1,MAP,True
58,Table 2: Accuracy of reproducing the set of signifi-,0,,False
59,cantly different run pairs,0,,False
60,a) TREC-8,1,TREC,True
61,infAP,1,AP,True
62,infNDCG,0,,False
63,infP10,0,,False
64,pool15,0,,False
65,0.906,0,,False
66,0.922,0,,False
67,0.959,0,,False
68,"[0.855, 0.926] [0.873, 0.945] [0.920, 0.975]",0,,False
69,pool25,0,,False
70,0.929,0,,False
71,0.947,0,,False
72,0.978,0,,False
73,"[0.889, 0.943] [0.901, 0.966] [0.950, 0.987]",0,,False
74,1stratum,0,,False
75,0.878,0,,False
76,0.884,0,,False
77,0.868,0,,False
78,"[0.839, 0.911] [0.836, 0.918] [0.834, 0.904]",0,,False
79,2strata,0,,False
80,0.918,0,,False
81,0.924,0,,False
82,0.953,0,,False
83,"[0.891, 0.939] [0.879, 0.943] [0.932, 0.962]",0,,False
84,3strata,0,,False
85,0.834,0,,False
86,0.912,0,,False
87,0.926,0,,False
88,"[0.786, 0.874] [0.880, 0.934] [0.912, 0.937]",0,,False
89,b) TREC 2012,1,TREC,True
90,infAP,1,AP,True
91,infNDCG,0,,False
92,infP10,0,,False
93,pool15,0,,False
94,0.888,0,,False
95,0.883,0,,False
96,0.917,0,,False
97,"[0.862, 0.911] [0.855, 0.907] [0.885, 0.956]",0,,False
98,1stratum,0,,False
99,0.930,0,,False
100,0.820,0,,False
101,0.895,0,,False
102,"[0.903, 0.948] [0.783, 0.858] [0.853, 0.924]",0,,False
103,2strata,0,,False
104,0.935,0,,False
105,0.904,0,,False
106,0.967,0,,False
107,"[0.901, 0.953] [0.880, 0.926] [0.953, 0.978]",0,,False
108,the quality of the top of the ranked list--though MAP less so than P(10) and NDCG--unjudged documents in these ranks increase the variability of the final estimated score much more than unjudged documents later in the ranked list do. Thus it is beneficial to concentrate more of the judging resources at the very top of the document ranked lists.,1,MAP,True
109,"The 3strata strategy is the strategy used for the TREC 2011 Medical Records collection whose noisy score estimates prompted this study. Its behavior on the TREC-8 collection confirms that it is a poor choice of sampling strategy. The problem is that in this strategy the stratum for which the least information is known receives the greatest emphasis. The computation of the inferred measures uses as an estimate of a run's number of relevant retrieved in a particular stratum, S, a smoothed fraction of the number of judged relevant documents in S retrieved by the run over the number of judged documents in S retrieved by the run. When a run retrieves few (or none) of the documents that were selected to be judged in the stratum but relatively many documents from the stratum, the estimate can be far afield of the true value. For example, if a run retrieves only one document in S that was judged, and that document is relevant, the estimated number of relevant retrieved is .99998 times the number of documents in S that are retrieved by the run. Any strategy that includes a large stratum that is sampled very sparsely will be affected by this behavior.",1,TREC,True
110,"The relative quality of the pool and 2strata strategies is more complex. The 2strata strategy is more accurate for the TREC 2012 collection. The pooling strategy is generally more accurate for the TREC-8 collection, but there is a clear dependency on the pool depth. The next section will show that the 2strata strategy produces better estimates of the number of relevant documents for both collections.",1,TREC,True
111,3.3 Estimating R,0,,False
112,"Part of the computation of extended inferred measures is creating an estimate of the total number of relevant documents for a topic. This estimate has a large impact on the quality of the final estimates of NDCG and MAP scores. Since NDCG is defined over a fixed set of ranks (100 in this work), while MAP considers the entire relevant set, the quality of the estimate of R has a bigger impact on infAP than it does on infNDCG.",1,MAP,True
113,1121,0,,False
114,Pool25,0,,False
115,2strata,0,,False
116,Figure 1: Estimates of R on the TREC-8 collection.,1,TREC,True
117,Table 3: Mean correlation between estimated and,0,,False
118,gold-standard R,0,,False
119,TREC-8 TREC 2012,1,TREC,True
120,1stratum,0,,False
121,0.97,0,,False
122,0.98,0,,False
123,2strata,0,,False
124,0.94,0,,False
125,0.99,0,,False
126,pool25,0,,False
127,0.91,0,,False
128,--,0,,False
129,pool15,0,,False
130,0.87,0,,False
131,0.94,0,,False
132,"Table 3 gives the mean over the 50 trials of the Pearson correlation between the per-topic estimate and goldstandard values of R, the number of relevant documents. The gold-standard value of R for the TREC 2012 collection is the estimate produced using the entire set of runs submitted to the Medical Records track. The estimates based on half of the runs are very highly correlated with the original estimates, though the pooling strategy produces less good estimates than the other strategies. The gold-standard value of R for the TREC-8 collection is the count of the the number of relevant in the original qrels. The estimated R values are again highly correlated with true R, but the correlations are weaker for the TREC-8 collection than the TREC 2012 collection. The pooling strategies exhibit the weakest correlations, and the correlations get weaker as the pool depth gets smaller.",1,TREC,True
133,"Despite a good overall correlation, the pooling strategies underestimate R for all but the smallest of relevant sets, meaning that they cannot distinguish topics that truly have a small relevant set. Figure 1 shows the estimates of R per topic for the TREC-8 collection for the pool25 (left) and 2strata (right) sampling strategies. Individual topics are plotted on the x-axis and the number of documents on the y-axis. The minimum and maximum estimates of R across the 50 trials are plotted by a line connecting the two points (which runs off the top of the graph if the maximum estimate exceeds the graph's largest y-value). The mean across the 50 trials of the estimated R is plotted as an x on that line. The gold-standard value of R is plotted as a circle. For the pool25 strategy, nine topics have a mean estimated R",1,TREC,True
134,"smaller than 20. The gold-standard and estimated R values for these topics are shown below, which has a correlation of just 0.42.",0,,False
135,Gold: 13 22 16 6 28 13 17 17 65 Est: 12.7 17.8 15.0 6.0 13.7 9.8 17.0 17.0 16.0,0,,False
136,4. CONCLUSION,0,,False
137,"The inferred measures framework can construct test collections that have high fidelity with their traditionallyconstructed counterparts using relatively few judgments provided an appropriate sampling strategy is used in the construction process. The framework does not work well with large, sparsely sampled strata nor with strategies that do not exhaustively judge a small top stratum. Restricting all judgments to the very top ranks (i.e., pooling) is also not a good strategy, though, because such samples are unable to accurately estimate R. Thus, what this paper called the 2strata strategy--using an exhaustively judged small initial stratum coupled with a moderate depth, moderate sampling rate stratum--is a practical and effective sampling strategy to use for inferred measures.",1,ad,True
138,5. REFERENCES,0,,False
139,"[1] T. Bompada, C.-C. Chang, J. Chen, R. Kumar, and R. Shenoy. On the robustness of relevance measures with incomplete judgments. In Proceedings of SIGIR 2007, pages 359­366, 2007.",1,ad,True
140,"[2] K. Sparck Jones and C. van Rijsbergen. Report on the need for and provision of an ""ideal"" information retrieval test collection. British Library Research and Development Report 5266, 1975.",0,,False
141,"[3] E. M. Voorhees. The TREC Medical Records track. In Proceedings of ACM Conference on Bioinformatics, Computational Biology and Biomedical Informatics, pages 239­246, 2013.",1,TREC,True
142,"[4] E. Yilmaz and J. A. Aslam. Estimating average precision when judgments are incomplete. Knowledge Information Systems, 16:173­211, 2008.",0,,False
143,"[5] E. Yilmaz, E. Kanoulas, and J. A. Aslam. A simple and efficient sampling method for estimating AP and NDCG. In Proceedings of SIGIR 2008, pages 603­610, 2008.",1,AP,True
144,1122,0,,False
145,,0,,False

,sentence,label,data,regex
0,Win-Win Search: Dual-Agent Stochastic Game in Session Search,1,Session,True
1,"Jiyun Luo, Sicong Zhang, Hui Yang",0,,False
2,"Department of Computer Science, Georgetown University",0,,False
3,"{jl1749,sz303}@georgetown.edu, huiyang@cs.georgetown.edu",0,,False
4,ABSTRACT,0,,False
5,"Session search is a complex search task that involves multiple search iterations triggered by query reformulations. We observe a Markov chain in session search: user's judgment of retrieved documents in the previous search iteration affects user's actions in the next iteration. We thus propose to model session search as a dual-agent stochastic game: the user agent and the search engine agent work together to jointly maximize their long term rewards. The framework, which we term ""win-win search"", is based on Partially Observable Markov Decision Process. We mathematically model dynamics in session search, including decision states, query changes, clicks, and rewards, as a cooperative game between the user and the search engine. The experiments on TREC 2012 and 2013 Session datasets show a statistically significant improvement over the state-of-the-art interactive search and session search algorithms.",1,Session,True
6,Categories and Subject Descriptors,0,,False
7,H.3.3 [Information Systems ]: Information Storage and Retrieval--Information Search and Retrieval,0,,False
8,Keywords,0,,False
9,Dynamic Information Retrieval Modeling; POMDP; Stochastic Game; Session Search,1,Session,True
10,1. INTRODUCTION,1,DUC,True
11,"Users often need a multi-query session to accomplish a complex search task. A session usually starts with the user writing a query, sending it to the search engine, receiving a list of ranked documents ordered by decreasing relevance, then examining the snippets, clicking on the interesting ones, and spending more time reading them; we call one such sequence a ""search iteration."" In the next iteration, the user modifies the query or issues a new query to start the search again. As a result, a series of search iterations form, which include a series of queries q1, ..., qn, a series of returned documents D1, ..., Dn, and a series of clicks C1, ..., Cn, some of which are SAT clicks (satisfactory clicked documents [9]). The session stops when the user's information need is satisfied or the user abandons the search [6]. The information retrieval (IR) task in this setting is called session search [7,",1,ad,True
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609629.",1,ad,True
13,Figure 1: A Markov chain of decision states in session search. (S: decision states; q: queries; A: user actions such as query changes; D: documents).,0,,False
14,"11, 17, 22, 24, 25, 30, 31]. Table 1 lists example information needs and queries in session search.",0,,False
15,"We are often puzzled about what drives a user's search in a session and why they make certain moves. We observe that sometimes the same user behavior, such as a drift from one subtopic to another, can be explained by opposite reasons: either the user is satisfied with the search results and moves to another sub information need, or the user is not satisfied with the search results and leaves the previous search path. The complexity of users' decision making patterns makes session search quite challenging [4, 29].",0,,False
16,"Researchers have attempted to find out the causes of topic drifting in session search. The causes under study include personalization [29], task types [20, 24], and previous documents' relevance [11]. A user study is usually needed to draw conclusions about user intent. However, the focus of this paper is not on identifying user intent. Instead, we simplify the complexity of users' decision states into a cross product of only two dimensions: whether previously retrieved documents are relevant and whether the user would like to explore the next sub information need. Our work differs from existing work in that we consider that a session goes through a series of hidden decision states, with which we design a statistical retrieval model for session search. Our emphasis is an effective retrieval model, not a user study to identify the query intent.",1,ad,True
17,"The hidden decision states form a Markov chain in session search. A Markov chain is a memoryless random process where the next state depends only on the current state [18]. Figure 1 illustrates a Markov chain of hidden decision states for TREC 2013 Session 9. In a session, a user's judgment of the retrieved documents in the previous iteration affects or even decides the user's actions in the next iteration. A user's actions could include clicks, query changes, reading the documents, etc. The user's gain, which we call reward, is the amount of relevant information that he or she obtains in the retrieved documents. The reward motives the later user actions. If the decision states are known, we can use a Markov Decision Process (MDP) to model the process. However, in session search, users' decision states are hidden. We therefore model session search as a Partially Observable Markov Decision Process (POMDP) [18].",1,TREC,True
18,587,0,,False
19,Table 1: Information needs and queries (examples are from TREC 2013 Session Track).,1,TREC,True
20,Session 2: Information Need,1,Session,True
21,Session 2: Queries,1,Session,True
22,"You want to buy a scooter. So you're inter- q1,scooter brands",0,,False
23,"q2,scooter brands reliable q3,scooter",0,,False
24,"ested in learning more facts about scooters q4,scooter cheap",0,,False
25,"q5,scooter review",0,,False
26,"q6,scooter price",0,,False
27,including:what brands of scooters are out,0,,False
28,"q7,scooter price",0,,False
29,"q8,scooter stores",0,,False
30,"q9,where to buy scooters",0,,False
31,there? What brands of scooters are reliable? Which scooters are cheap? Which stores sell scooters? which stores sell the best scooters?,0,,False
32,Session 9: Information Need,1,Session,True
33,Session 9: Queries,1,Session,True
34,"You want to know more about old US coins. q1,old us coins",0,,False
35,"q2,collecting old us coins q3,selling old us coins",0,,False
36,"Relevant information to you includes value of old US coins, types of old US coins, old US silver dollar, q4,""selling old """"usa coins""""""",0,,False
37,"how to start collecting old US coins, how to sell old US coins and how to buy them, where to buy those coins.",0,,False
38,Session 87: Information Need,1,Session,True
39,Session 87: Queries,1,Session,True
40,"Suppose you're planning a trip to the United States.You will be there for a month and able to travel within a 150-mile radius of your destination.With that constraint, what are the best cities to consider as possible destinations?",1,ad,True
41,"q1,best us destinations",0,,False
42,"q2,distance new york boston",0,,False
43,"q3,maps.bing.com",0,,False
44,"q4,maps",0,,False
45,"q5,bing maps",0,,False
46,"q6,hartford tourism",0,,False
47,"q7,bing maps",0,,False
48,"q8,hartford visitors",0,,False
49,"q9,hartford connecticut tourism",0,,False
50,"q10,hartford boston travel",0,,False
51,"q11,boston tourism",0,,False
52,"q12,nyc tourism",0,,False
53,"q13,philadelphia nyc distance",1,ad,True
54,"q14,bing maps",0,,False
55,"q15,philadelphia washington dc distance",1,ad,True
56,"q16,bing maps",0,,False
57,"q17,philadelphia tourism q18,washington dc tourism",1,ad,True
58,"q19,philadelphia nyc travel q20,philadelphia nyc train q21,philadelphia nyc bus",1,ad,True
59,"In fact, not only the user, but also the search engine, makes decisions in a Markov process. A search engine takes in a user's feedback and improves its retrieval algorithm iteration after iteration to achieve a better reward too. The search engine actions could include term weighting, turning on or turning off one or more of its search techniques, or adjusting parameters for the techniques. For instance, based on the reward, the search engine can select p in deciding the top p documents used in pseudo relevance feedback.",1,ad,True
60,"We propose to model session search as a dual-agent stochastic game. When there is more than one agent in a POMDP, the POMDP becomes a stochastic game (SG). The two agents in session search are the user agent and the search engine agent. In contrast to most two-player scenarios such as chess games in game theory, the two agents in session search are not opponents to each other; instead, they cooperate: they share the decision states and work together to jointly maximize their goals. We term the framework ""win-win search"" for its efforts in ensuring that both agents arrive at a winwin situation. One may argue that in reality a commercial search engine and a user may have different goals and that is why some commercial search engines put their sponsors high in the returned results. However, this paper focuses on the win-win setting and assume a common interest ­ fulfilling the information needs ­ for both agents.",1,ad,True
61,"The challenges of modeling session search as a stochastic game lie in how to design and determine the decision states and actions of each agent, how to observe their behaviors, and how to measure the rewards and set the optimization goals. We present the details in Sections 4 and 5. As a retrieval framework, we pay more attention to the search engine agent. When the search engine makes decisions, it picks a decision that jointly optimizes the common interest.",0,,False
62,"We evaluate the win-win search framework on TREC 2012 & 2013 Session data. TREC (Text REtrieval Conference) 2010 - 2013 Session Tracks [20, 21] have spurred a great deal of research in session search [3, 10, 11, 14, 24]. The tracks provide interaction data within a session and aim to retrieve relevant documents for the last query qn in the session. The interaction data include queries, top returned documents, user clicks, and other relevant information such as dwell time. Document relevance is judged based on information need for the entire session, not just the last query. In this paper, all examples are from TREC 2013. Our experiments show that the proposed framework achieves statisti-",1,TREC,True
63,cally significant improvements over state-of-the-art interactive search and session search algorithms.,0,,False
64,"The remainder of this paper is organized as follows: Section 2 presents the related work, Section 3 provides preliminaries for POMDP. Section 4 details the win-win search framework, Section 5 elaborates the optimization, Section 6 evaluates the framework and Section 7 concludes the paper.",0,,False
65,2. RELATED WORK,0,,False
66,2.1 Session search,1,Session,True
67,"Session search has attracted a great amount of research from a variety of approaches [3, 11, 22, 25, 33]. They can be grouped into log-based methods and content-based methods.",1,Session,True
68,"There is a large body of work using query logs to study queries and sessions. Feild and Allan [8] proposed a taskaware model for query recommendation using random walk over a term-query graph formed from logs. Song and He's work [27] on optimal rare query suggestion also used random walk, with implicit feedback in logs. Wang et al. [30] utilized the latent structural SVM to extract cross-session search tasks from logs. Recent log-based approaches also appear in the Web Search Click Data (WCSD) workshop series.1",1,WCS,True
69,"Content-based methods directly study the content of the query and the document. For instance, Raman et al. [25] studied a particular case in session search where the search topics are intrinsically diversified. Content-based session search also include most research generated from the recent TREC Session Tracks [20, 21]. Guan et al. [10] organized phrase structure in queries within a session to improve retrieval effectiveness. Jiang et al. [14] proposed an adaptive browsing model that handles novelty in session search. Jiang and He [13] further analyzed the effects of past queries and click-through data on whole-session search effectiveness.",1,TREC,True
70,"Others study even more complex search ­ search across multiple sessions [22, 24, 30]. Kotov et al. [22] proposed methods for modeling and analyzing users' search behaviors in multiple sessions. Wang et al. [30] identified cross-session search by investigating inter-query dependencies learned from user behaviors.",0,,False
71,"Our approach is a content-based approach. However, it uniquely differs from other approaches by taking a Markov process point of view to study session search.",0,,False
72,1http://research.microsoft.com/en-us/um/people/ nickcr/wscd2014,0,,False
73,588,0,,False
74,2.2 Relevance feedback,0,,False
75,"Session search is closely related to relevance feedback, a traditional IR research field. Classic relevance feedback methods include Rocchio [16], pseudo relevance feedback [2], and implicit relevance feedback [27] based on user behaviors such as clicks and dwell time. Recently, researchers have investigated new forms of relevance feedback. Jin et al. [15] employed a special type of click ­ ""go to the next page"" ­ as relevance feedback to maximize retrieval effectiveness over multi-page results. Zhang et al. [33] modeled query changes between adjacent queries as relevance feedback to improve retrieval accuracy in session search.",1,Session,True
76,"These relevance feedback approaches only considers oneway communication from the user to the search engine [15, 33]. On the contrary, this paper explicitly sets up a two-way feedback channel where both parties transmit information.",0,,False
77,2.3 MDP and POMDP in IR,0,,False
78,"Markov Decision Process (MDP) is an important topic in Artificial Intelligence (AI). An MDP can be solved by a family of reinforcement learning algorithms. Kaelbling et al. [18] brought techniques from operational research to choose the optimal actions in partially observable problems, and designed algorithms for solving Partially Observable Markov Decision Processes (POMDPs). IR researchers have just begun showing interests in MDP and POMDP [11, 30, 32] in finding solutions for IR problems.",0,,False
79,"Early work on interactive search modeling by Shen et al. [26] used a Bayesian decision-theoretic framework, which is closely related to the MDP approaches. The QCM model proposed by Guan et al. [11] models session search as an MDP and effectively improves the retrieval accuracy. However, [11] used queries as states while we use a set of welldesigned hidden decision states. In addition, we explicitly model the stochastic game played between two agents, the user and the search engine, while [11] focused on just the search engine. Another difference is that we model a wide range of actions including query changes, clicks, and document content while [11] only used query changes.",1,ad,True
80,"Yuan and Wang [32] applied POMDP for sequential selection of online advertisement recommendation. Their mathematical derivation shows that belief states of correlated ads can be updated using a formula similar to collaborative filtering. Jin et al. [15] modeled Web search as a sequential search for re-ranking documents in multi-page results. Their hidden states are document relevance and the belief states are given by a multivariate Gaussian distribution. They consider ""ranking"" as actions and ""clicking-on-the-next-page"" as observations. In win-win search, we present a different set of actions, observations, and messages between two agents. The fundamental difference between our approach and theirs is that we model the retrieval task as a dual-agent cooperative game while [15] uses a single agent.",1,ad,True
81,3. PRELIMINARIES: MDP AND POMDP,0,,False
82,"Markov Decision Process provides the basics for the winwin search framework. An MDP is composed by agents, states, actions, reward, policy, and transitions [19]. An agent takes inputs from the environment and outputs actions. The actions in turn influences the states of the environment. An MDP can be represented by a tuple < S, A, T, R >:",0,,False
83,"States S is a discrete set of states. In session search, they can be queries [11] or hidden decision states (Section 4.2).",0,,False
84,Actions A is a discrete set of actions that an agent can,0,,False
85,"take. For instance, user actions include query changes, clicks,",0,,False
86,and reading the returned documents or snippets.,1,ad,True
87,"Transition T is the state transition function T (si, a, sj) ,"" P (sj|si, a). It is the probability of starting in state si, taking action a, and ending in state sj. The sum over all actions gives the total state transition probability between si and sj T (si, sj) "", P (sj|si); which is similar to the state transition probability in the Hidden Markov Model (HMM) [1].",0,,False
88,"Reward r ,"" R(s, a) is the immediate reward, also known""",0,,False
89,as reinforcement. It gives the expected immediate reward of,0,,False
90,taking action a at state s.,0,,False
91,Policy  describes the behaviors of an agent. A non-,0,,False
92,stationary policy is a sequence of mapping from states to,0,,False
93,actions.  is usually optimized to decide how to move around,0,,False
94,in the state space to optimize the long term reward,0,,False
95," t,1",0,,False
96,r.,0,,False
97,Value function and Q-function Given a policy  at,0,,False
98,"time t, a value function V calculates the expected long term reward starting from state s inductively: V,t(s) ,"" R(s, t(s))+  s T (s, a "","" t(s), s )V,t+1(s ), where the initial value V,t"",1(s) ,"" R(s, a "", t,""1(s)), s is the current state, s""",0,,False
99,"is the next state, a ,"" t(s) is any valid action for s at t,""",0,,False
100, is a future discount factor. Usually an auxiliary func-,0,,False
101,"tion, called the Q-function, is used for a pair of (s, a): Q(st, a) ,"" R(st, a) +  a P (st|st+1, a) maxa Q(st+1, a), where R(st, a) is the immediate reward at t, a is any valid action at t + 1. Note that V (s) "","" maxa Q(s, a).""",0,,False
102,Q-Learning Reinforcement learning (RL) algorithms pro-,0,,False
103,vides solutions to MDPs [19]. The most influential RL al-,0,,False
104,gorithm is Q-learning. Given a Q-function and a starting,0,,False
105,"state s, the solution can be a greedy policy that at each",0,,False
106,"step, it takes the action that maximizes the Q-function:",0,,False
107,"(s) ,"" arg maxa R(s, a) +  a T (s, a, s )Q(s , a) , where""",0,,False
108,"the base case maximizes R(s1, a). Partially Observable MDP (POMDP) When states",0,,False
109,are unknown and can only be guessed through a probabilistic,0,,False
110,"distribution, an MDP becomes a POMDP [18]. POMDP is",0,,False
111,"represented by a tuple < S, A, T, , O, B, R >, where S, A, R",0,,False
112,"are the same as in MDP. Since the states are unknown, the",0,,False
113,"transition function T models transitions between beliefs, not",0,,False
114,transitions between states any more: T : B × A × B ,0,,False
115,"[0, 1]. Belief B is the set of beliefs defined over S, which",0,,False
116,indicates the probability that an agent is at a state s. It,0,,False
117,is also known as belief state. Observations  is a discrete,0,,False
118,set of observations that an agent makes about the states. O,0,,False
119,is the observation function which represents a probabilistic,0,,False
120,distribution for making observation  given action a and,0,,False
121,landing in the next state s . A major difference between an,0,,False
122,HMM and a POMDP is that POMDP considers actions and,0,,False
123,rewards while HMM does not.,0,,False
124,4. THE WIN-WIN SEARCH FRAMEWORK,0,,False
125,"A session can be viewed as a Markov chain of evolving states (Figure 1). Every time when a new query is issued, both the user and the search engine transition into a new state. In our setting, the two agents work together to achieve a win-win goal.",0,,False
126,4.1 Model Illustration,0,,False
127,"Figure 2 shows the proposed dual-agent SG, which is represented as a tuple < S, Au, Ase, u, se, u, se, O, B, T, R >.",0,,False
128,S is the decision states that we will present in Section 4.2.,0,,False
129,589,0,,False
130,Table 2: Symbols in the dual-agent stochastic game.,0,,False
131,Name,0,,False
132,Symbol Meanings,0,,False
133,State,0,,False
134,S the four hidden decision states in Figure 3,0,,False
135,User action,0,,False
136,"Au add query terms, remove query terms, keep query terms",1,ad,True
137,Search engine action,0,,False
138,"Ase increase/decrease/keep term weights, adjust search techniques, etc",1,ad,True
139,Message from user to search engine u clicked and SAT clicked documents,0,,False
140,Message from search engine to user se top k returned documents,0,,False
141,User's observation,0,,False
142,u observations that the user makes from the world,0,,False
143,Search engine's observation,0,,False
144,se observations that the search engine makes from the world and from the user,0,,False
145,User reward,0,,False
146,Ru relevant information the user gains from reading the documents,1,ad,True
147,Search engine reward,0,,False
148,Rse nDCG that the search gains by returning documents,0,,False
149,Belief state,0,,False
150,B belief states generated from the belief updater and shared by both agents,0,,False
151,Figure 2: Dual-agent stochastic game.,0,,False
152,"Au, Ase, u, and se are the actions. We divide the actions into two types: domain-level actions A, what an agent acts on the world directly, and communication-level actions , also known as messages, which only go between the agents. User actions Au are mainly query changes [11] while search engine actions Ase are term weighting schemes and adjustments to search techniques. Both u and se are sets of relevant documents, that an agent uses to inform the other agent about what they consider as relevant. (Section 4.3)",1,ad,True
153," is the observation that an agent can draw from the world or from the other agent. O is the observation function that maps states and actions to observations: O : S × A   or O : S ×   . Note that the actions can be domain-level actions A or messages , or a combination of both. (S. 4.4)",0,,False
154,"B is the set of belief states that shared by both agents. The beliefs are updated every time when an observation happens. There are two types of belief: B·, beliefs before the messages and B·, beliefs after the messages. (Section 4.5)",0,,False
155,The reward function R is defined over B×A  R. It is the amount of document relevance that an agent obtains from the world. Rse is the nDCG score (normalized Discounted Cumulative Gain [20]) that the search engine gains for the documents it returns. Ru is the relevance that the user gains from reading the documents. Our retrieval algorithm jointly optimize both Ru and Rse. (Section 5),1,ad,True
156,"Table 2 lists the symbols and their meanings in the dualagent SG. The two agents share the decision states and beliefs but differ in actions, messages, and policies. Although they also make different observations, both contribute to the belief updater; the difference is thus absorbed. As a retrieval model, we only pay attention to the search engine policy se : B  A. The following describes their interactions in the stochastic game:",0,,False
157,"1. At search iteration t ,"" 0, both agents begin in the same initial state S0.""",0,,False
158,"2. t increases by one: t ,"" t + 1. The user agent writes a query qt and takes the tth user agent action atu, which is a query change from the previous query.""",0,,False
159,"3. (*) The search engine agent makes observations se from the world and updates its before-message-beliefstate bt·se based on O(St, atu, se).",0,,False
160,"4. The search engine runs its optimization algorithm and picks the best policy se, which maximizes the joint long term rewards for both agents. Following the policy, it takes actions atse. This is where the model performs retrieval.",0,,False
161,"5. Search engine action atse results in a set of documents Dt, which are returned as message tse sent from the search engine agent to the user agent through the world.",0,,False
162,"6. (*) The user agent receives message tse and observes u. If the user would like to stop the search, the process ends. Otherwise, the user updates the aftermessage-belief-state bt·u based on O(St, tse, u).",0,,False
163,"7. Based on the current beliefs, the user agent sends its feedback messages tu to inform the search engine agent. tu are clicks, some of which are SAT clicks. It contains a set of documents Dclicked.",0,,False
164,"8. (*) The search engine agent observes se from the world and updates its after-message-belief-state bt·se based on O(St, tu, se).",0,,False
165,"9. The user agent picks a policy u, which we don't study here, and continues to send out actions atu+1 in the form of query changes. The world moves into a new state st+1. t , t + 1. The process repeats from step 3.",0,,False
166,"Steps 3, 6, and 8 happen after making an observation from the world or from the other agent. They then all involve a belief update. In the remainder of this section, we present the details of states (Section 4.2), actions (Section 4.3), observation functions (Section 4.4), and belief updates (Section 4.5) for win-win search.",0,,False
167,4.2 States,0,,False
168,"We often observe that the same user behavior in session search may be motivated by different reasons. For instance, in TREC 2013 Session 2 (Table 1), a user searches for ""scooter brands"" as q1 and finds that the 6th returned document with title ""Scooter Brands - The Scooter Review - The Scooter Review"" is relevant. The user clicks this document and reads it for 48 seconds, which we identify as a SAT click since it lasts more than 30 seconds [12]. Next, the user adds a new term `reliable' into q1 to get q2 ,""scooter brands reliable. We notice that `reliability' does not appear in any previously retrieved documents D1. It suggests that the user is inspired to add `reliability' from somewhere else, such as from personal background knowledge or the in-""",1,TREC,True
169,590,0,,False
170,"formation need. In this case, she finds relevant documents from the previously retrieved documents but still decides to explore other aspects about the search target.",0,,False
171,"On the contrary, in TREC 2013 Session 9 q1 (Table 1), another user searches for ""old US coins"" and also finds relevant documents, such as a document about ""... We buy collectible U.S.A. coins for our existing coin collector clients..."". He adds a new term `collecting' to get the next query q2 ""collecting old us coins"". After reducing the query terms and document terms into their stemmed forms, the added term `collecting' does appear in this document as we can see. It suggests that the user selects a term from the retrieval results and hones into the specifics. In this case, he finds relevant documents in the previously retrieved documents and decides to exploit the same sub information need and investigate it more.",1,TREC,True
172,"We observe that even if both users show the same search behavior, e.g. adding terms, the reasons vary: one is adding the new search term because the original search results are not satisfactory, while the other is because the user wants to look into more specifics. This makes us realize that document relevance and users' desire to explore are two independent dimensions in deciding how to form the next query.",1,ad,True
173,"Inspired by earlier research on user intent and task types [24, 28] and our own observations, we propose four hidden decision making states for session search. They are identified based on two dimensions: 1) ""relevant dimension"" ­ whether the user thinks the returned documents are relevant, and 2) ""exploration dimension"" ­ whether the user would like to explore another subtopic. The two dimensions greatly simplify the complexity of user modeling in session search. The relatively small number of discrete states enables us to proceed with POMDP and its optimization at low cost.",0,,False
174,"The cross-product of the two dimensions result in four states: i) user finds a relevant document from the returned documents and decides to explore the next sub information need (relevant and exploration, e.g. scooter price  scooter stores), ii) user finds relevant information and decides to stay in the current sub information need to look into more relevant information (relevant and exploitation, e.g. hartford visitors  hartford connecticut tourism), iii) user finds out that the returned documents are not relevant and decides to stay and try out different expressions for the same sub information need (non-relevant and exploitation, e.g. philadelphia nyc travel  philadelphia nyc train), iv) user finds out that documents are not relevant and decides to give up and move on to another sub information need (nonrelevant and exploration, e.g. distance new york boston  maps.bing.com ).",1,ad,True
175,"Figure 3 shows the decision state diagram for win-win search. The subscriptions stand for {RT ,"" Relevantexploi T ation, RR "","" RelevantexploRation, N RT "","" N onRelevant exploiT ation, N RR "", N onRelevantexploRation}. We insert a dummy starting query q0 before any real query and it always goes to SNRR. The series of search iterations in a session move in the decision states from one to the next. A sequence of states can be time stamped and presented as st ,"" Sm, where t "","" 1, 2, ..., n and m "","" {RT, RR, N RT, N RR}.""",0,,False
176,4.3 Actions,0,,False
177,"There are two types of actions in our framework, domainlevel actions and communications-level actions.",0,,False
178,Figure 3: States.,0,,False
179,4.3.1 Domain-Level Actions,0,,False
180,"The domain-level actions Au and Ase represent the actions directly performed on the world (document collection) by the user agent and by the search engine agent, respectively.",0,,False
181,"The common user actions include writing a query, clicking a document, SAT clicking a document, reading a snippet, reading a document, changing a query, and eye-tracking the documents. In this paper, we only study query changes and clicks as user actions. However, the framework can be easily adopted for other types of user actions.",1,ad,True
182,"Query changes q [11] consist of added query terms +qt ,"" qt\qt-1, removed query terms -qt "","" qt-1\qt, and theme terms qtheme "","" LongestCommon Subsequence(qt, qt-1). For example, in Session 87 , given q19"",philadelphia nyc travel and q20,""philadelphia nyc train, we obtain the following query changes: qtheme "","" LCS(q19, q20) "","" """"philadelphia"""", -q20 "","" """"travel"""", and +q20 "","" """"train"""". All stopwords and function words are removed.""",1,Query,True
183,"The search engine domain-level actions Ase include increasing, decreasing, and maintaining the term weights, as well as adjusting parameters in one or more search techniques. We present the details in Sections 5 and 6.",1,ad,True
184,4.3.2 Communications-Level Actions (Messages),0,,False
185,The second type of actions are communication-level actions (messages) u and se. They are actions that only performed between agents.,0,,False
186,"In our framework, the messages are essentially documents that an agent thinks are relevant. u is the set of documents that the user sends out; we define them as the clicked documents Dclicked. In TREC 2013 Session, 31% search iterations contain SAT clicked documents. 23.9% sessions contain 1 to 4 SAT clicked documents, and a few sessions, for instance Sessions 45, 57 and 72, contain around 10 SAT clicked documents. 88.7% SAT clicked documents appear in the top 10 retrieved results.",1,TREC,True
187,"Similarly, se is the set of documents that the search engine sends out. They are the top k returned documents (k ranges from 0 to 55 in the TREC setting). They demonstrate what documents the search engine thinks are the most relevant. In TREC 2013, 2.8% (10) search iterations return less than 10 documents, 90.7% (322) return exactly 10, 5.1% (18) return 1020, and 1.4% (5) return 2055 documents.",1,TREC,True
188,4.4 Observations,0,,False
189,Section 4.1 illustrates the win-win search framework and the interactions between agents. This section shows how we calculate the observation functions.,0,,False
190,"The observation function O(sj, at, t), defined as P (t|sj, at), is the probability of observing t   when agents take action at and land on state sj. The first type of observation",0,,False
191,591,0,,False
192,"is related to relevance. In Section 4.1 Step 8, after the user sends the message u (user clicks) out at Step 7, the search engine updates its after-message-belief-state b·se based on its observation of user clicks. The observation function for `Relevant' states is:",0,,False
193,"O(st,""Rel, u, t"",Rel) ,,de,f, P (t , Rel|st ,"" Rel, u)""",0,,False
194,(1),0,,False
195,It can be written as,0,,False
196,". P (t,""Rel,st"",""Rel,u)""",0,,False
197,"P (st,""Rel,u)""",0,,False
198,"By taking P (st ,",0,,False
199,"Rel, u) as a constant, we can approximate it by P (t ,",0,,False
200,"Rel, st ,"" Rel, u) "", P (st , Rel|t ,"" Rel, u)P (t "",",0,,False
201,"Rel, u). Given that user clicks u are highly correlated",0,,False
202,"to t, we can approximate P (st , Rel|t ,"" Rel, u) by""",0,,False
203,"P (st , Rel|t ,"" Rel). Further, by taking P () as a con-""",0,,False
204,"stant, we have",0,,False
205,"O(st,""Rel, u, t"",Rel)  P (st , Rel|t , Rel)P (t ,"" Rel, u)  P (st "", Rel|t , Rel)P (t , Rel|u)",0,,False
206,"(2) Similarly, we have",0,,False
207,"O(st,""Non-Rel, u, t"",Non-Rel)  P (st , Non-Rel|t , Non-Rel)P (t , Non-Rel|u)",0,,False
208,"(3) as well as O(st,""Non-Rel, u, t"",Rel) and O(st,""Rel, u, t"",Non-Rel).",0,,False
209,"Based on whether a SATClick exists or not, we calculate the probability of the SG landing at the ""Relevant"" states or the ""Non-Relevant"" states (the first dimension of hidden decision states). At search iteration t, if the set of previously returned documents leads to one or more SAT clicks, the current state is likely to be relevant, otherwise non-relevant. That is to say,",1,ad,True
210,st is likely to be,0,,False
211,Relevant,0,,False
212,if  d  Dt-1 and,0,,False
213,d is SATClicked,0,,False
214,Non-Relevant otherwise.,0,,False
215,"Based on this intuition, we calculate P (t , Rel|u) and P (t , Non-Rel|u) as:",0,,False
216,"P (t , Rel|u) , P ( SATClicks  Dct-lic1ked) (4)",0,,False
217,"P (t , Non-Rel|u) , P ( SATClicks  Dct-lic1ked) (5)",0,,False
218,"The conditional probability of observations P (st , Rel|t ,",0,,False
219,"Rel) and P (st , Non-Rel|t , Non-Rel) can be calculated",0,,False
220,"by maximum likelihood estimation (MLE). For instance,",0,,False
221,P (st,0,,False
222,",",0,,False
223,Rel|,0,,False
224,",",0,,False
225,Rel),0,,False
226,",",0,,False
227,#,0,,False
228,of #,0,,False
229,observed true relevant of observed relevant,0,,False
230,",",0,,False
231,"where ""#",0,,False
232,"of observed true relevant"" is the number of times where the",0,,False
233,previously returned document set Dt-1 contain at least one,0,,False
234,SAT clicks and those SAT clicked documents are indeed rel-,0,,False
235,"evant documents in the ground truth. ""# of observed rele-",0,,False
236,"vant"" is the number of times where Dt-1 contains at least",0,,False
237,one SAT clicks. The ground truth of whether the SG lands,0,,False
238,"on a ""Relevant"" state is generated by documents whose rele-",0,,False
239,vance grades  3 (relevant to highly relevant). The relevance,1,ad,True
240,are judged by NIST assessors [21].,0,,False
241,The second type of observation is related to exploitation,0,,False
242,vs. exploration. This corresponds to a combined observa-,0,,False
243,"tion at Step 3 and the previous Step 6 (Section 4.1), where",0,,False
244,the SG update the before-message-belief-state b·se for a,0,,False
245,user action au (query change) and a search engine message,0,,False
246,"se,""Dt-1, the top returned documents at the previous it-""",0,,False
247,eration. The search engine agent makes observations about,0,,False
248,exploitation vs. exploration (the second dimension of hidden,0,,False
249,decision states) by:,0,,False
250,"O(st,""Exploitation, au"",""qt, se"",""Dt-1, t"",Exploitation)  P (st , Exploitation|t , Exploitation) ×P (t ,"" Exploitation|qt, Dt-1)""",0,,False
251,"(6) O(st,""Exploration,au"",""qt, se"",""Dt-1, t"",Exploration)  P (st , Exploration|t , Exploration) ×P (t ,"" Exploration|qt, Dt-1)""",0,,False
252,(7),0,,False
253,The search engine can guess the hidden states based on,0,,False
254,the following intuition:,0,,False
255, Exploration  ,0,,False
256,st is likely to be Exploitation  ,0,,False
257,"if (+qt ,  and +qt / Dt-1 ) or (+qt ,  and -qt ,  ) if (+qt ,  and +qt  Dt-1 ) or (+qt ,  and -qt ,  )",0,,False
258,"The idea is that given that Dt-1 is the message from search engine and au ,"" q is the message from user, if added query terms +q appear in Dt-1, it is likely that the user stays at the same sub information need from iteration t - 1 to t for `exploitation'. On the other hand, if the added terms +q do not appear in Dt-1, it is likely that the user moves to the next sub information need from iteration t - 1 to t for `exploration'. In addition, if there is no added terms (+qt is empty) but there are deleted terms ( -qt is not empty), it is likely that the user goes to a broader topic to explore. If +qt and -qt are both empty, it means there is no change to the query, it is likely to fall into exploitation.""",1,ad,True
259,"Hence, P (t|qt, Dt-1) can be calculated as:",0,,False
260,"P (t ,"" Exploration|qt, Dt-1) "", P (+qt ,   +qt / Dt-1)",0,,False
261,"+P (+qt ,   -qt , )",0,,False
262,"(8) P (t ,"" Exploitation|qt, Dt-1) "", P (+qt ,   +qt  Dt-1)",0,,False
263,"+P (+qt ,   -qt , )",0,,False
264,(9),0,,False
265,where Dt-1 include all clicked documents and all snippets,0,,False
266,that are ranked higher than the last clicked document at,0,,False
267,iteration t - 1. User actions au include the current query,0,,False
268,"changes +qt and -qt. In fact, P (t|qt, Dt-1) needs to",0,,False
269,"be calculated for each specific case. For instance, P (t ,",0,,False
270,"Exploration|a ,"" `delete term', qt, Dt-1) "",",0,,False
271,#,0,,False
272,of #,0,,False
273,observed true explorations due of observed explorations due to,0,,False
274,to deleting terms deleting terms,0,,False
275,.,0,,False
276,Here we only,0,,False
277,"calculate for the actions with ""deleted terms"". ""# of ob-",0,,False
278,"served explorations"" is the number of observed explorations",0,,False
279,suggesting that the user is likely to explore another subtopic,0,,False
280,"based on Eq. 8, while ""# of observed true explorations"" is",0,,False
281,the number of observed explorations judged positive by hu-,0,,False
282,man accessors in a ground truth. The annotations can be found online.2,0,,False
283,"The conditional probability P (st , Exploitation|t ,",0,,False
284,Exploitation) is,0,,False
285,calculated,0,,False
286,as,0,,False
287,# of #,0,,False
288,observed true exploitations of observed exploitations,0,,False
289,",",0,,False
290,where,0,,False
291,# of observed exploitations is the number of observed ex-,0,,False
292,ploitations suggesting that the user is likely to exploit the,0,,False
293,"same subtopic (based on Eq. 9), and ""# of observed true ex-",0,,False
294,"ploitations"" is the number of observed exploitations that are",0,,False
295,"judged positive in the ground truth. P (st , Exploration|t ,",0,,False
296,Exploration) is calculated in a similar way.,0,,False
297,4.5 Belief Updates,0,,False
298,"At every search iteration the belief state b is updated twice; once at Step 3, another at Step 8. It reflects the interaction and cooperative game between the two agents. 2The manual annotations for ""exploration"" transitions can be found at www.cs.georgetown.edu/~huiyang/win-win.",0,,False
299,592,0,,False
300,"A belief bt(si) is defined as P (si|at, bt). The initial belief states can be calculated as: b0(si , Sz) , P (si , Sx)P (si ,"" Sy), where x  {R "","" Rel, N R "","" N on-Rel}, y  {R "","" exploRation, T "","" exploiT ation}, z is the cross-product of""",0,,False
301,Table 3: Dataset statistics.,0,,False
302,TREC 2012 TREC 2013,1,TREC,True
303,#Sessions,1,Session,True
304,98,0,,False
305,87,0,,False
306,#Search topics,0,,False
307,48,0,,False
308,49,0,,False
309,"x and y and z  {RR, RT, N RR, N RT }. In addition, 0 ",1,ad,True
310,#Queries,0,,False
311,297,0,,False
312,442,0,,False
313,"b(si)  1 and si b(si) , 1. The belief update function is bt+1(sj) ,"" P (sj|t, at, bt)""",0,,False
314,Avg. session length,0,,False
315,3.03,0,,False
316,Max session length,0,,False
317,11,0,,False
318,Avg. #sessions per topic,0,,False
319,2.04,0,,False
320,5.08 21 1.78,0,,False
321,by taking into account new observations t. It is updated,0,,False
322,from iteration t to iteration t + 1:,0,,False
323,The formula matches well with common search scenarios,0,,False
324,"bt+1(sj ) ,d,e,f,"" P (sj |t, at, bt)""",0,,False
325,"O(sj , at, t) ,",0,,False
326,"siS T (si, at, sj )bt(si)",0,,False
327,"P (t|at, bt)",0,,False
328,where the user makes decisions about their next actions,0,,False
329,based on the most relevant document(s) they examined in,0,,False
330,(10),0,,False
331,the previous run of retrieval. Such a document we call it,0,,False
332,maximum rewarding document(s). We use document with,0,,False
333,"where si and sj are two states, i, j  {RR, RT, N RR, N RT }. t indices the search iterations, and O(sj, at, t) ,"" P (t|sj, at) is calculated based on Section 4.4. P (t|at, bt) is the normalization factor to keep siS b(si) "","" 1. For notation simplicity, we will only use a to represent actions from now""",0,,False
334,the largest P (qt-1|dt-1) as the maximum rewarding docu-,0,,False
335,"ment. P (qt-1|dt-1) is calculated as 1- tqt-1 {1 - P (t|dt-1)},",0,,False
336,where,0,,False
337,P (t|dt-1),0,,False
338,",",0,,False
339,", #(t,dt-1 )",0,,False
340,|dt-1 |,0,,False
341,"#(t, dt-1) is",0,,False
342,the,0,,False
343,number,0,,False
344,of,0,,False
345,"occurrences of term t in document dt-1, and |dt-1| is the",0,,False
346,document length.,0,,False
347,"on. However, it is worthy noting that actions can be both",0,,False
348,By optimizing both long term rewards for the user and,0,,False
349,domain-level actions a and messages .,0,,False
350,"for the search engine, we learn the best policy  and use it",0,,False
351,"Transition probability T (si, at, sj) is defined as P (sj|si, at, bt).",0,,False
352,It,0,,False
353,is,0,,False
354,can,0,,False
355,be,0,,False
356,calculated,0,,False
357,as,0,,False
358,"T (si, at, sj )",0,,False
359,",",0,,False
360,", #T ransition(si,at,sj )",0,,False
361,"#T ransition(si,at,s)",0,,False
362,"where Transition (si, at, sj) is the sum of all transitions that",0,,False
363,"starts at state si, takes action at, and lands at state sj.",0,,False
364,to predict the next action for the search engine. The joint optimization for the dual-agent SG can be represented as:,0,,False
365,ase,0,,False
366,",",0,,False
367,arg max,0,,False
368,a,0,,False
369,"Qse(b, a) + Qu(b, au)",0,,False
370,(14),0,,False
371,"T ransition (si, at, s) is the sum of all transitions that starts at state si and lands at any state by action at.",0,,False
372,"where ase  Ase at t ,"" n and n is the number of search iterations in a session, i.e., the session length.""",0,,False
373,"Finally, equals to P",0,,False
374,"taking (t|sj ,",0,,False
375,"O(sj , at, t) ,"" P (t|sj , at), which at, bt) when we consider beliefs, and T""",0,,False
376,"also (si, at,",0,,False
377,sj ),0,,False
378,"In win-win search, Ase can include many search engine actions. One type of actions is adjusting a query's term weight.",1,ad,True
379,","" P (sj|si, at, bt), the updated belief can be written as:""",0,,False
380,Assuming the query is reformulated from the previous query,0,,False
381,bt+1(sj ),0,,False
382,",",0,,False
383,"P (t|sj , at, bt)",0,,False
384,"siS P (sj |si, at, bt)bt(si) P (t|at, bt)",0,,False
385,"P (t|sj , at, bt) ,",0,,False
386,"siS P (sj |si, at, bt)bt(si)",0,,False
387,"skS P (t|sk, at) siS P (sk|si, at)bt(si)",0,,False
388,(11),0,,False
389,"where bt(si) is P (si|at, bt), whose initial value is b0(si).",0,,False
390,"by adding +q or deleting -q. That is to say, Ase ,"" {increasing, decreasing, or keeping term weights}. The term weights are increased or decreased by multiplying a factor. We also use a range of search techniques/algorithms as action options for the search engine agent. They are reported in Section 6. Based on Eq. 14, the win-win search framework picks the optimal search engine action.""",1,ad,True
391,5. JOINT OPTIMIZATION AND RETRIEVAL 6. EVALUATION,0,,False
392,"After every search iteration, we decide the actions for the search engine agent. We employ Q-learning [18] to find out the optimal action. For all a  Ase, we write the search engine's Q-function, which represents the search engine agent's long term reward, as:",0,,False
393,"We evaluate the proposed framework on TREC 2012 and TREC 2013 Session Tracks [20, 21]. The session logs are collected from users through a search system by the track organizers. The topics, i.e., information need (Table 1), are provided to users. The session logs record all URLs dis-",1,TREC,True
394,"played to the user, snippets, clicks, and dwell time. Table 3",0,,False
395,"Qse(b, a) ,"" (b, a)+""",0,,False
396,P,0,,False
397,"(|b,",0,,False
398,"au,",0,,False
399,"se)P (|b,",0,,False
400,u),0,,False
401,max,0,,False
402,a,0,,False
403,Qse(b,0,,False
404,",",0,,False
405,a),0,,False
406,shows the dataset statistics. The task is to retrieve a ranked,0,,False
407,(12),0,,False
408,"list of 2,000 documents for the last query in a session. Doc-",0,,False
409,"where the reward for a belief state b is (b, a) ,"" sS b(s)R(s, a). P (|b, au, se) corresponds to Eq. 8 and Eq. 9 and P (|b, u)""",0,,False
410,corresponds to Eq. 4 and Eq. 5. b is the belief state up-,0,,False
411,"ument relevance is judged based on the whole-session relevance. We use the official TREC evaluation metrics in our experiments. They include nDCG@10, nERR@10, nDCG,",1,TREC,True
412,dated by Eq. 11.,0,,False
413,and MAP [21]. The ground truth relevant documents are,1,MAP,True
414,"In win-win search, we take into account both the search",0,,False
415,provided by TREC.,1,TREC,True
416,"engine reward and the user reward. As in [11], we have Qu calculated as the long term reward for the user agent:",0,,False
417,"The corpora used in our experiments are ClueWeb09 CatB (50 million English web pages crawled in 2009, used in TREC",1,corpora,True
418,"2012), and ClueWeb12 CatB (50 million English web pages",1,ClueWeb,True
419,"Qu(b, au) ,"" R(s, au) +  au T (st|st-1, Dt-1) maxst-1 Qu(st-1, au) crawled in 2012, used in TREC 2013). Documents with the""",1,TREC,True
420,","" P (qt|d) +  a P (qt|qt-1, Dt-1, a) maxDt-1 P (qt-1|Dt-1)""",0,,False
421,Waterloo spam scores [5] less than 70 are filtered out. All,0,,False
422,(13),0,,False
423,duplicated documents are removed.,0,,False
424,which recursively calculates the reward starting from q1 and continues with the policy until qt. P (qt|d) is the current reward that the user gains through reading the documents.,1,ad,True
425,"We compare our system with the following systems: Lemur [23] (language modeling + Dirichlet smoothing), PRF (Pseudo Relevance Feedback in Lemur assuming the top 20 docu-",0,,False
426,maxDt-1 P (qt-1|Dt-1) is the maximum of the past rewards.,0,,False
427,"ments are relevant), Rocchio (relevance feedback that as-",0,,False
428,593,0,,False
429,Table 4: Search accuracy on TREC 2012 Session (,1,TREC,True
430,indicates a statistical significant improvement over,0,,False
431,"Rocchio at p < 0.05 (t-test, one-sided));  indicates a",0,,False
432,statistical significant improvement over QCM+DUP,0,,False
433,"at p < 0.05 (t-test, one-sided)).",0,,False
434,Approach,0,,False
435,nDCG@10 nDCG MAP nERR@10,1,MAP,True
436,Lemur,0,,False
437,0.2474 0.2627 0.1274 0.2857,0,,False
438,TREC median 0.2608 0.2468 0.1440 0.2626,1,TREC,True
439,TREC best,1,TREC,True
440,0.3221 0.2865 0.1559 0.3595,0,,False
441,PRF,0,,False
442,0.2074 0.2335 0.1065 0.2415,0,,False
443,Rocchio,0,,False
444,0.2446 0.2714 0.1281 0.2950,0,,False
445,Rocchio-CLK,0,,False
446,0.2916 0.2866 0.1449,0,,False
447,0.3366,0,,False
448,Rocchio-SAT,0,,False
449,0.2889 0.2836 0.1467 0.3254,0,,False
450,QCM+DUP,0,,False
451,0.2742 0.2560 0.1537 0.3221,0,,False
452,QCM SAT,0,,False
453,0.3350 0.3054 0.1534 0.1534,0,,False
454,Win-Win,0,,False
455,0.2941 0.2691 0.1346 0.3403,0,,False
456,Table 5: Search accuracy on TREC 2013 Session (,1,TREC,True
457,indicates a statistical significant improvement over,0,,False
458,"Rocchio at p < 0.01 (t-test, one-sided));  indicates a",0,,False
459,statistical significant improvement over QCM+DUP,0,,False
460,"at p < 0.01 (t-test, one-sided)).",0,,False
461,Approach,0,,False
462,nDCG@10 nDCG MAP nERR@10,1,MAP,True
463,Lemur,0,,False
464,0.1147,0,,False
465,0.1758 0.0926,0,,False
466,0.1314,0,,False
467,TREC median 0.1531,1,TREC,True
468,­,0,,False
469,­,0,,False
470,­,0,,False
471,TREC best,1,TREC,True
472,0.1952,0,,False
473,­,0,,False
474,­,0,,False
475,­,0,,False
476,PRF,0,,False
477,0.1061,0,,False
478,0.1701 0.0787,0,,False
479,0.1245,0,,False
480,Rocchio,0,,False
481,0.1320,0,,False
482,0.1924 0.1060,0,,False
483,0.1549,0,,False
484,Rocchio-CLK,0,,False
485,0.1315,0,,False
486,0.1929 0.1060,0,,False
487,0.1546,0,,False
488,Rocchio-SAT,0,,False
489,0.1147,0,,False
490,0.1758 0.0926,0,,False
491,0.1314,0,,False
492,QCM+DUP,0,,False
493,0.1316,0,,False
494,0.1929 0.1060,0,,False
495,0.1547,0,,False
496,QCM SAT,0,,False
497,0.1186,0,,False
498,0.1754 0.0939,0,,False
499,0.1425,0,,False
500,Win-Win,0,,False
501,0.2026 0.2609 0.1290 0.2328,0,,False
502,"sumes the top 10 previous retrieved documents are relevant), Rocchio-CLK (implicit relevance feedback that assumes only previous clicked documents are relevant), Rocchio-SAT (implicit relevance feedback that assumes only previous SATclicked documents are relevant), QCM+DUP (the QCM approach proposed by [11]), and QCM SAT (a variation of QCM by [33]). We choose Rocchio (a state-of-the-art interactive search algorithm) and QCM+DUP (a state-of-the-art session search algorithm) as two baseline systems and all other systems are compared against them. TREC median and TREC best scores are also included for reference. Note that TREC best are an aggregation from the best scores of each individual submitted TREC runs; it is not a single search system.",1,TREC,True
503,6.1 Search Accuracy,0,,False
504,"Our run, win-win, implements six retrieval technologies. They are: (1) increasing weights of the added terms (+q) by a factor of x,""{1.05, 1.10, 1.15, 1.20, 1.25, 1.5, 1.75 or 2}; (2) decreasing weights of the added terms by a factor of y "",""{ 0.5, 0.57, 0.67, 0.8, 0.83, 0.87, 0.9 or 0.95}; (3) the term weighting scheme proposed in [11] with parameters , , ,  set as 2.2, 1.8, 0.4, 0.92; (4) a PRF (Pseudo Relevance Feedback) algorithm which assumes the top p retrieved documents are relevant while p ranges from 1 to 20; (5) an adhoc variation of win-win, which directly uses the last query in a session to perform retrieval; and (6) a brute-force variation of win-win, which combines all queries in a session, extracts all unique query terms from them, and weights them equally. Win-win examine 21 search engine action options in total to""",1,ad,True
505,"find out the optimal action that maximizes the joint long term reward Qse(b, a) + Qu(b, au) for both agents.",0,,False
506,"Table 4 shows the search accuracy of all systems under comparison for TREC 2012 Session Track. We can see that win-win search is better than most systems except QCM SAT. It statistically significantly outperforms Rocchio by 20%, Lemur by 18.9%, and PRF by 41.8% in nDCG@10 (p-value<.05, one-side t-test). It also outperforms RocchioCLK, Rocchio-SAT and QCM+DUP, but the results are not statistically significant. The trends for other evaluation metrics are similar to nDCG@10.",1,TREC,True
507,"Table 5 shows the search accuracy of all systems for TREC 2013 Session Track. Since we only indexed ClueWeb12 CatB, after spam reduction, many relevant CatA documents are not included in the CatB collection. To evaluate the systems fairly, we created a filtered ground truth which only consists of relevant documents in CatB. The results are shown in Table 5. We can see that win-win is the best run among all systems. It shows statistically significant gain (p-value<.01, one-sided t-test) over all other systems across all evaluation metrics. Particularly, the proposed approach achieves a significant 54% improvement of nDCG@10 comparing to QCM+DUP. The experimental results support that our approach is highly effective.",1,TREC,True
508,6.2 Immediate Search Accuracy,0,,False
509,"TREC Session tasks request for retrieval results for the last query in a session. Theoretically, however, win-win search can optimize at every search iteration throughout a session. We hence compare our approach (the Win-Win run) with the top returned documents provided by TREC (the Original run) in terms of immediate search accuracy. We define immediate search accuracy at i as an evaluation score that measures search accuracy at search iteration i. The evaluation scores used are nDCG@10 and nERR@10.",1,TREC,True
510,"We report the averaged immediate search accuracy for all sessions. It is worthy noting that session lengths vary. To average across sessions with different lengths, we make all sessions equals to the maximum session length in a dataset. TREC 2012 and 2013 Session have different maximum session lengths; they are 11 and 21, respectively. When a session is shorter than the maximum session length, we use the retrieval results from its own last iteration as the retrieval results for iterations beyond its own last iteration. In addition, since TREC did not provide any retrieval results for the last query, the Original runs has no value at the last iteration.",1,TREC,True
511,"Figures 4 and 5 plot the immediate search accuracy for TREC 2012 & 2013 Session Tracks averaged over all sessions. We observe that win-win search's immediate search accuracy is statistically significantly better than the Original run at every iteration. In Figure 4, win-win outperforms Original since iteration 2 in nDCG@10 and outperforms it since iteration 3 in nERR@10. At the last iteration, winwin outperforms Original by a statistically significant 27.1% in nDCG@10 (p-value<.05, one-sided t-test). We observe similar trends in Figure 5. Another interesting finding is that win-win search's immediate search accuracy increases while the number of search iterations increases. In Figure 4, the nDCG@10 starts at 0.2145 at the first iteration and increases dramatically 37.1% to 0.2941 at the last iteration. It suggests that by involving more search iterations, i.e., learning from more interactions between the user and the search",1,TREC,True
512,594,0,,False
513,Figure 4: TREC 2012 Im- Figure 5: TREC 2013 Immediate Search Accuracy. mediate Search Accuracy.,1,TREC,True
514,Figure 7: Factual and Specific sessions.,0,,False
515,Figure 8: Factual and Amorphous sessions.,0,,False
516,"Figure 6: Long sessions (length >,"" 4). Transition probabilities are listed with actions: Add (A), Remove (R), and Keep (K).""",0,,False
517,"engine, win-win is able to monotonically improve its search accuracy.",0,,False
518,6.3 State Transitions,0,,False
519,This experiment investigates how legitimate the proposed states are in presenting the hidden mental states of users.,0,,False
520,"First, we use examples to demonstrate the state transitions in sessions. Session 87 (Table 1) is a long session with 21 queries. The chain of decision states identified for this session based on techniques presented in Sections 4.2 and 4.4 is: SNRR(q1,best us destination)  SRT (q2,distance new york boston)  SNRT  SNRR  SNRR  SRR  SRR  SNRR  SRT  SRT  SRR(q11,boston tourism)  SNRR(q12,nyc tourism)  SNRR(q13,philadelphia nyc distance)  SNRR  SRT  SNRR  SRR  SNRT  SNRT (q19,philadelphia nyc travel)  SNRT (q20,philadelphia nyc train)  SNRT (q21 ,""philadelphia nyc bus). Our states correctly suggests that the user is in the exploration states (RR, NRR, NRR) from q11 to q13, while he keeps changing queries to explore from city to city (boston, new york city, and philadelphia). The user eventually finds the cities, philadelphia and nyc, that fulfill the information need ­ """"best US destinations within a 150-mile radius"""". During the last 3 queries, the user exploits the current subtopic (philadelphia and nyc) to find out more specifics on transportations (travel, train, bus) about them. Our system correctly recognizes the last three states as exploitation states (NRT, NRT, NRT). This example suggests that the proposed states are able to reflect the real user decision states quite accurately.""",1,Session,True
521,"Second, we examine state transition patterns in long sessions since they contain enough transitions for us to study. Figure 6 plots state probabilities, state transition probabilities, and that under different user actions for long sessions (sessions with 4 or more queries). The data are combined from both TREC 2012 & 2013 Session Tracks. We notice that NRR (non-relevant and exploration) is the most com-",1,TREC,True
522,"mon state (42.4%). This reflects that a user spend may a long time to explore while receiving non-relevant documents. On the contrary, the RR state (relevant and exploration) is the least common state (11.3%).Moreover, we see that state transitions are not uniformly distributed. For instance, the transition from NRT to both relevant states (RT and RR) are very rare (in total 5.65%). In addition, we notice that actions are related to state transition probabilities. There are 90.8% transitions generated by adding terms and among all the transitions with removing terms, 84.8% of them lead to exploitation states (RT or NRT).",1,ad,True
523,"Third, we find that state probability distribution and state probability transitions differ among different session types. We plot the state probabilities and transition probabilities in Figures 7 to 10 for four different TREC session types, which were created along two aspects: search target (factual or intellectual ) and goal quality (specific or amorphous). Suggested by [24], the difficulty levels of the session types usually are FactualSpecific < IntellectualSpecific < FactualAmorphous < IntellectualAmorphous. An interesting finding is that as the session difficult level increases, the transition probability from state NRT (non-relevant and exploitation) to state RT (relevant and exploitation) becomes lower: FactualSpecific (0.25), IntellectualSpecific (0.2), FactualAmorphous (0.12), IntellectualAmorphous(0.1). It suggests that the harder the task is, the greater the necessity to explore rather than to exploit, when the user is not satisfied with the current retrieval results. In addition, we observe that Intellectual sessions (Figures 9 and 10) have a larger probability, 0.1548, to be in the RR (relevant and exploration) state than the other session types (on average 0.1018).",1,TREC,True
524,7. CONCLUSION,0,,False
525,"This paper presents a novel session search framework, winwin search, that uses a dual-agent stochastic game to model the interactions between user and search engine. With a careful design of states, actions, and observations, the new framework is able to perform efficient optimization over a finite discrete set of options. The experiments on TREC Ses-",1,TREC,True
526,595,0,,False
527,Figure 9: Intellectual and Specific sessions.,0,,False
528,Figure 10: Intellectual and Amorphous sessions.,0,,False
529,sion 2012 and 2013 datasets show that the proposed framework is highly effective for session search.,0,,False
530,"Session search is a complex IR task. The complexity comes from the involvement of many more factors other than just terms, queries and documents in most existing retrieval algorithms. The factors include query reformulations, clicks, time spent to examine the documents, personalization, query intent, feedback, etc. Most existing work on sessions and task-based search focuses on diving into one aspect. Through significantly simplifying the factors, we realize the integration of all the factors in a unified framework. For example, we simplify users' decision states into only four states, and discretize user actions and search engine actions into a finite number of options. Such simplification is necessary in creating practical search systems.",1,Session,True
531,"This paper views the search engine as an autonomous agent, that works together with user, another autonomous agent, to collaborate on a shared task ­ fulfilling the information needs. This view assumes that the search engine is more like a ""decision engine"". Session search can be imagined as two agents exploring in a world full of information, searching for the goal in a trial-and-error manner. Here we assume a cooperative game between the two agents. However, as we mentioned in the introduction, the search engine agent can of course choose a different goal. It will be very interesting to see how to still satisfy the user to achieve winwin. We hope our work calls for future adventures in the fields of POMDP in IR and game theory in IR.",1,Session,True
532,8. ACKNOWLEDGMENT,0,,False
533,"This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.",0,,False
534,9. REFERENCES,0,,False
535,"[1] L. E. Baum and T. Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6), 1966.",0,,False
536,"[2] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good expansion terms for pseudo-relevance feedback. In SIGIR '08.",0,,False
537,"[3] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In CIKM '11.",0,,False
538,"[4] M.-A. Cartright, R. W. White, and E. Horvitz. Intentions and attention in exploratory health search. In SIGIR '11.",0,,False
539,"[5] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5), Oct. 2011.",0,,False
540,"[6] A. Diriye, R. White, G. Buscher, and S. Dumais. Leaving so soon?: Understanding and predicting web search abandonment rationales. In CIKM '12.",0,,False
541,"[7] C. Eickhoff, K. Collins-Thompson, P. N. Bennett, and S. Dumais. Personalizing atypical web search sessions. In WSDM '13.",0,,False
542,[8] H. Feild and J. Allan. Task-aware query recommendation. In SIGIR '13.,0,,False
543,"[9] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White. Evaluating implicit measures to improve web search. ACM Trans. Inf. Syst., 23(2), Apr. 2005.",0,,False
544,"[10] D. Guan, H. Yang, and N. Goharian. Effective structured query formulation for session search. In TREC '12.",1,TREC,True
545,"[11] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In SIGIR '13.",0,,False
546,[12] Q. Guo and E. Agichtein. Ready to buy or just browsing?: detecting web searcher goals from interaction data. In SIGIR '10.,1,ad,True
547,[13] J. Jiang and D. He. Different effects of click-through and past queries on whole-session search performance. In TREC '13.,1,TREC,True
548,"[14] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track. In TREC '12.",1,trec,True
549,"[15] X. Jin, M. Sloan, and J. Wang. Interactive exploratory search for multi page search results. In WWW '13.",0,,False
550,[16] T. Joachims. A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. 1997.,0,,False
551,[17] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM '08.,0,,False
552,"[18] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1):99­134, 1998.",0,,False
553,"[19] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: a survey. J. Artif. Int. Res., 4(1), May 1996.",0,,False
554,"[20] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2012 session track. In TREC'12.",1,trec,True
555,"[21] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2013 session track. In TREC'13.",1,trec,True
556,"[22] A. Kotov, P. N. Bennett, R. W. White, S. T. Dumais, and J. Teevan. Modeling and analysis of cross-session search tasks. In SIGIR '11.",0,,False
557,[23] Lemur Search Engine. http://www.lemurproject.org/. [24] J. Liu and N. J. Belkin. Personalizing information retrieval for,0,,False
558,multi-session tasks: The roles of task stage and task type. In SIGIR '10.,0,,False
559,"[25] K. Raman, P. N. Bennett, and K. Collins-Thompson. Toward whole-session relevance: Exploring intrinsic diversity in web search. In SIGIR '13.",0,,False
560,"[26] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for personalized search. In CIKM '05.",0,,False
561,[27] Y. Song and L.-w. He. Optimal rare query suggestion with implicit user feedback. In WWW '10.,0,,False
562,"[28] A. R. Taylor, C. Cool, N. J. Belkin, and W. J. Amadio. Relationships between categories of relevance criteria and stage in task completion. Information Processing & Management, 43(4), 2007.",1,ad,True
563,"[29] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize or not to personalize: Modeling queries with variation in user intent. In SIGIR '08.",0,,False
564,"[30] H. Wang, Y. Song, M.-W. Chang, X. He, R. W. White, and W. Chu. Learning to extract cross-session search tasks. In WWW '13.",0,,False
565,"[31] R. W. White, I. Ruthven, J. M. Jose, and C. J. V. Rijsbergen. Evaluating implicit feedback models using searcher simulations. ACM Trans. Inf. Syst., 23(3), July 2005.",0,,False
566,[32] S. Yuan and J. Wang. Sequential selection of correlated ads by pomdps. In CIKM '12.,1,ad,True
567,"[33] S. Zhang, D. Guan, and H. Yang. Query change as relevance feedback in session search. In SIGIR '13.",1,Query,True
568,596,0,,False
569,,0,,False

,sentence,label,data,regex
0,"On Run Diversity in ""Evaluation as a Service""",0,,False
1,"Ellen M. Voorhees,1 Jimmy Lin,2 and Miles Efron3",0,,False
2,"1 National Institute of Standards and Technology 2 The iSchool, University of Maryland, College Park 3 Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign",1,ad,True
3,"ellen.voorhees@nist.gov, jimmylin@umd.edu, mefron@illinois.edu",0,,False
4,ABSTRACT,0,,False
5,"Evaluation as a service (EaaS) is a new methodology that enables community-wide evaluations and the construction of test collections on documents that cannot be distributed. The basic idea is that evaluation organizers provide a service API through which the evaluation task can be completed. However, this concept violates some of the premises of traditional pool-based collection building and thus calls into question the quality of the resulting test collection. In particular, the service API might restrict the diversity of runs that contribute to the pool: this might hamper innovation by researchers and lead to incomplete judgment pools that affect the reusability of the collection. This paper shows that the distinctiveness of the retrieval runs used to construct the first test collection built using EaaS, the TREC 2013 Microblog collection, is not substantially different from that of the TREC-8 ad hoc collection, a high-quality collection built using traditional pooling. Further analysis using the `leave out uniques' test suggests that pools from the Microblog 2013 collection are less complete than those from TREC-8, although both collections benefit from the presence of distinctive and effective manual runs. Although we cannot yet generalize to all EaaS implementations, our analyses reveal no obvious flaws in the test collection built using the methodology in the TREC 2013 Microblog track.",1,AP,True
6,Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation,0,,False
7,Keywords: test collections; reusability; meta-evaluation,0,,False
8,1. INTRODUCTION,1,DUC,True
9,"Large-scale community-wide evaluations such as TREC operationalize the Cranfield paradigm by building retrieval test collections that support IR research [2]. A test collection consists of a corpus of documents, a set of information needs called topics, and relevance judgments that specify which documents are relevant for which topics. A funda-",1,TREC,True
10,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609484.",1,ad,True
11,"mental assumption in the paradigm is that researchers can acquire a test collection's document corpus. But what if this is not possible? This was the challenge in the TREC Microblog track evaluation, where the corpus is comprised of tweets posted on Twitter. Since the company's terms of service forbid redistribution of tweets, a test collection built on tweets cannot be distributed in a traditional manner.",1,ad,True
12,"The solution developed by the track organizers, termed ""evaluation as a service"" (EaaS), was to provide an API through which the tweet collection can be accessed for completing the evaluation task [4, 3]. This API, consisting of a basic keyword search interface, was the only method through which track participants could access the collection. The operational deployment of the EaaS model was successful in attracting participation at TREC 2013, which provides encouraging evidence that EaaS could be generalized to other evaluation scenarios involving sensitive data (e.g., medical or desktop search). Before expanding the use of the approach, however, we need to know whether EaaS has any substantive impact on the quality of the collections produced.",1,AP,True
13,"One concern about the EaaS model is a potential lack of diversity in the retrieval runs used to build the test collection. The criticism is this: if everyone must use the API, won't they all end up doing basically the same thing? Diversity is important from at least two perspectives. Previous work has tied the reusability of a collection built using pooling to the diversity of the retrieval runs that form the pools [1]. Diversity in participant submissions also serves as a proxy indicator that researchers are trying different (novel) retrieval techniques. The contribution of this paper is an analysis of the diversity of the pools in the TREC 2013 Microblog track (using EaaS) compared to previous evaluations--including the first two years of the Microblog track in TRECs 2011 and 2012--where participants had access to the entire corpus. We first propose a novel metric, called Run Average Overlap (RAO), that measures the overall distinctiveness of one retrieval run with respect to a given set of runs. In a second analysis, we apply another test of reusability, the leave out uniques (LOU) test. Analyses show no obvious differences in the characteristics of the pools, suggesting that the EaaS model can be used to build reusable collections.",1,AP,True
14,2. BUILDING POOLED COLLECTIONS,0,,False
15,Pooling [8] is a method for building test collections when the target document corpus is too large to make exhaustive relevance judgments for every document for every topic. A pooled collection is built from a set of retrieval runs sub-,0,,False
16,959,0,,False
17,Table 1: Test collection statistics.,0,,False
18,Name Docs Topics Depth Runs Groups,0,,False
19,TREC-8 528 K,1,TREC,True
20,50 100 71,0,,False
21,40,0,,False
22,MB2012 16 M,0,,False
23,60 100 121,0,,False
24,33,0,,False
25,MB2013 243 M,0,,False
26,60,0,,False
27,90 71,0,,False
28,20,0,,False
29,"mitted by participants of the process, typically as part of a community-wide evaluation such as TREC.1 A run is the output of a retrieval system for each topic in the collection; documents are assumed to be ordered by decreasing likelihood of relevance to the topic. The pool for a topic is comprised of the union of the top K (called the pool depth) documents retrieved for that topic by each run. Only documents in the pool are judged for relevance by a human; all other documents are assumed to be not relevant.",1,TREC,True
30,"Test collections are research tools that are most useful when they are reusable, that is, when they fairly evaluate retrieval systems that did not contribute to their construction [1]. Thus, a reusable test collection can be used to evaluate systems even after the original evaluation that created the collection has concluded. For a pooled collection to be reusable, the pool must contain enough of the relevant documents in the collection to be an unbiased sample of the relevant documents. In practice, the completeness of a pool is a function of (at least) the pool depth, the effectiveness of the runs that comprise it, the diversity of the runs, and the true number of relevant documents [10, 9].",0,,False
31,"Many of the test collections built in TREC, including the ad hoc collections and the Microblog collections, were built through pooling. The TREC ad hoc collections, especially the TREC-8 collection, have been subject to a number of analyses and have been used in hundreds of retrieval experiments whose outcomes have proven to generalize. Thus, these collections are generally accepted as high-quality test collections [5]. Our basic premise is that the TREC 2013 Microblog collection is reusable if it displays characteristics similar to the TREC ad hoc collections. The particular characteristics of interest are the distinctiveness of the runs over which the pools are formed and the number of relevant documents found by a single participant.",1,TREC,True
32,"More details about the three collections used in this paper are given in Table 1. (Our experiments also considered the ad hoc test collections from TREC-6 and TREC-7, but they yielded similar results, and so for brevity we only present results from TREC-8 here.) The first column of the table gives the name of the collection as used in the remainder of this paper. The corpus for TREC-8 consists of approximately half a million (mostly) newswire articles. The pools were predominantly constructed from the 71 ad hoc runs that were judged (from a total of 129 submitted runs), but runs from some other tracks contributed small numbers of documents to the pools as well.2 The other two collections were built as part of the TREC Microblog tracks in 2012 and 2013, whose document corpora consist of tweets posted to Twitter. The document corpus used for the MB2012 track (called Tweets11) contains approximately 16 million tweets, which were distributed by publishing a list of tweet ids that participants then fetched [7, 6]. The EaaS model enabled the MB2013 corpus to be much larger, about 243 million",1,ad,True
33,1http://trec.nist.gov 2Values reported in Table 1 include only the ad hoc runs.,1,trec,True
34,"tweets [4]. Relevance judgments for the MB collections were made on a 3-point scale (""not relevant"", ""relevant"", ""highly relevant""), but in this work we ignore the different degrees of relevance and use both higher grades as ""relevant"".",1,ad,True
35,"The final column of Table 1 gives the number of participants that contributed runs. TREC generally allows participating groups to submit more than a single run to a given task. In practice, runs submitted by the same participant tend to be much more similar to each other than they are to runs from other participants. Analyses of run diversity need to account for this phenomenon.",1,TREC,True
36,3. RUN AVERAGE OVERLAP,1,AP,True
37,"Our biggest concern regarding the EaaS model is whether the common API imposed on participants restricts their retrieval approaches to the extent that different participants are compelled to submit essentially similar runs. In order to answer this question, we introduce a novel metric, Run Average Overlap (RAO), which is a measure of the tendency of a set of runs to retrieve documents in common.",1,AP,True
38,"Run Average Overlap is computed for a given run with respect to a specific set of runs. In what follows we compute the RAO score for each run in a pool set with respect to the pool set. Say that a run O retrieves document d for a topic t if d occurs in the top K ranks for t in O. When K is set equal to the pool depth, as done here, O retrieves d if it contributes d to the pool. Let P be the total number of participants that have runs in the run set, and Pd be the number of distinct participants that retrieve d. Finally, let T be the total number of topics and n be the number of documents retrieved by O for t. Then:",0,,False
39,"1X1X 1 RAO(O) ,",0,,False
40,T t n d Pd,0,,False
41,"In words, the RAO score for a run is the mean over all topics",0,,False
42,"of the score computed for individual topics, where the per-",0,,False
43,topic score is the mean over the documents retrieved by O of,0,,False
44,the reciprocal of the number of participants (including itself),0,,False
45,that retrieve that document. The greater the RAO score the,0,,False
46,more distinctive the result set. The minimum RAO score is,0,,False
47,1 P,0,,False
48,which signifies that O retrieved only documents that all,0,,False
49,"other participants also retrieved. The maximum score is 1.0,",0,,False
50,which means that no other participant retrieved any docu-,0,,False
51,ment that O retrieved. Note that document distinctiveness,0,,False
52,for the purposes of computing RAO is computed over partic-,0,,False
53,"ipants, and thus the number of runs submitted by any given",0,,False
54,participant is not an issue.,0,,False
55,While RAO scores close to,0,,False
56,1 P,0,,False
57,for all runs in a pool set,0,,False
58,"indicate that the runs are not diverse, the presence of runs",0,,False
59,with high scores is not sufficient to conclude that the pool,0,,False
60,is appropriately diverse. The RAO score is computed over,0,,False
61,"all retrieved documents, not relevant documents. Distinc-",0,,False
62,"tive but ineffective runs are easy to produce (for example,",0,,False
63,by selecting random documents from the collection) but un-,0,,False
64,"helpful for pool building. Thus, the RAO score must be",0,,False
65,used in conjunction with an effectiveness score to ensure,0,,False
66,the presence of distinctive and effective runs in the pool set.,0,,False
67,We use R-precision as the effectiveness measure here since it,0,,False
68,was the primary metric for the TREC 2013 Microblog track.,1,TREC,True
69,Figure 1 shows plots of RAO vs. R-precision for the three,0,,False
70,collections. Each point in a graph represents one pool run.,0,,False
71,Manual runs (runs for which there was some human pro-,0,,False
72,cessing in the construction of the run) are plotted as open,0,,False
73,960,0,,False
74,TREC-8 1,1,TREC,True
75,MB2012 1,0,,False
76,MB2013 1,0,,False
77,0.8,0,,False
78,0.8,0,,False
79,0.8,0,,False
80,0.6,0,,False
81,0.6,0,,False
82,0.6,0,,False
83,RAO RAO RAO,0,,False
84,0.4,0,,False
85,0.4,0,,False
86,0.4,0,,False
87,0.2,0,,False
88,0.2,0,,False
89,0.2,0,,False
90,0,0,,False
91,0,0,,False
92,0.2 0.4 0.6 0.8,0,,False
93,1,0,,False
94,R-Precision,0,,False
95,0,0,,False
96,0,0,,False
97,0.2 0.4 0.6 0.8,0,,False
98,1,0,,False
99,R-Precision,0,,False
100,0,0,,False
101,0,0,,False
102,0.2 0.4 0.6 0.8,0,,False
103,1,0,,False
104,R-Precision,0,,False
105,Figure 1: Run Average Overlap (y-axis) vs. mean R-precision (x-axis) for pool runs. Manual runs are plotted as open squares and automatic runs as filled circles.,0,,False
106,"squares while automatic (i.e., non-manual) runs are plotted as filled circles. A highly distinctive and highly effective run would fall in the upper right-hand corner of the graph.",0,,False
107,"The plots for the MB2013 and the TREC-8 collections are very similar. These collections exhibit a general inverse relationship between effectiveness and distinctiveness (points run roughly from upper left to lower right). This is simply a manifestation of the fact that there are many fewer relevant documents than non-relevant documents, and thus many fewer effective retrieved sets than ineffective retrieved sets. But, importantly, we see that both the TREC-8 and MB2013 collections contain outlier runs that are both distinctive and effective--these represent high-quality manual runs that contributed to the pools.",1,TREC,True
108,"The graph for the MB2012 collection, which was not constructed using the EaaS model, is different from the others. The pool runs for MB2012 have a much narrower range of effectiveness and have a much greater RAO score on average. Note, however, that this is the one collection that does not have any runs that are outlier effective runs, and overall effectiveness is relatively poor. Thus, the large RAO scores are probably another manifestation of the tendency for ineffective runs to be different.",0,,False
109,4. UNIQUELY RETRIEVED RELEVANT,0,,False
110,"In our second analysis, we examined uniquely retrieved relevant documents, which are relevant documents that were contributed to the pool by only a single participant. The leave out uniques test (LOU) [1] gauges the reusability of a test collection by examining the effect on evaluation scores of a participant's runs when the participant's uniquely retrieved relevant documents are removed from the judgment set. The test computes the evaluation scores that would have resulted had the participant not been involved in the collection building process.",1,ad,True
111,"Figure 2 plots the results of the LOU test for each of the three collections. Each run is plotted as a point, with manual runs shown as empty boxes and automatic runs as filled circles. The x-axis is the MAP score of the run as computed using the official relevance judgment set for that collection, and the y-axis is the MAP score as computed using the judgment set with the participant's unique relevants removed. When both scores are the same, the point falls on the diagonal line. A run whose MAP score decreases",1,MAP,True
112,Table 2: Collection characteristics on uniquely retrieved relevant documents and LOU test results.,0,,False
113,% of total relevant that are unique max % of total uniques retrieved by a single participant,0,,False
114,TREC-8 24.0,1,TREC,True
115,42.2,0,,False
116,MB2012 30.7,0,,False
117,19.7,0,,False
118,MB2013 39.6,0,,False
119,43.2,0,,False
120,mean % diff in LOU scores (auto runs only) max % diff in LOU scores (auto runs only) # runs with diff in scores > 1% (auto only) / total,0,,False
121,0.14 0.84 0/54,0,,False
122,0.64 8.36 16/112,0,,False
123,1.12 7.95 15/57,0,,False
124,"when unique relevants are removed falls below the line, and a run whose MAP score improves when unique relevants are removed lies above the line.",1,MAP,True
125,"The presence of distinctive and effective manual runs in the TREC-8 and MB2013 collections is obvious in the LOU test results. These runs fall well below the line of equal scores, and are the only runs to do so. The degradation in MAP scores is not surprising given that the participants who submitted these runs contributed large numbers of unique relevant documents to the pools. As shown in the second row of Table 2, for these two collections the participant who submitted the most effective manual runs contributed more than 40% of the unique relevant documents. In contrast, the largest percentage of unique relevant documents contributed by a single participant in the MB2012 collection is just 20%. The percentage of total relevant documents that are uniquely retrieved relevants is given in the first row of the table. Here the TREC-8 and MB2013 collections differ: the MB2013 collection has a much larger percentage of total relevant documents that are unique.",1,TREC,True
126,"A large percentage of total relevant documents that are uniquely retrieved relevant documents is one indication that a test collection may be less reusable, and the LOU test results are consistent with this observation. In Figure 2, all of the automatic runs are on the line of equal scores for the TREC-8 collection, but there is more movement away from the line for the two Microblog collections. The final three rows of Table 2 quantify this movement. The table gives statistics regarding the distribution of the percentage difference in MAP scores in the LOU test when considering",1,TREC,True
127,961,0,,False
128,TREC-8 1,1,TREC,True
129,MB2012 1,0,,False
130,MB2013 1,0,,False
131,0.8,0,,False
132,0.8,0,,False
133,0.8,0,,False
134,LOU MAP LOU MAP LOU MAP,1,MAP,True
135,0.6,0,,False
136,0.6,0,,False
137,0.6,0,,False
138,0.4,0,,False
139,0.4,0,,False
140,0.4,0,,False
141,0.2,0,,False
142,0.2,0,,False
143,0.2,0,,False
144,0,0,,False
145,0,0,,False
146,0.2 0.4 0.6 0.8,0,,False
147,1,0,,False
148,Original MAP,1,MAP,True
149,0,0,,False
150,0,0,,False
151,0.2 0.4 0.6 0.8,0,,False
152,1,0,,False
153,Original MAP,1,MAP,True
154,0,0,,False
155,0,0,,False
156,0.2 0.4 0.6 0.8,0,,False
157,1,0,,False
158,Original MAP,1,MAP,True
159,"Figure 2: Results of ""leave out uniques"" test. Original MAP scores are plotted on the x-axis and LOU MAP scores on the y-axis. Manual runs are plotted as open squares and automatic runs as filled circles.",1,MAP,True
160,"only automatic runs whose original MAP scores were at least 0.1 (because percentage differences are artificially magnified for ineffective runs). The statistics computed are the mean of the percentage difference over these automatic runs, the maximum percentage difference observed in the automatic runs, and the count of the number of runs that have a percentage difference greater than 1% (out of the total). No TREC-8 automatic run has a percentage difference greater than 1%, and since differences that small are within the level of evaluation noise [9], the collection is regarded as highly reusable. The MB2013 collection has a mean percentage difference of about 1%, with 15 runs (out of 57) having at least a 1% difference and a maximum difference of about 8%.",1,MAP,True
161,"Note that this amount of change in LOU test results is not a consequence of using the EaaS model. In fact, the large percentage of relevant documents that are unique is actually confirmation that the evaluation method accommodated distinctive runs (i.e., the API did not hamper researchers' ability to generate runs that are both effective and distinctive), and the MB2012 collection, which was not built using EaaS, has similar characteristics. The likely explanation is the size of pools relative to the size of the corpus [1]. Corpora used in the two Microblog collections are much larger than the TREC-8 collection, so the judgment pools represent a much smaller percentage of the collection than for the TREC-8 collection. Also note that this level of change is unlikely to have much impact in the utility of the collection as a research tool. As can be seen in Figure 2, the relative ordering of runs is largely stable even in the presence of MAP score differences. At worst, researchers who encounter many unjudged tweets in top ranks in new runs should be more cautious in their conclusions.",1,AP,True
162,5. CONCLUSION,0,,False
163,"The evaluation as a service model for constructing retrieval test collections can offer significant advantages over traditional construction techniques, but only if it does not fundamentally hamper innovation and it leads to highquality resources. This paper examined the one collection that has been built using the EaaS model to date, the TREC 2013 Microblog collection, and found its run diversity to be similar to the high-quality TREC-8 ad hoc collection. The results of the leave out uniques test suggest that pools from the Microblog 2013 collection are less complete than pools",1,ad,True
164,"from TREC-8, but both collections strongly benefit from the presence of distinctive and effective manual runs.",1,TREC,True
165,"Of course, these findings are for a single collection and a specific API. This implementation provides a generous number of documents in response to a query and a generous allotment of queries per participant. These choices contribute to good design, as a more restrictive API would likely have impacted submitted runs and the resulting test collection negatively. Although we cannot yet make statements about the EaaS model in general, the TREC 2013 Microblog collection does offer an existence proof that a high-quality retrieval test collection can be constructed using this new method.",1,AP,True
166,6. ACKNOWLEDGMENTS,0,,False
167,"This work has been supported in part by NSF under awards IIS-1217279 and IIS-1218043. Any opinions, findings, conclusions, or recommendations expressed are the authors' and do not necessarily reflect those of the sponsor.",0,,False
168,7. REFERENCES,0,,False
169,"[1] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Information Retrieval, 10:491­508, 2007.",0,,False
170,"[2] D. Harman. Information Retrieval Evaluation. Morgan & Claypool Publishers, 2011.",0,,False
171,"[3] J. Lin and M. Efron. Evaluation as a service for information retrieval. ACM SIGIR Forum, 47(2):8­14, 2013.",0,,False
172,"[4] J. Lin and M. Efron. Overview of the TREC-2013 microblog track. In TREC, 2013.",1,TREC,True
173,"[5] C. D. Manning, P. Raghavan, and H. Schu¨tze. Evaluation in information retrieval. In Introduction to Information Retrieval, chapter 8, pages 153­154. Cambridge University Press, 2009.",0,,False
174,"[6] R. McCreadie, I. Soboroff, J. Lin, C. Macdonald, I. Ounis, and D. McCullough. On building a reusable Twitter corpus. In SIGIR, 2012.",1,ad,True
175,"[7] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. Overview of the TREC-2011 microblog track. In TREC, 2011.",1,TREC,True
176,"[8] K. Sparck Jones and C. van Rijsbergen. Report on the need for and provision of an ""ideal"" information retrieval test collection. British Library Research and Development Report 5266, 1975.",0,,False
177,"[9] E. M. Voorhees. The philosophy of information retrieval evaluation. In CLEF, 2002.",1,CLEF,True
178,"[10] J. Zobel. How reliable are the results of large-sacle information retrieval experiments? In SIGIR, 1998.",0,,False
179,962,0,,False
180,,0,,False

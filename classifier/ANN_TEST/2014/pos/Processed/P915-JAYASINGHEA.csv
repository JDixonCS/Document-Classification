,sentence,label,data,regex
0,Extending Test Collection Pools Without Manual Runs,0,,False
1,Gaya K. Jayasinghe,0,,False
2,"RMIT University Melbourne, Australia gaya.jayasinghe@rmit.edu.au",0,,False
3,William Webber,0,,False
4,"William Webber Consulting Melbourne, Australia",0,,False
5,william@williamwebber.com,0,,False
6,J. Shane Culpepper,0,,False
7,"RMIT University Melbourne, Australia shane.culpepper@rmit.edu.au",0,,False
8,Mark Sanderson,0,,False
9,"RMIT University Melbourne, Australia mark.sanderson@rmit.edu.au",0,,False
10,ABSTRACT,0,,False
11,"Information retrieval test collections traditionally use a combination of automatic and manual runs to create a pool of documents to be judged. The quality of the final judgments produced for a collection is a product of the variety across each of the runs submitted and the pool depth. In this work, we explore fully automated approaches to generating a pool. By combining a simple voting approach with machine learning from documents retrieved by automatic runs, we are able to identify a large portion of relevant documents that would normally only be found through manual runs. Our initial results are promising and can be extended in future studies to help test collection curators ensure proper judgment coverage is maintained across complete document collections.",1,ad,True
12,Categories and Subject Descriptors,0,,False
13,"H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval--clustering, retrieval models, search & selection process",0,,False
14,General Terms,0,,False
15,"Information retrieval, Evaluation, Test collection construction",0,,False
16,1. INTRODUCTION,1,DUC,True
17,"Successful evaluation and reproducibility of experiments in information retrieval (IR) depends on building reusable test collections composed of documents, topics, and relevance judgments. Ideally every document in a collection would be assessed against each topic, but this approach does not scale. So judgments are normally produced for a sample of the corpus, known as a pool, all other documents are assumed to be not relevant. This sample needs to be representative of the entire collection and robust enough to evaluate entirely new search algorithms. The genesis of pooling dates back to the 1970s [12].",0,,False
18,"To produce relevance judgments, the organizers of TREC, CLEF, NTCIR, and other such conferences invite researchers to submit the top-i documents retrieved for a set of topics from a specified corpus",1,TREC,True
19,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609473.",1,ad,True
20,"[14, 15] (typically i ,"" 1, 000). The sets of documents are known as automatic runs. Across the runs, the top-j ranked documents for each topic are gathered for relevance assessment (typically j is set to 50 or 100). Such a practice seems to consistently identify most of the relevant documents, but provides no guarantee on the judgment coverage for documents retrieved by new IR approaches [4, 9, 16]. Test collections tend to have a bias towards the systems contributing to the pool, and may not reliably evaluate novel IR systems that retrieve unjudged but relevant documents.""",0,,False
21,"In an attempt to ""future proof"" test collections, the organizers of the evaluation conferences commonly encourage submissions of manual runs, where humans can reformulate queries and/or merge results from multiple queries [1] before a final set of top-i documents is submitted. Such runs are generally highly effective and contribute many unique relevant documents to the judgment pool. However, manual runs are not always available when building a collection, so in this short paper we ask:",0,,False
22,Research question: Can we construct reliable IR test collections using only automatic retrieval runs?,0,,False
23,Our contribution: We describe a methodology that can be used to construct reusable test collections in the absence of manual retrieval runs. We evaluate a simple voting approach combined with machine learning to show that we can achieve collection coverage similar to pooling generated with manual runs.,0,,False
24,2. BACKGROUND,0,,False
25,"Efficiently building test collections for evaluation of IR systems is a well-studied problem [10]. Early research concentrated on more efficient ways for assessors to scan pools, with the objective of judging more documents with a given budget or identifying a sufficient number of relevant documents as quickly as possible. Zobel [16] showed that the number of relevant documents in a collection varies from topic to topic. He suggested that assessors should focus their effort on judging topics with more relevant documents. For each topic, the number of relevant documents found so far were used to estimate the expected ratio of relevant documents in the remaining unjudged block. Each topic was assessed until relevant documents were depleted beyond an economically viable limit to assess the block.",0,,False
26,"The idea of focusing assessor effort on the most fruitful sources of relevant documents was also applied to IR systems that contribute to a pool. Just as some topics have more relevant documents than others, some systems retrieve more relevant documents than others. Using this insight, Cormack et al. [5] described a move-tofront pooling approach which ensured that documents from the IR systems producing the most relevant documents were moved to the",0,,False
27,915,0,,False
28,916,0,,False
29,"We also use Kendall's  to measure pairwise inversions between two rankings of runs, the first using full TREC relevance assessments and the second using relevance assessments generated from the union of the first and second pools formed by each of our methods. Using a convention from Voorhees [13], if the Kendall's  correlation is  0.9, the rankings are considered equivalent.",1,TREC,True
30,Metric,0,,False
31,MAP P@10 P@20 P@30 P@100,1,MAP,True
32,Borda count,0,,False
33,0.0778 0.1306 0.1122 0.1020 0.0743,0,,False
34,ML,0,,False
35,0.0268 0.0531 0.0378 0.0361 0.0167,0,,False
36,Combined,0,,False
37,0.1507 0.1714 0.1500 0.1367 0.0916,0,,False
38,Table 1: Effectiveness on finding relevant documents in MRJ. A : significant improvement (p < 0.01) compared to Borda count.,0,,False
39,Depth (k) Borda count ML Combined,0,,False
40,50,0,,False
41,15.22 4.52,0,,False
42,19.00,0,,False
43,100,0,,False
44,24.19 5.45,0,,False
45,29.83,0,,False
46,150,0,,False
47,29.30 6.71,0,,False
48,37.28,0,,False
49,171,0,,False
50,31.36 7.11,0,,False
51,39.53,0,,False
52,200,0,,False
53,33.75 7.97,0,,False
54,42.33,0,,False
55,Table 2: Percentage of MRJ documents found in top (k) of the proposed rankings. implies a similar assessment effort to traditional pooling method. A : significant improvement (p < 0.05) compared to Borda count.,1,ad,True
56,5. RESULTS,0,,False
57,"The analysis is presented in Table 1. The combined method is significantly better than the other two when evaluated with MAP. The same trend is observed when measuring using precision, but none of the differences are significant. Using only the ML method produces worse results than either Borda count or combined.",1,MAP,True
58,"Note that the relatively low reported effectiveness numbers in Table 1 are largely a byproduct of evaluating using only the unique relevant documents in MRJ and not the entire second pool. We cannot make any claims about new documents retrieved by the ML method since a large portion of retrieved documents using this method are not judged, compared to other two approaches. In fact, 9, 817 of the top-200 documents returned across all 50 topics using only ML (98.17%) are currently unjudged. Therefore, we have to assume that these documents are not relevant until all of the documents returned are judged. In future work, we hope to investigate the full impact unjudged documents have on our classifier method in more detail.",0,,False
59,"In Table 2 we measure the proportion of documents that were found to be relevant in the second pool. Again a similar trend of differences are seen, but with significant improvements across all measurements up to k , 200 for the combined method.",0,,False
60,5.1 Discussion,0,,False
61,"As indicated in Figure 1, the majority of documents uniquely judged in the manual runs (MRJ) are also retrieved by the automatic runs (ARU+ARJ). However, few appear in the first pool as they (i.e. ARJ) are not ranked highly enough to be judged. In fact, 88% of the documents judged as relevant that are uniquely pooled by manual runs could be found in the first pool, if a pool depth of 1, 000 was used.",0,,False
62,"If there were no manual runs in a test collection (i.e. no MRJ), the effectiveness of IR systems producing results similar to such",0,,False
63,runs would be underestimated and any improvements would go unnoticed. It would appear that manual retrieval runs still play a critical role in improving the re-usability of test collections.,0,,False
64,Relevant documents exclusively pooled by manual retrieval runs,0,,False
65,10 1000,0,,False
66,Number of Relevant documents,0,,False
67,0,0,,False
68,Combined Estimates for combined,0,,False
69,0,0,,False
70,50,0,,False
71,100,0,,False
72,150,0,,False
73,200,0,,False
74,Depth of ranking (K),0,,False
75,"Figure 2: The number of MRJ documents, and estimated number of relevant documents in the top-k of the combined ranked list on TREC GOV2 dataset and TREC topics 801 ­ 850.",1,TREC,True
76,Metric,0,,False
77,MAP P@10 P@20 P@30 P@100,1,MAP,True
78,Borda count,0,,False
79,0.3415 0.3571 0.3551 0.3401 0.2337,0,,False
80,ML,0,,False
81,0.4872 0.5082 0.4684 0.4299 0.2624,0,,False
82,Combined,0,,False
83,0.5049 0.5694 0.4959 0.4497 0.2555·,0,,False
84,"Table 3: Just considering the documents in MRJ, how effective are ranking algorithms on retrieving relevant documents? Significant improvements (p < 0.01 and p < 0.05) compared to Borda count are denoted with a  and ·.",0,,False
85,"Judging the ranked lists of the combined method up to a depth k identifies a subset of the relevant documents uniquely pooled by manual retrieval runs. However, we still know little about the large number of unjudged documents in the ranked lists produced by the combined method. If we assume the proportion of relevant documents among unjudged documents in these ranked lists is the same as the proportion found among judged documents in the same ranked list up to the same depth, we can estimate the total number of relevant documents that would have been found in the same depth of the ranking. Figure 2 illustrates the estimated number of relevant documents, along with the number of known relevant documents found.",0,,False
86,"Missing judgments for a large portion of the ranked lists from the proposed methods is one potential reason for the low retrieval effectiveness of those methods. Therefore, we calculate retrieval effectiveness on the intersection of the second pool with MRJ, Table 3. (Note, the first pool and the ranking functions remains the same.) The ML method now re-ranks a subset of unique documents top-j ranked by manual runs. The ranking produced by ML show significant improvements for all considered evaluation metrics compared to Borda count. The combined method achieves a better effectiveness than ML for all evaluation metrics considered, except p@100. This is due to ranking only the subset of documents top ranked by the Borda count. Re-ranking a carefully retrieved",0,,False
87,917,0,,False
88,1,0,,False
89,1,0,,False
90,MAP P@10 P@20 P@30 P@100,1,MAP,True
91,0.9,0,,False
92,0.8,0,,False
93,Kendall's Tau,0,,False
94,0.9,0,,False
95,Kendall's Tau,0,,False
96,0.8,0,,False
97,0,0,,False
98,50,0,,False
99,100,0,,False
100,150,0,,False
101,200,0,,False
102,Depth,0,,False
103,0.7,0,,False
104,MAP P@10 P@20 P@30 P@100,1,MAP,True
105,5,0,,False
106,10,0,,False
107,15,0,,False
108,20,0,,False
109,25,0,,False
110,30,0,,False
111,Automatic systems,0,,False
112,Figure 3: Kendall's  correlation of IR system rankings for varying depths of assessing documents using combined method.,0,,False
113,subset of documents for topics with ML is an effective approach to locate new documents to be pooled and judged.,0,,False
114,"Whenever a new approach for pool composition is proposed, we would like to be able to quantify how well the approach ranks IR systems compared to the original method. A Kendall's  ranking correlation for varying depths of assessing documents with the proposed approach for various evaluation metric are shown in Figure 3. Here, we consider all 80 submitted runs rather than only the subset originally used for pooling. Manual retrieval runs are viewed as novel approaches to retrieval. The Kendall's  correlation for MAP is above 0.9 beyond a depth of 100. A budget similar to original assessment permits processing up to a depth of 171 documents, which demonstrates the validity of the proposed approach in the absence of manual retrieval runs.",1,MAP,True
115,Another question of interest is how small the automatic runs pool can be when there are no manual runs. In Figure 4 we introduce runs incrementally in order starting with the run contributing the fewest relevant documents. When 20 or more automatic retrieval runs are pooled the Kendall  correlation for MAP exceeds 0.9.,1,MAP,True
116,6. CONCLUSION,0,,False
117,"In this paper, we present a methodology for building reusable evaluation pools in the absence of manual retrieval runs. Our approach can discover many relevant documents that were previously only found by manual retrieval runs. The approach demonstrates the potential of finding relevant documents that are not currently possible using current pooling approaches. However, the true efficacy of our approach cannot be properly assessed until all of the newly retrieved documents are judged. We plan to investigate this in future work. Nonetheless, our initial results are promising as we are already able to achieve a similar IR system ranking to previous approaches which depended heavily on manual runs to add the necessary diversity to the assessment pool.",1,ad,True
118,Acknowledgments,0,,False
119,This work was supported in part by the Australian Research Council (DP130104007). Dr. Culpepper is the recipient of an ARC DECRA Research Fellowship (DE140100275).,0,,False
120,"Figure 4: Kendall's  correlation of IR system rankings with varying number of automatic systems in the pool. Automatic systems are added in the order least contributing system to most, and the ranking produced by the combined method is processed to a depth of 200.",1,ad,True
121,References,0,,False
122,"[1] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Information Retrieval, 10(6): 491­508, 2007.",0,,False
123,"[2] S. Büttcher, C. L. A. Clarke, and I. Soboroff. The TREC 2006 terabyte track. In TREC-2006, volume 6, page 39, 2006.",1,TREC,True
124,"[3] S. Büttcher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboroff. Reliable information retrieval evaluation with incomplete and biased judgements. In SIGIR, pages 63­70, 2007.",0,,False
125,"[4] B. Carterette, E. Gabrilovich, V. Josifovski, and D. Metzler. Measuring the reusability of test collections. In WSDM, pages 231­240, 2010.",0,,False
126,"[5] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In SIGIR, pages 282­289, 1998.",0,,False
127,"[6] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871­1874, June 2008.",0,,False
128,"[7] R. Krovetz. Viewing morphology as an inference process. In SIGIR, pages 191­202, Pittsburgh, Pennsylvania, USA, 1993.",0,,False
129,"[8] A. Moffat, W. Webber, and J. Zobel. Strategic system comparisons via targeted relevance judgments. In SIGIR, pages 375­382, 2007.",0,,False
130,"[9] T. Sakai. The unreusability of diversified search test collections. In EVIA, June 2013.",0,,False
131,"[10] M. Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4 (4):247­375, 2010.",0,,False
132,"[11] I. Soboroff and S. Robertson. Building a filtering test collection for TREC 2002. In SIGIR, pages 243­250, 2003.",1,TREC,True
133,"[12] K. Spärck Jones and C. J. Van Rijsbergen. Report on the need for and provision of an ""ideal"" information retrieval test collection. Technical report, British Library Research and Development Report 5266, 1975.",0,,False
134,"[13] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information processing & management, 36(5):697­716, 2000.",0,,False
135,"[14] E. M. Voorhees. The philosophy of information retrieval evaluation. In Evaluation of cross-language information retrieval systems, pages 355­370. Springer, 2002.",0,,False
136,"[15] E. M. Voorhees and D. K. Harman. TREC: Experiment and evaluation in information retrieval, volume 63. MIT press Cambridge, 2005.",1,TREC,True
137,"[16] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In SIGIR, pages 307­314, 1998.",0,,False
138,918,0,,False
139,,0,,False

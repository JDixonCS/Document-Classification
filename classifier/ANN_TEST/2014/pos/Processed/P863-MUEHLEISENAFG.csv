,sentence,label,data,regex
0,Old Dogs Are Great at New Tricks: Column Stores for IR Prototyping,0,,False
1,"Hannes Mühleisen,1 Thaer Samar,1 Jimmy Lin,2 and Arjen de Vries1",0,,False
2,"1 Centrum Wiskunde & Informatica, Amsterdam, The Netherlands 2 University of Maryland, College Park, Maryland, USA",0,,False
3,"{hannes|samar}@cwi.nl, jimmylin@umd.edu, arjen@acm.org",0,,False
4,ABSTRACT,0,,False
5,"We make the suggestion that instead of implementing custom index structures and query evaluation algorithms, IR researchers should simply store document representations in a column-oriented relational database and implement ranking models using SQL. For rapid prototyping, this is particularly advantageous since researchers can explore new scoring functions and features by simply issuing SQL queries, without needing to write imperative code. We demonstrate the feasibility of this approach by an implementation of conjunctive BM25 using two modern column stores. Experiments on a web collection show that a retrieval engine built in this manner achieves effectiveness and efficiency on par with custom-built retrieval engines, but provides many additional advantages, including cleaner query semantics, a simpler architecture, built-in support for error analysis, and the ability to exploit advances in database technology ""for free"".",1,ad,True
6,Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation,0,,False
7,Keywords: Relational Databases; BM25,0,,False
8,1. INTRODUCTION,1,DUC,True
9,"Information retrieval researchers and practitioners have long implemented specialized, custom-built data structures and query evaluation algorithms for document ranking [15]. Today, these techniques can be quite complex, especially with ""structured queries"" that span multiple nested clauses with a panoply of query operators [11]. We revisit the idea that the information retrieval community can (and should!) make use of general-purpose data management solutions, instead of building specialized backend technology. We demonstrate that storing posting lists as relations and expressing ranking models as SQL queries is now a viable alternative to custom inverted indexes if we leverage modern column-oriented relational databases (or `column stores').",1,ad,True
10,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609460.",1,ad,True
11,"The contribution of this work is to demonstrate empirically that we can now safely delegate the data management needs of IR engines to modern column stores without concerns regarding efficiency. We show experimentally that an IR engine built in this manner achieves not only effectiveness and efficiency on par with custom-built retrieval engines, but provides many additional advantages. This suggests a somewhat radical message: IR researchers should stop writing their own retrieval engines and just use column stores. We advocate this approach especially for rapid prototyping-- when developing new scoring models, features functions, etc.--but such an architecture has additional advantages. Although we are not the first to make this claim, there have been a number of developments since previous work that make our design more attractive than before.",1,ad,True
12,2. BACKGROUND AND RELATED WORK,0,,False
13,"What advantages does using a database for IR have? We see many, beginning with a precise formal framework. As more complex query operators are introduced for document ranking, it sometimes becomes unclear how to properly score documents, particularly in corner cases. Developers often resort to heuristics and other shortcuts. Relational databases provide a formal and theoretically-sound framework in which to express any query evaluation algorithm--namely, relational calculus (or, practically, SQL). This forces IR researchers to be precise about query semantics, which may be especially useful when complex query operators are introduced in document ranking.",1,ad,True
14,"Second, taking advantage of relational databases yields a cleaner architecture. Almost all IR systems today are monolithic agglomerations of components for text processing, document inversion, integer compression, memory/disk management, query evaluation, etc. By offloading the storage management to a relational database, we introduce a clean abstraction (via SQL) between the ""low-level"" components of the engine and the IR-specific components (e.g., learning to rank). This (hopefully) reduces overall system complexity and may allow different IR engines to inter-operate.",1,ad,True
15,"Third, retrieval systems can benefit from advances in data management. Performance is a dominant preoccupation of database researchers, who make regular breakthroughs that propagate to the IR community (for example, PForDelta [17] compression originated from database researchers). By using relational databases, IR systems can benefit from future advances more rapidly, and ""for free"".",1,ad,True
16,"Fourth, a database provides integrated analytical tools useful for error analysis. Consider a simple example, where",0,,False
17,863,0,,False
18,"we wish to examine whether our scoring model has a length bias. This might be accomplished by examining scatterplots of length vs. retrieval scores. With a database, generating these data can be accomplished with a straightforward join between qrels (easy to store in the database), document representations, and the results set. In contrast, with a custom-built retrieval engine, one would need to write additional code to dump out document lengths from internal data structures and perform the join in an ad hoc fashion. Furthermore, there is a rich ecosystem of external analytical toolkits that are able to interface directly with relational databases, which could also be helpful for IR researchers.",1,ad,True
19,"Finally, databases form a flexible rapid prototyping tool. Many IR researchers do not really care about index structures and query evaluation per se--they are merely means to an end, such as assessing the effectiveness of a particular ranking model or feature. In this case, forcing researchers to design data structures and query evaluation algorithms is a burden. Using a relational database, researchers can rapidly experiment by issuing declarative SQL queries without needing to write (error-prone) imperative or object-oriented code.",0,,False
20,"Since retrieval operates over collections of documents, analytics-optimized (OLAP) relational databases are more appropriate than transaction-optimized (OLTP) databases. There are many similarities between query evaluation in document retrieval and online analytical processing (OLAP) tasks in modern data warehouses. Both frequently involve scans, aggregations, and sorting. Thus, we believe that column-oriented databases, which excel at OLAP queries, are amenable to retrieval tasks. An overview of such databases is beyond the scope of this work, but the basic insight is to decompose relations into columns for storage and processing [4]. This storage model allows us to mask random access memory latencies and take full advantage of modern hardware. We use two different column stores, MonetDB [2, 9] and VectorWise [16], to illustrate document ranking on a portion of the ClueWeb12 collection.",1,AP,True
21,"We are not the first to claim that databases may have something to offer for IR. Examples of early work include [13, 10, 6]; key references on runtime efficiency are discussed in a 2005 survey [3]. Perhaps the first `real' SQL results for IR queries were presented in [7], but at excessively high costs in terms of the hardware required. More viable results were presented in the TREC 2006 terabyte track, but the approach required hand-written query plans [8]. Follow-up research by the same group allowed the retrieval model to be expressed at the conceptual level [5], however, using a rather `exotic' query language based on array comprehensions. After all these years, we have finally reached the state where database engines can take on IR workloads expressed in standard SQL, without forcing the database admin to resort to low-level tuning. As far as we know, this paper is the first report of experimental work where competitive IR results (in terms of both efficiency and effectiveness) have been obtained using standard relational database technology.",1,TREC,True
22,3. SYSTEM ARCHITECTURE,0,,False
23,"In a custom-built IR engine, a document collection must first be processed (e.g., tokenized) and indexed before retrieval can be performed. In an architecture based on column stores, there are equivalent steps: the collection must be processed and loaded into the database prior to query evaluation. This section provides details on our system architecture.",1,ad,True
24,table: dict,0,,False
25,table: terms,0,,False
26,termid term df,0,,False
27,termid docid pos,0,,False
28,1,0,,False
29,put 1,0,,False
30,1,0,,False
31,1,0,,False
32,2,0,,False
33,2,0,,False
34,robe 1,0,,False
35,2,0,,False
36,1,0,,False
37,5,0,,False
38,3,0,,False
39,wizard 1,0,,False
40,3,0,,False
41,1,0,,False
42,7,0,,False
43,4,0,,False
44,hat 1,0,,False
45,4,0,,False
46,1,0,,False
47,8,0,,False
48,table: docs,0,,False
49,docid name len,0,,False
50,1 doc1 8,0,,False
51,Figure 1: IR data structures in a relational database.,0,,False
52,3.1 Document Processing and Loading,1,ad,True
53,"We take advantage of the massive scale-out capabilities of Hadoop MapReduce to convert a document collection into a collection of relational tables. Document processing includes tokenization, stemming using the Krovetz stemmer, and stopword removal. The stemmed and filtered terms are mapped to integer ids and stored in a dictionary table. In the main terms table, we store all terms in a document (by term id), along with the position in which they occur. To give a concrete example, consider the document doc1 with the content ""I put on my robe and wizard hat"". After stopword removal, the relational tables generated from this document are as shown in Figure 1. In more detail, these tables are first generated as flat text files using a two-pass approach on Hadoop; the first pass builds the term to term id mapping, and the second pass builds the terms and the docs tables. These flat text files are then bulk loaded into the database.",1,ad,True
54,3.2 Query Evaluation,1,Query,True
55,"We implemented Okapi BM25 as an SQL query, but our approach can be easily extended to other ranking functions. Our experiments focused on conjunctive query evaluation, where the document must contain all query terms; previous work [1] has shown that this approach yields comparable end-to-end effectiveness to disjunctive query evaluation, but is faster. When scoring documents based on BM25, the only score component that depends on the query is the term frequency f (qi, D). An obvious opportunity for optimization here would be to precalculate the term frequencies for each term/document combination, since the positions of the terms do not influence the ranking score for conjunctive queries. By also sorting the terms table by term id (in effect performing document inversion), the database is able to avoid scanning the entire table and instead use binary search, with greatly improved efficiency. The complete ranking function can be expressed in SQL, shown in Figure 2.",1,ad,True
56,"We map conjunctive BM25 ranking to SQL in three parts: First, we find the entries in the terms table for the query terms (Lines 1 and 2). In this case, the query terms have ids 10575, 1285, and 191.1 The second step calculates the individual scores for all term/document combinations (Lines 4-13). To express the conjunctivity in the query, we filter this intermediate result to include only combinations with exactly three different term ids. (Lines 9-11). We collect information about document ids and lengths (Line 12) as well as the document frequencies of the terms (Line 13). We calculate the individual BM25 scores for each term/document combination (Lines 5 and 6), sum the results and sort (Lines 14 to 16). With the term frequency precalculation optimization,",0,,False
57,"1It would be simple to ""join in"" the dictionary by term, but the purpose of the shown query is to explain the basic concept.",0,,False
58,864,0,,False
59,"1 WITH qterms AS (SELECT termid, docid FROM terms",0,,False
60,2,0,,False
61,"WHERE termid IN (10575, 1285, 191)),",0,,False
62,3 subscores AS (,0,,False
63,"4 SELECT docs.docid, len, term_tf.termid,",0,,False
64,5,0,,False
65,"tf, df, (log((45174549-df+0.5)/(df+0.5))* ((tf*(1.2+1)/(tf+1.2*(1-",0,,False
66,6,0,,False
67,0.75+0.75*(len/513.67)))))) AS subscore,0,,False
68,7,0,,False
69,"FROM (SELECT termid, docid, COUNT(*) AS tf FROM qterms",0,,False
70,8,0,,False
71,"GROUP BY docid, termid) AS term_tf",0,,False
72,9,0,,False
73,JOIN (SELECT docid FROM qterms,0,,False
74,10,0,,False
75,"GROUP BY docid HAVING COUNT(DISTINCT termid) , 3)",0,,False
76,11,0,,False
77,"AS cdocs ON term_tf.docid,cdocs.docid",0,,False
78,12,0,,False
79,"JOIN docs ON term_tf.docid,docs.docid",0,,False
80,13,0,,False
81,"JOIN dict ON term_tf.termid,dict.termid)",0,,False
82,"14 SELECT name, score FROM (SELECT docid, sum(subscore) AS score",0,,False
83,15,0,,False
84,FROM subscores GROUP BY docid) AS scores JOIN docs ON,0,,False
85,16,0,,False
86,"scores.docid,docs.docid ORDER BY score DESC LIMIT 1000;",0,,False
87,Figure 2: Conjunctive BM25 in SQL. The numbers printed in bold are the only parts of the SQL query that depend on the document collection and query terms.,0,,False
88,the grouping of the qterms CTE (Line 8) would be replaced by a straight selection from the term frequencies table. Note that this approach can be extended to any scoring function that is a sum of matching query terms. Other modifications are straightforward: disjunctive query evaluation can be implemented by replacing the number of matching terms in Line 10. Phrase queries can be performed by arithmetic over term positions and enforcing distance constraints.,0,,False
89,4. EXPERIMENTS,0,,False
90,"The main point of this paper is that relational database technology ­ in particular, columnar storage ­ is suitable for rapid prototyping for IR. So far, we have shown how a retrieval model can be implemented without writing any imperative code. In further support of this claim, we present experimental results demonstrating that our approach achieves effectiveness and efficiency on par with custom-built retrieval engines. We compare two different relational backends to three open-source IR engines: the open-source columnar database MonetDB [9] (v11.17.13), the commercial database VectorWise [16] (v3.0.1); and Lucene (v4.3), Indri [14] (v5.5), and Terrier [12] (v3.5). Comparing MonetDB and VectorWise, the latter combines columnar storage with lightweight compression and a pipelining execution model, which makes it the current top performer on the well-known TPC-H benchmark for OLAP databases, and especially suited for very large workloads.",1,AP,True
91,"Our experiments used the first segment on the first disk of ClueWeb12 (45 million documents) with queries 201­250 from the TREC 2013 web track. This setup is realistic, since production search engines usually adopt a partitioned architecture (and we focus on a single partition). The qrels from the TREC topics were filtered to only include documents that are contained in the segment we used. To ensure that all the IR engines work on the same text, we used Hadoop to",1,ClueWeb,True
92,System Indri MonetDB/VectorWise Lucene Terrier,0,,False
93,MAP 0.246 0.225 0.216 0.215,1,MAP,True
94,P5 0.304 0.276 0.265 0.272,0,,False
95,Table 1: MAP and P5 effectiveness scores.,1,MAP,True
96,"pre-process the documents in the same manner as in the relational setup: this was accomplished by dumping the processed collection as plain text, turning off stemming/stopword removal in the IR engine, and tokenizing by whitespace. In all cases we retrieved the top 1000 results. All experiments were run on a Linux desktop computer (Fedora 20, Kernel 3.12.10, 64 Bit) with 8 cores (Intel Core i7-2600K, 3.4 GHz) and 16 GB of main memory.",0,,False
97,"Effectiveness results are shown in Table 1. MonetDB and VectorWise produce exactly the same rankings and scores, which is of course to be expected, given that both execute the same SQL queries. However, the effectiveness differences between Indri and Terrier do come as a surprise, since both systems implement BM25--these differences cannot be attributed to tokenization and stopword differences, given the unified document processing step described above.",0,,False
98,"From these results we draw two conclusions: First, our architecture yields effectiveness that is at least on par with existing custom-built IR engines. Second, these results highlight the advantage of SQL in providing concise yet precise semantics. While we do not claim to have the ""correct"" BM25 implementation, at the very least our model is concisely specified in a few lines (the SQL query) and thus easy to inspect, not buried in code. Furthermore, different backends produce exactly the same results.",1,ad,True
99,"In terms of efficiency, we measured query latency. For each system, queries were run sequentially in isolation. As mentioned previously, pre-calculating the term frequency for conjunctive queries is an obvious optimization in the relational representation, and halves the size of the terms table. In our experiments we examined this optimization independently (""Precalc"" vs. ""Full""). As an additional optimization, we have placed the MonetDB database on a compressed file system (BTRFS with zlib compression, marked as ""C""), since MonetDB does not natively compress data. This reduced the disk footprint of the MonetDB database to 1/3 of the original size. In addition, we consulted with the developers of the Terrier system after observing pathologically slow response times. They made two suggestions: 1) changing the index storage model from on-disk to in-memory, which effectively leads to parts of the index being copied into memory on startup, and 2) changing the retrieval model from term-at-atime (TAAT) to document-at-a-time (DAAT). We did not include the first optimization here, as it makes the cold cache runs pointless (see below). However, the second optimization led to greatly improved performance. For fairness, we report on both the original configuration for Terrier (Orig) as well as on the optimized configuration (DAAT).",1,ad,True
100,"Figure 3 shows the query latency distribution across topics as box plots; the boxes contain the observations between the 25th and 75th percentiles. In addition, the plots are annotated with the median query latency. Since the indexes are stored on a hard disk, we expect disk I/O to have a large impact on performance. To test this, we ran the query set two times in succession. Before the first run, we asked the operating system to empty all caches and restarted the",1,ad,True
101,865,0,,False
102,Indri VectorWise/Precalc,0,,False
103,VectorWise/Full Lucene,0,,False
104,MonetDB/C+Precalc MonetDB/C+Full Terrier/DAAT Terrier/Orig MonetDB/Precalc MonetDB/Full,0,,False
105,0.56,0,,False
106,O,0,,False
107,O,0,,False
108,0.94,0,,False
109,O,0,,False
110,O,0,,False
111,1.58,0,,False
112,OO,0,,False
113,1.69,0,,False
114,O,0,,False
115,4.08,0,,False
116,6.19,0,,False
117,6.57,0,,False
118,O,0,,False
119,8.56,0,,False
120,O,0,,False
121,O,0,,False
122,O,0,,False
123,O,0,,False
124,OO,0,,False
125,O O,0,,False
126,O,0,,False
127,17.82,0,,False
128,19.25,0,,False
129,0,0,,False
130,10,0,,False
131,20,0,,False
132,Query Time (sec),1,Query,True
133,(a) Cold Runs,0,,False
134,0.03,0,,False
135,Lucene OO,0,,False
136,0.2,0,,False
137,Indri,0,,False
138,O,0,,False
139,0.34,0,,False
140,Terrier/DAAT,0,,False
141,O,0,,False
142,OO,0,,False
143,O,0,,False
144,0.59,0,,False
145,VectorWise/Precalc,0,,False
146,O,0,,False
147,0.74,0,,False
148,VectorWise/Full,0,,False
149,O,0,,False
150,OO,0,,False
151,1.73,0,,False
152,MonetDB/Precalc,0,,False
153,OO,0,,False
154,1.84,0,,False
155,MonetDB/C+Precalc,0,,False
156,O,0,,False
157,O,0,,False
158,2,0,,False
159,OO,0,,False
160,Terrier/Orig,0,,False
161,OO,0,,False
162,2.5,0,,False
163,MonetDB/Full,0,,False
164,O,0,,False
165,O,0,,False
166,O,0,,False
167,3,0,,False
168,MonetDB/C+Full,0,,False
169,OO,0,,False
170,30,0,,False
171,0,0,,False
172,10,0,,False
173,20,0,,False
174,30,0,,False
175,Query Time (sec),1,Query,True
176,(b) Hot Runs,0,,False
177,Figure 3: Query latencies for TREC 2013 web queries on the first segment of ClueWeb12.,1,Query,True
178,"system under test. The two runs are presented in separate plots, the first (""cold"" caches) in (a) and the second in (b).",0,,False
179,"Considering cold runs, we see that Indri was the fastest system, taking a median of 0.56 seconds to complete a query. Second and third were VectorWise, with the precaclulated term frequencies clearly faster, followed by Lucene with a comparable time. MonetDB on a compressed file system and Terrier are comparable. The uncompressed MonetDB experiments clearly show the necessity of compressing an IR index. The vastly greater index size increased the cold response times substantially.",0,,False
180,"For the hot runs, Lucene leads the field by a large margin, which we suspect to be due to result caching. For Terrier, we see vastly improved timings for the DAAT setting when parts of the index are available in memory and do not have to be loaded from disk. We see that MonetDB (uncompressed) is able to take advantage of caching by the operating system, improving performance by about an order of magnitude compared to the cold runs.",1,ad,True
181,"Summarizing these results, it is clear that our database architecture achieves query latencies that are at least on par with existing custom-built IR engines.",0,,False
182,5. CONCLUDING REMARKS,0,,False
183,"This paper argues for the use of column stores for prototyping IR systems. The well-defined relational model allows for the precise expression of retrieval models independent of system implementations. Furthermore, clear abstraction via the declarative SQL interface allows the backend database engine to be swapped, allowing IR systems to quickly benefit from advances in the database field. We have described how to express a standard ranking function using an SQL query and experiments show that using a column store yields effectiveness and efficiency that are at least on par with custom-built IR engines. Hopefully, this will inspire additional IR researchers to think about using columnar databases for building future systems, as doing so would let us focus on the questions on how to best satisfy information needs.",1,ad,True
184,6. ACKNOWLEDGMENTS,0,,False
185,"This research was supported by the Netherlands Organization for Scientific Research (NWO project 640.005.001), the Dutch national program COMMIT/, and the U.S. National Science Foundation (IIS-1144034 and IIS-1218043). Any",0,,False
186,"opinions, findings, or recommendations expressed are the authors' and do not necessarily reflect those of the sponsors.",0,,False
187,7. REFERENCES,0,,False
188,"[1] N. Asadi and J. Lin. Effectiveness/efficiency tradeoffs for candidate generation in multi-stage retrieval architectures. SIGIR, 2013.",1,ad,True
189,"[2] P. A. Boncz. Monet: A Next-Generation DBMS Kernel For Query-Intensive Applications. PhD thesis, Universiteit van Amsterdam, May 2002.",1,Query,True
190,"[3] S. Chaudhuri, R. Ramakrishnan, and G. Weikum. Integrating DB and IR technologies: What is the sound of one hand clapping? CIDR, 2005.",0,,False
191,"[4] G. Copeland and S. Khoshafian. A decomposition storage model. SIGMOD, 1985.",0,,False
192,"[5] R. Cornacchia, S. H´eman, M. Zukowski, A. de Vries, and P. Boncz. Flexible and efficient IR using array databases. VLDB, 2008.",0,,False
193,"[6] N. Fuhr. Models for integrated information retrieval and database systems. IEEE Data Eng. Bull., 19(1):3­13, 1996.",0,,False
194,"[7] T. Grabs, K. Bhoem, and H.-J. Schek. PowerDB-IR: scalable information retrieval and storage with a cluster of databases. Knowledge and Information Systems, 6(4):465­505, 2004.",0,,False
195,"[8] S. H´eman, M. Zukowski, A. de Vries, and P. A. Boncz. MonetDB/X100 at the 2006 TREC terabyte track. TREC, 2006.",1,TREC,True
196,"[9] S. Idreos, F. Groffen, N. Nes, S. Manegold, K. S. Mullender, and M. L. Kersten. MonetDB: Two decades of research in column-oriented database architectures. IEEE Data Eng. Bull., 35(1):40­45, 2012.",1,ad,True
197,"[10] I. Macleod. Text retrieval and the relational model. JASIS, 42(3):155­165, 1991.",0,,False
198,"[11] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. IP&M, 40(5):735­750, 2004.",0,,False
199,"[12] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A high performance and scalable information retrieval platform. OSIR, 2006.",0,,False
200,"[13] H.-J. Schek and P. Pistor. Data structures for an integrated data base management and information retrieval system. VLDB, 1982.",0,,False
201,"[14] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: a language-model based search engine for complex queries. International Conference on Intelligent Analysis, 2005.",0,,False
202,"[15] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Computing Surveys, 38(6):1­56, 2006.",0,,False
203,"[16] M. Zukowski and P. A. Boncz. Vectorwise: Beyond column stores. IEEE Data Eng. Bull., 35(1):21­27, 2012.",0,,False
204,"[17] M. Zukowski, S. Heman, N. Nes, and P. Boncz. Super-Scalar RAM-CPU Cache Compression. ICDE, 2006.",0,,False
205,866,0,,False
206,,0,,False

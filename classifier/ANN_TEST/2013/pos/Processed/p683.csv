,sentence,label,data,regex
0,A Mutual Information-based Framework for the Analysis of Information Retrieval Systems,0,,False
1,Peter B. Golbus Javed A. Aslam,0,,False
2,"College of Computer and Information Science Northeastern University Boston, MA, USA",0,,False
3,"{pgolbus,jaa}@ccs.neu.edu",0,,False
4,ABSTRACT,0,,False
5,"We consider the problem of information retrieval evaluation and the methods and metrics used for such evaluations. We propose a probabilistic framework for evaluation which we use to develop new information-theoretic evaluation metrics. We demonstrate that these new metrics are powerful and generalizable, enabling evaluations heretofore not possible.",0,,False
6,"We introduce four preliminary uses of our framework: (1) a measure of conditional rank correlation, information  , a powerful meta-evaluation tool whose use we demonstrate on understanding novelty and diversity evaluation; (2) a new evaluation measure, relevance information correlation, which is correlated with traditional evaluation measures and can be used to (3) evaluate a collection of systems simultaneously, which provides a natural upper bound on metasearch performance; and (4) a measure of the similarity between rankers on judged documents, information difference, which allows us to determine whether systems with similar performance are in fact different.",1,ad,True
7,Categories and Subject Descriptors,0,,False
8,H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation (efficiency and effectiveness),0,,False
9,General Terms,0,,False
10,Experimentation; Theory; Measurement,0,,False
11,Keywords,0,,False
12,"Information Retrieval, Search Evaluation",0,,False
13,1. INTRODUCTION,1,DUC,True
14,"In order to improve search engines, it is necessary to accurately measure their current performance. If we cannot measure performance, how can we know whether a change",0,,False
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
16,"was beneficial? In recent years, much of the work on information retrieval evaluation has focused on user models [7, 18] and diversity measures [1, 10, 24] which attempt to accurately reflect the experience of the user of a modern internet search engine. However, these measure are not easily generalized. In this work, we introduce a probabilistic framework for evaluation that encompasses and generalizes current evaluation methods. Our probabilistic framework allows us to view evaluation using the tools of information theory [11]. While our framework is not designed to coincide with user experience, it provides immediate access to a large number of powerful tools allowing for a deeper understanding of the performance of search engines.",0,,False
17,"Our framework for evaluation is based on the observation that relevance judgments can also be interpreted as a preference between those documents with different relevance grades. This implies that relevance judgments can be treated as a retrieval system, and that evaluation can be considered as the ""rank"" correlation between systems and relevance judgments. To this end, we develop a probabilistic framework for rank correlation based on the expectation of random variables, which we demonstrate can also be used to compute existing evaluation metrics. However, the true value of our framework lies in its extension to new information-theoretic evaluation tools.",1,ad,True
18,"After a discussion of related work (Section 2), we introduce our framework in Section 3. In Section 4, we demonstrate that our framework allows for an information theoretic understanding of Kendall's  [17], information  , which we use to define a conditional version of the rank correlation between two lists conditioned on a third. In Section 5, we define a new evaluation measure based on our framework: relevance information correlation. We validate our measure by showing that it is highly correlated with existing measures such as average precision (AP) and normalized discounted cumulative gain (nDCG). As a demonstration of the versatility of our framework when compared to, for example, user models, we show that our measure can be used to evaluate a collection of systems simultaneously (Section 6), creating an upper bound on the performance of metasearch algorithms. Finally, in Section 7, we introduce information difference, a powerful new tool for evaluating the similarity of retrieval systems beyond simply comparing their performance.",1,AP,True
19,"This material is based upon work supported by the National Science Foundation under Grant No. IIS-1256172. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation (NSF).",0,,False
20,683,0,,False
21,2. RELATED WORK,0,,False
22,"Search systems are typically evaluated against test collections which consist of a corpus of documents, a set of topics, and relevance assessments--whether a subset of those documents are relevant with respect to each topic.1 For example, the annual, NIST-sponsored Text REtrieval Conference (TREC) creates test collections commonly used in academic research. The performance of systems is assessed with regards to a specific task. A traditional search task is to attempt to rank all relevant documents above any nonrelevant documents. For this task, systems are evaluated in terms of the average trade-off between their precision and recall with respect to multiple topics. For a given topic, Let gi  {0, 1} be the relevance grade of the document at rank i, and let R be the number of relevant documents in the collection. At rank k,",1,TREC,True
23,k,0,,False
24,gi,0,,False
25,"precision@k , i,1",0,,False
26,(1),0,,False
27,k,0,,False
28,k,0,,False
29,gi,0,,False
30,"recall@k , i,1",0,,False
31,(2),0,,False
32,R,0,,False
33,"The trade-off between the two is measured by average precision, which can be interpreted as the area under the precisionrecall curve.",1,ad,True
34,gi × precision@i,0,,False
35,"AP , i,1",1,AP,True
36,(3),0,,False
37,R,0,,False
38,"Average precision does not include information about document quality and degrees of relevance, and is an inherently recall-oriented measure. It is therefore not suitable for evaluating commercial web search engines.",0,,False
39,"With the growth of the World Wide Web, test collections began to include graded, non-binary relevance judgments, e.g. G ,"" {non-relevant, relevant, highly relevant} or G "","" {0, . . . , 4}. To make use of these graded assessments, J¨arvelin and Kek¨al¨ainen developed normalized discounted cumulative gain (nDCG) [15]. nDCG also has the advantage that it can be evaluated at arbitrary ranks, and can therefore be used for precision-oriented tasks like web search.""",1,ad,True
40,"Unlike average precision, which has a technical interpretation, nDCG can be best understood in terms of a model of a hypothetical user. In this model, a user will read the first k documents in a ranked list, deriving utility from each document. The amount of utility is proportional to the document's relevance grade and inversely proportional to the rank at which the document is encountered. We first define discounted cumulative gain (DCG).",1,ad,True
41,k 2gi - 1,0,,False
42,"DCG@k ,",0,,False
43,(4),0,,False
44,"i,1 log2(i + 1)",0,,False
45,"Since the range of DCG will vary from topic to topic, it is necessary to normalize these scores so that an average can",0,,False
46,"1For historical reasons, the set of relevance assessments is often referred to as a QREL.",0,,False
47,be computed. Normalization is performed with regard to an ideal ranked list. If DCG @k is the maximum possible DCG of any ranked list of documents in the collection then,0,,False
48,"nDCG@k , DCG@k",0,,False
49,(5),0,,False
50,DCG @k,0,,False
51,"However, one does not always know how many documents are relevant at each level, and therefore the ideal list used for normalization is only an approximation. Moffat and Zobel [18] introduced a measure, rank-biased precision (RBP), that addresses this issue. In RBP, the probability that a user will read the document at rank k is drawn from a geometric distribution, whose parameter,   [0, 1), models the user's persistence. Given a utility function u : G  [0, 1], commonly defined as",1,ad,True
52,2g - 1,0,,False
53,"u(g) , 2d",0,,False
54,(6),0,,False
55,"where d is the maximum possible relevance grade, RBP is defined as the expected utility of a user who browses according to this model.",1,ad,True
56,"RBP , (1 - ) u(gi) × i-1",0,,False
57,(7),0,,False
58,"i,1",0,,False
59,"Since RBP is guaranteed to be in the range [0,1) for any topic and , it does not require normalization.",0,,False
60,"Craswell et al. [12] introduced the Cascade model of user behavior. In this model, a user is still assumed to browse documents in order, but the probability that a user will view a particular document is no longer assumed to be independent of the documents that were viewed previously, i.e. a user is not assumed to stop at a particular rank, or at each rank with some probability. Instead, the user is assumed to stop after finding a relevant document. This implies that if a user reaches rank k, then all of the k - 1 documents ranked before it were non-relevant. Craswell et al. demonstrated empirically that this model corresponds well to observed user behavior in terms of predicting the clickthrough data of a commercial search engine.",1,ad,True
61,"Chapelle et al. [7] developed an evaluation measure, expected reciprocal rank (ERR), based on the Cascade model. Let Ri denote the probability that a user will find the document at rank i to be relevant. Then in the Cascade model, the likelihood that a user will terminate his or her search at rank r is",1,ad,True
62,r-1,0,,False
63,Rr (1 - Ri).,0,,False
64,(8),0,,False
65,"i,1",0,,False
66,"If we interpret the previously defined utility function (Equation 6) as the probability that a user will find a document relevant, i.e. Ri ,"" u(gi), then we can computed the expected reciprocal rank at which a user will terminate his or her search as""",0,,False
67, 1 r-1,0,,False
68,"ERR , r Rr (1 - Ri).",0,,False
69,(9),0,,False
70,"r,1",0,,False
71,"i,1",0,,False
72,"In this work, we propose an alternative, information-theoretic framework for evaluation. The first step is to reformulate these measures as the expected outcomes of random experiments. Computing evaluation measures in expectation is",0,,False
73,684,0,,False
74,"not uncommon in the literature, and we are not the first to suggest that reformulating an evaluation measure as an expectation allows for novel applications. For example, Yilmaz and Aslam [30] formulated average precision as the expectation of the following random experiment:",0,,False
75,"1. Pick a random relevant document,",0,,False
76,2. Pick a random document ranked at or above the rank of the document selected in step 1.,0,,False
77,"3. Output 1 if the document from step 2 is relevant, otherwise output 0.",0,,False
78,"Their intention was to accurately estimate average precision while collecting fewer relevance judgments (a process also applied to nDCG [32]). However, this formulation led to new uses, such as defining an information retrieval-specific rank correlation measure, AP [31], and a variation of average precision for graded relevance judgments, Graded Average Precision (GAP) [21].",1,AP,True
79,"Our work uses pairwise document preferences rather than absolute relevance judgments. The use of preferences is somewhat common in IR. For example, many learning-torank algorithms, such as LambdaMart [3] and RankBoost [13], use pairwise document preferences in their objective functions. Carterette et al. [4, 5] explored the collection of preference judgments for evaluation, showing that they are faster to collect and have lower levels of inter-assessor disagreement. More recently, Chandar and Carterette [6] crowdsourced the collection of conditional document preferences to evaluate the standard assumptions underlying diversity evaluation, for example that users always prefer novel documents. Relative document preferences can also be inferred from the clickthrough data collected in the logs of commercial search engines [16]. These preferences can be used for evaluation without undertaking the expense of collecting relevance judgments from assessors.",0,,False
80,3. A PROBABILISTIC FRAMEWORK FOR EVALUATION,0,,False
81,"Mathematically, one can view the search system as providing a total ordering of the documents ranked and a partial ordering of the entire collection, where all ranked documents are preferred to unranked documents but the relative preference among the unranked documents is unknown. Similarly, one can view the relevance assessments as providing a partial ordering of the entire collection: in the case of binary relevance assessments, for example, all judged relevant documents are preferred to all judged non-relevant and unjudged documents, but the relative preferences among the relevant documents and among the non-relevant and unjudged documents is unknown. Thus, mathematically, one can view retrieval evaluation as comparing the partial ordering of the collection induced by the search system with the partial ordering of the collection induced by the relevance assessments.",0,,False
82,"To formalize and instantiate a framework for comparing such partial orderings, consider the simplest case where we have two total orderings of objects, i.e., where the entire ""collection"" of objects is fully ranked in both ""orderings."" While such a situation does not typically arise in search system evaluation (since not all documents are ranked by the retrieval system nor are they fully ranked by relevance assessments), it does often arise when comparing the rankings",0,,False
83,of systems induced by two (or more) evaluation metrics; here,0,,False
84,Kendall's  is often the metric used to compare these (total,0,,False
85,order) rankings.,0,,False
86,"In what follows, we define a probabilistic framework within",0,,False
87,"which to compare two total orderings, and we show how tra-",0,,False
88,ditional metrics (such as Kendall's  ) are easily cast within,0,,False
89,this framework. The real power of such a framework is,0,,False
90,shown in subsequent sections: (1) the framework can be,0,,False
91,easily generalized to handle the comparison of two partial,0,,False
92,"orderings, such as arise in search system evaluation, and",0,,False
93,"(2) well-studied, powerful, and general information-theoretic",0,,False
94,metrics can be developed within this generalized framework.,0,,False
95,Consider two total orderings of n objects. There are,0,,False
96,n 2,0,,False
97,"(unordered) pairs of such objects, and a pair is said to be",0,,False
98,concordant if the two orderings agree on the relative rankings,0,,False
99,of the objects and discordant if the two orderings disagree.,0,,False
100,Let c and d be the number of concordant and discordant,0,,False
101,"pairs, respectively. Then Kendall's  is defined as follows:",0,,False
102,c-d,0,,False
103,",",0,,False
104,.,0,,False
105,(10),0,,False
106,c+d,0,,False
107,If we let C and D denote the fraction of concordant and discordant pairs then Kendall's  is defined as,0,,False
108," , C - D.",0,,False
109,(11),0,,False
110,"Note that c + d ,",0,,False
111,n 2,0,,False
112,if there are ties.2,0,,False
113,"To define a probabilistic framework, we must specify three",0,,False
114,"things: (1) a sample space of objects, (2) a distribution over",0,,False
115,"this sample space, and (3) random variables over this sample",0,,False
116,space. Let our sample space  be all possible 2 ·,0,,False
117,n 2,0,,False
118,ordered,0,,False
119,"pairs of distinct objects, and consider a uniform distribution",0,,False
120,"over this sample space. For a given ranking R, define a",0,,False
121,"random variable XR :   {-1, +1} that outputs +1 for",0,,False
122,any ordered pair concordant with R and -1 for any ordered,0,,False
123,pair discordant with R.,0,,False
124,"XR [(di, dj )] ,",0,,False
125,1 if di appears before dj in R. -1 otherwise.,0,,False
126,(12),0,,False
127,"We thus have a well-defined random experiment: draw an ordered pair of objects at random and output +1 if that ordered pair agrees with R's ranking and -1 otherwise. Since all ordered pairs of objects are considered uniformly, the expected value E[XR] of this random variable is zero.",0,,False
128,"Given a second ranked list S, one can similarly define an associated random variable XS. Now consider the random experiment of multiplying the two random variables: the product XR · XS will be +1 precisely when the pair is concordant--i.e. both lists agree that the ordering of the objects is correct (+1) or incorrect (-1), and the product will be -1 when the pair is discordant--i.e. the lists disagree. In this probabilistic framework, Kendall's  is the expected value",0,,False
129,"2Kendall defined two means by which  can account for ties, depending on the desired behavior. Imagine comparing two ranked lists, one of which is almost completely composed of ties. A, defined above, approaches 1. B includes the number of ties in the denominator, and therefore approaches 0. We believe that the former approach is appropriate in this context. Since QRELs are almost exclusively composed of ties (recall that all pairs of unjudged documents in the corpus are considered to be tied), using the latter would mean that effect of the relatively rare meaningful comparisons would be negligible.",0,,False
130,685,0,,False
131,of the product of these random variables:,0,,False
132," , E[XR · XS].",0,,False
133,(13),0,,False
134,"The real power of this framework is in the definition of these random variables: (1) the ability to generalize them to compare partial orderings as arise in system evaluation, and (2) the ability to measure the correlation of these random variables using information-theoretic techniques.",0,,False
135,4. INFORMATION-THEORETIC RANK CORRELATION,0,,False
136,"In Section 3, we defined Kendall's  as the expected product of random variables. The following theorem allows us to restate Kendall's  equivalently as the mutual information between the random variables.",0,,False
137,Theorem,0,,False
138,1.,0,,False
139,"I(XR; XS) ,",0,,False
140,1+ 2,0,,False
141,log(1+,0,,False
142,)+,0,,False
143,1- 2,0,,False
144,log(1- ).,0,,False
145,"(For a proof of Theorem 1, see Appendix). Unlike Kendall's  , the mutual information between ranked lists ranges from 0 on lists that are completely uncorrelated to 1 on lists that are either perfectly correlated or perfectly anti-correlated.",0,,False
146,"If we restrict our attention to pairs of lists that are not anti-correlated, then the relationship is bijective. Given this fact, we define a variant of Kendall's  , information  :",0,,False
147,"I (R, S) , I(XR; XS)",0,,False
148,(14),0,,False
149,"where XR is the ranked list random variable defined in Equation 12 observed with respect to the uniform probability distribution over all pairs of distinct objects. By reframing Kendall's  equivalently in terms of mutual information, we immediately gain access to a large number of powerful theoretical tools. For example, we can define a conditional information  between two lists given a third. For lists R and S given T ,",0,,False
150,"I (R, S | T ) , I(XR; XS | XT ).",0,,False
151,(15),0,,False
152,"Kendall's  can tell you whether two sets of rankings are similar, but it cannot tell you why. Information  can be used as a meta-evaluation tool to find the underlying cause of correlation between measures. We demonstrate the use of information  as a meta-evaluation tool by using it to analyze measures of the diversity of information retrieval systems. In recent years, several diversity measures (e.g. [1, 10, 24]) have been introduced to evaluate how well systems perform in response to ambiguous or underspecified queries that have multiple interpretations. These measures conflate several factors [14], including: a diversity model that rewards novelty and penalizes redundancy, and a measure of ad hoc performance that rewards systems for retrieving highly relevant documents. We wish to know not only whether two diversity measures are correlated, but also the similarity between their component diversity models. Using Kendall's  , we can observe whether the rankings of systems by each measure are correlated. But even if they are correlated, this could still be for one of two reasons: either both the diversity and the performance components evaluate systems similarly; or else one of the components is similar, and its effect on evaluation is dominant. However, if the measures are correlated when conditioned on their underlying performance components, then this must be due to similarities in their models of diversity.",1,ad,True
153,Figure 1: Per-query information  (conditional rank correlation) between the TREC and NTCIR gold standard diversity measures conditioned on their underlying performance measures.,1,TREC,True
154,"We measured this effect on the the TREC 2011 and 2012 Web collections [8, 9]. Note that the performance measures are evaluated using graded relevance, while the diversity measures use binary judgments for each subtopic. All evaluations are performed at rank 20. Figure 1 shows the rank correlation between ERR-IA and D#-nDCG, the primary measures reported by TREC and NTCIR [26], when conditioned on their underlying performance models. Each query is computed separately, with each datapoint in the figure corresponding to a different query. Table 1 shows the results of conditioning additional pairs of diversity measures (now averaged over queries in the usual way) on their performance models. The results in Figure 1 are typical of all pairs of measures on a per-query basis.",1,TREC,True
155,"Our results confirm that while diversity measures are very highly correlated, most of this correlation disappears when one conditions on the underlying performance model. This indicates that most of the correlation is due to the similarity between the performance components and not the diversity components. For example, in TREC 2010, ERR-IA and -nDCG have an information  of almost 0.9. However, when conditioned on ERR, the similarity falls to only 0.25. This means that while these two measures are mostly ranking systems for the same reason, that reason is simply ERR. However, of the 0.9 bits that are the same, 0.25 are due to some factor other than ERR. This other factor must presumably be the similarity in their diversity models.",1,TREC,True
156,686,0,,False
157,"I (ERR-IA ; -nDCG) I (ERR-IA ; -nDCG | nDCG) I (ERR-IA ; -nDCG | ERR) I (ERR-IA ; -nDCG | nDCG, ERR) I (ERR-IA ; D#-nDCG) I (ERR-IA ; D#-nDCG | nDCG) I (ERR-IA ; D#-nDCG | ERR) I (ERR-IA ; D#-nDCG | nDCG, ERR)",0,,False
158,TREC 2010 0.8290 0.4860 0.2499 0.2451 0.6390 0.3026 0.1222 0.1239,1,TREC,True
159,TREC 2011 0.8375 0.4434 0.3263 0.2805 0.5545 0.1728 0.1442 0.1003,1,TREC,True
160,Table 1: TREC 2010 and 2011 information  (conditional rank correlation) between diversity measures conditioned on ad hoc performance measures.,1,TREC,True
161,5. EVALUATION MEASURE,0,,False
162,"In this section, we demonstrate an extension of our probabilistic framework for evaluation to measuring the correlation between a system and the incomplete ranking generated by a set of relevance judgments. This allows us to define an information-theoretic evaluation measure, relevance information correlation. While our measure has novel applications, we will demonstrate that the evaluations produced are consistent with those of existing measures.",0,,False
163,"To compute mutual information, we must define a sample space, a probability distribution, and random variables. Let the sample space,  ,"" {(di, dj)}, be the set of all ordered pairs of judged documents. This means that we are ignoring unjudged documents, rather than considering them non-relevant. This is equivalent to computing an evaluation measure on the condensed list [23] created by removing all non-judged documents from the list. We define the probability distribution in terms of the QREL to ensure that all ranked lists will be evaluated using the same random experiment. Let P "", U |I(gi,""gj), where gi represents the relevance grade of document di, be the uniform probability distribution over all pairs of documents whose relevance grades are not equal. We define a QREL variable Q over ordered pairs of documents as""",1,ad,True
164,"Q [(di, dj )] ,",0,,False
165,1 0,0,,False
166,if gi > gj otherwise.,0,,False
167,(16),0,,False
168,Note that this definition can be applied to both graded and binary relevance judgments.,1,ad,True
169,"We now turn our attention to defining a ranked list random variable over ordered pairs of documents (di, dj). If both document di and dj appear in the ranked list, than our output can simply indicate whether di was ranked above dj. If document di appears in the ranked list and dj does not, then we will consider di as having been ranked above dj, and vice versa. If neither di nor dj is ranked, we will output a null value. If we were to instead restrict our attention only to judged document pairs where at least one document is ranked, then a ranked list consisting of a single relevant document followed by some number of non-relevant documents would have perfect mutual information with the QREL--all of the ranked relevant documents appear before all of the ranked non-relevant documents. However, this system must be penalized for preferring all of the ranked non-relevant documents to all of the unranked relevant documents. If we instead use a null value, our example ranked",1,ad,True
170,list would almost always output null. This behavior would,0,,False
171,"be independent of the QREL, meaning the two variables will",0,,False
172,"have almost no mutual information. In effect, the null value",0,,False
173,creates a recall component for our evaluation measure; no,0,,False
174,system can have a large mutual information with the QREL,0,,False
175,unless it retrieves most of the relevant documents.,0,,False
176,Another problem we must consider is that mutual infor-,0,,False
177,mation is maximized when two variables are completely cor-,0,,False
178,related or completely anti-correlated. Consider an example,0,,False
179,ranked list consisting of a few non-relevant documents fol-,0,,False
180,lowed by several relevant documents and then many more,0,,False
181,non-relevant documents. Since this example ranked list will,0,,False
182,"disagree with the QREL on almost all document pairs, its",0,,False
183,random variable will have a very high mutual information,0,,False
184,with the QREL variable. The system is effectively being,0,,False
185,rewarded for finding the subset of non-relevant documents,0,,False
186,"that happen to be present in the QREL. To address this,",1,ad,True
187,we truncate the list at the last retrieved relevant document,0,,False
188,prior to evaluation.,0,,False
189,Let ri represent the rank of document di in the list S. Then the ranked list variable RS is defined as,0,,False
190," 1  RS [(di, dj )] , 0",0,,False
191,-1,0,,False
192,if ri < rj if neither di nor djwere retrieved otherwise.,0,,False
193,(17),0,,False
194,"We define our new measure, Relevance Information Correlation, as the mutual information between the QREL variable Q and the truncated ranked list variable R",0,,False
195,"RIC(System) , I(RSystem; Q).",0,,False
196,(18),0,,False
197,"RIC is computed separately for each query, and then averaged, as with mean average precision.",0,,False
198,"In order to compute RIC we must estimate the joint probability distribution of document preferences over Q and R. This could be done in various ways. In this work, we use the maximum likelihood estimate computed separately for each query. Since the MLE requires a large number of observations, RIC is only accurate for recall-oriented evaluation. In future work, we intend to explore other means of estimating P (Q, R) that will allow RIC to be used for precision-oriented evaluation as well.",0,,False
199,"We also note that RIC has no explicit rank component, and would therefore seem to treat all relevant documents equally independent of the rank at which they were observed. However, there is an implicit rank component in that a relevant document that is not retrieved early in the list must be incorrectly ranked below many non-relevant documents. This argument is similar in spirit to Bpref [2].",0,,False
200,"Our measure is quite novel in its formulation, and makes many non-standard assumptions about information retrieval evaluation. Therefore it is necessary to validate experimentally that our measure prefers the same retrieval systems as existing measures. Note that for two evaluation measures to be considered compatible, it is sufficient that they rank systems in the same relative order; it is not necessary that they always assign systems similar absolute scores. For example, a system's nDCG is often higher than its average precision.",0,,False
201,"To show that RIC is consistent with AP and nDCG, we computed the RIC, AP, and nDCG of all systems submitted to TRECs 8 and 9. Figure 2 shows the output of RIC plotted against AP (top) and nDCG (bottom) on TRECs 8 (left) and 9 (right) [28, 29]. TREC 8 uses binary relevance",1,AP,True
202,687,0,,False
203,Figure 2: Correlation between RIC and AP (top) and nDCG (bottom). TREC 8 (left) uses binary relevance judgments. TREC 9 (right) uses graded relevance judgments.,1,AP,True
204,(G)AP nDCG,1,AP,True
205,MI,0,,False
206,TREC 8 0.716 0.713 0.719,1,TREC,True
207,TREC9 0.648 0.757 0.744,1,TREC,True
208,Table 2: Discriminative power of (graded) AP and nDCG vs. RIC,1,ad,True
209,"judgments. TREC 9 uses graded relevance judgments, requiring the use of graded average precision. Inset into each plot is the output of the measures on the top ten systems. For each experiment, we report the Kendall's  and Spearman's  [27] rank correlations for all systems, and for the top ten systems. With Kendall's  values of at least 0.799 on all systems and 0.644 on top ten systems, the ranking of systems by RIC is still highly correlated with those of both AP and nDCG. However, RIC is not as highly correlated with either AP or nDCG as AP and nDCG are with each other. Note that the correlation between RIC and GAP on TREC 9 is highly monotonic, even if is not particularly linear. This implies that the two measures do rank systems in a consistent relative order, even if RIC is a biased estimator of GAP.",1,TREC,True
210,"To further validate our measure, we also compute the discriminative power [22] of the various measures. Discriminative power is a widely used tool for evaluating a measure's sensitivity i.e. how often differences between systems can be",0,,False
211,"detected with high confidence. A high sensitivity can be seen as a necessary, though not sufficient, condition for a good evaluation measure. Discriminative power is defined as the percentage of pairs of runs that are found to be statistically significantly different by some significance test. As per Sakai, we use a two-tailed paired bootstrap test with 1000 bootstrap samples per pair of systems. Our results are displayed in Table 2. As measured by discriminatory power, we see that RIC is at least as sensitive, if not more so, than AP and nDCG.",1,AP,True
212,6. UPPER BOUND ON METASEARCH,0,,False
213,"In Section 5, we defined an evaluation measure in terms of mutual information. One advantage of this approach is that collections of systems can be evaluated directly by considering the output of their random variables jointly, without their needing to be combined. For a collection of systems, denoted S1 through Sn, the relevance information correlation can be defined as",1,ad,True
214,"RIC(S1, . . . , Sn) ,"" I(RS1 , . . . , RSn ; Q)""",0,,False
215,(19),0,,False
216,"In this section, we will show that this produces a natural upper bound on metasearch performance that is consistent with other upper bounds appearing in the literature.",0,,False
217,"We compare our upper bound against those of Montague [19]. Montague describes metasearch algorithms as sorting functions whose comparators, as well as the documents to be sorted, are defined in terms of collections of input systems.",0,,False
218,688,0,,False
219,"By also using the QREL as input, these algorithms can estimate upper bounds on metasearch performance. These bounds range from the ideal performance that cannot possibly be exceeded by any metasearch algorithm, to descriptions of reasonable metasearch behavior that should be similar to the performance of any quality metasearch algorithm.",0,,False
220,Montague defines the following upper bounds on metasearch:,0,,False
221,"1. Naive: Documents are sorted by comparison of relevance judgments, i.e. the naive upper bound is created by returning all relevant documents returned by any system in the collection above any non-relevant document. Relevant documents not retrieved by any system are not ranked.",0,,False
222,"2. Pareto: If document A is ranked above document B by all systems, then document A is considered ""greater"" than document B. Otherwise, the documents are sorted by comparison of relevance judgments.",0,,False
223,"3. Majoritarian: If document A is ranked above document B by at least half of the systems, then document A is considered ""greater"" than document B. Otherwise, the documents are sorted by comparison of relevance judgments.",0,,False
224,"We will compare our direct joint evaluation with these upper bounds, and several metasearch algorithms commonly used as baselines in the IR literature: the CondorcetFuse metasearch algorithm [20], and the comb family of metasearch algorithms [25].",0,,False
225,TREC 8 ANZ MNZ,1,TREC,True
226,Condorcet Majoritarian,0,,False
227,Pareto Naive,0,,False
228, 0.221 0.587 0.519 0.552 0.657 0.788,0,,False
229, 0.330 0.764 0.689 0.735 0.836 0.931,0,,False
230,RMSE 0.481 0.351 0.362 0.340 0.044 0.039,0,,False
231,"Table 3: Correlation between joint distribution and metasearch algorithms (Kendall's  , Spearman's , root mean square error).",0,,False
232,"perform comparably to the Majoritarian bound, and the Naive bound is not appreciably better than the Pareto bound. If direct evaluation and the Naive bound are both reasonable estimates of the actual upper bound, then these results should be confirmed by Figure 3 and Table 3, as indeed they are. Note that there is almost no correlation between the joint evalution and the weakest metasearch algorithm, combANZ: combANZ does not approximate the upper bound on metasearch. The correlation improves as the quality of the metasearch algorithm improves, and it does so in a manner consistent with Montague. The correlations between the joint evaluation and the output of combMNZ, CondorcetFuse, and the Majoritarian bound are similar; while they are still biased as estimators, the correlation is beginning to approach monotonicity. Finally, with a root mean square error of 0.039, the joint evaluation estimation of the upper bound is essentially identical to that of the Naive upper bound. If the Naive upper bound is a reasonable estimate of the upper bound on metasearch performance, then so is the joint evaluation of the input systems.",0,,False
233,"Figure 3: RIC of systems output by metasearch algorithms (Fusion System) versus RIC of systems computed directly (S1, . . . , S10) without combining.",0,,False
234,"We examined the direct evaluation and metasearch performance of collections of ten randomly selected systems. Experiments were performed on TREC 8 and 9, with both binary and graded relevance judgments. To conserve space, we only show the results from TREC 8. The results from TREC 9 were highly similar, both when using binary and graded relevance judgments.",1,TREC,True
235,"Figure 3 shows the RIC of the system output by a metasearch algorithm plotted against the joint RIC of the input systems, and Table 3 shows various measures of their correlation. Montague found that combANZ is inferior to CondorcetFuse and combMNZ, CondorcetFuse and combMNZ",0,,False
236,7. INFORMATION DIFFERENCE,0,,False
237,"In this section, we introduce a novel application of our probabilistic framework. Imagine that you are attempting to improve an existing ranker. On what basis do you decide whether or not your changes are beneficial? One typically evaluates both systems on a number of queries, and measures the difference in average performance. If one system outperforms the other, whether you have made an improvement is clear. But what happens when the systems perform similarly? It could be that your new system is essentially unchanged from your old system, but it is also possible that the two systems chose highly different document and just happened to have very similar evaluation scores. In the latter case, it may be possible to create a new, better system based on a combination of the two existing systems.",1,ad,True
238,"We propose to measure the magnitude of the difference between systems in their ranking of documents for which we have relevance information, rather than the magnitude of the difference between their performance. We denote this new quantity as the information difference between systems. Our definition of information difference is inspired by the Boolean Algebra symmetric difference operator as applied to information space (see Figure 4).",0,,False
239,"id(S1, S2) , I(S1; Q | S2) + I(S2; Q | S1)",0,,False
240,(20),0,,False
241,689,0,,False
242,System 1,0,,False
243,System 2,0,,False
244,QREL,0,,False
245,Figure 4: Information difference corresponds to the symmetric difference between the intersections of the systems with the QREL in information space (red portion of the Venn diagram).,0,,False
246,"As a preliminary validation of information difference, we analyzed the change in AP and information difference between pairs of systems submitted to TREC 8, selected at random. We expect the two to be somewhat directly correlated, since, in general, if two systems rank documents similarly, we would expect them to have similar AP. However, we expect that they will not be highly correlated, since we believe that information difference is much more informative. Our intuition is supported by Figure 5, which shows the magnitude of the change in AP on the horizontal axis, and the information difference on the vertical axis.",1,AP,True
247,Figure 5: Scatter plot of information difference and the magnitude of change in AP of random pairs of TREC 8 systems.,1,AP,True
248,"To demonstrate the utility of information difference, we sorted all the systems submitted to TREC 8 by AP and separated them into twenty equal-sized bins. By construction, each bin contained systems with small differences in performance. Our goal is to distinguish between similar and dissimilar systems within each bin. To this end, all systems within each bin were compared with one other (see Table 4). When the system pairs were sorted by their information difference, both systems in the first 27 pairs were submitted by the same group, whereas sorting by | AP| produced no discernible pattern. It is reasonable to assume that these systems were different instantiations of the same underlying",1,TREC,True
249,"technology. We can therefore conclude that information difference is able to determine whether systems with the same underlying performance are in fact similar, as desired.",0,,False
250,Rank,0,,False
251,1 2 3 4 5,0,,False
252,28 29 30 31 32,0,,False
253,System 1,0,,False
254,UB99T unc8al32 fub99tt nttd8al ibmg99a,0,,False
255,isa25t CL99SD ok8amxc tno8d4 uwmt8a2,0,,False
256,System 2,0,,False
257,UB99SW unc8al42 fub99tf nttd8alx ibmg99b,0,,False
258,...,0,,False
259,cirtrc82 CL99SDopt2 ok8alx MITSLStd uwmt8a1,0,,False
260,id,0,,False
261,0.010 0.012 0.017 0.023 0.027,0,,False
262,0.084 0.086 0.086 0.088 0.089,0,,False
263,| AP|,1,AP,True
264,0.005 0.002 0.000 0.002 0.012,0,,False
265,0.004 0.000 0.006 0.016 0.002,0,,False
266,"Table 4: The systems from TREC 8 were binned by average precision. Information difference and  AP were computed for all system pairs within each bin. Sorting by information difference, both systems in the first 27 pairs were submitted by the same group.",1,TREC,True
267,8. CONCLUSION,0,,False
268,"In this work, we developed a probabilistic framework for the analysis of information retrieval systems based on the correlation between a ranked list and the preferences induced by relevance judgments. Using this framework, we developed powerful information theoretic tools for better understanding information retrieval systems. We introduced four preliminary uses of our framework: (1) a measure of conditional rank correlation, information  , which is a powerful meta-evaluation tool whose use we demonstrated on understanding novelty and diversity evalution; (2) a new evaluation measure, relevance information correlation, which is correlated with traditional evaluation measures and can be used to (3) evaluate a collection of systems simultaneously, which provides a natural upper bound on metasearch performance; and (4) a measure of the similarity between rankers on judged documents, information difference, which allows us to determine whether systems with similar performance are actually different.",1,ad,True
269,"Our framework is based on the choice of sample space, probability distribution, and random variables. Throughout this work, we only used a uniform distribution on appropriate pairs of documents. However, not all document pairs are equal. The use of additional distributions is an immediate avenue for improvement that we intend to explore in future work. For example, a geometric distribution may be employed to force our evaluation tools to concentrate their attention at the top of a ranked list.",1,ad,True
270,"The primary limitation of our evaluation measure as implemented in this work is that it is only applicable to recalloriented retrieval tasks. In future work, we intend to develop a precision-oriented version that is applicable to web search. Given such a measure, judgments can be combined in the way systems were in our upper bound on metasearch. In that way, a small number of expensive to produce nominal relevance judgments, a somewhat larger number of somewhat less expensive preference judgments, and a gold-stander ranker could all be used simultaneously to evaluate systems.",0,,False
271,690,0,,False
272,"Finally, we intend to explore the application of information difference to the understanding of information retrieval models. For example, BM25 and Language Models have long been used as baselines in information retrieval experiments. On the surface, these two models appear to be completely different. And yet, the two share deep theoretical connections [33]. Using information difference, we can determine whether their theoretical similarities outweigh their superficial differences in terms of how they rank documents.",0,,False
273,9. REFERENCES,0,,False
274,"[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. Diversifying search results. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, WSDM '09, pages 5­14, New York, NY, USA, 2009. ACM.",0,,False
275,"[2] Chris Buckley and Ellen M. Voorhees. Retrieval evaluation with incomplete information. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '04, 2004.",0,,False
276,"[3] Christopher J.C. Burges. From ranknet to lambdarank to lambdamart: An overview. Technical Report MSR-TR-2010-82, Microsoft Research, 2010.",0,,False
277,"[4] Ben Carterette and Paul N. Bennett. Evaluation measures for preference judgments. In SIGIR, 2008.",0,,False
278,"[5] Ben Carterette, Paul N. Bennett, David Maxwell Chickering, and Susan T. Dumais. Here or there: preference judgments for relevance. In Proceedings of the IR research, 30th European conference on Advances in information retrieval, ECIR'08, 2008.",0,,False
279,"[6] Praveen Chandar and Ben Carterette. Using preference judgments for novel document retrieval. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, SIGIR '12, 2012.",0,,False
280,"[7] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM '09, pages 621­630, New York, NY, USA, 2009. ACM.",1,ad,True
281,"[8] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Gordon V. Cormack. Overview of the TREC 2010 Web Track. In 19th Text REtrieval Conference, Gaithersburg, Maryland, 2010.",1,TREC,True
282,"[9] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Ellen M. Voorhees. Overview of the TREC 2011 Web Track. In 20th Text REtrieval Conference, Gaithersburg, Maryland, 2011.",1,TREC,True
283,"[10] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ttcher, and Ian MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '08, pages 659­666, New York, NY, USA, 2008. ACM.",1,Novelty,True
284,"[11] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley-Interscience, 1991.",0,,False
285,"[12] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. An experimental comparison of click position-bias models. In Proceedings of the 2008",0,,False
286,"International Conference on Web Search and Data Mining, WSDM '08, pages 87­94, New York, NY, USA, 2008. ACM.",0,,False
287,"[13] Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer. An efficient boosting algorithm for combining preferences. J. Mach. Learn. Res., 4, December 2003.",0,,False
288,"[14] Peter B. Golbus, Javed A. Aslam, and Charles L.A. Clarke. Increasing evaluation sensitivity to diversity. In Journal of Information Retrieval, To Appear.",0,,False
289,"[15] Kalervo J¨arvelin and Jaana Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422­446, October 2002.",0,,False
290,"[16] Thorsten Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '02, 2002.",0,,False
291,"[17] M. G. Kendall. A New Measure of Rank Correlation. Biometrika, 30(1/2):81­93, June 1938.",0,,False
292,"[18] Alistair Moffat and Justin Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):2:1­2:27, December 2008.",0,,False
293,"[19] Mark Montague. Metasearch: Data Fusion for Document Retrieval. PhD thesis, Dartmouth College. Dept. of Computer Science, 2002.",0,,False
294,"[20] Mark Montague and Javed A. Aslam. Condorcet fusion for improved retrieval. In Proceedings of the eleventh international conference on Information and knowledge management, CIKM '02, 2002.",0,,False
295,"[21] Stephen E. Robertson, Evangelos Kanoulas, and Emine Yilmaz. Extending average precision to graded relevance judgments. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, SIGIR '10, 2010.",1,ad,True
296,"[22] Tetsuya Sakai. Evaluating evaluation metrics based on the bootstrap. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '06, 2006.",0,,False
297,"[23] Tetsuya Sakai. Alternatives to Bpref. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, 2007.",0,,False
298,"[24] Tetsuya Sakai and Ruihua Song. Evaluating diversified search results using per-intent graded relevance. In SIGIR, pages 1043­1052, 2011.",1,ad,True
299,"[25] Joseph A. Shaw and Edward A. Fox. Combination of multiple searches. In The Second Text REtrieval Conference (TREC-2), pages 243­252, 1994.",1,TREC,True
300,"[26] Ruihua Song, Min Zhang, Tetsuya Sakai, Makoto P. Kato, Yiqun Liu, Miho Sugimoto, Qinglei Wang, and Naoki Orii. Overview of the ntcir-9 intent task. In Proceedings of the 9th NTCIR Workshop, Tokyo, Japan, 2011.",0,,False
301,"[27] C. Spearman. The proof and measurement of association between two things. The American Journal of Psychology, 1904.",0,,False
302,"[28] E. M. Voorhees and D. Harman. Overview of the eighth text retrieval conference (TREC-8). In Proceedings of the Eighth Text REtrieval Conference (TREC-8), 2000.",1,TREC,True
303,691,0,,False
304,"[29] E. M. Voorhees and D. Harman. Overview of the ninth text retrieval conference (TREC-9). In Proceedings of the Ninth Text REtrieval Conference (TREC-9), 2001.",1,TREC,True
305,"[30] Emine Yilmaz and Javed A. Aslam. Estimating average precision with incomplete and imperfect judgments. In Proceedings of the 15th ACM international conference on Information and knowledge management, CIKM '06, 2006.",0,,False
306,"[31] Emine Yilmaz, Javed A. Aslam, and Stephen Robertson. A new rank correlation coefficient for information retrieval. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '08, 2008.",0,,False
307,"[32] Emine Yilmaz, Evangelos Kanoulas, and Javed A. Aslam. A simple and efficient sampling method for estimating AP and nDCG. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '08, 2008.",1,AP,True
308,"[33] Chengxiang Zhai and John Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, April 2004.",0,,False
309,10. APPENDIX,1,AP,True
310,Theorem,0,,False
311,1.,0,,False
312,"I(XR; XS) ,",0,,False
313,1+ 2,0,,False
314,log(1+,0,,False
315,)+,0,,False
316,1- 2,0,,False
317,log(1- ).,0,,False
318,Proof. Denote XR and XS as X and Y . Consider the following joint probability distribution table.,0,,False
319,Y -1 1 X -1 a b,0,,False
320,1 cd,0,,False
321,"Observe that: a + b + c + d , 1; C ,"" a + d, D "","" b + c, and therefore  "","" a + d - b - c; and since document pairs appear in both orders, a "", d and b , c.",0,,False
322,The joint probability distribution can be rewritten as follows.,0,,False
323,Y,0,,False
324,-1 1,0,,False
325,X -1,0,,False
326,C 2,0,,False
327,D 2,0,,False
328,1,0,,False
329,DC,0,,False
330,2,0,,False
331,2,0,,False
332,"Observe that the marginal probability P (X) , P (Y ) ,",0,,False
333,C 2,0,,False
334,+,0,,False
335,D 2,0,,False
336,",",0,,False
337,C 2,0,,False
338,+,0,,False
339,D 2,0,,False
340,",",0,,False
341,1 2,0,,False
342,",",0,,False
343,1 2,0,,False
344,.,0,,False
345,"I(X; Y ) ,"" KL(P (X, Y )||P (X)P (Y ))""",0,,False
346,"p(x, y)",0,,False
347,",",0,,False
348,"p(x, y) lg",0,,False
349,p(x)p(y),0,,False
350,"x,y",0,,False
351,1,0,,False
352,",",0,,False
353,"p(x, y) lg p(x, y) + p(x, y) lg",0,,False
354,.,0,,False
355,p(x)p(y),0,,False
356,"x,y",0,,False
357,"x,y",0,,False
358,"Since P (X, Y ) ,",0,,False
359,C 2,0,,False
360,",",0,,False
361,D 2,0,,False
362,",",0,,False
363,C 2,0,,False
364,",",0,,False
365,D 2,0,,False
366,"and P (X)P (Y ) ,",0,,False
367,1 4,0,,False
368,",",0,,False
369,1 4,0,,False
370,",",0,,False
371,1 4,0,,False
372,",",0,,False
373,1 4,0,,False
374,",",0,,False
375,"I(X, Y ) , 2 · C lg C + 2 · D lg D + 2 · C lg 4 + 2 · D lg 4",0,,False
376,22,0,,False
377,22,0,,False
378,2,0,,False
379,2,0,,False
380,", C lg C + D lg D + 2C + 2D",0,,False
381,2,0,,False
382,2,0,,False
383,", C lg C - C + D lg D - D + 2C + 2D",0,,False
384,", C lg C + D lg D + 1",0,,False
385,", C lg C + (1 - C) lg(1 - C) + 1",0,,False
386,"Since C + D , 1 and  ,"" C - D, we have that  "","" 2C - 1,""",0,,False
387,"C,",0,,False
388,1+ 2,0,,False
389,"and D , 1 - C ,",0,,False
390,1- 2,0,,False
391,.,0,,False
392,"In terms of C, if H2 represents the entropy of a Bernoulli",0,,False
393,"random variable ,3",0,,False
394,"I(X; Y ) , -H2(C) + 1",0,,False
395,"1+ , -H2 2 + 1",0,,False
396,1+ 1+ 1- 1-,0,,False
397,",",0,,False
398,lg,0,,False
399,+,0,,False
400,lg,0,,False
401,+1,0,,False
402,2,0,,False
403,2,0,,False
404,2,0,,False
405,2,0,,False
406,", 1 +  lg(1 +  ) - 1 +  + 1 -  lg(1 -  )",0,,False
407,2,0,,False
408,2,0,,False
409,2,0,,False
410,-1- +1 2,0,,False
411,1+,0,,False
412,1-,0,,False
413,",",0,,False
414,lg(1 +  ) +,0,,False
415,lg(1 -  ),0,,False
416,2,0,,False
417,2,0,,False
418,"Corollary 1. For two ranked lists R and S, I(XR; XS) ,",0,,False
419,1 - H2(K) where K,0,,False
420,",",0,,False
421,1- 2,0,,False
422,is the normalized Kendall's ,0,,False
423,distance between R and S.,0,,False
424,"3H2(p) , -p lg p - (1 - p) lg(1 - p). Note that H2(p) , H2(1 - p).",0,,False
425,692,0,,False
426,,0,,False

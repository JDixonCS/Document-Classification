,sentence,label,data,regex
0,An Information-Theoretic Account of Static Index Pruning,0,,False
1,Ruey-Cheng Chen,0,,False
2,"National Taiwan University 1 Roosevelt Rd. Sec. 4 Taipei 106, Taiwan",0,,False
3,rueycheng@turing.csie.ntu.edu.tw,0,,False
4,Chia-Jung Lee,0,,False
5,University of Massachusetts 140 Governors Drive,1,Gov,True
6,"Amherst, MA 01003-9264",0,,False
7,cjlee@cs.umass.edu,0,,False
8,ABSTRACT,0,,False
9,"In this paper, we recast static index pruning as a model induction problem under the framework of Kullback's principle of minimum cross-entropy. We show that static index pruning has an approximate analytical solution in the form of convex integer program. Further analysis on computation feasibility suggests that one of its surrogate model can be solved efficiently. This result has led to the rediscovery of uniform pruning, a simple yet powerful pruning method proposed in 2001 and later easily ignored by many of us. To empirically verify this result, we conducted experiments under a new design in which prune ratio is strictly controlled. Our result on standard ad-hoc retrieval benchmarks has confirmed that uniform pruning is robust to high prune ratio and its performance is currently state of the art.",1,ad-hoc,True
10,Categories and Subject Descriptors,0,,False
11,H.1.1 [Systems and Information Theory]: Information theory; H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.4 [Systems and Software]: Performance evaluation (efficiency and effectiveness),0,,False
12,Keywords,0,,False
13,Static index pruning; principle of minimum cross-entropy; model induction; uniform pruning,0,,False
14,1. INTRODUCTION,1,DUC,True
15,"Kullback discussed one famous problem in his seminal work [14] about inducing a probability measure based on some previous measurement. When one has some initial hypothesis about a system and seeks to update this measurement incrementally, she needs to choose a new hypothesis from a set of feasible measures that best approximates her current belief. Here, the difficulty lies in defining the notion of closeness in the probability space. While at the time this was an important issue in everyday probabilistic modeling, a genuine solution had yet to come.",1,ad,True
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
17,"To answer the question he had raised, Kullback introduced a method called ""minimum discrimination information,"" or in the recent literature known as the principle of minimum cross-entropy. This approach has later become one of the most influential inductive principles in statistics, and also has benefited numerous fields, including some subareas in information retrieval [15,20]. Kullback's solution was simple and elegant: One shall choose a measure that most closely resembles the previous measurement in terms of KullbackLeibler divergence. Specifically, this is equivalent to solving the following optimization problem, given some prior measure p and a set of feasible measures F:",1,ad,True
18,minimize subject to,0,,False
19,D(q||p) q  F.,0,,False
20,(1),0,,False
21,"In this paper, we apply this induction framework to a classic problem in information retrieval, called static index pruning. Static index pruning is a task that reduces the index size for improving disk usage and query throughput [1]. Size reduction is done by removing index entries. Generally, the aim in static index pruning is to find a subset of index entries that best approximates the full index in terms of retrieval performance. This aspect, as we will show later, is closely related to model induction.",0,,False
22,"One key assumption in this paper is that an inverted index is a nonparametric, conditional distribution of document D given term T , i.e., p(D|T ). This follows directly from Chen et al.'s definition [10], which allows us to measure the resemblance between two versions of inverted indexes the way we do probability distributions. Here, the following definitions put static index pruning into the framework of Equation (1):",0,,False
23,· The prior distribution p is defined as the full (unpruned) inverted index.,0,,False
24,"· The set of feasible hypotheses F contains all the possible pruned indexes of p that have reached some given prune ratio . In other words, each element q  F is a pruned version of the original inverted index p.",0,,False
25,"This conception marks the very beginning of our quest for developing an efficient solution of static index pruning. Through analysis, we first show that static index pruning is essentially a combinatorial optimization problem. Nevertheless, in Section 3, we manage to obtain a weaker analytical solution that is practically operable by trading off some mathematical rigor. We found that, under appropriate assumptions, static index pruning reduces to a convex integer program. But this is not a good solution in general, since the number of variables in the convex program is linear to the",1,ad,True
26,163,0,,False
27,"number of postings in the inverted index, which may easily exceed a few millions on any medium-sized text collection. That means this solution does not scale at all, even with the latest super-efficient convex solver.",0,,False
28,"We further attacked this problem using an alternative approach, called surrogate modeling. We created a surrogate problem that is easier to solve. As we will show in later sections, this analytical solution has pointed us to a general version of a simple pruning method called uniform pruning. Sharp-eyed readers might notice that uniform pruning is by no means a new invention. Uniform pruning was originally introduced to static index pruning in Carmel et al.'s paper as a baseline approach [9]. In a preliminary experiment, Carmel et al. compared this method with their termbased pruning method. Using TF-IDF as the score function, they found that, even though term-based method performed slightly better, in general the performance for both approaches was roughly comparable. While this was indeed a very interesting finding, the exploration was discontinued as they went ahead to study other important issues.",1,ad,True
29,"To the best of our knowledge, since then uniform pruning has not been studied in any follow-up work. It is easy to see why this has been the case. The lack of control on one experiment variable, prune ratio, has made the performance result difficult to interpret. When we make comparisons between methods, this variable needs to be strictly controlled so that the comparisons make sense. Nevertheless, very few in the previous work adopted this design. As a result, there was no obvious way to conduct any form of significance testing to static index pruning. Without serious scrutiny--by which we mean significance assessment--it is only reasonable to dismiss uniform pruning, for that it seemed like an ad-hoc and maybe inferior approach.",1,ad,True
30,"In our study, the rediscovery of uniform pruning has gained us a second chance to rethink this issue. Our answer was a redesigned empirical study, in which prune ratio for each experimental method is strictly controlled to minimize the experimental error, and the performance is analyzed using multi-way repeated-measure analysis of variance. As we will shortly cover, the experiment result suggests that uniform pruning with Dirichlet smoothing significantly outperformed the other term-based methods under diverse settings.",0,,False
31,"The rest of the paper is structured as follows. Section 2 covers an overview to static index pruning and the relevant research. In Section 3, we motivate static index pruning in the minimum cross-entropy framework and show that the analytical solution leads to the uniform pruning method. An empirical study is given in Section 4. We put the theoretical and empirical evidence together and discuss the implication in Section 5. Section 6 delivers the concluding remarks.",1,ad,True
32,2. RELATED WORK,0,,False
33,"In the coming subsections, we briefly review the literature and discuss the recent development of static index pruning. Following an overview, some notable pruning methods will be treated in slightly more details. Note that this is only aimed at providing enough background knowledge for the reader. A complete coverage is not attempted here.",1,ad,True
34,2.1 Overview,0,,False
35,The idea of static index pruning first appeared in the groundbreaking work of Carmel et al. [9] and has since garnered much attention for its implication to Web-scale re-,0,,False
36,"trieval [8, 11]. Static index pruning is all about reducing index size--by removing index entries from the inverted index. This technique was proposed to mitigate the efficiency issue caused by operating a large index, for that a smaller index loads faster, occupies less disk space, and has better query throughput. But since only partial term-document mapping is preserved, a loss in retrieval performance is inevitable.",1,ad,True
37,"Much effort has been driven towards developing importance measures of individual index entries, so that one can easily prioritize index entries on their way out of the index. Many such measures have been proposed and tested in various retrieval settings. One simple example is impact, the contribution of a term-document pair to the final retrieval score [8, 9]. Other approaches in this line include probability ranking principle (PRP) [7], two-sample two proportion (2P2N) [19], and information preservation (IP) [10]. Some measures assess only term importance [6], so the corresponding pruning algorithms can only choose between keeping the entire term posting list or not at all. Some others assess only documents importance [21].",0,,False
38,2.2 Methodologies,0,,False
39,"Term-based pruning (or term-centric pruning) is proposed by Carmel et al. [9]. It was so named because it attempts to reduce the posting list for each term in the index. The basic idea is to compute a cutting threshold for each term, and throw away those entries with smaller impact values. Since the cutting threshold depends on some order statistics (i.e., the k-th largest impact value) about the posting list, term-based pruning is less efficient than the other methods.",0,,False
40,"In contrast to the aforementioned term-centric approach, document-centric pruning seeks to reduce the posting list for each document. Bu¨ttcher and Clarke [8] considered the contribution for term t to the Kullback-Leibler divergence D(d||C) between document d and the collection model C. This quantity is used to measure the importance of a posting. Analogously, for each document, a cutting threshold has to be determined based on some order statistics.",0,,False
41,"There are also other pruning strategies that focus on removing an entire term posting list (whole-term) or an entire document (whole-document) all at once. Blanco and Barreiro [6] presented four term-importance measures, including inverse document frequency (idf), residual inverse document frequency (ridf), and two others based on term discriminative value (TDM). They adopted a whole-term pruning strategy. Analogously, Zheng and Cox [21] proposed an entropy-based measure in a whole-document pruning setting. Both parties have reported comparable performance to term-based pruning on some standard benchmark.",1,TD,True
42,"Blanco and Barreiro [7] developed a decision criterion based on the probability ranking principle [18]. The idea is to take every term in the index as a single-word query and calculate the odd-ratio of relevance p(r|t, d)/p(r|t, d). This quantity is used in prioritizing all the term-document pair. Since there is only one cutting threshold determined globally, the implementation is relatively easy and efficient.",0,,False
43,"Thota and Carterette [19] used a statistical procedure, called two-sample two-proportion (2P2N), to determine if the occurrence of term t within document d is significantly different from its occurrence within the whole collection. Chen et al. [10] developed a method called information preservation. They suggest using the innermost summand of the conditional entropy H(D|T ) to measure predictive power",0,,False
44,164,0,,False
45,contributed by individual term-document pairs to the index model. This quantity is claimed easier to compute than the probability ranking principle.,0,,False
46,"Altingovde et al. [2] proposed an interesting query-view technique that works orthogonally with the aforementioned measure-based approaches. The general idea is to count the number of time a document falls within the top-k window of any given query collected from the query log. The count collected from a query is then evenly distributed to individual query terms. Thus the larger this number, the greater importance the posting is. The query view algorithm would later use this information to prune the entries.",0,,False
47,"Our work in this paper departs from the previous effort in three major ways. First, our approach is model-based, meaning that we infer a pruned model as a whole rather than partially. This is a novel approach in contrast to all the previous methods. Second, other information-theoretic approaches, such as Zheng and Cox [21] and Chen et al. [10], focused on minimizing the loss of information, while ours focused on minimizing the divergence from the full index. These are entirely different concepts in information theory. Three, our result on the uniform pruning method is more general than Carmel et al.'s description because we considered the query model p(t). Our take of uniform pruning is a weighted version, which may be useful when such a query model (e.g., session logs) is available.",0,,False
48,3. MINIMUM CROSS-ENTROPY AND STATIC INDEX PRUNING,0,,False
49,3.1 Problem Definition,0,,False
50,"Let us first develop some notation for describing an inverted index. Let T denote the set of terms and D denote the set of documents. We define an index entry (posting) as a 3-tuple of the form (t, d, n), where t  T , d  D, and n  N+ (i.e., n is a positive integer.) This means that ""term t appears n times in document d."" We further consider an inverted index as a probabilistic model p(D|T ; ) that takes a set of index entries  as parameters. This model is therefore nonparametric because the number of its parameters is not fixed. For brevity, in this paper we sometimes abuse the notation and use one symbol, e.g., , to represent both a distribution and its parameters.",0,,False
51,"In static index pruning, one seeks to induce a pruned model  from a full model 0 such that the following two constraints are satisfied: (i)  is a subset of the full model 0, and (ii) the size of  is 1 -  times the size of 0. Here, 0 <  < 1 denotes the prune ratio. Note that these constraints only specify what we need as the output from static index pruning, not how pruning shall be done. As there are exponentially many ways to prune an index down to a given ratio, it is natural to ask how does one engineer this decision to avoid excessive performance loss.",0,,False
52,"Now, to illustrate this point, let us assume the existence of a function g() that measures the retrieval performance of model . With this hypothetical construct, we formally define static index pruning as follows.",0,,False
53,maximize g(),0,,False
54,subject to   0,0,,False
55,(2),0,,False
56,||/|0| reaches 1 - .,0,,False
57,"It is not difficult to envision static index pruning being formulated this way, as a constrained optimization problem.",0,,False
58,"For now, we shall focus on estimation of this hypothetical function. The conventional approach, as discussed in Section 2.2, is to devise an importance measure to take the role of g(·), which is expected to capture certain properties of an index relevant to retrieval performance. Yet one caveat is that sometimes we risk being arbitrary: The importance measure may only be empirically tested and does not necessarily come with any theoretical guarantee.",0,,False
59,"One simple idea that we had failed to see casted away all these doubts. We noticed the similarity between this formulation and Kullback's famous induction framework. As we replace g(·) in Equation (2) with the negative KullbackLeibler divergence (KL divergence), the static index pruning problem reduces to a model induction problem, written in a minimization form:",1,ad,True
60,minimize D(||0),0,,False
61,subject to   0,0,,False
62,(3),0,,False
63,||/|0| reaches 1 - .,0,,False
64,"In the following subsections, we shall develop a procedure to practically solve this optimization problem. For brevity, we write p(·) and p0(·), respectively, to denote the models parametrized by inverted indexes  and 0. The probability measures that we consider here are conditional distributions of D given T . To make this explicit, we define:",0,,False
65,D(||0)  D(p(D|T )||p0(D|T )).,0,,False
66,(4),0,,False
67,3.2 Assumptions,0,,False
68,"Before diving into the full analysis, we need to make explicit two important assumptions.",0,,False
69,"Assumption 1 (Query and Index Models). We can separate a joint distribution of D and T into a product of two models: (1) a distribution of T , called the query model, and (2) a conditional distribution of D given T , called the index model. We assume there is only one query model q(t) and it is independent of the index model in use. In other words, we have:",1,Query,True
70,"p(d, t) ,"" p(d|t)q(t), p0(d, t) "", p0(d|t)q(t).",0,,False
71,"Sometimes, we simply write p(t) or p0(t) to denote the query model when the meaning is clear in the context.",0,,False
72,Assumption 1 simply states that the query model q(t) (or p(t)) has to be estimated from somewhere else. It makes little sense to infer a query model from the index.,0,,False
73,"Assumption 2 (Normalization Factor). Let p(t|d) and p0(t|d) be the conditional distributions of T given D for the induced and the original models, respectively. Let It,d be a binary variable that indicates whether an index entry (t, d, n) (for some n  N+) in the original model is retained in the induced model. We have p(t|d)  It,dp0(t|d)/Zd, where Zd is the normalization factor for document d.",0,,False
74,"In Assumption 2, we introduce a normalization factor Zd for each document d. As we shall address later, setting an appropriate value for Zd is the key step in the subsequent analysis. To correctly normalize p(t|d), we need to set:",1,ad,True
75,"Zd ,"" It,dp0(t|d).""",0,,False
76,t,0,,False
77,165,0,,False
78,"But this would make the resulting formula intractable, since the value of It,d depends on other variables in the same document, i.e., I·,d. To deal with this issue, we suggest setting Zd ,"" k for all d  D, where k > 0 is some constant. Using this normalization trick results in weak inference and inevitably sacrifices mathematical rigors. We want to emphasize that this is a necessary compromise, without which the following analysis would not have been possible.""",0,,False
79,3.3 Analysis,0,,False
80,"Now, we shall go ahead and analyze the objective function. First of all, let us write out the objective in full:",1,ad,True
81,D(p(D|T )||p0(D|T )) ,0,,False
82,"t,d",0,,False
83,"p(d,",0,,False
84,t),0,,False
85,log,0,,False
86,p(d|t) p0(d|t),0,,False
87,.,0,,False
88,(5),0,,False
89,"We use Assumption 1 to dissect the joint distribution p(d, t) into the product of the query model p(t) and the index model p(d|t). Applying Bayes Theorem to p(d|t) and p0(d|t) and assuming uniform p(d) and p0(d), we have the objective organized as follows:",0,,False
90,p(t),0,,False
91,t,0,,False
92,d,0,,False
93,p(t|d) d p(t|d),0,,False
94,log,0,,False
95,p(t|d) p0(t|d),0,,False
96,d d,0,,False
97,p0(t|d) p(t|d),0,,False
98,.,0,,False
99,(6),0,,False
100,"Observe that, since in this optimization framework we look for a subset of 0, we are essentially dealing with a combinatorial problem (""assignment problem""). Each index entry (t, d, n)  0 either stays within the induced model  or gets removed. This combinatorial nature is best characterized via the indicator variables I·,· in Assumption 2.",0,,False
101,"Let us now replace all the occurrences of p(t|d). Note that, under the setting Zd ,"" k (suggested), all the normalization factors cancel out. We have:""",0,,False
102,p(t),0,,False
103,t,0,,False
104,d,0,,False
105,"It,dp0(t|d) d It,d p0(t|d)",0,,False
106,log,0,,False
107,"It,d",0,,False
108,"d p0(t|d) . (7) d It,d p0(t|d)",0,,False
109,"As we separate the support of the inner summation over d into two subsets according to whether It,d is switched on, i.e., one over {d|It,d ,"" 1} and the other over {d|It,d "","" 0}, the latter sub-summation disappears since 0 log 0 "", 0. The resulting equation becomes:",0,,False
110,p(t),0,,False
111,t,0,,False
112,"d:It,d ,1",0,,False
113,d,0,,False
114,"p0(t|d) It,d p0(t|d)",0,,False
115,log,0,,False
116,d,0,,False
117,"d p0(t|d) It,d p0(t|d)",0,,False
118,.,0,,False
119,(8),0,,False
120,"Notice that the innermost logarithm does not depend on d anymore. We can therefore move that entire term out of the inner summation. From there, we have the inner summation over d canceled out. The equation is now written as:",0,,False
121,p(t) log,0,,False
122,t,0,,False
123,d,0,,False
124,"d p0(t|d) It,d p0(t|d",0,,False
125,),0,,False
126,.,0,,False
127,(9),0,,False
128,"We can get rid of the numerator, i.e., d p0(t|d), in the logarithm when minimizing this equation, because the numerator does not depend any combinatorial choice we make. Once again, we rewrite it as a maximization problem by taking the negation. The final form of static index pruning is expressed as the following:",0,,False
129,maximize,0,,False
130,"t p0(t) log d It,dp0(t|d)",0,,False
131,"subject to It,d is binary, for all (t, d, ·)  0, (10)",0,,False
132,"t,d It,d , (1 - )|0|.",0,,False
133,Input: a global threshold  begin,0,,False
134,"for t  T do for d  postings(t) do compute A(t, d) ,"" p(t)p(t|d) if A(t, d) <  then remove d from postings(t) end end""",0,,False
135,end end,0,,False
136,Algorithm 1: The weighted uniform pruning algorithm.,0,,False
137,"Equation (10) is in general ill-posed even though it can be solved with a convex integer program solver. This is because the number of index entries can easily exceed a few millions in any production retrieval system. Solving this exactly is only possible for very small test collections. To tackle this issue, we resort to a technique called surrogate modeling (or optimization transfer), which approximates the original objective using a majorization/minorization function that is analytically or numerically efficient to compute. See Lange et al. [16] for a comprehensive treatment.",0,,False
138,"To the best of our knowledge, there are two major approaches for inducing such surrogate models: taking the first-order Taylor approximation, or using the Jensen's inequality. In this paper, we stick with the second approach1. Recall that Jensen's inequality states that the following properties hold for any convex (or concave) function f :",0,,False
139,Ef (X)  f (EX) (f is convex),0,,False
140,Ef (X)  f (EX) (f is concave).,0,,False
141,"Let f be the logarithmic function. The original objective in our problem (Equation 10) now corresponds to the lefthand side Ef (X). Since the logarithmic function is concave, we have the surrogate model f (EX) an upper bound of the original objective:",0,,False
142,"maximize log It,dp0(t)p0(t|d),",0,,False
143,(11),0,,False
144,"t,d",0,,False
145,or equivalently:,0,,False
146,maximize,0,,False
147,"It,dp0(t)p0(t|d).",0,,False
148,(12),0,,False
149,"t,d",0,,False
150,"This surrogate model has a simple analytical solution: Sort the index entries according to weighted query likelihood, i.e., p(t)p(t|d), and keep only the top (1 - )N entries. It can be shown that a simple maneuver such as Algorithm 1, called weighted uniform pruning, guarantees to maximize the objective. This corresponds to a weighted version of Carmel et al.'s uniform pruning method. This algorithm would fall back to the unweighted form when we supply a uniform p(t) and a plug-in estimate of the query likelihood. Note that the plug-in approach is valid only when the score function is proportional to the true likelihood.",0,,False
151,"For simplicity, we take a very loose definition of query likelihood in this paper so as to cover the well-known BM25 function. As we shall present shortly, the empirical result",0,,False
152,"1In our case, the first-order Taylor expansion leads to an even more sophisticated objective.",1,ad,True
153,166,0,,False
154,shows that the performance of BM25 is no worse than that of a rigorously defined language model (with Jelinek-Mercer smoothing). What is left unsettled is how to estimate  given a target prune ratio . This issue is treated in Section 4.2.,0,,False
155,4. EXPERIMENT,0,,False
156,"Thus far, we have established the theoretical ground for uniform pruning. Our next quest is to find empirical evidence that supports this result. In the coming subsections, we shall briefly describe the experiment settings and present the experimental result in greater detail.",0,,False
157,4.1 Setup,0,,False
158,"We used three test collections in this experiment: TREC disks 4 & 5, WT2G, and WT10G. The first two collections are tested against topics 401-450 and the latter against topics 451-550. For each topic, we tested both short (title) and long (title + description) queries. Details about the benchmark are summarized in Table 1. All three collections were indexed using the Indri toolkit2. To preprocess the documents, we applied the porter stemmer and used the standard 411 InQuery stoplist. No additional text processing is done to the test collections.",1,TREC,True
159,"According to how index traversal is preferred, a pruning method can be either term-centric or document-centric. Since different traversal strategies rely on different index creation procedures, it is difficult to have both sets implemented in one place. For simplicity, in this experiment we focused only on term-centric methods. Specifically, we tested the following methods:",0,,False
160,"1. Uniform pruning (UP) [9]: This method is the subject of this experiment. In this experiment, we tested three variations of uniform pruning, each using a different score function. These functions are BM25 (UP-bm25), language model using Dirichlet smoothing (UP-dir), and language model using Jelinek-Mercer smoothing (UP-jm). For BM25, we used the standard setting provided by Indri. For language models, we set µ , 2500 in Dirichlet smoothing and  , 0.6 for the JelinekMercer smoother.",0,,False
161,"2. Top-k term-centric pruning (TCP) [9]: We set k ,"" 10 as suggested to maximize the top-10 precision and used BM25 as the score function. Note that other score functions such as language models may also apply to this pruning method. Here, we simply comply with the previous work.""",0,,False
162,3. Probability ranking principle (PRP) [7]:,0,,False
163,"p(r|t, d) p(r|t, d)",0,,False
164,p(t|D)p(r|D) p(t|r)(1 - p(r|D)),0,,False
165,.,0,,False
166,"As suggested, we use the following equations to estimate these probabilities:",0,,False
167,"p(t|D) ,"" (1 - )pML(t|D) + p(t|C), (13)""",0,,False
168,p(r|D),0,,False
169,",",0,,False
170,1 2,0,,False
171,+,0,,False
172,1 10,0,,False
173,tanh,0,,False
174,dl,0,,False
175,- Xd Sd,0,,False
176,",",0,,False
177,(14),0,,False
178,"p(t|r) , p(t|C).",0,,False
179,(15),0,,False
180,2http://www.lemurproject.org/indri.php,0,,False
181,Collection Disks 4 & 5 WT2G WT10G,1,WT,True
182,# Documents 528k 247k 1692k,0,,False
183,Query Topics 401-450 401-450 451-550,1,Query,True
184,Table 1: Test collections and the corresponding query topics.,0,,False
185,"Note that dl is the document length. Xd and Sd respectively are the sample mean and sample standard deviation of document length. For query likelihood, we set  , 0.6.",0,,False
186,"4. Information preservation, with uniform document prior (IP-u) [10]:",0,,False
187,-,0,,False
188,p(t|d)p(d) d p(t|d)p(d),0,,False
189,log,0,,False
190,p(t|d)p(d) d p(t|d)p(d,0,,False
191,),0,,False
192,.,0,,False
193,"In this formula, the query likelihood p(t|d) is estimated using Jelinek-Mercer smoothing. Here, We set  , 0.6 and assumed uniform document prior p(d).",0,,False
194,"We are aware that document-length update may improve the TCP and PRP retrieval performance [5,7]. Nevertheless, in this study we did not implement this feature. This shall be addressed in the future work.",1,ad,True
195,4.2 Prune Ratio,0,,False
196,"In this experiment, we settled on 9 fixed prune levels at  ,"" 0.1, 0.2, . . . , 0.9. To control the prune ratio, comparison is only allowed between experimental runs at the same prune level. In each reference method, the true prune ratio depends on some parameter (e.g.,  in TCP and PRP), which we called the threshold parameter. To reduce the index down to the right size, we employed two different approaches to determine this cutting threshold:""",0,,False
197,"1. Sample percentile: Collect the prune scores on top of a sample of index entries and use the percentile estimates to determine the right cutting threshold. This is mostly useful when the prune score is globally determined. Here, we used Definition 8 from Hyndman and Fan [13] to estimate percentiles.",0,,False
198,"2. Bisection: Take an interval of feasible parameter values [a, b], and test-prune using the median value (a + b)/2. Return the current median if the test-prune reached the expected ratio; otherwise shrink the interval in half and repeat. This method is useful when the prune score for each index entry depends on the others in the same posting list, as in TCP.",0,,False
199,"Bisection requires several test-prune runs into the entire index and is therefore more time-consuming. Sample percentile needs only one pass through the index, but the resulting prune ratio can be less precise than that with the values learned using bisection. In this paper, we applied bisection to TCP to learn the parameter , and applied sample percentile to the rest of methods. Specifically, we used a sample size of 10% of the entire index. For either case, the prune ratio error is controlled to within ±0.2%.",0,,False
200,167,0,,False
201,MAP (t) 0.10,1,MAP,True
202,MAP (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,1,MAP,True
203,.1 .2 .3 .4 .5 .6 .7 .8 .9 160 157 152 145 139 134 127 118 095 159 158 154 148 144 140 135 127 102 160 157 153 149 145 143 142 137 120 160 158 153 146 140 133 126 110 085 156 152 148 142 133 123 110 088 045 156 153 148 140 135 123 107 092 046,0,,False
204,MAP (td) 0.00 0.10 0.20 0.00,1,MAP,True
205,MAP (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,1,MAP,True
206,.1 .2 .3 .4 .5 .6 .7 .8 .9 203 197 187 177 163 147 142 130 091 204 189 177 170 160 158 141 141 116 204 199 191 183 177 175 174 164 136 204 199 185 179 164 150 137 103 074 186 174 160 150 136 123 112 090 050 189 171 165 149 143 124 116 095 051,0,,False
207,P10 (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,0,,False
208,.1 .2 .3 .4 .5 .6 .7 .8 .9 260 256 252 246 245 236 209 201 174 259 258 254 239 226 225 204 192 168 261 257 254 248 249 242 241 236 222 261 259 253 249 243 237 225 190 168 256 250 239 224 201 182 157 125 107 257 248 243 214 202 193 164 133 106,0,,False
209,P@10 (t),0,,False
210,0.20,0,,False
211,Unpruned TCP UP-bm25 UP-dir,0,,False
212,0.10,0,,False
213,P10 (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,0,,False
214,.1 .2 .3 .4 .5 .6 .7 .8 .9 347 347 339 332 315 290 279 267 231 351 339 336 309 295 287 264 248 209 347 350 341 338 327 317 316 311 291 347 350 341 335 319 299 267 229 191 344 324 300 281 249 222 181 169 127 340 329 310 280 253 233 186 175 122,0,,False
215,P@10 (td) 0.10 0.20 0.30,0,,False
216,UP-jm PRP IP-u,0,,False
217,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,0,,False
218,"Figure 1: The overall performance result on WT10g. All the measurements are rounded to the 3rd decimal place, with preceding zero and decimal point removed. The best raw performance at each prune level is underlined. The unpruned baseline has achieved 0.160/0.204 (t/td) in MAP and 0.261/0.347 (t/td) in P@10.",1,WT,True
219,4.3 Retrieval Performance,0,,False
220,"We followed Blanco and Barreiro [7] for using BM25 as the post-pruning retrieval method. Retrieval performance is measured in mean average precision (MAP) and precisionat-10 (P@10). The result on the largest set WT10G is summarized in Figure 1 (see Figures 2 and 3 at the very end of this paper for results on the smaller sets). Each figure has four sets of measure-to-query-type combinations, and the result for each combination is given both as a table on the left and a plot on the right. These combinations from top to bottom, respectively, are MAP-t, MAP-td, P@10-t, and P@10-td. Table columns and x-axes in the plots indicate prune levels, from 0.1 to 0.9 (10% to 90%). Rows and curves indicate pruning methods.",1,MAP,True
221,"Our result shows that, at small prune levels ( 0.5), all these methods differ little in performance, and the difference at larger prune levels seems more evident. Both PRP and IP-u, whose performance was nearly identical, have consistently achieved the bottom performance in all settings. In general, the performance for the UP family and TCP is comparable, though UP-dir performed slightly better than the other. We noticed that the performance of UP-dir is also robust to high prune ratio. On WT10G, when tested under an extreme setting with 90% prune ratio, UP-dir still managed to retain 75% of the baseline MAP for short queries,",1,WT,True
222,"and 66.7% for long queries. Of the baseline P@10, UP-dir retained 85.1% for short queries and 83.9% for long queries. Under a less aggressive setting such as 50% prune ratio, UPdir have done even better by retaining 90.6% and 86.8% of baseline MAP, and 95.4% and 94.2% of baseline P@10, respectively for short and long queries.",1,MAP,True
223,4.4 Significance Tests,0,,False
224,"We further conducted an analysis of variance (ANOVA) to check if the performance difference is significant. Due to the unbalanced size of measurement, we tested each corpus independently. Here, we assume a fixed-effect, 4-way no interaction, repeated measure design, expressed as:",0,,False
225,"Yi,j,k,l ,"" ai + bj + ck + dl + i,j,k,l,""",0,,False
226,"where Yi,j,k,l is the measured performance, ai is the querytype effect, bj the prune-level effect, ck the method effect, and dl the topic effect, and i,j,k,l denotes the error.",0,,False
227,"The result is covered in Table 2. Each row indicates a measure-effect combination and each column a test corpus. Test statistics, such as degrees of freedom (DF) and F-values (F), are given for every test case. We used partial eta-square (p2) to measure the effect size [17]. We first ran an omnibus test to see if any main effect is significant. Of all three collections, all the main effects were tested significant for",0,,False
228,168,0,,False
229,Disks 4 & 5,0,,False
230,WT2G,1,WT,True
231,WT10G,1,WT,True
232,Response Main Effect DF,0,,False
233,MAP,1,MAP,True
234,"Query Type F(1, 5336)",1,Query,True
235,"F p2 DF 74.10 .01 F(1, 5336)",0,,False
236,F p2 DF,0,,False
237,F p2,0,,False
238,"42.57 .01 F(1, 10686) 192.25 .02",0,,False
239,"Prune Ratio F(8, 5336) 240.30 .26 F(8, 5336) 306.17 .31 F(8, 10686) 193.26 .13",0,,False
240,Method,0,,False
241,"F(5, 5336) 11.00 .01 F(5, 5336) 40.20 .04 F(5, 10686) 61.47 .03",0,,False
242,"Query Topic F(49, 5336) 885.35 .89 F(49, 5336) 335.89 .76 F(49, 10686) 422.46 .80",1,Query,True
243,P@10,0,,False
244,"Query Type F(1, 5336) 66.16 .01 F(1, 5336) 10.89 .00 F(1, 10686) 622.34 .06",1,Query,True
245,"Prune Ratio F(8, 5336) 105.00 .14 F(8, 5336) 133.98 .17 F(8, 10686) 122.43 .08",0,,False
246,Method,0,,False
247,"F(5, 5336) 20.34 .02 F(5, 5336) 44.06 .04 F(5, 10686) 71.01 .03",0,,False
248,"Query Topic F(49, 5336) 484.06 .82 F(49, 5336) 296.88 .73 F(49, 10686) 226.31 .68",1,Query,True
249,Table 2: The 4-way no-interaction ANOVA result. Each cell indicates a combination of performance measure,0,,False
250,"(row) and test collection (column). Degrees of freedom and F-values are given for testing all the main effects. Effect size is given in p2. In our experiment, all the main effects are significant for p < 0.001.",0,,False
251,MAP P@10,1,MAP,True
252,Disks 4 & 5,0,,False
253,Method Mean Group,0,,False
254,UP-bm25 .204 a..,0,,False
255,UP-dir,0,,False
256,.200 a..,0,,False
257,TCP,0,,False
258,.196 ab.,0,,False
259,UP-jm,0,,False
260,.191 .bc,0,,False
261,PRP,0,,False
262,.187 ..c,0,,False
263,IP-u,0,,False
264,.187 ..c,0,,False
265,UP-dir,0,,False
266,.433 a..,0,,False
267,TCP,0,,False
268,.433 a..,0,,False
269,UP-jm,0,,False
270,.424 a..,0,,False
271,UP-bm25 .417 a..,0,,False
272,PRP,0,,False
273,.392 .b.,0,,False
274,IP-u,0,,False
275,.389 .b.,0,,False
276,WT2G,1,WT,True
277,Method Mean,0,,False
278,UP-dir,0,,False
279,.223,0,,False
280,UP-bm25 .211,0,,False
281,TCP,0,,False
282,.204,0,,False
283,UP-jm,0,,False
284,.192,0,,False
285,IP-u,0,,False
286,.181,0,,False
287,PRP,0,,False
288,.179,0,,False
289,UP-dir,0,,False
290,.404,0,,False
291,TCP,0,,False
292,.385,0,,False
293,UP-jm,0,,False
294,.367,0,,False
295,UP-bm25 .359,0,,False
296,IP-u,0,,False
297,.322,0,,False
298,PRP,0,,False
299,.319,0,,False
300,Group a... .b.. .b.. ..c. ...d ...d a... ab.. .bc. ..c. ...d ...d,0,,False
301,WT10G,1,WT,True
302,Method Mean,0,,False
303,UP-dir,0,,False
304,.162,0,,False
305,UP-bm25 .151,0,,False
306,TCP,0,,False
307,.148,0,,False
308,UP-jm,0,,False
309,.145,0,,False
310,IP-u,0,,False
311,.129,0,,False
312,PRP,0,,False
313,.127,0,,False
314,UP-dir,0,,False
315,.286,0,,False
316,TCP,0,,False
317,.268,0,,False
318,UP-jm,0,,False
319,.265,0,,False
320,UP-bm25 .259,0,,False
321,IP-u,0,,False
322,.222,0,,False
323,PRP,0,,False
324,.219,0,,False
325,Group a.. .b. .b. .b. ..c ..c a.. .b. .b. .b. ..c ..c,0,,False
326,"Table 3: The overall result for Tukey's HSD test. For each combination of performance measure (row) and test collection (column), pruning methods are ordered in descending mean and tested for group difference. Methods that differ significantly do not share the same group label.",0,,False
327,"p < 0.001. Further analysis shows that query type and prune method have relatively small effect sizes, suggesting that query topic and prune ratio have much greater influence on the retrieval performance than the others do.",0,,False
328,"Post-hoc tests are then called for to examine the difference caused by different factor values. Since our experimental setting involves multiple comparison, we employed Tukey's honest significance difference (HSD) to control the overall Type I error [12]. Note that since Tukey's HSD is a one-way test, only one effect is tested in each run. In the following paragraphs, we summarize the HSD results for all the main effects. Here, since our focus is on the method effect, we shall briefly cover the other three for the sake of completeness.",1,hoc,True
329,Method Effect. Table 3 summarizes this HSD result for the,0,,False
330,"method effect. For each measure-corpus combination, we assigned group labels, e.g., ""a"" to ""d"", to individual methods based on the pairwise differences in their means. The difference between two methods is significant if and only if they share no common group label.",0,,False
331,"The result is briefly summarized as follows. First, the UP family and TCP consistently achieved top performance in both MAP and P@10 across different test settings. In the leading group, UP-dir delivers slightly better performance than the others. This is even more pronounced under the Web settings, in which UP-dir significantly outperformed the rest of methods in MAP (on both corpora) and in P@10 (on WT10G only). Second, the performance for the rest",1,MAP,True
332,"of UP family and TCP is in general comparable. Take UPbm25 and TCP. The performance difference between the two is subtle: UP-bm25 was shown superior in MAP on Disks 4 & 5 but inferior in P@10 on WT2G. Third, PRP and IP-u are inferior to all the other methods. This result is consistent with our analysis on the raw performance measurements.",1,MAP,True
333,Query Type Effect. Long queries (td) achieve better per-,1,Query,True
334,"formance than short queries (t), which is expected because short queries are less precise than longer ones. This difference is confirmed on all three test collections, and appears more evident in the largest set WT10G.",1,WT,True
335,Prune Ratio Effect. Small prune levels do better than large,0,,False
336,"ones in both metrics, which is also expected since more aggressive pruning results in less information in the index. According to the pairwise comparison made within the HSD test, this result is generally true except for a few small pairs such as 0.1-against-0.2. Specifically, WT10G has many such insignificant small pairs, suggesting that retrieval on larger Web collections is less sensitive to information loss.",1,ad,True
337,Topic Effect. The result is difficult to interpret due to the,0,,False
338,"size of topic pairs, e.g., topics 451-551 on WT10G has produced 4950 such pairwise comparisons. In general, only a small number of queries have significantly deviated from the average performance, meaning that most queries are designed to be about equally difficult.",1,WT,True
339,169,0,,False
340,5. DISCUSSION,0,,False
341,"The experiment result for uniform pruning is generally in line with our understanding to impact, much of this was contributed by the previous work in index compression and dynamic pruning. Since many ideas come from the same outlet in the indexing pruning community, it is no surprise that uniform pruning is related to many existing impact-based methods. For example, Anh et al. [3] concluded that impactsorted indexes combined with early termination heuristics can best optimize retrieval system performance. This technique is conceptually equivalent to uniform pruning. Further work in this line investigated impact-based pruning, an application of impact-sorting to dynamic query pruning [4]. And again, this is a dynamic version of uniform pruning. Adding to these results, our analysis shows that impactbased methods are good approximate solutions to the proposed model induction problem.",0,,False
342,"One further question that invites curious eyes is why Dirichlet smoothing worked so well with uniform pruning that it significantly outperformed all the other variations on our Web benchmark WT2G and WT10G. So far the answer is still unclear to us. Here, let us discuss a few possibilities:",1,WT,True
343,"· BM25 might be a poor approximation to the probability p(t|d) since the framework presented in this paper was tailored specifically for language models. While this may explain why BM25 was inferior to Dirichlet smoothing in our experiments, it does not tell us why the performance for Jelinek-Mercer smoother and for BM25 were comparable.",0,,False
344,"· Another possibility is that, since parameter optimization is lacking in our experiment, we might have failed in producing the most competitive result for BM25 and Jelinek-Mercer smoother. If this theory is true, score functions will need task-specific fine-tuning in their further use. But for that to make sense, one needs to point out in what major way the role of a score function in index pruning departs from that in ordinary ad-hoc retrieval. This may point to an interesting direction for future work, but based on the evidence collected so far this claim is difficult to verify.",1,ad-hoc,True
345,"With the argument given in Section 3 about the convex integer program, one may argue that it is important to prevent depleting any term posting since doing so would take the objective in Equation 10 to minus infinity. In other words, an additional constraint, called ""no depletion"", shall be added into the index pruning guideline. This is because, even though we do not attempt to solve the convex program, the constraint still needs to be enforced to guarantee that information loss is bounded. In this respect, it is necessary to adopt a top-k preserving strategy (i.e., skip any term posting that has less than k entries), such as the one in TCP, to avoid depleting term postings.",1,ad,True
346,6. CONCLUSION,0,,False
347,"In this paper, we review the problem of static index pruning from a brand new perspective. Given the appropriate assumptions, we show that this problem can essentially be tackled within a model induction framework, using the principle of minimum cross-entropy. The theory guarantees that the induced model best approximates the full model in terms",0,,False
348,"of probability divergence. We show that static index pruning can be written as a convex integer program. Yet exact inference, though possible as it might be, is generally computationally infeasible for large collections. So we further propose a surrogate model to address the computation issue, and show that uniform pruning is indeed an optimal solution to the formalism. To verify the correctness of our result, we conducted an extensive empirical study. The experiment was redesigned to take two factors, variable control and significance testing, into consideration. This setup has helped us reduce possible experimental bias or error.",1,ad,True
349,"Our result confirms that, when paired with the Dirichlet smoother, the performance of uniform pruning is state of the art. Significant improvement over the other methods were observed across diverse retrieval settings. Uniform pruning also exhibits an advantage in robustness with respect to large prune ratio. Specifically, our result on WT10G for short queries suggests that uniform pruning with the Dirichlet smoother retains at least 90% of the baseline performance at 50% prune ratio and 85% at 80% prune ratio. To the best of our knowledge, this is by far the best performance ever reported for static index pruning on the standard benchmark.",1,ad,True
350,"This research work has given rise to many technical issues, some have been addressed in Section 5 and some remain unsettled. It shall be interesting to see how uniform pruning responds to other test environments, such as different retrieval engines, corpora, or tasks. Document-length update and pseudo relevance feedback have been two landmark issues that we are ready to explore. Since we did not fine-tune the baseline performance, testing pruning methods against optimized, strong baseline shall provide more insight about this art. Besides all these possibilities, one promising direction is to extend the model induction idea to other type of structured data, such as lexicons or language models. Further investigation into the theory may shed us some light in the role that impact plays in different IR tasks.",1,ad,True
351,7. ACKNOWLEDGMENT,0,,False
352,"We would like to thank Wei-Yen Day, Ting-Chu Lin, and the anonymous reviewers for their useful comments.",0,,False
353,8. REFERENCES,0,,False
354,"[1] I. S. Altingovde, R. Ozcan, and O. Ulusoy. A practitioner's guide for static index pruning. In M. Boughanem, C. Berrut, J. Mothe, and C. Soule-Dupuy, editors, Advances in Information Retrieval, volume 5478 of Lecture Notes in Computer Science, chapter 65, pages 675­679. Springer Berlin / Heidelberg, Berlin, Heidelberg, 2009.",0,,False
355,"[2] I. S. Altingovde, R. Ozcan, and O. Ulusoy. Static index pruning in web search engines: Combining term and document popularities with query views. ACM Transactions on Information Systems, 30(1), Mar. 2012.",0,,False
356,"[3] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '01, pages 35­42, New York, NY, USA, 2001. ACM.",0,,False
357,[4] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In Proceedings of the,0,,False
358,170,0,,False
359,MAP (t) 0.00 0.10 0.20,1,MAP,True
360,MAP (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,1,MAP,True
361,.1 .2 .3 .4 .5 .6 .7 .8 .9 225 221 213 206 198 186 172 149 109 227 226 223 218 208 194 168 166 143 225 222 213 206 197 190 178 157 117 224 221 211 202 192 180 163 140 103 227 224 219 209 194 168 148 149 108 227 225 220 211 195 173 147 147 108,0,,False
362,MAP (td) 0.00 0.10 0.20,1,MAP,True
363,MAP (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,1,MAP,True
364,.1 .2 .3 .4 .5 .6 .7 .8 .9 251 246 239 232 219 201 184 162 117 250 246 235 229 222 202 179 176 158 251 247 239 232 220 210 196 173 133 250 246 238 228 216 194 173 148 110 253 246 228 212 195 168 148 146 129 252 240 230 215 195 173 145 138 125,0,,False
365,P@10 (t) 0.10 0.25 0.40,0,,False
366,P10 (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,0,,False
367,.1 .2 .3 .4 .5 .6 .7 .8 .9 438 434 434 430 428 426 424 384 326 438 440 436 438 438 412 370 358 294 438 434 432 428 426 434 416 398 344 438 434 432 428 424 416 410 370 318 436 442 444 452 428 400 340 284 204 438 440 440 450 430 406 338 278 190,0,,False
368,Unpruned TCP UP-bm25 UP-dir,0,,False
369,P10 (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,0,,False
370,.1 .2 .3 .4 .5 .6 .7 .8 .9 476 468 478 474 468 460 462 432 358 470 486 482 470 444 420 400 390 324 474 468 472 462 458 450 440 442 386 476 470 470 468 466 442 430 412 336 486 486 468 470 428 388 350 316 236 474 478 474 464 436 400 340 292 226,0,,False
371,P@10 (td) 0.10 0.25 0.40,0,,False
372,UP-jm PRP IP-u,0,,False
373,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,0,,False
374,"Figure 2: The performance result on Disk 4 & 5, with all measurements rounded to the 3rd decimal place, preceding zero and decimal point removed. The best raw performance at each prune level is underlined. The unpruned baseline has has achieved 0.228/0.256 (t/td) in MAP and 0.436/0.478 (t/td) in P@10.",1,MAP,True
375,"29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '06, pages 372­379, New York, NY, USA, 2006. ACM.",0,,False
376,"[5] R. Blanco and A. Barreiro. Boosting static pruning of inverted files. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 777­778, New York, NY, USA, 2007. ACM.",0,,False
377,"[6] R. Blanco and A. Barreiro. Static pruning of terms in inverted files. In G. Amati, C. Carpineto, and G. Romano, editors, Advances in Information Retrieval, volume 4425 of Lecture Notes in Computer Science, chapter 9, pages 64­75. Springer Berlin Heidelberg, Berlin, Heidelberg, 2007.",0,,False
378,"[7] R. Blanco and A. Barreiro. Probabilistic static pruning of inverted files. ACM Transactions on Information Systems, 28(1), Jan. 2010.",0,,False
379,"[8] S. Bu¨ttcher and C. L. A. Clarke. A document-centric approach to static index pruning in text retrieval systems. In Proceedings of the 15th ACM international conference on Information and knowledge management, CIKM '06, pages 182­189, New York, NY, USA, 2006. ACM.",0,,False
380,"[9] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer. Static index pruning for information retrieval systems. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '01, pages 43­50, New York, NY, USA, 2001. ACM.",0,,False
381,"[10] R.-C. Chen, C.-J. Lee, C.-M. Tsai, and J. Hsiang. Information preservation in static index pruning. In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM '12, pages 2487­2490, New York, NY, USA, 2012. ACM.",0,,False
382,"[11] E. S. de Moura, C. F. dos Santos, D. R. Fernandes, A. S. Silva, P. Calado, and M. A. Nascimento. Improving web search efficiency via a locality based static pruning method. In Proceedings of the 14th international conference on World Wide Web, WWW '05, pages 235­244, New York, NY, USA, 2005. ACM.",1,ad,True
383,"[12] D. Hull. Using statistical testing in the evaluation of retrieval experiments. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '93, pages 329­338, New York, NY, USA, 1993. ACM.",0,,False
384,171,0,,False
385,0.00 0.10 0.20 0.30 0.00 0.10 0.20,0,,False
386,MAP (t),1,MAP,True
387,MAP (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,1,MAP,True
388,.1 .2 .3 .4 .5 .6 .7 .8 .9 248 242 233 222 208 195 176 143 090 249 250 249 236 225 213 188 133 108 247 240 234 226 207 195 198 181 139 247 239 229 220 199 173 143 107 080 253 249 237 226 196 166 114 092 063 250 247 240 225 205 164 126 081 074,0,,False
389,MAP (td),1,MAP,True
390,MAP (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,1,MAP,True
391,.1 .2 .3 .4 .5 .6 .7 .8 .9 288 278 258 241 222 206 183 147 096 288 276 260 237 226 209 193 151 122 290 279 268 258 238 225 224 209 161 289 281 262 245 217 187 149 113 078 284 262 233 214 189 166 119 093 067 284 263 236 216 190 161 124 087 079,0,,False
392,P@10 (t) 0.10 0.25 0.40,0,,False
393,P10 (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,0,,False
394,.1 .2 .3 .4 .5 .6 .7 .8 .9 416 410 408 398 400 380 374 338 268 414 420 418 386 380 384 340 238 210 414 402 402 404 380 386 392 374 332 414 404 402 402 378 348 336 306 248 418 418 408 394 362 342 250 126 144 410 408 400 390 380 338 268 138 126,0,,False
395,Unpruned TCP UP-bm25 UP-dir,0,,False
396,P10 (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,0,,False
397,.1 .2 .3 .4 .5 .6 .7 .8 .9 464 446 428 410 410 388 356 328 302 464 446 412 370 378 384 326 268 218 460 446 432 436 422 428 422 408 346 460 446 430 420 388 354 340 276 260 442 434 392 382 340 334 260 142 146 450 430 402 392 348 324 258 158 170,0,,False
398,P@10 (td) 0.10 0.25 0.40,0,,False
399,UP-jm PRP IP-u,0,,False
400,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,0,,False
401,"Figure 3: The performance result on WT2G. All measurements were rounded to the 3rd decimal place, and preceding zero and decimal point removed. The best raw performance at each prune level is underlined. The unpruned baseline has has achieved 0.249/0.293 (t/td) in MAP and 0.414/0.460 (t/td) in P@10.",1,WT,True
402,"[13] R. J. Hyndman and Y. Fan. Sample quantiles in statistical packages. The American Statistician, 50(4):361­365, Nov. 1996.",0,,False
403,"[14] S. Kullback. Information Theory and Statistics. Wiley, New York, 1959.",0,,False
404,"[15] J. D. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In W. B. Croft, D. J. Harper, D. H. Kraft, J. Zobel, W. B. Croft, D. J. Harper, D. H. Kraft, and J. Zobel, editors, SIGIR, SIGIR '01, pages 111­119, New York, NY, USA, 2001. ACM.",0,,False
405,"[16] K. Lange, D. R. Hunter, and I. Yang. Optimization transfer using surrogate objective functions. Journal of Computational and Graphical Statistics, 9(1), 2000.",0,,False
406,"[17] D. C. Montgomery. Design and analysis of experiments (6th ed.). Wiley, 2004.",0,,False
407,"[18] S. Robertson. The probability ranking principle in IR. In K. S. Jones and P. Willett, editors, Reading in Information Retrieval, chapter The probability ranking principle in IR, pages 281­286. Morgan",1,ad,True
408,"Kaufmann Publishers Inc., San Francisco, CA, USA, 1997.",0,,False
409,[19] S. Thota and B. Carterette. Within-Document Term-Based index pruning with statistical hypothesis,0,,False
410,"testing. In P. Clough, C. Foley, C. Gurrin, G. Jones, W. Kraaij, H. Lee, and V. Mudoch, editors, Advances in Information Retrieval, volume 6611 of Lecture Notes in Computer Science, chapter 54, pages 543­554. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.",0,,False
411,"[20] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In Proceedings of the tenth international conference on Information and knowledge management, CIKM '01, pages 403­410, New York, NY, USA, 2001. ACM.",0,,False
412,"[21] L. Zheng and I. J. Cox. Entropy-Based static index pruning. In M. Boughanem, C. Berrut, J. Mothe, and C. Soule-Dupuy, editors, Advances in Information Retrieval, volume 5478 of Lecture Notes in Computer Science, chapter 72, pages 713­718. Springer Berlin / Heidelberg, Berlin, Heidelberg, 2009.",0,,False
413,172,0,,False
414,,0,,False

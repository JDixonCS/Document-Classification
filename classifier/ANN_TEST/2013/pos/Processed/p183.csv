,sentence,label,data,regex
0,Fast Document-at-a-time Query Processing using Two-tier Indexes,1,Query,True
1,Cristian Rossi,0,,False
2,Univ. Federal do Amazonas,0,,False
3,"cristiManan.ianufos,rA@Mg, Bmraazilil.com",0,,False
4,Edleno Silva de Moura,0,,False
5,Univ. Federal do Amazonas,0,,False
6,"edlenMoa@naicuso,mAMp,.uBfraamzil.edu.br",0,,False
7,Andre Luiz Carvalho Altigran Soares da Silva,0,,False
8,Univ. Federal do Amazonas,0,,False
9,Univ. Federal do Amazonas,0,,False
10,"andreM@anicauosm,ApM.u, fBarmaz.iel du.br alti@Miacnoamusp,.AuMfa, mBr.aezdilu.br",0,,False
11,ABSTRACT,0,,False
12,"In this paper we present two new algorithms designed to reduce the overall time required to process top-k queries. These algorithms are based on the document-at-a-time approach and modify the best baseline we found in the literature, Blockmax WAND (BMW), to take advantage of a two-tiered index, in which the first tier is a small index containing only the higher impact entries of each inverted list. This small index is used to pre-process the query before accessing a larger index in the second tier, resulting in considerable speeding up the whole process. The first algorithm we propose, named BMW-CS, achieves higher performance, but may result in small changes in the top results provided in the final ranking. The second algorithm, named BMW-t, preserves the top results and, while slower than BMW-CS, it is faster than BMW. In our experiments, BMW-CS was more than 40 times faster than BMW when computing top 10 results, and, while it does not guarantee preserving the top results, it preserved all ranking results evaluated at this level.",1,ad,True
13,Categories and Subject Descriptors,0,,False
14,"H.4 [Information Systems Applications]: Miscellaneous; D.2.8 [Software Engineering]: Metrics--complexity measures, performance measures",0,,False
15,Keywords,0,,False
16,"Top-k Query Processing, Efficiency, Two-tier indexes",1,Query,True
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
18,1. INTRODUCTION,1,DUC,True
19,"Computing ranking of results by using information retrieval (IR) models is one of the core tasks of search systems. While search systems often index a massive number of documents, usually their users are not interested in an in-depth list of results related to a query, but rather to a small list of highly relevant documents that will satisfy their informational needs. Thus, part of the recent research related to search systems is aimed at improving the quality of the top results presented to users, instead of the overall quality of the presented list. This focus on a narrower set of high quality results has led to the development of a number of technologies to improve the efficiency of methods to compute the top results in search systems.",1,ad,True
20,"When determining the best results for a given query, a search system usually deploys a number of different sources of relevance evidence. For instance, web search engines use information such as titles of the pages, URL tokenization, and link analysis, among others. These sources of relevance evidence may be combined using a myriad of approaches, such as the adoption of learning to rank techniques. Even in these cases, the initial process of computing the ranking consists of applying a basic IR model, such as BM25 [11] or the Vector Space Model [12], to compute an initial rank of top results, typically limited to the size of just about one thousand documents [5].",1,ad,True
21,"In this paper we propose two new algorithms which reduce the overall time required to compute the final query ranking. The algorithms modify the best baseline we found in the literature, the Blockmax WAND (BMW) [6], to take advantage of a two-tiered index. The first algorithm we proposed, named BMW-CS, from BMW using the first tier as a candidate selector, uses the first tier to select candidate documents that are taken into account to compute the final ranking when processing the second tier. It achieves higher performance, but may result in small changes in the top results provided in the final ranking. In BMW-CS, the entries of the first tier are not present in the second tier. The second algorithm, named BMW-t, from BMW using the first tier as a threshold selector, preserves the top results and, while slower compared to BMW-CS, it is faster than BMW. The first tier is used only to compute a safe initial threshold to be adopted when processing the second tier, thus allowing",1,ad,True
22,183,0,,False
23,a slightly faster query processing when compared to BMW. The price paid is that the second tier in BMW-t should contain the full index and the first tier is an extra index.,0,,False
24,"While there were previous approaches based on using a smaller index tier to improve efficiency, our proposed algorithms are designed to take advantage of dynamic pruning techniques, not to minimize the effort of processing answers from the first tier, but to reduce the time required to read and process entries from the second tier.",1,ad,True
25,"The remainder of this article is structured as follows. Section 2 presents the background and related research necessary to better understand the proposed methods. Section 3 presents our methods, BMW-CS and BMW-t. Section 4 presents the experimental results. Finally, Section 5 presents the conclusion and prospective future work.",0,,False
26,2. BACKGROUND AND RELATED WORK,0,,False
27,"Usually, most of the data needed by a search system to process user queries is stored in data structures known as inverted files [3]. They contain, for each term t, the list of documents where it occurs and an impact factor, or weight, associated with each document. This list of pairs of document and term impacts is called the inverted list of t and it is used to measure the relative importance of terms in the stored documents. Each document is represented in these lists by a value named document id, referred to as docId in this article. The inverted files may become huge in some systems, thus they are usually stored in a compressed format.",0,,False
28,2.1 TAAT approach,0,,False
29,"In a query processing approach named Term-At-A-Time (TAAT), the inverted lists are sorted by term impact in nonincreasing order. The query results are obtained by sequentially traversing one inverted list at a time. As an advantage, we can mention the fact that a sequential access behavior to inverted lists may speed up the process.",1,ad,True
30,"As the main disadvantage, we can mention that this strategy requires the usage of large amounts of memory to store partial scores achieved by each document when traversing each inverted list. These partial scores should be stored to accumulate the score results obtained by each document when traversing each inverted list. The final ranking can be computed only when all the inverted lists are processed. Several authors proposed methods to discard partial scores, thus reducing the amount of memory required to process queries in the TAAT mode [15, 2, 1]. Despite the significant reduction of required memory for query processing, the space required to store the partial scores remains one of the drawbacks of the TAAT query processing approach.",1,ad,True
31,"Anh and Moffat [1] studied the application of dynamic pruning over inverted indexes where the entries are sorted by impact, thus adopting a TAAT approach. In their work, a first phase processes blocks containing entries with higher impact in a disjunctive mode. Once all documents that might be present in the top-k results, k being a parameter, are found, the method starts a second phase applying a conjunctive mode query processing where only documents included as results by the first phase are considered. They also present a modified version of their algorithm, named as method B, where the top results may be changed, thus giving approximate results. In method B, only a percentage of the results obtained in the first phase are taken into account",1,ad,True
32,"in the second phase. This modified version results in even faster query processing, but does not guarantee top k results of the ranking will not be modified.",0,,False
33,"Strohman and Croft [15] proposed a new method for efficient query processing when documents are stored in main memory that modifies the method presented by Anh and Moffat [1]. In their proposal, a dynamic pruning is applied in each phase of query processing over the candidate documents with the goal of obtaining the final query results without requiring a full evaluation of all candidates. The query processing is performed over impact sorted inverted lists and the impact values are discretized in a small range of integer values.",0,,False
34,2.2 DAAT approach,0,,False
35,"Another approach to process queries is to adopt a DocumentAt-A-Time (DAAT) query processing. In this alternative, the inverted lists are sorted by docIds, which allows the algorithms to traverse all the inverted lists related to a query in parallel. As a consequence, the final scores of documents can be computed while traversing the lists and the system may store only the top results required by the user, so the memory requirements are fairly smaller. On the other hand the fragmented access may slow down the query processing and, since the inverted lists are sorted by docIds, important entries are spread along the inverted lists, making the pruning of entries more complex in this query processing approach.",1,ad,True
36,"As in the TAAT approach, several authors have presented algorithms and data structures to accelerate the query processing in the DAAT approach. For instance, data structures to allow fast jumps in the inverted lists, named as skiplists [7], are adopted to accelerate the query processing. Skiplists divide the inverted lists into blocks of entries and provide pointers for fast access to such blocks, so that a scan in the skiplist determines in which block a document entry may occur, if it does, in the inverted list.",1,ad,True
37,"The problem of efficiently computing the ranking of results for a given user query has been largely addressed in the literature in several research articles. We here detail the ones closer to our research, focusing on DAAT, which is the approach adopted by our algorithms.",1,ad,True
38,2.2.1 WAND,0,,False
39,"Broder et al [4] proposed a successful strategy for query processing, known as WAND, which allows fast query processing for both conjunctive and disjunctive queries, with the possibility of configuring the method for preserving the top k documents in the query answer or not. In the case where the top answers are not guaranteed to be preserved, the method leads to even faster query processing.",1,ad,True
40,"WAND processes the queries using DAAT approach, so the inverted lists of the query terms are traversed in parallel. A heap of scores is created to keep the top k documents with larger scores at each step of the query processing, k being the number of documents requested to the search system. The smaller score in the heap at each moment is taken as a discarding threshold to accelerate the query processing. A new document is evaluated and inserted in the heap only if it has a score higher than this discarding threshold.",0,,False
41,"The WAND has a two level evaluation approach. In the first level, a document pivot has its maximum possible score evaluated using the information of maximum score of each",0,,False
42,184,0,,False
43,"list where the document may occur. If the maximum possible score is greater than the actual discarding threshold, the document has its actual score evaluated. Otherwise, the document is discarded and a new pivot is selected.",0,,False
44,(matex rsmcore),0,,False
45,t1,0,,False
46,...,0,,False
47,(7),0,,False
48,60 70 ...,0,,False
49,t2,0,,False
50,(8),0,,False
51,... 40 90 ...,0,,False
52,t3,0,,False
53,(5),0,,False
54,...,0,,False
55,50 95 ...,0,,False
56,"Figure 1: Inverted lists when processing query with terms t1, t2, and t3. Marked entries are the ones currently processed.",0,,False
57,"At each moment in the DAAT query processing, there is a pointer to the next document to be processed in each inverted list associated with the query. For instance, if we have a query with terms t1, t2 and t3, there will be a pointer to the next document processed in each of their lists, as illustrated in Figure 1. In the Figure, documents 60, 40 and 50 are currently pointed in lists t1, t2 and t3, respectively. The WAND algorithm assures that previous occurrences in each list were already examined and that each of the pointed documents represents the smaller docId in the list that was not yet processed [4]. Thus, since the lists are organized by docIds, at this point we know the smaller docId (40) occurs only in the list of term t2, while document 50 occurs in t3 and might occur in t2. Finally, document 60 occurs in the list of t1 and might occur in the other two lists.",1,ad,True
58,"At this point, knowing the maximum score a document may reach in each list, we can estimate the maximum scores each of the currently pointed documents can obtain when processing the query: 40 can reach maximum possible score equal to the maximum score a document may achieve in the list of t2; 50 can reach the sums of maximum scores of t2 and t3, and 60 can reach the sum of maximum scores of the three terms as its maximum possible score. Using this information, we may discard the documents which are not able to reach the discarding threshold, i.e., cannot achieve a score higher than the minimum score among the top k results already processed at this moment.",1,ad,True
59,"So, to discard documents, we check the current documents pointed in each inverted list associated with the query and estimate their maximum possible score. The entry with smaller docId among the ones that reach a maximum possible score higher than the current discarding threshold is then chosen as a candidate to be included in the answer. This entry is known as the pivot. We then move all posting lists so that they point to docIds of at least the same as the pivot. Notice only pointers of lists where the current docIds are smaller than the pivot require a movement. If one of the posting lists with docId smaller than the pivot does not have the pivot document, the document is discarded, a new pivot is selected and the process repeated. Otherwise, if all these lists contain the document, its actual score is then computed. If the actual score of the pivot is higher than the discarding threshold, it is included in the answer set and the discarding threshold is updated. After processing the pivot,",0,,False
60,we move all the lists to the next document with docId bigger than the pivot and start the process again.,0,,False
61,"An interesting feature of the WAND method is that it can be configured as a more or less aggressive pruning method by either applying a reduction in the max score values of each list or by increasing the pruning threshold so that fewer documents will be accepted in the top results. When the pruning threshold is increased, there is no guarantee of preserving the exact top result set. Further details about the WAND method can be found in the article where it was first proposed [4].",0,,False
62,2.2.2 Blockmax WAND,0,,False
63,"Ding and Suel [6] have recently revisited the ideas presented in the WAND method, and proposed an even faster solution named the Blockmax WAND (BMW). In BMW, the entries of the inverted lists are grouped into compressed blocks that may be jumped without any decompression of their content. Each block contains information about the maximum impact among the entries found in it. Whenever such maximum impact is not relevant to change the results for a given query, the whole block is discarded, which avoids important query processing costs.",0,,False
64,"Authors present experiments which indicate BMW results in significant reduction of query processing times when compared to previous work, being currently the fastest query processing method found by us.",0,,False
65,"The BMW is based on the WAND algorithm and uses the same approach for selecting a document pivot during the query processing. In BMW, the pruning of entries is performed using two main pieces of information: (i) the maximum impact found inside an inverted list, which is also adopted in WAND; and (ii) The maximum impact found in each block pointed by the skiplist entries, known as the block max score, so that before accessing a block it is possible to predict the maximum impact of an entry among those found in such a block.",1,ad,True
66,"The basic idea of BMW is to take advantage of information (ii) to speedup query processing. Once a candidate document is selected to have its score computed, the algorithm uses the block max score information to accelerate the query processing by discarding documents with no chance of being in the top answers. The algorithm also allows skipping entire blocks of inverted list entries based on the block max score present on the skiplists, a procedure that they call shallow movement. Contrary to the regular movement present on the inverted lists, the shallow movement accesses only the skiplist entries, which avoids costs to decompress blocks of the inverted lists when processing queries.",1,ad,True
67,"The BMW algorithm starts with a pivoting phase, where a document is selected as a candidate to be inserted in the answer. Its pivoting phase is similar to the one performed in the WAND algorithm. Using the global max score of each list and the lists ordered by the value of the current docId pointed in each of them.",0,,False
68,"Before decompressing and evaluating the pivot document, BMW makes a shallow movement to align the inverted lists over the blocks that possibly have the document. After the alignment, the algorithm uses the information of block max score, stored in the skiplists, to estimate a local upper bound score of the candidate document. If this upper bound score is lower than a given pruning threshold, the document is discarded and one of the lists is advanced.",1,ad,True
69,185,0,,False
70,"As in WAND, the pruning threshold is dynamically updated according to the score of the evaluated candidate. As the processing is DAAT, each evaluated candidate has its complete score calculated, since all inverted lists are processed in parallel. Further details about the BMW algorithm can be found in the article where it was proposed [6].",0,,False
71,"Shan et al [13] show that the performance of BMW is degraded when static scores, such as Pagerank, are added in the ranking function. They study efficient techniques for Top-k query processing in the case where a page's static score is given and propose a set of new algorithms based on BMW which outperform the existing ones when static scores are taken into account when computing the final ranking. Their study can also be applied to our proposal as a future work, being orthogonal to the study presented here.",1,ad,True
72,2.3 Multi-tier indexes,0,,False
73,"Some authors proposed the splitting of the inverted index in more than one tier in an attempt to accelerate query processing. In these architectures, the query processing starts in a small tier and only if necessary proceeds to larger tiers. Risvik and Aasheim [10] divided the index into three tiers with the goal of achieving better scaling when distributing the index. According to static and dynamic sources of evidence, documents considered as more relevant are selected to compose the smaller tier. The query processing starts in the higher tier and only if the result set is not satisfactory, according to an evaluating algorithm, the query processing proceeds to the next tier. Performance gains are achieved when the processing does not visit the larger tiers. There is no guarantee that the result set is the exact set if compared to the exhaustive query processing.",0,,False
74,"In our proposed algorithms, we also kept the documents considered more important, in our case those with higher impact, in the smaller tier. However, in this paper we use each tier as part of the query processing. Our algorithms could, for instance, be applied to each tier proposed by Ravisk and Aesheim [10], being then ortoghonal to their proposal.",0,,False
75,"Ntoulas and Cho [8] presented a two-tiered query processing method that avoids any degradation of the quality of results, always guaranteeing the exact top-k results set. The first tier contains the documents considered the most important to the collection, selecting the entries by using static and dynamic pruning strategies that remove non-relevant documents and terms. The second tier contains the complete index. Their proposal uses the first tier as a cache level to accelerate query processing. Whenever the method detects that the results of the first tier assures that the top results will not be changed, it does not access the second tier, basing its results only on the first tier. Otherwise, they process the query using the second tier.",1,ad,True
76,"To guarantee that their method preserves the top answer results when compared to a system without pruning, the method evaluates the ranking function when processing the first tier, assigning the maximum possible score that could be achieved when processing the full index for entries that are not present in the first tier. The authors show how to determine the optimal size of a pruned index and experimentally evaluate their algorithms in a collection of 130 million Web pages. In their experiments, the presented method achieved good results for first tier index sizes varying from 10% to 20% of the full index. The two-tier strategy is also adopted in our article, but instead of using the first tier as",1,ad,True
77,"a cache, we use it as a candidate selection layer as part of our algorithms to compute the top results of a given query. Another important difference is that we always process the queries using both tiers, and we do not guarantee the exact top-k results.",0,,False
78,"Skobeltsyn et al [14] evaluate the impact of including a cache in the system when using the two-tier method presented by Ntoulas et al [8] and shows the query distribution workload is affected by the cache system. When using the cache, queries with a few terms, which would be the ones that would benefit more from the two-tier strategies, are usually solved by the cache system. As a consequence, the queries with more terms, which are difficult to process with pruning strategies, become more important in the workload when considering a system that adopts a cache of results.",1,ad,True
79,3. BMW-CS AND BMW-T,0,,False
80,"In this section, we present the details of the algorithms we proposed to accelerate the query processing when computing the ranking of results, which are named BMW-CS, from BWW with candidate selection, and BMW-t, from BMW with a threshold selection. These algorithms are based on a two-tiered index organization in which the first tier is a small index created using the entries with the highest impact from each term list, while the second tier is a larger index. The idea of relying on a high-quality tier is similar to the one adopted by Ntoulas et. al. [8].",1,ad,True
81,"The main objective of our algorithms is to use the first tier to accelerate the query processing in the second tier (i.e., the larger index) when computing the final ranking. In BMW-CS, the two tiers are disjointed, that is, the highimpact entries in the small index in first tier are not present in the second one. In BMW-t, the second tier contains the full index. Thus, even the high-impact entries in the first tier are also present in it.",0,,False
82,"In both cases, to select the entries for the first tier, we compute a global threshold to select entries so that the size of the first tier is about % of the full index. The parameter  provides an estimation of the final size of the first tier index. We adopted a minimum size of 1000 entries in each inverted list to prevent any individual list from becoming too small.",1,ad,True
83,"In our index organization, we used skiplists in both tiers in order to accelerate the query processing. For each block of 128 document entries, a skiplist entry is created that keeps the information of the current docId and the highest impact among the documents of the block. Also, for each term in the collection, the highest impact (max score) in the whole inverted list and the lowest impact found in this list in the first tier are computed and stored along with the term information. The lowest impact in the list at the first tier can be seen as an upper bound for the impact of the document entries that were not included in this list. Using this information, we can set the upper bound of contribution for the entries that are not present in the first tier, but appear in the inverted list in the second tier.",0,,False
84,3.1 BMW-CS,0,,False
85,"Listing 1 presents our first algorithm called BWM with candidate selection, or BMW-CS. In its first phase, BMWCS uses the first tier to select documents that are candidates to be present in the top results. Initially, the set of candidate documents, denoted by A, is generated by the function",1,WCS,True
86,186,0,,False
87,Listing 1: Algorithm BMW-CS,0,,False
88,"1 BMW-CS(queryTerms [ 1 . . q] , k)",0,,False
89,"2 A  SelectCandidates(queryTerms, k)",0,,False
90,3,0,,False
91,4 sort A by score 5 min score  Ak . score,0,,False
92,6,0,,False
93,7 //Remove the candidates with low upper score,0,,False
94,"8 for (i , 0 to |A|)",0,,False
95,9,0,,False
96,i f (Ai . upper score < min score) remove Ai,0,,False
97,10 end for,0,,False
98,11,0,,False
99,"12 R  CalculateCompleteScore(A , queryTerms, k)",0,,False
100,13,0,,False
101,14 return R,0,,False
102,"SelectCandidates, which computes the candidates for the top k results of a query composed of a set of terms (Listing 2). Then, the algorithm trims this set of documents, removing all candidates that cannot be present in the topk results. This trimming decreases the cost of the second phase. In the second phase, the second tier is processed to compute the final ranking of the top k results. This phase is performed by the function CalculateCompleteScore (Listing 6).",0,,False
103,The main idea behind BMW-CS is to take advantage of the fact that the first tier has entries with higher impact in order to significantly reduce the amount of documents analyzed in the second tier. We show in the experiments that this approach yields a quite competitive query processing algorithm.,1,ad,True
104,3.1.1 Candidates Selection,0,,False
105,"BMW-CS selects candidate documents from the first tier. However, the inverted lists in the first tier are not complete, which means, for instance, that a document which appears only in the list of one of the terms of a query in the first tier, may appear in lists of other terms of this query when considering the entries present in the second tier. Thus, during the candidate selection phase, the algorithm may discard a document that could have a high enough score when the full inverted lists are evaluated.",0,,False
106,"To reduce the possible negative impacts of this incompleteness of the lists in the first tier, we modified the BMW algorithm so that it considers the possibility of a missing pair (term, docId) in the first tier to occur in the second tier. During the pivoting phase and the upper boundary score checking, a lower boundary score is added for each missing term of the document that might be absent in the first tier, thus avoiding the possibility of discarding high score candidates due to incompleteness in the first tier.",1,ad,True
107,"This lower boundary score represents the max score that a document can achieve after processing the inverted list in the second index. With these two values, we can adjust the discarding threshold used to prune documents in BMW. A minimum heap is used to store the top-k documents with a higher score. The smallest score of the heap is used as the discarding threshold for BMW to dynamically prune entries with no chance to be part of the final top-k results. All evaluated documents that have a score higher than the discarding threshold when processing the first tier are added to the set of candidate documents.",1,ad,True
108,We can see the detailed algorithm for the candidate selection phase in Listing 2. The algorithm starts by selecting,0,,False
109,Listing 2: Algorithm SelectCandidates,0,,False
110,"1 SelectCandidates (queryTerms [ 1 . . q] , k) 2 Let H be the minimum heap to keep the top k results 3 Let A be the l i s t of candidates 4 Let Icand be the f i r s t t i e r index",0,,False
111,5,0,,False
112,6 l i s t s  Icand (queryTerms) ;// Gets inverted l i s t s 7   0;,0,,False
113,8 //Point to the f i r s t docId in each l i s t,0,,False
114,"9 for each {0  i < | l i s t s |} do Next( l i s t s [ i ] , 0) ;",0,,False
115,10,0,,False
116,11 repeat,0,,False
117,12,0,,False
118,sortByCurrentPointedDocId( l i s t s ) ;,0,,False
119,13,0,,False
120,"p  Pivoting( lists , ) ;",0,,False
121,14,0,,False
122,"i f (p ,, -1) break ; //No more candidates",0,,False
123,15,0,,False
124,d  l i s t s [p ] . curDoc;,0,,False
125,16,0,,False
126,"i f (d ,, M AXDOC ) break ; //End of the l i s t",0,,False
127,17,0,,False
128,18,0,,False
129,//Move only the skip pointers,0,,False
130,19,0,,False
131,"for each {0  i  p} do NextShallow( l i s t s [ i ] , d) ;",0,,False
132,20,0,,False
133,21,0,,False
134,"i f ( CheckBlockMax( , p) ,, T RU E )",0,,False
135,22,0,,False
136,"i f ( l i s t s [ 0 ] . curDoc ,, d)",0,,False
137,23,0,,False
138,doc . docId  d;,0,,False
139,24,0,,False
140,doc . score ,0,,False
141,"p i,0",0,,False
142,BM25( l i s t s [ i ] ) ;,0,,False
143,25,0,,False
144,doc . upper score  doc . score +,0,,False
145,26,0,,False
146,"|lists| i,p+1",0,,False
147,l i s t s [i ] . min score ;,0,,False
148,27,0,,False
149,28,0,,False
150,i f (|H| < k) H  H  doc ;,0,,False
151,29,0,,False
152,else i f (H0 . score < doc . score ),0,,False
153,30,0,,False
154,remove H0 ; // the one with smallest score,0,,False
155,31,0,,False
156,H  H  doc ;,0,,False
157,32,0,,False
158,  H0 . score ; //Update the threshold,0,,False
159,33,0,,False
160,end i f,0,,False
161,34,0,,False
162,35,0,,False
163,//Insert only documents with possible score > ,0,,False
164,36,0,,False
165,"i f ( <, doc . upper score)",0,,False
166,37,0,,False
167,doc . terms  queryTerms [ 0 . . p ] ;,0,,False
168,38,0,,False
169,A  A  doc ;,0,,False
170,39,0,,False
171,i f ({dLow  A| dLow. upper score < }),0,,False
172,40,0,,False
173,A  A - dLow ;,0,,False
174,41,0,,False
175,endif,0,,False
176,42,0,,False
177,endif,0,,False
178,43,0,,False
179,//Advance a l l evaluated l i s t s,0,,False
180,44,0,,False
181,"for each {0  i  p} do Next( l i s t s [ i ] , d+1);",0,,False
182,45,0,,False
183,else,0,,False
184,46,0,,False
185,j  {x| l i s t s [x ] . curDoc < d ^,0,,False
186,47,0,,False
187,"| l i s t s [x] | < | l i s t s [ y ] |, 0  y < p} ;",0,,False
188,48,0,,False
189,"Next( l i s t s [ j ] , d) ;",0,,False
190,49,0,,False
191,end i f,0,,False
192,50,0,,False
193,else,0,,False
194,51,0,,False
195,"d next  GetNewCandidate( l i s t s [ j ] , p) ;",0,,False
196,52,0,,False
197,"j  {x| | l i s t s [x] | < | l i s t s [ y ] | , 0  y  p} ;",0,,False
198,53,0,,False
199,"Next( l i s t s [ j ] , d next) ;",0,,False
200,54,0,,False
201,end i f,0,,False
202,55 end repeat,0,,False
203,56,0,,False
204,57 return A,0,,False
205,"the inverted lists to be processed (Line 6), which are the lists that represent each query term. The discarding threshold, , is initially set to 0(Line 7) and is updated to the minimum score stored in the heap H if it is full (Line 32). Line 9 makes each of the inverted lists point to their first document. The function N ext(l, d) searches in the skiplist associated with list l for the block where there is the first occurrence of a docId equal or bigger than d, setting l.current block to the found position. Then, it moves the pointer to the current document of the list, l.curDoc, to the smallest entry with value greater than d.",0,,False
206,"The lists shown in Listing 2 are represented by vector lists and each list has an internal pointer to the docId being processed at each moment, the current docId. In Line 12 we sort this vector into increasing order according to the current docId pointed by each of these lists. We then compute in Line 13 the next document that has a chance to be present in the top results, performing the pivoting, which is",0,,False
207,187,0,,False
208,Listing 3: Algorithm Pivoting,0,,False
209,"1 Pivoting ( lists , )",0,,False
210,2 accum  0;,0,,False
211,3 for each 0  i < |lists| do,0,,False
212,4,0,,False
213,accum  accum + l i s t s [ i ] . max score ;,0,,False
214,5,0,,False
215,accum min ,0,,False
216,"|lists| j,i+1",0,,False
217,l i s t s [j ] . min score,0,,False
218,6,0,,False
219,"i f (accum + accum min >, )",0,,False
220,7,0,,False
221,while(i+1<|l i s t s | AND,0,,False
222,8,0,,False
223,"l i s t s [ i+1].curDoc ,, l i s t s [ i ] . curDoc) do",0,,False
224,9,0,,False
225,i  i + 1;,0,,False
226,10,0,,False
227,end while,0,,False
228,11,0,,False
229,return i,0,,False
230,12,0,,False
231,end i f,0,,False
232,13 end for,0,,False
233,14 return -1;,0,,False
234,Listing 4: Algorithm CheckBlockMax,0,,False
235,"1 CheckBlockMax ( l i s t s , p, )",0,,False
236,2,0,,False
237,"3 //Sum the max score of each block , that d can appear",0,,False
238,4,0,,False
239,max ,0,,False
240,"p i,0",0,,False
241,l i s t s [ i ] . getBlockMaxScore() ;,0,,False
242,5,0,,False
243,6 //Add the min score of the l i s t s that d may appear in the f u l l index,0,,False
244,7,0,,False
245,max  max +,0,,False
246,"|lists| i,p+1",0,,False
247,l i s t s [i ] . min score ;,0,,False
248,8 i f (max > ) return true,0,,False
249,9 return f alse,0,,False
250,"the main step of the BMW heuristic. Our pivoting, however, is computed taking into consideration the possibility of some of the entries of a document being not included in the first tier, which let us add this information to compute the upper score, which is the maximum score when selecting the pivot. This procedure is described in Listing 3. Lines 15 to 17 test break conditions and set the current document to be analyzed by the algorithm.",1,ad,True
251,"Line 19 adopts the function NextShallow to move the current documents pointer in each list. The function NextShallow is the same presented in the original BMW proposal, and differs from function Next because it does not need to access the documents, accessing only the skiplists of the inverted lists to move their pointers and set a new current block in each inverted list. Using this function, we can skip entries without needing to access or decompress them. Line 21 calls function CheckBlockMax, detailed in Listing 4, which is also modified when compared to the original one proposed in BMW, since it also needs to deal with the incompleteness of the first tier.",1,ad,True
252,"The remaining algorithm checks whether a document has enough score to be included in the answer. Line 25 takes the incompleteness of the first tier into consideration when computing the upper score. The score of each document is used to include it in heap H, Lines 28 to 33. H is maintained to control the discarding threshold . The upper score of each document is used in Line 36 to check whether a document should be included in the candidate documents set A.",0,,False
253,"The threshold  changes as more documents are processed, so, whenever we add a document to A, we also check if there is at least one document in A with an upper score value lower than the current value of . In such cases, we remove the document (Lines 39 to 40). This procedure avoids wasting memory by keeping elements in A which will be discarded at the end of the process. By the end of the candidate selection algorithm, the list of candidate documents A is returned, so that the final result can be obtained by processing the remainder of the index in the second tier.",1,ad,True
254,3.1.2 Computing the Final Ranking,0,,False
255,"In function CalculateCompleteScore (Listing 6), the scores of candidates with missing terms are evaluated using the larger index in the second tier, which contains the index entries not present in the first tier. To avoid unnecessary costs with decompression, the shallow movement described in [6] is used to align all the term lists. Then, a second BlockMaxScore check is made to verify whether the document",1,ad,True
256,Listing 5: Algorithm GetNewCandidate,0,,False
257,"1 GetNewCandidate ( l i s t s , p)",0,,False
258,2 mindoc  M AXDOC,0,,False
259,3,0,,False
260,4 //Selects the lower docId between the blocks boundaries,0,,False
261,5 // of the l i s t s already checked,1,ad,True
262,6 for each {0  i  p} do,0,,False
263,7,0,,False
264,i f (mindoc > l i s t s [ i ] . getDocBlockBoundary() ),0,,False
265,8,0,,False
266,mindoc  l i s t s [ i ] . getDocBlockBoundary() ;,0,,False
267,9,0,,False
268,end i f,0,,False
269,10 end for,0,,False
270,11,0,,False
271,12 //Select the lower docId between the l i s t s not checked,0,,False
272,13 for each {p + 1  i < |lists|} do,0,,False
273,14,0,,False
274,i f (mindoc > l i s t s [ i ] . curDoc),0,,False
275,15,0,,False
276,mindoc  l i s t s [ i ] . curDoc;,0,,False
277,16,0,,False
278,end i f,0,,False
279,17 end for,0,,False
280,18,0,,False
281,19 return mindoc; //Return the smallest docId found,0,,False
282,"can be part of the top k results or not. Each document is evaluated only if it has a high enough score, otherwise it is discarded. As in the first phase, we keep a minimum heap with the documents with the greatest scores evaluated, and the minimum score of this set is used as a discarding threshold to prune candidates.",0,,False
283,"As the candidate set is small, and only documents with incomplete scores are evaluated, this phase is expected to be performed extremely fast, even considering that it processes a the larger tier.",0,,False
284,"In BWM-CS, the first tier may not contain enough information to assure all top documents are considered as candidate documents. Since only candidate documents can be included in the final results, it does not guarantee exact results in the final ranking. For instance, a document which contains entries for all three terms of a query, but whose entries are present only in the second tier, will not be included in the candidate selection. However, this document may achieve scores higher than the ones in the top documents found by the candidate selection, in cases where there are top results that do not contain all query terms. Notice however that such a situation tends to occur for documents that would be included in the final positions of the top results and, as we show in our experiments, is it does not affect the final results very much.",0,,False
285,3.2 BMW-t,0,,False
286,"In our second algorithm, BMW with threshold selection, or BMW-t, we use just the first tier to set the initial discarding threshold adopted by methods WAND and BMW. In these methods, this initial discarding threshold is set to 0 at the beginning, and grows as the documents with higher scores",1,ad,True
287,188,0,,False
288,Listing 6: Algorithm CalculateCompleteScore,0,,False
289,"1 CalculateCompleteScore( A , queryTerms [ 1 . . q] , k) 2 Let H be the minimum heap to hold the k most relevant",0,,False
290,candidates,0,,False
291,3 Let Isecond tier be the second index 4 l i s t s  Isecond tier (queryTerms) //Select the terms l i s t s 5 0,0,,False
292,6 H 7 sort A by docId ;,0,,False
293,8,0,,False
294,9 for each {0  i < |A|} do,0,,False
295,10,0,,False
296,i f (Ai . score < Ai . upper score),0,,False
297,11,0,,False
298,local upper score  Ai . score ;,0,,False
299,12,0,,False
300,13,0,,False
301,for each {0  j < |lists|} do,0,,False
302,14,0,,False
303,i f (queryT erm[j]  Ai . terms),0,,False
304,15,0,,False
305,"NextShallow( l i s t s [ j ] , Ai . docId) ;",0,,False
306,16,0,,False
307,local upper score  local upper score +,0,,False
308,17,0,,False
309,18,0,,False
310,end i f,0,,False
311,19,0,,False
312,end for,0,,False
313,l i s t s [ j ] . getBlockMaxScore() ;,0,,False
314,20,0,,False
315,21,0,,False
316,if (local upper score > ),0,,False
317,22,0,,False
318,for each {0  j < |lists|} do,0,,False
319,23,0,,False
320,i f (queryT erm[j]  Ai . terms),0,,False
321,24,0,,False
322,"Next( l i s t s [ j ] , Ai . docId) ;",0,,False
323,25,0,,False
324,end i f,0,,False
325,26,0,,False
326,end for,0,,False
327,27,0,,False
328,//Complete the score with the missing l i s t s,0,,False
329,28,0,,False
330,"for each {0  x < |lists||lists[x].curDoc , Ai.docId} do",0,,False
331,29,0,,False
332,Ai . score  Ai . score + BM25( l i s t s [ x ] ) ;,0,,False
333,30,0,,False
334,end for,0,,False
335,31,0,,False
336,end i f,0,,False
337,32,0,,False
338,end i f,0,,False
339,33,0,,False
340,34,0,,False
341,i f ( < Ai . score ),0,,False
342,35,0,,False
343,i f (|H| < k),0,,False
344,36,0,,False
345,H  H Ai ;,0,,False
346,37,0,,False
347,e l s e i f (H0 . score < Ai . score ),0,,False
348,38,0,,False
349,remove H0 ;,0,,False
350,39,0,,False
351,H  H Ai ;,0,,False
352,40,0,,False
353,  H0 . score ;,0,,False
354,41,0,,False
355,end i f,0,,False
356,42,0,,False
357,end i f,0,,False
358,43 end for,0,,False
359,44,0,,False
360,45 sort H by score ; 46 return H ; 47 end,0,,False
361,"are found and included in the answer. As a consequence, the query processing discards fewer documents at the beginning of the process, since the discarding threshold starts with a small value. We thus propose the usage of the first tier of the index to support a pre-processing stage just to compute an initial discarding threshold that is higher than 0. This simple strategy naturally may speed up the process if the gains when processing the full index are worth the cost of computing the initial discarding threshold when processing the first tier.",0,,False
362,"We then experiment with a variation of BMW, we named BMW-t, and a variation of WAND, we named WAND-t. These variations use the first tier to select an initial discarding threshold when processing the queries. This new usage of the two tier index presents the advantage of preserving the top k results, which does not happen in BMW-CS. The WAND-t performed worse than the BMW-t, thus we report only BMW-t in the experiments.",1,ad,True
363,4. EXPERIMENTAL EVALUATION,0,,False
364,"We used the TREC GOV2 collection for the experiments in this paper. The collection has 25 million web pages crawled from the .gov domain in early 2004. It has 426 gigabytes of text, composed of HTML pages and the extracted",1,TREC,True
365,"content of pdf and postscripts documents. The full index has about 7 gigabytes of inverted lists and a vocabulary of about 4 million distinct terms. We applied the Porter Stemmer [9] when indexing the collection. Our indexes use the frequency of the terms as the impact information. To evaluate the quality of query results, we randomly selected a set of 1000 queries from the TREC 2006 efficiency queries set and removed the stop-words from these queries. During the query processing, the entire index is loaded to memory, to avoid any possible bias in the query processing time. | All these setup options were chosen for being similar to those adopted in the baseline [6] and previous studies [15]. We ran the experiments in a 24-cores Intel(R) Xeon(R), with X5680 Processor, 3.33GHz and 64 GB of memory. All the queries were processed in a single core.",1,TREC,True
366,"We used Okapi BM25 as the rank function, but our method can be adopted to compute other ranking functions. The generated skiplists have one entry for each block of 128 documents. Each skiplist entry keeps the docId, to help the random decompression, and the maximum impact registered in the block. We also experimented with blocks of 64 entries, and the results and conclusions were about the same, thus we decided to report only results with 128. We varied the first tier % from 1 to 20 percent of the full index in the experiments.",1,ad,True
367,Another parameter evaluated in the experiments was the size of the top results required by the algorithms. We evaluated the algorithms requesting 10 and 1000 results. Retrieving top 1000 results was included to simulate an environment where the top results are computed to feed a more sophisticated ranking method that performs a re-rank of results. The top 10 results was included to simulate a more common scenario where the user is interested in getting only a small list of results.,0,,False
368,"We evaluated the methods in terms of query response time, the amount of accumulators required to process the queries, amount of decoded entries from the inverted lists, and finally the mean reciprocal rank Distance (MRRD), presented in Equation 1, which was the measure adopted by Broder et al [4] to evaluate the distance between the results when preserving top results to rankings that do not preserve the top results. Using the MRRD distance, we are able to know how much an approximated rank differs from the one that preserves the top results. This measure returns a value between 0 and 1, where identical results provide MRRD,""0, and completely distinct results provide MRRD"",1.",1,ad,True
369,"M RRD(B, P ) ,",0,,False
370,"k i,""1,di B -P""",0,,False
371,1/i,0,,False
372,"k i,1",0,,False
373,1/i,0,,False
374,(1),0,,False
375,4.1 Baselines,0,,False
376,"One of the implemented baseline algorithms was BMW [6]. As one of our methods may not preserve the top ranking results, we also have considered including as a baseline the version of WAND that does not preserve the top results, proposed by Broder et al [4]. However, the results achieved by the approximate WAND were even slower than the BMW. We thus removed it from the baselines, and modified the method BMW to implement the same approximation strategy proposed to WAND. As a result, we transformed BMW to an approximated top-k query processing algorithm, which does not guarantee preserving top results, but may be faster",0,,False
377,189,0,,False
378,0.5,0,,False
379,BMW-SP-10%,0,,False
380,BMW-SP-30%,0,,False
381,BMW-SP-50%,0,,False
382,0.4,0,,False
383,BMW-F1.5,0,,False
384,BMW-F2,0,,False
385,BMW-CS-1%,0,,False
386,BMW-CS-5%,0,,False
387,0.3,0,,False
388,BMW-CS-10%,0,,False
389,MRR,0,,False
390,0.2,0,,False
391,0.1,0,,False
392,0 0 100 200 300 400 500 600 700 800 900 1000 k,0,,False
393,Figure 2: MRRD values compared to a exact rank for baselines BMW-SP and BMW-f,0,,False
394,"than original BMW. Basically, we artificially increase the pruning threshold during the query processing by multiplying this threshold for a pruning factor f . If f is 1, the exact top-k is guaranteed. When f is greater than 1, there is no guarantee of preserving the exact rank, but less entries will be evaluated, resulting in performance gains.",0,,False
395,"Finally, a reader could wonder if the results would also be good if we had adopted just the first tier for processing queries, considering the first tier as a statically pruned index. Thus, in order to avoid these doubts, we included a naive method using only the first tier for processing queries. We named it BMW-SP (applying BMW with static pruning).",1,ad,True
396,4.2 Results,0,,False
397,"We begin the report of our experimental results by answering the possible doubts about the advantage of using the BMW-CS algorithm, which does not guarantee that all the top results are preserved, when compared to a simple static pruning strategy that adopts only the first tier to compute the ranking. Figure 2 presents the MRRD results when computing the top k results by using our method with the first tier of 1%, 5% and 10% compared to the usage of the static pruning approach, named BMW-SP. As can be seen in Figure 2, even when using BMW-SP with the first tier with 50% of the full index, the error level (MRRD) obtained by BMW-CS with 1% is still smaller than it. Even considering this observation, for the sake of completeness, we still report the time efficiency results obtained when using the BMW-SP with the first tier being 50%.",1,ad,True
398,"Figure 2 also presents the BMW-f MRRD results when varying parameter f . As can be seen in Figure 2, BMW-f achieves error levels worse than BMW-CS even when setting the factor f to the low value of 1.5. Lower factor values would slow down the performance of the method, thus we adopt this factor in our efficiency experiments, even considering that its error level is higher than those achieved by our method. We stress that both BMW-f and BMW-SP are included in the experiments to avoid doubts about these possible variations in the usage of BMW. These methods were not explicitly proposed in the literature.",1,ad,True
399,"Next we present the percentage of entries we included in the first tier of BMW-CS. This choice affects three main factors: the time for processing queries, the MRRD results of",0,,False
400,"our methods and the amount of memory required to process queries. Variation of these parameters are illustrated in Figure 3. As can be seen, when computing the top 1000 results, the MRRD results decrease as the size of the first tier increases, being almost zero for first tier sizes higher than 8%. When computing the top 10 results, MRRD was always zero for all sizes of first tier experimented. Time tends to increase as the first tier increases in both cases. On the other hand, the number of accumulators presents more complex behavior. In the top 1000, it first increases as the size of the first tier increases. Then, at some point, it starts to decrease, since the candidate selection procedure starts to perform better pruning, thus reducing the set of candidate documents.",0,,False
401,"When looking to the general results presented in Figure 3, we conclude that a good size for the first tier in BMW-CS is 2% when tuning the method to compute the top 10 results and 10% when tuning the method to top 1000 results. These parameters provide a good combination of a low number of candidate documents, low query processing times and low MRRD. Notice however that even if choosing other first tier sizes among the ones presented in the experiments, still our method would be quite fast and competitive.",0,,False
402,"Figure 3 (c) and (d) also indicates how much memory our algorithm needs to store the candidate documents. We can see the requirement is not so high in both the top 10 and top 1000 scenarios, being limited to a few times greater than the size of top results required to be computed.",0,,False
403,"Finally, regarding the MRRD error level achieved by BMWCS it is noteworthy that a user would almost not perceive the differences in top k results when using BMW-CS even when considering higher values of k. For instance, as k ,"" 1000, the error of BMW-CS is still smaller than 0.0001 in terms of MRRD for the 10% first tier size. To better illustrate what this error level means, analyzing the results in detail, we perceived that BMW-CS with a 10% first tier resulted in no changes in the top 1000 results for 90% of the evaluated queries. Further, in all experimented first tier sizes, the top 10 results were preserved for all queries. Changes in the ranking, when they occur, are more common in the bottom results. For instance, we preserved the top 100 results for all queries, and preserved the top 200 results for 99.9% of the queries.""",1,WCS,True
404,"We also studied the MRR variation of method BMW-t, but do not present the variation due to space restrictions. Its performance is close to the best when using 1% for the first tier, becoming slower as the partition increases and not improving so much when it decreases. We report the results on the remaining experiments with our methods using 2% and 10% first tier sizes in case of BMW-CS, and 1% in case of BMW-t.",0,,False
405,"In Tables 1 and 2, we can observe the performance of the algorithms in terms of MRRD, decoded integers and query time when processing the top-10 and top-1000. The results of time are presented with confidence level of 95%. BMWCS provides extremely low MRRD results, which means these answers are almost the same as the correct top k results. On the other hand, its performance is considerably better than BMW and BMW-f , which do not have a preprocessing phase during query evaluation, and thus are performed directly over the full index.",1,WCS,True
406,"We can see the performance of BMW-t, which preserves the top results, presents an improvement of around 10% in",0,,False
407,190,0,,False
408,Query Time,1,Query,True
409,MRRD value,0,,False
410,a) top10:MRRD and Time,0,,False
411,1,0,,False
412,20,0,,False
413,MRRD,0,,False
414,0.8,0,,False
415,Time,0,,False
416,15 0.6,0,,False
417,0.4,0,,False
418,10,0,,False
419,0.2,0,,False
420,5,0,,False
421,0 0,0,,False
422,0 2 4 6 8 10 12 14 16 18 20 Size of first tier,0,,False
423,c) top10:Candidates and Time,0,,False
424,900 20,0,,False
425,800,0,,False
426,#candidates,0,,False
427,Time,0,,False
428,700,0,,False
429,15 600,0,,False
430,500,0,,False
431,400,0,,False
432,10,0,,False
433,300,0,,False
434,200,0,,False
435,5,0,,False
436,100,0,,False
437,0,0,,False
438,0,0,,False
439,0 2 4 6 8 10 12 14 16 18 20,0,,False
440,Size of first tier,0,,False
441,Query Time # of candidates,1,Query,True
442,Query Time MRRD value,1,Query,True
443,b) top1000: MRRD and Time,0,,False
444,0.018,0,,False
445,80,0,,False
446,0.016,0,,False
447,MRRD Time,0,,False
448,70,0,,False
449,0.014,0,,False
450,60,0,,False
451,0.012,0,,False
452,50,0,,False
453,0.01 40,0,,False
454,0.008,0,,False
455,0.006,0,,False
456,30,0,,False
457,0.004,0,,False
458,20,0,,False
459,0.002,0,,False
460,10,0,,False
461,0,0,,False
462,0,0,,False
463,0 2 4 6 8 10 12 14 16 18 20,0,,False
464,Size of first tier,0,,False
465,d) top1000: Candidates and Time,0,,False
466,30000,0,,False
467,80,0,,False
468,#candidates,0,,False
469,70,0,,False
470,25000,0,,False
471,Time,0,,False
472,60,0,,False
473,20000,0,,False
474,50,0,,False
475,15000,0,,False
476,40,0,,False
477,10000,0,,False
478,30,0,,False
479,20,0,,False
480,5000,0,,False
481,10,0,,False
482,0,0,,False
483,0,0,,False
484,0 2 4 6 8 10 12 14 16 18 20,0,,False
485,Size of first tier,0,,False
486,Query Time,1,Query,True
487,# of candidates,0,,False
488,Figure 3: Variation in time and MRRD(a and b); and between time and number of candidate documents stored in (c and d) when processing first tier for method BMW-CS when using distinct sizes of first tier.,0,,False
489,"query processing times when compared to BMW. The approximated version BMW-f , using the factor f ,""1.5, processes query 20% faster than the exact BMW. However, BMW-CS is not only faster than BMW-f , but also presents lower MRRD values.""",0,,False
490,"Still regarding Tables 1 and 2, we can see that BMWCS preserves the result set more than the other approximated methods implemented and is about 40 times faster than BMW when computing the top 10 results (using a 2% tier), and about 4.75 times faster when computing the top 1000 results(using a 10% tier). These gains can also be seen when analyzing the number of decoded integers, which is one of the main costs when elements are stored in memory.",1,WCS,True
491,top 10,0,,False
492,Algorithm MRRD Decoded,0,,False
493,time,0,,False
494,BMW,0,,False
495,0 2353066 100.1 ± 9.9,0,,False
496,BMW-f1.5 0.3386 1797451 78.5 ± 7.8,0,,False
497,BMW-SP-50 0.0032 1700488 79.8 ± 8.0,0,,False
498,BMW-t,0,,False
499,0 2082782 89.5 ± 8.8,0,,False
500,BMW-CS-2,0,,False
501,0 48989 2.4 ± 0.4,0,,False
502,BMW-CS-10,0,,False
503,0 217627 10.4 ± 0.5,0,,False
504,"Table 1: MRRD, number of decoded entries, and time(ms) efficiency achieved by the experimented methods when computing top 10 results.",0,,False
505,"Table 3 presents the performance of the algorithms when processing distinct query sizes for both the top 10 and top 1000 results computation. BMW-CS was the fastest option for all query sizes. Although the gain is smaller for queries with more than 5 terms, our method still results in",0,,False
506,top 1000,0,,False
507,Algorithm MRRD Decoded,0,,False
508,time,0,,False
509,BMW,0,,False
510,0 5032834 226.0 ± 18.0,0,,False
511,BMW-f1.5 0.1149 4463099 193.7 ± 15.4,0,,False
512,BMW-SP-50 0.0584 3495885 174.8 ± 13.0,0,,False
513,BMW-t,0,,False
514,0 4799477 205.7 ± 16.9,0,,False
515,BMW-CS-2 0.0043 1258859 39.8 ± 4.6,0,,False
516,BMW-CS-10 0.0001 921687 47.6 ± 4.2,0,,False
517,"Table 2: MRRD, number of decoded entries, and time(ms) efficiency achieved by the experimented methods when computing top 1000 results.",0,,False
518,impressive gains when compared to all baselines even for long queries.,0,,False
519,"One explanation for the smaller gain in long queries is that in the first phase of the process, as we have many terms in the query, the upper score of a candidate will be higher because it sums the minimum score of the missing lists. Thus, as we have an index with only a small fraction of the full index, the number of documents in the first phase with a complete score will be lower according to the number of terms in the query, making the threshold values lower when compared to the estimated upper scores. This configuration will lead to a less effective pruning during the candidate selection, increasing the costs of the whole process.",1,ad,True
520,"We observed differences in time results when comparing our experiments to the ones presented by Ding et al [6]. While we adopt exactly the same dataset, the query processing times we obtained are lower than the ones presented in their article. However we see that the number of decoded",1,ad,True
521,191,0,,False
522,Algorithm 2,0,,False
523,BMW,0,,False
524,12.0,0,,False
525,BMW-f1.5 7.8,0,,False
526,BMW-SP-50 10.3,0,,False
527,Query time (ms) 3 4 5 >5 top 10,1,Query,True
528,46.6 100.5 197.9 442.7 34.0 77.3 155.5 367.7 38.7 82.4 160.1 331.6,0,,False
529,BMW-t,0,,False
530,10.4 41.1 87.7 179.3 401.7,0,,False
531,BMW-CS-2 0.58 1.42 2.42 3.74 9.96,0,,False
532,BMW-CS-10 2.3 5.7 11.6 19.7 36.8,0,,False
533,top 1000,0,,False
534,BMW,0,,False
535,37.9 122.3 243.6 437.9 848.6,0,,False
536,BMW-f1.5 29.9 102.5 206.3 390.1 725.0,0,,False
537,BMW-SP-50 32.6 102.1 192.0 333.5 610.3,0,,False
538,BMW-t,0,,False
539,29.8 104.6 220.9 406.5 803.5,0,,False
540,BMW-CS-2 8.49 20.62 44.58 79.07 139.1,0,,False
541,BMW-CS-10 14.6 29.3 51.8 82.2 160.8,0,,False
542,Table 3: Time achieved by the experimented methods when processing queries with distinct sizes and computing top 10 and top 1000 results.,0,,False
543,"integers still similar. The final difference in times can be due to the choice of the queries, since we do not know the exact set of queries adopted by them, the architecture of the implemented system, and the machines used for the experiments. These differences do not affect the conclusions presented in our study because they affect all the experimented methods.",1,ad,True
544,Acknowledgements,0,,False
545,"This work is partially supported by INWeb (MCT/CNPq grant 57.3871/2008-6), DOMAR (MCT/CNPq 476798/20116), TTDSW (FAPEAM), by CNPq fellowship grants to Edleno Moura (307861/2010-4) and Altigran Silva (308380/2010-0), and FAPEAM scholarship to Cristian Rossi.",1,NP,True
546,5. CONCLUSION,0,,False
547,"The algorithms proposed and studied by us outperform the existent state-of-the-art algorithms for DAAT query processing. BMW-CS presents the advantage of being about 40 times faster than BMW when computing top 10 results and 4.75 times faster when computing top 1000 results. While it does not guarantee to preserve the top results, we show through experiments that the application of the algorithm does not change the results very much. The MRRD error level is quite small and the algorithm provides an impressive gain in performance. Thus, in situations where preserving the top results is not mandatory, the BMW-CS algorithm is an interesting alternative.",1,ad,True
548,"The price paid for this fast query processing is the necessity of more memory for processing queries. As we show, the number of candidate documents stored by the algorithm, which is the extra memory required by it, is not prohibitive, being, for instance, around 10 times the size of the final results in our experiments. Further, in practice a search system usually processes queries in multiple threads per machines, and the reduction in the query processing times cooperates to increase the throughput, thus compensating for the extra memory required by each thread. We intend to better study this question in future studies, since this was not the focus of this current study.",1,ad,True
549,"The second algorithm proposed by us, BMW-t, presents the advantage of preserving the top results. It delivers smaller, but significant gains when compared to the application of plain BMW, being about 10% faster.",1,ad,True
550,"Finally, in the future, we also want to study the combination of our algorithm BMW-CS to ranking strategies that take more information into account. In this regard, we plan to study how our algorithms can be adapted to strategies as those presented by Shan et al [13], where the authors study the impact of including external sources of relevance evidence into the performance of query processing algorithms, and such as [5], in which the authors show how to encode several features into a single impact value.",1,ad,True
551,6. REFERENCES,0,,False
552,"[1] V. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In ACM SIGIR, pages 372­379, 2006.",0,,False
553,"[2] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. In ACM SIGIR, pages 35­42, 2001.",0,,False
554,"[3] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley Publishing Company, USA, 2nd edition, 2011.",0,,False
555,"[4] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In ACM CIKM, pages 426­434, 2003.",0,,False
556,"[5] A. Carvalho, C. Rossi, E. S. de Moura, D. Fernandes, and A. S. da Silva. LePrEF: Learn to Pre-compute Evidence Fusion for Efficient Query Evaluation. JASIST, 55(92):1­28, 2012.",1,Query,True
557,"[6] S. Ding and T. Suel. Faster top-k document retrieval using block-max indexes. In ACM SIGIR, pages 993­1002, 2011.",0,,False
558,"[7] A. Moffat and J. Zobel. Self-indexing inverted files for fast text retrieval. ACM TOIS, 14(4):349­379, 1996.",0,,False
559,"[8] A. Ntoulas and J. Cho. Pruning policies for two-tiered inverted index with correctness guarantee. In ACM SIGIR, pages 191­198, 2007.",0,,False
560,"[9] M. Porter. An algorithm for suffix stripping. Program: electronic library and information systems, 40(3):211­218, 2006.",0,,False
561,"[10] K. Risvik, Y. Aasheim, and M. Lidal. Multi-tier architecture for web search engines. In First Latin American Web Congress, pages 132­143, 2003.",0,,False
562,"[11] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In ACM SIGIR, pages 232­241, 1994.",0,,False
563,"[12] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Technical report, Ithaca, NY, USA, 1974.",0,,False
564,"[13] D. Shan, S. Ding, J. He, H. Yan, and X. Li. Optimized top-k processing with global page scores on block-max indexes. In WSDM, pages 423­432, 2012.",0,,False
565,"[14] G. Skobeltsyn, F. Junqueira, V. Plachouras, and R. Baeza-Yates. ResIn: a combination of results caching and index pruning for high-performance web search engines. In ACM SIGIR, pages 131­138, 2008.",0,,False
566,"[15] T. Strohman and W. B. Croft. Efficient document retrieval in main memory. In ACM SIGIR, pages 175­182, 2007.",0,,False
567,192,0,,False
568,,0,,False

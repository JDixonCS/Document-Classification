,sentence,label,data,regex
0,Shame to be Sham: Addressing Content-Based Grey Hat Search Engine Optimization,0,,False
1,Fiana Raiber,0,,False
2,Technion -- Israel Institute of Technology,0,,False
3,fiana@tx.technion.ac.il,0,,False
4,Kevyn Collins-Thompson,0,,False
5,Microsoft Research,0,,False
6,1 Microsoft Way,0,,False
7,"Redmond, WA, USA 98052",0,,False
8,kevynct@microsoft.com,0,,False
9,Oren Kurland,0,,False
10,Technion -- Israel Institute of Technology,0,,False
11,kurland@ie.technion.ac.il,0,,False
12,ABSTRACT,0,,False
13,"We present an initial study identifying a form of contentbased grey hat search engine optimization, in which a Web page contains both potentially relevant content and manipulated content: we call such pages sham documents, because they lie in the grey area between `ham' (clearly normal) and `spam' (clearly fake). Sham documents are often ranked artificially high in response to certain queries, but also may contain some useful information and cannot be considered as absolute spam. We report a novel annotation effort performed with the ClueWeb09 benchmark where pages were labeled as being spam, sham, or legitimate content. Significant inter-annotator agreement rates support the claim that there are sham documents that are highly ranked by a very effective retrieval approach, yet are not spam. We also present an initial study of predictors that may indicate whether a query is the target of shamming. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models",1,ClueWeb,True
14,"Keywords: sham, spam, search engine optimization",0,,False
15,1. INTRODUCTION,1,DUC,True
16,"The Web constitutes an adversarial retrieval setting: in Web search, content creators try to have their pages ranked high in response to queries, to increase visibility and traffic to their sites. To that end, they often employ various search engine optimization (SEO) techniques [11]. Search engines, on the other hand, aim to rank pages in response to queries by the overall relevance of their content and strive to minimize the effect of potential adversarial methods.",1,ad,True
17,"A distinction is made between white hat SEO, which typically does not involve deceptive techniques and obeys search engine anti-spam guidelines, and black hat SEO, which uses deceptive content, links and other manipulation that ultimately may result in the site using them being penalized or banned by search engines. The black hat SEO area has attracted much research attention; e.g., there is a large body of work on spam classification [11, 5, 8, 19], where a page is",1,ad,True
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
19,"deemed spam if it bears no useful content. Grey hat SEO, on the other hand, is an intermediate form of search ranking manipulation that yields neither a spam page, nor a (very) high-quality representative of relevant content for queries seeking information on that topic. Grey hat SEO has attracted very little research attention in the literature [13]. There is no rigorous definition of what makes for a grey hat SEO attempt: because grey hat documents can often contain relevant information as well as SEO content, identifying such documents is often a subjective decision.",0,,False
20,"In this paper we address the problem of identifying a specific, highly important type of grey hat SEO: pages whose content has been manipulated so as to have the page ranked higher than it should with respect to some queries. However, these pages are not spam in the absolute sense, in that they can still satisfy information needs. We use the term sham to refer to such pages1. For example, excessive use of query variants can turn an otherwise useful page to sham but not to spam, along with more obvious manipulations like keyword stuffing [11]. An example of a sham passage is shown in Figure 1.",1,ad,True
21,"By definition, sham pages can have considerable negative impact on the effectiveness of relevance ranking. For example, the frequency of query terms in them can be a misleading signal as to the actual information needs these pages can satisfy and the extent to which they can satisfy them. Our motivation for focusing on content-based shamming is threefold. First, relevance is typically determined by the ability to satisfy an information need using the page content. Second, content-based features are highly important when applying learning to rank methods for Web search [16]. Third, it is relatively easier to manipulate the content of pages compared to other derived relevance signals such as clickthrough rate, hyperlink structure, etc.",1,ad,True
22,"This paper provides two main contributions toward an initial understanding of the nature of sham. First, we describe the results of a sham annotation pilot effort applied to the publicly available ClueWeb09 benchmar2 [7]. Documents that were highly ranked in response to queries by an effective learning-to-rank (LTR) method were classified into one of three categories: spam, sham, or legitimate. We found significant inter-annotator agreement for the task of labeling sham pages. Furthermore, many top-ranked pages were agreed to be sham: some were relevant to the queries, while most were not. Second, we present an initial study of predic-",1,ClueWeb,True
23,"1Appropriately, in English the word `sham' can denote something fake or not genuine. 2www.lemurproject.org",0,,False
24,1013,0,,False
25,"Your good accident lawyer will deal with whiplash injury claims, spinal injury claims, personal injury claims, and back injury claims. It is the job of the car accident lawyer to negotiate with the insurance company to get you a high auto accident insurance settlement as well as perform other duties such as...",0,,False
26,"Figure 1: Example sham passage for the query [auto injury attorney]. In sham documents, content is manipulated, e.g., with excessive related queries (underlined), while retaining some degree of relevance.",0,,False
27,tors for when a query is likely to be the target of shamming efforts. Our results show that query-specific features outperform an approach that uses spam classification for this task as well as content-based query-independent document quality measures [2].,0,,False
28,2. RELATED WORK,0,,False
29,"There is much work on spam classification (e.g., [11, 14, 20, 8, 5, 19]) and on nullifying its effects on retrieval effectiveness [14, 9]. As already noted, the task of sham classification that we pursue here is different from spam classification by definition. Specifically, sham documents are manipulated for specific queries, and do bear useful content, while spam documents are absolutely useless. There has been some work on using query-dependent features to classify spam in its absolute query-independent sense [20]. We use query-dependent features for sham classification.",1,ad,True
30,"Using query-independent document quality measures is known to improve retrieval effectiveness [3, 2]. We note that sham documents are not necessarily of low quality. Furthermore, we show that the methods we study for the task of predicting whether a query is the target of shamming outperform methods that use content-based document quality measures which are very effective for search [2].",0,,False
31,"The realization that ""not all content that complicates ranking is spam"" was echoed in previous work [13]. However, we are not aware of any previous studies that study the difficulty of labeling Web pages as grey hat SEO, or using preand post-retrieval features to predict how likely a query is to be the target of such SEO attempts.",0,,False
32,3. SHAM ANNOTATION PROCESS,0,,False
33,"As a first step in examining the shamming phenomenon, we conducted an annotation effort, wherein a total of 300 documents, 30 queries × 10 documents per query, were evaluated. (Further details regarding the set of documents and the queries are provided in Section 5.) The documents were evenly divided between five Ph.D. and M.Sc. students in the Information Retrieval field. Each document was labeled by two annotators as being either spam, sham, or legitimate page. In case of a disagreement between the first two annotators, a third annotator was asked to label the document. Thus, for each document we have either two or three labels.",0,,False
34,"Annotators were instructed to label a document as sham if any of the following conditions was met. (i) The document contained text that did not seem to satisfy any information need, including information needs satisfied by other parts of the same document (ii) The page contained text that appeared to be related to information needs satisfied by this page, but the text contained many artificial, repeated, or otherwise unnecessary extra words, phrases or sentences added to the page solely for the purpose of promoting the page in the result lists; and (iii) The page contained sections of content that were copied from other pages (e.g.,",1,ad,True
35,from a Wikipedia page) for presumably the sole purpose of promoting the page. A document was labeled as spam if it contained no useful information at all for satisfying any information need.,1,Wiki,True
36,"The annotators were asked to base their decision only upon the content of the document, and to ignore non-content elements such as incoming and outgoing links, page URL, site domain, or ads and sponsored links that might appear on the page. The documents were presented to the annotators in a random query-independent order and the query itself was not presented.",1,ad,True
37,4. PREDICTORS FOR QUERY SHAMMING,0,,False
38,"We next examine the problem of predicting which queries are more susceptible to sham, as measured, e.g., by the fraction of sham documents in the top 10 results. To this end, we study the use of various predictors which were developed to estimate retrieval effectiveness [4]. As we show below, some of these are negatively correlate with the percentage of sham, typically because most sham documents are nonrelevant, while others have a positive correlation.",0,,False
39,"Pre-retrieval predictors are based on properties of the query and the corpus. Post-retrieval predictors utilize, in addition, information about the result list of documents, Dres, which was returned in response to the query [4]. Details regarding the retrieval model used in our experiments are provided in Section 5. In what follows we present the different classes of predictors analyzed for our prediction task.",1,ad,True
40,Pre-retrieval predictors. The SumSCQ predictor mea-,0,,False
41,"sures the similarity between the query and the collection using the sum of the TF.IDF values of the query terms [21]. SumVar [21] is the sum over query terms of the variance of the TF.IDF values of the term in documents in the corpus in which it appears. The SumIDF predictor is the sum of the IDF values of the query terms [10]. Another pre-retrieval predictor that we use is QueryLength. Since sham documents are assumed to negatively affect relevance ranking, a lower pre-retrieval predicted value is assumed to be correlated with higher percentage of sham.",1,Query,True
42,Post-retrieval predictors. The Clarity [10] prediction value,0,,False
43,is the KL divergence between a relevance language model,0,,False
44,"[15], R, induced from the documents in the result list Dres",0,,False
45,and the (unsmoothed) language model of the collection. For-,0,,False
46,"mally, let p(w|d) and s(d) be the probability assigned to",0,,False
47,term w by an unsmoothed language model induced from,0,,False
48,"document d, and the score assigned to d by the search al-",0,,False
49,"gorithm that was used to create Dres, respectively. Then,",0,,False
50,def,0,,False
51,"the probability assigned to w by R is defined as p(w|R) ,",0,,False
52,dDres p(w|d),0,,False
53,s(d) di Dres,0,,False
54,s(di) .,0,,False
55,The,0,,False
56,assumption,0,,False
57,is,0,,False
58,that,0,,False
59,sham,0,,False
60,documents for a given query are similar to one another; for,0,,False
61,"example, due to the repeated terms that they contain. Thus,",0,,False
62,1014,0,,False
63,these documents presumably form a cluster whose language,0,,False
64,model is focused with respect to that of the corpus.,0,,False
65,We also consider a variant of WIG [22] which is sim-,1,WIG,True
66,ply the average retrieval score in Dres:,0,,False
67,1 |Dres |,0,,False
68,dDres s(d).,0,,False
69,The premise is that sham documents are likely to be as-,0,,False
70,"signed high retrieval scores, as by definition they have been",0,,False
71,manipulated to be promoted in result lists.,0,,False
72,Query-independent document-quality measures. The en-,1,Query,True
73,"tropy of the term distribution in a document, Entropy, the stopwords to non-stopwords ratio in a document, SW1, and the percentage of stopwords that appear in the document, SW2, were shown to be highly effective content-based query-independent quality measures for Web search [2], and for predicting query performance over the Web [17]. We used INQUERY's stopword list [1]. The document PageRank score [3] is another measure widely used for improving retrieval effectiveness. We use these measures for our sham prediction task. The prediction value for the result list Dres is the sum of the per-document values assigned by a measure. While Entropy, SW1 and SW2 are based on the content of a document, PageRank is based on the hyperlink structure. The content-based measures quantify the presumed textual content breadth in the document. We hypothesize that the content breadth in sham documents is low, as there is focus on a small subset of terms that are the target of shamming. Conversely, since by definition sham documents can still satisfy information needs, we assume that the PageRank score of such documents is likely to be high.",1,ad,True
74,Spam-based predictor. To study the connection between,0,,False
75,"spam and sham documents, we use the recently proposed NS predictor [17]. Documents in Dres that are classified as spam by Waterloo's spam classifier are treated as ""nonrelevant""; the rest are treated as ""relevant"". Then, the mean average precision (MAP) at cutoff |Dres| is computed based on these artificially created ""relevant"" and ""non-relevant"" labels. This score is then multiplied by the total number of ""relevant"" (non-spam) documents.",1,MAP,True
76,"Query-log-based predictors. For each query, we derived",1,Query,True
77,"the following features from the query logs of a commercial search engine. RawImpressions: the total frequency the query (with stopwords) was used to search; ClicksOnResult: percentage of clicks on the search engine results page (SERP) that were on a search result vs. other parts of the page, such as query suggestions; ClicksForPaging: the percentage of clicks on the SERP used to get to the next or previous page of results. For all features we used one month of traffic from January 2013 originating from the U.S. locale.",0,,False
78,5. EVALUATION,0,,False
79,"Our experiments were conducted using the ClueWeb09 Category B collection, which contains about 50 million English Web pages. We used queries 1-150 from TREC 20092011. The Indri toolkit (www.lemurproject.org/indri) was used for experiments. We applied Krovetz stemming upon queries and documents, and removed stopwords on the INQUERY list [1] only from queries.",1,ClueWeb,True
80,"To create a highly effective result list, Dres, we did the following. First, for each query q, the documents in the collection were ranked using the negative cross entropy between",0,,False
81,"the unsmoothed unigram language model induced from q, and the Dirichlet-smoothed unigram language model induced from a document (with the smoothing parameter  set to 1000). Then, following previous work [9], documents assigned by Waterloo's spam classifier with a score below 50 were removed top to bottom from that ranking until 100 (presumably non-spam) documents were accumulated. These documents were then re-ranked using SVMrank [12] applied with 130 features. Ten-fold cross validation was performed.",0,,False
82,"We used a subset of the features used by Microsoft's learning to rank datasets3. Our features do not include the Boolean Model, Vector Space Model, LMIR.ABS, and the number of outgoing links. The SiteRank score, the two quality measures, QualityScore and QualityScore2, and the clickbased features were also not included as these information types are not available for the ClueWeb09 collection. Three additional features that we used, and which were found to be highly effective for Web retrieval [2], are Entropy, SW1, and SW2 which were described in Section 3. As is the case with the features mentioned thus far, these three features were also computed separately for all the text in the document, its anchor text, URL, body, and title. Waterloo's spam classifier score [9] is another feature that was used. The BM25 score was computed with k1,1 and b,""0.5, following experiments with b  {0.5, 0.75} and k1  {1, 1.2, 1.5, 1.7}. LMIR.DIR and LMIR.JM, the language-model-based features, were computed with "",1000 and ,""0.9, respectively. Lastly, 30 queries were randomly sampled, and the top 10 non-Wikipedia documents were used to form Dres. We assume that Wikipedia pages are not sham.""",1,LM,True
83,"We used two approaches to aggregate the labels assigned by the annotators to the documents. According to the first approach, a document is considered sham if it was labeled as such by at least one of the annotators. This `weak' label definition is henceforth referred to as AtLeastOne. The second approach, denoted AtLeastTwo, is more strict: a document is considered sham if it was marked as sham by at least two annotators.",0,,False
84,"To evaluate the quality of the various predictors, we report Pearson's correlation [4] between the values assigned by a predictor to each of the tested queries, and the percentage of sham documents computed for a query, according to the AtLeastOne and AtLeastTwo approaches described above.",0,,False
85,"The number of terms used to construct the relevance language model, which is used by the Clarity predictor, is set to 50. Given that documents assigned with a score below 50 by Waterloo's spam classifier are removed in the process of creating Dres, to determine the non-spam documents that are used by the NS predictor we use a threshold. The threshold is selected from {60, 70, 80, 90} so as to optimize the prediction quality of NS. Documents assigned with a score above that threshold are considered as non-spam (""relevant"").",0,,False
86,"Annotation Results. The inter-annotator agreement on the label pairs from the two main judges for each document was 0.69, computed using the free-marginal multi-rater kappa measure [18]. Overall, 79% of the 300 label pairs were in agreement. The average percentage of sham documents per query was 30% based on AtLeastOne labels, and 21% based on AtLeastTwo labels.",0,,False
87,Query shamming predictor analysis. The correlations of the predictors described in Section 4 with the per-,1,Query,True
88,3www.research.microsoft.com/en-us/projects/mslr,0,,False
89,1015,0,,False
90,Predictor Type,0,,False
91,Pre-retrieval,0,,False
92,Post-retrieval QueryIndependent Spam Query Log,1,Query,True
93,Predictor Name,0,,False
94,SumSCQ SumVar SumIDF QueryLength,1,Query,True
95,Clarity WIG,1,WIG,True
96,Entropy SW1 SW2 PageRank,0,,False
97,NS,0,,False
98,RawImpressions ClicksOnResult ClicksForPaging,0,,False
99,Sham Label Defn. AtLeastOne AtLeastTwo,0,,False
100,-0.500 -0.398 -0.424 -0.553,0,,False
101,-0.401 -0.337 -0.363 -0.432,0,,False
102,+0.276 +0.361,0,,False
103,+0.235 +0.351,0,,False
104,-0.324 -0.001 -0.024 +0.226,0,,False
105,-0.242 -0.022 +0.046 +0.129,0,,False
106,+0.099,0,,False
107,+0.183,0,,False
108,+0.178 -0.216 +0.277,0,,False
109,+0.231 -0.320 +0.387,0,,False
110,"Table 1: Pearson's correlation (r) of various predictor variables (Section 4) with % sham documents in the top 10 results, for weak (AtLeastOne) and strict (AtLeastTwo) sham label definitions.",0,,False
111,"centage of sham pages retrieved for a query are presented in Table 1. We can see that the pre-retrieval predictors are negatively correlated with shamming. As these predictors were designed to predict the retrieval effectiveness of using the query for search, we can conclude that sham documents are prevalent in queries whose performance is predicted to be low. Indeed, we found that about 75% of sham documents are non-relevant, for both AtLeastOne and AtLeastTwo.",0,,False
112,"QueryLength is negatively correlated with the percentage of sham documents, for both weak (AtLeastOne) and strict (AtLeastTwo) sham label definitions. That is, a shorter query has a higher chance of being the target of shamming. This might be attributed to the fact that short queries are more ambiguous, and as a result documents returned in response to these queries might be promoting the page with respect to different information needs, not necessarily the information need that the current query expresses.",1,Query,True
113,"The post-retrieval predictors, Clarity and WIG, are positively correlated with the percentage of sham documents, as hypothesized in Section 4. However, the prediction quality is lower than that for the pre-retrieval predictors.",1,WIG,True
114,"We can also see that the correlation between the queryindependent document-quality measures and shamming is relatively low. Furthermore, the NS predictor is not correlated with sham. These findings suggest that sham documents might be of either high or low quality, and that there is no evident connection between sham and spam documents.",0,,False
115,"For query log features, the positive correlation of RawImpressions with the level of sham is consistent with the fact that frequent queries are more susceptible to shamming. ClicksForPaging is positively correlated with the percentage of sham, while the ClicksOnResult feature is negatively correlated. These findings resonate with the fact that shamming degrades retrieval performance.",1,ad,True
116,"Finally, for the task of predicting sham percentage for a given query at low (< 0.3), medium ( [0.3, 0.6)), or high ( 0.6) levels, we performed ordinal regression [6] using all the predictors with default regression settings. The mean absolute error (MAE), the absolute difference between the predicted class ordinal and the true ordinal, averaged over 100 randomized trials with a 2:1 train/test split, was 0.532. For comparison, an oracle run using one rater's sham labels to predict sham levels derived from other raters' labels, had",1,ad,True
117,MAE 0.37; while always guessing category `medium' had MAE 0.67. These initial results show that different predictors can be effectively integrated for predicting per-query sham levels.,1,ad,True
118,6. CONCLUSION,0,,False
119,"We have provided an initial study on the identification of sham documents: a form of grey-hat SEO using contentbased feature manipulation, along with an analysis of features associated with low- or high-sham queries. Even search engines that use highly effective ranking methods still suffer from sham documents in the top-most rankings that adversely affect retrieval quality. These findings call for a principled treatment of query-specific grey hat SEO.",1,ad,True
120,Acknowledgments. We thank the reviewers for their comments. This work was supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center.,0,,False
121,7. REFERENCES,0,,False
122,"[1] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proc. of TREC, pages 551­562, 2000.",1,TREC,True
123,"[2] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of web documents. In Proc. of WSDM, pages 95­104, 2011.",0,,False
124,"[3] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In Proc. of WWW, pages 107­117, 1998.",0,,False
125,"[4] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool, 2010.",1,Query,True
126,"[5] C. Castillo, D. Donato, L. Becchetti, P. Boldi, S. Leonardi, M. Santini, and S. Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11­24, 2006.",0,,False
127,"[6] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. J. Machine Learning Res., 6:1019­1041, 2005.",0,,False
128,"[7] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 Web track. In Proc. of TREC, 2009.",1,TREC,True
129,"[8] G. V. Cormack. TREC 2007 spam track overview. In Proc. of TREC, 2007.",1,TREC,True
130,"[9] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 14(5):441­465, 2011.",0,,False
131,"[10] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proc. of SIGIR, pages 299­306, 2002.",0,,False
132,"[11] Z. Gy¨ongyi and H. Garcia-Molina. Web spam taxonomy. In Proc. of AIRWeb, pages 39­47, 2005.",0,,False
133,"[12] T. Joachims. Training linear SVMs in linear time. In Proc. of KDD, pages 217­226, 2006.",0,,False
134,"[13] T. Jones, R. S. Sankaranarayana, D. Hawking, and N. Craswell. Nullification test collections for web spam and SEO. In Proc. of AIRWeb, pages 53­60, 2009.",0,,False
135,"[14] V. Krishnan and R. Raj. Web spam detection with Anti-Trust rank. In Proc. of AIRWeb, pages 37­40, 2006.",0,,False
136,"[15] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. of SIGIR, pages 120­127, 2001.",0,,False
137,"[16] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3), 2009.",0,,False
138,"[17] F. Raiber and O. Kurland. Using document-quality measures to predict web-search effectiveness. In Proc. of ECIR, pages 134­145, 2013.",0,,False
139,"[18] J. J. Randolph. Online kappa calculator (2008). Retrieved February 16, 2013, http://justus.randolph.name/kappa.",0,,False
140,"[19] D. Sculley and G. Wachman. Relaxed online SVMs for spam filtering. In Proc. of SIGIR, pages 415­422, 2007.",0,,False
141,"[20] K. M. Svore, Q. Wu, C. Burges, and A. Raman. Improving web spam classification using rank-time features. In Proc. of AIRWeb, 2007.",0,,False
142,"[21] Y. Zhao, F. Scholer, and Y. Tsegay. Effective pre-retrieval query performance prediction using similarity and variability evidence. In Proc. of ECIR, pages 52­64, 2008.",0,,False
143,"[22] Y. Zhou and B. Croft. Query performance prediction in web search environments. In Proc. of SIGIR, pages 543­550, 2007.",1,Query,True
144,1016,0,,False
145,,0,,False

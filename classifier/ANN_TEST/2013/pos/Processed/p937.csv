,sentence,label,data,regex
0,"Relating Retrievability, Performance and Length",0,,False
1,Colin Wilkie and Leif Azzopardi,0,,False
2,"School of Computing Science, University of Glasgow",0,,False
3,"Glasgow, United Kingdom",0,,False
4,"{Colin.Wilkie,Leif.Azzopardi}@glasgow.ac.uk",0,,False
5,ABSTRACT,0,,False
6,"Retrievability provides a different way to evaluate an Information Retrieval (IR) system as it focuses on how easily documents can be found. It is intrinsically related to retrieval performance because a document needs to be retrieved before it can be judged relevant. In this paper, we undertake an empirical investigation into the relationship between the retrievability of documents, the retrieval bias imposed by a retrieval system, and the retrieval performance, across different amounts of document length normalization. To this end, two standard IR models are used on three TREC test collections to show that there is a useful and practical link between retrievability and performance. Our findings show that minimizing the bias across the document collection leads to good performance (though not the best performance possible). We also show that past a certain amount of document length normalization the retrieval bias increases, and the retrieval performance significantly and rapidly decreases. These findings suggest that the relationship between retrievability and effectiveness may offer a way to automatically tune systems.",1,TREC,True
7,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Performance Evaluation,0,,False
8,"Terms Theory, Experimentation Keywords Retrievability, Simulation",0,,False
9,1. INTRODUCTION,1,DUC,True
10,"Traditionally IR systems are evaluated in terms of efficiency and performance [13]. However IR systems can also be evaluated in terms of retrievability [3], which assesses how often documents are retrieved (as opposed to the speed of retrieval and the quality of retrieval respectively). Retrievability is fundamental to IR because it precedes relevancy [3]. In this paper, we investigate the relationship between retrievability and retrieval performance in the context of ad-hoc topic retrieval for two standard best match retrieval models. To this end, we shall first formally define retrievability be-",1,ad,True
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
12,"fore discussing how it relates to performance. Next, we perform an empirical analysis where we hypothesize that lower retrievability bias will lead to better retrieval performance.",1,ad,True
13,2. BACKGROUND,0,,False
14,"In [3], retrievability was defined as a measure that provides an indication of how easily a document could be retrieved under a particular configuration of an IR system. Put formally the retrievability r(d) of a document d with respect to an IR system is:",0,,False
15,"r(d)  f (kdq, {c, g})",0,,False
16,(1),0,,False
17,qQ,0,,False
18,"where q is a query in a very large query set Q, kdq is the rank at which d is retrieved given q, and f (kdq, c) is an access function which denotes how retrievable the document d is for the query q given the rank cutoff c. This implies that retrievability of a document is obtained by summing over all possible queries Q. The more queries that retrieve the document, the more retrievable the document is.",0,,False
19,"A simple measure of retrievability is a cumulative-based approach, which employs an access function f (kdq, c), such that if d is retrieved in the top c documents given q, then f (kdq, c) ,"" 1 else f (kdq, c) "","" 0. Essentially, this measure provides an intuitive value for each document as it is simply the number of times that the document is retrieved in the top c documents. A gravity based approach instead weights the rank at which the document is retrieved, as follows: f (kdq, g) "","" 1/kdqg. This introduces a discount g, such that documents lower down the list are assumed to be less retrievable. Retrievability Bias: To quantify the retrievability bias across the collection, the Gini coefficient [8] is often used [3, 5, 6] as it measures the inequality within a population. For example, if all documents were equally retrievable according to r(d), then the Gini coefficient would be zero (denoting equality), but if all but one document had r(d) "","" 0 then the Gini coefficient would be one (denoting total inequality). Usually, most documents have some level of retrievability and the Gini coefficient is somewhere between zero and one. Essentially, the Gini coefficient provides an indication of the level of inequality between documents given how easily they can be retrieved using a particular retrieval system and configuration. Uses: Retrievability - and the theory of - has been used in numerous contexts, for example, in the creation of reverted indexes that improve the efficiency and performance of retrieval systems by capitalizing on knowing what terms""",1,ad,True
20,937,0,,False
21,"within a document makes that document more retrievable [10]. Retrievability has also been used to study search engine and retrieval system bias on the web [2] and within patent collections [4] to improve the efficiency of systems when pruning [14]. It has also been related to navigability when tagging information to improve how easily users browsing through the collection could find documents [12]. While these show that retrievability is useful in a number of respects, here, we are interested in how retrievability relates to performance. There have been a few works exploring this research direction in different ways [3, 1, 5, 6]. Relating Retrievability and Performance: In [1], Azzopardi discusses the relationship with respect to the definition of retrievability and claim that a purely random IR system would ensure equal retrievability (resulting in a Gini ,"" 0). However, the author argues that this would also result in very poor retrieval performance. Conversely, if an oracle IR system retrieved only the set of known relevant documents, and only these documents, then there would be a very high inequality in terms of retrievability across the collection. They suggest that neither extreme is desirable and instead suggest that there is likely to be a trade-off between retrievability and retrieval performance. In [3], it is acknowledged that some level of bias is necessary because a retrieval system must try and discriminate relevant from non-relevant. The preliminary study conducted in [1] investigating this relationship on two small TREC test collections showed that as retrieval performance increased, the retrievability bias tended to decrease: suggesting a positive and useful relationship.""",1,ad,True
22,"In [5], Bashir and Rauber studied the effect of Pseudo Relevance Feedback (PRF) on performance and retrievability. They found that standard Query Expansion methods, while increasing performance, also increased the retrievability bias. To combat the increase in bias, they devised a method of PRF that used clustering; this resulted in a reduction in bias, as well as an increase in performance over other QE techniques. When employing their PRF technique to patent retrieval, they showed that the decrease in bias led to improved recall for prior art search [6]. This later work suggested that there may be a positive correlation between recall and retrievability. In [7], they compared a number of retrieval models in terms of retrievability bias and performance as a way to rank different retrieval systems. They found that there was a positive correlation between the bias and performance for standard best match retrieval models. It is important to note that since retrievability can be estimated without recourse to relevance judgements it provides an attractive alternative for evaluation.",1,Query,True
23,3. EXPERIMENTAL METHOD,0,,False
24,"Previous work suggests that the relationship between retrievability bias and retrieval performance is much more complicated than previously thought and the relationship is somewhat unclear. In this paper, we shall perform a more detailed analysis across larger TREC collections and explicitly plot the relationship between the retrievability bias and retrieval performance in order to form a deeper understanding of how they relate to each other.",1,TREC,True
25,"To this end, three TREC test collections were used AP, Aquaint (AQ) and DotGov (DG) along with their corresponding ad-hoc retrieval topics (see Table 1 for details). These collections had stop words removed and were Porter",1,TREC,True
26,"stemmed. For the purposes of this study, we will focus on the effect of document length normalization within two standard retrieval models (Okapi BM25 and DFRs PL2). This is due to the fact that retrieval models will often favour longer documents which introduces an undesirable retrieval bias that if accounted for, has been shown to improve performance [11]. For each of these retrieval models we shall investigate how retrievability bias and performance changes as we manipulate the normalization parameter b and c respectively. In our experiments for BM25's b parameter we used values between 0 and 1 with steps of 0.11, and for PL2's c parameter we used values 0.1, 1, . . . 10, 100 with steps of 1 between 1 and 10.",0,,False
27,"For each collection, model and parameter value, we recorded the retrieval performance (Mean Average Precision (MAP) and Precision @ 10 (P@10)) and calculated the retrievability bias using the approach employed in prior work [3, 1, 5, 7]. This was performed as follows; we extracted all terms that co-occurred more than once with each other from the collection and used these as bigram queries (Table 1 shows the number of queries per collection issued). These queries were issued to the retrieval system given its parametrization and the results were used to compute retrievability measures. We computed both cumulative scores with cut-offs of 5, 10, 20, 50, 70, 100, and gravity based scores with cut-off of 100 and  (where  is the discount factor) values of 0.5, 1, 1.5 and 2. For brevity, we only report on a subset of these. These were used to compute the corresponding Gini coefficient for each measure, model, normalization parameter and collection.",1,MAP,True
28,Collections Docs,0,,False
29,Collection Type Trec Topics,0,,False
30,# Bigrams/Queries,0,,False
31,"AP 164,597",1,AP,True
32,News 1-200 81964,0,,False
33,"AQ 1,033,461",0,,False
34,News 303-369 273245,0,,False
35,"DG 1,247,753",0,,False
36,Web 551 - 600,0,,False
37,337275,0,,False
38,Table 1: Collection Statistics,0,,False
39,4. RESULTS,0,,False
40,During the course of our analysis we examined the following research questions:,0,,False
41,"How does the retrievability bias change across length normalization parameters? In Figure 1, the left hand plots show the relationship between the retrievability bias (denoted by the Gini coefficient for different retrievability measures) and the parameter settings for BM25 and PL2. Immediately we can see that as the parameter setting is manipulated, the retrievability bias changes. For BM25, the minimum bias was when b,""0.7 on AP and AQ, and b"",""0.9 on DG, whereas on PL2 the minimum bias was was between c"",1 and c,""3 (see Table 2). This was regardless of the Gini measure used i.e. the cumulative and gravity based measures resulted in the same minimum. Subsequently, for the remainder of this paper, we will use the gravity based retrievability measure when g "","" 1. What was different between Gini measures was the magnitude of the bias. For example, when the cut-off is low then the bias observed was higher than when the cutoff was increased. This finding is consistent with prior work [3]. When the parameter value of the retrieval was increased or decreased, then we observed that retrieval biased increased, sometimes quite dramatically. This suggests that either longer or shorter documents""",1,AP,True
42,"1When b ,"" 0, this is equivalent to BM15 and when b "", 1 this is equivalent to BM11",0,,False
43,938,0,,False
44,Gini Coefficient,0,,False
45,1 0.95,0,,False
46,0.9 0.85,0,,False
47,0.8 0.75,0,,False
48,0.7 0.65,0,,False
49,0.6 0.55,0,,False
50,0.5 0,0,,False
51,1 0.95,0,,False
52,0.9 0.85,0,,False
53,0.8 0.75,0,,False
54,0.7 0.65,0,,False
55,0,0,,False
56,Aquaint BM25 gini vs b,0,,False
57,C10 C100 G0.5 G1.0 G1.5,0,,False
58,0.2,0,,False
59,0.4,0,,False
60,0.6,0,,False
61,0.8,0,,False
62,BM25 b Parameter Values,0,,False
63,.Gov BM25 gini vs b,1,Gov,True
64,C10 C100 G0.5 G1.0 G1.5,0,,False
65,0.2,0,,False
66,0.4,0,,False
67,0.6,0,,False
68,0.8,0,,False
69,BM25 b Parameter Values,0,,False
70,Gini Coefficient,0,,False
71,1 0.95,0,,False
72,0.9 0.85,0,,False
73,0.8 0.75,0,,False
74,0.7 0.65,0,,False
75,1,0,,False
76,0,0,,False
77,0.98,0,,False
78,0.96,0,,False
79,0.94,0,,False
80,Gini Coefficient,0,,False
81,0.92,0,,False
82,0.9,0,,False
83,0.88,0,,False
84,0.86,0,,False
85,0.84,0,,False
86,0.82,0,,False
87,0.8,0,,False
88,1,0,,False
89,0,0,,False
90,Aquaint PL2 gini vs. c,0,,False
91,C10 C100 G0.5 G1.0 G1.5,0,,False
92,2,0,,False
93,4,0,,False
94,6,0,,False
95,8,0,,False
96,10,0,,False
97,PL2 c Parameter Values,0,,False
98,.Gov PL2 gini vs. c,1,Gov,True
99,2,0,,False
100,4,0,,False
101,6,0,,False
102,8,0,,False
103,PL2 c Parameter Values,0,,False
104,C10 C100 G0.5 G1.0 G1.5,0,,False
105,10,0,,False
106,Average R(d),0,,False
107,Average R(d),0,,False
108,Aquaint BM25 (G1.0) R(d) vs. Length,0,,False
109,10,0,,False
110,0.1,0,,False
111,9,0,,False
112,0.7,0,,False
113,8 0.9,0,,False
114,7,0,,False
115,6,0,,False
116,5,0,,False
117,4,0,,False
118,3,0,,False
119,2,0,,False
120,1,0,,False
121,0,0,,False
122,0,0,,False
123,500,0,,False
124,1000,0,,False
125,1500,0,,False
126,Average Length in Bucket,0,,False
127,.Gov BM25 (G1.0) R(d) vs. Length,1,Gov,True
128,20,0,,False
129,0.1,0,,False
130,18,0,,False
131,0.7,0,,False
132,16 0.9,0,,False
133,14,0,,False
134,12,0,,False
135,10,0,,False
136,8,0,,False
137,6,0,,False
138,4,0,,False
139,2,0,,False
140,0,0,,False
141,0,0,,False
142,5000,0,,False
143,10000,0,,False
144,15000,0,,False
145,Average Length in Bucket,0,,False
146,Average R(d),0,,False
147,Average R(d),0,,False
148,Aquaint PL2 (G1.0) R(d) vs. Length,0,,False
149,10,0,,False
150,1,0,,False
151,9,0,,False
152,3,0,,False
153,8 10,0,,False
154,7,0,,False
155,6,0,,False
156,5,0,,False
157,4,0,,False
158,3,0,,False
159,2,0,,False
160,1,0,,False
161,0,0,,False
162,0,0,,False
163,500,0,,False
164,1000,0,,False
165,1500,0,,False
166,Average Length in Bucket,0,,False
167,.Gov PL2 (G1.0) R(d) vs. Length,1,Gov,True
168,20,0,,False
169,1,0,,False
170,18,0,,False
171,3,0,,False
172,16,0,,False
173,10,0,,False
174,14,0,,False
175,12,0,,False
176,10,0,,False
177,8,0,,False
178,6,0,,False
179,4,0,,False
180,2,0,,False
181,0,0,,False
182,0,0,,False
183,5000,0,,False
184,10000,0,,False
185,Average Length in Bucket,0,,False
186,15000,0,,False
187,Gini Coefficient,0,,False
188,Figure 1: Left: Retrieval Bias versus b/c. Right: Retrievability versus Document Length,0,,False
189,Collections AP,1,AP,True
190,Aquaint .Gov,1,Gov,True
191,Model,0,,False
192,BM25 PL2,0,,False
193,BM25 PL2,0,,False
194,BM25 PL2,0,,False
195,Minimum Gini (G1.0) b/c Gini MAP P10,1,MAP,True
196,0.7 0.643 0.232 0.354 3 0.701 0.235 0.374,0,,False
197,0.7 0.701 0.162 0.316 2 0.746 0.169 0.331,0,,False
198,0.9 0.749 0.167 0.222 1 0.872 0.182 0.220,0,,False
199,Maximum Map (G1.0) b/c Gini MAP P10,1,MAP,True
200,0.3 0.708 0.239* 0.378* 3 0.701 0.235 0.374,0,,False
201,0.3 0.785 0.185* 0.390* 6 0.774 0.178 0.374*,0,,False
202,0.6 0.707 0.179 0.216 2 0.883 0.184 0.222,0,,False
203,Min. Gini (G1.0) Corr. Gini-MAP Gini-P10,1,MAP,True
204,-0.075 -0.978*,0,,False
205,0.184 -0.925,0,,False
206,0.186 -0.844*,0,,False
207,0.451 -0.523,0,,False
208,-0.646 -0.887,0,,False
209,-0.775 -0.916*,0,,False
210,Table 2: Minimum Gini/Maximum MAP and corresponding parameter settings/values.  represent statistical significance. Far right columns show the Pearson's Correlation between Retrievability Bias and performance.,1,MAP,True
211,"were being favoured depending on the setting of b and c. In BM25, as b tends to 1, longer documents are penalized, while when b tends to 0, longer documents are favoured. For PL2, as c tends to 0 then longer documents are penalized, and as c tends to infinity, longer documents are favoured.",0,,False
212,"How does the retrievability of documents change across length normalization parameters? To examine this intuition, we sorted the documents in each collection by length and placed them into buckets. We then computed the mean length within each bucket, and the mean retrievability r(d) given the documents in the bucket. The plots on the right in Figure 1 show how the retrievability changes when the length normalization parameter changes on BM25/PL2 and AQ/DG for a subset of parameters with a retrievability measure of g ,"" 1. The plots clearly indicate that when the retrieval bias is minimized (for example, when b "", 0.7 on AQ) the retrievability across length is about as equal/fair as it gets across the collection. This shows that the measures of retrievability bias (i.e. the Gini coefficients) capture the bias towards longer and shorter documents as the document length parameter is manipulated.",0,,False
213,"What is the relationship between Performance and Retrievability? In Figure 2, we plotted the Gini coefficient against MAP for each collection using the results from BM25. Similar plots can be found in Figure 3 for other retrieval models and other performance measures. On these plots, a large diamond has been added to the lines to indicate the length normalization parameter that favours longer documents (ie. b , 0 and c , 100). As the parameter value is increased/decreased for BM25/PL2 the retrieval model",1,MAP,True
214,setting favours shorter documents (as shown in the plots in Figure 1).,0,,False
215,"The plots provide a number of key insights into the relationship between retrievability bias and performance. Firstly, the relationship is non-linear with a trend that suggests that minimizing bias leads to better performance. If we consider the relationship in Figure 2 for DotGov, we can see that a reduction in bias leads to successive improvements in terms of performance for MAP (and P@10). With this collection, the effect is the most pronounced and minimizing retrieval bias does tend to the best retrieval performance. Once too much length normalization has occurred, such that shorter documents are overly favoured, then the retrievability bias increases and performance begins to decrease.",1,ad,True
216,"When we examine the relationship for AP and AQ, we see that as shorter documents become more favoured, a trade-off between bias and performance quickly develops, but again once the minimum bias point has been reached, the kick back (i.e. loss in performance and increase in bias) is much more pronounced. These findings suggest that as document length normalization is applied in order to penalize longer documents, going past the point of minimum bias will degrade performance and retrievability.",1,AP,True
217,"Table 2 shows the performance of the retrieval models when bias is minimized and when performance is maximized. Included in the table are the results of t-tests conducted to determine whether there was a statistical difference in performance between the two settings (assuming p < 0.05,  indicates a significant difference). For AP and AQ, we found there was a significant difference for BM25 (both MAP and",1,AP,True
218,939,0,,False
219,"P@10), however the differences were less pronounced for PL2 and on DotGov. These findings suggest that minimizing bias may not lead to the best performance. It does however, tend to give reasonably good retrieval performance that on many occasions, is not significantly different from the best possible retrieval performance. We speculate that the mis-match between may be due to the length bias with the TREC relevance pools as discovered in [9]. Thus, in the absence of relevance judgements tuning a retrieval system such that it minimizes retrieval bias is likely to be a good starting point.",1,Gov,True
220,"AP, Aquaint & .Gov BM25 (G1.0) MAP vs. gini",1,AP,True
221,1,0,,False
222,AP,1,AP,True
223,0.95,0,,False
224,Aquaint,0,,False
225,.Gov,1,Gov,True
226,0.9,0,,False
227,0.85,0,,False
228,Gini,0,,False
229,0.8,0,,False
230,0.75,0,,False
231,0.7,0,,False
232,0.65,0,,False
233,0.05,0,,False
234,0.1,0,,False
235,0.15,0,,False
236,0.2,0,,False
237,0.25,0,,False
238,MAP,1,MAP,True
239,Figure 2: MAP vs. Gini for all collections on BM25.,1,MAP,True
240,5. SUMMARY,0,,False
241,"In this paper we have analysed the relationship between retrievability bias and performance in the context of adhoc retrieval across length normalization parameters for two standard IR models. Empirically, we have shown that the relationship is much more complex that previously thought and in fact non-linear. Nonetheless, we have shown that reducing the bias within the collection leads to reasonably good performance, and crucially, if too much document length normalization is performed then this will invariably result in a degradation of performance and an increase in bias. This is a useful finding suggesting that retrievability could be used to tune retrieval systems without recourse to relevance judgements. Future work will be directed to studying the relationship between retrievability and performance (in terms of MAP and P@10) in more detail across other collections and across different parameter settings (i.e. query parameters, smoothing parameters) and with different retrieval models (language models, query expansion, link/click evidence, etc).",1,adhoc,True
242,"Acknowledgments This work is supported by the EPSRC Project, Models and Measures of Findability (EP/K000330/1).",0,,False
243,6. REFERENCES,0,,False
244,"[1] L. Azzopardi and R. Bache. On the relationship between effectiveness and accessibility. In Proc. of the 33rd international ACM SIGIR, pages 889­890, 2010.",0,,False
245,"[2] L. Azzopardi and C. Owens. Search engine predilection towards news media providers. In Proc. of the 32nd ACM SIGIR, pages 774­775, 2009.",0,,False
246,Gini,0,,False
247,Gini,0,,False
248,"AP, Aquaint & .Gov BM25 (G1.0) MAP vs. gini",1,AP,True
249,1 AP,1,AP,True
250,0.95,0,,False
251,Aquaint,0,,False
252,.Gov 0.9,1,Gov,True
253,0.85,0,,False
254,0.8,0,,False
255,0.75,0,,False
256,0.7,0,,False
257,0.65,0,,False
258,0.05,0,,False
259,0.1,0,,False
260,0.15,0,,False
261,0.2,0,,False
262,0.25,0,,False
263,MAP,1,MAP,True
264,"AP, Aquaint & .Gov BM25 (G1.0) P@10 vs. gini",1,AP,True
265,1 AP,1,AP,True
266,0.95,0,,False
267,Aquaint,0,,False
268,.Gov 0.9,1,Gov,True
269,0.85,0,,False
270,0.8,0,,False
271,0.75,0,,False
272,0.7,0,,False
273,0.65,0,,False
274,0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 P@10,0,,False
275,Gini,0,,False
276,Gini,0,,False
277,"AP, Aquaint & .Gov PL2 (G1.0) MAP vs. gini",1,AP,True
278,1 AP,1,AP,True
279,0.95,0,,False
280,Aquaint,0,,False
281,.Gov,1,Gov,True
282,0.9,0,,False
283,0.85,0,,False
284,0.8,0,,False
285,0.75,0,,False
286,0.7,0,,False
287,0.65,0,,False
288,0.05,0,,False
289,0.1,0,,False
290,0.15,0,,False
291,0.2,0,,False
292,0.25,0,,False
293,MAP,1,MAP,True
294,"AP, Aquaint & .Gov PL2 (G1.0) P@10 vs. gini",1,AP,True
295,1 AP,1,AP,True
296,0.95,0,,False
297,Aquaint,0,,False
298,.Gov,1,Gov,True
299,0.9,0,,False
300,0.85,0,,False
301,0.8,0,,False
302,0.75,0,,False
303,0.7,0,,False
304,0.65 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 P@10,0,,False
305,Figure 3: MAP (Top plots) and P@10 (Bottom plots) vs. Gini for all collections on BM25 (right plots) and PL2 (left plots).,1,MAP,True
306,"[3] L. Azzopardi and V. Vinay. Retrievability: An evaluation measure for higher order information access tasks. In Proc. of the 17th ACM CIKM, pages 561­570, 2008.",0,,False
307,"[4] R. Bache. Measuring and improving access to the corpus. In Current Challenges in Patent Information Retrieval, volume 29 of The Information Retrieval Series, pages 147­165. 2011.",0,,False
308,"[5] S. Bashir and A. Rauber. Improving retrievability of patents with cluster-based pseudo-relevance feedback documents selection. In Proc. of the 18th ACM CIKM, pages 1863­1866, 2009.",0,,False
309,"[6] S. Bashir and A. Rauber. Improving retrievability of patents in prior-art search. In Proc. of the 32nd ECIR, pages 457­470, 2010.",0,,False
310,"[7] S. Bashir and A. Rauber. Improving retrievability and recall by automatic corpus partitioning Transactions on large-scale data- and knowledge-centered systems ii. chapter I, pages 122­140. 2010.",0,,False
311,"[8] J. Gastwirth. The estimation of the lorenz curve and gini index. The Review of Economics and Statistics, 54:306­316, 1972.",0,,False
312,"[9] D. E. Losada, L. Azzopardi, and M. Baillie. Revisiting the relationship between document length and relevance. In Proc. of the 17th ACM CIKM'08, pages 419­428, 2008.",1,ad,True
313,"[10] J. Pickens, M. Cooper, and G. Golovchinsky. Reverted indexing for feedback and expansion. In Proc. of the 19th ACM CIKM, pages 1049­1058, 2010.",0,,False
314,"[11] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proc. of the 19th ACM SIGIR, pages 21­29, 1996.",0,,False
315,"[12] A. Singla and I. Weber. Tagging and navigability. In Proc. of the 19th WWW, pages 1185­1186, 2010.",0,,False
316,[13] C. J. van Rijsbergen. Information Retrieval. 1979. [14] L. Zheng and I. J. Cox. Document-oriented pruning of,0,,False
317,"the inverted index in information retrieval systems. In Proc. of the 2009 WIANA, pages 697­702, 2009.",0,,False
318,940,0,,False
319,,0,,False

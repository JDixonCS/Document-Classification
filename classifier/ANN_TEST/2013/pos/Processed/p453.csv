,sentence,label,data,regex
0,Utilizing Query Change for Session Search,1,Query,True
1,"Dongyi Guan, Sicong Zhang, Hui Yang",0,,False
2,Department of Computer Science Georgetown University,0,,False
3,"37th and O Street, NW, Washington, DC, 20057",0,,False
4,"{dg372, sz303}@georgetown.edu, huiyang@cs.georgetown.edu",0,,False
5,ABSTRACT,0,,False
6,"Session search is the Information Retrieval (IR) task that performs document retrieval for a search session. During a session, a user constantly modifies queries in order to find relevant documents that fulfill the information need. This paper proposes a novel query change retrieval model (QCM), which utilizes syntactic editing changes between adjacent queries as well as the relationship between query change and previously retrieved documents to enhance session search. We propose to model session search as a Markov Decision Process (MDP). We consider two agents in this MDP: the user agent and the search engine agent. The user agent's actions are query changes that we observe and the search agent's actions are proposed in this paper. Experiments show that our approach is highly effective and outperforms top session search systems in TREC 2011 and 2012.",1,Session,True
7,Categories and Subject Descriptors,0,,False
8,H.3.3 [Information Systems ]: Information Storage and Retrieval--Information Search and Retrieval,0,,False
9,Keywords,0,,False
10,Session search; query change model; retrieval model,1,Session,True
11,1. INTRODUCTION,1,DUC,True
12,"Session search is the Information Retrieval (IR) task that retrieves documents for a search session [4, 8, 13, 14, 15, 25, 32]. During a search session, a user keeps modifying queries in order to find relevant documents that fulfill his/her information needs. In session search, many factors, such as relevance feedback, clicked data, changes in queries, and user intentions, are intertwined together and make it a quite challenging IR task. TREC (Text REtrieval Conference) 20102012 Session tracks [18, 19, 20] studied session search with a focus on the ""current query"" task, which retrieves relevant documents for the current/last query in a session based on previous queries and interactions. Table 1 shows examples from the TREC 2012 Session track.1",1,Session,True
13,"1All examples mentioned in this paper are from TREC 2012. For simplicity, we use `sx' to refer to a TREC 2012 session where x is the session identification number.",1,TREC,True
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM or the author must be honored. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
15,Table 1: Examples of TREC 2012 Session queries.,1,TREC,True
16,session 6 1.pocono mountains pennsylvania 2.pocono mountains pennsylvania hotels 3.pocono mountains pennsylvania things to do 4.pocono mountains pennsylvania hotels 5.pocono mountains camelbeach 6.pocono mountains camelbeach hotel 7.pocono mountains chateau resort 8.pocono mountains chateau resort attractions 9.pocono mountains chateau resort getting to 10.chateau resort getting to 11.pocono mountains chateau resort directions session 85 1.glass blowing 2.glass blowing science 3.scientific glass blowing,0,,False
17,session 28 1.france world cup 98 reaction stock market 2.france world cup 98 reaction 3.france world cup 98 session 32 1.bollywood legislation 2.bollywood law session 37 1.Merck lobbists 2.Merck lobbying US policy,0,,False
18,"From Table 1, we notice that queries change constantly in a session. The patterns of query changes include general to specific (pocono mountains  pocono mountains park), specific to general (france world cup 98 reaction  france world cup 98), drifting from one to another (pocono mountains park  pocono mountains shopping), or slightly different expressions for the same information need (glass blowing science  scientific glass blowing). These changes vary and sometimes even look random (gun homicides australia  martin bryant port arthur massacre), which increases the difficulty of understanding user intention. However, since query changes are made after the user examines search results, we believe that query change is an important form of feedback. We hence propose to study and utilize query changes to facilitate better session search.",1,ad,True
19,"One approach to handle query change is to classify them based on various types of explorations [20], such as specification, generalization, drifting, or slight change, then perform retrieval. Another approach is mapping queries into semantic graphical representations, such as ontologies [7] or query flow graphs developed from query logs [2], then studying how queries move in the graphs. However, ontology mapping is challenging [17], which may introduce inaccurate intermediate results and hurt the search accuracy. Moreover, relying on large scale query logs may not be applicable due to lack of such data. Therefore, although these approaches have been applied to IR tasks such as query reformulation [3] and query suggestion [2, 30], they have yet to be directly applied to session search. It is therefore necessary to explore new solutions to utilize query change for session search.",0,,False
20,"We propose to model session search as a Markov Decision Process (MDP) [16, 28], which is applicable to many human decision processes. MDP models a state space and an action space for all agents participating in the process. Actions from the agents influence the environment/states and the environment/states influence the agents' subsequent actions",0,,False
21,453,0,,False
22,Figure 1: Session search MDP. (example is from s32),1,Session,True
23,"based on certain policies. A transition model between states indicates the dynamics of the entire system. In our MDP, queries are modeled as states. Previous queries that the user wrote influence the search results; the search results again influence the user's decision of the next query. This interaction continues until the search stops.",0,,False
24,"As illustrated in Figure 1, we consider two agents in this entire process: the user agent and the search engine agent. The user agent's actions are mainly human actions that are able to change search results, such as adding and deleting query terms, i.e. query change. Clicking is a human action; however, it does not explicitly impact the retrieval. Therefore, it is not considered as a user action here. Query change is the only form of user action in this paper. Based on the user actions, we design corresponding policies for the search engine agent; which is the main focus of this paper.",1,ad,True
25,"It is difficult to interpret the user intent [5, 31] behind query change. For instance, for a query change from Kurosawa to Kurosawa wife (s38), there is no indication about `wife' in the search results returned for the first query. However, Kurosawa's wife is actually among the information needs provided to the user by NIST's topic descriptions. Our experience with TREC Session tracks suggests that information needs and previous search results are two main factors that influence query change. However, knowing information needs before search could not easily be achieved. This paper focuses on utilizing evidence found in previous search results and the relationship between previous search results and query change to improve session search.",1,TREC,True
26,"In this paper, we summarize various types of query changes based on potential user intents into user agent policies. We further propose corresponding policies for the search engine agent and model them in the query change retrieval model (QCM), a novel reinforcement learning [16] inspired framework. The relevance of a document to the current query is recursively calculated as the reward beginning from the starting query and continuing until the current query. This research is perhaps the first to employ reinforcement learning to tackle session search. Our experiments demonstrate that the proposed approach is highly effective and outperforms the best performing TREC 2011 and 2012 session search systems.",1,TREC,True
27,"The remainder of this paper is organized as follows. Section 2 analyzes query change and summarizes policies for the user agent. Section 3 proposes policies for the search engine agent. Section 4 elaborates the query change retrieval model. Section 5 discusses how to handle duplicated queries. Section 6 evaluates our approach, followed by a discussion in Section 7. Section 8 presents the related work and Section 9 concludes the paper.",0,,False
28,Table 2: Evidence that query change q appears in previous,0,,False
29,search results Di-1.,0,,False
30,qtheme +q,0,,False
31,# in TREC'11,1,TREC,True
32, Di-1,0,,False
33,/ Di-1,0,,False
34,184,0,,False
35,20,0,,False
36,80,0,,False
37,124,0,,False
38,# in TREC'12,1,TREC,True
39, Di-1,0,,False
40,/ Di-1,0,,False
41,178,0,,False
42,21,0,,False
43,97,0,,False
44,102,0,,False
45,-q,0,,False
46,141,0,,False
47,63,0,,False
48,112,0,,False
49,87,0,,False
50,Total,0,,False
51,"204 adjacent query pairs, 76 sessions",1,ad,True
52,"199 adjacent query pairs, 98 sessions",1,ad,True
53,2. USER AGENT: QUERY CHANGE AS A,0,,False
54,FORM OF FEEDBACK,0,,False
55,"We define a search session S ,"" {Q, D, C} as a combination of a series of queries Q "","" {q1, ..., qi, ..., qn}, retrieved document sets D "","" {D1, ..., Di, ..., Dn}, and clicked information C "","" {C1, .., Ci, ..., Cn}, where n is the number of queries in the session (i.e., the session length) and i indexes the queries. In TREC 2010-2012 Session tracks, each retrieved document set Di contains the top 10 retrieval results di1, ..., di10 ranked in decreasing relevance for qi. Each clicked data Ci contains the user-clicked documents, clicking order, and dwell time. For instance, for s6 q6, pocono mountains camelbeach hotel (Table 1), C6 tells us that the user clicked the 4th ranked search result, followed by the 2nd, with dwell time 15 seconds and 17 seconds, respectively.""",1,TREC,True
56,"TREC 2010-2012 Session Tracks aim to retrieve a list of documents for the current query, i.e. the last query qn in a session, ordered in decreasing relevance. Without loss of specificity, we assume that any query between q1 to qn could be the last query. We therefore study the problem of retrieving relevant documents for qi, given all previous queries q1 to qi-1, previous retrieval results D1 to Di-1, and previous clicked data C1 to Ci-1.",1,TREC,True
57,We define query change qi as the syntactic editing changes between two adjacent queries qi-1 and qi:,1,ad,True
58,"qi , qi - qi-1",0,,False
59,"qi can be written as a combination of the shared portion between qi and qi-1 and query change: qi , (qi qi-1)+qi.",0,,False
60,"The query change qi comes from two sources. First, the added terms, which we call positive q, are new terms that the user adds to the previous query. Second, the removed terms, which we call negative q, are terms that the user deletes from the previous query. For example, in Table 1 s37, `US' and `policy' are the added terms; while in s28, `stock' and `market' are the removed terms.",1,ad,True
61,"We call the common terms shared by two adjacent queries theme terms since they often represent the main topic of a session. For example, in Table 1 s37 the theme terms are ""Merck lobby"".2",1,ad,True
62,"We thus decompose a query into three parts as theme terms, added terms, and removed terms and write it as:",1,ad,True
63,"qi , (qiqi-1)+(+qi)-(-qi) , qtheme+(+qi)-(-qi)",0,,False
64,"where qtheme are the theme terms, +qi and -qi represent added terms and removed terms, respectively.",1,ad,True
65,"Our observations suggest that documents that have been examined by the user factor in deciding the next query change. We therefore propose the following important assumption between qi, the query change between adjacent",1,ad,True
66,"2We perform K-stemming to all query terms. For instance, `lobbists' and `lobbying' are both stemmed to `lobby'.",0,,False
67,454,0,,False
68,Table 3: User agent's policies and actions about a query term t  qi-1. (Refer to sessions shown in Table 1),0,,False
69,t  Di-1,0,,False
70,user intention,0,,False
71,1. find more information about t 2. satisfied & move to the next information need 3. satisfied,0,,False
72,user likes Di-1 user action example,0,,False
73,add new terms s85,1,ad,True
74,t about t,0,,False
75,q1  q2,0,,False
76,remove t & add new terms t as new focus keep t,1,ad,True
77,s6 q8  q9,0,,False
78,theme term,0,,False
79,type specification,0,,False
80,drift,0,,False
81,no change,0,,False
82,user intention,0,,False
83,user dislikes Di-1 user action example,0,,False
84,5. remove the wrong remove t,0,,False
85,s28,0,,False
86,terms,0,,False
87,q1  q2,0,,False
88,6. not satisfied & move to the next information need 7. try different expression for t,0,,False
89,remove t & add new terms t slight change of t to t,1,ad,True
90,s6 q6  q7,0,,False
91,s85 q2  q3,0,,False
92,type generalization,0,,False
93,drift,0,,False
94,slight change,0,,False
95,4. inspired by add terms t t / Di-1 terms t in Di-1 about t,1,ad,True
96,s37 q1  q2,0,,False
97,specification 8. try different ex- slight,0,,False
98,s32,0,,False
99,pression for t to get change of t q1  q2,0,,False
100,more documents for t to t,0,,False
101,slight change,0,,False
102,"queries qi and qi-1, and Di-1, the search results for qi-1:",0,,False
103,qi  Di-1.,0,,False
104,"The assumption basically says that previous search results decide query change. In fact, previous search results Di-1 could influence query change qi in quite complex ways. For instance, the added terms in s37 (Table 1) q1 to q2, are `US' and `policy'. D1 contains several mentions of `policy', such as ""A lobbyist who until 2004 worked as senior policy advisor to Canadian Prime Minister Stephen Harper was hired last month by Merck"". However, these `policy'-related mentions are about ""Canada policy"" whereas the user adds ""US policy"" in q2. This suggests that the user might have been inspired by `policy' in D1, however he preferred the policy in US, not in Canada. Therefore, instead of simply cutting and pasting identical terms from Di-1, the user creates related terms to add for the next search.",1,ad,True
105,"In another example, s28 (Table 1) q1, `stock' and `market' are frequent terms that are similar to stopwords. Documents in D1 are hence all about them and totally ignore the theme terms ""france world cup 98."" In q2, the user removes ""stock market"" to boost rankings for documents about the theme terms. In this case, removing terms is not only about generalization, but also about document re-ranking.",0,,False
106,"To provide a convincing foundation for our approach, we look for evidence to support our assumption. We investigate whether qi (at least) appears in Di-1. Table 2 shows how often theme terms, added terms, and removed terms are present in Di-1 for both TREC 2011 and 2012 datasets. Around 90% of the time theme terms occur in Di-1 and most removed terms (>60%) appear in Di-1.3 Added terms are new terms for the previous query qi-1; we thus expect to see few occurrences of added terms in Di-1. Surprisingly, however, more than a third of them appear in Di-1. It suggests that it is quite probable that previous search results motivate the subsequent query change.",1,ad,True
107,"Table 3 summarizes various types of query changes into possible policies for the user agent. This table mainly serves as a guide for us to design the policies for the search engine agent. We do not perform a thorough user study to validate this table. However, we believe that it is a good representative of various search scenarios and can help design a good session search agent.",0,,False
108,"Along two dimensions, Table 3 summarizes the user agent's actions and possible policies. The dimensions are whether a previous query term t  qi-1 appears in previous search",0,,False
109,3A third of query terms that do not appear in Di-1 are removed by the user.,0,,False
110,"results Di-1 (the left most column) and whether the user likes Di-1 and the occurrence of t in Di-1 (the top most row). Combinations of the two dimensions yield 4 main cases (as in a contingency table) and 8 sub-cases. For each case, we identify four items: a rough guess of user intention, the user's actual action, an example, and the semantic exploration type for this action. For example, query change in s6 q8  q9, pocono mountains chateau resort attractions  pocono mountains chateau resort getting to can be interpreted as the following. Previous query term `attractions' appears in Di-1 and the user likes the returned documents Di-1. One possibility is that he is satisfied with what he reads and moves to the next information need. Therefore, the user removes `attraction' and adds new terms ""getting to"" as the new query focus. This is a drift in search focus. (case 2 in Table 3)",1,ad,True
111,"We further group the cases in Table 3 by types of user actions, i.e., query change, and summarize them into:",0,,False
112,"· Theme terms (qtheme), terms that appear in both qi-1 and qi. In fact, they often appear in many queries in a session. It implies a strong preference for those terms from the user. If they appear in Di-1, it shows that the user favors them since the user issues them again in qi. If they do not appear in Di-1, the user still favors towards them and insists to include them in the new query. This corresponds to t in cases 1 and 3 in Table 3.",0,,False
113,"· Added terms (+q), terms that appear only in qi, not in qi-1. They indicate specification, destination of drifting, or destination of slight change. If they appear in Di-1, for the sake of novelty [14], they will not be favored in Di. If they do not appear in Di-1, which means that they are novel and the user favors them now. This corresponds to t in cases 1, 2, 6, 7, and 8, and t in case 4 in Table 3.",0,,False
114,"· Removed terms (-q), terms that appear only in qi-1, not in qi. They indicate generalization, source of drifting, and source of slight change. If they appear in Di-1, removing them means that the user observes them and dislikes them. If they do not appear in Di-1, the user still dislikes the terms since they are not in qi anyway. This corresponds to t in cases 2, 5, 6, 7, and 8 in Table 3.",0,,False
115,3. SEARCH ENGINE AGENT: STRATEGIES TO IMPROVE SEARCH,0,,False
116,"The search engine agent observes query change from the user agent and takes corresponding actions. For each type of query change, theme terms, added terms, and removed terms, we propose to adjust the term weights accordingly for better retrieval accuracy. The search engine agent's action include",1,ad,True
117,455,0,,False
118,"Table 4: Search engine agent's policy. Actions are adjustments on the term weights.  means increasing,  means decreasing, and  means keeping the original term weight.",1,ad,True
119,qtheme,0,,False
120, Di-1 Y,0,,False
121,N,0,,False
122,+q,0,,False
123,Y N,0,,False
124,-q,0,,False
125,Y N,0,,False
126,action Example,0,,False
127," ""pocono mountain"" in s6",0,,False
128,"france world cup 98 reaction in s28, q1  q2",0,,False
129," `policy' in s37, q1  q2  `US' in s37, q1  q2",0,,False
130," `reaction' in s28, q2  q3  `legislation' in s32, q2  q3",0,,False
131,"increasing, decreasing, and maintaining the term weights. Based on the observed query change as well as whether the query terms appeared in the previous search results Di-1, we can sense whether the user will favor the query terms in the current run of search. Table 4 illustrates the policies that we propose for the search engine agent.",0,,False
132,"As shown in Section 2, theme terms qtheme often appear in many queries in a session and there is a strong preference for them. Thus, we propose to increase the weights of theme terms no matter whether they appeared in Di-1 or not (rows 1 and 2 in Table 4). In the latter case, if a theme term was not found in Di-1 (top retrieval results), it is likely that the documents containing them were ranked low. Therefore, the weights of theme terms need to be raised to boost the rankings of those documents (row 2 in Table 4). However, since theme terms are topic words in a session, they could appear like stopwords within the session. To avoid biasing too much towards them, we lower their term weights proportionally to their numbers of occurrences in Di-1.",0,,False
133,"For added terms +q, if they occurred in previous search results Di-1, we propose to decrease their term weights for the sake of novelty [14]. For example, in s5 q1  q2, ""pocono mountains""""pocono mountains park"", the added term `park' appeared in a document in D5. If we use the original weight of `park', this document might still be ranked high in D2 and the user may dislike it since he read it before. We hence decrease added terms' weights if they are in Di-1 (row 3 in Table 4). On the other hand, if the added terms did not occur in Di-1, they are the new search focus and we increase their term weights (row 4 in Table 4). In an interesting case (s37 q1  q2), part of +q, `policy', occurred in D1 whereas the other part, `US', did not. To respect the user's preference, we increase the weight of `US' while decreasing that of `policy' to penalize documents about other `polices' including ""Canada policy"".",1,ad,True
134,"For removed terms -q, if they appeared in Di-1, their term weights are decreased since the user dislikes them by deleting them (row 5 in Table 4). For example, in s28 q2  q3, `reaction' existed in D2 and is removed in q3. However, if the removed terms are not in Di-1, we do not change their weights since they are already removed from qi by the user (row 6 in Table 4).",1,ad,True
135,"In the sections below, we follow policies proposed for the search engine agent as shown in Table 4 and incorporate them into a novel query change retrieval model (QCM).",1,corpora,True
136,4. MODELING SESSION SEARCH,0,,False
137,"Markov Decision Process (MDP) [16, 28] models a state space S and an action space A. Its states S ,"" {s1, s2, ...} change from one to another according to a transition model T "","" P (si+1|si, ai), which models the dynamics of the entire""",0,,False
138,"system. A policy (s) ,"" a indicates that at a state s, what""",0,,False
139,are the actions a can be taken by the agent. In session,0,,False
140,"search, we employ queries as states. Particularly, we denote",0,,False
141,"q as state, T as the transition model P (qi|qi-1, ai-1), D",0,,False
142,"as documents, and A as actions. Actions include keeping,",0,,False
143,"adding, and removing query terms for the user agent and",1,ad,True
144,"increasing, decreasing, and maintaining the term weights for",0,,False
145,the search engine agent.,0,,False
146,"In a MDP, each state is associated with a reward function",0,,False
147,R that indicates possible positive reward or negative loss,0,,False
148,"that a state and an action may result. In session search, we",0,,False
149,consider the reward function to be the relevance function.,0,,False
150,Reinforcement learning [16] offers general solutions to MDP,0,,False
151,and seeks for the best policy for an agent. Each policy has,0,,False
152,"a value associated with the policy and denoted as V(s),",0,,False
153,which is the expected long-term reward starting from state,0,,False
154,"s and continuing with policy  from then on. In a MDP, it",0,,False
155,is believed that a future reward is not worth quite as much,0,,False
156,"as a current reward and thus a discount factor   (0, 1) is",0,,False
157,applied to future rewards. By considering the discount fac-,0,,False
158,"tor, the value function starting from s0 for a policy  can be",0,,False
159,"written as: V(s0) , E[R(s0) + R(s1) + 2R(s2) + ...] ,",0,,False
160,E [,0,,False
161," t,0",0,,False
162,tR(si,0,,False
163,optimal value V,0,,False
164,)]. The  for a,0,,False
165,Bellman equation [16] describes the state s in the long run and is often,0,,False
166,used to obtain the best value for a MDP:,0,,False
167,"V (s) ,"" max R(s, a) +  P (s |s, a)V (s )""",0,,False
168,a s,0,,False
169,"where s is the next state after s, V (s) and V (s ) are the optimal values for s and s .",0,,False
170,"For session search, we observe that the influence of previous queries and previous search results to the current queries, becomes weaker and weaker. The user's desire for novel documents also supports this argument. We hence propose to employ the reinforcement learning model backwards. That is, instead of discounting the future rewards, we discount the past rewards, i.e. the relevant documents that appeared in the previous search results.",1,ad,True
171,"We propose the query change retrieval model (QCM) as the following. We consider the task of retrieving relevant documents for qi as ranking documents based on the reward, i.e., how relevant it is to qi. Inspired by the Bellman equation, we model the relevance of a document d to the current query qi as:",0,,False
172,"Score(qi, d) ,"" P (qi|d)+ P (qi|qi-1, Di-1, a) max P (qi-1|Di-1)""",0,,False
173,a,0,,False
174,Di-1,0,,False
175,(1),0,,False
176,which recursively calculates the reward starting from q1 and,0,,False
177,continues with the search engine agent's policy until qi.  ,0,,False
178,"(0, 1) is the discount factor, maxDi-1 P (qi-1|Di-1) is the maximum of the past rewards, P (qi|d) is the current reward,",0,,False
179,"and P (qi|qi-1, Di-1, a) is the query transition model.",0,,False
180,"The first component in Eq.1, P (qi|d), measures the rel-",0,,False
181,evance between qi and a document d that is under evalu-,0,,False
182,ation. This component can be estimated by the Bayesian,0,,False
183,"belief network model [27]: P (qi|d) ,"" 1 - tqi (1 - P (t|d)), where P (t|d) is calculated by the multinomial query genera-""",0,,False
184,"tion language model with Dirichlet smoothing [33]: P (t|d) ,",0,,False
185,"#(t,d)+µP |d|+µ",0,,False
186,(t|C),0,,False
187,",",0,,False
188,where,0,,False
189,"#(t, d)",0,,False
190,denotes,0,,False
191,the,0,,False
192,number,0,,False
193,of,0,,False
194,occur-,0,,False
195,"rences of term t in document d, P (t|C) calculates the prob-",0,,False
196,ability that t appears in corpus C based on Maximum Like-,0,,False
197,456,0,,False
198,"lihood Estimation (MLE), |d| is the document length, and",0,,False
199,ognize added terms +q and removed terms -q. Gener-,1,ad,True
200,µ is the Dirichlet smoothing parameter (set to 5000).,0,,False
201,"ally, the terms that occur in the current query but not in",0,,False
202,The remaining challenges of calculating Eq.1 include max-,0,,False
203,the previous query constitute +q; while the terms occur in,0,,False
204,"imizing the reward function maxDi-1 P (qi-1|Di-1) and estimating the transition model P (qi|qi-1, Di-1, a). They are",0,,False
205,"the previous query but not in the current query constitute -q. In the above example, -q7 ,"" """"camelbeach hotel"""",""",0,,False
206,"described in Section 4.1 and Section 4.2, respectively.",0,,False
207,"and +q7 ,"" """"chateau resort"""".""",0,,False
208,4.1 Maximizing the Reward Function,0,,False
209,"The search engine actions are decreasing, increasing, and maintaining term weights. According to Table 4 rows 3 and",0,,False
210,"When considering the past/future rewards, MDP uses only 5, we decrease a term's weight if the query change, either",0,,False
211,the optimal (the maximum possible) values from those past,0,,False
212,"+q or -q, occurred in the effective previous search re-",0,,False
213,/future rewards. This is reflected in maxDi-1 P (qi-1|Di-1) as part of Eq. 1.,0,,False
214,"sults Di-1. We propose to deduct term t's weight by P (t|d), i.e. t's default contribution to the relevance score between",0,,False
215,"Prior research [10, 22] suggests that Satisfying (SAT) clicks, i.e., clicked documents with dwell time longer than 30 seconds [10, 22], are probably the only ones that are effective",0,,False
216,"qi and the document under evaluation (denoted as d). Furthermore, since t already occurred in Di-1, for the sake of novelty, we deduct more weight that is proportional to t's",1,ad,True
217,at predicting user behaviors and relevance judgments. Since,0,,False
218,"the user also skims snippets in search interactions, in this",0,,False
219,"work, we consider both the top 10 returned snippets and",0,,False
220,SAT clicks as effective previous search results and denote them as Die-1.,0,,False
221,To obtain an maximum reward from all possible reward,0,,False
222,"frequency in Di-1 such that the more frequently t occurred in Di-1, the more heavily t's weight is deducted from the current query qi and d. We formulate this weight deduction for a term t  +q or t  -q as:",0,,False
223,"log Pnew(t|d) , (1 - P (t|di-1)) log P (t|d)",0,,False
224,(2),0,,False
225,"functions P (qi-1|di-1), i.e., the text relevance of previous query qi-1 and all previous search results di-1  Di-1, we",0,,False
226,"where di-1 denotes the maximum rewarded document, d is the document under evaluation, and P (t|d) is calculated by",0,,False
227,"propose to generate a maximum rewarding document, denoted as di-1. We further propose that the candidates for the di-1 should only be selected from the effective previous search results Die-1. We define di-1 as the document(s) that is the most relevant to qi-1. To discover di-1, we first",0,,False
228,rank all the documents (either a snippet or a document),0,,False
229,MLE. We apply the log function to avoid numeric underflow. We notice that Eq. 2 has an interesting connection with,0,,False
230,the Kullback-Leibler divergence (KL divergence) [33]:,0,,False
231,-,0,,False
232,P,0,,False
233,(t|di-1),0,,False
234,log,0,,False
235,P,0,,False
236,(t|d),0,,False
237,",",0,,False
238,P,0,,False
239,(t|di-1),0,,False
240,log,0,,False
241,P,0,,False
242,1 (t|d),0,,False
243,"di-1  Die-1 by measuring the relevance between qi-1 and di-1 as: P (qi-1|di-1) ,"" 1 - tqi-1 {1 - P (t|di-1)}, where""",0,,False
244,"ra,nk",0,,False
245,P,0,,False
246,(t|di-1),0,,False
247,log,0,,False
248,P (t|di-1) P (t|d),0,,False
249,(3),0,,False
250,P (t|di-1),0,,False
251,is,0,,False
252,calculated by MLE: P (t|di-1),0,,False
253,",",0,,False
254,", #(t,di-1 )",0,,False
255,|di-1 |,0,,False
256,"#(t, di-1)",0,,False
257,"is the number of occurrences of term t in document di-1, and",0,,False
258,"ra,nk KLDt di-1 ||d",0,,False
259,"|di-1| is the document length. We do not apply smoothing here since P (t|di-1) can be zero, i.e., t / Die-1. In fact, we rely on this property in later calculation.",0,,False
260,"After ranking documents di-1 in Die-1, we generate di-1 by the following options: (1) using the document with the largest P (qi-1|di-1), (2) concatenating the top k documents in Die-1 with the largest P (qi-1|di-1), or (3) concatenating all documents in Die-1. Experiments show that option (1) works the best and we use this setting throughout the paper. For notation simplicity, we use Di-1 from now on to denote effective previous search results.",0,,False
261,4.2 Estimating the Transition Model,0,,False
262,"where KLDt di-1 ||d denotes the contribution of term t to the KL divergence between two documents' language models di-1 and d. In Eq. 3, the larger the divergence between di-1 and d, the more novel document d is compared to Di-1, and the less deduction to the relevance score. In this sense, Eq. 2 models novelty for the added terms and the removed terms during a query transition.",1,ad,True
263,"According to Table 4 row 4, we increase a term's weight if it is an added term and did not occur in Di-1. We propose to raise the term weight proportional to its inverse document frequency (idf). This is to make sure that while increasing a preferred term's weight, we avoid increasing its weight",1,ad,True
264,"The transition model indicated in Eq. 1 is a P (qi|qi-1, Di-1, a). It includes the probabilities of query transitions",0,,False
265,under various actions. We incorporate polices designed in,1,corpora,True
266,too much if it is a common term in many documents. We formulate this weight increase for a novel added term t (t  +q and t / Di-1) as:,1,ad,True
267,Table 4 to calculate it. Search engine agent performs actions based on user agent's,0,,False
268,"log Pnew(t|d) , (1 + idf (t)) log P (t|d)",0,,False
269,(4),0,,False
270,"actions. We need to identify user's actions, i.e. query change",0,,False
271,where idf (t) is the inverse document frequency of t in Corpus,0,,False
272,"q before search engine takes actions. Particularly, we rec-",0,,False
273,C and P (t|d) is calculated by MLE. Note that this term,0,,False
274,"ognize q by the following procedure. First, we generate",0,,False
275,weight adjustment is in a form of tf-idf.,1,ad,True
276,qtheme based on the Longest Common Subsequence (LCS),0,,False
277,"The increasing in term weights also applies to theme terms,",0,,False
278,[11] in both qi-1 and qi. A subsequence is a sequence that,0,,False
279,which corresponds to rows 1 and 2 in Table 4. Theme terms,0,,False
280,appears in two strings in the same relative order but is not,0,,False
281,"repeatedly appear in a session, which implies the impor-",0,,False
282,necessarily continuous. The LCS can be the common prefix,0,,False
283,"tance of them. Similar to the novel added terms, we should",1,ad,True
284,or the common suffix of the two queries; it can also consist,0,,False
285,avoid increasing their weights too much. We could discount,0,,False
286,of several discontinuous common parts from the two queries.,0,,False
287,"the increment proportional to idf. However, theme terms",0,,False
288,"Take s6 q6  q7 as an example: q6,""pocono mountains",0,,False
289,"are topical/common terms within a session, not necessarily",0,,False
290,"camelbeach hotel, q7"",""pocono mountains chateau resort,""",0,,False
291,"common terms in the entire corpus. Therefore, idf may not",0,,False
292,"qtheme ,"" LCS(q6, q7) "","" """"pocono mountains"""". Next, we rec-""",0,,False
293,be applicable here. We hence employ the negation of the,0,,False
294,457,0,,False
295,"number of occurrences of t in previous maximum rewarding document, 1 - P (t|di-1), to substitute idf. We formulate this weight increase for a theme term t  qtheme as:",0,,False
296,"log Pnew(t|d) , (1 + (1 - P (t|di-1))) log P (t|d) (5)",0,,False
297,where di-1 denotes the maximum rewarded document and P (t|d) is calculated by MLE.,0,,False
298,Table 5: Dataset statistics for TREC 2011 and 2012 Session.,1,TREC,True
299,#topics #sessions #queries #dups,0,,False
300,2011 62 76 280 16,0,,False
301,2012 48 98 297 5,0,,False
302,#queries/session #sessions/topic #pages judged #sessions w/o rel. docs,0,,False
303,"2011 3.68 1.23 19,413",0,,False
304,2,0,,False
305,"2012 3.03 2.04 17,861",0,,False
306,4,0,,False
307,Next we determine exact string matches between every query,0,,False
308,"For removed terms that did not appear in Di-1 (Table 4 row 6), the search agent does not change their term weights.",0,,False
309,pair. The exactly matched query pairs are identified as duplicated queries.,0,,False
310,By considering all possible cases for the transition model,0,,False
311,Since the user may dislike the queries and their corre-,0,,False
312,"as defined in Eq. 1, the relevance score between the current",0,,False
313,"sponding search results between two duplicated queries, we",0,,False
314,query qi and a document d is represented as below:,0,,False
315,propose to eliminate from the MDP the undesired queries,0,,False
316,"Score(qi, d) , log P (qi|d) + ",0,,False
317,and their interactions. We achieve this by setting the dis[1 - P (t|di-1)] log P (t|d) count factor to zero for any interaction between two dupli-,0,,False
318,tqtheme,0,,False
319,cated queries as well as that for the earlier query in the two.,0,,False
320,-,0,,False
321,P (t|di-1) log P (t|d) +,0,,False
322,idf (t) log P (t|d),0,,False
323,The new discount factor  can be calculated as:,0,,False
324,t+q tdi-1,0,,False
325,-,0,,False
326,P (t|di-1) log P (t|d),0,,False
327,t+q t/di-1,0,,False
328,"i ,",0,,False
329,0 i,0,,False
330,"{i|i  [j, k), qj ,"" qk, j < k)} otherwise""",0,,False
331,(9),0,,False
332,t-q,0,,False
333,"(6) where , , , and  are parameters for each types of actions. Note that we apply different parameters  and  on +q and -q, since added terms and removed terms may affect the retrieval differently. We report the parameter selection in Section 6.",1,ad,True
334,"where i is the original discount factor for the ith query, i is the updated discount factor for the ith query after deduplication.",0,,False
335,"For the above example s6, the effects from q2 and q3 on the session are eliminated. The entire session is now equivalent to q1, q4, q5, ..., q11.",0,,False
336,4.3 Scoring the Entire Session,1,Session,True
337,"It is worth noting that Eq. 6 is valid only when i > 1. When i ,"" 1, there is no previous result for q1. We thus use""",0,,False
338,"Score(q1, d) , log P (q1|d)",0,,False
339,(7),0,,False
340,as a base case. P (q1|d) is calculated by Eq. 4. Using Eq. 7 as the base case for the recursive function,0,,False
341,"described in Eq. 1, we obtain the overall document relevance score Scoresession(qn, d) for a session that starts at q1 and ends at qn by considering all queries in the session:",0,,False
342,"Scoresession(qn, d) ,"" Score(qn, d) + Scoresession(qn-1, d)""",0,,False
343,","" Score(qn, d) +  [Score(qn-1, d) + Scoresession(qn-2, d)]""",0,,False
344,n,0,,False
345,","" n-iScore(qi, d)""",0,,False
346,"i,1",0,,False
347,(8),0,,False
348,"where q1, q2, · · · , qn are in the same session, and   (0, 1) is the discount factor. Eq. 8 provides a form of aggregation over the relevance functions of all the queries in a session.",0,,False
349,5. DUPLICATED QUERIES,0,,False
350,"Duplicated queries sometimes occur in a search session. Prior work shows that removing duplicated queries could effectively boost the search accuracy [8, 19]. Duplicated queries often occur when a user is frustrated by irrelevant documents in search results and comes back to one of the previous queries for a fresh start. For example, in s6 (Table 1), q2 and q4 are duplicates and both search for pocono mountains pennsylvania hotels. The query between them is q3: pocono mountains pennsylvania things to do. It suggests that the user might dislike the search results for q3 and he returns to q2 to search again (q2 , q4).",0,,False
351,"To detect query duplicates, we first remove punctuations and white spaces in queries, then apply stemming on them.",0,,False
352,6. EVALUATION,0,,False
353,"The evaluation datasets are from TREC 2011 and 2012 Session tracks [18, 19]. Table 5 lists the statistics about these two datasets. Each search session includes several queries and the corresponding search results. The users (NIST assessors) were given a topic description about information needs before they searched. For example, s85 (Table 1) are related to topic 43 ""When is scientific glass blowing used? What are the purposes? What organizations do scientific glass blowing?"" Multiple sessions can relate to the same topic. The search engine used to create the sessions was Yahoo! BOSS. The top 10 returned documents were shown to the users and they clicked documents that were interesting to them and interacted with the system. We use TREC's official ground truth and official evaluation metrics nDCG@10 and MAP.",1,TREC,True
354,"The corpus used in this evaluation is ClueWeb09 Category B collection (CatB).4 CatB contains the first 50 million English pages crawled from the Web during January to February 2009. We filter out the spam documents by removing documents whose WateQCMoo's ""GroupX"" spam ranking scores [6] are less than 70.",1,ClueWeb,True
355,We compare the following systems in this evaluation:,0,,False
356,· Lemur : Directly submitting the current query qn (with punctuations removed) to the Lemur search engine [21] (language modeling + Dirichlet smoothing) and obtain the returned documents.,0,,False
357,"· TREC best : The top TREC system as reported by NIST [13, 14]. It adopts a query generation model with relevance feedback and handles document novelty. CatB was used in their TREC submissions. This system is used as the baseline system in this evaluation.",1,TREC,True
358,· Nugget: Another top TREC 2012 session search system groups semantically coherent query terms as nuggets and,1,TREC,True
359,4http://lemurproject.org/clueweb09/.,0,,False
360,458,0,,False
361,"Figure 2: nDCG@10 for TREC 2012 against the parameters. (a), (b), (c), and (d) are about , , , and  respectively.",1,TREC,True
362,"Table 6: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2012 sessions. The runs are sorted by nDCG@10. A statistical significant improvement over the baseline is indicated with a  at p < 0.05 level.",1,MAP,True
363,Approach Lemur TREC median Nugget TREC best,1,TREC,True
364,QCM,0,,False
365,QCM+Dup,0,,False
366,nDCG@10 0.2474 0.2608 0.3021 0.3221,0,,False
367,0.3353,0,,False
368,0.3368,0,,False
369,%chg -21.54% -17.29% -4.19% 0.00% 4.10% 4.56%,0,,False
370,MAP 0.1274 0.1440 0.1490 0.1559,1,MAP,True
371,0.1529,0,,False
372,0.1537,0,,False
373,%chg -18.28% -7.63% -4.43% 0.00%,0,,False
374,-1.92%,0,,False
375,-1.41%,0,,False
376,"Table 7: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2011 sessions. The runs are sorted by nDCG@10. A statistical significant improvement over the baseline is indicated with a  at p < 0.05 level.",1,MAP,True
377,Approach Lemur TREC median TREC best,1,TREC,True
378,QCM,0,,False
379,QCM+Dup Nugget,0,,False
380,nDCG@10 0.3378 0.3544 0.4409,0,,False
381,0.4728,0,,False
382,0.4821 0.4836,0,,False
383,%chg -23.38% -19.62% 0.00% 7.24% 9.34% 9.68%,0,,False
384,MAP 0.1118 0.1143 0.1508,1,MAP,True
385,0.1713,0,,False
386,0.1714 0.1724,0,,False
387,%chg -25.86% -24.20% 0.00% 13.59% 13.66% 14.32%,0,,False
388,"creates structured Lemur queries [8]. We re-implement and apply it on both TREC 2011 and 2012. · TREC median: The median TREC system as reported by NIST [18, 19]. · QCM : The proposed query change retrieval model. · QCM + De-Duplicate (Dup): The proposed query change retrieval model with duplicated queries removed.",1,TREC,True
389,6.1 Search Accuracy,0,,False
390,"Table 6 and Table 7 demonstrate search accuracy for all systems under comparison for TREC 2012 and TREC 2011, respectively. The evaluation metrics are nDCG@10 and MAP, the same as in the official TREC evaluations. TREC best serves as the baseline.",1,TREC,True
391,"Table 6 shows that the proposed QCM approach outperforms the best TREC 2012 system on nDCG@10 by 4.1%, which is statistically significant (one sided t-test, p , 0.05). The search accuracy is further improved by 0.46% through removing the duplicated queries. The experimental results strongly suggest that our approach is highly effective.",1,TREC,True
392,"Table 7 shows that for TREC 2011, our approach again outperforms the baseline by a statistically significant 7.24% (one sided t-test, p ,"" 0.05) and achieves a further improvement of 9.34% by the QCM+Dup approach. For TREC 2011, the performance gain by performing de-dup is 2.1%, which is bigger than that for TREC 2012 (0.46%). The reason is probably because that TREC 2012 only has 5 duplicated queries while TREC 2011 has 16 (shown in Table""",1,TREC,True
393,"5). However, the best approach for TREC 2011 is the nugget approach, which is slightly better than QCM+Dup.",1,TREC,True
394,"Table 5 illustrates the dataset differences between TREC 2011 and 2012. These differences may affect search accuracy. The average number of sessions per topic is 2.04 in 2012, that is more than that in 2011 (1.23). Moreover, on average, TREC 2012 sessions contain less queries per session (3.03) than 2011 (3.68). As a result, the shorter sessions in 2012 may make the search task more difficult than 2011 since less information are provided by previous interactions. Another difference is that 2012 sessions have fewer (sometimes even none) relevant documents than 2011 sessions in CatB ground truth. It unavoidably hurts the performance for any retrieval system. Generally, we observe lower search accuracy in 2012 (Table 6) than in 2011 (Table 7).",1,TREC,True
395,6.2 Parameter Tuning,0,,False
396,"We investigate good values for parameters in Eq. 6. A supervised learning-to-rank method should be able to find the optimal values for those parameters. However, in this paper, we take a step-by-step parameter tuning procedure and leave the supervised learning method as future work.",0,,False
397,"We add each component, i.e., theme terms, added terms, and removed terms, one by one into Eq. 6. The tuning is performed for QCM only and the parameters are shared between QCM and QCM+Dup.",1,ad,True
398,"First, we plot nDCG@10 against  while setting other parameters to 0 (Figure 2(a)).  represents the parameter for theme terms.  ranges over [1.1, 2.5] by an interval of 0.1. We notice that nDCG@10 reaches its maximum at  , 2.2. We find 2 other local maximums at 1.6 and 1.2 for .",0,,False
399,"Next, we fix  to the above values and plot nDCG@10 against  (Figure 2(b)).  is the parameter for added terms that appeared in effective previous search results; we call them old added terms.  ranges over [1.0, 2.4] by an interval of 0.2. We choose the top 2 local values from each curve and pick 6 combinations for (, ) as indicated in Figure 2(c).",1,ad,True
400,"Then, we fix (, ) and plot nDCG@10 against (Figure 2(c)). is the parameter for added terms that did not appear in effective previous search results; we call them novel added terms. ranges over [0.05, 0.1] by an interval of 0.01. All the curves show similar trends and reach the highest nDCG@10 at around 0.07. We hence fix to 0.07.",1,ad,True
401,"Finally, we plot nDCG@10 against  (Figure 2(d)) with the parameter combinations that we discover eerlier. Eventually, nDCG@10 reaches its peak 0.3353 at  ,"" 2.2,  "","" 1.8, "","" 0.07, and  "", 0.4. We apply this set of parameters to both QCM and QCM+Dup.",0,,False
402,"As we can see, , , and  are much larger than . This is because that in Eq. 4, idf (t) ,"" log(N/nd) falls in the range of [1, 10], while in Eq. 2 and Eq. 5, P (t|di-1) falls""",0,,False
403,459,0,,False
404,"by an interval of 0.02. Figure 3 illustrates the relationship between nDCG@10 and . nDCG@10 climbs to its peak 0.3368 when  ,"" 0.92. The result suggests that a good discount factor  is very close to 1, implying that previous queries contribute to the overall search accuracy nearly the same as the last query. It suggests that in QCM, a discount between two adjacent queries should be mild.""",1,ad,True
405,7. DISCUSSION,0,,False
406,Figure 3: Discount factor .,0,,False
407,Figure 4: Error types.,0,,False
408,7.1 Advantages of Our Approach,0,,False
409,Table 8: Aggregation schemes.,0,,False
410,A main contribution of our approach is that we treat a search session as a continuous process by studying changes,0,,False
411,Approach,0,,False
412,Query change model,1,Query,True
413,Aggregation Scheme,0,,False
414,Uniform PvC,0,,False
415,Distance-based,0,,False
416,"qn 1 n , 1 n , 1 - p",0,,False
417,"n , 1 - p",0,,False
418,"qi(i  [1, n - 1])  n-i",0,,False
419,"i , 1",0,,False
420,"i , p",0,,False
421,i,0,,False
422,",",0,,False
423,p n-i,0,,False
424,"among query transitions and modeling the dynamics in the entire session. Through the reinforcement learning style framework, our system provides the best aggregation scheme for all queries in a session (Table 9). This allows us to better handle sessions that demonstrate evolution and exploration",0,,False
425,Table 9: nDCG@10 for various aggregation schemes. p is 0.4 in PvC.  is 0.92 in QCM and QCM+Dup. TREC 2012 best serves as the baseline. A significant improvement over the baseline is indicated with a  at p < 0.05 level.,1,TREC,True
426,Aggregation Scheme,0,,False
427,Distance-based TREC best,1,TREC,True
428,Uniform,0,,False
429,TREC 2011,1,TREC,True
430,nDCG@10 %chg,0,,False
431,0.4431,0,,False
432,-2.40%,0,,False
433,0.4540,0,,False
434,0.00%,0,,False
435,0.4626,0,,False
436,1.89%,0,,False
437,TREC 2012,1,TREC,True
438,nDCG@10 %chg,0,,False
439,0.3111 -3.42%,0,,False
440,0.3221,0,,False
441,0.00%,0,,False
442,0.3316,0,,False
443,2.95%,0,,False
444,"in nature than most existing systems do. On the contrary, for sessions that are clear in search goals and lack of a exploratory nature, the advantage of our system over other systems looks less significant.",1,ad,True
445,"This can be seen in Table 10, which illustrates the search accuracy for the TREC best, Nugget, and our system for various classes of sessions. The TREC best is used as the baseline and we also show the percentile improvement over it in Table 10. TREC 2012 sessions were created by consider-",1,TREC,True
446,PvC,0,,False
447,0.4713,0,,False
448,3.81%,0,,False
449,0.3351,0,,False
450,4.04%,0,,False
451,ing and hence can be classified into two facets: search target,0,,False
452,QCM,0,,False
453,0.4728 4.14% 0.3353 4.10% (factual or intellectual) and goal quality (specific/good or,0,,False
454,QCM+Dup,0,,False
455,0.4821 6.19% 0.3368 4.56%,0,,False
456,"amorphous/ill) [19]. Table 10 shows that QCM works very well for all classes of sessions. Specifically, QCM works even",0,,False
457,"in the range of [0,0.1]. Therefore, the values of are two",0,,False
458,"better, i.e. outperforms the TREC best even more signifi-",1,TREC,True
459,magnitudes less than that for the other parameters. Among,0,,False
460,"cantly, for sessions that search for intellectual targets as well",0,,False
461,", , and , we find that  and  are larger than , which",0,,False
462,as sessions that search with amorphous goals. In our opin-,0,,False
463,implies that theme terms and added terms may play more,1,ad,True
464,"ion, this is due to that intellectual tasks produce new ideas",0,,False
465,important roles in session search than removed terms.,0,,False
466,or new findings (e.g. learn about a topic or make decision,0,,False
467,6.3 Aggregation for the Entire Session,1,Session,True
468,based on the information collected so far) while searching. Both intellectual and amorphous sessions rely more on pre-,0,,False
469,QCM proposes an effective way to aggregate all queries in,0,,False
470,"vious search results. Thus, users reformulate queries based",0,,False
471,a session as in Eq.8. We compare how effective it is to prior,0,,False
472,"more on what they have retrieved, not the vague informa-",0,,False
473,query aggregation methods. A query aggregation scheme,0,,False
474,tion need. This is a scenario where our approach is good at,0,,False
475,"can be represented as: Score(session, d) ,",0,,False
476,"n i,1",0,,False
477,i,0,,False
478,·,0,,False
479,"Score(qi,",0,,False
480,"d),",0,,False
481,since,0,,False
482,we,0,,False
483,employ,0,,False
484,previous,0,,False
485,search,0,,False
486,results,0,,False
487,to,0,,False
488,guide,0,,False
489,the,0,,False
490,search,0,,False
491,"where Score(qi, d) is the relevance scoring function of d and",0,,False
492,"engine's action. For specific and factual sessions, users are",0,,False
493,qi and i is the query weight for qi.,0,,False
494,"clearer in search goals, query changes may come less from the",0,,False
495,[8] proposed several aggregation schemes for TREC 2012,1,TREC,True
496,"previous search results. In summary, our good performance",0,,False
497,Session track. The schemes are: uniform (all queries are,1,Session,True
498,on both intellectual task and amorphous task is consistent,0,,False
499,"equally weighted), previous vs. current (known as PvC;",0,,False
500,with our efforts of modeling query changes.,0,,False
501,"all previous queries are discounted by p, while the current",0,,False
502,"Moreover, we benefit from term-level manipulation in var-",0,,False
503,"query uses a complementary and higher coefficient (1 - p),",0,,False
504,ious aspects in our system. The first aspect is novelty. Both,0,,False
505,and distance-based (previous queries are discounted based,0,,False
506,the TREC best system and our system handle novelty in,1,TREC,True
507,on a reciprocal function of queries' positions in the session).,0,,False
508,a session. The TREC best system only deals with novelty,1,TREC,True
509,We express various query aggregation schemes in terms,0,,False
510,at the document level. They consider documents that have,0,,False
511,of the discount factor  in order to compare them with our,0,,False
512,been examined by the user in a previous interaction not,0,,False
513,"approach. From Table 8, we find that QCM degenerates to",0,,False
514,"novel and the rest are novel [14]. That is, they determine",0,,False
515,"uniform when  , 1. Previous queries in PvC and Distance-",0,,False
516,"novelty purely based on document identification number, not",0,,False
517,"based schemes are also discounted as they are in QCM, but",0,,False
518,the actual content. Through studying whether query terms,0,,False
519,with different decay functions.,0,,False
520,"appeared in previous search results, our approach evaluates",0,,False
521,The search accuracy for different aggregation schemes are,0,,False
522,"and models novelty at the term level (or concept level),",0,,False
523,compared in Table 9. QCM performs the best for both,0,,False
524,which we believe better represents the evolving informa-,0,,False
525,TREC 2011 and 2012. The PvC scheme is the second best,1,TREC,True
526,tion needs in a session. The second aspect is query han-,0,,False
527,"scheme, which confirms what is reported in [8]. The Distance- dling. The Nugget approach [8] treats queries at the phrase",0,,False
528,based scheme gives the worst performance.,0,,False
529,level and formulates structured queries based on phrase-like,0,,False
530,"We explore the best discount factor  for QCM over (0, 1)",0,,False
531,"nuggets. The approach achieves good performance, espe-",0,,False
532,460,0,,False
533,Table 10: nDCG@10 for different classes of sessions in TREC 2012.,1,TREC,True
534,TREC best Nugget QCM QCM+DUP,1,TREC,True
535,Intellectual 0.3369 0.3305 0.3870 0.3900,0,,False
536,%chg 0.00% -1.90% 14.87% 15.76%,0,,False
537,Amorphous 0.3495 0.3397 0.3689 0.3692,0,,False
538,%chg 0.00% -2.80% 5.55% 5.64%,0,,False
539,Specific 0.3007 0.2736 0.3091 0.3114,0,,False
540,%chg 0.00% -9.01% 2.79% 3.56%,0,,False
541,Factual 0.3138 0.2871 0.3066 0.3072,0,,False
542,%chg 0.00% -8.51% -2.29% -2.10%,0,,False
543,"cially for TREC 2011. However, due to complexity in natural language, nugget detection is sensitive to dataset and the approach's performance is not quite as stable as ours on different datasets.",1,TREC,True
544,"Lastly, our system benefits from trusting the user. Our approach does not use too much materials from other resources such as anchor texts, meta data, or click orders, as many other approaches do [8, 26]. We believe that the most direct and valuable feedback is the next query that the user enters. In this work, we manage to capture the query change and investigate the reasons behind it. We use ourselves as users to summarize possible human users' reasoning and actions. More detailed analysis about user intent might be useful for researchers to understand web users, however, it might be overwhelming (too fine-grained or too much semantics) for a search engine that essentially only counts words.",0,,False
545,7.2 Error Analysis & Future Work,0,,False
546,"Our system retrieves nothing for 22 out of 98 sessions in TREC 2012. To analyze the reason for the poor performance for those sessions, we study their topic descriptions, queries, and ground truth documents. We summarize the types of errors as ""two theme concepts"", ""ill query"", ""few relevant documents"", and others. Figure 4 shows how many sessions that we fail to retrieve under each error type.",1,TREC,True
547,"We call the first type of errors ""two theme concepts"". It comes from a type of session where the information need cover more than one concepts. For instance, s17 and s18 share the the same topic ""... To what extent can decisions and policies of the Indian government be credited with these wins?"". Queries in s17 and s18 ask about both concepts ""indian politics' and ""miss universe"". Unfortunately, very few relevant documents about both theme concepts exist in the corpus. The retrieved documents are about either concept, but none is about both. Eight sessions belong to this type. As future work, we can improve our system by incorporating structures in queries, and enable more sophisticated operators such as Boolean and proximity search.",1,corpora,True
548,"The second type of errors is ""ill query"", where in such sessions, queries themselves are ill formulated and do not well-represent the information needs indicated in the given topic. A common mistake is that the user misses some subinformation need. For example, the topic for s16 is: ""... you want to reduce the use of air conditioning in your house ... you could protect the roof being overly hot due to sun exposure... Find information of ... how it could be done."" A good query for this topic should include roof and air conditioning. However, the queries that the user issued for s60, ""reduce airconditioning"" and ""attic insulation air conditioning costs"", do not mention roof at all. Because of this ill query formulation, our system yields no relevant documents for s60. On the other hand, for s59, which shares the same information need with s60, our system achieves a nDCG@10 of 0.48 simply because s59 queries ""cool roof"". It suggests that ill queries mislead the search engine and yield poor retrieval performance. Four sessions belong to this type. As",1,ad,True
549,"future work, we will explore effective query suggestion by studying sessions that share the same topic.",0,,False
550,"The third type of errors is ""too few relevant documents"". For sessions with too few relevant documents in the ground truth, our system do not perform well. In total 2,573 relevant documents exist in CatB for all 48 TREC 2012 topics; on average 53.6 relevant documents per topic. However, topics 10, 45, 47 and 48, each has no more than 2 relevant documents and topic 47 (s92 to s95) has no relevant document in CatB (Table 5). This problem could be reduced if we index the entire ClubWeb09 CatA collection.",1,TREC,True
551,"Figure 4 also indicates in which classes of sessions these errors lie. We find that all ""two theme concept"" errors belong to sessions created with amorphous goals while all ""too few relevant documents"" errors belong to those with specific goals. Moreover, ""ill queries"" tend to occur more in sessions with amorphous goals. Note that ""ill query"" and ""few relevant documents"" are errors due to either the user or the data. There might not be much room for our system to improve over them. However, ""two theme concepts"" is where our system can certainly make further improvements.",0,,False
552,8. RELATED WORK,0,,False
553,"Session search is a challenging IR task [4, 8, 13, 14, 25, 32]. Existing approaches investigate session search from various aspects such as semantic meanings of search tasks [23], document novelty [14], and phrase structure in queries [8]. The best TREC system [13, 14] employs an adaptive browsing model by considering both relevance and novelty; however it does not demonstrate improvement by handling novelty. In this paper, we successfully model query and document novelty by investigating the relationship between query change and previous search results. Moreover, our analysis on query change does not require knowledge of semantic types for the sessions as [23] proposed.",1,Session,True
554,"Our proposed work is perhaps the most similar to the problem of query formulation [1, 9, 12, 24] and query suggestion [29]. [12] showed that certain query changes such as adding/removing words, word substitution, acronym expansion, and spelling correction are more likely to cause clicks, especially on higher ranked results. The finding is generally consistent with our view of query change. However, their work only emphasized on understanding of query changes, without showing how to apply it to help session search. [24] examined the relationship between task types and how users change queries. They classified query changes by semantic types: Generalization, Specialization, Word Substitution, Repeat, and New. Similar to [12], however, [24] stopped at understanding query changes and didn't apply their findings to help session search. This probably makes us the first to utilize query changes in actual retrieval. [1] derived queryflow graph, a graph representation of user query behavior, from user query logs. The approach detected query chains in the graph and recommended queries based on maximum weights, random walk, or just the previous query. Other mining approaches [1, 29] identify the importance of query",1,ad,True
555,461,0,,False
556,"change in sessions; however, they require the luxury of large user query logs.",0,,False
557,"This research is perhaps the first to employ reinforcement learning to solve the Markov Decision Process demonstrated in session search. Reinforcement learning is complex and difficult to solve. Its solutions include model-based approaches and model-free approaches [16]. The former learn the transition model and the reward function for every possible states and actions and mainly employ MLE to estimate the model parameters. Others also use matrix inversion or linear programming to solve the Bellman equation. It works well when state spaces are small. However, in our case, the state space is large since we use natural language queries as the states; hence we could not easily apply model-based approaches in practice. In this work, we effectively reduce the search space by summarizing users' and search engine's actions into a few types and employ a model-free approach to learn value functions directly.",0,,False
558,9. CONCLUSION,0,,False
559,"This paper presents a novel session search approach (QCM) by utilizing query change and modeling the dynamic of the entire session as a Markov Decision Process. We assume that query change is an important form of feedback. Based on this assumption, through studying editing changes between adjacent queries, and their relationship with previous retrieved documents, we propose corresponding search engine actions to handle individual term weights for both the query and the document. In a reinforcement learning inspired framework, we incorporate various ingredients present in session search, such as query changes, satisfactory clicks, desire for document novelty, and duplicated queries. The proposed framework provides a theoretically sound and general foundation that allows more novel features to be incorporated. Experiments on both TREC 2011 and 2012 Session tracks show that our approach is highly effective and outperforms the best session search systems in TREC. This research is perhaps the first to employ reinforcement learning in session search. Our MDP view of modeling session search can potentially benefit a wide range of IR tasks.",1,ad,True
560,10. ACKNOWLEDGMENT,0,,False
561,"This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.",0,,False
562,11. REFERENCES,0,,False
563,"[1] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and S. Vigna. The query-flow graph: model and applications. In CIKM '08.",0,,False
564,"[2] I. Bordino, C. Castillo, D. Donato, and A. Gionis. Query similarity by projecting the query-flow graph. In SIGIR '10.",1,Query,True
565,"[3] P. Bruza, R. McArthur, and S. Dennis. Interactive internet search: keyword, directory and query reformulation mechanisms compared. In SIGIR '00.",0,,False
566,"[4] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In CIKM '11.",0,,False
567,"[5] M.-A. Cartright, R. W. White, and E. Horvitz. Intentions and attention in exploratory health search. In SIGIR'11.",0,,False
568,"[6] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5), Oct. 2011.",0,,False
569,[7] D. Guan and H. Yang. Increasing stability of result organization for session search. In ECIR '13.,0,,False
570,"[8] D. Guan, H. Yang, and N. Goharian. Effective structured query formulation for session search. In TREC '12.",1,TREC,True
571,"[9] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and discriminative model for query refinement. In SIGIR '08.",0,,False
572,[10] Q. Guo and E. Agichtein. Ready to buy or just browsing?: detecting web searcher goals from interaction data. In SIGIR '10.,1,ad,True
573,"[11] D. S. Hirschberg. Algorithms for the longest common subsequence problem. J. ACM, 24(4), Oct. 1977.",0,,False
574,[12] J. Huang and E. N. Efthimiadis. Analyzing and evaluating query reformulation strategies in web search logs. In CIKM '09.,1,ad,True
575,"[13] J. Jiang, S. Han, J. Wu, and D. He. Pitt at trec 2011 session track. In TREC '11.",1,trec,True
576,"[14] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track. In TREC '12.",1,trec,True
577,[15] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM '08.,0,,False
578,"[16] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: a survey. J. Artif. Int. Res., 4(1), May 1996.",0,,False
579,"[17] Y. Kalfoglou and M. Schorlemmer. Ontology mapping: the state of the art. Knowl. Eng. Rev., 18(1), Jan. 2003.",0,,False
580,"[18] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2011 session track. In TREC'11.",1,trec,True
581,"[19] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2012 session track. In TREC'12.",1,trec,True
582,"[20] E. Kanoulas, P. D. Clough, B. Carterette, and M. Sanderson. Session track at trec 2010. In TREC'10.",1,Session,True
583,"[21] Lemur Search Engine. http://www.lemurproject.org/. [22] C. Liu, N. J. Belkin, and M. J. Cole. Personalization of",0,,False
584,search results using interaction behaviors in search sessions. In SIGIR '12.,0,,False
585,"[23] C. Liu, M. Cole, E. Baik, and J. N. Belkin. Rutgers at the trec 2012 session track. In TREC'12.",1,trec,True
586,"[24] C. Liu, J. Gwizdka, J. Liu, T. Xu, and N. J. Belkin. Analysis and evaluation of query reformulations in different task types. In ASIST '10.",0,,False
587,[25] J. Liu and N. J. Belkin. Personalizing information retrieval for multi-session tasks: the roles of task stage and task type. In SIGIR '10.,0,,False
588,"[26] A. M-Dyaa, K. Udo, N. Nikolaos, N. Brendan, L. Deirdre, and F. Maria. University of essex at the trec 2011 session track. In TREC '11.",1,trec,True
589,"[27] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Inf. Process. Manage., 40(5), Sept. 2004.",0,,False
590,"[28] S. P. Singh. Learning to solve markovian decision processes. Technical report, Amherst, MA, USA, 1993.",0,,False
591,[29] Y. Song and L.-w. He. Optimal rare query suggestion with implicit user feedback. In WWW '10.,0,,False
592,"[30] Y. Song, D. Zhou, and L.-w. He. Query suggestion by constructing term-transition graphs. In WSDM '12.",1,Query,True
593,"[31] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize or not to personalize: modeling queries with variation in user intent. In SIGIR '08.",0,,False
594,"[32] R. W. White, I. Ruthven, J. M. Jose, and C. J. V. Rijsbergen. Evaluating implicit feedback models using searcher simulations. ACM Trans. Inf. Syst., 23(3), July 2005.",0,,False
595,"[33] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, Apr. 2004.",0,,False
596,462,0,,False
597,,0,,False

,sentence,label,data,regex
0,A Comparison of the Optimality of Statistical Significance Tests for Information Retrieval Evaluation,0,,False
1,Julián Urbano jurbano@inf.uc3m.es,0,,False
2,Mónica Marrero mmarrero@inf.uc3m.es,0,,False
3,Diego Martín dmartin@dit.upm.es,0,,False
4,University Carlos III of Madrid Department of Computer Science,1,ad,True
5,"Leganés, Spain",0,,False
6,Technical University of Madrid Department of Telematics Engineering,1,ad,True
7,"Madrid, Spain",1,ad,True
8,ABSTRACT,0,,False
9,"Previous research has suggested the permutation test as the theoretically optimal statistical significance test for IR evaluation, and advocated for the discontinuation of the Wilcoxon and sign tests. We present a large-scale study comprising nearly 60 million system comparisons showing that in practice the bootstrap, t-test and Wilcoxon test outperform the permutation test under different optimality criteria. We also show that actual error rates seem to be lower than the theoretically expected 5%, further confirming that we may actually be underestimating significance.",1,ad,True
10,Categories and Subject Descriptors,0,,False
11,H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation.,0,,False
12,Keywords,0,,False
13,"Evaluation, Statistical significance, Randomization, Permutation, Bootstrap, Wilcoxon test, Student's t-test, Sign test.",0,,False
14,1. INTRODUCTION,1,DUC,True
15,"An Information Retrieval (IR) researcher is often faced with the question of which of two IR systems, A and B, performs better. She conducts an experiment with a test collection, and chooses an effectiveness measure such as Average Precision or nDCG. Based on the effectiveness difference she concludes that, for instance, system A is better. But we know there is inherent noise in the evaluation for a wealth of reasons concerning document collections, topic sets, relevance assessors, etc. Therefore the researcher needs the conclusion to be reliable, that is, the observed difference unlikely to have happened just by random chance. She employs a statistical significance test to compute this probability (the p-value). If p   (the significance level, usually  , 0.05 or  , 0.01) the difference is considered statistically significant (A B). In practice this means that she can be confident that the difference measured with a similar test collection",0,,False
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
17,"will be (at least) as large as currently observed. If p >  the difference is not significant (A B), and she can not be confident that the observed difference is indeed real.",0,,False
18,"Unfortunately, there has been a debate regarding statistical significance testing in IR evaluation. Classical tests such as the paired t-test, the Wilcoxon test and the sign test make different assumptions about the distributions, and effectiveness scores from IR evaluations are known to violate these assumptions. The bootstrap test is an alternative that makes fewer assumptions and has other advantages over classical tests, and the permutation or randomization test is an even less stringent test in terms of assumptions that theoretically provides exact p-values. Because IR evaluations violate most of the assumptions, it is very important to know how robust these tests are in practice and which one is optimal.",1,ad,True
19,"Previous work [4, 5] compared these five tests with TREC Ad Hoc data, reaching the following conclusions: a) the bootstrap, t-test and permutation test largely agree with each other, so there is hardly any practical difference in using one or another; b) the permutation test should be the test of choice, though the t-test seems suitable as well; the bootstrap test shows a bias towards small p-values; c) the Wilcoxon and sign tests are unreliable and should be discontinued for IR evaluation. However, all these conclusions were based on the assumption that the permutation test is optimal. For example, authors showed that the Wilcoxon and sign tests fail to detect significance when the permutation test does and vice versa. That is, they are unreliable according to the permutation test.",1,TREC,True
20,"But we may follow different criteria to chose an optimal test. We may want the test to be powerful, that is, to produce significant results as often as possible. Additionally, we may want it to be safe and yield low error rates so that it is unlikely that we draw wrong conclusions. But power and safety are inversely related; different tests show different relations depending on the significance level. The lower  the lower the power, because we need p   for the result to be significant. Error rates are expected to be at the nominal  level, so the higher the significance level the higher the expected error rate. The test is exact if we can trust that the actual error rate is as dictated by the significance level. If it is below it means we are being too conservative and we are missing significant results; if it is above it means we are deeming as significant results that probably are not.",0,,False
21,"This paper presents a large-scale empirical study that compares all five tests according to these optimality criteria, providing significance and error rates at various significance levels for 50-topic sets. Our main findings are:",0,,False
22,925,0,,False
23,"· In practice the bootstrap test is optimal in terms of power, the t-test is optimal in terms of safety, and the Wilcoxon test is optimal in terms of exactness.",0,,False
24,"· For all tests the actual error rate seems to be lower than the nominal 0.05 level, meaning that we are actually being too conservative.",0,,False
25,· In practice the permutation test is not found to be optimal under any criterion.,0,,False
26,2. DATA AND METHODS,0,,False
27,"To compare the five statistical significance tests at hand, we employed data from the TREC 2004 Robust Track. A total of 249 topics were used, 100 of which were originally developed in the TREC 7 and 8 Ad Hoc tracks (50 and 50). A total of 110 runs were submitted by 14 different groups. This dataset is unusually large both in terms of topics and runs, given that TREC tracks usually employ 50 topics. The subset with the 100 Ad Hoc topics is especially interesting: all 100 topics were developed and judged by the same assessors for the most part, and they were developed using the same methodology and pooling protocol with roughly the same number of runs contributing to the pools [6]. Additionally, all three tracks used disks 4 and 5 as document collection. Therefore, we can consider these two sets of 50 topics as two different samples drawn from the same universe of topics.",1,TREC,True
28,"We randomly split these 100 topics into two disjoint subsets of 50 topics each: T and T . For each of these two subsets we evaluated all 110 runs as per Average Precision. This provides us with 5,995 system pairwise comparisons with T and another 5,995 with T . We ran all five statistical significance tests between each of these system pairs1. This gives us a total of 5,995 pairs of p-values per test, which can be regarded as the two p-values observed with two different test collections for any two systems. We performed 1,000 random trials of this experiment, so we have a total of 5,995,000 system pairwise comparisons and the corresponding 5,995,000 with another topic subset. Thus, this paper reports results on nearly 12 million p-values for each of the five tests, for a grand total of nearly 60 million p-values. To our knowledge, this is to date the largest study of this type.",0,,False
29,"Given an arbitrary topic set split, the 5,995 pairs of pvalues provided by a test can be used to study its optimality. Consider a researcher that used topic subset T and ran a test to compute a p-value; under the significance level  he draws a conclusion. What can he expect with a different topic subset T ?. One of these situations can occur:",0,,False
30,· Non-significance. The result with T is A B. We can really expect any result with T ; there is a lack of statistical power in the experiment.,0,,False
31,· Success. The result with both T and T is A B. Both experiments show evidence of one system outperforming the other.,0,,False
32,· Lack of power. The difference is A B with T but it is A B with T . There is evidence of a lack of power in the second experiment.,0,,False
33,"· Minor error. The result with T is A B, but with T it is A  B. The second experiment shows some evidence of a wrong conclusion in the first one.",0,,False
34,"· Major error. The result with T is A B, but with T it is A  B. The two experiments conflict.",0,,False
35,"1As in [4, 5], we calculated 100,000 samples in the permutation and bootstrap tests.",0,,False
36,"A powerful test minimizes the non-significance rate, a safe test minimizes the minor and major error rates, and an exact test maintains the global error rate at the nominal  level.",0,,False
37,3. RESULTS,0,,False
38,"For every statistical significance test we computed the non-significance, success, lack of power and error rates at 32 significance levels   {0.0001, ..., 0.0009, 0.001, ..., 0.009, ..., 0.1, ..., 0.5}. Tables 1 and 2 report the results for a selection of significance levels, and Figures 1 and 2 plot detailed views in the arguably most interesting [0.001, 0.1] range. Please note that all plots are log-scaled.",0,,False
39,"Non-significance rate. The bootstrap test consistently produces smaller p-values, and it is therefore the most powerful of all tests across significance levels. Next are the permutation test for  < 0.01 and the Wilcoxon test for the usual   0.01. The t-test is consistently less powerful, though the difference is as small as roughly 1% fewer significant results at the usual  ,"" 0.05. The sign test is by far the least powerful of all five. Its stair-like behavior is explained by its resolution: p-values depend only on the sign of the score differences, not on the magnitude (see Figure 5 in [4]).""",0,,False
40,"Success rate. The bootstrap and Wilcoxon tests are the most successful overall. For small significance levels   0.001 the bootstrap test shows the highest success rate, but for the more usual levels 0.001 <   0.05 the Wilcoxon test performs better. Next are again the permutation test and the t-test, with very similar success rates about 0.3% lower than the Wilcoxon and bootstrap tests at the usual  levels. The sign test is clearly the worst of all.",0,,False
41,"Lack of power rate. Most of the unsuccessful comparisons are due to a lack of power with the second topic subset T . Relative results are comparable to results above: the bootstrap test dominates at small significance levels and the Wilcoxon tests dominates at the usual levels, again followed by the permutation test and the t-test.",0,,False
42,"Minor error rate. Except for rare occasions where the sign test's step-like behavior results in the smallest minor error rate, the t-test is generally the safest of all five across significance levels. The permutation test follows next with rates about 0.03% higher. The bootstrap test is consistently outperformed by the t-test and the permutation test; it yields 0.13% more minor errors at  ,"" 0.05. The Wilcoxon test performs even better than the permutation test for low significance levels, but it performs worse at the usual levels. As mentioned, the sign test wiggles between the other tests.""",0,,False
43,"Major error rate. Similarly the t-test consistently performs best in terms of major errors, followed by the permutation and bootstrap tests. It is noticeable that for small significance levels neither of these three tests show any major error at all. For instance, at  ,"" 0.005 the t-test provides as many as 3,006,441 (50.2%) significant comparisons, and yet none of them results in a major error with the second topic subset. The Wilcoxon test outperforms the permutation test sporadically, but it performs worse overall. In general though, it is important to the bear in mind the magnitudes of the major error rates. For instance, at  "","" 0.05 the t-test produced 1,082 major errors and the bootstrap test produced 1,523. While the difference may seem small compared to the total of significants (0.0277% vs 0.0383%), this is actually a large +41% relative increase. The sign test is clearly the worst of all, having an extremely large major error rate at small significance levels.""",1,ad,True
44,926,0,,False
45,Non-significance rate,0,,False
46, t-test perm. boot. Wilcox. sign .0001 .67698 .65006 .6402 .67189 .72367 .0005 .61184 .59202 .58186 .60471 .6782 .001 .5807 .56367 .5532 .5722 .63438 .005 .49842 .48755 .47647 .48911 .58347 .01 .45752 .44937 .43847 .4485 .53308 .05 .34779 .34539 .33613 .34215 .42762 .1 .29264 .29235 .28412 .28725 .37308 .5 .12398 .12581 .12153 .11957 .14934,0,,False
47,Success rate,0,,False
48,t-test perm. boot. Wilcox. sign,0,,False
49,.78749 .79451 .79757 .78859 .73691 .80788 .8107 .8123 .80765 .75479 .81328 .81491 .81547 .8147 .76847 .82051 .82145 .82365 .82598 .78018 .82777 .82893 .83233 .83338 .79225 .85579 .85565 .85856 .85935 .81999 .86905 .86899 .87086 .86941 .83013 .8836 .88369 .8836 .88429 .85641,0,,False
50,Lack of power rate,0,,False
51,t-test perm. boot. Wilcox. sign,0,,False
52,.21222 .20503 .20191 .21107 .26264 .19138 .18827 .18653 .19146 .24431 .18556 .18359 .18285 .18392 .22998 .1764 .17503 .17243 .17039 .2169 .16753 .16595 .16205 .16115 .20276 .13157 .1314 .12743 .12624 .16709 .1107 .11072 .10736 .10805 .15031 .05175 .05232 .05088 .04985 .07511,0,,False
53,"Table 1: Non-significance rates over total of pairs (lower is better), success rates over total of significants (higher is better), and lack of power rates over total of significants (lower is better). Best per  in bold face.",0,,False
54,0.65,0,,False
55,Non-significance rate,0,,False
56,t-test permutation bootstrap Wilcoxon sign,0,,False
57,Success rate,0,,False
58,Lack of power rate,0,,False
59,0.14 0.16 0.18 0.20 0.22,0,,False
60,Lacks of power / Total significants,0,,False
61,0.80 0.82 0.84 0.86,0,,False
62,Successes / Total significants,0,,False
63,0.35 0.4 0.45 0.5 0.55,0,,False
64,Non-significants / Total,0,,False
65,0.78,0,,False
66,0.12,0,,False
67,0.3,0,,False
68,0.76,0,,False
69,.001,0,,False
70,.005 .01,0,,False
71,.05,0,,False
72,.1,0,,False
73,.001,0,,False
74,.005 .01,0,,False
75,.05,0,,False
76,.1,0,,False
77,.001,0,,False
78,.005 .01,0,,False
79,.05,0,,False
80,.1,0,,False
81,Significance level ,0,,False
82,Significance level ,0,,False
83,Significance level ,0,,False
84,"Figure 1: Non-significance rates over total of pairs (lower is better), success rates over total of significants (higher is better), and lack of power rates over total of significants (lower is better).",0,,False
85,Minor error rate,0,,False
86, t-test perm. boot. Wilcox. sign .0001 .00029 .00046 .00051 .00034 .00045 .0005 .00074 .00104 .00117 .00089 .00089 .001 .00116 .00149 .00168 .00138 .00155 .005 .00309 .00352 .00392 .00362 .00282 .01 .00469 .00511 .0056 .00546 .00484 .05 .01236 .01264 .01363 .014 .01251 .1 .01903 .01906 .02027 .02123 .01862 .5 .03403 .03409 .03389 .03645 .03518,0,,False
87,Major error rate,0,,False
88,t-test perm. boot. Wilcox. sign,0,,False
89,0,0,,False
90,0,0,,False
91,0,0,,False
92,5.08e-7 6.04e-7,0,,False
93,0,0,,False
94,0,0,,False
95,0,0,,False
96,4.22e-7 5.18e-7,0,,False
97,0,0,,False
98,0,0,,False
99,0,0,,False
100,3.9e-7 4.56e-7,0,,False
101,0,0,,False
102,0,0,,False
103,6.37e-7 1.96e-6 .0001,0,,False
104,.00001 .00001 .00002 .00001 .00016,0,,False
105,.00028 .0003 .00038 .0004 .00041,0,,False
106,.00122 .00123 .00152 .00131 .00095,0,,False
107,.03062 .0299 .03163 .02941 .0333,0,,False
108,Global error rate,0,,False
109,t-test perm. boot. Wilcox. sign,0,,False
110,.00029 .00046 .00051 .00034 .00045 .00074 .00104 .00117 .00089 .00089 .00116 .00149 .00168 .00138 .00155 .00309 .00352 .00392 .00362 .00292 .0047 .00512 .00562 .00547 .00499 .01264 .01294 .01402 .01441 .01292 .02025 .02029 .02178 .02254 .01956 .06465 .06399 .06552 .06586 .06849,0,,False
111,"Table 2: Minor error rates over total of significants (lower is better), major error rates over total of significants (lower is better), and global error rates over total of significants (errors ,  is better). Best per  in bold face.",0,,False
112,0.020,0,,False
113,Minor error rate,0,,False
114,t-test permutation bootstrap Wilcoxon sign,0,,False
115,"y,x",0,,False
116,5e-04,0,,False
117,Major error rate,0,,False
118,2e-02 5e-02,0,,False
119,Global error rate,0,,False
120,0.010,0,,False
121,2e-03 5e-03,0,,False
122,Minor and Major errors / Total significants,0,,False
123,5e-05,0,,False
124,Major errors / Total significants,0,,False
125,0.005,0,,False
126,Minor errors / Total significants,0,,False
127,5e-06,0,,False
128,0.002,0,,False
129,5e-04,0,,False
130,5e-07,0,,False
131,0.001,0,,False
132,.001,0,,False
133,.005 .01,0,,False
134,.05,0,,False
135,.1,0,,False
136,.001,0,,False
137,.005 .01,0,,False
138,.05,0,,False
139,.1,0,,False
140,.0001 .0005.001,0,,False
141,.005 .01,0,,False
142,.05 .1,0,,False
143,.5,0,,False
144,Significance level ,0,,False
145,Significance level ,0,,False
146,Significance level ,0,,False
147,"Figure 2: Minor error rates over total of significants (lower is better), major error rates over total of significants (lower is better), and global error rates over total of significants (errors ,  is better).",0,,False
148,927,0,,False
149,"Global error rate. Aggregating minor and major errors we have a global error rate that can be used as an overall indicator of test safety and exactness. Given the relative size of minor and major error rates, the trends are here nearly the same as fwith minor errors, but for the sake of completeness we plot the full range of significance levels. The t-test approximates best the nominal error rate for low significance levels, but the Wilcoxon test does better for the usual levels and best overall. Surprisingly the permutation test does not seem to be the most exact at any significance level.",0,,False
150,4. DISCUSSION,0,,False
151,"Zobel [7] compared the t-test, Wilcoxon test and ANOVA at  ,"" 0.05, though with only one random split in 25-25 topics. He found lower error rates with the t-test than with the Wilcoxon test, and generally lower than the nominal 0.05 level. Given that the latter showed higher power and has more relaxed assumptions, he recommended it over the t-test. Sanderson and Zobel [3] ran a larger study also with splits of up to 25-25 topics. They found that the sign test has higher error rates than the Wilcoxon test, which has itself higher error rates than the t-test. They also suggested that the actual error rate is below the nominal 0.05 level when using 50 topic sets. Voorhees [6] also observed error rates below the nominal 0.05 level for the t-test, but more unstable effectiveness measures resulted in higher rates. Cormack and Lynam [1] used 124-124 topic splits and various significance levels. They found the Wilcoxon test more powerful than the t-test and sign test; and the t-test safer than the Wilcoxon and sign test. Sakai [2] proposed the bootstrap method for IR evaluation, but did not compare it with other tests.""",0,,False
152,"Smucker et al. [4] compared the same five tests we study in this paper, arguing that the t-test, permutation and bootstrap tests largely agree with each other. Nonetheless, they report RMS Errors among their p-values of roughly 0.01, which is a large 20% for p-values of 0.05. Based on the argument that the permutation test is theoretically exact, they concluded that the Wilcoxon and sign tests are unreliable, suggesting that they should be discontinued for IR evaluation. They find the bootstrap test to be overly powerful, and given the appealing theoretical optimality of the permutation test they propose its use over the others, though the t-test admittedly performed very similarly. In a later paper [5] they found that the tests tended to disagree with smaller topic sets, though the t-test still showed acceptable agreement with the permutation test, again assumed to be optimal. The bootstrap test tended again to produce smaller p-values, so authors recommend caution if using it.",1,ad,True
153,"In this paper we ran a large-scale study to revisit these issues under different optimality criteria. In terms of safety, the t-test produced the smallest error rates across significance levels, followed by the Wilcoxon test for low levels and the permutation test for usual levels. In general, all tests yielded error rates higher than expected for low significance levels, but much lower for the usual levels. This suggests that we are being too conservative when assessing statistical significance at  ,"" 0.05; we expect 5% of our significant results to be wrong, but in practice only about 1.3% do indeed seem wrong. We must note though that this global error rate, as the sum of minor and major errors, is just an approximation of the true Type I error rates [1].""",0,,False
154,Table 3 shows the agreement of the five tests with themselves: p-values with topic subset T compared to those with,0,,False
155,t-test perm. boot. Wilcox. sign p .0001 .03603 .04348 .04475 .03514 .0556 .0001< p .0005 .10124 .11635 .11923 .09976 .13014 .0005< p .001 .13059 .12999 .1623 .14516 .14619 .001< p .005 .16716 .17044 .20032 .17841 .18024 .005< p .01 .20724 .21624 .2387 .21454 .21737 .01< p .05 .25275 .26685 .29801 .25779 .26114 .05< p .1 .29734 .31101 .33996 .30015 .30344 .1< p .5 .31624 .31855 .33816 .31804 .31802,0,,False
156,Table 3: RMS Error of all five tests with themselves (lower is better). Best per bin in bold face.,0,,False
157,"subset T . The Wilcoxon test turns out to be the most stable of all for very small p-values, and generally more so than the permutation test. The t-test is the most stable overall. Indeed, if we compute the difference between the actual and nominal error rates we find that the Wilcoxon test is the one that best tracks the significance level and therefore seems to be the most exact (RMSE 0.1146), followed by the bootstrap, t-test, sign and permutation tests (RMSEs 0.1148, 0.1153, 0.1153 and 0.1155). This is particularly interesting for the bootstrap test: it provides the most significant results and the actual error rate is still lower than expected.",0,,False
158,"In summary, a researcher that wants to maximize the number of significant results may use the more powerful bootstrap test and still be safe in the usual scenario. Researchers that want to maximize safety may use the t-test, and researchers that want to be able to trust the significance level may proceed with the Wilcoxon test. For large meta-analysis studies we encourage the use of the t-test and Wilcoxon test because they are far less computationally expensive and show near-optimal behavior. Unlike previous work concluded, our results suggest that in practice the permutation test is not optimal under any criterion. Further analysis with varied test collections and effectiveness measures should be conducted to clarify this matter, besides devising methods to better approximate what actual Type I error rates we have in IR evaluation. We further support the argument of discontinuing the sign test.",0,,False
159,5. REFERENCES,0,,False
160,"[1] G. V. Cormack and T. R. Lynam. Validity and Power of t-test for Comparing MAP and GMAP. In ACM SIGIR, pages 753­754, 2007.",1,MAP,True
161,"[2] T. Sakai. Evaluating Evaluation Metrics Based on the Bootstrap. In ACM SIGIR, pages 525­532, 2006.",0,,False
162,"[3] M. Sanderson and J. Zobel. Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability. In ACM SIGIR, pages 162­169, 2005.",0,,False
163,"[4] M. D. Smucker, J. Allan, and B. Carterette. A Comparison of Statistical Significance Tests for Information Retrieval Evaluation. In ACM CIKM, pages 623­632, 2007.",0,,False
164,"[5] M. D. Smucker, J. Allan, and B. Carterette. Agreement Among Statistical Significance Tests for Information Retrieval Evaluation at Varying Sample Sizes. In ACM SIGIR, pages 630­631, 2009.",0,,False
165,"[6] E. M. Voorhees. Topic Set Size Redux. In ACM SIGIR, pages 806­807, 2009.",0,,False
166,"[7] J. Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In ACM SIGIR, pages 307­314, 1998.",0,,False
167,928,0,,False
168,,0,,False

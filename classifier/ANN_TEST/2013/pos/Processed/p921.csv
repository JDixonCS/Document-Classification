,sentence,label,data,regex
0,The Impact of Intent Selection on Diversified Search Evaluation,0,,False
1,Tetsuya Sakai,0,,False
2,"Microsoft Research Asia,",0,,False
3,P.R.C.,0,,False
4,tetsuyasakai@acm.org,0,,False
5,Zhicheng Dou,0,,False
6,Charles L. A. Clarke,0,,False
7,"Microsoft Research Asia,",0,,False
8,"University of Waterloo,",0,,False
9,P.R.C.,0,,False
10,Canada,1,ad,True
11,zhichdou@microsoft.com claclark@plg.uwaterloo.ca,0,,False
12,ABSTRACT,0,,False
13,"To construct a diversified search test collection, a set of possible subtopics (or intents) needs to be determined for each topic, in one way or another, and per-intent relevance assessments need to be obtained. In the TREC Web Track Diversity Task, subtopics are manually developed at NIST, based on results of automatic click log analysis; in the NTCIR INTENT Task, intents are determined by manually clustering ""subtopics strings"" returned by participating systems. In this study, we address the following research question: Does the choice of intents for a test collection affect relative performances of diversified search systems? To this end, we use the TREC 2012 Web Track Diversity Task data and the NTCIR-10 INTENT-2 Task data, which share a set of 50 topics but have different intent sets. Our initial results suggest that the choice of intents may affect relative performances, and that this choice may be far more important than how many intents are selected for each topic.",1,TREC,True
14,Categories and Subject Descriptors,0,,False
15,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,0,,False
16,General Terms,0,,False
17,Experimentation,0,,False
18,Keywords,0,,False
19,"diversity, evaluation, intents, subtopics, test collections",0,,False
20,1. INTRODUCTION,1,DUC,True
21,"Given an ambiguous or underspecified query, diversified search aims to cover different possible search intents with a single search engine result page, by balancing relevance and diversity. TREC1 started a Diversity Task in the Web Track2 in 2009, while NTCIR3 started a related task called INTENT4 in 2011. Unlike traditional retrieval evaluation where pooled documents are assessed in terms",1,TREC,True
22,1http://trec.nist.gov/ 2http://plg.uwaterloo.ca/~trecweb/ 3http://research.nii.ac.jp/ntcir/ 4http://research.microsoft.com/INTENT/,1,trec,True
23,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
24,"quit smoking (TREC topicID,182; NTCIR topicID,0432)",1,TREC,True
25,"(a) TREC ""subtopics""",1,TREC,True
26,1. What are the ways you can quit smoking?,0,,False
27,2. What are the benefits of quitting,0,,False
28,smoking?,0,,False
29,3. Can you quit smoking using the cold turkey method?,0,,False
30,4. How can hypnosis help someone,0,,False
31,quitting smoking?,0,,False
32,"(b) NTCIR ""intents"" 1. effects (0.15) 2. ways (0.15) 3. benefit (0.14) 4. reasons (0.14) 5. products (0.14)",0,,False
33,6. public resource (0.10) 7. aids (0.10),0,,False
34,8. people (0.08),0,,False
35,"(c) NTCIR ""subtopic strings""",0,,False
36,women weight gain quit smoking;what happens when you quit smoking;...,0,,False
37,what to do to quit smoking;ways to quit smoking cigarettes;...,0,,False
38,quit smoking health benefits;quit smoking health;...,0,,False
39,why quit smoking;reasons to quit smoking,0,,False
40,wellbutrin quit smoking;using guided imagery to quit smoking;...,0,,False
41,women quit smoking forums;tobacco products scientific advisory committee;... Stop Smoking Aids;quit smoking tobacco,1,ad,True
42,advertising;... the people behind quitsmoking;the man who,1,ad,True
43,quit smoking;...,0,,False
44,Figure 1: TREC subtopics vs. NTCIR intents and subtopic strings.,1,TREC,True
45,"of relevance with respect to each topic, diversity evaluation requires a set of subtopics (or intents) for each topic, and pooled documents are assessed with respect to each intent. In the TREC Diversity Task, subtopics are manually developed at NIST, based on results of automatic click log analysis [4]; in the NTCIR INTENT Task, intents are determined by manually clustering ""subtopics strings"" returned by participating systems [6, 7]. However, it is difficult to say exactly what the most appropriate intents are for a given topic for the purpose of evaluating diversified search.",1,TREC,True
46,"One may be tempted to hypothesise that an effective diversified search system should be effective regardless of the particular choice of intents used to evaluate it. Thus, in this study, we address the following research question: Does the choice of intents for a test collection affect relative performances of diversified search systems? To this end, we use the TREC 2012 Web Track Diversity Task data and the NTCIR-10 INTENT-2 Task data, which share a set of 50 topics but have different intent sets [6, 7]. Figure 1 provides an actual example from the 50 topics we used in our experiments: for this topic (""quit smoking""), four subtopics were obtained for TREC at NIST as shown in Figure 1(a), by following the aforementioned click-based methodology; whereas, at NTCIR, subtopic strings were returned by the participating system of the INTENT2 Subtopic Mining Subtask as shown in Figure 1(c), which were then manually clustered and filtered to form a set of eight intents as shown in Figure 1(b)5. As (a) and (b) were obtained completely independently using different methods, they obviously differ, although they may partially overlap as indicated by the dotted lines in the figure. For example, the first TREC subtopic for this topic is ""What are the ways you can quit smoking?"" which is probably similar to the second NTCIR intent ""(quit smoking) ways.""",1,ad,True
47,"5As shown in the figure, the INTENT task also estimates the probability of each intent given the query based on assessor voting. However, following the practice at TREC, we assume that the probability distribution is uniform across all intents throughout this study.",1,INTENT,True
48,921,0,,False
49,Table 1: Test collection and pseudo-qrels statistics. Eight,0,,False
50,TREC intents with no relevant documents have been removed.,1,TREC,True
51,"In Part (b), statistics for the truncated pseudo-qrels are shown",0,,False
52,in parentheses.,0,,False
53,(a) TREC 2012,1,TREC,True
54,(b) English INTENT-2,1,INTENT,True
55,Diversity and pseudo-qrels derived,0,,False
56,topics,0,,False
57,50 (provided from TREC to NTCIR),1,TREC,True
58,intents/topic,0,,False
59,3.7 all: 7.8; matched: 7.7 (3.7),0,,False
60,subtopic strings/topic,0,,False
61,­,0,,False
62,82.7,0,,False
63,pooled non-junk docs/topic,0,,False
64,303.9,0,,False
65,­,0,,False
66,unique relevant/topic 4-relevant/topic 3-relevant/topic 2-relevant/topic 1-relevant/topic,0,,False
67,111.2 49.7,0,,False
68,2.6 23.5 111.6,0,,False
69,287.0 (263.8) ­,0,,False
70,11.8 (8.8) 289.7 (182.7) 881.7 (435.3),0,,False
71,"foreach topic t do { foreach NTCIR intent i for t do { foreach TREC pooled document d for t do { matchcount , 0; foreach reduced subtopic s for i do { if d contains s by exact match then matchcount + +; } relevancelevel (d) ,"" max(0, trunc(log(matchount ) + 1)); //the function trunc takes the integer part of the argument. }""",1,TREC,True
72,},0,,False
73,Figure 2: Algorithm for automatically generating pseudo-qrels.,0,,False
74,"This intent was devised at NTCIR based on a cluster of subtopic strings obtained from participating systems, including ""what to do quit smoking"" and ""ways to quit smoking cigarettes"" amongst others. Hereafter, we shall also refer to TREC subtopics as ""intents"" to avoid confusion.",1,TREC,True
75,"To address the above research question, we replace the TREC intents with the NTCIR intents and then re-evaluate the runs submitted to the TREC 2012 diversity task. Unfortunately, while the NTCIR-10 INTENT-2 Task had Document Ranking (i.e., diversified search) subtasks for Chinese and Japanese, it only had a Subtopic Mining subtask for English [6, 7], and therefore the English intents from NTCIR lack document relevance assessments. While it would be ideal to actually conduct relevance assessments of the TREC pooled documents with respect to each NTCIR intent, we explore a cheaper alternative in this paper, namely, to automatically construct pseudo-qrels by simply matching the TREC pooled documents against the NTCIR subtopic strings6. While the lack of true relevance assessments for the NTCIR intents is a limitation of this study, our pseudo-qrels do provide partial answers to our research question: our initial results suggest that the choice of intents may in fact affect relative performances, and that this choice may be more important than how many intents are selected for each topic.",1,ad,True
76,2. EXPERIMENTAL SETTING,0,,False
77,2.1 Data,0,,False
78,"Table 1(a) shows some statistics of the TREC 2012 Web Diversity topics, after having removed eight intents from the original qrels.diversity file as they did not have any relevant documents. At TREC, there were six relevance levels: 4 (""navigational""), 3 (""key""), 2 (""highly relevant""), 1 (""relevant""), 0 (""nonrelevant"") and -2 (""junk"") [4]. The table refers to the first four as 4-, 3-, 2- and 1-relevant, respectively; we treat the other two",1,TREC,True
79,"6In TREC parlance, qrels means relevance assessments. When qrels are obtained automatically without involving manual relevance assessments, they are often referred to as pseudo-qrels [9].",1,TREC,True
80,"as nonrelevant. Also, as shown in the table, we had 303.9 pooled non-junk documents per topic on average, which we obtained from qrels.diversity7.",1,ad,True
81,"To re-evaluate the TREC 2012 diversity runs after replacing the TREC intents with the NTCIR intents, we created pseudo-qrels as follows: The original NTCIR intents have an average of 7.8 intents and 82.7 subtopic strings per topic (recall Figure 1(b) and (c)). To obtain pseudo-relevant documents from the TREC pools for each NTCIR intents, we first removed the topic string from each NTCIR subtopic string automatically: for example, ""women weight gain quit smoking"" in Figure 1 was turned into ""women weight gain."" We call the resultant strings reduced subtopics. We then used the simple algorithm shown in Figure 2 to obtain pseudo-relevant documents for each NTCIR intent. Note that each pooled document is tested whether it matches with any of the reduced subtopic, under the assumption that pooled documents already contain the actual topic string (e.g., ""quit smoking"") or some related term. The algorithm also determines the relevance level of each document based on the number of matches with reduced subtopics: the actual number of matches (matchcount ) varied from 0 to 19; the (natural) log-based function in the algorithm maps them to 0-3. A total of 28 pooled documents were removed during the process, as they have been detected as containing a virus.",1,TREC,True
82,"Table 1(b) shows some statistics of the NTCIR-10 INTENT-2 intents and the pseudo-qrels we derived from them. Note that, of the 303.9 TREC pooled documents per topic, as many as 293.6 matched with at least one reduced subtopic and are treated as relevant. This strongly suggests that our pseudo-qrels contain a lot of false matches. Moreover, because of this problem, note that the average number of intents per topic in the pseudo-qrels is 7.7, which is easily twice as large as the corresponding number for TREC, namely, 3.7. Thus, if the evaluation outcome with the pseudo-qrels is different from that with the true qrels, this may be because either (i) the two intent sets contain different intents; or (ii) the two intent sets differ in size (the NTCIR intents sets are larger so may require systems to diversify more aggresively); or both.",1,INTENT,True
83,"In order to separate the above two effects, we also created another version of pseudo-qrels, called truncated pseudo-qrels (or simply ""truncated"" for short). This was done by cutting down ""less popular"" intents from the original pseudo-qrels to ensure that the TREC and NTCIR intent sets are equal in size for each topic. For the example shown in Figure 1, although the original pseudo-qrels has eight intents, the truncated pseudo-qrels has only the first four intents with the highest intent probabilities. The statistics for the truncated pseudo-qrels are shown in parentheses in Table 1(b).",1,TREC,True
84,2.2 Evaluation Metrics and Analysis Methods,0,,False
85,"We primarily consider four diversity evaluation metrics: D-nDCG, D -nDCG [8], -nDCG and ERR-IA [3]. D-nDCG is a version of normalised Cumulative Discounted Gain (nDCG) [5] which combines per-intent graded relevance and intent probabilities to compute the gain value of each document. D -nDCG is a simple average of D-nDCG and intent recall (I-rec), a.k.a. subtopic recall [11]. D -nDCG summarises a graph that plots D-nDCG (i.e. overall relevance) against I-rec (pure diversity). -nDCG is a version of nDCG which defines graded relevance as the number of intents covered by a document, and discounts the value of a retrieved relevant document for each intent based on relevant documents already seen. This property is known as diminishing return [2]. ERR-IA first",1,ad,True
86,"7At TREC 2012, a common pool was created across the ad hoc task and the diversity task for each topic. Hence, the pooled documents obtained from qrels.diversity are identical to those from qrels.adhoc.",1,TREC,True
87,922,0,,False
88,"computes an Expected Reciprocal Rank value for each intent and then combines them across intents. It also possesses the diminishing return property. Both -nDCG and ERR-IA may be expressed in terms of a common framework, differing primarily in the discounts they apply for document rank [3].",0,,False
89,"The NTCIR INTENT task uses I-rec, D-nDCG and D -nDCG as the primary metrics for ranking the runs. Following the task's practice, we compute the values using NTCIREVAL8, by using the relevance levels as the gain values. However, it should be noted that I-rec is not a good stand-alone metric for our purpose: although we measure performance at document cutoffs of 10 and 20 (denoted by ""@10"" and ""@20""), recall that the average number of intents per topic with the true qrels is only 3.7: thus it should be fairly easy for systems to cover most intents, especially with 20 documents. Furthermore, I-rec does not work well with our pseudo-qrels: because of the aforementioned false match problem, I-rec is heavily overestimated for all of the TREC runs when the pseudo-qrels are used. Nevertheless, we include the results with I-rec for separating the effects of diversity and relevance in diversified search evaluation [8].",1,INTENT,True
90,"The TREC Web Track Diversity Task uses -nDCG and ERR-IA along with some other metrics. Following the practice at TREC, we computed these metrics using ndeval9. It should be noted that, while NTCIREVAL utilises the per-intent graded relevance data to compute D( )-nDCG, ndeval reduces the data to per-intent binary relevance data before computing -nDCG and ERR-IA. Thus the computation of relevance levels in Figure 2 does not affect these two metrics.",1,TREC,True
91,"In order to compare the relative performances of the TREC 2012 diversity runs before and after replacing the original TREC intents with the NTCIR ones, we compare the run rankings in terms Kendall's  , and its variant called ap [10]. These measures count the number of pairwise system swaps; ap is more sensitive to the swaps near the top ranks than  is. However, what is perhaps more important is whether replacing the intent sets affects statistical significance testing, which is often used for forming research conclusions in the IR community. We therefore conduct a randomised version of the two-sided Tukey's Honestly Significantly Different (HSD) test [1] at  ,"" .05 for the entire set of runs before and after replacing the intent sets. Given the entire set of runs, this kind of test is more appropriate than those that test one run pair at a time while ignoring the others. We then compare the two sets of significantly different run pairs. For example, is a significantly different run pair obtained according to the TREC intents still significantly different according to the NTCIR intents with its pseudo-qrels?""",1,TREC,True
92,3. RESULTS AND DISCUSSIONS,0,,False
93,"Table 2 shows the  and ap between rankings produced by two different metrics based on the true qrels, to show how the diversity metrics behave differently. Table 3 is more important for our purpose: for each metric, the  and ap for the ranking with the true qrels and that with the pseudo-qrels are shown. It can be observed that the rankings with the pseudo-qrels (i.e., those based on the NTCIR intents) are quite different from those with the true qrels (i.e., those based on the TREC intents). This is true even for the truncated pseudo-qrels, as shown in Part (b) of the table, which suggests that the discrepancies between TREC and NTCIR may arise not from how many intents are used but from the actual choice of intents.",1,TREC,True
94,8http://research.nii.ac.jp/ntcir/tools/ ntcireval-en.html 9http://trec.nist.gov/data/web2012.html,1,trec,True
95,Table 2:  /ap between two metrics using true qrels (20 TREC,1,TREC,True
96,2012 diversity runs).,0,,False
97,@10,0,,False
98,D-nDCG D -nDCG -nDCG ERR-IA,0,,False
99,I-rec,0,,False
100,.347/.323 .642/.493 .547/.451 .568/.496,0,,False
101,D-nDCG,0,,False
102,- .705/.780 .695/.719 .674/.700,0,,False
103,D -nDCG,0,,False
104,-,0,,False
105,- .779/.769 .695/.706,0,,False
106,-nDCG,0,,False
107,-,0,,False
108,-,0,,False
109,- .895/.895,0,,False
110,@20,0,,False
111,D-nDCG D -nDCG -nDCG ERR-IA,0,,False
112,I-rec,0,,False
113,.179/.319 .705/.661 .495/.537 .453/.522,0,,False
114,D-nDCG,0,,False
115,- .474/.580 .621/.656 .600/.637,0,,False
116,D -nDCG,0,,False
117,-,0,,False
118,- .705/.745 .579/.635,0,,False
119,-nDCG,0,,False
120,-,0,,False
121,-,0,,False
122,- .853/.855,0,,False
123,Table 3:  /ap between rankings by the same metric using true,0,,False
124,and (truncated) pseudo-qrels (20 TREC 2012 diversity runs).,1,TREC,True
125,@10,0,,False
126,@20,0,,False
127,(a) true,0,,False
128,I-rec,0,,False
129,.632/.392 .568/.302,0,,False
130,vs.,0,,False
131,D-nDCG .653/.698 .684/.692,0,,False
132,pseudo,0,,False
133,D -nDCG .611/.651 .632/.652 -nDCG .674/.673 .716/.704,0,,False
134,ERR-IA .589/.611 .589/.614,0,,False
135,(b) true,0,,False
136,I-rec,0,,False
137,.579/.317 .526/.271,0,,False
138,vs.,0,,False
139,D-nDCG .621/.660 .653/.666,0,,False
140,truncated D -nDCG .684/.682 .695/.668 -nDCG .663/.668 .716/.706,0,,False
141,ERR-IA .579/.614 .579/.616,0,,False
142,1 0.9 0.8 0.7 0.6 0.5 0.4,0,,False
143,0.3 (a) I-rec@20,0,,False
144,0.8 0.7 0.6 0.5 0.4 0.3 0.2,0,,False
145,0.1 (b) D-nDCG@20,0,,False
146,true pseudo truncated,0,,False
147,uogTrA44xu uogTrB44xu utw2012c1 uogTrA44xl UDInfoDivSt utw2012lda utw2012sc1 srchvrs12c00 srchvrs12c10 UDInfoDivC2 DFalah120D DFalah121D UDInfoDivC1 ICTNET12DVR3 ICTNET12DVR2 ICTNET12DVR1,0,,False
148,lcm4res manualSTA,0,,False
149,autoSTB autoSTA uogTrA44xu uogTrB44xu DFalah120D DFalah121D uogTrA44xl lcm4res srchvrs12c10 srchvrs12c00 autoSTA autoSTB utw2012c1 manualSTA utw2012lda utw2012sc1 UDInfoDivSt ICTNET12DVR1 ICTNET12DVR2 ICTNET12DVR3 UDInfoDivC1 UDInfoDivC2,1,TB,True
150,0.9 0.8 0.7 0.6 0.5 0.4 0.3,0,,False
151,(c) D#-nDCG@20,0,,False
152,0.2,0,,False
153,0.9 0.8 0.7 0.6 0.5 0.4,0,,False
154,0.3 (d) ERR-IA@20,0,,False
155,0.2,0,,False
156,uogTrA44xu uogTrB44xu uogTrA44xl DFalah120D DFalah121D utw2012c1 srchvrs12c00 srchvrs12c10 utw2012lda utw2012sc1 UDInfoDivSt,0,,False
157,lcm4res UDInfoDivC2 UDInfoDivC1 ICTNET12DVR3 ICTNET12DVR2 ICTNET12DVR1,0,,False
158,manualSTA autoSTB autoSTA,1,TB,True
159,uogTrA44xu uogTrB44xu DFalah121D DFalah120D utw2012c1 utw2012lda uogTrA44xl utw2012sc1 srchvrs12c00 srchvrs12c10 ICTNET12DVR1,0,,False
160,autoSTA ICTNET12DVR3 ICTNET12DVR2,0,,False
161,lcm4res autoSTB manualSTA UDInfoDivSt UDInfoDivC1 UDInfoDivC2,1,TB,True
162,Figure 3: Run rankings: true vs. pseudo vs. truncated. The x axis represents runs sorted by a metric with true relevance data from TREC.,1,TREC,True
163,"Figure 3 visualises the ""@20"" column of Table 3 for selected metrics. Recall that I-rec is a pure diversity metric; that D-nDCG is an overall relevance metric; and that D -nDCG and ERR-IA consider both aspects. It can be observed that I-rec with pseudo-qrels is almost completely useless for ranking runs. On the other hand, D-nDCG with pseudo-qrels does better: for example, the top two runs in terms of D-nDCG with the true qrels (uogTrA44xu and uogTrB44xu) are still the top two in terms of D-nDCG with the (truncated) pseudo-qrels. The same two runs are also top performers in terms of D -nDCG as well, regardless of the qrels being used. As for ERR-IA, while the same two runs are the top performer in terms of the true qrels, the second run uogTrB44xu is ranked third with the (truncated) pseudo-qrels. To sum up, while our pseudoqrels cannot properly estimate systems's intent recall, the top run at TREC, namely, uogTrA44xu, is still the top run when evaluated with D( )-nDCG and ERR-IA based on the NTCIR intents and the pseudo-qrels. However, the overall rankings do differ when the TREC intents are replaced with those from NTCIR. Again, since the graphs for the original and truncated pseudo-qrels behave very",1,TREC,True
164,923,0,,False
165,1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1,0,,False
166,0,0,,False
167,151 156 161 166 171 176 181 186 191 196,0,,False
168,1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1,0,,False
169,0,0,,False
170,151 156 161 166 171 176 181 186 191 196,0,,False
171,(a) D#-nDCG @20,0,,False
172,true pseudo truncated,0,,False
173,(b) ERR-IA @20,0,,False
174,true pseudo truncated,0,,False
175,Figure 4: Per-topic performance values for lcm4res.,0,,False
176,"similarly, the discrepancies between TREC and NTCIR are probably due to the choice of intents.",1,TREC,True
177,"In Figure 3, as indicated by the arrows, Run lcm4res is heavily overestimated with D( )-nDCG and ERR-IA based on the pseudoqrels. Figure 4 provides a per-topic diagnosis for this run with D nDCG and ERR-IA, which reveals that the pseudo-qrels overestimate the run's performance for almost all topics. Perhaps the worst case is Topic 170 (""scooters""), for which there are three TREC intents and eight NTCIR intents: even though the true D -nDCG and ERR-IA values are zero (as indicated by the arrows), the corresponding values with the pseudo-qrels are .8698 and .8750 (.9378 and 1 when truncated). As we have not conducted relevance assessments, we cannot rule out the possibility that this run actually retrieved many documents that are relevant to the NTCIR intents yet nonrelevant to the TREC intents for this topic. However, the overall trend across the topics strongly suggests that our pseudoqrels do not provide accurate estimates of intent recall. We leave the analysis of the TREC runs using true relevance assessments for the NTCIR intents for future work.",1,TREC,True
178,"We now discuss the effect of replacing the TREC intent sets with the NTCIR ones on statistical significance testing: Table 4 summarises the results. Note that, if the significance test results with true and pseudo-qrels are identical, the number of significantly different pairs in the TR, PS and TRPS will be the same, and that the TR-PS and PS-TR will contain zeroes. Such is not the case. That is, conclusions drawn from an experiment based on the original TREC intents and those drawn from one based on the intents derived from NTCIR can be quite different. For example, in Table 4(b), ERR-IA@20 obtains 31 significantly different run pairs with the true qrels and 18 significantly different run pairs with the pseudo-qrels; but only 9 pairs overlap. Again, truncating the pseudo-qrels (Table 4(c)(d)) does not seem to solve any problems, which again suggests that the choice of intents do matter for the purpose of comparing diversified search systems.",1,TREC,True
179,4. CONCLUSIONS AND FUTURE WORK,0,,False
180,"We addressed the following research question: Does the choice of intents for a test collection affect relative performances of diversified search systems? To this end, we used the TREC 2012 Web Track Diversity Task data and the NTCIR-10 INTENT-2 Task data, which share a set of 50 topics but have different intent sets. Our initial results suggest that the choice of intents may in fact affect relative performances, and that this choice may be more important than how many intents are selected for each topic.",1,ad,True
181,One limitation of the present work is that we used automaticallygenerated pseudo-qrels for the NTCIR intents instead of conducting relevance assessments of TREC pooled documents for the NT-,1,ad,True
182,Table 4: Significance test concordances and discordances be-,0,,False
183,tween true qrels and pseudo-qrels (190 TREC 2012 diver-,1,TREC,True
184,sity run pairs; randomised two-sided Tukey's HSD test at,0,,False
185," , .05). TR (PS): significant differences obtained with true",0,,False
186,qrels (pseudo-qrels); TR-PS (PS-TR): pairs significant with,0,,False
187,true qrels (pseudo-qrels) but not significant with pseudo-qrels,0,,False
188,(true qrels); TRPS: pairs significant with both true qrels and,0,,False
189,pseudo-qrels.,0,,False
190,TR PS TR-PS TRPS PS-TR,0,,False
191,(a) true,0,,False
192,I-rec,0,,False
193,12 21,0,,False
194,6,0,,False
195,6,0,,False
196,15,0,,False
197,vs.,0,,False
198,D-nDCG 51 56,0,,False
199,9,0,,False
200,42,0,,False
201,14,0,,False
202,pseudo D -nDCG 25 29,0,,False
203,9,0,,False
204,16,0,,False
205,13,0,,False
206,@10,0,,False
207,-nDCG 26 24,0,,False
208,16,0,,False
209,10,0,,False
210,14,0,,False
211,ERR-IA 29 19,0,,False
212,20,0,,False
213,9,0,,False
214,10,0,,False
215,(b) true,0,,False
216,I-rec,0,,False
217,9 11,0,,False
218,7,0,,False
219,2,0,,False
220,9,0,,False
221,vs.,0,,False
222,D-nDCG 60 60,0,,False
223,15,0,,False
224,45,0,,False
225,15,0,,False
226,pseudo D -nDCG 35 40,0,,False
227,9,0,,False
228,26,0,,False
229,14,0,,False
230,@20,0,,False
231,-nDCG 26 24,0,,False
232,15,0,,False
233,11,0,,False
234,13,0,,False
235,ERR-IA 31 18,0,,False
236,22,0,,False
237,9,0,,False
238,9,0,,False
239,(c) true,0,,False
240,I-rec,0,,False
241,12 11,0,,False
242,8,0,,False
243,4,0,,False
244,7,0,,False
245,vs.,0,,False
246,D-nDCG 51 47,0,,False
247,16,0,,False
248,35,0,,False
249,12,0,,False
250,truncated D -nDCG 25 14,0,,False
251,11,0,,False
252,14,0,,False
253,0,0,,False
254,@10,0,,False
255,-nDCG 26 9,0,,False
256,24,0,,False
257,2,0,,False
258,7,0,,False
259,ERR-IA 29 9,0,,False
260,25,0,,False
261,4,0,,False
262,5,0,,False
263,(d) true,0,,False
264,I-rec,0,,False
265,9 28,0,,False
266,3,0,,False
267,6,0,,False
268,22,0,,False
269,vs.,0,,False
270,D-nDCG 60 55,0,,False
271,20,0,,False
272,40,0,,False
273,15,0,,False
274,truncated D -nDCG 35 35,0,,False
275,12,0,,False
276,23,0,,False
277,12,0,,False
278,@20,0,,False
279,-nDCG 26 13,0,,False
280,24,0,,False
281,2,0,,False
282,11,0,,False
283,ERR-IA 31 10,0,,False
284,27,0,,False
285,4,0,,False
286,6,0,,False
287,"CIR intents. In particular, we found that the pseudo-qrels estimate intent recall very poorly. On the other hand, we have also found that the official top performer at the TREC 2012 diversity task is still the top performer even after the intent sets have been replaced with the ones from NTCIR. In order to obtain a more clear answer to our research question, we hope to come back to it with true relevance assessments for the NTCIR intents.",1,TREC,True
288,5. REFERENCES,0,,False
289,"[1] B. Carterette. Multiple testing in statistical analysis of systems-based information retrieval experiments. ACM TOIS, 30(1), 2012.",0,,False
290,"[2] O. Chapelle, S. Ji, C. Liao, E. Velipasaoglu, L. Lai, and S.-L. Wu. Intent-based diversification of web search results: Metrics and algorithms. Information Retrieval, 14(6):572­592, 2011.",0,,False
291,"[3] C. L. A. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In Proceedings of ACM WSDM 2011, pages 75­84, 2011.",1,ad,True
292,"[4] C. L. A. Clarke, N. Craswell, and E. M. Voorhees. Overview of the TREC 2012 web track. In Proceedings of TREC 2012, 2013.",1,TREC,True
293,"[5] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM TOIS, 20(4):422­446, 2002.",0,,False
294,"[6] T. Sakai, Z. Dou, T. Yamamoto, Y. Liu, M. Zhang, M. P. Kato, R. Song, and M. Iwata. Overview of the NTCIR-10 INTENT-2 task. In Proceedings of NTCIR-10, 2013.",1,INTENT,True
295,"[7] T. Sakai, Z. Dou, T. Yamamoto, Y. Liu, M. Zhang, M. P. Kato, R. Song, and M. Iwata. Summary of the NTCIR-10 INTENT-2 task: Subtopic mining and search result diversification. In Proceedings of ACM SIGIR 2013, 2013.",1,INTENT,True
296,"[8] T. Sakai and R. Song. Evaluating diversified search results using per-intent graded relevance. In Proceedings of ACM SIGIR 2011, pages 1043­1042, 2011.",1,ad,True
297,"[9] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In Proceedings of ACM SIGIR 2001, pages 66­73, 2001.",0,,False
298,"[10] E. Yilmaz, J. Aslam, and S. Robertson. A new rank correlation coefficient for information retrieval. In Proceedings of ACM SIGIR 2008, pages 587­594, 2008.",0,,False
299,"[11] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. In Proceedings of ACM SIGIR 2003, pages 10­17, 2003.",0,,False
300,924,0,,False
301,,0,,False

,sentence,label,data,regex
0,A Novel TF-IDF Weighting Scheme for Effective Ranking,0,,False
1,Jiaul H. Paik,0,,False
2,"Indian Statistical Institute, Kolkata, India",0,,False
3,jia.paik@gmail.com,0,,False
4,ABSTRACT,0,,False
5,"Term weighting schemes are central to the study of information retrieval systems. This article proposes a novel TF-IDF term weighting scheme that employs two different within document term frequency normalizations to capture two different aspects of term saliency. One component of the term frequency is effective for short queries, while the other performs better on long queries. The final weight is then measured by taking a weighted combination of these components, which is determined on the basis of the length of the corresponding query.",0,,False
6,Experiments conducted on a large number of TREC news and web collections demonstrate that the proposed scheme almost always outperforms five state of the art retrieval models with remarkable significance and consistency. The experimental results also show that the proposed model achieves significantly better precision than the existing models.,1,TREC,True
7,Categories and Subject Descriptors,0,,False
8,H.3.3 [Information Systems]: Information Search and Retrieval : Retrieval Models,0,,False
9,General Terms,0,,False
10,"Algorithm, Experimentation, Performance",0,,False
11,Keywords,0,,False
12,"Document ranking, Retrieval model, Term weighting",0,,False
13,1. INTRODUCTION,1,DUC,True
14,"Term weighting schemes are the central part of an information retrieval system. Effectiveness of IR systems are thus crucially dependent on the underlying term weighting mechanism. Almost all retrieval models integrate three major variables to determine the degree of importance of a term for a document: (i) within document term frequency, (ii) document length and (iii) the specificity of the term in the",0,,False
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
16,"collection. Term frequency and document length combination is used to infer the saliency of a term in a document, and when a query contains more than one term, term specificity is used to reward the documents that contain the terms rare in the collection.",0,,False
17,"Retrieval models can be broadly classified into two major families based on their term weight estimation principle. Vector space model casts queries and documents as finite dimensional vectors, where the weight of an individual component is computed using numerous variations of tf-idf scores. On the other hand, probabilistic models [16, 17] primarily focus on estimating the probabilities of the terms in the documents, and the estimation techniques differ from one approach to the other. But in essence all of them use the same basic principles that we have outlined before.",1,ad,True
18,"Most of the existing models (possibly all) employ a single term frequency normalization mechanism that does not take into account various aspects of a term's saliency in a document. For example, frequency of a term in a document relative to the frequency of the other terms in the same document gives us an important clue that can not be achieved by the commonly used document length based normalization scheme. On the contrary, length based normalization can restrict the likelihood of retrieval of extremely long documents which can not be taken care of by the relative frequency based term weighting.",0,,False
19,"Another major limitation of the present models is that they do not balance well in preferring short and long documents. Such limitation makes a system to retrieve low quality documents at the top of the ranked list when they face queries of varying length. For example, in pivoted document length normalization scheme, if the parameter is set to a smaller value, it performs better for shorter queries, and when the parameter value is larger, longer queries are benefited more than the shorter queries [10]. Similar observation can be made for other models such as BM25, language model or relatively recent divergence from randomness based models [13, 10].",1,ad,True
20,"The main reason is that when the parameter is set to a static value, most of the models prefer either short documents or long documents. If a weighting scheme prefers long documents, it pulls up extremely long documents when longer queries are encountered, since the longer documents have higher verbosity level it matches more query terms[28]. On the other direction, preference of short documents may degrade the overall retrieval performance, since it violates the likelihood of relevance versus retrieval pattern suggested by Singhal et al. [28].",1,ad,True
21,343,0,,False
22,"This article proposes a term weighting scheme that can address these weaknesses in an effective way. In particular, we argue that the two aspects of term frequencies, when combined appropriately, leads to significant performance benefit. In this article we make the following contributions.",1,ad,True
23,"· It introduces a two aspect term frequency normalization scheme, that combines relative tf weighting and the tf normalization based on document length. One component of the term frequency tends to prefer long documents, while the other component prefers short documents and therefore, it maintains a good balance in preferring short and long documents.",0,,False
24,"· It uses the query length information to emphasize the appropriate component. In particular, when the system faces a long query, it down-weights the part that prefers long documents in order to compensate the effect and vice versa.",0,,False
25,· It modifies the usual term discrimination measure (idf) by integrating mean term frequency of a term in the set of documents the term is contained in.,0,,False
26,"· Finally, we use asymptotically bounded function (similar to Robertson and Walker [22]) to transform the tf factors that better handles the term coverage issues in the documents and also helps to combine the two tf factors more easily. As a bi-product of such transformation, the ranking function easily produces the similarity values in the range of [0-1].",0,,False
27,"In order to assess the effectiveness of the proposed model we carry out a set of experiments on a large number of standard test collections containing news and web data. The experimental results show that the proposed weighting function consistently and significantly outperforms five state of the art retrieval models (from vector space as well as probabilities families) measured in terms of the standard metrics such as MAP and NDCG. The experimental outcomes also attest that the model achieves significantly better precision than all the other models when measured in terms of a recently popularized metric, namely, expected reciprocal rank (ERR) [6].",1,MAP,True
28,"The remainder of the article is organized as follows. In Section 2 we review the state of the art. Section 3 describes the proposed weighing scheme. Description about the test collections, evaluation metrics and the details of the baselines are given in Section 4. Experimental results are presented in Section 5, where we compare the performance of the proposed model with the state of the art vector space models, followed by the comparison with the probabilistic models. Finally, we conclude in Section 6.",0,,False
29,2. STATE OF THE ART,0,,False
30,"Information retrieval systems, when encounter a query, tries to rank documents by their likelihood of relevance. Most IR systems assign a numeric score to the documents and then they are ranked based on these scores. Three widely used models in IR are the vector space model [26, 25], the probabilistic models [21] and the inference network based model [30]. In this section we review some of the state of the art models.",0,,False
31,"In vector space model, queries and documents are represented as the vector of terms. To compute a score between a",0,,False
32,document and a query the model measures the similarity be-,0,,False
33,tween the query and document vector using cosine function.,0,,False
34,The central part of the vector space model is to determine,0,,False
35,the weight of the terms that are present in the query and,0,,False
36,the documents. Three main factors that come into play to,0,,False
37,compute the weight of a term are: (i) frequency of the term,0,,False
38,"in the document, (ii) document frequency of the term in the",0,,False
39,collection (first proposed in [29]) and (iii) the length of the,0,,False
40,document that contains the term. Fang et al. [10] give a,0,,False
41,comprehensive analysis of four retrieval models by defining,0,,False
42,a set of constraints that needs to be satisfied for effective,0,,False
43,retrieval. Using these constraints the strengths and weak-,0,,False
44,nesses of some well known models are analyzed and some,0,,False
45,of the models are modified. There are also a number of re-,0,,False
46,cent works that focus on the constraint based analysis of the,0,,False
47,"retrieval models [8, 9].",0,,False
48,Salton and Buckley [24] summarize a number of term,0,,False
49,weighting approaches which use various types of normalization. It is evident that document length is an important,0,,False
50,component in effective term weighting. Singhal et al. [28],0,,False
51,identify a number of weaknesses of cosine and maximum tf,0,,False
52,normalization and they observe that a weighting formula,0,,False
53,"that retrieves documents with chances similar to their probability of relevance performs better. Following this observation, they propose a pivoted normalization scheme that",0,,False
54,acts as a correction factor of old normalization and is one,0,,False
55,of the most effective term weighting schemes in the vector,0,,False
56,space framework. The pivoted length normalization scheme,0,,False
57,computes the term weight as follows [27]:,0,,False
58,X,0,,False
59,tQD,0,,False
60,1,0,,False
61,+,0,,False
62,"ln(1 + ln(T F (t, D)))",0,,False
63,1,0,,False
64,-,0,,False
65,s,0,,False
66,+,0,,False
67,s,0,,False
68,len(D) ADL(C),0,,False
69,·,0,,False
70,"T F (t, Q)",0,,False
71,·,0,,False
72,ln N + 1 df (t),0,,False
73,(1),0,,False
74,"The parameter s controls the extent of normalization with respect to the document. Typically, the term weighting functions in vector space model are designed heuristically, which are based on the researchers experience. Several work tried to use the data to learn the patterns that satisfy the data. For example, Greiff [11] uses exploratory data analysis to uncover some important relationship between the document frequency and the relevance of a document.",0,,False
75,The key part of the probabilistic models is to estimate the probability of relevance of the documents for a query. This is where most probabilistic model differs from one another. Binary independence model is perhaps the most widely accepted technique in the classical probabilistic model. A number of weighting formula have been developed and BM25 [20] has been the most effective among the formulae. The major differences between BM25 and the other commonly used TFIDF models are the slightly variant IDF formulation and the use of the query term frequency. The length normalization factor uses the average document length and a parameter has been introduced to control the relative length effect.,0,,False
76,"Probabilistic language modeling technique [19, 14] is another effective ranking model that is widely used today. Typically, language modeling approaches compute the probability of generating a query from a document, assuming that the query terms are chosen independently. Unlike TFIDF models, language modeling approaches do not explicitly use document length factor and the idf component. It seems that the length of the document is an integral part of this formula and that automatically takes care of the length normalization issue. However, smoothing is crucial and it has very similar effect as the parameter that controls the",0,,False
77,344,0,,False
78,"length normalization components in pivoted normalization or BM25 model. Three major smoothing techniques (Dirichlet, Jelinek-Mercer and Two-stage) are commonly used in this model [31].",0,,False
79,"Relatively recent, another probabilistic model is proposed in [3] that computes the weight of a term by measuring the informative content of a term by computing the amount of divergence of the term frequency distribution from the distribution based on a random process. Like most of the well known models, they also use the same basic components. However, the integration of various component are derived theoretically. This family of formula also uses the average document length as an ideal length of the documents and the term frequencies are normalized with respect to the average document length.",0,,False
80,"In inference network, document retrieval is modeled as an inference process [30]. A document instantiates a term with a certain strength and given a query the credit from multiple terms is accumulated to compute a relevance that is very equivalent to the similarity score of vector space model. From an operational angle, the strength of instantiation of a term for a document can be considered as weight of the term in a document. The strength of instantiation of a term can be computed using any reasonable formula.",0,,False
81,3. PROPOSED WORK,0,,False
82,3.1 Preliminaries,0,,False
83,"Given a query Q and a document D, the main task of a ranking function is to assign a score to D with respect to the query Q. The main objective of a term weighting scheme is to quantify the saliency of the query terms in the document. This section describes a novel TF-IDF term weighting scheme that serves above purpose. En-route to the development, we are guided by a number of hypotheses that are commonplace in quantifying the importance of a term. Thus, before we give the main motivation behind our work, let us first revisit the three key hypotheses.",0,,False
84,"1. Term Frequency Hypothesis (TFH): The weight of term in a document should increase with the increase in term frequency (T F ). However, it seems unlikely that the importance of a term grows linearly with the increase in T F . Therefore, researchers have used dampened TF instead of the raw TF for ranking. The most widely used damping function has been log(T F ) and the basis of this damping can be best captured by the following advanced hypothesis.",1,ad,True
85,"Advanced TF Hypothesis (AD-TFH): The modified term frequency hypothesis captures the intuition that the rate of change of weight of a term should decrease with the larger TF. For example, the change in the weight caused by increasing TF from 2 to 3 should be higher than that caused by increasing TF from 25 to 26 [10]. Thus, the raw T F has to be transformed to fulfill the above goal. Formally, we hypothesize that, a function Ft(T F ), that maps the original T F to the resultant value (which will be used for final weighting), should possess the following two properties.",0,,False
86,(a) Ft (T F ) > 0,0,,False
87,(b) Ft (T F ) < 0,0,,False
88,"2. Document Length Hypothesis (DLH): This hypothesis captures the relationship between the term frequency and the document length. Long documents tend to use a term repeatedly, thus term frequency can be higher in a long document. Therefore, if T F is considered in isolation (disregarding the document length), long documents are given undue preference. Thus, it is necessary to regulate the T F value in accordance with the document length. The general principle is that if two documents have different lengths and the same T F values for a term t, then the contribution of T F (of t) should be higher for the shorter document.",0,,False
89,"3. Term Discrimination Hypothesis (TDH): If a query contains more than one term, then a good weighting scheme should prefer a document that contains the rare term (in the collection).",1,TD,True
90,3.2 Two Aspects of TF,0,,False
91,"Most existing weighting schemes employ the above heuristics to quantify the term importance. However, they generally normalize the term frequency that captures a single aspect of the saliency of the terms and hence disregards another important aspect that we detail next. In particular, we consider the following two aspects:",0,,False
92,"1. Relative Intra-document TF (RITF) : In this factor, the importance of a term is measured by considering its frequency relative to the average T F of the document. Thus, a natural formulation for this could be",0,,False
93,"RIT F (t, D) ,"" T F (t, D)""",0,,False
94,(2),0,,False
95,Avg.T F (D),0,,False
96,"where T F (t, D) and Avg.T F (t, D) denote the frequency of the term t in D and average term frequency of D respectively. However, Equation 2 may too much prefer excessively long documents, since the denominator is close to 1 for a very long document [28]. Hence, a sublinear damping of T F seems to be a better choice over the raw T F and thus we use the following function.",0,,False
97,"RIT F (t, D) ,"" log2(1 + T F (t, D))""",0,,False
98,(3),0,,False
99,log2(1 + Avg.T F (D)),0,,False
100,"Indeed, such a formula has been used by Singhal et al. [28] in the pivoted length normalization framework to normalize the tf values in accordance with the number of unique terms in the document.",0,,False
101,2. Length Regularized TF (LRTF) : This factor nor-,0,,False
102,malizes the term frequency by considering the number,0,,False
103,of terms present in a document. Similar to Robert-,0,,False
104,"son's [22] notion, we assume that the appropriate length",0,,False
105,of a document should be the average document length,0,,False
106,of the collection and the frequency of the terms of an,0,,False
107,average length document should remain unchanged.,0,,False
108,"Thus, a reasonable starting point could be T F (t, D) ×",0,,False
109,ADL(C) len(D),0,,False
110,",",0,,False
111,where,0,,False
112,ADL(C),0,,False
113,is,0,,False
114,the,0,,False
115,average,0,,False
116,document,0,,False
117,length,0,,False
118,of the collection and len(D) is the length of the doc-,0,,False
119,"ument D. But once again, it seems unlikely that the",0,,False
120,increase in term frequency follows a linear relationship,0,,False
121,"with the document length, and thus the above formula",0,,False
122,over-penalizes the long documents. To overcome this,0,,False
123,345,0,,False
124,"bias, we employ the following function (used in [3]) to",0,,False
125,achieve the length dependent normalization.,0,,False
126,",,",0,,False
127,«,0,,False
128,"LRT F (t, D) ,"" T F (t, D) × log2""",0,,False
129,1 + ADL(C) len(D),0,,False
130,(4),0,,False
131,Equation 4 still punishes the long documents but with diminishing effect.,0,,False
132,"However, we believe that any document length normalization can be used to achieve this purpose. Some of the possible alternatives might be the length normalization component of BM25 or that of the pivoted normalization scheme.",0,,False
133,3.2.1 Motivation,0,,False
134,"In order to motivate the use of two T F factors, let us consider the following two somewhat toy examples. We use these examples just to introduce the basic idea.",0,,False
135,"Example 1 Let D1 and D2 be two documents of equal lengths, with the following statistics.",0,,False
136,"1. len(D1) ,"" 20, # distinct term of D1 "","" 5, T F (t, D1)"",4",0,,False
137,"2. len(D2) ,"" 20, # distinct term of D2 "","" 15, T F (t, D2)"",4",0,,False
138,"In both of the cases, LRT F considers t equally important. A little thought will convince us that this is not appropriate, since the focus of the document D1 seems to be divided equally among 5 terms and therefore t should not be considered salient, while t seems to be important for D2. Thus, in the later case RIT F seems to be a better choice to LRT F .",0,,False
139,Let us now turn to the other direction and consider the second example.,0,,False
140,Example 2 Let D1 and D2 be two documents with the following statistics.,0,,False
141,"1. len(D1) ,"" 20, # distinct term of D1 "","" 15, T F (t, D1)"",4",0,,False
142,"2. len(D2) ,"" 200, # distinct term of D2 "","" 150, T F (t, D2)"",4",0,,False
143,"For this instance however, RIT F considers the term t equally important for both D1 and D2, which is not right, since D2 contains more distinct terms and thus seems to cover many other topics (also possibly uses t repeatedly). Therefore, in this case, the use of LRT F seems to be a potential choice over RIT F .",0,,False
144,"Motivated by the above examples, our main goal now is to integrate the above two factors into our weighting scheme. However, we do not use the T F factors as defined in the Equations 3 and 4. We transform these T F values for our final use that in some sense makes use of the hypothesis ADTFH. The next section details the transformation procedure and the underlying motivation.",0,,False
145,3.2.2 Transforming TF Factors,0,,False
146,"Recall that the main motivation behind the advanced hypothesis on term frequency (AD-TFH) is that a good weighting function, while emphasizing on term frequencies and term discrimination factors, should also pay attention to the term coverage issue (i. e number of match). For example, if a document D1 contains a query term 10 times and another document D2 contains two query terms (of the same query)",1,ad,True
147,"5 times each, then the assigned score should be higher for D2",0,,False
148,(assuming that both the query terms have equal term dis-,0,,False
149,crimination values). That is probably the most important,0,,False
150,"reason why raw T F does not work well in practice. Second,",0,,False
151,another common trait of many weighting schemes (for exam-,0,,False
152,"ple, pivoted normalization) is that they use the T F functions",0,,False
153,that are not bounded above. We transform the T F factors,0,,False
154,using a function f (x) that possesses the following proper-,0,,False
155,"ties: (i) vanishes at 0, (ii) satisfies the two conditions of",0,,False
156,"AD-TFH hypothesis (f (x) > 0 and f (x) < 0), and (iii)",0,,False
157,asymptotically upper bounded to 1.,0,,False
158,One of the simplest functions that satisfies the above prop-,0,,False
159,"erties is f (x) ,",0,,False
160,x 1+x,0,,False
161,.,0,,False
162,"Indeed, similar functions have been",0,,False
163,"employed before in [22] and in [3]. Using this function, we",0,,False
164,now transform the two T F factors as follows:,0,,False
165,"RIT F (t, D)",0,,False
166,"BRIT F (t, D) ,",0,,False
167,(5),0,,False
168,"1 + RIT F (t, D)",0,,False
169,"BLRT F (t, D) ,"" LRT F (t, D)""",0,,False
170,(6),0,,False
171,"1 + LRT F (t, D)",0,,False
172,3.2.3 Combining Two TF Factors,0,,False
173,"Now the key question that we face: how should we combine BRIT F (t, D) and BLRT F (t, D)? A natural way to do this is as follows:",0,,False
174,"T F F (t, D) ,"" w × BRIT F (t, D) + (1 - w) × BLRT F (t, D) (7)""",0,,False
175,where 0 < w < 1. The next important issues that arise out of Equation 7 are the following:,0,,False
176,"· Should we prefer BRIT F (t, D) (w > 0.5)?",0,,False
177,"· Should we prefer BLRT F (t, D) (w < 0.5)?",0,,False
178,"In order to answer these questions, we now analyze the properties of the two TF components. From Equation 5, it is clear that BRIT F (t, D) has a tendency to prefer long documents, since for long documents the denominator part of RIT F (t, D) is close to 1, and T F is usually larger. On the other hand, BLRT F (t, D) tends to prefer short documents, since LRT F (t, D)  0 as len(D)  . Therefore, when a query is long, BRIT F (t, D) heavily prefers extremely long documents, since the number of matches is more or less proportional to the length of the document [28]. On the contrary, since BLRT F (t, D) prefers short documents it can penalize extremely long documents when it faces longer queries, and thus it is preferable when longer queries are encountered. Another interesting property of BRIT F (t, D) is that it emphasizes on the number of matches, since the main component of this formula RIT F (t, D) heavily punishes the term frequency, and thus important for the short queries. Hence, the foregoing discussion suggests that, for short queries BRIT F (t, D) should be preferred, while for longer queries, BLRT F (t, D) should be given more weight",0,,False
179,"Based on the discussion given in the previous section, we now turn to incorporate the query length information into our weighting formula. The value of w should decrease with the increase in query length, while it must lie between [0-1]. Specifically, we characterize the query length factor (QLF (Q)) by the following variables. (i) QLF (Q) , 1 for |Q| ,"" 1, (ii) QLF (Q) < 0 and (iii) 0 < QLF (Q) < 1. Numerous different functions can be constructed that satisfy the above conditions. We used the following three different""",1,corpora,True
180,346,0,,False
181,functions.,0,,False
182,1,0,,False
183,"QLF1(Q) , log2(1 + |Q|)",0,,False
184,(8),0,,False
185,2,0,,False
186,"QLF2(Q) , 1 + log2(1 + |Q|)",0,,False
187,(9),0,,False
188,3,0,,False
189,"QLF3(Q) , 2 + log2(1 + |Q|)",0,,False
190,(10),0,,False
191,"The first function descends more rapidly than the second function, while the second function descends more rapidly than the third function. Our experiments suggest that function 9 performs consistently better than the other two functions on all the collections. Hence, we set w , QLF2(Q). We leave this issue for further investigation.",0,,False
192,3.3 Term Discrimination Factor,0,,False
193,The goal of the term discrimination factor in weighting,0,,False
194,is to assign higher score to the documents that contain the,0,,False
195,terms which are rare in the collection. Inverse document,0,,False
196,frequency (IDF ) is a well known measure that serves the,0,,False
197,above purpose. A number of IDF formulation are prevalent,0,,False
198,"in the IR literature, all of which essentially quantify the",0,,False
199,above intuition. We use the following standard idf measure.,0,,False
200,",,",0,,False
201,«,0,,False
202,"IDF (t, C) , log",0,,False
203,"CS(C) + 1 DF (t, C)",0,,False
204,(11),0,,False
205,The above IDF measure considers only the presence or,0,,False
206,absence of a term in a document and does not take into ac-,0,,False
207,count the document specific term occurrence. We hypoth-,0,,False
208,esize that the term discrimination is a combination of the,0,,False
209,"above two factors. In particular, we hypothesize that if two",0,,False
210,"terms have equal document frequencies, then the term dis-",0,,False
211,crimination should increase with the increase in average elite,0,,False
212,set term frequency. The average elite set term frequency,0,,False
213,(AEF ),0,,False
214,is,0,,False
215,defined,0,,False
216,as,0,,False
217,"CT F (t,C) DF (t,C)",0,,False
218,",",0,,False
219,where,0,,False
220,"CT F (t, C)",0,,False
221,denotes,0,,False
222,the,0,,False
223,total occurrence of the term t in the entire collection. In,0,,False
224,"fact, Kwok [18] used AEF for term weighting, but the pur-",0,,False
225,"pose was different. However, the combination of raw AEF",0,,False
226,"with IDF may disturb the overall term discrimination value,",0,,False
227,since the IDF values are obtained by dampening through,0,,False
228,"log function. Hence, we employ a slowly increasing function",0,,False
229,to transform the AEF values for this combination. Once,0,,False
230,"again, we use the function f (x) , x/(1 + x) to transform",0,,False
231,the AEF values for the final use. The final term discrimi-,0,,False
232,nation value of term t is computed as,0,,False
233,T,0,,False
234,DF,0,,False
235,"(t,",0,,False
236,C),0,,False
237,",",0,,False
238,IDF,0,,False
239,"(t,",0,,False
240,C),0,,False
241,×,0,,False
242,1,0,,False
243,"AEF (t, C) + AEF (t, C)",0,,False
244,(12),0,,False
245,"Our experiments reveal that the use of the above term discrimination has not very significant effect on the overall performance. However, it is observed that the improvements, although are small, consistent across the collections.",0,,False
246,3.4 Final Formula,0,,False
247,Integrating the above factors we now obtain the following final scoring formula.,0,,False
248,X |Q|,0,,False
249,"Sim(Q, D) ,"" T F F (qi, D) × T DF (qi, C)""",0,,False
250,(13),0,,False
251,"i,1",0,,False
252,"Again, since T F F (qi, D) < 1, we obtain the following rela-",0,,False
253,tionship.,0,,False
254,X |Q|,0,,False
255,"Sim(Q, D) < T DF (qi, C)",0,,False
256,(14),0,,False
257,"i,1",0,,False
258,"Therefore, we can easily modify Equation 13 to get the normalized similarity scores (0 < Sim(Q, D) < 1) as follows:",0,,False
259,"|PQ| T F F (qi, D) × T DF (qi, C)",0,,False
260,"Simnorm(Q, D) , i,1",0,,False
261,"|PQ| T DF (qi, C)",0,,False
262,"i,1",0,,False
263,(15),0,,False
264,"Equations 13 and 15 are equivalent in the sense that they produce the same ranked lists. However, an application that requires normalized scores, Equation 15 can be used as a suitable alternative.",0,,False
265,4. EXPERIMENTAL SETUP,0,,False
266,"In this section we describe the details of our experimental setup. First, in Section 1 we give the details of the test collections used in our experiments. In Section 4.2 and Section 4.3 we describe the evaluation measures and the baseline retrieval models respectively.",0,,False
267,4.1 Data,0,,False
268,"Table 1 summarizes the statistics on test collections used in our experiments. The experiments are conducted on a large number of standard test collections, that vary both by type, the size of the document collections and the number of queries.",0,,False
269,"TREC 6,7,8 and ROBUST are news collections containing 528,155 documents and supplemented by 150 (queries 301-450) and 100 (601-700) queries respectively. WT10G is a web collection of moderate size supplemented by 100 queries (451-550), while GOV2 is another web collection of larger size, which is crawled from .gov domain. There are 150 (queries 701-850) queries attached with GOV2 collection which were used in TREC terabyte [4] track for three years.",1,TREC,True
270,Table 1: Test Collection Statistics,0,,False
271,Name,0,,False
272,# of docs # of queries,0,,False
273,"TREC 6,7,8 528,155",1,TREC,True
274,150,0,,False
275,ROBUST,0,,False
276,"528,155",0,,False
277,100,0,,False
278,WT10G,1,WT,True
279,"1,692,096",0,,False
280,100,0,,False
281,GOV2,0,,False
282,"25,205,179",0,,False
283,150,0,,False
284,MQ-07,1,MQ,True
285,"25,205,179",0,,False
286,1778,0,,False
287,MQ-08,1,MQ,True
288,"25,205,179",0,,False
289,784,0,,False
290,"The MQ-07 and MQ-08 set of queries are based on the Million Query Track 2007 [2] and 2008 [1] respectively. This track was designed to serve two purposes. First, it was an exploration of ad-hoc retrieval on a large collection of documents. Second, it investigated questions of system evaluation, particularly whether it is better to evaluate using many shallow judgments or fewer thorough judgments. Both million query track use GOV2 as document collection. Topics for this task were drawn from a large collection of queries that were collected by a large Internet search engine. The queries also vary by their length, with short (2-3 words) to long (6-10 words). Specifically, MQ-07 and MQ-08 collections contain 505 and 433 queries of length higher than 5",1,MQ,True
291,347,0,,False
292,"respectively. Therefore, the test collections provide us a diverse experimental setup for assessing the effectiveness of the proposed weighting method.",0,,False
293,"Except TREC-6,7,8, all the test collections have three scale graded relevance assessment. The grades are 0, 1 and 2- meaning non-relevant, relevant and highly relevant respectively. TREC-6,7,8 collection uses binary relevance assessment.",1,TREC,True
294,4.2 Evaluation Measures and IR System,0,,False
295,"All our experiments are carried out using TERRIER1 retrieval system (version 3.5). Terrier is a flexible Information retrieval system which provides the implementation of many well known models. We use title field of the topics (note that two million query data contain more than 1000 queries that contain more than 5 terms). From all the collections we removed stopwords during indexing. Documents and queries are stemmed using Porter stemmer. Statistical significance tests are done using two sided paired t-test at 95% confidence level (i,e p < 0.05).",0,,False
296,We use the following metrics to evaluate the systems.,0,,False
297,· Mean Average Precision (MAP): This is a standard metric for binary relevance assessment.,1,MAP,True
298,"· Normalized DCG at k (NDCG@k) [15]: Discounted cumulative gain (DCG) is an evaluation measure that can leverage the relevance judgment in terms of multiple grades, and has an explicit position-wise discount factor. NDCG is the normalized version of DCG.",1,ad,True
299,"· Expected Reciprocal Rank at k (ERR@k) [6]: To relax the additive nature and the underlying independent assumption in NDCG, another evaluation measure, namely, Expected Reciprocal Rank (ERR) is proposed in [6]. It discounts the documents which are shown below very relevant documents, and is defined as the expected reciprocal length of time that a user will take to find a relevant document. ERR@k is computed as follows:",1,ad,True
300,ERR@k,0,,False
301,",",0,,False
302,Xk,0,,False
303,R(gi) i,0,,False
304,iY -1 (1,0,,False
305,-,0,,False
306,R(gi)),0,,False
307,"i,1",0,,False
308,"j,1",0,,False
309,(16),0,,False
310,where,0,,False
311,R(g),0,,False
312,",",0,,False
313,", 2g -1",0,,False
314,2hg,0,,False
315,hg,0,,False
316,"is the highest grade and g1, g2 . . . gk",1,ad,True
317,are the relevance grades associated with the top k doc-,1,ad,True
318,"uments. The value of mg is 2 for all the collections,",0,,False
319,"except TREC 6,7&8 (for this mg , 1).",1,TREC,True
320,"The first two metrics are used to reflect the overall performance of the systems, while the last evaluation measure reflects better the precision of search results, thereby making more important for the precision oriented systems. ERR has been chosen as one of the official metrics for recent TREC web tracks [7].",1,TREC,True
321,"Note that, two million query collections (MQ-07 and MQ08) have incomplete relevance assessment. Therefore, for the sake of more reliable conclusions, we evaluate the million query sets in two different ways. First, we skip the unjudged documents from the ranked lists to compute the values of well known metrics following the recommendation made in [23]. Additionally, we also present the statistical",1,MQ,True
322,1http://terrier.org/,0,,False
323,average precision2 [5] which was one of the official metrics for the million query tracks [2].,0,,False
324,4.3 Baselines,0,,False
325,"We have compared the performance of the proposed weighting scheme with five state of the art retrieval models. Since the proposed weighting function is a TF-IDF based formula, we have taken two well known state of the art TF-IDF models. We have also chosen BM25, language model with Dirichlet smoothing (LM), and relatively recent divergence from randomness based formula (PL2) as the other state of the art baselines. The choice of our baselines are primarily motivated by [10], which provides a thorough and detailed description of all the state of the art models along with the parameter sensitivity issues.",1,LM,True
326,"The performances of all the baseline models are dependent on the parameters they contain. Therefore, for the sake of more reliable comparisons with the baselines, we carry out two experiments by taking first 50 judged queries from MQ07 and MQ-08 collections. We search parameters by optimizing NDCG@20. The parameters values are given in the description of the corresponding baselines. Our experiments mostly agree with the findings reported by Fang et al. [10]. In particular, we find that the performances of Pivoted TFIDF and PL2 are very sensitive with the variation of the parameters. For example, the parameter value (s ,"" 0.2) suggested for pivoted TF-IDF in the original paper [28] gives 12% poorer MAP than that we find here by training. Similarly, the default PL2 parameter (c "","" 1) is 14% poorer than the one we find. Therefore, for fair comparisons, we use these optimal parameter values for the baselines. The details of the baselines are given below.""",1,MQ,True
327,1. Pivoted length normalized TF-IDF model: This model is one of the best performing TF-IDF formula in the vector space model framework. The value of the parameter s is set to 0.05.,0,,False
328,"2. Lemur TF-IDF model: This model is another TF-IDF model that uses Robertson's tf and the standard idf . The parameter of this model is set to its default value, 0.75.",0,,False
329,"3. Classical Probabilistic model (BM25): BM25 is chosen as a state of the art representative of the classical probabilistic model. The main differences with this model and the previous model are that BM25 uses query term frequency in a different way and the idf also differs with the standard one. The parameters of this model is set to k1 ,"" 1.2, b "", 0.6 and k3 , 1000. Note that we found (on training data) slightly better results for b , 0.6 than the default 0.75.",0,,False
330,4. Dirichlet smooth language model (LM): Language model is another probabilistic model that performs very effectively. For this model we set the value of Dirichlet smoothing (µ) to 1700.,1,LM,True
331,"5. Divergence from Randomness model (PL2): Finally, PL2 [12] represents the recently proposed non-parametric probabilistic model from divergence from randomness",0,,False
332,2the code available at TREC million query page is used to compute stat AP,1,TREC,True
333,348,0,,False
334,"(DFR) family. Similar to the previous models, its performance also depends on a parameter value (c in the formula). We conduct experiments for this model by setting c , 13.",0,,False
335,5. RESULTS,0,,False
336,"In this section we present the experimental results of our proposed work and compare them with the state of the art retrieval models. In Section 5.1 we compare the performance of the proposed model (MATF for Multi Aspect TF) with the two TF-IDF models, followed by the comparison with three probabilistic models- BM25, language model (LM) and PL2. We use three evaluation measures to evaluate the performance of all the methods.",1,LM,True
337,5.1 Comparison with TF-IDF Models,0,,False
338,"In this section we focus on to compare the performance of the proposed model (MATF) with the Lemur TF-IDF and and Pivoted TF-IDF models. Table 2 presents the experimental results for six test collections measured in terms MAP, NDCG@20 and ERR@20.",1,MAP,True
339,"First, we describe the results in terms of MAP. Table 2 clearly shows that MATF gains significantly better MAP than both of the TF-IDF models on two news collections. MATF performs 12% and 15.7% better than Lemur TF-IDF model on TREC-678 and ROBUST respectively. MATF is also significantly surpasses the Pivoted TF-IDF model on these collections with a margin of 8.8% and 6.3% respectively.",1,MAP,True
340,"The behavior of MATF is similar when we see the results for two web collections, namely, WT10G and GOV2. Once again, MATF outperforms Lemur TF-IDF model by a margin of more than 20% in both of the occasions, which is clearly highly significant as confirmed by the paired t test. Like the previous two news collections, MATF maintains its superior behavior over Pivoted TF-IDF in these web collections also. In particular MATF gains more than 8% and 19% average precision than Pivoted TF-IDF for both of the collections and paired t test once again attests the significance.",1,WT,True
341,"We now turn to describe the results on two million query data sets. These two collections are particularly interesting, since they contain real search queries collected from a commercial search engine and also because of their variations in length. Table 2 once again demonstrates that MATF unequivocally outperforms the two TF-IDF models with significantly large margin. The MAP achieved by MATF is nearly 11% and 7% better than that achieved by Lemur TF-IDF on MQ-07 and MQ-08 collections respectively. Similarly, MATF surpasses the Pivoted TF-IDF by more than 10% margin on both of the occasions. Significance tests show that the performance differences are always statistically significant.",1,MAP,True
342,"Among the two TF-IDF models, Lemur TF-IDF often seems to perform poorer than Pivoted (except MQ-08 where Lemur TF-IDF is nearly 4% better than pivoted). One potentially interesting outcome that we can see from Table 2 is that, when the document collection is larger MATF outperforms Pivoted TF-IDF with larger margin. In particular, MATF gains a MAP on GOV2 collection which is almost 20% better than the pivoted TF-IDF, which is a clear sign of effectiveness of MATF over the state of the TF-IDF models.",1,MQ,True
343,"So far our discussion of experimental outcomes primarily confined on the basis of the binary relevance assessment. Note that five out of six test collections used in our evaluation have graded assessment in three scales (0,1,2). Therefore, we now turn to describe the results measured in terms of NDCG, that leverages the graded assessment.",1,ad,True
344,"The middle segment of Table 2 presents the results in terms of NDCG@20. It is once again clear that the performances are more or less consistent with MAP. Specifically, MATF surpasses the Lemur TF-IDF models with consistently and significantly large margin on all six collections and often the differences are higher or close to 10%, which once again clearly demonstrates the effectiveness of MATF. Performance of pivoted TF-IDF is once again very similar under the graded assessment and it achieves larger NDCG than Lemur TF-IDF except in one occasion. MATF once again is significantly better than the pivoted TF-IDF on all the collections and the differences are larger for larger web collections.",1,MAP,True
345,"Our final comparison between the proposed model and the TF-IDF models focus on precision enhancing capabilities measured in terms of a metric ERR, that consider three things simultaneously: rank of the document, quality conveyed by the assessor assigned grade (non-relevant, relevant and highly relevant) and the quality of the documents that have been seen before the document of our focus.",1,ad,True
346,"The last segment of Table 2 reports the ERR@20 values achieved by the competing models on six collections. We can easily infer that MATF once again unanimously beats the two TF-IDF models. Only on WT10G, pivoted TF-IDF performs slightly better than MATF. Consistent with the previous measures, ERR@20 results demonstrate that on larger web collections the performance differences between MATF and the two TF-IDF models are larger.",1,WT,True
347,Table 3: Comparison with TF-IDF models (statAP). Lemur means Lemur TF-IDF. Superscripts have their usual meaning.,1,AP,True
348,"Lemur pivot MATF (% improv) MQ-07 29.0 29.7 34.4lp (18.2, 15.8) MQ-08 28.4 27.6 32.5lp (14.9, 17.8)",1,MQ,True
349,"The performances of MATF and the two TF-IDF models on two million query data, measured by statistical average precision, are shown in Table 3. MATF transcends Lemur TF-IDF by a margin of 18% and 15% on MQ-07 and MQ08 respectively, while it is better than pivoted TF-IDF with more than 15% on both of the collections.",1,MQ,True
350,"In summary, based on the results shown in Table 2 and Table 3 we can infer that MATF outperforms two state of the art TF-IDF models with remarkable significance and consistency, and the performance differences are often noticeably large. The performance measured by three evaluation metrics unequivocally demonstrate that MATF is highly effective in ranked retrieval. Moreover, the results also show that MATF is more effective for larger web collections.",0,,False
351,5.2 Comparison with Probabilistic Models,0,,False
352,In the last section we compare the performance of our model with two TF-IDF models. In this section we compare the performance of MATF with three well known state of the,0,,False
353,349,0,,False
354,"Table 2: Comparison with the TF-IDF models measured in terms of MAP, NDCG@20 and ERR@20. MATF denotes the proposed model. The best results are boldfaced. Superscripts l and p denote that the performance difference is statistically significant (p < 0.05) compared to Lemur TF-IDF and Pivot TF-IDF respectively.",1,MAP,True
355,Metric,0,,False
356,Method,0,,False
357,Lemur.TF-IDF,0,,False
358,MAP,1,MAP,True
359,Pivot.TF-IDF,0,,False
360,MATF,0,,False
361,% better than Lemur.TF-IDF,0,,False
362,% better than Pivot.TF-IDF,0,,False
363,TREC-678 20.9 21.5 23.4lp 12.0 8.8,1,TREC,True
364,ROBUST 26.1 28.4 30.2lp 15.7 6.3,0,,False
365,WT10G 18.4 20.5 22.2lp 20.7 8.3,1,WT,True
366,GOV2 24.8 26.5 31.7lp 27.8 19.6,0,,False
367,MQ-07 39.6 40.0 44.2lp 11.6 10.5,1,MQ,True
368,MQ-08 42.8 41.2 45.7lp 6.8 10.9,1,MQ,True
369,Lemur.TF-IDF NDCG@20 Pivot.TF-IDF,0,,False
370,MATF % better than Lemur.TF-IDF % better than Pivot.TF-IDF,0,,False
371,40.0 41.5 44.6lp,0,,False
372,11.5 7.5,0,,False
373,37.5 40.2 41.5lp,0,,False
374,10.7 3.2,0,,False
375,31.6 33.4 34.6l,0,,False
376,9.5 3.6,0,,False
377,43.8 46.8 51.0lp,0,,False
378,16.4 9.0,0,,False
379,46.8 48.3 51.1lp,0,,False
380,9.2 5.8,0,,False
381,50.1 48.7 52.6lp,0,,False
382,5.0 8.0,0,,False
383,ERR@20,0,,False
384,Lemur.TF-IDF Pivot.TF-IDF MATF,0,,False
385,40.7,0,,False
386,41.9 43.9lp,0,,False
387,45.7,0,,False
388,46.3 48.5lp,0,,False
389,34.7,0,,False
390,37.9 37.1l,0,,False
391,48.3,0,,False
392,49.4 53.4lp,0,,False
393,40.6,0,,False
394,42.9 44.9lp,0,,False
395,44.5,0,,False
396,44.6 47.3lp,0,,False
397,"art probabilistic retrieval models. Our evaluation strategy is once again similar to the previous section. We compare the performances of the models under MAP, NDCG@20 and ERR@20.",1,MAP,True
398,"First we compare the performance of MATF with the BM25 model. Table 4 shows the summary of the retrieval results on six test collections. It is clear from the table that MATF is superior to BM25 model. This result holds for all the collections and for all three evaluation measure. When the performance differences between them are measured in terms of MAP, we notice that MATF is significantly effective for news as well as web corpora compared to BM25. In fact, MATF is nearly 10% better than BM25 on two news data, while on two web collections (WT10G and GOV2), MATF achieves 17% and 12% more MAP than BM25. The differences on MQ-07 and MQ-08 are similarly significant with substantial margins. The performance differences between MATF and BM25 revealed by NDCG metric are consistent with that revealed by MAP and once again, all the differences are statistically significant. ERR@20 depicts that for all the collections, MATF remains consistently superior to BM25, which clearly confirms that MATF is very effective for precision oriented systems.",1,MAP,True
399,"We now compare the effectiveness of MATF and language model with Dirichlet prior language model. From Table 4 we clearly see that the performance differences between MATF and LM are larger in three out of six cases than that we had observed when comparing the performance of MATF and BM25. Specifically, MATF achieves close to or more than 10% MAP than LM on four out out six instances (except WT10G). The performance measured on graded relevance assessment also demonstrates that MATF unequivocally beats the Dirichlet prior language model based approach, and the differences are substantially large. On GOV2, MQ-07 and MQ-08 data, MATF surpasses LM with a margin of 14%, 8% and nearly 9% respectively. The comparison of precision enhancing abilities of MATF and LM also clearly indicates that MATF is always better than LM, which is very concordant with the experimental findings captured by MAP and NDCG.",1,LM,True
400,"We now compare the performance of the proposed model with another probabilistic model from the divergence from randomness family, namely, PL2. This model is relatively recent compared to the previous two probabilistic models and was also found to be better than BM25 in the experiments reported in [3]. Table 4 reflects two major facts. First, it appears from the table that PL2 is most effective among the probabilistic models and in particular only on MQ-08 data it performs worse than BM25 as reflected by both MAP and NDCG. The second major observation that can be made from Table 4 is that MATF beats this model also with harmonious consistency and performance differences are statistically significant on TREC-678, GOV2, MQ07 and MQ-08 data. Similar to the previous outcomes, on web collections the performance differences between MATF and PL2 are larger than that for the news collections. Lastly, ERR metric depicts that MATF is better than PL2 across all six collection.",1,MQ,True
401,Table 5: Comparison with probabilistic models (statAP).,1,AP,True
402,"BM25 LM PL2 MATF (% improvement) MQ-07 30.6 29.7 30.4 34.4blp (12.8, 15.8, 13.2) MQ-08 29.6 27.6 27.4 32.5blp (10.5, 17.8, 18.6)",1,LM,True
403,Table 5 compares the performance of four models for million query collections measured in terms of statistical average precision. It is once again clearly evident that MATF is consistently better than all three models and all the differences are very large and it is very consistent with the performance measured in terms other metrics presented in Table 4.,0,,False
404,"Overall, the comparative analysis clearly shows that MATF is the most effective retrieval model, which unequivocally outperforms all three probabilistic models, when the performances are measured in terms of MAP, NDCG and a precision biased metric, namely, ERR. Also, the relative per-",1,MAP,True
405,350,0,,False
406,"Table 4: Comparison with probabilistic models measured in terms of MAP, NDCG@20 and ERR@20. MATF denotes the proposed model. The best results are boldfaced. Superscripts b, l and p denote that the performance differences are statistically significant compared to BM25, LM and PL2 respectively.",1,MAP,True
407,Metric,0,,False
408,Method,0,,False
409,BM25,0,,False
410,MAP,1,MAP,True
411,LM,1,LM,True
412,PL2,0,,False
413,MATF,0,,False
414,% better than BM25,0,,False
415,% better than LM,1,LM,True
416,% better than PL2,0,,False
417,TREC-678 21.3 21.3 22.7 23.4blp 9.9 9.9 3.1,1,TREC,True
418,ROBUST 27.7 28.4 29.5 30.2bl 9.0 6.9 2.4,0,,False
419,WT10G 18.9 21.3 21.3 22.2b 17.5 4.2 4.2,1,WT,True
420,GOV2 28.3 29.1 29.7 31.7blp 12.1 8.9 6.7,0,,False
421,MQ-07 41.2 40.1 40.9 44.2blp 7.3 10.2 8.1,1,MQ,True
422,MQ-08 43.6 41.0 41.5 45.7blp 4.8 11.5 10.1,1,MQ,True
423,BM25 NDCG@20 LM,1,LM,True
424,PL2 MATF % better than BM25 % better than LM % better than PL2,1,LM,True
425,41.2 40.2 42.9 44.6blp,0,,False
426,8.3 10.9 4.0,0,,False
427,39.3 39.3 41.1 41.5bl,0,,False
428,5.6 5.6 1.0,0,,False
429,32.4 32.5 33.1 34.6bl,0,,False
430,6.8 6.5 4.5,0,,False
431,45.2 44.6 46.1 51.0blp,0,,False
432,12.8 14.3 10.6,0,,False
433,48.1 47.2 48.0 51.1blp,0,,False
434,6.2 8.3 6.5,0,,False
435,50.9 48.3 49.0 52.6blp,0,,False
436,3.3 8.9 7.3,0,,False
437,ERR@20,0,,False
438,BM25 LM PL2 MATF,1,LM,True
439,41.1 41.2 43.0 43.9blp,0,,False
440,45.6 46.5 47.0 48.5b,0,,False
441,34.7 35.4 35.2 37.1b,0,,False
442,48.2 47.6 47.7 53.4blp,0,,False
443,41.3 39.9 40.7 44.9blp,0,,False
444,44.8 42.7 42.7 47.3blp,0,,False
445,Table 6: Performance of two TF factors on short,0,,False
446,and long query. The values are MAP.,1,MAP,True
447,short,0,,False
448,long,0,,False
449,MQ-07 MQ-08 MQ-07 MQ-08,1,MQ,True
450,LRTF 43.5 43.3 39.9 44.3,0,,False
451,RITF 45.4 45.0 37.7 41.8,0,,False
452,"formance differences are often substantially large and the differences are even larger for the web collections that contain large number of queries. Among the three probabilistic models, PL2 and Dirichlet prior language model perform almost equally, with PL2 having a marginal edge over LM.",1,LM,True
453,5.3 Analysis,0,,False
454,"In this section we analyze the effect of the two TF factors on short and long queries. For this analysis we choose the two million query collections, primarily because the collections have large number of queries. We divide the queries in two sets. The queries having at least 5 terms are denoted as short, while the rest of the queries (longer than 5 words) are treated as long. The main goal of this section is to validate the hypothesis made in the proposed section that relative intra-document based TF (RITF) performs better on short queries, while length regularized TF (RLTF) performs better on long queries.",1,ad,True
455,"Table 6 presents the experimental results on two million query data. The results seem to confirm our aforesaid assumption. LRTF always performs better than RITF on both of the collections, while RITF does better for short queries. However, the performance differences between the methods on longer queries are noticeably better than that for shorter queries.",0,,False
456,6. CONCLUSION,0,,False
457,"In this paper, we present a novel TF-IDF term weighting scheme. The proposed term weighting scheme employs two aspects of within document term frequency normalization to determine the importance of a term. One component of the term frequency tends to prefer short documents, while the other tends to prefer long documents. We then combine these two TF components using the query length information, that maintains a balanced trade-off in retrieving short and long documents, when the ranking function faces queries of varying lengths.",1,ad,True
458,"Experiments carried out on a set of news and web collections show that the proposed model outperforms two well known state of the art TF-IDF baselines with significantly large margin, when measured in terms of MAP and NDCG. The model also surpasses three state of the art probabilistic models with remarkable significance almost always. Moreover, the proposed model is also significantly better than all of the five baselines in improving precision.",1,MAP,True
459,Acknowledgments,0,,False
460,"I would like to thank Dipasree Pal, Mandar Mitra and Swapan Parui for their comments, suggestions and help.",0,,False
461,7. REFERENCES,0,,False
462,"[1] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million query track 2008 overview. In E. M. Voorhees and L. P. Buckland, editors, The Sixteenth Text REtrieval Conference Proceedings (TREC 2008). National Institute of Standards and Technology, December 2009.",1,TREC,True
463,"[2] J. Allan, B. Carterette, B. Dachev, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million query track 2007 overview. In TREC, 2007.",1,TREC,True
464,351,0,,False
465,"[3] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, Oct. 2002.",0,,False
466,"[4] S. Bu¨ttcher, C. L. A. Clarke, and I. Soboroff. The trec 2006 terabyte track. In TREC, 2006.",1,trec,True
467,"[5] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over thousands of queries. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '08, pages 651­658, New York, NY, USA, 2008. ACM.",0,,False
468,"[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM '09, pages 621­630, New York, NY, USA, 2009. ACM.",1,ad,True
469,"[7] C. L. A. Clarke, N. Craswell, I. Soboroff, and E. M. Voorhees. Overview of the trec 2011 web track. In TREC, 2011.",1,trec,True
470,"[8] S. Clinchant and E. Gaussier. Retrieval constraints and word frequency distributions a log-logistic model for ir. Inf. Retr., 14(1):5­25, Feb. 2011.",0,,False
471,"[9] R. Cummins and C. O'Riordan. A constraint to automatically regulate document-length normalisation. In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM '12, pages 2443­2446, New York, NY, USA, 2012. ACM.",0,,False
472,"[10] H. Fang, T. Tao, and C. Zhai. Diagnostic evaluation of information retrieval models. ACM Trans. Inf. Syst., 29(2):7:1­7:42, Apr. 2011.",0,,False
473,"[11] W. R. Greiff. A theory of term weighting based on exploratory data analysis. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '98, pages 11­19, New York, NY, USA, 1998. ACM.",0,,False
474,"[12] B. He and I. Ounis. A study of the dirichlet priors for term frequency normalisation. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '05, pages 465­471, New York, NY, USA, 2005. ACM.",0,,False
475,"[13] B. He and I. Ounis. On setting the hyper-parameters of term frequency normalization for information retrieval. ACM Trans. Inf. Syst., 25(3), July 2007.",0,,False
476,"[14] D. Hiemstra, S. Robertson, and H. Zaragoza. Parsimonious language models for information retrieval. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '04, pages 178­185, New York, NY, USA, 2004. ACM.",0,,False
477,"[15] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422­446, Oct. 2002.",0,,False
478,"[16] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval: development and comparative experiments - part 1. Inf. Process. Manage., 36(6):779­808, 2000.",0,,False
479,"[17] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval:",0,,False
480,"development and comparative experiments - part 2. Inf. Process. Manage., 36(6):809­840, 2000.",0,,False
481,"[18] K. L. Kwok. A new method of weighting query terms for ad-hoc retrieval. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '96, pages 187­195, New York, NY, USA, 1996. ACM.",1,ad-hoc,True
482,"[19] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '98, pages 275­281, New York, NY, USA, 1998. ACM.",0,,False
483,"[20] S. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333­389, Apr. 2009.",0,,False
484,"[21] S. E. Robertson. Readings in information retrieval. chapter The probability ranking principle in IR, pages 281­286. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1997.",1,ad,True
485,"[22] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '94, pages 232­241, New York, NY, USA, 1994. Springer-Verlag New York, Inc.",0,,False
486,"[23] T. Sakai. Alternatives to bpref. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 71­78, New York, NY, USA, 2007. ACM.",0,,False
487,"[24] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Inf. Process. Manage., 24(5):513­523, Aug. 1988.",0,,False
488,"[25] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., New York, NY, USA, 1986.",0,,False
489,"[26] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613­620, Nov. 1975.",0,,False
490,"[27] A. Singhal. Modern information retrieval: A brief overview. IEEE Data Eng. Bull., 24(4):35­43, 2001.",0,,False
491,"[28] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '96, pages 21­29, New York, NY, USA, 1996. ACM.",0,,False
492,"[29] K. Sparck Jones. Document retrieval systems. chapter A statistical interpretation of term specificity and its application in retrieval, pages 132­142. Taylor Graham Publishing, London, UK, UK, 1988.",0,,False
493,"[30] H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Trans. Inf. Syst., 9(3):187­222, July 1991.",0,,False
494,"[31] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, Apr. 2004.",0,,False
495,352,0,,False
496,,0,,False

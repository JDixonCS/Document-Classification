,sentence,label,data,regex
0,Bias-Variance Decomposition of IR Evaluation,0,,False
1,"Peng Zhang1, Dawei Song1,2, Jun Wang3, Yuexian Hou1",0,,False
2,"1Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, China 2The Computing Department, The Open University, United Kingdom",0,,False
3,"3Department of Computer Science, University College London, United Kingdom",0,,False
4,"{darcyzzj, dawei.song2010}@gmail.com, jun_wang@acm.org, yxhou@tju.edu.cn",0,,False
5,ABSTRACT,0,,False
6,"It has been recognized that, when an information retrieval (IR) system achieves improvement in mean retrieval effectiveness (e.g. mean average precision (MAP)) over all the queries, the performance (e.g., average precision (AP)) of some individual queries could be hurt, resulting in retrieval instability. Some stability/robustness metrics have been proposed. However, they are often defined separately from the mean effectiveness metric. Consequently, there is a lack of a unified formulation of effectiveness, stability and overall retrieval quality (considering both). In this paper, we present a unified formulation based on the bias-variance decomposition. Correspondingly, a novel evaluation methodology is developed to evaluate the effectiveness and stability in an integrated manner. A case study applying the proposed methodology to evaluation of query language modeling illustrates the usefulness and analytical power of our approach.",1,MAP,True
7,Category and Subject Descriptors: H.3.3 [Information Search and Retrieval],0,,False
8,"General Terms: Theory, Measurement, Performance",0,,False
9,"Keywords: Bias-Variance, Decomposition, Effectiveness, Stability, Robustness, Evaluation",1,Robust,True
10,1. INTRODUCTION,1,DUC,True
11,"Recently, it has been noticed that when we are trying to improve the mean retrieval effectiveness (e.g., measured by MAP [3]) over all queries, the stability of performance across different individual queries could be hurt. For example, compared with a baseline (using the original query in the first round retrieval), query expansion based on pseudo relevance feedback can generally achieve better MAP, but it can hurt the performance for some individual queries [2].",1,MAP,True
12,"In the literature, various stability (or robustness) measures, e.g., R-Loss [2], Robustness Index [2], and < Init [9] have been proposed. R - Loss computes the averaged net loss of relevant documents (due to query expansion failure) in the retrieved documents. <Init calculates the percent-",1,Robust,True
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
14,"age of queries for which the retrieval performance of a query model is worse than that of the original query model. The robustness index is defined as RI(Q) ,"" (n+ - n-)/|Q| [2], where n+ is the number of queries for which the performance is improved, n- is the number of queries hurt, over the original model, and |Q| is the number of all queries.""",0,,False
15,"These existing measures have some limitations, as shown by the example in Table 1. Let us consider model A as the original query model, as well as regard B and C as two query expansion models. The example shows that A is less effective (with a lower MAP), but more stable (with a lower < Init) than B. In this case, we can not judge which model (A or B) has the better overall performance (considering both effectiveness and stability). For comparison of B and C, both <Init and robustness index can not distinguish the stability between them. The MAPs of B and C are also the same. Intuitively, B would seem more stable due to the less variance of its AP values for different queries. However, the above stability metrics do not take into account such variance (denoted as V ar in Table 1). For example, one way of computing the variance of AP for model A can be [(0.3 - 0.2)2 + (0.1 - 0.2)2]/2 ,"" 0.01, which calculates the derivation of AP from its MAP (i.e., 0.2). In Table 1, V ar distinguishes the models A, B and C: the smaller V ar can indicate the better retrieval stability.""",1,MAP,True
16,"Now let us change the AP values of C to 0.32 and 0.11 (for q1 and q2 respectively), then its MAP, < Init and Robustness Index become 0.215, 0 and 1 respectively, suggesting C is more stable but less effective than B. We are not sure which one (B or C) is overall better when considering both effectiveness and stability. This is mainly because the existing stability metrics are often defined separately from the effectiveness metric (MAP). There is a lack of a unified formulation of the effectiveness and stability to allow them to be looked at in an integrated manner.",1,AP,True
17,"In this paper, we present a formulation based on a fundamental concept in Statistics, namely the bias-variance decomposition, to tackle the problem. In a nutshell, we view the unsatisfactory overall performance as one total error, which can be decomposed into bias and variance of the AP values of different queries. The bias captures the expected difference of the APs from their upper bounds (the best AP obtained by model T in Table 1). The variance has been illustrated earlier. The detailed formulation will be given in the next section. Briefly speaking, the smaller bias or variance reflects the better retrieval effectiveness or stability, respectively. The total error (denoted as Bias2 +V ar) can reflect the overall quality of a model. As an illustration,",1,AP,True
18,1021,0,,False
19,Table 1: Examples of Bias-Variance (AP),1,AP,True
20,Model AP,1,AP,True
21,A q1 q2 0.3 0.1,0,,False
22,B q1 q2 0.6 0.08,0,,False
23,C,0,,False
24,q1,0,,False
25,q2,0,,False
26,0.65 0.03,0,,False
27,T q1 q2 0.7 0.2,0,,False
28,MAP,1,MAP,True
29,0.2,0,,False
30,0.34,0,,False
31,0.34,0,,False
32,0.45,0,,False
33,Robust Index,1,Robust,True
34,0,0,,False
35,0,0,,False
36,0,0,,False
37,1,0,,False
38,< Init,0,,False
39,0,0,,False
40,0.5,0,,False
41,0.5,0,,False
42,0,0,,False
43,Bias,0,,False
44,V ar Bias2 + V ar,0,,False
45,0.25 0.01 0.0725,0,,False
46,0.11 0.0646 0.0797,0,,False
47,0.11 0.0961 0.182,0,,False
48,0 0.0625 0.0625,0,,False
49,"in Table 1, Bias2 +V ar indicates that (1) the target model T has the best overall retrieval quality; (2) the model B is more desirable than C; (3) in comparison with the baseline A, both query expansion models B and C reduce the bias but enlarge the variance as well as the total error.",0,,False
50,"Recently, Wang and Zhu [7] studied the mean-variance analysis regarding the document relevance scores. The pertopic variance of AP (of each query) was investigated in [6]. In this paper, we explore the variance of AP (across queries) and the bias-variance decomposition.",1,AP,True
51,2. BIAS-VARIANCE DECOMPOSITION OF,0,,False
52,RETRIEVAL PERFORMANCE,0,,False
53,"In Statistics [1], bias and variance, which are measurements of estimation quality in different aspects, are decomposed from the expected squared error of the estimated values with respect to the target value. In this paper, we formulate the bias-variance decomposition in the IR evaluation scenario. Let us first assume there exists a target model T that can have an upper-bound performance for each query 1.",0,,False
54,2.1 Bias-Variance of AP,1,AP,True
55,"We can let AP be a random variable over queries. Bias(AP) essentially calculates the average difference between the target AP (i.e. the AP of the target model T, denoted APT ) and the actual AP of a retrieval model over all different queries. Specifically,",1,AP,True
56,"Bias(AP) , E(APT - AP) , E(APT ) - E(AP) (1)",1,AP,True
57,where the expectation E(·) is over a set of queries that are,0,,False
58,"assumed to be uniformly distributed. E(APT ) corresponds to the target MAP (i.e., the MAP of the target model T,",1,AP,True
59,"denoted MAPT ), and E(AP) corresponds to the MAP of",1,MAP,True
60,the retrieval model under evaluation.,0,,False
61,It turns out that the smaller bias indicates the better mean,0,,False
62,"retrieval effectiveness over queries. In Table 1, the bias for",0,,False
63,model,0,,False
64,A,0,,False
65,is,0,,False
66,1 2,0,,False
67,[(0.7,0,,False
68,-,0,,False
69,0.3),0,,False
70,+,0,,False
71,(0.2,0,,False
72,-,0,,False
73,0.1)],0,,False
74,", 0.25.",0,,False
75,According,0,,False
76,to,0,,False
77,"Eq. 1, it can be equivalently calculated as 0.45-0.2 ,"" 0.25,""",0,,False
78,where 0.45 is the target MAP and 0.2 is the MAP of A.,1,MAP,True
79,"Similarly, the variance of the APs for different queries is",1,AP,True
80,defined as:,0,,False
81,"V ar(AP) , E(AP - E(AP))2",1,AP,True
82,(2),0,,False
83,The smaller V ar(AP) indicates the better retrieval stability of AP across queries.,1,AP,True
84,1We will relax this constraint in Sections 2.4.,0,,False
85,Table 2: Examples of Additional Bias-Variance (),0,,False
86,Model ,0,,False
87,A q1 q2 0.4 0.1,0,,False
88,B q1 q2 0.1 0.12,0,,False
89,C,0,,False
90,q1,0,,False
91,q2,0,,False
92,0.05 0.17 0,0,,False
93,T q1 q2 00,0,,False
94,Bias,0,,False
95,0.25,0,,False
96,0.11,0,,False
97,0.11,0,,False
98,0,0,,False
99,V ar,0,,False
100,0.0225,0,,False
101,0.0001,0,,False
102,0.0036,0,,False
103,0,0,,False
104,Bias2 + V ar 0.0850,0,,False
105,0.0122,0,,False
106,0.0157,0,,False
107,0,0,,False
108,"We now add up the bias and variance, yielding:",1,ad,True
109,"Bias2(AP) + V ar(AP) , [E(APT ) - E(AP)]2 + E(AP - E(AP))2 (3) , E(AP - MAPT )2",1,AP,True
110,"It turns out E(AP - MAPT )2 is not exactly the expected squared error E(AP - APT )2. However, we will show later that E(AP - MAPT )2 is a simplified version of an expected squared error E(AP-1)2 in Section 2.3. This error formulation will help us understand the difference between different bias-variance decompositions (see Section 2.3).",1,AP,True
111,2.2 Additional Bias-Variance,0,,False
112,"To clarify the above observation, let us first look at the decomposition of the expected squared error E(AP-APT )2. We need to define another random variable",1,AP,True
113," , APT - AP",1,AP,True
114,(4),0,,False
115,"As an illustration, for the model A in Table 2,  is 0.4 (0.70.3) and 0.1 (0.2-0.1), for q1 and q2 respectively.",0,,False
116,"Let T be the target value of , which is indeed 0, since for model T,  , APT - APT ,""0. We need T in the following analysis, since the target value is an important component""",1,AP,True
117,in the standard definition of bias. We define,0,,False
118,"Bias() , E() - T",0,,False
119,(5),0,,False
120,"where E() is the averaged  over all queries. Since T is 0, Bias() equals to E() ,"" E(APT -AP), which is Bias(AP).""",1,AP,True
121,"The variance based on , denoted V ar(), computes the variance of the difference between the AP (of the test model)",1,AP,True
122,"and the AP target across all queries. Formally,",1,AP,True
123,"V ar() , E( - E())2",0,,False
124,(6),0,,False
125,"As shown in Table 2, V ar() of B and C are smaller than V ar() of A, indicating that B and C are more stable than A. This observation is different from < Init and V ar(AP) which shows that B and C are less stable than A in Table 1.",1,AP,True
126,"Now, we derive the decomposition of E(AP - APT )2:",1,AP,True
127,"E(AP - APT )2 , E( - T )2",1,AP,True
128,", E( - E())2 + (E() - T )2 (7)",0,,False
129,", V ar() + Bias2()",0,,False
130,"The above equations show the expected squared error E(AP - APT )2 can be exactly decomposed into bias and variance on . The expected squared error E(AP - APT )2 always computes the error of the target model as zero, no",1,AP,True
131,matter whether or not the target model still has room for,0,,False
132,improvement. It is likely that the current best performance,0,,False
133,for some queries can be further advanced in the near future.,1,ad,True
134,1022,0,,False
135,2.3 Further Investigation on Decomposition of Expected Squared Error,0,,False
136,"In order to further investigate two aforementioned biasvariance decompositions, we set 1 (the maximum AP) as the upper-bound AP for each query. We can have an expected squared error E(AP - 1)2 and its decomposition as:",1,AP,True
137,E(AP - 1)2,1,AP,True
138,", E(AP - APT + APT - 1)2",1,AP,True
139,", E(AP - APT )2 + E(APT - 1)2 + 2E(AP - APT )(APT - 1) (8)",1,AP,True
140,"which shows that E(AP-APT )2 is only one part of E(AP- 1)2, and the target model T still has an error E(APT - 1)2,",1,AP,True
141,suggesting there is still a room for improvement. We can also decompose E(AP - 1)2 as:,1,AP,True
142,E(AP - 1)2,1,AP,True
143,", E[AP - E(AP) + E(AP) - 1]2",1,AP,True
144,", E(AP - E(AP))2 + [E(AP) - 1]2",1,AP,True
145,(9),0,,False
146,", V ar(AP) + (MAP - 1)2",1,AP,True
147,", (1 - MAP)2 + V ar(AP)",1,MAP,True
148,"It turns out the variance parts in Eq. 3 and Eq. 9 are the same (i.e., V ar(AP)). The term (1 - MAP)2 in Eq. 9 has the same trend as Bias2(AP) (i.e., (MAPT -MAP)2), where MAPT is the upper bound of the MAP. The above observations show that the decomposition in Eq. 3 can be considered",1,AP,True
149,as a simplified version of the decomposition in Eq. 9.,0,,False
150,2.4 Comparison between Two Bias-Variance,0,,False
151,"First, the bias-variance of  requires a target AP for every query. On the other hand, the bias-variance of AP (in Eq. 3) can be used when only a target MAP (i.e., MAPT ) is given. Specifically, two biases (i.e., Bias(AP) and Bias()) are equivalent and both can be computed by MAPT -MAP. Regarding variance, the variance of  requires APT for each query (see Eq. 6 and Eq. 4), while the variance of AP can be calculated without knowing APT (see Eq. 2). Thus, the bias-variance based on AP is more flexible for practical use.",1,AP,True
152,"Second, the bias-variance of  is an exact decomposition of E(AP - APT )2 and it always regards the target model as a zero-error model. However, the decomposition of E(AP-1)2 in Eq. 8 shows that the target model can still has error. On the other hand, under the bias-variance of AP, the variance for the target model still exists, indicating that although we can assume a target model as an upper-bound at a certain stage, it can be further improved.",1,AP,True
153,2.5 Bias-Variance Evaluation,0,,False
154,"To carry out IR evaluation based on the proposed the bias-variance decomposition methods, one needs to choose the upper-bound settings (i.e., the target model T). There can be a number of ways. First, the upper-bound AP for each query can be simply set as 1, which corresponds to a perfect target model. Second, the upper-bound AP for each query can be obtained based on the best AP among evaluated retrieval models in the historic TREC results. Third, one can simply set an upper-bound MAP (i.e., MAPT ).",1,AP,True
155,"The first and second upper-bound settings correspond to a virtual target model. For example, in the second setting, the target model collects the best AP among many retrieval",1,AP,True
156,"models. The third setting can correspond to a real target model (see the model settings in the experiments.) With the first and second upper-bound settings, either of biasvariance metrics (based on AP or ) can be adopted. The third setting is suitable for bias-variance of AP (see Section 2.4). Once an upper-bound setting and a variable (AP or ) are chosen, we can calculate bias-variance figures for a series of models and then see which model can have the minimum bias, or minimum variance, or the sum of them (i.e., the total error). We can also get an overview on the trend of bias and variance over parameters.",1,AP,True
157,3. EMPIRICAL EVALUATION,0,,False
158,"We use the query modeling as an example of bias-variance evaluation. Due to the page limit, we only report the evaluation results on two standard TREC collections, i.e., WSJ (87-92) over queries 151-200 and ROBUST 2004 over queries 601-700. The title field of the queries is used. Lemur 4.7 [5] is used for indexing and retrieval. The top n , 30 ranked documents in the initial ranking by the original query model are selected as the pseudo-relevance feedback (PRF) documents. The number of expanded terms is fixed as 100. 1000 documents are retrieved by the KL-divergence model [5].",1,TREC,True
159,"Model Settings We customize a number of query models according to three factors (i.e., model complexity, model combination, ground-truth data) that can generally influence the bias-variance tradeoff [1]. The first model is the original query model which is a maximum likelihood estimation of original query. We also evaluate an expanded query model RM (i.e., RM1 in [4]) which is generated from feedback documents D. Compared with the original query model, the expanded query model is more complex due to the fact that it has more terms and additional assumptions (e.g., assuming that all feedback documents are relevant). The above two models can be combined, leading to a combined query model (also called as RM3). Let  be the combination coefficient which is in the range (0,1). When  gets close to 0, the combined model is towards the RM, and otherwise towards the original model. In RM, we can gradually remove a percentage (denoted by rn) of nonrelevant documents DN along the initial ranking of feedback documents, based on the available relevance judgements (as ground-truth) [8]. We finally derive a target model which are generated by keeping only relevant documents DR in D:",1,ad,True
160,p(w|q(t)) ,0,,False
161,p(w|d),0,,False
162,dDR,0,,False
163,(10),0,,False
164,"which gives the best MAP over the aforementioned query models. In Eq. 10, the document weights are set as uniform (different from the weights in RM), since the relevant judgements of relevant documents are the same (i.e., 1).",1,MAP,True
165,"Bias-Variance Metrics Setting We report the biasvariance on AP in our evaluation, since the target query model used in our evaluation only guarantees the upperbound MAP. According to the discussion in Sections 2.4 and 2.5, the bias-variance of AP is more suitable.",1,AP,True
166,"Evaluation Results We first look at the bias-variance results in Figure 1. Recall that the smaller bias or variance (based on AP) corresponds to the better retrieval effectiveness or stability, respectively. Figure 1(a) shows that on WSJ8792, the original query model has the smallest variance (the highest stability), but the largest bias (the lowest MAP). This is a tradeoff between bias and variance. One",1,AP,True
167,1023,0,,False
168,Var(AP),1,AP,True
169,0.066 0.064 0.062,0,,False
170,0.06,0,,False
171,Bias-Variance (AP),1,AP,True
172,Original Model Expanded Model RM Combinaed Model Removing DN Target Model,0,,False
173,0.058,0,,False
174,0.056,0,,False
175,0.054 0,0,,False
176,0.005 0.01 0.015 0.02 0.025 0.03 0.035 Bias2(AP),1,AP,True
177,(a) WSJ8792,0,,False
178,Var(AP),1,AP,True
179,0.06 0.058 0.056 0.054 0.052,0,,False
180,0.05 0.048 0.046 0.044 0.042,0,,False
181,0.04 0,0,,False
182,Bias-Variance (AP),1,AP,True
183,Original Model Expanded Model RM Combinaed Model Removing DN Target Model 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045,0,,False
184,Bias2(AP),1,AP,True
185,(b) ROBUST2004,0,,False
186,Figure 1: Results of Bias and Variance of AP of all the concerned query models. The x-axis shows the squared bias and the y-axis shows the variance.,1,AP,True
187,"reason in Statistics language is that the expanded models are all complex than the original one. The bias of the expanded model RM is much smaller, while its variance is much larger, than the original model. The different variants of the combined model (with  in [0.1, 0.9] with increment 0.1) are lying between the expanded and original models. We can also see that 2 (out of 9) combined models (when ,""0.1 and 0.2) has smaller bias and smaller variance than the expanded query model RM. This suggests that the combined query model with good parameters can achieve better effectiveness and stability over RM. Among the combined query models, bias and variance are negatively correlated, indicating a bias-variance tradeoff occurs. On the other hand, if we remove the non-relevant documents DN (with rn in [0.1, 1] with increment 0.1) in RM, the bias and variance are positively correlated, and 9 (out of 10) models (when rn "","" 0.2 to 1) can reduce both the bias and variance over RM. The target model has zero bias, although there is a variance of its AP across different queries. On ROBUST2004, the results are similar. The differences are that: 1) by removing non-relevant documents, only 6 (out of 10) models (when rn "", 0.5 to 1) reduce both bias and variance over RM; 2) there are 3 (out of 9) (when  , 0.1 to 0.3) variants of the combined model for which both bias and variance can be smaller than those of RM.",1,ad,True
188,"Figure 2 shows Bias2 + V ar results for all the concerned models. On both collections, the target method achieves the smallest Bias2 + V ar and the original query model achieves the largest Bias2 +V ar. Recall that Bias2 +V ar represents the total error and the smaller error can reflect the better overall quality (or performance), by considering both the effectiveness and stability in one single criterion (Bias2 + V ar). The results also suggest that the model combination and the relevance judgements are helpful to reduce Bias2 + V ar over the original model and the expanded model RM.",0,,False
189,4. CONCLUSIONS AND FUTURE WORK,0,,False
190,"In this paper, a novel evaluation strategy based on the bias-variance decomposition has been presented. We formulate two forms of the bias-variance decomposition and theoretically compare them. We also use query expansion as an example to demonstrate the use of the bias-variance for IR evaluation and analysis, in terms of bias, variance or sum of them. In the future, we will further explore the other upper-bound settings and variables discussed in Section 2.5,",0,,False
191,Bias2(AP)+Var(AP) Bias2(AP)+Var(AP),1,AP,True
192,Bias-Variance (AP) 0.09,1,AP,True
193,0.085,0,,False
194,Bias-Variance (AP),1,AP,True
195,0.085,0,,False
196,0.08,0,,False
197,0.08,0,,False
198,0.075,0,,False
199,0.075 0.07,0,,False
200,0.065 0.06 0,0,,False
201,Original Model Expanded Model RM Combinaed Model Removing DN Target Model,0,,False
202,0.005 0.01 0.015 0.02 0.025 0.03 0.035 Bias2(AP),1,AP,True
203,(a) WSJ8792,0,,False
204,0.07,0,,False
205,0.065 0.06,0,,False
206,0.055 0,0,,False
207,Original Model Expanded Model RM Combinaed Model Removing DN Target Model,0,,False
208,0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 Bias2(AP),1,AP,True
209,(b) ROBUST2004,0,,False
210,Figure 2: Results of Bias2 and Bias2+V ar of AP of all the concerned query models. The x-axis shows the squared bias and the y-axis shows the Bias2 + V ar.,1,AP,True
211,test with more retrieval models and explore the deep insights behind the bias-variance trends.,0,,False
212,5. ACKNOWLEDGMENTS,0,,False
213,"This work is supported in part by the Chinese National Program on Key Basic Research Project (973 Program, grant No. 2013CB329304), the Natural Science Foundation of China (grant No. 61272265, 61070044), and EU's FP7 QONTEXT project (grant No. 247590).",0,,False
214,6. REFERENCES,0,,False
215,"[1] C. M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., 2006.",0,,False
216,"[2] K. Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In CIKM '09, pages 837­846, 2009. ACM.",0,,False
217,"[3] G. V. Cormack and T. R. Lynam. Statistical Precision of Information Retrieval Evaluation. In SIGIR '06, pages 533­540, 2006. ACM.",0,,False
218,"[4] V. Lavrenko and W. B. Croft. Relevance-based language models. In SIGIR '01, pages 120­127, 2001.",0,,False
219,"[5] P. Ogilvie and J. Callan. Experiments using the lemur toolkit. In TREC '02, pages 103­108, 2002.",1,TREC,True
220,"[6] S. E. Robertson and E. Kanoulas. On per-topic variance in ir evaluation. In SIGIR '12, pages 891­900, 2012.",0,,False
221,"[7] J. Wang and J. Zhu. Portfolio theory of information retrieval. In SIGIR '09, pages 115­122, 2009.",0,,False
222,"[8] P. Zhang, Y. Hou and D. Song. Approximating true relevance distribution from a mixture model based on irrelevance data. In SIGIR '09, pages 107­114, 2009.",0,,False
223,"[9] L. Zighelnic and O. Kurland. Query-drift prevention for robust query expansion. In SIGIR '08, pages 825­826, 2008.",1,Query,True
224,1024,0,,False
225,,0,,False

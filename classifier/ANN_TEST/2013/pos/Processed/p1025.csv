,sentence,label,data,regex
0,An Adaptive Evidence Weighting Method for Medical Record Search,0,,False
1,Dongqing Zhu and Ben Carterette,0,,False
2,Department of Computer & Information Sciences University of Delaware,0,,False
3,"Newark, DE, USA 19716",0,,False
4,[zhu | carteret]@cis.udel.edu,0,,False
5,ABSTRACT,0,,False
6,"In this paper, we present a medical record search system which is useful for identifying cohorts required in clinical studies. In particular, we propose a query-adaptive weighting method that can dynamically aggregate and score evidence in multiple medical reports (from different hospital departments or from different tests within the same department) of a patient. Furthermore, we explore several informative features for learning our retrieval model.",1,ad,True
7,Categories and Subject Descriptors,0,,False
8,H.3.3 [Information Search and Retrieval]: Retrieval models,0,,False
9,Keywords,0,,False
10,medical record search; EMR; information retrieval; cohort identification; language models,0,,False
11,1. INTRODUCTION,1,DUC,True
12,"The rich health information contained in electronic medical records (EMR) is useful for improving quality of care. One important application is to search EMR to identify cohorts for clinical studies, which requires retrieval systems specifically designed with medical domain knowledge.",0,,False
13,"To promote research on medical information retrieval, particularly for EMR retrieval, the Text REtrieval Conference (TREC) organized a Medical Records track in 2011 and 2012 [11, 10]. The task is an ad hoc search task for patient visits based on unstructured text in EMR. One particular problem in EMR search is how to aggregate and score evidence that distributes across multiple documents. This is because a patient can have multiple medical reports generated from several hospital departments or even from different tests within a single department.",1,TREC,True
14,"In this paper, we propose a novel weighting method that can adaptively weight evidence with respect to different queries. We evaluate our algorithm on TREC test collections. The",1,ad,True
15,"cross-validation results show that our weighting method is better than a fixed-weighting method across several evaluation metrics. Though the improvement is not statistically significant, we believe that our method has the potential to be further improved when more test collections are available.",0,,False
16,"Our work makes the following contributions: 1) we propose a novel adaptive weighting method for aggregating and scoring evidence in medical records, 2) we propose and explore several features that are based on semantic similarity between medical concepts for predicting the weights of our adaptive weighting method.",1,ad,True
17,2. RETRIEVAL TASK AND DATA,0,,False
18,"We use the official test collection of the TREC 2011 & 2012 Medical Records Track [11, 10] for our experiments. The test collection contains 100,866 de-identified medical reports, mainly containing clinical narratives, from the University of Pittsburgh NLP Repository.",1,TREC,True
19,"The retrieval task1 is an ad hoc search task for patient visits. A patient visit to the hospital usually results in multiple medical reports, meaning there is a 1-to-n relationship between visits and reports.",1,ad,True
20,ID Topic 107 Patients with ductal carcinoma in situ (DCIS) 118 Adults who received a coronary stent during an admission 109 Women with osteopenia 112 Female patients with breast cancer with mastectomies during admission,1,ad,True
21,Table 1: Example topics of medical records track.,0,,False
22,"NIST released 81 information needs (or ""topics"" in TREC terminology) which were designed to require information mainly from the free-text fields (i.e., topics are not answerable solely by the diagnostic codes). Topics are meant to reflect the types of queries that might be used to identify cohorts for comparative effectiveness research [11]. Table 1 lists several TREC topics as examples. The topic specifies the patient's condition, disease, treatment, etc. Relevance judgments for the topics were also developed by TREC assessors based on the pooled results from TREC participants.",1,TREC,True
23,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
24,3. ADAPTIVE EVIDENCE AGGREGATION,1,AP,True
25,"Evidence in a visit can have different forms of distribution. Generally, there are two extreme cases: 1) Strong evidence exists in only one report of the visit; 2) Evidence spreads almost evenly across the majority of reports associated with the visit.",1,ad,True
26,1 http://www- nlpir.nist.gov/projects/trecmed/2011/tm2011.html,1,trec,True
27,1025,0,,False
28,"Report-based Retrieval For the first case, we can estimate the relevance of a visit based on its most relevant report. Thus, we use reports as the initial retrieval units (i.e., building an index for reports and applying the retrieval model to each report), and then transform a report ranking into a visit ranking based on the strongest report-level evidence, which is equivalent to using the following report score merging method for ranking visits:",0,,False
29,"scoreR(V, Q) ,"" MAX(score(R1V , Q), score(R2V , Q), ...), (1)""",0,,False
30,"where RjV is a report associated with visit V based on the report-to-visit mapping, score(RjV , Q) is the relevance score of the report with respect to query Q.",0,,False
31,"Visit-based Retrieval Eq.1 cannot handle case 2 well. For example, if visit V1 has strong evidence in multiple reports and visit V2 has strong evidence in only one report, V1 and V2 will have the same relevance score by using Eq.1. To deal with this problem,",0,,False
32,"we aggregate evidence by merging reports from a single visit field by field into a single visit document V , and then performing retrieval from an index of visits.",0,,False
33,"Like report-based retrieval, this visit-based retrieval has",0,,False
34,"its own disadvantages since it cannot handle case 1 well. For example, if visits V1 and V2 both have strong evidence in only one of their reports but V1 has three times more reports than V2, the strong evidence in V1 will be weakened after merging, resulting in V1 receiving a lower relevance score than V2.",1,ad,True
35,3.1 A Novel Scoring Function,0,,False
36,"The comparison of the report-based and visit-based retrievals shows that these two strategies complement each other. Thus, we propose a new query-adaptive scoring function as shown below:",1,ad,True
37,"score(V, Q) ,"" Q ·scoreR(V, Q)+(1-Q)·scoreV(V, Q), (2)""",0,,False
38,"where scoreR(V, Q) and scoreV(V, Q) are the relevance scores of document V from report-based and visit-based retrievals respectively, and Q is the query-adaptive coefficient for scoring merging. If we can adjust Q appropriately, Eq.2 should be able to deal with all the evidence distribution",1,ad,True
39,cases mentioned above.,0,,False
40,3.2 Learning Algorithm,0,,False
41,"In this paper, we propose to adaptively set Q with respect to different queries by learning the weight Q based on a set of features.",1,ad,True
42,"In particular, we can view Q as a mixing probability: the probability that the evidence clusters in only one report rather than spreads across multiple reports. Then, assuming the log-odds of that probability can be expressed as a linear combination of feature values, we may write:",1,ad,True
43,log,0,,False
44,1,0,,False
45,Q - Q,0,,False
46,m,0,,False
47,", 0 + ixi +",0,,False
48,"i,1",0,,False
49,Q,0,,False
50,"where 0 is a model intercept (or bias term), xi is the value of feature number i, i is the weight coefficient of that feature, and Q is a slack variable.",0,,False
51,This is essentially a logistic regression model2. Logistic,0,,False
52,regression is fit using iteratively reweighted least squares to,0,,False
53,"2While logistic regression is often used for 0/1 classification problems,",0,,False
54,"find the values of the  coefficients that are the best fit to training data. Given feature values and their  coefficients, we can then predict the mixing probability Q for new queries.",0,,False
55,3.3 Features,0,,False
56,"We propose 14 features that are possibly related to the evidence distribution in visits, and can be used to predict the weight Q in Eq.2. All these features are based on characteristics of the medical concepts contained in the query. We detect these medical concepts using MetaMap [1], a medical NLP tool developed at the National Library of Medicine (NLM) to map biomedical text to concepts in the Unified Medical Language System (UMLS) Metathesaurus. The concepts are represented by the Concept Unique Identifier (CUI) in UMLS Metathesaurus. Thus, we use QC to represent a concept query that is converted from the original text query Q and contains only CUIs. Next, we describe these 14 features in detail:",1,LM,True
57,"1. Length of the query Intuitively, evidence is more likely to resides across reports for long queries. Thus, we use the length of query |Q| as the feature to estimate the evidence distribution. It is defined formally as |Q| ,"" wQ cnt(w, Q), where c(w, Q) is the count of term w in Q.""",0,,False
58,2. Number of concepts in the query,0,,False
59,"Similarly, if a query contains more medical concepts, it is",0,,False
60,more likely to find that the evidence distributes across mul-,0,,False
61,"tiple reports. We define this feature formally as |QC | ,",0,,False
62,w,0,,False
63,"winQQCCc.nQt(Cw,aQbCe)t,tewrhfeeraetucrnet(twh,aQn CQ)",0,,False
64,is the count of term because if the query,0,,False
65,contains a medical concept whose name is very long then Q,0,,False
66,might not be a good indicator of the evidence distribution.,0,,False
67,3. Broad/narrow query concepts,1,ad,True
68,"A text query can contain several medical concepts, for each",0,,False
69,of which the MetaMap program will return 1 to 10 candi-,0,,False
70,dates. We hypothesize that a concept with more candidates,0,,False
71,"is less specific, and thus more likely to be a broad concept",1,ad,True
72,"and appears in multiple reports. Thus, the average number",0,,False
73,of returned MetaMap candidates for concepts in a query,0,,False
74,may be a good indicator of evidence distribution. We define,0,,False
75,"this feature as RC ,",0,,False
76,wQC |Meta(w)|,0,,False
77,|QC |,0,,False
78,",",0,,False
79,where,0,,False
80,|QC |,0,,False
81,the,0,,False
82,orig-,0,,False
83,"inal concept query length (i.e., the length before expansion),",0,,False
84,|Meta(w)| is the number of concept candidates returned by,0,,False
85,MetaMap for term w in concept query QC .,0,,False
86,"4. Semantic similarity among query concepts Intuitively, if QC contains concepts that are semantically close, the associated evidence in a visit may also co-occur in a single report. However, if the concepts are semantically distant, the corresponding evidence may tend to distribute across reports. Thus, we use the semantic distance among query concepts to estimate how the evidence distributes.",0,,False
87,"We use YTEX3 to measure semantic similarity. Given a pair of UMLS concepts, YTEX can produce knowledge based and distributional based similarity measures. The former uses knowledge sources such as dictionaries, taxonomies,",0,,False
88,it can also be used when the target variable is a real number between,0,,False
89,"0 and 1. In this case it is sometimes called a ""quasibinomial"" model. 3 http://code.google.com/p/ytex/wiki/SemanticSim_V06",1,wiki,True
90,1026,0,,False
91,Type Knowledge-based Distributional-based,0,,False
92,Method Path-Finding,0,,False
93,Intrinsic IC based Corpus IC based,0,,False
94,Notation WUPALMER,1,LM,True
95,LCH PATH RADA IC LIN IC LCH IC PATH IC RADA JACCARD SOKAL CIC LIN,0,,False
96,Name Wu & Palmer Leacock & Chodorow Path Rada Lin Leacock & Chodorow Jiang & Conrath Rada Jaccard Sokal & Sneath Lin,1,ad,True
97,Table 2: Semantic similarity measures.,0,,False
98,"and semantic networks, while the latter mainly uses the distribution of concepts within some domain-specific corpus [3].",0,,False
99,"We use the 11 measures listed in Table 2 as our features. Due to the limited space, we will not describe these features; Garla and Brandt provide a detailed overview [3].",0,,False
100,"For each query and each specific measure, we take the mean of the semantic similarity scores for all UMLS concept pairs in the query. This averaged semantic similarity score will be the feature score.",0,,False
101,4. EXPERIMENTAL SETUP,0,,False
102,"We use the Indri4 retrieval system for indexing and retrieving. In particular, we use the Porter stemmer to stem words in both text documents and queries, and use a standard medical stoplist [4] for stopping words in queries only.",0,,False
103,"Our retrieval model is a linear combination of the Markov random field model (MRF) [8] and a mixture of external collection-based relevance models (MRM) [2] for query expansion. Our collections for expansion are the ClueWeb09 Category B (excluding the Wikipedia pages) corpus, the 2009 Genomics Track corpus, 2012 Medical Subject Headings (MeSH), and the medical records corpus itself. Both report and visit-based retrievals use this system.",1,ClueWeb,True
104,"Because the focus of this work is to evaluate the adaptive scoring function as shown in 2, we will set the parameters of the MRF and MRM models to some default values. We use the same set of parameter values for both the report and visit-based retrievals. We set the Dirichlet smoothing parameter  to 2500. For MRF model, we follow Metzler and Croft [8] and set the feature weights (T , O, U ) to (0.8, 0.1, 0.1). For MRM model, we take take the top-weighted 10 terms from the top-ranked 50 documents for each expansion collection. More detail about our model is presented in recent work [13].",1,ad,True
105,"To evaluate our learning algorithm as described in Section 3, we first obtain the optimal coefficient Q-opt for each topic Q by sweeping [0, 1] at a step size of 0.1. Then we conduct leave-one-out cross-validation (LOOCV), in each iteration of which the system predicts Q for one new topic based on Q-opt's for the other 80 topics. With limited topics available for learning a relatively complex prediction model, using LOOCV can maximize the size of training data we can use in each iteration of the cross-validation, and lead to a better estimate for each feature weight.",1,ad,True
106,We train our systems on MAP. This is because: 1) training on MAP is most commonly used in IR to improve retrieval performance; 2) we find that training on MAP improves the retrieval performance on other evaluation metrics as well while training on other evaluation measures does not,1,MAP,True
107,4 http://www.lemurproject.org/indri/,0,,False
108,Feature IC RADA WUPALMER,1,LM,True
109,RADA JACCARD,0,,False
110,Significance 0.0112 0.0299 0.0368 0.0647,0,,False
111,Feature RC,0,,False
112,SOKAL IC LIN IC PATH,0,,False
113,Significance 0.0654 0.0671 0.0824 0.0876,0,,False
114,"Table 3: Features in the pruned set using LOOCV, sorted by their statistical significance scores.",0,,False
115,"improve the overall performance. Thus, MAP will be the primary evaluation measure in this work. In fact, MAP correlates well with other evaluation measures as we will show in the Section 5.",1,MAP,True
116,"To access the statistical significance of differences in the performance of two systems, we perform one-tailed paired t-test for MAP (since we train systems on MAP). We report scores for MAP, R-precision (Rprec), bpref, and precision at rank 10 (P10).",1,MAP,True
117,5. RESULTS AND ANALYSIS,0,,False
118,5.1 Feature Selection,0,,False
119,"To choose a good feature combination, we use a greedy feature elimination approach in which we start with a full set of features and iteratively eliminate exactly one feature at a time that has the greatest negative impact on the retrieval performance until when further removing any feature will degrade the performance.",1,ad,True
120,"After the above feature set pruning step, there are 8 features left as shown in Table 3. We further study the importance of each feature by analyzing the prediction model trained in a randomly selected iteration of LOOCV using these 8 features. Based on the statistical significance of each feature as shown in Table 3, we can infer that:",0,,False
121,"1) All the intrinsic IC based features except IC LCH are in the pruned feature set, indicating that these types of similarity measures are generally more effective for predicting Q than other measures. In fact, the intrinsic IC similarity measure incorporates taxonomical evidence explicitly modeled in ontologies (such as the number of leaves/hyponyms and subsumers), which are not captured by the path-finding based measure. Furthermore, the intrinsic IC similarity measure avoids dependence on the availability of domain corpora, thus is considered more scalable and easily applicable than the distributional-based measure [9].",1,corpora,True
122,2) RC is a good feature though it only uses similarity information about each query concept and its neighbors (rather than other query concepts) in the semantic network.,0,,False
123,"3) Neither |Q| nor QC is in the pruned set, indicating that non-semantic-similarity-related features are generally not useful for estimating the evidence distribution.",0,,False
124,4) RADA is a feature that might worth further exploration because both the Path-finding based and the intrinsic IC based RADA features are in the pruned set.,0,,False
125,5.2 Adaptive Weighting,0,,False
126,"Fixed Weighting We first evaluate the performance of Eq. 2 when  is fixed (i.e., not adaptive). In each iteration of the LOOCV, we obtain the best value setting for  on the 80 training topics by sweeping [0, 1] at a step size of 0.1, and then apply the trained  value to the single testing topic. We show the results in the `Fixed-weighting' row of Table 4. Note that",1,ad,True
127,1027,0,,False
128,System Visit-based Report-based Fixed-weighting Adaptive-weighting Optimal-weighting,0,,False
129,MAP,1,MAP,True
130,"0.4122 0.4354V 0.4472V,R 0.4485V,R 0.4639V,R,F,A",0,,False
131,R-prec 0.422 0.435 0.443 0.447 0.457,0,,False
132,bpref 0.499 0.511 0.520 0.523 0.539,0,,False
133,P10 0.619 0.607 0.631 0.642 0.656,0,,False
134,Pred. MSE ­ ­,0,,False
135,0.128 0.125 0.000,0,,False
136,"Table 4: Performance comparison. A superscript on the MAP score of system X corresponds to the initial of system Y, and indicates statistical significance (p < 0.05) in the MAP difference between X and Y. The last column is the mean square error of the predicted weights. `Fixed-weighting' corresponds to one of the top-ranked TREC systems as mentioned in Sections 4 and 5.2.",1,MAP,True
137,this system is a better version of system udelSUM [13] which is one of the top-ranked 2012 Medical Records track systems.,0,,False
138,"Optimal Weighting We also obtain the optimal Q-opt for each topic separately by sweeping  from 0 to 1 with a step size of 0.1. Then, we use the Q-opt's to compute the best retrieval performance (i.e., an upper-bound) Eq. 2 can possibly achieve, as shown in the `Optimal-weighting' row of Table 4.",0,,False
139,"Performance Comparison Table 4 shows performance comparison of our adaptive merging method with fixed-weighting, optimal-weighting, and two other baselines (report-based retrieval and visit-based retrieval). Our adaptive merging method is better than the fixed weighting method on all the evaluation metrics. The improvement is not statistically significant (p ,"" 0.191), possibly because 81 topics may not be enough to train a good prediction model for our adaptive weighting method. In addition, the data are slightly skewed as Figure 1 showing that Q-opt "", 1 or 0.9 on about one third of the topics.",1,ad,True
140,Figure 1: Distribution of topics against Q-opt.,0,,False
141,6. RELATED WORK,0,,False
142,"Due to the sensitivity of patient data, methods emerging from research on information retrieval for EMR retrieval have not been well explored by academic researchers. Fortunately, the Text REtrieval Conference (TREC) organized the Medical Records track in 2011 & 2012 making a set of real medical records and human judgments of relevance to search queries available to the research community.",1,ad,True
143,Some interesting work have been done using the TREC collection. Limsopatham et al. [5] proposed an effective term representation to handle negated phrases in clinical text. They also incorporated dependence information of the negated terms into the term representation and achieved significant improvement over a baseline system that had no negation handling mechanism.,1,TREC,True
144,"More recently, Limsopatham et al. [7] proposed an effective representation for EMR retrieval, in which medical",0,,False
145,"records and queries are represented by medical concepts that directly relate to symptom, diagnostic, test, diagnosis, and treatment. We have built on their work, combining a concept representation with text-based retrieval to improve on both and provide a base in which additional medical knowledge can be incorporated easily.",1,ad,True
146,"Among more relevant works, Limsopatham et al. [6] explored using the type of medical records for enhancing retrieval performance. They demonstrated that incorporating department level evidence of the medical reports in their extended voting model and federated search model could improve the retrieval effectiveness. Their work opens another interesting direction for exploring evidence distribution and score merging. Zhu and Carterette's system [12] aggregated report-level evidence and visit-level evidence, and achieved significant improvement over a strong baseline.",1,corpora,True
147,7. CONCLUSION AND FUTURE WORK,0,,False
148,"In this paper, we present a medical record search system which is useful for identifying cohorts required in clinical studies. In particular, we propose a query-adaptive weighting method that can dynamically aggregate and score evidence within multiple medical reports. We show by crossvalidation that our weighting method is better than a fixedweighting method across several evaluation metrics. Though the improvement is not statistically significant, we believe that our method has the potential to be further improved by incorporating other useful features or by using advanced prediction models. Furthermore, we explore several informative features for weight prediction. We believe these features might be useful for improving medical IR systems.",1,ad,True
149,"8. REFERENCES [1] A. R. Aronson. Effective mapping of biomedical text to the UMLS metathesaurus: The MetaMap program. Proceedings of AMIA Symposium, pages 17­21, 2001. [2] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In Proceedings of SIGIR, pages 154­161, 2006. [3] V. Garla and C. Brandt. Semantic similarity in the biomedical domain: an evaluation across knowledge sources. BMC Bioinformatics, 13:261, 2012. [4] W. Hersh. Information Retrieval: A Health and Biomedical Perspective. Health Informatics. Springer, 3rd edition, 2009. [5] N. Limsopatham, C. Macdonald, R. McCreadie, and I. Ounis. Exploiting term dependence while handling negation in medical search. In Proceedings of SIGIR, pages 1065­1066, 2012. [6] N. Limsopatham, C. Macdonald, and I. Ounis. Aggregating evidence from hospital departments to improve medical records search. In Proceedings of ECIR, 2013. [7] N. Limsopatham, C. Macdonald, and I. Ounis. A task-specific query and document representation for medical records search. In Proceedings of ECIR, 2013. [8] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, page 472, 2005. [9] D. Sa´nchez and M. Batet. Semantic similarity estimation in the biomedical domain: An ontology-based information-theoretic perspective. Journal of Biomedical Informatics, 44(5):749 ­ 759, 2011. [10] E. M. Voorhees. DRAFT: Overview of the TREC 2012 medical",1,corpora,True
150,"records track. In TREC, 2012. [11] E. M. Voorhees and R. M. Tong. DRAFT: Overview of the",1,TREC,True
151,"TREC 2011 medical records track. In TREC, 2011. [12] D. Zhu and B. Carterette. Combining multi-level evidence for",1,TREC,True
152,"medical record retrieval. In Proceedings of SHB, 2012. [13] D. Zhu and B. Carterette. Exploring evidence aggregation",0,,False
153,"methods and external expansion sources for medical record search. In Proceedings of TREC, 2012.",1,TREC,True
154,1028,0,,False
155,,0,,False

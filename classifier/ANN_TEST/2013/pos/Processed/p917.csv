,sentence,label,data,regex
0,Composition of TF Normalizations: New Insights on Scoring Functions for Ad Hoc IR,0,,False
1,"François Rousseau*, Michalis Vazirgiannis*",0,,False
2,"*LIX, École Polytechnique, France Department of Informatics, AUEB, Greece Institut Mines-Télécom, Télécom ParisTech, France",0,,False
3,"rousseau@lix.polytechnique.fr, mvazirg@aueb.gr",0,,False
4,ABSTRACT,0,,False
5,"Previous papers in ad hoc IR reported that scoring functions should satisfy a set of heuristic retrieval constraints, providing a mathematical justification for the normalizations historically applied to the term frequency (TF). In this paper, we propose a further level of abstraction, claiming that the successive normalizations are carried out through composition. Thus we introduce a principled framework that fully explains BM25 as a variant of TF-IDF with an inverse order of function composition. Our experiments over standard datasets indicate that the respective orders of composition chosen in the original papers for both TF-IDF and BM25 are the most effective ones. Moreover, since the order is different between the two models, they also demonstrated that the order is instrumental in the design of weighting models. In fact, while considering more complex scoring functions such as BM25+, we discovered a novel weighting model in terms of order of composition that consistently outperforms all the rest. Our contribution here is twofold: we provide a unifying mathematical framework for IR and a novel scoring function discovered using this framework.",1,ad,True
6,Categories and Subject Descriptors,0,,False
7,H.3.3 [Information Search and Retrieval]: Retrieval models,0,,False
8,General Terms,0,,False
9,"Theory, Algorithms, Experimentation",0,,False
10,Keywords,0,,False
11,IR theory; scoring functions; TF normalizations; heuristic retrieval constraints; function composition,0,,False
12,1. MOTIVATION,0,,False
13,Fang et al. introduced in [3] a set of heuristic retrieval constraints that any scoring function used for ad hoc infor-,1,ad,True
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
15,"mation retrieval (IR) should satisfy. In particular, these constraints involve term frequency, term discrimination, document length and the interactions between them. For instance, they stated that a scoring function should favor document matching more distinct query terms. It is one of the earliest works that formally defined the properties that both the TF and the IDF components of any weighting model should possess. It is a unifying theory in IR that applies to the vector space model (TF-IDF [13]), probabilistic (BM25 [10]), language modeling (Dirichlet prior [15]) and information-based (SPL [2]) approaches and the divergence from randomness framework (PL2 [1]).",0,,False
16,"The definition of these constraints contributed to the improvement of the overall effectiveness of most modern scoring functions. Constraints on the term frequency result in successive normalizations on the raw TF, each one satisfying one or more properties. In our work, we intended to go one step further and we propose the use of composition to explain how the normalizations are applied successively in the general TF×IDF weighting scheme. In section 2, we describe in details the mathematical framework we designed. In section 3, we present the experiments we conducted over standard datasets and the results obtained that indicate how important the order of composition is, along with a novel and effective weighting model, namely TFlp×IDF. Finally, in section 4, we conclude and mention future work.",0,,False
17,2. MATHEMATICAL FRAMEWORK,0,,False
18,"In ad hoc IR, a scoring function associates a score to a term appearing both in a query and a document. This function consists of three components supposedly independent of one another: one at the query level (QF), one at the document level (TF) and one at the collection level (IDF). These components are aggregated through multiplication to obtain a final score for the term denoted hereinafter by TF×IDF. We omit voluntarily to mention QF in the name since it is a function of the term frequency in the query and it has usually a smaller impact on the score, in particular for Web queries that tend to be short. We make here a difference between the TF×IDF general weighting scheme and TF-IDF, the pivoted normalization weighting defined in [13]. Note that because we rank documents, these term scores will be aggregated through sum to obtain a document score but this is beyond the scope of the current paper.",1,ad,True
19,2.1 A set of TF normalizations,0,,False
20,"Since the early work of Luhn [4], term frequency (TF) has been claimed to play an important role in information",0,,False
21,917,0,,False
22,"retrieval and is at the center of all the weighting models. Intuitively, the more times a document contains a term of the query, the more relevant this document is for the query. Hence, it is commonly accepted that the scoring function must be an increasing function of the term frequency and the simplest TF component can be defined as follows:",0,,False
23,"T F (t, d) ,"" tf (t, d)""",0,,False
24,(1),0,,False
25,"where tf (t, d) is the term frequency of the term t in the document d. However, as the use of the raw term frequency proved to be non-optimal in ad hoc IR, the research community started normalizing it considering multiple criteria, mainly concavity and document length normalization. Later, these normalizations were explained as functions satisfying some heuristic retrieval constraints as aforementioned [3].",1,ad,True
26,Concave normalization.,0,,False
27,"The marginal gain of seeing an additional occurrence of a term inside a document is not constant but rather decreasing. Indeed, the change in the score caused by increasing TF from 1 to 2 should be much larger than the one caused by increasing TF from 100 to 101. Mathematically, this corresponds to applying a concave function on the raw TF. We prefer the term concave like in [2] to sublinear like in [5] since the positive homogeneity property is rarely respected (and actually not welcomed) and the subadditivity one, even though desirable, not sufficient enough to ensure a decreasing marginal gain.",1,ad,True
28,There are mainly two concave functions used in practice: the one in TF-IDF [13] and the one in BM25 [10] that we respectively called log-concavity (TFl) and k-concavity (TFk):,0,,False
29,"T Fl(t, d) ,"" 1 + ln[1 + ln[tf (t, d)]]""",0,,False
30,(2),0,,False
31,"T Fk(t, d)",0,,False
32,",",0,,False
33,"(k1 + 1) · tf (t, d) k1 + tf (t, d)",0,,False
34,(3),0,,False
35,where k1 is a constant set by default to 1.2 corresponding to the asymptotical maximal gain achievable by multiple occurrences compared to a single occurrence.,0,,False
36,Document length normalization.,0,,False
37,"When collections consist of documents of varying lengths (like web pages for the Web), longer documents will ­ as a result of containing more terms ­ have higher TF values without necessary containing more information. For instance, a document twice as long and containing twice as more times a term should not get a score twice as large but rather a very similar score. As a consequence, it is commonly accepted that the scoring function should be an inverse function of the document length to compensate that effect. Early works in vector space model suggested to normalize the score by the norm of the vector, be it the L1 norm (document length), the L2 norm (Euclidian length) or the L norm (maximum TF value in the document) [11]. These norms still mask some subtleties about longer documents ­ since they contain more terms, they tend to score higher anyway. Instead, the research community has been using a more complex normalization function known as pivoted document length normalization and defined as follows:",1,ad,True
38,"tf (t, d)",0,,False
39,"T Fp(t, d)",0,,False
40,",",0,,False
41,1-,0,,False
42,b,0,,False
43,+,0,,False
44,b,0,,False
45,·,0,,False
46,|d| avdl,0,,False
47,(4),0,,False
48,"where b  [0, 1] is the slope parameter, |d| the document",0,,False
49,length and avdl the average document length across the collection of documents as defined in [12].,0,,False
50,2.2 Composition of TF normalizations,0,,False
51,"Based on this set of properties and their associated functions, it seems natural to apply them to the raw TF successively by composing them. In the literature, the document length normalization has usually been applied to either the overall term score or the document score like one would normally normalize a vector. However, it was then hard to fully fit BM25 in the TF×IDF weighting scheme. With composition, it is just a matter of ordering the functions. We present here the two compositions behind TF-IDF and BM25, respectively TFpl (, TFp  TFl) and TFkp (, TFk  TFp):",0,,False
52,"1 + ln[1 + ln[tf (t, d)]]",0,,False
53,"T Fpl(t, d) ,",0,,False
54,1,0,,False
55,-b+,0,,False
56,b,0,,False
57,·,0,,False
58,|d| avdl,0,,False
59,(5),0,,False
60,"T Fkp(t, d)",0,,False
61,",",0,,False
62,(k1,0,,False
63,+,0,,False
64,1),0,,False
65,·,0,,False
66,"tf (t,d)",0,,False
67,1-b+b·,0,,False
68,|d| avdl,0,,False
69,"k + tf (t,d)",0,,False
70,1,0,,False
71,1-b+b·,0,,False
72,|d| avdl,0,,False
73,",",0,,False
74,"(k1 + 1) · tf (t, d)",0,,False
75,k1,0,,False
76,·,0,,False
77,[1,0,,False
78,-,0,,False
79,b,0,,False
80,+,0,,False
81,b,0,,False
82,·,0,,False
83,|d| avdl,0,,False
84,],0,,False
85,+,0,,False
86,"tf (t, d)",0,,False
87,","" (k1 + 1) · tf (t, d)""",0,,False
88,(6),0,,False
89,"K + tf (t, d)",0,,False
90,where,0,,False
91,K,0,,False
92,",",0,,False
93,k1,0,,False
94,·,0,,False
95,(1,0,,False
96,-,0,,False
97,b,0,,False
98,+,0,,False
99,b,0,,False
100,·,0,,False
101,|d| avdl,0,,False
102,),0,,False
103,as,0,,False
104,defined,0,,False
105,in,0,,False
106,[10].,0,,False
107,"Note that under this form, it is not obvious that TFkp is",0,,False
108,really a composition of two functions with the same proper-,0,,False
109,ties as the one in TFpl. We think this is the main reason,0,,False
110,why composition has never been considered before. By do-,0,,False
111,"ing so, we provide not only a way to fully explain BM25 as a",0,,False
112,TF×IDF weighting scheme but also a way to easily consider,0,,False
113,variants of a weighting model by simply changing the order,0,,False
114,"of composition. As we will see in section 3, this led us to a",0,,False
115,new TF×IDF weighting scheme that outperforms BM25.,0,,False
116,2.3 Inverse Document Frequency,0,,False
117,"While the higher the frequency of a term in a document is, the more salient this term is supposed to be, this is no longer true at the collection level. This is actually quite the inverse since these terms have a presumably lower discrimination power. Hence, the use of a function of the term specificity, namely the Inverse Document Frequency (IDF) as defined in [14] and expressed as follows:",0,,False
118,N +1,0,,False
119,"IDF (t) , log",0,,False
120,(7),0,,False
121,df (t),0,,False
122,where N is the number of documents in the collection and df (t) the document frequency of the term t.,0,,False
123,2.4 TF-IDF versus BM25,0,,False
124,"By TF-IDF, we refer to the TF×IDF weighting model defined in [13], often called pivoted normalization weighting. The weighting model corresponds to TFpl×IDF:",0,,False
125,"1 + ln[1 + ln[tf (t, d)]]",0,,False
126,N +1,0,,False
127,"T F -IDF (t, d) ,",0,,False
128,1,0,,False
129,-,0,,False
130,b,0,,False
131,+,0,,False
132,b,0,,False
133,·,0,,False
134,|d| avdl,0,,False
135,× log,0,,False
136,(8),0,,False
137,df (t),0,,False
138,"By BM25, we refer to the scoring function defined in [10], often called Okapi weighting. It corresponds to TFkp×IDF when we omit QF (k-concavity of parameter k3 for tf (t, q)). Thus, it has an inverse order of composition between the",0,,False
139,918,0,,False
140,"concavity and the document length normalization compared to TF-IDF. The within-document scoring function of BM25 is written as follows when using the IDF formula defined in subsection 2.3 to avoid negative values, following [3]:",0,,False
141,"BM 25(t, d) ,"" (k1 + 1) · tf (t, d) × log N + 1 (9)""",0,,False
142,"K + tf (t, d)",0,,False
143,df (t),0,,False
144,2.5 Lower-bounding TF normalization,0,,False
145,"Through composition, we also allow additional constraints",1,ad,True
146,for the TF component to be satisfied easily. Subadditiv-,1,ad,True
147,ity is for instance a desirable property ­ if two documents,0,,False
148,"have the same total occurrences of all query terms, a higher",0,,False
149,score should be given to the document covering more dis-,0,,False
150,"tinct query terms. Here, it just happens that TFl and TFk",0,,False
151,already satisfy it as noted in [3].,1,ad,True
152,"Recently, Lv and Zhai introduced in [6] two new con-",0,,False
153,straints to the work of Fang et al. to lower-bound the TF,0,,False
154,"component. In particular, there should be a sufficiently large",0,,False
155,gap in the score between the presence and absence of a query,0,,False
156,term even for very long documents where TFp tends to 0 and,0,,False
157,"a fortiori the overall score too. Mathematically, this corre-",0,,False
158,sponds to be composing with a third function TF that is,0,,False
159,always composed after TFp since it compensates the poten-,0,,False
160,tial null limit introduced by TFp and it is defined as follows:,0,,False
161,{,0,,False
162,"tf (t, d) +  if tf (t, d) > 0",0,,False
163,"T F(t, d) , 0",0,,False
164,(10) otherwise,0,,False
165,"where  is the gap, set to 0.5 if TF is composed immediately after TFp and 1 if concavity is applied in-between. These are the two values defined in the original papers [6, 7] and we just interpreted their context of use in terms of order of composition. We did not change nor tune the values.",0,,False
166,"The weighting models Piv+ and BM25+ defined in [6] correspond respectively to TFpl×IDF and TFkp×IDF while BM25L defined in [7] to TFkp×IDF. We clearly see that the only difference between BM25+ and BM25L is the order of composition: this is one of the advantages of our framework ­ easily represent and compute multiple variants of a same general weighting model. In the experiments, we considered all the possible orders of composition between TFk or TFl, TFp and TF with the condition that TFp always precedes TF as explained before.",1,ad,True
167,"For instance, we will consider a novel model TFlp×IDF defined as follows:",0,,False
168,"tf (t, d)",0,,False
169,"T Flp ×IDF (t, d) , 1+ln[1+ln[ 1-b+b·",0,,False
170,|d| +]],0,,False
171,avdl,0,,False
172,(11),0,,False
173,where b is set to 0.20 and  to 0.5.,0,,False
174,3. EXPERIMENTS,0,,False
175,"Following our mathematical framework that relies on composition, we wondered why the order of composition was different between two widely used scoring functions ­ TF-IDF and BM25. In the original papers [11, 9], there was no mention of the difference in the order and this motivated us to investigate the matter. Our initial thought was that using an inverse order of composition in BM25 could improve it or vice-versa for TF-IDF. As a consequence, we tried exhaustively the combinations among TFk, TFl and TFp and report the results. Thereafter, as mentioned in subsection 2.5, we followed the same procedure considering a third function to compose with: TF. Indeed, we wanted to explore",0,,False
176,"the extensions considered by the research community [6, 7] in terms of composition. This led us to a novel weighting model that outperforms them (see subsection 3.4).",0,,False
177,3.1 Datasets and evaluation,0,,False
178,"We used two TREC collections to carry out our experiments: Disks 4&5 (minus the Congressional Record) and WT10G. Disks 4&5 contains 528,155 news releases while WT10G consists of 1,692,096 crawled pages from a snapshot of the Web in 1997. For each collection, we used a set of TREC topics (title only to mimic Web queries) and their associated relevance judgments: 301-450 and 601-700 for Disks 4&5 (TREC 2004 Robust Track) and 451-550 for WT10G (TREC9-10 Web Tracks).",1,TREC,True
179,"We evaluated the scoring functions in terms of Mean Average Precision (MAP) and Precision at 10 (P@10) considering only the top-ranked 1000 documents for each run. Our goal is to compare weighting models that use the same functions but with a different order of composition and select the best ones on both metrics. For example, in Table 1, TFpl×IDF is compared with TFlp×IDF and TFkp×IDF with TFpk×IDF. The statistical significance of improvement was assessed using the Student's paired t-test considering p-values less than 0.01 to reject the null hypothesis.",1,MAP,True
180,3.2 Platform and models,0,,False
181,"We have been using Terrier version 3.5 [8] to index, retrieve and evaluate over the TREC collections. For both datasets, the preprocessing steps involved Terrier's built-in stopword removal and Porter's stemming. We did not tune the slope parameter b of the pivoted document length normalization on each dataset. We set it to the default value suggested in the original papers: 0.20 when used with logconcavity [13] and 0.75 when used with k-concavity [10].",1,TREC,True
182,3.3 Results for TF-IDF versus BM25,0,,False
183,"We report in Table 1 the results we obtained on the aforementioned datasets when considering concavity and pivoted document length normalization. To the best of our knowledge, experiments regarding the same functions (TFk, TFl and TFp) with a different order of composition have never been reported before. They indeed show that the original order chosen for both TF-IDF and BM25 is the most effective one: TFpl×IDF outperforms TFlp×IDF and TFkp×IDF outperforms TFpk×IDF. But since the order is different between the two, this also indicates that the order does matter depending on which function is chosen for each property.",0,,False
184,"For these two models (TF-IDF and BM25), the use of a different concave function to meet the exact same constraints requires the pivoted document length normalization to be applied before or after the function. The impact is even more significant on the Web dataset (WT10G) that corresponds the most to contemporary collections of documents.",1,WT,True
185,3.4 Results for lower-bounding normalization,0,,False
186,"In Table 2, we considered in addition the lower-bounding normalization function TF defined in subsection 2.5. The best-performing weighting model on both datasets is a novel one ­ TFlp×IDF ­ and it even outperforms BM25+ and BM25L (significantly using the t-test and p < 0.01). This model has never been considered before in the literature to the best of our knowledge. In fact, the results from Table 1 establish that TFl should apparently be applied before TFp",1,ad,True
187,919,0,,False
188,Table 1: TF-IDF vs. BM25: an inverse order of,0,,False
189,composition; bold indicates significant performances,0,,False
190,Weighting model,0,,False
191,TREC 2004 Robust TREC9-10 Web MAP P@10 MAP P@10,1,TREC,True
192,IDF,0,,False
193,0.1396 0.2040 0.0539 0.0729,0,,False
194,TF,0,,False
195,0.0480 0.0867 0.0376 0.0833,0,,False
196,"TFp [b,0.20] TFp [b,0.75] TFl TFk",0,,False
197,0.0596 0.0640 0.1591 0.1768,0,,False
198,0.1193 0.1289 0.3141 0.3269,0,,False
199,0.0531 0.0473 0.1329 0.1522,0,,False
200,0.1021 0.1000 0.2063 0.2104,0,,False
201,TFpk TFlp TFpl TFkp,0,,False
202,0.0767 0.1645 0.1797 0.2045,0,,False
203,0.1932 0.3651 0.3647 0.3863,0,,False
204,0.0465 0.0622 0.1260 0.1702,0,,False
205,0.0604 0.1854 0.1875 0.2208,0,,False
206,TFpk ×IDF TFlp×IDF TFpl×IDF [TF-IDF] TFkp×IDF [BM25],0,,False
207,0.1034 0.1939 0.2132 0.2368,0,,False
208,0.2293 0.3964 0.4064 0.4161,0,,False
209,0.0507 0.0750 0.1430 0.1870,0,,False
210,0.0833 0.2125 0.2271 0.2479,0,,False
211,"like in TF-IDF. With lower-bounding normalization, it no longer holds. The formula for TFlp×IDF was given in equation 11. Without the use of our formal framework and composition, it would have been harder to detect and test these variants that can outperform state-of-the-art scoring functions when the order of composition is chosen carefully.",0,,False
212,Table 2: TFlp×IDF vs. BM25+ and BM25L; bold indicates significant performances.,0,,False
213,Weighting model,0,,False
214,TREC 2004 Robust TREC9-10 Web MAP P@10 MAP P@10,1,TREC,True
215,TFpk TFlp TFlp TFpl TFkp TFkp TFpk ×IDF TFlp×IDF TFlp×IDF TFpl×IDF [Piv+] TFkp×IDF [BM25L] TFkp×IDF [BM25+],0,,False
216,0.1056 0.1807 0.2130 0.2002 0.2155 0.2165 0.1466 0.2096 0.2495 0.2368 0.2472 0.2466,0,,False
217,0.2349 0.3751 0.4064 0.3876 0.3936 0.3956 0.2723 0.4048 0.4305 0.4157 0.4217 0.4145,0,,False
218,0.0556 0.0668 0.1907 0.1436 0.1806 0.1835 0.0715 0.0806 0.2084 0.1643 0.2000 0.2026,0,,False
219,0.0771 0.2021 0.2625 0.2021 0.2292 0.2354 0.1000 0.2292 0.2771 0.2438 0.2563 0.2521,0,,False
220,4. CONCLUSIONS AND FUTURE WORK,0,,False
221,"Scoring function design is a cornerstone issue in information retrieval. In this short paper, we intended to provide new insights on scoring functions for ad hoc IR. In particular, we proposed a unifying mathematical framework that explains how weighting models articulate around a set of heuristic retrieval constraints introduced in related work.",1,ad,True
222,"Using composition to combine the successive normalizations historically applied to the term frequency, we were able to fully explain BM25 as a TF×IDF weighting scheme with just an inverse order of composition between the concavity and the document length normalization compared to TF-IDF. Besides, the framework also allowed us to discover and report a novel weighting model ­ TFlp×IDF ­ that consistently and significantly outperformed BM25 and its extensions on two standard datasets in MAP and P@10.",1,MAP,True
223,Future work might involve the design of novel retrieval constraints and their compositions with existing ones. We,0,,False
224,are confident that refining the mathematical properties behind scoring functions will continue to improve the effectiveness of these models in ad hoc IR.,1,ad,True
225,5. ACKNOWLEDGMENTS,0,,False
226,We thank the anonymous reviewers for their useful feedbacks. This material is based upon work supported by the French DIGITEO Chair grant LEVETONE.,0,,False
227,6. REFERENCES,0,,False
228,"[1] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems, 20(4):357­389, Oct. 2002.",0,,False
229,"[2] S. Clinchant and E. Gaussier. Information-based models for ad hoc IR. In Proceedings of SIGIR'10, pages 234­241, 2010.",1,ad,True
230,"[3] H. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. In Proceedings of SIGIR'04, pages 49­56, 2004.",0,,False
231,"[4] H. P. Luhn. A statistical approach to mechanized encoding and searching of literary information. IBM Journal of Research and Development, 1(4):309­317, Oct. 1957.",0,,False
232,"[5] Y. Lv and C. Zhai. Adaptive term frequency normalization for BM25. In Proceedings of CIKM'11, pages 1985­1988, 2011.",0,,False
233,"[6] Y. Lv and C. Zhai. Lower-bounding term frequency normalization. In Proceedings of CIKM'11, pages 7­16, 2011.",0,,False
234,"[7] Y. Lv and C. Zhai. When documents are very long, BM25 fails! In Proceedings of SIGIR'11, pages 1103­1104, 2011.",0,,False
235,"[8] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A high performance and scalable information retrieval platform. In Proceedings of SIGIR'06, 2006.",0,,False
236,"[9] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of SIGIR'94, pages 232­241, 1994.",0,,False
237,"[10] S. E. Robertson, S. Walker, K. Spärck Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In Proceedings of the TREC-3, pages 109­126, 1994.",1,TREC,True
238,"[11] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513­523, 1988.",0,,False
239,"[12] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proceedings of SIGIR'96, pages 21­29, 1996.",0,,False
240,"[13] A. Singhal, J. Choi, D. Hindle, D. Lewis, and F. Pereira. AT&T at TREC-7. In Proceedings of TREC-7, pages 239­252, 1999.",1,TREC,True
241,"[14] K. Spärck Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11­20, 1972.",0,,False
242,"[15] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR'01, pages 334­342, 2001.",1,ad,True
243,920,0,,False
244,,0,,False

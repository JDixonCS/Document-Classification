,sentence,label,data,regex
0,A Document Rating System for Preference Judgements,0,,False
1,"Maryam Bashir, Jesse Anderton, Jie Wu, Peter B. Golbus, Virgil Pavlu, Javed A. Aslam",0,,False
2,"College of Computer and Information Science, Northeastern University Boston, Massachusetts, USA",0,,False
3,"{maryam,jesse,evawujie,pgolbus,vip,jaa@ccs.neu.edu}",0,,False
4,ABSTRACT,0,,False
5,"High quality relevance judgments are essential for the evaluation of information retrieval systems. Traditional methods of collecting relevance judgments are based on collecting binary or graded nominal judgments, but such judgments are limited by factors such as inter-assessor disagreement and the arbitrariness of grades. Previous research has shown that it is easier for assessors to make pairwise preference judgments. However, unless the preferences collected are largely transitive, it is not clear how to combine them in order to obtain document relevance scores. Another difficulty is that the number of pairs that need to be assessed is quadratic in the number of documents. In this work, we consider the problem of inferring document relevance scores from pairwise preference judgments by analogy to tournaments using the Elo rating system. We show how to combine a linear number of pairwise preference judgments from multiple assessors to compute relevance scores for every document.",1,ad,True
6,Categories and Subject Descriptors,0,,False
7,H.3.3 [Information Storage and Retrieval ]: Information Search and Retrieval,0,,False
8,General Terms,0,,False
9,Theory,0,,False
10,Keywords,0,,False
11,"Evaluation, Preference Judgment",0,,False
12,1. INTRODUCTION,1,DUC,True
13,"Traditional methods of collecting relevance judgments make binary assumption about relevance i.e. a document is assumed to be either relevant or non-relevant to the information need of a user. This assumption turns relevance judgment into a classification problem. In the modern world,",1,ad,True
14,"We gratefully acknowledge support provided by NSF IIS1256172. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
15,"search engines can easily retrieve thousands of documents at least somewhat relevant to the user's information need. Therefore it becomes necessary to assign a ranking to these documents based on their degree of relevance. This somewhat more continuous notion of relevance cannot be expressed through binary relevance judgments; researchers have developed two ways to express non-binary relevance judgments: either consider relevance as a relative notion such that one document is more or less relevant than another document, or consider relevance as a quantitative notion and create multiple grades of relevance. The first notion of relevance can be expressed as pairwise preference judgments; the second notion can be expressed as nominal graded relevance judgments, which appear far more prevalently in the literature.",1,ad,True
16,"Graded relevance has two significant shortcomings. First, the total number of grades must be defined in advance, and it is not clear how this choice effects the relative measurement of system performance. Second, graded judgments require assessors to choose between arbitrarily defined grades, a choice on which different assessors can easily disagree. The alternative, pairwise preference judgments, allows the assessor to make a binary decision, freeing him or her from the difficulty of deciding between multiple relevance grades. Another advantage of using preferences is that many popular learning-to-rank algorithms, e.g. RankBoost and RankNet, are naturally trained on preferences; thus a better training set can be obtained from direct preference judgments, as opposed to pairwise preferences inferred from nominal judgments.",1,ad,True
17,"Pairwise preference judgments have not been explored extensively in the literature. There have been several attempts to use preference judgments by inferring them from absolute judgments [4] and from click data [8]. Nie et al. [9] used preferences for relevance assessments and showed that labelling effort can be reduced by focussing on top ranked documents. Chen et al. [2] also used preferences but focused more on estimating worker quality. To the best of our knowledge, the only work where assessors were asked for direct pairwise preferences as well as absolute relevance judgments for the comparison of the two assessment approaches is by Carterette et al. [1]. The authors showed that rate of interassessor agreement is higher on preference judgments, and that assessors take longer to make absolute judgments than preference judgments.",0,,False
18,"If a simple routine is to be used to infer document relevance from pairwise preferences, it is essential that the preferences be transitive, so that we may sort documents by",0,,False
19,909,0,,False
20,"preference and decide which and how many pairs to judge. Carterette et al., by collecting all O(n2) preference judgments found that the preferences they collected are transitive 99% of the time. However, the study used experts assessors. The critical property of transitivity might not hold when judgments are collected through the much noisier process of crowdsourcing.",0,,False
21,"In order to obtain document grades (or scores) from a smaller number of preference judgments, we draw an analogy to the tournament problem. In a typical tournament, pairs of players or teams compete in matches of one or more games. The desired outcome is a final ranking (or scoring) of each competitor. A common solution is to use the Elo rating system [3], in which players are assigned ratings which are updated iteratively each time the player competes in a match. Using the Elo rating system to combine preference judgments into document grades has the following benefits:",1,ad,True
22,1. The judgments do not need to be transitive. We cannot simply sort the documents by preference since humans assessors can be intransitive in their assessments; especially when we are combining preference judgments from noisy assessments (e.g. through crowdsourcing). The Elo rating system produces a ranking of documents even if the preferences are not transitive.,0,,False
23,"2. We do not need a quadratic number of pairwise assessments for inferring the relevance of documents. The Elo rating system can be applied to any number of assessments. Indeed, it can infer highly reliable relevance scores using only a linear number of pairwise assessments.",1,ad,True
24,"3. For any pair of documents, the document scores produced using the Elo rating system can be used to compute the likelihood of one document is more relevant than the other. In this way we can predict all O(n2) preferences while only collecting O(n) judgments.",0,,False
25,2. THE ELO RATING SYSTEM,0,,False
26,"The Elo rating system is a method for calculating the relative rating of players in two player games [3]. The system assigns each player a rating score, with a higher number indicating a better player. Each player's rating is updated after he or she has played a certain number of matches, increasing or decreasing in value depending on whether the player won or lost each match, and on the ratings of both players competing in each match--beating a highly rated player increases one's rating more than beating a player with a low rating, while losing to a player with a low rating decreases one's score more than losing to a player with a high rating. These scores are used in two ways: 1) players are ranked by their scores, and 2) the scores are used to compute the likelihood that one player will beat another. If the matches are selected intelligently, the stable ratings can be achieved after only O(n) matches played.",0,,False
27,"Given the two player's ratings before the match, denoted RA and RB, an expected match outcome is calculated for each player: EA and EB. The actual output of the match from the perspective of each player (since a win for player A is assumed to be a loss for player B) is denoted as SA and SB. The ratings are updated after each match, based on how the expected aligns with the actual outcome.",0,,False
28,"The Elo rating system can be applied directly to our problem by treating the documents as players, their scores as the ratings to be learned, and document-pairwise preference assessments as matches. All documents begin the ""tournament"" rated equally. After each document ""plays"" a match, we update its rating according to equation 2. Each match corresponds to a fixed number of assessors expressing a preference between the pair of documents. The actual outcome of the match for each document, S, is the number of assessors that preferred that document plus half the number of assessors who considered the documents to be ""tied."" After all the matches are played, we can rank the documents by their final score. This list can be thresholded to produce absolute relevance judgments. We can also use the scores directly to compute transitive preference judgments.",0,,False
29,2.1 Math Details of the Elo Rating System,0,,False
30,"If, before a match, document A has a rating of RA and document B has a rating of RB, then the expected outcome of the match according to the Elo rating system is:",0,,False
31,1,0,,False
32,EA,0,,False
33,",",0,,False
34,1,0,,False
35,+,0,,False
36,10,0,,False
37,RB,0,,False
38,-RA F,0,,False
39,;,0,,False
40,1,0,,False
41,EB,0,,False
42,",",0,,False
43,1,0,,False
44,+,0,,False
45,10,0,,False
46,RA -RB F,0,,False
47,(1),0,,False
48,where F is a rating disparity parameter used to control how quickly ratings can change.,0,,False
49,"If EA is greater than EB, then we expect document A to win the match. Once the match is played and we can observe SA and SB, the documents' Elo rating is updated as follows:",0,,False
50,"RA , RA + K(SA - EA); RB , RB + K(SB - EB) (2)",0,,False
51,where K is a game importance parameter that can be varied so as to give some matches more weight than others.,0,,False
52,2.1.1 Elo Rating with Variance,0,,False
53,"The Elo rating system assumes that the uncertainty about a player's skill rating does not change over time. Therefore, all skill rating updates are computed with the same variance, and any change in the uncertainty about the player's skills over time is not modeled. Glickman proposed to solve this problem by incorporating the variance over time in the player's skill rating [5]. Other researchers have used Glickman's system for the purpose of ranking documents based on clickthrough data [10]. Glickman presented the idea of modeling the belief about a player's skills as a Gaussian distribution whose mean corresponds to the player's rating. As a player plays more matches, the uncertainty about his her her skills is decreased, and this is reflected by a decrease in the variance of the player's associated Gaussian distribution. Rather than using equation 2, the mean rating (RA) and variance (2) of each document is updated using equation 3 and equation 4 as follows:",1,corpora,True
54,"RA , RA + Kg(B2 )(SA - EA)",0,,False
55,(3),0,,False
56,"g(2) ,",0,,False
57,1,0,,False
58,(4),0,,False
59,1,0,,False
60,+,0,,False
61,3q2 2 2,0,,False
62,"where,",0,,False
63,1,0,,False
64,EA,0,,False
65,",",0,,False
66,1,0,,False
67,+,0,,False
68,10-g(B2 ),0,,False
69,RB -RA F,0,,False
70,(5),0,,False
71,q,0,,False
72,"K,",0,,False
73,1 A2,0,,False
74,+,0,,False
75,1 2,0,,False
76,;,0,,False
77,"2 ,",0,,False
78,1 2,0,,False
79,1,0,,False
80,+,0,,False
81,1 2,0,,False
82,;,0,,False
83,"q , log 10 F",0,,False
84,(6),0,,False
85,910,0,,False
86,"2 , q2",0,,False
87,1,0,,False
88,"m j,1",0,,False
89,nj,0,,False
90,g(j2,0,,False
91,)2,0,,False
92,EA,0,,False
93,(1,0,,False
94,-,0,,False
95,EA),0,,False
96,(7),0,,False
97,"Throughout this work, we set F , 200. Each document is initialized with a mean of 100 and a variance of 10.",0,,False
98,2.2 Selection of Preference Pairs,0,,False
99,"For our preliminary experiments, we select O(n) matches stochastically. Each document in the list will be compared against five other documents. We wish to sample pairs in such a way that we create a bias towards relevant documents. In this way, relevant documents will play more matches than non-relevant documents, giving them more opportunities to improve their ratings and move up the list. First, we calculate an initial relevance score for each document using BM25. This produces an initial ranking of the documents for each topic. We collected complete pairwise preferences between the top six documents. For each document below the top six, we select five documents from the set of documents with higher BM25 scores, uniformly at random. We collected four worker assessments for each preference pair which we selected for judgment. We sort all documents based on their Elo ratings after all O(n) matches have been played.",0,,False
100,3. EXPERIMENTS,0,,False
101,"We will compare our methodology for collecting relevance grades from pairwise preferences to the results of the TREC 2012 Crowdsourcing track1. The goal of the track was to evaluate approaches to crowdsourcing high quality relevance judgments for text documents and images. Track participants were asked to provide new binary relevance grades, as well as probabilities of relevance, for 18,260 documents that had previously been judged with respect to ten topics selected randomly from the TREC 8 ad-hoc collection.",1,ad,True
102,3.1 Crowdsourcing,0,,False
103,"We crowdsourced our preference judgments using Amazon Mechanical Turk (AMT)2. Each crowd worker was shown the interface presented in Figure 1. Workers were shown the title, description, and narrative fields of a TREC topic, and presented with two documents. Worker's were asked which document ""does a better job of answering the questions at the top of the page."" They were allowed to select either document, as well as the options ""They're Equally Good"" and ""They're Equally Bad."" Internally, these latter two options were treated equivalently as ties. Each task, known on AMT as a HIT, consisted of 20 preference pairs for the same topic, and had a time limit of 30 minutes. Workers were paid $0.15 for each approved HIT. The order in which the document pairs were displayed, as well as which document appeared on which side of the interface, was randomized.",1,TREC,True
104,3.1.1 Quality Control,0,,False
105,"The workers we employed have no particular training in assessing document relevance. Therefore, we need a means of verifying the quality of their work. We used trap questions, a document pair for which the ""correct"" preference is already known, in our study to ensure that workers are giving us reasonable results, and not just clicking randomly. We asked five graduate students studying information retrieval to create our trap questions by pairing documents which",1,ad,True
106,1http://sites.google.com/site/treccrowd 2http://www.mturk.com,1,trec,True
107,ELO progress after the first iteration though all pairs,0,,False
108,9,0,,False
109,percent of inverted pairs within top 200 documents,0,,False
110,8,0,,False
111,427,0,,False
112,445,0,,False
113,417,0,,False
114,416,0,,False
115,7,0,,False
116,446,0,,False
117,447,0,,False
118,432,0,,False
119,6,0,,False
120,420,0,,False
121,411,0,,False
122,438,0,,False
123,5,0,,False
124,4,0,,False
125,3,0,,False
126,2,0,,False
127,1,0,,False
128,0,0,,False
129,2,0,,False
130,3,0,,False
131,4,0,,False
132,5,0,,False
133,6,0,,False
134,7,0,,False
135,8,0,,False
136,9,0,,False
137,10,0,,False
138,ELO iteration after the first one,0,,False
139,"Figure 2: Relationship of Number of Elo rating iterations to percent of pairs inverted, separately for each query.",0,,False
140,"they deemed highly relevant with documents they deemed highly non-relevant. We then inserted five of these trap questions, selected at random, into each HIT. As a result, each assignment consisted of five trap questions and fifteen ""real"" questions. Worker's submission were not accepted unless at least two of the five trap questions were answered correctly. Although, answering two of the five trap questions is not strict criteria but it makes sure that the worker's performance is not worse than random answers.",0,,False
141,"As another means of ensuring the quality of the collected judgments, we also employed Expectation Maximization (EM). In this context EM, is a means of estimating the ""true"" pairwise preferences from crowd workers as latent variables in a model of worker quality. For every pair of documents about which we collected judgments from workers, EM provides a probability that one document beats the other. EM has been shown to work well for aggregating labels from multiple crowd workers on AMT [7], and in particular with regarding to collecting relevance judgments [6].",0,,False
142,3.2 Iterations of Elo Rating,0,,False
143,"In Elo rating system, the score of each document depends on the score of its opponent document in a match. The order in which matches are played has an impact on scores of documents. For example, if a document wins a match against a relevant document, and the relevant document has not played any match yet, then the score of the document would not increase significantly. If the relevant document has already played few matches and has raised its score, then wining a match against it would increase the score of a document to a large extent. Because of this, if we run only one iteration of Elo rating algorithm (through all pairs) then some document scores may not be reliable; we instead run several iterations of Elo rating algorithm so that scores of documents converge. Figure 2 shows the relationship of number of Elo rating iterations to percentage of pairs inverted, after the initial run through all pairs. Note that as we run more iterations, the percentage of pairs whose order is changed decreases.",1,ad,True
144,3.3 Baseline,0,,False
145,"In order to measure the quality of our Elo-based system, we also implemented a naive system as a baseline. In our naive system, each document is given a score based on the percentage of its matches that it won and the number of matches it competed in. The score of a document A is calculated as:",0,,False
146,"score(A) ,  winsA + (1 - ) matchesA",0,,False
147,(8),0,,False
148,matchesA,0,,False
149,matches,0,,False
150,911,0,,False
151,Figure 1: Preference pair selection interface,0,,False
152,Topic ID,0,,False
153,# Documents # Relevant in Collection Documents,0,,False
154,Median Score of TREC Baseline Participant Runs,1,TREC,True
155,AUC Elo Without Vari- Elo ance,0,,False
156,Elo+EM,0,,False
157,411,0,,False
158,2056,0,,False
159,416,0,,False
160,1235,0,,False
161,417,0,,False
162,2992,0,,False
163,420,0,,False
164,1136,0,,False
165,427,0,,False
166,1528,0,,False
167,432,0,,False
168,2503,0,,False
169,438,0,,False
170,1798,0,,False
171,445,0,,False
172,1404,0,,False
173,446,0,,False
174,2020,0,,False
175,447,0,,False
176,1588,0,,False
177,27,0,,False
178,0.86,0,,False
179,42,0,,False
180,0.85,0,,False
181,75,0,,False
182,0.75,0,,False
183,33,0,,False
184,0.71,0,,False
185,50,0,,False
186,0.73,0,,False
187,28,0,,False
188,0.71,0,,False
189,173,0,,False
190,0.78,0,,False
191,62,0,,False
192,0.83,0,,False
193,162,0,,False
194,0.82,0,,False
195,16,0,,False
196,0.76,0,,False
197,0.809 0.919 0.848 0.808 0.864 0.544 0.725 0.750 0.700 0.935,0,,False
198,0.811 0.940 0.897 0.834 0.871 0.536 0.731 0.748 0.716 0.995,0,,False
199,0.857 0.944 0.887 0.823 0.882 0.637 0.708 0.790 0.720 0.859,0,,False
200,0.862 0.939 0.914 0.853 0.907 0.558 0.774 0.843 0.865 1.000,0,,False
201,All,0,,False
202,18260,0,,False
203,668,0,,False
204,Not Reported,0,,False
205,0.790,0,,False
206,0.808,0,,False
207,0.811 0.851,0,,False
208,"Table 1: Evaluation Results using AUC for Preference based Relevance Judgements. Elo+EM is statistically significantly better than Baseline, Elo is not significantly better than baseline.",0,,False
209,"where winsA is number of matches won by document A, matchesA is total number of matches played by a document A, and matches is total number of matches played. Since we did not have enough data to properly tune ,  is set to 0.5.",0,,False
210,3.4 Results,0,,False
211,"Table 1 shows the Area Under the ROC Curve (AUC), one of the primary measures used in the TREC 2012 Crowdsourcing, of our Elo and Baseline systems, with and without EM, and the median scores of the 33 systems that participated in the Crowdsourcing track. For most topics, our Elo-based system outperforms both the Baseline naive system and the median TREC participant. When we also use EM, our results improve. The results using Elo+EM are significantly3 better than the simple baseline.",1,TREC,True
212,4. CONCLUSION AND FUTURE WORK,0,,False
213,"Preference judgments are easier for assessors to produce and are more useful for training learning-to-rank algorithms. However, their use has been limited due to the polynomial increase in the number of judgments that need to be collected. In this work, we have shown how the Elo rating system can be used to combine a linear number of preferences to obtain either an ordered list of documents or document relevance scores. The results of our experiments are encouraging and demonstrate the potential of our Elo-based system for inferring the relevance of documents from a linear number of pairwise preference judgments.",0,,False
214,"In future work, we plan to use active learning to intelligently select which pairs of documents to judge in an online manner. The pairwise preference judgments collected",0,,False
215,3Statistical significance is determined using a two-tailed TTest and is measured at a significance level of 0.05.,0,,False
216,in each phase of active learning will dictate which pairs are selected to be judged in the next phase.,0,,False
217,5. REFERENCES,0,,False
218,"[1] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. Here or there. In ECIR, 2008.",0,,False
219,"[2] X. Chen, P. N. Bennett, K. Collins-Thompson, and E. Horvitz. Pairwise ranking aggregation in a crowdsourced setting. In Proceedings of WSDM. ACM, 2013.",0,,False
220,"[3] A. Elo and S. Sloan. The Rating of Chess Players, Past and Present. Arco Publishing, 1978.",0,,False
221,"[4] H. P. Frei and P. Schauble. Determining the effectiveness of retrieval algorithms. Inf. Process. Manage., 27(2-3), 1991.",0,,False
222,"[5] M. E. Glickman. Parameter estimation in large dynamic paired comparison experiments. In Applied Statistics, pages 48­377, 1999.",0,,False
223,"[6] M. Hosseini, I. J. Cox, N. Mili´c-Frayling, G. Kazai, and V. Vinay. On aggregating labels from multiple crowd workers to infer relevance of documents. In ECIR. Springer-Verlag, 2012.",0,,False
224,"[7] P. G. Ipeirotis, F. Provost, and J. Wang. Quality management on Amazon Mechanical Turk. In SIGKDD Workshop on Human Computation. ACM, 2010.",0,,False
225,"[8] T. Joachims. Optimizing search engines using clickthrough data. In SIGKDD. ACM, 2002.",0,,False
226,"[9] S. Niu, J. Guo, Y. Lan, and X. Cheng. Top-k learning to rank: labeling, ranking and evaluation. In Proceedings of SIGIR. ACM, 2012.",0,,False
227,"[10] F. Radlinski and T. Joachims. Active exploration for learning rankings from clickthrough data. In Proceedings of SIGKDD. ACM, 2007.",1,ad,True
228,912,0,,False
229,,0,,False

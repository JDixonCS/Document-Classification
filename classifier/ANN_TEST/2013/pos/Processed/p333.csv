,sentence,label,data,regex
0,Ranking Document Clusters Using Markov Random Fields,0,,False
1,Fiana Raiber fiana@tx.technion.ac.il,0,,False
2,Oren Kurland kurland@ie.technion.ac.il,0,,False
3,"Faculty of Industrial Engineering and Management, Technion Haifa 32000, Israel",0,,False
4,ABSTRACT,0,,False
5,"An important challenge in cluster-based document retrieval is ranking document clusters by their relevance to the query. We present a novel cluster ranking approach that utilizes Markov Random Fields (MRFs). MRFs enable the integration of various types of cluster-relevance evidence; e.g., the query-similarity values of the cluster's documents and queryindependent measures of the cluster. We use our method to re-rank an initially retrieved document list by ranking clusters that are created from the documents most highly ranked in the list. The resultant retrieval effectiveness is substantially better than that of the initial list for several lists that are produced by effective retrieval methods. Furthermore, our cluster ranking approach significantly outperforms stateof-the-art cluster ranking methods. We also show that our method can be used to improve the performance of (stateof-the-art) results-diversification methods.",0,,False
6,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,0,,False
7,"General Terms: Algorithms, Experimentation",0,,False
8,"Keywords: ad hoc retrieval, cluster ranking, query-specific clusters, markov random fields",1,ad,True
9,1. INTRODUCTION,1,DUC,True
10,The cluster hypothesis [33] gave rise to a large body of work on using query-specific document clusters [35] for improving retrieval effectiveness. These clusters are created from documents that are the most highly ranked by an initial search performed in response to the query.,0,,False
11,"For many queries there are query-specific clusters that contain a very high percentage of relevant documents [8, 32, 25, 14]. Furthermore, positioning the constituent documents of these clusters at the top of the result list yields highly effective retrieval performance; specifically, much better than that of state-of-the art retrieval methods that rank documents directly [8, 32, 25, 14, 10].",0,,False
12,"As a result of these findings, there has been much work on ranking query-specific clusters by their presumed relevance",0,,False
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",1,ad,True
14,"to the query (e.g., [35, 22, 24, 25, 26, 14, 15]). Most previous approaches to cluster ranking compare a representation of the cluster with that of the query. A few methods integrate additional types of information such as inter-cluster and cluster-document similarities [18, 14, 15]. However, there are no reports of fundamental cluster ranking frameworks that enable to effectively integrate various information types that might attest to the relevance of a cluster to a query.",1,ad,True
15,"We present a novel cluster ranking approach that uses Markov Random Fields. The approach is based on integrating various types of cluster-relevance evidence in a principled manner. These include the query-similarity values of the cluster's documents, inter-document similarities within the cluster, and measures of query-independent properties of the cluster, or more precisely, of its documents.",0,,False
16,"A large array of experiments conducted with a variety of TREC datasets demonstrates the high effectiveness of using our cluster ranking method to re-rank an initially retrieved document list. The resultant retrieval performance is substantially better than that of the initial ranking for several effective rankings. Furthermore, our method significantly outperforms state-of-the-art cluster ranking methods. Although the method ranks clusters of similar documents, we show that using it to induce document ranking can help to substantially improve the effectiveness of (state-of-the-art) retrieval methods that diversify search results.",1,TREC,True
17,2. RETRIEVAL FRAMEWORK,0,,False
18,Suppose that some search algorithm was employed over a corpus of documents in response to a query. Let Dinit be the list of the initially highest ranked documents. Our goal is to re-rank Dinit so as to improve retrieval effectiveness.,0,,False
19,"To that end, we employ a standard cluster-based retrieval paradigm [34, 24, 18, 26, 15]. We first apply some clustering method upon the documents in Dinit; C l(Dinit) is the set of resultant clusters. Then, the clusters in C l(Dinit) are ranked by their presumed relevance to the query. Finally, the clusters' ranking is transformed to a ranking of the documents in Dinit by replacing each cluster with its constituent documents and omitting repeats in case the clusters overlap. Documents in a cluster are ordered by their query similarity.",1,ad,True
20,"The motivation for employing the cluster-based approach just described follows the cluster hypothesis [33]. That is, letting similar documents provide relevance status support to each other by the virtue of being members of the same clusters. The challenge that we address here is devising a (novel) cluster ranking method -- i.e., we tackle the second step of the cluster-based retrieval paradigm.",1,ad,True
21,333,0,,False
22,"Figure 1: The three types of cliques considered for graph G. G is composed of a query node (Q) and three (for the sake of the example) nodes (d1, d2, and d3) that correspond to the documents in cluster C. (i) lQD contains the query and a single document from C; (ii) lQC contains all nodes in G; and, (iii) lC contains only the documents in C.",0,,False
23,"Formally, let C and Q denote random variables that take as values document clusters and queries respectively. The cluster ranking task amounts to estimating the probability that a cluster is relevant to a query, p(C|Q):",0,,False
24,p(C |Q),0,,False
25,",",0,,False
26,"p(C, Q) p(Q)",0,,False
27,"ra,nk",0,,False
28,"p(C, Q).",0,,False
29,(1),0,,False
30,The rank equivalence holds as clusters are ranked with respect to a fixed query.,0,,False
31,"To estimate p(C, Q), we use Markov Random Fields (MRFs). As we discuss below, MRFs are a convenient framework for integrating various types of cluster-relevance evidence.",0,,False
32,2.1 Using MRFs to rank document clusters,0,,False
33,"An MRF is defined over a graph G. Nodes represent random variables and edges represent dependencies between these variables. Two nodes that are not connected with an edge correspond to random variables that are independent of each other given all other random variables. The set of nodes in the graph we construct is composed of a node representing the query and nodes representing the cluster's constituent documents. The joint probability over G's nodes, p(C, Q), can be expressed as follows:",0,,False
34,"p(C, Q) , lL(G) l(l) ;",0,,False
35,(2),0,,False
36,Z,0,,False
37,"L(G) is the set of cliques in G and l is a clique; l(l) is a potential (i.e., positive function) defined over l; Z ,",0,,False
38,"C,Q lL(G) l(l) is the normalization factor that serves to ensure that p(C, Q) is a probability distribution. The normalizer need not be computed here as we rank clusters with respect to a fixed query.",0,,False
39,A common instantiation of potential functions is [28]:,0,,False
40,"l(l) d,""ef exp(lfl(l)),""",0,,False
41,"where fl(l) is a feature function defined over the clique l and l is the weight associated with this function. Accordingly, omitting the normalizer from Equation 2, applying the rank-preserving log transformation, and substituting the potentials with the corresponding feature functions results in our ClustMRF cluster ranking method:",0,,False
42,"p(C|Q) ra,nk",0,,False
43,l fl (l).,0,,False
44,(3),0,,False
45,lL(G),0,,False
46,"This is a generic linear (in feature functions) cluster ranking function that depends on the graph G. To instantiate a specific ranking method, we need to (i) determine G's structure,",0,,False
47,"specifically, its clique set L(G); and, (ii) associate feature functions with the cliques. We next address these two tasks.",1,ad,True
48,2.1.1 Cliques and feature functions,0,,False
49,We consider three types of cliques in the graph G. These are depicted in Figure 1. In what follows we write d  C to indicate that document d is a member of cluster C.,0,,False
50,"The first clique (type), lQD, contains the query and a single document in the cluster. This clique serves for making inferences based on the query similarities of the cluster's constituent documents when considered independently. The second clique, lQC , contains all nodes of the graph; that is, the query Q and all C's constituent documents. This clique is used for inducing information from the relations between the query-similarity values of the cluster's constituent documents. The third clique, lC, contains only the cluster's constituent documents. It is used to induce information based on query-independent properties of the cluster's documents.",0,,False
51,"In what follows we describe the feature functions defined over the cliques. In some cases a few feature functions are defined for the same clique, and these are used in the summation in Equation 3. Note that the sum of feature functions is also a feature function. The weights associated with the feature functions are set using a train set of queries. (Details are provided in Section 4.1.)",0,,False
52,The lQD clique. High query similarity exhibited by C's,0,,False
53,constituent documents can potentially imply to C's rele-,0,,False
54,"vance [26]. Accordingly, let d ( C) be the document in",0,,False
55,lQD .,0,,False
56,We,0,,False
57,define fgeo-qsim;lQD (lQD),0,,False
58,"d,ef",0,,False
59,1,0,,False
60,"log sim(Q, d) |C| ,",0,,False
61,"where |C| is the number of documents in C, and sim(·, ·) is",0,,False
62,"some inter-text similarity measure, details of which are pro-",0,,False
63,vided in Section 4.1. Using this feature function in Equation,0,,False
64,3 for all the lQD cliques of G amounts to using the geometric,0,,False
65,mean of the query-similarity values of C's constituent docu-,0,,False
66,ments. All feature functions that we consider use logs so as,0,,False
67,to have a conjunction semantics for the integration of their,0,,False
68,assigned values when using Equation 3.1,0,,False
69,The lQC clique. Using the lQD clique from above results,0,,False
70,"in considering the query-similarity values of the cluster's documents independently of each other. In contrast, the lQC clique provides grounds for utilizing the relations between these similarity values. Specifically, we use the log",0,,False
71,"1Before applying the log function we employ add- (, 10-10) smoothing.",1,ad,True
72,334,0,,False
73,"of the minimal, maximal, and standard deviation2 of the {sim(Q, d)}dC values as feature functions for lQC, denoted min-qsim, max-qsim, and stdv-qsim, respectively.",0,,False
74,"The lC clique. Heretofore, the lQD and lQC cliques served",0,,False
75,for inducing information from the query similarity values of,0,,False
76,C's documents. We now consider query-independent proper-,0,,False
77,ties of C that can potentially attest to its relevance. Doing so,0,,False
78,amounts to defining feature functions over the lC clique that,0,,False
79,contains C's documents but not the query. All the feature,0,,False
80,functions that we define for lC are constructed as follows.,0,,False
81,"We first define a query-independent document measure, P,",0,,False
82,and apply it to document d ( C) yielding the value P(d).,0,,False
83,"Then, we use log A({P(d)}dC) where A is an aggregator function: minimum, maximum, and geometric mean. The",0,,False
84,"resultant feature functions are referred to as min-P, max-",0,,False
85,"P, and geo-P, respectively. We next describe the document",0,,False
86,measures that serve as the basis for the feature functions.,0,,False
87,The cluster hypothesis [33] implies that relevant docu-,0,,False
88,"ments should be similar to each other. Accordingly, we mea-",0,,False
89,sure for document d in C its similarity with all documents,0,,False
90,in C:,0,,False
91,"Pdsim(d) d,ef",0,,False
92,1 |C|,0,,False
93,"diC sim(d, di).",0,,False
94,The next few query-independent document measures are,0,,False
95,based on the following premise. The higher the breadth of,1,ad,True
96,"content in a document, the higher the probability it is rel-",0,,False
97,"evant to some query. Thus, a cluster containing documents",0,,False
98,with broad content should be assigned with relatively high,1,ad,True
99,probability of being relevant to some query.,0,,False
100,High entropy of the term distribution in a document is a,0,,False
101,"potential indicator for content breadth [17, 3]. This is be-",1,ad,True
102,"cause the distribution is ""spread"" over many terms rather",1,ad,True
103,"than focused over a few ones. Accordingly, we define",0,,False
104,"Pentropy(d) d,""ef - wd p(w|d) log p(w|d), where w is a term and p(w|d) is the probability assigned to w by an unsmoothed""",0,,False
105,"unigram language model (i.e., maximum likelihood estimate)",0,,False
106,induced from d.,0,,False
107,"Inspired by work on Web spam classification [9], we use",0,,False
108,"the inverse compression ratio of document d, Picompress(d),",0,,False
109,as an additional measure. (Gzip is used for compression.),1,ad,True
110,High compression ratio presumably attests to reduced con-,0,,False
111,tent breadth [9].,1,ad,True
112,Two additional content-breadth measures that were pro-,1,ad,True
113,posed in work on Web retrieval [3] are the ratio between the,0,,False
114,"number of stopwords and non-stopwords in the document,",0,,False
115,"Psw1(d); and, the fraction of stopwords in a stopword list",0,,False
116,"that appear in the document, Psw2(d). We use INQUERY's",0,,False
117,stopword list [2]. A document containing many stopwords,0,,False
118,is presumably of richer language (and hence content) than,0,,False
119,"a document that does not contain many of these; e.g., a",0,,False
120,document containing a table composed only of keywords [3].,0,,False
121,For some of the Web collections used for evaluation in,0,,False
122,"Section 4, we also use the PageRank score [4] of the docu-",0,,False
123,"ment, Ppr(d), and the confidence level that the document is",0,,False
124,"not spam, Pspam(d). The details of the spam classifier are",0,,False
125,provided in Section 4.1.,0,,False
126,We note that using the feature functions that result from,0,,False
127,applying the geometric mean aggregator upon the query-,0,,False
128,"independent document measures just described, except for",0,,False
129,"2It was recently argued that high variance of the querysimilarity values of the cluster's documents might be an indicator for the cluster's relevance, as it presumably attests to a low level of ""query drift"" [19].",0,,False
130,"dsim, could have been described in an alternative way. That",0,,False
131,1,0,,False
132,"is, using log P(d) |C| as a feature function over a clique con-",0,,False
133,"taining a single document. Then, using these feature functions in Equation 3 amounts to using the geometric mean.3",0,,False
134,3. RELATED WORK,0,,False
135,"The work most related to ours is that on devising cluster ranking methods. The standard approach is based on measuring the similarity between a cluster representation and that of the query [7, 34, 35, 16, 24, 25, 26]. Specifically, a geometric-mean-based cluster representation was shown to be highly effective [26, 30, 15]. Indeed, ranking clusters by the geometric mean of the query-similarity values of their constituent documents is a state-of-the-art cluster ranking approach [15]. This approach rose as an integration of feature functions used in ClustMRF, and is shown in Section 4 to substantially underperform ClustMRF.",0,,False
136,"Clusters were also ranked by the highest query similarity exhibited by their constituent documents [22, 31] and by the variance of these similarities [25, 19]. ClustMRF incorporates these methods as feature functions and is shown to outperform each.",1,corpora,True
137,"Some cluster ranking methods use inter-cluster and clusterdocument similarities [14, 15]. While ClustMRF does not utilize such similarities, it is shown to substantially outperform one such state-of-the-art method [15].",0,,False
138,"A different use of clusters in past work on cluster-based retrieval is for ""smoothing"" (enriching) the representation of documents [20, 16, 24, 13]. ClustMRF is shown to substantially outperform one such state-of-the-art method [13].",0,,False
139,"To the best of our knowledge, our work is first to use MRFs for cluster ranking. In the context of retrieval tasks, MRFs were first introduced for ranking documents directly [28]. We show that using ClustMRF to produce document ranking substantially outperforms this retrieval approach; and, that which augments the standard MRF retrieval model with query-independent document measures [3]. MRFs were also used, for example, for query expansion, passage-based document retrieval, and weighted concept expansion [27].",0,,False
140,4. EVALUATION 4.1 Experimental setup,0,,False
141,corpus AP,1,AP,True
142,ROBUST,0,,False
143,WT10G GOV2 ClueA ClueAF ClueB ClueBF,1,WT,True
144,"# of docs 242,918",0,,False
145,"528,155",0,,False
146,"1,692,096 25,205,179",0,,False
147,"503,903,810",0,,False
148,data Disks 1-3,0,,False
149,Disks 4-5 (-CR),0,,False
150,WT10g GOV2,1,WT,True
151,ClueWeb09 (Category A),1,ClueWeb,True
152,queries,0,,False
153,"51-150 301-450, 600-700 451-550 701-850",0,,False
154,1-150,0,,False
155,"50,220,423 ClueWeb09 (Category B) 1-150",1,ClueWeb,True
156,Table 1: Datasets used for experiments.,0,,False
157,"The TREC datasets specified in Table 1 were used for experiments. AP and ROBUST are small collections, composed mostly of news articles. WT10G and GOV2 are Web",1,TREC,True
158,"3Similarly, we could have used the geometric mean of the query-similarity values of the cluster constituent documents as a feature function defined over the lQC clique rather than constructing it using the lQD cliques as we did above.",0,,False
159,335,0,,False
160,"collections; the latter is a crawl of the .gov domain. For the ClueWeb Web collection both the English part of Category A (ClueA) and the Category B subset (ClueB) were used. ClueAF and ClueBF are two additional experimental settings created from ClueWeb following previous work [6]. Specifically, documents assigned by Waterloo's spam classifier [6] with a score below 70 and 50 for ClueA and ClueB, respectively, were filtered out from the initial corpus ranking described below. The score indicates the percentage of all documents in ClueWeb Category A that are presumably ""spammier"" than the document at hand. The ranking of the residual corpus was used to create the document list upon which the various methods operate. Waterloo's spam score is also used for the Pspam(·) measure that was described in Section 2.1. The Pspam(·) and Ppr(·) (PageRank score) measures are used only for the ClueWeb-based settings as these information types are not available for the other settings.",1,ClueWeb,True
161,The titles of TREC topics served for queries. All data was stemmed using the Krovetz stemmer. Stopwords on the INQUERY list were removed from queries but not from documents. The Indri toolkit (www.lemurproject.org/indri) was used for experiments.,1,TREC,True
162,Initial retrieval and clustering. As described in Section,0,,False
163,"2, we use the ClustMRF cluster ranking method to re-rank an initially retrieved document list Dinit. Recall that after ClustMRF ranks the clusters created from Dinit, these are ""replaced"" by their constituent documents while omitting repeats. Documents within a cluster are ranked by their query similarity, the measure of which is detailed below. This cluster-based re-ranking approach is employed by all the reference comparison methods that we use and that rely on cluster ranking. Furthermore, ClustMRF and all reference comparison approaches re-rank a list Dinit that is composed of the 50 documents that are the most highly ranked by some retrieval method specified below. Dinit is relatively short following recommendations in previous work on cluster-based re-ranking [18, 25, 26, 13]. In Section 4.2.7 we study the effect of varying the list size on the performance of ClustMRF and the reference comparisons.",0,,False
164,"We let all methods re-rank three different initial lists Dinit. The first, denoted MRF, is used unless otherwise specified. This list contains the documents in the corpus that are the most highly ranked in response to the query when using the state-of-the-art Markov Random Field approach with the sequential dependence model (SDM) [28]. The free parameters that control the use of term proximity information in SDM, T , O, and U , are set to 0.85, 0.1, and 0.05, respectively, following previous recommendations [28]. We also use MRF's SDM with its free parameters set using cross validation as one of the re-ranking reference comparisons. (Details provided below.) All methods operating on the MRF initial list use the exponent of the document score assigned by SDM -- which is a rank-equivalent estimate to that of log p(Q, d) -- as simMRF (Q, d), the document-query similarity measure. This measure was used to induce the initial ranking using which Dinit was created. More generally, for a fair performance comparison we maintain in all the experiments the invariant that the scoring function used to create an initially retrieved list is rank equivalent to the documentquery similarity measure used in methods operating on the list. Furthermore, the document-query similarity measure is",0,,False
165,used in all methods that are based on cluster ranking (including ClustMRF) to order documents within the clusters.,0,,False
166,"The second initial list used for re-ranking, DocMRF (discussed in Section 4.2.4), is created by enriching MRF's SDM with query-independent document measures [3].",0,,False
167,"The third initial list, LM, is addressed in Section 4.2.5. The list is created using unigram language models. In contrast, the MRF and DocMRF lists were created using retrieval methods that use term proximity information. Let pDz ir[µ](·) be the Dirichlet-smoothed unigram language model induced from text z; µ is the smoothing parameter. The LM similarity between texts x and y is simLM (x, y) d,ef",1,LM,True
168,"exp -CE pDx ir[0](·) pDy ir[µ](·) [37, 17], where CE is",0,,False
169,"the cross entropy measure; µ is set to 1000.4 Accordingly, the LM initial list is created by using simLM (Q, d) to rank the entire corpus.5 This measure serves as the documentquery similarity measure for all methods operating over the LM list, and for the inter-document similarity measure used by the dsim feature function.",1,LM,True
170,"Unless otherwise stated, to cluster any of the three initial lists Dinit, we use a simple nearest-neighbor clustering approach [18, 25, 14, 26, 13, 15]. For each document d ( Dinit), a cluster is created from d and the k - 1 documents di in Dinit (di ,"" d) with the highest simLM (d, di); k is set to a value in {5, 10, 20} using cross validation as described below. Using such small overlapping clusters (all of which contain k documents) was shown to be highly effective for cluster-based document retrieval [18, 25, 14, 26, 13, 15]. In Section 4.2.6 we also study the performance of ClustMRF when using hierarchical agglomerative clustering.""",1,LM,True
171,Evaluation metrics and free parameters. We use MAP,1,MAP,True
172,"(computed at cutoff 50, the size of the list Dinit that is reranked) and the precision of the top 5 documents (p@5) and their NDCG (NDCG@5) for evaluation measures.6 The free parameters of our ClustMRF method, as well as those of all reference comparison methods, are set using 10-fold cross validation performed over the queries in an experimental setting. Query IDs are the basis for creating the folds. The two-tailed paired t-test with p  0.05 was used for testing statistical significance of performance differences.",1,Query,True
173,"For our ClustMRF method, the free-parameter values are set in two steps. First, SVMrank [12] is used to learn the values of the l weights associated with the feature functions. The NDCG@k of the k constituent documents of a cluster serves as the cluster score used for ranking clusters in the learning phase7. (Recall from above that documents in a",0,,False
174,"4The MRF SDM used above also uses Dirichlet-smoothed unigram language models with µ ,"" 1000. 5Queries for which there was not a single relevant document in the MRF or LM initial lists were removed from the evaluation. For the ClueWeb settings, the same query set was used for ClueX and ClueXF. 6We note that statAP, rather than AP, was the official TREC evaluation metric in 2009 for ClueWeb with queries 1­50. For consistency with the other queries for ClueWeb, and following previous work [3], we use AP for all ClueWeb queries by treating prel files as qrel files. We hasten to point out that evaluation using statAP for the ClueWeb collections with queries 1­50 yielded relative performance patterns that are highly similar to those attained when using AP. 7Using MAP@k as the cluster score resulted in a slightly less effective performance. We also note that learning-to-""",1,LM,True
175,336,0,,False
176,AP ROBUST WT10G GOV2 ClueA ClueAF ClueB ClueBF,1,AP,True
177,MAP p@5 NDCG@5,1,MAP,True
178,MAP p@5 NDCG@5,1,MAP,True
179,MAP p@5 NDCG@5,1,MAP,True
180,MAP p@5 NDCG@5,1,MAP,True
181,MAP p@5 NDCG@5 MAP p@5 NDCG@5,1,MAP,True
182,MAP p@5 NDCG@5,1,MAP,True
183,MAP p@5 NDCG@5,1,MAP,True
184,Init,0,,False
185,10.1 50.7 50.6,0,,False
186,19.9 51.0 52.5,0,,False
187,15.8 37.5 37.2,0,,False
188,12.7 59.3 48.6,0,,False
189,4.5 19.1 12.6 8.6 46.3 32.4,0,,False
190,12.5 33.1 24.4,0,,False
191,15.8 44.8 33.2,0,,False
192,TunedMRF,0,,False
193,9.9 48.7 49.4,0,,False
194,20.0 51.0 52.7,0,,False
195,15.4 36.9 35.3i,0,,False
196,12.7 60.8,0,,False
197,49.5 4.9i,0,,False
198,21.1 15.6i,0,,False
199,8.7 47.8 33.1 13.5i,0,,False
200,35.5,0,,False
201,27.0 16.3i 46.8 34.3,0,,False
202,ClustMRF,0,,False
203,10.8,0,,False
204,53.0,0,,False
205,54.4t 21.0it 52.4,0,,False
206,54.7 18.0it 44.9it 42.8it 14.2it 70.1it 56.2it 6.3it 44.6it 29.4it,0,,False
207,8.9,0,,False
208,50.2,0,,False
209,33.9 16.1it 48.7it 37.4it 17.0,0,,False
210,48.5,0,,False
211,36.9,0,,False
212,"Table 2: The performance of ClustMRF and a tuned MRF (TunedMRF) when re-ranking the MRF initial list (Init). Boldface: the best result in a row. 'i' and 't' mark statistically significant differences with Init and TunedMRF, respectively.",0,,False
213,"cluster are ordered based on their query similarity.) A ranking of documents in Dinit is created from the cluster ranking, which is performed for each cluster size k ( {5, 10, 20}), using the approach described above; k is then also set using cross validation by optimizing the MAP performance of the resulting document ranking. The train/test split for the first and second steps are the same -- i.e., the same train set used for learning the l's is the one used for setting the cluster size. As is the case for ClustMRF, the final document ranking induced by any reference comparison method is based on using cross validation to set free-parameter values; and, MAP serves as the optimization criterion in the training (learning) phase.",1,MAP,True
214,"Finally, we note that the main computational overhead, on top of the initial ranking, incurred by using ClustMRF is the clustering. That is, the feature functions used are either query-independent, and therefore can be computed offline; or, use mainly document-query similarity values that have already been computed to create the initial ranking. Clustering of a few dozen documents can be computed efficiently; e.g., based on document snippets.",1,ad,True
215,4.2 Experimental results,0,,False
216,4.2.1 Main result,0,,False
217,"Table 2 presents our main result. Namely, the performance of ClustMRF when used to re-rank the MRF initial list. Recall that the initial ranking was induced using MRF's SDM with free-parameter values set following previous recommendations [28]. Thus, we also present for reference the re-ranking performance of using MRF's SDM with its three free parameters set using cross validation as is the case for",0,,False
218,"rank methods [23] other than SVMrank, which proved to result in highly effective performance as shown below, can also be used for setting the values of the l weights.",0,,False
219,ClustMRF,0,,False
220,AP ROBUST WT10G GOV2,1,AP,True
221,MAP p@5 NDCG@5,1,MAP,True
222,MAP p@5 NDCG@5,1,MAP,True
223,MAP p@5 NDCG@5,1,MAP,True
224,MAP p@5 NDCG@5,1,MAP,True
225,10.8 53.0 54.4 21.0 52.4 54.7 18.0 44.9 42.8 14.2 70.1 56.2,0,,False
226,ClustMRF,0,,False
227,ClueA ClueAF ClueB ClueBF,1,Clue,True
228,MAP p@5 NDCG@5,1,MAP,True
229,MAP p@5 NDCG@5,1,MAP,True
230,MAP p@5 NDCG@5,1,MAP,True
231,MAP p@5 NDCG@5,1,MAP,True
232,6.3 44.6 29.4 8.9 50.2 33.9 16.1 48.7 37.4 17.0 48.5 36.9,0,,False
233,stdvqsim,0,,False
234,9.4 43.7c 45.0c 19.0c 50.7 52.4 15.4c 38.4c 37.8c 12.7c 59.3c 48.2c,0,,False
235,maxsw2,0,,False
236,5.4c 28.7c 20.3c 8.6 47.2 32.5 14.2c 41.9c 30.1c 16.3 45.0 35.5,0,,False
237,maxsw2,0,,False
238,9.7 44.6c 45.8c 17.7c 46.9c 49.1c 12.2c 31.7c 28.6c 12.9c 62.3c 48.8c,0,,False
239,maxsw1,0,,False
240,5.3c 29.3c 20.5c 7.8c 40.4c 28.9c,0,,False
241,15.4 42.9c 32.5c 15.7c 42.3c 32.8,0,,False
242,geoqsim,0,,False
243,10.6 50.9 52.0 20.6 50.4 52.4 16.3c 39.3c 39.0c 13.2c 58.0c 46.6c,0,,False
244,maxqsim,0,,False
245,4.5c 18.7c 12.4c 8.3 49.3 34.3,0,,False
246,12.8c 33.9c 25.5c 14.8c 42.9c 32.8,0,,False
247,minsw2,0,,False
248,9.6 49.1 50.4 16.8c 44.7c 45.9c 14.2c 33.9c 32.4c,0,,False
249,14.2,0,,False
250,66.3 52.3,0,,False
251,geoqsim,0,,False
252,4.8c 20.9c 14.0c 8.6 48.7 33.9 12.9c 34.2c 25.6c 15.9 43.2 33.6,0,,False
253,Table 3: Using each of ClustMRF's top-4 feature functions by itself for ranking the clusters so as to re-rank the MRF initial list. Boldface: the best performance per row. 'c' marks a statistically significant difference with ClustMRF.,0,,False
254,"the free parameters of ClustMRF; TunedMRF denotes this method. We found that using exhaustive search for finding SDM's optimal parameter values in the training phase yields better performance (on the test set) than using SVMrank [12] and SVMmap [36]. Specifically, T , O, and U were set to values in {0, 0.05, . . . , 1} with T + O + U , 1.",0,,False
255,"We first see in Table 2 that while TunedMRF outperforms the initial MRF ranking in most relevant comparisons (experimental setting × evaluation measure), there are cases (e.g., for AP and WT10G) for which the reverse holds. The latter finding implies that optimal free-parameter values of MRF's SDM do not necessarily generalize across queries.",1,AP,True
256,"More importantly, we see in Table 2 that ClustMRF outperforms both the initial ranking and TunedMRF in all relevant comparisons. Many of the improvements are substantial and statistically significant. These findings attest to the high effectiveness of using ClustMRF for re-ranking.",0,,False
257,4.2.2 Analysis of feature functions,0,,False
258,"We now turn to analyze the relative importance attributed to the different feature functions used in ClustMRF; i.e., the l weights assigned to these functions in the training phase by SVMrank. We first average, per experimental setting and cluster size, the weights assigned to a feature function using the different training folds. Then, the feature function is assigned with a score that is the reciprocal rank of its corresponding (average) weight. Finally, the feature functions are ordered by averaging their scores across experimental settings and cluster sizes. Two feature functions, pr and spam, are only used for the ClueWeb-based settings. Hence, we perform the analysis separately for the ClueWeb and nonClueWeb (AP, ROBUST, WT10G, and GOV2) settings.",1,ClueWeb,True
259,337,0,,False
260,MAP,1,MAP,True
261,AP,1,AP,True
262,p@5,0,,False
263,NDCG@5,0,,False
264,MAP,1,MAP,True
265,ROBUST p@5,0,,False
266,NDCG@5,0,,False
267,MAP,1,MAP,True
268,WT10G p@5,1,WT,True
269,NDCG@5,0,,False
270,GOV2,0,,False
271,MAP p@5,1,MAP,True
272,NDCG@5,0,,False
273,MAP,1,MAP,True
274,ClueA p@5,1,Clue,True
275,NDCG@5,0,,False
276,MAP,1,MAP,True
277,ClueAF p@5,1,Clue,True
278,NDCG@5,0,,False
279,MAP,1,MAP,True
280,ClueB,1,Clue,True
281,p@5,0,,False
282,NDCG@5,0,,False
283,MAP,1,MAP,True
284,ClueBF p@5,1,Clue,True
285,NDCG@5,0,,False
286,Init,0,,False
287,10.1 50.7 50.6,0,,False
288,19.9c 51.0 52.5,0,,False
289,15.8c 37.5c 37.2c 12.7c 59.3c 48.6c 4.5c 19.1c 12.6c 8.6 46.3 32.4,0,,False
290,12.5c 33.1c 24.4c 15.8 44.8 33.2,0,,False
291,Inter,0,,False
292,10.4 55.9i,0,,False
293,56.0i,0,,False
294,20.8i 52.2 53.9,0,,False
295,15.1c 38.0c 36.8c 12.9c 62.9c 50.2c 5.3c 24.3c 17.8c 8.9,0,,False
296,44.8 32.6 14.9i 44.5i 34.3i,0,,False
297,16.7 48.2 36.4,0,,False
298,AMean,0,,False
299,10.6 51.1,0,,False
300,52.2,0,,False
301,20.3c 49.1c 51.2c 16.6ic 39.6ic 38.5c 13.1ic 58.8c 47.8c,0,,False
302,4.6c 19.3c 13.2c 8.8 49.8i 35.0i,0,,False
303,13.0ic 34.7c 26.1ic 15.9 45.6 34.4,0,,False
304,GMean,0,,False
305,10.6 50.9 52.0 20.6i 50.4 52.4,0,,False
306,16.3c 39.3c 39.0c 13.2ic 58.0c 46.6c,0,,False
307,4.8c 20.9c 14.0c 8.6 48.7 33.9,0,,False
308,12.9c 34.2c 25.6c,0,,False
309,15.9 43.2 33.6,0,,False
310,CRank,0,,False
311,10.0 50.0,0,,False
312,50.5,0,,False
313,19.7c 46.6ic 49.1ic 14.5c 34.2c 32.7ic 12.7c 62.3c 48.4c,0,,False
314,5.2c 24.3c 18.5ic 8.3,0,,False
315,41.5c 30.0 16.0i 46.6i 35.3i 17.7i,0,,False
316,50.3,0,,False
317,38.0i,0,,False
318,CMRF,0,,False
319,10.8,0,,False
320,53.0 54.4 21.0i,0,,False
321,52.4,0,,False
322,54.7 18.0i 44.9i 42.8i 14.2i 70.1i 56.2i 6.3i 44.6i 29.4i,0,,False
323,8.9,0,,False
324,50.2,0,,False
325,33.9 16.1i 48.7i 37.4i,0,,False
326,17.0 48.5 36.9,0,,False
327,"Table 4: Comparison with cluster-based retrieval methods used for re-ranking the MRF initial list. (CMRF is a shorthand for ClustMRF.) Boldface marks the best result in a row. 'i' and 'c' mark statistically significant differences with the initial ranking and ClustMRF, respectively.",0,,False
328,"For the non-ClueWeb settings, the feature functions, in descending order of attributed importance, are: stdv-qsim, max-sw2, geo-qsim, min-sw2, max-sw1, max-qsim, min-dsim, geo-sw2, min-icompress, min-qsim, min-sw1, geo-icompress, max-dsim, geo-dsim, max-icompress, geo-entropy, min-entropy, geo-sw1, max-entropy. For the ClueWeb settings the feature functions are ordered as follows: max-sw2, max-sw1, maxqsim, geo-qsim, max-spam, geo-sw2, min-icompress, minsw2, geo-sw1, min-sw1, min-qsim, stdv-qsim, max-pr, mindsim, min-entropy, max-entropy, min-spam, geo-icompress, geo-entropy, max-icompress, geo-spam, geo-pr, geo-dsim, minpr, max-dsim.",1,ClueWeb,True
329,"Two main observations rise. First, each of the three types of cliques used in Section 2.1 for defining the MRF has at least one associated feature function that is assigned with a relatively high weight. For example, the geo-qsim function defined over lQD, the max-qsim function defined over lQC , and the max-sw2 function defined over lC , are among the 4, 6 and 2 most important functions in both cases (nonClueWeb and ClueWeb settings). Second, for the ClueWeb settings, the feature functions defined over the lC clique and which are based on query-independent document measures (e.g., max-sw1, max-sw2, max-spam) are attributed with high importance. In fact, among the top-10 feature functions for the ClueWeb settings only two (max-qsim and geoqsim) are not based on a query-independent measure. This is not the case for the non-ClueWeb settings where different statistics of the query-similarity values are among the top10 feature functions. We note that using some of the queryindependent document measures utilized here was shown in work on Web retrieval to be effective for ranking documents directly [3]. We demonstrated the merits of using such measures for ranking document clusters.",1,ClueWeb,True
330,"In Table 3 we present the performance of using each of the top-4 feature functions (for the non-ClueWeb and ClueWeb settings) by itself as a cluster ranking method. As in Section 4.2.1, we use the cluster ranking to re-rank the MRF initial list. We see in Table 3 that in almost all relevant comparisons ClustMRF is more effective -- often to a substantial and statistically significant degree -- than using one of its top-4 feature functions alone. Thus, we conclude that ClustMRF's effective performance cannot be attributed to a single feature function that it utilizes.",1,ClueWeb,True
331,"We also performed ablation tests as follows. ClustMRF was trained each time without one of its top-10 feature functions. This resulted in a statistically significant performance decrease with respect to at least one of the three evaluation metrics of concern (MAP, p@5 and NDCG@5) for all top-10 feature functions for the ClueWeb settings. (Actual numbers are omitted as they convey no additional insight.) Yet, there was no statistically significant performance decrease for any of the top-10 feature functions for the non-ClueWeb settings. These findings attest to the redundancy of feature functions when employing ClustMRF for the non-ClueWeb settings and to the lack thereof in the ClueWeb settings.",1,MAP,True
332,"Finally, we computed the Pearson correlation of the learned l's values (averaged over the train folds and cluster sizes) between experimental settings. We found that for pairs of non-ClueWeb settings, excluding AP, the correlation was at least 0.5; however, the correlation with AP was much smaller. For the ClueWeb settings, the correlation between ClueB and ClueBF was high (0.83) while that for other pairs of settings was lower than 0.5. Thus, we conclude that the learned l values can be collection, and setting, dependent.",1,ClueWeb,True
333,4.2.3 Comparison with cluster-based methods,0,,False
334,We next compare the performance of ClustMRF with that,0,,False
335,of highly effective cluster-based retrieval methods. All meth-,0,,False
336,ods re-rank the MRF initial list.,0,,False
337,The InterpolationF method (Inter in short) [13] ranks,0,,False
338,documents directly using the score function:,0,,False
339,"Score(d; Q) d,ef (1 - )",0,,False
340,"+ sim(Q,d)",0,,False
341,"dDinit sim(Q,d )",0,,False
342,". CC l(Dinit) sim(Q,C)sim(C,d)",0,,False
343,"dDinit CC l(Dinit) sim(Q,C)sim(C,d )",0,,False
344,This state-of-the-,0,,False
345,art re-ranking method represents the class of approaches,0,,False
346,"that use clusters to ""smooth"" document representations [13].",0,,False
347,"In contrast to Inter, ClustMRF belongs to a class of meth-",0,,False
348,"ods that rely on cluster ranking. Accordingly, the next ref-",0,,False
349,erence comparison methods represent this class. Section 4.1,0,,False
350,provided a description of how the cluster ranking is trans-,0,,False
351,formed to a ranking of the documents in Dinit. The AMean,0,,False
352,"method [26, 15], for example, scores cluster C by the arith-",0,,False
353,metic mean of the query similarity values of its constituent,0,,False
354,documents.,0,,False
355,"Formally,",0,,False
356,"Score(C; Q) d,ef",0,,False
357,1 |C|,0,,False
358,"dC sim(Q, d).",0,,False
359,Scoring C by the geometric mean of the query-similarity,0,,False
360,"values of its constituent documents, Score(C; Q) d,ef",0,,False
361,"|C| dC sim(Q, d), was shown to yield state-of-the-art clus-",0,,False
362,"ter ranking performance [15]. This approach, henceforth",0,,False
363,"referred to as GMean, results from aggregating several fea-",0,,False
364,ture functions (geo-qsim) that are used in our ClustMRF,0,,False
365,method. (See Section 2.1 for details.),0,,False
366,An additional state-of-the-art cluster ranking method is,1,ad,True
367,ClustRanker (CRank in short) [15]. Cluster C is scored by,0,,False
368,"Score(C; Q) d,ef (1 - )",0,,False
369,"+ sim(Q,C)p(C)",0,,False
370,"C C l(Dinit) sim(Q,C )p(C )",0,,False
371,338,0,,False
372,AP ROBUST WT10G GOV2 ClueA ClueAF ClueB ClueBF,1,AP,True
373,MAP p@5 NDCG@5,1,MAP,True
374,MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP,1,MAP,True
375,p@5,0,,False
376,NDCG@5 MAP p@5 NDCG@5,1,MAP,True
377,MAP,1,MAP,True
378,p@5,0,,False
379,NDCG@5,0,,False
380,MAP,1,MAP,True
381,p@5,0,,False
382,NDCG@5,0,,False
383,DocMRF,0,,False
384,9.9 50.7 51.0 20.3 52.1 54.0 17.1 42.0 40.4,0,,False
385,15.0 66.3 54.0 9.8 42.4 28.4 9.5,0,,False
386,52.6,0,,False
387,35.7,0,,False
388,16.6 45.6 33.6 17.6 50.3 37.5,0,,False
389,ClustMRF,0,,False
390,11.0 53.5 53.5 21.2d 53.2 55.3,0,,False
391,17.7 42.5 40.3 15.3 68.7 55.8,0,,False
392,10.0 49.3d 33.4d,0,,False
393,9.5 49.6 35.7 18.9d 52.9d 39.9d 19.4d 55.3d 41.9d,0,,False
394,Table 5: Using ClustMRF to re-rank the DocMRF [3] list. Boldface: best result in a row. 'd' marks a statistically significant difference with DocMRF.,0,,False
395,"; dC sim(Q,d)sim(C,d)p(d)",0,,False
396,"C C l(Dinit) dC sim(Q,d)sim(C ,d)p(d)",0,,False
397,p(C),0,,False
398,and,0,,False
399,p(d),0,,False
400,are,0,,False
401,estimated based on inter-cluster and inter-document (across,0,,False
402,"clusters) similarities, respectively. These similarities, com-",0,,False
403,"puted using the language-model-based measure simLM (·, ·),",1,LM,True
404,are not utilized by ClustMRF that uses inter-document sim-,0,,False
405,ilarities only within a cluster.,0,,False
406,"Following the original reports of Inter [13] and CRank [15],",0,,False
407,"we estimate sim(Q, C) and sim(C, d) in these methods using",0,,False
408,"simLM (·, ·); C is represented by the concatenation of its con-",1,LM,True
409,"stituent documents. For a fair comparison with ClustMRF,",0,,False
410,"sim(Q, d) is set in all reference comparisons considered here",0,,False
411,"to simMRF (·, ·), which was used to create the initial MRF",0,,False
412,list that is re-ranked.,0,,False
413,All free parameters of the methods are set using cross val-,0,,False
414,"idation. Specifically,  which is used by Inter and CRank",0,,False
415,"is set to values in {0, 0.1, . . . , 1}. The graph out degree",0,,False
416,and the dumping factor used by CRank are set to values,0,,False
417,"in {4, 9, 19, 29, 39, 49} and {0.05, 0.1, . . . , 0.9, 0.95}, respec-",0,,False
418,tively. The cluster size used by each method is selected from,0,,False
419,"{5, 10, 20} as is the case for ClustMRF. Table 4 presents the",0,,False
420,performance numbers.,0,,False
421,We can see in Table 4 that in a vast majority of the rele-,0,,False
422,vant comparisons ClustMRF outperforms the reference com-,0,,False
423,parison methods. Many of the improvements are substantial,0,,False
424,and statistically significant. In the few cases that ClustMRF,0,,False
425,"is outperformed by one of the other methods, the perfor-",0,,False
426,mance differences are not statistically significant.,0,,False
427,4.2.4 Using ClustMRF to re-rank the DocMRF list,0,,False
428,"Heretofore, we studied the performance of ClustMRF when used to re-rank the MRF initial list. The analysis presented in Section 4.2.2 demonstrated the effectiveness -- especially for the ClueWeb settings -- of using feature functions that utilize query-independent document measures. Thus, we now turn to explore ClustMRF's performance when employed over a document ranking that is already based on using query-independent document measures.",1,ClueWeb,True
429,"To that end, we follow some recent work [3]. We re-rank the 1000 documents that are the most highly ranked by MRF's SDM that was used above to create the MRF initial list. Re-ranking is performed using an MRF model that is enriched with query-independent document measures [3]. We use the same document measures utilized by ClustMRF, except for dsim which is based on inter-document similarities and which was not considered in this past work that ranked documents independently of each other [3]. The resultant ranking, induced using SVMrank for learning parameter values, is denoted DocMRF. (SVMrank yielded better performance than SVMmap.) We then let ClustMRF rerank the top-50 documents. In doing so, we use the exponent of the score assigned by DocMRF to document d, which is a rank equivalent estimate to that of log p(Q, d), as the sim(Q, d) value used by ClustMRF. Thus, we maintain the invariant mentioned above that the scoring function used to induce the ranking upon which ClustMRF operates is rank equivalent to the document-query similarity measure used in ClustMRF. We note that ClustMRF is different from DocMRF in two important respects. First, by the virtue of ranking clusters first and transforming the ranking to that of documents rather than ranking documents directly as is the case in DocMRF. Second, by the completely different ways that document-query similarities are used.",0,,False
430,Comparing the performance of DocMRF in Table 5 with that of the MRF initial ranking in Table 2 attests to the merits of using DocMRF for re-ranking. We can also see in Table 5 that applying ClustMRF over the DocMRF list results in performance improvements in almost all relevant comparisons. Many of the improvements for the ClueWeb settings are substantial and statistically significant.,1,ClueWeb,True
431,4.2.5 Using ClustMRF to re-rank the LM list,1,LM,True
432,"The third list we re-rank using ClustMRF is LM, which was created using unigram language models. For reference comparison we use the cluster-based Inter method which was used in Section 4.2.3. Experiments show -- actual numbers are omitted due to space considerations -- that for reranking the LM list, the GMean cluster ranking method is more effective in most relevant comparisons than the other two cluster ranking methods used in Section 4.2.3 for reference comparison (AMean and CRank). Hence, GMean is used here as an additional reference comparison.",1,LM,True
433,"ClustMRF, Inter and GMean use the simLM (·, ·) similarity measure, which was used for inducing the initial ranking, for sim(Q, d). All other implementation details are the same as those described above. As a result, ClustMRF, as well as Inter and GMean, use only unigram language models in the LM setting considered here. This is in contrast to the MRF-list setting considered above where term-proximities information was used.",1,LM,True
434,"An additional reference comparison that uses unigram language models is relevance model number 3 [1], RM3, which is a state-of-the-art query expansion approach. RM3 is also used to re-rank the LM list. All (50) documents in the list are used for constructing RM3. Its free-parameter values are set using cross validation. Specifically, the number of expansion terms and the interpolation parameter that controls the reliance on the original query are set to values in {5, 10, 25, 50} and {0.1, 0.3, . . . , 0.9}, respectively. Dirichletsmoothed language models are used with µ , 1000.",1,ad,True
435,339,0,,False
436,MAP,1,MAP,True
437,AP,1,AP,True
438,p@5,0,,False
439,NDCG@5,0,,False
440,MAP,1,MAP,True
441,ROBUST p@5,0,,False
442,NDCG@5,0,,False
443,MAP WT10G p@5,1,MAP,True
444,NDCG@5,0,,False
445,MAP,1,MAP,True
446,GOV2 p@5,0,,False
447,NDCG@5,0,,False
448,MAP,1,MAP,True
449,ClueA p@5,1,Clue,True
450,NDCG@5,0,,False
451,MAP ClueAF p@5,1,MAP,True
452,NDCG@5,0,,False
453,MAP,1,MAP,True
454,ClueB,1,Clue,True
455,p@5,0,,False
456,NDCG@5,0,,False
457,MAP,1,MAP,True
458,ClueBF p@5,1,Clue,True
459,NDCG@5,0,,False
460,Init,0,,False
461,9.9 49.6 49.9 19.3c 49.5c 51.6c 15.0,0,,False
462,36.4c 35.8 11.8c 56.6c 46.5c 3.3c 16.1c 10.7c 8.0c 47.4 32.3 11.4c 29.0c 21.2c 14.7c 42.9c 32.1c,0,,False
463,Inter,0,,False
464,10.6i 56.1ic 55.6i,0,,False
465,20.1i 50.9,0,,False
466,53.1 14.9 37.5 37.1 12.6ic 62.4ic 50.4i 5.0i 24.6ic 17.9ic 8.5i 46.7 32.6 13.8ic 40.5i 29.6i,0,,False
467,15.6,0,,False
468,46.3,0,,False
469,34.6,0,,False
470,GMean RM3,0,,False
471,10.8i 9.9,0,,False
472,50.7 49.1,0,,False
473,51.8 20.6i,0,,False
474,52.1 53.8 14.9,0,,False
475,49.3 19.7ic 49.7c,0,,False
476,52.1c 14.5,0,,False
477,37.5 35.5 12.4ic 60.8ic 48.8c 3.7ic 17.2c,0,,False
478,11.5c,0,,False
479,8.2 45.7 32.3 12.0ic 31.6ic 23.4ic 15.5,0,,False
480,43.4,0,,False
481,33.4c,0,,False
482,36.6c 35.9 12.7ic 60.4ic 49.1c 3.8ic 17.4c,0,,False
483,11.0c 8.7i,0,,False
484,47.6 34.3 13.9ic 40.2i 30.0i 16.4i 48.9i 36.6i,0,,False
485,ClustMRF,0,,False
486,10.5 51.3 51.7 20.5i 52.9i 55.6i,0,,False
487,14.6 42.2i,0,,False
488,39.3 13.5i 68.4i 54.3i 5.5i 43.3i 27.7i 8.7i,0,,False
489,51.5,0,,False
490,35.6 16.0i 46.0i 34.8i 16.8i 49.2i 38.7i,0,,False
491,"Table 6: Re-ranking the LM initial list. Boldface: the best result in a row. 'i' and 'c' mark statistically significant differences with the initial ranking and ClustMRF, respectively.",1,LM,True
492,"We see in Table 6 that ClustMRF outperforms the reference comparisons in a vast majority of the relevant comparisons. Many of the improvements are substantial and statistically significant. These results, along with those presented in Sections 4.2.1 and 4.2.4, attest to the effectiveness of using ClustMRF to re-rank different initial lists.",0,,False
493,4.2.6 Varying the clustering algorithm,0,,False
494,"Thus far, we used ClustMRF and the reference compar-",0,,False
495,isons with nearest-neighbor (NN) clustering. In Table 7 we,0,,False
496,present the retrieval performance of using hierarchical ag-,0,,False
497,glomerative clustering (HAC) with the complete link mea-,0,,False
498,sure. This clustering was shown to be among the most ef-,0,,False
499,fective hard clustering methods for cluster-based retrieval,0,,False
500,"[24, 13].",0,,False
501,We,0,,False
502,use,0,,False
503,"1 simLM (d1 ,d2)",1,LM,True
504,+,0,,False
505,"1 simLM (d2,d1)",1,LM,True
506,for an inter-,0,,False
507,"document dissimilarity measure; and, cut the clustering den-",0,,False
508,drogram so that the resultant average cluster size is the clos-,0,,False
509,"est to a value k ( {5, 10, 20}). Doing so somewhat equates",0,,False
510,the comparison terms with using the NN clusters whose size,0,,False
511,"is in {5, 10, 20}. Cross validation is used in all cases for",0,,False
512,setting the value of k.,0,,False
513,The MRF initial list is clustered and serves as the ba-,0,,False
514,sis for re-ranking. Experiments show (actual numbers are,0,,False
515,omitted due to space considerations) that among the three,0,,False
516,cluster ranking methods which were used above for refer-,0,,False
517,"ence comparison (AMean, GMean, and CRank) CRank is",0,,False
518,"the most effective when using HAC. Hence, CRank serves",0,,False
519,as a reference comparison here.,0,,False
520,We see in Table 7 that in the majority of relevant com-,0,,False
521,"parisons, ClustMRF improves over the initial ranking when",0,,False
522,"using HAC. In contrast, CRank is outperformed by the ini-",0,,False
523,"tial ranking in most relevant comparisons for HAC. Indeed,",0,,False
524,ClustMRF outperforms CRank in most cases for both NN,0,,False
525,and HAC. We also see that ClustMRF is (much) more effec-,0,,False
526,tive when using the overlapping NN clusters than the hard,0,,False
527,AP ROBUST WT10G GOV2 ClueA ClueAF ClueB ClueBF,1,AP,True
528,MAP p@5 NDCG@5,1,MAP,True
529,MAP p@5 NDCG@5,1,MAP,True
530,MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP p@5 NDCG@5,1,MAP,True
531,MAP p@5 NDCG@5,1,MAP,True
532,MAP p@5 NDCG@5,1,MAP,True
533,Init,0,,False
534,10.1 50.7 50.6 19.9 51.0 52.5 15.8 37.5 37.2 12.7 59.3 48.6 4.5 19.1 12.6 8.6 46.3 32.4 12.5 33.1 24.4 15.8 44.8 33.2,0,,False
535,HAC,0,,False
536,NN,0,,False
537,CRank ClustMRF CRank ClustMRF,0,,False
538,9.9,0,,False
539,49.8,0,,False
540,50.5,0,,False
541,19.1 50.1 51.7 14.8 36.6 34.4 13.2i 61.5 49.7 5.6i 23.7 16.9i 8.4 43.9 32.0 14.4i 39.5i 30.6i,0,,False
542,15.3,0,,False
543,43.9,0,,False
544,32.7,0,,False
545,9.6i 46.5i 46.8i 19.6,0,,False
546,50.4,0,,False
547,51.9,0,,False
548,15.8,0,,False
549,38.2,0,,False
550,37.0 13.6i,0,,False
551,63.9,0,,False
552,51.5 5.8i 31.7ic 21.0i,0,,False
553,9.2 48.9 33.4 14.5i 39.7i 30.3i 15.2 43.1 32.5,0,,False
554,10.0 50.0 50.5,0,,False
555,19.7 46.6i 49.1i,0,,False
556,14.5 34.2 32.7i,0,,False
557,12.7 62.3 48.4,0,,False
558,5.2 24.3 18.5i 8.3 41.5 30.0 16.0i 46.6i 35.3i 17.7i,0,,False
559,50.3,0,,False
560,38.0i,0,,False
561,10.8,0,,False
562,53.0,0,,False
563,54.4 21.0ic 52.4c 54.7c 18.0ic 44.9ic 42.8ic 14.2ic 70.1ic 56.2ic 6.3ic 44.6ic 29.4ic 8.9,0,,False
564,50.2c 33.9 16.1i 48.7i 37.4i,0,,False
565,17.0 48.5 36.9,0,,False
566,"Table 7: Using nearest-neighbor clustering (NN) vs. (complete link) hierarchical agglomerative clustering (HAC). The MRF initial list is used. Boldface: the best result in a row per clustering algorithm; underline: the best result in a row. 'i' and 'c': statistically significant differences with the initial ranking and CRank, respectively.",0,,False
567,"clusters created by HAC. The improved effectiveness of using NN in comparison to HAC echoes findings in previous work on cluster-based re-ranking [13]. For CRank, the performance of using neither NN nor HAC dominates that of using the other.",0,,False
568,4.2.7 The effect of the size of the initial list,0,,False
569,"Until now, ClustMRF and all reference comparison methods were used to re-rank an initial list of 50 documents. Using a short list follows common practice in work on clusterbased re-ranking [18, 25, 26, 13] as was mentioned in Section 4.1. We now turn to study ClustMRF's performance when re-ranking longer lists. To that end, we use for the initial list the n ( {50, 100, 250, 500}) documents that are the most highly ranked by MRF's SDM [28] which was used above for creating the MRF initial list. For reference comparisons we use TunedMRF (see Section 4.2.1); and, the AMean and GMean cluster ranking methods described in Section 4.2.3. Nearest-neighbor clustering is used.",0,,False
570,"We see in Figure 2 that in almost all cases -- i.e., experimental settings and values of n -- ClustMRF outperforms both the initial ranking and TunedMRF; often, the performance differences are quite substantial. Furthermore, in most cases (with the notable exception of AP) ClustMRF outperforms AMean and GMean.",1,AP,True
571,4.2.8 Diversifying search results,0,,False
572,"We next explore how ClustMRF can be used to improve the performance of search-results diversification approaches. Specifically, we use the MMR [5] and the state-of-the-art xQuAD [29] diversification methods.",0,,False
573,340,0,,False
574,MAP,1,MAP,True
575,13.5 13.0 12.5 12.0 11.5 11.0 10.5 10.0 9.5,0,,False
576,50 100,0,,False
577,11.0 10.0 9.0 8.0 7.0 6.0 5.0 4.0,0,,False
578,50 100,0,,False
579,AP,1,AP,True
580,Init TunedMRF,0,,False
581,AMean GMean ClustMRF,0,,False
582,250,0,,False
583,500,0,,False
584,n,0,,False
585,ClueA,1,Clue,True
586,Init TunedMRF,0,,False
587,AMean GMean ClustMRF,0,,False
588,250,0,,False
589,500,0,,False
590,n,0,,False
591,MAP,1,MAP,True
592,MAP,1,MAP,True
593,23.0 22.5 22.0 21.5 21.0 20.5 20.0 19.5,0,,False
594,50 100,0,,False
595,10.0,0,,False
596,9.5,0,,False
597,9.0,0,,False
598,8.5,0,,False
599,8.0,0,,False
600,7.5 50 100,0,,False
601,ROBUST,0,,False
602,Init TunedMRF,0,,False
603,AMean GMean ClustMRF,0,,False
604,250,0,,False
605,500,0,,False
606,n,0,,False
607,ClueAF,1,Clue,True
608,Init TunedMRF,0,,False
609,AMean GMean ClustMRF,0,,False
610,250,0,,False
611,500,0,,False
612,n,0,,False
613,MAP,1,MAP,True
614,MAP,1,MAP,True
615,19.0 18.5 18.0 17.5 17.0 16.5 16.0 15.5 15.0,0,,False
616,50 100,0,,False
617,20.0 19.0 18.0 17.0 16.0 15.0 14.0 13.0 12.0 11.0,0,,False
618,50 100,0,,False
619,WT10G,1,WT,True
620,Init TunedMRF,0,,False
621,AMean GMean ClustMRF,0,,False
622,250,0,,False
623,500,0,,False
624,n,0,,False
625,ClueB,1,Clue,True
626,Init TunedMRF,0,,False
627,AMean GMean ClustMRF,0,,False
628,250,0,,False
629,500,0,,False
630,n,0,,False
631,MAP,1,MAP,True
632,MAP,1,MAP,True
633,18.0 17.0 16.0 15.0 14.0 13.0 12.0,0,,False
634,50 100,0,,False
635,19.0 18.0 17.0 16.0 15.0 14.0,0,,False
636,50 100,0,,False
637,GOV2,0,,False
638,Init TunedMRF,0,,False
639,AMean GMean ClustMRF,0,,False
640,250,0,,False
641,500,0,,False
642,n,0,,False
643,ClueBF,1,Clue,True
644,Init TunedMRF,0,,False
645,AMean GMean ClustMRF,0,,False
646,250,0,,False
647,500,0,,False
648,n,0,,False
649,MAP,1,MAP,True
650,Figure 2: The effect on MAP(@50) performance of the size n of the MRF initial list that is re-ranked.,1,MAP,True
651,ClueA ClueAF ClueB ClueBF,1,Clue,True
652,-NDCG ERR-IA P-IA -NDCG ERR-IA P-IA -NDCG ERR-IA P-IA -NDCG ERR-IA P-IA,0,,False
653,Init,0,,False
654,24.5 16.0 11.8 42.6 32.0 21.0 33.2 21.1 15.4 41.6 29.7 18.9,0,,False
655,MRF,0,,False
656,26.2c 17.3c 10.3c 42.9 32.3 20.2c 33.6c 21.3c 14.4ic 42.6ic 30.2ic 18.4,0,,False
657,MMR QClust ClustMRF,0,,False
658,25.4c,0,,False
659,17.5c 9.6ic 39.0ic 29.8c 14.9ic,0,,False
660,33.9c,0,,False
661,21.5c 12.8ic 38.7ic 27.0ic 14.5ic,0,,False
662,38.7i 30.5i 16.7i,0,,False
663,43.8,0,,False
664,34.2 17.6i 43.7i 32.0i 17.4i 45.4i 33.3i,0,,False
665,17.8,0,,False
666,MRF,0,,False
667,27.4ic 17.9ic 13.3c 44.3i 33.4i,0,,False
668,21.0,0,,False
669,39.7ic 25.9ic 19.4ic 46.1ic 33.2i 21.4ic,0,,False
670,xQuAD QClust ClustMRF,0,,False
671,28.9ic 19.6ic 13.6ic 43.7,0,,False
672,33.1,0,,False
673,38.8i 30.6i 17.2i 45.5i 34.9i,0,,False
674,20.0,0,,False
675,39.3ic 25.3ic 19.2ic 44.2ic 31.2c 20.9ic,0,,False
676,20.6 45.5i 32.9i 21.0i,0,,False
677,48.1i 34.8i 22.0i,0,,False
678,"Table 8: Diversifying search results. Underline and boldface mark the best result in a row, and per diversification method in a row, respectively. 'i' and 'c' mark statistically significant differences with the initial ranking (Init) and ClustMRF, respectively. The MRF initial list is used.",0,,False
679,MMR and xQuAD iteratively re-rank an initial list Dinit. In each iteration the document in Dinit \ S assigned with the highest score is added to the set S; S is empty at the beginning. The final ranking is determined by the order of insertion to S.,1,ad,True
680,"The score MMR assigns to document d ( Dinit \ S) is sim1(Q, d)-(1-) maxdiS sim2(d, di);  is a free parameter; sim1(·, ·) and sim2(·, ·) are discussed below. In contrast to MMR, xQuAD uses information about Q's subtopics, T (Q), and assigns d with the score p(d|Q)+",0,,False
681,"(1 - ) tT (Q) p(t|Q)p(d|t) diS (1 - p(di|t)) ; p(t|Q) is the relative importance of subtopic t with respect to Q; p(d|Q) and p(d|t) are the estimates of d's relevance to Q and t, respectively.",0,,False
682,The parameter  controls in both methods the tradeoff between using relevance estimation and applying diversification. Our focus is on improving the former and evaluating the resulting (diversification based) performance. This was also the case in previous work that used cluster ranking,1,ad,True
683,"for results diversification [11]. Hence, this work serves for",0,,False
684,reference comparison below.8,0,,False
685,"We study three different estimates for sim1(Q, d) (used in MMR) which we also use for p(d|Q) (used in xQuAD).9",0,,False
686,"The first, simMRF (Q, d), is that employed in the evalua-",0,,False
687,tion above to create the MRF initial list that is also used,0,,False
688,here for re-ranking. (Further details are provided below.),0,,False
689,The next two estimates are based on applying cluster rank-,0,,False
690,ing and transforming it to document ranking using the ap-,0,,False
691,proach,0,,False
692,described,0,,False
693,in Section,0,,False
694,4.1.,0,,False
695,In these,0,,False
696,"cases,",0,,False
697,1 r(d),0,,False
698,serves,0,,False
699,"for sim1(Q, d), where r(d) is the rank of d in the document",0,,False
700,result list produced by using the cluster ranking method.,0,,False
701,"The first cluster ranking method is ClustMRF. The second,",0,,False
702,"QClust, was used in the work mentioned above on utilizing",0,,False
703,"cluster ranking for results diversification [11]. Specifically,",0,,False
704,"cluster C is scored by simLM (Q, C) (see Section 4.1 for de-",1,LM,True
705,"8There is work on using information induced from clusters for the diverisification itself (e.g., [21]). Using ClustMRF for cluster ranking in these approaches is future work. 9For scale compatibility, the two resultant quantities that are interpolated (using ) in MMR and xQuAD are sum normalized with respect to all documents in Dinit before the interpolation is performed.",0,,False
706,341,0,,False
707,"tails of simLM (·, ·)); C is represented by the concatenation",1,LM,True
708,of its documents.,0,,False
709,We use MMR and xQuAD to re-rank the MRF initial,0,,False
710,"list that contains 50 documents. simLM (·, ·) serves for the",1,LM,True
711,"sim2(·, ·) measure used in MMR and for p(d|t) that is used in",0,,False
712,"xQuAD. The official TREC subtopics, which are available",1,TREC,True
713,"for the ClueWeb settings that we use here, were used for",1,ClueWeb,True
714,"experiments. Following the findings in [29], we set p(t|Q) d,ef",0,,False
715,|T,0,,False
716,1 (Q)|,0,,False
717,.,0,,False
718,"The value of  is selected from {0.1, 0.2, . . . , 0.9}",0,,False
719,using cross validation; -NDCG (@20) is the optimization,0,,False
720,"metric. In addition to -NDCG (@20), ERR-IA (@20) and",1,ad,True
721,P-IA (@20) are used for evaluation.,0,,False
722,Table 8 presents the results. We see that using the MRF,0,,False
723,similarity measure in MMR and xQuAD outperforms the ini-,0,,False
724,"tial ranking, which was created using this measure, in most",0,,False
725,relevant comparisons. This attests to the diversification ef-,0,,False
726,fectiveness of MMR and xQuAD. Using QClust outperforms,0,,False
727,"the initial ranking in most cases, but is consistently out-",0,,False
728,performed by using the MRF measure and our ClustMRF,0,,False
729,"method. More generally, the best performance for each di-",0,,False
730,versification method (MMR and xQuAD) is almost always,0,,False
731,"attained by ClustMRF, which often outperforms the other",0,,False
732,methods in a substantial and statistically significant man-,0,,False
733,"ner. Thus, although ClustMRF ranks clusters of similar",0,,False
734,"documents, using the resultant document ranking can help",0,,False
735,to much improve results-diversification performance.,0,,False
736,5. CONCLUSIONS,0,,False
737,We presented a novel approach to ranking (query specific) document clusters by their presumed relevance to the query. Our approach uses Markov Random Fields that enable the integration of various types of cluster-relevance evidence. Empirical evaluation demonstrated the effectiveness of using our approach to re-rank different initially retrieved lists. The approach also substantially outperforms state-of-the-art cluster ranking methods and can be used to substantially improve the performance of results diversification methods.,0,,False
738,6. ACKNOWLEDGMENTS,0,,False
739,We thank the reviewers for their comments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center.,0,,False
740,7. REFERENCES,0,,False
741,"[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 -- novelty and hard. In Proc. of TREC-13, 2004.",1,ad,True
742,"[2] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proc. of TREC-9, 2000.",1,TREC,True
743,"[3] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of web documents. In Proc. of WSDM, pages 95­104, 2011.",0,,False
744,"[4] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In Proc. of WWW, pages 107­117, 1998.",0,,False
745,"[5] J. G. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR, pages 335­336, 1998.",0,,False
746,"[6] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Informaltiom Retrieval Journal, 14(5):441­465, 2011.",0,,False
747,"[7] W. B. Croft. A model of cluster searching based on classification. Information Systems, 5:189­195, 1980.",0,,False
748,"[8] D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W. Tukey. Scatter/Gather: A cluster-based approach to browsing large document collections. In Proc. of SIGIR, pages 318­329, 1992.",0,,False
749,"[9] D. Fetterly, M. Manasse, and M. Najork. Spam, damn spam, and statistics: Using statistical analysis to locate spam web pages. In Proc. of WebDB, pages 1­6, 2004.",0,,False
750,"[10] N. Fuhr, M. Lechtenfeld, B. Stein, and T. Gollub. The optimum clustering framework: implementing the cluster hypothesis. Information Retrieval Journal, 15(2):93­115, 2012.",0,,False
751,"[11] J. He, E. Meij, and M. de Rijke. Result diversification based on query-specific cluster ranking. JASIST, 62(3):550­571, 2011.",0,,False
752,"[12] T. Joachims. Training linear svms in linear time. In Proc. of KDD, pages 217­226, 2006.",0,,False
753,"[13] O. Kurland. Re-ranking search results using language models of query-specific clusters. Journal of Information Retrieval, 12(4):437­460, August 2009.",0,,False
754,"[14] O. Kurland and C. Domshlak. A rank-aggregation approach to searching for optimal query-specific clusters. In Proc. of SIGIR, pages 547­554, 2008.",0,,False
755,"[15] O. Kurland and E. Krikon. The opposite of smoothing: A language model approach to ranking query-specific document clusters. Journal of Artificial Intelligence Research (JAIR), 41:367­395, 2011.",0,,False
756,"[16] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In Proc. of SIGIR, pages 194­201, 2004.",1,ad,True
757,"[17] O. Kurland and L. Lee. PageRank without hyperlinks: Structural re-ranking using links induced by language models. In Proc. of SIGIR, pages 306­313, 2005.",0,,False
758,"[18] O. Kurland and L. Lee. Respect my authority! HITS without hyperlinks utilizing cluster-based language models. In Proc. of SIGIR, pages 83­90, 2006.",0,,False
759,"[19] O. Kurland, F. Raiber, and A. Shtok. Query-performance prediction and cluster ranking: Two sides of the same coin. In Proc. of CIKM, pages 2459­2462, 2012.",1,Query,True
760,"[20] K.-S. Lee, Y.-C. Park, and K.-S. Choi. Re-ranking model based on document clusters. Inf. Process. Manage., 37(1):1­14, 2001.",0,,False
761,"[21] T. Leelanupab, G. Zuccon, and J. M. Jose. When two is better than one: A study of ranking paradigms and their integrations for subtopic retrieval. In Proc. of AIRS, pages 162­172, 2010.",1,ad,True
762,"[22] A. Leuski. Evaluating document clustering for interactive information retrieval. In Proc. of CIKM, pages 33­40, 2001.",0,,False
763,"[23] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.",0,,False
764,"[24] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proc. of SIGIR, pages 186­193, 2004.",0,,False
765,"[25] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.",0,,False
766,"[26] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proc. of ECIR, pages 454­462, 2008.",0,,False
767,"[27] D. Metzler. A feature-centric view of information retrieval. Springer, 2011.",0,,False
768,"[28] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proc. of SIGIR, pages 472­479, 2005.",0,,False
769,"[29] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proc. of WWW, pages 881­890, 2010.",0,,False
770,"[30] J. Seo and W. B. Croft. Geometric representations for multiple documents. In Proc. of SIGIR, pages 251­258, 2010.",0,,False
771,"[31] J. G. Shanahan, J. Bennett, D. A. Evans, D. A. Hull, and J. Montgomery. Clairvoyance Corporation experiments in the TREC 2003. High accuracy retrieval from documents (HARD) track. In Proc. of TREC-12, pages 152­160, 2003.",1,TREC,True
772,"[32] A. Tombros, R. Villa, and C. van Rijsbergen. The effectiveness of query-specific hierarchic clustering in information retrieval. Inf. Process. Manage., 38(4):559­582, 2002.",0,,False
773,"[33] C. J. van Rijsbergen. Information Retrieval. Butterworths, second edition, 1979.",0,,False
774,"[34] E. M. Voorhees. The cluster hypothesis revisited. In Proc. of SIGIR, pages 188­196, 1985.",0,,False
775,"[35] P. Willett. Query specific automatic document classification. International Forum on Information and Documentation, 10(2):28­32, 1985.",1,Query,True
776,"[36] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average precision. In Proc. of SIGIR, pages 271­278, 2007.",1,ad,True
777,"[37] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. of SIGIR, pages 334­342, 2001.",1,ad,True
778,342,0,,False
779,,0,,False

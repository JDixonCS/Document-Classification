,sentence,label,data,regex
0,Reusable Test Collections Through Experimental Design,0,,False
1,"Ben Carterette, Evangelos Kanoulas, Virgil Pavlu, Hui Fang carteret@cis.udel.edu, e.kanoulas@sheffield.ac.uk, vip@ccs.neu.edu, hfang@ece.udel.edu",0,,False
2," Department of Computer & Information Sciences, University of Delaware, Newark, DE  Information Studies Department, University of Sheffield, Sheffield, UK",0,,False
3," College of Computer and Information Science, Northeastern University, Boston, MA Department of Computer & Electrical Engineering, University of Delaware, Newark, DE",0,,False
4,ABSTRACT,0,,False
5,"Portable, reusable test collections are a vital part of research and development in information retrieval. Reusability is difficult to assess, however. The standard approach-- simulating judgment collection when groups of systems are held out, then evaluating those held-out systems--only works when there is a large set of relevance judgments to draw on during the simulation. As test collections adapt to larger and larger corpora, it becomes less and less likely that there will be sufficient judgments for such simulation experiments. Thus we propose a methodology for information retrieval experimentation that collects evidence for or against the reusability of a test collection while judgments are being made. Using this methodology along with the appropriate statistical analyses, researchers will be able to estimate the reusability of their test collections while building them and implement ""course corrections"" if the collection does not seem to be achieving desired levels of reusability. We show the robustness of our design to inherent sources of variance, and provide a description of an actual implementation of the framework for creating a large test collection.",1,ad,True
6,Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval] Performance Evaluation,0,,False
7,"General Terms: Experimentation, Measurement",0,,False
8,"Keywords: information retrieval, test collections, reusability, evaluation",0,,False
9,1. INTRODUCTION,1,DUC,True
10,Test collections are a vital part of research and development in information retrieval. They enable rapid development of new approaches to retrieval. They allow us to identify subtle distinctions between retrieval methods that could not be identified by users but that can add up to improved user experience over time. They support feature selection and parameter tuning by allowing us to efficiently test many possible combinations and values.,1,ad,True
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19Г23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
12,"Unfortunately, test collections are expensive. They require judgments of the relevance of individual documents to topics in a sample. To properly control for variance, a test collection must have many topics and many judgments, and these require a great deal human effort. This expense makes reusability desirable: the cost of a test collection can be justified by the fact that it is amortized over many uses.",0,,False
13,"Constructing reusable test collections is difficult. The relevance judgments must be complete enough that future users of that collection can have confidence that their systems will be accurately evaluated. The majority of reusable test collections in the field exist as a result of the efforts of the organizers and participants of TREC (the Text REtrieval Conference), CLEF (the Cross Language Evaluation Forum), NTCIR (NII Test Collections for IR), and INEX (INitiative for the Evaluation of XML retrieval). These test collections arose by conducting an experiment to evaluate different approaches to a particular retrieval problem, and their reusability is a function of their large size and the diversity of approaches that were included in the experiment.",1,TREC,True
14,"The standard experimental design for IR evaluation is a simple repeated-measures design, in which experimental units are topics/queries, treatments are systems, and each system provides ranked results for each query. This is the design that has been used for virtually every TREC, CLEF, NTCIR, and INEX track that has resulted in the release of a test collection. Measurements on experimental units are evaluation measures such as average precision (AP) calculated over relevance judgments; judging a pool of documents retrieved by the participating systems ensures that the measurements will be as accurate as possible.",1,TREC,True
15,"Reusability emerges as a result of using a large and diverse set of retrieval systems and making sure they are judged to a substantial depth using pooling: there are simply so many judgments that it is unlikely any new system will ever be developed that does not retrieve many of the same documents that were judged as part of the original experiment. But as test collections grow larger and larger, pooling becomes more infeasible. Furthermore, recent work suggests that, for an experiment like that described above, it is actually more cost-effective to use many queries with very few judgments each. Thus TREC has begun adopting alternatives to pooling [15]: statistical sampling, which attempts to pick out the judgments that will result in a low-variance unbiased estimator [2], or algorithmic approaches that try to pick out the judgments that will reduce variance regardless of bias introduced [6]. But while these are more cost-effective for answering the original evaluation question, it is not at all",1,TREC,True
16,547,0,,False
17,"clear that they are cost-effective in the sense of producing test collections that can be reused many times. Because there are many fewer judgments, it is much more likely that a future system will retrieve many documents that were not judged, and therefore much more likely that we will not be able to accurately measure the performance of that system.",0,,False
18,"Our goal is to elevate reusability to a basic consideration along with evaluation. We do that by proposing an experimental design that collects evidence for or against reusability while judgments are being collected. The method that is used to select judgments does not matter, but the design is tied to the notion that more queries with fewer judgments is the correct way to build a test collection. It relies on having a large number of queries that can be partitioned into a combinatorial number of blocks.",0,,False
19,"In Section 2 we define what it means for a test collection to be reusable and discuss previous work on the topic. In Section 3, the main body of this work, we describe our design and the statistical analyses that it supports, and anticipate and answer some questions about its validity. In Section 5 we demonstrate the use of the design and analysis in the construction of an actual large test collection.",0,,False
20,2. TEST COLLECTION REUSABILITY,0,,False
21,"A test collection consists of a corpus of documents, a set of topics that are representative of a particular task, and judgments of relevance of documents to topics. These judgments are generally taken from a set of retrieval systems performing the task. We define reusability as follows: A test collection is reusable if and only if we can use it for precise measurements of the performance of systems that did not contribute to its judgments. By ""precise"" we mean that the measurements fall within some given error intervals with high probability. By ""systems that did not contribute judgments"" we mean systems that are likely to be developed with current technology--it is always possible that new, unforeseen technology could produce retrieval systems that are both good and unlike anything seen before; since we will never be able to predict such cases, we do not want to tie reusability to them too much.",0,,False
22,"Test collections are used for many purposes beyond simple evaluation. Furthermore, evaluation comes in many different flavors. Below we discuss some aspects of reusability and previous work on this topic.",0,,False
23,2.1 Applications of Test Collections,0,,False
24,"In addition to evaluation, test collections are used for training and optimization, including model selection, feature selection, and parameter tuning, for failure analysis, for data exploration, and many other purposes. While our focus is on evaluation, these other uses are important. Some can be seen as being related to evaluation: optimization uses an objective function based on an evaluation measure; the goal of failure analysis is to find reasons for an evaluation measure being different than expected.",1,ad,True
25,2.2 Evaluating Reusability,0,,False
26,"The question of reusability has been studied primarily in the context of depth pooling. TREC and other fora form pools from the top documents retrieved by each submitted run for each topic; under the assumption that documents not highly ranked could be considered nonrelevant, test collections based on such pools are likely very reusable.",1,TREC,True
27,"Harman [11] tested this by examining a pool formed by the documents in ranks 101-200 over the TREC-2 and TREC3 collections. Her study showed up to 21% more relevant documents could be found. Along the same line, Zobel [17] extrapolated the number of relevant documents found by depth to suggest that there could be up to twice as many relevant documents in the collection as there are in the pool. To examine the effect of the missing relevant documents on new systems that had not contributed any documents to the formation of the pool, he performed a leave-one-runout simulation. For each participating run, he removed all the documents it uniquely retrieved from the judgments and compared the evaluation over this reduced set of judgments to the evaluation with the full set. His study showed that the effect of the missing documents was minimal.",1,TREC,True
28,"Voorhees adapted the leave-one-out methodology to leave out all of the runs contributed by a particular site at a time, under the assumption that runs submitted by the same participating site are similar enough that they retrieve very similar documents [14]. This leave-sites-out simulation has since become the standard approach to evaluating reusability.",1,ad,True
29,"Buеttcher et al. [3] employed the leave-one-site-out over the TREC 2006 Terabyte collection and confirmed Zobel's conclusion. Further, the reusability of the collection by leaving out all manual runs was also tested. Given that manual runs are usually among the best performing ones, this did lead to somewhat different evaluation results.",1,TREC,True
30,"Sakai [13] employed leave-one-site-out, take-just-one-site, and take-just-three-sites over TREC and NTCIR data. His goal was to identify the effects of missing judgments on a number of different evaluation metrics. He considered all pairs of runs over the full judgment set and found the number with statistically significant differences, then repeated the process with judgments obtained by one of the three aforementioned methods and counted the errors. The results demonstrate that while the rankings of systems over the full and reduced set of judgments are similar, missing relevant documents leads to many errors of commission, i.e. finding differences significant even though they are not.",1,TREC,True
31,"Carterette et al. proposed that reusability should be evaluated in terms of the ability of the test collection to produce high confidence in evaluation results, specifically pairwise comparisons between systems [4] or width of confidence interval on an evaluation measure [7]. The former work used judgments from two systems to evaluate a larger set of 10 systems; the latter employed the leave-sites-out methodology discussed above to predict confidence interval width when evaluating new systems.",0,,False
32,"The simulation approaches above depend on having a fairly large number of judgments in the first place: any document that is selected for judging in the simulation phase must already have an actual judgment made by a human assessor. Without a fairly complete set of judgments it is likely that documents selected for judging will not actually have judgments; it is not possible to apply simulation to evaluate the reusability of TREC Million Query collections, for instance, because holding systems out would result in different documents being selected for judging than were originally judged for the track.",1,ad,True
33,2.3 Types of Reusability,0,,False
34,"Based on the work above, we identify three types of analysis that test collections are used for in evaluation:",0,,False
35,548,0,,False
36,"1. ""within-site"" analysis, in which a research/development site is conducting an experiment to determine which of several possible (internally-developed) systems to publish or deploy. We believe this is the most common use of test collections.",0,,False
37,"2. ""between-site"" analysis, in which one research/development site compares their results to those of another site, possibly relying on published results.",0,,False
38,"3. ""participant comparison"" analysis, in which a research/development site compares their results to those of the systems that are on record as participating in a particular track or task.",0,,False
39,"Site is TREC terminology, but it can be defined loosely; within a particular setting, any group of systems that are similar in some sense could be considered a ""site"". We use the term in that general sense throughout this work.",1,TREC,True
40,Our goal is to develop a methodology that can be used to test all three types of reusability when simulation is impossible due to the process used to select documents to judge.,0,,False
41,3. EXPERIMENTAL DESIGN,0,,False
42,"As discussed above, the standard design used in systembased IR evaluations is the repeated-measures design. This is appropriate for drawing conclusions about differences between systems, but it does not tell us anything about reusability. Furthermore, as discussed in Section 2.2, post-hoc evaluations of reusability are impossible when refinements to the implementation of the repeated-measures design such as statistical sampling or algorithmic selection were used.",1,hoc,True
43,"In our design, each system is held out from actual judgment collection for some queries. After the judging is complete, ""new"" systems are constructed by putting together all the queries from which a system was held out and evaluating it with the judgments contributed by the non-held-out systems for those same queries. Note that this means the reusability experiment can be performed only once.",0,,False
44,"Our design is meant to serve two ends: to draw conclusions about differences between systems, and to draw conclusions about the future reusability of the test collection that will result. It is meant to be ""fair"" in the sense that each system contributes judgments to the same number of queries. Since it can introduce bias or variance depending on which systems are held out from which queries, it attempts to minimize/control that as much as possible by ensuring that no two systems are held out of the same queries consistently. The complete description follows.",0,,False
45,3.1 Description of Design,0,,False
46,"We partition N topics into b + 1 sets T0, T1, . . . , Tb. The first set, T0, consists of n topics to which all systems contribute judgments. This is the standard repeated-measures design to ensure that we can answer questions about differences between these systems. It provides a baseline for answering questions about reusability.",0,,False
47,"In each subsequent set, a subset of systems are held out during judgment collection for each topic. The held-out set is different for each topic. Choosing which systems to held out can be done by site (if multiple sites have contributed systems): if there are m sites, k are held out from each query in the set; which k to hold out can be determined using round robin. The total number of queries must be a",0,,False
48,subset topic S1 S2 S3 S4 S5 S6,0,,False
49,T0,0,,False
50,t1 + + + + + +,0,,False
51,all-site и и и,0,,False
52,baseline tn + + + + + +,0,,False
53,T1,0,,False
54,tn+1 + + + + Г Г,0,,False
55,tn+2 + + + Г + Г,0,,False
56,tn+3 + + Г + + Г,0,,False
57,tn+4 + Г + + + Г,0,,False
58,tn+5 Г + + + + Г,0,,False
59,tn+6 + + + Г Г +,0,,False
60,tn+7 + + Г + Г +,0,,False
61,tn+8 + Г + + Г +,0,,False
62,tn+9 Г + + + Г +,0,,False
63,tn+10 + + Г Г + +,0,,False
64,tn+11 + Г + Г + +,0,,False
65,tn+12 Г + + Г + +,0,,False
66,tn+13 + Г Г + + +,0,,False
67,tn+14 Г + Г + + +,0,,False
68,tn+15 Г Г + + + +,0,,False
69,T2,0,,False
70,tn+16 + + + + Г Г,0,,False
71,иии иии,0,,False
72,tn+30 Г Г + + + +,0,,False
73,T3,0,,False
74,иии,0,,False
75,"Table 1: Illustration of proposed experimental design at the site level with m , 6 sites and k , 2 held out from each topic. Each column shows which topics a site contributed to. A + indicates that all of the sites' runs contributed judgments to the topic; Г indicates that the sites' runs did not contribute judgments. Each subset T1 . . . Tb has the same contribution pattern as subset T1.",0,,False
76,multiple of,0,,False
77,m k,0,,False
78,to ensure that each site is held out of the,0,,False
79,same number of queries.,0,,False
80,"This design is essentially a standard randomized, repeated-",0,,False
81,measures block design in which blocks are defined by which,0,,False
82,sites have been held out; there are,0,,False
83,m k,0,,False
84,blocks and b ob-,0,,False
85,servations in each block. Statistical tools such as mixed-,0,,False
86,effects ANOVA can be applied directly to answer questions,0,,False
87,about differences between individual systems. Answering,0,,False
88,questions about reusability will require some additional tools,1,ad,True
89,that we describe in the next section.,0,,False
90,The design is illustrated in Table 1 to give a sense of how,0,,False
91,it provides data for each of our three types of reusability:,0,,False
92,"1. ""within-site"": Within each subset Ti, each site con-",0,,False
93,tributes to,0,,False
94,m-1 k,0,,False
95,topics and is held out from,0,,False
96,m-1 k-1,0,,False
97,topics. Thus in addition to the n topics that all sites,1,ad,True
98,contribute,0,,False
99,"to,",0,,False
100,each,0,,False
101,site,0,,False
102,contributes,0,,False
103,to,0,,False
104,b,0,,False
105,m-1 k,0,,False
106,topics,0,,False
107,that,0,,False
108,can,0,,False
109,be,0,,False
110,used,0,,False
111,as,0,,False
112,a,0,,False
113,site,0,,False
114,"baseline,",0,,False
115,and,0,,False
116,to,0,,False
117,b,0,,False
118,m-1 k-1,0,,False
119,top-,0,,False
120,ics that can be used for testing reusability by compar-,0,,False
121,ing results on those topics to results on the site base-,0,,False
122,"line topics. In Table 1, for instance, the within-site",0,,False
123,reusability set for site S6 includes the first five topics,0,,False
124,"in each subset, e.g. topics numbered n+1 through n+5",0,,False
125,in subset T1. The within-site baseline includes the first,0,,False
126,n all-site baseline topics along with the last 10 in each,0,,False
127,"subset, e.g. those numbered n + 6 through n + 15 in",0,,False
128,subset T1.,0,,False
129,"2. ""between-site"": Within each subset Ti, each pair of",0,,False
130,sites contributes to the same,0,,False
131,m-2 k,0,,False
132,topics and is held,0,,False
133,out of the same,0,,False
134,m-2 k-2,0,,False
135,topics.,0,,False
136,The,0,,False
137,n,0,,False
138,+b,0,,False
139,m-2 k,0,,False
140,total,0,,False
141,549,0,,False
142,topics those two sites contribute to form a baseline for,0,,False
143,comparisons between those sites.,0,,False
144,The,0,,False
145,b,0,,False
146,m-2 k-2,0,,False
147,topics,0,,False
148,they were both held out from can be used to deter-,0,,False
149,"mine the between-site reusability. In Table 1, the first",0,,False
150,topic in each subset can be used for testing reusability,0,,False
151,"between sites S5 and S6 against the last six that both contributed to, along with the first n in the baseline.",0,,False
152,"3. ""participant comparison"": Within each subset Ti, there",0,,False
153,are,0,,False
154,m-2 k-1,0,,False
155,topics that one site contributes to and an-,0,,False
156,other site does not. These topics can be used to eval-,0,,False
157,uate comparing the non-contributing site to the con-,0,,False
158,"tributing site. In Table 1, if S5 is the ""participant baseline"" and S6 is the ""new system"", topics numbered n + 2 through n + 5 are part of the set used to test",0,,False
159,reusability.,0,,False
160,"The values b, n, k are parameters that need to be set by the researchers. Suppose we have an idea of how many total topics (N ) will be judged and how many total judgments there will be. This may be based on budget constraints, power analysis, previous work, or most likely a combination of all three. We can express the total number of queries as:",0,,False
161,m,0,,False
162,"N ,b",0,,False
163,+ n.,0,,False
164,k,0,,False
165,Let us further suppose that we want to guarantee that at least n0 topics are part of the baseline set that all systems contribute to. Then:,0,,False
166,N b m k,0,,False
167,+ n0,0,,False
168,b,0,,False
169,N,0,,False
170,- n0,0,,False
171,m,0,,False
172,k,0,,False
173,"For a given m and k, we can set",0,,False
174,"b,",0,,False
175,N - n0,0,,False
176,m,0,,False
177,k,0,,False
178,and,0,,False
179,"n,N -b m",0,,False
180,k,0,,False
181,"Determining k is then a matter of creating a table of values and determining which produces the best distribution of topics among the three types of reusability for answering the questions important to the researchers. Note that larger k provides more topics for between-site experiments, but requires more total topics; smaller k provides more topics for within-site experiments. All design parameters and their relationships to each other are summarized in Table 2.",0,,False
182,3.2 Statistical Methods for Analysis,0,,False
183,"We need to be able to determine whether the evaluation results over ""new systems"" (restricted to the held-out topics) match the evaluation results over the same systems when they contribute to the judgments. If we were using this design for the TREC Robust track in 2004, for example, we might like to know whether the runs submitted by Johns Hopkins' Applied Physics Lab (APL) are ranked the same when evaluated over 210 topics they contributed judgments to as when evaluated over 39 topics they did not contribute to. This is a statistical question: even if the collection is perfectly reusable, we are evaluating systems over two different sets of topics with two different sample sizes, and we therefore must expect that some evaluation results will change due to chance alone. This must therefore have a statistical answer, i.e. a p-value that will allow us to reject reusability if the evidence is against it.",1,TREC,True
184,"More specifically, there are three questions of interest:",0,,False
185,number of sites,0,,False
186,m,0,,False
187,total number of topics,0,,False
188,N,0,,False
189,min. size of baseline set,0,,False
190,n0,0,,False
191,number of held-out sites,0,,False
192,k,0,,False
193,number of topic subsets,0,,False
194,b,0,,False
195,size of all-site baseline set n,0,,False
196,size of within-site baseline,0,,False
197,size of between-site baseline,0,,False
198,size of within-site reuse set,0,,False
199,size of between-site reuse set,0,,False
200,size of participant-comparison set,0,,False
201,fixed by researchers,0,,False
202,fixed by budget,0,,False
203,fixed by researchers,0,,False
204,variable,0,,False
205,"b, n,",0,,False
206,(N - n0)/,0,,False
207,N,0,,False
208,-b,0,,False
209,m k,0,,False
210,m k,0,,False
211,n+b,0,,False
212,m-1 k,0,,False
213,n+b,0,,False
214,m-2 k,0,,False
215,b,0,,False
216,m-1 k-1,0,,False
217,b,0,,False
218,m-2 k-2,0,,False
219,b,0,,False
220,m-2 k-1,0,,False
221,Table 2: A summary of parameters of the experimental design and how they relate to each other. Some parameters can be treated as fixed values. At least one is a variable that must be chosen in consideration of certain tradeoffs. The rest are functions of those.,1,ad,True
222,1. Are systems that are significantly different over topics they contributed to also significantly different over topics they did not contribute to? (Likewise with nonsignificant differences.),0,,False
223,2. Is the relative ordering of systems over topics they contributed to the same as the relative ordering over topics they did not contribute to?,0,,False
224,3. Do the system scores averaged over the topics it contributed to match the scores averaged over the topics it did not contribute to?,0,,False
225,"The first--agreement in statistical significance--is the most important but also the most difficult to discern, so we focus on that. If the first fails, the second--relative orderings being the same--still provides some reusability. The third is a sufficient but not necessary condition for the second; we care about the measures being the same to the extent that they have some extrinsic meaning that we want to keep.",0,,False
226,3.2.1 Agreement in statistical significance,0,,False
227,"Testing for agreement in statistical significance is somewhat complicated. The first step is simple: use some significance test to determine whether pairs of systems are significantly different. We recommend a t-test, possibly adjusting the p-values to account for the family-wise error rate growing with the number of pairwise comparisons (the so-called ""multiple comparisons problem"" [12]). After performing two sets of pairwise tests (one set for all pairs of systems over the baseline topics, one for the same pairs over the reusability topics), we can form a contingency table showing the agreement in significance between the two sets of tests. The five runs submitted by APL to the TREC 2004 Robust track provide an example:",1,ad,True
228,reuse tests p < 0.05 p  0.05,0,,False
229,baseline tests,0,,False
230,p < 0.05 p  0.05,0,,False
231,6,0,,False
232,0,0,,False
233,3,0,,False
234,1,0,,False
235,"Among the 10 pairwise comparisons, six resulted in a significant difference being found over both the baseline and reusability topics. Three had a significant difference over the baseline topics but not over the reusability topics. One had no significant difference in either set.",1,ad,True
236,550,0,,False
237,So we can see that there are three errors of omission and,0,,False
238,none of commission. The question is whether these errors,0,,False
239,are outside the realm of what is expected. Note that we must,0,,False
240,expect some errors just because of the difference in topic set,0,,False
241,sizes between the two experiments. Thus the next step is to,0,,False
242,construct a contingency table of expected agreement between,0,,False
243,"the two sets of tests, then compare our observed values to",0,,False
244,the expected in a statistically sound way.,0,,False
245,We will use power analysis to construct the expected con-,0,,False
246,"tingency table. Power analysis is a very deep topic, and we",0,,False
247,unfortunately do not have space to go into details. For more,0,,False
248,information we suggest Cohen's book [10] or two recent pa-,0,,False
249,"pers in the IR literature [9, 16]. The high-level view is that",0,,False
250,the power of a test is equivalent to the probability that the,0,,False
251,p-value would be deemed significant for any sample of the,0,,False
252,"same size. Power is a function of the effect size, the sample",0,,False
253,"size, and the significance level. Effect size is a measure of the",0,,False
254,degree of difference between two systems over the hypotheti-,0,,False
255,cal population of topics; for the t-test effect size is estimated,0,,False
256,as the mean difference in average precisions divided by the,0,,False
257,standard deviation of the differences. Power monotonically,0,,False
258,increases with both effect size and sample size.,0,,False
259,We can estimate the power of a given pairwise test by,0,,False
260,estimating the effect size and plugging that along with sam-,0,,False
261,ple size and significance level (usually 0.05) into a power,0,,False
262,function (available for most widely-used statistical software,0,,False
263,packages). This power estimate can then be treated as the,0,,False
264,expectation that the test would be found significant at the,0,,False
265,0.05 level. The power of the comparison over the reusability,0,,False
266,"topics uses the same process, only with the smaller sample",0,,False
267,size instead of the baseline topic sample size.,1,ad,True
268,"For example, the mean difference in average precision be-",0,,False
269,tween APL runs rsTs and rsDw is 0.046 over the 210 baseline,1,AP,True
270,"topics, and the standard deviation is 0.176. The effect size",0,,False
271,"is 0.046/0.176 ,"" 0.260, which would be considered a mod-""",0,,False
272,erate effect. The power of a test comparing those two runs,0,,False
273,"over 210 topics is 0.964, i.e. there is a 96% chance that a",0,,False
274,significant difference between them would be found for any,0,,False
275,set of 210 topics. If the sample size is reduced to 39 (the,0,,False
276,"size of the reusability set), the power drops to 0.354.",0,,False
277,Now the expectation that both tests come out significant,0,,False
278,is simply the product of their estimated powers. For the two,0,,False
279,"runs above, that is 0.964 и 0.354 , 0.341; we add 0.341 to the",1,ad,True
280,number of expected positive agreements. The probability,0,,False
281,that the first comes out significant but the second does not,0,,False
282,"is .964 и (1 - 0.354) ,"" 0.623, so we add that to the number""",1,ad,True
283,"of expected errors of omission. We add (1 - 0.964) и 0.354 ,",1,ad,True
284,"0.013 to the number of expected errors of commission, and",0,,False
285,"(1 - 0.964) и (1 - 0.354) , 0.023 to the number of expected",0,,False
286,negative agreements. Continuing in the same way for all 10,0,,False
287,pairs of APL's runs produces the table of expected values:,1,AP,True
288,baseline expectation,0,,False
289,reuse expect. p < 0.05 p  0.05,0,,False
290,p < 0.05,0,,False
291,7.098,0,,False
292,0.073,0,,False
293,p  0.05,0,,False
294,2.043,0,,False
295,0.786,0,,False
296,"By inspection this table is not very different from the observed values. The final step is to verify that statistically. We do that using a 2 goodness-of-fit test for whether the observed values match the expected1. In this case they do: the p-value of the 2 test is 0.88, meaning we cannot",0,,False
297,"1In this case, because the number of observations is small, we actually use a randomized ""exact"" version of the 2 test.",0,,False
298,"conclude that the tables are different, and therefore cannot conclude that the collection is not reusable for this site--we tentatively would say that other sites that are creating runs ""like"" APL's can trust in the reusability of this collection.",1,AP,True
299,"To test between-site reusability, we use the same process, but only test significance between pairs of runs from different sites. For example, if the two sites are APL and IBM, we would only look at significant differences between each APL run and each IBM run (over the intersection of topics they contributed to or were held out from), but not between two APL runs or two IBM runs. Apart from that consideration, the analysis proceeds in exactly the same way. Likewise, participant-comparison uses the same process but uses the topics that one site contributed to and the other did not.",1,AP,True
300,3.2.2 Relative ordering of systems,0,,False
301,"There are many well-known rank correlation statistics that can be used to determine whether the systems are ordered the same between the two sets of topics. Kendall's  is the most frequently used; it is calculated by subtracting the number of pairs of systems that have been swapped between two rankings from the number in the same order. Like our significance test procedure above, it calculates counts over pairs of systems; to adapt it to between-site and participantcomparison reusability, we can count pairs that are different between sites while ignoring those from the same site.  does not have a notion of the expected number of errors that is meaningful for reusability.",1,ad,True
302,"Carterette introduced an alternative measure of rank similarity called drank that takes into account similarity of systems amongst themselves [5]. If the systems are more similar, some reordering is expected, and the measure is smaller. drank can provide a p-value for the reusability ranking being similar to the baseline ranking.",0,,False
303,3.2.3 Agreement in system scores,0,,False
304,"To determine whether the system scores agree, we can calculate a point estimator such as root mean square error: RM SE , 1/n ni,""1(M APi - M APi )2, where M APi is the baseline M AP and M APi is the reusability MAP. The larger this is, the more error is present between the two sets of scores. However, RMSE does not have a known distribution that can be used to determine a p-value, so its interpretation is somewhat subjective.""",1,AP,True
305,4. VALIDATION,0,,False
306,We present three validation experiments. The first simply demonstrates that it is indeed possible to use our analysis in Section 3.2.1 to disprove reusability. The next two show conversely that if a collection is reusable the p-value is not likely to be low.,0,,False
307,4.1 Disproving reusability,0,,False
308,"A very simple way to validate that reusability will be rejected when it is not true is to simulate evaluation over a non-reusable collection. For example, we can use random number generation to simulate evaluation measures for m systems and show that the 2 p-value will be low when the simulation is explicitly set up so that the evaluation measures differ between the baseline and reusability sets. We drew measures from beta distributions (ensuring they would be between 0 and 1) such that the measures drawn for reusability topics for one run would be lower than those",0,,False
309,551,0,,False
310,reuse p < 0.05,0,,False
311,p  0.05,0,,False
312,"baseline p < 0.05 O , 196 E , 189.5 O , 57 E , 62.1",0,,False
313,"p  0.05 O,2 E , 4.3 O , 45",0,,False
314,"E , 44.1",0,,False
315,"Table 3: Observed versus expected agreement in significance results for within-site reusability aggregated over all Million Query 2008 sites. The 2 p-value is 0.58, indicating no evidence to reject reusability.",1,Query,True
316,"drawn for the baseline topics for the same run and as a result the significance tests involving those runs would not agree. The result is that as the number of reusability topics increases, the p-values decrease, with 50 reusability topics in this scenario producing a p-value less than 0.01.",0,,False
317,4.2 Robustness to differences in topic samples,1,Robust,True
318,"By robustness to differences in topic samples, we mean that conclusions about reusability are not expected to be confounded by the fact that the tests are based on different size samples of different topics (as in Section 3.2.1 above). To show this, we set up an idealized scenario in which the test collection must be reusable and show that our analysis will not reject reusability.",0,,False
319,"Our data is the 2008 TREC Million Query (MQ) track data consisting of 564 topics with 15,000 total judgments collected from 25 systems submitted by 9 different sites [1]. Every run contributed judgments to every topic; none were held out. We chose n0 ,"" 200 topics to be the baseline that all systems """"contribute"""" to. For each of the remaining 364 topics, we """"held out"""" k "", 2 sites. Plugging into the formula above results in b , 10 topic sets.",1,TREC,True
320,"The AP value for each system/topic is simply that calculated for the track. This makes a 100% reusable collection: the AP estimates on the ""held-out"" topics are exactly the same as they were when the systems actually contributed judgments. This is (intentionally) highly artificial, but note that it is not meant to be a simulation of judgment collection or of evaluation. It is a boundary case to demonstrate that our conclusions will not be confounded by variance in the topic samples when reusability is true. There are other sources of bias and variance that this test does not address.",1,AP,True
321,"Rather than apply the procedure described in Section 3.2.1 to each site individually (running the risk of multiple comparisons problem), we aggregated the contingency tables and expected contingency tables across sites to obtain two tables representing all within-site comparisons. They are shown together in Table 3. The 2 p-value is 0.58, indicating no evidence to suggest the difference in topic samples is causing a problem. We did the same for between-site reusability and participant-comparison reusability; the 2 p-values are 0.54 and 0.36, respectively.",0,,False
322,4.3 Robustness to held-out systems,1,Robust,True
323,"Another possibility is that holding certain systems out will inject bias into topic evaluations. For example, if a very good system that retrieves many relevant documents is held out, evaluation results for the other systems may not be as accurate, even when reusability holds in other cases. To test this we use simulation in the Robust 2004 data described",1,Robust,True
324,reuse p < 0.05,0,,False
325,p  0.05,0,,False
326,"baseline p < 0.05 O , 130 E , 135.4 O , 127 E , 121.6",0,,False
327,"p  0.05 O , 17 E , 13.9 O , 160 E , 163.1",0,,False
328,Table 4: Observed versus expected agreement in,0,,False
329,significance results for within-site reusability aggregated over all Robust 2004 sites. The 2 p-value is,1,Robust,True
330,"0.74, indicating no evidence to reject reusability.",0,,False
331,within site bseline queries (average 211 quereis) site reusability queries (average 38 queries),0,,False
332,"site baseline depth,100 0.45",0,,False
333,JUR,0,,False
334,0.4,0,,False
335,NLP,0,,False
336,SAB,0,,False
337,0.35,0,,False
338,APL,1,AP,True
339,FUB,0,,False
340,0.3,0,,False
341,HUM,0,,False
342,ICL,0,,False
343,0.25,0,,False
344,MPI,0,,False
345,PIR,0,,False
346,0.2,0,,False
347,POL,0,,False
348,UIC,0,,False
349,0.15,0,,False
350,UOG,0,,False
351,VTU,0,,False
352,0.1,0,,False
353,WDO,0,,False
354,0.05,0,,False
355,0 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45,0,,False
356,allsites baseline (all 249 queries),0,,False
357,"site reusability depth,100 0.45",0,,False
358,JUR,0,,False
359,0.4,0,,False
360,NLP,0,,False
361,SAB,0,,False
362,0.35,0,,False
363,APL,1,AP,True
364,FUB,0,,False
365,0.3,0,,False
366,HUM,0,,False
367,ICL,0,,False
368,0.25,0,,False
369,MPI,0,,False
370,PIR,0,,False
371,0.2,0,,False
372,POL,0,,False
373,UIC,0,,False
374,0.15,0,,False
375,UOG,0,,False
376,VTU,0,,False
377,0.1,0,,False
378,WDO,0,,False
379,0.05,0,,False
380,0 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45,0,,False
381,all sites baseline (all 249 queries),0,,False
382,Figure 1: Robust 2004 simulation with 2 sites held out per topic. Judgments were based on a pool of depth 100. The left plot compares MAP over the 210 site baseline topics to true MAP calculated with all judgments. The right compares MAP over the 39 site reusability topics to true MAP.,1,Robust,True
383,"above. Note that Robust 2004, despite being large by TREC standards, is fairly small for this design; because there are only 249 topics, we have no all-site baseline set, and we cannot hold more than k ,"" 2 sites out. The number of topics we have for each of the three types of tests is limited (39 for within-site, only 3 for between-site). The advantage is that Robust 2004 has very many relevance judgments, so we can simulate pools of any depth.""",1,Robust,True
384,"Once again, this is validation that the design works when reusability is true. To ensure reusability to the greatest degree possible, we simulated a depth-100 pool. That is, for each topic, runs submitted by two sites were held out of simulated judging; the other 12 sites had their top 100 ranked documents judged according to the existing judgments in the TREC qrels file. We then evaluated all runs using that pool and separated them into systems that contributed and systems that were held out.",1,ad,True
385,"We only have enough topics for within-site analysis. The observed and expected significance results are shown in Table 4; the p-value is 0.74, indicating no evidence to reject reusability. We performed the same test on shallower pools; for pools of depth 10, 20, and 50, the p-values are 0.63,0.58, and 0.60, respectively. Figure 1 shows the comparison of evaluation results on different topic sets in the depth-100 pool. Note that the fact that we have only 39 topics for reusability testing is somewhat limiting, however.",0,,False
386,5. IN SITU REUSABILITY EXPERIMENT,0,,False
387,The analysis above provides evidence that our design is correct. We next observe it in a real experimental setting: judgment collection for the 2009 TREC Million Query (MQ) track [8]. Eight participating sites submitted a total of 35,1,TREC,True
388,552,0,,False
389,"runs over 1,000 queries. The corpus was the Category B subset of the new ClueWeb09 web collection. 638 of the 1,000 queries were converted to full topics and judged; of those, 146 formed the all-site baseline to which every run contributed judgments. The remaining 492 topics had two sites held out during judging. Held-out sites were selected by round-robin scheduling. Assessors did not know whether they were judging a reusability topic or not, and topic order was randomized, so there is no reason to suppose that the reusability topic sample is biased compared to the baseline sample. Assessors made a total of 34,534 judgments (54 per topic on average), of which 26% were either relevant or highly relevant. There were 95 topics for which no relevant documents were found.",1,ClueWeb,True
390,"The Million Query track uses two official evaluation measures, statMAP and MTC's ""expected"" MAP. Both are estimates of average precision, but they are designed for different purposes. statMAP is an unbiased estimator of average precision. MTC EMAP is a biased estimator meant to provide good comparative evaluation.",1,Query,True
391,Our goal in this section is to determine the extent to which this test collection is reusable.,0,,False
392,5.1 Results,0,,False
393,"Reusability results for MQ are illustrated in Figure 2, which shows statMAP (top) and MTC EMAP (bottom) scores of runs over (a) 145 baseline against 170 site baseline topics (left), (b) 170 site baseline against 160 site reuse topics (center), and (c) 145 baseline against 160 site reuse topics (right). Each run was evaluated over all the topics it contributed to and all the topics it was held out from, but since different sites contributed to different topics, no two sites were evaluated over exactly the same set of topics.",1,MQ,True
394,"As mentioned in Section 4, differences in mean scores over baseline topics and mean scores over reusability topics for a given site may be due to a number of different effects: (1) the baseline and reuse topics are two different topic sets of different size; (2) apart from the site under study there are two other sites that did not contribute documents to each reusability topic; (3) the site under study itself did not contribute documents to the reuse topics (this is the actual effect we would like to quantify); and finally, (4) for this particular study the fact that both methods evaluate runs with a very small number of documents introduces some variability even in the baseline topics.",0,,False
395,"The plots in Figure 2 attempt to separate the second and third effects. Essentially, the comparison of the mean scores between the 145 baseline topics and the 160 site reuse topics (right) summarizes the results of the reusability experiment, and it is what an actual new site would observe by using the MQ 2009 collection. StatMAP scores over the reuse topics are positively correlated with the statMAP scores over the baseline topics, though the correlation is rather weak. MTC EMAP scores over these two sets of topics are well correlated. One can consider the other two plots as the decomposition of the effects seen in the right plot. The left plot illustrates the effect of holding out sites other than the site under study. For the statMAP case this has a rather strong effect on the scores computed, though it is minimal for the MTC scores. The middle plots try to isolate the effect of holding out the site under study. As can be seen, this also has a strong effect on the statMAP scores, while the effect is mild in the case of the MTC scores.",1,MQ,True
396,reuse p < 0.05,0,,False
397,p  0.05,0,,False
398,"baseline p < 0.05 O , 257 E , 302.5 O , 133 E , 85.1",0,,False
399,"p  0.05 O , 41 E , 26.2 O , 100 E , 117.2",0,,False
400,"Table 5: Observed versus expected agreement in significance results for between-site reusability aggregated over all Million Query 2009 sites. The 2 p-value is 0, indicating sufficient evidence to reject reusability.",1,Query,True
401,"The plots give a visual sense of reusability, suggesting within-site may be acceptable at the level of rank agreement if not score agreement, but between-site is likely not acceptable. To quantify this, we computed three correlation statistics as described in Section 3.2.2. First we computed the overall Kendall's tau between the ranking induced by the scores in the two topic sets. This is a rough estimate of the between-site reusability. For statMAP scores this is 0.7643, while for MTC EMAP scores this is 0.8350, both of which are rather low. Next we computed the Kendall's  among the runs of each individual site to estimate withinsite reusability; Table 6 shows these. Note that the values are not comparable across sites since the number of runs compared affects the Kendall's  values. Finally, we computed a  -like correlation to quantify the ability to compare ""new"" runs to contributing participants. For each site, we count the number of its reusability runs that are correctly ordered against the baseline runs and the number that have been swapped with a baseline run. Every comparison involves exactly one run for that site; for this measure we do not compare two runs from the same site or two runs from a different site. The final value is determined identically to Kendall's  ; the set of values can be seen in Table 6.",1,MAP,True
402,"The significance test agreement procedure, when applied to this data, suggests that there is not enough evidence to reject within-site reusability (p > 0.5), but there is more than enough to reject between-site reusability (p < 0.01). To explain how within-site reusability holds despite some of the low  correlations in Table 6, we note that  is not able to capture anything about whether swaps are ""reasonable"". The lowest  is -0.6 for UIUC, but by inspection (Fig. 2) UIUC's systems are all very close to each other. It is perfectly reasonable that they would be ordered differently over another set of topics, and thus the low  is not a concern. For between-site reusability, however, we have seen that it is unlikely; that the 2 test confirms this is a point in its favor. The full contingency table for between-site reusability is shown in Table 5.",0,,False
403,6. CONCLUSIONS,0,,False
404,"We have proposed an experimental design that can be used during construction of large test collections to collect evidence for or against the future reusability of the collection. It is appropriate for when the set of judgments is too small to be able to evaluate reusability through simulation; since test collections are moving in this direction, some framework will be necessary for determining whether these collections can be reused. We presented tools for statistical analysis and demonstrated their use in artificial data, a",0,,False
405,553,0,,False
406,statAP over site reusability queries (average 160),1,AP,True
407,statAP over site reusability queries (average 160),1,AP,True
408,statAP over within site baseline queries (average 170),1,AP,True
409,0.25,0,,False
410,0.2,0,,False
411,0.15 0.1 0.08,0,,False
412,iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr,0,,False
413,0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.26 0.28 statAP over all sites baseline queries (all 145 queries),1,AP,True
414,0.25,0,,False
415,0.2,0,,False
416,0.15 0.1 0.08,0,,False
417,iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr,0,,False
418,0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.26 0.28 statAP over within site baseline queries (average 170),1,AP,True
419,0.25,0,,False
420,0.2,0,,False
421,0.15 0.1 0.08,0,,False
422,iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr,0,,False
423,0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.26 0.28 statAP over all sites baseline queries (all 145),1,AP,True
424,MTC over site reusability queries (average 160),0,,False
425,MTC over site reusability queries (average 160),0,,False
426,MTC over within site baseline queries (average 170),0,,False
427,0.12,0,,False
428,0.1,0,,False
429,0.08 0.06 0.04,0,,False
430,0.03,0,,False
431,iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr,0,,False
432,0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11 0.12 0.13 MTC over all sites baseline queries (all 145 queries),0,,False
433,0.12,0,,False
434,0.1,0,,False
435,0.08 0.06 0.04,0,,False
436,0.03,0,,False
437,iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr,0,,False
438,0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11 0.12 0.13 MTC over within site baseline queries (average 170),0,,False
439,0.12,0,,False
440,0.1,0,,False
441,0.08 0.06 0.04,0,,False
442,0.03,0,,False
443,iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr,0,,False
444,0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11 0.12 0.13 MTC over all sites baseline queries (all 145 queries),0,,False
445,"Figure 2: StatMAP and MTC EMAP scores of systems over (a) 145 baseline against 170 site baseline topics, (b) 170 site baseline against 160 site reuse topics, and (c) 145 baseline against 160 site reuse topics.",1,MAP,True
446,within-site  participant comparison,0,,False
447,statAP MTC statAP MTC,1,AP,True
448,iiith 0.333 0.333 0.750 0.938,0,,False
449,IRRA 1.000 0.800 0.547 1.000,0,,False
450,NEU 0.200 1.000 1.000 1.000,0,,False
451,Sabir 0.333 1.000 0.987 0.840,0,,False
452,UDel 0.800 0.600 0.573 0.933,0,,False
453,ECEUdel 0.800 0.800 0.773 0.707,0,,False
454,UIUC -0.600 0.800 0.773 0.947,0,,False
455,uogTr 1.000 1.000 0.939 0.909,0,,False
456,"Table 6: Rank correlations based on Kendall's  for site baseline to site reusability (top) and for comparison of site reusability to the ""original"" TREC runs excluding those treated as new (bottom).",1,TREC,True
457,"simulation experiment, and a real-life implementation of the design; in general their results confirm our intuitions about the evaluation.",0,,False
458,"Clearly there is much more and much deeper analysis we could do. For this work we chose to present some of the topics we felt were most important in presenting this methodology, but we certainly intend to continue investigating other tools for analysis, more sophisticated statistical methods, and of course IR-centric implications for the failure (or lack of failure) of reusability when it happens.",0,,False
459,Acknowledgements,0,,False
460,The authors gratefully acknowledge support by the European Commission who funded parts of this research within the Accurat project (FP7-ICT-248347) and by the Marie Curie IIF (FP7-PEOPLE-2009-IIF-254562).,0,,False
461,7. REFERENCES,0,,False
462,"[1] J. Allan, J. A. Aslam, B. Carterette, V. Pavlu, and E. Kanoulas. Overview of the TREC 2008 million query track. In Proceedings of TREC, 2008.",1,TREC,True
463,"[2] J. A. Aslam and V. Pavlu. A practical sampling strategy for efficient retrieval evaluat ion, technical report.",0,,False
464,"[3] S. Buеttcher, C. Clarke, P. Yeung, and I. Soboroff. Reliable information retrieval evaluation with incomplete and biased judgements. In Proceedings of SIGIR, pages 63Г70, 2007.",0,,False
465,"[4] B. Carterette. Robust test collections for retrieval evaluation. In Proceedings of SIGIR, pages 55Г62, 2007.",1,Robust,True
466,"[5] B. Carterette. On rank correlation and the distance between rankings. In Proceedings of SIGIR, 2009.",0,,False
467,"[6] B. Carterette, J. Allan, and R. K. Sitaraman. Minimal test collections for retrieval evaluation. In Proceedings of SIGIR, pages 268Г275, 2006.",0,,False
468,"[7] B. Carterette, E. Gabrilovitch, V. Josifovsky, and D. Metzler. Measuring the reusability of test collections. In Proceedings of WSDM, 2009.",0,,False
469,"[8] B. Carterette, V. Pavlu, H. Fang, and E. Kanoulas. Overview of the TREC 2009 million query track. In Notebook Proceedings of TREC, 2009.",1,TREC,True
470,"[9] B. Carterette and M. D. Smucker. Hypothesis testing with incomplete relevance judgments. In Proceedings of CIKM, pages 643Г652, 2007.",0,,False
471,"[10] J. Cohen. Statistical Power Analysis for the Behavioral Sciences. Lawrence Erlbaum, 2nd edition, 1998.",0,,False
472,"[11] D. Harman. Overview of the second text retrieval conference (trec-2). Inf. Process. Manage., 31(3):271Г289, 1995.",1,trec,True
473,"[12] J. P. A. Ionnidis. Why most published research findings are false. PLoS Med., 2(8), 2005.",0,,False
474,"[13] T. Sakai. Comparing metrics across trec and ntcir: the robustness to system bias. In Proceedings of CIKM, pages 581Г590, 2008.",1,trec,True
475,"[14] E. M. Voorhees. The philosophy of information retrieval evaluation. In CLEF '01: Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems, pages 355Г370, London, UK, 2002. Springer-Verlag.",1,CLEF,True
476,"[15] E. M. Voorhees. Overview of trec 2009. In Proceedings of TREC, 2009. Notebook draft.",1,trec,True
477,"[16] W. Webber, A. Moffat, and J. Zobel. Statistical power in retrieval experimentation. In Proceedings of CIKM, pages 571Г580, 2008.",0,,False
478,"[17] J. Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In Proceedings of SIGIR, pages 307Г314, 1998.",0,,False
479,554,0,,False
480,,0,,False

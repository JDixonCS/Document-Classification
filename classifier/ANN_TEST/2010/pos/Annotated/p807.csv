,sentence,label,data,regex
0,Learning to Rank Query Reformulations,1,Query,True
1,"Van Dang, Michael Bendersky and W. Bruce Croft",0,,False
2,"Center for Intelligent Information Retrieval Department of Computer Science University of Massachusetts Amherst, MA 01003",0,,False
3,"{vdang, bemike, croft}@cs.umass.edu",0,,False
4,ABSTRACT,0,,False
5,"Query reformulation techniques based on query logs have recently proven to be effective for web queries. However, when initial queries have reasonably good quality, these techniques are often not reliable enough to identify the helpful reformulations among the suggested queries. In this paper, we show that we can use as few as two features to rerank a list of reformulated queries, or expanded queries to be specific, generated by a log-based query reformulation technique. Our results across five TREC collections suggest that there are consistently more useful reformulations in the first positions in the new ranked list than there were initially, which leads to statistically significant improvements in retrieval effectiveness.",1,Query,True
6,Categories and Subject Descriptors,0,,False
7,H.3.3 [Information Search and Retrieval]: Query Formulation,1,Query,True
8,General Terms,0,,False
9,"Algorithms, Measurement, Performance, Experimentation.",0,,False
10,Keywords,0,,False
11,"Query reformulation, query expansion, query log, query performance predictor, learning to rank.",1,Query,True
12,1. INTRODUCTION,1,DUC,True
13,"Query logs have become an important resource for many tasks including query reformulation [3, 6]. Most log-based reformulation techniques, however, are evaluated using nonstandard approaches and proprietary query logs, making it hard to compare one to another. A more recent study [2] compares different techniques using TREC collections and finds that when intial queries have relatively high quality, query expansion is much more reliable than substitution.",1,Query,True
14,"Although the log-based expansion technique [2] can generate some good reformulations for high-quality TREC queries, it also produces many bad reformulations and it does not generate a reliable ranking of the reformulations by quality.",1,TREC,True
15,"In this paper, we show that we can effectively rerank the list of reformulated queries obtained with this log-based expansion approach. By using as few as two features, SCQ",0,,False
16,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
17,"(Similarity Collection Query) [8] and query clarity [1], we can substantially improve the ranking of reformulated queries in terms of the quality of the reformulations in the top two ranks (measured by NDCG@2 ), which then leads to significant improvements in retrieval effectiveness.",1,Query,True
18,2. METHOD,0,,False
19,2.1 Log-based Query Expansion,1,Query,True
20,"The log-based query expansion method [2] (referred to as LQE) is a slight modification of the query substitution method proposed by Wang and Zhai [6]. It first estimates a context distribution for terms occuring in a query log. It then constructs a translation model that can suggest similar words based on their distributional similarity. Given any query, the expansion model will try to expand it with candidates suggested by the translation model for each query term. The model decides whether to expand the query based on how similar the candidate is to the query term and how appropriate it is to the context of the query. For more details, see [2].",0,,False
21,2.2 The Reranking Approach,0,,False
22,"Query quality predictors aim to predict a query's quality without explicit relevance judgements. Thus, given a ranked list of reformulated queries, it is intuitive to think about reorganizing this list based on the ""quality"" score given by some predictor.",1,Query,True
23,We tried some of the top-performing predictors that Kumaran and Carvalho [4] used in a similar task and found that,0,,False
24,"[8] and clarity score [1] are the most effective for our problem. Therefore, we rerank the list of expanded queries by",0,,False
25,"( ), 1×",0,,False
26,( )+ 2×,0,,False
27,(),0,,False
28,where 1 and 2 are weight of the two predictors.,0,,False
29,Table 1: Statistics of queries used for reformulation,0,,False
30,AP WSJ Robust-04 WT10G Gov-2,1,AP,True
31,Title Q. 133 133,0,,False
32,200,0,,False
33,66,0,,False
34,119,0,,False
35,Desc. Q. 150 150,0,,False
36,246,0,,False
37,94,0,,False
38,134,0,,False
39,807,0,,False
40,Table 3: Evaluation of retrieval effectiveness in terms of MAP.  and  indicate significant difference to the,1,MAP,True
41,original query and LQE's ranked list respectively. Best result in each column is marked in bold.,0,,False
42,Title Query,1,Query,True
43,Description Query,1,Query,True
44,AP,1,AP,True
45,WSJ,0,,False
46,RBT-04 WT10G Gov-2,1,WT,True
47,AP,1,AP,True
48,WSJ RBT-04 WT10G Gov-2,1,WT,True
49,Orig-Q 0.1694 0.2594,0,,False
50,0.2247,0,,False
51,0.1904 0.2829,0,,False
52,0.1660 0.2358 0.2519 0.1770 0.2518,0,,False
53,LQE,0,,False
54,0.1741 0.2563,0,,False
55,0.2297,0,,False
56,0.1911 0.2559,0,,False
57,0.1694 0.2391 0.2538,0,,False
58,0.1775,0,,False
59,0.2497,0,,False
60,Rerank 0.1749 0.2663 0.2382 0.1962 0.2901 0.1820 0.2374 0.2584 0.1836 0.2579,0,,False
61,"Table 2: Our approach (""Rerank"") consistently out-",0,,False
62,performs LQE in NDCG@2. All differences are signif-,0,,False
63,icant at < 0.05,0,,False
64,Collection,0,,False
65,Title Query,1,Query,True
66,Desc. Query,1,Query,True
67,LQE Rerank LQE Rerank,0,,False
68,AP,1,AP,True
69,0.2434 0.4805 0.2307 0.3728,0,,False
70,WSJ,0,,False
71,0.2318 0.5040 0.2250 0.3296,0,,False
72,Robust-04 0.2905 0.5559 0.2138 0.3687,1,Robust,True
73,WT10G 0.2673 0.5499 0.1680 0.3847,1,WT,True
74,Gov-2 0.1933 0.5830 0.2059 0.4093,1,Gov,True
75,3. EVALUATION,0,,False
76,3.1 Experiment Settings,0,,False
77,"In this section, we evaluate the performance of our reranking technique. Evaluation is done on five TREC collections: AP, WSJ, Robust-04, WT10G and Gov-2, with both title and description queries. We use the language modeling framework and remove all stop words at indexing time. We adopt the parameter settings for LQE from the authors [2].",1,TREC,True
78,"Due to the limited coverage of the available query log [5], we use only a subset of TREC queries where the LQE can generate at least one reformulation. Information about these subsets is given in Table 1.",1,TREC,True
79,"On each collection, we first use LQE to generate a list of expanded queries ( ,"" 30) for each original query. We append to this list the original query - in the case when all generated reformulations are bad, the reranking approach has a chance to choose not to reformulate. We then use our approach to rerank this list and compare its performance with that of the intial list as well as original query.""",1,ad,True
80,3.2 Training Data,0,,False
81,"We run LQE with the MSN log to obtain a list of reformulations for each original query. We use all these queries to do retrieval and record their MAP and use them to create our dataset. Training and testing are done using 5-fold cross validation on this dataset. 1 and 2 are learned using AdaRank [7] to maximize the average NDCG@2. The algorithm ends up choosing either ( 1 ,"" 1, 2 "", 0) or ( 1 ,"" 0, 2 "", 1) depending on the collection.",1,MAP,True
82,3.3 Reranking Effectiveness,0,,False
83,"We use NDCG@2 to measure the quality of the ranked list of reformulations given by our approach. Reformulations are graded on a scale from zero to four with respect to the improvement they provide over the original query. In particular, improvement larger than 0.03 corresponds to a 4, or ( > 0.03)  4. Similarly, (0.01 <  0.03)  3, (0 <  0.01)  2, ( , 0)  1 and ( < 0)  0.",1,ad,True
84,Table 2 summarizes the result: the list of reformulations ranked by our approach has a much higher average NDCG@2,0,,False
85,than the initial list. All improvements are statistically significant at < 0.05 using a two-tailed t-test.,0,,False
86,3.4 Retrieval Effectiveness,0,,False
87,"We define the MAP of a ranked list of reformulations as the best MAP observed among its top queries. In this section, we compare the MAP obtained by (i) the original query, (ii) the list of reformulations generated by LQE, and (iii) the list reranked by our method.",1,MAP,True
88,"As can be seen in Table 3, the best of the top two reformulated queries ranked by our approach is almost always significantly better than the original query. This is not the case in LQE. In many cases, our method also provides significant improvements over LQE. This result suggests that the reranking can push better reformulations to the first two positions in the ranked list.",0,,False
89,4. CONCLUSIONS,0,,False
90,"In this paper, we have shown that by reranking the list of reformulations generated by the log-based query expansion technique [2] with only two features, we can push more good reformulations into the first two positions in the list. This is reflected in the huge gain of NDCG@2 and statistically significant improvement in retrieval effectiveness. In the future, we will investigate more features. We hope this will lead to greater improvement in NDCG@1, helping retrieval systems to reformulate queries implicitly without user involvement.",1,ad,True
91,5. ACKNOWLEDGMENTS,0,,False
92,"This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS0711348, and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.",0,,False
93,6. REFERENCES,0,,False
94,"[1] S. Cronen-Townsend, Y. Zhou, and W.B. Croft. Predicting Query Performance. In Proc. of SIGIR, pages 299-306, 2002.",1,Query,True
95,"[2] V. Dang and W.B. Croft. Query Reformulation Using Anchor Text. In Proc. of WSDM, pages 41-50, 2010.",1,Query,True
96,"[3] R. Jones, B. Rey and O. Madani. Generating Query Substitutions. In Proc. of WWW, pages 387-396, 2006.",1,ad,True
97,"[4] G. Kumaran and V.R. Carvalho. Reducing Long Queries Using Query Quality Predictors. In Proc. of SIGIR, pages 564-571, 2009.",1,Query,True
98,"[5] Proc. of the 2009 workshop on Web Search Click Data, Barcelona, Spain. ACM New York, NY, USA, 2009.",0,,False
99,"[6] X. Wang and C. Zhai. Mining Term Association Patterns from Search Logs for Effective Query Reformulation. In Proc. of CIKM, pages 479-488, 2008.",1,Query,True
100,"[7] J. Xu and H. Li. AdaRank: A Boosting Algorithm for Information Retrieval. In Proc. of SIGIR, pages 391-398, 2007.",0,,False
101,"[8] Y. Zhao, F. Scholer, and Y. Tsegay. Effective Pre-retrieval Query Performance Prediction Using Similarity and Variability Evidence. In Proc. of ECIR, pages 52-64, 2008.",1,Query,True
102,808,0,,False
103,,0,,False

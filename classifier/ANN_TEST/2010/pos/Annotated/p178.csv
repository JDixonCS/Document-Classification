,sentence,label,data,regex
0,Analysis of Structural Relationships for Hierarchical Cluster Labeling,0,,False
1,Markus Muhr,0,,False
2,"Know-Center Graz Inffeldgasse 21a 8010 Graz, Austria",0,,False
3,mmuhr@know-center.at,0,,False
4,Roman Kern,0,,False
5,"Know-Center Graz Inffeldgasse 21a 8010 Graz, Austria",0,,False
6,rkern@know-center.at,0,,False
7,Michael Granitzer,0,,False
8,Know-Center Graz Graz University of Technology,0,,False
9,"Inffeldgasse 21a 8010 Graz, Austria",0,,False
10,mgrani@know-center.at,0,,False
11,ABSTRACT,0,,False
12,"Cluster label quality is crucial for browsing topic hierarchies obtained via document clustering. Intuitively, the hierarchical structure should influence the labeling accuracy. However, most labeling algorithms ignore such structural properties and therefore, the impact of hierarchical structures on the labeling accuracy is yet unclear. In our work we integrate hierarchical information, i.e. sibling and parentchild relations, in the cluster labeling process. We adapt standard labeling approaches, namely Maximum Term Frequency, Jensen-Shannon Divergence, 2 Test, and Information Gain, to take use of those relationships and evaluate their impact on 4 different datasets, namely the Open Directory Project, Wikipedia, TREC Ohsumed and the CLEF IP European Patent dataset. We show, that hierarchical relationships can be exploited to increase labeling accuracy especially on high-level nodes.",1,ad,True
13,Categories and Subject Descriptors,0,,False
14,H.3.1 [Content Analysis and Indexing]: Linguistic processing; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,0,,False
15,General Terms,0,,False
16,Algorithms,0,,False
17,Keywords,0,,False
18,"Cluster Labeling, Statistical Methods, Topic Hierarchies, Structural Information",0,,False
19,1. INTRODUCTION,1,DUC,True
20,"Browsing large-scale document collections usually requires a structural organization form like topic hierarchies. Unsupervised machine learning techniques, foremost document clustering, overcome the labor intensive, manual creation of",0,,False
21,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
22,"such topic hierarchies by automatic partitioning of unstructured document collections into browse-able cluster hierarchies. This cluster based browsing approach has been shown to successfully improve access to unstructured document collections [5, 16].",0,,False
23,"However, even if an algorithm achieves a perfect hierarchical partitioning, users must guess the content of each cluster somehow. Automatically generated labels provide such a descriptive cluster summary. Obviously, the label quality strongly influences the navigation effectiveness as shown by human created topic hierarchies like the Open Directory Project: while users may disagree on the exact label of a topic, every user can exploit labeling information for navigation purposes as long as labeling is of high quality - this is especially true for high level nodes like sports, computers etc.",0,,False
24,"Most existing labeling approaches extract labels by comparing term distributions of a cluster to a reference collection and taking the statistically most discriminative terms. Intuitively, for a flat partitioning this seems to be sufficient, but insufficient for creating topic hierarchies similar to the Open Directory Project (ODP) 1; Child clusters have to be described in the context of their parent cluster and must not contain the same labels. Such a constraint cannot be ensured without taking structural relationships between clusters into account. Moreover, label quality tends to decrease on higher levels due to higher degree of abstraction. Most state of the art labeling approaches (e.g. [11, 1]) do not use structural relationships. Although there are approaches considering hierarchical relationships - either through supervised learning [19] or through hierarchical post-processing of flat cluster labels [14] - to our best knowledge there is no systematic investigation whether the intuitive claim above holds or not. Intuitively it also seems to be natural that labeling performance as well as the influence of structural relationships depends on a topic hierarchies structural properties - a claim hard to investigate due to missing test corpora.",1,ODP,True
25,"In this paper we investigate the influence of hierarchical relationships on the cluster labeling process. We extend standard labeling approaches, namely Maximum Term Frequency, Jensen-Shannon Divergence, 2, and Information Gain, to include structural information. First, Maximum Term Frequency labeling is extended by a sibling based weighting scheme, yielding to a new labeling algorithm called ICWL (Inverse Cluster frequency Weighted) labeling. Second, we extend all labeling approaches with parent-",1,CW,True
26,1http://dmoz.org,0,,False
27,178,0,,False
28,"child relationships. Comparing all labeling approaches on four datasets, namely the Open Directory Project (ODP), Wikipedia, TREC-Ohsumed and European Patents (EP), shows that hierarchical information influences the labeling process. Especially Wikipedia shows the biggest dependency on hierarchical information, followed by the ODP dataset. This finding yields to an important point for future work on this topic: top level nodes, which are most important in the users browsing process, are labeled badly. Hence, the browsing process of automatically created cluster hierarchies could be improved by using hierarchical cluster label algorithms.",1,ODP,True
29,With our work we contribute to the field of cluster labeling by,0,,False
30,· extending standard labeling approaches to take use of hierarchical information,0,,False
31,"· showing, that sibling relationships can be exploited to improve statistical labeling methods",0,,False
32,"· showing, that the structure of the hierarchy and the domain of the test dataset have a strong influence on the labeling accuracy, especially for top level nodes, which are crucial for user navigation",0,,False
33,"· using traditional, but also new test corpora for evaluating cluster labeling algorithms",1,ad,True
34,"The paper is structured as follows: first a related work section provides an overview of the state of the art relevant for our work, second we introduce a formal definition of structure based labeling, followed by the third part, a description of the utilized corpora (ODP, Wikipedia, Ohsumed, European Patents) together with the implemented preprocessing steps. The paper ends by outlining results with an obligatory discussion pointing out our findings, and finally a summary of our work with a discussion of the implications for future work.",1,corpora,True
35,2. RELATED WORK,0,,False
36,"Standard labeling approaches extract most prominent terms in a specific cluster by statistical feature selection [11]. A straightforward feature selection method takes the maximum sum of the individual term frequencies of documents assigned to a cluster [5]. In [14], a weighting schema has been introduced to improve maximum sum of term frequencies by neglecting stop-words or general words.",0,,False
37,"However, maximum-sum approaches prefer terms which are over-represented in the whole document collection. This increases the probability that all cluster are getting similar labels. More sophisticated approaches consider a terms discriminative power compared to a reference collection usually the documents of all sibling clusters. Well-known methods include an adapted versions of Information Gain [7], 2 Test [14], and the Jensen-Shannon Divergence (JSD) [2]. Comparative studies of reference collection based labeling approaches done in [1] favor JSD for the non-hierarchical case. However, in the hierarchical case such an in-depth comparison between labeling approaches is missing. Further, the evaluation conducted in [1] favors leaf nodes of a hierarchy without addressing questions on the influence of hierarchical structures.",1,ad,True
38,"Besides statistical term selection methods, researchers focused on using different document parts like title [5], hyperlink anchors [9] etc. or different features like named entities",0,,False
39,"[17], frequent phrases [13] or text summarization [15]. However, our work focuses not on finding the very best labeling approach including the best document representation, but to include structural properties into statistical labeling approaches. Treeratpituk et. al. [19] addressed this problem via supervised learning. Weights for different term importance measures depending on parent-child relationships are estimated using supervised learning. However, their method needs training data to determine the actual weights. Through supervised training and the use of synonyms in the labeling process the actual influence of hierarchical relationships remains unclear. Further work in hierarchical labeling has been done by Popescul and Ungar [14], where parent child relationships are exploited in a post processing step and not directly included into the feature selection process. Hence, errors from the labeling process are propagated to the post processing step.",1,ad,True
40,"In the past years researchers investigated the usage of external knowledge to enhance machine learning tasks through extending the given term set with synonyms, hypernyms, hyponyms etc. Most prominent resources for this task are WordNet [3] and Wikipedia [1]. Especially Carmel et. al. [1] showed that using Wikipedia as a thesaurus and applying this thesaurus as post-processing step to statistical labeling approaches improves cluster labeling dramatically. Although they used statistical labeling approaches in a first step, hierarchical information was not included. So by improving the statistical labeling their approach may be further enhanced.",1,Wiki,True
41,"Evaluating cluster labeling approaches face the dataset sparsity problem. Most methods mentioned above use the Open Directory Project (ODP) [12] as well as the flat 20 newsgroup dataset. This basically restricts the obtained results on the domain of web resources. Whether the results apply also to other domains like patents, medicine etc. remains an open question. We address this question by conducting experiments, in addition to the ODP, also on Wikipedia [6], the TREC-Ohsumed collection [18] and the European Patents provided by the 2009 CLEF IP Task [4].",1,ODP,True
42,3. STRUCTURE BASED LABELING,0,,False
43,"In order to analyze the impact of hierarchical relationships we gradually incorporate structure information into well-known labeling techniques. We use a maximum term weight labeling approach as well as a reference collection based labeling approach to estimate the labels of a cluster. Through weighting methods based on the position of a document relative to the label candidate cluster, we introduce structure information in terms of (i) sibling relations and (ii) parent-child relations. All approaches consider bag-of-word document representations and hierarchical relationships among clusters. Before describing the labeling approaches in detail, we introduce a formalism for denoting cluster hierarchies, document sets as well as a notation for selecting particular sub-trees and and sub document sets.",1,ad,True
44,"Formally, we define D as the set of all documents and a cluster cl as a sub set of documents cl  D; C ,"" {c1 . . . ck} denotes as set of non-overlapping clusters, i.e. i"",jci  cj , . The hierarchical structure between clusters is formalized as is-a relationship cj  ci (cj ,"" ci), defining that cj is the parent (direct parent) of ci. For specifying the set of clusters with parent cj, we write Ccj and denote all documents contained in this sub-hierarchy as Dcj. Se-""",0,,False
45,179,0,,False
46,"mantically the is-a relationship should resemble a classical topic hierarchy, which assumes that if a document is assigned to a topic, it is also assigned to its parent topic. For the rest of this paper we consider the is-a relationship as implicitly given when referring to the set of clusters C or any subset of them. Further, documents are represented as a term frequency vector d  d. Using a vector representation allows us to consider different weighting methods to assess the a-priori importance of terms in a corpus resp. a cluster. Labels Lj of a cluster cj are represented as set of terms Lj , {t1 . . . tl}.",0,,False
47,3.1 Maximum Term Weight Labeling,0,,False
48,"Labeling can now be seen as function Lj  label(C, cj) selecting the most suitable terms for describing cluster cj as label Lj. The simplest labeling functions are basic feature selection techniques [11]: Documents assigned to a cluster are aggregated and the k largest features are taken as labels. We refer to this as Maximum Term Weight Labeling (MTWL): given cluster cj as labeling candidate the MTWL can be written as",0,,False
49,X ,0,,False
50,Lj  bestk,0,,False
51,di,0,,False
52,(1),0,,False
53,diDcj ,0,,False
54, where bestk v is a function returning the terms associated,0,,False
55,with the k largest dimensions of vector v. In our experiments we refer to this approach as M T W Lraw.,0,,False
56,"Maximum Term Weight Labeling strongly depends on the document representation. Given only term frequency document vectors, labeling will likely extract terms occurring in a large number of documents with high frequency. Such labels do not necessarily discriminate between clusters. Global weighting schemes like TFIDF or Okapi BM25 [11] allow to increase the discrimination capability of labels based on the underlying document distribution. Since documents in the same cluster share similar terms, the inverse document frequency reduces the influence of terms occurring in a high number of documents and therewith in a high number of cluster. To introduce such global weighting, MTWL can be simply extended as follows:",0,,False
57,"X

",0,,False
58,Lj  bestk,0,,False
59,idfglobal · tf W eight(di) (2),0,,False
60,diDcj ,0,,False
61,"where idfglobal  d is a vector containing the corpus dependent inverse document frequencies, tf W eight() is a function applying the document specific part of the weighting scheme and · is the Hadamard point-wise product. The inverse document frequency for a term k is calculated as",1,ad,True
62," |D|

",0,,False
63,"idfglobal,k , log",0,,False
64,"+1 #(tk, D)",0,,False
65,(3),0,,False
66,"with #(tk, D) returning the number of documents in the collection containing term k. For the term frequencies we used",0,,False
67,the document specific part of the standard Okapi BM25 as well as plain term frequencies2. Since idfglobal is defined over the collection D of all documents we refer to this approach,0,,False
68,as global weighting approach.,0,,False
69,2We split the weighting scheme in a collection and a document specific part in order to have a homogeneous notation over our different labeling approaches.,0,,False
70,"Global weighting penalizes terms, which are over-represented in the whole collection. However, terms over-represented in a particular cluster sub-tree only, will be likely selected for all siblings in the cluster hierarchy. For example, given that term tk is over-represented in cluster cj and equally distributed among the direct children ci with cj ,"" ci, then it is very likely that term tk will become a label of the direct child ci. Hence, term distributions among siblings have to be taken into account to avoid siblings getting similar labels.""",0,,False
71,"For considering sub-tree dependent term distribution we again add a local, sub-tree based inverse document frequency term to eq. 2. Formally, the labeling function is defined as",1,ad,True
72,"X

",0,,False
73,Lj  bestk,0,,False
74,"idfglobal · idflocal,j · tf W eight(di)",0,,False
75,diDcj ,0,,False
76,(4),0,,False
77,"where idflocal,j is the inverse document frequency vector over the document collection Dcpwith cp , cj where cp is defined as the parent cluster of cj. Simply speaking this document collection consists of all documents in the subtree",0,,False
78,spanned by the parent cluster cp. In particular the idf entry for term k in cluster cj is calculated as,0,,False
79,"idflocal,j",0,,False
80,",",0,,False
81," log

|Dcp  |

#(tk, Dcp)

+

 1",0,,False
82,(5),0,,False
83,"with #(tk, Dcp) returning the number of documents in the reference collection which contain term k. In our exper-",0,,False
84,iments we refer to this approach as M T W Lidf .,0,,False
85,3.2 Reference Collection based Labeling,0,,False
86,"Besides taking the largest dimension of a centroid vector, comparative statistics like the 2-Test or the JensenShannon Divergence (JSD) can estimate whether occurrences of a term differ between a cluster and a reference collection with statistical significance. Such terms yield good labels for a cluster. In our Reference Collection based Labeling approach (RCL) we use well known comparative statistics, namely the Jensen-Shannon Divergence, Information Gain and 2, in order to compare terms contained in clusters to terms contained in a reference collection of documents, denoted as Dref . The k terms with the best test values are taken as labels. Hierarchical information is incorporated through the selection of the reference collection. Formally, we denote",1,corpora,True
87,"

",0,,False
88,"Lj  bestk J SD(Dref , Dcj )",0,,False
89,(6),0,,False
90,"as labeling function where J SD(Dref , Dcj)  d returns the Jensen-Shannon Divergence (see [2]) for each dimen-",0,,False
91," sion in form of a d-dimensional vector. Again, bestk v",0,,False
92,is used to select the k terms with the best statistical test,0,,False
93,"value. Similarly we abbreviate the Information Gain as IG(Dref , Dcj ) [7] and 2 as 2(Dref , Dcj ) [14]. The probability for a term is estimated in a standard manner as",0,,False
94,the number of occurrences of a term divided by the total,0,,False
95,number of occurrences of terms.,0,,False
96,For labeling cluster cj we define the reference collection as all documents belonging to the cluster sub-tree of its di-,0,,False
97,"rect parent excluding all documents contained in cj. Formally, the reference collection is given as Dref , Dcp \ Dcj with cp ,"" cj , where cp is the parent of cj . In the non-hierarchical case this corresponds to the best known""",0,,False
98,standard labeling approaches [1].,0,,False
99,180,0,,False
100,3.3 Inverse Cluster Weight Labeling,0,,False
101,"MTWL incorporates hierarchical information through a specialized weighting function idflocal,j. The weighting function depends on the term distribution over documents in the sub-tree, but does not take the term distribution over sibling cluster into account.",1,corpora,True
102,"To integrate sibling information we add another weighting factor. The weighting factor is inspired by the recently introduced Class-Feature-Centroid (CFC) classifier using class discriminative terms for achieving classification accuracies similar to Support Vector Machines [10]. Similarly we want to take use of clusters discriminative terms. Basically, if one term occurs often in one sibling cluster only, this term should be preferred over terms occurring in all sibling cluster: a term k in cluster cj is weighted by its inverse cluster frequency calculated as",1,ad,True
103,"icfj,k",0,,False
104,",",0,,False
105,"exp "" #(tk, Dcj) "" log "" #(cp)",0,,False
106,|Dcj |,0,,False
107,"#(tk, cp)",0,,False
108,+,0,,False
109," 1

(7)

with cp being the direct parent of cj, #(tk, cp) being the number of direct subcluster of cp containing term k and #(cp) being the number of direct sub-clusters. The exponential component, similar to the CFC classifier, promotes terms occurring in a larger fraction of documents. In our experiments we refer to this approach as ICW Lraw if M T W Lraw is extended by the inverse cluster frequency weight and to ICW Lidf in the case of M T W Lidf .

3.4 Hierarchical Labeling

Especially in the hierarchical case, relying only on sibling information may be problematic; a term occurring often in the parent cluster (and all its documents) may also occur often in one or several child clusters. While incorporating sibling information potentially removes parent labels equally distributed over potentially all siblings, it cannot overcome parent labels occurring often in a few child clusters. Hence, parent child relationships have to be taken into account.
Hierarchical labeling extends all labeling approaches introduced before, by weighting the influence of a term inverse proportional to the path length of the child cluster to the label candidate cluster cj. By denoting the path length between two clusters as l(j, i), the labeling function of cluster cj can be formally written as

X 1",1,CW,True
110,"

Lj  bestk

l(j, i)  cfl(j,i) · vj,i

(8)

ciCcj 

where cfl(j,i) is the sibling based cluster frequency term vector and vj,i is the result of the comparison statistics formally written as

X

vj,i = idfglobal · idflocal,j ·

tf W eight(do) (9)

do ci

in case of MTWL labeling strategy. Simply speaking, vj,i is the centroid vector of cluster ci weighted in the local context of the label candidate cluster cj. This principle is similarly applied to the ICWL and the RCL approach.
Contrary to the inverse cluster frequency weighting above, the cluster frequency term vector penalizes terms occurring only in a single cluster on a particular hierarchy level (i.e. l(j, i)). The cluster frequency weight for term tk is simply the number of clusters a term occurs in divided by the total number of clusters on this particular hierarchy level.

The idea behind the cluster frequency weight is to promote terms occurring in a higher number of child clusters since those terms are most likely representative labels for their parent. In our experiments, all results involving hierarchical weighting are prefixed with Hier"".",1,CW,True
111,4. DATASETS,0,,False
112,"For our experiments we used 4 different datasets: two general domain corpora, namely Wikipedia and Open Directory Project, and two domain specific corpora, namely European Patents (EP) and Ohsumed. All datasets have been preprocessed in the same way: document tokenization has been done using OpenNLP3; Tokens haven been stemmed afterwards using the Snowball4 stemmer. Finally, stop-words have been removed by using the list supplied by the Snowball stemmer.",1,corpora,True
113,"Open Directory Project (ODP): We imported a large part of the hierarchy including the top categories arts, business, games, health, home, news, society, and sports with their complete subtree. We took only hard links into account ignoring symbolic links and related topic links. Letter categorizations are ignored as well. This yielded about 150,000 categories and about 800,000 documents. In order to compare the effect of the manually created descriptions and titles for each ODP entry, we created two sub-datasets. In the first dataset, named ODP Title & Description, each document consists of the description and title as provided in the ODP hierarchy. For the second dataset, named ODP HTML, we crawled the HTML page a ODP entry pointed to and all HTML pages links in the crawled page pointed to, i.e. we performed a crawl of depth 2. This crawling strategy should provide sufficient content rich pages producing a rather different dataset compared to the classical ODP Title & Description dataset.",1,ODP,True
114,"Wikipedia: To extract the structural information out of the Wikipedia we started with the XML dump of the English version. From each entry within the dump we extracted the title and all links that indicate an assignment to a Wikipedia category. This was done for all articles and category pages such that we were able to reconstruct the classification relationships. We filtered out categories that do not carry any semantic information and assembled a category blacklist (e.g. ""Wikipedia maintenance') and filtered out categories containing the word ""by"", ""of"" and ""in"" to eliminate categories like ""Authors by Year"". In order to create a tree structure of of the acyclic category graph we started by each main topic - namely arts, computing, health, and sports we traversed the graph in a breath-first manner with a maximum depth of 10. Since a breath-first search of depth 10 would return a too large portion of the Wikipedia graph, we randomly chose 10 outgoing links and 80 documents for each topic. Roughly we had about 50,000 categories with about 400,000 documents as test dataset.",1,Wiki,True
115,"TREC Ohsumed: We used the Ohsumed collection from the 2001 TREC evaluation. The hierarchical structure has been obtained downloading the Mesh Tree hierarchy5 of 2004 with 7724 different categories and 348,564 documents.",1,TREC,True
116,European Patents (EP): European patents are taken from,0,,False
117,3http://opennlp.sourceforge.net/ 4http://snowball.tartarus.org/ 5http://www.nlm.nih.gov/mesh/,0,,False
118,181,0,,False
119,80,0,,False
120,Document - Label Overlap,0,,False
121,ODP - Title & Description ODP - HTML Wikipedia Oshumed European Patents,1,ODP,True
122,60,0,,False
123,40,0,,False
124,Percent,0,,False
125,20,0,,False
126,0,0,,False
127,1,0,,False
128,2,0,,False
129,3,0,,False
130,4,0,,False
131,5,0,,False
132,6,0,,False
133,7,0,,False
134,8,0,,False
135,Hierarchy Depth,0,,False
136,Figure 1: Document-Label-Overlap: Fraction of documents containing all label terms depending on the distance to the category.,0,,False
137,"the dataset which has been created for the Intellectual Property track of CLEF 2009 (CLEF-IP) [8]. From this dataset we selected only patents that were granted and limited the timespan from 1991 to 2000. We ended up with 265,409 patents, each of them having at least one assignment to the IPC classification scheme6. This IPC classification hierarchy consists of over 60,000 classes arranged in a tree-like manner with 8 root categories. The claims section has been used as document content.",1,CLEF,True
138,5. RESULTS,0,,False
139,"In order to measure labeling accuracy we use the mean average precision (MAP) averaged over all categories. To calculate the MAP, category labels - similar to documents - are tokenized, stemmed and stopword filtered resulting in a set of terms. This set of terms is compared to the ranked list of candidate terms returned by the labeling algorithm, which gives the MAP value for one category.",1,MAP,True
140,"We did not use any synonyms or external linguistic resources. To ensure that terms in the document set contain the terms extracted from the category labels, we estimated the Document-Label-Overlap, as outlined in the next section.",0,,False
141,5.1 Document Label Overlap,0,,False
142,"The Document-Label-Overlap estimates whether a certain label of a cluster is contained in its connected documents at all. The Overlap is calculated as the fraction of documents containing all label terms to the number of total documents in the sub-tree with depth d. Thus, the overlap determines the baseline on getting a correct label for a topic. Further, by considering the overlap of documents with a particular path",0,,False
143,6http://www.wipo.int/classifications/ipc/en/,0,,False
144,length d to the label candidate cluster we get an evidence on the influence of documents on particular hierarchy depths.,0,,False
145,"Figure 1 depicts the document label overlap for all datasets. Clearly, results show a significant decrease in the fraction of documents containing the actual label with the hierarchy level; a correct label is more likely found in documents close to the cluster. Furthermore, documents with high path lengths are more specialized and thus tend to use a more specialized vocabulary. For example, an article on Support Vector Machines might not mention the words machine learning explicitly, since it is a specialized topic in the field of machine learning. Hence, this analysis supports the evidence that structural properties play a role in cluster labeling.",0,,False
146,"A comparison between datasets point out interesting differences: for Ohsumed, ODP with Title & Description and the Wikipedia dataset the overlap drops significantly with increasing depth while it decreases rather slowly for the ODP HTML dataset. Clearly, ODP HTML contains more terms per document therewith increasing the likelihood of finding the correct label. The European Patents dataset shows its special nature: the overlap is constantly low over all hierarchy depths.",1,ODP,True
147,5.2 Labeling Accuracy,0,,False
148,"To evaluate the influence of the hierarchy depth on the labeling process, we plot the MAP on each dataset and labeling approach for sub-hierarchies of depth 1-8. We limit our analysis to sub-hierarchies with a maximum depth of 8 since there are too few sub-hierarchies with a larger depth making a statistical evaluation infeasible. Note also that the distribution of hierarchies is skewed: there are far more hierarchies with depth 1 than with depth > 1. Therefore, the overall labeling accuracy is approximately the labeling accuracy of depth 1 sub-hierarchies. This is contrary to the browsing behavior of a user who needs high labeling accuracy on the top nodes, i.e. on sub-trees with depth >1.",1,MAP,True
149,"Figure 2 shows the labeling accuracy for the ODP dataset, split into description based documents and crawled HTML based documents. Figure 3 reports results on the Ohsumed and Wikipedia dataset. Note that for the clarity of presentation we only show the JSD labeling approach for RCL based labeling techniques. Compared to IG and 2 (as well as their hierarchical counterparts), JSD always achieved the best performance. This supplements the findings in literature and extends them also to the hierarchical case, see [1].",1,ODP,True
150,"Comparison of labeling techniques: Comparing the different labeling techniques it can be seen that the sibling based labeling approach with local and global weighting ICW Lidf is performing about as good as the maximum term weighting approach, with exception of the the Wikipedia dataset where the integration of the sibling information does improve the accuracy. The labeling method that uses a reference collection JSD provides good results for every dataset. This is especially pronounced for the ODP Title & Description dataset when incorporating the hierarchical structure into the creation of the reference collection.",1,CW,True
151,"Flat vs. Hierarchical labeling: Table 1 depicts the absolute MAP differences between hierarchical and flat approaches. For each of the tested combinations of datasets and labeling algorithms, the integration of hierarchical information always improves the accuracy. One exception is the Ohsumed dataset, where hierarchical and flat methods perform ap-",1,MAP,True
152,182,0,,False
153,ODP Title & Description,1,ODP,True
154,ODP HTML,1,ODP,True
155,0.6,0,,False
156,0.6,0,,False
157,0.5,0,,False
158,0.5,0,,False
159,0.4,0,,False
160,0.4,0,,False
161,0.3,0,,False
162,MAP,1,MAP,True
163,0.3,0,,False
164,MAP,1,MAP,True
165,0.2,0,,False
166,0.2,0,,False
167,0.1,0,,False
168,0.1,0,,False
169,0.0,0,,False
170,MTWLraw MTWLidf JSD ICWLraw ICWLidf,1,CW,True
171,hierMTWLraw hierMTWLidf hierJSD hierICWLraw hierICWLidf,1,CW,True
172,1,0,,False
173,2,0,,False
174,3,0,,False
175,4,0,,False
176,5,0,,False
177,6,0,,False
178,7,0,,False
179,8,0,,False
180,Hierarchy Level,0,,False
181,0.0,0,,False
182,MTWLraw MTWLidf JSD ICWLraw ICWLidf,1,CW,True
183,hierMTWLraw hierMTWLidf hierJSD hierICWLraw hierICWLidf,1,CW,True
184,1,0,,False
185,2,0,,False
186,3,0,,False
187,4,0,,False
188,5,0,,False
189,6,0,,False
190,7,0,,False
191,8,0,,False
192,Hierarchy Level,0,,False
193,Figure 2: Performance of the different labeling algorithms for the Open Directory Project dataset. Algorithms that exploit the hierarchical structure generally produce better results.,0,,False
194,proximately equal. Analyzing the sub-trees of top nodes we could observe a high variance of the influence of hierarchical labeling approaches which deserves further analysis. We assume that the increase depends on structural properties. A assumption to be validated in future work.,0,,False
195,"Ohsumed comes from a rather narrow domain compared to ODP and Wikipedia and is structurally different from all other datasets: the MESH-categorization includes documents only at leaf categories. Further, descriptions are rather small even compared to the ones from ODP. One can expect that these descriptions will not contain information about categories despite their direct parent - a fact also supported by the Document-Label-Overlap. For this reason, it is not surprising that hierarchical information increases labeling accuracy only slightly, since an already sparse information on term distributions is further reduced. This is also reflected by the fact that no labeling algorithm could outperform the other. The relative average difference for the MAP measure of the hierarchical labeling approaches in comparison with their flat counterparts is 0.004 for this dataset.",1,ODP,True
196,"The Wikipedia dataset and the ODP datasets show a completely different picture. Flat labeling accuracy decreases significantly with the depth of the hierarchy; hierarchical labeling gives a slight average accuracy increase of 0.059 in case of the ODP HTML dataset, an accuracy increase of around 0.099 MAP on average in case of the ODP Description & Title dataset and a larger accuracy increase of around 0.132 MAP on average in case of Wikipedia (see also table 1). In case of the ODP dataset, the increase through hierarchical relationships on the Title & Description dataset is slightly better than on the HTML dataset. As depicted by the Document-Label-Overlap analysis, the HTML crawled documents most likely contain a broader range of terms",1,Wiki,True
197,"compared to the title and description of the original ODP dataset. A broader range of terms increases the likelihood that a document contains a cluster label, even if it is farther away. This indicates that non-specialized documents on deeper nodes in a tree do not decrease labeling accuracy - a rather seldom case in topic hierarchies. In such a case, hierarchical weighting actually removes information on the topic instead of reducing the impact of certain more specific documents.",1,ODP,True
198,"Regarding Wikipedia, the graph like category structure has to be considered, which has been adopted to a treestructure in our case. For this reason, we took four main categories (Arts, Computing, Health, Sports) and created a topic hierarchy using the mentioned sampling strategy. While the sampling strategy seems to be fair, it does not reproduce the correct Wikipedia Category graph. Instead, we get a rather balanced tree like structure. Nevertheless, given such a balanced structure it is quite obvious that local idf weighted as well as hierarchical approaches benefit to a large degree. Especially the labeling of high level nodes could be dramatically increased through incorporating hierarchical information and significantly outperforms the nonhierarchical approaches. Moreover, this holds for all different labeling approaches.",1,Wiki,True
199,"The results for the European Patents are not depicted due to rather low accuracies. Nevertheless we mention them to support recent findings in the CLEF-IP challenge where it was shown that patents are a rather specialized domain [8]. Well-known information retrieval approaches failed to achieve good results in the challenge and it seems to be the same with cluster labeling methods. All approaches fail completely by only achieving values in the field of 0.03, although it seems to be the case the ICWL again improved the results. Also, the Document-Label Overlap shows that",1,CLEF,True
200,183,0,,False
201,0.6,0,,False
202,Wikipedia,1,Wiki,True
203,MTWLraw MTWLidf JSD ICWLraw ICWLidf,1,CW,True
204,hierMTWLraw hierMTWLidf hierJSD hierICWLraw hierICWLidf,1,CW,True
205,0.6,0,,False
206,Oshumed,1,Oshumed,True
207,MTWLraw MTWLidf JSD ICWLraw ICWLidf,1,CW,True
208,hierMTWLraw hierMTWLidf hierJSD hierICWLraw hierICWLidf,1,CW,True
209,0.5,0,,False
210,0.5,0,,False
211,0.4,0,,False
212,0.4,0,,False
213,0.3,0,,False
214,MAP,1,MAP,True
215,0.3,0,,False
216,MAP,1,MAP,True
217,0.2,0,,False
218,0.2,0,,False
219,0.1,0,,False
220,0.1,0,,False
221,0.0,0,,False
222,0.0,0,,False
223,1,0,,False
224,2,0,,False
225,3,0,,False
226,4,0,,False
227,5,0,,False
228,6,0,,False
229,7,0,,False
230,8,0,,False
231,Hierarchy Level,0,,False
232,1,0,,False
233,2,0,,False
234,3,0,,False
235,4,0,,False
236,5,0,,False
237,6,0,,False
238,7,0,,False
239,8,0,,False
240,Hierarchy Level,0,,False
241,"Figure 3: Performance of the different labeling algorithms for the Wikipedia and the Oshumed dataset. Both datasets demonstrate different characteristics, for the Wikipedia dataset the structural information increases the labeling performance, whereas for the Oshumed dataset neither the sibling nor the child-parent relationships help to find matching labels.",1,Wiki,True
242,ODP - Title & Description ODP - HTML Wikipedia Oshumed,1,ODP,True
243,M T W Lraw,0,,False
244,0.06 0.04 0.08 0.00,0,,False
245,M T W Lidf,0,,False
246,0.09 0.07 0.12 0.01,0,,False
247,JSD,0,,False
248,0.15 0.09 0.19 0.01,0,,False
249,ICW Lraw,1,CW,True
250,0.08 0.05 0.12 0.00,0,,False
251,ICW Lidf,1,CW,True
252,0.12 0.05 0.16 0.00,0,,False
253,Average,0,,False
254,0.099 0.059 0.132 0.004,0,,False
255,"Table 1: Average relative difference of the MAP for all hierarchy levels greater than 2 for all datasets between the different methods either with and without exploitation of hierarchical information. Exploiting the hierarchical structure always improves the accuracy, although for the Oshumed dataset the difference is not pronounced.",1,MAP,True
256,only below 10 % of the documents connected to a category contain a label term.,0,,False
257,Overall our results imply that incorporating hierarchical information improves labeling accuracy on average.,1,corpora,True
258,"Moreover, our results have impact on the evaluation of cluster labeling approaches for browsing topic hierarchies. Viewed from a users point of view, flat labeling approaches support the browsing of leaf nodes rather than the browsing of high level nodes - a result quite contradictory to the users need. Especially in the case of the de-facto standard benchmark dataset, the ODP Description & Title dataset, this has to be taken into account for future evaluations. First, sampling data for an evaluation should consider the a-priori distribution of sub-hierarchies with different depths. By using depth independent random samples for cluster labeling evaluation it is very likely to draw hierarchies of depth 1 and to achieve good labeling performance using flat labeling approaches. Second, hierarchies of different depth should be",1,ad,True
259,evaluated separately in order to deduce the impact of the labeling strategy on the users navigational support.,0,,False
260,6. CONCLUSION AND OUTLOOK,0,,False
261,Our results show that structural relationships influence the labeling accuracy. Using sibling information increases labeling accuracy in some datasets; integrating hierarchical information produces better labeling results for all datasets. This insight has several consequences.,0,,False
262,"Firstly, evaluation of cluster labeling approaches have to take hierarchical properties into account, especially if the goal is to support user navigation.",0,,False
263,"Secondly, correlations between the properties of a hierarchy, like for example maximum depth, branching factor, documents per leaf node etc., the richness of the assigned documents and the achievable labeling accuracy should be further analyzed. While we followed the evaluation approach conducted by other researchers in the field, there should be a closer evaluation whether cluster hierarchies and manually created hierarchies resemble the same statistical properties w.r.t the document collection.",0,,False
264,184,0,,False
265,"Thirdly, more sophisticated approaches like for example the extension of JSD with hierarchical information may further increase the accuracy in the hierarchical case. Although we integrated parent-child relationships in an ad-hoc manner, we observed an effect on the labeling accuracy. Clearly we would expect more sophisticated approaches to increase accuracy further.",1,ad-hoc,True
266,"Fourthly, labeling accuracy is strongly domain dependent. The generalization of labeling approaches to different domains remains an open issue.",0,,False
267,"Finally, external knowledge in form of thesauri, ontologies etc. has to be considered also in the hierarchical case. We restricted our work to term frequency vectors only and focused poorly on statistical approaches that do not incorporate any external knowledge in form of thesauri, ontologies etc. However, our labels depend solely on the document representation and hence the term frequency vectors may be replaced by more sophisticated preprocessing utilizing external knowledge. Furthermore, labeling approaches using external knowledge most often depend on good statistical label selection and thus our approach contributes to their improvement.",1,corpora,True
268,Acknowledgments,0,,False
269,"The Know-Center GmbH Graz is funded within the Austrian COMET Program - Competence Centers for Excellent Technologies - under the auspices of the Austrian Federal Ministry of Transport, Innovation and Technology, the Austrian Federal Ministry of Economy, Family and Youth and by the State of Styria. COMET is managed by the Austrian Research Promotion Agency FFG.",0,,False
270,7. REFERENCES,0,,False
271,"[1] D. Carmel, H. Roitman, and N. Zwerdling. Enhancing cluster labeling using wikipedia. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 139­146. ACM Press, 2009.",1,wiki,True
272,"[2] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 390­397. ACM Press, 2006.",0,,False
273,"[3] O. S. Chin, N. Kulathuramaiyer, and A. W. Yeo. Automatic discovery of concepts from text. In Web Intelligence, pages 1046­1049. IEEE Computer Society, 2006.",0,,False
274,"[4] E. P. (CLEF-IP). European patents (clef-ip), 2009. [Online; accessed 07-January-2010].",1,CLEF,True
275,"[5] D. R. Cutting, J. O. Pedersen, D. Karger, and J. W. Tukey. Scatter/gather: A cluster-based approach to browsing large document collections. In SIGIR '92: Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 318­329. ACM Press, 1992.",0,,False
276,"[6] W. English. Wikipedia english, 2009. [Online; accessed 12-May-2009].",1,Wiki,True
277,"[7] F. Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani. Cluster generation and labeling for web snippets: A fast, accurate hierarchical solution. Internet Mathematics, 3(4):413­443, 2007.",0,,False
278,"[8] F. P. Giovanna Roda, John Tait and V. Zenz. Clef-ip 2009: Retrieval experiments in the intellectual property domain. In Working Notes for the CLEF 2009 Workshop, 2009.",1,CLEF,True
279,"[9] E. J. Glover, D. M. Pennock, S. Lawrence, and R. Krovetz. Inferring hierarchical descriptions. In CIKM '02: Proceedings of the eleventh international conference on Information and knowledge management, pages 507­514. ACM Press, 2002.",0,,False
280,"[10] H. Guan, J. Zhou, and M. Guo. A class-feature-centroid classifier for text categorization. In WWW '09: Proceedings of the 18th international conference on World wide web, page 201. ACM Press, 2009.",0,,False
281,"[11] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, Cambridge, UK, 2008.",0,,False
282,"[12] O. D. P. (ODP). Open directory project (odp), 2009. [Online; accessed 29-September-2009].",1,ODP,True
283,"[13] S. Osinski and D. Weiss. A concept-driven algorithm for clustering search results. IEEE Intelligent Systems, 20(3):48­54, 2005.",0,,False
284,"[14] A. Popescul and L. H. Ungar. Automatic labeling of document clusters, 2000.",0,,False
285,"[15] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization of multiple documents. 2004.",1,ad,True
286,"[16] V. Sabol, W. Kienreich, M. Muhr, W. Klieber, and M. Granitzer. Visual knowledge discovery in dynamic enterprise text repositories. In IV '09: Proceedings of the 2009 13th International Conference Information Visualisation, pages 361­368. IEEE Computer Society, 2009.",0,,False
287,"[17] H. Toda and R. Kataoka. A clustering method for news articles retrieval system. In WWW '05: Special interest tracks and posters of the 14th international conference on World Wide Web, pages 988­989. ACM Press, 2005.",0,,False
288,"[18] O. T. C. TREC-9. Ohsumed test collection trec-9, 2000. [Online; accessed 14-December-2009].",1,TREC,True
289,"[19] P. Treeratpituk and J. Callan. Automatically labeling hierarchical clusters. In DGO '06: Proceedings of the 2006 International Conference on Digital Government Research, pages 167­176, 2006.",1,Gov,True
290,185,0,,False
291,,0,,False

,sentence,label,data,regex
0,Information-Based Models for Ad Hoc IR,0,,False
1,Stéphane Clinchant,0,,False
2,"XRCE & LIG, Univ. Grenoble I Grenoble, France",0,,False
3,stephane.clinchant@xrce.xerox.com,0,,False
4,Eric Gaussier,0,,False
5,"LIG, Univ. Grenoble I Grenoble, France",0,,False
6,eric.gaussier@imag.fr,0,,False
7,ABSTRACT,0,,False
8,"We introduce in this paper the family of information-based models for ad hoc information retrieval. These models draw their inspiration from a long-standing hypothesis in IR, namely the fact that the difference in the behaviors of a word at the document and collection levels brings information on the significance of the word for the document. This hypothesis has been exploited in the 2-Poisson mixture models, in the notion of eliteness in BM25, and more recently in DFR models. We show here that, combined with notions related to burstiness, it can lead to simpler and better models.",1,ad,True
9,Categories and Subject Descriptors,0,,False
10,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,0,,False
11,General Terms,0,,False
12,"Theory, Algorithms, Experimentation",0,,False
13,Keywords,0,,False
14,"IR Theory, Probabilistic Models, Burstiness",0,,False
15,1. INTRODUCTION,1,DUC,True
16,"The purpose of this paper is to introduce the family of information based model for ad hoc information retrieval (IR). By information, we refer to Shannon information when observing a statistical event. The informativeness of a word in a document has a rich tradition in information retrieval since the influential indexing methods developed by Harter ([11]). The idea that the respective behaviors of words in documents and in the collection bring information on word type is, de facto, not a novel idea in IR. It has inspired the 2-Poisson mixture model, the concept of eliteness in BM25 models and is at the heart of DFR models. In this paper, we come back to this idea in order to present a new family of IR models: information models. To do so, we first present,",1,ad,True
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
18,"in section 2, the conditions a retrieval function should satisfy, on the basis of the heuristic retrieval constraints proposed by Fang et al. [9]. Section 3 is then devoted to the presentation of information models, and their link with the retrieval conditions and the phenomenon known as burstiness. We present two instances of information models, based on two power law distributions, and show how to perform pseudo-relevance feedback for information models. Section 4 provides an experimental validation of our models. Our experiments show that the information models we introduce significantly outperform language models and Okapi BM25. They are on par with DFR models, while being conceptually simpler, when pseudo-relevance feedback is not used. When using pseudo-relevance feedback, they significantly outperform all models, including DFR ones.",0,,False
19,2. PRELIMINARIES,0,,False
20,"The notations we use throughout the paper are summarized in table 2 (w represents a term). They slightly differ from standard notations for convenience reasons, i.e. their easiness of use in the mathematical framework we deploy. We",0,,False
21,Notation xqw xdw tdw yd m L N M Fw,0,,False
22,Nw,0,,False
23,zw,0,,False
24,Description,0,,False
25,Number of occurrences of w in query q,0,,False
26,Number of occurrences of w in document d,0,,False
27,Normalized version of xdw Length of document d,0,,False
28,Average document length,0,,False
29,Length of collection d,0,,False
30,Number of documents in the collection,0,,False
31,Number of terms in the collection,0,,False
32,Number of occurrences of w in collection:,0,,False
33,Fw,0,,False
34,",",0,,False
35,P,0,,False
36,d,0,,False
37,xdw,0,,False
38,Number of documents containing w:,0,,False
39,Nw,0,,False
40,",",0,,False
41,P,0,,False
42,d,0,,False
43,I (xdw,0,,False
44,>,0,,False
45,0),0,,False
46,"zw , Fw or zw , Nw",0,,False
47,Table 1: Notations,0,,False
48,"consider here retrieval functions, denoted RSV , of the form:",0,,False
49,"RSV (q, d) ,"" X a(xqw)h(xdw, yd, zw, )""",0,,False
50,wq,0,,False
51,"where  is a set of parameters and where h, the form of which depends on the IR model considered, is assumed to be of class1 C2 and defined over R+ ×R+ ×R+ ×, where 1A function of class C2 is a function for which second derivatives exist and are continuous.",0,,False
52,234,0,,False
53," represents the domain of the parameters in  and a is often the identity function. Language models [21], Okapi [15] and Divergence from Randomness [3] models as well as vector space models [16] all fit within the above form. For example, for the pivoted normalization retrieval formula [17],  ,"" (s, m, N ) and:""",0,,False
54,"h(x,",0,,False
55,"y,",0,,False
56,"z,",0,,False
57,),0,,False
58,",",0,,False
59,I (x,0,,False
60,>,0,,False
61,0),0,,False
62,1,0,,False
63,+,0,,False
64,ln(1 + ln(x)I(x>0)),0,,False
65,1,0,,False
66,-,0,,False
67,s,0,,False
68,+,0,,False
69,s,0,,False
70,y m,0,,False
71,ln(,0,,False
72,N,0,,False
73,+ z,0,,False
74,1,0,,False
75,),0,,False
76,"where I is an indicator function which equals 1 when its argument is true and 0 otherwise. A certain number of hypotheses, experimentally validated, sustain the development of IR models. In particular, it is important that documents with more occurrences of query terms get higher scores than documents with less occurrences. However, the increase in the retrieval score should be smaller for larger term frequencies, inasmuch as the difference between say 110 and 111 is not as important as the one between 1 and 2 (the number of occurrences has doubled in the second case, whereas the increase is relatively marginal in the first case). In addition, longer documents, when compared to shorter ones with exactly the same number of occurrences of query terms, should be penalized as they are likely to cover additional topics than the ones present in the query. Lastly, it is important, when evaluating the retrieval score of a document, to weigh down terms occurring in many documents, i.e. which have a high document/collection frequency, as these terms have a lower discrimination power. These different considerations can be analytically formalized as a set of simple conditions the retrieval function h should satisfy:",1,ad,True
77,"(y, z, ),",0,,False
78,"h(x, y, z, x",0,,False
79,),0,,False
80,>,0,,False
81,0,0,,False
82,(condition 1),0,,False
83,"(y, z, ),",0,,False
84,"2h(x, y, z, ) x2",0,,False
85,<,0,,False
86,0,0,,False
87,(condition 2),0,,False
88,"(x, z,",0,,False
89,"),",0,,False
90,"h(x, y, z, y",0,,False
91,),0,,False
92,<,0,,False
93,0,0,,False
94,(condition 3),0,,False
95,"(x, y, ), h(x, y, z, ) < 0 (condition 4) z",0,,False
96,"Conditions 1, 3 and 4 directly state that h should be increasing with the term frequency, and decreasing with the document length and the document/collection frequency. Conditions 1 and 2, already mentioned in this form by Fang et al. [9], state that h should be an increasing, concave function of the term frequency, the concavity ensuring that the increase in the retrieval score will be smaller for larger term frequencies. We will refer to the above conditions as the form conditions inasmuch as they define the general shape the function h should have. They respectively correspond to the heuristic retrieval constraints TFC1, TFC2, LNC1 and TDC2 introduced by Fang et al. [9]. In addition to this form conditions, Fang et al. [9] used two additional constraints to regulate the interaction between frequency and document length, i.e. between the derivatives wrt to x and y. These conditions, which we will refer to as adjustment conditions, allow to adjust the functions h satisfying the form conditions 1, 2, 3 and 4. They correspond to:",1,ad,True
97,"2Condition 4 is in fact a special case of TDC, but this is beyond the scope of the current paper.",1,TD,True
98,"Condition 5 LNC2: Let q a query. k > 1, if d1 and d2 are two documents such that yd1 ,"" k × yd2 and for all words w, xdw1 "","" k × xdw2, then RSV (d1, q)  RSV (d2, q)""",0,,False
99,"Condition 6 TF-LNC: Let q , w a query with only word w. if xdw1 > xdw2 et yd1 ,"" yd2 + xdw1 - xdw2, then RSV (d1, q) > RSV (d2, q).""",0,,False
100,We are now ready to proceed to the presentation of information models.,1,ad,True
101,3. INFORMATION MODELS,0,,False
102,"In order to take into account the fact that one is comparing documents of different length, most IR models do not rely directly on the raw number of occurrences of words in documents, but rather on normalized versions of it. Language models for example use the relative frequency of words in the document and the collection. Other classical term normalization schemes include the well know Okapi normalization, as well as the pivoted length normalization [17]. More recently, [14] propose another formulation for the language model using the notion of verbosity. DFR models usually adopt one of the two following term frequency normalizations (c is a multiplying factor):",1,ad,True
103,tdw,0,,False
104,",",0,,False
105,xdw,0,,False
106,c,0,,False
107,m yd,0,,False
108,or,0,,False
109,xdw,0,,False
110,log(1,0,,False
111,+,0,,False
112,c,0,,False
113,m yd,0,,False
114,),0,,False
115,(1),0,,False
116,"The concept of the information brought by a term in a document has been considered in several IR models. Harter [11] observed that 'significant', 'specialty' words of a document do not behave as 'functional' words. Indeed, the more a word deviates in a document from its average behavior in the collection, the more likely it is 'significant' for this particular document. This can be easily captured in terms of information: If a word behaves in the document as expected on the collection, then it has a high probability of occurrence in the document p, according to the distribution collection, and the information it brings to the document, - log(p), is small. On the contrary, if it has a low probability of occurrence in the document, according to the distribution collection, then the amount of information it conveys is more important. Because of the above consideration, this idea, at the basis of DFR models, has to be applied to the normalized form of the term frequency. This leads to the general and simple retrieval function:",1,ad,True
117,"RSV (q, d) , X -xqw log P rob(Xw  tdw|w) (2)",0,,False
118,wq,0,,False
119,"where tdw is the normalized form of xdw and w is a parameter for the probability distribution of w in the collection. We simply consider here that w is set to either the average number of occurrences of w in the collection, or to the average number of documents in which w occurs, that is:",0,,False
120," , zw , Fw or Nw",0,,False
121,(3),0,,False
122,NNN,0,,False
123,It is interesting to note that the retrieval function defined,0,,False
124,"by equation 2, which is rank invariant by the change of the logarithmic base, satisfies the heuristic retrieval conditions 1 and 3. Indeed, P rob(Xw  tdw|w) is a decreasing function of tdw. So, as long as tdw is an increasing function of xdw and a decreasing function of yd, which is the case for all the normalization functions we are aware of, conditions 1 and 3",0,,False
125,are satisfied for this family of models.,0,,False
126,235,0,,False
127,3.1 Burstiness (and condition 2),0,,False
128,"Church and Gale [6] were the first to study, to our knowledge, the phenomenon of burstiness in texts. The term ""burstiness"" describes the behavior of words which tend to appear in bursts, i.e., once they appear in a document, they are much more likely to appear again. The notion of burstiness is similar to the one of aftereffect of future sampling ([10]), which describes the fact that the more we find a word in a document, the higher the expectation to find new occurrences. Burstiness has recently received a lot of attention from different communities. Madsen [13], for example, proposed to use the Dirichlet Compound Multinomial (DCM) distribution in order to model burstiness in the context of text categorization and clustering. Elkan [8] then approximated the DCM distribution by the EDCM distribution, which learning time is faster, and showed the good behavior of the model obtained on different text clustering experiments. A related notion is the one of preferential attachment ([4] and [5]) often used in large networks, such as the web or social networks. It conveys the same idea: the more we have, the more we will get. In the context of IR, Xu and Akella [19] studied the use of a DCM model within the Probability Ranking Principle for modeling the dependency of word repetitive occurrences (a notion directly related to burstiness), and argue that multinomial distributions alone are not appropriate for IR within this principle. More formally, Clinchant and Gaussier [7] introduced the following definition (slightly simplified here for clarity's sake) in order to characterize discrete distributions which can account for burstiness:",1,ad,True
129,"Definition 1. [Discrete case] A discrete distribution P is bursty iff for all integers (n, n), n  n:",0,,False
130,P (X  n + 1|X  n) > P (X  n + 1|X  n),0,,False
131,We generalize this definition to the continuous case as follows:,0,,False
132,Definition 2. [General case] A distribution P is bursty iff the function g defined by:,0,,False
133," > 0, g(x) , P (X  x + |X  x)",0,,False
134,is a strictly increasing function of x. A distribution which verifies this condition is said to be bursty.,0,,False
135,"which translates the fact that, with a bursty distribution, it is easier to generate higher values of X once lower values have been observed. We now show that this notion is directly related to the heuristic retrieval condition 2.",0,,False
136,"In the retrieval function defined by equation 2, the function h we have considered so far corresponds to:",0,,False
137,- log(P rob(X  tdw)),0,,False
138,"In this case, condition 2 can be re-expressed as:",0,,False
139,"2h(x, y, z, ) x2",0,,False
140,<,0,,False
141,0,0,,False
142,2,0,,False
143,log(P rob(X  (xdw )2,0,,False
144,tdw )),0,,False
145,>,0,,False
146,0,0,,False
147,But:,0,,False
148,2f t2,0,,False
149,",",0,,False
150,2f x2,0,,False
151,(,0,,False
152,x t,0,,False
153,)2,0,,False
154,+,0,,False
155,f x,0,,False
156,2x t2,0,,False
157,.,0,,False
158,"Furthermore,",0,,False
159,f x,0,,False
160,is,0,,False
161,here,0,,False
162,negative,0,,False
163,as,0,,False
164,f,0,,False
165,is,0,,False
166,log(P rob(X,0,,False
167,tdw )).,0,,False
168,"So,",0,,False
169,as,0,,False
170,long,0,,False
171,as,0,,False
172,2x t2,0,,False
173,0,0,,False
174,(which is the case for all the normalization functions we are,0,,False
175,"aware of, in particular the ones provided by equation 1), a",0,,False
176,sufficient condition for condition 2 is:,0,,False
177,2 log(P rob(X  (tdw )2,0,,False
178,tdw )),0,,False
179,>,0,,False
180,0,0,,False
181,The following theorem (the proof of which is given in the appendix) shows that bursty distributions satisfy this condition.,0,,False
182,"Theorem 3. Let P be a ""bursty"" probability distribution of class C2. Then:",0,,False
183,2 log(P (X x2,0,,False
184,x)),0,,False
185,>,0,,False
186,0,0,,False
187,"We thus see that under certain assumptions, IR models de-",0,,False
188,"fined by equation 2 satisfy the form conditions 1, 2 and 3.",0,,False
189,We now summarize these assumptions which characterize,0,,False
190,information models.,0,,False
191,3.2 Characterization of Information Models,0,,False
192,We characterize information models by the following three elements:,0,,False
193,"1. Normalization function The normalization function tdw, function of xdw and yd (respectively the number of occurrences of the word in the document and the length of the document), satisfies:",0,,False
194, tdw  xdw,0,,False
195,> 0;,0,,False
196, tdw yd,0,,False
197,<,0,,False
198,0;,0,,False
199, 2 xdw  (tdw )2,0,,False
200,0,0,,False
201,2. Probability distribution The probability distribution at the basis of the model has to be:,0,,False
202,"· Continuous, the random variable under consideration, tdw, being continuous;",0,,False
203,"· Compatible with the domain of tdw, i.e. if tmin is the minimum value of tdw, then P rob(Xw  tmin|w) ,"" 1 (because of the first inequality above, tmin is obtained when xdw "", 0);",0,,False
204,· Bursty according to definition 2 above.,0,,False
205,"3. Retrieval function The retrieval function satisfies equation 2, i.e.:",0,,False
206,"RSV (q, d) , X -xqw log P rob(Xw  tdw|w)",0,,False
207,wq,0,,False
208,", X -xqw log P rob(Xw  tdw|w)",0,,False
209,wqd,0,,False
210,"where the second equality derives from the fact that the probability function verifies P rob(Xw  tmin|w) ,"" 1, with tmin obtained when xdw "","" 0. The above ranking function corresponds to the mean information a document brings to a query (or, equivalently, to the average of the document information brought by each query term). Furthermore, the parameter w is set as in equation 3:""",0,,False
211," , zw , Fw or Nw NNN",0,,False
212,"The general form of the retrieval function and the first two inequalities on the normalization function ensure that the model satisfies conditions 1 and 3. Theorem 3, in conjunction with the last condition on the normalization function, additionally ensures that it satisfies condition 2. Hence, information models satisfy three (out of four) form conditions. The choice of the particular bursty distribution to be used has to be made in such a way that the last form condition and the two adjustment conditions are satisfied.",1,ad,True
213,236,0,,False
214,3.3 Two Power-law Instances,0,,False
215,"We present here two power law distributions which are bursty and lead to information models satisfying all form and adjustment conditions. The use of power law distributions to model burstiness is not entirely novel, as other studies ([4, 5]) have used similar distributions to model preferential attachment, a notion equivalent to burstiness.",1,ad,True
216,Log-Logistic Distribution,0,,False
217,"The log-logistic (LL) distribution is defined by, for X  0:",0,,False
218,PLL (X,0,,False
219,<,0,,False
220,"x|r, )",0,,False
221,",",0,,False
222,x x + r,0,,False
223,"We consider here a restricted form of the log-logistic distribution where  ,"" 1, so that the the log-logistic information model takes the form:""",0,,False
224,"RSV (q, d) , X -xqw log(PLL(X  tdw|w))",0,,False
225,wqd,0,,False
226,",",0,,False
227,X,0,,False
228,wqd,0,,False
229,-xqw,0,,False
230,log,0,,False
231,(,0,,False
232,tdw,0,,False
233,w + w,0,,False
234,),0,,False
235,(4),0,,False
236,"The log-logistic motivation resorts to previous work on text modeling. Following Church and Gale [6] and Airoldi [1], Clinchant and Gaussier [7] studied the negative binomial distribution in the context of text modeling. They then assumed a uniform Beta prior distribution over one of the parameters, leading to a distribution they refer to as the Beta negative binomial distribution, or BNB for short. One problem with the BNB distribution is that it is a discrete distribution and cannot be used for modeling tdw. However, the log-logistic distribution, with its  parameter set to 1, is a continuous counterpart of the BNB distribution since PLL(x  X < x + 1; r) , PBNB (x).",1,ad,True
237,A Smoothed Power-Law (SPL) Distribution,0,,False
238,"We consider here the distribution, which we will refer to as SPL, defined, for x > 0, by:",0,,False
239,x,0,,False
240,f (x; ),0,,False
241,",",0,,False
242,- log  1-,0,,False
243, x+1 (x + 1)2,0,,False
244,(0,0,,False
245,<,0,,False
246,<,0,,False
247,1),0,,False
248,P (X > x|),0,,False
249,",",0,,False
250,Z f (x; ),0,,False
251,x,0,,False
252,",",0,,False
253,x,0,,False
254, x+1 -  1-,0,,False
255,"where f denotes the probability density function. Based on this distribution, the SPL information model thus takes the form:",0,,False
256,tdw,0,,False
257,RSV,0,,False
258,"(q,",0,,False
259,d),0,,False
260,",",0,,False
261,X,0,,False
262,wqd,0,,False
263,-xqw,0,,False
264,log(,0,,False
265, tdw +1,0,,False
266,w,0,,False
267,1-,0,,False
268,- w w,0,,False
269,),0,,False
270,(5),0,,False
271,"From equations 4 or 5, and using the normalization functions defined by equation 1, one can verify (a) that the log-logistic and SPL distributions are bursty, and (b) that their corresponding information models additionally satisfy conditions 4, 5 and 6 (the demonstration is purely technical, and is skipped here). The log-logistic and SPL information models thus satisfy all the form and adjustment conditions.",1,ad,True
272,"Figure 1 illustrates the behavior of the log-logistic model, the SPL model and the InL2 DFR model (referred to as INL for short). To compare these models, we used a value of 0.005 for  and computed the term weight obtained for term frequencies varying from 0 to 15. For information models, the weight corresponds to the quantity - log P rob, whereas",0,,False
273,"r , 0.005",0,,False
274,8,0,,False
275,6,0,,False
276,4,0,,False
277,2,0,,False
278,0,0,,False
279,loglogistic inl spl,0,,False
280,0,0,,False
281,5,0,,False
282,10,0,,False
283,15,0,,False
284,Figure 1: Plot of Retrieval Functions,0,,False
285,"in the case of DFR models, this quantity is corrected by the Inf2 part, leading to, with the underlying distributions retained:",1,ad,True
286,8,0,,False
287,> >,0,,False
288,-,0,,False
289,log(,0,,False
290,w tdw +w,0,,False
291,),0,,False
292,> > <,0,,False
293,tdw,0,,False
294,"weight , >",0,,False
295,-,0,,False
296,log(,0,,False
297,wtdw +1 -w 1-w,0,,False
298,),0,,False
299,> > > :,0,,False
300,- tdw,0,,False
301,tdw +1,0,,False
302,log(,0,,False
303,Nw +0.5 N +1,0,,False
304,),0,,False
305,(log-logistic),0,,False
306,(SPL) (InL2),0,,False
307,"As one can note, the weight values obtained with the two information models are always above the ones obtained with the DFR model, the log-logistic model having a sharper increase than the other ones for low frequency terms.",0,,False
308,3.4 PRF in Information Models,0,,False
309,"Pseudo-relevance feedback (PRF) in information models can be performed following the same approach as the one used in other models: The weight of a term in the original query is updated on the basis of the information brought by the top retrieved documents on the term. Denoting by R the set of top n documents retrieved for a given query, R ,"" (d1, . . . , dn), the average information this set brings on a given term w can directly be computed as:""",0,,False
310,InfoR(w),0,,False
311,",",0,,False
312,1 n,0,,False
313,X - log(P (Xw,0,,False
314,>,0,,False
315,tdw |w )),0,,False
316,(6),0,,False
317,dR,0,,False
318,"where the mean is taken over all the documents in R. This is a major difference with the approach in [2] where all documents in R are merged into a single document. Considering the documents in R as different documents allows one to take into account the differences in document lengths and number of occurrences. The original query is then modified, following standard approaches to PRF, to take into account the words appearing in R as:",0,,False
319,xqw2,0,,False
320,",",0,,False
321,xqw maxw,0,,False
322,xqw,0,,False
323,+,0,,False
324,InfoR(w) maxw InfoR(w),0,,False
325,(7),0,,False
326,where  is a parameter controlling the modification brought by R to the original query. xqw2 denotes the updated weight of w in the query.,0,,False
327,4. EXPERIMENTAL VALIDATION,0,,False
328,"To assess the validity of our models, we used standard IR collections, from two evaluation campaigns: TREC (trec.nist.gov) and CLEF (www.clef-campaign.org). Table 2 gives the number of documents (N ), number of unique terms (M ), aver-",1,TREC,True
329,237,0,,False
330,"age document length and number of test queries for the collections we retained: ROBUST (TREC), TREC3, CLEF03 AdHoc Task, GIRT (CLEF Domain Specific Task, from the years 2004 to 2006). For the ROBUST and TREC3 collections, we used standard Porter stemming. For the CLEF03 and GIRT collections, we used lemmatization, and an additional decompounding step for the GIRT collection which is written in German.",1,TREC,True
331,Table 2: Characteristics of the different collections,0,,False
332,N,0,,False
333,M Avg DL # Queries,0,,False
334,ROBUST 490 779 992 462 289,0,,False
335,250,0,,False
336,TREC-3 741 856 668 648 438,1,TREC,True
337,50,0,,False
338,CLEF03 166 754 80 000 247,1,CLEF,True
339,60,0,,False
340,GIRT 151 319 179 283 109,1,GIRT,True
341,75,0,,False
342,We evaluated the log-logistic and the SPL model against,0,,False
343,"language models, with both Jelinek-Mercer and Dirichlet",0,,False
344,"Prior smoothing, as well as against the standard DFR mod-",0,,False
345,"els and Okapi BM25. For each dataset, we randomly split",0,,False
346,queries in train and test (half of the queries are used for,0,,False
347,"training, the other half for testing). We performed 10 such",0,,False
348,splits on each collection. The results we provide for the,0,,False
349,Mean Average Precision (MAP) and the precision at 10 doc-,1,MAP,True
350,uments (P10) is the average of the values obtained over the,0,,False
351,10 splits. The parameters of the different models are opti-,0,,False
352,mized (respectively for the MAP and the precision at 10) on,1,MAP,True
353,the training set. The performance is then measured on the,0,,False
354,"test set. To compare the different methods, a two-sided t-",0,,False
355,test (at the 0.05 level) is performed to assess the significance,0,,False
356,of the difference measured between the methods. All our,0,,False
357,experiments were carried out thanks to the Lemur Toolkit,0,,False
358,"(www.lemurproject.org). In all the following tables, ROB-t",0,,False
359,"represents the robust collection with query titles only, ROB-",0,,False
360,d the robust collection with query titles and description,0,,False
361,"fields,CL-t represent titles for the CLEF collection, CL-d",1,CLEF,True
362,queries with title and descriptions and T3-t query titles for,0,,False
363,TREC-3 collection. The GIRT queries are just made up of,1,TREC,True
364,a single sentence.,0,,False
365,The version of the log-logistic model used in all our ex-,0,,False
366,"periments is based on w ,",0,,False
367,nw N,0,,False
368,and the second length nor-,0,,False
369,malization in equation 1 (called L2 in DFR). We refer to,0,,False
370,this model as the LGD model. The same settings are cho-,0,,False
371,sen for the SPL model. As the parameter c in equation 1,0,,False
372,"is not bounded, we have to define a set of possible values",0,,False
373,from which to select the best value on the training set. We,0,,False
374,make use of the typical range proposed in works on DFR,0,,False
375,"models, which also rely on equation 1 for document length",0,,False
376,"normalization. The set of values we retained is: {0.5, 0.75,",0,,False
377,"1, 2, 3, 4, 5, 6, 7, 8, 9}.",0,,False
378,Comparison with Jelinek-Mercer and Dirichlet language models,0,,False
379,"As the smoothing parameter of the Jelinek-Mercer language model is comprised between 0 and 1, we use a regular grid on [0, 1] with a step size of 0.05 in order to select, on the training set, the best value for this parameter. Table 3 shows the comparison of our models, LGD and SPL, with the JelinekMercer language model (LM). On all collections, on both short and long queries, the LGD model significantly outperforms the Jelinek-Mercer language model. This is an interesting finding as the complexity of the two models is the same (in a way, they are both conceptually simple). Further-",1,LM,True
380,"more, as the results displayed are averaged over 10 different splits, this shows that the LGD model consistently outperforms the Jelinek-Mercer language model and thus yields a more robust approach to IR. Lastly, the SPL model is better than the Jelinek-Mercer model for most collections for MAP and P10.",1,MAP,True
381,Table 3: LGD and SPL versus LM-Jelinek-Mercer after 10 splits; bold indicates significant difference,1,LM,True
382,MAP ROB-d ROB-t GIR T3-t CL-d CL-t JM 26.0 20.7 40.7 22.5 49.2 36.5 LGD 27.2 22.5 43.1 25.9 50.0 37.5 P10 ROB-d ROB-t GIR T3-t CL-d CL-t JM 43.8 35.5 67.5 40.7 33.0 26.2 LGD 46.0 38.9 69.4 52.4 33.6 26.6,1,MAP,True
383,MAP JM SPL P10 JM SPL,1,MAP,True
384,ROB-d 26.6 26.7,0,,False
385,ROB-d 44.4 47.6,0,,False
386,ROB-t 23.1 25.2,0,,False
387,ROB-t 39.8 45.3,0,,False
388,GIR 39.2 41.7 GIR 66.0 69.8,0,,False
389,T3-t 22.3 26.6 T3-t 43.9 56.0,0,,False
390,CL-d 47.2 44.1 CL-d 34.0 34.0,0,,False
391,CL-t 37.2 37.7 CL-t 25.6 25.6,0,,False
392,"For the Dirichlet prior language model, we optimized the smoothing parameter from a set of typical values, defined by: {10, 50, 100, 200, 500, 800, 1000, 1500, 2000, 5000, 10000}. Table 4 shows the results of the comparison between our models and the Dirichlet prior language model (DIR). These results parallel the ones obtained with the JelinekMercer language model on most collections, even though the difference is less marked. For the ROB collection with short queries, the Dirichlet prior language model outperforms in average the log-logistic model (the difference being significant for the precision at 10 only). On the other collections, with both short and long queries and on both the MAP and the precision at 10, the log-logistic model outperforms in average the Dirichlet prior language model, the difference being significant in most cases. The Dirichlet model has a slight advantage in MAP over the SPL model, but SPL is better for precision. Overall, the information-based models outperform in average language models.",1,MAP,True
393,Table 4: LGD and SPL versus LM-Dirichlet after 10 splits; bold indicates significant difference,1,LM,True
394,MAP ROB-d ROB-t GIR T3-t CL-t CL-d DIR 27.1 25.1 41.1 25.6 36.2 48.5 LGD 27.4 25.0 42.1 24.8 36.8 49.7 P10 ROB-d ROB-t GIR T3-t CL-t CLF-d DIR 45.6 43.3 68.6 54.0 28.4 33.8 LGD 46.2 43.5 69.0 54.3 28.6 34.5,1,MAP,True
395,MAP DIR SPL P10 DIR SPL,1,MAP,True
396,ROB-d 26.7 25.6,0,,False
397,ROB-d 45.2 46.6,0,,False
398,ROB-t 25.0 24.9,0,,False
399,ROB-t 43.8 44.7,0,,False
400,GIR 40.9 42.1 GIR 68.2 70.8,0,,False
401,T3-t 27.1 26.8 T3-t 52.8 55.3,0,,False
402,CL-t 36.2 36.4 CL-t 27.3 27.1,0,,False
403,CL-d 50.2 46.9 CL-d 32.8 32.9,0,,False
404,Comparison with BM25,0,,False
405,We adopt the same methodology to compare information models with BM25. We choose only to optimize the k1 pa-,1,ad,True
406,238,0,,False
407,"rameter of BM25 among the following values: {0.3, 0.5, 0.8, 1.0, 1.2, 1.5, 1.8, 2, 2.2, 2.5}. The others parameters b and k3 take their default values implemented in Lemur (0.75 and 7). Table 5 shows the comparison of the log-logistic and SPL models with Okapi BM25. The log-logistic is either better (4 collections out of 6 for mean average precision, 3 collections out of 6 for P10) or on par with Okapi BM25. The same thing holds for the SPL model, which is 3 times better and 3 times on par for the MAP, and 4 times better, 1 time worse and 1 time on a par for the precision at 10 documents. Overall, information models outperform in average Okapi BM25.",1,MAP,True
408,Table 5: LGD and SPL versus BM25 after 10 splits; bold indicates best performance significant difference,0,,False
409,MAP ROB-d ROB-t GIR T3-t CL-t CL-d BM25 26.8 22.4 39.8 25.4 34.9 46.8 LGD 28.2 23.5 41.4 26.1 34.8 48.0 P10 ROB-d ROB-t GIR T3-t CL-t CL-d BM25 45.9 42.6 62.6 50.6 28.5 33.7 LGD 46.5 44.3 66.6 53.8 28.7 34.4,1,MAP,True
410,MAP BM25 SPL P10 BM25 SPL,1,MAP,True
411,ROB-d 26.9 27.1,0,,False
412,ROB-d 45.7 47.6,0,,False
413,ROB-t 24.2 25.4,0,,False
414,ROB-t 41.4 44.1,0,,False
415,GIR 38.5 40.5 GIR 62.8 67.9,0,,False
416,T3-t 25.3 26.8 T3-t 51.0 57.0,0,,False
417,CL-t 35.1 34.5 CL-t 28.5 28.0,0,,False
418,CL-d 47.3 47.0 CL-d 36.1 35.4,0,,False
419,"Comparison with DFR models To compare our model with DFR ones, we chose, in this latter family, the InL2 model, based on the Geometric distribution and Laplace law of succession, and the PL2 model based on the Poisson distribution and Laplace law. These models have been used with success in different works ([3, 7, 18] for example). All the models considered here make use of the same set of possible values for c, namely: {0.5, 0.75, 1, 2, 3, 4, 5, 6, 7, 8, 9}. It is however interesting to note that both PL2 and InL2 make use of discrete distributions (Geometric and Poisson) over continuous variables (tdw) and are thus theoretically flawed. This is not the case of the information models which rely on a continuous distribution.",0,,False
420,"The results obtained, presented in tables 6 and 7 are more contrasted than the ones obtained with language models and Okapi BM25. In particular, for the precision at 10, LGD and InL2 perform similarly (LGD being significantly better on GIRT whereas InL2 is significantly better on ROB with long queries, the models being on a par in the other cases). For the MAP, the LGD model outperforms the InL2 model as it is significantly better on ROB (for both sort and long queries) and GIRT, and on a par on CLEF. SPL is better than InL2 for precision but on a par for MAP. Moreover, LGD and PL2 are on a par for MAP, while PL2 is better for P10. Lastly, PL2 is better than SPL for MAP but not for the precision at 10 documents. Overall, DFR models and information models yield similar results. This is all the more so interesting that information models are simpler than DFR ones: They rely on a single information measure (see equation 2) without the re-normalization (Inf2 part) used in DFR models.",1,GIRT,True
421,Table 6: LGD and SPL versus INL after 10 splits; bold indicates significant difference,0,,False
422,MAP ROB-d ROB-t GIR T3-t CL-t CL-d INL2 27.7 24.8 42.5 27.3 37.5 47.7 LGD 28.5 25.0 43.1 27.3 37.4 48.0 P10 ROB-d ROB-t GIR T3-t CL-t CL-d INL2 47.7 43.3 67.0 52.4 27.3 33.4 LGD 47.0 43.5 69.4 53.2 27.2 33.3,1,MAP,True
423,MAP INL SPL P10 INL SPL,1,MAP,True
424,ROB-d 26.9 26.6,0,,False
425,ROB-d 47.6 47.8,0,,False
426,ROB-t 24.3 24.6,0,,False
427,ROB-t 42.8 44.1,0,,False
428,GIR 40.4 40.7 GIR 63.4 68.0,0,,False
429,T3-t 24.8 25.4 T3-t 52.5 53.9,0,,False
430,CL-t 35.5 34.6 CL-t 28.8 28.7,0,,False
431,CL-d 49.4 48.1 CL-d 33.8 33.6,0,,False
432,Table 7: LGD and SPL versus PL2 after 10 splits; bold indicates significant difference,0,,False
433,MAP ROB-d ROB-t GIR T3-t CL-t CL-d PL2 26.2 24.8 40.6 24.9 36.0 47.2 LGD 27.3 24.7 40.5 24.0 36.2 47.5 P10 ROB-d ROB-t GIR T3-t CL-t CL-d PL2 46.4 44.1 68.2 55.0 28.7 33.1 LGD 46.6 43.2 66.7 53.9 28.5 33.7,1,MAP,True
434,MAP ROB-d ROB-t GIR T3-t CL-t CL-d PL2 26.3 25.2 42.8 25.8 37.3 45.7 SPL 26.3 25.2 42.7 25.3 37.4 44.1 P10 ROB-d ROB-t GIR T3-t CL-t CL-d PL2 46.0 45.2 69.3 54.8 26.2 32.7 SPL 47.0 45.2 69.8 55.4 25.9 32.9,1,MAP,True
435,Pseudo-relevance feedback,0,,False
436,"There are many parameters for pseudo-relevance feedback algorithms: The number of document to consider (N ), the number of terms to add the query (T C) and the weight to give to those new query terms (parameter  in equation 7). Optimizing all these parameters and smoothing ones at the same time would be very costly. We thus modify here our methodology. For each collection, we choose the optimal smoothing parameters for each model (c,µ,k1) on all queries. The results obtained in this case are given in table 8, where LM+MIX corresponds here to the Dirichlet language model. They show, for example, that on the ROBUST collection there is no difference between the baseline systems we will use for pseudo-relevance feedback in terms of MAP. Overall, the precision at 10 is very similar for the different systems, so that there is no bias, with the setting chosen, towards a particular system. We compare here the results obtained with the information models to two state-ofthe-art pseudo-relevance feedback models: Bo2, associated with DFR models ([2]), and the mixture model associated with language models ([20]). For each collection, we average the results obtained over 10 random splits, the variation of N and T C being made on each split so as to be able to compare the results of the different settings. For each setting, we optimize the weight to give to new terms:  (within {0.1, 0.25, 0.5, 0.75, 1, 1.5, 2}) in information and Bo2 models,  ( within {0.1, 0.2, . . . , 0.9}) in the mixture-model for",1,ad,True
437,239,0,,False
438,"feedback in language models. In this latter case, we set the feedback mixture noise to its default value (0.5). As before, we used Lemur to carry our experiments and optimize here only the mean average precision. Table 9 displays the results for the different models (as before, a two-sided t-test at the 0.05 level is used to assess whether the difference is statistically significant, which is indicated by a ). As one can note, the information models significantly outperform the pseudo-relevance feedback versions of both language models and DFR models. The SPL model is the best one for N , 5 and T C ,"" 5, while the LGD model yields the best performance in most other cases. Altough DFR and information models perform similarly when no feedback is used, their pseudo-relevance feedback versions do present differences, information models outperforming significantly both language and DFR models in this latter case.""",0,,False
439,"Table 8: Performances of baseline setting for PRF (N ,"" 0, T C "", 0): bold indicates significant difference",0,,False
440,MAP LM+MIX,1,MAP,True
441,LGD P10 LM+MIX LGD,1,LM,True
442,ROB-t 25.4 25.4,0,,False
443,ROB-t 44.6 44.1,0,,False
444,GIRT 41.1 42.4 GIRT 68.3 68.7,1,GIRT,True
445,T3-t 28.3 27.1 T3-t 56.3 55.3,0,,False
446,CLEF-t 37.0 37.5,1,CLEF,True
447,CLEF-t 27.5 27.2,1,CLEF,True
448,"Table 9: Mean average precision of PRF experiments; bold indicates best performance,  significant",0,,False
449,difference over LM and Bo2 models Model N TC ROB-t GIR T3-t,1,LM,True
450,CL-t,0,,False
451,LM+MIX 5 5 27.5 44.4 30.7 36.6,1,LM,True
452,LGD,0,,False
453,5 5 28.3 44.3 32.9 37.6,0,,False
454,INL+Bo2 5 5 26.5 42.0 30.6 37.6,0,,False
455,SPL,0,,False
456,5 5 28.9 45.6 32.9 39.0,0,,False
457,LM+MIX 5 10 28.3 45.7 33.6 37.4,1,LM,True
458,LGD,0,,False
459,5 10 29.4 44.9 35.0 40.2,0,,False
460,INL+Bo2 5 10 27.5 42.7,0,,False
461,SPL,0,,False
462,5 10 29.6 47.0,0,,False
463,32.6 34.6,0,,False
464,37.5 39.5,0,,False
465,LM+MIX 10 10 28.4 45.5 31.8 37.6 LGD 10 10 30.0 46.8 35.5 38.9,1,LM,True
466,INL+Bo2 10 10 27.2 43.0 32.3 37.4,0,,False
467,SPL,0,,False
468,10 10 30.0 48.9 33.8 39.1,0,,False
469,LM+MIX 10 20 29.0 46.2 33.7 38.2 LGD 10 20 30.3 47.6 37.4 38.6,1,LM,True
470,INL+Bo2 10 20 27.7 43.5 33.8 37.7,0,,False
471,SPL,0,,False
472,10 20 29.9 50.2 34.3 39.7,0,,False
473,LM+MIX 20 20 28.6 47.9 32.9 37.8 LGD 20 20 29.5 48.9 37.2 41.0,1,LM,True
474,INL+Bo2 20 20 27.4 44.3 33.5 36.8,0,,False
475,SPL,0,,False
476,20 20 28.8 50.3 33.9 39.0,0,,False
477,5. DISCUSSION,0,,False
478,"The Divergence from Randomness (DFR) framework proposed by Amati and van Rijsbergen [3] is based on the informative content provided by the occurrences of terms in documents, a quantity which is then corrected by the risk of accepting a term as a descriptor in a document (first",0,,False
479,"normalization principle) and by normalizing the raw occurrences by the length of a document (second normalization principle). The informative content Inf1(tdw) is based on a first probability distribution and is defined as: Inf1(tdw) , - log P rob1(tdw). The first normalization principle is associated with a second information defined from a second probability distribution through: Inf2(tdw) , 1 - P rob2(tdw). The overall IR model is then defined as a combination of Inf1 and Inf2:",0,,False
480,"RSV (q, d) , X xqwInf2(tdw)Inf1(tdw)",0,,False
481,wqd,0,,False
482,", X -xqwInf2(tdw) log P rob1(tdw)",0,,False
483,wqd,0,,False
484,"The above form shows that DFR models can be seen as information models, as defined by equation 2, with a correction brought by the Inf2 term. If Inf2(tdw) was not used in DFR models, the models with Poisson, Geometric, Binomial distributions would not respect condition 2, i.e would not be concave. In contrast, the use of bursty distributions in information models, together with the conditions on the normalization functions, ensure that condition 2 is satisfied. Another important difference between the two models is that DFR models make use of discrete distributions for realvalued variables, a conceptual flaw that information models do not have. Lastly, if the log-logistic, SPL and INL models have very simple forms (see for example the formulas given above for the weight they generate), the PL2 DFR model, one of the top performing DFR models, has a much more complex form ([18]). Information models are thus not only conceptually simpler, they also lead to simpler formulas.",1,ad,True
485,6. CONCLUSION,0,,False
486,"We have presented in this paper the family of information models. These models draw their inspiration from a long standing idea in information retrieval, namely the one that a word in a document may not behave statistically as expected on the collection. Shannon information can be used to capture whenever a word deviates from its average behavior, and we showed how to design IR models based on this information. In particular, we showed that the choice of the distribution to be used in such models was crucial for obtaining good retrieval models, the notion of good retrieval models being formalized here on the basis of the heuristic retrieval constraints developed in [9]. Our theoretical development also emphasized the notion of ""burstiness"", which has been central to several studies. We showed how this notion relates to heuristic retrieval constraints, and how it can be captured through, e.g., power-law distributions. From these two distributions, we have proposed two effective IR models. The experiments we have conducted on four different collections illustrate the good behavior of these models. They outperform in average the Jelinek-Mercer and Dirichlet prior language models as well as the Okapi BM25 model. They yield results similar to state-of-the-art DFR models (InL2 and PL2) when no pseudo-relevance feedback is used. When using pseudo-relevance feedback, however, the information models we have considered significantly outperform all the other models.",0,,False
487,240,0,,False
488,Acknowledgements,0,,False
489,This research was partly supported by the Pascal-2 Network of Excellence ICT-216886-NOE and the French project Fragrances ANR-08-CORD-008.,0,,False
490,7. REFERENCES,0,,False
491,"[1] E. M. Airoldi, W. W. Cohen, and S. E. Fienberg. Bayesian methods for frequent terms in text: Models of contagion and the 2 statistic.",0,,False
492,"[2] G. Amati, C. Carpineto, G. Romano, and F. U. Bordoni. Fondazione Ugo Bordoni at TREC 2003: robust and web track, 2003.",1,TREC,True
493,"[3] G. Amati and C. J. V. Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.",0,,False
494,"[4] A. L. Barabasi and R. Albert. Emergence of scaling in random networks. Science, 286(5439):509­512, October 1999.",0,,False
495,"[5] D. Chakrabarti and C. Faloutsos. Graph mining: Laws, generators, and algorithms. ACM Comput. Surv., 38(1):2, 2006.",0,,False
496,"[6] K. W. Church and W. A. Gale. Poisson mixtures. Natural Language Engineering, 1:163­190, 1995.",0,,False
497,"[7] S. Clinchant and E´. Gaussier. The BNB distribution for text modeling. In Macdonald et al. [12], pages 150­161.",0,,False
498,"[8] C. Elkan. Clustering documents with an exponential-family approximation of the dirichlet compound multinomial distribution. In W. W. Cohen and A. Moore, editors, ICML, volume 148 of ACM International Conference Proceeding Series, pages 289­296. ACM, 2006.",0,,False
499,"[9] H. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. In SIGIR '04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, 2004.",0,,False
500,"[10] W. Feller. An Introduction to Probability Theory and Its Applications, Vol. I. Wiley, New York, 1968.",0,,False
501,"[11] S. P. Harter. A probabilistic approach to automatic keyword indexing. Journal of the American Society for Information Science, 26, 1975.",0,,False
502,"[12] C. Macdonald, I. Ounis, V. Plachouras, I. Ruthven, and R. W. White, editors. Advances in Information Retrieval , 30th European Conference on IR Research, ECIR 2008, Glasgow, UK, March 30-April 3, 2008. Proceedings, volume 4956 of Lecture Notes in Computer Science. Springer, 2008.",0,,False
503,"[13] R. E. Madsen, D. Kauchak, and C. Elkan. Modeling word burstiness using the dirichlet distribution. In L. D. Raedt and S. Wrobel, editors, ICML, volume 119 of ACM International Conference Proceeding Series, pages 545­552. ACM, 2005.",1,ad,True
504,"[14] S.-H. Na, I.-S. Kang, and J.-H. Lee. Improving term frequency normalization for multi-topical documents and application to language modeling approaches. In Macdonald et al. [12], pages 382­393.",0,,False
505,[15] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR '94:,0,,False
506,"Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 232­241, New York, NY, USA, 1994. Springer-Verlag New York, Inc.",0,,False
507,"[16] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., New York, NY, USA, 1983.",0,,False
508,"[17] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In SIGIR '96: Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 21­29, New York, NY, USA, 1996. ACM.",0,,False
509,"[18] I. O. V. Plachouras, B. He. University of Glasgow at TREC 2004: Experiments in web, robust and terabyte tracks with terrier, 2004.",1,TREC,True
510,"[19] Z. Xu and R. Akella. A new probabilistic retrieval model based on the dirichlet compound multinomial distribution. In SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 427­434, New York, NY, USA, 2008. ACM.",0,,False
511,"[20] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In CIKM '01: Proceedings of the tenth international conference on Information and knowledge management, pages 403­410, New York, NY, USA, 2001. ACM.",0,,False
512,"[21] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, 2004.",0,,False
513,APPENDIX,1,AP,True
514,A. PROOF OF THEOREM 3,0,,False
515,Let us recall what property 3 states: Let P be a probability distribution of class C2. A necessary condition for P to be bursty is:,0,,False
516,2 log(P (X x2,0,,False
517,x)),0,,False
518,>,0,,False
519,0,0,,False
520,"Proof Let P be a continuous probability distribution of class C2. y > 0, the function gy defined by:",0,,False
521,"y > 0,",0,,False
522,"gy(x) , P (X",0,,False
523, x + y|X,0,,False
524," x) ,",0,,False
525,P (X  x + y) P (X  x),0,,False
526,is increasing in x (by definition of a bursty distribution).,0,,False
527,"Let F be the cumulative function of P . Then: gy(x) ,",0,,False
528,F (x+y)-1 F (x)-1,0,,False
529,.,0,,False
530,For,0,,False
531,y,0,,False
532,sufficiently,0,,False
533,"small,",0,,False
534,using,0,,False
535,a,0,,False
536,Taylor,0,,False
537,expansion,0,,False
538,"of F (x + y), we have:",0,,False
539,gy (x),0,,False
540,F (x) + yF (x) - 1 F (x) - 1,0,,False
541,",",0,,False
542,g(x),0,,False
543,where,0,,False
544,F,0,,False
545,denotes,0,,False
546,F x,0,,False
547,.,0,,False
548,ering only the sign of,0,,False
549,"Then, derivating g, we get:",0,,False
550,g,0,,False
551,wrt,0,,False
552,x,0,,False
553,and,0,,False
554,consid-,0,,False
555,sg[g],0,,False
556,",",0,,False
557,sg[F F,0,,False
558,-,0,,False
559,F ,0,,False
560,-,0,,False
561,F 2],0,,False
562,",",0,,False
563,sg[(,0,,False
564,F,0,,False
565,F -,0,,False
566,1 )],0,,False
567,", sg[(log(1 - F ))] , sg[(log P (X  x))]",0,,False
568,As,0,,False
569,gy,0,,False
570,is,0,,False
571,increasing,0,,False
572,in,0,,False
573,"x,",0,,False
574,so,0,,False
575,is,0,,False
576,"g,",0,,False
577,and,0,,False
578,thus,0,,False
579,2 log(P (Xx)) x2,0,,False
580,>,0,,False
581,"0,",0,,False
582,which establishes the property.,0,,False
583,241,0,,False
584,,0,,False

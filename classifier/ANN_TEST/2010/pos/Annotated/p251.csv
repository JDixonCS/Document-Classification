,sentence,label,data,regex
0,Geometric Representations for Multiple Documents,0,,False
1,Jangwon Seo jangwon@cs.umass.edu,0,,False
2,W. Bruce Croft croft@cs.umass.edu,0,,False
3,Center for Intelligent Information Retrieval Department of Computer Science,0,,False
4,"University of Massachusetts, Amherst Amherst, MA 01003",0,,False
5,ABSTRACT,0,,False
6,"Combining multiple documents to represent an information object is well-known as an effective approach for many Information Retrieval tasks. For example, passages can be combined to represent a document for retrieval, document clusters are represented using combinations of the documents they contain, and feedback documents can be combined to represent a query model. Various techniques for combination have been introduced, and among them, representation techniques based on concatenation and the arithmetic mean are frequently used. Some recent work has shown the potential of a new representation technique using the geometric mean. However, these studies lack a theoretical foundation explaining why the geometric mean should have advantages for representing multiple documents. In this paper, we show that the arithmetic mean and the geometric mean are approximations to the center of mass in certain geometries, and show empirically that the geometric mean is closer to the center. Through experiments with two IR tasks, we show the potential benefits for geometric representations, including a geometry-based pseudo-relevance feedback method that outperforms state-of-the-art techniques.",1,ad,True
7,Categories and Subject Descriptors,0,,False
8,H.3.3 [Information Search and Retrieval]: Retrieval Models,0,,False
9,General Terms,0,,False
10,"Algorithms, Measurement, Experimentation",0,,False
11,Keywords,0,,False
12,"multiple documents, information geometry, geometric mean",0,,False
13,1. INTRODUCTION,1,DUC,True
14,"A typical goal in Information Retrieval (IR) is to find relevant documents, where we rank the documents using a representation for a single document. Often, however, a",0,,False
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
16,"representation for multiple documents is needed. For example, tasks such as relevance feedback, passage retrieval and resource selection in distributed information retrieval or in aggregated search, use representations for sets of multiple documents.",0,,False
17,"One of standard approaches for relevance feedback is to estimate an underlying relevance model from given feedback documents and sample likely terms from the model for query expansion. That is, the estimated underlying model can be considered as a representation of the feedback documents. In passage retrieval, representations of text passages can be used to rank passages or documents. In the latter case, we represent a document using a combination of some or all of its passages. In resource selection tasks, the resource or collection is represented using the documents in the collection.",0,,False
18,"As many tasks require representations for multiple documents, various approaches have been introduced. Among them, representation techniques based on the arithmetic mean and concatenation are frequently used. Representation techniques based on the arithmetic mean literally compute the arithmetic mean of multiple language models or vector representations. Representation techniques based on concatenation make a large document by concatenating multiple documents and use a language model or vector to represent the large document.",0,,False
19,"In addition to traditional group representation techniques, some recent studies show the potential of a new representation technique, the geometric mean representation of language models [26, 30, 11, 31]. Liu and Croft [26] compared various representation techniques for cluster retrieval and demonstrated that representations using the geometric mean outperformed others via empirical evaluation. Seo and Croft [30] applied a resource selection technique based on the geometric mean to blog site search. Moreover, Elsas and Carbonell [11] and Seo et al. [31] showed that a thread representation using the geometric mean of postings in the thread can be a good choice for online forum search.",1,ad,True
20,"The previous work which uses the geometric mean to represent a group of documents, however, did not theoretically analyze the geometric mean in the language modeling framework. In other words, although they have demonstrated the performance of representation techniques based on the geometric mean empirically, theoretical evidence or the assumptions behind the geometric mean have not been sufficiently addressed to justify its use in IR.",1,ad,True
21,"Therefore, in this paper, we give a theoretically grounded explanation for geometric mean-based techniques for representing multiple documents. To do this, we consider Information Geometry as a tool and discuss how the arithmetic mean as well as the geometric mean can be inter-",0,,False
22,251,0,,False
23,"preted in certain geometries. More specifically, we show that the arithmetic mean and the geometric mean relate to the Fr´echet sample mean which minimizes the Fr´echet sample function. Furthermore, we empirically show that the geometric mean is closer to the Fr´echet mean.",0,,False
24,"In addition, we address two applications considering the geometric interpretation: cluster retrieval and pseudo-relevance feedback. Particularly, for pseudo-relevance feedback, we introduce a variation of the relevance model [21], the geometric relevance model, and show that this new approach performs better than the relevance model.",1,ad,True
25,"The remainder of this paper is organized as follows. Section 2 reviews previous work. In Section 3, we introduce the Fr´echet mean and geometric representations correspond to the Fr´echet mean in two different metric spaces using Information Geometry. In Section 4, we provide empirical evidence for the geometric representations through experiments for two IR tasks. Section 5 discusses other evidence for the geometric representations. Section 6 concludes this paper.",0,,False
26,2. PREVIOUS WORK,0,,False
27,"Combining multiple evidence is one of the most frequently addressed topics in Information Retrieval. Belkin et al. [2] showed that different representations of the same information object leads to different results and combinations of such representations can improve retrieval performance. Various combination heuristics suggested by Fox and Shaw [12] and analyzed by Lee [23] are still used in many IR tasks such as passage retrieval and resource selection. Using passage-level evidence [7, 25, 3] for document retrieval necessarily requires combination techniques. Resource selection where a collection is represented by its own documents [6, 32] actively uses combination techniques as well.",1,ad,True
28,"Relevance feedback (and pseudo-relevance feedback) is another task using combination-based representation techniques. To estimate a query model for query expansion, the top ranked documents are combined. Rocchio [29] introduced a feedback technique to combine positive or negative feedback documents in vector spaces. Lavrenko and Croft [21] introduced a technique that estimates a underlying relevance model in the language modeling framework. In fact, these standard relevance feedback approaches implicitly use the arithmetic mean. Recently, Collins-Thompson and Callan [9] used a parametric approach using re-sampling to estimate a posterior Dirichlet distribution for the documents. That is, they use the mean and the variance of the Dirichlet distribution to get a feedback model.",0,,False
29,"The geometric mean-based representation technique was relatively recently introduced. Liu and Croft [26] demonstrated that representation by the geometric mean works well for cluster retrieval via comparisons with vairous representation techniques. Seo and Croft [30] suggested a resource selection technique by the geometric mean for blog site retrieval. Furthermore, the technique was shown to work well for thread search in online forums [11, 31]. The geometric mean is often used in other fields. For example, Kogan et al. [18] used the geometric mean for k-means clustering. Veldhuis [34] showed that a centroid of the symmetrical Kullback-Leibler divergence is related to the arithmetic mean and the normalized geometric mean.",1,blog,True
30,"In this paper, to justify the use of the geometric mean in IR, we find evidence from Information Geometry. Rao",0,,False
31,"[28] and Jeffreys [14] are the first people who considered the Fisher information metric as a Riemannian metric. Later, Efron [10] focused on differential geometry in statistics considering the curvature of statistical models. Recently, Lebanon [22] applied the theory to many machine learning tasks. See Amari and Nagaoka [1] and Kass and Vos [16] for comprehensive introduction to Information Geometry.",0,,False
32,3. GEOMETRY OF MULTIPLE DOCUMENTS,0,,False
33,"We introduce the Fr´echet mean and derive the mean in two different metric spaces, i.e., the Euclidean metric space and the Riemannian manifold defined by the Fisher information metric.",0,,False
34,3.1 Fréchet Mean,0,,False
35,"Let us consider a Riemannian manifold M with a distance measure dist(x, y) where x and y are points on the manifold. Assume that we have a distribution Q on a convex set U  M. Now we define a function F : M  R as follows:",0,,False
36,"(c) ,",0,,False
37,"dist2(c, p)Q(dp)",0,,False
38,pU,0,,False
39,"This function is known as the Fr´echet function. A set of points which minimize the function is called the Fr´echet mean set of Q. If there is only a point in the set, the point is called the Fr´echet mean. This general notation for a center or centroid associated with a probability distribution was introduced by Fr´echet [13] and Karcher [15]. This mean is called by various names, e.g., the center of mass, barycenter, Karcher mean and Fr´echet mean. In this work, we refer to this mean as the Fr´echet mean1. The concept of the Fr´echet mean is general and not limited to any specific metric; accordingly, this can be applied to any metric space. Indeed, as we will see soon, it also generalizes the ordinary Euclidean mean.",0,,False
40,"Kendall [17] proved that if the support of Q is in a geodesic ball of sufficiently small radius r, then one Fr´echet mean uniquely exists. As we see later, we consider a statistical manifold for multinomial distributions, and the distributions are mapped onto a simplex or a positive sphere. Since the mapped area is sufficiently small, a unique Fr´echet mean exists. For example, in case of a sphere, the radius of the geodesic ball is /4 and the positive sphere is contained in the ball.",1,ad,True
41,"If we have n unique points p1, p2, · · · , pn in m i.i.d. samples from distribution Q, then we consider the sample Fr´echet mean which minimizes the Fr´echet sample function given by",0,,False
42,n,0,,False
43,"¯ (c) ,"" dist2(c, pi)Q^(pi)""",0,,False
44,(1),0,,False
45,"i,1",0,,False
46,where Q^ is an empirical distribution estimated from the samples.,0,,False
47,"Bhattacharya and Patrangenaru [5] showed that every measurable choice from the Fr´echet sample mean set of Q^ is a strongly consistent estimator of the Fr´echet mean of Q. In this paper, we consider multiple documents to represent as samples and the Fr´echet sample mean as a representation.",0,,False
48,"1Strictly speaking, this is the intrinsic Fr´echet mean in that we use a geodesic distance. However, since we address only the intrinsic Fr´echet means in this paper, we omit term ""intrinsic"".",1,ad,True
49,252,0,,False
50,1,0,,False
51,2,0,,False
52,0,0,,False
53,0,0,,False
54,0,0,,False
55,0,0,,False
56,0,0,,False
57,0,0,,False
58,1,0,,False
59,1,0,,False
60,2,0,,False
61,2,0,,False
62,"Figure 1: Assuming the Euclidean metric space, a n + 1 dimensional multinomial distribution is mapped to a point in the n-simplex in Euclidean space (left). Assuming the Riemannian manifold defined by the Fisher information metric, the same point is mapped to a point in the positive n-sphere of radius 2 (right).",1,ad,True
63,"Therefore, we address how to compute the sample Fr´echet mean from the multiple documents in the following sections.",1,ad,True
64,3.2 Euclidean Metric space,0,,False
65,"Let's begin with the Euclidean metric space. We assume that terms observed in a document are samples from a multinomial distribution and each document has a distinct distribution. Assuming a conjugate Dirichlet prior, we estimate the multinomial distribution, i.e. a language model, using Dirichlet smoothing [35] as follows:",0,,False
66,Pr(w|D),0,,False
67,",",0,,False
68,"tfw,D",0,,False
69,+  · cfw/|C| |D| + ,0,,False
70,(2),0,,False
71,"where tfw,D is the occurrence of term w in document D, cfw is the occurrence of w in a set of observations C considered for the prior distribution (typically, a corpus), |D| is the number of observations, i.e. the length of D, |C| is the length of C, and  is the Dirichlet smoothing parameter. Note that P (w|D) is a parameter which corresponds to outcome w in the multinomial distribution.",0,,False
72,"The size of vocabulary of a language model is defined as the number of terms observed in C, which also determines the number of dimensions of the Euclidean metric space for a multinomial distributions. When the number of dimensions is n + 1, a multinomial distribution corresponds to a point in n-simplex Pn which is defined as follows:",0,,False
73,"Pn ,",0,,False
74,n+1,0,,False
75,"x  Rn+1 : i, x(i) > 0, x(i) , 1",0,,False
76,"i,1",0,,False
77,An example of 2-simplex embedded in 3-dimensional Euclidean space is shown in Figure 1.,0,,False
78,"Since a geodesic linking two points in n-simplex is a straight line, the distance between two multinomial distributions is calculated by the Euclidean distance as follows:",0,,False
79,"dist(x, y) ,",0,,False
80,n+1,0,,False
81,(x(i) - y(i))2,0,,False
82,"i,1",0,,False
83,"Consider multinomial distributions of k given documents, p1, p2, · · · , pk as samples from distribution Q over the nsimplex. Then, the Fr´echet sample function is given by",0,,False
84,k,0,,False
85,n+1,0,,False
86,"¯ (c) , Q^(pi) (c(j) - p(ij))2",0,,False
87,"i,1",0,,False
88,"j,1",0,,False
89,"Therefore, we have the following optimization problem to",0,,False
90,obtain the Fr´echet sample mean.,0,,False
91,minimize,0,,False
92,k,0,,False
93,n+1,0,,False
94,Q^(pi) (c(j) - p(ij))2,0,,False
95,"i,1",0,,False
96,"j,1",0,,False
97,n+1,0,,False
98,subject to,0,,False
99,"c(j) ,"" 1,""",0,,False
100,"j, c(j) > 0",0,,False
101,(3),0,,False
102,"j,1",0,,False
103,"It is trivial to solve this problem using the method of Lagrange multipliers. Finally, we have a solution as follows:",0,,False
104,k,0,,False
105,"c(j) ,",0,,False
106,p(ij ) Q^ (pi ),0,,False
107,(4),0,,False
108,"i,1",0,,False
109,"This is the Fr´echet sample mean in the Euclidean metric space. Indeed, if Q^(pi) is uniform, i.e, 1/k, then this is the same as the ordinary Euclidean mean or the arithmetic mean. Therefore, the Fr´echet sample mean in the Euclidean metric space generalizes the arithmetic mean.",0,,False
110,We use the Fr´echet sample mean as a representative multinomial distribution for the given group of multiple documents.,0,,False
111,3.3 Riemannian manifold defined by the Fisher information metric,0,,False
112,"Many IR approaches assume that data is embedded in the Euclidean geometry. However, assumptions of non-Euclidean geometries may lead to a better understanding of data. We here consider a Riemannian space where a Riemannian metric is the Fisher information metric. This metric space is used for investigating the geometric structures of statistical models in most of the Information Geometry literature [28, 1, 16]. Furthermore, a number of approaches assume this metric space for statistical inference and machine learning [20, 22, 1]. Particularly, for text classification, Lafferty and Lebanon [20] showed that techniques based on this metric space perform better than techniques based on the Euclidean metric.",1,ad,True
113,The Fisher information metric is defined as follows:,0,,False
114,"gi,j() ,",0,,False
115,log p(x;  (i),0,,False
116,),0,,False
117,log p(x;  (j ),0,,False
118,),0,,False
119,p(x;,0,,False
120,)dx,0,,False
121, log p(x; )  log p(x; ),0,,False
122,", E",0,,False
123, (i),0,,False
124, (j ),0,,False
125,"where  is a point in a differential manifold and corresponds to a statistical model in a parametric familty p(x; ), i and j are indices for a coordinate system. In this work, it is easy",0,,False
126,253,0,,False
127,to think that  is a multinomial model for a document while i and j are indices for unique terms in vocabulary.,0,,False
128,"This metric has some nice properties. By Cram´er-Rao inequality [28], the variance of unbiased estimators is bounded by the inverse of the metric. Particularly, an unbiased estimator achieving the bound is called an efficient estimator which is the best unbiased estimator because it minimizes the variance. Furthermore, by Chentsov's theorem [8], the Fisher information metric is the only Riemannian metric which is invariant under basic probabilistic transformations.",0,,False
129,"We now look into the Riemannian geometry with the Fisher information metric as a Riemannian metric. First of all, let us consider the positive n-sphere of radius 2, S~n+ instead of n-simplex Pn.",1,ad,True
130,"S~n+ ,",0,,False
131,n+1,0,,False
132,"x  Rn+1 : i, x(i) > 0, (x(i))2 , 22",0,,False
133,"i,1",0,,False
134,Figure 1 shows an example of the positive 2-sphere of radius,1,ad,True
135,2. We can define transformation  : Pn  S~n+ by,0,,False
136," z(j) , (x)(j) , 2 x(j)",0,,False
137,"The inverse transformation -1 is well known to pull back the Fisher information metric on Pn to the Euclidean metric on S~n+ [16, 22]. Therefore, the transformation is an isometry, and we can compute the distance between two statistical models by the Fisher information metric using the geodesic distance between two corresponding points on the sphere. In other words, the distance is the length of the shortest curve linking two corresponding points on the sphere and is given by",0,,False
138,n+1,0,,False
139,"dist(x, y) , 2 arccos",0,,False
140,"j,1",0,,False
141,x(j ) y (j ),0,,False
142,"This is called the information distance. With this distance, we have the following Fr´echet sample",0,,False
143,function.,0,,False
144,k,0,,False
145,n+1,0,,False
146,"¯ (c) , 4 arccos2",0,,False
147,x(j)y(j) Q^(pi),0,,False
148,"i,1",0,,False
149,"j,1",0,,False
150,"Unfortunately, there is no closed form solution for the Fr´echet sample mean which minimizes this function. Although we can use some convex optimization techniques, such approaches may be impractical in case that n is large. Indeed, in many IR tasks, n + 1 is the size of vocabulary and can be very large.",0,,False
151,"Therefore, to find the Fr´echet sample mean, we try an approximation approach using the Kullback-Leibler (KL) divergence which is defined as follows:",0,,False
152,D(x||y),0,,False
153,",",0,,False
154,n+1,0,,False
155,x(j),0,,False
156,log,0,,False
157,x(j) y(j),0,,False
158,"j,1",0,,False
159,"As y  x, approximately by the Taylor expansion,",0,,False
160,log x(j) - log y(j),0,,False
161,",",0,,False
162,-,0,,False
163,(y,0,,False
164,(j) - x(j)) x(j),0,,False
165,+,0,,False
166,(y(j) - x(j))2 2(x(j))2,0,,False
167,+ O((y(j),0,,False
168,- x(j))3),0,,False
169,"From this,",0,,False
170,D(x||y) + D(y||x),0,,False
171,n+1,0,,False
172,",",0,,False
173,x(j) log x(j) - log y(j) + y(j) log y(j) - log x(j),0,,False
174,"j,1",0,,False
175,",",0,,False
176,1 2,0,,False
177,n+1,0,,False
178,(y(j) - x(j))2 x(j),0,,False
179,+,0,,False
180,1 2,0,,False
181,n+1,0,,False
182,(x(j) - y(j))2 y(j),0,,False
183,+,0,,False
184,O(||y,0,,False
185,-,0,,False
186,x||3),0,,False
187,"j,1",0,,False
188,"j,1",0,,False
189,(5),0,,False
190,"Since y approaches x along geodesic c linking them, we can parameterize the path by arclength s so that c(s0) ,"" x, c(s1) "", y and s1 - s0 ,"" dist(x, y). The difference between two points is expressed by a product of the geodesic length and the tangent vector to the curve as follows:""",0,,False
191,y(j) - x(j),0,,False
192,",",0,,False
193,(s1,0,,False
194,-,0,,False
195,s0),0,,False
196, c(j ) s,0,,False
197,","" dist(x, y) c(j) s""",0,,False
198,"Then, the first term in Equation (5) can be rewritten as follows:",0,,False
199,1 n+1 1,0,,False
200,2,0,,False
201,x(j),0,,False
202,"j,1",0,,False
203," c(j ) dist(x, y)",0,,False
204,s,0,,False
205,2,0,,False
206,",",0,,False
207,"1 dist2(x, y) n+1 2",0,,False
208,1 c(j)(s),0,,False
209,"j,1",0,,False
210,c(j) 2 s,0,,False
211,","" 1 dist2(x, y) n+1 c(j)(s)  log c(j) 2 "","" 1 dist2(x, y)I(s)""",0,,False
212,2,0,,False
213,s,0,,False
214,2,0,,False
215,"j,1",0,,False
216,"where I(s) is the Fisher information for s. By definition of the length of the curve,",0,,False
217,s1,0,,False
218,"I(s)ds ,"" dist(x, y) "", s1 - s0",0,,False
219,s0,0,,False
220,"Hence, I(s) ,"" 1, and we finally have the following:""",0,,False
221,1 2,0,,False
222,n+1,0,,False
223,(y(j) - x(j))2 x(j),0,,False
224,",",0,,False
225,"1 dist2(x, y) 2",0,,False
226,(6),0,,False
227,"j,1",0,,False
228,"Similarly, the second term in Equation (5) can be also written as Equation (6). Therefore, we have an approximation of Equation (5) as follows:",0,,False
229,"D(x||y) + D(y||x) ,"" dist2(x, y) + O(||y - x||3)  dist2(x, y)""",0,,False
230,"Similar relationships between divergences and distances can be founded in various texts [1, 16].",0,,False
231,"From this approximation, we can express the Fr´echet sample mean with the KL divergence as follows:",0,,False
232,k,0,,False
233,¯ (c)  (D(pi||c) + D(c||pi)) Q^(pi),0,,False
234,(7),0,,False
235,"i,1",0,,False
236,This means that finding the Fr´echet sample mean is reduced to finding the symmetrized Bregman centroid cF [27] which,0,,False
237,is defined as follows:,0,,False
238,"cF , arg min c",0,,False
239,k,0,,False
240,1 2,0,,False
241,(DF,0,,False
242,(pi||c),0,,False
243,+,0,,False
244,DF,0,,False
245,(c||pi)),0,,False
246,Q^(pi),0,,False
247,"i,1",0,,False
248,"where DF (x||y) is the Bregman divergence defined by F (x)- F (y)- x-y, F (y) and F is a generator function. For example, if F is the negative Shannon entropy, i.e. j x(j) log x(j),",0,,False
249,254,0,,False
250,then the Bregman divergence is the same as the KL diver-,0,,False
251,"gence. That is, the Bregman divergence is a generalized divergence. In addition, right-sided centroid cFR and left-sided centroid cFL are defined as follows:",1,ad,True
252,k,0,,False
253,cFR,0,,False
254,",",0,,False
255,arg,0,,False
256,min c,0,,False
257,DF (pi||c)Q^(pi),0,,False
258,"i,1",0,,False
259,k,0,,False
260,cFL,0,,False
261,",",0,,False
262,arg min c,0,,False
263,DF (c||pi)Q^(pi),0,,False
264,"i,1",0,,False
265,Nielsen and Nock [27] show that symmetrized Bregman centroid cF lies on a geodesic linking cFR and cFL via the Bregman Pythagoras' theorem. We can apply the result to,0,,False
266,the KL divergence. We can easily compute cFR using the method of Lagrange,0,,False
267,"multipliers with the same constraints as Equation (3), and",0,,False
268,the solution coincides with the arithmetic mean as follows:,0,,False
269,k,0,,False
270,"cFR(j) ,",0,,False
271,Q^ (pi )p(ij ),0,,False
272,"i,1",0,,False
273,"Similarly, using the method of Lagrange multipliers, we compute cFL as follows:",0,,False
274,k,0,,False
275,"cFL (j) ,",0,,False
276,"i,1",0,,False
277,p(ij ) Q^ (pi ),0,,False
278,n+1 k,0,,False
279,/,0,,False
280,"j,1 i,1",0,,False
281,p(ij ) Q^ (pi ),0,,False
282,"If Q^ ,"" 1/k, then this is the ordinary normalized geometric mean.""",0,,False
283,"Therefore, the symmetrized Bregman centroid when F is the negative Shannon entropy, or the approximated Fr´echet sample mean lies on the geodesic linking the arithmetic mean and the normalized geometric mean.",0,,False
284,We consider the two means as approximations to the Fr´echet sample mean and take the following approach to decide a representation among them:,0,,False
285,1. Compute the arithmetic mean cA and the normalized geometric mean cG from multinomial models of multiple documents.,0,,False
286,2. Compute ¯ (cA) and ¯ (cG) by Equation (1),0,,False
287,"3. As a representation, choose cG if ¯ (cA) > ¯ (cG), cA otherwise.",0,,False
288,"That is, we choose a point which is closer to the Fr´echet sample mean as a representation. We call this approach ""geometric selection"".",0,,False
289,4. EXPERIMENTS,0,,False
290,"To evaluate representation techniques derived in the previous section, we conduct experiments for two different tasks: cluster retrieval and pseudo-relevance feedback.",0,,False
291,"For the experiments, we use 3 standard collections from TREC. Table 1 shows the statistics of the collections. To estimate a language model from each document, we use the Dirichlet smoothing. For each task, the initial results are obtained by query-likelihood scores which are computed under an independence assumption as follows:",1,TREC,True
292,"P (Q|D) , P (q|D)",0,,False
293,qQ,0,,False
294,where P (q|D) is estimated by Equation (2).,0,,False
295,AP,1,AP,True
296,WSJ,0,,False
297,GOV2,0,,False
298,TREC topics 51-200 51-200 701-800,1,TREC,True
299,#docs,0,,False
300,"242,918 173,252 25,205,179",0,,False
301,Table 1: Test collections.,0,,False
302,"For index building, we used the Indri system [33]. Each document was stemmed by the Krovetz stemmer and stopped by a standard stopword set. To test the significance of results, we performed a randomization test.",0,,False
303,4.1 Cluster Retrieval,0,,False
304,"Cluster retrieval involves finding the best document cluster [24, 26]. We first retrieve the top 100 documents for each query according to query-likelihood scores. Next, we perform kNN clustering [19]. That is, assuming that each returned document is a cluster centroid, a cluster is formed by its k - 1 nearest neighbors (k is set to 5). We use cosine similarity as a similarity measure. In fact, since cosine similarity assumes the Euclidean metric space, other similarity measures may perform better for our representation technique which assumes a different metric. However, since arbitrary clusters are assumed in cluster retrieval, we use the same similarity measure as used in previous work [26].",0,,False
305,"Once we have clusters, we represent each cluster by the arithmetic mean of language models of documents in a cluster assuming the Euclidean metric. On the other hand, assuming the Fisher information metric, we can determine a representation via geometric selection between the arithmetic mean and the normalized geometric mean of the documents.",0,,False
306,"Evaluation of various representation techniques such as concatenation or CombMax [12] for cluster retrieval has been already done by Liu and Croft [26]. They concluded that the geometric mean representation outperforms other techniques. Therefore, we do not intend to repeat the same work. Instead, we focus on geometric interpretations for experimental results.",1,ad,True
307,"For a fair comparison, the same clusters are given to each representation technique. The only parameter to be tuned is the smoothing parameter for the initial results. We set the parameter so that Mean Average Precision (MAP) for the initial results by the query-likelihood P (Q|D) is maximized. Evaluation is performed using all topics. Since our goal is to find the best cluster, we use Precision at 5 (P@5) in order to evaluate the cluster first ranked by each representation technique, i.e. how many relevant documents the cluster has. Table 2 shows the results. In addition to the arithmetic mean and geometric selection, we present results using the geometric mean as well.",1,MAP,True
308,"For all collections, representations by the geometric mean and geometric selection show better performance than representations by the arithmetic mean. Except for GOV2, The improvements are statistically significant. These experiments indicate some interesting points. First, in geometric selection, the normalized geometric means were selected as representations which minimize the Fr´echet sample function for all queries across all collections. In other words, the normalized geometric means are better approximations to the Fr´echet sample mean. Second, since the normalized geometric means selected by geometric selection lead to consistently better retrieval results, we may say that the goodness of a representation for this task is related to how close the rep-",1,ad,True
309,255,0,,False
310,A-MEAN G-MEAN SELECT,0,,False
311,AP,1,AP,True
312,0.3053 0.3347 0.3347,0,,False
313,WSJ,0,,False
314,0.4747 0.5040 0.5027,0,,False
315,GOV2 0.5374 0.5576 0.5556,0,,False
316,"Table 2: Results for cluster retrieval. A-MEAN, GMEAN and SELECT mean representations by the arithmetic mean, by the geometric mean, and by geometric selection, respectively. The numbers are P@5 scores. A * indicates a statistically significant improvement over A-MEAN (p < 0.05).",0,,False
317,"resentation is to the center of mass, i.e. the Fr´echet sample mean. Moreover, this justifies the assumption of the geometry defined by the Fisher information metric. Lastly, since geometric selection does not consider the geometric mean but the normalized geometric mean, the results in the `SELECT' row are exactly the same as those by the normalized geometric means. Therefore, the differences between the `G-MEAN' row and the `SELECT' row are caused by the normalization. As you see, since the differences are small, we suggest that the geometric mean without normalization can be a better choice in practice.",0,,False
318,4.2 Pseudo-Relevance Feedback,0,,False
319,"Lavrenko and Croft's relevance model [21] is one of the standard language modeling approaches for pseudo-relevance feedback. The model assumes that the top k retrieved documents for query q are sampled from an underlying relevance model for q. That is, a hidden multinomial model relevant to a user information need exists, and we estimate the model from the top k documents. Then, we sample terms which describe the information need better than the original query and use the terms for query expansion.",0,,False
320,Estimation of the relevance model is done by the following formula:,0,,False
321,"P (w|q) ,",0,,False
322,"k i,1",0,,False
323,p(w|Di,0,,False
324,)P,0,,False
325,(q|Di,0,,False
326,)P,0,,False
327,(Di),0,,False
328,(8),0,,False
329,p(q),0,,False
330,"where q is a user query, w is a candidate for expansion terms, and Di is a document in the top k initial results, respectively.",0,,False
331,"Although this is derived from a Bayesian model, we can see this as a representation for the top k documents by the arithmetic mean rewriting Equation (8) as follows:",0,,False
332,k,0,,False
333,p(w|Di,0,,False
334,),0,,False
335,P,0,,False
336,(q|Di)P p(q),0,,False
337,(Di,0,,False
338,),0,,False
339,",",0,,False
340,k,0,,False
341,p(w|Di)P (Di|q),0,,False
342,"i,1",0,,False
343,"i,1",0,,False
344,"This has the same form as the weighted arithmetic mean of Equation (4). In other words, P (w|Di) is a multinomial parameter and P (Di|q) represents a distribution over a sample space limited by q, i.e, Q^. In the standard implementation",0,,False
345,"of the relevance model by the Indri system [33], P (D) is",0,,False
346,"assumed to be uniform. Hence,",0,,False
347,"P (Di|q) ,",0,,False
348,P (q|Di)P (D),0,,False
349,"k i,1",0,,False
350,P,0,,False
351,(q|Di,0,,False
352,)P,0,,False
353,(D),0,,False
354,",",0,,False
355,P (q|Di),0,,False
356,"k i,1",0,,False
357,P,0,,False
358,(q|Di,0,,False
359,),0,,False
360,"That is, the weight Q^ ,"" P (Di|q) is the normalized querylikelihood scores obtained in the initial retrieval phase. Therefore, we can say that the relevance model represents a group of the top k documents combining the language models by the arithmetic mean weighted by the initial search results.""",0,,False
361,RM GRM,1,GRM,True
362,AP,1,AP,True
363,0.2541 0.2769,0,,False
364,WSJ,0,,False
365,0.3531 0.3851,0,,False
366,GOV2,0,,False
367,0.3204 0.3300,0,,False
368,"Table 3: Results for pseudo-relevance feedback. RM and GRM mean the relevance model and the geometric relevance model, respectively. The numbers are MAP scores. A * indicates a statistically significant improvement over RM (p < 0.01).",1,GRM,True
369,"In this sense, we can say that the relevance model implicitly assumes the Euclidean metric space.",0,,False
370,We can replace the arithmetic mean by the normalized geometric mean to develop a new representation as follows:,0,,False
371,k,0,,False
372,"P (w|q) , p(w|Di)P (Di|q)/",0,,False
373,k,0,,False
374,p(w|Di)P (Di|q) (9),0,,False
375,"i,1",0,,False
376,"wV i,1",0,,False
377,"We can consider the original relevance model and this model as two approximated representations in the Riemannian manifold defined by the Fisher information metric. To determine a representation, we use geometric selection and call the selected model the ""geometric relevance model"".",0,,False
378,"We compare the geometric relevance model with the relevance model. For each query, we first retrieve the top k documents by query-likelihood scores and build a relevance model or geometric relevance model for the documents. Then, we choose the top M terms according to probabilities of the terms in the models. Finally, we expand the original query combining the expansion terms using an interpolation weight  in the Indri query language. The paremeters k, M and  are tuned so that MAP scores by the relevance model are maximized. The same parameters are used for the geometric relevance model. Topic 51-150 for AP and WSJ and topic 701-750 for GOV2 are used as training topics to learn the parameters. Topic 151-200 for AP and WSJ and topic 751800 for GOV2 are used as test topics. We retrieve up to 1000 results for each expanded query and use MAP as the evaluation metric.",1,MAP,True
379,"Table 3 shows the results. The geometric relevance model significantly outperforms the relevance model for all three collections. Similar to cluster retrieval, geometric selection selected models by Equation (9) rather than the original relevance model as representations for all queries except for three queries of GOV2. That is, the geometric mean is a better approximation to the center of mass for this task. This provides more empirical evidence that the geometric mean can be an appropriate choice for representation.",0,,False
380,5. DISCUSSIONS,0,,False
381,5.1 Visualization of geometries,0,,False
382,"To show how multiple documents, the arithmetic mean and the normalized geometric mean are distributed in each geometry, we use the following visualization. First, we construct a weighted complete graph, where each node is a document or the mean and a weight is determined by a kernel reflecting each geometry.",0,,False
383,"For the Euclidean metric, we use the following heat kernel:",0,,False
384,"K(x1, x2) , exp",0,,False
385,n+1,0,,False
386,-,0,,False
387,x(1j) - x(2j),0,,False
388,2,0,,False
389,/4t,0,,False
390,"j,1",0,,False
391,256,0,,False
392,"Figure 2: Geometric visualization of the top 20 documents for Topic 770 (GOV2), the arithmetic mean (AM) and the normalized geometric mean (GM) for different metrics, i.e. the Euclidean metric (left) and the Fisher information metric (right).",0,,False
393,TxM,0,,False
394,x,0,,False
395,V m' V,0,,False
396,M,0,,False
397,m,0,,False
398,y',0,,False
399,y,0,,False
400,Figure 3: Determinination of a middle point m on a geodesic linking x and y,0,,False
401,"where t is a time parameter. For the Fisher information metric, we use the following",0,,False
402,information diffusion kernel [20]:,0,,False
403,n+1,0,,False
404,"K(x1, x2) , exp - arccos2",0,,False
405,x(1j)x(2j) /4t,0,,False
406,"j,1",0,,False
407,"We visualize each geometry using CCVisu [4] which is a tool implementing energy models so that the higher weight between two points results in the smaller Euclidean distance between them. A visualization example is shown in Figure 2. As you see, the arithmetic mean appears closer to the center in the Euclidean metric space while the normalized geometric mean appears closer in the Riemannian manifold defined by the Fisher information metric. Since the visualization tool uses random seeds to initialize the layout, the results vary every time. However, the trend for the locations of the means was consistent.",0,,False
408,5.2 More accurate estimation,0,,False
409,"Geometric selection is a somewhat simple approach to determine the approximated Fr´echet sample mean. That is, we choose one among only two options: the normalized geometric mean and the arithmetic mean. We now consider a more accurate estimation technique for the Fr´echet sample mean.",0,,False
410,"A point which minimizes the approximated Fr´echet sample function of Equation (7) lies on a geodesic linking the arithmetic mean and the normalized geometric mean. Let M , x, y and c be the statistical manifold defined by the Fisher information metric, the arithmetic mean, the normalized geometric mean and a geodesic linking the two points, respectively. First, we get vector V on tangent space TxM via log map logx : M  TxM . In case of a sphere, the log",0,,False
411,"Figure 4: Relative locations of the more accurately estimated Fr´echet sample means. The x-axis corresponds to the relative locations, and the y-axis corresponds to queries for each collection. As a relative location is closer to 1.0, the estimated mean for the topic is located near the normalized geometric mean.",0,,False
412,AP WSJ GOV2 GRM+ 0.2769 0.3852 0.3309,1,AP,True
413,Table 4: Pseudo-relevance feedback results of the more accurately estimated Fr´echet sample mean in the Riemannian manifold defined by the Fisher information metric.,0,,False
414,map is given by:,0,,False
415,V (j),0,,False
416,",",0,,False
417,logx (y)(j ),0,,False
418,",",0,,False
419,"arccos( x, y ) 1 - x, y 2",0,,False
420,"y(j) - x, y x(j)",0,,False
421,"Then, V links x to y on TxM corresponding to y on M .",0,,False
422,"m denotes a middle point between x and y on TxM , reached by V (0    1). We now get a middle point m on c via exponential map expx : TxM  M . The exponential map of a sphare is:",0,,False
423,m(j),0,,False
424,",",0,,False
425,expx(V,0,,False
426,)(j),0,,False
427,",",0,,False
428,cos (||V,0,,False
429,||),0,,False
430,+,0,,False
431,sin,0,,False
432,(||V ||V ||,0,,False
433,||) V,0,,False
434,(j),0,,False
435,Figure 3 illustrates this procedure. Note that the arithmetic mean x and the geometric mean y are interchangeable in the above formulation because a sphere is symmetric.,0,,False
436,"We apply this result to pseudo-relevance feedback experiments. We perform grid search on the geodesic varying  in [0,1] by step-size 0.1, and a point which minimizes the Fr´echet sample function of Equation (1) is selected as a representation. Figure 4 shows 's selected for test queries for each collection. For all test topics except for three topics of GOV2, the selected 's are equal to or greater than 0.5. That is, the more accurately estimated Fr´echet sample means are also closer to the normalized geometric mean than the arithmetic mean. Table 4 shows the results when the representations are used for pseudo-relevance feedback. All results are equal to or a little bit better than the results of the GRM in the Table 3, but not significantly. Therefore, we can say that the geometric relevance model is a reasonable approximation to the Fr´echet sample mean for this task.",1,GRM,True
437,5.3 Anoher reason for the geometric mean,0,,False
438,We have addressed so far theoretical and empirical reasons explaining why the geometric mean should have advantages,1,ad,True
439,257,0,,False
440,"for many IR tasks. There can be many other explanations. One of them is the log-linearity of the geometric mean. As more documents contain a specific term, the geometric mean for the term increases exponentially while the arithmetic mean increases linearly. Accordingly, the arithmetic mean can be sensitive to a few dominant terms in a small number of documents. On the other hand, the geometric mean favors the common terms across a whole set of documents and is relatively insensitive to such a few dominant terms. This shows the robustness of the geometric mean which can lead to a good representation for multiple documents.",1,ad,True
441,6. CONCLUSIONS,0,,False
442,"Previous work which uses the geometric mean as a representation technique does not provide enough theoretical evidence explaining why the geometric mean should have advantages as a representation for IR. There are various explanations. In this work, we showed that using Information Geometry, the arithmetic mean and the normalized geometric mean are approximation points to the center of mass in the Euclidean space or in a statistical manifold. In particular, through empirical evidence, we demonstrated that the normalized geometric mean is closer to the center in the statistical manifold. In addition to this discovery, we introduced a new approach to pseudo-relevance feedback that outperformed the relevance model. For future work, we will investigate how geometric interpretations can be applied to other IR tasks. We expect that this effort will lead to not only the discovery of novel IR theories but also development of effective algorithms.",1,ad,True
443,7. ACKNOWLEDGMENTS,0,,False
444,"This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS0711348, and in part by NSF grant #IIS-0534383. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.",0,,False
445,8. REFERENCES,0,,False
446,"[1] S. Amari and H. Nagaoka. Methods of Information Geometry. American Mathematical Society, 2000.",0,,False
447,"[2] N. J. Belkin, C. Cool, W. B. Croft, and J. P. Callan. The effect multiple query representations on information retrieval system performance. In SIGIR '93, 1993.",0,,False
448,"[3] M. Bendersky and O. Kurland. Utilizing passage-based language models for document retrieval. In ECIR '08, 2008.",0,,False
449,"[4] D. Beyer. CCVisu: Automatic visual software decomposition. In Proc. Int'l Conf. on Software Engineering, 2008.",0,,False
450,"[5] R. Bhattacharya and V. Patrangenaru. Nonparametic estimation of location and dispersion on riemannian manifolds. Journal of Statistical Planning and Inference, 108, 2002.",0,,False
451,"[6] J. Callan. Distributed information retrieval. In W. B. Croft, editor, Advances in Information Retrieval. Kluwer Academic Publishers, 2000.",1,ad,True
452,"[7] J. P. Callan. Passage-level evidence in document retrieval. In SIGIR '94, 1994.",0,,False
453,"[8] N. N. Chentsov. Statistical Decision Rules and Optimal Inference. American Mathematical Society, 1982.",0,,False
454,"[9] K. Collins-Thompson and J. Callan. Estimation and use of uncertainty in pseudo-relevance feedback. In SIGIR '07, 2007.",0,,False
455,"[10] B. Efron. Defining the curvature of a statistical problem. The Annals of Statistics, 3(6).",0,,False
456,"[11] J. L. Elsas and J. G. Carbonell. It pays to be picky: an evaluation of thread retrieval in online forums. In SIGIR '09, 2009.",1,ad,True
457,"[12] E. A. Fox and J. A. Shaw. Combination of multiple searches. In TREC-2, 1994.",1,TREC,True
458,"[13] M. Fr´echet. Les ´el´ements al´eatoires de nature quelconque dans un espace distanci´e. Ann. Inst. H. Poincar´e, 10, 1948.",0,,False
459,"[14] H. Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, 186(1007), 1946.",0,,False
460,"[15] H. Karcher. Riemannian center of mass and mollifier smoothing. Communications on pure and applied mathematics, 30(5), 1977.",0,,False
461,"[16] R. E. Kass and P. W. Vos. Geometrical Foundations of Asymptotic Inference. Wiley-Interscience, 1997.",0,,False
462,"[17] W. Kendall. Probability, convexity, and harmonic maps with small image i: Uniqueness and fine existence. Proc. London Math. Soc., 61, 1990.",0,,False
463,"[18] J. Kogan, M. Teboulle, and C. Nicholas. The entropic geometric means algorithm: An approach for building small clusters for large text datasets. In the Workshop on Clustering Large Data Sets, 2003.",0,,False
464,"[19] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In SIGIR '04, 2004.",1,ad,True
465,"[20] J. Lafferty and G. Lebanon. Diffusion kernels on statistical manifolds. The Journal of Machine Learning Research, 6, 2005.",0,,False
466,"[21] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR' 01, 2001.",0,,False
467,"[22] G. Lebanon. Riemannian Geometry and Statistical Machine Learning. PhD thesis, 2005.",0,,False
468,"[23] J. H. Lee. Analyses of multiple evidence combination. In SIGIR '97, 1997.",0,,False
469,"[24] A. Leuski. Evaluating document clustering for interactive information retrieval. In CIKM '01, 2001.",0,,False
470,"[25] X. Liu and W. B. Croft. Passage retrieval based on language models. In CIKM '02, 2002.",0,,False
471,"[26] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In ECIR '08, 2008.",0,,False
472,"[27] F. Nielsen and R. Nock. Sided and symmetrized Bregman centroids. IEEE Transactions on Information Theory, 55(6), 2009.",0,,False
473,"[28] C. Rao. Information and the accuracy attainable in the estimation of statistical parameters. Bulletin of the Calcutta Mathematical Society, 37, 1945.",0,,False
474,"[29] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, editor, The SMART Retrieval System Experiments in Automatic Document Processing. Prentice Hall, 1971.",0,,False
475,"[30] J. Seo and W. B. Croft. Blog site search using resource selection. In CIKM '08, 2008.",0,,False
476,"[31] J. Seo, W. B. Croft, and D. A. Smith. Online community search using thread structure. In CIKM '09, 2009.",1,ad,True
477,"[32] L. Si and J. Callan. Unified utility maximization framework for resource selection. In CIKM '04, 2004.",0,,False
478,"[33] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: A language model-based search engine for complex queries. In Proc. of the Intl. Conf. on Intelligence Analysis, 2005.",0,,False
479,"[34] R. Veldhuis. The centroid of the symmetrical Kullback-Leibler distance. IEEE Signal Processing Letters, 9(3), 2002.",0,,False
480,"[35] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In SIGIR '01, 2001.",1,ad,True
481,258,0,,False
482,,0,,False

,sentence,label,data,regex
0,Visual Concept-based Selection of Query Expansions for Spoken Content Retrieval,1,Query,True
1,"Stevan Rudinac, Martha Larson, Alan Hanjalic",0,,False
2,"Multimedia Information Retrieval Lab, Delft University of Technology, Delft, The Netherlands",0,,False
3,"{s.rudinac, m.a.larson, a.hanjalic}@tudelft.nl",0,,False
4,ABSTRACT,0,,False
5,"In this paper we present a novel approach to semantic-themebased video retrieval that considers entire videos as retrieval units and exploits automatically detected visual concepts to improve the results of retrieval based on spoken content. We deploy a query prediction method that makes use of a coherence indicator calculated on top returned documents and taking into account the information about visual concepts presence in videos to make a choice between query expansion methods. The main contribution of our approach is in its ability to exploit noisy shot-level concept detection to improve semantic-theme-based video retrieval. Strikingly, improvement is possible using an extremely limited set of concepts. In the experiments performed on TRECVID 2007 and 2008 datasets our approach shows an interesting performance improvement compared to the best performing baseline.",1,TRECVID,True
6,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,0,,False
7,"General Terms: Algorithms, Performance, Experimentation",0,,False
8,"Keywords: Semantic-theme-based video retrieval, video-level retrieval, query expansion, concept-based video indexing, query performance prediction",0,,False
9,1. INTRODUCTION,1,DUC,True
10,"The semantic theme (or subject matter) of a video is encoded in both its speech track and visual channel. This paper presents a novel multimodal retrieval approach aiming at videos covering the semantic theme expressed by the query. The approach improves the results of a speech-based retrieval system by exploiting concepts (e.g. human, car, house, female, children, indoor) detected in the visual channel. In contrast with most previous works on video retrieval, which focus on shot-level retrieval, e.g., [2, 5], our approach is designed to retrieve entire videos. These larger retrieval units are more appropriate than shots in cases where the searcher is looking for informational material or for entertainment, typical for the semantic-theme-based retrieval scenario (e.g., find a video about archaeology or psychology). Recent work on retrieval beyond the shot level includes [1]. Moving from the shot to the video level requires the combination of multiple shot-level concepts into an effective video-level representation. The novel contribution of our work is the successful use of such a video-level representation to combine the output of automatic speech recognition and visual-concept detection, both known to be noisy, and achieve an overall improvement in retrieval of videos on the basis of semantic theme. The key insight motivating our approach is that the presence, frequency and co-occurrence of",0,,False
11,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
12,"visual concepts are potentially powerful indicators of the similarity of videos with respect to their semantic themes. In this initial realization, we focus on exploiting the effects of concept presence and frequency.",0,,False
13,"In order to combine speech-based retrieval with visual concepts, we select a coherence-based query prediction framework [4]. This technique automatically chooses between results lists produced by a range of different query expansions by making use of a coherence-based indicator calculated over top documents of the results list returned by each expansion. Our specific innovation is use of concept-based representations of videos for calculation of the query prediction indicator. In the following, we first introduce our concept-based video representations and our query prediction method. Then we report on results of experiments that confirm the viability of our approach for effectively exploiting visual concepts to refine spoken content retrieval of videos at the semantic theme level.",0,,False
14,2. APPROACH,1,AP,True
15,"The input for creating video-level representations is a vector in which each component represents a single concept. In order to weight each concept, we make use of term frequency (TF) and inverse document frequency (IDF) analogously to their use in text retrieval. However, concept detectors do not output binary concept occurrences, but rather shot-level lists of confidence scores. Each score represents the probability that a particular concept occurred in each shot. Our concept weights are created by accumulating the confidence scores for each concept over all shots in the video and then normalizing with the total shot count.",0,,False
16,"Following the previous feature generation step, we select in the next (feature selection) step the concepts that are potentially the most helpful in discriminating between videos with respect to their semantic themes. Because the output of concept detectors is notoriously noisy (in TRECVID 2009 the best performance failed to exceed 0.25 in the terms of MAP [5]), feature selection is a key aspect of our approach since it introduces a noise control effect. We developed our feature selection method by performing exploratory experimentation. Our experiments make use of TRECVID 2007 dataset and 46 semantic theme labels introduced by the VideoCLEF 2009 (www.multimediaeval.org) benchmark. The labels are manually assigned by professional archivists from the Netherlands Institute for Sound and Vision. We use a set of publicly available concept detection scores, generated for a set of 374 concepts selected from the LSCOM (www.lscom.org) ontology. In order to determine the most representative concepts, we trained classifiers that can identify videos related to each semantic theme and ranked the concepts according to their usefulness to the classifier. A simple voting approach was then applied to merge the lists into a single list that ranked the concept from most to least valuable for semantic discrimination. Our investigation revealed that it",1,TRECVID,True
17,891,0,,False
18,is the most frequently occurring concepts in the videos that best,0,,False
19,support discrimination between videos in terms of semantic class.,0,,False
20,We use this result to select features in the coherence-based query,0,,False
21,prediction step. The fact that the most frequent concepts are most,0,,False
22,discriminative suggests that it is not so much the occurrence of a,0,,False
23,particular visual concept in a video that distinguishes that video's,0,,False
24,"semantic class, but rather its relative frequency.",0,,False
25,Our approach compares results of multiple query expansions,0,,False
26,"and returns, as the final output, the results list with the highest",0,,False
27,coherence score over the Top N videos. The coherence score is,0,,False
28,calculated as:,0,,False
29," ( ) Co(TopN ) ,",0,,False
30," v , v i j{1,...,N}",0,,False
31,ij,0,,False
32,1 N ( N -1),0,,False
33,(1),0,,False
34,2,0,,False
35,"Here  is a function defined on the pair of videos (vi,vj) that is",0,,False
36,equal to 1 if their similarity is higher than the similarity of particu-,0,,False
37,"larly close video pairs from the collection (i.e., closer than TP% of",0,,False
38,"pair-wise similarities, where TP is the threshold defined below).",0,,False
39,The sum is taken over all video pairs in the set of Top N videos.,0,,False
40,As a similarity measure between vectors of concept frequencies,0,,False
41,we used the cosine similarity. Use of alternative similar-,0,,False
42,ity/distance measures such as Kullback-Leibler divergence and,0,,False
43,Euclidean distance yielded similar results.,0,,False
44,3. EXPERIMENTAL SETUP,0,,False
45,"We test our approach on TRECVID 2007 and TRECVID 2008 datasets, using the 46 VideoCLEF 2009 semantic labels as queries. We index the Dutch speech recognition transcripts and the English machine translation and carry out retrieval using the Lemur toolkit. The initial results lists are produced using the original query and three query expansions: 1) Conventional PRF, where the number of feedback documents and terms used for expansion are selected for the optimal performance, 2) WordNet (http://wordnet.princeton.edu/) expansion and 3) Google Sets (http://labs.google.com/sets) expansion, where each query is expanded with a set of up to 15 related items (words or multi-word phrases). For each query, the results list yielding the highest coherence indicator is selected. In the (rare) cases of the same indicator values the priority is given to the baseline or the expansions following the ordering as above. We use the concept detection output provided by [3], as mentioned above. In the experiments on both collections we swept the parameter space for the following parameters: number of most frequent (discriminative) semantic concepts (NC), number of documents from the top of results list (N) and the threshold TP used to calculate the coherence score. We reported the results for the optimal parameter setting.",1,TRECVID,True
46,4. EXPERIMENTAL RESULTS,0,,False
47,The quality of the initial results list for the original query and three expansion methods is reported in terms of Mean Average Precision (MAP) in Table 1.,1,MAP,True
48,Table 1. MAPs of the Baseline and Expansion Methods,1,MAP,True
49,Baseline PRF WordNet Google Sets,0,,False
50,TRECVID 2007 0.326 0.332 0.260,1,TRECVID,True
51,0.120,0,,False
52,TRECVID 2008 0.245 0.265 0.268,1,TRECVID,True
53,0.142,0,,False
54,"Table 2 contains the MAPs of our concept-based selection of query expansion for TRECVID 2007 and TRECVID 2008 datasets. Statistical significance w.r.t. Wilcoxon Signed Rank Test, p < 0.02 is indicated with `^'.",1,MAP,True
55,Table 2. MAPs After Query Expansion Selection (QES),1,MAP,True
56,QES Best Baseline,0,,False
57,TRECVID 2007 0.355^ 0.332,1,TRECVID,True
58,TRECVID 2008 0.296 0.268,1,TRECVID,True
59,"The results confirm the viability of exploiting visual concepts for refining the output of spoken-content-based video retrieval at the level of a semantic theme. Recall that our feature selection approach was optimized using TRECVID 2007 as a development set. The fact that TRECVID 2008 yielded similar performance demonstrates the ability of our feature selection method to generalize to new data. The optimal parameter settings for TRECVID 2007 and TRECVID 2008 are not the same for both datasets, but are in the similar range: N,""5-10, TP"",""80-90%, NC "", 5-10.",1,TRECVID,True
60,5. CONCLUSIONS AND OUTLOOK,0,,False
61,"We have proposed a multimodal approach to semantic-themebased retrieval of entire videos that exploits frequencies of (semantic) concepts detected in a video to enhance the initial retrieval result obtained at the spoken-content level. We have demonstrated that our approach can be effectively used to decide whether the query should be expanded and which of several query expansions to use. Further, we are making use of only a fraction (5-10) of the set of available concepts (374). This result suggests that concept detectors that focus on a very small number of concepts have large potential to be useful for improving the results of semantic-theme-based video retrieval. In our future work we will further study the characteristics of concept detector output that contribute to effective performance of our approach, investigating, for example, whether the relatively larger performance improvement achieved on the TRECVID 2008 set (cf. Table 2) can be attributed to better performing concept detectors. We will also work to take into account concept co-occurrences and to combine proposed concept-based and text-based indicators to further improve query prediction.",1,TRECVID,True
62,6. ACKNOWLEDGMENTS,0,,False
63,The research leading to these results has received funding from the European Commission's 7th Framework Programme (FP7) under grant agreement n° 216444 (NoE PetaMedia).,1,ad,True
64,7. REFERENCES,0,,False
65,"[1] Aly, R., Doherty, A., Hiemstra, D., and Smeaton, A. 2010. Beyond shot retrieval: searching for broadcast news items using language models of concepts. In ECIR, Milton Keynes, UK, 2010.",1,ad,True
66,"[2] Hsu, W. H., Kennedy, L. S., and Chang, S. 2006. Video search reranking via information bottleneck principle. In ACM MM, Santa Barbara, CA, USA, 2006.",1,Video,True
67,"[3] Jiang, Y-G., Yanagawa, A., Chang, S-F., and Ngo, C-W. 2008. CU-VIREO374: Fusing Columbia374 and VIREO374 for Large Scale Semantic Concept Detection. Columbia University ADVENT Technical Report #223-2008-1.",0,,False
68,"[4] Rudinac, S., Larson, M., and Hanjalic, A. 2010. Exploiting result consistency to select query expansions for spoken content retrieval. In ECIR, Milton Keynes, UK, 2010.",0,,False
69,"[5] Snoek, C. G. M., van de Sande, K. E. A., de Rooij, O., et al. 2009. The MediaMill TRECVID 2009 semantic video search engine. In TRECVID Workshop, Gaithersburg, USA, 2009.",1,TRECVID,True
70,892,0,,False
71,,0,,False

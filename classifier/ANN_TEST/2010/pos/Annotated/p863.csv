,sentence,label,data,regex
0,Retrieval System Evaluation: Automatic Evaluation versus Incomplete Judgments,0,,False
1,Claudia Hauff,0,,False
2,"University of Twente Enschede, The Netherlands",0,,False
3,c.hauff@ewi.utwente.nl,0,,False
4,Franciska de Jong,0,,False
5,"University of Twente Enschede, The Netherlands",0,,False
6,f.m.g.dejong@ewi.utwente.nl,0,,False
7,ABSTRACT,0,,False
8,"In information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with reduced (i.e. incomplete) amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The proposed methods in both areas are commonly evaluated by comparing their performance estimates for a set of systems to a ground truth (provided for instance by evaluating the set of systems according to mean average precision). In contrast, in this poster we compare an automatic system evaluation approach directly to two evaluations based on incomplete manual relevance assessments. For the particular case of TREC's Million Query track, we show that the automatic evaluation leads to results which are highly correlated to those achieved by approaches relying on incomplete manual judgments.",1,TREC,True
9,"Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms: Experimentation, Performance Keywords: Automatic System Evaluation",0,,False
10,1. INTRODUCTION,1,DUC,True
11,"In information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with reduced amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The two most important approaches in the first category are the determination of good documents to assess (the MTC approach) [6] and the proposal of alternative pooling methods (the statAP approach) [4]. Both, MTC and statAP, are now accepted system evaluation metrics at TREC1. They stand in contrast to the depth pooling methodology which has until recently been employed at TREC; due to the ever increasing size of test collections and query sets though, pooling the top 100 documents of each retrieval run participating in a benchmark and assessing those documents manually for their relevance, has become infeasible. The earliest method for a fully automatic evaluation was proposed by Soboroff et al.",1,AP,True
12,1http://trec.nist.gov/,1,trec,True
13,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
14,(the RS approach) [7]. It relies on drawing random samples from the pool of top retrieved documents.,0,,False
15,"The quality of statAP, MTC and RS is usually evaluated by comparing the performances of a set of retrieval runs for which sufficient relevance judgments are available according to a standard effectiveness metric (mean average precision) with the estimated system performances. Generally missing though is a direct comparison between statAP /MTC and an automatic method such as RS.",1,AP,True
16,"In recent work [3], we found the commonly reported problem of automatic evaluation approaches (the severe misranking of the very best retrieval runs [5]) not to be inherent to automatic system evaluation methods. The extent of this problem is strongly related to the degree of human intervention in the best retrieval runs: the larger the amount of human intervention, the less able automatic approaches are to identify the best runs correctly.",0,,False
17,"In this poster, we turn to investigating how closely the automatic evaluation of retrieval runs approximates the evaluation with incomplete manual relevance assessments. We perform this analysis in a setting which favors automatic evaluation: TREC's Million Query tracks of 2007 (MQ2007) [2] and 2008 (MQ-2008) [1]. Due to the size of the query sets, creating retrieval runs with a great amount of human intervention is virtually impossible. We thus expect the RS approach to lead to similar estimates of system performances as statAP and MTC respectively. If this would indeed be the case, it would bring into question the need for manual assessments in this type of setting.",1,TREC,True
18,2. EXPERIMENTS,0,,False
19,"For our experiments, we relied on the twenty-nine retrieval runs submitted to MQ-2007 and the twenty-four2 runs submitted to MQ-2008. Both sets of retrieval runs as well as their retrieval effectiveness scores according to statAP and MTC are available from the TREC website. Specifically, for MQ-2007, TREC provides the statAP measures3, while for MQ-2008 both, MTC and statAP, are provided. Of the 10000 queries that were released for each year, 1153 (MQ2007) and 564 (MQ-2008) queries respectively have valid statAP measurements; 784 (MQ-2008) queries have valid MTC measurements. These are the queries we also rely on in the RS approach.",1,MQ,True
20,"2In total, twenty-five runs exist, though one is not accessible from the TREC website and thus had to be ignored. 3The MTC measures are not accessible from the TREC website.",1,TREC,True
21,863,0,,False
22,Pool Depth p Avg. Sampled Documents,0,,False
23,MQ-2007 statAP,1,MQ,True
24,10,0,,False
25,4.6,0,,False
26,50,0,,False
27,22.2,0,,False
28,100,0,,False
29,43.0,0,,False
30,250,0,,False
31,102.6,0,,False
32,MQ-2008 statAP,1,MQ,True
33,10,0,,False
34,6.0,0,,False
35,50,0,,False
36,28.8,0,,False
37,100,0,,False
38,55.7,0,,False
39,250,0,,False
40,132.4,0,,False
41,MQ-2008 MTC,1,MQ,True
42,10,0,,False
43,6.1,0,,False
44,50,0,,False
45,29.4,0,,False
46,100,0,,False
47,56.8,0,,False
48,250,0,,False
49,135.0,0,,False
50,Kendall's Tau,0,,False
51,0.803 0.783 0.754 0.719,0,,False
52,0.768 0.812 0.833 0.841,0,,False
53,0.722 0.759 0.773 0.780,0,,False
54,Table 1: Kendall's Tau rank correlation coefficient between the automatic RS approach and statAP/MTC respectively. All correlations are significant (p < 0.01). Column 2 contains the average number of sampled documents from the pool.,1,AP,True
55,"For the automatic evaluation, we implemented the random sampling approach [7]: first, the top p retrieved documents of all retrieval runs for a particular query are pooled together such that a document that is retrieved by x runs, appears x times in the pool. Then, a number m of documents are drawn at random from the pool; those are now considered to be the pseudo relevant documents. This process is performed for each query and the subsequent evaluation of each system is performed with pseudo relevance judgments instead of relevance judgments. Due to the randomness of the sampling, we performed 20 trials per query and averaged the pseudo relevance based system performance. We fixed the number m of documents to sample 5% of the number of unique documents in the pool and evaluated pool depths of p ,"" {10, 50, 100, 250}.""",1,ad,True
56,"In Table 1 (column 3) we report the rank correlation coefficient Kendall's Tau ( ) between the performance scores estimated by the automatic RS approach and the performance scores estimated by statAP /MTC which exploit manual relevance assessments. In the ideal case,  ,"" 1.0, that is, RS leads to the same rank estimate of system performances as statAP /MTC. It is apparent, that although the correlations are not perfect, the correlation coefficients are consistently high; in the worst instance the correlation reaches  "", 0.72 for MQ-2007 statAP and a pool depth of p , 250; at best the correlation reaches  , 0.84 for MQ-2008 statAP and p , 250.",1,AP,True
57,Figures 1 and 2 show scatter plots of MQ-2007 statAP scores versus RS scores and of MQ-2008 MTC scores versus RS scores respectively. It is evident that the best retrieval runs as identified by statAP /MTC are also identified correctly by the automatic RS approach.,1,MQ,True
58,3. DISCUSSION AND CONCLUSION,0,,False
59,"In this poster, we investigated the ability of an automatic system evaluation approach (RS [7]) to approximate the system performance estimates as derived by two evaluation methods that rely on manually derived incomplete relevance judgments: statAP and MTC. Experiments on TREC's Mil-",1,AP,True
60,random sampling,0,,False
61,0.18 0.16 0.14 0.12,0,,False
62,0.1 0.08 0.06 0.04 0.02,0,,False
63,0,0,,False
64,",0.803",0,,False
65,0.05,0,,False
66,0.1,0,,False
67,0.15,0,,False
68,0.2,0,,False
69,0.25,0,,False
70,0.3,0,,False
71,statAP,1,AP,True
72,"Figure 1: MQ-2007 statAP scores (x-axis) versus RS scores (y-axis) for a pool depth of p , 10.",1,MQ,True
73,random sampling,0,,False
74,0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01,0,,False
75,0 0,0,,False
76,",0.780",0,,False
77,0.02,0,,False
78,0.04,0,,False
79,0.06,0,,False
80,0.08,0,,False
81,0.1,0,,False
82,MTC,0,,False
83,"Figure 2: MQ-2008 MTC scores (x-axis) versus RS scores (y-axis) for a pool depth of p , 250.",1,MQ,True
84,"lion Query tracks showed that RS is highly correlated to statAP and MTC, an outcome which implies that retrieval runs, which are automatic in nature, can be evaluated by an automatic approach such as RS which requires no manual assessments at all.",1,Query,True
85,One direction of future work will be the adaptation of RS to further improve the method's correlation with statAP and MTC by for instance taking advantage of the relationship between queries of a query set (as is possible for larger sets of queries) in contrast to the current approach where each query is viewed in isolation.,1,ad,True
86,4. REFERENCES,0,,False
87,"[1] J. Allan, J. A. Aslam, V. Pavlu, E. Kanoulas, and B. Carterette. Million Query Track 2008 Overview. In TREC 2008, 2008.",1,Query,True
88,"[2] J. Allan, B. Carterette, B. Dachev, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million Query Track 2007 Overview. In TREC 2007, 2007.",1,Query,True
89,"[3] C. Hauff, D. Hiemstra, L. Azzopardi, and F. de Jong. A Case for Automatic System Evaluation. In ECIR '10, pages 153­165, 2010.",0,,False
90,"[4] J. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR '06, pages 541­548, 2006.",0,,False
91,"[5] J. A. Aslam and R. Savell. On the effectiveness of evaluating retrieval systems in the absence of relevance judgments. In SIGIR '03, pages 361­362, 2003.",0,,False
92,"[6] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR '06, pages 268­275, 2006.",0,,False
93,"[7] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In SIGIR '01, pages 66­73, 2001.",0,,False
94,864,0,,False
95,,0,,False

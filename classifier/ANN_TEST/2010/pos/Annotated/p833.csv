,sentence,label,data,regex
0,Probabilistic Latent Maximal Marginal Relevance,0,,False
1,Shengbo Guo,0,,False
2,"ANU & NICTA Canberra, Australia",0,,False
3,shengbo.guo@nicta.com.au,0,,False
4,Scott Sanner,0,,False
5,"NICTA & ANU Canberra, Australia",0,,False
6,scott.sanner@nicta.com.au,0,,False
7,ABSTRACT,0,,False
8,"Diversity has been heavily motivated in the information retrieval literature as an objective criterion for result sets in search and recommender systems. Perhaps one of the most well-known and most used algorithms for result set diversification is that of Maximal Marginal Relevance (MMR). In this paper, we show that while MMR is somewhat adhoc and motivated from a purely pragmatic perspective, we can derive a more principled variant via probabilistic inference in a latent variable graphical model. This novel derivation presents a formal probabilistic latent view of MMR (PLMMR) that (a) removes the need to manually balance relevance and diversity parameters, (b) shows that specific definitions of relevance and diversity metrics appropriate to MMR emerge naturally, and (c) formally derives variants of latent semantic indexing (LSI) similarity metrics for use in PLMMR. Empirically, PLMMR outperforms MMR with standard term frequency based similarity and diversity metrics since PLMMR maximizes latent diversity in the results.",1,adhoc,True
9,Categories and Subject Descriptors,0,,False
10,H.3.3 [Information Search and Retrieval]: Retrieval Models,0,,False
11,General Terms,0,,False
12,Algorithms,0,,False
13,Keywords,0,,False
14,"diversity, graphical models, maximal marginal relevance",0,,False
15,1. INTRODUCTION,1,DUC,True
16,Maximal marginal relevance (MMR) [2] is perhaps one of the most popular methods for balancing relevance and diversity in set-based information retrieval and has been cited over 530 times1 since its publication in 1998.,0,,False
17,"The basic idea of MMR is straightforward: suppose we have a set of items D and we want to recommend a small subset Sk  D (where |Sk| , k and k  |D|) relevant to a given query q. MMR proposes to build Sk in a greedy",0,,False
18,1According to Google Scholar.,0,,False
19,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
20,"manner by selecting sj given Sj-1 ,"" {s1, . . . , sj-1} (where Sj "", Sj-1  {sj }) according to the following criteria",0,,False
21,sj,0,,False
22,","" arg max [(Sim1(sj, q)) - (1""",0,,False
23,sj D\Sj-1,0,,False
24,-,0,,False
25,) max,0,,False
26,si Sj-1,0,,False
27,"Sim2(sj, si)]",0,,False
28,(1),0,,False
29,"where Sim1(·, ·) measures the relevance between an item and a query, Sim2(·, ·) measures the similarity between two items, and the manually tuned   [0, 1] trades off relevance and similarity. In the case of s1, the second term disappears.",1,ad,True
30,"While MMR is a popular algorithm, it was specified in a",0,,False
31,rather ad-hoc manner and good performance typically relies,1,ad-hoc,True
32,"on careful tuning of the  parameter. Furthermore, MMR is",0,,False
33,"agnostic to the specific similarity metrics used, which indeed",0,,False
34,"allows for flexibility, but makes no indication as to the choice",0,,False
35,of similarity metrics for Sim1 and Sim2 that are compatible with each other and also appropriate for good performance.,0,,False
36,"In the next section, we address these concerns by taking a",1,ad,True
37,more principled approach to set-based information retrieval,0,,False
38,via maximum a posteriori probabilistic inference in a latent,0,,False
39,variable graphical model of marginal relevance (PLMMR).,1,LM,True
40,"As an elegant and novel contribution, we note that natural",0,,False
41,relevance and diversity metrics emerge from this derivation,0,,False
42,(with no analogous manually tuned  parameter) and that,0,,False
43,these metrics also formally motivate variants of similarity,0,,False
44,metrics used in latent semantic indexing (LSI) [3].,0,,False
45,2. PROBABILISTIC LATENT MMR,0,,False
46,Figure 1: Graphical model used in PLMMR.,1,LM,True
47,"We begin our discussion of PLMMR by introducing a graphical model of (marginal) relevance in Figure 1. Shaded nodes represent observed variables while unshaded nodes are latent; we do not distinguish between variables and their assignments. The observed variables are the vector of query terms q and the selected items s1  D and s2  D. For the latent variables, let T be a discrete topic set; variables t1  T and t2  T respectively represent topics for s1 and",1,LM,True
48,833,0,,False
49,"s2 and t  T represents a topic for query q. r1  {0, 1} and",0,,False
50,"r2  {0, 1} are variables that indicate whether the respective",0,,False
51,selected items s1 and s2 are relevant (1) or not (0). The conditional probability tables (CPTs) in this discrete,0,,False
52,"directed graphical model are defined as follows. P (t1|s1) and P (t2|s2) represent topic models of the items and P (t|q) represents a topic model of the query. There are a variety of ways to learn these topic CPTs based on the nature of the items and query; for an item set D consisting of text documents and a query that can be treated as a text document, a natural probabilistic model for P (ti|si) and P (t|q) can be derived from Latent Dirichlet Allocation (LDA) [1]. Finally, the CPTs for relevance ri have a very natural definition:",0,,False
53,"P (r1|t, t1) ,",0,,False
54,1 0,0,,False
55,"if t1 , t if t1 , t",0,,False
56,"P (r2|t, r1 ,"" 0, t1, t2) "",",0,,False
57,1 0,0,,False
58,"if (t2 , t1)  (t2 , t) if (t2 , t1)  (t2 , t)",0,,False
59,"Simply, s1 is relevant if its topic t1 , t (the query topic).",0,,False
60,s2 is relevant with the same condition and the addition that,1,ad,True
61,"if s1 was irrelevant (r1 ,"" 0), then topic t2 for s2 should also""",0,,False
62,"not match t1. Following the click-chain model [4], we assume",0,,False
63,"the user only examines s2 if s1 was irrelevant (r1 , 0).",0,,False
64,Let us assume that like MMR we use a greedy item set se-,0,,False
65,"lection algorithm given S1 ,"" {s1},""",0,,False
66,and we have already we want to select s2,1,ad,True
67,"selected s1 , s1. Now in order to maximize",0,,False
68,"its marginal relevance w.r.t. q given S1, formally defined as",0,,False
69,"MR(S1, s2, q) and derived as a query in the graphical model:",0,,False
70,"s2 ,"" arg max MR(S1, s2, q) "","" arg max P (r2|s1, s2, q)""",0,,False
71,s2 D\S1,0,,False
72,s2D\{s1 },0,,False
73,", arg max",0,,False
74,"P (r2|r1 ,"" 0, t1, t2, t)P (t1|s1)""",0,,False
75,"s2 D\{s1 } t1,t2,t",0,,False
76,"P (r1 ,"" 0|t1, t)P (t2|s2)P (t|q)""",0,,False
77,", arg max",0,,False
78,s2 D\{s1 },0,,False
79,"P (t|q)P (t2 , t|s2) -",0,,False
80,t,0,,False
81,relevance,0,,False
82,"P (t|q)P (t1 , t|s1)P (t2 , t|s2)",0,,False
83,(2),0,,False
84,t,0,,False
85,diversity,0,,False
86,The basic insight leading to this fascinating result is the,1,ad,True
87,exploitation of the indicator structure of the relevance vari-,0,,False
88,"ables r1 and r2 to make convenient variable substitutions. We note that in this special case for MR(S1, s2, q), a very",0,,False
89,"natural mapping to the MMR algorithm in (1) when  ,"" 0.5 has emerged automatically from the derivation that maximized MR. This derivation automatically balances relevance and diversity without an analogous  and it suggests very specific (and different) relevance and diversity metrics, both effectively variants of similarity metrics used in latent semantic indexing (LSI) [3]. To make this clear, we examine""",0,,False
90,"tlqehuteerTryeleqavananndcdeTitm2emebtersic2reSwsipimtehP1cLtviMevcM etRotrogpeivilceemnpebrnoytbsPaTbLiMliitM,y RPve(wctthoe,rrsei|fwqoer) and T2i , P (t2 ,"" i|s2) and using ·, · for the inner product:""",0,,False
91,"SimP1 LMMR(q, s2) , P (t|q)P (t2 , t|s2) ,"" T, T2 .""",1,LM,True
92,t,0,,False
93,"A similar analysis gives diversity metric SimP2 LMMR(s1, s2), yielding a variant LSI similarity metric reweighted by the query topic probability P (t|q). This points out the important correction to MMR that item set diversity should be",1,LM,True
94,Table 1: Weighted subtopic loss (WSL) of three methods using all words and first 10 words. Standard error estimates are shown for PLMMR-LDA.,1,LM,True
95,Method MMR-TF MMR-TFIDF PLMMR-LDA,1,LM,True
96,WSL (first 10 words) 0.555 0.549,0,,False
97,0.458 ± 0.0058,0,,False
98,WSL (all words) 0.534 0.493,0,,False
99,0.468 ± 0.0019,0,,False
100,"query-relevant! Given these definitions of SimP1 LMMR and SimP2 LMMR, we can now substitute these into the MMR algorithm defined in (1) to arrive at a definition of PLMMR.",1,LM,True
101,3. EXPERIMENTAL COMPARISON,0,,False
102,"We report experiments on a subset of TREC 6-8 data focusing on diversity. We follow the same experimental setup as [6] who measure the weighted subtopic loss (WSL) of recommended item sets where in brief, WSL gives higher penalty for not covering popular subtopics. We do not compare directly to [6] as their method was supervised while MMR and PLMMR are inherently unsupervised.",1,TREC,True
103,"Standard query and item similarity metrics used in MMR applied to text data include the cosine of the term frequency (TF) and TF inverse document frequency (TFIDF) vector space models [5]. We denote these variants of MMR as MMR-TF and MMR-TFIDF. PLMMR specifically suggests the use of LSI-based similarity metrics defined in the last section; thus, we use LDA to derive these models, referring to the resulting algorithm as PLMMR-LDA. LDA was trained with  ,"" 2.0,  "","" 0.5, |T | "", 15; we note the results were not highly sensitive to these parameter choices.",1,LM,True
104,"Average WSL scores are shown in Table 1 on the 17 queries examined by [6]. We use both full documents and also just the first 10 words of each document. For both MMR algorithms, the best performing  ,"" 0.5 is shown. We note that due to the power of the latent topic model and derived similarity metrics, PLMMR-LDA is able to perform better than MMR with standard TF and TFIDF metrics and without a  parameter to be tuned. In addition, PLMMRLDA works very well with short documents since intrinsic document and query similarities are automatically derived from the latent PLMMR relevance and diversity metrics.""",1,LM,True
105,4. REFERENCES,0,,False
106,"[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 3:993­1022, 2003.",0,,False
107,"[2] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR, 335­336. 1998.",0,,False
108,"[3] S. Deerwester, S. T. Dumaisand, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. JASIS, 41:391­407, 1990.",0,,False
109,"[4] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, Y.-M. Wang, and C. Faloutsos. Click chain model in web search. In WWW-09, Madrid, Spain, 2009. ACM.",1,ad,True
110,"[5] G. Salton and M. McGill. Introduction to modern information retrieval. McGraw-Hill, 1983.",0,,False
111,"[6] Y. Yue and T. Joachims. Predicting diverse subsets using structural SVMs. In ICML, 1224­1231, 2008.",0,,False
112,834,0,,False
113,,0,,False

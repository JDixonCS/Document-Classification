,sentence,label,data,regex
0,Entity Summarization of News Articles,0,,False
1,Gianluca Demartini,0,,False
2,L3S Research Center Appelstrasse 9a,0,,False
3,"30167 Hannover, Germany",0,,False
4,demartini@L3S.de,0,,False
5,Malik Muhammad Saad Missen,1,ad,True
6,"IRIT Toulouse, France",0,,False
7,missen@irit.fr,0,,False
8,"Roi Blanco, Hugo Zaragoza",0,,False
9,Yahoo! Research Diagonal 177,1,Yahoo,True
10,"08018 Barcelona, Spain",0,,False
11,"{roi,hugoz}@yahooinc.com",0,,False
12,ABSTRACT,0,,False
13,"In this paper we study the problem of entity retrieval for news applications and the importance of the news trail history (i.e. past related articles) to determine the relevant entities in current articles. We construct a novel entitylabeled corpus with temporal information out of the TREC 2004 Novelty collection. We develop and evaluate several features, and show that an article's history can be exploited to improve its summarization.",1,TREC,True
14,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,0,,False
15,"General Terms: Algorithms, Measurement, Experimentation",0,,False
16,"Keywords: Entity Summarization, Time-aware Search",0,,False
17,1. INTRODUCTION,1,DUC,True
18,"Entity retrieval is becoming a major area of interest in IR research and it is quickly being adopted in commercial applications. One of the promising areas applying entity retrieval models in the commercial world is news search. News retrieval has also been the focus of much attention in the IR research community, but to our knowledge there have been no entity ranking tasks defined for news.",1,ad,True
19,"Consider the following user scenario: a user types a query (or topic) into a news search engine and obtains a list of relevant results, ordered by time. Furthermore, the user subscribes to this query so in the future she will continue to receive the latest news on this query. We are interested in entity ranking tasks related to this user scenario. For instance, standard entity ranking could be used to show the most interesting entities for the query. In practice, the temporal dimension is not needed here. However, if the user is observing a current document, we may want to show the most relevant entities of the document for her query taking into account features extracted from previous documents. This prompts the Entity Summarization (ES) task definition: given a query, a relevant document and possibly a set of previous related documents (the history of the document),",0,,False
20,Work performed while intern at Yahoo! Research. This work is partially supported by the EU Large Scale Integrated Project LivingKnowledge (contract no. 231126).,1,Yahoo,True
21,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
22,"retrieve a set of entities that best summarize the document. This is a newly defined task that can be useful, for example, in vertical search for presenting the user more than just a ranked list of documents.",0,,False
23,2. TIME-AWARE ENTITY SUMMARIZATION,0,,False
24,"More formally, we define a ""news thread"" relevant to a query as the list of relevant documents D ,"" [d1 . . . dn] chronologically ordered. Then, given a document di we define its history as the list of relevant documents H "","" [d1 . . . di-1] chronologically ordered pre-dating the document di. Given an entity e, we note as de,1 the first document in which the entity occurred in the news thread. Note that such a document is not necessarily the first document in D as entities may appear only in subsequent documents. Moreover, we note as de,-1 as the last document in H which contains e.""",1,ad,True
25,"For addressing this task, we propose features both from the local document as well as from H. The first feature we consider is the frequency of an entity e in a document d, noted F (e, d). In the following we will use this feature as our baseline. It is possible to consider if an entity appears as a subject of a sentence as this is generally the person or thing carrying out an action (after running a dependency parsing over the sentence collection). Hence, we define the Fsubj(e, d) as the number of times an entity e appears as subject of a sentence in the document d.",1,ad,True
26,"Additionally, we propose two position-based features that take into account where in document d an entity e appears. Let F irstSenLen(e, d) be the length of the first sentence where e appears in document d and F irstSenP os(e, d) be the position of the first sentence where e appears in d (e.g, the fourth sentence in the document).",0,,False
27,"We now introduce a number of features that take into consideration the document history H. Let F (e, H) be the frequency (i.e., the number of times it appears) of the entity e in the history H. Instead of counting each entity occurrence a simpler variation considers the number of documents in which the entity e has appeared so far. We thus define DF (e, H) as the document frequency of e in H.",1,ad,True
28,"Furthermore, it is possible to examine single documents from the past to extract more features; we then define F (e, de,-1) as the frequency of entity e in the previous document where the entity appeared and F (e, de,1) as the frequency of entity e in the first document where the entity appeared.",0,,False
29,"We can also compute CoOcc(e, H), the number of other entities with which the entity co-occurred in a sentence in the set of past documents H.",0,,False
30,795,0,,False
31,3. EXPERIMENTAL EVALUATION,0,,False
32,"We selected the 25 event topics of the latest TREC Novelty collection (2004) consisting of news articles. We annotated the documents associated with those topics using state of the art NLP tools1 in order to extract entities of type person, location, organization, and product based on WSJ annotations. The system detected 7481 entity occurrences in the collection: 26% persons, 10% locations, 57% organizations, and 7% products. Human judges assessed the relevance of the entities in each document with respect to the topic grading each entity on the 3-points scale: Relevant, Related, Not Relevant. An additional category was used, i.e., 'Not an entity', to mark entities which had been wrongly annotated by the NLP tool. A total of 21213 entitydocument-topic judgements were obtained in the collection2.",1,TREC,True
33,"We compare the effectiveness of different features and some feature combinations using several performance metrics. We report values for Precision@3 (P@3), Precision@5 (P@5), and Mean Average Precision (MAP) considering Related entities as non-relevant and using tie-aware metrics [2].",1,MAP,True
34,Feature,0,,False
35,P@3 P@5 MAP,1,MAP,True
36,All Ties,0,,False
37,.34,0,,False
38,.34,0,,False
39,.42,0,,False
40,Individual Features (Local and History),0,,False
41,"F(e,d)",0,,False
42,.65,0,,False
43,.56,0,,False
44,.60,0,,False
45,FirstSenLen .37,0,,False
46,.36,0,,False
47,.45,0,,False
48,FirstSenPos .31,0,,False
49,.31,0,,False
50,.43,0,,False
51,Fsubj,0,,False
52,.49,0,,False
53,.44,0,,False
54,.50,0,,False
55,"F (e, de,1)",0,,False
56,.58,0,,False
57,"F (e, de,-1) .64",0,,False
58,"DF (e, H) .63",0,,False
59,"F (e, H)",0,,False
60,.66,0,,False
61,"CoOcc(e, H) .62",0,,False
62,.53,0,,False
63,.56 .57 .59,0,,False
64,.57,0,,False
65,.56 .62 .65 .66 .65,0,,False
66,"Features combined with F(e,d)",0,,False
67,F irstSenLen .65 F irstSenP os .67,0,,False
68,.57 .58,0,,False
69,.62 .62,0,,False
70,"Fsubj F (e, de,1) F (e, de,-1) F (e, H)",0,,False
71,.65,0,,False
72,.65 .68 .70,0,,False
73,.56 .57 .60 .62,0,,False
74,.61 .61 .65 .68,0,,False
75,"CoOcc(e, H) .68 .61 .67",0,,False
76,"DF (e, H)",0,,False
77,.69 .61 .68,0,,False
78,"Table 1: Effectiveness of individual features and of features when combined with F (e, d). Bold values indicate the best performing runs. * (**) indicates statistical significance w.r.t. F(e,d) and () w.r.t. F(e,H) with paired t-test p<0.05(0.01).",0,,False
79,"Individual Features. The upper part of Table 1 shows effectiveness values obtained when ranking entities in a document according to individual features. For comparison, a feature that assigns the same value to each entity would obtain a MAP value of 0.42. The feature F (e, d) obtains the best MAP value (0.60) among features from the local article. In general, history features perform better than local features and the highest performance is obtained by ranking entities according to their frequency in the past documents. Interestingly, when identifying relevant entities for a docu-",1,MAP,True
80,1http://sourceforge.net/projects/supersensetag/ 2The evaluation collection we have created is available for download at: http://www.l3s.de/~demartini/deert/,1,ad,True
81,"ment, the frequency of the entity in the previous document in the story F (e, de,-1) is a better evidence than the frequency in the current document. This may be an indication of how people read news: some entities become relevant to readers after repeated occurrences. If an entity appears also in the previous documents it is more likely to be relevant.",1,ad,True
82,"Given these results we conclude that the evidence from the past is very important for ranking entities appearing in a document. We expect effectiveness of methods that exploit the past to improve as the size of H grows. That is, the more history is available the better we can rank entities for the current news. For |H|  20 the average effectiveness of F (e, H) grows together with |H| up to values of 0.7 MAP.",1,MAP,True
83,"Combined Features. So far we have presented different features for ranking entities that appear in a document. Combining them in an appropriate manner yields a better ranking of entities; however, because the probability distribution of relevance given a feature is different among features we need a way for combining them. The following experiments rank entities in a document according to a score obtained after combining several features together. We consider linear combination of features (transformed with a function as explained in [1]).",0,,False
84,Let the score for an entity e and a vector f of n features,0,,False
85,"be score(e, f ) ,",0,,False
86,"n i,1",0,,False
87,"wig(fi,",0,,False
88,i),0,,False
89,",",0,,False
90,where,0,,False
91,wi,0,,False
92,is,0,,False
93,the,0,,False
94,weight,0,,False
95,of each feature and g is a transformation function for the,0,,False
96,feature fi using a given parameter i. In this paper we,0,,False
97,"employ a transformation function of the form: g(x, ) ,",0,,False
98,x x+,0,,False
99,"as suggested in [1], where x is the feature to transform",0,,False
100,and  is a parameter. We also tried a linear transformation,0,,False
101,but it did not perform as well (more complex non-linear,0,,False
102,transformations could also be explored). In order to combine,0,,False
103,features we then need to find a parameter i for the function,0,,False
104,g and a weight wi for each feature fi. We tested two and,0,,False
105,"three features combinations, where the variables i, and the",0,,False
106,combination weights wi have been tuned with 2-fold cross,0,,False
107,validation of 25 topics training to optimize MAP. In order to,1,MAP,True
108,find the best values we used a optimization algorithm that,0,,False
109,performs a greedy search over the parameter space [3].,0,,False
110,"Combining F (e, d) with another feature is able to outper-",0,,False
111,form the baseline for some range of the weight w that can be,0,,False
112,learned on a training set. The best effectiveness is obtained,0,,False
113,"when combining F (e, d) and F (e, H) obtaining an improve-",0,,False
114,"ment of 13% in terms of average precision. Other features,",0,,False
115,"when combined with the baseline, also obtain high improve-",0,,False
116,"ments performing as good as the combination with F (e, H)",0,,False
117,"(CoOcc(e, H) having 12% and DF (e, H) having 13% im-",0,,False
118,provement in terms of MAP).,1,MAP,True
119,"As future work, besides testing our features on different",0,,False
120,"time-aware document collections, we aim at adopting ma-",1,ad,True
121,chine learning techniques to combine the proposed features.,0,,False
122,4. REFERENCES,0,,False
123,"[1] N. Craswell, S. Robertson, H. Zaragoza, and M. Taylor. Relevance weighting for query independent evidence. In SIGIR '05, USA. ACM.",0,,False
124,"[2] F. McSherry and M. Najork. Computing information retrieval performance measures efficiently in the presence of tied scores. In ECIR, 2008.",0,,False
125,"[3] S. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval, (4), 2009.",0,,False
126,796,0,,False
127,,0,,False

,sentence,label,data,regex
0,Mining the Blogosphere for Top News Stories Identification,0,,False
1,Yeha Lee Hun-young Jung Woosang Song Jong-Hyeok Lee,0,,False
2,Division of Electrical and Computer Engineering Pohang University of Science and Technology,0,,False
3,"Pohang, Gyungbuk, Republic of Korea",0,,False
4,"{sion, blesshy, woosang, jhlee}@postech.ac.kr",0,,False
5,ABSTRACT,0,,False
6,"The analysis of query logs from blog search engines show that news-related queries occupy a significant portion of the logs. This raises a interesting research question on whether the blogosphere can be used to identify important news stories. In this paper, we present novel approaches to identify important news story headlines from the blogosphere for a given day. The proposed system consists of two components based on the language model framework, the query likelihood and the news headline prior. For the query likelihood, we propose several approaches to estimate the query language model and the news headline language model. We also suggest several criteria to evaluate the news headline prior that is the prior belief about the importance or newsworthiness of the news headline for a given day. Experimental results show that our system significantly outperforms a baseline system. Specifically, the proposed approach gives 2.62% and 10.19% further increases in MAP and P@5 over the best performing result of the TREC'09 Top Stories Identification Task.",1,blog,True
7,Categories and Subject Descriptors,0,,False
8,H.3.3 [Information Search and Retrieval]: Information Search and Retrieval ­ Retrieval models,0,,False
9,General Terms,0,,False
10,"Algorithms, Experimentation, Performance",0,,False
11,Keywords,0,,False
12,"Blog Retrieval, Blogosphere, Top News Stories Identification",0,,False
13,1. INTRODUCTION,1,DUC,True
14,"A blog, ""web log"", is a special type of website in which users (individuals or groups) express their opinions or thoughts on several subjects. Blog posts consist of a wide variety of topics. As the number of blog users increase, the popularity and the importance of blogs are growing, and several",1,blog,True
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
16,commercial search engines such as Google1 and Technorati2 have provided blog search services.,1,blog,True
17,"Users' information needs for blog search differ from those for general web search. A large portion of the query logs from blog search engines are news-related queries [21, 22]. In other words, many users find information about news stories in the blogosphere. This implies that the blogosphere may be helpful when locating news stories.",1,blog,True
18,"A large number of news stories from various news channels are generated and updated day after day. However, a relatively few news among huge number of them receive attention from users. Therefore, it is one of the most important issues to evaluate the importance of news stories and rank them.",0,,False
19,"We investigate how to take advantage of the blogosphere for identifying top news stories. To this end, given a certain day, we retrieve and rank news headlines according to their importance or newsworthiness, using the blogosphere. Furthermore, this task is worthwhile in that it identifies the top news stories from blog users' point of view, instead of the news providers. The task is also called Top Stories Identification Task (TSIT) which was first introduced at the TREC 2009 Blog Track [21].",1,ad,True
20,"TSIT is a new pilot task that aims to ""address the news dimension in the blogosphere"" [21]. The task uses a date (day) as a query. For a date query, the system for the task ranks the news headlines in the order of their importance. Furthermore, for each news headline, the task requires a certain number of blog posts that capture diverse aspects relevant to the news headline.",1,ad,True
21,"TSIT has some characteristics that distinguish it from previous news-related studies such as Topic Detection and Tracking (TDT). First, the data given for TSIT contains only news headlines but no news contents. Therefore, the system for the task should rank news headlines utilizing the blogosphere (i.e. Blog08 corpus) instead of the contents of news articles. Second, unlike the corpus of news stories, blog posts are generally neither well-written articles nor topically coherent. They also include a lot of non-topical contents such as spam blogs and blog comment spam that advertise commercial products and services [16], making the task difficult.",1,Track,True
22,"In this paper, we present novel approaches to identify the top news stories in the blogosphere. The proposed approaches are based on the language model framework, which is widely used in information retrieval tasks. We propose a",1,blog,True
23,1http://blogsearch.google.com/ 2http://www.technorati.com/,1,blog,True
24,395,0,,False
25,"series of approaches to estimate a query language model and a news headline language model based on the blogosphere, and to rank the news headlines according to the distance between two language models. We also suggest several criteria to evaluate the prior probability that a news headline will be a top news story for a given day, and verify that these criteria are useful to identify top news stories. The experimental results show that our approach significantly improves the best performance submitted in the TREC 2009 Top Stories Identification Task.",1,ad,True
26,"The rest of the paper is organized as follows. In section 2, we briefly survey related work on new event detection. In section 3, we address the framework of our system, and propose several approaches to identify the top news headlines. In section 4, we conduct several experiments to evaluate the performances of our approach. Finally, we conclude the paper and discuss future work in section 5.",1,ad,True
27,2. RELATED WORK,0,,False
28,"As a new pilot task, TSIT aims to identify top news stories in the blogosphere, and to provide a ranked list of news headlines. There are few researches for identifying and ranking top news stories in the blogosphere. One of the research directions closely related to TSIT may be the New Event Detection.",1,blog,True
29,"New Event Detection, one of the five tasks in TDT, aims to detect whether a given news story is concerned with already known events to a system or not. For the event detection problem, many approaches have been based on clustering or classification to estimate the similarity between the events and documents (e.g. the news stories); these approaches differ in the ways by which they evaluate the similarity [3, 5, 17, 25, 28, 29]. All of them compare each document with existing events. If the similarity between the document and the events is lower than some predefined criteria, the document is considered to address a new event. Otherwise, the document is assigned to the event to which it is most similar.",1,TD,True
30,"Various features have been proposed, including timeline analysis, burstiness and named entities. Chen et al proposed an aging theory to capture the life cycle of a news event, and improved the performance for event detection [7]. Chen et al used an aging theory and a sentence modeling to extract hot topics from news documents [8]. They analyzed the timeline to identify the key terms. The burstiness of terms was used by many researchers for the event detection [9, 12, 15, 24]. Kleinberg proposed an approach to identify the bursty features for the event detection from e-mail streams [15]. They used the infinite-state automaton to model the stream. He et al identified bursts of (a)periodic features using a Gaussian distribution, and then used them to detect (a)periodic events [12]. Kumaran and Allan used named entities for the event detection [17]. They showed that the usefulness of named entities can change according to certain situations. Kuo et al classified terms within news stories based on named entity type and parts-of-speech tags, and assigned a different weight to each term according to the type and class of news story [28].",0,,False
31,"The main difference between previous work for the event detection and our approach stems from the difference in source data for identifying events or news stories. In contrast with previous work, we identify the top news headlines",1,ad,True
32,"using only the unorganized blogosphere, not the well-defined contents of news articles.",1,blog,True
33,3. TOP STORIES RANKING MODEL,0,,False
34,"To identify the top news stories, we rank them according to their importance or newsworthiness on a specific day. The newsworthiness of a news story can be decided by several criteria3 as follows:",0,,False
35,· Timing News stories that is happening now are often more newsworthy than those that happened a week ago.,0,,False
36,· Significance The number of people involved in a news story is important.,0,,False
37,· Proximity News stories that occur near us are more important than distant ones.,0,,False
38,· Prominence News stories about famous people are more newsworthy than stories about ordinary people.,0,,False
39,· Human-Interest Human-interest stories are generally soft news. They appeal to emotions.,0,,False
40,"We assume that a news story mentioned in more blog posts or comments is more important or newsworthy on a specific day, because a top news story satisfying the above criteria may receive attention from many blog users, who express their thoughts or opinions about the news story in their blogs.",1,blog,True
41,"To measure the importance of a news story using the blogosphere, we adopt the language model framework, which is widely used in information retrieval tasks. Motivated by our assumption, we evaluate the importance of a news headline using the probability that blog posts published on a query day generate the headline. Let H be a news headline and let Qd and Qp be a given (date) query and a set of blog posts published on the query day Qd, respectively.",1,blog,True
42,"Score(Qd, H)  P (H|Qp)  P (Qp|H) P (H) (1)",0,,False
43,Importance score of a news headline,1,ad,True
44,Query Headline Likelihood Prior,1,Query,True
45,3.1 The Query Likelihood,1,Query,True
46,"In the language model framework, the query likelihood means the probability that a document generates a given query. TSIT uses a date (day) as a query. Therefore, we regard the query likelihood as the probability that a news headline generates blog posts published on the query day (i.e. Qp).",1,ad,True
47,"To this end, we should estimate two language models, the Query Language Model (QLM) and the News Headline Language Model (NHLM). Both of the language models are estimated, based on blog posts.",1,Query,True
48,3.1.1 Query Language Model,1,Query,True
49,"For a query day Qd, we estimate the QLM using blog posts Qp. However, the blog posts may discuss various topics from individual daily affairs to important events recently happened. If we model the blog posts using a single language model, the language model cannot correctly capture",1,LM,True
50,3http://www.mediacollege.com/journalism/news/ newsworthy.html,0,,False
51,396,0,,False
52,"the contents of the blog posts. As a result, we will get the",1,blog,True
53,wrong QLM.,1,LM,True
54,"To solve this problem, we divide the documents into K",0,,False
55,clusters. We assume that each cluster can accurately reflect,0,,False
56,one of the various topics mixed in the blog posts. To esti-,1,blog,True
57,"mate the QLM, we first gather blog posts which are pub-",1,LM,True
58,"lished on a query day. Then, we cluster them using the",0,,False
59,K-means algorithm. We represent each document using the,0,,False
60,"term vector d : wi ,"" tfi × idfi, where tfi indicates the fre-""",0,,False
61,quency,0,,False
62,of,0,,False
63,term,0,,False
64,wi,0,,False
65,within,0,,False
66,a,0,,False
67,document,0,,False
68,"d,",0,,False
69,and,0,,False
70,idfi,0,,False
71,",",0,,False
72,log(,0,,False
73,|T D| dfi,0,,False
74,),0,,False
75,means inverse document frequency: |T D| is the total number,0,,False
76,of documents in a collection and dfi is document frequency of a term wi. We use the cosine similarity as the distance function between two documents.,0,,False
77,"Similarity di, dj , di · dj",0,,False
78,(2),0,,False
79,|di| × |dj |,0,,False
80,"where |di| and |dj | indicate the length of di and dj, respectively.",0,,False
81,"After clustering, we can generate the QLMs from the K document sets. Let Dk ,"" {dk1 , dk2 , · · · , dkn } be the kth document set, QLMk be the kth QLM. The document set Dk contains information relevant to a topic of the document set, but also contains background information. We assume""",1,LM,True
82,that the documents are generated by a mixture model of,0,,False
83,QLMk and the collection language model C that reflects the background information.,1,LM,True
84,"P (Dk) ,",0,,False
85,{(1 - )P (w|QLMk ) + P (w|C )}c(w;dki ) (3),1,LM,True
86,iw,0,,False
87,where c(w; dki ) is the number of times term w occurred in,0,,False
88,"a document dki , P (w|C ) ,",0,,False
89,ctfw |C|,0,,False
90,:,0,,False
91,ctfw,0,,False
92,is the number of,0,,False
93,"times term w occurred in the entire collection, |C| is the",0,,False
94,"length of the collection, and  is a weighting parameter. In",0,,False
95,"our experiments, we set  as 0.8.",0,,False
96,"Then, we can estimate QLMk using the EM algorithm [11]. The EM updates for p(w|QLMk ) are as follows:",1,LM,True
97,tnw,0,,False
98,",",0,,False
99,(1 - )P n(w|QLMk ) (1 - )P n(w|QLMk ) + P n(w|C ),1,LM,True
100,(4),0,,False
101,"P n+1(w|QLMk ) ,",1,LM,True
102,"n i,1",0,,False
103,c(w;,0,,False
104,dki,0,,False
105,)tnw,0,,False
106,w,0,,False
107,"n i,1",0,,False
108,c(w,0,,False
109,;,0,,False
110,dki,0,,False
111,)tn w,0,,False
112,(5),0,,False
113,3.1.2 News Headline Language Model,1,ad,True
114,"To estimate the NHLM, for each news headline, we first retrieve blog posts relevant to its topic using the news headline itself as query. To this end, we evaluate the relevance between a news headline H and a blog post d using the the KL-divergence language model [18] with Dirichlet smoothing [27].",1,LM,True
115,"Score(H, d)  P (w|H) log P (w|d)",0,,False
116,(6),0,,False
117,w,0,,False
118,where P (w|H) is the maximum likelihood estimates of the,0,,False
119,"news headline, and P (w|d) ,",1,ad,True
120,: c(w;d)+dP (w|C ) |d|+d,0,,False
121,c(w; d) is,0,,False
122,"the number of times a term w occurred in a blog post d, and",1,blog,True
123,"d is a smoothing parameter, and |d| is the length of the",0,,False
124,document d.,0,,False
125,"Among the search results, we use only the blog posts",1,blog,True
126,whose issued date is within a certain period from a query,0,,False
127,"day, because the time gap between the issued day of a blog",1,blog,True
128,"post and a query day often means that the blog post is mentioning an event different from those that happened on that day [25]. In other words, the blog post is likely to be relevant to a topically similar, but different news headline.",1,blog,True
129,"We gather only the blog posts between -3 and +28 days from a query day. Then, we choose 10 blog posts that can provide as diverse aspects about the news headline as possible. We call the 10 blog posts the supporting relevant posts of the news headline.",1,blog,True
130,"We propose two approaches to make the supporting relevant posts reflect the diverse aspects relevant to the news headline: Relevance-Based Selection (RBS), Feed-Based Selection (FBS).",1,ad,True
131,RBS is an intuitive but naive approach to choose the supporting relevant posts. This approach selects the supporting relevant posts according to a relevance score of each blog post obtained from Eq. 6. We define this approach as a baseline for our experiments.,1,blog,True
132,"FBS chooses the supporting relevant posts based on blog feeds which belong to each of them. Individual blog users may have different interests and tendencies for the same events, and these differences can be represented through their blog feed [19]. That is, for a given news headline, blog posts from different blog feeds can provide different aspects, even if they address information on the same news story. Therefore, to increase the diversity of the supporting relevant posts, we select them from as wide a range of blog feeds as possible. In a similar way to RBS, FBS also chooses blog posts according to their relevance score, but FBS selects only one blog post from one blog feed.",1,blog,True
133,"We estimate the NHLM using the maximum likelihood estimate of the 10 supporting relevant posts and the Dirichlet smoothing [27]. Let NHLM and C be the NHLM and the collection language model, respectively, and let SRP be a set of the 10 supporting relevant posts.",1,LM,True
134,P (w|NHLM ),1,LM,True
135,",",0,,False
136,c(w; SRP ) + hP (w|C ) |SRP | + h,0,,False
137,(7),0,,False
138,where c(w; SRP ) is the number of times a term w occurred in SRP and h is a smoothing parameter.,0,,False
139,3.1.3 Score Function,0,,False
140,"To evaluate the query likelihood, we use the KL-divergence language model [18], one of the-state-of-the-art information retrieval models, to rank news headlines in response to a given query. We use the maximum value among scores between the QLMs and the NHLM as the relevance score of a news headline.",1,ad,True
141,"Let ScoreQLH (Qd, H) be the relevance score of a news headline H with respect to a given query Qd. We define ScoreQLH (Qd, H) as follows:",1,ad,True
142,"ScoreQLH(Qd,H) , max k",0,,False
143,P (w|QLMk) logP (w|NHLM) (8),1,LM,True
144,w,0,,False
145,3.2 The News Headline Piror,1,ad,True
146,"We suggest two criteria to estimate the news headline prior P (H) that is the prior belief about the importance or newsworthiness of a news headline for a given day: Temporal Profiling and Term Importance. Although the proposed approaches depend on a date such as the query day, we re-",1,ad,True
147,397,0,,False
148,gard them as the priors of a news headline in that they are independent of the query language model.,1,ad,True
149,3.2.1 Temporal Profiling,0,,False
150,"The Temporal Profiling criterion uses the temporal information of blog posts relevant to a news headline. We assume that if a news headline is important for a query day, many blog posts relevant to its topic will be posted on that day.",1,blog,True
151,"To generate the temporal profile of each news headline, we use a temporal profiling approach proposed in [14] with some modifications. The temporal profile of a news headline H is defined as follows:",1,ad,True
152,"P (t|H) ,"" P (t|d) Score(H, d)""",0,,False
153,(9),0,,False
154,dR,0,,False
155,"d R Score(H, d )",0,,False
156,"where t is a date (day), and R is a document set that consists of 500 blog posts selected by an order of a relevance score Score(H, d) from Eq. 6, and",1,blog,True
157,"P (t|d) ,",0,,False
158,1 if t is equal to the document date 0 otherwise,0,,False
159,We then smoothed the temporal profile P (t|H) using the background model as follows:,0,,False
160,"P (t|H) , (1 - )P (t|H) + P (t|C)",0,,False
161,(10),0,,False
162,where,0,,False
163,P (t|C),0,,False
164,",",0,,False
165,1 |T D|,0,,False
166,dC P (t|d): |T D| is the total num-,0,,False
167,"ber of documents in the collection, and  is a smoothing",0,,False
168,"parameter. In our experiments, we set  , 0.5.",0,,False
169,This temporal profile is defined on each single day. How-,0,,False
170,"ever, if a news story is important for a query day Qd, the blog posts relevant to it may be published over a certain period",1,blog,True
171,"following the day due to the bursty nature [15]. Therefore,",0,,False
172,we smooth the temporal profile model with the model for,0,,False
173,"adjacent days. Let ScoreT P (Qd, H) be a score of a news",1,ad,True
174,headline estimated using the temporal profile of the news,1,ad,True
175,headline.,1,ad,True
176,"ScoreT P (Qd, H)",0,,False
177,",",0,,False
178,1 Zw,0,,False
179,"w(t, Qd)P (t|H)",0,,False
180,t,0,,False
181,(11),0,,False
182,"where  indicates a period from Qd, and Zw ,"" t w(t, Qd). We define a weight function for w(t, Qd) using the Cosine (Hamming) kernel function [20] as follows:""",0,,False
183,"w(t, Qd) ,",0,,False
184,1 2,0,,False
185,1 + cos,0,,False
186,|t-Qd |× ,0,,False
187,0,0,,False
188,t,0,,False
189,(12),0,,False
190,otherwise,0,,False
191,3.2.2 Term Importance,0,,False
192,"The Term Importance criterion uses term information of a news headline. We believe that each term has a different importance for a given day. If a news headline consists of important terms, it is likely to be a top news story and vice-versa. For example, a news headline that consists of common words or stopwords may not be a top news story.",1,ad,True
193,"We only consider named entities, not all terms in a news headline. Named entities were used by many event detection systems, improving the performance of the systems [17, 26, 28]. We extract named entities from each news headline using the Stanford Named Entity Recognizer4. Then, we gather all n-gram (n  3) from the named entities.",1,ad,True
194,4http://nlp.stanford.edu/software/CRF-NER.shtml,0,,False
195,We evaluate the importance of the n-gram terms based on the T F · IDF approach that is widely used for term weighting in many information retrieval tasks.,0,,False
196,"Let nt be the extracted n-gram term and T F (nt, Qd) be a term frequency of the term nt for a query day Qd. Intuitively, if a term nt occurs frequently within news headlines issued on a query day, it is likely to be important. In a similar way to the bursty nature of blog posts, news headlines relevant to important events that happen on the query day may be published over several subsequent days. Therefore, we define the term frequency T F (nt, Qd) as the number of a term nt within news headlines that are issued during a certain interval containing a query day Qd.",1,ad,True
197,"T F (nt, Qd) , c(nt; t)",0,,False
198,(13),0,,False
199,t,0,,False
200,"where  indicates the period, and c(nt; t) means the number of a term nt occurring in news headlines issued on day t.",1,ad,True
201,"Let IDF (nt) be the inverse ""date"" frequency, and T N D be the total number of days that the news headline corpus spans. We define IDF (nt) as follows:",1,ad,True
202,"IDF (nt) , T N D",0,,False
203,(14),0,,False
204,DF (nt) + ,0,,False
205,"where DF (nt) indicates the number of days on which nt occurs in news headlines, and  is a constant which controls the influence of DF (nt) on IDF (nt).",1,ad,True
206,"The inverse date frequency IDF (nt) corresponds to the inverse document frequency. In other words, a term nt with a high IDF (nt) value may be a keyword that distinguishes important events that happened on a query day from those that happened on other days.",0,,False
207,"Let ScoreT I (Qd, H) be the importance of a news headline H evaluated using the term importance.",1,ad,True
208,"ScoreT I (Qd, H) ,"" max (T F (nt, Qd) × IDF (nt)) (15) ntH""",0,,False
209,3.3 Integration of Query Likelihood and News Headline Prior,1,Query,True
210,"We proposed several approaches for the query likelihood and the news headline prior in section 3.1 and 3.2. They capture the different characteristics of important news headlines. For the query likelihood, we analyze the contents of the blog posts, and model the dominant topics buried in them. Then, we rank the news headlines according to the probability that each headline generates one of the topics. For the news headline prior, we proposed two criteria to reflect the properties of important news headlines, Temporal Profiling and Term Importance.",1,ad,True
211,"To identify the top news headline, we integrate the query likelihood with the news headline prior. To achieve this, we first adjust each score from 0 to 1.",1,ad,True
212,S corei (H ),0,,False
213,",",0,,False
214,Scorei(H) - mini maxi - mini,0,,False
215,(16),0,,False
216,"mini , min Scorei(H ) and maxi , max Scorei(H )",0,,False
217,H,0,,False
218,H,0,,False
219,"where Scorei(H) indicates one score of ScoreQLH (Qd, H), ScoreT P (Qd, H) and ScoreT I (Qd, H).",0,,False
220,398,0,,False
221,"Finally, we define the ranking function as follows:",0,,False
222,"Score(Qd,H),""(1 - 1)ScoreQLH(Qd, H)""",0,,False
223,"+1 (1 - 2)ScoreTI(Qd,H)+2ScoreTP(Qd,H) (17)",0,,False
224,"where 1 is the weighting parameter that adjusts the importance between the query likelihood and the news headline prior, and 2 is the parameter that controls the weights between two criteria for the news headline prior.",1,ad,True
225,4. EXPERIMENTS,0,,False
226,"We conducted several experiments to evaluate our system for TSIT. We measured the performance of the query likelihood and the news headline prior, respectively. We also investigated the influence of the combination of two components on the performance of TSIT, with a varying weight parameter 1.",1,ad,True
227,4.1 Setup,0,,False
228,4.1.1 Data Set,0,,False
229,"The Blogs08 corpus and the news headline corpus from the New York Times (NYT) [21] were used for experiments. The Blogs08 corpus was created by monitoring 1 million blogs from January 14, 2008 to February 10, 2009, and consisted 808GB of feeds, 1445GB of permalink documents and 56GB of homepages. The news headline corpus consisted of headlines of articles published by NYT during the interval covered by the Blogs08 corpus.",1,ad,True
230,"Our experiments were performed using only Blog08 and the news headline corpus without resorting to any other resources. For the evaluation, we used the 55 topics and relevance judgments from the TREC 2009 Top Stories Identification Task.",1,ad,True
231,"We only used the permalinks (blog post) for the experiments. We discarded the HTML tags of each blog post, and applied the DiffPost algorithm [23] to remove non-relevant contents5 of each blog post. Each blog post was also processed by stemming using the Porter stemmer and eliminating stopwords using the INQUERY words stoplist [2].",1,blog,True
232,4.1.2 Evaluation Method,0,,False
233,"In response to each query, we retrieved 100 news headlines according to their importance on that day, and provide 10 supporting relevant posts for each news headline, as in TSIT.",1,ad,True
234,"The evaluation consists of two phases. In the first phase, we assess the performances of the proposed approaches for identifying the top news headlines for a query day. For each query, we considered only the news headlines corresponding to Qd ± 1 days as ranking candidates, because of the time discrepancy between the day on which the headline was and the day Qd on which the news story actually happened [21]. We used the mean average precision (MAP) and the precision at rank 5 and 10 (P@5 and P@10) as the evaluation measures.",1,ad,True
235,"In the second phase, the supporting relevant posts are evaluated. The posts should provide diverse aspects relevant to their news headline. To assess the diversity of the supporting relevant posts, we used the -nDCG [10] and IA-Precision [1] measures.",1,ad,True
236,"5In [23], the non-relevant contents of a blog post means the",1,blog,True
237,0.14,0,,False
238,0.12,0,,False
239,MAP of Query Likelihood,1,MAP,True
240,0.10,0,,False
241,0.08,0,,False
242,0.06 0.04,0,,False
243,"Query Likelihood Baseline (K,0)",1,Query,True
244,0.02 30,0,,False
245,300,0,,False
246,600,0,,False
247,900,0,,False
248,1200,0,,False
249,The number of clusters K,0,,False
250,1500,0,,False
251,Figure 1: The MAP scores of the query likelihood according to varying the number of clusters K. The NHLM is estimated using the RBS.,1,MAP,True
252,Table 1: The performances of the query likelihood estimated using the RBS and the FBS approaches for the NHLM. The number of clusters K is set to 500 for the QLM estimation.,1,LM,True
253,Model MAP P@5 P@10,1,MAP,True
254,QLHRBS 0.1303 0.2291 0.2255 QLHF BS 0.1315 0.2473 0.2355,0,,False
255,4.2 Results and Discussion,0,,False
256,4.2.1 The Query Likelihood,1,Query,True
257,We conducted several experiments to evaluate the performance of the query likelihood for TSIT. The aim of the experiments is to determine (1) how correctly the QLMs reflect the various topics buried in blog posts; and (2) how the proposed approaches for NHLM estimation affect the performance of the query likelihood.,1,LM,True
258,"The query likelihood has a few parameters, the number of clusters K for QLMs and the smoothing parameter h for NHLM. For our experiments, the smoothing parameter h is set to 2000 without further parameter tuning.",1,LM,True
259,"To determine how correctly the QLMs the reflect various topics buried in blog posts, we evaluated the performance of the query likelihood according to varying K values.",1,LM,True
260,"K : 1, 30, 50, 100, 300, 500, 1000, 1500",0,,False
261,"For the NHLM estimation, the supporting relevant posts were chosen using the RBS approach.",1,LM,True
262,"Figure 1 shows the MAP scores according to varying K values. We set the baseline using K , 1. This means that blog posts are modeled using a single QLM.",1,MAP,True
263,"Compared with the baseline, the performances for all K > 1 were significantly improved, and the best performance was obtained when using K ,"" 500. From these results, we can confirm that a single QLM cannot correctly capture the contents of the blog posts, because of the topical diversity of the blog posts. This weakness reduced its ability for identifying the top news headlines.""",1,LM,True
264,"useless contents for the blog search, such as menu, banner and site description",1,blog,True
265,399,0,,False
266,Table 2: The performances of the news headline,1,ad,True
267,"prior estimated using Temporal Profiling, Term Im-",0,,False
268,"portance and their combination (2 , 0.8). The best performances are shown in bold.",0,,False
269,Model Period MAP P@5 P@10,1,MAP,True
270,P RIT P,0,,False
271,1,0,,False
272,0.1410 0.2545 0.2691,0,,False
273,2,0,,False
274,0.1745 0.2982 0.3109,0,,False
275,3 0.1800 0.3055 0.3273,0,,False
276,P RIT I,0,,False
277,1,0,,False
278,0.0263 0.0400 0.0509,0,,False
279,2,0,,False
280,0.0448 0.1127 0.0873,0,,False
281,3 0.0458 0.1273 0.0891,0,,False
282,P RIT P +T I,0,,False
283,3,0,,False
284,0.1957 0.3673 0.3364,0,,False
285,"As the number of clusters K increased to 500, the respective topics buried in the blog post were captured by the K clusters. The QLM estimated using each cluster led to the improved performance of the query likelihood. When K  500, the clusters have been overfitted, and did not provide enough information relevant to each topic. As a result, the performance decreased.",1,blog,True
286,"To investigate how the proposed approaches for NHLM estimation affect the performance of the query likelihood, we measured the performances with two approaches to select the supporting relevant posts. For these experiments, we set K , 500 to estimate the QLMs.",1,LM,True
287,"Let QLHRBS and QLHF BS be the query likelihood using the NHLM estimated using RBS and FBS approaches, respectively. Table 1 shows the performances of QLHRBS and QLHF BS. The performances of QLHF BS are better than those of QLHF BS for all measures. These results means that the supporting relevant posts selected using FBS provide more diverse aspects of a news headline than those chosen using RBS. As a result, the performance of the query likelihood increased.",1,LM,True
288,4.2.2 The News Headline Prior,1,ad,True
289,We proposed two criteria to estimate the news headline prior: Temporal Profiling and Term Importance. We experimentally confirmed the usefulness of the proposed criteria to estimate the news headline prior.,1,ad,True
290,"First, we evaluated the performance of each approach according to varying the period . The approaches consider a certain period from a query day to gather evidence for the news headline prior. Generally, blog posts and news headlines related to events that happened on a query day are published on that day or in the following days. However, they can be published on preceding days, because of the time discrepancy described in section 4.1.2.",1,ad,True
291,We defined several periods as follows:,0,,False
292,· 1 :  is set between -1 and +1 days from Qd.,0,,False
293,· 2 :  is set between -3 and +7 days from Qd.,0,,False
294,· 3 :  is set between -3 and +14 days from Qd.,0,,False
295,"Let P RIT P and P RIT I be the news headline prior estimated using the Temporal Profiling and Term Importance, respectively. Table 2 shows the performances of each approach according to varying periods and those obtained from the combination of the two approaches. For the experiments, we set the parameters,  in Eq.12 and  in Eq.14, by maximizing the MAP using an exhaustive search in the following",1,ad,True
296,MAP of News Headline Prior,1,MAP,True
297,0.20,0,,False
298,0.18,0,,False
299,0.16,0,,False
300,0.14,0,,False
301,0.12,0,,False
302,0.10,0,,False
303,0.08,0,,False
304,0.06,0,,False
305,0.04,0,,False
306,0,0,,False
307,0.2,0,,False
308,0.4,0,,False
309,0.6,0,,False
310,0.8,0,,False
311,1,0,,False
312,"The weighting parameter, 2",0,,False
313,"Figure 2: The Map scores of the news headline prior according to varying the parameter 2 (1 , 1).",1,ad,True
314,Table 3: The performances of systems integrating,0,,False
315,"the query likelihood and the news headline prior,",1,ad,True
316,"QLHRBS+P RIT P+T I and QLHF BS+P RIT P+T I (1 , 0.8 and 2 ,"" 0.8). uogTrTStimes: The best performance in TREC'09 Top Stories Identification Task,""",1,TREC,True
317,"QLHF BS: the best performance of the query likelihood, P RIT P +T I : the best performance of the news headline prior. The best performances are shown",1,ad,True
318,in bold. Statistical significance at the 0.05 and 0.01,0,,False
319,level is indicated by  and  for improvement from,0,,False
320,"the query likelihood, respectively, § and ¶ for im-",0,,False
321,"provement from the news headline prior, respec-",1,ad,True
322,tively.,0,,False
323,Model,0,,False
324,MAP,1,MAP,True
325,P@5,0,,False
326,P@10,0,,False
327,uogTrTStimes,0,,False
328,0.1862,0,,False
329,0.3236,0,,False
330,0.3127,0,,False
331,QLHF BS P RIT P +T I QLHRBS+P RIT P+T I QLHF BS+P RIT P+T I,0,,False
332,0.1315 0.1957 0.2081¶ 0.2124¶,0,,False
333,0.2473 0.3673 0.4145¶ 0.4255¶,0,,False
334,0.2255 0.3364 0.3455 0.3527§,0,,False
335,values.,0,,False
336,",  : 30, 40, 50, 60, 70",0,,False
337,"For both approaches, as we consider a longer period, we obtain better results. This observation confirms our assumption that if a news story is important for a given day, blog posts and news headlines relevant to it will be posted during several days. Although Temporal Profiling resulted in good performance, the performances of the Term Importance were relatively low. For event detection, the usefulness of the named entities can change depending on which circumstances they are used in [17]. We think that the poor performance of Term Importance is because we used the named entities without considering the circumstances. However, the combination of the two approaches led to the best performance ( , 50 and  ,"" 40). Compared with the best performance of Temporal Profiling, we achieved 1.57% and 6.18% improvement in MAP and P@5, respectively. These results verify the usefulness of the named entities for identifying important news stories.""",1,blog,True
338,"To explore the influence of two criteria when identifying the top news story, we measured the MAP score according",1,MAP,True
339,400,0,,False
340,Table 4: The performances of the supporting relevant posts chosen using RBS and FBS.,0,,False
341,Model,0,,False
342,-nDCG@5 -nDCG@10 IA-P@5 IA-P@10,0,,False
343,QLHRBS+P RIT P+T I QLHF BS+P RIT P+T I,0,,False
344,0.479 0.485,0,,False
345,0.486 0.492,0,,False
346,0.169 0.171,0,,False
347,0.145 0.147,0,,False
348,"to varying the weighting parameter 2 (1 ,"" 1, i.e. we are utilizing only the news headline prior to evaluate the performance), in Figure 2. The weight parameter 2 controls the relative importance of Temporal Profiling and Term Importance as the news headline prior. The best performance was obtained when 2 "","" 0.8. From these results, we can again confirm that Temporal Profiling and Term Importance should be considered together to improve the performance.""",1,ad,True
349,4.2.3 Integration of the Query Likelihood and the News Headline Prior,1,Query,True
350,"Finally, we measured the performance of our system that integrates the query likelihood and the new headline prior. To integrate these components, we used QLHRBS and QLHF BS for the query likelihood, and P RIT P +T I for the news headline prior.",1,ad,True
351,"Table 3 shows the performances of our systems, QLHRBS+ P RIT P+T I and QLHF BS + P RIT P+T I. In addition, for comparison purposes, we reported the best performing results of the TREC-2009 Top Stories Identification Task [21], and the best performances of the query likelihood and the news headline prior. We performed the Wilconxon signed rank test to examine whether the improvement of the performance over that of each component was statistically significant.",1,ad,True
352,"Integrating the two components significantly improved the performance of TSIT. For the MAP, the best performance was 8.09% and 1.67% higher than those of the query likelihood and the news headline prior, respectively. Specifically, our system achieved 2.62%, 10.19% and 4.00% further increases in MAP, P@5 and P@10 over the best performance of TREC'09 TSIT.",1,MAP,True
353,"This result implies that the performance can be improved by combining the query likelihood and the news headline prior. They reflect different characteristics that important news headlines should be satisfying. The query likelihood identifies the important news headlines based on modeling the dominant topics in blog posts, but the news headline prior identifies them using various features such as the number of relevant posts, and the importance of terms within news headlines.",1,ad,True
354,"Figure 3 shows the MAP scores of QLHF BS + P RIT P+T I according to varying the parameter 1 (2 , 0.8). The weight parameter 1 controls the relative importance of the query likelihood and the news headline prior. The best performance was obtained when 1 ,"" 0.8. From these results, we can again verify that the performance for TSIT can be improved when integrating the query likelihood and the news headline prior. We do not show the graph for QLHRBS+P RIT P+T I, because it was almost identical to that of QLHF BS + P RIT P+T I.""",1,MAP,True
355,4.2.4 Diversity of Supporting Relevant Posts,0,,False
356,"The supporting relevant posts should provide the diverse aspects relevant to a news headline. We proposed two approaches to choose the supporting relevant posts, the RBS and the FBS. We evaluated the diversity of the supporting",1,ad,True
357,MAP of integrated system,1,MAP,True
358,0.22,0,,False
359,0.21,0,,False
360,0.20,0,,False
361,0.19,0,,False
362,0.18,0,,False
363,0.17,0,,False
364,0.16,0,,False
365,0.15,0,,False
366,0.14,0,,False
367,0.13,0,,False
368,0,0,,False
369,0.2,0,,False
370,0.4,0,,False
371,0.6,0,,False
372,0.8,0,,False
373,1,0,,False
374,The weighting parameter ,0,,False
375,1,0,,False
376,"Figure 3: The Map scores of the integrated system according to varying the parameter 1 (2 , 0.8)",0,,False
377,relevant posts selected by each approach and displayed the results in Table 4.,0,,False
378,"FBS performed better than RBS. We can verify that the supporting relevant posts of FBS provided more diverse aspects of a news headline than those of RBS. These results confirm that the use of blog feeds can improve the diversity of the supporting relevant posts. Furthermore, these results agree with the results from identifying the top news headlines in Table 3. That is, compared with RBS, FBS chose the supporting relevant posts that provide more correct and diverse aspects of a news headline. As a result, QLHF BS + P RIT P+T I outperformed QLHRBS + P RIT P+T I.",1,ad,True
379,5. CONCLUSION AND FUTURE WORK,0,,False
380,"In this study, we presented several approaches for identifying top news stories in the blogosphere. Our system utilizes the query likelihood and the news headline prior, based on the language model framework. For the query likelihood, we proposed several approaches to estimate the QLM and the NHLM. The QLM can be estimated using blog posts issued on the query day. We divided the blog posts into K clusters so that each cluster can accurately contain one of the various topics buried in blog posts. Then, we estimated the K number of QLMs respective to their clusters. We also proposed two approaches to choose the supporting relevant posts for a news headline. The posts were also able to cover many different aspects of the news headline.",1,blog,True
381,"Furthermore, for the news headline prior, we suggested two criteria, Temporal Profiling and Term Importance. They measure the importance of a news headline in two different ways. Temporal profiling measures it using the temporal information of blog posts relevant to the news headline. Term Importance measures it using the meaningfulness of terms in the news headline.",1,ad,True
382,We obtained the best performance for TSIT by considering the query likelihood and the news headline prior at,1,ad,True
383,401,0,,False
384,"the same time. From experimental results, we can verify the the proposed approaches are effective in identifying top news headlines.",1,ad,True
385,"Many studies remain for future work. We used K-means clustering to model various topics buried in blog posts. It would be interesting to utilize several approaches such as PLSA [13] and LDA [4] to capture topics of blog posts. To improve the diversity of the supporting relevant posts, various ways such as MMR [6] are also worthy of research. Furthermore, we believe that various features such as comments or tags can be used to improve the performance when identifying top news stories.",1,blog,True
386,6. ACKNOWLEDGMENTS,0,,False
387,This work was supported in part by MKE & IITA through IT Leading R&D Support Project and also in part by the BK 21 Project in 2010.,1,ad,True
388,7. REFERENCES,0,,False
389,"[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of WSDM 2009, pages 5­14. ACM, 2009.",0,,False
390,"[2] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. Inquery and trec-9. In Proceedings of TREC-9, pages 551­562, 2000.",1,trec,True
391,"[3] J. Allan, R. Papka, and V. Lavrenko. On-line new event detection and tracking. In Proceedings of SIGIR 1998, pages 37­45. ACM, 1998.",0,,False
392,"[4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993­1022, 2003.",0,,False
393,"[5] T. Brants, F. Chen, and A. Farahat. A system for new event detection. In Proceedings of SIGIR 2003, pages 330­337. ACM, 2003.",0,,False
394,"[6] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR 1998, pages 335­336. ACM, 1998.",0,,False
395,"[7] C. C. Chen, Y.-T. Chen, Y. Sun, and M. C. Chen. Life cycle modeling of news events using aging theory. In Proceedings of ECML 2003, pages 47­59, 2003.",0,,False
396,"[8] K.-Y. Chen, L. Luesukprasert, and S.-c. T. Chou. Hot topic extraction based on timeline analysis and multidimensional sentence modeling. IEEE Trans. on Knowl. and Data Eng., 19(8):1016­1025, 2007.",0,,False
397,"[9] H. L. Chieu and Y. K. Lee. Query based event extraction along a timeline. In Proceedings of SIGIR 2004, pages 425­432. ACM, 2004.",1,Query,True
398,"[10] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of SIGIR 2008, pages 659­666, New York, NY, USA, 2008. ACM.",1,Novelty,True
399,"[11] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1­38, 1977.",0,,False
400,"[12] Q. He, K. Chang, and E.-P. Lim. Analyzing feature trajectories for event detection. In Proceedings of SIGIR 2007, pages 207­214. ACM, 2007.",0,,False
401,"[13] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of SIGIR 1999, pages 50­57. ACM, 1999.",0,,False
402,"[14] R. Jones and F. Diaz. Temporal profiles of queries. ACM Trans. Inf. Syst., 25(3):14, 2007.",0,,False
403,"[15] J. Kleinberg. Bursty and hierarchical structure in streams. In Proceedings of SIGKDD 2002, pages 91­101. ACM, 2002.",0,,False
404,"[16] P. Kolari, A. Java, and T. Finin. Characterizing the splogosphere. In Proceedings of 3rd Annl. Workshop on Weblogging Ecosystem: Aggregation, Analysis and Dynamics, 15th Word Wide Web Conf., 2006.",1,blog,True
405,"[17] G. Kumaran and J. Allan. Text classification and named entities for new event detection. In Proceedings of SIGIR 2004, pages 297­304. ACM, 2004.",0,,False
406,"[18] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR 2001, pages 111­119. ACM, 2001.",0,,False
407,"[19] Y. Lee, S.-H. Na, and J.-H. Lee. An improved feedback approach using relevant local posts for blog feed retrieval. In Proceeding of CIKM 2009, pages 1971­1974. ACM, 2009.",1,blog,True
408,"[20] Y. Lv and C. Zhai. Positional language models for information retrieval. In Proceedings of SIGIR 2009, pages 299­306. ACM, 2009.",0,,False
409,"[21] C. Macdonald, I. Ounis, and I. Soboroff. Overview of the TREC-2009 Blog Track. In Proceedings of TREC 2009, 2010.",1,TREC,True
410,"[22] G. Mishne and M. de Rijke. A study of blog search. In Proceedings of ECIR 2006, pages 289­301. Springer, 2006.",1,blog,True
411,"[23] S.-H. Nam, S.-H. Na, Y. Lee, and J.-H. Lee. Diffpost: Filtering non-relevant content based on content difference between two consecutive blog posts. In Proceedings of ECIR 2009, pages 791­795. Springer-Verlag, 2009.",1,blog,True
412,"[24] C. Wang, M. Zhang, L. Ru, and S. Ma. Automatic online news topic ranking using media focus and user attention based on aging theory. In Proceeding of CIKM 2008, pages 1033­1042. ACM, 2008.",0,,False
413,"[25] Y. Yang, T. Pierce, and J. Carbonell. A study of retrospective and on-line event detection. In Proceedings of SIGIR 1998, pages 28­36. ACM, 1998.",0,,False
414,"[26] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. Topic-conditioned novelty detection. In Proceedings of SIGKDD 2002, pages 688­693. ACM, 2002.",0,,False
415,"[27] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, 2004.",0,,False
416,"[28] K. Zhang, J. Zi, and L. G. Wu. New event detection based on indexing-tree and named entity. In Proceedings of SIGIR 2007, pages 215­222. ACM, 2007.",0,,False
417,"[29] Y. Zhang, J. Callan, and T. Minka. Novelty and redundancy detection in adaptive filtering. In Proceedings of SIGIR 2002, pages 81­88. ACM, 2002.",1,Novelty,True
418,402,0,,False
419,,0,,False

,sentence,label,data,regex
0,On Performance of Topical Opinion Retrieval,0,,False
1,Giambattista Amati,0,,False
2,"Fondazione Ugo Bordoni Rome, Italy gba@fub.it",0,,False
3,Giuseppe Amodeo,0,,False
4,"University of L'Aquila L'Aquila, Italy",0,,False
5,gamodeo@fub.it,0,,False
6,Valerio Capozio,0,,False
7,"University ""Tor Vergata"" Rome, Italy",0,,False
8,capozio@mat.uniroma2.it,0,,False
9,Carlo Gaibisso,0,,False
10,"IASI - CNR Rome, Italy carlo.gaibisso@iasi.cnr.it",0,,False
11,Giorgio Gambosi,0,,False
12,"University ""Tor Vergata"" Rome, Italy",0,,False
13,gambosi@mat.uniroma2.it,0,,False
14,ABSTRACT,0,,False
15,"We investigate the effectiveness of both the standard evaluation measures and the opinion component for topical opinion retrieval. We analyze how relevance is affected by opinions by perturbing relevance ranking by the outcomes of opiniononly classifiers built by Monte Carlo sampling. Topical opinion rankings are obtained by either re-ranking or filtering the documents of a first-pass retrieval of topic relevance. The proposed approach establishes the correlation between the accuracy and the precision of the classifier and the performance of the topical opinion retrieval. Among other results, it is possible to assess the effectiveness of the opinion component by comparing the effectiveness of the relevance baseline with the topical opinion ranking.",0,,False
16,Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Performance evaluation (efficiency and effectiveness),0,,False
17,"General Terms: Theory, Experimentation",0,,False
18,"Keywords: Sentiment Analysis, Opinion Retrieval, Classification",0,,False
19,1. INTRODUCTION,1,DUC,True
20,"Opinion mining aims to classify sentences or documents by polarity of opinions. The application of opinion mining to IR (named Topical Opinion Retrieval) deals with ranking documents according to both topic relevance and opinion content. Topical Opinion Retrieval goes back to the novelty track of TREC 2003 [11] and the Blog tracks of TREC [7, 4, 8].However, there is not yet a comprehensive study of the interaction and the correlation between relevance and sentiment assessments. For example, the best runs based on the best official topic relevance baseline (baseline4) in the blog track of TREC 2008 (short topics 1001-1050) [8] achieve the MAPR value equal to 0.4724, that drops to the MAPO|R of opinion equal to 0.4189, and to MAP equal to 0.1566 and 0.1329 for the polarity tasks (positive and negative opinionated rankings respectively). Performance degradation is intuitively expected because any variable which is additional to relevance, for example opinion, deteriorates system performance.",1,TREC,True
21,There is no way to separate and evaluate the effective-,0,,False
22,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
23,"ness of the opinion detection component, or to determine whether and to which extent the relevance and opinion detection components are influenced by each other. It seems evident that an evaluation methodology or at least some benchmarks are needed to assess how much effective the opinion component is. At the moment, the only way to assess MAPO|R after opinionated re-ranking is to compare the increment of MAPO|R with respect to the relevance baseline, that is to assume the relevance baseline as a random ranking of opinionated documents about a given topic. Continuing with the same example, the MAPO|R of the relevance baseline is 0.3822, so that the actual MAPO|R increment is 9.6% after opinion re-ranking. Changing baselines and thus initial MAPR, one would have different increment rates even if the same polarity or opinion mining and re-ranking techniques were used. It is also a matter of fact that opinion MAPO|R seems to be highly dependent on the initial relevance MAPR of the first-pass retrieval [7, 4, 8]. To exemplify: how effective is the performance value of opinion MAPO|R 0.4189 when we start from an initial relevance MAPR of 0.4724? What would be the MAPO|R and P@10O|R by filtering documents as in a binary classification approach, and what would be the accuracy of such opinion classifier?",1,MAP,True
24,"In conclusion, can absolute values of MAP be used to compare different tasks, such as topical opinion and the ad hoc retrieval, and to compare and assess the state of the art of different techniques on opinion finding? At this aim, we introduce a completely novel methodological framework to: - provide best achievable topical opinion MAPO|R, for a given relevance document ranking MAPR; - predict the performance of topical opinion retrieval given the performance of topic retrieval and opinion classification; - reciprocally, assess the opinion detection component accuracy from the overall topical opinion retrieval performance; - study the robustness of standard evaluation measures (MAP, P@10 etc,) for opinion retrieval; - study best re-ranking or filtering strategies on top of opinion classifiers independently from the adopted ad hoc relevance model.",1,MAP,True
25,This paper focuses on a few of these issues.,0,,False
26,2. METHODOLOGY,0,,False
27,"In the first step all documents that are relevant to the test queries R , qQR(q) are pooled. From the subset O , qO(q)  R of opinionated and relevant documents one obtains the conditional probability of occurrence of opin-",0,,False
28,777,0,,False
29,0.55 0.5,0,,False
30,0.45 0.4,0,,False
31,0.35 0.3,0,,False
32,0.25 0.2,0,,False
33,0.15 0.1 50%,0,,False
34,0.8,0,,False
35,0.75,0,,False
36,0.7,0,,False
37,0.65,0,,False
38,0.6,0,,False
39,0.55,0,,False
40,0.5,0,,False
41,0.45 50%,0,,False
42,60% 60%,0,,False
43,70% 70%,0,,False
44,80% 80%,0,,False
45,90%,0,,False
46,100%,0,,False
47,90%,0,,False
48,100%,0,,False
49,Figure 1: MAPO|R and P@10O|R by classifier accuracy. Opinionated document filtering with Monte Carlo sampling from the five TREC official baselines.,1,MAP,True
50,"ions with respect to relevance, P (O|R). Assuming that each document has an unknown topic as a hidden variable, O is a sample of opinionated documents of the whole collection, and the prior for opinionated but not relevant documents, i.e. P (O|R), is provided by P (O|R) (i.e. it is de facto postulated the independence between relevance and opinion content). The second step consists in constructing a Monte Carlo sampling of opinionated documents from test data, and in obtaining thus opinion-only classifiers with different accuracy k. The MAPO|R is averaged on rankings built with all classifiers with the same accuracy k. The relevance rankings are modified according to:",1,MAP,True
51,"· filtering, that is not opinionated documents are removed from the relevance baseline;",0,,False
52,"· re-ranking, that is opinionated documents receive a ""reward"" in their relevance ranking.",0,,False
53,"Topical opinion retrieval by Monte Carlo sampling is easily conducted with the filtering approach. Re-ranking approach requires the combination of two different scores, related to content and to opinion (e.g. [2, 9]). Opinion scores can be easily obtained with a lexicon-based approach [5, 10, 6, 3, 1]. The official relevance baselines and the topical opinion scores are provided by the blog TREC, while the opinion scores are not available, so that we use the lexiconbased scores and the re-ranking methodology that can be found in [1]. This lexicon-based approach has a good performance and achieves the MAPO|R of 0.4006 with respect the same baseline (baseline4 of the blog TREC 2008). Monte Carlo sampling consists in assigning the lexicon-based score to opinionated documents and a null score to non opinionated ones, assuming an accuracy 0  k  1 of the classifier.",1,blog,True
54,3. CONCLUSIONS,0,,False
55,"To improve M APO|R with respect to its baseline requires a high accuracy of the opinion-only classifiers (at least around 80% with the filtering approach and 70% with the re-ranking technique of [1] as shown in Figures 1 and 2). However, smaller accuracy even close to that of the relevance baseline improves P@10 more easily than for MAP. The best value of M APO|R with the filtering approach achieves an empirical value around the M APR of the relevance baseline. There is almost a linear correlation between the M APO|R and the ac-",1,AP,True
56,0.55 0.5,0,,False
57,0.45 0.4,0,,False
58,0.35 0.3,0,,False
59,0.25 0.2,0,,False
60,0.15 0.1 50%,0,,False
61,0.85 0.8,0,,False
62,0.75 0.7,0,,False
63,0.65 0.6,0,,False
64,0.55 0.5,0,,False
65,0.45 50%,0,,False
66,60% 60%,0,,False
67,70% 70%,0,,False
68,80% 80%,0,,False
69,90%,0,,False
70,100%,0,,False
71,90%,0,,False
72,100%,0,,False
73,Figure 2: MAPO|R and P@10O|R by classifier accuracy. Relevant document re-ranking of the five TREC official baselines with lexicon-based scores and Monte Carlo sampling.,1,MAP,True
74,"curacy k of the opinion-only classifier, both in filtering and re-ranking approaches. Finally, interpolating values of Figures 1 and 2 one can show that the best run of TREC 2008 based on the re-ranking approach must have an opinion-only classifier accuracy greater than or equal to 78%, while using the lexicon-based classifier of [1] an accuracy of 74%.",1,TREC,True
75,"Within this evaluation framework we can now compare performances of different opinion mining techniques, investigate how relevance and opinion are influenced by each other, and assess effectiveness of re-ranking strategies.",0,,False
76,4. REFERENCES,0,,False
77,"[1] G. Amati, E. Ambrosi, M. Bianchi, C. Gaibisso, and G. Gambosi. Automatic construction of an opinion-term vocabulary for ad hoc retrieval. In Proc. 30th ECIR, vol. 4956, LNCS, pages 89­100, 2008.",1,ad,True
78,"[2] K. Eguchi and V. Lavrenko. Sentiment retrieval using generative models. In Proc. ACL-EMNLP Conf., pp. 345­354, 2006.",0,,False
79,"[3] X. Huang and W. B. Croft. A unified relevance model for opinion retrieval. In Proc. 18th ACM-CIKM, pp. 947­956, 2009.",0,,False
80,"[4] C. Macdonald, I. Ounis, and I. Soboroff. Overview of the TREC-2007 Blog Track. In Proc. 16th TREC, 2007.",1,TREC,True
81,"[5] G. Mishne. Multiple ranking strategies for opinion retrieval in blogs. In Proc. 15th TREC, 2006.",1,blog,True
82,"[6] S. H. Nam, S. H. Na, Y. Lee, and J. H. Lee. Diffpost: Filtering non-relevant content based on content difference between two consecutive blog posts. In Proc. 31st BCS-ECIR, vol. 5478, LNCS, pp. 791­795, 2009.",1,blog,True
83,"[7] I. Ounis, M. de Rijke, C. Macdonald, G. A. Mishne, and I. Soboroff. Overview of the TREC-2006 Blog Track. In Proc. 15th TREC, 2006.",1,TREC,True
84,"[8] I. Ounis, C. Macdonald, and I. Soboroff. Overview of the TREC-2008 Blog Track. In Proc. 17th TREC, 2008.",1,TREC,True
85,"[9] B. Pang and L. Lee. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1­2):1­135, 2008.",0,,False
86,"[10] J. Skomorowski and O. Vechtomova. Proc. 29st BCS-ECIR, vol. 4425, LNCS, pp. 405­417, 2007.",0,,False
87,"[11] I. Soboroff and D. Harman. Overview of the TREC-2003 Novelty Track. In Proc. 12th TREC, pp. 38­53, 2003.",1,TREC,True
88,778,0,,False
89,,0,,False

,sentence,label,data,regex
0,The Importance of Anchor Text for Ad Hoc Search Revisited,0,,False
1,"Marijn Koolen1 Jaap Kamps1,2",0,,False
2,"1 Archives and Information Studies, University of Amsterdam, The Netherlands 2 ISLA, Informatics Institute, University of Amsterdam, The Netherlands",0,,False
3,"{m.h.a.koolen,kamps}@uva.nl",0,,False
4,ABSTRACT,0,,False
5,"It is generally believed that propagated anchor text is very important for effective Web search as offered by the commercial search engines. ""Google Bombs"" are a notable illustration of this. However, many years of TREC Web retrieval research failed to establish the effectiveness of link evidence for ad hoc retrieval on Web collections. The ultimate resolution to this dilemma was that typical Web search is very different from the traditional ad hoc methodology. So far, however, no one has established why link information, like incoming link degree or anchor text, does not help ad hoc retrieval effectiveness. Several possible explanations were given, including the collections being too small for anchors to be effective, and the density of the link graph being too low.",1,TREC,True
6,"The new TREC 2009 Web Track collection is substantially larger than previous collections and has a dense link graph. Our main finding is that propagated anchor text outperforms full-text retrieval in terms of early precision, and in combination with it, gives an improvement in overall precision. We then analyse the impact of link density and collection size by down-sampling the number of links and the number of pages respectively.",1,TREC,True
7,"Other findings are that, contrary to expectations, (inter-server) link density has little impact on effectiveness, while the size of the collection has a substantial impact on the quantity, quality and effectiveness of anchor text. We also compare the diversity of the search results of anchor text and full-text approaches, which show that anchor text performs significantly better than full-text search and confirm our findings for the ad hoc search task.",1,ad,True
8,Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--performance evaluation (efficiency and effectiveness),0,,False
9,"General Terms: Experimentation, Measurement, Performance",0,,False
10,"Keywords: Ad hoc, Anchor text, Collection size, Link density",1,hoc,True
11,1. INTRODUCTION,1,DUC,True
12,"The use of anchor text for Web retrieval is well studied, with the broad conclusion that it is very effective for finding entry pages of sites­often outperforming approaches based on document text alone­but not for ad hoc search. Based on claims from commer-",1,ad,True
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
14,"cial search engine companies, over the course of several years of Web search experiments at TREC [34], organisers and participants have tried to establish the effectiveness of link information, including anchor text, for retrieval. Despite the enthusiasm and effort of many participating groups, in the first two years, 1999­2000, participants failed to show any improvements due to link information [17]. After the first year, the main reason was deemed to be the low number of inter-server links [19]. For the second year, an artificially crafted collection with more inter-server links (WT10g, [4]) was used, and participants combined content information with link evidence such as PageRank, HITS and anchor text, but again without success. This time, the main difference between the results at TREC and the general belief that link information is valuable, was considered to be the difference in search tasks [15, 33]. Typical Web search behaviour is very different from the user model assumed for the traditional ad hoc methodology. Web searchers tend to ""prefer the entry page of a well-known topical site to an isolated piece of text, no matter how relevant"" [17]. According to Hawking and Craswell [17], p.215:",1,TREC,True
15,"Hyperlink and other web evidence is highly valuable for some types of search task, but not for others.",0,,False
16,"Although the switch to more Web-centric search tasks like home page and named page finding showed link information to be very effective for these tasks [9, 24, 26, 29], there is no clear explanation of why anchor text is not effective for ad hoc retrieval. Anchor text provides short summaries often by different authors about the topic of a page. This could potentially improve the document representation of web pages that have incoming links, and thereby the precision of search results. On the Web, which is infinitely large, early precision is the important criterion. Recall is almost impossible to measure, but also not important for most users.",1,ad,True
17,"Gurrin and Smeaton [14] pointed out that the inter-server link density of the WT10g collection was still very low, and extracted a subset of the collection, WT-dense, which has a much higher interserver link density. Within this tiny subset they found that a combination of content and link information could improve precision on the ad hoc topics of the TREC-9 Web track. This led them to come up with a list of requirements a representative test-collection must satisfy to study the value of link information. A good Web collection needs to be sufficiently large and have sufficiently high inter- and intra-server link densities.",1,WT,True
18,"The size issue was addressed in the Terabyte Tracks of 2004­ 2006, which used the GOV2 collection, based on a crawl of the .gov domain in 2004, consisting of 25 million documents.1 Again, anchor text was found to be highly effective for Web-centric tasks,",1,ad,True
19,"1The crawl on the .gov domain was exhausted before reaching the targeted 100 million pages, and plans to rectify this by crawling additional pages from the .edu domain were never realised.",1,ad,True
20,122,0,,False
21,"but not for ad hoc search [22, 23]. However, the .gov domain is very different in nature from the .com domain used for the crawl on which the WT10g collection is based, and the .GOV2 collection has fewer incoming links per page. Thus, although it is larger than the earlier Web Track collections, its link density is much lower, making it hard to investigate the impact of collection size.",1,ad,True
22,"At the TREC 2009 Web Track [6] a new, large Web collection-- ClueWeb09 [7]--was introduced and the traditional Ad hoc Task was paired with the new Diversity task. This new collection is much larger than the collections used at TREC 8 and 9, and was crawled to reflect Tier 1 of a commercial search engine, so should have a relatively dense link structure, allowing us to study both aspects of collection size and link density. If a large number of documents and a high link density are indeed requirements for anchor text to be effective, this new collection might finally reveal us its potential. This urges us to revisit the question:",1,TREC,True
23,· What is the importance of anchor text for ad hoc search?,1,ad,True
24,"Surely, the issue of having enough (inter-server) links for anchor text is critical for its success on any search task, but perhaps the link density needs to be higher for anchor text to be effective for ad hoc retrieval than for entry page finding. Intuitively, the number of links in the collection plays a direct role in the quantity of anchor text and might therefore affect its quality as well. Links within the same site are often navigational links, with anchor terms such as `click', `here' and `next' [11]. Therefore, it is generally assumed that links between sites are more meaningful, including their anchor text [27].",1,ad,True
25,"The other factor mentioned by Gurrin and Smeaton [14] is the size of the collection. We know that precision increases with collection size [18], which holds for document text indexes, but should hold for anchor text indexes as well. With larger collections, the number of documents that have incoming links increases and as a consequence, so does the number of those documents relevant to a given topic. The new ClueWeb collection used at the TREC 2009 Web Track is much larger than the collections used at TREC 8 and 9 and should have many incoming links per page, resulting in more anchor text and thereby possibly better document representations.",1,ClueWeb,True
26,"The direct relation between anchor text and link in-degree, and thereby the relation between anchor text and the popularity or importance of a page, helps locate home pages and popular pages described by the search terms. This offers an interesting perspective on the task of finding diverse search results. If anchor text is effective for ad hoc search as well, it has the potential to find good results for both informational and navigational information needs.",1,ad,True
27,"These considerations lead us to break down our main research question into several, more specific research questions:",1,ad,True
28,· Is anchor text effective for improving ad hoc retrieval?,1,ad,True
29,· What is the impact of link density on the effectiveness of anchor text? And what is the relative importance of interand intra-server links?,0,,False
30,· What is the impact of collection size on the effectiveness of anchor text?,0,,False
31,· What is the importance of anchor text for the diversity of ad hoc search results? And what is the impact of anchor text on informational and navigational information needs?,1,ad,True
32,"The rest of the paper is organised as follows. We will first discuss related work in Section 2. In Section 3 we describe our initial experiments to compare the effectiveness of anchor text and full-text search. Then, in Section 4 we analyse how link density and collection size affect the quantity and quality of anchor text, followed by Section 5 where we investigate the diversity of anchor text search results. We draw conclusions in Section 6.",0,,False
33,2. RELATED WORK,0,,False
34,"The importance of anchor text has been studied extensively at TREC. At TREC 8, participants could not show consistent improvements over content-only baselines using link information [19]. This unexpected result led many to believe that the collection had too few inter-server links for link evidence to be effective, in response to which a new collection was constructed focusing on interserver link density [4]. At the TREC 9 Web Track, the first reported attempts at exploiting anchor text for ad hoc retrieval did not show any improvements either [15]. Singhal and Kaszkiel [32] raised doubts about the TREC evaluation methodology used to model Web search, as they found different results for anchor text when comparing TREC results against their in-house tests.",1,TREC,True
35,"Several studies [8, 33] pointed at the differences between traditional ad hoc search (as evaluated at TREC) and Web search behaviour. As new, more realistic Web tasks were introduced [16], the value of link information was finally shown [9, 26] and anchor text was found to be be very effective for site- and home page finding tasks. Craswell et al. [10] recently showed the effectiveness of anchor text for diversity. Eiron and McCurley [11] showed that anchor text behaves much like user queries. If Web authors use the same labels to describe pages as Web searchers use to find pages, anchor text can potentially bridge the gap between queries and pages and lead to high precision, if the anchors and pages in the collection are of high quality.",1,ad,True
36,"The quality is another important difference between the new ClueWeb collection and previous TREC Web collections, which is related to the way it is constructed, and which directly affects the density of (inter-server) links. Several studies have looked at the impact of crawling policy on the quality [3] and search effectiveness [12, 13] of the crawled collection. Page importance metrics can be used to schedule the most important or useful pages to be crawled first. Since page importance is usually derived using linkbased measures such as PageRank [30] or On-line Page Importance Computation [OPIC, 1], which give a higher score to a page if it has more incoming links, the first part of a crawl based on such policies tends to have a high link density. One of the primary goals of creating the ClueWeb data set was ""to approximate Tier 1 of a web search engine index"" [5]. The category B data set, which we use here, consists of the first 50 million English pages of this crawl.",1,ClueWeb,True
37,3. INITIAL EXPERIMENTS,0,,False
38,We will first describe our initial experiment with plain full-text and anchor text approaches to see if the relative effectiveness of anchor text merits further investigation.,0,,False
39,"3.1 Data, Index and Runs",0,,False
40,"We use the ClueWeb09 category B, which contains a sample of 50 million English pages of the larger ClueWeb09 crawl [7]. As the pages were crawled based on a large set of seed URLs with high PageRank and at later stages were crawled in order of OPIC value, we assume this data set contains many of the most important Web pages and a relatively dense Web graph.",1,ClueWeb,True
41,"We used Indri [21] for indexing. Stopwords are removed and all other terms are stemmed with the Krovetz stemmer. We created two indexes, a full-text index containing only the document text and an anchor text index containing only the propagated anchor text.",0,,False
42,"With anchor text we mean the underlined text to which a hyperlink is anchored. We only use the string of text appearing in the anchor text. To extract anchor text from the ClueWeb09 category B collection, we used the harvestlinks method, which comes with Indri. Because harvestlinks does not compress its data during processing, which would use up more disk space than we have avail-",1,ClueWeb,True
43,123,0,,False
44,"able, we only used the harvesting option to extract the anchor text",0,,False
45,"with source url, document ID and target url per bundle of pages,",0,,False
46,"compressed the output, then mapped the target url to a document",0,,False
47,ID ourselves. We extracted over 1.5 billion links pointing to pages,0,,False
48,"within the collection, with anchor text for more than 75% of all the",0,,False
49,pages. Quite a large number of pages have multiple links to the,0,,False
50,same target URL (repeated links). If we collapse those repeated,0,,False
51,links we end up with 1.18 billion links between just over 50 mil-,0,,False
52,"lion pages, which leads to a mean in-degree of 23.30. The median",1,ad,True
53,"in-degree is 2. For anchor text, repeated links mean more (and po-",0,,False
54,tentially different) descriptions from the same source page.,0,,False
55,The full-text and anchor text runs use the Indri language model,0,,False
56,"approach and linear smoothing with collection , 0.15. For ad hoc",1,ad,True
57,"search, the length of a document is related with the probability of",0,,False
58,relevance. Documents are scored using the document length as a,0,,False
59,prior probability p(d),0,,False
60,",",0,,False
61,|d| |D|,0,,False
62,",",0,,False
63,where,0,,False
64,d,0,,False
65,is,0,,False
66,a,0,,False
67,document in collection,0,,False
68,D. The length prior on the anchor text is determined by the total,0,,False
69,length of the all the anchor text of a particular page. This means,0,,False
70,that pages with many incoming links have long document repre-,0,,False
71,"sentations, while pages with a single incoming link have a very",0,,False
72,short document representation. The length prior boosts pages with,0,,False
73,"many links, i.e. pages with high in-degrees. We report on the ef-",0,,False
74,fectiveness of the length prior in [25]. We also made a mixture,1,ad,True
75,"run, combining the full-text and anchor runs using the weighting",0,,False
76,"Scoremix(d) ,"" 0.7 · Scorefull(d) + 0.3 · Scoreanchor(d), where""",0,,False
77,the scores are normalised by the sum of the top 1000 scores before,0,,False
78,"addition. If a document d is retrieved by only one index, it receives",1,ad,True
79,a zero probability score for the other index. The mixture run was,0,,False
80,submitted as an official run at the TREC 2009 Web Adhoc task and,1,TREC,True
81,contributed to the pool. The other two runs can be considered as,0,,False
82,baseline runs. No tuning was done after the relevance judgements,0,,False
83,"were made available. Furthermore, we have created two runs us-",1,ad,True
84,ing the incoming link degree of the pages returned by the Text run.,0,,False
85,We chose to use in-degree as it was to found to be as effective as,0,,False
86,"PageRank [2, 28] but easier to compute. For the In-degree run, the",0,,False
87,full-text results are ranked only by in-degree. For the Text · In-,0,,False
88,degree run we use the in-degree as another document prior in the,0,,False
89,language model of Indri.,0,,False
90,3.2 Results,0,,False
91,"The results are shown in Table 1. To put this into perspective, we compared them against the results of the best official submissions of other participants. Both MTC and statAP were used to construct the judgement pools. We use statAP [35], as it is more robust when evaluating runs that did not contribute to the pool. We test for significant changes with respect to the full-text baseline using a one-tailed bootstrap test with 100,000 resamples.",1,AP,True
92,"Is anchor text effective for ad hoc retrieval? The Anchor run has a low statMAP compared to the Text run. A possible explanation is that many pages in the collection have no or few incoming links, including many relevant pages. With no or only a few words as document representation, these pages are hard to find using anchor text only, which has consequences for average precision. In contrast, anchor text is very effective for early precision. The Anchor run scores much better on MPC(30) than the Text run and supports the above explanation for its low statMAP score. The anchor text run ranks the relevant pages in its index highly, but seems to miss many relevant pages. We will further investigate this issue below. More importantly, the Mix run leads to significant improvements in statMAP showing that the two indexes are complementary and that Web structure can be used to improve ad hoc search.",1,ad,True
93,The in-degree priors only hurt the underlying Text run. This is not surprising given that in-degree is blind to the topic of a query.,0,,False
94,"Table 1: Results for the 2009 Adhoc Task. Significance tests are with respect to the full text run, confidence levels are 0.95 (), 0.99 (·) and 0.999 (·)",1,hoc,True
95,Full collection,0,,False
96,No Wikipedia,1,Wiki,True
97,Run,0,,False
98,statMAP MPC(30) statMAP MPC(30),1,MAP,True
99,Text,0,,False
100,0.1442 0.3079 0.1038 0.2557,0,,False
101,Anchor Mix,0,,False
102,0.0567 0.5558 0.1643 0.4812,0,,False
103,0.0617 0.4289 0.1213 0.4773,0,,False
104,In-degree,0,,False
105,0.0823 0.1876 0.0592 0.1258,0,,False
106,Text · In-degree 0.1098 0.2694 0.0746 0.2059,0,,False
107,UDWAxQEWeb 0.1999 0.5010,0,,False
108,­,0,,False
109,­,0,,False
110,uogTrdphCEwP 0.2072 0.4966,0,,False
111,­,0,,False
112,­,0,,False
113,ICTNETADRun4 0.1746 0.4368,0,,False
114,­,0,,False
115,­,0,,False
116,"Within a much larger collection containing spam pages and other pages with low PageRank, degree-based link ranking algorithms have an important function of separating the high quality pages from the rest. Within this particular collection, which is crawled to reflect Tier 1, most pages are of high quality, so the work of finding important and reliable pages is already done. Further use of in-degrees only disrupts the subtle relevance ranking of text-based retrieval models. Anchor text gives more precise results because it focuses on the subset of links that have query terms in the anchors, and is thus more sensitive to the topical context than in-degree.",1,ad,True
117,"The best runs of the top 3 groups of the TREC 2009 Web Ad hoc task, according to MPC(30), score substantially better on statMAP, but lower on MPC(30). This shows that anchor text alone can meet or exceed the precision of the top-performing systems.",1,TREC,True
118,"Over the top 1000, the overlap between the Anchor and Text runs is 4.6%, showing that anchor text really leads to very different document representations and targets very different pages. The overlap between the Ad hoc judgements and the Anchor run is also very small. In the top 5 results of the Anchor run, over 30% of the pages are unjudged, while at rank 16, more than half of the results are unjudged. Indeed, it seems that the Anchor run is also very different from all runs that contributed to the assessment pool. In contrast, the overlap between the Ad hoc judgements and the Text run is much higher. At rank 5, on average less than 11% is unjudged, while at rank 16 just over 22% is unjudged. These percentages are probably much closer to the actual sampling rates. Thus, the improvement of the Mix run over the Anchor run might simply be caused by the larger number of judged results in that run.",1,ad,True
119,"Perhaps anchor text is more effective than in previous TREC experiments because this collection contains the full Wikipedia, which has a dense link structure and many anchors matching the titles of the target pages. Wikipedia pages are edited by many contributors, so the quality might be higher than that of many Web pages. The relevance judgements reveal that more than 21% of the relevant pages in ClueWeb B are Wikipedia pages, while the whole Wikipedia forms only 12% of all the pages in the collection. We built separate full- and anchor text indexes of all non-Wikipedia pages. If the presence of Wikipedia is the main reason for the effectiveness of anchor text, we would expect the non-Wikipedia Anchor run to perform worse than the non-Wikipedia Text run. Columns 4 and 5 in Table 1 show the results of these runs. Although scores are lower than over the full collection indexes--perhaps partly explaining why ClueWeb B runs tend to outperform ClueWeb A runs [6]--the Anchor run still has higher early precision and the Mix run still has higher statMAP than the Text run. Wikipedia is not the sole reason for the effectiveness of anchor text.",1,TREC,True
120,"Is anchor text effective for improving ad hoc retrieval? On a large collection of high quality pages, anchor text gives good precision",1,ad,True
121,124,0,,False
122,and in combination with full-text leads to significant improvements in overall precision. This new Web collection finally shows the long expected value of Web link structure for ad hoc search. Gurrin and Smeaton [14] suggested that the benefits of link information for retrieval would become clear with a sufficiently large collection and a high inter-server link density. Our results support their statement and urge us to address these issues.,1,ad,True
123,4. WHY ANCHOR TEXT WORKS,0,,False
124,"In this section we seek to understand what makes the anchor text representation effective. We look at the impact of link density and collection size, which we do by down-sampling either links or pages. If we down-sample the pages, we can investigate the impact of collection size on the effectiveness of anchor text. If, on the other hand, we keep the number of pages the same, but instead down-sample the links, we can see the impact of link density.",1,ad,True
125,"If we randomly sample 50% of the pages and remove the outgoing links of those pages, we would expect to end up with roughly 50% of all the links. If we remove the pages themselves from the collection, we lose both the outgoing and incoming links of those pages. Thus, if we sample 50% of the pages, we remove more than 50% of the links. Previous TREC Web collections were smaller which could explain why anchor text was not effective earlier. The number of links does not grow linearly with collection size. How does page sampling affect the link density of the collection?",1,TREC,True
126,"Randomly sampling pages is different from using earlier stages of the crawl as a smaller collection. The first 25 million pages of the crawl have a different composition from randomly sampling 25 million pages [13]. With random sampling, the most important pages will be affected in the same way as the rest of the pages. If the crawl is stopped at half the number of pages, the collection will still have most of the important pages, as modern crawling strategies focus on crawling the most important pages first. However, since ClueWeb B is assumed to be a subset of Tier 1 of a Web search index, based on a large number of seed URLs, we expect that the composition of any sample of the ClueWeb B collection approximates the composition of the full collection. One of the favourable aspects of randomly sampling pages is that the probability of relevance is unaffected [18].",1,ClueWeb,True
127,"Down-sampling either pages or links means the anchor text representation of a page changes. For each sample, we have to filter the link graph and anchors, and build a separate anchor text index. For the full-text index, we only have to make separate indexes for the page filtered samples. Sampling links has no impact on the fulltext document representations. Since page sampling has an effect on both collection size and link density and link sampling only affects link density, we will first look at the impact of sampling links.",0,,False
128,4.1 The Impact of Link Density,0,,False
129,"If we remove links from the collection, we can expect the performance of anchor text to go down. If we remove all links, the anchor text index is empty and no page will ever be returned. The more links in the collection, the more information we have to distinguish between pages. Therefore, we expect to see effectiveness increase with link density, at least while the density is low. Beyond a certain point the effectiveness might stabilise or become worse.",0,,False
130,"We filter links by randomly selecting n% of all documents and removing their outgoing links. The impact of sampling outgoing links on the number of inter- and intra-server links is shown in Figure 1. Reading from right to left, both the number of inter- and intra-server links decrease, linear to the sample size. How does this affect the pages with anchor text? If we remove 50% of the links, pages will lose roughly half of their incoming anchors. For",1,ad,True
131,1e+10 1e+09,0,,False
132,Inter-server (Link filtered) Intra-server (Link filtered) Inter-server (Page filtered) Intra-server (Page filtered),0,,False
133,Number of links,0,,False
134,1e+08,0,,False
135,1e+07,0,,False
136,1e+06,0,,False
137,100000 1,0,,False
138,10,0,,False
139,100,0,,False
140,Sample size (% of collection),0,,False
141,Figure 1: The impact of outgoing link sampling on the number of inter- and intra-server links per sample.,0,,False
142,Table 2: Impact of link filtering on the percentage of pages with,0,,False
143,anchor text,0,,False
144,All pages,0,,False
145,Relevant pages,0,,False
146,Percent Inter Intra All Inter Intra All,0,,False
147,100.000 15.30 70.26 75.43 25.54 74.46 80.96,0,,False
148,50.000 11.41 56.35 61.51 21.04 64.84 71.96,0,,False
149,25.000 8.24 43.79 48.36 17.14 54.87 61.97,0,,False
150,12.500 5.78 33.06 36.75 13.77 44.15 50.75,0,,False
151,6.250 3.94 24.17 26.96 10.94 35.81 41.80,0,,False
152,3.125 2.61 17.00 19.00 8.35 28.41 33.33,0,,False
153,"pages with high in-degree, this might not affect the document representation much. Pages with only one or a few incoming anchors could lose most or all of their anchor text. In other words, the most important pages are robust against random link sampling.",0,,False
154,"In Table 2 we see how filtering affects the percentage of pages that have at least one incoming link with anchor text. The interserver links cover only 15% of all pages but 25% of the relevant pages. Apparently, pages with incoming links from other sites have a higher probability of being relevant. The intra-server links cover a much larger part of the collection (70%). The ratio of inter- to intraserver links is about 1 to 5.5. At a sample size of 12.5% the number of intra-server links is lower than the number of inter-server links at a sample size of 100%. However, at 12.5%, the intra-server links cover 33% of the pages (44% of the relevant pages), while at 100% the inter-server links cover only 15% of the pages and 25.54% of the relevant pages. The intra-server links are thus more uniformly distributed, leading to fewer anchors per page. What does this mean for the effectiveness of inter- and intra-server links?",1,ad,True
155,"The impact of sampling links on the effectiveness of full-text and anchor text is shown in Figure 2. The full-text index is not affected by link sampling, hence the straight line in the figures. The statMAP of the Anchor run slowly decreases as we remove more links because the index covers fewer pages. The Mix run scores better at statMAP with even the smallest samples of links, indicating that even very few links can improve the Text run. We also look at traditional MAP and found very similar results. Contrary to our expectations, with smaller samples, the MPC(30) scores of the anchor text run stay well above the Text score. We note that below 12.5% of the links (less than 3 incoming links per page), the density is well below the link densities of earlier TREC Web collections. The impact of link density seems small. One possible explanation is that the highest quality pages have so many incoming links that they are robust against link sampling. This is reflected in the percentages shown in Table 2. With fewer links, the number",1,MAP,True
156,125,0,,False
157,StatMAP MPC(30),1,MAP,True
158,0.18 0.16 0.14 0.12,0,,False
159,0.1 0.08 0.06 0.04 0.02,0,,False
160,0 0,0,,False
161,Text Anchor,0,,False
162,Mix,0,,False
163,10 20 30 40 50 60 70 80 90 100 Percentage sampled of collection,0,,False
164,0.6 Text,0,,False
165,Anchor,0,,False
166,0.5,0,,False
167,Mix,0,,False
168,0.4,0,,False
169,0.3,0,,False
170,0.2,0,,False
171,0.1,0,,False
172,0 0 10 20 30 40 50 60 70 80 90 100,0,,False
173,Percentage sampled of collection,0,,False
174,MRR,0,,False
175,0.7 Text,0,,False
176,Anchor,0,,False
177,0.6,0,,False
178,Mix,0,,False
179,0.5,0,,False
180,0.4,0,,False
181,0.3,0,,False
182,0.2,0,,False
183,0.1,0,,False
184,0 0 10 20 30 40 50 60 70 80 90 100,0,,False
185,Percentage sampled of collection,0,,False
186,"Figure 2: Impact of link sampling on effectiveness of full-text, anchor text and mixture runs.",0,,False
187,MPC(30) MRR,0,,False
188,0.5 0.45,0,,False
189,0.4 0.35,0,,False
190,0.3 0.25,0,,False
191,0.2 0.15,0,,False
192,0.1 0,0,,False
193,0.55 Inter-server links Intra-server links,0,,False
194,0.5,0,,False
195,0.45,0,,False
196,0.4,0,,False
197,0.35,0,,False
198,0.3,0,,False
199,0.25,0,,False
200,0.2,0,,False
201,2e+08 4e+08 6e+08 8e+08 1e+09 1.2e+09 1.4e+09 Number of links,0,,False
202,0.15 0,0,,False
203,Inter-server links Intra-server links,0,,False
204,2e+08 4e+08 6e+08 8e+08 1e+09 1.2e+09 1.4e+09 Number of links,0,,False
205,Figure 3: Comparison of inter- and intra-server link anchor effectiveness.,0,,False
206,"of pages with anchors goes down, but the number of relevant pages decreases more slowly. As the link density goes down, the relevant pages form a larger part of the index. On the other hand, the Anchor run might find relevant pages ranked much lower by the runs that contributed the pages to the pool, which represent many estimated relevant pages. To rule out that the MPC(30) score is over-estimated we transformed the relevance judgements to traditional binary judgements and looked at the Mean Reciprocal Rank (MRR, right side of Figure 2). The MRR never over-estimates as it simply counts the rank of the highest ranked relevant document. It supports that anchor text gives better early precision than full-text.",1,ad,True
207,"What is the qualitative difference between inter- and intra-server links? We already saw there is a big quantitative difference. Most of the links are between pages on the same server (85%). The left side in Figure 3 shows the MPC(30) scores for the inter- and intraserver Anchor runs. Of course, any observed difference could be due to the larger quantity of intra-server links. Therefore, we show the scores with the actual number of links on the x-axis. Even at a similar number of links, the intra-server Anchor run scores better. An explanation is that the intra-server link anchors cover more pages, because they are more evenly distributed, and can thus find more relevant pages. The difference in MRR between the inter- and intra-server links is smaller, and both scores go up with more links, showing that both are better able to identify relevant pages with higher link density. However, the impact of link density quickly stabilises beyond a certain point. In the first tier of a Web index, containing high quality pages, there seems to be little qualitative difference between inter- and intra-server links. Insight in search engine optimisation may have taught Web site owners to make internal anchor text more meaningful. The larger quantity of intraserver links, and their more even distribution makes them more effective for finding multiple relevant pages.",1,ad,True
208,"What is the impact of link density on the effectiveness of anchor text? It plays a role at low densities, but its impact stabilises quickly. Inter- and intra-server links have different distributions and a different coverage of the collection. Within a collection of high PageRank pages, the difference between inter- and intra-server links is more quantitative than qualitative. Without a crawling pol-",0,,False
209,Table 3: Number of documents and topics per sample,0,,False
210,Percent Size in Docs # Rel. Docs Topics # Rel./topic,0,,False
211,"100.000 50,220,423",0,,False
212,"4,002",0,,False
213,49,0,,False
214,81.57,0,,False
215,"50.000 25,110,211",0,,False
216,"1,987",0,,False
217,49,0,,False
218,40.55,0,,False
219,"25.000 12,555,105",0,,False
220,965,0,,False
221,47,0,,False
222,20.53,0,,False
223,"12.500 6,277,552",0,,False
224,486,0,,False
225,46,0,,False
226,10.57,0,,False
227,"6.250 3,138,776",0,,False
228,253,0,,False
229,44,0,,False
230,5.75,0,,False
231,"3.125 1,569,388",0,,False
232,132,0,,False
233,42,0,,False
234,3.14,0,,False
235,"icy that focuses on finding high quality pages first, the quality of the crawl and therefore the quality of the intra-server links might go down. Inter-server links might be more robust, as they tend to have less navigational anchors and are harder to use nepotistically.",0,,False
236,4.2 The Impact of Collection Size,0,,False
237,"Next, we look at the impact of the collection size. How effective is anchor text if we reduce the size of the collection? To see this, we need to down-sample the collection, which we do by randomly selecting half of the pages. We show the number of (relevant) pages in each filtered sample in Table 3. At each step, the ratio of all pages and relevant pages is roughly the same. If we remove 50% of the pages in the collection, and remove those from the relevance judgements as well, we end up with about 50% of the relevant pages. For smaller samples this has consequences for the number of topics with any relevant document. At 3.125%, we have 132 relevance judgements left for 42 topics (3.14 relevant pages per topic). Although the number of topics is still large enough to be representative, the low number of relevant pages per topic might make per topic results unreliable.2",0,,False
238,"As mentioned above, if we sample pages, the number of links does not decrease linearly to the sample size. In Table 4 we see the percentage of pages in the sample that have at least one incoming link. One interesting observation is that page sampling has a similar impact on the coverage of inter-server links as link sampling, but a very different impact on the coverage of intra-server links. If we sample 3.125% of the links (Table 2), the intra-server anchors cover 17% of all pages, while if we sample 3.125% of the pages, the intra-server anchors cover 43% of the remaining pages. Yet",0,,False
239,2The relevance judgements of the Ad hoc task include the proba-,1,hoc,True
240,bility of a page being included in the assessment pool. This proba-,0,,False
241,"bility is used to estimate how many pages a pooled page represents,",0,,False
242,which is based on the size of the collection. If a relevant page has,0,,False
243,a pool probability of 0.2 it represents,0,,False
244,1 0.2,0,,False
245,",",0,,False
246,5 relevant pages in,0,,False
247,"the full collection. With uniform down-sampling, the probability",0,,False
248,"of a page being sampled stays the same, but the number of pages",0,,False
249,it represents is proportional to the sample size. At a sample size of,0,,False
250,25% of the full collection it represents 25% of the relevant pages,0,,False
251,it represents in the full collection. Most judged pages have a unit,0,,False
252,"pooling probability, so are not affected by down-sampling.",0,,False
253,126,0,,False
254,Table 4: Impact of page filtering on the percentage of pages,0,,False
255,with anchor text All pages,0,,False
256,Relevant pages,0,,False
257,Percent Inter Intra All Inter Intra All,0,,False
258,100.000 15.30 70.26 75.43 25.54 74.46 80.96,0,,False
259,50.000 11.40 61.09 65.49 19.23 67.54 73.63,0,,False
260,25.000 8.23 53.96 57.50 15.34 61.04 66.32,0,,False
261,12.500 5.77 48.73 51.41 14.40 59.26 63.99,0,,False
262,6.250 3.94 45.07 46.98 10.28 52.57 56.52,0,,False
263,3.125 2.61 42.59 43.88 7.58 50.76 53.79,0,,False
264,"the inter-server anchors cover 2.61% of the collection, whether we sample 3.125% of the links or the pages.",0,,False
265,"The impact of sampling pages on the effectiveness of full-text and anchor text is shown in Figure 4. The statMAP (left figure) of the Text run goes up slowly--possibly due to losing topics with little relevance--while for the Anchor run it goes down slowly. Theory suggests that statMAP should remain relatively stable over random samples of a collection [18]. The drop in statMAP for the Anchor run can be explained by looking at the precision scores. The Text run gains precision at rank 30 (MPC(30), centre figure) as the collections grows, as predicted [18]. The anchor text precision is more affected by collection size. With half the collection, anchor text is nowhere near as effective as full-text. The MRR (right figure) of the Text run is similar to that of the statMAP. The average rank of the first relevant document increases slowly, partly due to losing topics. However, the MRR of the Anchor run drops rapidly with smaller samples. With fewer relevant documents left, and an increasingly smaller coverage of the collection, it becomes harder to find relevant pages through anchor text.",1,MAP,True
266,"What is the impact of the collection size on anchor text? For precision at a fixed cut-off, the impact of the collection size is much larger for anchor text than for full-text. First, collection size affects the anchor text representation but not the full-text representation. Second, the number of pages in the full-text index grows linearly with collection size, but more than linearly for anchor text. For ad hoc search, where the task is to find pages with relevant text no matter their popularity, this coverage is essential. We stress again the importance of having a collection of high quality pages; expanding the collection with low quality pages will probably also lower the quality of link anchors.",1,ad,True
267,"In summary, link density does not explain the effectiveness of anchor text as even a small number of links lead to improved performance. Intra-server links are at least as effective as inter-server links because they cover a larger part of the collection. The size of the collection plays a larger role than link density, because it has a larger impact on the number of pages with anchor text and the quality of their anchor text representation. The effectiveness of anchor text rapidly increases as we expand the collection. We now look at how these findings hold up in a more Web-oriented search task.",1,ad,True
268,5. DIVERSITY TASK,0,,False
269,"We now look at the effectiveness of full-text and anchor text approaches for result diversity, where the task is to present a diverse set of results in the top 10 or 20 results. The Diversity task uses the same topics, but breaks them down into a number of informational and navigational sub-topics. This allows a deeper analysis of the various strengths and weaknesses of full-text and anchor text approaches. The task thus combines both ad hoc search and entry and named page finding. Given the earlier successes with anchor text for the latter tasks [9, 29], and its good performance on ad hoc",1,ad,True
270,alpha-nDCG@10 alpha-nDCG@10,0,,False
271,Table 5: Ad hoc and Diversity evaluation using the Diversity,1,hoc,True
272,"relevance judgements. Significance tests are with respect to the full text run, confidence levels are 0.95 (), 0.99 (·) and 0.999 (·)",0,,False
273,Run,0,,False
274,-nDCG@10 nDCG@10 IA-P@10 P@10,0,,False
275,T ext Anchor T ext + Anchor,0,,False
276,0.120 0.257· 0.223·,0,,False
277,0.1564 0.2780· 0.2459·,0,,False
278,0.054 0.1700 0.082 0.2460 0.083· 0.2420·,0,,False
279,uogTrDYCcsB,0,,False
280,0.282,0,,False
281,­,0,,False
282,0.132 ­,0,,False
283,ICTNETDivR3,1,TD,True
284,0.272,0,,False
285,­,0,,False
286,0.095 ­,0,,False
287,UamsDancTFb1,0,,False
288,0.250,0,,False
289,­,0,,False
290,0.079 ­,0,,False
291,0.3,0,,False
292,0.3,0,,False
293,Text,0,,False
294,Text,0,,False
295,Anchor,0,,False
296,Anchor,0,,False
297,0.25,0,,False
298,Mix,0,,False
299,0.25,0,,False
300,Mix,0,,False
301,0.2,0,,False
302,0.2,0,,False
303,0.15,0,,False
304,0.15,0,,False
305,0.1,0,,False
306,0.1,0,,False
307,0.05,0,,False
308,0.05,0,,False
309,0 0 10 20 30 40 50 60 70 80 90 100,0,,False
310,Percentage sampled of collection,0,,False
311,0 0 10 20 30 40 50 60 70 80 90 100,0,,False
312,Percentage sampled of collection,0,,False
313,"Figure 5: Impact of link sampling (left) and page sampling (right) on diversity of full-text, anchor text and mixture runs.",0,,False
314,"precision described in the previous sections, we conjecture anchor text to perform better than full-text on the diversity task as well.",0,,False
315,"Anchor text shares characteristics with queries [11]. A wellknown problem with IR research is the fact that different users can type the same query but have different information needs. Thus, a query can and will be used to search for different types of information, or different aspects or facets of a topic. The same might hold for anchor text. Two Web page authors can use the same anchor text to link to different target pages, covering different topics or different aspects of the same topic.The Diversity relevance judgements are based on a pool of top 20 results of all official runs and were made independently from the ad hoc judgements, thus provide a sanity check on our findings from the ad hoc experiments.",1,ad,True
316,"We first compare the performance of the anchor text and full-text runs on the full collection in Table 5. The official measures are nDCG@10 and IA-P@10. The differences are very clear. Anchor text performs significantly better than full-text search. In the diversity score, the ad hoc relevance ranking plays an important role. If the relevance ranking is low, that is, there are many irrelevant documents in the top ranks, the diversity score will therefore be low as well. Therefore, we turned the diversity qrels into standard TREC qrels by making a page relevant for a topic if it is relevant for at least one sub-topic. This allows us to compare the diversity specific measures with their non-diversity counterparts (columns 3 and 5 in Table 5). We see very similar patterns for -nDCG@10 and normal nDCG@10. The same holds for IA-P@10 and P@10. These results suggest the anchor text scores much better on the diversity measures simply because it has a better underlying relevance ranking, supporting our findings in the previous section.",1,ad,True
317,"What is the impact of link density and collection size on the diversity of anchor text results? We use the same indexes and runs as described in the previous section. The results are shown in Figure 5. Sampling links (left figure) has a similar effect on diversity as on ad hoc search. Again, we see that anchor text becomes more effective with more links, but the improvements become smaller beyond 25% of the links. However, even with 3.125% of the links, the anchor text run is at least as good as the full-text run. Again, link density seems to matter only at very low densities. We looked",1,ad,True
318,127,0,,False
319,StatMAP MPC(30),1,MAP,True
320,MRR,0,,False
321,0.4 Text,0,,False
322,Anchor,0,,False
323,0.35,0,,False
324,Mix,0,,False
325,0.3,0,,False
326,0.25,0,,False
327,0.2,0,,False
328,0.15,0,,False
329,0.1,0,,False
330,0.05,0,,False
331,0 0 10 20 30 40 50 60 70 80 90 100,0,,False
332,Percentage sampled of collection,0,,False
333,0.6 Text,0,,False
334,Anchor,0,,False
335,0.5,0,,False
336,Mix,0,,False
337,0.4,0,,False
338,0.3,0,,False
339,0.2,0,,False
340,0.1,0,,False
341,0 0 10 20 30 40 50 60 70 80 90 100,0,,False
342,Percentage sampled of collection,0,,False
343,0.7 Text,0,,False
344,Anchor,0,,False
345,0.6,0,,False
346,Mix,0,,False
347,0.5,0,,False
348,0.4,0,,False
349,0.3,0,,False
350,0.2,0,,False
351,0.1,0,,False
352,0 0 10 20 30 40 50 60 70 80 90 100,0,,False
353,Percentage sampled of collection,0,,False
354,"Figure 4: Impact of page sampling on effectiveness of full-text, anchor text and mixture runs.",0,,False
355,Table 6: Impact of page sampling on diversity of the TREC,1,TREC,True
356,2009 Diversity topics,0,,False
357,Qrels,0,,False
358,Found in top 10,0,,False
359,Sub-topics Full-text Anchor,0,,False
360,Percentage Topics Inf. Nav. Avg. Inf. Nav. Inf. Nav.,0,,False
361,100.000,0,,False
362,50 168 31 3.98 40 2 63 8,0,,False
363,50.000,0,,False
364,50 145 20 3.30 42 2 47 7,0,,False
365,25.000,0,,False
366,48 126 15 2.94 53 2 32 3,0,,False
367,12.500,0,,False
368,45 103 9 2.49 44 2 22 2,0,,False
369,6.250,0,,False
370,41 78 5 2.02 42 1 14 1,0,,False
371,3.125,0,,False
372,37 62 4 1.78 38 2 11 2,0,,False
373,"at the impact of inter- and intra-server links and found that interserver links find more relevant pages for navigational topics than intra-server links, while the reverse is true for informational topics. Inter-server links tend to point to entry pages of sites, while intraserver links cover a much larger part of sites, which is in line with earlier studies [20]. In other words, for ad hoc search, intra-server links are at least as important as inter-server links.",1,ad,True
374,"On the right side of Figure 5 we see the impact of sampling pages on the effectiveness of anchor text and full-text. We only evaluate on the topics that have at least one relevant page in the sampled collections. Similar to the impact on the ad hoc performance of anchor text, the size of the collection plays a huge role. Performance drops as the collection becomes smaller, although the drop from 100% to 50% of the collection is less severe than for the ad hoc task. Oddly enough, the full-text run gets better with smaller collections. How can these observations be explained?",1,ad,True
375,"The impact of page sampling on the relevance judgements for the Diversity task are shown in Table 6. The number of informational sub-topics with at least one relevant document drops from 168 to 62, while the number of navigational sub-topics drops from 31 to 4. At the smallest samples, the Diversity relevance judgements have almost been reduced to ad hoc judgements. With only 1.78 subtopics per topic, there is not much to diversify and the incurred penalty for retrieving pages on the same sub-topic is small.",1,ad,True
376,"What is the impact of anchor text on informational and navigational topics? The final four columns show the number of subtopics for which the full-text and anchor text runs find relevant pages in the top 10. The diversity of the full-text run is hardly affected by the sample size, explaining why at smaller samples with less sub-topics available, its score goes up. The Anchor run finds both more informational and navigational topics in the full collection, showing it actually does better on the informational part of the topics than the Text run. However, it suffers greatly from the reduced collection size, especially for the informational sub-topics. Collection size plays a large role in the effectiveness of anchor text for informational search.",0,,False
377,6. CONCLUSIONS,0,,False
378,"The history of scientific benchmarking for Web IR is plagued with the apparent contradiction between the experiences of Internet search engines, and the results of experiments at TREC [15, 16, 19, 33]. This led to Google's Larry Page calling the entire formal evaluation process ""irrelevant"" during a heated panel debate at the 2000 Infornotics Search Engine Meeting [31]. After several years of disappointing results at TREC Web Tracks, it was surmised that Web structure is simply not effective for ad hoc search tasks. TREC moved on to Web-centric tasks, where link topology, anchor text, and URL structure were proven very effective for navigational search, such as site finding and home page finding.",1,ad,True
379,"The availability of a new test collection, ClueWeb09 [7], which is a much closer approximation of the index of Internet Search Engines than earlier collections, prompts us to revisit the standing question of the importance of anchor text for ad hoc search. Our main finding is that in contrast with earlier results, the anchor text leads to significant improvements in retrieval effectiveness for ad hoc informational search. More specifically, the pure anchor text runs lead to substantially higher precision than the full-text runs but the full-text runs have better recall. The straightforward combination of document and anchor text runs leads to significantly better scores throughout.",1,ClueWeb,True
380,"A negative finding is that link evidence like in-degree does not contribute to retrieval effectiveness. In contrast with the anchor text, link degree is ignorant of the query at hand, and its main use is in separating the authoritative or important pages from the less popular ones. An underlying difference with earlier collections is that the ClueWeb crawl is based on a PageRank/OPIC policy rather than the standard breadth-first strategy [12]. As a result, all pages in the collection have a relatively high level of `importance,' and on top of that there is no additional value in link degrees.",1,ClueWeb,True
381,"The main focus of this paper is what makes the anchor text representation effective. Previous research pointed at the need of high (inter-server) link density [e.g., 4]. Our finding is that link density has little impact on anchor text effectiveness. Anchor text proved remarkably robust, and even with a small number of links it is effective for high early precision. Contrary to expectations, we find that intra-server link anchors are at least as effective as inter-server link anchors, even at equal density. Within a collection of high quality pages, such as the first tier of Web search engine indexes, the qualitative difference between inter- and intra-server links is minimal. But the greater quantity of intra-server links makes them more effective than inter-server links. Another factor is crawl or collection size, and ClueWeb09 is substantially larger than earlier testbeds. Our finding is that collection size has a big impact on the anchor text representations, affecting quantity, quality and effectiveness. A larger collection has more anchor-text covering a larger part of the collection. Especially within a crawl of high quality pages, more",1,ClueWeb,True
382,128,0,,False
383,"links mean more high quality anchor text, leading to higher early precision than full-text search.",1,ad,True
384,"We also looked at the diversity task, which is a more Web-centric search task and does away with the notion that information is relevant no matter how often the user has seen it. The diversity task used the same topics and collection, but different judgments and measures. Our finding is that anchor text significantly outperforms full-text search, with greater differences and significance than for the ad hoc search task. This result also broadly confirms our findings for the ad hoc task. Anchor text is effective even at low link density; however, on smaller collections, the anchor-text covers an increasingly small part of the collection and loses its power. Fulltext search is less affected by collection size.",1,ad,True
385,"Perhaps the main contribution of this paper is that it solves the apparent contradiction between the experiences of Internet search engines, and the results of experiments at TREC. Negative results for ad hoc informational search using Web structure have tainted the reputation of reproducible IR evaluation. The positive results in this paper may help to set the record straight. This turns, the earlier negative results into something positive in a sense: they aid to our understanding of when and why link evidence works, and when not.",1,ad,True
386,Acknowledgments,0,,False
387,"This work was generously supported by the Netherlands Organization for Scientific Research (NWO, grants # 612.066.513, 639.072.601, and 640.001.501).",0,,False
388,We will make the link anchors and samples available upon request. For details see: http://staff.science.uva.nl/ ~kamps/museum/anchors.,0,,False
389,REFERENCES,0,,False
390,"[1] S. Abiteboul, M. Preda, and G. Cobena. Adaptive on-line page importance computation. In WWW, pages 280­290, 2003.",0,,False
391,"[2] B. Amento, L. Terveen, and W. Hill. Does `authority' mean quality? predicting expert quality ratings of web documents. In SIGIR, pages 296­303. ACM, 2000.",0,,False
392,"[3] R. A. Baeza-Yates, C. Castillo, M. Marín, and A. Rodríguez. Crawling a country: better strategies than breadth-first for web page ordering. In WWW, pages 864­872. ACM, 2005.",1,ad,True
393,"[4] P. Bailey, N. Craswell, and D. Hawking. Engineering a multipurpose test collection for web retrieval experiments. Inf. Process. Manage., 39(6):853­871, 2003.",0,,False
394,"[5] J. Callan, C. Yoo, and L. Zhao. Web08-PR Dataset, 2008. Project planning document.",0,,False
395,"[6] C. A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 Web Track. In TREC 2009.",1,TREC,True
396,"[7] CMU-LTI. The ClueWeb09 Dataset, 2009. URL http:// boston.lti.cs.cmu.edu/Data/clueweb09/.",1,ClueWeb,True
397,"[8] N. Craswell, P. Bailey, and D. Hawking. Is it fair to evaluate Web systems using TREC ad hoc methods? In ACM SIGIR Workshop on Evaluation of Web Document Retrieval, 1999.",1,TREC,True
398,"[9] N. Craswell, D. Hawking, and S. E. Robertson. Effective site finding using link anchor information. In SIGIR, pages 250­ 257, 2001.",0,,False
399,"[10] N. Craswell, D. Fetterly, M. Najork, S. Robertson, and E. Yilmaz. Microsoft Research at TREC 2009. In TREC, 2009.",1,TREC,True
400,"[11] N. Eiron and K. S. McCurley. Analysis of anchor text for web search. In SIGIR '03, pages 459­460. ACM, 2003.",0,,False
401,"[12] D. Fetterly, N. Craswell, and V. Vinay. The impact of crawl",0,,False
402,"policy on web search effectiveness. In SIGIR, pages 580­587.",0,,False
403,"ACM, 2009.",0,,False
404,"[13] D. Fetterly, N. Craswell, and V. Vinay. Measuring the search effectiveness of a breadth-first crawl. In ECIR, pages 388­",1,ad,True
405,"399, 2009.",0,,False
406,"[14] C. Gurrin and A. F. Smeaton. Replicating web structure in small-scale test collections. Inf. Retr., 7:239­263, 2004.",0,,False
407,"[15] D. Hawking. Overview of the TREC-9 Web Track. In TREC,",1,TREC,True
408,2000.,0,,False
409,"[16] D. Hawking and N. Craswell. Overview of the TREC-2001 Web Track. In Proceedings of TREC-2001, 2001.",1,TREC,True
410,"[17] D. Hawking and N. Craswell. Very large scale retrieval and web search. In TREC: Experiment and Evaluation in Information Retrieval, chapter 9. MIT Press, 2005.",1,TREC,True
411,"[18] D. Hawking and S. Robertson. On collection size and retrieval effectiveness. Inf. Retr., 6(1):99­105, 2003.",0,,False
412,"[19] D. Hawking, E. M. Voorhees, N. Craswell, and P. Bailey. Overview of the trec-8 web track. In TREC, 1999.",1,trec,True
413,"[20] D. Hawking, F. Crimmins, N. Craswell, and T. Upstill. How",0,,False
414,"valuable is external link evidence when searching enterprise webs? In ADC, pages 77­84, 2004.",0,,False
415,"[21] Indri. Language modeling meets inference networks, 2009. http://www.lemurproject.org/indri/.",0,,False
416,"[22] J. Kamps. Effective smoothing for a terabyte of text. In TREC,",1,TREC,True
417,2005.,0,,False
418,"[23] J. Kamps. Experiments with document and query representations for a terabyte of text. In TREC, 2006.",1,TREC,True
419,"[24] J. Kamps. Web-centric language models. In O. Herzog, H.-",0,,False
420,"J. Schek, N. Fuhr, A. Chowdhury, and W. Teiken, editors, CIKM, pages 307­308. ACM, 2005. ISBN 1-59593-140-6.",0,,False
421,"[25] R. Kaptein, M. Koolen, and J. Kamps. Result diversity and",0,,False
422,"entity ranking experiments: Text, anchors, links, and wikipedia. TREC, 2009.",1,wiki,True
423,"[26] W. Kraaij, T. Westerveld, and D. Hiemstra. The importance of prior probabilities for entry page search. In SIGIR, pages",0,,False
424,"27­34. ACM, 2002.",0,,False
425,"[27] D. Metzler, J. Novak, H. Cui, and S. Reddy. Building enriched",0,,False
426,"document representations using aggregated anchor text. In SIGIR '09, pages 219­226. ACM, 2009.",0,,False
427,"[28] M. A. Najork, H. Zaragoza, and M. J. Taylor. HITS on the Web: How does it compare? In SIGIR '07, pages 471­478.",0,,False
428,"ACM, 2007.",0,,False
429,"[29] P. Ogilvie and J. P. Callan. Combining document representations for known-item search. In SIGIR, pages 143­150, 2003.",0,,False
430,"[30] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank",1,ad,True
431,"citation ranking: Bringing order to the web. Technical report,",0,,False
432,"Stanford Digital Library Technologies Project, 1998.",0,,False
433,[31] C. Sherman. `old economy' info retrieval clashes with,0,,False
434,"`new economy' web upstarts at the fifth annual search engine conference. Information Today Newsbreaks, 2000. http:// web.archive.org/web/20001217211000/www.",0,,False
435,"infotoday.com/newsbreaks/nb000424-2.htm. [32] A. Singhal and M. Kaszkiel. AT&T at TREC-9. In TREC,",1,TREC,True
436,2000.,0,,False
437,"[33] A. Singhal and M. Kaszkiel. A case study in web search using TREC algorithms. In WWW10, pages 708­716, 2001.",1,TREC,True
438,"[34] TREC. Text-REtrieval Conference, 2009. http://trec. nist.gov/.",1,TREC,True
439,"[35] E. Yilmaz and J. A. Aslam. Estimating average precision with incomplete and imperfect judgments. In CIKM '06, pages",0,,False
440,"102­111, New York, NY, USA, 2006. ACM.",0,,False
441,129,0,,False
442,,0,,False

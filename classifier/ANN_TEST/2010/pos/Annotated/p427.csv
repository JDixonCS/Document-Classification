,sentence,label,data,regex
0,A Content based Approach for Discovering Missing Anchor Text for Web Search,0,,False
1,Xing Yi and James Allan,0,,False
2,Center for Intelligent Information Retrieval Computer Science Department,0,,False
3,"University of Massachusetts, Amherst, MA, USA",0,,False
4,"{yixing,allan}@cs.umass.edu",0,,False
5,ABSTRACT,0,,False
6,"Although anchor text provides very useful information for web search, a large portion of web pages have few or no incoming hyperlinks (anchors), which is known as the anchor text sparsity problem. In this paper, we propose a language modeling based technique for overcoming anchor text sparsity by discovering a web page's plausible missing anchor text from its similar web pages' in-link anchor text. We design experiments with two publicly available TREC web corpora (GOV2 and ClueWeb09) to evaluate different approaches for discovering missing anchor text. Experimental results show that our approach can effectively discover plausible missing anchor terms. We then use the web named page finding task in the TREC Terabyte track to explore the utility of missing anchor text information discovered by our approach for helping retrieval. Experimental results show that our approach can statistically significantly improve retrieval performance, compared with several approaches that only use anchor text aggregated over the web graph.",1,TREC,True
7,"Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval­Search process,Retrieval models;H.3.5 [Information Storage and Retrieval]:Online Information Services­ Web-based services",0,,False
8,"General Terms: Algorithms, Experimentation",0,,False
9,"Keywords: anchor text, anchor text sparsity, language models, relevance models, content similarity, web search",0,,False
10,1. INTRODUCTION,1,DUC,True
11,"There are rich dynamic human generated hyperlink structures on the web. Most web pages contain some hyperlinks, referred to as anchors, that point to other pages. Each anchor consists of a destination URL and a short piece of text, which is called anchor text. Anchors play an important role in helping web users conveniently navigate to their interested web information. Although some anchor text only functions as a navigational shortcut which does not have direct semantic relation to the destination URL (e.g.,""click",0,,False
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
13,# of web pages # of pages having inlinks # of pages having original,0,,False
14,or enriched inlinks[14],0,,False
15,GOV2,0,,False
16,"25,205,179 376,121 (1.5%) 977,538 (3.9%)",0,,False
17,ClueWeb09-T09B,1,ClueWeb,True
18,"50,220,423 7,640,585 (15.2%) 19,096,359 (38.0%)",0,,False
19,Table 1: Summary of in-link statistics on two TREC web corpora used in our study.,1,TREC,True
20,"here"" and ""next""), many times anchor text provides succinct description of the destination URL's content, e.g. ""SIGIR 2010(Geneva, Switzerland)"" is from an anchor linked to http://www.sigir2010.org/. Anchor text instances are usually reasonable queries that web users may issue to search for the associated URL and have been used to simulate plausible web queries relevant to the associated web pages in some web search research [15]. Therefore, anchor text is highly useful for bridging the lexical gap between user issued web queries and the relevant web pages. It is arguably the most important piece of evidence used in web ranking functions[14].",0,,False
21,"However, previous research has shown that the distribution of the number of inlinks on the web follows a power law [1], where a small portion of web pages have a large number of inlinks while most have few or no inlinks. Thus, most web pages do not have in-link associated anchor text, a problem originally referred to as the anchor text sparsity problem by Metzler et al. [14]. This problem presents a major obstacle for any web search algorithms that want to use anchor text to improve retrieval effectiveness. Table 1 shows the anchor text sparsity problem in two large TREC1 web corpora (GOV22 and ClueWeb09-T09B3). To address this problem, Metzler et al. [14] proposed aggregating, or propagating, anchor text across the web hyperlink graph so that web pages' lack of anchor text can be enriched with their linked web pages' associated anchor text. Table 1 shows that the number of URLs associated with some anchor text (original or propagated) in the two TREC web corpora is significantly increased by using their linked-based anchor text enrichment approach. Nevertheless, in Table 1 we notice that large portion of web pages still do not have any associated anchor text after having been enriched. This observation motivated us to consider another possible approach, which utilizes the content similarity between web pages, to alleviate anchor text sparsity.",1,TREC,True
22,1http://trec.nist.gov/ 2http://ir.dcs.gla.ac.uk/test_collections/ gov2-summary.htm 3http://boston.lti.cs.cmu.edu/Data/clueweb09/,1,trec,True
23,427,0,,False
24,"Specifically, we hypothesize that the anchor text associated with a web page's inlinks typically has close semantic relations to the web page so that web pages that are similar in content may be pointed to by anchors having similar anchor text. Under this assumption, in this paper we propose a language modeling based technique for discovering a web page's plausible missing in-link anchor text by using its most similar web pages' in-link anchor text. We then test the effectiveness of our approach by using the discovered missing anchor text information for some TREC web search tasks. We find that even on the GOV2 data where a serious anchor text sparsity problem exists as shown in Table 1, our approach can significantly improve retrieval performance. Our content based approach can be combined with the hyperlink based approach to further reduce anchor text sparsity and benefit web search. Our enriched document and anchor text representations can also be used for many other tasks beyond web search, including estimating better document models and extracting advanced textual features for content match and document classification.",1,TREC,True
25,"Our work has four chief contributions: 1) although content similarity has been used widely in other applications, we are the first to propose using web content similarity to address the anchor text sparsity problem. 2) We develop a language modeling based technique, which stems from ideas in one effective retrieval technique ­ relevance based language models [10], to effectively discover plausible missing anchor text information and use it for retrieval. 3) We empirically show that our approach performs better than Metzler et al.'s linked-based approach [14] in terms of discovering plausible missing anchor terms in two standard large TREC web corpora. 4) We show that our approach statistically significantly improves retrieval effectiveness, compared with several approaches that only use aggregated anchor text over the web graph, in the web named page finding task of the TREC Terabyte track [4].",1,ad,True
26,"We begin by reviewing related work in §2. Next, we describe different approaches of discovering missing anchor text to enrich document representations in §3. Then we describe the experimental setup and results of evaluating different approaches for anchor text discovery in §4. After that, we present how to use discovered anchor text information for retrieval in a language modeling approach and report the experimental results in §5. We conclude in §6.",0,,False
27,2. RELATED WORK,0,,False
28,"Metzler et al. [14] first directly addressed the anchor text sparsity problem by using the web hyperlink graph and propagating anchor text over the web graph. Our work also addresses the same problem but using a different approach, which is based on the content similarity between web pages. Our approach is similar in nature to other similarity based techniques, such as cluster-based smoothing from the language modeling framework[8, 9, 11], except we focus on enriching web documents' anchor text representation by using their similar documents' associated anchor text.",1,ad,True
29,"Anchor text can be modeled in many different ways. Westerveld et al. [20] and Nallapati et al. [15] model anchor text in the language modeling approach [17] and calculate an associated anchor language model to update the original document model for retrieval. Fujii [6] further considers differently weighting each line of anchor text associated with the same page thus obtaining a more robust anchor language model. Here, we also adopt the language modeling approach",1,ad,True
30,"but focus on discovering a plausible associated anchor language model for web pages with no or few inlinks. Our approach can be easily used together with any language model based retrieval model (e.g., Ogilvie and Callan's model [16]) that takes document structure into account.",0,,False
31,"Our approach of overcoming anchor text sparsity stems from ideas in the relevance based language models(RMs), proposed by Lavrenko and Croft [10]. Their original work introduces the RMs to find plausible useful terms missing in the original query for query expansion. Here we adapt the RMs to compute a web content dependent associated anchor language model for discovering missing anchor terms and using anchor text for retrieval. Thus, our approach, although similar in spirit to, differs from document expansion [18] and graph-based document smoothing[13].",1,ad,True
32,3. DISCOVERING MISSING ANCHOR TEXT,0,,False
33,We now describe three different approaches for discovering plausible missing anchor text for web pages with few or no inlinks. The goal of each is to produce a ranked list of plausible anchor text terms for a page.,0,,False
34,3.1 Aggregating Anchor Text,0,,False
35,"To overcome anchor text sparsity, Metzler et al.[14] originally proposed to augment web pages with auxiliary anchor text (denoted as ) that is derived by aggregating anchor text over the web graph. We first briefly review the procedure they have used to build , which is very important for our discussions and comparisons in this research. Given a web page 0's URL 0 , the procedure first collects all pages (0), within the same site (domain), that link to 0 . These links are known as 0 's internal inlinks. Then the procedure collects all anchor text  from pages (denoted as (0)) that are linked to any page in (0) from outside the site. The anchor text set  is known as external anchor text and is used as  for 0 .",0,,False
36,"Figure 1 illustrates the procedure by using a real-world example from the TREC GOV2 collection. We collect the auxiliary anchor text  for the page 0. 0's original anchor text (denoted as ), which comes from all pages (denoted as (0)) that are directly linked to 0 from outside the site, consists of lines including ""Optima National Wildlife Refuge"" and ""Optima NWR"". 0's  consists of lines including ""Oklahoma Refuge Websites"" and ""Oklahoma National Wildlife Refuges"".",1,TREC,True
37,"Note that the above procedure does not use any anchor text associated with internal inlinks, because internal inlinks are typically generated by the owner of the site for navigational purposes and their associating anchor text tends to be navigational in nature (e.g., ""home"",""next page"", etc.; refer to [14] for more discussions on this issue). We emphasize that in the remainder part of this paper we follow Metzler et al. and do not use the anchor text associated with internal inlinks in any way.",0,,False
38,"In this paper we are specifically interested in the effectiveness of using  to serve as a surrogate for possibly missing original anchor text. In other words, we consider how effectively we may use  to discover plausibly missing original anchor text of the URL of the interest so that anchor text sparsity can be effectively reduced. Therefore, we focus on the discovered anchor terms themselves in the . We use two typical methods to rank the relative importance of each anchor term . The first method, denoted as AUX-TF, is to use each term 's term frequency () in the .",0,,False
39,428,0,,False
40,http://saltplains.fws.gov/index.html,0,,False
41,Oklahoma Refuge Websites,0,,False
42,... P5,0,,False
43,auxiliary anchortext (aggregated),0,,False
44,Pages within,0,,False
45,P2,0,,False
46,the same site,0,,False
47,P1 P0,0,,False
48,http://saltplains.fws.gov/just4kid s.html,0,,False
49,Oklahoma National Wildlife Refuges,0,,False
50,P6,0,,False
51,http://ifw2irm2.irm1.r2.fws.gov/texas.html,0,,False
52,Buffalo Lake NWR,0,,False
53,P7,0,,False
54,Similar Pages:,0,,False
55,P3,0,,False
56,P4,0,,False
57,Optima National Wildlife Refuge,0,,False
58,P8,0,,False
59,original anchortext,0,,False
60,..... Optima NWR,0,,False
61,..... P9,0,,False
62,"PIn(P0),""{P1, P2} POrig (P0)"",""{P8, P9} PAux (P0)"",""{P5, P6} POrig (P4)"",{P7}",0,,False
63,http://ifw2es.fws.gov/Oklahoma/refuges.htm l http://ifw2irm2.irm1.r2.fws.gov/toklahoma.html,0,,False
64,P0: http://southwest.fws.gov/refuges/oklahoma/optima.ht ml. P1 :http://southwest.fws.gov/oklahoma.html . P2: http://southwest.fws.gov/refuges/okrefuges.html . P4 : http://southwest.fws.gov/refuges/texas/buffalo.html .,0,,False
65,"Figure 1: Illustration of how to aggregate anchor text over the web graph or use similar web pages' anchor text for discovering more anchor text for a web page (0 in this example). The page 0 is a GOV2 web page, whose DocID is GX010-01-9459902 and URL is http://southwest.fws.gov/refuges/oklahoma/optima.html.",0,,False
66,"The second method, denoted as AUX-TFIDF, is to use each term 's    () score, computed by multiplying () with 's  score in the web collection. The quality of the discovered anchor term rank lists produced from these two link based approaches implies the effectiveness of using auxiliary anchor text as a surrogate of missing original anchor text. We will compare these two approaches with our content based approach in §4.",0,,False
67,3.2 Discovering Anchor Text through Finding Similar Web Pages,0,,False
68,"Note that in the link based approach, a web page 0 still",0,,False
69,cannot obtain the auxiliary anchor text if it has no internal,0,,False
70,inlinks or if all pages in its (0) have no external anchor,0,,False
71,"text. Indeed, Metzler et al. reported only 38% anchor text",0,,False
72,sparsity reduction on a web sample with the link based ap-,0,,False
73,"proach[14]. Therefore, we propose a content based approach,",0,,False
74,which does not have specific link structure requirements on,0,,False
75,"the target web page, to discover its plausible missing an-",0,,False
76,"chor text. Intuitively, our approach assumes that web pages",0,,False
77,that are similar in content may be described by similar as-,0,,False
78,"sociated anchor text. For example, in Figure 1, the target",0,,False
79,"page 0, which is about Optima national wildlife refuge, is",0,,False
80,"similar in content with the page 4, which is about Buffalo",0,,False
81,Lake national wildlife refuge. We observe that the anchor,0,,False
82,"term ""NWR"", which appears in 0's and 4's  but not",0,,False
83,"in 0's , can be used to partially describe both 0 and",0,,False
84,4 although two pages are concerned about different places.,0,,False
85,We consider a language modeling approach to better use,0,,False
86,"document similarity and anchor text information, based on",0,,False
87,ideas from the relevance-based language models (RM)[10].,0,,False
88,"In brief, given a query , RM first calculates the posterior",0,,False
89,() of each document  in the collection  generating,0,,False
90,"the query , then calculates a query dependent language",0,,False
91,model ():,0,,False
92,"() ,",0,,False
93,"() × (),",0,,False
94,(1),0,,False
95,"where  is a word from the vocabulary  of . Similarly, given an target page 0, our approach aims to calculate a relevant anchor text language model (RALM) (0) by:",1,LM,True
96,"(0) ,",0,,False
97,"() × (0),",0,,False
98,(2),0,,False
99,"where  denotes the complete original anchor text that should be associated with  but may be missing,  denotes the complete original anchor text space for all pages, () is a multinomial distribution over the anchor text vocabulary . To compute (0) in Equation 2 where 0 and  information may be missing, we view each page 's content as its anchor text 's context and use 's document language model  , {()} as 's contextual model. Then we can calculate a translation model (0) by using 0 and 's contextual models and use (0) to approximate (0). This contextual translation approach is also used in Wang and Zhai's work [19].",0,,False
100,"When calculating a page 's document language model {()}, we employ Dirichlet smoothing on the maximum likelihood (ML) estimate of observing a word  in the page (()) with the word's collection probability ():",0,,False
101,(),0,,False
102,",",0,,False
103,  +,0,,False
104,(,0,,False
105,),0,,False
106,+,0,,False
107," (),  + ",0,,False
108,(3),0,,False
109,"where  is the length of 's content and  is the Dirichlet smoothing parameter ( , 2500 in our experiments). Then",0,,False
110,"given two pages 0 and , we use the Kullback-Leibler divergence (KL) () between their document models",0,,False
111,0 and  to measure their similarity and view that as the contextual similarity between the associated anchor text 0 and . Then the contextually based translation probability (0) is calculated by:,0,,False
112,(0),0,,False
113,",",0,,False
114,. exp(-(0  )),0,,False
115, exp(-(0)),0,,False
116,(4),0,,False
117,429,0,,False
118,This (0) is then used to approximate (0) in Equation 2 to get:,0,,False
119,(0) ,0,,False
120,() × (0).,0,,False
121,(5),0,,False
122,A few transformations of Equation 4 can obtain:,0,,False
123,"(0)   ()(0),",0,,False
124,(6),0,,False
125,"which is the likelihood of generating 0's context 0 from 's context 's smoothed language model and being normalized by 0's context length. This likelihood can be easily obtained by issuing 0 as a long query to any language model based search engine. In addition, we use the observed incomplete original anchor text language model () associated with  to approximate () in Equation 5, and let () ,"" 0 if  has no . In this way, the RALM (0) can be computed.""",1,ad,True
126,"In practice, for efficiency the RALM of the target page 0 is computed from 0's top- most similar pages'  (original anchor text) because (0) in Equation 4 is very small for the other pages. Due to the anchor text sparsity, we set  ,"" 2000 in our experiments. Because some of these similar pages do not have associated , we use another parameter  to denote the number of most similar pages whose associated original anchor text is not missing and contributes information in the RALM, and we tune  in the experiments. Intuitively, increasing  can increase the number of anchor text samples to better estimate RALM but may also introduce more noise when the sample size is large.""",1,LM,True
127,"The probability (0) of an anchor term  in the RALM directly reflects the goodness of the term  used as original anchor text for the page 0, thus we use the anchor terms that have the largest probabilities (0) in the RALM to evaluate the effectiveness of our content based approach. Theoretically our approach can associate any web page with some anchor term distribution information if there is some anchor text in the corpus, thus it can further reduce the anchor text sparsity.",1,LM,True
128,3.3 Using Keywords as Anchor Text,0,,False
129,The keyword based approaches come from the intuition,0,,False
130,that important keywords in a web page may be good de-,0,,False
131,"scription terms for the page, thus may be arguably used as",0,,False
132,anchor text. We use three typical term weighting schemes,0,,False
133,to identify the keywords and rank the words in a web page's,0,,False
134,"content. The first method, denoted as DOC-TF, uses each",0,,False
135,"word 's term frequency 0 () in the page 0 for term weighting. The second method, denoted as DOC-TFIDF,",0,,False
136,"uses each word 's 0   () score, computed by multiplying 0 () with 's  score in the web collection. The third method, denoted as DOC-OKAPI, uses each word",1,AP,True
137,"'s Okapi BM25 score  250 (), computed by:",0,,False
138," 25() ,",0,,False
139,0 ()(1+1),0,,False
140,0,0,,False
141,()+1,0,,False
142,(1-+,0,,False
143,0  ,0,,False
144,"  (),",0,,False
145,(7),0,,False
146,"where  is the average document length of the pages in the collection. We use the typical setting 1 ,"" 2,  "", 0.75 in Equation 7 in our experiments.",0,,False
147,The top ranked terms in a page 0 by three methods are used as the possible missing original anchor terms for 0. We will use three keyword based methods as baselines in §4.,0,,False
148,4. EVALUATING DISCOVERY,0,,False
149,"We now compare the capability of discovering missing anchor text by different approaches described in §3, including two link based approaches (AUX-TF and AUX-TFIDF), our content based approach (RALM), and three keyword based approaches (DOC-TF, DOC-TFIDF and DOC-OKAPI).",1,LM,True
150,4.1 Data and Methodology,0,,False
151,"We use two publicly available large TREC web collections (GOV2 and ClueWeb09-T09B). GOV2 is a standard TREC web collection [4] crawled from government web sites during early 2004. The ClueWeb09 collection is a much larger and more recent web crawl, which contains over 1 billion pages. ClueWeb09-T09B is a subset of ClueWeb09 and contains about 50 million English web pages. Compared with GOV2 crawled only from the gov domain, ClueWeb09T09B is crawled from the general web thus is a less biased web sample; in another aspect, GOV2 contains relatively high quality government web pages thus having less noise than ClueWeb09-T09B. Thus we use both GOV2 and ClueWeb09-T09B in our experiments to show how different approaches perform in web collections that have different characteristics. The Indri Search Engine4 was used to index both collections by removing a standard list of 418 INQUERY [2] stopwords and applying Krovetz stemmer. In a separate process, we run Indri Search Engine's harvestlinks utility on the two collections to collect web page inlinks and raw anchor text information where we do not perform stopping or stemming.",1,TREC,True
152,"To evaluate the quality of discovered anchor text for a web page 0, we utilize the original anchor text  associated with all inlinks of 0. Specifically, we first hide the page 0's , apply different anchor text discovery approaches on 0, then compare the discovered anchor text with 0's . This procedure can be run automatically so that we can leverage large volumes of web pages to evaluate the performance of different approaches with no human labeling effort. More specifically, we consider each anchor term in a page 0's  as a good description term, or a relevant term, for 0 while terms not in  as non-relevant ones; in this way, we can generate term relevance judgments for 0. Then we employ each different approach to discover a ranked list of plausible missing anchor terms for 0 and then use the relevant judgments to evaluate the ranked anchor term list. Note that for fair comparison 0's  is not used in Equation 2 for calculating RALM in our approach. In the experiments, we perform slight stopping on the raw anchor text by removing a short list of 39 stopwords, which includes 25 common stopwords[12, pp.26] and 14 additional anchor terms5 that are either common navigational purposed words or part of URLs ­ it is common that anchor text contains some URL.",1,LM,True
153,"We calculate some typical TREC style evaluation measurements including Mean Average Precision (MAP), Mean Reciprocal Rank(MRR), Precision at the number of relevant terms(R-Prec), Precision at  (P@) and also normalized discounted cumulative gain (NDCG) [7]. In the experiments, we are specifically interested in the quality of top ranked discovered anchor terms; thus, we only use the",1,TREC,True
154,"4http://www.lemurproject.org/indri/ 5The additional terms are: http, https, www, gov, com, org, edu, net, html, htm, click, here, next, home.",1,ad,True
155,430,0,,False
156,DOC-TF DOC-TFIDF DOC-OKAPI,1,AP,True
157,AUX-TF AUX-TFIDF,0,,False
158,RALM,1,LM,True
159,MAP 0.3162 0.2936 0.2936 0.1969 0.1716 0.3183,1,MAP,True
160,NDCG 0.4585 0.4348 0.4348 0.2598 0.2423 0.4275,0,,False
161,MRR 0.5441 0.5400 0.5400 0.3707 0.3442 0.5050,0,,False
162,P@2 0.3833 0.3700 0.3700 0.2833 0.2433 0.3467,0,,False
163,P@5 0.2800 0.2613 0.2613 0.1773 0.1720 0.2840,0,,False
164,P@10 0.2060 0.1827 0.1827 0.1153 0.1140 0.1860,0,,False
165,P@20 0.1333 0.1240 0.1240 0.0643 0.0647 0.1140,0,,False
166,R-Prec 0.2716 0.2530 0.2530 0.1643 0.1428 0.3051,0,,False
167,Discovered Rel. 400 372 372 193 194 342,0,,False
168,"Table 2: Performances on the GOV2 collection. There are 708 relevant anchor terms overall. Column 10 shows overall relevant anchor terms discovered by each different approach. RALM performs statistically significantly better than AUX-TF and AUX-TFIDF by each measurement in columns 2­9 according to the one-sided t-test ( < 0.005). There exists no statistically significant difference between each pair of RALM, DOC-TF, DOC-TFIDF and DOC-OKAPI by each measurement according to the one-sided t-test ( < 0.05).",1,LM,True
169,DOC-TF DOC-TFIDF DOC-OKAPI,1,AP,True
170,AUX-TF AUX-TFIDF,0,,False
171,RALM,1,LM,True
172,MAP 0.3517 0.3107 0.3107 0.1840 0.1634 0.2612,1,MAP,True
173,NDCG 0.4891 0.4388 0.4388 0.2507 0.2347 0.3615,0,,False
174,MRR 0.5588 0.5145 0.5145 0.3309 0.3116 0.4630,0,,False
175,P@2 0.3467 0.3133 0.3133 0.2248 0.2047 0.2833,0,,False
176,P@5 0.2373 0.2213 0.2213 0.1463 0.1383 0.1733,0,,False
177,P@10 0.1360 0.1173 0.1173 0.0729 0.0676 0.0911,0,,False
178,P@20 0.1090 0.0983 0.0983 0.0577 0.0560 0.0770,0,,False
179,R-Prec 0.2990 0.2608 0.2608 0.1675 0.1402 0.2398,0,,False
180,Discovered Rel. 327 295 295 172 167 231,0,,False
181,Table 3: Performances on the ClueWeb09-T09B collection. There are 582 relevant anchor terms overall. Column 10 shows overall relevant anchor terms discovered by each different approach. DOC-TF performs statistically significantly better than both RALM and AUX-TF by each measurement in columns 2­9 according to the one-sided t-test ( < 0.05). RALM performs statistically significantly better than AUX-TF and AUX-TFIDF by each measurement in columns 2­9 according to the one-sided t-test ( < 0.05).,1,ClueWeb,True
182,top-20 terms in the discovered term rank lists by different approaches to calculate the measurements.,0,,False
183,"Note that web pages that can be used in our evaluation procedure need to satisfy two requirements: (1) they need to have some associated  and (2) they can collect some auxiliary anchor text from the web graph as described in §3.1. Thus, for each of two collections, we randomly sample 150 pages satisfying the two requirements for training and another 150 pages for testing. On both training sets, RALM's parameter  , 15 described in §3.2 achieves the highest MAPs.",1,LM,True
184,4.2 Results and Analysis,0,,False
185,"The performance of discovering original anchor text by different approaches on the testing set of GOV2 and ClueWeb09-T09B are shown in Table 2 and Table 3, respectively. The results show that our approach (RALM) can effectively discover missing original anchor terms. On both collections RALM performs statistically significantly better than two link based approaches (AUX-TF and AUX-TFIDF). This indicates that, for discovering a page's missing anchor text, the anchor text associated with the similar pages provides more useful information than that associated with the linked web neighbors. The numbers of discovered relevant anchor terms by different approaches, shown in the last column of two tables, also indicate that only using auxiliary anchor text misses more original anchor text information than our content based approach.",1,ClueWeb,True
186,Another observation is that RALM performs worse on ClueWeb09-T09B and not statistically significantly better on GOV2 than the keyword based approaches. This indicates that words having high IR utility like  or    scores are often good description terms for the page and used by human being as the anchor text. Removing a long list of stopwords from web page content has also helped the keyword based approaches to effectively select good descrip-,1,LM,True
187,"(AUX-TF, DOC-TF) (AUX-TF, RALM) (RALM, DOC-TF)",1,LM,True
188,GOV2 30.5% 47.6% 26.0%,0,,False
189,ClueWeb09-T09B 26.0% 46.3% 22.3%,1,ClueWeb,True
190,"Table 4: The average percentage (,  ) of the terms discovered by the  approach appearing in the ones discovered by the  approach.",0,,False
191,"tion words from the web content. One plausible reason that RALM performs relatively poorly on ClueWeb09-T09B is that, compared with the high quality GOV2 pages, ClueWeb pages are crawled from the general web, where the inlinks and anchor text may be generated in a more noisy way (e.g. spam), degrading RALM's performance. To better understand the performance of different approaches, in Table 5 and Table 6 we show the top-10 words of the anchor term rank lists discovered by different approaches for one evaluation web page in GOV2 and ClueWeb09-T09B, respectively.",1,LM,True
192,"Although using keyword information can discover some good anchor terms, the content-generated anchor terms do not help bridging the lexical gap between a web page and varied queries that attempt to search the page. Indeed, human generated anchor text is highly useful for reducing the word mismatch problem because the lexical gap between anchor text and queries is relatively small[14]. Here, we do some lexical gap analysis to show that our approach can also discover anchor terms similar in nature to human-generated ones but different from content-generated ones.",0,,False
193,"For each web page  in the testing set, we calculate the percentage (,  ) of the terms discovered by the  approach also appearing in the ones discovered by the  approach, then compute the average percent (,  ) with all the pages. We use the outputs from the keyword based DOC-TF, the link based AUX-TF, and the RALM in this analysis. Table 4 shows three average percentages (,  )",1,LM,True
194,431,0,,False
195,"which we have specific interest in. We observe that AUXTF's discovered terms have much higher average per query overlap ratio with RALM's than with DOC-TF's. Moreover, RALM's discovered anchor terms have small overlap with DOC-TF's.",1,LM,True
196,5. USING DISCOVERED ANCHOR TEXT FOR WEB SEARCH,0,,False
197,"We now describe how we use the discovered anchor text by different approaches for retrieval in a language modeling approach [17]. We point out that our focus here is not to evaluate different schemes to aggregate or combine anchor text [14]; instead, we focus on comparing the utility of RALM and auxiliary anchor text for helping retrieval.",1,ad,True
198,5.1 Retrieval Models,0,,False
199,We follow the typical language modeling based retrieval,0,,False
200,approach[17] and score each web page  for a query  by the,0,,False
201,likelihood of the page  's document language model ( ),0,,False
202,generating the query :,0,,False
203,"( ) , ( ).",0,,False
204,(8),0,,False
205,"When using Dirichlet smoothing, the document language model ( ) can be calculated by Equation 3 and then used in Equation 8 for retrieval. We call this baseline QL. We only fix  ,"" 2500 in Equation 3 for the document models used to calculate RALM, but tune the  for QL to achieve the best retrieval performance in our experiments in §5.2.""",1,LM,True
206,"We follow the mixture model approach [15, 16] to use the discovered anchor text information for helping retrieval. In this approach, a web page  's document language model is assumed to be a mixture of multiple component distributions where each component is associated with a prior probability, or a mixture weight. Therefore, we can estimate a language model () from anchor text discovered by each different approach for the page  and use () as a component of  's document model thus obtaining a better document language model ~( ):",0,,False
207,"~( ) ,"" ( ) + (1 - )(),""",0,,False
208,(9),0,,False
209,where ( ) is the original smoothed document model in the QL baseline. Then we can plug ~( ) into equation 8 for retrieval. We compare the retrieval performance of document language models updated by different discovered anchor text information.,0,,False
210,"We consider three different anchor text sources to update a web page  's document model: (1) the observed original anchor text  associated with  , (2) the auxiliary anchor text  of  , and (3) the RALM computed by our approach for  . We estimate the anchor text language model () and () by using the ML estimate of observing each word  in  and , respectively. Here, we design the following five retrieval methods that use the above three anchor text sources: 1. M-ORG, which only uses the observed original anchor text language (). 2. M-AUX, which only uses the auxiliary anchor text language (). 3. M-ORG-AUX, which uses both () and () to update the document model ( ) by:",1,LM,True
211,"~( ) , (( ) + (1 - )()) +(1 - )().",0,,False
212,(10),0,,False
213,QL M-ORG M-AUX M-ORG-AUX M-RALM M-ORG-RALM,1,LM,True
214,MRR 0.3132 0.3696 0.3187 0.3711 0.3388 0.3975,0,,False
215,%Top10 49.7 57.5 50.8 57.5 53.6 59.7,0,,False
216,Opt. Param.,0,,False
217," , 0.95  , 0.99  ,"" 0.95,  "", 0.99  ,"" 20,  "","" 0.95 ,  "","" 0.95,  "", 20",0,,False
218,Table 7: Retrieval performance of different approaches with TREC 2006 NP queries. The star indicates statistically significant improvement over MRRs of M-ORG and M-ORG-AUX by one-sided t-test ( < 0.05). The triangle indicates statistically significant improvement over MRRs of QL and MAUX by one-sided t-test ( < 0.05).,1,TREC,True
219,"4. M-RALM, which only uses the RALM (0) in Equation 2. The original anchor text of 0 is not used in Equation 2 for calculating RALM. 5. M-ORG-RALM, which uses both () and the RALM (0) in Equation 2 by:",1,LM,True
220,"~( ) , (( ) + (1 - )()) +(1 - )(0).",0,,False
221,(11),0,,False
222,The original anchor text of 0 is not used in Equation 2 for calculating RALM.,1,LM,True
223,"Note that we can update each page's document model offline, thus this computationally expensive procedure has little impact on the online query processing time. Moreover, different from experiments in §4.1, we use all anchor terms instead of the top-20 most important terms discovered by different approaches.",1,ad,True
224,5.2 Experiments,0,,False
225,"We use the TREC web named page finding tasks in Terabyte Track[4, 5] to evaluate the performance of different retrieval methods described in §5.1. The objective of the named page (NP) finding task is to find a particular page in the GOV2 collection, given a topic that describes it. We use the NP topics and their relevance judgments for our experiments. In this experiment, we used Porter stemmer and did not remove stopwords when indexing the GOV2 collection.",1,TREC,True
226,"For each NP query, we first run it against the GOV2 collection to obtain the QL baseline; then we use five retrieval methods described in §5.1 to rerank the top-100 web pages returned by QL. The reranked lists are evaluated by two TREC measurements previously used for the task [5]: MRR which is the mean reciprocal rank of the first correct answer and the %Top10 which is the proportion of queries for which a correct answer was found in the first 10 search results. We use the TREC 2005 NP topics (NP601-872) for training and the TREC 2006 NP topics (NP901-1081) for testing. We first tune the Dirichlet parameter  , 500 for QL to achieve the highest MRR on the training set and obtain QL's top-100 web pages for reranking. We then fix  ,"" 500 to calculate the smoothed document model component ( ) in the five retrieval methods but tune the mixture parameters  and  for them to achieve the highest MRRs with the training queries. For the two approaches that use RALM, the parameter  of RALM is also tuned. After that, we run different methods on the testing set.""",1,NP,True
227,Table 7 shows the retrieval performance of different methods and the tuned parameters in each method. We observe: (1) M-ORG-RALM performs statistically significantly bet-,1,LM,True
228,432,0,,False
229,"""Optima National Wildlife Refuge"", ""Optima NWR"", ""Washita Optima National Wildlife Refuge near Butler OK""",0,,False
230,DOC-TF,0,,False
231,refuge wildlife oklahoma optima species hawk habitat,0,,False
232,area prairie national,0,,False
233,0 () 15 10 10 8 6 6 6 6 5 5,0,,False
234,DOC-TFIDF,0,,False
235,refuge optima hardesty hawk oklahoma wildlife guymon habitat species quail,0,,False
236,0  () 79.69 74.30 47.48 36.20 36.03 31.98 29.35 26.42 23.70 21.74,0,,False
237,DOC-OKAPI,1,AP,True
238,refuge optima hardesty hawk oklahoma wildlife guymon habitat species quail,0,,False
239, 250 () 153.76 143.37 91.63 69.86 69.53 61.71 56.63 50.98 45.73 41.95,0,,False
240,AUX-TF,0,,False
241,oklahoma wildlife refuge website u service s office national fish,0,,False
242,() 6 2 2 1 1 1 1 1 1 1,0,,False
243,AUX-TFIDF,0,,False
244,oklahoma refuge wildlife fish u website office s national service,0,,False
245, () 21.62 10.62 6.40 3.11 3.03 2.36 1.54 1.29 1.22 1.09,0,,False
246,RALM,1,LM,True
247,nwr wildlife refuge national general brochure kansas,0,,False
248,lake tear sheet,0,,False
249, (0) 0.1164 0.0834 0.0834 0.0834 0.0657 0.0657 0.0601 0.0522 0.0308 0.0308,0,,False
250,Rel.,0,,False
251,butler national,0,,False
252,near nwr optima refuge washita wildlife,0,,False
253,"Table 5: Discovered missing anchor terms and their term weights by applying different approaches on one GOV2 web page (TREC DocID in GOV2: GX010-01-9459902) . The first row shows the original three pieces of anchor text associated with the page. The Rel column in bold font shows the term relevance judgments extracted from the first row. RALM can discover some term like ""NWR"", which may not appear in both the page and the auxiliary anchor text, thus may help to bridge the lexical gap between pages and web queries as using the original anchor text does.",1,TREC,True
254,"Weight Loss Resolutions, ""Weight Loss New Year's Resolution to Lose Weight"",""Resolve to Lose Weight""",0,,False
255,DOC-TF,0,,False
256,weight loss lose new year,0,,False
257,resolution time make goal diet,0,,False
258,0 () 46 26 20 17 15 13 12 10 9 9,0,,False
259,DOC-TFIDF,0,,False
260,weight loss lose,0,,False
261,resolution diet goal eat year,0,,False
262,calorie pound,0,,False
263,0  () 96.38 78.65 64.47 46.57 34.27 26.01 25.61 23.90 15.73 15.34,0,,False
264,DOC-OKAPI,1,AP,True
265,weight loss lose,0,,False
266,resolution diet goal eat year,0,,False
267,calorie pound,0,,False
268, 250 () 112.53 91.83 75.28 54.38 40.02 30.37 29.90 27.90 18.36 17.91,0,,False
269,AUX-TF,0,,False
270,weight loss diet,0,,False
271,weightloss guide scott jennifer contact site s,0,,False
272,() 709 705 32 21 20 8 8 8 6 4,0,,False
273,AUX-TFIDF,0,,False
274,loss weight weightloss,0,,False
275,diet guide jennifer scott guidesite,0,,False
276,em mlibrary,0,,False
277, () 2132.63 1485.49 157.70 121.86 37.26 33.96 28.52 22.04 13.15 11.37,0,,False
278,RALM,1,LM,True
279,weight loss diet easy lose way myth warn ppa fda,0,,False
280, (0) 0.2245 0.1737 0.0550 0.0436 0.0422 0.0412 0.0396 0.0232 0.0232 0.0232,0,,False
281,Rel.,0,,False
282,lose loss new resolution resolve,0,,False
283,s weight,0,,False
284,"Table 6: Discovered missing anchor terms and their term weights by applying different approaches on one ClueWeb09 web page (ClueWeb09 RecordID: clueweb09-en0004-60-01628). The first row shows the original three pieces of anchor text associated with the page. The Rel column in bold font shows the term relevance judgments extracted from the first row. The keyword approaches discovered ""new year resolution"", which may be hard to be discovered by using the page's web-graph neighbor pages' anchor text or using the page's similar pages' anchor text.",1,ClueWeb,True
285,433,0,,False
286,"ter than M-ORG. This indicates that missing anchor text discovered by RALM provides additional information not in the original anchor text so that combining them can further improve the retrieval performance. (2) M-ORG-RALM and M-RALM performs statistically significantly better than MORG-AUX and M-AUX, respectively. This indicates that in GOV2 missing anchor text information discovered by our content based approach helps retrieval more effectively than the auxiliary anchor text.6",1,LM,True
287,"In Table 7, we observe that the auxiliary anchor text helps the performance very little in this task. There are two plausible reasons: first, TREC NP queries are short queries and Metzler et al. observed that auxiliary anchor text does not help or even hurts the performance of short navigational web queries[14]; second, the anchor text sparsity problem is serious on the GOV2, thus very small percentage of pages can collect some auxiliary anchor text as shown in Table 1 to benefit the search task. However, even when serious anchor text sparsity exists and queries are short, our content based approach still helps improving retrieval effectiveness.",1,TREC,True
288,"We expect our technique can enhance the retrieval performance of general web search engines where there are large portion of short navigational queries. As is well known, in the general web search environment there are many lowquality web pages and spam; thus, we need to address issues about web page quality and noise filtering for better benefitting general web search. We leave this as future work.",1,ad,True
289,6. CONCLUSIONS AND FUTURE WORK,0,,False
290,"In this paper, we proposed a language modeling based technique to overcome the anchor text sparsity problem by using web content similarity. Our approach computes a relevant anchor text language model, called RALM, from its similar web pages' associated anchor text to discover its plausible missing anchor text. Compared with a link based approach [14], our content based approach has no specific link structure requirements on the web page of interest and thus can further reduce anchor text sparsity.",1,LM,True
291,"We designed experiments with two TREC web corpora to evaluate the effectiveness of discovering missing anchor terms by three different approaches: the link based approach, the RALM approach, and the keyword based approach. Experimental results show that the RALM approach can effectively discover missing original anchor text and performs statistically significantly better than the two link based approaches on both collections. Moreover, RALM's discovered anchor text is similar in nature to auxiliary anchor text while different from the keywords in the web page.",1,TREC,True
292,"By using the mixture model[15, 16], we used different discovered anchor text information within the language modeling framework for retrieval. We evaluated using different approaches for improving retrieval effectiveness with the TREC named page finding task. The results show that (1) RALM helps retrieval more than using the auxiliary anchor text collected over the web graph and (2) combining RALM and the original anchor text can statistically significantly improve the retrieval performance of only using the original anchor text. Furthermore, RALM can help improving retrieval effectiveness for short navigational queries even when serious anchor text sparsity exists. This makes RALM a promising technique for improving general web search engines.",1,TREC,True
293,"6Our goal is not to compare ranking schemes, but to show the utility of the discovered anchor text. However, we note that these scores match or beat top-performing approaches [4].",0,,False
294,"There are several interesting directions of future work. Metzler et al. found that auxiliary anchor text can effectively help longer, informational queries [14]; we will explore how well RALM can help long informational queries. We also want to explore using RALM's discovered missing anchor text information beyond the language modeling based retrieval framework, e.g. using it to extract useful features for learning-to-rank retrieval approaches [3].",1,LM,True
295,7. ACKNOWLEDGMENTS,0,,False
296,"This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.",0,,False
297,8. REFERENCES,0,,False
298,"[1] A. Broder et al. Graph structure in the web. Comput. Netw., 33(1-6):309­320, 2000.",0,,False
299,"[2] J. Broglio, J. P. Callan, and W. B. Croft. An overview of the INQUERY system as used for the TIPSTER project. Technical report, Amherst, MA, USA, 1993.",0,,False
300,"[3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proc. of ICML, pp. 89­96, 2005.",1,ad,True
301,"[4] S. Bu¨ttcher, C. L. A. Clarke, and I. Soboroff. The TREC 2006 Terabyte Track. In TREC, 2006.",1,TREC,True
302,"[5] C. L. A. Clarke, F. Scholer, and I. Soboroff. The TREC 2005 Terabyte Track. In TREC, 2005.",1,TREC,True
303,"[6] A. Fujii. Modeling anchor text and classifying queries to enhance web document retrieval. In Proc. of WWW, pp. 337­346, 2008.",0,,False
304,"[7] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422­446, 2002.",0,,False
305,"[8] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In SIGIR, pp. 194­201, 2004.",1,ad,True
306,"[9] O. Kurland and L. Lee. Respect my authority!: Hits without hyperlinks, utilizing cluster-based language models. In SIGIR, pp. 83­90, 2006.",0,,False
307,"[10] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR, pp. 120­127, 2001.",0,,False
308,"[11] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In SIGIR, pp. 186­193, 2004.",0,,False
309,"[12] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge Univ. Press. 2008.",0,,False
310,"[13] Q. Mei, D. Zhang, and C. Zhai. A general optimization framework for smoothing language models on graph structures. In SIGIR, pp. 611­618, 2008.",0,,False
311,"[14] D. Metzler, J. Novak, H. Cui, and S. Reddy. Building enriched document representations using aggregated anchor text. In SIGIR, pp. 219­226, 2009.",0,,False
312,"[15] R. Nallapati, B. Croft, and J. Allan. Relevant query feedback in statistical language modeling. In Proc. of CIKM, pp. 560­563, 2003.",0,,False
313,"[16] P. Ogilvie and J. Callan. Combining document representations for known-item search. In SIGIR, pp. 143­150, 2003.",0,,False
314,"[17] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR, pp. 275­281, 1998.",0,,False
315,"[18] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion. In Proc. of NAACL-HLT, pp. 407­414, 2006.",0,,False
316,"[19] X. Wang and C. Zhai. Mining term association patterns from search logs for effective query reformulation. In Proc. of CIKM, pp. 479­488, 2008.",0,,False
317,"[20] T. Westerveld, W. Kraaij, and D. Hiemstra. Retrieving web pages using content, links, urls and anchors. In Proc. of TREC, pp. 663­672, 2001.",1,TREC,True
318,434,0,,False
319,,0,,False

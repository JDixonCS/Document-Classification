,sentence,label,data,regex
0,"Score Distribution Models: Assumptions, Intuition, and Robustness to Score Manipulation",1,Robust,True
1,Evangelos Kanoulas,0,,False
2,Keshi Dai,0,,False
3,Virgil Pavlu,0,,False
4,Javed A. Aslam,0,,False
5,Department of Information Studies University of Sheffield,0,,False
6,"Regent Court, 211 Portobello Street Sheffield S1 4DP, UK",0,,False
7,e.kanoulas@sheff.ac.uk,0,,False
8,College of Computer and Information Science Northeastern University,0,,False
9,"360 Huntington Ave, #202 WVH Boston, MA 02115, USA",0,,False
10,"{daikeshi, vip, jaa}@ccs.neu.edu",0,,False
11,ABSTRACT,0,,False
12,"Inferring the score distribution of relevant and non-relevant documents is an essential task for many IR applications (e.g. information filtering, recall-oriented IR, meta-search, distributed IR). Modeling score distributions in an accurate manner is the basis of any inference. Thus, numerous score distribution models have been proposed in the literature. Most of the models were proposed on the basis of empirical evidence and goodness-of-fit. In this work, we model score distributions in a rather different, systematic manner. We start with a basic assumption on the distribution of terms in a document. Following the transformations applied on term frequencies by two basic ranking functions, BM25 and Language Models, we derive the distribution of the produced scores for all documents. Then we focus on the relevant documents. We detach our analysis from particular ranking functions. Instead, we consider a model for precision-recall curves, and given this model, we present a general mathematical framework which, given any score distribution for all retrieved documents, produces an analytical formula for the score distribution of relevant documents that is consistent with the precision-recall curves that follow the aforementioned model. In particular, assuming a Gamma distribution for all retrieved documents, we show that the derived distribution for the relevant documents resembles a Gaussian distribution with a heavy right tail.",1,ad,True
13,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval] Retrieval models,0,,False
14,"General Terms: Theory, Measurement",0,,False
15,"Keywords: information retrieval, score distribution, density functions, recall-precision curve",0,,False
16,1. INTRODUCTION,1,DUC,True
17,Given a user request an information retrieval system assigns scores to each document in the underlying collection,0,,False
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
19,"according to some definition of relevance of each document to the user's request and returns a ranked list of documents to the user. In reality, this ranked list of documents is a mixture of both relevant and non-relevant documents. For a wide range of retrieval applications (e.g. information filtering, topic detection, meta-search, distributed IR), modeling and inferring the distribution of relevant and non-relevant documents over scores in a reasonable way can be highly beneficial. For instance, in information filtering, topic detection and recall-oriented retrieval, modeling the score distributions of relevant and non-relevant documents can be utilized to find the appropriate threshold between relevant and non-relevant documents [16, 17, 2, 19, 9, 15]. In distributed IR and meta-search it can be used to normalize document scores and combine different collections or the outputs of several search engines [5, 12].",0,,False
20,"Inferring the score distribution for relevant and non-relevant documents in the absence of any relevance information is an extremely difficult task, if at all possible. Modeling score distributions is often the basis of any possible inference. Due to this, numerous combinations of statistical distributions have been proposed in the literature to model score distributions of relevant and non-relevant documents. In the 1960s and 70s, Swets attempted to model the score distributions of non-relevant and relevant documents with two Gaussians of equal variance [16], two Gaussians of unequal variance, and two exponentials [17]. Bookstein instead proposed a two Poisson model [7] and Baumgarten a two Gamma model [5]. A negative exponential and a Gamma distribution [12] has also been proposed in the literature. The dominant model has been a negative exponential for the non-relevant documents and a Gaussian for the relevant ones [2, 12, 19]. Bennett [6] observed that when using a two-Gaussians model for text classification, document scores outside the modes of the two Gaussians (corresponding to ""extremely irrelevant"" and ""obviously relevant"" documents) demonstrated different empirical behavior than the scores between the two modes (corresponding to ""hard to discriminate"" documents). This motivated him to introduce several asymmetric distributions to capture these differences. Kanoulas et al. [11] recently proposed a Gamma distribution for the non-relevant documents and a mixture of Gaussians for the relevant documents.",1,ad,True
21,The complexity of the underlying process that generates document scores makes it hard to theoretically argue about the actual distribution of document scores. Most of the aforementioned models were proposed on the basis of empirical fits to scores produced over different document corpora.,1,corpora,True
22,242,0,,False
23,"There have also been several attempts to intuitively argue about the shape of the different distributions. The starting point for most of these attempts has been some basic assumptions about the frequency of query term occurrences in documents (e.g. in Manmatha et al. [12]). Harter [10] and Bookstein and Swanson [8] used a mixture of Poisson distributions to model the distribution of words in a document, with one Poisson corresponding to the distribution of words in relevant documents and the other to the distribution of words in non-relevant documents.",0,,False
24,"In a different line of work, Arampatzis and van Hameren [2] showed that the distribution of relevant document scores rapidly converges to a Gaussian via the Central Limit Theorem as the number of query terms increases, under some basic assumptions. Further, they claimed that this is not true in the case of non-relevant documents.",0,,False
25,"Finally, Robertson [14] considered various combinations of distributions and examined whether these combinations exhibit anomalous behavior with respect to theoretical properties of precision and recall. Arampatzis et al. [1] proposed two truncated versions of the exponential-Gaussian model to overcome the theoretical problems associated with the original exponential-Gaussian model.",0,,False
26,"In this work, we model score distributions in a rather different, systematic manner. We start with a basic assumption on the distribution of terms in a document. Following the transformations applied on term frequencies by two basic ranking functions, BM25 and Language Models, we derive the distribution of the produced scores for all documents in an analytical form and illustrate that the derived distribution can be well approximated by a Gamma distribution.",0,,False
27,"Further, we also consider the score distribution for relevant documents. We detach our analysis from particular ranking functions. Instead, we consider a simple model for precision-recall curves proposed by Aslam and Yilmaz [3], which makes some very basic assumptions about the shapes of precision-recall curves that are produced by reasonable retrieval system on average. Given this model, we present a general mathematical framework which, given any score distribution for all retrieved documents, produces an analytical formula for the score distribution of relevant documents that is consistent with the precision-recall curves that follow the aforementioned model. In particular, assuming a Gamma distribution for all retrieved documents, we show that the derived distribution for the relevant documents resembles a Gaussian distribution with a heavy right tail.",1,ad,True
28,2. FROM TERM FREQUENCIES TO RETRIEVAL SCORES,0,,False
29,"Traditional retrieval models score documents based on how well their language matches the language of the user's request. Thus, the essential component of all traditional scoring functions is the number of occurrences of query terms within a document (term frequency, TF). Different retrieval models apply different transformations over the term frequencies to produce a score per query term. The final score of a document is usually an aggregate of the document scores for each individual term.",1,ad,True
30,Before we consider the distribution of term frequencies and the transformation applied by ranking functions over them in an analytical manner we illustrate the evolution of the term frequency distribution for all retrieved documents,0,,False
31,"(documents that contain at least one of the query terms) for a sample query from the TREC 8 ad hoc collection (Ireland Peace Talks) and for two different retrieval models, BM25 and Language Models, in Figure 1.",1,TREC,True
32,"The left panel corresponds to the transformation of TF distribution by BM25, while the right panel corresponds to the transformation by the Jelinek-Mercer Language Model.1 Each column then, in both panels, corresponds to an individual query term and each row to progressively more complex transformations of the term frequency. The bottom row plots illustrate the final score distribution by the two retrieval models.",0,,False
33,"As can be observed, for both retrieval models, there is a critical step in the term frequency transformation (from Row 2 to Row 3) after which the score distribution radically changes and appears to be closer to the final score distribution. Furthermore, the shape of the final score distribution appears to be dominated by the most frequent query term in the collection (as expected) -- for the sample query this is the term talk -- and thus our main goal will be to derive the score distribution for each individual query term.",1,ad,True
34,3. DERIVING THE DISTRIBUTION OF RAW,0,,False
35,STATISTICS,0,,False
36,"For a fixed query, consider a partition of the collection into relevance classes, such that DQ is the class of documents that satisfy the information need to a certain degree Q>0. Depending on several factors like the user, the information need, the collection of documents etc, Q can take a range of values from ""completely irrelevant"" (the lowest Q) to ""extremely relevant"" (the highest Q). Note that in test collections (such as TREC) for simplicity only two or three classes are considered. The discussion in this section assumes a fixed quality/relevance class Q, and assumes all documents in the class contain all query terms at least once.",1,TREC,True
37,"A query term t has a certain contribution to the document quality in response to the user query. For a given document quality Q, we assume an approximately constant probability of seeing the term t at any position in a document in class DQ; hence we can model term t occurrences in documents in class DQ with a Poisson process with rate  , t ,"" f (g, Q), where g "","" gt relates to the general rarity of the term in the language. Such a model is memoryless and implies that the query term appears equally likely at any moment. We do not model the dependence f -- any monotonic function can be used, depending on the class model.""",0,,False
38,"Counting the occurrences of a term t when reading a random document d  DQ is analogous to counting buses at a bus station: arrive at the station, wait for the first bus, for the second bus, etc., and leave at some point (when the document ends). It is well known that the waiting times w1, w2, w3, . . . among Poisson generated events are exponentially distributed i.i.d. random variables",1,ad,True
39,wl  e-x.,0,,False
40,(1),0,,False
41,"The average waiting time is  ,"" 1/, the mean of the exponential distribution. Intuitively,  corresponds to a notion of the expected ratio of document length to term frequency, i.e., DL/TF.""",0,,False
42,"1The parameter values used for BM25 are k1,1.2 and b,""0.75, and  "", 0.2 for the Jelenik-Mercer Language Model.",0,,False
43,243,0,,False
44,6000 4000 2000,0,,False
45,0 0,0,,False
46,600 400 200,0,,False
47,0 0,0,,False
48,200,0,,False
49,"ireland, 6.155, 7698 docs TF",0,,False
50,5,0,,False
51,10,0,,False
52,15,0,,False
53,20,0,,False
54,25,0,,False
55,30,0,,False
56,DL / TF,0,,False
57,200,0,,False
58,400,0,,False
59,600,0,,False
60,800,0,,False
61,(k1+1)*TF / (k1*DL/AVDL + TF),0,,False
62,1000,0,,False
63,100,0,,False
64,0 0,0,,False
65,200,0,,False
66,0.5,0,,False
67,1,0,,False
68,1.5,0,,False
69,2,0,,False
70,2.5,0,,False
71,(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF),0,,False
72,100,0,,False
73,0,0,,False
74,0,0,,False
75,0.5,0,,False
76,1,0,,False
77,1.5,0,,False
78,2,0,,False
79,2.5,0,,False
80,Roberston's IDF*(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF) 200,0,,False
81,100,0,,False
82,0 0,0,,False
83,4000,0,,False
84,5,0,,False
85,10,0,,False
86,15,0,,False
87,x 104 2,0,,False
88,"peac, 3.876, 35454 docs TF",0,,False
89,1,0,,False
90,0 0,0,,False
91,1500 1000,0,,False
92,500 0 0,0,,False
93,1000,0,,False
94,5,0,,False
95,10,0,,False
96,15,0,,False
97,20,0,,False
98,25,0,,False
99,30,0,,False
100,DL / TF,0,,False
101,200,0,,False
102,400,0,,False
103,600,0,,False
104,800,0,,False
105,(k1+1)*TF / (k1*DL/AVDL + TF),0,,False
106,1000,0,,False
107,500,0,,False
108,0 0,0,,False
109,1000,0,,False
110,0.5,0,,False
111,1,0,,False
112,1.5,0,,False
113,2,0,,False
114,2.5,0,,False
115,(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF),0,,False
116,500,0,,False
117,0,0,,False
118,0,0,,False
119,0.5,0,,False
120,1,0,,False
121,1.5,0,,False
122,2,0,,False
123,2.5,0,,False
124,Roberston's IDF*(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF) 1000,0,,False
125,500,0,,False
126,0,0,,False
127,0,0,,False
128,2,0,,False
129,4,0,,False
130,6,0,,False
131,8,0,,False
132,10,0,,False
133,BM25 Scores,0,,False
134,x 104 6,0,,False
135,"talk, 2.777, 70795 docs TF",0,,False
136,4,0,,False
137,2,0,,False
138,0 0,0,,False
139,2000,0,,False
140,5,0,,False
141,10,0,,False
142,15,0,,False
143,20,0,,False
144,25,0,,False
145,30,0,,False
146,DL / TF,0,,False
147,1000,0,,False
148,0 0,0,,False
149,1500 1000,0,,False
150,500 0 0,0,,False
151,2000,0,,False
152,200,0,,False
153,400,0,,False
154,600,0,,False
155,800,0,,False
156,(k1+1)*TF / (k1*DL/AVDL + TF),0,,False
157,1000,0,,False
158,0.5,0,,False
159,1,0,,False
160,1.5,0,,False
161,2,0,,False
162,2.5,0,,False
163,(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF),0,,False
164,1000,0,,False
165,0,0,,False
166,0,0,,False
167,0.5,0,,False
168,1,0,,False
169,1.5,0,,False
170,2,0,,False
171,2.5,0,,False
172,Roberston's IDF*(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF) 2000,0,,False
173,1000,0,,False
174,0,0,,False
175,0,0,,False
176,1,0,,False
177,2,0,,False
178,3,0,,False
179,4,0,,False
180,5,0,,False
181,6,0,,False
182,2000,0,,False
183,0,0,,False
184,0,0,,False
185,5,0,,False
186,10,0,,False
187,15,0,,False
188,20,0,,False
189,25,0,,False
190,30,0,,False
191,6000 4000 2000,0,,False
192,0 0,0,,False
193,1000,0,,False
194,"ireland, 6.155, 7698 docs TF",0,,False
195,5,0,,False
196,10,0,,False
197,15,0,,False
198,20,0,,False
199,25,0,,False
200,30,0,,False
201,Normalized TF,0,,False
202,500,0,,False
203,0 0,0,,False
204,300,0,,False
205,0.02,0,,False
206,0.04,0,,False
207,0.06,0,,False
208,0.08,0,,False
209,0.1,0,,False
210,log(Normalized TF),0,,False
211,200,0,,False
212,100,0,,False
213,0,0,,False
214,-14 -12 -10 -8,0,,False
215,-6,0,,False
216,-4,0,,False
217,-2,0,,False
218,log(Normalized TF + CTF/TN) 300,0,,False
219,200,0,,False
220,100,0,,False
221,0,0,,False
222,-10,0,,False
223,-8,0,,False
224,-6,0,,False
225,-4,0,,False
226,-2,0,,False
227,log(lambda*Normalized TF + (1-lambda)*CTF/TN) 200,0,,False
228,100,0,,False
229,0,0,,False
230,-10,0,,False
231,-8,0,,False
232,-6,0,,False
233,-4,0,,False
234,-2,0,,False
235,4000,0,,False
236,x 104 2,0,,False
237,"peac, 3.876, 35454 docs TF",0,,False
238,1,0,,False
239,0 0,0,,False
240,4000,0,,False
241,5,0,,False
242,10,0,,False
243,15,0,,False
244,20,0,,False
245,25,0,,False
246,30,0,,False
247,Normalized TF,0,,False
248,2000,0,,False
249,0 0,0,,False
250,2000,0,,False
251,0.02,0,,False
252,0.04,0,,False
253,0.06,0,,False
254,0.08,0,,False
255,0.1,0,,False
256,log(Normalized TF),0,,False
257,1000,0,,False
258,0,0,,False
259,-14 -12 -10 -8,0,,False
260,-6,0,,False
261,-4,0,,False
262,-2,0,,False
263,1000,0,,False
264,log(Normalized TF + CTF/TN),0,,False
265,500,0,,False
266,0,0,,False
267,-8,0,,False
268,-7,0,,False
269,-6,0,,False
270,-5,0,,False
271,-4,0,,False
272,-3,0,,False
273,-2,0,,False
274,log(lambda*Normalized TF + (1-lambda)*CTF/TN) 1000,0,,False
275,500,0,,False
276,0,0,,False
277,-8,0,,False
278,-7,0,,False
279,-6,0,,False
280,-5,0,,False
281,-4,0,,False
282,-3,0,,False
283,BM25 Scores,0,,False
284,x 104 6,0,,False
285,"talk, 2.777, 70795 docs TF",0,,False
286,4,0,,False
287,2,0,,False
288,0 0,0,,False
289,15000,0,,False
290,5,0,,False
291,10,0,,False
292,15,0,,False
293,20,0,,False
294,25,0,,False
295,30,0,,False
296,Normalized TF,0,,False
297,10000,0,,False
298,5000,0,,False
299,0 0,0,,False
300,3000,0,,False
301,0.02,0,,False
302,0.04,0,,False
303,0.06,0,,False
304,0.08,0,,False
305,0.1,0,,False
306,log(Normalized TF),0,,False
307,2000,0,,False
308,1000,0,,False
309,0,0,,False
310,-12,0,,False
311,-10,0,,False
312,-8,0,,False
313,-6,0,,False
314,-4,0,,False
315,-2,0,,False
316,2000,0,,False
317,log(Normalized TF + CTF/TN),0,,False
318,1000,0,,False
319,0,0,,False
320,-8,0,,False
321,-7,0,,False
322,-6,0,,False
323,-5,0,,False
324,-4,0,,False
325,-3,0,,False
326,-2,0,,False
327,log(lambda*Normalized TF + (1-lambda)*CTF/TN) 3000,0,,False
328,2000,0,,False
329,1000,0,,False
330,0,0,,False
331,-8,0,,False
332,-7,0,,False
333,-6,0,,False
334,-5,0,,False
335,-4,0,,False
336,-3,0,,False
337,2000,0,,False
338,0,0,,False
339,-24,0,,False
340,-22,0,,False
341,-20,0,,False
342,-18,0,,False
343,-16,0,,False
344,-14,0,,False
345,-12,0,,False
346,"Figure 1: The empirical histograms of term frequencies evolving and resulting to the final scores for a sample query (Ireland Peace Talks) over the TREC 8 Ad Hoc Track collection for both BM25 (left) and JM Language Model (right). Each column corresponds to single query term while the rows correspond to progressively more complex transformations of the term frequency (TF) up to the final score for the two ranking functions. DL is the document length, ADL is the average document length, CTF is the collection term frequency, and TN is the number of terms in the collection.",1,TREC,True
347,Our purpose is to model the distribution of the random variable DL/TF for documents in class DQ. We will do so separately for each frequency and then express the general distribution as a mixture.,0,,False
348,"Let us now fix a term frequency k ,"" 1, 2, 3, . . . and denote DQk "","" {d  DQ | T F (t, d) "","" k} the set of documents in DQ that contain term t exactly k times. Here, we make the approximation that the document ends exactly after the k-th othcecusrurmencoef ,kanwdaistoinwg eticmaneswPritkl"",""e 1thwel ,dwohcuicmh einmtmleendgitahteDlyLimasplies that DL is Gamma distributed (and more specifically Erlang-distributed), with shape k and scale  "", 1/ :",0,,False
349,"DLQk  Gamma(k, ).",0,,False
350,(2),0,,False
351,"Since k is a constant for the subclass DQk, the waiting time XQk is also Gamma distributed:",0,,False
352,XQk,0,,False
353,",",0,,False
354,DocLength T ermF requency,0,,False
355,",",0,,False
356,DL k,0,,False
357,"Gamma(k, /k).",0,,False
358,(3),0,,False
359,"Since the quality class DQ is partitioned into the classes DQk for k ,"" 1, 2, 3, . . ., the waiting time X on DQ follows a mixture of Gamma distributions with a constant mean ,""",0,,False
360,while DL on DQ follows a mixture of Gamma distributions,0,,False
361,with a constant scale :,0,,False
362,X,0,,False
363,"DLQ  PQ(k) · Gamma(k, )",0,,False
364,(4),0,,False
365,k,0,,False
366,X,0,,False
367,"XQ  PQ(k) · Gamma(k, /k)",0,,False
368,(5),0,,False
369,k,0,,False
370,"where PQ(k) ,"" P r[T F (d, t) "", k | d  DQ] denotes the probability that a document in class DQ contains the term t exactly k times.",0,,False
371,"Assuming a constant probability p that a term occurrence gives quality Q, PQ(k) can be expressed as probability of k - 1 failures (term occurrences that do not imply quality Q) followed by one success (term occurrence when quality Q is reached); therefore we model the mixture probabilities",0,,False
372,PQ(k) with a geometric distribution (equivalent to a nega-,0,,False
373,"tive binomial distribution with  ,"" 1),""",0,,False
374,"PQ(k) , p(1 - p)k-1",0,,False
375,(6),0,,False
376,"where p , pt ,"" /ADLQ expresses the correlation between the term and the information need on the class DQ (the average document length, the general rarity of the term t, and the quality Q). For example, p "", 0.5 implies that there are twice as many documents containing k terms than documents containing k + 1 terms in the class DQ. Intuitively p can be thought as a notion of inverse term frequency:",0,,False
377,"p , /ADLQ  avg(DL/T F )/ADLQ  avg(1/T F ).",0,,False
378,"Note that a number of different mixtures could be used,",0,,False
379,"perhaps based on the query type. For instance, an infor-",0,,False
380,mational query could use a negative binomial or a Poisson,0,,False
381,mixture. For the particular case of a geometric mixture how-,0,,False
382,"ever, an interesting result follows: Neuts and Zachs [13] show",0,,False
383,"that under certain conditions similar to ours, a negative bi-",0,,False
384,nomial mixture of Gamma distributions with constant scale,0,,False
385,is actually itself a Gamma distribution. With a different,0,,False
386,"noX tation, their result is pk · Gamma( + k, ) ,"" Gamma(, /p) when (7)""",0,,False
387,k,0,,False
388,!,0,,False
389,"pk ,"" N egBinomial(p, ) "",",0,,False
390,k + -1  -1,0,,False
391,p(1 - p)k,0,,False
392,(8),0,,False
393,"Applying this on DL (with ,""1) implies that DL is exponentially distributed on DQ with mean /p. Of course this must hold for all query terms, not only for t, which requires a proportionality /p "", constant ,"" ADLQ. In practice, for a given quality class, the document length variable will not be exactly exponentially distributed for two reasons: (1) relevance judgments cover a range of qualities inducing an average effect, (2) our Poisson process model for query term occurrence works reasonably well for frequent terms, but can fail on rare terms. However, this model is fairly accurate in that DL can be modeled well by a Gamma distribution""",0,,False
394,244,0,,False
395,"with a small shape parameter (the exponential distribution is Gamma with shape , 1.)",0,,False
396,"Figure 2 illustrates the empirical histogram of DL/TF for the query term system. As can be observed, a Gamma distribution appears to be a good approximation of the empirical score distribution, offering empirical evidence that the assumptions and approximations in our theory are reasonable.2",0,,False
397,0.03,0,,False
398,Empirical Histogram MLE Gamma Fit,0,,False
399,0.025,0,,False
400,0.02,0,,False
401,0.015,0,,False
402,0.01,0,,False
403,0.005,0,,False
404,0,0,,False
405,0,0,,False
406,100,0,,False
407,200,0,,False
408,300,0,,False
409,400,0,,False
410,500,0,,False
411,600,0,,False
412,700,0,,False
413,800,0,,False
414,900 1000,0,,False
415,Figure 2: The empirical histogram and the Gamma,0,,False
416,density,0,,False
417,function,0,,False
418,fit,0,,False
419,over,0,,False
420,the,0,,False
421,DL TF,0,,False
422,scores,0,,False
423,for,0,,False
424,term,0,,False
425,sys-,0,,False
426,tem in TREC8.,1,TREC,True
427,A rudimentary transformation of interest is just the in-,0,,False
428,"verse of X ,"" DL/TF, which gives the normalized term fre-""",0,,False
429,"quency T F/DL. DL/T F  fX ,",0,,False
430,PAccording to k1 PQ(k) ,0,,False
431,"the previous section, X ,",0,,False
432,"Gamma(k,",0,,False
433, k,0,,False
434,).,0,,False
435,It is known,0,,False
436,that a mixture of Gamma can approximate any smooth func-,0,,False
437,tion [18]. By approximating PQ(k) with a geometric distribution inverting T F/DL has the effect displayed in Figure 3.,0,,False
438,A relevant class of documents (high Q) implies:,0,,False
439,"· the geometric rates 1 - p ,"" 1 - 1/( · ADL) for query terms are higher, which means the mean 1/p is higher, or the mixture PQ will have non-negligible coefficients for higher scale parameters k. This will make the mixture look more """"hill""""-like due to more effective components.""",0,,False
440,"· for each query term, the Poisson generating process will be governed by a higher rate, 1/, which dictates a lower mean to all Gamma components of the mixture, or a ""light"" right-side tail. When the inverse transformation is performed (see below), the result distribution will have a heavier tail.",0,,False
441,"Conversely, a lower quality Q implies a mixture with effectively significant coefficients only for the lower k values, and also that the components of the mixture are less skewed towards the left-side, overall producing a more exponentiallike distribution (after inversion).",0,,False
442,4. DERIVING THE SCORE DISTRIBUTION FROM SCORING FUNCTIONS,0,,False
443,"In this section, we derive the score distribution of the retrieved documents in a systematic manner. We consider the transformation applied on the distribution of the elementary statistics described in the previous section by two scoring functions, BM25 and Jelinek-Mercer Language Model. The derivations presented here can be applied in the case of other retrieval models, such as TF-IDF and Divergence From Randomness (DFR).",0,,False
444,4.1 Score Transformations,0,,False
445,"Consider a transformation of the random variable X by a monotonic, differentiable function r, Y ,"" r(X). The probability density function (pdf) of Y , fY (y), can then be computed as a function of the pdf of X, fX (x) [4]. Let FY (y) and FX (x) be the cumulative density function (cdf) of Y and X, respectively. Without loss of generality let r be a non-decreasing function. Then,""",0,,False
446,"FY (y) , P r{Y  y} , P r{r(X)  y} , P r{X  r-1(y)} , FX (r-1(y)) and",0,,False
447,fY (y),0,,False
448,",",0,,False
449,d dy FY (y),0,,False
450,",",0,,False
451,d dy,0,,False
452,FX,0,,False
453,(r-1(y)),0,,False
454,",",0,,False
455,r-1(y) y,0,,False
456,· fX (r-1(y)),0,,False
457,"In the general case of a monotonic function r,",0,,False
458,fY,0,,False
459,(y),0,,False
460,",",0,,False
461,|,0,,False
462,r-1(y) y,0,,False
463,|,0,,False
464,·,0,,False
465,fX (r-1(y)),0,,False
466,"2Some fits will be better than others, depending on the ex-",0,,False
467,"ample. No theoretical model will fit all empirical examples,",0,,False
468,of course.,0,,False
469,"Figure 3: Mixture of gamma before and after the inversion, for different quality classes",0,,False
470,"Note that in practice fitting a Gamma, an inverse Gamma or an inverse Gaussian distribution in the T F/DL scores of existing collections/judgments (like TREC) are likely to differ in goodness-of-fit mostly due to random effects than other theoretical reasons - this is primarily due to complex score manipulations, and due to the sparsity and inaccuracy of the judgment process.",1,TREC,True
471,4.2 BM25 and Jelinek-Mercer LM,1,LM,True
472,Assuming that query terms appear only once within a,0,,False
473,query the BM25 for a single query term can be calculated,0,,False
474,as:,0,,False
475,BM25,0,,False
476,score,0,,False
477,",",0,,False
478,(k1 + 1)TF,0,,False
479,k1,0,,False
480,((1,0,,False
481,-,0,,False
482,b),0,,False
483,+,0,,False
484,b,0,,False
485,DL ADL,0,,False
486,),0,,False
487,+,0,,False
488,TF,0,,False
489,· IDF,0,,False
490,(9),0,,False
491,"where TF is the term frequency, IDF is the BM25 inverse document frequency, DL is the document length, and ADL is the average document length in the collection. By setting",0,,False
492,245,0,,False
493,Frequency Frequency,0,,False
494,0.045 0.04,0,,False
495,0.035,0,,False
496,BM25 score histogram Analytically Numerical MLE Gamma fit Model (theory),0,,False
497,0.03,0,,False
498,0.025,0,,False
499,0.02,0,,False
500,0.015,0,,False
501,0.01,0,,False
502,0.005,0,,False
503,0,0,,False
504,0,0,,False
505,1,0,,False
506,2,0,,False
507,3,0,,False
508,4,0,,False
509,5,0,,False
510,6,0,,False
511,BM25 score,0,,False
512,0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01,0,,False
513,0 -7,0,,False
514,BM25 score histogram Analytically Numerical MLE Gamma fit Model (theory),0,,False
515,-6,0,,False
516,-5,0,,False
517,-4,0,,False
518,-3,0,,False
519,-2,0,,False
520,-1,0,,False
521,LM (Jelinek-Mercer smoothing) score,1,LM,True
522,"Figure 4: The empirical histograms, analytically numerical plot, and MLE Gamma fitting of the distribution of approximated BM25 scores and JM language model scores for term system in TREC8.",1,TREC,True
523,"the parameter b equal to 1 (fixing the document length normalization) and defining the variable X ,"" DL/TF , BM25 can be approximated by,""",0,,False
524,"Y , r(X) ,"" IDF (k1 + 1) , X > 0""",0,,False
525,(10),0,,False
526,CX + 1,0,,False
527,"where C , k1/ADL. Given Equation 10 it can be shown",0,,False
528,that,0,,False
529,r-1(Y ),0,,False
530,",",0,,False
531,IDF (k1 + 1) - Y CY,0,,False
532,.,0,,False
533,"Now,",0,,False
534,let,0,,False
535,fX (x) be the,0,,False
536,pdf,0,,False
537,of X and fY (y) the pdf of Y . Since function r is a monotonic,0,,False
538,"and differentiable when X is positive, based on the principle",0,,False
539,"of function transformations of random variables [4], we can",0,,False
540,"calculate the pdf of Y as a function of the pdf of X,",0,,False
541,fY (y),0,,False
542,",",0,,False
543,-IDF (k1 Cy2,0,,False
544,+,0,,False
545,1),0,,False
546,fX,0,,False
547,(,0,,False
548,IDF,0,,False
549,(k1 + Cy,0,,False
550,1),0,,False
551,-,0,,False
552,y,0,,False
553,),0,,False
554,(11),0,,False
555,when 0 < y < IDF (k1 + 1) and 0 otherwise. In other words we can model the pdf of an approxima-,0,,False
556,"tion of BM25 as a function of the density function of the reverse relative term frequency. Essentially, one can plug in the above formula any distribution for the relative term frequency and get an analytical form distribution of BM25.",0,,False
557,"Based on the previous section DL/T F approximately follows a Gamma distribution. Let k^ and ^ are estimated parameters of the Gamma distribution from X via maximum likelihood estimation (MLE) for all retrieved documents (see Figure 2). Then, the approximated pdf of BM25 score for a single term can be reached as follows,",0,,False
558,fY,0,,False
559,(y),0,,False
560,",",0,,False
561,-IDF (k1 Cy2,0,,False
562,+,0,,False
563,1) Gamma( IDF (k1 + Cy,0,,False
564,1),0,,False
565,"- y ; k^, ^)",0,,False
566,(12),0,,False
567,We repeat the exact same derivation in the case of lan-,0,,False
568,guage models with Jelinek-Mercer smoothing. The score for,0,,False
569,"each term is computed as,",0,,False
570,JMLM,1,LM,True
571,score,0,,False
572,",",0,,False
573,TF log (,0,,False
574,+ C(1 - )),0,,False
575,(13),0,,False
576,DL,0,,False
577,"where C ,"" CTF /TN . CTF is collection term frequency and TN is the number of unique terms in the collection. As before, we let X "","" DL/TF , then the LM score can be""",1,LM,True
578,"written as,",0,,False
579,"Y , r(X) , log (  + C(1 - ))",0,,False
580,(14),0,,False
581,X,0,,False
582,"Using the previous assumption that DL/TF is modeled by a Gamma distribution and since the function r is a monotonic and differentiable, after the random variable transform over X we get the pdf of the LM scores as a function of the Gamma distribution that models the reverse relative term frequency.",1,LM,True
583,fY,0,,False
584,(y),0,,False
585,",",0,,False
586,(ey,0,,False
587,-ey - C(1 - ))2 Gamma( ey,0,,False
588,-,0,,False
589, C(1,0,,False
590,-,0,,False
591,),0,,False
592,;,0,,False
593,"k^,",0,,False
594,^),0,,False
595,(15),0,,False
596,Figure 4 shows the comparison among the empirical his-,0,,False
597,"togram, the analytical model derived from the distribution",0,,False
598,"of DL/T F , and the Gamma distribution obtained by MLE",0,,False
599,over BM25 and JM language model scores all retrieved docu-,0,,False
600,ments for query system in TREC8 collection. As illustrated,1,TREC,True
601,"on the plots, the analytical model has more freedom than",0,,False
602,"the Gamma distribution, but the Gamma is still a reason-",0,,False
603,"able approximation to the term score distribution. Further,",0,,False
604,the mixture model presented in the previous section with,0,,False
605,the best-fit  is also shown on Figure 4 (black line denoted,0,,False
606,"as ""Model (theory)"" in the legend).",0,,False
607,Remark on the Shape of the Distribution,0,,False
608,"Most term frequency weighting functions are nonlinear monotonically increasing functions of the raw term frequency. In BM25 Roberston's TF grows fast when the raw term frequency is small and gets gradually saturated. The parameter k1 controls the speed of the saturation. The logarithm function in Language Models also has this saturation property but without the power of controlling the saturating speed. Therefore, the JM language model scoring function has a similar to BM25 impact on transforming the distribution of low level statistics, such as DL/TF or normalized TF to the final score distribution.",1,ad,True
609,As it is illustrated in Figure 2 the typical shape of the distribution for the DL/TF tends to have a long right tail but a fast rising-up left tail. After applying a transforma-,0,,False
610,246,0,,False
611,"tion function with the saturation property, the imbalance between two tails of the original distribution is alleviated, so the peak of the new distribution is right shifted, and with a shorter right tail compared to the original one. The amount of difference is dominated by the parameter controlling the saturating speed. This can be viewed in Figure 5. As k1 becomes larger and the weighting function more linear the empirical histograms of BM25 looks more similar to the distribution of DL/TF in Figure 2. This implies that the term score distribution can also be approximated by a Gamma distribution by adjusting the shape and the scale parameters.",1,ad,True
612,4.3 Summation over Query Terms,1,Query,True
613,"In this paper we have considered scorinP g functions with the following property: score(d,query) ,"" tquery r(Xt), where Xt "","" DL/tf (t, d). This class of scoring functions includes BM25, TF-IDF, some Language Models etc, but does not include scores like PageRank. Assuming terPm independence, the intuition for the summation score "", t r(Xt) is as follows:",0,,False
614,"· For non-relevant documents (low quality Q) each r(Xt) will be distributed approximately as a Gamma(low shape, low scale). If the scales are approximately equal their sum follows a Gamma distribution with the same scale (gamma distribution exhibits infinite divisibility).",0,,False
615,"· For relevant documents, the mixture for each term has more effective components, thus making the sum a rich mixture, usually Gaussian like (or Gamma-like with higher scale and shape).",0,,False
616,"Thus, the distribution of the summation of several term scores could also be modeled using a Gamma distribution if we use a Gamma distribution to model the term score distribution. Figure 6 shows this summation process.",0,,False
617,5. INFERRING THE SCORE DISTRIBUTION,0,,False
618,OF RELEVANT DOCUMENTS,0,,False
619,"In this section, we relate the score distributions for rele-",0,,False
620,vant and non-relevant documents with precision-recall curves.,0,,False
621,That the score distributions for relevant and non-relevant,0,,False
622,documents are related to precision-recall curves is well known,0,,False
623,"and unsurprising: Given the two score distributions, one can",0,,False
624,"easily infer a precision-recall curve [14], and we shall do so",0,,False
625,below as part of the treatment that follows. More interest-,0,,False
626,"ingly, we demonstrate that one can infer the score distri-",0,,False
627,bution for relevant documents given a score distribution for,0,,False
628,"non-relevant documents and a precision-recall curve, and we",0,,False
629,use the technique described to show that the score distribu-,0,,False
630,tions for relevant documents will tend to have a Gaussian-,0,,False
631,"like form, with a heavy right tail.",0,,False
632,Let fR(s) and fN (s) be the score distributions for rel-,0,,False
633,"evant and non-relevant documents, respectively. For any",0,,False
634,"score threshold t, consider the set of documents whose scores",0,,False
635,are t or higher. The recall and fallout associated with this,0,,False
636,document set are easily defined in terms of fR(s) and fN (s),0,,False
637,as follows:,0,,False
638,Z,0,,False
639,"r(t) ,",0,,False
640,fR(s) ds,0,,False
641,(16),0,,False
642,Zt ,0,,False
643,"fo(t) ,",0,,False
644,fN (s) ds.,0,,False
645,(17),0,,False
646,t,0,,False
647,Robertson's TF,0,,False
648,5,0,,False
649,"k1,1",0,,False
650,4,0,,False
651,"k1,3",0,,False
652,"k1,5",0,,False
653,3,0,,False
654,2,0,,False
655,1,0,,False
656,0,0,,False
657,0,0,,False
658,2,0,,False
659,4,0,,False
660,6,0,,False
661,8 10 12 14 16 18 20,0,,False
662,TF,0,,False
663,Frequency,0,,False
664,0.035 0.03,0,,False
665,0.025 0.02,0,,False
666,"k1,1 k1,3 k1,5",0,,False
667,0.015,0,,False
668,0.01,0,,False
669,0.005,0,,False
670,0,0,,False
671,0,0,,False
672,1,0,,False
673,2,0,,False
674,3,0,,False
675,4,0,,False
676,5,0,,False
677,6,0,,False
678,BM25 Scores,0,,False
679,Figure 5: Roberston's TF and empirical histograms of BM25 scores with different k1 for term system in TREC8,1,TREC,True
680,0.04 0.03 0.02 0.01,0,,False
681,0 0,0,,False
682,0.05,0,,False
683,0.04,0,,False
684,0.03,0,,False
685,0.02,0,,False
686,0.01,0,,False
687,0,0,,False
688,5,0,,False
689,10,0,,False
690,15,0,,False
691,0,0,,False
692,0.05,0,,False
693,0.04,0,,False
694,0.03,0,,False
695,0.02,0,,False
696,0.01,0,,False
697,0,0,,False
698,5,0,,False
699,10,0,,False
700,0,0,,False
701,2,0,,False
702,4,0,,False
703,6,0,,False
704,0.05 0.04 0.03 0.02 0.01,0,,False
705,0 0,0,,False
706,BM25 Scores Histogram MLE Gamma Fit,0,,False
707,5,0,,False
708,10,0,,False
709,15,0,,False
710,20,0,,False
711,25,0,,False
712,30,0,,False
713,"Figure 6: MLE Gamma fitting over scores of all retrieved documents for all query terms and query ""Ireland Peace Talks""",0,,False
714,"Now let C be the size of the collection and let  be the fraction of the collection that is relevant to a given query. Then there are R ,  C total relevant documents and N ,"" (1 - )C total non-relevant documents. At score t or above, there are""",0,,False
715,"R · r(t) ,  C · r(t)",0,,False
716,relevant documents and,0,,False
717,"N · fo(t) , (1 - )C · fo(t)",0,,False
718,"non-relevant documents. Thus, the precision associated with this document set is simply",0,,False
719, C · r(t),0,,False
720,r(t),0,,False
721,"p(t) ,  C · r(t) + (1 - )C · fo(t) , r(t) + O · fo(t) (18)",0,,False
722,"where O , (1 - )/ is the odds of non-relevance in the collection. Equations 16 and 18 are parametric equations",0,,False
723,247,0,,False
724,defining a precision-recall curve: Given the score distribu-,0,,False
725,"tions fR(s) and fN (s) (and ), one can vary the score threshold t in Equations 16 and 18 to obtain the precision-recall",0,,False
726,curve. (A substantially similar treatment can be found in,0,,False
727,Robertson [14].) Now suppose that one has a candidate score distribution,0,,False
728,for either relevant or non-relevant documents and one has a,0,,False
729,candidate form for a precision-recall curve: Can one derive,0,,False
730,"a form for the other score distribution? In what follows,",0,,False
731,"we show how this can be accomplished, and using the score distributions described in Section 4 and a simple form for precision-recall curves, we infer a form for the score distributions of relevant documents.",0,,False
732,Consider the simple model for precision-recall curves de-,0,,False
733,scribed by Aslam and Yilmaz [3] and shown in Figure 7.,0,,False
734,"This family of precision-recall curves is defined by the following equation, implicitly parameterized by the value of R-precision rp:",0,,False
735,p(r),0,,False
736,",",0,,False
737,1-r 1+·,0,,False
738,. r,0,,False
739,(19),0,,False
740,"(Here  ,"" (1/rp - 1)2 - 1 governs the """"shape"""" of the curve.)""",0,,False
741,"While it is certainly the case that""real"" precision-recall curves",0,,False
742,"are never this ""clean"", this simple model captures many",0,,False
743,"properties found in real precision-recall curves, such as high",0,,False
744,"precisions at low recall levels, low precisions at high recall",0,,False
745,"levels, and so on. Furthermore, Aslam and Yilmaz show that",0,,False
746,this simple model allows one to explicitly and accurately re-,0,,False
747,"late average precision, R-precision, precision-at-cutoff, and",0,,False
748,other seemingly disparate measures of retrieval performance.,0,,False
749,"Using such a model for precision-recall curves, we can re-",0,,False
750,late the score distributions for relevant and non-relevant doc-,0,,False
751,uments as follows. We first parameterize Equation 19 by the,0,,False
752,"score threshold t, obtaining",0,,False
753,p(t),0,,False
754,",",0,,False
755,p(r(t)),0,,False
756,",",0,,False
757,1,0,,False
758,1 - r(t) +  · r(t),0,,False
759,.,0,,False
760,(20),0,,False
761,We now equate Equations 18 and 20,0,,False
762,r(t) r(t) + O · fo(t),0,,False
763,",",0,,False
764,1 - r(t) 1 +  · r(t),0,,False
765,and solve for r(t) as a function of fo(t),0,,False
766,p,0,,False
767,-O · fo(t) + (O · fo(t))2 + 4(1 + )O · fo(t),0,,False
768,"r(t) ,",0,,False
769,(21),0,,False
770,2(1 + ),0,,False
771,"Differentiating Equation 21 by t immediately establishes a closed-form relationship between the score distributions for relevant and non-relevant documents, since by Equations 16 and 17 and the Fundamental Theorem of Calculus, we have",0,,False
772,"r (t) , -fR(t) fo (t) , -fN (t).",0,,False
773,"As an example of this methodology, let us assume that the score distribution for all documents follows a Gamma distribution, as we argued in Section 4. Since the overwhelming majority of documents are non-relevant, the score distribution for non-relevant documents will then tend to follow a Gamma distribution as well. Now consider the Gamma that fits the non-relevant documents for the TREC8 query ""Estonia Economy"". Using this Gamma distribution for the nonrelevant documents, together with a precision-recall curve3",1,TREC,True
774,"3We set  and  , (1/rp -1)2 -1 to match those parameters from the BM25 run on that query.",0,,False
775,"from the family show in Equation 19, and employing the method described above, we obtain the score distribution for relevant documents shown in Figure 8.",0,,False
776,"While Figure 8 gives just one such example, the form of this curve is quite consistent across all tested input distributions from the Gamma family (which includes the negative exponential distribution) and all tested precision-recall curves from the family defined by Equation 19: The distribution is roughly Gaussian in form, but with a heavy right tail. That the score distribution is ""Gaussian-like"" is much assumed (as discussed in the introduction), but the heavy right tail is also necessary to avoid problems with a simple Gaussian, such as those described by Manmatha et al. [12] and others. Figure 9 shows the typical form of the relevant document score distribution we obtained in TREC 8. We here for the first time derive such a form, given reasonable forms for non-relevant score distributions and precisionrecall curves.",1,TREC,True
777,"Our results in this section are descriptive rather than prescriptive, and as such, we conclude the following:",0,,False
778,The tendency of the score distributions for relevant documents to look Gaussian with a heavy right tail is a natural and inevitable consequence of the facts that (1) the score distributions of non-relevant documents tend to look Gamma and (2) precision-recall curves tend to have the form shown in Figure 7.,0,,False
779,6. CONCLUSIONS,0,,False
780,"In this work, we attempt to model score distributions in a rather systematic manner. We start with a basic assumption that query terms are generated via a Poisson process and induced that the distribution the relative term frequency in a document is a inverse Gamma distribution. Following the mathematical transformations applied on the relative term frequencies by two basic ranking functions, BM25 and Language Models, we derived the distribution of the produced scores, in an analytical form and illustrate that the derived distribution can be well approximated by a Gamma distribution. Further, we also considered the score distribution for relevant documents by relating score distributions with precision-recall curves. In particular, we adopted a precision-recall curve model that has previously been proposed and given this model we presented a general mathematical framework under which given any score distribution for all retrieved documents we can derive an analytical formula for the score distribution of relevant documents. The framework is general enough such that the same derivations can be repeated for different models of precision recall curves. Finally, under the assumption that non-relevant documents follow a Gamma distribution for all retrieved documents, we show that there is a tendency of the derived distribution for the relevant documents to look Gaussian with a heavy right-hand tail.",1,ad,True
781,7. ACKNOWLEDGEMENTS,0,,False
782,We gratefully acknowledge the support provided by the NSF grants IIS-0533625 and IIS-0534482 and by the European Commission grant FP7-ICT-248347 (Accurat project) and the Marie Curie Fellowship FP7-PEOPLE-2009-IIF-254562.,0,,False
783,248,0,,False
784,precision,0,,False
785,Precision-recall curves for various values of rp,0,,False
786,1,0,,False
787,0.9,0,,False
788,0.8,0,,False
789,0.7,0,,False
790,0.6,0,,False
791,0.5,0,,False
792,0.4,0,,False
793,0.3,0,,False
794,0.2,0,,False
795,0.1,0,,False
796,0,0,,False
797,0,0,,False
798,0.2,0,,False
799,0.4,0,,False
800,0.6,0,,False
801,0.8,0,,False
802,1,0,,False
803,recall,0,,False
804,"Figure 7: A family of precisionrecall curves fit through the points {(0, 1), (rp, rp), (1, 0)} for rp ,"" 0.1, 0.2, . . . , 0.9.""",0,,False
805,0.2,0,,False
806,0.18,0,,False
807,0.16,0,,False
808,0.14,0,,False
809,0.12,0,,False
810,0.1,0,,False
811,0.08,0,,False
812,0.06,0,,False
813,0.04,0,,False
814,0.02,0,,False
815,0,0,,False
816,5,0,,False
817,6,0,,False
818,7,0,,False
819,8,0,,False
820,9,0,,False
821,10,0,,False
822,11,0,,False
823,"Figure 8: Inferred relevant document score distribution and empirically histogram for the TREC8 query ""Estonia, economy"".",1,TREC,True
824,0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01,0,,False
825,0 0,0,,False
826,5,0,,False
827,10,0,,False
828,15,0,,False
829,20,0,,False
830,25,0,,False
831,30,0,,False
832,Figure 9: Typical form of the relevant document score distribution in TREC8.,1,TREC,True
833,8. REFERENCES,0,,False
834,"[1] A. Arampatzis, J. Kamps, and S. Robertson. Where to stop reading a ranked list?: threshold optimization using truncated score distributions. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 524­531, New York, NY, USA, 2009. ACM.",1,ad,True
835,"[2] A. Arampatzis and A. van Hameren. The score-distributional threshold optimization for adaptive binary classification tasks. In SIGIR '01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 285­293, New York, NY, USA, 2001. ACM.",1,ad,True
836,"[3] J. A. Aslam and E. Yilmaz. A geometric interpretation and analysis of R-precision. In Proceedings of the Fourteenth ACM International Conference on Information and Knowledge Management, pages 664­671. ACM Press, October 2005.",0,,False
837,"[4] R. D. Barr and W. P. Zehna. Probability: Modelling Uncertainty. Addison-Wesley, 1983.",0,,False
838,"[5] C. Baumgarten. A probabilistic solution to the selection and fusion problem in distributed information retrieval. In SIGIR '99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 246­253, New York, NY, USA, 1999. ACM.",0,,False
839,"[6] P. N. Bennett. Using asymmetric distributions to improve text classifier probability estimates. In SIGIR '03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 111­118, New York, NY, USA, 2003. ACM.",0,,False
840,"[7] A. Bookstein. When the most ""pertinent"" document should not be retrieved--an analysis of the swets model. Information Processing & Management, 13(6):377­383, 1977.",0,,False
841,"[8] A. Bookstein and D. R. Swanson. Probabilistic models for automatic indexing. Journal of the American Society for Information Science, 25(5):312­318, 1974.",0,,False
842,"[9] K. Collins-Thompson, P. Ogilvie, Y. Zhang, and J. Callan. Information filtering, novelty detection, and named-page finding. In In Proceedings of the 11th Text Retrieval Conference, 2003.",0,,False
843,"[10] S. P. Harter. A probabilistic approach to automatic keyword indexing: Part i. on the distribution of specialty words in a technical literature. Journal of the American Society for Information Science, 26(4):197­206, 1975).",0,,False
844,"[11] E. Kanoulas, V. Pavlu, K. Dai, and J. A. Aslam. Modeling the score distributions of relevant and non-relevnat documents. In In Proceedings of the 2nd International Conference on the Theory of Information Retrieval, September 2009.",0,,False
845,"[12] R. Manmatha, T. Rath, and F. Feng. Modeling score distributions for combining the outputs of search engines. In SIGIR '01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 267­275, New York, NY, USA, 2001. ACM.",0,,False
846,"[13] M. F. Neuts and S. Zacks. On mixtures of 2- and f-distributions which yield distributions of the same family. Annals of the Institute of Statistical Mathematics, 19(1):527­536, 1966.",0,,False
847,"[14] S. Robertson. On score distributions and relevance. In G. Amati, C. Carpineto, and G. Romano, editors, Advances in Information Retrieval, 29th European Conference on IR Research, ECIR 2007, volume 4425/2007 of Lecture Notes in Computer Science, pages 40­51. Springer, June 2007.",0,,False
848,"[15] M. Spitters and W. Kraaij. A language modeling approach to tracking news events. In In Proceedings of TDT workshop 2000, pages 101­106, 2000.",1,TD,True
849,"[16] J. A. Swets. Information retrieval systems. Science, 141(3577):245­250, July 1963.",0,,False
850,"[17] J. A. Swets. Effectiveness of information retrieval methods. American Documentation, 20:72­89, 1969.",0,,False
851,"[18] M. Wiper, D. R. Insua, and F. Ruggeri. Mixtures of gamma distributions with applications. Journal of Computational and Graphical Statistics, 10(3):440­454, September 2001.",0,,False
852,"[19] Y. Zhang and J. Callan. Maximum likelihood estimation for filtering thresholds. In SIGIR '01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 294­302, New York, NY, USA, 2001. ACM.",0,,False
853,249,0,,False
854,,0,,False

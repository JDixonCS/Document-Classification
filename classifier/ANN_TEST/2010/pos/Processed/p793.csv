,sentence,label,data,regex
0,Hierarchical Pitman-Yor Language Model for Information Retrieval,0,,False
1,"Saeedeh Momtazi, Dietrich Klakow",0,,False
2,"Spoken Language Systems Saarland University, Saarbrücken, Germany",0,,False
3,"{saeedeh.momtazi,dietrich.klakow}@lsv.uni-saarland.de",0,,False
4,ABSTRACT,0,,False
5,"In this paper, we propose a new application of Bayesian language model based on Pitman-Yor process for information retrieval. This model is a generalization of the Dirichlet distribution. The Pitman-Yor process creates a power-law distribution which is one of the statistical properties of word frequency in natural language. Our experiments on Robust04 indicate that this model improves the document retrieval performance compared to the commonly used Dirichlet prior and absolute discounting smoothing techniques.",1,Robust,True
6,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]:Information Search and Retrieval,0,,False
7,"General Terms: Theory, Algorithm, Experimentation",0,,False
8,"Keywords: information retrieval, language modeling, PitmanYor process, smoothing methods",0,,False
9,1. INTRODUCTION,1,DUC,True
10,"Statistical language modeling has successfully been used in speech recognition and many natural language processing tasks. Language models for information retrieval have been the topics of intense research interest in recent years. The efficiency of this approach, its simplicity, the state-ofthe-art performance it provides, and straightforward probabilistic interpretation are the most important factors which contribute to its popularity [3].",0,,False
11,"Smoothing plays an essential role when estimating a language model for retrieving relevant documents. A large number of smoothing methods have been proposed for language modeling; among them, three different techniques-- namely Jelinek-Mercer, Bayesian smoothing with Dirichlet priors, and absolute discounting--have shown significant improvements in information retrieval performance [6].",0,,False
12,"A hierarchical Bayesian language model based on PitmanYor processes has been recently proposed by Teh [5]. This model which is a nonparametric generalization of the Dirichlet distribution [5] has been shown to produce results superior to the state-of-the-art smoothing methods. Hierarchical Pitman-Yor language model has also been applied in speech recognition task and improved the system performance significantly [1]. However, to the best knowledge of the authors this method has not been used for language model-based information retrieval.",0,,False
13,"In this work, we propose using the hierarchical Pitman-",0,,False
14,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
15,"Yor language model for the document retrieval task, and compare this approach with the state-of-the-art smoothing methods widely studied for language model-based information retrieval.",0,,False
16,2. METHOD,0,,False
17,"In language model-based document retrieval, P (Q|d) is estimated by the probability of generating each query term:",0,,False
18,"P (Q|d) ,",0,,False
19,P (qi|d),0,,False
20,(1),0,,False
21,"i,1...M",0,,False
22,"where M is the number of terms in the query, qi denotes the ith term of query Q ,"" {q1, q2, ..., qM }, and d is the document model. Therefore, the goal is to estimate P (w|d) which can""",0,,False
23,be simply calculated by the maximum likelihood estimation:,0,,False
24,"Pml(w|d) ,",0,,False
25,"c(w, d) w c(w, d)",0,,False
26,(2),0,,False
27,"However, having the problem of unseen words, we need to use a smoothing technique to give a non-zero probability to the unseen words. We hypothesized that Bayesian smoothing based on Pitman-Yor process can be used as a new approach to solve the zero probability problem in document retrieval.",0,,False
28,"Pitman-Yor process is a nonparametric Bayesian model which recursively placed as prior for predicting probabilities in language model. Considering P (w|d) as the probability of word w given the observation of document d to be estimated, the Pitman-Yor process can be defined as:",0,,False
29,"P (w|d)  P Y (, , PBG(w))",0,,False
30,(3),0,,False
31,"where  is a discount parameter,  is a strength parame-",0,,False
32,"ter, and PBG is the prior/background probability of word w",0,,False
33,before observing any document.,0,,False
34,The procedure of drawing word probabilities from the,0,,False
35,"Pitman-Yor process can be described using the ""Chines restau-",0,,False
36,"rant"" analogy. Imagine a Chinese restaurant with an infinite",0,,False
37,"number of tables, each with an infinite number of seats. Cus-",0,,False
38,"tomers, which correspond to word tokens, enter the restau-",0,,False
39,rant and seat themselves at a table. Each customer can sit at,0,,False
40,an,0,,False
41,occupied,0,,False
42,table,0,,False
43,k,0,,False
44,with,0,,False
45,probability,0,,False
46,ck -   + c.,0,,False
47,where,0,,False
48,ck,0,,False
49,is,0,,False
50,the,0,,False
51,"number of customers already sitting there and c. , k ck;",1,ad,True
52,the customer can also sit at a new unoccupied table with,0,,False
53,probability,0,,False
54, + t.  + c.,0,,False
55,where,0,,False
56,t.,0,,False
57,is,0,,False
58,the,0,,False
59,current,0,,False
60,number,0,,False
61,of,0,,False
62,occu-,0,,False
63,pied tables. It is necessary to mention that all customers,0,,False
64,that correspond to the same word type w can sit at different,0,,False
65,793,0,,False
66,"tables, in which tw denotes the number of tables occupied by customers w.",0,,False
67,"One of the advantages of Pitman-Yor process is improving the Dirichlet prior by using a discounting parameter  (0 <  < 1) deriving from absolute discounting method. Another key advantage of Pitman-Yor process is generating a power-law distribution in the language model, which is one of the statistical properties of word frequencies in natural language. This property, which is based on the scenario of rich-get-richer, implies that in the statistical property of word counts, words with low frequency have a high probability and words with high frequency occur with low probability. Benefiting from this idea in the document smoothing can help us to have different discounting value for each word based on the frequency of that word in the document.",1,ad,True
68,"Given the seating arrangement of customers as described above, the estimated probability of word w having the observation of document d is given by:",0,,False
69,P (w|d),0,,False
70,",",0,,False
71,"c(w, d) - tw + ( + t.)PBG(w) w c(w, d) + ",0,,False
72,(4),0,,False
73,"If we set the discounting parameter  ,"" 0, then the model reduces to the Dirichlet process. If we set the strength parameter  "", 0 and limit tw ,"" 1, then the model reverts to the absolute discounting method.""",0,,False
74,"Although this formula is based on unigram model, the hierarchical behavior of the Pitman-Yor process allows us to use this model for higher level n-grams as well.",0,,False
75,"The most important and computationally expensive part of the above formula is calculating tw for each word which should have a relation to the word count c(w, d). Towards this end, we use the power-law discounting model proposed by Huang and Renals [2]:",0,,False
76,"tw , 0 tw ,"" f (c(w, d)) "","" c(w, d)""",0,,False
77,"if c(w, d) ,"" 0 if c(w, d) > 0""",0,,False
78,(5),0,,False
79,"They showed that the above formula is a near optimum estimate for tw, which can be obtained without a computationally expensive training procedure.",0,,False
80,3. EXPERIMENTAL RESULTS,0,,False
81,"To evaluate our methods, we used TREC ad hoc testing collections from disk 4 and 5 minus CR which includes Financial Times (1991-1994) and Federal Register (1994) from disk 4 and Foreign Broadcast Information Service (1996) and Los Angeles Times (1989-1990) from disk 5. The total number of documents are 528,155.",1,TREC,True
82,"We used Robust04 topics for our experiment such that topics 301-450 have been used as development set and topics 601-700 for test set. For each of the topics, the set of top 1000 documents retrieved by Indri [4] was selected and then the documents are ranked with LSVLM, the language modeling toolkit developed by our chair, in the second step.",1,Robust,True
83,"Table 1 shows the results of our experiments in which Mean Average Precision (MAP) and Precision at 10 (P@10) serve as the primary metrics, and results are marked as significant* (p < 0.05), highly significant** (p < 0.01), or neither according to 2-tailed paired t-test. This table presents our main results evaluating the accuracy of Bayesian smoothing with Dirichlet prior, absolute discounting and our proposed Bayesian smoothing based on Pitman-Yor process.",1,MAP,True
84,"As shown by the tabulated results, the Pitman-Yor language model significantly outperforms both Dirichlet prior",0,,False
85,Table 1: Retrieval results with different smoothings.,0,,False
86,Significant differences with absolute discounting and,0,,False
87,Dirichlet prior are marked by a and d respectively.,0,,False
88,Model,0,,False
89,MAP P@10,1,MAP,True
90,Absolute Discounting Dirichlet Prior,0,,False
91,"Pitman-Yor Process Pitman-Yor Process ( , 0)",0,,False
92,0.3138 0.3147 0.3271da 0.3222a,0,,False
93,0.4484 0.4518 0.4657a 0.4566,0,,False
94,"and absolute discounting. As mentioned, the major features of the Pitman-Yor process are generalizing Dirichlet prior and generating power-law distribution by having different discounting parameters for each word based on its frequency. We believe that the power-law distribution is the main contribution of the Pitman-Yor language model which causes such an improvement in retrieval performance. We also applied Pitman-Yor language model while setting  ,"" 0; i.e. the model became more similar to absolute discounting, but it still creates power-law distribution by benefiting from tw parameter. The results are presented in the last raw of the table. From the results we can see that although setting  "","" 0 decreases the performance, the reduction is not significant; and the simplified version of Pitman-Yor smoothing which only has one parameter still beat the other smoothing methods.""",0,,False
95,4. CONCLUDING REMARKS,0,,False
96,"We proposed a new smoothing method for language modelbased document retrieval, named Bayesian smoothing based on Pitman-Yor process, and verified that this language model provides better performance than other state-of-the-art smoothing techniques. The key advantage of Pitman-Yor language model is generating a power-law word distribution, which is the primary reason for its superior performance.",1,ad,True
97,Acknowledgments,0,,False
98,Saeedeh Momtazi is funded by the German research foundation DFG through the International Research Training Group (IRTG 715).,0,,False
99,5. REFERENCES,0,,False
100,"[1] S. Huang and S. Renals. Hierarchical Pitman-Yor language models for ASR in meetings. In Proceedings of IEEE ASRU International Conference, pages 124­129, 2007.",0,,False
101,"[2] S. Huang and S. Renals. Power law discounting for n-gram language models. In Proceedings of IEEE ICASSP International Conference, 2010.",0,,False
102,"[3] J. Ponte and W. Croft. A language modeling approach to information retrieval. In Proceedings of ACM SIGIR International Conference, pages 275­281, 1998.",0,,False
103,"[4] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A language model-based search engine for complex queries. In Proceedings of International Conference on Intelligence Analysis, 2005.",0,,False
104,"[5] Y. Teh. A hierarchical Bayesian language model based on Pitman-Yor process. In Proceedings of ACL International Conference, 2006.",0,,False
105,"[6] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of ACM SIGIR International Conference, 2001.",1,ad,True
106,794,0,,False
107,,0,,False

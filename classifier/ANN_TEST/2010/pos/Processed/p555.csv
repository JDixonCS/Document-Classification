,sentence,label,data,regex
0,Do User Preferences and Evaluation Measures Line Up?,0,,False
1,"Mark Sanderson, Monica Lestari Paramita, Paul Clough, Evangelos Kanoulas",0,,False
2,"Department of Information Studies, University of Sheffield Regent Court, 211 Portobello St, Sheffield, S1 4DP, UK",0,,False
3,+44 114 22 22648,0,,False
4,m.sanderson@shef.ac.uk,0,,False
5,ABSTRACT,0,,False
6,"This paper presents results comparing user preference for search engine rankings with measures of effectiveness computed from a test collection. It establishes that preferences and evaluation measures correlate: systems measured as better on a test collection are preferred by users. This correlation is established for both ""conventional web retrieval"" and for retrieval that emphasizes diverse results. The nDCG measure is found to correlate best with user preferences compared to a selection of other well known measures. Unlike previous studies in this area, this examination involved a large population of users, gathered through crowd sourcing, exposed to a wide range of retrieval systems, test collections and search tasks. Reasons for user preferences were also gathered and analyzed. The work revealed a number of new results, but also showed that there is much scope for future work refining effectiveness measures to better capture user preferences.",0,,False
7,Categories and Subject Descriptors,0,,False
8,H.3.3 [Information Search and Retrieval],0,,False
9,General Terms,0,,False
10,"Measurement, Experimentation.",0,,False
11,Keywords,0,,False
12,"Mechanical Turk, User Experiment, Evaluation Measures",0,,False
13,1. INTRODUCTION,1,DUC,True
14,"There is a long tradition of encouraging conducting, and researching evaluation of search systems in the IR community. A test collection and an evaluation measure are together used as a tool to make a prediction about the behavior of users on the IR systems being measured. If measurement using the collection reveals that system A is more effective than system B, it is assumed that users will prefer A over B in an operational setting. One of the striking aspects of almost all the early work in test collections is that the predictions about users implied from such measurements were rarely, if ever, validated. Given that test collections are used to simulate users, that so little validation took place is perhaps surprising.",1,ad,True
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.",1,ad,True
16,"In the last ten years a series of papers employing a range of methods conducted such validation. The papers produced contradictory results, some failing to find any link between test collection measures and user preferences, performance, or satisfaction; others finding links, but only when differences between IR systems were large.",1,ad,True
17,"Much of the past work involved a small number of topics, systems, and users; and/or introduced some form of artificial manipulation of search results as part of their experimental method. There was also a strong focus on test collections and not on the relative merits of different evaluation measures.",0,,False
18,"Therefore, it was decided to examine, on a larger scale, if test collections and their associated evaluation measures do in fact predict user preferences across multiple IR systems, examining different measures and topic types. The study involved 296 users, working with 30 topics, comparing user preferences across 19 runs submitted to a recent TREC evaluation. The research questions of the study were as follows",1,TREC,True
19,1. Does effectiveness measured on a test collection predict user preferences for one IR system over another?,0,,False
20,"2. If such a predictive power exists, does the strength of prediction vary across different search tasks and topic types?",0,,False
21,"3. If present, does the predictive power vary when different effectiveness measures are employed?",0,,False
22,"4. When choosing one system over another, what are the reasons given by users for their choice?",0,,False
23,"The rest of this paper starts with a literature review, followed by a description of the data sets and methods used in the study. Next, the results of experiments are described, the methods are reflected upon, conclusions are drawn, and future work is detailed.",0,,False
24,2. PAST LITERATURE,1,TERA,True
25,"The past work described here is grouped into two sections, based on the methods used to measure users. Contradictions between the results of the two groups are then discussed.",1,ad,True
26,2.1 Measures rarely predict users,0,,False
27,"The power to predict user preferences using a test collection and evaluation measure was first examined in the work of Hersh et al [15] who used the 14 topics and qrels of TREC 6 and 7's interactive track to determine which of two retrieval systems was significantly better. They then conducted an experiment involving 24 searchers, retrieving over six topics of TREC-8: three topics on one system, three on the other. The researchers reported that there was no significant difference in the effectiveness of the searchers when using the different systems. This work was repeated on another test collection [25] drawing the same conclusion.",1,TREC,True
28,Allan et al [4] created artificial document rankings from TREC data each with controlled levels of effectiveness. Users were,1,TREC,True
29,555,0,,False
30,"shown selections of the generated rankings and asked to identify relevant information. Unlike the work described above, a correlation between user behavior and test collection based evaluation measures was found, but mainly when measured differences were large. Turpin & Scholer [26] repeated the artificial document ranking method, getting thirty users to examine fifty topics. No significant difference in the time users took to find the first relevant document was found. A small significant difference in the number of relevant documents identified was observed for large differences in the MAP of the artificial ranks.",1,MAP,True
31,"Inspired by Hersh and Turpin' s method Al-Maskari et al [3] measured how well groups of users performed on two IR systems. Fifty six users searched from a selection of 56 topics. The researchers showed that test collection based measures were able to predict user behavior, and to some extent a user's level of satisfaction, however only when measured differences between the systems were large.",0,,False
32,"Although test collection based work is relatively recent, there is a longer tradition of correlating user outcomes with effectiveness measures calculated on actual searching systems. Tagliacozzo [22] showed that 18% of ~900 surveyed MEDLINE users appeared unsatisfied with search results despite retrieving a large number of relevant documents. As part of a larger study, Su [21] examined correlations between precision and user satisfaction; finding no significant link. Hersh et al [14] examined medical students' ability to answer clinical questions after using a medical literature search engine. No correlation between search effectiveness measures and the quality of the student's answers was found. Huuskonen et al [16] conducted a similar medical searching experiment reporting the same lack of correlation.",1,ad,True
33,"Smith and Kantor [20] engaged 36 users to each search 12 information gathering topics on two versions of a web search engine: one, the normal searching system and the other, a degraded version which displayed results starting from rank 300. Users weren't aware they were being shown the different versions. Although no actual effectiveness measures were taken, it is reasonable to assume that there was a significant difference in precision between the versions. However, there was no significant difference in user success in finding relevant items. Smith and Kantor reported that users of the poorer system issued more queries, which appeared to mitigate the smaller number of relevant documents retrieved in each search.",1,ad,True
34,2.2 Measures predict user behavior,0,,False
35,"Measuring users through an analysis of query logs, Joachims [17] described an experiment showing users different sets of search results; as with previous work although there were measurable differences between the quantity and rank of relevant documents, Joachims saw little difference in users' click behavior. Users given poorer search results still choose top ranked documents. He proposed an alternative approach, which was to interleave the retrieval outputs of the two systems into a single ranking and observe if users tended to click on documents from one ranking more often than the other. The results showed users consistently chose documents from the better part of the interleaved ranking. This method of giving users (unknowingly) a choice and observing their preference was repeated [18] producing similar results. In this work, small, but measurable changes in document rankings were compared, and significant differences in user",0,,False
36,"behavior were observed. Further analysis of query logs to model user click behavior was conducted by many researchers, e.g. [10].",0,,False
37,Thomas et al [24] described another preference methodology where two sets of search results were presented side-by-side to users who were then asked which of the two they preferred. The method was used to compare the top 10 results of Google and the (presumably worse) Google results in ranks 21-30. They reported a clear preference for the top ranked results over the lower ranked.,0,,False
38,2.3 Lessons drawn from past work,0,,False
39,"After reading the first set of research results, one might question the value of all test collection based research, as the only time users show any difference in behavior, success in their work, or preference for searching systems is when large differences in effectiveness between IR systems are measured. In direct contradiction to this, is the smaller body of work in the following section measuring clear preferences by users even for subtle differences in retrieval results. What might be the cause of this apparent contradiction?",1,ad,True
40,"Smith and Kantor's work appears to be the clearest in demonstrating that if it is important for users to locate relevant documents they can cope with the burden of a poorer search engine by re-formulating their query. In addition, Joachims' work appears to show that users will often make do with poorer results. The work in Section 2.1 could be failing to observe differences across users because these two traits simply make human searchers hard to measure.",1,ad,True
41,"As can be seen, there is only limited work using the preference based approach and to the best of our knowledge there is no work using this method to test the correlations between users and evaluations based on test collections. Further, none of the past work has addressed the more nuanced questions of whether certain evaluation measures or search tasks show better prediction of user behavior over others. Although there are a plethora of papers comparing different evaluation measures, almost without exception they report cross-measure correlations or use some form of stability statistic to imply which might be better. The only exception is Al-Maskari et al who examined correlations between user satisfaction and evaluation measures [2] finding that Cumulative Gain (CG) correlated better with user preferences than P(10), DCG and nDCG, but the experiment was based on a small sample of people.",1,ad,True
42,"Because examination of different measures is almost unexplored, we addressed it here. With a growth of interest in search systems supporting diversity, there is as yet little research examining the predictive power of test collections in relation to diverse queries. Therefore, this paper conducted such a broad investigation into the predictive power of test collections and evaluation measures.",1,ad,True
43,3. METHOD,0,,False
44,The experiment required six components: a test collection with diverse topics and QRELS; multiple IR systems; a population of users; a method of measuring them; the selection of effectiveness measures; and a method of selecting which systems to show to users. These components are now described.,0,,False
45,3.1 The test collection,0,,False
46,The 50 million document Category B set of the ClueWeb09 collection was chosen as it was used in a diversity task for,1,ClueWeb,True
47,556,0,,False
48,...,0,,False
49,...,0,,False
50,"Figure 1 - Screen shown to MTurkers: containing query, subtopic, instructions, paired rankings, input buttons, and text box",0,,False
51,"TREC's 2009 Web track. Given a short ill specified query, the goal of the diversity task was for participating groups to build IR systems that returned a ranked list of documents that collectively fulfilled the multiple information needs represented by the query. For the diversity track, each topic was structured as a set of subtopics, each related to a different user need [13]. The documents returned in the submitted runs were judged with respect to each subtopic. For each retrieved document, TREC assessors made a binary judgment as to whether or not the document satisfied the subtopic's information need.",1,TREC,True
52,"Each one of the subtopics was categorized as being either navigational or informational (from Broder [9]). The query was also classified as either ambiguous or faceted, with ambiguous queries having multiple distinct interpretations while faceted queries had a single interpretation but with many aspects.",1,ad,True
53,"The structuring of subtopics judged in their own right into aggregated diverse topics, allowed (in this paper) both an experiment on diverse search and on non-diverse search: the first using the aggregated topics, the second treating the subtopics as a large set of ordinary topics.",0,,False
54,3.2 IR systems,0,,False
55,"A source of different outputs was needed against which user preferences could be measured. Al Maskari et al in their experiments drew from a pool of three live searching systems, however, the researchers often found that the systems performed very differently from each other, which unsurprisingly resulted in large differences in user preference. In the design of the experiments here, it was judged desirable to have more explicit control over the differences between the systems being compared. Allan et al and others achieved this by artificially creating search results; we judged it preferable to use actual search output.",0,,False
56,"Arni et al [7] used the runs of an evaluation exercise as a source of search outputs to draw from to show users. From that pool of runs the researchers were able to select those runs that had similar effectiveness scores. For the category B ClueWeb09 collection, 19 diversity runs were submitted from ten research groups, these were the pool of search outputs used. Their use is detailed in 3.5.",1,ad,True
57,3.3 Measuring user preference,0,,False
58,"To measure user preferences between different search results, the side-by-side method from Thomas et al [24] was chosen. For a particular topic, a pair of runs was selected from the pool and the top ten results (showing title, snippet and URL) were shown to users along with the topic title that generated the search and the subtopic description (referred to as an ""aspect"" in the interface) that expressed the information need behind the search request (example in Figure 1). The snippets were generated using a web service from the Bing search engine. Not all ClueWeb09 collection URLs still exist, which meant that 35% of results did not have a snippet. A post hoc analysis of data showed that missing snippets did not appear to influence user preferences.",1,ClueWeb,True
59,"Users were asked to indicate which of the two results they preferred. Using QREL data from the web track, effectiveness was measured on the two rankings and the agreement between users and the measures was assessed.",0,,False
60,"The aim of the diversity track was to promote searching systems that retrieved documents covering multiple interpretations of the same query, thereby ensuring that the search output was of value to the widest possible range of users. In a pilot experiment, an attempt was made to elicit user preferences for one IR system over another by asking individual users to indicate their preference for a ranking based on the ambiguous topic title alone. The expectation was that users would judge the value of search results relative to the multiple interpretations of a topic. However, it was found that the users were not able to do this reliably.",1,ad,True
61,"Therefore, in the experiments reported here, users were asked to focus on a particular subtopic and judge pairs of rankings in that context. They were asked to imagine they were searching for the subtopic using the query title text. The instructions were worded avoiding terms such as ""diversity"", so as not to bias choices. No other information about the experiment was given to the users. Users could indicate that the left or right result was better, both were equally good, or none of them were relevant (the ordering of paired systems was randomized). They were also asked to write a reason for their choice.",0,,False
62,Different users were given the different subtopics of a topic and their preferences were aggregated to form a judgment on the diverse topic as a whole.,0,,False
63,557,0,,False
64,3.4 Population of users,0,,False
65,The goal of the research work was to examine the preferences of a large number of users across many IR systems searching on a wide range of topics. It was decided to use the crowd sourcing system Mechanical Turk [5] to provide the large population.,0,,False
66,"Mechanical Turk users (MTurkers) were asked to judge a set of paired rankings for a set of subtopics. As it was assumed that there could be some disagreement amongst MTurkers, each pairing was seen on average by eight. A ""trap question"" was shown in an attempt to identify those who were not conducting the experiment in good faith. For every five comparisons shown to an MTurker one was a trap, which was built by pairing a run relevant to the required subtopic with a run for an entirely different topic. MTurkers who did not answer such pairings correctly had all of their answers rejected from the study (example in Figure 2). In total 342 MTurkers were used, 46 were rejected for failing a trap question (13%), which left 296 whose responses contributed to the results. We did not gather any demographic information from them. MTurkers were paid 8¢ for each block of five pairs they were shown. Many MTurkers worked on more than one block. The median time taken to complete the five pairs was just over 6 minutes. The total cost of the study including initial pilot studies was just under $60.",1,ad,True
67,3.5 Selecting measures,0,,False
68,The aim of the work was to examine how well evaluation measures predicted user preferences. Measures for both diversity and conventional IR were examined in this experiment.,0,,False
69,3.5.1 Diversity measures,0,,False
70,"With the growth of interest in diversity, a number of evaluation measures were proposed. These measures include Cluster Recall (CR) used in ImageCLEFPhoto 2008 [7], Clarke et al's -nDCG [11], Agrawal et al's intent aware Precision (IA-PC) [1], and Clarke et al.'s [12] novelty- and rank-biased precision (NRBP).",1,CLEF,True
71,"Cluster Recall (CR) is based on the subtopic recall (or S-Recall) proposed by Zhai et al. [28] to assess diversity. The CR at a cutoff rank k, CR(k), is defined as the percentage of subtopics covered by the first k documents in a ranked list. This is a pure diversity measure, i.e. it is not affected by the number of documents covering each cluster, or by their position in the ranked list. Further, it does not incorporate any notion of document novelty within a given subtopic.",1,corpora,True
72,"Contrary to CR, both -nDCG and NRBP consider both the number of relevant documents and their rank position over the subtopics of a query. For both measures, each document is assigned a gain value that is a function of the number of subtopics",0,,False
73,"the document covers, and for each subtopic, the number of documents ranked above the given document that cover the same subtopic. The variable  is used to control how important diversity is in the measure. The -nDCG metric is based on the traditional nDCG metric utilizing the aforementioned gain function, while the NRBP metric is based on Rank-Biased Precision (RBP). It is defined by replacing the traditional binary relevance of a document with the aforementioned gain. Thus, for a given subtopic, for ,""0 the two metrics do not assess the novelty of the subsequent documents that cover this subtopic, while for "",""1 they only consider relevant the first document that covers the given subtopic, ignoring all the subsequent ones.""",1,ad,True
74,"Finally, intent aware Precision at rank k accounts for diversity and the number of relevant documents. Given a query, the precision value at cut-off k is computed for each subtopic separately (i.e. only the documents that cover the subtopic under consideration are considered relevant ­ called aspect precision) and the weighted average of the precision values is computed, with weights being the popularity of each subtopic. In the Web Track data the subtopics of a query were assumed to be equally popular.",1,Track,True
75,3.5.2 Conventional evaluation measures,0,,False
76,"By considering each of the subtopics in the test collection as individual topics with their own QRELS, it was possible to examine differences across alternate conventional evaluation measures. Here nDCG, Mean Reciprocal Rank (MRR) and Precision measured at rank 10, P(10), were the measures chosen.",0,,False
77,3.6 Selecting the pairs to show users,0,,False
78,"As seen in Section 2.1, existing research showed that differences in user performance could be measured on IR systems with large differences in search effectiveness. The challenge was in measuring user preference when searching on IR systems with far smaller differences. Therefore, the selection of run pairs to show the MTurkers focused on finding pairs that were similar to each other. There was a concern that using runs with low effectiveness could result in confusion when choosing between rankings. Therefore, topics where all runs had two or fewer relevant documents in the top ten were removed. This left thirty topics in the dataset.",1,ad,True
79,"A search was conducted across the remaining topics to locate pairs of runs that had the same number of relevant documents in the top ten, done to ensure that the rankings were similar. To enable diversity measures to be tested, runs were only paired when there was more than a minimum difference in subtopic coverage: CR(10) and -nDCG(10) 0.1. Runs submitted by the same research group were not paired together.",1,ad,True
80,Figure 2 ­ Partial screen shot of a trap question shown to MTurkers 558,0,,False
81,"In total, 79 system pairs matching the search criteria were found. Each system pair was shown to, on average, eight MTurkers for each of a topic's subtopics. The MTurker judgments for one of the 79 pairs were gathered as follows. Each system pair displayed the two retrieval results for a search based on the query text of a particular topic. The MTurkers were asked to indicate their preference for one of the paired systems in relation to a particular subtopic. Multiple MTurkers were shown the same system/subtopic pair; although if an MTurker failed a trap question, their preference judgments were removed. MTurker preferences were treated as votes for one system or another normalized by the number of MTurkers who examined the system/subtopic pair.",0,,False
82,"This process was repeated for each of a topic's subtopics and the mean of the resulting normalized majority values was taken. The system that the majority of MTurkers preferred across the subtopics was selected as the best system for that topic. At the same time a diversity measure was calculated for the two system rankings, the best was the one with the highest effectiveness score. Across the 79 pairs, the number of times that MTurkers agreed/disagreed with the diversity measure was counted. If there was a tie, the MTurkers were judged to have said that the ranks from the systems were equal.",0,,False
83,4. RESULTS,0,,False
84,The predictive power of test collections/measures was examined on both the diverse web search topics (section 4.1) and their component subtopics (section 4.2).,0,,False
85,4.1 User preferences in diversity search,0,,False
86,"The results of the initial experiment are shown in Table 1. As can be seen, there was a preference amongst users for systems that were measured to be more diverse. Assuming a null hypothesis that MTurkers saw no difference between the paired systems, and the level of agreement was simply due to chance; using a t-test1 it was found that p<0.05; the null hypothesis was rejected and the level of agreement in Table 1 was found to be significant.",0,,False
87,Users,0,,False
88,-nDCG Small  Large ,0,,False
89,Agreed,0,,False
90,54 68% 26 60% 28 78%,0,,False
91,Ranks equal,0,,False
92,3 4% 3 7% 0 0%,0,,False
93,Disagreed,0,,False
94,22 28% 14 33% 8 22%,0,,False
95,79,0,,False
96,43,0,,False
97,36,0,,False
98,Table 1 ­ Agreement differences in small and large -nDCG,0,,False
99,"Next, the 79 pairs were placed into one of two bins: respectively those with a small and large difference in -nDCG. The pairs were sorted by their difference; those pairs greater than the mean of the differences were placed in the large  bin; the others in the small  bin. The figures for user agreement are also shown in Table 1. Although the agreement appeared to grow as the size of difference between the two rankings increased, a significance test between large and small  showed p>0.05.",0,,False
100,"The range of different cluster evaluation measures described above were also examined, see Table 2. In this test no significant difference between any pair of measures was found. It is notable that the measure CR provided as effective a prediction of user preference as the other measures. Cluster Recall is simply",0,,False
101,"1 A 2 tailed, 2 sample unequal variance test was used throughout.",0,,False
102,counting the percentage of topic interpretations that are covered in the ranking.,0,,False
103,Users,0,,False
104,-nDCG CR NRBP IA-PC,0,,False
105,Agreed,0,,False
106,54,0,,False
107,55,0,,False
108,54,0,,False
109,51,0,,False
110,Ranks equal,0,,False
111,3,0,,False
112,3,0,,False
113,3,0,,False
114,2,0,,False
115,Disagreed,0,,False
116,22,0,,False
117,21,0,,False
118,22,0,,False
119,26,0,,False
120,79,0,,False
121,79,0,,False
122,79,0,,False
123,79,0,,False
124,Table 2 ­ MTurkers' agreements to the diversity measures,0,,False
125,Kendall's  -nDCG@10 CR@10 NRBP,0,,False
126,CR@10 0.7956,0,,False
127,NRBP 0.8523 0.7159,0,,False
128,IA-PC@10 0.8424 0.7219 0.7010,0,,False
129,AP-correl. -nDCG@10 CR@10 NRBP,1,AP,True
130,CR@10 0.6719,0,,False
131,NRBP 0.8736 0.6282,0,,False
132,IA-PC@10 0.7867 0.5492 0.6839,0,,False
133,Table 3 ­ Correlations between diversity measures,0,,False
134,"Given that we have observed similar degrees of correlation between different diversity measures and user preferences, we next investigated how these different measures correlated with each other. Given that these measures assess somewhat different aspects of system effectiveness, a strong correlation would indicate that better systems are good in all aspects of effectiveness assessed by these measures. A weak correlation will indicate that different users prefer different qualities of the ranked lists and the sets of users whose preferences agree with each individual measure do not fully overlap even though it so happens to be of similar size.",0,,False
135,"For each measure, we considered the mean values for all systems/runs submitted to the TREC track and over all 50 queries were calculated. For each two measures, we calculated Kendall's , and the AP-correlation [27], see Table 3. Kendall's  is a function of the minimum number of pair wise adjacent interchanges needed to convert one ranking into the other. The AP-correlation is a similar metric, which however mostly accounts for the swaps towards the top of the system rankings, i.e. the disagreements over the top ranked systems. It can be seen that, there is a positive correlation among all measures, the strength of which however differs among different measures. In particular, the most correlated measures are -nDCG and NRBP. IA-PC and -nDCG are also well correlated, however, they mostly agree on the poorly performing systems as indicated by lower AP-correl. Further, there is a positive correlation between CR and -nDCG; however it also concerns the bottom performing systems. Finally, CR and IA-PC correlate well regarding the bottom performing systems but they rank the top performing systems differently.",1,TREC,True
136,"Therefore, the weak correlation among several of these measures indicates that indeed they assess different aspects of system performance. However, given the results in Table 2 it seems that all of these aspects are important for an average user.",0,,False
137,4.2 User preferences in traditional search,1,ad,True
138,"If one treats each subtopic of the test collection as a distinct test collection topic, with its own QRELS, one can compare user",0,,False
139,559,0,,False
140,preferences against traditional test collection measures. In total there were 252 subtopic/system pairs shown to MTurkers.,1,ad,True
141,"Three standard evaluation measures ­ nDCG, MRR, and P(10) ­ were applied to the pairs and a prediction of which ranking users would prefer was made based on each measure. The measures were selected as exemplars of particular features in evaluation: P(10) is a simple count of the number of relevant documents in the top 10; MRR measures the rank of the highest relevant; nDCG combines both number of relevant documents and their rank; nDCG's ability to handle degrees of relevance was not exploited as the diversity track QRELS contained binary judgments only.",1,ad,True
142,"If the effectiveness measure for the two rankings were the same, user preferences were not examined. Therefore the number of pairs considered differed across the three measures. The results of this analysis are shown in Table 4.",0,,False
143,Users,0,,False
144,nDCG,0,,False
145,MRR,0,,False
146,P(10),0,,False
147,Agreed,0,,False
148,160 63% 127 53% 106 50%,0,,False
149,Ranks equal,0,,False
150,28 11% 27 11% 24 11%,0,,False
151,Disagreed,0,,False
152,64 25% 87 36% 84 39%,0,,False
153,252,0,,False
154,241,0,,False
155,214,0,,False
156,Table 4 ­ MTurkers's agreement with traditional measures,1,ad,True
157,Use of the t-test showed that nDCG was significantly more in agreement with user preferences than the other two measures. There was no significant difference between MRR and P(10).,0,,False
158,"We considered the P(10) result in more detail. Because system pairs that were measured to be the same were removed from the figures in Table 4, in all of the 214 system comparisons shown to MTurkers, we can infer that one of the runs had retrieved more relevant documents than the other. Yet for only 50% of the pairs did MTurkers agree that the system with more relevant documents was the better one. In 39% of the cases MTurkers preferred the system with fewer relevant documents and in 11% they judged the system retrieving more relevant to be no better than the system returning fewer. The number of relevant documents retrieved is not the only factor that influences users.",1,ad,True
159,"The figures for MRR indicated that just focusing on the position of the top ranked document provided slightly more user agreement with user preferences. However, combining the two features into the nDCG measure appeared to be the best of the three strategies.",0,,False
160,"Focusing on nDCG, as in Section 4.1, the pairs were split into two bins: one for pairs with a large  and one for a small . The split was defined by the mean difference between the 252 pairs. The figures for user agreement are shown in Table 5.",0,,False
161,Users,0,,False
162,nDCG,0,,False
163,Small  Large ,0,,False
164,Agreed,0,,False
165,160 63% 93 60% 67 68%,0,,False
166,Ranks equal,0,,False
167,28 11% 20 13% 8 8%,0,,False
168,Disagreed,0,,False
169,64 25% 41 27% 23 23%,0,,False
170,252,0,,False
171,154,0,,False
172,98,0,,False
173,Table 5 ­ Agreement differences in small and large nDCG,0,,False
174,"It can be seen that users agree more when there was a large difference in the evaluation measure than when the difference is small. However, as with the comparison in Table 1 no significance was found.",0,,False
175,An alternate way to split the 252 pairs was on whether one of the two rankings contained no relevant documents. Table 6 and Table,0,,False
176,"7 show the agreement figures based on this split. Contrasting the strength of user agreement between Table 6 and Table 7, for all three columns MTurkers agreed more strongly when one pair of runs had nDCG,0. This was confirmed with a statistically significant difference being found between the figures in the first columns (nDCG) of the two tables.",1,ad,True
177,"If there was a larger difference between nDCG values in the system pairs in Table 6 compared to the pairs in Table 7 that could explain the differences in agreement level. However, an examination of mean  in nDCG between each of the 126 pairs in Table 6 and Table 7 showed little difference, respectively 0.164 and 0.169. The best explanation for the difference was that it was due to the presence of a zero nDCG in one of the pairs. The results suggest a need for evaluation measures, e.g. GMAP [19] which penalize systems that fail to return any relevant documents for particular topics.",1,MAP,True
178,Users,0,,False
179,nDCG,0,,False
180,Small  Large ,0,,False
181,Agreed,0,,False
182,88 70% 54 66% 34 77%,0,,False
183,Ranks equal,0,,False
184,13 10% 11 13% 2 5%,0,,False
185,Disagreed,0,,False
186,25 20% 17 21% 8 18%,0,,False
187,126,0,,False
188,82,0,,False
189,44,0,,False
190,"Table 6 ­ Analysis of pairs which had nDCG,0 in one result",1,ad,True
191,Users,0,,False
192,nDCG,0,,False
193,Small  Large ,0,,False
194,Agreed,0,,False
195,72 57% 39 54% 33 61%,0,,False
196,Ranks equal,0,,False
197,15 12% 9 13% 6 11%,0,,False
198,Disagreed,0,,False
199,39 31% 24 33% 15 28%,0,,False
200,126,0,,False
201,72,0,,False
202,54,0,,False
203,Table 7 ­ Analysis of pairs with nDCG>0 in both results,0,,False
204,"The final analysis of this data was to examine different types of topic. Within the TREC Web collection, a small number of the subtopics were navigational, most were informational [9]. User agreement was measured split across these 2 topic types (see Table 8).",1,TREC,True
205,Users,0,,False
206,nDCG,0,,False
207,Informational Navigational,0,,False
208,Agreed,0,,False
209,160 63% 146 62%,0,,False
210,14 82%,0,,False
211,Ranks equal 28 11%,0,,False
212,28 12%,0,,False
213,0 0%,0,,False
214,Disagreed,0,,False
215,64 25%,0,,False
216,61 26%,0,,False
217,3 18%,0,,False
218,252,0,,False
219,235,0,,False
220,17,0,,False
221,Table 8 ­ Analysis on different aspect types,0,,False
222,"For the small number of navigational topics, there was a strong agreement between users and the predictions made by the evaluation measures. No significance was found between the columns in this table. Given the small number of navigational topics, it would be valuable to repeat this experiment with a even balance in the number of navigational and informational topics.",1,ad,True
223,4.3 MTurker comments on differences,0,,False
224,"In addition to indicating their preferences, MTurkers could also provide comments about their choices. In total, 96% of the judgments had associated comments that often indicated the reason(s) behind, or affecting, a decision. These often highlighted factors beyond the results simply having more relevant documents on a topic (informational) or a link to a required webpage (navigational). There were 11.6 words per comment, on average, and using an inductive approach to data analysis [23] comments in which the users made a specific preference (54% of those submitted, 1,307) were categorized. Fifteen classes were derived",1,ad,True
225,560,0,,False
226,"and in 88 cases, comments were assigned multiple categories, e.g. ""the left one has more useful results higher in the search"" was assigned the classes 'position' and 'number' indicating that the number of results and their position in the ranking would have likely influenced their preference judgment, see Table 9.",0,,False
227,"Although these are factors which researchers often highlight as affecting relevance [8], we see these mentioned unprompted by the MTurkers in this study, again highlighting the benefit of using MTurk to gather data for this kind of study, beyond implicit feedback strategies such as query logs.",0,,False
228,5. REFLECTIONS ON METHOD,0,,False
229,Here we discuss using MTurk as a source of user preferences; and using preference as a means of determining the impact of search on users.,0,,False
230,5.1 Quality of the MTurk data,0,,False
231,"With an anonymous monetized crowd sourcing system, there is always a concern that data gathered will be overwhelmed with noise from spammers. However, evidence in our analysis such as time taken to complete tasks (median ~6 min.) of this set indicated that the majority of data was created in good faith. Indeed this gathering from hundreds of users of not only quantitative data, but also qualitative data gave this set a value that query/click logs do not have.",0,,False
232,"Nevertheless there are collections of data points in the set which we do not fully understand. Unexpected user responses to search results is not uncommon: Tagliacozzo [22], surveying ~900 MEDLINE users, described how nearly 18% declared dissatisfaction with search results despite earlier indicating that a large number of relevant documents were returned in the search they were asked to judge. Alonzo described how the development of MTurk experiments required multiple iterations to ensure that MTurkers were given clear instructions on what to do and to be certain that the responses from them are given in good faith [6].",0,,False
233,"The data set yielded a set of significant results on evaluation measures, but we view the method used here as a first step towards developing a more refined approach. Improvements will come not only from avoiding erroneous output from MTurkers, but also from building a more refined model of how users determine their preference for one searching system over another.",0,,False
234,5.2 Does preference imply satisfaction?,0,,False
235,"The ultimate aim of this and past research was to understand if differences found between IR systems based on test collections were real: with a better system, would users be able to search more effectively, achieve their goals quicker, ultimately be more satisfied? Results from past work indicated that measuring such broader outcomes were challenging as users were adept at adapting either by searching more intensively or by managing to exploit relevant but poorer output.",1,ad,True
236,The past work showed significant differences measured in a test collection did not necessarily show practical differences in user behavior. Was this simply because there was no practical difference to measure or was there simply a challenge in the way we measure users?,0,,False
237,"This and previous work showed that a preference based methodology can measure significant differences in users for relatively small differences in retrieval effectiveness. However, it",0,,False
238,"is worth remembering that the side-by-side method is a simulation of search and the measurement of preference says nothing about whether users are more satisfied with their search or will achieve their information seeking tasks more effectively. One might wish to hypothesize that such a link exists, but testing that hypothesis is for future work.",0,,False
239,Category Example,0,,False
240,#,0,,False
241,"On topic ""All the results about secret garden"" /",0,,False
242,332,0,,False
243,Contains work information in Michigan.,0,,False
244,"Specific ""the Wikipedia link for Hoboken will have most 265 links of the information I would be looking for"" / ""Right side has a link to the ESPN home page as asked for."" / ""Right links to the desired information, the left does not.""",1,Wiki,True
245,"Not ""Each result would be helpful, but the left was",0,,False
246,181,0,,False
247,"classifiable easier."" / ""I thought the left side was better.""",0,,False
248,"Irrelevant ""More non-relevant results in right column."" /",0,,False
249,144,0,,False
250,"#5 on the left side is not for flame design at

all",0,,False
251,"More ""more relevant"" / ""more relevant results on the 132 relevant right""",0,,False
252,"Number ""right has more map links"" / ""There are more",0,,False
253,123,0,,False
254,results for finding houses or apartments in,0,,False
255,"Hoboken""",0,,False
256,"Position ""Both lists include a link to reviews, but it's",0,,False
257,69,0,,False
258,"higher on the list on the left than on the right."" /",0,,False
259,Top results more closely align to the request.,0,,False
260,"Range of ""The right column has a broader range of",1,ad,True
261,66,0,,False
262,"results relevant topics."" / ""seemed to include more of a",0,,False
263,"variety of dino sites that would have what the person was looking for""",0,,False
264,"Presentation ""Results on the left are clearer and easier to",0,,False
265,36,0,,False
266,"read."" / ""right results are more descriptive""",1,ad,True
267,"Quality/ ""right had a porn result, left is better"" / ""Left",1,ad,True
268,16,0,,False
269,"authority side has more relevant results, as well as listing",0,,False
270,"more credible webpages""",0,,False
271,"Spam/ ""Left seems to be legit. Right is more of junk",0,,False
272,15,0,,False
273,"adverts and ads"" / ""Most of the right results are",1,ad,True
274,"advertisements"" / ""less spammy""/ ""Less",1,ad,True
275,"commercial, more focused on the real subject results."" / ""The right column just lists pet",0,,False
276,"adoption classified ads""",1,ad,True
277,"Duplication ""right results has three repeated listings of",0,,False
278,11,0,,False
279,"Verizon wireless and broad band services"" /",1,ad,True
280,Almost all of the right results were just Wikipedia pages.,1,Wiki,True
281,"Availability ""we can download the maps"" / ""more free data""",1,ad,True
282,6,0,,False
283,"Language ""Left results have more relevancy, every link",0,,False
284,6,0,,False
285,has something about pampered chef or related,0,,False
286,"equipment in English""",0,,False
287,"Dead links ""'The right had dead links"" / ""The left had a few",1,ad,True
288,5,0,,False
289,links which did not work but the majority did,0,,False
290,"and returned results on appraisals.""",0,,False
291,"Table 9 ­ Analysis of 1,307 MTurk comments",0,,False
292,6. CONCLUSIONS AND FUTURE WORK,0,,False
293,The research questions posed at the start of the paper were answered through the gathering and examination of a large dataset,0,,False
294,561,0,,False
295,of user preferences for a wide range of different IR systems tested over many queries and query types.,0,,False
296,Clear evidence was found that effectiveness measured on a test collection predicted user preferences for one IR system over another. The strength of user prediction by test collection measures appeared to vary across different search tasks such as navigational or informational queries.,0,,False
297,"For diverse queries, little difference between diversity measures was found. A conventional analysis of correlation between the measures was conducted confirming that they are similar. When comparing nDCG, MRR and P(10) it was found that nDCG most effectively modeled user preferences. However, user preferences between pairs of systems where one had failed to retrieve any relevant documents were notably stronger than when both rankings had at least one relevant document, which suggests a need for adjustments to these measures to allow for this user view.",1,ad,True
298,"Finally, an examination and grouping of the written reasons that users provided for choosing one system ranking over another was outlined. Here it was shown that although relevance, rank, and information content of the documents was an important factor when users chose one system over another, a wide range of other reasons was also provided by users. Specific web sites were sought by some; avoidance of commercial sites or documents in other languages was desired by others. Such information from the MTurkers suggested that the test collections, relevance judgments and evaluation measures could be improved to provide a more effective model of user preferences than is currently available.",0,,False
299,"Such ideas are left to future work, where we will also continue to analyze our gathered data set as well as consider how to refine our collection methods to gather larger more informative sets in the future.",0,,False
300,7. ACKNOWLEDGMENTS,0,,False
301,This work was co-funded by the EU FP7 project ACCURAT contract FP7-ICT-248347 and Marie Curie Fellowship FP7PEOPLE-2009-IIF-254562.,0,,False
302,8. REFERENCES,0,,False
303,"[1] Agrawal, R., Gollapudi, S., Halverson, A., & Ieong, S. 2009. Diversifying search results. ACM WSDM, 5-14.",0,,False
304,"[2] Al-Maskari, A., Sanderson, M. & Clough, P., 2007. The relationship between IR effectiveness measures and user satisfaction. ACM SIGIR, 773-774.",0,,False
305,"[3] Al-Maskari, A., Sanderson, M., Clough, P., & Airio, E. 2008. The good and the bad system: does the test collection predict users' effectiveness? ACM SIGIR, 59-66.",1,ad,True
306,"[4] Allan, J., Carterette, B., & Lewis, J. 2005. When will information retrieval be ""good enough""? ACM SIGIR, 433440.",0,,False
307,"[5] Alonso, O., Rose, D. E., & Stewart, B. 2008. Crowdsourcing for relevance evaluation. SIGIR Forum 42, 2, 9-15.",0,,False
308,"[6] Alonso, O. & Mizzaro, S., 2009. Can we get rid of TREC assessors? Using Mechanical Turk for relevance assessment.",1,TREC,True
309,"In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation. 15­16. [7] Arni, T., Tang, J., Sanderson, M., Clough, P. 2008. Creating a test collection to evaluate diversity in image retrieval. Workshop on Beyond Binary Relevance, SIGIR 2008.",0,,False
310,"[8] Barry, C. L. 1994. User-defined relevance criteria: an exploratory study. J. Am. Soc. Inf. Sci. 45, 3, 149-159.",0,,False
311,"[9] Broder, A. 2002. A taxonomy of web search. SIGIR Forum 36(2) 3-10.",0,,False
312,"[10] Chapelle, O. and Zhang, Y., 2009. A dynamic bayesian network click model for web search ranking. Proc. 18th WWW Conf, 1-10",0,,False
313,"[11] Clarke, C., Kolla, M., Cormack, G., Vechtomova, O., Ashkan, A., Büttcher, S., MacKinnon, I. 2008. Novelty & diversity in information retrieval evaluation. ACM SIGIR, 659-666.",1,Novelty,True
314,"[12] Clarke, C., Kolla, M., & Vechtomova, O. 2009. An Effectiveness Measure for Ambiguous and Underspecified Queries. Advances in Information Retrieval Theory, 188-199.",0,,False
315,"[13] Clarke, C., Craswell, N., and Soboroff, I. 2009. Preliminary Report on the TREC 2009 Web Track. TREC 2009 Notebook.",1,TREC,True
316,"[14] Hersh, W.R. et al., 2002. Factors associated with success in searching MEDLINE and applying evidence to answer clinical questions, Am Med Inform Assoc.",0,,False
317,"[15] Hersh, W., Turpin, A., Price, S., Chan, B., Kramer, D., Sacherek, L., & Olson, D. 2000. Do batch and user evaluations give the same results? ACM SIGIR, 17-24.",0,,False
318,"[16] Huuskonen, S. & Vakkari, P. 2008. Students' search process and outcome in Medline in writing an essay for a class on evidence-based medicine. Journal of Documentation, 64(2), 287-303.",0,,False
319,"[17] Joachims, T., 2002. Evaluating retrieval performance using click through data. Workshop on Mathematical/Formal Methods in IR, 12­15.",0,,False
320,"[18] Radlinski, F., Kurup, M., Joachims, T. 2008. How does click through data reflect retrieval quality? ACM CIKM, 43-52.",1,ad,True
321,"[19] Robertson, S. 2006. On GMAP: and other transformations, ACM CIKM, 78-83",1,MAP,True
322,"[20] Smith, C.L. & Kantor, P.B., 2008. User adaptation: good results from poor systems. ACM SIGIR, 147-154.",1,ad,True
323,"[21] Su, L.T., 1992. Evaluation measures for interactive information retrieval. IP&M, 28(4), 503-516.",0,,False
324,"[22] Tagliacozzo, R., 1977. Estimating the satisfaction of information users. Bulletin of the Medical Library Association, 65(2), 243-249.",0,,False
325,"[23] Thomas, D.R. 2006. A General Inductive Approach for Analyzing Qualitative Evaluation Data. American Journal of Evaluation, 27(2), 237-246",0,,False
326,"[24] Thomas, P. & Hawking, D., 2006. Evaluation by comparing result sets in context. ACM CIKM, 94-101.",0,,False
327,"[25] Turpin, A.H., Hersh, W. 2001. Why batch and user evaluations do not give the same results. ACM SIGIR, 225231.",0,,False
328,"[26] Turpin, A. & Scholer, F. 2006. User performance versus precision measures for simple search tasks. ACM SIGIR, 1118.",0,,False
329,"[27] Yilmaz, E., Aslam, J., Robertson, S. 2008. A new rank correlation coefficient for information retrieval. ACM SIGIR, 587-594.",0,,False
330,"[28] Zhai, C.X., Cohen, W.W. & Lafferty, J., 2003. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. ACM SIGIR, 10-17.",0,,False
331,562,0,,False
332,,0,,False

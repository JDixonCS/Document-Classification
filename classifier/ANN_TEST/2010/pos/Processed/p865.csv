,sentence,label,data,regex
0,Aspect Presence Verification Conditional on Other,0,,False
1,Aspects,0,,False
2,Dmitri Roussinov,0,,False
3,"University of Strathclyde 16 Richmond Street, Glasgow, UK G1 1XQ",0,,False
4,+44 141 548 3706,0,,False
5,dmitri.roussinov@cis.strath.ac.uk,0,,False
6,ABSTRACT,0,,False
7,"I have shown that the presence of difficult query aspects that are revealed only implicitly (e.g. exploration, opposition, achievements, cooperation, risks) can be improved by taking advantage of the known presence of other, easier to verify query aspects. The approach proceeds by mining a large external corpus and results in substantial improvements in re-ranking the subset of the top retrieved documents.",1,ad,True
8,Categories and Subject Descriptors,0,,False
9,H.3.3 [Information Search and Retrieval]: Retrieval models; ­ Retrieval Models,0,,False
10,General Terms,0,,False
11,"Algorithms, Experimentation, Theory.",0,,False
12,Keywords,0,,False
13,"Information retrieval, machine learning, external corpus.",0,,False
14,1. INTRODUCTION,1,DUC,True
15,"It has been noticed that a common reason for the search results to be of poor quality is missing one or more aspects of the user information need [1], where an aspect can be represented by a subset of query words. E.g., in the query antarctica exploration, the word antarctica is relatively rare and specific. Along with its same-stem variants, it explicitly occurs in virtually all documents about Antarctica. On the other side, the word exploration, capturing the other aspect of the query, is not only more frequent, but also represents a higher level concept (theme, topic, etc.), often revealing itself only implicitly by such statements like ""... scientists began to collect data..."" The ranking of the retrieved documents by currently state-of-the-art algorithms would be primarily determined by antarctica, mildly affected by the explicit occurrence of the word exploration, and not affected by the implicit presence of the exploration theme. Although techniques aimed at detecting and representing missing aspects have been suggested [2][3] they have not been yet methodologically studied or revealed convincing improvements. I suggest that it was because they were essentially limited to the bag-of-words representations and the query expansion models. On the contrary, this research builds on the prior works studying the prediction of occurrence of given words (concepts) in a given text context (e.g. a sentence) such as [5][7]. Specifically, I proceeded by the exploring the following innovations: 1) Going beyond the bag of words by looking for the indicators of implicit aspect presence among all the sequences of words",0,,False
16,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
17,"(up to a certain length) from the document (e.g. polar station, oil drilling proposed, expedition ships, etc.) rather than relying on a finite number of expansion terms. 2) Considering the problem of aspect verification conditional on the presence of other, often easier to establish aspects. E.g., in a document about Antarctica, station is a good indication of the exploration theme, while not necessary so in unrestricted context. 3) Modeling subsumption of indicators, e.g. if the word station is a part of train station then its connection to the exploration theme is weaker.",0,,False
18,2. FRAMEWORK,0,,False
19,"Here, I considered the simplest, but a common situation with the two aspects in a query, each can be specified by one or more keywords: 1) Ap is already known to be present in the document d and 2) the other aspect Am, explicit representation of which is missing in the document, thus simply referred below as the ""missing"" aspect. While Am is typically a difficult aspect to verify, as I have demonstrated here, the task of estimating the presence of Am conditional on the presence of Ap happens to be easier. As the possible indicators of the aspect implicit presence, I considered all the sequences of words (up to length of 3) in a document. For each indicator i, the algorithm estimated P(Am|i,Ap) , the probability of the occurrence of Am within the proximity (e.g. same sentence) of i, conditional on the occurrence of Ap. In order to do that, a regression (M5P model tree [6]) was trained independently for each topic, using the normalized frequency counts obtained from Microsoft's Bing search engine as independent variables. For example, the high ratio #((station NEAR exploration) AND Antarctica)/#(station AND A ntarctica) would indicate the high probability of occurrence of exploration near the word station in the external corpus, conditional on the presence of Antarctica. The snippets returned by Bing for the query representing both aspects (e.g. antarctica NEAR exploration) served as training examples, thus no parameter tuning was necessary. The advantage of using M5P was learning automatically the reliability thresholds for the frequency counts. The subsumption step of the algorithm discarded all the subsequences (e.g. station) within the word sequences (e.g. train station) for which the reliable estimates of the conditional probabilities were obtained. This resulted in the average of 531 preserved indicators per document, with 90% of the strongest 100 indicators having 2 or more words, some of those presented in Table 2 below. To rank the documents, the sum of all the conditional probability estimates over all the remaining indicators was treated as the total number of ""implied"" occurrences of the missing aspect:",1,ad,True
20,865,0,,False
21," tf m ,",0,,False
22,"P ( Am | i , Ap ) , and the bm25 formula was",0,,False
23,i d,0,,False
24,applied.,0,,False
25,3. EMPIRICAL EVALUATION,0,,False
26,B1,0,,False
27,B2,0,,False
28,Entire Wikipedi,1,Wiki,True
29,Web,0,,False
30,a only,0,,False
31,MAP,1,MAP,True
32,0.39,0,,False
33,0.43,0,,False
34,0.65**,0,,False
35,.53*,0,,False
36,Standard,0,,False
37,0.13,0,,False
38,0.11,0,,False
39,0.16,0,,False
40,.14,0,,False
41,Deviation,0,,False
42,% change -9% n/a,0,,False
43,+51%,0,,False
44,+23%,0,,False
45,over B2,0,,False
46,Table 1. The results of the evaluation. Statistically significant differences at the levels of 0.05 and .1 are marked with ** and * accordingly.,0,,False
47,"Since the approaches studied here involved a large number of time-consuming external queries, I built a small but ""clean"" data set that was sensitive enough to evaluate various configurations and the overall verification accuracy of the techniques suggested here based on the top ranked documents retrieved by bm25 ranking function (described as B1 below) using the HARD 2005 TREC topics, which are known to be difficult and often result in missing aspects [1]. I chose only those topics in which 1) It was possible to interpret the title as consisting of missing and present aspects. 2) The present aspect was occurring in at least 90% of the top 20 documents. 3) The missing aspect was not explicitly occurring in more than half of the top 20 documents. This left me with 18 topics, some examples of them listed in Table 2. Only up to 20 top ranked irrelevant and relevant documents were selected, and only those that did not have the missing aspect mentioned explicitly. I also removed (approx. 10% of) the documents assessed by TREC as irrelevant but still having both aspects present in order to reduce the sensitivity of the evaluation to the details supplied only in the narratives of the topics and thus not available to neither the baseline nor the suggested algorithms. The resulting data set was almost ""balanced"" with approximately 15 negative and 10 positive examples on average per each topic. I compared the performance against two strong baselines: B1 was obtained by applying bm25 ranking formula using the topic titles only, with",1,TREC,True
48,"stemming and pseudo-relevance feedback using Lemur retrieval engine default settings. Since the approach here involves an external corpus, to make a fair comparison, I also involved B2 obtained and optimized the same way as in [4], which was essentially an application of a relevance model with an external corpus (here, using Bing's snippets).",0,,False
49,4. CONCLUSIONS,0,,False
50,"As the results in Table 1 indicate, it was possible to significantly improve verifying the presence of the difficult implicit aspects by the techniques suggested here. Also, no specific topic has been harmed as a result. While the actual impact on a larger set of topics remains to be seen, I have nevertheless been able to handle a very common scenario with the implicit presence of a difficult aspect (e.g. exploration) and the explicit presence of an easier aspect (e.g. Antarctica). Although the dataset involved in experiments here was very small, it still represented the top ranked results from the state of the art techniques, and thus being able to improve them by re-ranking has great practical implications. The ablation studies (not reported here due to the space limitations) confirmed that each of the novelties 1-4 stated in the introduction was crucial. While sending queries to a search portal delayed the processing substantially, in order to achieve the real time speeds, future implementations can make use of the processed large corpus data such as Google's 1T or a faster access to a search engine index.",0,,False
51,5. REFERENCES,0,,False
52,"[1] Buckley, C. Why current IR engines fail. SIGIR 2004. [2] Collins-Thompson, K., Callan, J. Query expansion using",1,Query,True
53,"random walk models. CIKM 2005. [3] Crabtree, D.W., Andreae, P. Gao, X. Exploiting",0,,False
54,"underrepresented query aspects for automatic query expansion. SIGKDD 2007. [4] Diaz, F. and Metzler, D. Improving the estimation of relevance models using large external corpora. SIGIR 2006. [5] Edmonds, P. Choosing the word most typical in context using a lexical co-occurrence network, ACL 1997. [6] Monz, C. Model Tree Learning for Query Term Weighting in Question Answering. Advances in Information Retrieval, Volume 4425/2007, pp. 589-596. [7] SzeWang F., Roussinov, D., Skillicorn, D.B. Detecting Word Substitutions in Text. IEEE TKDE, 2008.",1,corpora,True
55,Table 2. Top indicators of implicit aspect presence for some topics. The missing aspects are underlined. Bold font highlights the,0,,False
56,indicators that are specific to the present aspect.,0,,False
57,Topic,0,,False
58,Strongest Indicators of Presence,0,,False
59,"Transportation train caught fire, people were killed, caused by sparks, the midst of, described the crash, in spite",0,,False
60,Tunnel,0,,False
61,"of, millions of dollars, quickly as possible, billions of dollars, around the corner, was caused by,",0,,False
62,Disasters,0,,False
63,"turned into a, the worst, rescue, emergency, explosion, coal, killed, fire",0,,False
64,Black Bear,0,,False
65,"were forced to, bear attempted to, fire, the bear was, officials say, reduced, kill, defense,",0,,False
66,Attacks,0,,False
67,"occurred in, officials said, carcass, attract, documented, bear was sighted, dangerous, killed",0,,False
68,"by, ripped, threatening",0,,False
69,Iran Iraq,0,,False
70,"Iran will ship, minister to visit, delegation will visit, positive, visit to Iraq, oil exports, Iranian",0,,False
71,"Cooperation delegation, normalizing, relations, the joint, agreement with Iran, mutual, to bilateral, visit to Iran,",0,,False
72,"accepted an invitation, invitation to visit, normalization, war ended in, the withdrawal of, gesture",0,,False
73,"Journalist Risks journalist was shot, press freedom, murdered, circumstances, in prison, Iraq, Russia, body, killer,",0,,False
74,"gunned down, win, posed, detained",0,,False
75,866,0,,False
76,,0,,False

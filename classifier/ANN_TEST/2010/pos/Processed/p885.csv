,sentence,label,data,regex
0,Achieving High Accuracy Retrieval using Intra-Document Term Ranking,0,,False
1,Hyun-Wook Woo hwwoo@nlp.korea.ac.kr,0,,False
2,Jung-Tae Lee jtlee@nlp.korea.ac.kr,0,,False
3,Seung-Wook Lee swlee@nlp.korea.ac.kr,0,,False
4,Young-In Song yosong@microsoft.com,0,,False
5,Hae-Chang Rim rim@nlp.korea.ac.kr,0,,False
6," Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea  Microsoft Research Asia, Beijing, China",1,ad,True
7,ABSTRACT,0,,False
8,"Most traditional ranking models roughly score the relevance of a given document by observing simple term statistics, such as the occurrence of query terms within the document or within the collection. Intuitively, the relative importance of query terms with regard to other individual non-query terms in a document can also be exploited to promote the ranks of documents in which the query is dedicated as the main topic. In this paper, we introduce a simple technique named intra-document term ranking, which involves ranking all the terms in a document according to their relative importance within that particular document. We demonstrate that the information regarding the rank positions of given query terms within the intra-document term ranking can be useful for enhancing the precision of top-retrieved results by traditional ranking models. Experiments are conducted on three standard TREC test collections.",1,ad,True
9,Categories and Subject Descriptors,0,,False
10,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Retrieval models,0,,False
11,General Terms,0,,False
12,"Algorithms, Experimentation",0,,False
13,Keywords,0,,False
14,"inter-document term ranking, precision at top ranks",0,,False
15,1. INTRODUCTION,1,DUC,True
16,"With the rapid growth of Web document collection sizes in recent years, achieving high precision at the top of the retrieved result has become a major issue for search engine users [1]. In traditional IR ranking models [2, 3], the relevance of a document for a given query is scored primarily based on simple term statistics, such as the number of occurrence of query terms within the document or within the whole document collection. Basically, the more query terms occur in a document, the higher the chance that the document would be relevant to the query and thus would be",1,ad,True
17,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
18,"provided at the top part of the ranked list. Despite its simplicity, such a scoring method has been effective in many previous document ranking experiments.",0,,False
19,"However, traditional models virtually do not consider the relative importance of query terms compared to other individual non-query terms within a document in ranking. For example, consider the query word ""q"" and the following two equal-length documents at top ranks in which ""q"" occurs twice: ""q q b c d e f "" and ""q q b b b b c"". Traditional ranking models would assume that the weight of ""q"" is roughly the same for both documents. However, this is indeed not true if we examine the relative importance of ""q"" within individual documents. In the first document, ""q"" occurs relatively more than any of the other non-query terms, which implies that the document may be mainly about ""q"". In contrast, in the second document, ""q"" occurs relatively less than the word ""b"", which implies that ""q"" may be a subtopic of that document. Intuitively, given two documents that both contain the same number of query terms, we would like to rank the document in which the query is dedicated as a main topic above the one where the query is regarded as a minor topic.",1,ad,True
20,"The relative importance of query terms with regard to other terms in a document can be observed by ranking all the terms in the document with some weighting scheme, such as the tf-idf method. We refer to this technique as intra-document term ranking. In this paper, we present two heuristic document ranking methods based on the rank positions of given query terms in intra-document term ranking lists of individual documents. Our hypothesis is two-fold:",0,,False
21,"· First, when query terms are ranked in relatively higher term rank positions than other terms in a given document, it implies that the query is dedicated as the central topic of the document. Thus, there is a higher chance that the document is relevant to the query.",0,,False
22,"· Second, individual query terms having similar term rank positions to each other in a document reflect that they are treated equally. Thus, there is a chance that the query terms have close relationships to each other within the document, which implies higher relevance.",0,,False
23,The succeeding section will elaborate on how we rank documents based on the two hypotheses. We validate our method on three standard TREC test collections.,1,TREC,True
24,885,0,,False
25,2. PROPOSED METHOD,0,,False
26,"To analyze the relative importance of individual terms that occurred in a given document, we rank the terms by their standard tf-idf weights. We use the term frequency normalized with the document length as the tf component, and the logarithm of the inverse document frequency as the idf component. We normalize the ranks in range 0 to 1 so that the term with highest tf-idf weight will be at rank 0 and the term with the lowest weight at rank 1.",0,,False
27,"Given this ranked list of terms within a document, we derive two document ranking heuristics based on the rank positions of query terms within the list. The first heuristic, which corresponds to our first hypothesis, ranks documents according to the average rank position of query terms. Here we basically assume that the higher the rank of the query term within a document, the more likely that the document is relevant. Given a query Q and the term ranking result of a document D, the average rank position of query terms is calculated as follows:",0,,False
28,"R1(Q, D) ,",0,,False
29,"qiQ{1 - rank(qi, D)} |Q|",0,,False
30,(1),0,,False
31,"where qi represents ith query term, |Q| is the size of the query in words, and rank(qi, D) is a function that returns the normalized term rank of qi in D.",0,,False
32,"The second heuristic, which corresponds to our second hypothesis, assumes that the more cohesive the rank positions of query terms (i.e. have similar rank positions to each other) in a document, the higher the probability that the document is relevant. There are a number of ways to measure the cohesiveness of the term ranks. In this paper, we calculate such measure by the maximum rank difference of all pairs of query terms as follows:",0,,False
33,"R2(Q, D) ,"" 1 - { max {dif f (qi, qj , D)}} (2) qi ,qj QD,qi"",qj",0,,False
34,"where dif f (qi, qj , D) is a function that returns the absolute value of the difference in rank positions of qi and qj . For example, if three query terms were ranked at 0, 0.1, and 0.4, the maximum difference would be 0.3.",0,,False
35,3. EXPERIMENTS,0,,False
36,"For test collections, we use the following standard TREC collections: AP88-90 (Associated Press news 1988-90), WT2g (2 gigabyte Web data), and Blogs06 (Web data used in TREC Blog Track 2006-08). For queries, we use the title field of the TREC topics 50-150 for AP88-90, 401-450 for WT2g, and 851-950 and 1001-1050 for Blogs06.",1,TREC,True
37,"To investigate whether the two new heuristics can infer the true relevance for documents at top ranks of the retrieved list by traditional ranking models, we retrieved the top 20 documents from the collections using query-likelihood language models [2] with dirichlet prior1 (LM) and computed the correlation between the output value of either of the heuristics and the relevance. We observe that both have positive correlations with document relevance (0.21 for R1 and 0.31 for R2). This result is consistent with our two hypotheses.",1,ad,True
38,"We validate the effectiveness of the two ranking heuristics in a re-ranking scheme. First, given a query, we generate a",0,,False
39,"1We tuned µ to be optimal for each collection (1000, 3000, and 5000 for AP88-90, WT2g, and Blogs06, respectively).",1,AP,True
40,Table 1: Retrieval performance.,0,,False
41,Corpus Method MRR,0,,False
42,P@1,0,,False
43,P@5,0,,False
44,LM,1,LM,True
45,0.4598 0.3100 0.2840,0,,False
46,+R1,0,,False
47,0.4433 0.3000 0.2740 (-3.59%) (-3.23%) (-3.52%),0,,False
48,AP88-90 +R2,1,AP,True
49,0.4671 0.3200 0.2900 (1.59%) (3.23%) (2.11%),0,,False
50,+R1+R2,0,,False
51,0.4743 (3.15%),0,,False
52,0.3200 (3.23%),0,,False
53,0.3020 (6.34%),0,,False
54,LM,1,LM,True
55,0.6342 0.6000 0.4880,0,,False
56,+R1,0,,False
57,0.6874 0.5800 0.4640 (8.39%) (-3.33%) (-4.92%),0,,False
58,WT2g +R2,1,WT,True
59,0.7151 0.6200 0.4640 (12.76%) (3.33%) (-4.92%),0,,False
60,+R1+R2,0,,False
61,0.7097 (11.90%),0,,False
62,0.6200 (3.33%),0,,False
63,0.5000 (2.64%),0,,False
64,LM,1,LM,True
65,0.6634 0.5333 0.5747,0,,False
66,+R1,0,,False
67,0.7090 0.6000 0.5987 (6.87%) (12.51%) (4.18%),0,,False
68,Blogs06 +R2,1,Blogs06,True
69,0.7404 0.6467 0.6067 (11.61%) (21.26%) (5.57%),0,,False
70,+R1+R2,0,,False
71,0.7384 (11.31%),0,,False
72,0.6400 (20.01%),0,,False
73,0.6453 (12.28%),0,,False
74,"ranked list consisting of top n documents from a test collection using LM, which represents a state-of-the-art baseline. Then, we create a new ranked list of the n documents using each heuristic function. We finally re-rank the documents in the initial list in the ascending order of the mean rank of documents among individual ranked lists. If a tie occurs, a document that has a higher rank in the initial ranked list is promoted. We compare the top results of the re-ranked result with the baseline result using Mean Reciprocal Rank (MRR) and Precision at k ranks (P@1 and P@5). The Indri implementation2 is used for indexing and retrieval; stemming and stopword removal are performed.",1,LM,True
75,"The retrieval performances of the baseline and the proposed method are presented in Table 1. We observe that when the baseline ranking is aggregated with either one of the heuristic rankings, the improvement is not consistent across different data collections. However, when the baseline ranking is merged with all two heuristic rankings, the improvement over the baseline is consistent across all collections for all evaluation measures. This demonstrates that the analysis of rank positions of query terms in interdocument term ranking is effective in improving the precision at top ranks. For future work, we plan to explore other features that capture various aspects of inter-document term ranking. We also plan to investigate on designing a unified ranking model that combines the proposed heuristics with the features of the traditional retrieval models.",1,ad,True
76,4. REFERENCES,0,,False
77,"[1] I. Matveeva, C. Burges, T. Burkard, A. Laucius, and L. Wong. High accuracy retrieval with multiple nested ranker. In SIGIR '06, 2006.",0,,False
78,"[2] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR '98, 1998.",0,,False
79,"[3] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613­620, 1975.",0,,False
80,2http://www.lemurproject.org/indri/,0,,False
81,886,0,,False
82,,0,,False

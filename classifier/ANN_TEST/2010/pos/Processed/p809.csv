,sentence,label,data,regex
0,Many are Better Than One: Improving Multi-Document Summarization via Weighted Consensus,0,,False
1,Dingding Wang Tao Li,0,,False
2,School of Computer Science Florida International University,0,,False
3,"Miami, FL 33199",0,,False
4,"{dwang003,taoli}@cs.fiu.edu",0,,False
5,ABSTRACT,0,,False
6,"Given a collection of documents, various multi-document summarization methods have been proposed to generate a short summary. However, few studies have been reported on aggregating different summarization methods to possibly generate better summarization results. We propose a weighted consensus summarization method to combine the results from single summarization systems. Experimental results on DUC2004 data sets demonstrate the performance improvement by aggregating multiple summarization systems, and our proposed weighted consensus summarization method outperforms other combination methods.",1,DUC,True
7,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval.,0,,False
8,"General Terms: Algorithms, Experimentation.",0,,False
9,"Keywords: Weighted consensus, summarization.",0,,False
10,1. INTRODUCTION,1,DUC,True
11,"Various multi-document summarization methods base on different strategies and usually produce diverse outputs. A natural question arises: can we perform ensemble or consensus summarization by combining different summarization methods to improve summarization performance? In general, the terms of ""consensus methods"" or ""ensemble methods"" are commonly reserved for the aggregation of a number of different (input) systems. Previous research has shown that ensemble methods, by combining multiple input systems, are a popular way to overcome instability and increase performance in many machine learning tasks, such as classification, clustering and ranking. The success of ensemble methods in other learning tasks provides the main motivation for applying ensemble methods in summarization. To the best of our knowledge, so far there are only limited attempts on using ensemble methods in multi-document summarization.",0,,False
12,"As a good ensemble requires the diversity of the individual members, here we study several widely used multi-document summarization systems based on a variety of strategies, and evaluate different baseline combination methods for obtaining a consensus summarizer to improve the summarization performance. Motivated from [5], we also propose a novel weighted consensus scheme to aggregate the results from",0,,False
13,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
14,"individual summarization methods, in which, the relative contribution of an individual summarizer to the consensus is determined by its agreement with other members of the summarization systems. Note that usually a high degree of agreements does not automatically imply the correctness since the systems could agree on a faulty answer. However, each of the summarization systems has shown its effectiveness individually, so the agreement measure can be used in the consensus summarization.",0,,False
15,2. WEIGHTED CONSENSUS SUMMARIZATION (WCS),1,WCS,True
16,2.1 Notations,0,,False
17,"Suppose there are K single summarization methods, each of which produces a ranking for the sentences containing in the document collection. Then we have K ranking lists {r1, r2, · · · , rK} and ri  RN , i ,"" 1, · · · , K, where N is the total number of sentences in the documents. The task is to find a weighted consensus ranking of the sentences r with a set of weights {w1, w2, · · · , wK } assigning to each of the individual summarization methods.""",0,,False
18,2.2 Formulation,0,,False
19,"Our goal is to minimize the weighted distance between r and all the ri. Let w ,"" [w1, w2, . . . , wK ]T  RK . The problem can be formulated as follows.""",0,,False
20,arg min,0,,False
21,w,0,,False
22,s.t.,0,,False
23,K,0,,False
24,(1 - ) wi r - ri 2 +  w 2,0,,False
25,"i,1",0,,False
26,K,0,,False
27,"wi ,"" 1; wi  0 i,""",0,,False
28,"i,1",0,,False
29,where 0    1 is the regularization parameter which spec-,0,,False
30,ifies the tradeoff between the minimization of the weighted,1,ad,True
31,"distance and the smoothness enforced by w. In experiments,",0,,False
32," is set to 0.3 empirically. For simplicity, we use Euclidean",0,,False
33,distance to measure the discordance of the consensus rank-,0,,False
34,ing r and each of individual sentence rankings ri.,0,,False
35,We,0,,False
36,initialize,0,,False
37,wi,0,,False
38,",",0,,False
39,1 K,0,,False
40,",",0,,False
41,and,0,,False
42,this,0,,False
43,optimization,0,,False
44,problem,0,,False
45,can,0,,False
46,be solved by iterating the following two steps:,0,,False
47,Step 1: Solve for r while fixing w. The optimal solution,0,,False
48,is,0,,False
49,"the weighted average r , Step 2: Solve for w while",0,,False
50,i wi fixing,0,,False
51,ri. r,0,,False
52,.,0,,False
53,Let,0,,False
54,"d ,"" [ r - r1 2, r - r2 2, · · · , r - rK 2]  RK .""",0,,False
55,809,0,,False
56,Note that,0,,False
57,K,0,,False
58,"(1 - ) wi r - ri 2 +  w 2 , (1 - )d w + w w",0,,False
59,"i,1",0,,False
60,",",0,,False
61,w-,0,,False
62,-1 2 d,0,,False
63,2-,0,,False
64,( - 1)2 4,0,,False
65,d2,0,,False
66,"For fixing r, the optimization problem becomes",0,,False
67,arg min,0,,False
68,w,0,,False
69,w,0,,False
70,-,0,,False
71,-,0,,False
72,1 d,0,,False
73,2,0,,False
74,s.t.,0,,False
75,2,0,,False
76,K,0,,False
77,"wi , 1;",0,,False
78,"i,1",0,,False
79,"wi  0,",0,,False
80,i,0,,False
81,This is a quadratic function optimization problem with,1,ad,True
82,linear constraints with K variables. This is a problem of,0,,False
83,"just about tens of variables (i.e., weights for each input sum-",0,,False
84,marization system) and thus can be computed quickly. It,0,,False
85,can,0,,False
86,also,0,,False
87,be,0,,False
88,solved,0,,False
89,by,0,,False
90,simply,0,,False
91,projecting,0,,False
92,vector,0,,False
93,-1 2,0,,False
94,d,0,,False
95,onto,0,,False
96,"(K - 1)-simplex. With step 1 and 2, we iteratively update",0,,False
97,w and r until convergence. Then we sort r in ascending,0,,False
98,order to get the consensus ranking.,0,,False
99,3. EXPERIMENTS,0,,False
100,"In the experiments, we use four typical multi-document summarization methods as individual summarizers and compare our WCS method with other eight aggregation methods. The four individual summarization methods are: (a) Centroid [7], (b) LexPageRank [1], (c) LSA [2], and (d) NMF [4]. And the baseline aggregation methods are: (1) average score (Ave Score), which normalizes and averages the raw scores from different summarization systems; (2) average rank (Ave Rank), which averages individual rankings; (3) median aggregation (Med Rank); (4) Round Robin (RR); (5) Borda Count (BC); (6) correlation-based weighting (CW), which weights individual systems by their average Kendall's Tau correlation between the ranking list they generated and all the other lists; (7) ULTRA [3], which aims to find a consensus ranking with the minimum average Spearman's distance [8] to all the individual ranking lists; (8) graph-based combination (Graph), the basic idea of which is similar to the work proposed in [9], however, we use cosine similarity so that we can compare this method with other combination methods fairly. We conduct experiments on DUC benchmark data for generic multi-document summarization and use ROUGE [6] toolkit (version 1.5.5) to measure the summarization performance.",1,WCS,True
101,3.1 Overall Summarization Performance,0,,False
102,"Table 1 show Rouge-1, Rouge-2, and Rouge-SU scores of different individual and combination methods using DUC2004 data sets (intuitively, the higher the scores, the better the performance). From the results, we observe that (1) Most of the combination summarization systems outperform all the individual systems except the round robin combination. The results demonstrate that in general consensus methods can improve the summarization performance. (2) Weighted combinations (e.g., CW, ULTRA, and WCS) outperform average combination methods which treat each individual system equally. (3) Our WCS method outperforms other weighted combination methods because WCS optimizes the weighted distance between the consensus sentence ranking to individual rankings and updates the weights and consensus ranking iteratively, which is closer to the nature of consensus summarization than other approximation based weighted methods.",1,DUC,True
103,Systems,0,,False
104,DUCBest Centroid LexPageRank,1,DUC,True
105,LSA NMF Ave Score Ave Rank Med Rank RR BC CW ULTRA Graph WCS,1,CW,True
106,R-1,0,,False
107,0.382 0.367 0.378 0.341 0.367 0.388 0.385 0.385 0.364 0.378 0.378 0.392 0.379 0.398,0,,False
108,R-2,0,,False
109,0.092 0.073 0.085 0.065 0.072 0.089 0.087 0.087 0.072 0.085 0.085 0.090 0.086 0.096,0,,False
110,R-SU,0,,False
111,0.132 0.125 0.130 0.119 0.129 0.132 0.131 0.131 0.126 0.129 0.131 0.133 0.132 0.135,0,,False
112,Table 1: Overall performance comparison on DUC2004. Remark: DUCBest shows the best results from DUC 2004 competition.,1,DUC,True
113,3.2 Diversity of Individual Summarizers,0,,False
114,"In this set of experiments, we further examine if the four individual summarization methods are complementary to each other. We use our WCS method to aggregate any three of the four summarization methods and compare the results with the aggregation utilizing all the four methods. Table 2 shows the comparison results. From the results, we observe that adding any of the four individual methods improves the summarization performance. This is because these individual summarization methods are diverse and their performance is data dependant.",1,WCS,True
115,Systems,0,,False
116,Centroid+LexPageRank+LSA Centroid+LexPageRank+NMF,0,,False
117,Centroid+LSA+NMF LexPageRank+LSA+NMF,0,,False
118,All,0,,False
119,R-1,0,,False
120,0.383 0.385 0.376 0.382 0.398,0,,False
121,R-2,0,,False
122,0.088 0.090 0.082 0.087 0.096,0,,False
123,R-SU,0,,False
124,0.132 0.133 0.131 0.132 0.135,0,,False
125,Table 2: WCS results on DUC2004.,1,WCS,True
126,Acknowledgements: The work is partially supported by an FIU Dissertation Year Fellowship and NSF grants IIS0546280 and DMS-0915110.,0,,False
127,4. REFERENCES,0,,False
128,"[1] G. Erkan and D. Radev. Lexpagerank: Prestige in multi-document text summarization. In EMNLP, 2004.",1,ad,True
129,"[2] Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. In SIGIR, 2001.",0,,False
130,"[3] A. Klementiev, D. Roth, and K. Small. An unsupervised learning algorithm for rank aggregation. In ECML, 2007.",0,,False
131,"[4] D. Lee and H. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, pages 788­791, 1999.",0,,False
132,"[5] T. Li and C. Ding. Weighted consensus clustering. SIAM Data Mining, 2008.",0,,False
133,"[6] C.-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In NLT-NAACL, 2003.",0,,False
134,"[7] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, pages 919­938, 2004.",1,ad,True
135,"[8] C. Spearman. The proof and measurement of association between two things. Amer. J. Psychol., 1904.",0,,False
136,"[9] V. Thapar, A. A. Mohamed, and S. Rajasekaran. A consensus text summarizer based on meta-search algorithms. In Proceedings of 2006 IEEE International Symposium on Signal Processing and Information Technology, 2006.",0,,False
137,810,0,,False
138,,0,,False

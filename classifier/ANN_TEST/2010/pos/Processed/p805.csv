,sentence,label,data,regex
0,Feature Subset Non-Negative Matrix Factorization and its Applications to Document Understanding,0,,False
1,Dingding Wang,0,,False
2,School of Computer Science Florida International University,0,,False
3,"Miami, FL 33199",0,,False
4,dwang003@cs.fiu.edu,0,,False
5,Chris Ding,0,,False
6,CSE Department University of Texas at Arlington,0,,False
7,"Arlington, TX 76019",0,,False
8,chqding@uta.edu,0,,False
9,Tao Li,0,,False
10,School of Computer Science Florida International University,0,,False
11,"Miami, FL 33199",0,,False
12,taoli@cs.fiu.edu,0,,False
13,ABSTRACT,0,,False
14,"In this paper, we propose feature subset non-negative matrix factorization (NMF), which is an unsupervised approach to simultaneously cluster data points and select important features. We apply our proposed approach to various document understanding tasks including document clustering, summarization, and visualization. Experimental results demonstrate the effectiveness of our approach for these tasks. Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval. General Terms: Algorithms, Experimentation. Keywords: Feature subset selection, NMF.",0,,False
15,1. INTRODUCTION,1,DUC,True
16,"Keyword (Feature) selection enhances and improves many IR tasks such as document categorization, automatic topic discovery, etc. Many supervised keyword selection techniques have been developed for selecting keywords for classification problems. In this paper, we propose an unsupervised approach that combines keyword selection and document clustering (topic discovery) together.",0,,False
17,The proposed approach extends non-negative matrix factorization (NMF) by incorporating a weight matrix to indicate the importance of the keywords. This work considers both theoretically and empirically feature subset selection for NMF and draws the connection between unsupervised feature selection and data clustering.,1,corpora,True
18,"The selected keywords are discriminant for different topics in a global perspective, unlike those obtained in co-clustering, which typically associate with one cluster strongly and are absent from other clusters. Also, the selected keywords are not linear combinations of words like those obtained in Latent Semantic Indexing (LSI): our selected words provide clear semantic meanings of the key features while LSI features combine different words together and are not easy to interpret. Experiments on various document understanding applications demonstrate the effectiveness of our proposed approach.",0,,False
19,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
20,2. FEATURE SUBSET NON-NEGATIVE MA-,0,,False
21,TRIX FACTORIZATION (FS-NMF),0,,False
22,"Let X ,"" {x1, · · · , xn) contains n documents with m keywords (features). In general, NMF factorizes the input nonnegative data matrix X into two nonnegative matrices,""",0,,False
23,"X  F GT ,",0,,False
24,"where G  Rn+×k is the cluster indicator matrix for clustering columns of X and F ,"" (f1, · · · , fk)  R+m×k contains k cluster centroids. In this paper, we propose a new objective""",0,,False
25,to simultaneously factorize X and rank the features in X as,0,,False
26,follows:,0,,False
27,W,0,,False
28,min,0,,False
29,"0,F 0,G0",0,,False
30,||X,0,,False
31,-,0,,False
32,F GT,0,,False
33,||2W,0,,False
34,",",0,,False
35,s.t.,0,,False
36,"Wj , 1",0,,False
37,(1),0,,False
38,j,0,,False
39,"where W  Rm + ×m which is a diagonal matrix indicating the weights of the rows (keywords or features) in X, and  is a",0,,False
40,parameter (set to 0.7 empirically).,0,,False
41,2.1 Optimization,0,,False
42,We will optimize the objective with respect to one variable while fixing the other variables. This procedure repeats until convergence.,0,,False
43,2.1.1 Computation of W,0,,False
44,Optimizing Eq.(1) with respect to W is equivalent to op-,0,,False
45,timizing,0,,False
46,"J1 ,"" Wibi - ( Wi - 1), bi "", (X - F GT )2ij .",0,,False
47,i,0,,False
48,i,0,,False
49,j,0,,False
50,"Now,",0,,False
51,setting,0,,False
52,J1 Wi,0,,False
53,", bi - Wi-1",0,,False
54,","" 0,""",0,,False
55,we,0,,False
56,obtain,0,,False
57,the,0,,False
58,fol-,0,,False
59,lowing updating formula,0,,False
60,1,0,,False
61,Wi,0,,False
62,",",0,,False
63,i,0,,False
64,1,0,,False
65,bi-1,0,,False
66,1,0,,False
67,bi-1,0,,False
68,(2),0,,False
69,2.1.2 Computation of G,0,,False
70,Optimizing Eq.(1) with respect to G is equivalent to optimizing,0,,False
71,"J2 , T r(XT W T X - 2GF T W T X + F T W T F GT G).",0,,False
72,Setting,0,,False
73,J2 G,0,,False
74,",",0,,False
75,-2XT W F,0,,False
76,+ 2GF T W T F,0,,False
77,",",0,,False
78,"0,",0,,False
79,we,0,,False
80,obtain,0,,False
81,the,0,,False
82,following updating formula,0,,False
83,Gik,0,,False
84,Gik,0,,False
85,(XT W F )ik (GF T W F )ik,0,,False
86,(3),0,,False
87,The correctness and convergence of this updating rule can,0,,False
88,be rigorously proved. Details are skipped here.,0,,False
89,805,0,,False
90,2.1.3 Computation of F,0,,False
91,Optimizing Eq.(1) with respect to F is equivalent to optimizing,0,,False
92,"J3 , T r[W XXT - 2W XGF T + W F GT GF ].",0,,False
93,Setting,0,,False
94,J3 F,0,,False
95,",",0,,False
96,-2W XG + W F GT G + (GT GF T W )T,0,,False
97,","" 0,""",0,,False
98,we,0,,False
99,obtain the following updating formula,0,,False
100,Fik,0,,False
101,Fik,0,,False
102,(W XG)ik (W F GT G)ik,0,,False
103,(4),0,,False
104,3. EXPERIMENTS,0,,False
105,3.1 Document Clustering,0,,False
106,"First of all, we examine the clustering performance of FS-NMF using four text datasets as described in Table 1, and compare the results of FS-NMF with seven widely used document clustering methods: (1) K-means; (2) PCA-Km: PCA is firstly applied to reduce the data dimension followed by the K-means clustering; (3) LDA-Km [2]: an adaptive subspace clustering algorithm by integrating linear discriminant analysis (LDA) and K-means; (4)Euclidean coclustering (ECC) [1]; (5) minimum squared residueco-clustering (MSRC) [1]; (6) Non-negative matrix factorization (NMF) [5]; (7) Spectral Clustering with Normalized Cuts (Ncut) [9]. More description of the datasets can be found in [6]. The accuracy evaluation results are presented in Figure 2.",1,ad,True
107,Datasets CSTR Log Reuters,1,Reuters,True
108,WebACE,1,WebACE,True
109,# Samples 475 1367 2900 2340,0,,False
110,# Dimensions 1000 200 1000 1000,0,,False
111,# Class 4 8 10 20,0,,False
112,Table 1: Dataset descriptions.,0,,False
113,K-means PCA-Km LDA-Km,0,,False
114,ECC MSRC NMF Ncut FS-NMF,0,,False
115,WebACE,1,WebACE,True
116,0.4081 0.4432 0.4774 0.4081 0.4432 0.4774 0.4513 0.5577,0,,False
117,Log,0,,False
118,0.6979 0.6562 0.7198 0.7228 0.5655 0.7608 0.7574 0.7715,0,,False
119,Reuters,1,Reuters,True
120,0.4360 0.3925 0.5142 0.4968 0.4516 0.4047 0.4890 0.4697,0,,False
121,CSTR,0,,False
122,0.5210 0.5630 0.5630 0.5210 0.5630 0.5630 0.5435 0.6996,0,,False
123,Table 2: Clustering accuracy on text datasets.,0,,False
124,"From the results, we clearly observe that FS-NMF outperforms other document clustering algorithms in most of the cases, and the effectiveness of FS-NMF for document clustering is demonstrated.",0,,False
125,3.2 Document Summarization,0,,False
126,"In this set of experiments, we apply our FS-NMF algorithm on document summarization. Let X be the documentsentence matrix, which can be generated from the documentterm and sentence-term matrices, and now each feature (column) in X represents a sentence. Then the sentences can be ranked based on the weights in W . Top-ranked sentences are included into the final summary. We use the DUC benchmark dataset (DUC2004) for generic document summarization to compare our method with other state-ofart document summarization methods using ROUGE evaluation toolkit [7]. The results are demonstrated in Table 3.",1,DUC,True
127,Systems,0,,False
128,DUC Best Random Centroid [8] LexPageRank [3] LSA [4] NMF [5] FS-NMF,1,DUC,True
129,R-1,0,,False
130,0.382 0.318 0.367 0.378 0.341 0.367 0.388,0,,False
131,R-2,0,,False
132,0.092 0.063 0.073 0.085 0.065 0.072 0.101,0,,False
133,R-L,0,,False
134,0.386 0.345 0.361 0.375 0.349 0.367 0.381,0,,False
135,R-W,0,,False
136,0.133 0.117 0.124 0.131 0.120 0.129 0.139,0,,False
137,R-SU,0,,False
138,0.132 0.117 0.125 0.130 0.119 0.129 0.134,0,,False
139,Table 3: Overall performance comparison on DUC2004 data using ROUGE evaluation methods.,1,DUC,True
140,"From the results, we observe that the summary generated by FS-NMF outperforms those created by other methods, and the scores are even higher than the best results in DUC competition. The good results benefit from good sentence feature selection in FS-NMF.",1,DUC,True
141,3.3 Visualization,0,,False
142,"In this set of experiments, we calculate the pairwise document similarity using the top 20 word features selected by different methods, and Figure 1 demonstrates the document similarity matrix visually. Note that in the document dataset (CSTR dataset), we order the documents based on their class labels.",0,,False
143,50,0,,False
144,100,0,,False
145,150,0,,False
146,200,0,,False
147,250,0,,False
148,300,0,,False
149,350,0,,False
150,400,0,,False
151,50,0,,False
152,100,0,,False
153,150,0,,False
154,200,0,,False
155,250,0,,False
156,300,0,,False
157,350,0,,False
158,400,0,,False
159,50,0,,False
160,100,0,,False
161,150,0,,False
162,200,0,,False
163,250,0,,False
164,300,0,,False
165,350,0,,False
166,400,0,,False
167,450,0,,False
168,50,0,,False
169,100 150 200 250 300 350 400 450,0,,False
170,50,0,,False
171,100,0,,False
172,150,0,,False
173,200,0,,False
174,250,0,,False
175,300,0,,False
176,350,0,,False
177,400,0,,False
178,450,0,,False
179,50,0,,False
180,100 150 200 250 300 350 400 450,0,,False
181,(a) FS-NMF,0,,False
182,(b) NMF,0,,False
183,(c) LSI,0,,False
184,Figure 1: Visualization results on the CSTR Dataset. Note that CSTR has 4 clusters.,0,,False
185,"From Figure 1, we have the following observations. (1) Word features selected by FS-NMF can effectively reflet the document distribution. (2) NMF tends to select some irrelevant or redundant words thus Figure 1(b) shows no obvious patterns at all. (3) LSI can also find meaningful words, however, the first two clusters are not clearly discovered in Figure 1(c).",0,,False
186,Acknowledgements: The work is partially supported by an FIU Dissertation Year Fellowship and NSF grants DMS-0915110 and DMS-0915228.,0,,False
187,4. REFERENCES,0,,False
188,"[1] H. Cho, I. Dhillon, Y. Guan, and S. Sra. Minimum sum squared residue co-clustering of gene expression data. In Proceedings of SDM 2004.",0,,False
189,"[2] C. Ding and T. Li. Adaptive dimension reduction using discriminant analysis and k-means c lustering. In ICML, 2007.",0,,False
190,"[3] G. Erkan and D. Radev. Lexpagerank: Prestige in multi-document text summarization. In EMNLP, 2004.",1,ad,True
191,"[4] Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. In SIGIR, 2001.",0,,False
192,"[5] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS, 2000.",0,,False
193,"[6] T. Li and C. Ding. The relationships among various nonnegative matrix factorization methods for clustering. In ICDM, 2006.",0,,False
194,"[7] C.-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In NLT-NAACL, 2003.",0,,False
195,"[8] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, pages 919­938, 2004.",1,ad,True
196,"[9] S. X. Yu and J. Shi. Multiclass spectral clustering. In ICCV, 2003.",0,,False
197,806,0,,False
198,,0,,False

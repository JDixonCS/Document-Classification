,sentence,label,data,regex
0,Author Interest Topic Model,0,,False
1,Noriaki Kawamae,0,,False
2,"NTT Comware 1-6 Nakase Mihama-ku Chiba-shi, Chiba 261-0023 Japan",0,,False
3,kawamae@gmail.com,0,,False
4,ABSTRACT,0,,False
5,"This paper presents a hierarchical topic model that simultaneously captures topics and author's interests. Our proposed model, the Author Interest Topic model (AIT), introduces a latent variable with a probability distribution over topics into each document. Experiments on a research paper corpus show that AIT is very useful as a generative model.",0,,False
6,Categories and Subject Descriptors,0,,False
7,H.3.1 [Content Analysis and Indexing]:,0,,False
8,General Terms,0,,False
9,"Algorithms, experimentation",0,,False
10,Keywords,0,,False
11,"Topic Modeling, Latent Variable Modeling",0,,False
12,1. INTRODUCTION,1,DUC,True
13,"Attention is being focused on how to model users' interests in several fields. A model of interest allows us to infer which topics each user prefers and to measure the similarity between them in terms of their interests. For example, the Author-Topic(AT) [3] groups all papers associated with a given author by using a single topic distribution associated with this author. Author-Persona-Topic(APT) [2] introduces a persona, which is also a latent variable, under a single given author. Thus, these models allow each author's documents to be divided into one or more clusters, each with its own separate topic distribution specific to that persona",1,AP,True
14,"This paper presents the Author Interest Topic(AIT) model; it is a generalization of known author interest models such as AT and APT. AIT allows a number of possible latent variables to be associated with author's interest, while previous models limit this number. Therefore, AIT can describe a wider variety of authors' interests than other models, which reduces the perplexity. Moreover, AIT can infer the overall interest in the training data and so can assign probabilities to previously unseen documents.",1,AP,True
15,2. AUTHOR INTEREST TOPIC MODEL,0,,False
16,This section details our model. Table 1 shows the notations used in this paper. Figure 1 shows graphical models to,0,,False
17,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
18,"Figure 1: Graphical models: In this figure, shaded and unshaded variables indicate observed and latent variables, respectively. An arrow indicates a conditional dependency between variables and the plates indicate a repeated sampling with the iteration number shown. This figure shows that each author produces words from a set of topics that are preferred by the author in (a), persona associated with the author in (b), each document class in (c). In learning a document written by multiple authors, AIT makes copies of the document and associates one copy with each author.",1,ad,True
19,"describe the generative process. For modeling each author's interest, our proposal, AIT, incorporates document class cd; it provides an indicator variable that describes which mixture of topics each document d takes, into d. Accordingly, AIT represents documents of similar topics as the same document class in the same way that topic models represent cooccurrence words as the same topic variable. Therefore, the difference between AIT and AT, APT is that rather than representing author's interest as a mixture of topic variables a(AT) or Pa (APT) in each document layer, AIT represents each author's interest as a mixture of document classes a in each author layer. Although both a(AT) and Pa (APT) are associated with only authors, the document class can be shared among authors. This class allows AIT to represent documents having similar topics as the same document class by merging parameters; this reduces the number of possible parameters without losing generality. Accordingly, as the size of training data is increased, relatively fewer parameters are needed. On the contrary, the parameters of the other models track the order of authors and so experience linear growth with the size of the training data. Moreover we decide the number of latent variables following CRP [1]. Consequently, AIT increases the number of possible latent variables for explaining all authors' interests.",1,corpora,True
20,"AIT employs Gibbs sampling to perform inference approximation. In the Gibbs sampling procedure, we need to cal-",0,,False
21,887,0,,False
22,Table 1: Notations used in this paper,0,,False
23,SYMBOL DESCRIPTION,0,,False
24,A,0,,False
25,number of authors,0,,False
26,J,0,,False
27,number of document classes,0,,False
28,T,0,,False
29,number of topics,0,,False
30,D,0,,False
31,number of documents,0,,False
32,V,0,,False
33,number of unique words,0,,False
34,Ad,0,,False
35,authors associated with document d,0,,False
36,Da,0,,False
37,number of documents written by author a,0,,False
38,Nd,0,,False
39,number of word tokens in document d,0,,False
40,ai,0,,False
41,author associated with ith token in document d,0,,False
42,pd,0,,False
43,persona associated with document d,0,,False
44,cd,0,,False
45,document class associated with document d,0,,False
46,zdi,0,,False
47,topic associated with the ith token in document,0,,False
48,d,0,,False
49,wdi,0,,False
50,ith token in document d,0,,False
51,a,0,,False
52,multinomial distribution of document classes,0,,False
53,specific to author a (a|  Dirichlet() ),0,,False
54,j,0,,False
55,multinomial distribution of topics specific to in-,0,,False
56,terest j (j |  Dirichlet() ),0,,False
57,t,0,,False
58,multinomial distribution of words specific to,0,,False
59,topic t (t|  Dirichlet() ),0,,False
60,"culate the conditional distributions. The predictive distribution of adding interest class cd in documents written by author a to topic cd , j is given by",1,ad,True
61,8 (Pt,0,,False
62,njt\d +t),0,,False
63,Q t,0,,False
64,(njt +t ),0,,False
65,">>< n , aj\d",0,,False
66,Q t,0,,False
67,(njt\d +t),0,,False
68,(Pt,0,,False
69,njt +t ),0,,False
70,"P (j|c\d, a, z, , )  if j is an existing class class",0,,False
71,> > :,0,,False
72,j,0,,False
73,(Pt njt\d +t),0,,False
74,Q t,0,,False
75,(njt\d +t),0,,False
76,", Q t",0,,False
77,(njt +t,0,,False
78,),0,,False
79,(Pt njt+t),0,,False
80,otherwise,0,,False
81,(1),0,,False
82,"where naj\d represents the number of documents assigned to j in all documents written by author a, except d, and njt\di represents the total number of tokens assigned to topic t",0,,False
83,"in the documents associated with document class j, except",0,,False
84,token di.,0,,False
85,The predictive distribution of adding word wdi in docu-,1,ad,True
86,"ment d written by a to topic zd , t is given by",0,,False
87,8,0,,False
88,> > <,0,,False
89,"n , ntw\di+w",0,,False
90,jt\di,0,,False
91,PV v,0,,False
92,(ntv\di +v ),0,,False
93,"P (t|j, z\di, w, , )  if t is an existing class",0,,False
94,> > :,0,,False
95," , ntw\di+w",0,,False
96,t,0,,False
97,PV v,0,,False
98,(ntv\di +v,0,,False
99,),0,,False
100,otherwise,0,,False
101,(2),0,,False
102,"where ntw\di represents the total number of tokens assigned to word w in topic t, except token di, and njt\di represents the total number of tokens assigned to topic t in",0,,False
103,"all tokens assigned to j, except token di.",0,,False
104,3. EXPERIMENTS,0,,False
105,"We focus here on the extraction of interests from given documents, and demonstrate AIT's performance as a generative model. The dataset used in our experiments consisted of research papers in the proceedings of ACM CIKM, SIGIR, KDD, and WWW gathered over the last 8 years (2001-2008). We removed stop words, numbers, and the words that appeared less than five times in the corpus. Accordingly, we obtained a total set of 3078 documents and 20286 unique words from 2204 authors. Additionally, we applied both AT and APT to this dataset for training and comparison.",1,AP,True
106,"In our evaluation, the smoothing parameters ,  and",0,,False
107,"Table 2: Perplexity of AT, APT and AIT: This difference between AIT and APT is significant according to one-tailed t-test with the number of samples G ,"" 100. For fair comparison, the number of topic variables T was fixed at 200, the number of document classes J was fixed at 40(AIT). Results that differ significantly by t-test p < 0.01, p < 0.05 from APT are marked with '**', '*' respectively. The value of Avg means the average computing time for each iteration in gibbs sampling.""",1,AP,True
108,Iteration AT APT AIT,1,AP,True
109,2000 1529 1454 1321,0,,False
110,4000 1488 1304 1217,0,,False
111,6000 1343 1180 1103,0,,False
112,8000 1339 1059 988,0,,False
113,10000 1333 1027 964,0,,False
114,Avg 3.2s 10.4s 11.7s,0,,False
115," were set at 0.1, 10(APT),1(AIT) and 1, respectively. We ran single Gibbs sampling chains for 10000 iterations on machines with Dual Core 2.66 GHz Xeon processors.",1,AP,True
116,"To measure the ability of a model to act as a generative model, we computed test-set perplexity under estimated parameters and compared the resulting values.",0,,False
117,"Perplexity, which is widely used in the language modeling community to assess the predictive power of a model, is algebraically equivalent to the inverse of the geometric mean per-word likelihood (lower numbers are better). Table 2 shows the results of the perplexity comparison. This table shows that AIT yielded significantly lower perplexity on the test set than AT or APT, which shows that AIT is better as a topic model. This is due to the ability of AIT to allow the document class to be shared across authors and to group documents under the various topic distributions rather than grouping documents by a given author or persona under a few topic distributions. This implies that clustered documents contain less noise than otherwise. If the number of document classes is overly restricted, the difference between the observed data and the data generated by the model under test increases, which raises the perplexity.",1,AP,True
118,4. CONCLUSION,0,,False
119,"Our proposed model, AIT, supports the expression of topics in text documents and can identify the interests of authors in these documents. Future work includes extending AIT by taking other metadata such as time, references and link structure into account, for tracking the dynamics of interests and topics.",1,ad,True
120,5. REFERENCES,0,,False
121,"[1] D. J. D. Aldous. Exchangeability and related topics, volume 1117 of Lecture Notes in Math. Springer, Berlin, 1985.",0,,False
122,"[2] D. Mimno and A. McCallum. Expertise modeling for matching papers with reviewers. In KDD, pages 500­509, 2007.",0,,False
123,"[3] M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Griffiths. Probabilistic author-topic models for information discovery. In KDD, pages 306­315, 2004.",0,,False
124,888,0,,False
125,,0,,False

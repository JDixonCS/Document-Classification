,sentence,label,data,regex
0,Proximity-Based Opinion Retrieval,0,,False
1,Shima Gerani,0,,False
2,"University of Lugano Faculty of Informatics Lugano, Switzerland",0,,False
3,shima.gerani@usi.ch,0,,False
4,Mark J. Carman,0,,False
5,"University of Lugano Faculty of Informatics Lugano, Switzerland",0,,False
6,mark.carman@usi.ch,0,,False
7,Fabio Crestani,0,,False
8,"University of Lugano Faculty of Informatics Lugano, Switzerland",0,,False
9,fabio.crestani@usi.ch,0,,False
10,ABSTRACT,0,,False
11,"Blog post opinion retrieval aims at finding blog posts that are relevant and opinionated about a user's query. In this paper we propose a simple probabilistic model for assigning relevant opinion scores to documents. The key problem is how to capture opinion expressions in the document, that are related to the query topic. Current solutions enrich general opinion lexicons by finding query-specific opinion lexicons using pseudo-relevance feedback on external corpora or the collection itself. In this paper we use a general opinion lexicon and propose using proximity information in order to capture opinion term relatedness to the query. We propose a proximity-based opinion propagation method to calculate the opinion density at each point in a document. The opinion density at the position of a query term in the document can then be considered as the probability of opinion about the query term at that position. The effect of different kernels for capturing the proximity is also discussed. Experimental results on the BLOG06 dataset show that the proposed method provides significant improvement over standard TREC baselines and achieves a 2.5% increase in MAP over the best performing run in the TREC 2008 blog track.",1,blog,True
12,Categories and Subject Descriptors,0,,False
13,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,0,,False
14,General Terms,0,,False
15,"Experimentation, Performance",0,,False
16,Keywords,0,,False
17,"Opinion, Sentiment, Blog, Retrieval, Proximity",0,,False
18,1. INTRODUCTION,1,DUC,True
19,Blog post opinion retrieval is the problem of finding blog posts that express opinion about a given query topic. This,1,blog,True
20,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
21,"problem was introduced in the Text REtrieval Conference (TREC) 2006 blog track and continued to 2008 [15, 12, 16]. The proposed approaches mostly follow a three step framework. In the first step, traditional IR is used to find documents that are relevant to the query. In the second step, opinion scores are generated for the relevant documents. Finally, a ranking method is used to rank documents according to their relevance and opinionatedness about the query.",1,TREC,True
22,"Blog post opinion retrieval faces two main challenges. The first challenge is to find the best way to combine relevance and opinion scores to produce a single ranking. In previous work, researchers mostly used linear combinations of relevance and opinion scores [16]. We use a probabilistic approach and propose a simple model for combining probabilities of relevance and opinionatedness about a query.",0,,False
23,The second challenge is assigning query-related opinion scores to documents. The problem is how to identify opinion expressions in the document that are directed at the concepts in the query. Simple averaging over the opinion weights of terms or sentences in a document to generate an opinion score is not an optimal approach. The reason is that documents can be relevant to many different topics at the same time but the opinion being expressed in them may be directed towards topics other than the query. So we need an opinion finding method that takes the query into account and ignores opinionated content that is not related to the query. In this paper we propose using proximity-based density functions to model the notion of query-relatedness for opinionated content. Our aim is to see how much improvement can be achieved using proximity information alone without the need for query-specific opinion-lexicon. Our contributions are:,0,,False
24,· Presenting a novel probabilistic opinion retrieval model that is based on proximity between opinion lexicons and query terms.,0,,False
25,· Investigating different ways of estimating the relevance probability of documents from their relevance scores.,0,,False
26,· Investigating the impact of different types of proximity functions in our model.,0,,False
27,"We evaluate our model on the BLOG06 collection using five standard TREC 2008 baselines. We report significant improvements over these strong TREC baselines and over non proximity-based opinion retrieval scores in the experiments. The results show the effectiveness of utilizing the simple proximity information in enhancing opinion retrieval, compared to systems which utilize components such as query",1,TREC,True
28,403,0,,False
29,expansion and query-specific opinion-lexicon refinement via pseudo-relevance feedback.,0,,False
30,2. RELATED WORK,0,,False
31,"Research on opinion mining and sentiment analysis started mostly on review-type data with the intention to classify documents as expressing either a positive or negative opinion. The proposed approaches can be categorized into two main groups: lexicon-based [21, 20, 24] and classificationbased [17, 1, 13, 5]. Both of these approaches rely on word occurrences. The first approach (lexicon based), uses a manually or automatically built list of subjective words, such as `good' and `like', and assumes that the presence of these words in a document is the evidence of document opinionatedness. A term's opinion score can be used in different ways to assign an opinion score to the whole document. The second approach (classification-based) utilizes word occurrence and sometimes linguistic features and builds a classifier based on positive (opinionated) and negative (nonopinionated) documents using Machine Learning techniques. Nevertheless, most of the early research in this area neglected the problem of retrieving documents that are related to the topic of the user's interest. It also did not target the problem of ranking opinionated documents according to the degree with which they are opinionated (either in positive or negative way). Relevance of an opinion to a topic was considered for the first time in Yi et. al [24] and then in Hurst and Nigam's work [7] but they did not consider the ranking of documents. Instead they only classified documents as to whether they expressed an opinion about the topic. Opinion ranking was considered for the first time in Eguchi and Lavrenko's work [2].",1,ad,True
32,"In TREC 2006, the Opinion Retrieval task appeared in the Blog track. It considered the opinion mining problem from an Information Retrieval perspective and introduced the problem of retrieving and ranking blog posts that were relevant and opinionated about a given topic (query) [15]. There has been lots of research on blog opinion retrieval in TREC [15, 12, 16] and other conferences [26, 25, 19] in which people follow the opinion retrieval definition used in the TREC blog track. Following the categorization mentioned earlier, the proposed methods belong to two classes of lexicon-based [23, 25, 6, 9, 19] and classification-based [26, 8] approaches and usually follow the three-step framework mentioned earlier.",1,TREC,True
33,In this paper we follow the TREC opinion retrieval problem definition. We follow the lexicon-based approach in opinion finding and focus on the problem of finding topic related opinion expressions. In the rest of this section we explain in greater detail relevant previous work in handling the opinion retrieval challenge.,1,TREC,True
34,2.1 Capturing Topic Related Opinion Expression,0,,False
35,"Since the aim of opinion retrieval is to find documents that express an opinion about the query, unrelated opinion should not be considered in the scoring of a document. Therefore, the main challenge in opinion finding is to score documents by opinion expressions that refer to the query. In previous work, researchers followed two orthogonal approaches. In the first approach they built a query-specific opinion lexicon by starting from a general opinion lexicon and refining the opinion weights of terms in the lexicon via feedback style",0,,False
36,"learning on the top retrieved documents in response to the query [9, 14].",0,,False
37,"The second approach uses the proximity of subjective terms or sentences to the topic terms as a measure of relatedness [1, 26, 25, 19, 22]. Dave et al. [1] tried to capture proximity using higher order n-grams as units to represent text. However, n-grams cannot capture the dependency of nonadjacent terms. Although such dependencies can be captured by increasing the length of the n-gram, this can be impractical due to a lack of sufficient training data. Zhang et al. [25] calculate the proximity of opinion terms to query terms by computing the probability of query term and opinion term co-occurrence within a window. Vechtomova [22] considered the distance in the number of non-stopwords between a query term and subjective lexical units occurring within the window of n words around the query term. Although they considered proximity information in their models, Zhang et al. [25] did not find any advantage of using the proximity information while Vechtomova [22] did show some improvement in terms of opinion MAP. In this paper we introduce the proximity information in a more principled way and show that it can improve the performance over a non proximity-based opinion retrieval baseline.",1,ad,True
38,"Proximity information is also considered in [26, 19], with the difference being that they first find opinionated sentences and then consider proximity of opinionated sentences to the query term. Zhang et al. [26] use a SVM classifier to classify document's sentences as either opinionated or non-opinionated. They then apply a NEAR operator to classify an opinionated sentence as either relevant or not relevant to the query. Santos et al. [19] use a divergence from randomness proximity model to integrate the proximity of query terms to the opinionated sentences identified by a general opinion finding system. They further combine the proximity scored opinion sentences by the relevance score of the document using a linear combination. Our work is similar to this method in the sense that we also use a general opinion lexicon without refining it with query specific opinion terms, but our method differs in that we do not work on the sentence level but use the opinion weights and proximity of terms to the query directly. We also consider a proximity-based opinion density functions to capture the proximity information that has not been used in previous studies in opinion retrieval. The way we incorporate the relevance score in our model is also different from the previous studies in that we investigate different ways of estimating the relevance probability from the document's relevance score.",1,corpora,True
39,2.2 Combining Relevance and Opinion Scores,0,,False
40,"In order to produce a final ranking of documents by the degree of relevance and opinionatedness toward a query, previous works linearly combined the opinion and relevance scores without theoretical justification. In [25], Zhang et al. proposed a formal generative model for opinion retrieval that considers the relevant score as a weight for the opinion score of a document. Although their proposed model proved to be effective compared to previous work, it failed to take advantage of the component of the model that aimed to capture topic related opinion expression through proximity. The other shortcoming of their model is that it treats all opinion terms in the lexicon equally, while it is natural to think that some terms are more indicative of an opinion than others.",1,ad,True
41,404,0,,False
42,In this paper we propose a novel probabilistic method that considers the opinionatedness of terms in the lexicon together with its relatedness to the query. We will show the effectiveness of our proposed method in the experimental section.,0,,False
43,3. TOPIC RELATED OPINION RETRIEVAL,0,,False
44,"Blog post opinion retrieval aims at developing an effective retrieval function that ranks blog posts according to the likelihood that they are expressing an opinion about a particular topic. We follow the typical generative model in Information Retrieval that estimates the likelihood of generating a document given a query, p(d|q). In opinion retrieval, we also need to estimate the probability of generating an opinion about the query. We introduce the random variable o which denotes the event that the document expresses an opinion about the query. Thus, for opinion retrieval we can rank documents by their likelihood given the query and opinion, p(d|o, q). We then factorize this probability as follows:",1,blog,True
45,"p(d|o, q)  p(d, o, q) ,"" p(d)p(q|d)p(o|q, d) (1)""",0,,False
46,"As was also mentioned in [25], we can see two components in this formula: p(d)p(q|d) which considers the relevance of document to the query, and p(o|q, d) which deals with its ""opinionatedness"". The relevance probability can be estimated using any existing IR method such as language models [18] or classical probabilistic models [4]. The difference in our model is in the second component, p(o|q, d), that is the opinion score of the document. We propose using a proximity-based estimate as a measure of opinion relatedness to the query.",0,,False
47,In the remainder of this section we first explain the non proximity-based method for calculating the opinion score of the document. We then explain our proposed proximitybased opinion scoring.,0,,False
48,3.1 Non-Proximity Opinion Score,0,,False
49,"The first studies on opinion retrieval assumed conditional independence between o and q given the document d. So, p(o|q, d) in those models was calculated as p(o|d). Such models assume that each document discusses only one topic and so if a document is relevant to a query, all opinion expressions in the document are about the query. In order to calculate a non proximity-based (general) opinion score for a document, we can simply calculate the average opinion score over all terms in the document:",0,,False
50,X,0,,False
51,"p(o|q, d) , p(o|d) , p(o|t)p(t|d)",0,,False
52,(2),0,,False
53,td,0,,False
54,"where p(t|d) ,"" c(t, d)/|d| is the relative frequency of term t in document d and p(o|t) shows the probability of opinionatedness of the term.""",0,,False
55,3.2 Proximity Opinion Score,0,,False
56,"The assumption that a document is only relevant to a single topic and that all opinion expressions are about that topic is overly simplistic. In fact, a document can be relevant to multiple topics and just be opinionated about one of them. Therefore, for assigning opinion scores to documents, we need to identify opinion expressions that are directed toward the query topic. One possible approach is to find opinion lexicons that are mostly used to express opinion about the",0,,False
57,opinion density,0,,False
58,0.0018,0,,False
59,0.0016,0,,False
60,0.0014,0,,False
61,0.0012,0,,False
62,0.001,0,,False
63,0.0008,0,,False
64,0.0006,0,,False
65,0.0004,0,,False
66,0.0002,0,,False
67,0 100 200 300 400 500 600 700 800 900,0,,False
68,document positions,0,,False
69,Figure 1: Example of Opinion density at different positions of a document,0,,False
70,"query topic. For instance, the word ""delicious"" may be used more for expressing opinion about a food type query than an electronic product. Having access to query-related opinion lexicons, we can either ignore the word delicious or give it a low weight if the query is about electronics. For example Na et al. [14] used an opinion lexicon refinement via pseudo relevance feedback in order to build a query-related opinion lexicon.",0,,False
71,"Another approach is to use the documents' structure. In this approach the distance of an opinion term to the query term is used as a measure of their relatedness. Accordingly, we assume that an opinion term refers with higher probability to the terms closer to its position. On the other hand, opinion terms can refer not only to the entities they proceed or follow, but also to the entities which may be a couple of words, or even sentences, before or after. Bi-gram or tri-gram models have limitations in capturing such dependencies between opinion and topic terms. In order to model this dependency, we propose considering proximitybased density kernels, centered at each opinion term, which favor positions closer to the opinion term's position. As a kernel we can use any non-increasing function of the distance between the position of an opinion term and any other position in a document [10]. We weight this kernel by the probability of opinionatedness of the term. Therefore, the opinion density at each position in the document is the accumulated opinion density from different opinion terms at that position. We define this accumulated probability to be the probability of the opinion expressed in the document about the term at that position. Figure 1 shows the opinion density at different positions in a sample document.",0,,False
72,"In order to present our model more formally, we first introduce some notation. We denote a document with the vector d ,"" (t1, ..., ti, ..., tj, ..., t|d|) where the subscripts i and j indicate positions in the document and ti indicates the term occurring at the position i. To find the opinion probability at i, we calculate the accumulated opinion probability from all positions of the document at that position. So, for every position j in a document we consider the opinion weight of the term at that position which we denote by p(o|tj), and we""",0,,False
73,405,0,,False
74,All Distributions,0,,False
75,0.7,0,,False
76,Gaussian,0,,False
77,0.6,0,,False
78,Laplace Triangular,0,,False
79,Cosine,0,,False
80,0.5,0,,False
81,Circle Rectangular,0,,False
82,0.4,0,,False
83,0.3,0,,False
84,0.2,0,,False
85,0.1,0,,False
86,0,0,,False
87,-3,0,,False
88,-2,0,,False
89,-1,0,,False
90,0,0,,False
91,1,0,,False
92,2,0,,False
93,3,0,,False
94,Distance,0,,False
95,Figure 2: Proximity kernel functions with the same variance.,0,,False
96,"weight it by the probability that the term at that position j is about the query term at position i. We represent this probability by P (j|i, d) and calculate it as follows:",0,,False
97,"k(j, i)",0,,False
98,"p(j|i, d) ,",0,,False
99,P|d|,0,,False
100,"j ,1",0,,False
101,k(j,0,,False
102,", i)",0,,False
103,(3),0,,False
104,"here k(i, j), is the kernel function which determines the weight of propagated opinion from tj to ti. Thus the probability of opinion at position i in the document can be estimated as:",0,,False
105,|d|,0,,False
106,X,0,,False
107,"p(o|i, d) ,"" p(o|tj)p(j|i, d)""",0,,False
108,(4),0,,False
109,"j,1",0,,False
110,"In the rest of this section we present the different kernels used in our experiments. We investigate the five different density functions used in [10], namely the Gaussian, Triangular, Cosine, Circle and Rectangular kernel. We also present Laplace kernel as an additional kernel in our experiments. Figure 2 shows the different kernels all with the same variance.",1,ad,True
111,"In the following formulas, we present normalized kernel functions with their corresponding variance formula.",0,,False
112,1. Gaussian Kernel,0,,False
113,1,0,,False
114,» -(i - j)2 ­,0,,False
115,"k(i, j) ,  exp",0,,False
116,(5),0,,False
117,2,0,,False
118,22,0,,False
119,2. Laplace Kernel,0,,False
120,"k(i, j) , 1 exp » - |i - j| ­",0,,False
121,2b,0,,False
122,b,0,,False
123,(6),0,,False
124,"where 2 , 2b2",0,,False
125,3. Triangular Kernel,0,,False
126,"k(i,",0,,False
127,j),0,,False
128,",",0,,False
129,(,0,,False
130,1,0,,False
131,a,0,,False
132," 1

-

|i-j| ",0,,False
133,a,0,,False
134,if |i - j|  a,0,,False
135,0,0,,False
136,otherwise,0,,False
137,(7),0,,False
138,"where 2 , a2 6",0,,False
139,4. Cosine Kernel,0,,False
140,"k(i,",0,,False
141,j),0,,False
142,",",0,,False
143,(,0,,False
144,1 2s,0,,False
145,h 1,0,,False
146,+,0,,False
147,cos,0,,False
148,"

|i-j|. s

i",0,,False
149,if |i - j|  s,0,,False
150,0,0,,False
151,otherwise (8),0,,False
152,"where 2 ,"" s2 ,, 1 - 2 « 3 2""",0,,False
153,5. Circle Kernel,0,,False
154,"k(i,",0,,False
155,j),0,,False
156,",",0,,False
157,2 r2,0,,False
158,pr2,0,,False
159,-,0,,False
160,(i,0,,False
161,-,0,,False
162,j)2,0,,False
163,if |i - j|  r,0,,False
164,0,0,,False
165,otherwise,0,,False
166,(9),0,,False
167,"where 2 , r2",0,,False
168,4,0,,False
169,6. Rectangular Kernel,0,,False
170,1,0,,False
171,"k(i, j) ,",0,,False
172,2a,0,,False
173,0,0,,False
174,if |i - j|  a otherwise,0,,False
175,(10),0,,False
176,"where 2 , a2",0,,False
177,3,0,,False
178,"As one baseline, we compare proximity kernels to the uniform kernel which gives the same importance to all positions in the document and simulates the non proximitybased opinion retrieval presented in section 3.1. Our aim is to investigate whether it is better to use kernels which favor opinion occurrence in close proximity of query term or not.",0,,False
179,3.3 Probability of Document Opinionatedness about the Query,1,Query,True
180,"Now that we can compute the probability of opinion at each position in the document, we need to calculate an overall probability that the document is expressing an opinion about the query, p(o|d, q). In the following we will suggest different ways for calculating this probability.",0,,False
181,|d|,0,,False
182,X,0,,False
183,"p(o|d, q) ,",0,,False
184,"p(o, i|d, q)",0,,False
185,"i,1",0,,False
186,|d|,0,,False
187,X,0,,False
188,",",0,,False
189,"p(o|i, d, q)p(i|d, q)",0,,False
190,(11),0,,False
191,"i,1",0,,False
192,"We assume that o and q are conditionally independent given the position in the document, oq|(i, d). Thus p(o|i, d, q) reduces to p(o|i, d) which can be estimated using methods proposed in the section 3.2. Here we suggest different methods for estimating p(i|d, q), the probability of position i given the query q and the document. One method assumes that all query terms' positions in the document are equally important. Thus we have:",0,,False
193,"1 p(i|d, q) , |pos(q)|",0,,False
194,if ti  q,0,,False
195,(12),0,,False
196,0 otherwise,0,,False
197,"Here pos(q) is the set of all query terms' positions in the document. We can then calculate p(o|d, q) as follows:",0,,False
198,1X,0,,False
199,"p(o|d, q) ,",0,,False
200,"p(o|i, d)",0,,False
201,(13),0,,False
202,|pos(q)|,0,,False
203,ipos(q),0,,False
204,"As an alternative, we can assume that only the query term position where p(o|i, d) is maximum is important. Thus:",0,,False
205,"p(o|q, d) ,"" maxipos(q)p(o|i, d)""",0,,False
206,(14),0,,False
207,406,0,,False
208,"This approach is similar to first ranking the passages in the document by their degree of opinionatedness and then choosing the most relevant passage for scoring the document. We refer the first and second methods, avg and max in the experimental section.",0,,False
209,"We also considered placing a density kernel over each query term position in the document. Since the resulting formula for calculating p(o|q, d) was cubic, the approach was not efficient. It also didn't improved the performance greatly.",0,,False
210,3.4 Smoothed Proximity Model,0,,False
211,The proximity-based estimate can be further refined by smoothing it with the non proximity-based estimation as follows:,0,,False
212,"p(o|q, d) ,"" (1 - )p(o|q, d) + p(o|d)""",0,,False
213,(15),0,,False
214,"Smoothing the proximity model with the non-proximity score lets us capture the proximity at different ranges. This can be useful because there are some documents in which the exact query term occurs rarely. In such documents opinion expressions refer to the query indirectly through anaphoric expressions such as he, she, it, the film, etc. Since we don't do any query expansion or reference resolution in our model, we investigate whether smoothing the proximity model with the non-proximity score helps us capture further related opinion expressions in the document.",0,,False
215,4. EXPERIMENTAL SETUP,0,,False
216,In this section we explain our experimental setup for evaluating the effectiveness of the proposed methods.,0,,False
217,Test Collection.,0,,False
218,"Our experiments are based on the BLOG06 collection and the set of 150 topics for the blog post opinion retrieval task in TREC 2006 through 2008 and their corresponding relevance assessments. The relevance assessments provide information about whether a given blog post is relevant to a topic and also reflects the post opinionatedness nature. Each topic contains three fields of title, description and narrative. We extracted query terms from the title field of a topic. Each permalink component was indexed as a retrieval unit. The preprocessing of the collection was minimal and involved only stopword removal.",1,blog,True
219,"In order to be able to compare with TREC 2008 participants, we used 100 topics from TREC 2006 and TREC 2007 numbered 851 to 950 as our training set and 50 topics from TREC 2008, numbered 1001 to 1050, for testing.",1,TREC,True
220,Opinion Lexicon.,0,,False
221,"In our experiments we used the opinion lexicon that was proposed in [9], since it has been shown to be effective in TREC 2008. This lexicon was made using sentiWordNet [3] and an automatically learned model from the Amazon.com product review and specification corpus. The opinion lexicon only contains words from the review corpus that are also present in sentiWordNet (i.e. the intersection of the two word sets). The opinion lexicon model gives us the probability of subjectiveness of each word, p(sub|w), which we use as the probability of opinionatedness (subjectivity) of the word in our model.",1,TREC,True
222,Retrieval Baselines.,0,,False
223,"In order to facilitate direct comparison between systems in TREC 2008 five relevance retrieval baselines were provided by the TREC organizers, selected from the best performing retrieval runs. Each of these baselines covers all 150 topics and contains a list of relevant documents to those topics. Note that the baseline TREC runs are purely relevance retrieval and not opinion retrieval systems. They score relatively well at opinion retrieval simply because the majority of blog posts that are relevant to a topic are also expressing an opinion about it. When evaluating opinion retrieval systems therefore, one must compare the Mean Average Precision (MAP) score of the opinion retrieval system with the MAP (for opinionated posts) of the baseline system. A recent study showed that it is very difficult to improve opinion retrieval performance over a strong baseline on the Blog06 collection[11]. In the following experiments we show the effectiveness of the proposed method in improving the opinion retrieval performance over the best baselines in TREC 2008.",1,TREC,True
224,Evaluation.,0,,False
225,"We used the opinion relevance judgements provided by TREC for evaluation. We report the MAP as well as RPrecision (R-Prec), binary Preference (bPref), and Precision at 10 documents (P@10) in terms of opinion finding.",1,TREC,True
226,Throughout our experiments we used the Wilcoxon signedrank matched pairs test with a confidence level of 0.01 level for testing statistical significance.,0,,False
227,5. EXPERIMENTAL RESULTS,0,,False
228,In this section we explain the experiments that we conducted in order to evaluate the usefulness of different setting of the proposed method.,0,,False
229,5.1 Normalizing Relevance Scores,0,,False
230,"In order to use the standard TREC baselines in the relevance retrieval component of our model, we need to estimate the probability of relevance of each document to the query. The TREC baselines provide us with relevance score of the document, not the relevance probability. In order to estimate the probability of relevance of a document we investigated different normalization techniques for transforming the relevance score into a probability estimate. The easiest transformation is to use the relevance score directly (rank equivalent to dividing the score by the sum of all document scores for the same query). We also tried normalizing the relevance score using the minimum, min(score)q, maximum max(score)q, mean mean(score)q and standard deviation stdev(score)q of document's scores for the query q:",1,TREC,True
231,"N 1 , score - min(score)q",0,,False
232,(16),0,,False
233,max(score)q - min(score)q,0,,False
234,"N 2 , score - mean(score)q",0,,False
235,(17),0,,False
236,stdev(score)q,0,,False
237,"In addition, we experimented with Logistic Regression for learning a transformation from relevance scores to probability estimates. We trained the model using the relevance judgements from the training set. We used variations on the score or rank of the documents in the TREC baselines as a feature for logistic regression. Thus the model we used to estimate the relevance probability of a document in each",1,ad,True
238,407,0,,False
239,8,0,,False
240,16,0,,False
241,32,0,,False
242,64,0,,False
243,128,0,,False
244,Score 0.3262 0.3355 0.3364 0.3323 0.3304,0,,False
245,N1 0.3641 0.3701 0.3708 0.3685 0.3662,0,,False
246,N2 0.3713 0.3732 0.3740 0.3720 0.3709,0,,False
247,LRS 0.3205 0.33 0.3307 0.3272 0.3254,0,,False
248,LRLS 0.3216 0.3289 0.3285 0.3248 0.3228,0,,False
249,LRN1 0.331 0.3402 0.3421 0.3395 0.3371,0,,False
250,LRN2 0.2966 0.3048 0.3055 0.3012 0.2969,0,,False
251,LRR 0.3324 0.33,0,,False
252,0.3291 0.3297 0.3302,0,,False
253,LRLR 0.3613 0.3672 0.3688 0.3682 0.3673,0,,False
254,Table 1: MAP over TREC baseline4 using laplace kernel with different sigma values. Rows show MAP using different relevant probability estimation methods. An uparrow() and downarrow() indicate statistically significant increase and decrease over using the score directly.,1,MAP,True
255,8,0,,False
256,16,0,,False
257,32,0,,False
258,64,0,,False
259,128,0,,False
260,avg 0.3526 0.3573 0.3617 0.3642 0.3668,0,,False
261,max 0.3713 0.3732 0.3740 0.3720 0.3709,0,,False
262,Table 2: MAP over TREC baseline4 with different opinion scoring method for laplace kernel with different sigma values. A star() indicates statistically significant improvement over avg method,1,MAP,True
263,0.38 0.375,0,,False
264,laplace gaussian,0,,False
265,triangle,0,,False
266,cosine circle,0,,False
267,square,0,,False
268,uniform,0,,False
269,0.37,0,,False
270,MAP,1,MAP,True
271,0.365,0,,False
272,baseline was the following:,0,,False
273,e+.x,0,,False
274,"p(d is relevant|TREC baselinej ) , 1 + e+.x",1,TREC,True
275,(18),0,,False
276,"Where x is one of the normalized scores, the rank or the log of the rank (or score) of document d. In order to estimate this probability, we learn values for  and . We used the logistic regression implementation provided in LingPipe1, the TREC 2006 topics, and the set of relevant and non-relevant documents for learning these parameters.",1,TREC,True
277,"Table 1 shows the opinion retrieval performance of our proposed system, using different probability estimation methods on TREC baseline 4. We report the results for exponentially increasing values of sigma. In this table LRS, LRLS, LRN1 and LRN2, LRR and LRLR denote logistic regression using the score, log of the score, normalized score using equation 16 and 17, using rank of documents instead of score and log of rank as the explanatory variable respectively. As can be seen form Table 1, N1, N2 and LRLR have the highest MAP over all sigma values on TREC baseline 4 and the improvement over the score is statistically significant, but there is no statistically significant difference between these three methods. We chose N2 for TREC baseline 4 as it had the highest MAP over training topics. For the other baselines, we found that LRLR performed best on baseline 1,3 and 5, and N2 on baseline 2.",1,TREC,True
278,5.2 Probability of Document Opinionatedness about the Query,1,Query,True
279,"In section 3.3 we proposed two different techniques for calculating the relevant opinion probability. Table 2 shows the result of using avg and max techniques for different sigma values. The results show that the max is statistically better than the avg method. Therefore, we use the max method in the rest of our experiments.",0,,False
280,5.3 Parameter Selection for the Proximity-based Relevant Opinion Model,0,,False
281,In section 3.2 we proposed a proximity-based opinion propagation method in which a proximity kernel is considered around each opinion term occurrence position in the document. The opinion density at a query term position is then calculated by counting the accumulated opinion density from,0,,False
282,1http://alias-i.com/lingpipe/,0,,False
283,0.36,0,,False
284,0.355 0,0,,False
285,20 40 60 80 100 120 sigma,0,,False
286,Figure 3: Parameter sweep for the kernel functions,0,,False
287,"different opinion terms at that position. In this way, a query term which occurs at a position close to many opinionated terms will receive high opinion density.",0,,False
288,"The proposed relevant opinion model has two parameters: the type of kernel function and its bandwidth parameter  which adjusts the scope of opinion propagation (referencing) over the document. Performance of different kernels on training topics using the best parameter for each kernel is reported in Table 3. The result shows that all proximity kernels improve significantly over the non-proximity baseline, but there is no statistically significant difference between different proximity kernels when using the best parameters for each kernel. Fig. 3 reports the sensitivity (in terms of MAP) of the different kernels to different values of  parameters ranging from 2 to 128. Although there was no statistically significant difference between kernels, the Laplace kernel has the most effective and stable MAP over different parameter settings. Thus, we used it as the proximity kernel for our system evaluation on the test query set.",1,ad,True
289,kernel  MAP R-prec bPref p@10,1,MAP,True
290,Laplace 22 0.3744 0.4113 0.4305 0.6200,0,,False
291,Gaussian 26 0.3730 0.4099 0.4305 0.6160,0,,False
292,Cosine 24 0.3729 0.4095 0.4305 0.6170,0,,False
293,Triangle 24 0.3728 0.4086 0.4302 0.6180,0,,False
294,Square 28 0.3728 0.4100 0.4300 0.6130,0,,False
295,Circle,0,,False
296,16 0.3723 0.4080 0.4298 0.6120,0,,False
297,Uniform  0.3606 0.4011 0.4231 0.6190,0,,False
298,Table 3: The performance of proximity-based opinion retrieval for the best  for each kernel.  indicates statistically significant improvement over constant kernel.,0,,False
299,408,0,,False
300,kernel   MAP R-prec bPref p@10,1,MAP,True
301,Laplace 0.4 12 0.3775 0.4166 0.4325 0.6400,0,,False
302,Gaussian 0.6 4 0.3772 0.4147 0.4317 0.6360,0,,False
303,Triangle 0.5 4 0.3764 0.4121 0.4317 0.6420,0,,False
304,Cosine 0.6 4 0.3762 0.4142 0.4318 0.6390,0,,False
305,Circle,0,,False
306,0.7 4 0.3764 0.4167 0.4333 0.6360,0,,False
307,Square 0.4 18 0.3757 0.4092 0.4326 0.6230,0,,False
308,Table 4: The performance of Opinion Mixture model for the best  and  for each kernel.,0,,False
309,5.4 Parameter Selection for the Smoothed Proximity Model,0,,False
310,"We now report our experimental results in finding the best parameters for the smoothed proximity model presented in section 3.4. This model has three parameters: kernel type,  and the  parameter which is the interpolation coefficient. In order to find the best parameters, we tried different  values for each  value in the range of [2,128]. Table 4 reports the performance of different kernels using the best  and  parameter pairs for each kernel. It shows that, interpolating the proximity score with the no proximity opinion score improves the performance. It also shows that there is no statistically significant difference between kernels when the proximity score is interpolated with the general opinion score of the document. Here again, we choose the Laplace kernel for the evaluation on the test set.",0,,False
311,basline1 noprox laplace laplaceInt baseline2 noprox laplace laplaceInt basline3 noprox laplace laplaceInt baseline4 noprox laplace laplaceInt baseline5 noprox laplace laplaceInt,0,,False
312,MAP 0.3239 0.3751 0.3960 0.4020 0.2639 0.2791 0.2881 0.2886 0.3564 0.3819 0.3989 0.4043 0.3822 0.4129 0.4267 0.4292 0.2988 0.2918 0.3188 0.3223,1,MAP,True
313,R-prec 0.3682 0.4154 0.4369 0.4412 0.3145 0.3299 0.3401 0.3411 0.3887 0.4188 0.4369 0.4389 0.4284 0.4460 0.4545 0.4578 0.3524 0.3455 0.3732 0.3785,0,,False
314,bPref 0.3514 0.4082 0.4291 0.4326 0.2902 0.3066 0.3166  0.3166 0.3677 0.4075 0.4207 0.4247 0.4112 0.4368 0.4472 0.4485 0.3395 0.3497 0.3698 0.3715,0,,False
315,p@10 0.5800 0.6720 0.6860 0.6920 0.5500 0.5740 0.5820 0.5860 0.5540 0.6400 0.6600 0.6660 0.6160 0.6880 0.7080 0.7140 0.5300 0.5980 0.6080 0.6120,0,,False
316,Table 5: Opinion finding MAP results over five standard TREC baselines using different proximity methods for TREC 2008 topics. A star() and dagger() indicate statistically significant improvement over the relevance and non-proximity opinion retrieval baselines respectively.,1,MAP,True
317,5.5 Experimental Results on the Test Query Set,1,Query,True
318,"In this section, we present the evaluation results of our approaches on the TREC 2008 query topics. Table 5 presents the retrieval performances of the proposed methods over the five standard TREC baselines in terms of opinion retrieval. We also compare the performance of the proposed techniques to the no proximity opinion retrieval method. In Table 5, we show results for the non-proximity method (noprox), the laplace kernel (laplace) and the smoothed model using the laplace kernel (laplaceInt). Table 5 shows that the proposed methods are consistently effective across all five standard TREC baselines.",1,TREC,True
319,We also performed per topic analysis of performance for 2008 topics. The results showed that the proposed opinion retrieval methods improves Average Precision of 40 out of 50 test topics over TREC baseline 4. Topics which achieved the highest improvement over the baseline are mostly single word topics such as topic 1001 (Carmax) and topic 1023 (Yojimbo). They performed poorly on topics with multiple terms such as topic 1013 (Iceland European Union). The reason is that topics with multiple terms usually define a concept which is different from each single term in the query. Thus a more precise model for capturing the occurrence of all query terms and their proximity is required to handle such queries.,1,TREC,True
320,"Finally we compare our proposed approaches with the best runs at TREC 2008 blog track and report the comparison result in table 6 and table 7. Table 6 shows the performance of our proposed methods on the standard TREC baseline4, comparing to the best TREC run and the later proposed method in [22](KLD+dist-FD-FV-subj-b4) on the same baseline. Interestingly, both proposed methods outperform the best reported results in TREC (B4PsgOpinAZN)",1,TREC,True
321,Run laplaceInt laplace KLD+dist-FD-FV-subj-b4 B4PsgOpinAZN,0,,False
322,Map 0.4292 0.4267 0.4229 0.4189,0,,False
323, MAP 12.30% 11.64% 10.65% 9.60%,1,MAP,True
324,"Table 6: Opinion finding results for best runs on standard baseline 4, ranked by Mean  MAP using TREC 2008 new topics",1,MAP,True
325,and KLD+dist-FD-FV-subj-b4. B4PsgOpinAZN is based on a query specific lexicon which is built via feedback-style learning. KLD+dist-FD-FV-subj-b4 uses wikipedia for finding different facets in the query and query expansion. It then used Kullback-Leibler divergence to weight subjective units occurring near query terms. The distance of the query term to the subjective units is also considered in this model.,1,wiki,True
326,Table 7 reports the mean MAP and the mean of their relative improvements over the five standard baselines (MAP). We observe that the proposed methods have the highest mean of MAP and mean of MAP across the five standard baselines. This indicates that the proposed methods are effective and stable across different relevance retrieval baselines.,1,MAP,True
327,6. CONCLUSION AND FUTURE WORK,0,,False
328,"In this paper, we proposed a novel probabilistic model for blog opinion retrieval. We focused on the problem of opinion topic relatedness and we showed that using proximity information of opinionated terms to query terms is a good indicator of opinion and query-relatedness. We studied the parameter selection for our model and we have shown that when kernels are compared using the best parameter for",1,blog,True
329,409,0,,False
330,Run,0,,False
331,laplaceInt laplace uicop1bl1r B1PsgOpinAZN,0,,False
332,Map Mean 0.3693 0.3657 0.3614 0.3565,0,,False
333,stdev 0.06 0.06 0.04 0.05,0,,False
334, MAP Mean stdev 13.41% 6.38% 12.33% 5.94% 11.76% 6.93% 9.67% 0.77%,1,MAP,True
335,"Table 7: Opinion finding results for best runs using all five standard baselines, ranked by Mean  MAP using TREC 2008 new topics",1,MAP,True
336,"each, there is no statistically significant difference between them. We proposed using Laplace kernel as it was more stable on different parameter values. We also analyzed the effect of normalizing the relevance score before applying it in the model. Our results show that normalization can be important, and that the best normalization strategy is dependent on the underling relevance retrieval baseline.",0,,False
337,"We have evaluated the proposed method on the BLOG06 collection. The proposed model was shown to be effective across five standard relevance retrieval baselines. It achieved the highest improvement over the best standard TREC baseline (baseline 4), comparing to other reported results on the same baseline.",1,TREC,True
338,For future work we plan to investigate the effect of using reference resolution techniques on the performance of the proposed method.,0,,False
339,7. ACKNOWLEDGMENTS,0,,False
340,"We thank Seung-Hoon Na from the KLE group of Pohang University of Science and Technology for providing the opinion lexicon. This research was partly funded by the ""Secr´etariat d'´etat a` l'E´ducation et a` la Recherche (SER)"" and COST Action IC0702 ""Combining Soft Computing Techniques and Statistical Methods to Improve Data Analysis Solutions"".",0,,False
341,8. REFERENCES,0,,False
342,"[1] K. Dave, S. Lawrence, and D. M. Pennock. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of WWW '03, pages 519­528, 2003.",0,,False
343,"[2] K. Eguchi and V. Lavrenko. Sentiment retrieval using generative models. In Proceedings of EMNLP'06, pages 345­354, 2006.",0,,False
344,"[3] A. Esuli and F. Sebastiani. Sentiwordnet: A publicly available lexical resource for opinion mining. In Proceedings of LREC '06, pages 417­422, 2006.",0,,False
345,"[4] N. Fuhr. Probabilistic models in information retrieval. Proceedings of Comput. J., 35(3):243­255, 1992.",0,,False
346,"[5] M. Gamon. Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis. In COLING '04, page 841, 2004.",0,,False
347,"[6] B. He, C. Macdonald, I. Ounis, J. Peng, and R. L. Santos. University of glasgow at TREC 2008: Experiments in blog, enterprise, and relevance feedback tracks with terrier. In Proceedings of TREC'08, 2008.",1,TREC,True
348,"[7] M. Hurst and K. Nigam. Retrieving topical sentiments from online document collections. In Document Recognition and Retrieval XI, pages 27­34, 2004.",0,,False
349,"[8] L. Jia, C. T. Yu, and W. Zhang. UIC at TREC 2008 blog track. In Proceedings of TREC'08.",1,TREC,True
350,"[9] Y. Lee, S.-H. Na, J. Kim, S.-H. Nam, H.-Y. Jung, and J.-H. Lee. KLE at TREC 2008 blog track: Blog post and feed retrieval. In Proceedings of TREC'08, 2008.",1,TREC,True
351,"[10] Y. Lv and C. Zhai. Positional language models for information retrieval. In SIGIR '09, pages 299­306, 2009.",0,,False
352,"[11] C. Macdonald, B. He, I. Ounis, and I. Soboroff. Limits of opinion-finding baseline systems. In SIGIR '08, pages 747­748, 2008.",0,,False
353,"[12] C. Macdonald, I. Ounis, and I. Soboroff. Overview of the TREC-2007 blog track. In Proceedings of TREC'07, 2007.",1,TREC,True
354,"[13] T. Mullen and N. Collier. Sentiment analysis using support vector machines with diverse information sources. In Proceedings of EMNLP'04, pages 412­418, 2004.",0,,False
355,"[14] S.-H. Na, Y. Lee, S.-H. Nam, and J.-H. Lee. Improving opinion retrieval based on query-specific sentiment lexicon. In ECIR '09, pages 734­738, 2009.",0,,False
356,"[15] I. Ounis, M. de Rijke, C. Macdonald, G. Mishne, and I. Soboroff. Overview of the TREC-2006 blog track. In Proceedings of TREC'06, 2006.",1,TREC,True
357,"[16] I. Ounis, C. Macdonald, and I. Soboroff. Overview of the TREC-2008 blog track. In Proceedings of TREC'08, 2008.",1,TREC,True
358,"[17] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of EMNLP '02, pages 79­86, 2002.",0,,False
359,"[18] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of SIGIR '98, pages 275­281, 1998.",0,,False
360,"[19] R. L. Santos, B. He, C. Macdonald, and I. Ounis. Integrating proximity to subjective sentences for blog opinion retrieval. In Proceedings of ECIR'09, pages 325­336, 2003.",1,blog,True
361,"[20] P. D. Turney. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In ACL '02, pages 417­424, 2002.",0,,False
362,"[21] P. D. Turney and M. L. Littman. Measuring praise and criticism: Inference of semantic orientation from association. ACM Trans. Inf. Syst., 21(4):315­346, 2003.",0,,False
363,"[22] O. Vechtomova. Facet-based opinion retrieval from blogs. Inf. Process. Manage., 46(1):71­88, 2010.",1,blog,True
364,"[23] K. Yang. WIDIT in TREC 2008 blog track: Leveraging multiple sources of opinion evidence. In Proceedings of TREC'08, 2008.",1,TREC,True
365,"[24] J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In ICDM '03, page 427, 2003.",0,,False
366,"[25] M. Zhang and X. Ye. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In SIGIR '08, pages 411­418, 2008.",0,,False
367,"[26] W. Zhang, C. Yu, and W. Meng. Opinion retrieval from blogs. In CIKM '07, pages 831­840, 2007.",1,blog,True
368,410,0,,False
369,,0,,False

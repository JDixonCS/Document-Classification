,sentence,label,data,regex
0,A Joint Probabilistic Classification Model for Resource Selection,0,,False
1,"Dzung Hong , Luo Si",0,,False
2,Department of Computer Science Purdue University,0,,False
3,"250 N. University Street West Lafayette, IN 47907, USA",0,,False
4,"{dthong, lsi}@cs.purdue.edu",0,,False
5,"Paul Bracke, Michael Witt",0,,False
6,Purdue University Libraries Purdue University,0,,False
7,"504 West State Street West Lafayette, IN 47907, USA",0,,False
8,"{pbracke,mwitt}@purdue.edu",0,,False
9,Tim Juchcinski,0,,False
10,Department of Computer Science Purdue University,0,,False
11,"250 N. University Street West Lafayette, IN 47907, USA",0,,False
12,tjuchcin@purdue.edu,0,,False
13,ABSTRACT,0,,False
14,"Resource selection is an important task in Federated Search to select a small number of most relevant information sources. Current resource selection algorithms such as GlOSS, CORI, ReDDE, Geometric Average and the recent classificationbased method focus on the evidence of individual information sources to determine the relevance of available sources. Current algorithms do not model the important relationship information among individual sources. For example, an information source tends to be relevant to a user query if it is similar to another source with high probability of being relevant. This paper proposes a joint probabilistic classification model for resource selection. The model estimates the probability of relevance of information sources in a joint manner by considering both the evidence of individual sources and their relationship. An extensive set of experiments have been conducted on several datasets to demonstrate the advantage of the proposed model.",1,ad,True
15,Categories and Subject Descriptors,0,,False
16,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,0,,False
17,General Terms,0,,False
18,"Algorithms, Design, Performance",0,,False
19,Keywords,0,,False
20,"Federated Search, Resource Selection, Joint Classification",0,,False
21,1. INTRODUCTION,1,DUC,True
22,"Federated text search provides a unified search interface for multiple search engines of distributed text information sources. There are three major research problems in federated search as resource representation, resource selection",0,,False
23,Vietnam Education Foundation Fellow,0,,False
24,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
25,"and results merging. This paper focuses on resource selection, which selects a small number of most relevant information sources to search for any particular user query.",0,,False
26,"Resource selection for federated search has been a popular research topic in the last two decades. Many methods treat each information source as a single big document and rank available sources either by using statistics from the sample documents (CORI [6]), or by building a language model for each source (Xu and Croft [27], Si and Callan [22]). Other methods such as GlOSS [12], Geometric Average [17], ReDDE [20], CRCS [18] and SUSHI [24] look further inside an information source by estimating the relevance of each document and calculate the source's score as an aggregate function of the documents that the source contains. More recent methods such as the classification-based method [1] and the work in [2] treat resource selection as a classification problem and build probabilistic models by combining multiple types of evidence of individual sources.",1,ad,True
27,"Existing resource selection methods judge an information source by its own characteristics, but miss an important piece of evidence, which is the relationship between available sources. In practice, we notice that relationship can be meaningful and indicative. An information source that is ""similar"" to another highly relevant source has a better chance of being relevant. The evidence of source relationship can be very valuable for real world federated search solutions. In particular, the resource representation (e.g., sample documents) of each information source is often limited and prevents resource selection algorithms from identifying relevant sources, while the relationship between sources can help to alleviate the problem by providing more evidence from similar sources. Our study of a real world federated search application with digital libraries also suggests that it is difficult to obtain thorough resource representation from many sources (i.e., digital libraries). For example, some sources may only provide the abstracts of its documents instead of the full texts.",1,ad,True
28,"This paper proposes a novel probabilistic discriminative model for resource selection that explicitly models the relationship between information sources. In particular, the new research combines both the evidence of individual sources and the relationship evidence between sources into a single probabilistic model for estimating the joint probability of relevance of a set of sources. Different similarity metrics have been studied to explore the relationship between information sources. An extensive set of experiments have been conducted on two TREC testbeds for federated search",1,TREC,True
29,98,0,,False
30,research and one real world application for searching digital libraries. The experiment results demonstrate the effectiveness and robustness of the proposed resource selection algorithm with the joint classification model.,0,,False
31,The rest of this paper is organized as follows: the next section discusses the related research work. Section 3 presents the classification model for resource selection. Section 4 proposes the joint classification model. Section 5 discusses experimental methodology. Section 6 presents experimental results and related discussions. Section 7 concludes and points out some future research work.,0,,False
32,2. RELATED WORK,0,,False
33,"There has been considerable research on all of the three subtasks of federated search as resource representation, resource selection and results merging. Since this paper focuses on the resource selection task, we mainly survey most related prior research work in resource selection and briefly talk about resource representation and results merging.",0,,False
34,"The first step in federated search is to obtain representative resource descriptions from available information sources. The START protocol [11] provides accurate information in collaborative federated search environments, but it does not work for uncooperative environments. On other side, the query-based sampling technique [4] has been widely used in federated search to obtain sample documents from each source by issuing randomly generated queries. In particular, the query-based sampling approach is used in this work to acquire sample documents from available sources. After that, all sample documents are merged together as a centralized sample database.",0,,False
35,"Resource selection selects a small set of most relevant sources for each user query [3][8][13]. Most early resource selection algorithms treat each individual source as a single big document which they extract summary statistics from. Those big document methods such as KL [27], CORI [6], and CVV [28] utilize different types of summary statistics of sources and finally rank available sources by matching the statistics with the user's query. These methods ignore the boundaries of individual documents within individual sources, which limits their performance of identifying sources with a large number of relevant documents.",0,,False
36,"Some recent resource selection algorithms such as ReDDE [20], DTF [9][10], CRCS [18] and SUSHI [24] step away from treating each source as a single big document. Those algorithms often analyze individual sample documents within resource representation for ranking sources. For example, the ReDDE selection algorithm estimates the distribution of relevant documents by treating top-ranked sample documents as a representative subset of relevant documents in available sources. Related algorithms such as UUM [22], RUM [23] and CRCS [18] have been proposed, which use different methods to weight top-ranked documents and estimate the probability of relevance.",0,,False
37,More recent resource selection algorithms such as the classification-based resource selection in federated search [1] or vertical search [2] treat resource selection as a classification problem. A classification model can be learned from a set of training queries and is used to predict the relevance of a source for test queries. It has been shown [1] that the classification approach can outperform state-of-the-art resource selection algorithms like ReDDE.,0,,False
38,Existing resource selection methods utilize evidence within,0,,False
39,"individual sources to judge their relevance but ignore the evidence of the relationship between available sources. However, the relationship evidence is a valuable piece of information, which promises to improve the accuracy of resource selection.",0,,False
40,"Two other related research work in [25][7] learn from the results of past queries for resource selection. However, these two methods do not model the relationship between information sources and do not use formal models based on classification.",0,,False
41,"The last step of federated search is results merging, which merges returned documents from selected sources into a single list. The most effective method is to download and recalculate scores for all returned documents within a centralized retrieval model, but this is often inefficient. More efficient methods such as the CORI merging formula, the SSL [21] and the SAFE merging algorithms [19] try to approximate the results of centralized retrieval in different ways.",1,ad,True
42,3. CLASSIFICATION MODEL,0,,False
43,3.1 Classification Approach,0,,False
44,"Many resource selection algorithms are unsupervised and provide one source of evidence. To combine different evidence in a unified framework, one needs a training dataset, usually in the form of binary judgments on sources. Specifically, given a set of sources C and a set of training queries Q, the objective is to find a mapping F of the form",0,,False
45,"F : Q × C  {+1, -1}",0,,False
46,"where +1 indicates the relevance between the query and the source, and -1 indicates irrelevance.",0,,False
47,"Arguello et al.[1] have proposed a method to construct those judgments. Each query q  Q will be issued to a fulldataset index for searching. A source Ci  C is considered to be relevant with q if more than  documents from Ci are present in top T of the full-dataset result. Otherwise, it is marked as irrelevant.",0,,False
48,"While this method can produce a rank list that mimics the rank list produced by a full-dataset retrieval, it is difficult to apply in a real world environment because of the absence of a full-dataset. We propose an alternative method that could be more feasible. A query q is now issued to each remote source Ci and we only count their returned documents that are relevant. Top T documents from each source will be inspected, then a source is marked as relevant if it has more than  relevant documents presenting in that list. In our work, we set T ,"" 100. For dataset with a large average number of relevant documents per query (over 100), we set  "", 3; otherwise  is equal to 1.",0,,False
49,3.2 Sources of Evidence,0,,False
50,This section presents different types of evidence of individual sources for building our classification model.,0,,False
51,3.2.1 Big Document,0,,False
52,"Big Document (BIGDOC) approach treats each information source as a big document that contains all of its sample documents. A query is then issued to an index which contains a set of big documents, each representing one source. Sources are then ranked by how their merged sample documents match the query. The disadvantage of this method is that it does not take into account the variation of sources'",1,ad,True
53,99,0,,False
54,"sizes. Assuming that the sampling process is uniform, for a very big source, the sampling process only covers a small fraction of its documents. Therefore, it may present fewer relevant documents in the centralized sample database than a much smaller one, although the absolute number of relevant documents in the big source is higher. Without considering the sources' sizes, it would be misleading to conclude that the small source is the better choice. Nevertheless, when combined with other features, BIGDOC could have a good contribution, especially in the case that many sources contain roughly the same number of documents. While CORI (discussed in the next part) also treats each source as one document, BIGDOC approach is more flexible since it can be used with different retrieval algorithms. In our experiments, the algorithm is Indri [14]. For each pair of a query and a source, one BIGDOC feature is built from the sample documents.",1,ad,True
55,3.2.2 CORI,0,,False
56,"The CORI resource selection algorithm [6] uses Bayesian Inference Network model to rank sources. The belief P (q|Ci) that a source Ci satisfies query q is the combination of multiple P (rj|Ci), the belief corresponding to each term rj of query q. CORI applies a variant of tf.idf formula to determine each P (rj|Ci) and combine them together to calculate the final belief score of each source. CORI was proven to have robust performance for resource selection. In our experiments, one CORI feature is used for each pair of a query and a source.",0,,False
57,3.2.3 Geometric Average,0,,False
58,"In this method, a query is first issued to a centralized sample database, which was mentioned in section 2. Then, each source Ci is scored according to the geometric average query likelihood of its top K sample documents [17],",0,,False
59,  K,0,,False
60,1 K,0,,False
61,"GAV Gq(Ci) ,",0,,False
62,P (q|dij ),0,,False
63,"j,1",0,,False
64,"where dij is the j-th sample document in the rank list of source Ci. If Ci presents less than K documents in the rank list, the product above is padded with the minimum query likelihood score.",1,ad,True
65,3.2.4 Modified ReDDE & ReDDE.top,0,,False
66,Recall that ReDDE score [20] is calculated according to :,0,,False
67,ReDDEq (Ci ),0,,False
68,",",0,,False
69,Niest Nisamp,0,,False
70,×,0,,False
71,I (d,0,,False
72,dRsNamp,0,,False
73,Ci) × Pq(rel|d),0,,False
74,"where RsNamp is the top N documents returned from searching the centralized sample database. Niest is the estimated size of source Ci, Nisamp is the sample size of Ci, and I(.) is the indicator function. The number of top returned documents, N , is equal to  × Naelslt, where Naelslt is the estimated total number of documents of all sources and  is a constant,",0,,False
75,which is usually in the range 0.002-0.005.,0,,False
76,"ReDDE uses a step function to estimate Pq(rel|d), the probability that the document d is relevant to query q. For",0,,False
77,"all top N documents, that probability is equal to a constant.",0,,False
78,"In our experiment, we use a modified version of ReDDE,",0,,False
79,"which replaces Pq(rel|d) by P (q|d), the retrieval score of document d with respect to query q. The Indri retrieval",0,,False
80,algorithm [14] is used for searching the centralized sample database. The modified ReDDE feature has been shown empirically better than the original ReDDE feature. There is one modified ReDDE feature for each pair of a query and a source.,0,,False
81,"ReDDE.top [1] is another variant of ReDDE. Unlike ReDDE, ReDDE.top set a specific number to N . In our experiment, we add another two ReDDE.top features with N , 100 and N , 1000 respectively.",1,ad,True
82,4. JOINT CLASSIFICATION MODEL,0,,False
83,4.1 Probabilistic Discriminative Model,0,,False
84,"We propose a novel joint probabilistic model for the resource selection task. First of all, a logistic model is built to combine all the features of individual sources. We refer to this model as the independent model (Ind).",0,,False
85,"Let v ,"" {v1, ..., vn} be the relevance vector. vi "","" 1 indicates that the i-th source is relevant, otherwise vi "", 0. The relevance probability of a source ci given its feature vector f(ci) is calculated as:",0,,False
86,P (vi,0,,False
87,",",0,,False
88,1|ci),0,,False
89,",",0,,False
90,1,0,,False
91,exp(f(ci) · ) + exp(f(ci) · ),0,,False
92,"where  denotes the combination weight vector. For simplicity, the vector f(ci) contains the bias feature (which is 1 for every pair of a query and a source) and the weight vector ",0,,False
93,contains the bias element 0. The conditional probability of v given n sources is:,0,,False
94,P (v|c),0,,False
95,",",0,,False
96,1 Z,0,,False
97,n exp,0,,False
98, log P,0,,False
99,(vi,0,,False
100,",",0,,False
101,1|ci)vi P (vi,0,,False
102,",",0,,False
103,0|ci)1-vi ,0,,False
104,i,0,,False
105,where Z is the normalizing constant. Our joint classification model (Jnt) expands the above,0,,False
106,formula with a new term to model the relationship between sources. The conditional probability of v given n sources is now:,0,,False
107,P (v|c),0,,False
108,",",0,,False
109,1 Z,0,,False
110,n exp,0,,False
111, log P (vi,0,,False
112,",",0,,False
113,1|ci)vi P (vi,0,,False
114,",",0,,False
115,0|ci)1-vi ,0,,False
116,+,0,,False
117, |v|,0,,False
118,i,0,,False
119," sim(ci, cj )vivj",0,,False
120,"i,j(i<j)",0,,False
121,which can be rewritten as:,0,,False
122,P (v|c),0,,False
123,",",0,,False
124,1 Z,0,,False
125,n exp,0,,False
126,(1,0,,False
127,-,0,,False
128,vi,0,,False
129,)(f(ci),0,,False
130,·,0,,False
131,),0,,False
132,-,0,,False
133,log(1,0,,False
134,+,0,,False
135,exp(f(ci),0,,False
136,·,0,,False
137,)),0,,False
138,+,0,,False
139, |v|,0,,False
140,i,0,,False
141," sim(ci, cj )vivj",0,,False
142,"i,j(i<j)",0,,False
143,"where sim(ci, cj) denotes the similarity between two sources ci and cj, and Z is another normalizing constant.",0,,False
144,"The parameter  controls the influence of similarity. If || is high, the model tends to promote only similar (or dissimilar) sources. When  ,"" 0, we get back to the independent model.""",0,,False
145,"In the learning step, we learn the feature weight vector  from the independent model by using logistic regression. This vector is then used in the joint model. Learning , however, is generally intractable. One can see that the space of vector v is 2n, and so inferencing and estimation become",0,,False
146,100,0,,False
147,"impossible when n is large. We resolve this issue by first ranking the sources using the independent model, and then apply the joint classification model only to the top K , 10 sources. This is equivalent to reranking the top K sources.",0,,False
148,"From the set of training queries, we use maximum loglikelihood estimation to learn the parameter . Because there is no closed-form solution for the maximum of this loglikelihood function, gradient search method is used instead.",1,ad,True
149,"In the prediction step, for a test query, the score of each source ci is assigned by its probability of being relevant:",0,,False
150," R(ci) , P (vi , 1|c) ,"" P (v1, v2, ..., vi "","" 1, ..., vn|c)""",0,,False
151,vi,0,,False
152,"where vi denotes the set of variables in v with variable vi omitted. In practice, the summation is taken over K - 1 variables and so is feasible when K is small. After that, the top K sources will be reranked according to the new score.",0,,False
153,4.2 Similarity Metrics,0,,False
154,4.2.1 Similarity Metric based-on Evaluation,0,,False
155,"Given a set of training queries, the similarity between two sources can be measured by looking at the set of queries for wich they are both relevant. The bigger that set is, the more related they are. Specifically, we apply a cross-product formula to measure this metric:",0,,False
156," SM E(ci, cj) ,"" rel(ci, q)rel(cj, q)""",0,,False
157,qQ,0,,False
158,"where Q is the set of training queries, rel(ci, q) is equal to 1 if source ci is relevant to query q based on the classification approach described above, otherwise it is 0. This method is called Similarity Metric based-on Evaluation (SME).",0,,False
159,4.2.2 Similarity Metric based-on Query-specific,1,Query,True
160,Evaluation,0,,False
161,"One issue with the SME is that it is independent of the query. A source may be highly related with another source with respect to a query but unrelated with that source with respect to another query. Therefore, it is better to incorporate the similarity between queries into this formula. By extending the above SME, we derive another metric called Similarity Metric based on Query-specific Evaluation (SMQE).",1,corpora,True
162,"SM QEq(ci, cj ) ,",0,,False
163,"sim(q, q)rel(ci, q)rel(cj , q)",0,,False
164,q Q,0,,False
165,"where sim(q, q) denotes the correlation (or similarity) between the test query q and a training query q. There are many studies that explore the topicality or classification of queries, however, in this paper, we choose one simple approach. A query in consideration is issued to the centralized sample database, and the number of documents from each source that appear in top M documents of the result is recorded. In our work, M is equal to 100. The correlation between two queries is derived by a cosine-like formula:",0,,False
166,"sim(q, q)",0,,False
167,",",0,,False
168,i,0,,False
169,"numdoc(q,",0,,False
170,ci)numdoc(q,0,,False
171,",",0,,False
172,ci,0,,False
173,),0,,False
174,"i numdoc(q, ci)2 i numdoc(q, ci)2",0,,False
175,"where numdoc(q, ci) is the number of documents of source ci that appear in the top M documents returned from query q.",0,,False
176,"Both the SME and SMQE metrics can be modified in many ways. First of all, the term rel(i, q) can be represented either by a binary number or the absolute number of relevant documents. Or we can set different thresholds to the searching on the centralized sample database. Another choice is to normalize the relevance vector. However, in our experiments, those changes do not have much effect on the results. In fact, SMQE provides the best result, proving that it better reflects the relationship between sources.",1,MQ,True
177,4.2.3 Similarity-Metric based-on Kullback-Leibler,0,,False
178,divergence,0,,False
179,"This method tries to reveal the similarity between sources by looking at their own vocabularies. Specifically, a language model [16] is built for each sample source. Then we calculate the Kullback-Leibler divergence between those two language models. Recall that the Kullback-Leibler divergence is actually the distance between two probabilistic models, which is the inverse of their similarity. However, because our model can adapt this change by inferring a negative similarity coefficient , we keep the KL-value as it is. This metric is referred to as SMKL.",1,ad,True
180,5. EXPERIMENTAL METHODOLOGY,0,,False
181,"We evaluate our proposed algorithms on 3 datasets. The first two datasets are well-known TREC testbeds, the last one comes from a real world application.",1,TREC,True
182,"· TREC123-100col-bysource (TREC123): 100 collections (information sources) were created from TREC CDs 1,2 and 3 [3]. They are organized by publication source and publication date. The size of each source varies from 7,000 to 39,700 documents (see Table 1 for more statistics). This testbed comes with 100 queries (TREC topics 51-150) with judgments.",1,TREC,True
183,"· TREC4-100col-bysource (TREC4): 100 collections were created according to the publication source of documents in TREC4 [26]. One actual publication source is distributed across a number of information sources depending on its total number of documents. Each information source has roughly 5,675 documents. This testbed comes with 50 queries (TREC topics 201250) with judgments. More statistics about both TREC123 and TREC4 are presented in Table 1.",1,TREC,True
184,"· Digital Library (DIGLIB): This real world dataset contains 80 sources (i.e., digital libraries) that are accessible from Purdue University Libraries1. This testbed presents a heterogeneous sources of information. Each document from those sources composes of many fields. Three fields that convey rich information are the abstract, subject heading (or document's category) and full text. Not all sources provide all those three fields. A document from one source may not be provided with its full text, while a document from another source may not have the subject heading. Table 2 shows that only 65% of 80 sources provide abstracts, 65% provide subject headings, and only 30% provide full texts. In our current work, we temporally merge all those available information into one document. Future research may",1,ad,True
185,1We make the dataset available as feature file at http://www.cs.purdue.edu/homes/dthong/,0,,False
186,101,0,,False
187,Table 1: Summary Statistics of TREC123 and TREC4,1,TREC,True
188,Testbed,0,,False
189,TREC123 TREC4,1,TREC,True
190,Size (GB),0,,False
191,3.2 2.0,0,,False
192,Number of Documents (x1000) Min Avg Max 0.7 10.8 39.7 5.6 5.6 5.6,0,,False
193,Size(MB),0,,False
194,Min Avg Max 28 32 42 4 20 138,0,,False
195,Rk,0,,False
196,",",0,,False
197,"k ki,1",0,,False
198,"i,1",0,,False
199,Ei Bi,0,,False
200,"Let Ei denote the number of relevant documents of the i-th source according to the ranking E, Bi denote the same thing with respect to ranking B. We also report the results at P @{1, 3, 5, 10} as in source level accuracy.",0,,False
201,Table 2: Statistical Information about DIGLIB: Number of Sources Corresponding to their Available Information Fields,0,,False
202,Abstract Subject Full Text Number of Sources 52(65%) 52(65%) 24(30%),0,,False
203,"· Document Level, High Precision: To make it independent from result merging algorithm, we use fulldataset retrieval as our merging method. The chosen retrieval algorithm is Inquery in Lemur Toolkit [5]. For each query, top 5 sources from the joint classification rank list are selected for this step. Documents not from selected sources are filtered out from the full-dataset rank list. The remaining list is checked by their precision at P @{5, 10, 15, 20, 30} accordingly. We also report a full-dataset precision which includes all sources.",0,,False
204,consider to treat each source differently according the type of information that is available.,0,,False
205,"We also build a set of 100 queries, some of them are extracted from the library log, which are real queries. For each pair of a query and a source, we manually assign a binary value to indicate their relevance.",0,,False
206,"On TREC123 and TREC4, all tests at different levels are presented. On DIGLIB dataset, we only report the results at source level because the document judgments are difficult to make as many sources do not provide their full text information. In most of the experiments, SMQE is used as our default similarity metric. However, in section 6.4, we also discuss the experimental results with different other metrics.",1,TREC,True
207,"A note on resource specific retrieval algorithm: DIGLIB is a real world application of digital libraries, each of its sources implements a different retrieval algorithm, which is not known. We can only access those sources through a unified interface. For TREC123 and TREC4, we assign one retrieval algorithm to each source in a round-robin manner. The set of assigned algorithms is Inquery, Language Model and Vector Space (tf.idf). These algorithms influence the query-based sampling process, as well as the classification process. A less effective retrieval algorithm like Vector Space model may reduce a source's chance of being marked as relevant.",1,TREC,True
208,"For each testbed, we repeat every experiment 5 times. In each trial, we randomly select 50% of the queries as training set, and test on the other 50%. All the results shown in the next section are averaged over 5 trials.",0,,False
209,Each source is sampled with 300 documents. We also compare the main results with 100 sample documents. The experiments are measured on several levels:,0,,False
210,"· Source Level (Resource Selection), Accuracy: This level measures the precision of the resource selection algorithms. Top sources (i.e., top 10) are judged by their precisions at different levels. The judgments come from the classification method, as described in section 3.1. We report the results at P @{1, 3, 5, 10} accordingly.",0,,False
211,"· Source Level, Recall Metric (R-metric): This metric is widely used in comparing resource selection algorithms [3]. Let E denote the ranking produced by a resource selection algorithm, B denote the based line ranking, in this case, the Relevance-Based Ranking. At level k, the R-metric is defined as:",0,,False
212,6. EXPERIMENTAL RESULTS,0,,False
213,"In all of our experiments, we use paired t-test on queries to check significance. A  denotes a significance on p < 0.1 level;  corresponds to p < 0.05 level and  corresponds to p < 0.01 level.",0,,False
214,6.1 TREC123 & TREC4,1,TREC,True
215,"First of all, we compare the joint classification model with the independent model on the two TREC testbeds. Table 3 represents the source level results in accuracy on TREC123 and TREC4. The second column of each dataset is the joint classification model. Numbers in parentheses show the relative improvement of the joint classification model (denoted as ""Jnt"") over the independent model (denoted as ""Ind"").",1,TREC,True
216,"Table 4 shows the R-metric comparison between the independent model and joint classification model. Table 5 shows the high precision at document level. We also report the full centralized retrieval, which includes all sources. This is denoted as the ""Full"" column in the table.",0,,False
217,"It can be seen that joint classification model always leads to better results than independent model, as it shows in all three tables. Both models have the same source level accuracy and R-metric values at top 10 because of the fact that we rerank the top 10 sources. The results are more statistically significant on TREC123 than on TREC4. This can be explained as in TREC123, we have trained on 50 queries; whereas in TREC4, we use only 25 queries out of 50 for training.",1,ad,True
218,6.2 Digital Library,0,,False
219,"The result at source level of Digital Library is reported in Table 6. In this real world dataset, the joint classification model significantly outperforms the independent model. This accounts to the fact that many sources only provide",0,,False
220,102,0,,False
221,Table 3: Source Level Results in Accuracy on TREC123 & TREC4 with 300 Sample Documents,1,TREC,True
222,Table 5: Document Level Results in High Precision on TREC123 & TREC4 with 300 Sample Documents,1,TREC,True
223,Src Rank,0,,False
224,@1 @3 @5 @10,0,,False
225,TREC123,1,TREC,True
226,Ind 0.512 0.456 0.451 0.439,0,,False
227,Jnt 0.524(2.3%) 0.499(9.4%) 0.484(7.3%),0,,False
228,0.439(0%),0,,False
229,TREC4,1,TREC,True
230,Ind 0.480 0.451 0.430 0.414,0,,False
231,Jnt 0.536(11.7%) 0.475(5.3%) 0.446(3.7%),0,,False
232,0.414(0%),0,,False
233,Docs Rank,0,,False
234,@5 @10 @15 @20 @30,0,,False
235,Full 0.446 0.444 0.435 0.430 0.414,0,,False
236,TREC123,1,TREC,True
237,Ind 0.392 0.355 0.332 0.309 0.280,0,,False
238,Jnt 0.410(4.6%) 0.360(1.4%) 0.347(4.5%) 0.326(5.5%) 0.300(7.1%),0,,False
239,Table 4: Source Level Results in R-metric on TREC123 & TREC4 with 300 Sample Documents,1,TREC,True
240,Src Rank,0,,False
241,@1 @3 @5 @10,0,,False
242,TREC123,1,TREC,True
243,Ind 0.262 0.309 0.354 0.426,0,,False
244,Jnt 0.319(21.8%) 0.364(17.8%) 0.400(13.0%),0,,False
245,0.426(0%),0,,False
246,TREC4,1,TREC,True
247,Ind 0.287 0.324 0.343 0.414,0,,False
248,Jnt 0.309(7.7%) 0.340(4.9%) 0.355(3.5%),0,,False
249,0.414(0%),0,,False
250,Docs Rank,0,,False
251,@5 @10 @15 @20 @30,0,,False
252,Full 0.549 0.459 0.422 0.384 0.354,0,,False
253,TREC4,1,TREC,True
254,Ind 0.282 0.238 0.209 0.186 0.167,0,,False
255,Jnt 0.290(2.8%) 0.254(6.7%) 0.224(7.2%) 0.200(7.5%) 0.170(1.8%),0,,False
256,partial information about themselves. This also shows that the joint classification model can alleviate the problem of missing information.,0,,False
257,6.3 Tests with Different Sample Sizes,0,,False
258,"We conduct experiments on three datasets with only 100 documents sampled from each source. This test is to show the robustness of the model, as well as the effect of sampling size on the results. The results of source level (both in accuracy and R-metric) and document level are reported for TREC123 and TREC4 (Table 7, Table 8 and Table 9 respectively), while only source level is reported for DIGLIB (Table 10).",1,TREC,True
259,"The sample size clearly affects TREC123. Its performance of the independent model drops significantly. However, this also leaves room for joint classification model to show its effectiveness: the accuracy on source level is statistically more significant. On document level, the improvement is a bit weaker. This can be explained as the initial choice of top 10 sources from the independent model is less precise, so is the joint classification model, which uses the initial ranking list directly.",1,TREC,True
260,"Most results on TREC4 from Table 7 to Table 9 indicate the advantage of the joint classification model against independent model with a small number of sample documents, although the difference is smaller than TREC123 due to the limited amount of training information.",1,TREC,True
261,"The results on DIGLIB (Table 10) are also consistent. The performances of both resource selection algorithms drop with 100 sample documents. However, the results of the joint classification method are still significantly better than those of the independent method.",0,,False
262,6.4 Test with Different Similarity Metrics,0,,False
263,We conduct tests on three testbeds with different similarity metrics discussed in Section 4.2. Figure 1 shows the,0,,False
264,Table 6: Source Level Results in Accuracy on DIGLIB with 300 Sample Documents,0,,False
265,Src Rank,0,,False
266,@1 @3 @5 @10,0,,False
267,DIGLIB,0,,False
268,Ind 0.552 0.460 0.419 0.356,0,,False
269,Jnt 0.640(15.9%) 0.536(16.5%) 0.487(16.2%),0,,False
270,0.356(0%),0,,False
271,Table 7: Source Level Results in Accuracy on TREC123 & TREC4 with 100 Sample Documents,1,TREC,True
272,Src Rank,0,,False
273,@1 @3 @5 @10,0,,False
274,TREC123,1,TREC,True
275,Ind 0.320 0.299 0.318 0.319,0,,False
276,Jnt 0.380(18.8%) 0.373(24.7%) 0.357(12.3%),0,,False
277,0.319(0%),0,,False
278,TREC4,1,TREC,True
279,Ind,0,,False
280,0.496 0.405 0.379 0.367,0,,False
281,Jnt 0.480(-3.2%) 0.411(1.5%) 0.403(6.3%),0,,False
282,0.367(0%),0,,False
283,Table 8: Source Level Results in R-metric on TREC123 & TREC4 with 100 Sample Documents,1,TREC,True
284,Src Rank,0,,False
285,@1 @3 @5 @10,0,,False
286,TREC123,1,TREC,True
287,Ind 0.183 0.214 0.244 0.311,0,,False
288,Jnt 0.233(27.3%) 0.262(22.4%) 0.279(14.3%),0,,False
289,0.311(0%),0,,False
290,TREC4,1,TREC,True
291,Ind 0.278 0.264 0.293 0.341,0,,False
292,Jnt 0.317(14%) 0.293(11%) 0.311(6.1%),0,,False
293,0.341(0%),0,,False
294,103,0,,False
295,Table 9: Document Level Results in High Precision on TREC123 & TREC4 with 100 Sample Documents,1,TREC,True
296,TREC123,1,TREC,True
297,0.3,0,,False
298,0.42,0,,False
299,SMQE,1,MQ,True
300,SME,0,,False
301,0.4,0,,False
302,Independent,0,,False
303,0.28,0,,False
304,SMKL,0,,False
305,TREC4,1,TREC,True
306,SMQE SME Independent SMKL,1,MQ,True
307,Docs Rank,0,,False
308,@5 @10 @15 @20 @30,0,,False
309,TREC123,1,TREC,True
310,Ind 0.328 0.302 0.288 0.277 0.253,0,,False
311,Jnt 0.329(0.3%) 0.316(4.6%) 0.306(6.2%) 0.296(6.9%) 0.268(5.9%),0,,False
312,TREC4,1,TREC,True
313,Ind 0.283 0.243 0.223 0.195 0.165,0,,False
314,Jnt 0.301(6.4%) 0.254(4.5%) 0.227(1.8%) 0.204(4.6%) 0.166(0.6%),0,,False
315,Table 10: Source Level Results in Accuracy of DIGLIB with 100 Sample Documents,0,,False
316,Precision Precision,0,,False
317,0.38 0.36 0.34 0.32,0,,False
318,0.3 0.28 0.26,0,,False
319,0,0,,False
320,10,0,,False
321,20,0,,False
322,Document Rank,0,,False
323,0.26,0,,False
324,0.24,0,,False
325,0.22,0,,False
326,0.2,0,,False
327,0.18,0,,False
328,0.16,0,,False
329,30,0,,False
330,0,0,,False
331,10,0,,False
332,20,0,,False
333,30,0,,False
334,Document Rank,0,,False
335,Src Rank,0,,False
336,@1 @3 @5 @10,0,,False
337,DIGLIB,0,,False
338,Ind 0.436 0.383 0.375 0.318,0,,False
339,Jnt 0.620(42.2%) 0.531(38.6%) 0.474(26.4%),0,,False
340,0.318(0%),0,,False
341,"results of TREC123 and TREC4 at document level. From this figure, we notice that the SMQE method outperforms all other metrics, due to the fact the it considers the similarity between queries. The SME produces a quite close-to-best result, but the SMKL tends not to be a good choice for the joint classification model. On TREC4, SMKL is comparable with independent model, but it is worse than SMQE and SME.",1,TREC,True
342,"Figure 2 shows the results of DIGLIB at source level. In this case, both SMQE and SME are comparable, except for the precision at top 1. Again SMKL is not a good choice.",1,MQ,True
343,Figure 1: Document Level High Precision on TREC123 & TREC4 with Different Similarity Metrics,1,TREC,True
344,Precision,0,,False
345,DIGLIB,0,,False
346,0.65,0,,False
347,SMQE,1,MQ,True
348,SME,0,,False
349,Independent,0,,False
350,0.6,0,,False
351,SMKL,0,,False
352,0.55,0,,False
353,0.5,0,,False
354,0.45,0,,False
355,0.4,0,,False
356,0.35,0,,False
357,0,0,,False
358,2,0,,False
359,4,0,,False
360,6,0,,False
361,8,0,,False
362,10,0,,False
363,Source Rank,0,,False
364,7. CONCLUSION & FUTURE WORK,0,,False
365,"This paper proposes a novel joint probabilistic classification model for the resource selection task in federated text search. Existing resource selection algorithms only utilize evidence of individual information sources to select relevant sources, but they do not model the valuable relationship information between the sources. The proposed algorithm estimates the probability of relevance of information sources in a joint manner by combining both the evidence of individual sources and the relationship between the sources. The importance of different types of evidence is determined in a discriminative manner for maximizing the accuracy of resource selection with some training queries. Different types of similarity metrics have been explored to model source similarity based on the performance of available sources on training queries and the Kullback-Leibler divergence on the contents of the sources. A set of experiments were conducted with two TREC datasets and one real world application with digital libraries. The empirical results in different configurations have demonstrated the effectiveness of the proposed joint classification model.",1,TREC,True
366,"There are several directions to extend the research work in the paper. First, one advantage of the proposed joint probabilistic model is to integrate different types of evidence of",1,ad,True
367,Figure 2: Source Level Accuracy on DIGLIB with,0,,False
368,Different Similarity Metrics,0,,False
369,"individual sources and their relationship. We plan to explore more features for improving the performance of resource selection. For example, we can combine multiple types of similarity evidence in a single framework (with different  weights), which may better model sources' relationship for more accurate resource selection. Second, the joint model in this paper utilizes a reranking approach in resource selection with a small set of information sources (e.g., top 10) to avoid large computational complexity. It is possible to break this limit by utilizing some other approximate inference algorithms (e.g., the pseudo likelihood approach [15]) or making further assumptions on the sources' relationship. For example, one strategy is to first divide available sources into groups of closely related sources. Inference can be conducted by building a small model in each group and assuming independence of sources between different groups.",0,,False
370,8. ACKNOWLEDGMENTS,0,,False
371,This research was partially supported by the Vietnam Education Foundation (VEF) and the NSF grant IIS-0749462.,0,,False
372,104,0,,False
373,"The opinions, findings, and conclusions stated herein are those of the authors and do not necessarily reflect those of the sponsors.",0,,False
374,9. REFERENCES,0,,False
375,"[1] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. Proceeding of",0,,False
376,"the 18th ACM Conference on Information and Knowledge Management, 2009. [2] J. Arguello, F. D´iaz, J. Callan, and J. Crespo. Sources of evidence for vertical selection. In Proceedings of the",0,,False
377,"32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2009. [3] J. Callan. Distributed information retrieval. Advances in Information Retrieval, pages 127­150, 2000. [4] J. Callan and M. Connell. Query-based sampling of text databases. ACM Transactions on Information Systems, 19(2):97­130, 2001. [5] J. Callan, W. B. Croft, and S. M. Harding. The inquery retrieval system. In Proceedings of the Third",1,Query,True
378,"International Conference on Database and Expert Systems Applications, 1992. [6] J. Callan, Z. Lu, and W. B. Croft. Searching distributed collection with inference networks. In",0,,False
379,Proceedings of the 18th Annual International ACM,0,,False
380,"SIGIR Conference on Research and Development in Information Retrieval, 1995. [7] S. Cetintas, L. Si, and H. Yuan. Learning from past queries for resource selection. In Proceeding of the 18th",0,,False
381,"ACM Conference on Information and Knowledge Management. ACM, 2009. [8] N. Craswell, P. Bailey, and D. Hawking. Server selection on the world wide web. In Proceedings of the 5th ACM Conference on Digital Libraries. ACM, 2000. [9] N. Fuhr. A decision-theoretic approach to database selection in networked ir. ACM Transactions on Information Systems (TOIS), 17(3):229­249, 1999. [10] N. Fuhr. Resource discovery in distributed digital libraries. In In Digital Libraries '99: Advanced Methods and Technologies, Digital Collections, 1999. [11] L. Gravano, K. Chang, C-C., H. Garc´ia-Molina, and A. Paepcke. Starts: Stanford proposal for internet meta-searching. In Proceedings of the ACM-SIGMOD",0,,False
382,"International Conference on Management of Data (SIGMOD). ACM, 1997. [12] L. Gravano, H. Garc´ia-Molina, and A. Tomasic. Gloss: Text-source discovery over the internet. ACM Transactions on Database Systems, 24(2):229­264, 1999. [13] W. Meng, C. Yu, and K. Liu. Building efficient and effective metasearch engines. ACM Computing Surveys (CSUR), 34(1):48­89, 2002. [14] D. Metzler and W. Croft. Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5):735­750, 2004. [15] S. Parise and M. Welling. Learning in markov random fields: An empirical study. In Joint Statistical Meeting (JSM2005), volume 4, 2005. [16] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of",0,,False
383,the 25th Annual International ACM SIGIR,0,,False
384,"Conference on Research and Development in Information Retrieval. ACM, 1998. [17] J. Seo and W. B. Croft. Blog site search using resource selection. In CIKM '08: Proceeding of the",0,,False
385,"17th ACM Conference on Information and Knowledge Management, pages 1053­1062, New York, NY, USA, 2008. ACM. [18] M. Shokouhi. Central-rank-based collection selection in uncooperative distributed information retrieval. In",0,,False
386,"Proceedings of the 29th European Conference on Information Retrieval, 2007. [19] M. Shokouhi and J. Zobel. Robust result merging using sample-based score estimates. ACM Transactions on Information Systems, 27(3):1­29, 2009. [20] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. In",1,Robust,True
387,Proceedings of the 26th Annual International ACM,0,,False
388,"SIGIR Conference on Research and Development in Information Retrieval. ACM, 2003. [21] L. Si and J. Callan. A semi-supervised learning method to merge search engine results. ACM Transactions on Information Systems, 21(4):457­491, 2003. [22] L. Si and J. Callan. Unified utility maximization framework for resource selection. In Proceedings of",0,,False
389,"13th ACM International Conference on Information and Knowledge Management (CIKM), 2004. [23] L. Si and J. Callan. Modeling search engine effectiveness for federated search. In Proceedings of the",0,,False
390,"28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2005. [24] P. Thomas and M. Shokouhi. Sushi: scoring scaled samples for server selection. In SIGIR '09: Proceedings",0,,False
391,"of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2009. [25] E. Voorhees, N. K. Gupta, and B. Johnson-Laird. Learning collection fusion strategies. In Proceedings of",0,,False
392,the 18th Annual International ACM SIGIR,0,,False
393,"Conference on Research and Development in Information Retrieval. ACM, 1995. [26] J. Xu and J. Callan. Effective retrieval with distributed collections. In Proceedings of the 21st",0,,False
394,"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1998. [27] J. Xu and W. B. Croft. Cluster-based language models for distributed retrieval. In Proceedings of the 22nd",0,,False
395,"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1999. [28] B. Yuwono and D. L. Lee. Server ranking for distributed text retrieval systems on the internet. In",0,,False
396,Proceedings of the 5th Annual International,0,,False
397,"Conference on Database Systems for Advanced Applications, 1997.",0,,False
398,105,0,,False
399,,0,,False

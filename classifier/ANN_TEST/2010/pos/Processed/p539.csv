,sentence,label,data,regex
0,The Effect of Assessor Errors on IR System Evaluation,0,,False
1,Ben Carterette,0,,False
2,"Dept. of Computer and Information Sciences University of Delaware Newark, DE 19716",0,,False
3,carteret@cis.udel.edu,0,,False
4,Ian Soboroff,0,,False
5,"National Institute of Standards and Technology Gaithersburg, MD 20899",0,,False
6,ian.soboroff@nist.gov,0,,False
7,ABSTRACT,0,,False
8,"Recent efforts in test collection building have focused on scaling back the number of necessary relevance judgments and then scaling up the number of search topics. Since the largest source of variation in a Cranfield-style experiment comes from the topics, this is a reasonable approach. However, as topic set sizes grow, and researchers look to crowdsourcing and Amazon's Mechanical Turk to collect relevance judgments, we are faced with issues of quality control. This paper examines the robustness of the TREC Million Query track methods when some assessors make significant and systematic errors. We find that while averages are robust, assessor errors can have a large effect on system rankings.",1,TREC,True
9,"Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation, H.3.5 Online Information Services",0,,False
10,"General Terms: Experimentation, Measurement",0,,False
11,"Keywords: assessor error, retrieval test collections",0,,False
12,1. INTRODUCTION,1,DUC,True
13,"Since TREC began in 1992 as the first large-scale use of pooling to create test collections, a great deal of research has focused on examining the quality of pooled test collections in spite of violations of the Cranfield assumptions [9, 17, 13, 14, 15] and on refining pooling to reduce costs and/or maximize quality [8, 12, 11, 3, 16, 5]. The TREC Million Query track [1] has emerged as a testbed for modern testcollection building methods, primarily those of Aslam and Pavlu [2] and Carterette et al. [6].",1,TREC,True
14,"A primary motivation in many modern collection-building methods is to reduce the costs associated with making relevance judgments. Relevance assessors can be expensive to hire, train, and use, and particularly in the academic community where funding may not be available for collection building, low-cost (or zero-cost) methods have broad appeal, if not as yet broad application.",1,ad,True
15,"Recently, crowdsourcing and Amazon's Mechanical Turk (MTurk)1 have been used as sources of relevance judgments.",0,,False
16,1https://www.mturk.com/mturk/welcome,0,,False
17,"Copyright 2010 Association for Computing Machinery. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of the U.S. Government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,Gov,True
18,"These approaches have a very low cost per judgment, but they may have somewhat higher design costs and require additional cost and effort to control for assessor error. In particular, if an MTurk worker's only goal is to complete the task, the judgments may not look very different from random. Soboroff et al. has simulated a family of cases of random assessor errors: the assessors know roughly how many relevant documents there are (give or take a standard deviation or two), but they pay little to no attention to which documents they are judging relevant [12]. Soboroff et al. assigned relevance to documents in the pool randomly, creating a pseudo-rels that they used to evaluate systems. Surprisingly, evaluation results over the pseudo-rels correlated significantly to evaluation results over the ""true"" relevance judgments from the assessors.",1,ad,True
19,"That work had the relative luxury of deep pools of documents, which may have resulted in emergence of patterns. But even an assessor making judgments at random would take a fair amount of time to make, say, 12,000 relevance judgments. Recent work on test collections suggests that assessing more topics with fewer judgments each is a more cost-effective approach, since the topics are a larger source of variance than the missing judgments [11, 7]. But that work assumes the judgments are in some sense ""perfect"", i.e. that errors made by assessors have inconsequential variance and no bias. This is almost certainly not the case; assessor make mistakes due to misunderstandings of the task or documents, fatigue, boredom, and for many other reasons. These mistakes surely have a larger impact on evaluation when there are only a few relevance judgments to begin with.",1,ad,True
20,"Furthermore, most TREC collections are built using trained relevance assessors. Practitioners and researchers using modern collection building methods such as those pioneered in the TREC Million Query track may wish to create their own test collections using available sources of labor, such as Amazon's Mechanical Turk or other crowdsourcing methods. Bailey et al. [4] found that assessors of differing task and domain expertise could affect system rankings markedly; Kinney et al. [10] found that non-expert assessors judging domain-specific queries make significant errors affecting system evaluation. When assessors are not closely managed or highly trained, mistakes must be common.",1,TREC,True
21,"Our goal is to investigate the effect of assessor errors by simulating different assessor ""archetypes"" in a low-cost largescale test collection scenario. We propose several models of how assessors might make errors, then use the models to simulate assessors going through the process of making judgments. We focus specifically on the effect in the TREC",1,TREC,True
22,539,0,,False
23,"Million Query track test collections, where there are many queries with very few judgments each.",1,Query,True
24,2. ASSESSOR BEHAVIOR AND ERRORS,0,,False
25,"We mined a large log of assessor interaction data from the TREC 2009 Million Query (MQ) track for patterns of assessor behavior. The log contains a record for each judgment; each record consists of a timestamp, the query number, the document ID, the assessor ID, and the judgment itself. We examined the first 32 judgments per topic and excluded inter-judgment times in excess of 200 seconds. Based on this data, and further on our experience managing relevance assessors for various projects, we present some types of errors assessor might make, some ways we can model them, and hypothesized effects on evaluation in a MQ-type setting.",1,TREC,True
26,First we identify the following broad trends that can be used to model assessor behavior. These formed the basis for some of our models below.,1,ad,True
27,1. Time between judgments decreases slightly from the first judgments for a topic to the last judgment for a topic (Fig. 1(a)).,0,,False
28,2. It takes less time to judge a nonrelevant document than to judge a document with any degree of relevance (Fig. 1(b)).,0,,False
29,3. Assessors can vary in their judging times (Fig. 1(c)) Our assessors do not display much variation but we might see more so in a larger set.,0,,False
30,"4. Measured across non-overlapping but large sets of topics, assessors vary in the proportion of documents they judge relevant (Fig. 1(d)).",0,,False
31,"These observations clearly indicate that assessors behave differently (i.e., there is variance due to the assessor), and moreover that there is interaction between assessor and document. There may also be interaction between assessor and topic. Because each topic was judged only once we will not find evidence of that in aggregate, but we looked for a particular type of interaction: topics that an assessor gave less attention to than normal, possibly due to unhappiness with the documents they were asked to judge. To do this, we modeled time between judgments as a function of assessor, document judgment, and where in the sequence the judgment fell. We then looked for topics for which the actual judgment times were lower than those predicted by the model across most of the sequence, specifically cases where at least 90% of the judgments were faster than expected. There were about 30 such topics, and for all 30 all documents were nonrelevant.",0,,False
32,"Finally, we looked for evidence of autocorrelation in judgments. We calculated the proportion of times an assessor judged a document relevant conditional on judging the previous document relevant, and contrasted that with the proportion conditional on judging the previous document nonrelevant. We used a subset of queries with roughly equal proportions of relevance so any effect would not be confounded by differing numbers of relevant documents. Assessors are in fact more likely to make the same judgment twice in a row: P (ji , 1|ji-1 , 1) ,"" 0.22, while P (ji "", 1|ji-1 , 0) is only 0.18. This difference is significant by a two-sample two-proportion test.",0,,False
33,540,0,,False
34,200,0,,False
35,Time to judge (seconds),0,,False
36,150,0,,False
37,100,0,,False
38,50,0,,False
39,0 0,0,,False
40,10,0,,False
41,20,0,,False
42,30,0,,False
43,Judgments in sequence,0,,False
44,(a) Time to judge a document decreases slightly over the sequence of documents in a topic.,0,,False
45,relevant,0,,False
46,0,0,,False
47,50,0,,False
48,100,0,,False
49,150,0,,False
50,200,0,,False
51,related,0,,False
52,G,0,,False
53,G,0,,False
54,not relevant,0,,False
55,highly relevant,0,,False
56,G,0,,False
57,G,0,,False
58,0,0,,False
59,50,0,,False
60,100,0,,False
61,150,0,,False
62,200,0,,False
63,Time to judge (seconds),0,,False
64,"(b) Relevant, highly relevant, and related documents take longer to judge than nonrelevant ones.",0,,False
65,200,0,,False
66,Time to judge (seconds),0,,False
67,150,0,,False
68,100,0,,False
69,50,0,,False
70,G,0,,False
71,G,0,,False
72,G,0,,False
73,G,0,,False
74,G,0,,False
75,G,0,,False
76,0,0,,False
77,4,0,,False
78,5,0,,False
79,6,0,,False
80,7,0,,False
81,8,0,,False
82,9,0,,False
83,Assessor,0,,False
84,(c) Our assessors vary only slightly in their judgments times.,0,,False
85,not-rel,0,,False
86,rel,0,,False
87,high-rel,0,,False
88,related,0,,False
89,9,0,,False
90,8,0,,False
91,Assessor,0,,False
92,7,0,,False
93,6,0,,False
94,5,0,,False
95,4,0,,False
96,0,0,,False
97,1000,0,,False
98,2000,0,,False
99,3000,0,,False
100,4000,0,,False
101,5000,0,,False
102,Judgments,0,,False
103,"(d) Assessors vary in the proportion of documents they judge relevant, as measured over large non-overlapping topic sets.",0,,False
104,Figure 1: Interaction log observations.,0,,False
105,2.1 Model Distributions and Priors,0,,False
106,"We will simulate different types of assessors using models of systematic errors based on observations above. Given a set of (binary) judgments for a topic, we convert nonrelevant judgments to relevant and vice versa according to a model.",0,,False
107,"If we are not careful, we can easily inject too much bias into the evaluation. For instance, if we model an assessor that has a tendency to overrate relevance by changing a judgment of nonrelevance to relevance with fixed probability p, the expected effect is to simply increase the number of relevant documents to pn, where n is the number of judged documents. Increasing the number of relevant documents in this way does not really address the question; some of those topics may very clearly have no relevant documents and even an optimistic assessor would not say they do. We therefore tied model parameters to the number of known relevant documents for each topic.",1,ad,True
108,"To do this, we define a model in terms of ""background"" parameters modeling average behavior; these parameters are then adjusted by topic. This is easy to do using discrete distributions such as Bernoulli and Poisson distributions. The parameters of both distributions have natural conjugate priors that allow updating based on existing judgments.",1,ad,True
109,"The Bernoulli distribution can be seen as modeling the probability p that a document is relevant; the parameter p can be modeled as having a Beta distribution. A Beta distribution is specified with two parameters ,  and has mean /( + ). The posterior of a Beta random variable also has a Beta distribution (hence it is conjugate); the posterior parameters are  + r,  + (n - r), where n is a number of observations and r is the number of positive observations. We can therefore view a Beta prior as a model of an assessor's average probability of judging a document relevant, and a Beta posterior using the number of relevant documents rq among nq judgments for a topic q as a model of an assessor's probability of judging a document relevant to q.",0,,False
110,"Similarly, the Poisson distribution can be seen as modeling the number of documents that will be judged relevant in some sequence length based on a rate parameter ;  can be modeled as having a Gamma distribution. Like a Beta distribution, a Gamma distribution is specified with two parameters , , but its mean is /. The posterior of a Gamma random variable has a Gamma distribution with parameters  + r,  + n. We can therefore view a Gamma prior as a model of an assessor's rate of judging documents relevant, and a Gamma posterior using rq and nq as the rate adjusted for the topic.",1,ad,True
111,2.2 Assessor Models,0,,False
112,"Our baseline model is that an assessor makes judgments randomly. Real assessors of course do not make random relevance judgments; this model only serves as comparison to the more informed models below. Using the Beta distribution as described above, the probability that a document will be judged relevant to a topic q will be ( + rq)/( +  + nq). So, for example, an assessor modeled by prior parameters  ,"" 2,  "", 8 (expected to judge 20% of documents relevant) confronted by a topic for which all 32 documents have been judged nonrelevant will have a 2/42 ,"" 0.05 probability of judging each of those documents relevant. The effect of this is that the number of simulated relevant documents for each topic can be expected to be """"near"""" the actual number of""",0,,False
113,"relevant documents, but with enough noise that evaluation results will change.",0,,False
114,"Our first realistic model is an unenthusiastic assessor, that is, the assessor is not interested in reading or understanding documents and simply wants to complete the job. Judgments by this assessor may be characterized by a pattern such as judging everything nonrelevant, or alternating judgments in some pattern. Our discovery of topics that were completed faster than usual and with all nonrelevant judgments lends support to this model. For the unenthusiastic assessor we do not suppose that there is any particular probability model. The assessor just follows a fixed pattern, such as judging everything nonrelevant or alternating between relevant and nonrelevant judgments. This is in some sense the most biased model, in that the simulated judgments have nothing to do with the actual judgments.",1,ad,True
115,"Our second model is the optimistic assessor; he or she takes an overly-broad view of the topic and ends up judging things relevant that are not. It is well-known from work by Harman [9] and Voorhees [13] that assessors do differ reasonably in their judgment of relevance; in this model and the one following we presume a view of the topic which most observers would consider beyond reasonable, for example judging relevance solely by the presence of certain terms or not correctly identifying a spam document as not relevant. We model optimistic assessors as being more likely to judge a nonrelevant document relevant. The model parameters are again Beta parameters , , and the probability that a document will be judged relevant to a particular topic is ( + rq)/( +  + nq). However, for this model we can only change nonrelevant judgments to relevant; we will not change any of the relevant documents to nonrelevant.",1,ad,True
116,"Conversely, we model a pessimistic assessor as taking an overly-narrow view of the topic and judging documents nonrelevant that should be considered relevant. The model is identical to the optimistic model, except that relevant documents become nonrelevant with probability ( + (nq - rq))/( +  + nq) (which comes from treating a nonrelevant judgment as the ""positive"" outcome).",0,,False
117,"Another model we explore we call topic-disgruntled. This assessor chose a query for some reason (interest in topic, seemed easy), but the documents turned out to be something else (different topic, harder than expected). Disgruntled by the topic, the assessor begins to click through rapidly after the first few judgments. Again, the presence of topics completed faster than usual lends support to this model. The model is time-based. After k judgments, the assessor becomes disgruntled and judges the remaining documents nonrelevant. The parameter is a ""patience parameter"" ; after nq judgments (which are identical to the ""true"" judgments), the assessor judges everything else nonrelevant. This assessor has a Gamma prior specified by parameters ,  (resulting in prior patience  ,"" /), and their posterior patience for a given topic is based on how many relevant documents there are:  "", ( + rq)/( + nq).",0,,False
118,Similar in execution to the disgruntled model is the lazy/ overfitting assessor. The assessor sees a few very nonrelevant (or a few very relevant) documents at the start of judging and unjustly assumes all subsequent documents are likely to be the same. He or she begins rapidly entering judgments conforming to his early judgments. The model is implemented roughly the same as the topic-disgruntled,0,,False
119,541,0,,False
120,"model, except that it only kicks in if the first nq judgments are all nonrelevant (or all relevant).",0,,False
121,"Another model is that the assessor is fatigued. The assessor starts each day alert and attentive, but tires as time passes. Judgments become more random as a result. This is also a time-based model. We again begin with a Beta prior. For this model, however, there is an assessor model with parameters ,  as well as separate priors for each judgment; we will denote the parameters i, i. The posterior for a given judgment will be (i +rqi)/(i +i +i), where rqi is the number of documents judged relevant to q up to judgment i. For i ,"" 0 we will set i, i to zero. The first judgment for a topic, then, will be the same. For each subsequent judgment, the Beta parameters will grow with assessor parameters , : after each judgment, i and i increase by  and  respectively. The effect is that after k judgments, the posterior probability of judging a document relevant will be (k + rqk)/(k + k + k). As k increases, the assessor converges to judging every document according to their prior probability /( + ).""",0,,False
122,"Our final model is Markovian, that is, the assessor's judgments are conditional on previous judgments. This could simulate an assessor who ""feels bad"" about judging too many nonrelevant documents in a row and thus takes a broader view of the topic over time, or one who takes a narrower view after judging many relevant documents in a row. The observation that assessors are more likely to make the same judgment twice in a row supports this model.",1,ad,True
123,3. ASSESSOR SIMULATION,0,,False
124,"To analyze the effects of particular types of systematic error, we simulated assessors judging documents in a TREClike setting: an assessor is given a topic description and reads documents to judge whether they are relevant to the topic. Since we are particularly interested in test collections with very many lightly-judged topics, we used the TREC 2009 Million Query track data as the starting point for our simulations. Before describing the simulation procedure and results, we briefly describe the track.",1,TREC,True
125,3.1 TREC Million Query Track,1,TREC,True
126,"The Million Query track was designed to study the use of low-cost evaluation methods in the TREC setting. It produces test collections that consist of a large number of lightly-judged topics. Judgments in the Million Query track are either not relevant, related (but not relevant), relevant, or highly relevant. In 2009, 638 topics received a total of 34,534 judgments (54 per topic on average), of which 26% were either relevant or highly relevant. There were 95 topics for which no relevant documents were found.",1,Query,True
127,"The low-cost methods used by the track attempt to target judgments that are going to be more useful in evaluation. The Million Query track uses two methods to select documents to judge. One (statAP) is an approach based on statistical sampling, in which each judged relevant documents is taken to be representative of some population of relevant documents in the same ""region"" from which it was sampled [2]. The other (MTC) is an algorithmic approach that weighs documents according to how informative a judgment to them is expected to be; after each judgment, it recomputes the weights given the new information and presents the top-weighted document for judging [6].",1,Query,True
128,Errors in judging can have unpredictable effects in both,0,,False
129,"methods. In statAP, an erroneous judgment of relevance can have a major impact on the estimated number of relevant documents for the topic, particularly if the judgment is to a document that has a low probability of being sampled. Conversely, an erroneous judgment of nonrelevance will cause the number of relevant documents to be underestimated. In MTC, an erroneous judgment can result in the algorithm taking an entirely different path.",1,AP,True
130,"The two different approaches to selecting judgments have different approaches to evaluation that are based on different assumptions. The sampling approach takes each judgment of relevance as representative of a set of relevant documents; this set is used to calculate an unbiased estimator of average precision. The algorithmic approach can take one of two tacks: it can either bound the differences in average precision between pairs of systems or it can compute a probability that the difference in average precision is less than zero over the space of possible judgments that could be made to unjudged documents. The probabilistic approach is generally more useful, and it has the advantage of being able to produce an estimate of average precision called ""expected average precision"" (EAP). Unlike the statMAP estimate, EAP is highly biased, but because it is meant for pairwise comparisons the bias can be expected to cancel out. Plots of EAP typically have very low values compared to plots of statAP.",1,ad,True
131,"We have decided to limit our focus to the effects on statAP. There are two reasons: first, because statAP estimates ""look like"" standard average precision, it is easy to see how errorful judgments are causing errors in the estimates. EAP estimates look very different from average precision, and because it is already very biased, changing the judgments will not necessarily cause obvious differences in their values. The second reason to prefer statAP is that MTC cannot effectively be simulated in the Million Query track data. If one judgment changes, it is likely that some future document selected for judging will be one that we do not already have a judgment on.",1,AP,True
132,3.1.1 statAP,1,AP,True
133,"Some understanding of statAP is necessary to understand how an errorful judgment affects the estimate. statAP is a method for sampling a set of documents S to be judged, then using those judgments to estimate average precision. The statAP estimate is calculated as:",1,AP,True
134,statAP,1,AP,True
135,",",0,,False
136,1 R,0,,False
137,dS,0,,False
138,xdprec@r(d) d,0,,False
139,"where xd is the relevance of document d (1 for relevant, 0 for not relevant), d is an inclusion probability calculated",0,,False
140,"for sampling, R is an estimate of the number of relevant",0,,False
141,"documents, and prec@r(d) is an estimate of the precision at the rank at which document d appears. The estimates of precision and the number of relevant documents are:",0,,False
142,"R,",0,,False
143,"xd ,",0,,False
144,dS d,0,,False
145,"prec@k , 1",0,,False
146,xd,0,,False
147,"k dS,r(d)k d",0,,False
148,"The ratio xd/d can be thought of as the number of relevant documents that xd is representative of in the same ""region"" of documents with similar inclusion probabilities. A lower d gives greater weight to a relevant document, increasing the estimated numbers of relevant documents compared to a relevant document with a higher d.",0,,False
149,542,0,,False
150,statMAP statMAP,1,MAP,True
151,0.3 true statMAP,1,MAP,True
152,simulated statMAP,1,MAP,True
153,0.25,0,,False
154,0.2,0,,False
155,0.15,0,,False
156,0.1,0,,False
157,0.05 0,0,,False
158,5,0,,False
159,10,0,,False
160,15,0,,False
161,20,0,,False
162,25,0,,False
163,30,0,,False
164,35,0,,False
165,System number (ordered by statMAP),1,MAP,True
166,"(a) Judgments from a random assessor give a small (but significant) rank correlation;  , 0.35.",0,,False
167,0.28,0,,False
168,true statMAP,1,MAP,True
169,0.26,0,,False
170,simulated statMAP,1,MAP,True
171,0.24,0,,False
172,0.22,0,,False
173,0.2,0,,False
174,0.18,0,,False
175,0.16,0,,False
176,0.14,0,,False
177,0.12,0,,False
178,0.1,0,,False
179,0.08,0,,False
180,0.06 0,0,,False
181,5,0,,False
182,10,0,,False
183,15,0,,False
184,20,0,,False
185,25,0,,False
186,30,0,,False
187,35,0,,False
188,System number (ordered by statMAP),1,MAP,True
189,"(b) Judgments from an unenthusiastic assessor give a small (but significant) rank correlation;  , 0.33.",0,,False
190,Figure 2: statMAP system scores after applying the two simplest assessor models.,1,MAP,True
191,3.2 Simulation Procedure,0,,False
192,"The simulation proceeds as follows: for a given topic, we start with the sequence of judgments in the same order they were originally made (this information is provided in the ""fullrels"" file distributed with the Million Query track data). We alter the judgment according to each of the models above. For those models that involve random sampling, we perform 25 trials on each judgment for each topic. We used increasing powers of two for parameter values, i.e.  and  ranged from 1 to 1024 independently. When complete, we have an errorful ""prels"" file that we can use to evaluate the Million Query track systems with statAP.",1,ad,True
193,"After re-evaluating Million Query track systems, the simplest approach to determining the effect of errors is to measure how well the new evaluation correlates to the ""true"" evaluation resulting from using the original relevance judgments. Kendall's  rank correlation is widely-used for this. Kendall's  is a function of the number of system pairs that swap between two rankings. The more swaps, the lower  is; when  , 1 the rankings are identical.",1,Query,True
194,3.3 Simulation Results,0,,False
195,"System evaluation results based on the judgments from the first two assessor models--random judging with prior parameters  ,"" 1,  "","" 8 and an unenthusiastic assessor that alternates between nonrelevant and relevant judgments to stay amused--are shown in Figure 2. In both cases the Kendall's  correlation to the official ranking is around 0.34, and remains consistently around 0.34 no matter what the prior parameters are and no matter what judging pattern is used (among those we tried). This can be thought of as a baseline for an assessor who is not actively malicious but is not interested in making an effort.""",0,,False
196,"Figure 3 illustrates altered system rankings based on the other models for selected parameter values. For models with random sampling, error bars indicate the distribution of MAP estimates observed over 25 trials.",1,MAP,True
197,"The optimistic and pessimistic models give very different results even when the prior parameter give equal probability of changing the judgment. Rank correlations based on optimistic judgments quickly degrade, while rank correlations",1,ad,True
198,"based on pessimistic judgments degrade much more slowly. Figures 3(a) and 3(b) demonstrate this for  ,"" 1,  "", 16 (for the optimist) and  ,"" 16,  "","" 1 (for the pessimist). Roughly the same number of judgments changed in both cases, but the effect on performance is much worse when those changes create more relevant documents than when they create more nonrelevant documents. With the pessimistic model, in fact, the correlation is nearly perfect; the scores have simply shifted downward.""",1,ad,True
199,"The disgruntled and lazy models are similar to the pessimistic model in that they result in fewer relevant documents than exist in the ""true"" judgments. However, they produce worse results in general. In the disgruntled case (Fig. 3(c), despite labeling roughly the same number of documents relevant as the pessimist, the  correlations are on average 10% lower. The lazy assessor (Fig. 3(d)) actually found many more relevant documents than either the pessimist or the disgruntled assessor with the same prior parameters, but apparently found ""worse"" relevant documents than the pessimist, as its  correlation is lower.",0,,False
200,"The fatigued and Markovian models are rather similar to each other in how they rerank systems. The Markovian model produces somewhat more pronounced effects on some of the systems, resulting in a lower  correlation. Both result in more documents being judged relevant.",0,,False
201,"One conclusion we draw from these results is that it is generally better to underestimate relevance than to overestimate it. The models that result in fewer documents being judged relevant--the pessimist, the disgruntled, and the lazy--generally produce more accurate rankings of systems than those that result in more documents being judged relevant. This suggests that low-cost evaluation methods are sensitive to noise in the relevant documents. Among models that result in fewer relevant documents, the pessimist produces the best rankings overall, though the system scores are strongly biased downward.",0,,False
202,"Of course, we do not conclude from this that assessors should be trained to be pessimists. This is an abstract model; the altered judgments had no relationship to any properties of the actual documents apart from their original relevance judgments.",1,ad,True
203,543,0,,False
204,0.28,0,,False
205,true statMAP,1,MAP,True
206,0.26,0,,False
207,simulated statMAP,1,MAP,True
208,0.24,0,,False
209,0.22,0,,False
210,0.2,0,,False
211,statMAP,1,MAP,True
212,0.18,0,,False
213,0.16,0,,False
214,0.14,0,,False
215,0.12,0,,False
216,0.1,0,,False
217,0.08,0,,False
218,0.06 0,0,,False
219,5,0,,False
220,10,0,,False
221,15,0,,False
222,20,0,,False
223,25,0,,False
224,30,0,,False
225,35,0,,False
226,System number (ordered by statMAP),1,MAP,True
227,"(a) Optimistic assessor ( ,"" 1,  "", 16) judges many more documents relevant.  , 0.72",0,,False
228,0.3 true statMAP,1,MAP,True
229,simulated statMAP,1,MAP,True
230,0.25,0,,False
231,0.2,0,,False
232,statMAP,1,MAP,True
233,0.15,0,,False
234,0.1,0,,False
235,0.05,0,,False
236,0,0,,False
237,0,0,,False
238,5,0,,False
239,10,0,,False
240,15,0,,False
241,20,0,,False
242,25,0,,False
243,30,0,,False
244,35,0,,False
245,System number (ordered by statMAP),1,MAP,True
246,"(b) Pessimistic assessor ( ,"" 16,  "", 1) judges many fewer documents relevant.  , 0.92",0,,False
247,0.3 true statMAP,1,MAP,True
248,simulated statMAP,1,MAP,True
249,0.25,0,,False
250,0.35 true statMAP,1,MAP,True
251,simulated statMAP,1,MAP,True
252,0.3,0,,False
253,statMAP,1,MAP,True
254,statMAP,1,MAP,True
255,0.25 0.2,0,,False
256,0.2,0,,False
257,0.15 0.15,0,,False
258,0.1,0,,False
259,0.1,0,,False
260,0.05 0,0,,False
261,5,0,,False
262,10,0,,False
263,15,0,,False
264,20,0,,False
265,25,0,,False
266,30,0,,False
267,35,0,,False
268,System number (ordered by statMAP),1,MAP,True
269,"(c) Disgruntled assessor ( ,"" 1,  "", 16) gives up early.  , 0.81",0,,False
270,0.05 0,0,,False
271,5,0,,False
272,10,0,,False
273,15,0,,False
274,20,0,,False
275,25,0,,False
276,30,0,,False
277,35,0,,False
278,System number (ordered by statMAP),1,MAP,True
279,"(d) Lazy assessor ( ,"" 1,  "", 16) assumes first few judgments indicate the rest.  , 0.9",0,,False
280,0.28,0,,False
281,true statMAP,1,MAP,True
282,0.26,0,,False
283,simulated statMAP,1,MAP,True
284,0.24,0,,False
285,0.22,0,,False
286,0.2,0,,False
287,statMAP,1,MAP,True
288,0.18,0,,False
289,0.16,0,,False
290,0.14,0,,False
291,0.12,0,,False
292,0.1,0,,False
293,0.08,0,,False
294,0.06 0,0,,False
295,5,0,,False
296,10,0,,False
297,15,0,,False
298,20,0,,False
299,25,0,,False
300,30,0,,False
301,35,0,,False
302,System number (ordered by statMAP),1,MAP,True
303,"(e) Fatigued assessor ( ,"" 0.05,  "", 1) becomes more random over time.  , 0.9",0,,False
304,0.28,0,,False
305,true statMAP,1,MAP,True
306,0.26,0,,False
307,simulated statMAP,1,MAP,True
308,0.24,0,,False
309,0.22,0,,False
310,0.2,0,,False
311,statMAP,1,MAP,True
312,0.18,0,,False
313,0.16,0,,False
314,0.14,0,,False
315,0.12,0,,False
316,0.1,0,,False
317,0.08,0,,False
318,0.06 0,0,,False
319,5,0,,False
320,10,0,,False
321,15,0,,False
322,20,0,,False
323,25,0,,False
324,30,0,,False
325,35,0,,False
326,System number (ordered by statMAP),1,MAP,True
327,"(f) Markov assessor ( ,"" 1,  "", 16) makes each judgment based on the previous one.  , 0.84",0,,False
328,"Figure 3: Comparison between ""true"" statMAP system scores calculated over all Million Query 2009 topics+judgments and statMAP scores after each assessor model is applied to all topics with the specified parameters. The new rankings are evaluated by Kendall's  rank correlation (averaged over 25 trials when appropriate).",1,MAP,True
329,544,0,,False
330,1000 800 600,0,,False
331, 1.0 0.9 0.8 0.7,0,,False
332,1000 800 600,0,,False
333, 1.0 0.9 0.8 0.7,0,,False
334,0.6,0,,False
335,0.6,0,,False
336,400,0,,False
337,400,0,,False
338,0.5,0,,False
339,0.5,0,,False
340,200,0,,False
341,200,0,,False
342,0.4,0,,False
343,0.4,0,,False
344,0.3,0,,False
345,200,0,,False
346,400,0,,False
347,600,0,,False
348,800,0,,False
349,1000,0,,False
350,0.3,0,,False
351,200,0,,False
352,400,0,,False
353,600,0,,False
354,800,0,,False
355,1000,0,,False
356,"Figure 4: Contour maps illustrating the change in  with prior parameters ,  in the optimistic model (left) and pessimistic model (right). Lighter areas indicate higher values of  .",0,,False
357,3.4 Worst Case Analysis,0,,False
358,"The results presented above represent relatively good parameter settings for each of the models. Depending on the prior parameters, the results can become quite bad. Figure 4 shows contour maps demonstrating the change in  with prior parameters  and  in the optimistic and pessimistic models. The optimist is best (indicated by lighter shading) when  is low and  is very high, which is when it is least likely to incorrectly judge a document relevant. Its performance quickly degrades from there. The pessimist is best when  is low and  is high, which is when it is least likely to incorrectly judge a document nonrelevant, but it maintains good performance until  is high and  is low. Note that the optimistic is much darker in much more of the space than the pessimistic, indicating substantially lower  correlations for any parameter settings.",1,ad,True
359,Other models are similar to these two. Those that produce more relevant documents than originally existed in the relevance judgments tend to exhibit a faster drop-off in performance when parameters move away from the low-probability regions. Those that produce fewer relevant documents than originally existed tend to exhibit a slower drop-off.,0,,False
360,4. ADJUSTING FOR ASSESSOR ERRORS,0,,False
361,"The effect of assessor errors is to add unplanned variance and bias into the evaluation. This increases the cost indirectly--though the judgments can be made for the same cost, the cost of the errors they introduce adds up. Thus it may be worth expending some extra cost to ensure that errors made by assessors cannot cause too much damage in the aggregate. Here we consider some simple approaches to adjust or correct their errors.",1,ad,True
362,"Since we are interested in cases where the assessors may be distributed around the world rather than present in person, and cases with many more assessors judging fewer topics each, we do not want to spend too much time on solutions that involve a great deal of interaction with the assessors.",0,,False
363,4.1 Multiple Judgments,0,,False
364,"One possible solution is to have some documents judged multiple times. The cost clearly depends in part on how many rejudgments are made and how documents are chosen for rejudging, but it also depends on how the extra judgments are incorporated into the evaluation. Some of the",1,ad,True
365,"differences observed in the extra judgments will be due to reasonable disagreements about relevance rather than errors. While such differences could possibly be resolved by adjudication, this essentially adds another assessor--one who must be able to make a decision based on conflicting evidence--to the process, and that carries significant cost.",1,ad,True
366,"One alternative is to use a simple process like majority vote. If rejudgments converge on a particular decision, it is more likely that the original judgment was in error. This requires more duplicated effort, though, especially since rejudgments themselves are not immune to error.",0,,False
367,"Along similar lines, since pessimistic models seem to hurt performance less, we could require a supermajority of positives to call a rejudged document relevant. Thus it would take two of two judgments, or two of three judgments, being relevant before we are confident in concluding that a document really is relevant. This would only apply in the cases we actually decide to have a document rejudged; because of that and the additional cost in duplicated effort, the choice of documents to have rejudged must be made very carefully.",1,ad,True
368,"We hypothesize that for statAP evaluation, documents with lower inclusion probabilities are better candidates for rejudgment. These documents, if erroneously judged relevant, can have a much greater effect on the evaluation than documents with higher inclusion probabilities. The simulations bear this out: those models that resulted in worse ranking performance had lower inclusion probabilities on average among the judgments that changed. For example, the average inclusion probability among documents that the optimist in Figure 3(a) judged relevant was 0.09, while the average inclusion probability among documents the pessimist in Figure 3(b) judged relevant was 0.12.",1,AP,True
369,"To test the effect of rejudging low probability documents, we ran a second simulation to rejudge a few documents with low inclusion probabilities from a prior simulation. In this case, the simulated assessor uses the same model as the original, but only judges documents with inclusion probability less than 0.01. The new judgments are then merged with the existing judgments using the supermajority approach: any document that has been judged relevant twice is considered relevant, while the rest are nonrelevant. Since 90% of the judgments have inclusion probabilities greater than 0.01, most will not change, and most of the judged relevant documents will stay relevant.",0,,False
370,545,0,,False
371,rank correlation,0,,False
372,1.05 1,0,,False
373,0.95 0.9,0,,False
374,0.85 0.8,0,,False
375,0.75 0.7,0,,False
376,0.65 0,0,,False
377,100,0,,False
378,200,0,,False
379,300,0,,False
380,400,0,,False
381,500,0,,False
382,600,0,,False
383,number of errorful queries,0,,False
384,Figure 5: Kendall's  decreases linearly (within the given error bars) as the number of errorful topics among in an evaluation increases. Average  does not fall below 0.9 until 232 of the original judged topics have been replaced with errorful versions.,0,,False
385,"The effect of this on the optimist is a small improvement in the  correlation from 0.72 to 0.75, which may not be worth the cost of the extra judgments. However it seems that this could potentially be a useful starting point for selecting documents for rejudging and incorporating the rejudgments into the evaluation.",1,corpora,True
386,4.2 Quality Assurance,0,,False
387,"Another approach to handling erroneous errors is to treat relevance judgments as a quality-assurance problem. Given an estimate of the permissible number of badly-judged topics, we can sample the topics that have been judged and check whether those seem to be errorful in order to estimate the total number of problem cases. If the number is above what is permissible, we can impose tighter controls for a brief time until judging seems to be going smoothly again.",1,ad,True
388,"We investigated the permissible number of bad topics by starting with the full evaluation over the original judgments and gradually replacing topics with their errorful doubles from the models above. The goal was to see how many ""bad"" topics we could inject into the evaluation before we reached a  correlation below 0.9, the threshold at which we might feel uncomfortable with the ranking.",1,ad,True
389,"The result is shown in Figure 5. The decrease in  is roughly linear in the number of errorful topics, but it does not drop below 0.9 on average until 232--over 40% of the total number of topics--have been replaced. This suggests that statAP is actually fairly robust to errors in judgments, at least in terms of its ability to rank systems. An evaluation could proceed for a fairly long time before tighter controls would need to be enforced.",1,AP,True
390,5. CONCLUSION,0,,False
391,"We argue that as test collection construction continues to take lower-cost routes away from well-trained, managed assessors to crowdsourcing or cheaper, faster assessors, the errors in evaluation estimates will have to be quantified and potentially adjusted for the errors that will almost certainly occur in judging. We presented eight models of possible errors and showed how each affects an estimate of average precision. We proposed two possible means to adjust for errors: 1) have certain documents selected for rejudging, then",1,ad,True
392,use a voting algorithm to combine the judgments; 2) estimate how many problem cases there seem to be to determine whether judging needs to be more strictly observed.,0,,False
393,"As a next step, we plan to undertake a true crowdsourcing experiment using Mechanical Turk to investigate the degree to which the behaviors we posit actually occur in that population and the effect resulting errors have on evaluation. Beyond that, future work must consider that these errors will seldom happen independently. Most evaluations will be affected by some mixture of errors, and the parameters of that mixture could have a substantial effect on both the evaluation and adjustments.",1,ad,True
394,6. REFERENCES,0,,False
395,"[1] James Allan, Javed A. Aslam, Ben Carterette, Virgil Pavlu, and Evangelos Kanoulas. Overview of the TREC 2008 million query track. In Proceedings of TREC, 2008.",1,TREC,True
396,"[2] Javed A. Aslam and Virgil Pavlu. A practical sampling strategy for efficient retrieval evaluat ion, technical report.",0,,False
397,"[3] Javed A. Aslam, Virgil Pavlu, and Emine Yilmaz. A statistical method for system evaluation using incomplete judgments. In Proceedings of SIGIR, pages 541­548, 2006.",0,,False
398,"[4] Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas, Arjen P. de Vries, and Emine Yilmaz. Relevance assessment: Are judges exchangeable and does it matter? In Proceedings of SIGIR, pages 667­674, 2008.",0,,False
399,"[5] Ben Carterette. Robust evaluation of information retrieval systems. In Proceedings of SIGIR, 2007.",1,Robust,True
400,"[6] Ben Carterette, James Allan, and Ramesh K. Sitaraman. Minimal test collections for retrieval evaluation. In Proceedings of SIGIR, pages 268­275, 2006.",0,,False
401,"[7] Ben Carterette, Virgil Pavlu, Evangelos Kanoulas, Javed A. Aslam, and James Allan. Evaluation over thousands of queries. In Proceedings of SIGIR, pages 651­658, 2008.",0,,False
402,"[8] Gordon V. Cormack, Christopher R. Palmer, and Charles L.A. Clarke. Efficient construction of large test collections. In Proceedings of SIGIR, pages 282­289, 1998.",0,,False
403,"[9] Donna Harman. Overview of the fourth Text REtrieval Conference. In Proceedings of the Fourth Text REtrieval Conference (TREC-4), pages 1­24, 1995. NIST Special Publication 500-236.",1,TREC,True
404,"[10] Kenneth A. Kinney, Scott Huffman, and Juting Zhai. How evaluator domain expertise affects search result relevance judgments. In Proceedings of CIKM, pages 591­598, 2008.",0,,False
405,"[11] Mark Sanderson and Justin Zobel. Information retrieval system evaluation: Effort, sensitivity, and reliability. In Proceedings of SIGIR, pages 186­193, 2005.",0,,False
406,"[12] Ian Soboroff, Charles Nicholas, and Patrick Cahan. Ranking Retrieval Systems without Relevance Judgments. In Proceedings of SIGIR, pages 66­73, 2001.",0,,False
407,"[13] Ellen Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In Proceedings of SIGIR, pages 315­323, 1998.",0,,False
408,"[14] Ellen M. Voorhees. The philosophy of information retrieval evaluation. In CLEF '01: Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems, pages 355­370, London, UK, 2002. Springer-Verlag.",1,CLEF,True
409,"[15] Ellen M. Voorhees and Donna K. Harman, editors. TREC: Experiment and Evaluation in Information Retrieval. MIT Press, 2005.",1,TREC,True
410,"[16] Emine Yilmaz and Javed Aslam. Estimating average precision with incomplete and imperfect relevance judgments. In Proceedings of CIKM, pages 102­111, 2006.",0,,False
411,"[17] Justin Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In Proceedings of SIGIR, pages 307­314, 1998.",0,,False
412,546,0,,False
413,,0,,False

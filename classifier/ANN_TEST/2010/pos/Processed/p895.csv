,sentence,label,data,regex
0,A Co-learning Framework for Learning User Search Intents from Rule-Generated Training Data,0,,False
1,Jun Yan1 Zeyu Zheng1,0,,False
2,"1Microsoft Research Asia Sigma Center, No.49, Zhichun Road Beijing, 100190, China",1,ad,True
3,"{junyan, v-zeyu, zhengc}@microsoft.com",0,,False
4,Li Jiang2 Yan Li2 Shuicheng Yan3 Zheng Chen1,0,,False
5,2Microsoft Corporation,0,,False
6,3Department of Electrical and,0,,False
7,One Microsoft Way,0,,False
8,Computer Engineering,0,,False
9,"Redmond, WA 98004",0,,False
10,National University of Singapore,0,,False
11,"{lij, roli}@microsoft.com",0,,False
12,"117576, Singapore",0,,False
13,eleyans@nus.edu.sg,0,,False
14,ABSTRACT,0,,False
15,"Learning to understand user search intents from their online behaviors is crucial for both Web search and online advertising. However, it is a challenging task to collect and label a sufficient amount of high quality training data for various user intents such as ""compare products"", ""plan a travel"", etc. Motivated by this bottleneck, we start with some user common sense, i.e. a set of rules, to generate training data for learning to predict user intents. The rule-generated training data are however hard to be used since these data are generally imperfect due to the serious data bias and possible data noises. In this paper, we introduce a Co-learning Framework (CLF) to tackle the problem of learning from biased and noisy rule-generated training data. CLF firstly generates multiple sets of possibly biased and noisy training data using different rules, and then trains the individual user search intent classifiers over different training datasets independently. The intermediate classifiers are then used to categorize the training data themselves as well as the unlabeled data. The confidently classified data by one classifier are added to other training datasets and the incorrectly classified ones are instead filtered out from the training datasets. The algorithmic performance of this iterative learning procedure is theoretically guaranteed.",1,ad,True
16,Categories and Subject Descriptors,0,,False
17,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process.,0,,False
18,"Though various popular machine learning techniques could be applied to learn the underlying search intents of the users, it is generally laborious or even impossible to collect sufficient and label high quality training data for such learning task [1]. Despite of the laborious human labeling efforts, many intuitive insights, which could be formulated as rules, can help generate small scale possibly biased and noisy training data. For example, to identify whether the users have intents to compare different products, several assumptions may help make the judgment. Generally, we may assume that if a user submits a query with explicit intent expression, such as ""Canon 5d compare with Nikon D300"", he/she may want to compare products. Though the rules satisfy the human common sense, there are two major limitations if we directly use them to infer ground truth. First, the coverage of each rule is often small and thus the training data may be seriously biased and insufficient. Second, the training data are usually not clean since no matter which rule we use, there may exist exceptions. In this paper, we propose a co-learning framework (CLF) for learning user intent from the rule-generated training data, which are possibly biased and noisy. The problem is,",0,,False
19,"Without laborious human labeling work, is it possible to train",0,,False
20,"user search intent classifier using the rule-generated training data,",0,,False
21,which are generally noisy and biased? Given sets of rule-,0,,False
22,"generated training datasets , 1,2, ... , how to train the",0,,False
23,classifier :,0,,False
24,on top of these biased and noisy training data,0,,False
25,sets with good performance?,0,,False
26,General Terms,0,,False
27,"Algorithms, Experimentation",0,,False
28,Keywords,0,,False
29,"User intent, search engine, classification.",0,,False
30,1. INTRODUCTION,1,DUC,True
31,"The classical relevance based search strategies may fail in satisfying the end users due to the lack of consideration on the real search intents of users. For example, when different users search with the same query ""Canon 5D"" under different contexts, they may have distinct intents such as to buy Canon 5D, to repair Canon 5D, etc. The search results about Canon 5D repairing obviously cannot satisfy the users who want to buy a Canon 5D camera. Learning to understand the true user intents behind the users' search queries is becoming a crucial problem for both Web search and behavior targeted online advertising.",1,ad,True
32,"Copyright is held by the author/owner(s). SIGIR'10, July 19-23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",0,,False
33,2. THE CO-LEARNING FRAMEWORK,0,,False
34,"Suppose we have sets of rule-generated training data ,",0,,False
35,"1,2, ... , which are possibly noisy and biased, and a set of",0,,False
36,unlabeled user behavioral data . Each data sample in the,0,,False
37,"training datasets is represented by a triple , ,",0,,False
38,"1,",0,,False
39,"1,2, ... | |, where stands for the feature vector of the data",0,,False
40,"sample in the training data , is its class label and | | is the",0,,False
41,"total number of training data in . On the other hand, each",0,,False
42,"unlabeled data sample, i.e. the user search session that could not",0,,False
43,"be covered by our rules, is represented as , ,",0,,False
44,"0,",0,,False
45,"1,2, ... | |. Suppose for any",0,,False
46,", all the features constituting",0,,False
47,the feature space are represented as a set,0,,False
48,"| 1,2, ... .",0,,False
49,"Suppose among all the features F, some have direct correlation to",0,,False
50,"the rules, that is they are used to generate the training dataset .",0,,False
51,These features are denoted by,0,,False
52,", which constitute a subset",0,,False
53,"of F. Let ,",0,,False
54,be the subset of features having no direct,0,,False
55,correlation to the rules used for generating training dataset .,0,,False
56,Given a classifier :,0,,False
57,", where",0,,False
58,"is any subset of F, we",0,,False
59,use to represent an untrained classifier and use to represent,0,,False
60,the classifier trained by the training data . Suppose,0,,False
61,|,0,,False
62,895,0,,False
63,means to train the classifier by training dataset using the,0,,False
64,features,0,,False
65,", we have",0,,False
66,"trained classifier , let",0,,False
67,"| , 1,2, ... . For the | stands for classifying",0,,False
68,using features F. We assume for each output result of trained,0,,False
69,"classifier , it can output a confidence score. Let",0,,False
70,|,0,,False
71,",",0,,False
72,where is the class label of assigned by and the is the corresponding confidence score.,0,,False
73,"After generating a set of training data , 1,2, ... , based on rules, we first train the classifier by , 1,2, ... ,",0,,False
74,"independently. Then we can get a set of K classifiers,",0,,False
75,"| , 1,2, ... .",0,,False
76,Note that the reason why we use to train classifier on top of,0,,False
77,instead of using the full set of features F is that is generated,1,ad,True
78,"from some rules correlated to , which may overfit the classifier",0,,False
79,if we do not exclude them. After each classifier is trained,0,,False
80,"by , we use to classify the training dataset itself. A basic assumption of CLF is that the confidently classified instances by",0,,False
81,"classifier , 1,2, ... , have high probability to be correctly",0,,False
82,"classified. Based on this assumption, for any",0,,False
83,", if the",0,,False
84,"confidence score of the classification is larger than a threshold, i.e. > and the class label assigned by the classifier is different",0,,False
85,"from the class label assigned by the rule, i.e.",0,,False
86,", then",0,,False
87,is considered as noise in the training data . Note that here,0,,False
88,"is the label of assigned by classifier, is its observed",0,,False
89,"class label in training data, and is the true class label, which is",0,,False
90,not observed. We exclude it from and put it into the unlabeled dataset . Thus we update the training data by,0,,False
91,",",0,,False
92,.,0,,False
93,"Then we use the classifier ,",0,,False
94,"1,2, ... , to classify the",0,,False
95,unlabeled data independently. Based on the same assumption,0,,False
96,"that the confidently classified instances by classifier have high probability to be correctly classified, for any data belonging to ,",0,,False
97,if the confidence score of the classification is larger than a,0,,False
98,"threshold, i.e. > , where",0,,False
99,|,0,,False
100,", we",0,,False
101,"include into the training dataset. In other words,",0,,False
102,",",0,,False
103,", 1,2 ... , .",0,,False
104,"Through this way, we can gradually reduce the bias of the rulegenerated training data.",1,ad,True
105,"On the other hand, some unlabeled data are added into the training",1,ad,True
106,"datasets. Suppose the ,",0,,False
107,1| is the probability of a,0,,False
108,data sample to be involved in the training data at the iteration n,0,,False
109,conditioned on this data sample is represented as a feature vector,0,,False
110,and,0,,False
111,1 is the probability of any data sample in D is,0,,False
112,considered as a training data sample. It can be proved that after n,0,,False
113,"iterations using CLF, for each training dataset, we have",0,,False
114,",",0,,False
115,1|,0,,False
116,1.,0,,False
117,The remaining questions are when to stop the iteration and how to,0,,False
118,"train the classifier after iteration stops. In this work, we define the",0,,False
119,"iteration stopping criteria as ""if |{ |",0,,False
120,",",0,,False
121,}| < n,0,,False
122,"or the number of iterations reaches N, then we stop the iteration"".",0,,False
123,"After the iterations stop, we obtain K updated training datasets",0,,False
124,"with both noise and bias reduced. Finally, we merge all these",0,,False
125,training datasets into one. Thus we can train the final classifier as,0,,False
126,.,0,,False
127,3. EXPERIMENTS,0,,False
128,"In this short paper, we utilize the real user search behavioral dataset, which comes from the search click-through log of a commonly used commercial search engine. It contains 3,420 user search sessions, in each of which, the user queries and clicked Web pages are all logged. Six labelers are asked to label the user intents according to the user behaviors as ground truth for results validation. We name this dataset as the ""Real User Behavioral Data"". The n-gram features are used for classification in the Bag of Words (BOW) model. One of the most classical evaluation metrics for classification problems, F1, which is a tradeoff between Precision (Pre) and recall (Rec) is used as the evaluation metric. For comparison purpose, we utilize several baselines to show the effectiveness of the proposed CLF. Firstly, since we can use different rules to initialize several sets of training data, directly utilizing one training dataset or the combination of all rule-generated training datasets to train the same classification model can give us a set of classifiers. Among them, we take the classifier with the best performance as the first baseline, referred to as ""Baseline"" in the remaining parts of this section. The second baseline is the DL-CoTrain algorithm, which is a variant of cotraining algorithm. It also starts from the rule-generated training data for classification and thus has the same experiments configuration as CLF. The classification method selected in CLF is the classical Support Vector Machine (SVM). In Table 3, we show the experimental results of CLF after 25 rounds of iterations compared with the baseline algorithms. From the results we can see that, in terms of F1, the CLF can improve the classification performance as high as 47% compared with the baseline.",1,ad,True
129,Table 3. Results of CLF after 25 iterations,0,,False
130,Baseline,0,,False
131,DL-CoTrain,0,,False
132,Pre,0,,False
133,0.78,0,,False
134,0.78,0,,False
135,Rec,0,,False
136,0.24,0,,False
137,0.12,0,,False
138,F1,0,,False
139,0.36,0,,False
140,0.21,0,,False
141,CLF 0.81 0.39 0.53,0,,False
142,4. CONCLUSION,0,,False
143,"One bottleneck of user search intent learning for Web search and online advertising is the laborious training data collection. In this paper, we proposed a co-learning framework (CLF), which aims to classify the users' search intents without laborious human labeling efforts. We firstly utilize a set of rules coming from the common sense of humans to automatically generate some initial training datasets. Since the rule-generated training data are generally noisy and biased, we propose to iteratively reduce the bias of the training data and control the noises in the training data. Experimental results on both real user search click data and public dataset show the good performance of the co-learning framework.",1,ad,True
144,5. REFERENCE,0,,False
145,"[1] Russell, D.M., Tang, D., Kellar, M. and Jeffries, R. 2009. Task behaviors during web search: the difficulty of assigning labels. Proceedings of the 42nd Hawaii International Conference on System Sciences (Hawaii, United States, January 05 - 08, 2009). HICSS '09. IEEE Press, 1-5. DOI, 10.1109/HICSS.2009.417.",0,,False
146,896,0,,False
147,,0,,False

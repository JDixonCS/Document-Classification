,sentence,label,data,regex
0,On Statistical Analysis and Optimization of Information Retrieval Effectiveness Metrics,0,,False
1,Jun Wang and Jianhan Zhu,0,,False
2,"Department of Computer Science, University College London, UK",0,,False
3,"wang.jun@acm.org, j.zhu@cs.ucl.ac.uk",0,,False
4,ABSTRACT,0,,False
5,"This paper presents a new way of thinking for IR metric optimization. It is argued that the optimal ranking problem should be factorized into two distinct yet interrelated stages: the relevance prediction stage and ranking decision stage. During retrieval the relevance of documents is not known a priori, and the joint probability of relevance is used to measure the uncertainty of documents' relevance in the collection as a whole. The resulting optimization objective function in the latter stage is, thus, the expected value of the IR metric with respect to this probability measure of relevance. Through statistically analyzing the expected values of IR metrics under such uncertainty, we discover and explain some interesting properties of IR metrics that have not been known before. Our analysis and optimization framework do not assume a particular (relevance) retrieval model and metric, making it applicable to many existing IR models and metrics. The experiments on one of resulting applications have demonstrated its significance in adapting to various IR metrics.",1,ad,True
6,Categories and Subject Descriptors,0,,False
7,H.3 [Information Storage and Retrieval]: H3.1Content analysis and Indexing; H.3.3 Information Search and Retrieval,0,,False
8,General Terms,0,,False
9,"Algorithms, Experimentation, Measurement, Performance",0,,False
10,1. INTRODUCTION,1,DUC,True
11,"In Information Retrieval Modelling, the main efforts have been devoted to, for a specific information need (query), automatically scoring individual documents with respect to their relevance states. Representative examples include the Probabilistic Indexing model that studies how likely a query term is assigned to a relevant document [17], the RSJ model that derives a scoring function on the basis of the log-ratio of probability of relevance [20], to name just a few. And yet, given the fact that in many practical situations relevance information is not steadily available, major developments have shifted their focus to estimating text statistics in the documents and queries and then building up the link through these statistics[12, 21, 34]. For example, scoring functions",1,ad,True
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
13,"such as TF·IDF, Vector Space Model, and the Divergence from Randomness (DFR) model [1] have been developed [16]. A practical approximation of the RSJ model led to the popular BM25 scoring function [21]. Another direction in probabilistic modelling was to build a ""language model"" of a document and assess its likelihood of generating a given query [34]; a query language model is also covered under the Kullback-Leibler divergence based loss function [15].",0,,False
14,"Despite the efforts for retrieval, when in the evaluation phase, many IR tasks have evaluation criteria that go beyond simply counting the number of relevant documents in a ranked list. Measuring IR effectiveness by different metrics is critical because, for different retrieval goals, we need to capture different aspects of retrieval performance. In the case where the preference goes strongly towards earlyretrieved documents, MRR (Mean Reciprocal Rank) is a good measure [28], whereas if we try to capture a broader summary of retrieval performance, MAP (Mean Average Precision) becomes suitable [13]. Thus, there is a gap between the underlying (ranking) decision process of retrieval models and the final evaluation criterion used to measure success in a task. Ideally, it is desirable to have retrieval systems adapted to the specific IR effectiveness metrics.",1,ad,True
15,"In fact, IR researchers have already started to explore the opportunity. One extreme case is learning to rank ; it directly constructs a document ranking model from training data, bypassing the step of estimating the relevance states of individual documents [8]. Under this paradigm, some attempts have been made to directly optimizing IR metrics such as NDCG (Normalized Discounted Cumulated Gain) and MAP [23, 33]. However, it is known that some evaluation metrics are less informative than others [4]. As argued in [32], some IR metrics thus do not necessarily summarize the (training) data well; if we begin optimizing IR metrics right from the data, the statistics of the data may not be fully explored and utilized.",1,ad,True
16,"A somewhat opposite direction is to focus still on designing a scoring function of a document, but with the acknowledgement of various retrieval goals and the final rank context. The ""less is more"" model proposed in [10] is one of the examples. By treating the previously retrieved documents as non-relevant when calculating the relevance of documents for the current rank position, the algorithm is shown to be equivalent to maximizing the Reciprocal Rank measure. In [35], a more general and flexible treatment in this direction is proposed. In the framework, Bayesian decision theory is applied to incorporate various ranking strategies through predefined loss functions. Despite its generality, the resulting IR models, however, lack the ability of directly incorporating IR metrics into the rank decision.",1,corpora,True
17,"In this paper, we argue that regarding the retrieval task solely as either optimizing IR metrics or deriving a (rele-",0,,False
18,226,0,,False
19,Figure 1: The two distinct stages in the statistical document ranking process.,0,,False
20,"vance) scoring function presents a partial view of the underlying problem; a more unified view is to divide the retrieval process into two distinct stages, namely relevance prediction and ranking decision optimization stages, and solve them sequentially. In the first stage, the aim is to estimate the relevance of documents as accurate as possible, and summarize it by the joint probability of documents' relevance. Only in the second stage is the rank preference specified, possibly by an IR metric. The rank decision making is a stochastic one due to the uncertainty about the relevance. As a result, the optimal ranking action is the one that maximizes the expected value of the IR metric. We shall show that statistical analysis of the expected value of IR metrics gives insight into the properties of the metrics. One of the findings is that AP (Average Precision) encourages documents whose relevance is positively correlated with previous retrieved documents, while RR (Reciprocal Rank) does otherwise. It follows that if a rank achieves superior results on AP, it must pay with inferiority on RR. Apart from a theoretical contribution, our experiments on TREC data sets demonstrate the significance of our probabilistic framework.",1,AP,True
21,"The remainder of the paper is organized as follows. We first establish our optimization scheme, and study major expected IR metrics and practical issues. We then provide an empirical evaluation, and finally conclude our work.",0,,False
22,2. STATISTICAL RANKING MECHANICS,0,,False
23,"In this section, we present the framework of optimizing IR metrics in the situation where the relevance of documents is unknown. To keep our discussion simple, we consider binary relevance, while graded relevance can be extended similarly. Given an information need, let us assume each document in the corpus is either relevant or non-relevant. We denote them jointly as a vector r  (r1, ..., rk, ..., rN )  {0, 1}N , where k ,"" {1, ..., N }, N denotes the number of documents. rk "", 1 if document k is relevant; otherwise rk,0.",1,ad,True
24,"Our view is the following: firstly the IR model should focus on estimating the relevance of documents. The relevance in this stage is the ""true"" topical relevance [18], different from the user ""perceived"" relevance that will be qualified in the next stage. In statistical modelling, we assign to every possible relevance state r a number p(r|q), which we interpret as the probability that a user, who issues query q, will find the documents' relevance states as r. Given the observation so far (the query, the user's interaction etc), the posterior probability p(r|q) presents our (or the IR model's) belief about the relevance states of the documents in the collection as a whole. Note that we use the joint distribution of relevance instead of the marginal distribution p(rk|q) to cover the dependency of relevance among documents.",1,ad,True
25,"It is argued that only in the second stage does the retrieval model make a ranking decision under the uncertainty specified by the joint probability of relevance. To formulate this, we follow the terminology in natural language processing [6]; a ranking order is represented by a vector a  (a1, ..., ai, ..., aN ), where ai  {1, ..., N }. If a document k is in rank position i, then ai ,"" k. The retrieval task is, thus, to find an optimal rank order a to maximize a certain retrieval objective. Formally, an IR metric (measure) m(a|r) is defined as a score function of a given r. A good metric should be able to measure the user's gain or utility of a rank order a when the true relevance states of all the documents, r, are known. m(a|r) can be also seen as a measure""",0,,False
26,"of the user's perceived relevance in the context of a ranked list. For example, Precision concerns a solution that finds relevant documents as many as possible in the list regardless of their order, while Reciprocal Rank (inverse of the rank of the first relevant document retrieved) makes sure to retrieve the first relevant document as early as possible regardless of the rank positions of remaining relevant documents.",0,,False
27,"Given the fact that different IR effectiveness metrics are useful for capturing different aspects of retrieval quality, it is desirable to optimize a with respect to the specific metric m. Bayesian decision theory suggests that the optimal rank order ^a is obtained by maximizing the expected IR metric:",0,,False
28,X,0,,False
29,"^a , argmax Er[m|q] , argmax",0,,False
30,a,0,,False
31,a,0,,False
32,"m(a|r)p(r|q), (1)",0,,False
33,"r{0,1}N",0,,False
34,"where E[·|q] denotes an expectation with respect to a conditional distribution p(·|q). The subscript r indicates it is averaged over all possible r. Eq. (1) shows that: firstly the true relevance state of documents, r, is generated from probability p(r|q) estimated by an IR model. Under the relevance state r, the score of a given rank order a is calculated. Er[m|q], the expected score of the rank order, is the one averaging over all possible relevance states of r. Finally, the optimal rank order is chosen by maximizing Er[m|q].",0,,False
35,"Although the formulation can be thought of as a special instantiation of the general retrieval decision framework in [15, 35], our underlying idea and development are quite different from their instantiated models. The advantage is that, as illustrated in Figure 1, in our framework, the IR metric (utility) relies only on the true relevance and ranking order, while (relevance) IR models are for estimating the relevance. Decoupling them is essential to directly use any retrieval metric and plug it into the optimization procedure. More discussion can be found in Section 4.",1,ad,True
36,"To obtain Eq. (1), we analyze the expected IR metrics Er[m|q] in Section 2.1 and present a practical implementation and maximization (search) method in Section 2.2.",0,,False
37,2.1 Analysis of Expected IR metrics,0,,False
38,2.1.1 Expected Average Precision,0,,False
39,"Average Precision (AP) is a widely-adopted metric. For each query, it is the average of the precision scores obtained across rank positions where each relevant document is retrieved; relevant documents that are not retrieved receive a precision score of zero [7]. The metric, in fact, is the area under the Precision-Recall curve, capturing a broad summary of retrieval performance with a single value [4].",1,AP,True
40,"By definition, the Average Precision measure is as follows:",0,,False
41,mA(a|r),0,,False
42,1 NR,0,,False
43,M,0,,False
44,X rai,0,,False
45,"i,1",0,,False
46,(1,0,,False
47,+,0,,False
48,Pi-1,0,,False
49,"j,1",0,,False
50,i,0,,False
51,"raj ) ,",0,,False
52,(2),0,,False
53,"where M  N (Pij-,11 raj  0 when i,1). NR is the num-",0,,False
54,"ber of relevant documents, and its expected value equals",0,,False
55,PN,0,,False
56,"i,1",0,,False
57,p(rai,0,,False
58,",",0,,False
59,"1),",0,,False
60,the,0,,False
61,summation,0,,False
62,of,0,,False
63,the,0,,False
64,marginal,0,,False
65,probability,0,,False
66,"of relevance. For simplicity, we define p(rai , 1)  p(Rai )",0,,False
67,in the remainder of the paper. Because during retrieval r,0,,False
68,"is hidden, mA(a|r) cannot be calculated exactly. Instead,",1,ad,True
69,its expected value under the joint probability of relevance,0,,False
70,is derived by making use of the properties of expectation,0,,False
71,(Throughout this paper the expectation is all conditioned,0,,False
72,"on a given query q and with respect to r. For simplicity, we",0,,False
73,drop the subscript r and notation q in E[·] from now on):,0,,False
74,"E[mA] , X p(NR|q)E[mA|NR]",0,,False
75,(3),0,,False
76,NR,0,,False
77,227,0,,False
78,Weight Ratio Weight Ratio Weight Ratio,0,,False
79,1,0,,False
80,0.95,0,,False
81,0.9 1,0,,False
82,0.85 0.8,0,,False
83,0.8 0.6,0,,False
84,0.75 0.4,0,,False
85,0.7 0.2,0,,False
86,0.65,0,,False
87,0,0,,False
88,1 0.8,0,,False
89,0.6 10,0,,False
90,0.6 0.4,0,,False
91,8 0.55,0,,False
92,6,0,,False
93,0.2,0,,False
94,4,0,,False
95,p(r),0,,False
96,0,0,,False
97,2,0,,False
98,i,0,,False
99,0.5,0,,False
100,(a),0,,False
101,0.9,0,,False
102,0.8,0,,False
103,1,0,,False
104,0.7,0,,False
105,0.8,0,,False
106,0.6,0,,False
107,0.6,0,,False
108,0.5,0,,False
109,0.4 0.4,0,,False
110,0.2 0.3,0,,False
111,0,0,,False
112,1,0,,False
113,0.2,0,,False
114,0.8,0,,False
115,10,0,,False
116,0.6 0.4,0,,False
117,8,0,,False
118,0.1,0,,False
119,6,0,,False
120,0.2,0,,False
121,4,0,,False
122,p(r),0,,False
123,0,0,,False
124,2,0,,False
125,i,0,,False
126,0,0,,False
127,(b),0,,False
128,1,0,,False
129,0.9,0,,False
130,0.8,0,,False
131,0.7,0,,False
132,0.6,0,,False
133,0.5,0,,False
134,0.4,0,,False
135,"Expected AP: P(r) ,0.9 0.3",1,AP,True
136,"Expected AP: P(r) ,0.5",1,AP,True
137,Expected DCG 0.2,0,,False
138,"Expected RR: P(r) ,0.5",0,,False
139,"Expected RR: P(r) ,0.9 0.1",0,,False
140,0,0,,False
141,1,0,,False
142,2,0,,False
143,3,0,,False
144,4,0,,False
145,5,0,,False
146,6,0,,False
147,7,0,,False
148,8,0,,False
149,9,0,,False
150,10,0,,False
151,i,0,,False
152,(c),0,,False
153,"Figure 2: (a) The adaptive weight wiA of the expected Average Precision, (b) The adaptive weight wiR of the expected Reciprocal Rank, and (c) Comparison of the weights in different expected IR metrics.",1,ad,True
154,",",0,,False
155,X,0,,False
156,NR,0,,False
157,p(NR,0,,False
158,|q,0,,False
159," )

1 NR

M
X(

E

[rai |NR i

]

i=1

+

i-1
X
j=1

E

[rai

raj i

|NR

]

 )",0,,False
160,",",0,,False
161,X,0,,False
162,NR,0,,False
163,p(NR,0,,False
164,|q,0,,False
165," )

1 NR

M
X(

E

[rai |NR i

]

+

i=1

i-1
X

C ov (rai

,

raj

|NR )

+

E [rai i

|NR ]E [raj

|NR ]

 ),",0,,False
166,"j,1",0,,False
167,"where Cov(rai , raj |NR) denotes the correlation between the relevance values of documents at rank i and j given the number of relevant documents is NR. Eq. (3) shows that the expected AP can be interpreted as: for the given query, an IR model first estimates the number of relevant documents in the collection, and then estimates the expected AP for that number of relevant documents. The final expected measure is the average, weighted by p(NR|q), across all the possible numbers of relevant documents.",1,AP,True
168,"We can obtain more insight into the expected AP by making a simple approximation to the average over NR. By assuming that the posterior distribution of NR is sharply peaked around the most probable value (the mode) N^R, we can use the mode to approximate the average [5]. This gives:",1,AP,True
169,ER[mA],0,,False
170,1 N^R,0,,False
171,M,0,,False
172,"X ""wiAp(Rai )",0,,False
173,"i,1",0,,False
174,+,0,,False
175,i-1,0,,False
176,X,0,,False
177,"j,1",0,,False
178,C,0,,False
179,ov(rai i,0,,False
180,",",0,,False
181,raj,0,,False
182,),0,,False
183," ,

(4)

where

E[rai ]

=

P
rai

rai p(rai )

=

p(Rai ),

the

marginal

prob-

ability of a document's relevance at rank i. Note that the

equation removes the dependency of N^R because the con-

ditional expectation and variance are well approximated by

the non-conditional ones when P (N^R|q)  1. To simplify

the

equation,

we

also

define

wiA



, 1+Pij-=11 p(Raj )
i

which

is

regarded as an adaptive weight of rank i.

The first term in this simple approximation indicates that

the expected AP is a weighted average of the scores across

all rank positions, and as we increase the marginal probabil-

ity of relevance p(Rai) in the ranked list, the expected AP increases. Furthermore, because the weight ratio:

wia+1 wia

=

i

i +

1

(1

+

1

+

p(Rai )

Pi-1
j=1

p(Raj

)

)

(5)

is

in

the

range

between

i 1+i

and

2i 1+i

.The

ratio

is

adaptive

to

the

expected

relevance

(defined

as

Pi-1
j=1

p(Raj

))

received

so far. To get the insight into it, we approximate the weight

by setting p(Rai ) all equal to p(r). We plot the weight ratio against the marginal distribution p(r) and rank position

i in Figure 2 (a). It illustrates that when we have more

confidence about the relevance of the early retrieved docu-

ments (p(r) approaches one), the weight ratio becomes near

one. As a result, the metric is less worried about the early

retrieved documents, thus putting equal weights to the later-

retrieved documents. This is similar to the Precision metric.

But once less confident documents (p(r) approaches zero)

are retrieved, particularly in the top ranked positions, the

weight

ratio

approaches

its

lower

bound

i i+1

.

As a conse-

quence, the weight penalizes more the later-retrieved rele-

vant documents, and the ratio of the expected AP behaves

more like that of the expected DCG, which will be discussed

later.

The second term in Eq. (4) indicates that a document

will contribute more to the expected AP if its relevance is

more positively correlated with those of previous retrieved

documents. The consequence is that it will push positively

correlated documents up in the ranked list. This is an in-

teresting finding because it shows that the expected AP is

in fact nonlinear ­ it models well the dependencies between

documents' relevance and incorporates them in deciding the

preferred rank order. The rational of encouraging positively

correlated relevant documents is that if a document is rele-

vant, it is likely that its positively correlated documents are

also relevant. It theoretically explains why pseudo relevance

feedback, i.e., the top ranked documents are generally likely

to be relevant, and finding other documents similar to these

top ranked ones helps improve MAP [24].

2.1.2 Expected DCG and Precision
Discounted Cumulative Gain (DCG) is another popular measure for ranking effectiveness, especially in web search. DCG measures the usefulness, or gain, of a document based on its (graded) relevance[14] (for the moment, let us consider rai to cover the graded relevance too); the gain is accumulated from the top of the result list to the bottom. To penalize late-retrieved relevant documents, the gain of each result is discounted by a function of its rank position. By definition, we have the DCG measure as:

M

mD(a|r) = X wiDg(rai ),

(6)

i=1

where wiD is the discount weight for rank position i, and g(rai) is a gain function mapping the relevance value to the retrieval gain. Unlike the expected AP, the expect DCG is
linear with respect to rank positions. We thus have:

M

Er[mD] = X wiDE[g(rai )]

(7)

i=1

Since g(rai) is infinitely differentiable in the neighborhood

228

of the mean of rai , i.e., r^ai  E[rai], the mean of g(rai) can be represented by a Taylor power series as:

E[g(rai )] =E[g(r^ai )] + E[(rai - r^ai )g(r^ai )]+

E

[

1 2

(rai

-

r^ai )2g(r^ai )]

+

...

=g(r^ai )

+

0

+

1 2

V

ar(rai )g(r^ai )

+

...

(8)

g(r^ai

)

+

V

ar(rai

)

g(r^ai 2

)

,

The expected DCG is thus approximated by:

Er [mD ]



M

X

wiD

 g",1,ad,True
184,`r^ai,0,,False
185,´,0,,False
186,+,0,,False
187,1 2,0,,False
188,g,0,,False
189,(r^ai,0,,False
190,)V,0,,False
191," AR(rai) ,

(9)

i=1

where V AR(rai) denotes the variance of rai . Eq. (9) shows that the expected value of DCG is determined by both the

mean and variance of the relevance of documents at rank

positions from 1 to M . Whether it should add variance or

minus variance depends on the sign of the second deriva-

tive of the gain function. In the case of graded relevance,

if consider highly relevant documents more valuable than

marginally relevant documents and give them more gain, we

can then use a gain function like g(rai) = 2rai - 1. In this case, we need to add variance.

It is shown ment with the

that when w1D highest score of

> w2D...

g(r^ai

)

+

1 2

g>(r^waiM D)V,

the docuAR(rai ) is

retrieved first, the document with the next highest score

is retrieved second, and so on. It is common to define

wiD



log2

1 (i+1)

.

Compared to the adaptive weight in the

expected AP, it penalizes more the late-retrieved relevant

documents. Figure 2 (c) compares their weight ratios.

Precision at M is a special case of DCG, where the

discount is a constant and the gain function is linear. Thus,

the expected Precision measure is

E[mP ]

=

1 M

M
X E(rai )



1 M

M
X p(Rai )

i=1

i=1

(10)

2.1.3 Expected Reciprocal Rank

In the cases like web search and question answering tasks, we quite often expect a relevant document to be retrieved as early as possible [10, 28]. Expected Search Length and Reciprocal Rank (RR) are strongly biased towards earlyretrieved documents. This section analyzes RR, while Expected Search Length can be derived similarly. RR is the inverse of the rank of the first relevant document and bounded between 0 and 1. It is formally defined as:

mR(a|r)

=ra1

1 1

+

ra2 (1

-

ra1

)

1 2

+

ra3 (1

-

ra1 )(1

-

ra2

)

1 3

+

...

(11)

N
X =

rai i

i-1
Y (1 - raj )

=

N
X

1 i

vi

rai

,

i=1

j=1

i=1

where

we

define

vi



Qi-1
j=1

(1

-

raj ),

a

function

of

the

rel-

evance values of documents ranked above i; (vi  1 when

i = 1). Conceptually, RR measure can be thought of as a

weighted average of relevance values at different rank po-

sitions, where the weights are adaptive to earlier retrieved

documents.

The expected value of the RR measure is the following:

E[mR]

M
=E[X

1 i

virai ]

=

M
X

E[virai ] i

i=1

i=1

=

M
X

E[vi]E[rai ]

+

Cov(rai ,

vi )

i

i=1

M
= X `wiRp(Rai )

+

1 i

C

ov(rai

,

vi

)´,

i=1

(12)

where,

similarly,

we

consider

E[vi] i

as

an

adaptive

weight

and

denote it as wiR. It can be approximated by assuming that

the irrelevance of documents above rank i is independent

when

calculating

wiR,

i.e.,

wiR



E[vi] i



1 i

Qi-1
j=1

(1

-

p(Raj

)).

Thus wiR > wiR+1. On the one hand, similar to the expected DCG, the weight wiR is a discount factor penalizing late retrieved relevant documents. As a result, maximiz-

ing the measure intends to push documents that have high

marginal distribution of relevance p(rj) to the top. However,

the penalty is much larger than the ones in expected DCG

and expected AP. To see this, let us again approximate the

weight by setting p(Rai)  p(r). The weight ratio is compared with those of the expected AP and expected DCG in

Figure 2 (c). It shows that expected RR has the smallest

weight ratio, while expected AP has the largest. Expected

DCG is the one in the middle.

One the other hand, the weight is updated in a completely

different way compared to expected AP. Figure 2 (b) plots

the weight ratio against the marginal distribution p(r) and

rank position i. Different from expected AP, the weight

ratio of expected RR becomes larger when p(r) is larger, re-

inforcing the discount further. As a consequence, it entirely

focuses on the quality of a few early retrieval documents.

For

example,

the

upper

bound

for

w3R

is

1 12

.

If

we

consider

p(Rai ) > 0.5 for i = {1, 2, 3}, while for DCG it usually

equals

1 log2 4

=

1 2

and

for

expected

AP

even

larger.

The covariance bit in Eq. (12) shows that overall the ex-

pected value of RR increases when relevance of a document

is more positively correlated with vi, the product of non-

relevancies (1 - raj ) of the documents above. The effect is that negatively correlated documents will have higher ex-

pected RR than positively correlated documents. Such effect

will be discounted by a factor 1/i at rank i. This is an en-

tirely opposite preference compared to the expected AP. To

see this, suppose we have two documents to rank:

E [mRR ]

=E[Ra1 ]

+

E[Ra2 ] 2

-

E[ra1 ra2 ] 2

=p(Ra1 )

+

p(Ra2 ) 2

-

Cov[ra1 , ra2 ]

+ p(Ra1 )p(Ra2 ) 2

(13)

=p(Ra1 )

+

w2Rp(Ra2 )

-

C

ov[ra1 2

,

ra2

]

,

where

w2R

=

(1-p(Ra1 2

))

.

It

shows

that

negatively

correlated

document has a higher value of the expected RR, confirming

the findings in [10, 29] that the RR metric is optimized by

diversifying the ranked list of documents.

2.1.4 A General View

Through our analysis, it can be seen that the expected IR metrics roughly have two components. A unified definition is given as follows:

E[m(a|r)]



M
X

",1,ad,True
192,"

Wip(Rai )

+

M
X

V

(rai , ..., i

ra1 )

,

i=1

i=1

(14)

where Wi is the discount weight in position i, and V is a

229

Definition:
Wi V (rai , ..., ra1 )

Table 1: A unified view of expected IR metrics.

Precision

DCG

AP

RR

PM
i=1

rai

PM 2rai -1
i=1 log2(i+1)

1 NR

PM
i=1

rai

(1+Pij-=11 raj ) i

PM
i=1

rai i

Qi-1
j=1

(1

-

raj

)

Expected Precision Expected DCG

Expected AP

Expected RR

1

1 log2 (i+1)

1+Pij-=11 p(Raj ) i

Qi-1 j=1

(1-p(Raj

))

i

0

0

Pi-1
j=1

C ov(rai

,

raj

)

Cov(rai , Qij-=11(1 - raj ))

function defining the correlation between documents. The specific definitions with respect to different metrics are summarized in Table 1. Notice that for DCG, in the case of binary relevance, g(rai) = 2rai - 1 can be approximated as a linear function, and the variance bit vanishes in Eq. (9).
The first bit is a linear one with respect to the marginal probability p(Rai). Strictly speaking, this is untrue as W is adaptive to previously retrieved documents. But since the weight ratio Wi+1/Wi is usually smaller than one, the maximum value of the first bit is still achieved by ranking in the decreasing order of the marginal probability of relevance. This is identical to what the Probability Ranking Principle has suggested [19]. We call it the general ranking preference. The second bit makes the IR metrics different from each other. It is called the specific ranking preference. A more detailed discussion and comparison about it is presented in Section 3.1 through a simulation.
2.2 Practical Considerations
Stack Search Maximizing Eq. (14) is a non-trivial task because it needs to search over all possible ranking combinations. We use stack search similar to [30], which keeps a list of the best n ranking combinations as candidates seen so far. These candidates are incomplete solutions till rank i. It then iteratively expands each of the best partial solutions by adding a document at rank i + 1. For each candidate, we select top-n documents that have the maximum increases of the expected IR metric in Eq. (14). We then put all resulting partial solutions (in this case, n × n) onto the stack and then trim the resulting list of partial solutions to the top n candidates again. We repeat the loop until the end of the rank list is reached. The solution is the one having the maximum value among the candidate solutions. Such a sequential update may not necessarily provide a global optimization solution, but it provides an excellent trade off between accuracy and efficiency by adjusting n. When n is 1, it goes back to the greedy approach. When we increase n, better solutions may be found at the expense of more computational cost. For details refer to [30].
IR Model Calibration To calculate the expected IR metrics during retrieval, we need to estimate the joint probability of relevance. An obvious solution is to directly estimate it from the (training) data [20]. Relevance information is, however, not steadily available in many practical situations to build a robust relevance model. In this paper, we intend to conduct an indirect estimation using existing IR models. It is observed that in many text retrieval experiments that the calculated ranking scores can serve as robust indicators of documents' relevance with respect to queries. Thus, a mapping function can be developed to map from the ranking scores to the probability of relevance. Similar to [29], the joint probability of relevance p(r|q) is summarized by the marginal probability p(rai|q) and covariance Cov[rai , raj ].
Let us first look at p(rai|q), and treat it as the utility of ranking scores. We expect the utility, defined as u, to be a non-decreasing function of the ranking score. Thus the first derivative u > 0. It is also expected that u has a maximum value as the ranking score increases. Thus the

Figure 3: By adjusting the correlation between documents from -0.2 to 1.0, the gain on performance for average precision, DCG, and RR, respectively.
second derivative u < 0. Our experiment (Section 3.2) on TREC data has confirmed our intuition. Applying an exponential utility function (u > 0 and u < 0) [2] gives the mapping function as:

p(Rai|q)  u(s) = 1 - e-bs,

(15)

where u(s), in the range [0, 1), is the utility of the ranking score s, where s  0. b denotes a constant. For the empirical

study of the mapping, we refer to Section 3.2.

The next question is how to estimate the covariance

q Cov[rai , raj ] = (rai , raj ) V ar[rai ]V ar[raj ],

(16)

where V ar[rai ] = (1-p(Rai))p(Rai) if rai follows a Bernoulli distribution. The correlation coefficient (rai , raj ) models the dependency of relevance between documents at rank i

and j. During retrieval, it is reasonable to use the docu-

ments' score correlation to estimate the relevance correla-

tion, i.e., (rai, raj )  (sai, saj ). Strictly speaking, the score correlation is query-dependent. A practical solution

is, however, to approximate it by sampling queries and cal-

culating the correlation between documents' ranking scores

from an IR model. In our implementation, we construct each

of these queries by randomly sampling query terms from the

vocabulary of a data set.

For the expected RR, we need to compute the covari-

ance between document ai and variable vi, where vi is the

meta-relevance"" of previously retrieved i-1 documents, i.e.,",1,AP,True
193,vi,0,,False
194,Qi-1,0,,False
195,"j,1",0,,False
196,(1,0,,False
197,-,0,,False
198,raj ),0,,False
199,as,0,,False
200,defined,0,,False
201,in,0,,False
202,Section,0,,False
203,2.1.3.,0,,False
204,In our im-,0,,False
205,"plementation, we aggregate the content of the top i-1 doc-",0,,False
206,"uments as a meta document, and estimate the correlation",0,,False
207,between rai and vi as 1 minus the correlation between the meta document's ranking score and document ai's ranking,0,,False
208,score.,0,,False
209,3. EXPERIMENTS,0,,False
210,3.1 Simulation,0,,False
211,"In this section, we carried out a simulation as a confirmation of our analysis about the effect of correlation between different documents' relevance on a range of IR metrics. The relevance states of documents were generated for 10,000 trials. At each trial, for each rank position i, we kept",0,,False
212,230,0,,False
213,Figure 4: Probability that a result from each bin is relevant against the median of each bin.,0,,False
214,"the marginal probability of relevance p(Rai |q) unchanged and generated the relevance/nonrelevance states of the document. The samples were then randomly perturbed so that the correlation between each pair of variables increases from negative to positive (x axis in Figure (3) ). For each sample in each trial we calculated the value of an IR metric. We then averaged the metric values across all the trials to obtain the average value. We used the value of the IR metric when the correlation is set as zero as the basis for calculating the gain on the metric when the correlation changes. The results for AP, DCG, and RR are shown in Figure (3). It confirms our derivation of the expected DCG that it is insensitive to correlation. AP value increases when correlation increases, whereas RR does otherwise.",1,AP,True
215,"We tried with different settings such as the number of documents, and marginals etc, and got similar findings to the reported above. Previous empirical studies on TREC data have found out that one cannot optimize both the RR and AP metrics at the same time [24, 29]. The analytical forms and the simulation provide direct evidence that the AP metric encourage positively correlated documents whereas the RR metric encourages the opposite.",1,TREC,True
216,3.2 IR Model Calibration,0,,False
217,"In this section, TREC data is used to get an insight into how the mapping function u looks like. Similar to the experimental setup in [22], we measured the utility of ranking scores by the probability that documents given the ranking scores are judged relevant. Documents were binned based on their ranking scores for analysis; we judged the probability that a randomly picked document from each bin is judged as relevant. More specifically, we ran the Jelinek-Mercer smoothing language model on the TREC2004 Robust Track 249 topics with the parameter  set as its typical value 0.1 [34]. The top 1000 documents were returned for each topic, and there were in total 241,606 results returned for these 249 queries, among which there are 7,029 relevant documents out of a total number of 17,412 relevant documents in the track. The queries contain different numbers of terms. To making the ranking scores comparable across queries, we normalized the ranking scores for all results of each query by dividing these ranking scores by the number of terms in the query.",1,TREC,True
218,"We sorted the 241,606 results in the descending order in terms of their scores, and divided this ranked list into bins of 1,500 results each, yielding 161 bins: the first 160 bins containing 1,500 results each, and the last bin containing the 1606 documents with the lowest scores. We selected the median score in each bin to represent the bin. In Figure 4, the utility of each bin, i.e., the probability that a randomly chosen result from the bin is relevant, is estimated as the number of relevant documents in each bin divided by the bin size. The data points are based on the pairs of the median of each bin and probability of relevance, and the data points are connected by smoothed curves.",0,,False
219,Table 2: Overview of six TREC collections.,1,TREC,True
220,Name Description,0,,False
221,Size # Docs Topics,0,,False
222,"TREC8 TREC disks 1.86 GB 528,155 401-450",1,TREC,True
223,4&5 minus CR,0,,False
224,"Robust TREC disks 1.86 GB 528,155 301-450 and 601-",1,Robust,True
225,2004,0,,False
226,4&5 minus CR,0,,False
227,700 minus 672,0,,False
228,"Robust TREC disks 1.86 GB 528,155 50 difficult Ro-",1,Robust,True
229,Hard,0,,False
230,4&5 minus CR,0,,False
231,bust2004 topics,0,,False
232,"WT10g TREC Web 11 GB 1,692,096 501-550",1,WT,True
233,collection,0,,False
234,"CSIRO CSIRO crawl 4.2 GB 370,715 1-50 minus 8 un-",1,CSIRO,True
235,judged topics,0,,False
236,.Gov,1,Gov,True
237,"2002 crawl of 18 GB 1,247,753 551-600",0,,False
238,.gov domain,0,,False
239,"Figure 4 confirms our intuition that the mapping function is approximately a concave curve (u > 0 and u < 0) and fitting Eq. (15) to the data in Figure 4 gives b,"" 9.133. Our experiments showed that the performance of our approach is robust with respect to the choice of b, and a value of b anywhere between 7.0 and 12.0 results in negligible changes of the performance on all the test collections. For the remaining experiments, we fix the parameter b as 9, while bearing in mind that tuning it from training data might have potentials for further performance improvement.""",0,,False
240,3.3 Performance,0,,False
241,"We continued our empirical study of the proposed probabilistic retrieval framework, focusing on understanding its ability of optimizing IR metrics. Dirichlet and Jelinek-Mercer smoothing language models were chosen as the two baseline IR models since they are frequently reported for good performance on TREC test collections [34]. For each query, the ranking score of each document, calculated by either of the two IR models, is normalized by dividing them over the number of terms in the query. It is used as the input to estimate the marginal probabilities and covariance on the basis of the discussion in Section 2.2. The stack search is then applied to find an optimal ranking list that maximizes a given IR metric in Eq. (14). For the stack search, we simply set n,""1, i.e., equivalent to a greedy approach, while leaving this line of research to future work.""",1,TREC,True
242,"Standard stemming and stopword removing were carried out for both queries and documents. The smoothing parameters of the language models were tuned for the optimal performance for a metric on each data set. The results are reported on six TREC test collections, described in Table 2. TREC8, Robust 2004, and Robust 2004 Hard topics are three plain text collections, and TREC 2001 ad hoc task on WT10g data, TREC 2007 enterprise track document search task on CSIRO data, and TREC 2002 topic distillation task on .Gov data are on three Web collections.",1,TREC,True
243,"The results in Table 3 indicate that if we choose a certain IR metric to maximize, we obtained in most cases the best performance on this metric than optimizing other metrics and the baselines. More specifically, our approach always had the best performance with respect to MAP and MRR when the objective was to maximize the expected AP and RR, respectively. When we aimed to optimize the expected DCG, our approach improved the baseline on 8 out of 12 occasions in terms of NDCG. It is worth mentioning that no parameter was needed when optimizing the metrics. Without any parameter tuning, our approach consistently outperformed the two baseline models, and eight improvements are statistically significant.",1,ad,True
244,"Recall the analysis in Section 2 that the expected AP and RR have a rather ""opposite"" rank preference (utility) ­ the expected AP favors a document whose relevance is positively correlated with those of the documents ranked above, whereas the expected RR suggests otherwise. Table 3 demonstrates that the optimization of the expected RR always leads to better performance on MRR than optimization",1,AP,True
245,231,0,,False
246,"Table 3: Performance on MAP, NDCG and MRR when the objective is to optimize AP, DCG, and RR,",1,MAP,True
247,"respectively. We used the Dirichlet and Jelinek-Mercer smoothing language models, whose smoothing pa-",0,,False
248,"rameters were tuned for the optimal performance of a metric on each data set, as the baselines in optimization.",0,,False
249,We highlight the highest performance in bold. A Wilcoxon signed-rank test (p <0.05) is conducted and sta-,0,,False
250,tistically significant improvements over the baselines are marked with .,0,,False
251,TREC8,1,TREC,True
252,MAP NDCG MRR,1,MAP,True
253,Robust2004,1,Robust,True
254,MAP NDCG MRR,1,MAP,True
255,Robust hard,1,Robust,True
256,MAP NDCG MRR,1,MAP,True
257,Dirichlet (Baseline) 0.224 0.428 0.606 Dirichlet (Baseline) 0.221 0.410 0.596 Dirichlet (Baseline) 0.088 0.21 0.393,0,,False
258,Maximize AP,1,AP,True
259,0.236 0.428 0.602,0,,False
260,Maximize AP,1,AP,True
261,0.227 0.412 0.593,0,,False
262,Maximize AP,1,AP,True
263,0.089 0.21 0.387,0,,False
264,Maximize DCG,0,,False
265,0.224 0.44 0.615,0,,False
266,Maximize DCG,0,,False
267,0.219 0.411 0.593,0,,False
268,Maximize DCG,0,,False
269,0.0890.235 0.399,0,,False
270,Maximize RR,0,,False
271,0.189 0.436 0.628,0,,False
272,Maximize RR,0,,False
273,0.208 0.391 0.597,0,,False
274,Maximize RR,0,,False
275,0.076 0.23 0.410,0,,False
276,Jelinek-Mercer (Baseline) 0.228 0.404 0.458 Jelinek-Mercer (Baseline) 0.221 0.401 0.542 Jelinek-Mercer (Baseline) 0.09 0.225 0.36,0,,False
277,Maximize AP,1,AP,True
278,0.239 0.44 0.469,0,,False
279,Maximize AP,1,AP,True
280,0.228 0.412 0.593,0,,False
281,Maximize AP,1,AP,True
282,0.092 0.23 0.358,0,,False
283,Maximize DCG,0,,False
284,0.227 0.416 0.476,0,,False
285,Maximize DCG,0,,False
286,0.22 0.406 0.543,0,,False
287,Maximize DCG,0,,False
288,0.09 0.245 0.37,0,,False
289,Maximize RR,0,,False
290,0.196 0.404 0.477,0,,False
291,Maximize RR,0,,False
292,0.18 0.364 0.546,0,,False
293,Maximize RR,0,,False
294,0.087 0.24 0.374,0,,False
295,WT10g,1,WT,True
296,MAP NDCG MRR,1,MAP,True
297,CSIRO,1,CSIRO,True
298,MAP NDCG MRR,1,MAP,True
299,.Gov,1,Gov,True
300,MAP NDCG MRR,1,MAP,True
301,Dirichlet (Baseline) 0.202 0.4 0.550 Dirichlet (Baseline) 0.398 0.692 0.782 Dirichlet (Baseline) 0.147 0.272 0.419,0,,False
302,Maximize AP,1,AP,True
303,0.204 0.392 0.546,0,,False
304,Maximize AP,1,AP,True
305,0.408 0.692 0.785,0,,False
306,Maximize AP,1,AP,True
307,0.151 0.272 0.417,0,,False
308,Maximize DCG,0,,False
309,0.199 0.405 0.551,0,,False
310,Maximize DCG,0,,False
311,0.395 0.692 0.779,0,,False
312,Maximize DCG,0,,False
313,0.148 0.2930.428,0,,False
314,Maximize RR,0,,False
315,0.181 0.316 0.552,0,,False
316,Maximize RR,0,,False
317,0.367 0.636 0.789,0,,False
318,Maximize RR,0,,False
319,0.132 0.238 0.427,0,,False
320,Jelinek-Mercer (Baseline) 0.168 0.360 0.472 Jelinek-Mercer (Baseline) 0.374 0.684 0.849 Jelinek-Mercer (Baseline) 0.167 0.286 0.45,0,,False
321,Maximize AP,1,AP,True
322,0.176 0.376 0.48,0,,False
323,Maximize AP,1,AP,True
324,0.384 0.704 0.85,0,,False
325,Maximize AP,1,AP,True
326,0.1870.306 0.449,0,,False
327,Maximize DCG,0,,False
328,0.168 0.360 0.472,0,,False
329,Maximize DCG,0,,False
330,0.371 0.676 0.850,0,,False
331,Maximize DCG,0,,False
332,0.169 0.286 0.444,0,,False
333,Maximize RR,0,,False
334,0.153 0.36 0.481,0,,False
335,Maximize RR,0,,False
336,0.349 0.644 0.870,0,,False
337,Maximize RR,0,,False
338,0.147 0.245 0.454,0,,False
339,"of the expected AP, and vice versa. The result supports our theoretical finding that RR and AP are two different types of metrics, and optimizing either of them cannot lead to the optimal performance of the other.",1,AP,True
340,"Table 3 also shows that optimization of AP can sometimes lead to better performance on NDCG than direct optimization of DCG. Similar finding appeared in the learning to rank paradigm, and it was argued that the reason is due to the fact that MAP is more informative than DCG [32]. Yet, we think that the informative explanation, although true in learning to rank, does not necessarily hold in our probabilistic framework since we do not use IR metrics to summarize the training data. Our belief is supported by the results from the simulation in Section 3.1 that the expected DCG is invariant to the changes of relevance correlation between documents; and as a result, optimzing AP (prompting documents whose relevance is positively correlated with previous documents) shouldn't do any better than directly optimizing DCG for the NDCG metric. We thus believe the somewhat contradicted finding in the real data set may be attributed to the estimation of the joint probability of relevance, more specifically the relevance correlation, given the fact we used textual content to infer relevancy. As the cluster hypothesis suggests that relevant documents tend to be similar to each other to form clusters [25], a document is likely to be relevant if it is similar to relevant documents. As a result, the expected AP biases towards putting documents similar with each other in the top rank positions. When top ranked documents are relevant, these other documents are also likely to be relevant - their marginal probabilities of relevance might be higher than the estimated. As a result, metrics such as NDCG and Precision are improved.",1,AP,True
341,"Finally, we provide a further account of RR and AP, the two differently behaving metrics. Recall that in Figure 2 the properties of the expected RR and AP were depicted by adjusting the weight functions wiA and wiR using a single parameter p(r). Figure (5) used TREC8 test collection to further show the effect of p(r) on the resulting MRR and MAP performance. For comparison, the performance of the baseline Dirichlet smoothing language model, and the exact optimization of RR, MAP and DCG was also plotted.",1,AP,True
342,It shows that adjusting p(r) to approximate AP is very stable since the solution keeps roughly the same for all eight values of p(r). This could be explained by the fact that the weight ratio between wiA+1 and wiA saturates at 1 for,1,ad,True
343,Figure 5: MRR v.s. MAP,1,MAP,True
344,"all values of p(r) when i increases above 4. By contrast, the RR approximation is more volatile with respect to p(r). As p(r) increases from 0.1 to 0.5, the MRR performance increases whereas the MAP performance decreases. This is due to the fact that as p(r) decreases, the weight ratio of RR becomes similar to that of DCG and AP. p(r) can be used to trade off between the performance of MAP and MRR. When p(r) ,"" 0.3 and 0.4, the performance on MRR even slightly exceeds that on the exact optimization of RR. This suggests that there might be still scope to improve our stack search algorithm by setting n higher than 1.""",1,MAP,True
345,4. LINKS TO OTHER WORK,0,,False
346,"To complement Section 1, we continue the discussion of related work. In the learning to rank paradigm, optimizing IR metrics is conducted in a discriminative manner where Support Vector Machines or Neural Networks were commonly used [23, 33]. By contrast, we study the problem in a probabilistic framework where the intention is to combine both the generative and discriminative processes. Our formulation of optimal ranking also fundamentally departs from the idea in [26], where a probability distribution over document permutations (rank) is defined, and the expectation of IR metrics is considered under this distribution. In this paper, we, however, believe that the expectation of IR metrics should be with respect to a distribution of relevance, because the uncertainty comes only from the fact that we cannot know the relevance of documents with absolute certainty.",1,ad,True
347,"For the purpose of evaluation, the estimation of IR metrics, particularly MAP, has been investigated in the past.",1,MAP,True
348,232,0,,False
349,"For example, to reduce the variability of test collection, a normalization technique was introduced [11]; to deal with incomplete judgements, sampling approaches were proposed [3, 31]. Empirically, their error rates were measured [7]; and the uncertainty from the variability of relevance judgments in TREC were also examined [27]. By contrast, our study is for the purpose of retrieval, and thus the IR metric estimation and optimization were explored in a complete different situation where the relevance is not known a priori.",1,TREC,True
350,"The most relevant work can be found in [10, 15, 35]. The study in [10] argued that in some tasks users would be satisfied with a limited number of relevant documents, rather than requiring all relevant documents. The authors therefore proposed to maximize the probability of finding a relevant document among the top n. By treating the previously retrieved documents as non-relevant ones, their algorithm is equivalent to optimizing Reciprocal Rank. A more general solution is proposed in [35] on the basis of the Bayesian rank decision framework in [15]. In their solutions, different rank preferences are expressed by different utility functions and can be incorporated when calculating the score for each of the documents. The two ideas are close in spirit to the Maximal Marginal Relevance (MMR) criterion in [9], and can be called ""marginal relevance"" IR models because they are designed to calculate the additional information a document contributes in a result list. But unfortunately this framework does not allow the capacity to model and optimize different IR metrics.",1,corpora,True
351,"This paper takes a rather different view, although similar to [15, 35] we also follow the Bayesian decision theory. We argue that the rank utility is nothing to do with the (relevance) model parameters but only with the hidden true topical relevance; and the relevance states of documents need to be estimated before knowing any user (rank) utility. A good IR metric could be able to specify one type of rank utilities. Once we summarize our belief about the true relevance by the joint probability of relevance, the utility, expressed by an evaluation metric, can be estimated under such uncertainty, and the optimal decision is the one that optimizes that expected value. The two distinct retrieval steps do not assume a particular (relevance) retrieval model, making it applicable to many existing IR models and IR metrics.",0,,False
352,"Our work is also related to the portfolio theory of document ranking [29]. By an analogy with the financial problems, they argued that an optimal rank order is the one that balances the overall relevance (mean) of the ranked list against its risk level (variance). This paper follows the idea of using mean and variance to summarize a distribution and to analyze the expected IR metrics. Our analytical forms of expected IR metrics on the basis of the mean and variance reveal some interesting properties that have not been shown in the past.",0,,False
353,5. CONCLUSIONS,0,,False
354,"In this paper, we have studied the statistical properties of expected IR metrics when the relevance of documents is unknown. An implementation based on our analysis and the two-stage framework has already shown its ability of optimizing major IR metrics in a probabilistic framework. In the future, it is of great interest to seek its usage in web search where click-through data can be viewed as indirect evidence of documents' relevance. Also, during evaluation, the ""Cranfield paradigm"" considers relevance as deterministic values, either binary or graded ones. It is, however, more general to consider IR evaluation as a stochastic process too. Thus, despite the fact that our study of the expected IR metrics is for retrieval, the analysis and development are also rel-",1,ad,True
355,evant to evaluation if the disagreement between relevance assessors needs to be modelled.,0,,False
356,"6. REFERENCES [1] G. Amati and C. J. V. Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.",0,,False
357,"[2] K. Arrow. Aspects of the Theory of Risk-Bearing. Helsinki: Yrj¨o Hahnsson Foundation, 1965.",0,,False
358,"[3] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR, 2006.",0,,False
359,"[4] J. A. Aslam, E. Yilmaz, and V. Pavlu. The maximum entropy method for analyzing retrieval measures. In SIGIR, 2005.",0,,False
360,"[5] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.",0,,False
361,"[6] P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. Mercer. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 1993.",0,,False
362,"[7] C. Buckley and E. M. Voorhees. Evaluating evaluation measure stability. In SIGIR, 2000.",0,,False
363,"[8] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In ICML '05, 2005.",1,ad,True
364,"[9] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR, 1998.",0,,False
365,"[10] H. Chen and D. R. Karger. Less is more: probabilistic models for retrieving fewer relevant documents. In SIGIR, 2006.",0,,False
366,"[11] G. V. Cormack and T. R. Lynam. Statistical precision of information retrieval evaluation. In SIGIR, 2006.",0,,False
367,"[12] W. B. Croft and D. J. Harper. Using probabilistic models of document retrieval without relevance information. Document Retrieval Systems, 1988.",0,,False
368,"[13] D. Harman. Overview of the second text retrieval conference (trec-2). In HLT '94, 1994.",1,trec,True
369,"[14] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 2002.",0,,False
370,"[15] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In SIGIR, 2001.",0,,False
371,"[16] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, 2008.",0,,False
372,"[17] M. E. Maron and J. L. Kuhns. On relevance, probabilistic indexing and information retrieval. J. ACM, 1960.",0,,False
373,"[18] S. Mizzaro. Relevance: The whole history. Journal of the American Society of Information Science, 1997.",0,,False
374,"[19] S. E. Robertson. The probability ranking principle in IR. Journal of Documentation, pages 294­304, 1977.",0,,False
375,"[20] S. E. Robertson and K. Sp¨arck Jones. Relevance weighting of search terms. Journal of the American Society for Information Science, 27(3):129­46, 1976.",0,,False
376,"[21] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR, 1994.",0,,False
377,"[22] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In SIGIR, pages 21­29, 1996.",0,,False
378,"[23] M. Taylor, J. Guiver, S. Robertson, and T. Minka. Softrank: optimizing non-smooth rank metrics. In WSDM, 2008.",0,,False
379,"[24] S. Tomlinson. Early precision measures: implications from the downside of blind feedback. In SIGIR, 2006.",0,,False
380,"[25] C. J. van Rijsbergen. Information Retrieval. Butterworths, London, London, UK, 1979.",0,,False
381,"[26] M. N. Volkovs and R. S. Zemel. Boltzrank: learning to maximize expected ranking gain. In ICML '09, 2009.",0,,False
382,"[27] E. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In Information Processing and Management, pages 315­323. ACM Press, 1998.",0,,False
383,"[28] E. M. Voorhees. The TREC-8 question answering track report. In TREC-8, pages 77­82, 1999.",1,TREC,True
384,"[29] J. Wang and J. Zhu. Portfolio theory of information retrieval. In SIGIR, 2009.",0,,False
385,"[30] Y. Wang and A. Waibel. Decoding algorithm in statistical machine translation. In EACL, 1997.",0,,False
386,"[31] E. Yilmaz, E. Kanoulas, and J. A. Aslam. A simple and efficient sampling method for estimating ap and ndcg. In SIGIR, 2008.",0,,False
387,"[32] E. Yilmaz and S. Robertson. On the choice of effectiveness measures for learning to rank. Information Retrieval, 2009.",0,,False
388,"[33] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average precision. In SIGIR, 2007.",1,ad,True
389,"[34] C. Zhai. Statistical language models for information retrieval a critical review. Found. Trends Inf. Retr., 2(3):137­213, 2008.",0,,False
390,"[35] C. Zhai and J. D. Lafferty. A risk minimization framework for information retrieval. Inf. Process. Manage., 42(1):31­55, 2006.",0,,False
391,233,0,,False
392,,0,,False

,sentence,label,data,regex
0,Discriminative Models of Integrating Document Evidence and Document-Candidate Associations for Expert Search,0,,False
1,Yi Fang,0,,False
2,Department of Computer Science Purdue University,0,,False
3,"West Lafayette, IN 47907, USA",0,,False
4,fangy@cs.purdue.edu,0,,False
5,Luo Si,0,,False
6,Department of Computer Science Purdue University,0,,False
7,"West Lafayette, IN 47907, USA",0,,False
8,lsi@cs.purdue.edu,0,,False
9,Aditya P. Mathur,0,,False
10,Department of Computer Science Purdue University,0,,False
11,"West Lafayette, IN 47907, USA",0,,False
12,apm@cs.purdue.edu,0,,False
13,ABSTRACT,0,,False
14,"Generative models such as statistical language modeling have been widely studied in the task of expert search to model the relationship between experts and their expertise indicated in supporting documents. On the other hand, discriminative models have received little attention in expert search research, although they have been shown to outperform generative models in many other information retrieval and machine learning applications. In this paper, we propose a principled relevance-based discriminative learning framework for expert search and derive specific discriminative models from the framework. Compared with the state-ofthe-art language models for expert search, the proposed research can naturally integrate various document evidence and document-candidate associations into a single model without extra modeling assumptions or effort. An extensive set of experiments have been conducted on two TREC Enterprise track corpora (i.e., W3C and CERC) to demonstrate the effectiveness and robustness of the proposed framework.",1,TREC,True
15,Categories and Subject Descriptors,0,,False
16,H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval; H.3.4 Systems and Software,0,,False
17,General Terms,0,,False
18,"Algorithms, Design, Experimentation",0,,False
19,Keywords,0,,False
20,"Expert search, enterprise search, discriminative models",0,,False
21,1. INTRODUCTION,1,DUC,True
22,"With vast amount of information available within large organizations, the key challenge is to harness existing knowledge and expertise in a timely and effective manner. In consequence, enterprise information retrieval systems are increasingly demanded to return people with specific knowledge and skills in response to a user's query. A class of",0,,False
23,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",1,ad,True
24,vertical search engines known as expert finder have emerged for enterprise organizations.,0,,False
25,"As an important IR application, expert search (also known as expert finding) has received substantial attention in the IR research community. Rapid progress has been made in modeling and evaluation since the launch of TREC Enterprise Track in 2005 [12]. A notable observation is that probabilistic generative models have dominated the literature of expert search. In particular, many statistical language modeling techniques were proposed to model the relationship between a candidate expert and a query. These models usually characterize a generative process of how a query is generated from supporting documents of an expert. The key ingredient in these methods is to determine associations between people and documents because the associations are ambiguous in the TREC scenarios as well as in many realistic settings. Previous works have investigated different metrics or a combination of them to measure the associations, but the way of choosing or combining them is rather often heuristic and lacks of a clear justification. Furthermore, document evidence such as document or expert authority information, internal and external document structures, global evidence and so on is shown to be able to significantly improve expert retrieval performance, but to incorporate these features often requires many modeling assumptions and is often unwieldy.",1,ad,True
26,"On the other hand, discriminative models, another important class of probabilistic models with solid statistical foundation, are nearly absent in the research of expert search, especially on the TREC evaluations. In fact, discriminative models have been preferred over generative models in the recent past in many machine learning applications, partly because of their attractive theoretical properties. In the domain of IR, various discriminative models have also been applied to many retrieval problems (e.g., [23]). However, very limited research has been conducted to design discriminative models for expert search.",1,TREC,True
27,"In this work, we present a relevance-based discriminative learning framework for expert search and derive specific discriminative models from the framework. Similar to some prominent language models, the proposed models aggregate document evidence and document-candidate associations through supporting documents. Unlike the language models, we directly model the conditional probability of relevance given a query and an expert. As a result, heterogeneous or even arbitrary features can be naturally included into a single model. The parameters associated with the features are automatically learned from training data. We",0,,False
28,683,0,,False
29,report an extensive set of experiments on two TREC corpora to evaluate the effectiveness and robustness of the proposed discriminative framework.,1,TREC,True
30,"The next section discusses related work. Section 3 introduces the state-of-the-art generative language models for expert search. Section 4 presents our proposed approaches. In section 5, we discuss the advantages of discriminative models in the context of expert search. Section 6 explains our experimental methodology and Section 7 presents the experimental results. Section 8 concludes and points out some future work.",1,ad,True
31,2. RELATED WORK,0,,False
32,"The early work on expert finding systems was initiated in the Knowledge Management community, usually in the form of yellow pages [9]. These systems relied on experts to judge and input their skills by themselves against a predefined set of keywords, and thus the task was time-consuming. More recent techniques locate experts in an automatic fashion. An overview of early automatic expert finding systems is provided in [36]. The task of expert search has received a significant amount of attention as the task had been included in the TREC Enterprise track from 2005 to 2008 [12, 32, 1, 7]. The TREC Enterprise tracks provided a common platform for researchers to empirically evaluate methods for expert search. They demonstrated the feasibility of expert search on heterogeneous data collections. In the TREC corpora, the relationship between documents and experts is ambiguous and thus to model the document-candidate associations is a key issue in expert search research.",1,ad,True
33,"Most of the recent work on expert search generally falls into two categories: profile-centric and document-centric approaches. Balog et al. [3] formalizes the two methods by proposing two generative language models. Their Model 1 directly models the knowledge of an expert from associated documents, which is equivalent to a profile-centric approach, and their Model 2 first locates documents on the topic and then finds the associated experts, which is a documentcentric approach. It has been shown in [3] that Model 2 is generally more effective than Model 1 and since then it becomes one of the most prominent language models for expert search. In [8], a two-stage language model combining a document relevance and co-occurrence model is proposed, which is essentially equivalent to Model 2. An attempt to further improve their models is made by proposing a proximitybased document representation for incorporating sequential information in text [25]. There are many other generative probabilistic models proposed for expert finding. For example, Serdyukov and Hiemstra [30] propose an expert-centric language model. Fang and Zhai [14] derive two families of generative models by applying probability ranking principle. Probabilistic topic models are also proposed to simultaneously model the topical distribution of expertise evidence and experts [34].",1,ad,True
34,Some alternative approaches to expert search exist beyond language modeling. One effective approach is to treat the problem of ranking experts as a voting problem based on data fusion techniques [21]. Eleven different voting strategies were proposed to aggregate over the documents associated to an expert. Another approach is to model the process of expert finding by probabilistic random walks on so-called expertise graphs [31]. Many other expert finding methods were proposed during TREC Enterprise tracks.,1,TREC,True
35,"Besides the models, some researchers have shown that suitable features can help significantly boost the performance of expert finding. These features include document authority information such as the PageRank, indegree, and URL length [38], graph-based expert authority [10], internal document structures that indicate the experts' associations with the content of documents [6], non-local evidence [2], and the evidence that can be acquired outside of an enterprise [29]. Additional evidence can be integrated by identifying home pages of candidate experts and clustering relevant documents [20]. Proximity features that characterize the cooccurrence of query and expert mentions in the document are also shown indicative by the top runs in the TREC evaluations [16]. This led to several window-based approaches including [25, 4, 20].",1,TREC,True
36,"On the other hand, the early work of applying discriminative models in IR can date back to the early 1980s in which the maximum entropy approach was investigated to get around term independence assumptions in probabilistic generative models [11]. More recently, Nallapati [23] compared the performance of the maximum entropy model and support vector machines with that of language modeling in ad hoc retrieval and homepage finding, and argued that SVMs are preferred over language models because of their ability to learn arbitrary features automatically. Furthermore, it has been shown that feature-based discriminative models can consistently and significantly outperform current state of the art retrieval models with the correct choice of features [22]. Discriminative models have received increasing attention in IR, as another related area, learning to rank for IR, sparked genuine interest among researchers in the community [18]. Most of the learning to rank models are discriminative in nature and they have been shown improvements over their generative counterparts in ad hoc retrieval. Benchmark data sets such as LETOR [19] are also available for research on learning to rank. Although valuable work has been done on discriminative models for ad hoc retrieval and other IR domains, very limited research has been conducted to design discriminative models for expert search. The only relevant work that we are aware of is [15], which addressed the issue of differentiating heterogeneous sources according to specific queries and experts by learning associated weights from data, but the work did not model document-candidate relationship nor address how to incorporate new document evidence, which are two key issues in expert search.",1,ad,True
37,3. GENERATIVE MODELS,0,,False
38,"To predict a class  given an observation , the desired choice of  is given by the conditional class probabilities  (). Depending on how to compute  (), the existing classification techniques can be broadly classified into two major categories: generative models and discriminative models. In a discriminative approach, a parametric model is introduced for  (), and the values of the parameters are inferred from a set of labeled training data. In contrast, the generative approach attempts to capture the manner in which an observation  is generated from given classes  by specifying a prior distribution  () over classes and a class-conditional distribution  () over the observation. The posterior  () is obtained from Bayes' Theorem as",1,ad,True
39, ()   () (),0,,False
40,(1),0,,False
41,"In the context of expert search, the task is to find out what",0,,False
42,684,0,,False
43,"is the probability of a candidate  being an expert given a query topic . In other words, we want to know  () in order to rank candidate  according to this probability. Similarly, by invoking Bayes' Theorem, we have:",0,,False
44, ()   () (),0,,False
45,(2),0,,False
46,"where  () is the prior probability of a candidate, which is generally assumed uniform. Thus, the key quantity to estimate in the generative models is the probability of a query given the candidate,  (). Many language modeling techniques are proposed to estimate this quantity. One of the most prominent and effective one was called document models (often referred as Model 2) [3] where documents act as a hidden variable in the process which accumulates expertise evidence. Formally, it is expressed as",0,,False
47," () ,  () ()",0,,False
48,(3),0,,False
49,",1",0,,False
50,"where  () is the probability of the document  to generate the query  and can be calculated using a standard language model.  () is the probability of association between the document  and the candidate .  is the number of documents in the collection. Model 2 mimics the process one might use to find experts using a document retrieval system. Here, relevant documents are retrieved for the expertise requested, and they are used as evidence to indicate whether the associated candidates are experts. After aggregating all such evidence, the experts can be identified. As  () is relatively easy to determine in language models, the key ingredient in this model (and also in many other language models for expert search) is to estimate the document-candidate associations:  (), or  () if  () is assumed to be uniform.  () can be estimated by various methods. The simplest form is the boolean model where associations are binary decisions:  () ,"" 1 if the candidate appears in the document; otherwise,  () "", 0. More sophisticated methods are frequency based which consider the number of times that a candidate appears in the document. A set of heuristic combinations of all these metrics are also compared and investigated in [6].",0,,False
51,4. DISCRIMINATIVE MODELS FOR,0,,False
52,EXPERT SEARCH,0,,False
53,4.1 Discriminative Learning Framework for Expert Search,0,,False
54,"For the text-based retrieval, conventional relevance-based probabilistic models rank documents by sorting the conditional probability that each document would be judged relevant to the given query [17]. The underlying principle using probabilistic models for information retrieval is called probability ranking principle [26]. The Binary Independence Model (BIM) [27] is a realization of this principle. In the domain of expert search, the similar principle can be used where experts are ranked according to the descending order of the conditional probability of relevance given an expert and a query. Fang and Zhai [14] applied this principle in studying expert search problem. Both BIM and [14]'s models are generative and they use Bayes' theorem to reverse the original conditional probability.",0,,False
55,We propose a discriminative learning framework to directly model the conditional probability of relevance by a,0,,False
56,"parametric probability function. We cast expert search into a binary classification problem that treats the relevant queryexpert pairs as positive data and irrelevant pairs as negative data. Formally, we use a relevance variable   {1, 0} to denote whether two entities are relevant or not and thus the conditional probability of relevance (, ) represents the extent to which the expert  is relevant to the query . In our framework, (, ) can take any function form with parameter  that needs to estimate from training data. Based on different forms of , the resulting discriminative models are different. Given the relevance judgment  for the training expert-query pair (, ) which is assumed independently generated, the conditional likelihood  of the training data is as follows",0,,False
57,",",0,,False
58,"( ,"" 1, ) ( "","" 0, )1- (4)""",0,,False
59,where  is the number of queries and  is the number of experts. The parameters can then be estimated by maximizing the following log likelihood function,0,,False
60,    (,0,,False
61," , arg max",0,,False
62," log ( ,"" 1, )""",0,,False
63,(5),0,,False
64,(,0,,False
65,)),0,,False
66,"+ (1 - ) log 1 - ( ,"" 1, )""",0,,False
67,"The estimated parameters can then be plugged back in ( ,"" 1, ). According to the probability ranking principle, the experts are presented to users in the descending order of ( "","" 1, ). In the next section, we propose a specific discriminative model by defining the form of ( "","" 1, ).""",0,,False
68,4.2 A Discriminative Model,0,,False
69,"According to the previous work, Model 2 turned out to be one of the most effective formal models for expert search. The success of the model lies in its effective process to collect expertise evidence from documents. Our discriminative model builds on the same process in which the supporting document  serves as a bridge to connect expert  and query . Given a document , whether  and  are relevant depends on two factors: document evidence and document-candidate associations. More specifically, we consider: 1) whether the document  is relevant to the query ; 2) whether the expert  is relevant to the document . The final relevance decision for (, ) is made by averaging over all the documents. Formally, this can be expressed as",1,ad,True
70," ( ,"" 1, ) "",  (1 ,"" 1, ) (2 "","" 1, ) ()""",0,,False
71,",1",0,,False
72,(6),0,,False
73,"where  (1 ,"" 1, ) allows us to model the probability that a document  matches a topic , which indicates the document evidence.  (2 "","" 1, ) allows us to model the probability that a supporting document  mentions a candidate , which indicates the document-candidate associations. A""",0,,False
74,"document  with higher values on both probabilities would contribute more to the value of  ( ,"" 1, ). The prior""",0,,False
75,"probability of a document,  (), is generally assumed uni-",0,,False
76,form,0,,False
77,"(i.e.,",0,,False
78," () ,",0,,False
79,1 ,0,,False
80,).,0,,False
81,We,0,,False
82,model,0,,False
83,both,0,,False
84, (1,0,,False
85,","" 1, )""",0,,False
86,and,0,,False
87," (2 ,"" 1, ) by logistic functions on a linear combination""",0,,False
88,685,0,,False
89,"of features. Formally, they are parameterized as follows:",0,,False
90,(  ,0,,False
91,),0,,False
92," (1 ,"" 1, ) "", ",0,,False
93,"(, )",0,,False
94,(7),0,,False
95,",1",0,,False
96,(  ,0,,False
97,),0,,False
98," (2 ,"" 1, ) "", ",0,,False
99,"  (, )",0,,False
100,(8),0,,False
101,",1",0,,False
102,"where () ,"" 1/(1 + exp (-)) is the standard logistic function.  is the weight for the  query-document feature (, ) and  is the weight for the  document-candidate feature (, ). Specifically, (, ) is the document evidence such as document retrieval scores that indicates how relevant the document is to the query. (, ) is the feature such as the boolean associations that describe the strength of associations between a document and a candidate.  denotes the number of document evidence features and  denotes the number of document-candidate association features. The weight parameters can be learned by maximizing the conditional log-likelihood of the data (i.e., Eqn. 5). Because there is no analytical solution, we use the BFGS Quasi-Newton for the optimization [13]. The method requires the objective function and its gradients. The partial derivatives of the log-likelihood  with respect to  and  are given as""",1,ad,True
103,",",0,,False
104,(,0,,False
105,  (1,0,,False
106,- -,0,,False
107,  ),0,,False
108,),0,,False
109,"(1 - ) (, )",0,,False
110,",1",0,,False
111,",",0,,False
112,(   (1,0,,False
113,- -,0,,False
114,  ),0,,False
115,",1",0,,False
116, (1,0,,False
117,-,0,,False
118," ) (,",0,,False
119,) ),0,,False
120,"where ,  and  denote the probabilities of Eqn. 6, Eqn. 7, and Eqn. 8, respectively. The main computation of the gradient method is evaluating the log likelihood function and its gradients against param( eters. Both of )them have computational complexity of   ( + ) . In practice, we only have a small number of relevance judgments for training and thus  is relatively small. In addition, the number of documents associated with each expert and the number of features used are also usually relatively small. Therefore, the training procedure can be efficient.",1,ad,True
121,"We can see that both Model 2 and this discriminative model try to aggregate document evidence and documentcandidate associations through the bridge of documents, but they are different in how to estimate these two probabilities. In Model 2, the document evidence (i.e.,  ()) is calculated by standard language models and the documentcandidate associations (i.e.,  ()) are estimated by a heuristic combination of document-candidate association features. In our proposed discriminative model, both quantities are modeled by logistic functions with arbitrary features and the parameters are automatically determined from training data. From Eqn. 6, we can see that ( ,"" 1, ) is essentially the arithmetic mean of  ( "","" 1, , ) with respect to . Thus we refer the model as the arithmetic mean discriminative (AMD) model.""",0,,False
122,4.3 An Alternative Discriminative Model with Geometric Mean,0,,False
123,It has been shown that in certain cases geometric mean (the product rule) is better than arithmetic mean (the sum rule) in combining evidences [35]. This observation mo-,0,,False
124,"tivates an alternative discriminative model which we refer as the geometric mean discriminative (GMD) model where ( ,"" 1, ) is modeled by the geometric mean as follows:""",0,,False
125, (,0,,False
126,",",0,,False
127,"1, )",0,,False
128,",",0,,False
129,1 ,0,,False
130, (  (1,0,,False
131,",",0,,False
132,"1, ) (2",0,,False
133,",",0,,False
134,)1 ,0,,False
135,"1, )",0,,False
136,",1",0,,False
137,(9),0,,False
138,where  is the normalization factor that scales the geometric,0,,False
139,mean to be a proper probability distribution as follows,0,,False
140, (,0,,False
141,)1 ,0,,False
142,",",0,,False
143," (1, ) (2, )",0,,False
144,(10),0,,False
145,"1{0,1},2{0,1} ,1",0,,False
146,"Both  (1 ,"" 1, ) and  (2 "","" 1, ) here take the same form with Eqn. 7 and Eqn. 8. By plugging them and Eqn. 10 into Eqn. 9, we can get""",0,,False
147, (,0,,False
148,",",0,,False
149,"1, )",0,,False
150,",",0,,False
151,1,0,,False
152,+ exp(-) +,0,,False
153,1 exp(- ),0,,False
154,+ exp(-),0,,False
155,(11),0,,False
156,where,0,,False
157,",",0,,False
158,(,0,,False
159,1 ,0,,False
160,),0,,False
161,"(, ) , ",0,,False
162,",",0,,False
163,(,0,,False
164,1 ,0,,False
165,),0,,False
166," (, )",0,,False
167,",1",0,,False
168,",1",0,,False
169,",1",0,,False
170,",1",0,,False
171,",",0,,False
172,( ,0,,False
173,1 ,0,,False
174,"(,",0,,False
175,) ),0,,False
176,+,0,,False
177,( ,0,,False
178,1 ,0,,False
179," (,",0,,False
180,) ),0,,False
181,",1",0,,False
182,",1",0,,False
183,",1",0,,False
184,",1",0,,False
185,"We can notice that in Eqn. 11 there are three exponential terms in the denominator, which means that either querydocument features (, ) or document-candidate features (, ) alone cannot dominate the final relevance  ( ,"" 1, ). The parameters of the model can also be estimated by maximizing the conditional log-likelihood function using BFGS. The GMD model has the same computational complexity with AMD.""",0,,False
186,4.4 Advantages of Discriminative Models for Expert Search,0,,False
187,"Some theoretical results show that discriminative models tend to have a lower asymptotic error [24]. Besides the theoretical considerations, we believe there are specific reasons for the domain of expert search that make discriminative models a suitable choice. First of all, the proposed discriminative models can effortlessly incorporate features. As shown in Section 2 and prior research, expert search can benefit from including various types of features. Language modeling approaches often require many modeling assumptions and extra modeling effort to include new features especially when the heterogeneous features are present. Secondly, discriminative models typically make fewer model assumptions than their generative counterparts. For example, many state-of-the-art generative models, including Model 2, the candidate-generation model [14] and the twostage language model approach [8], assume that the query  and candidate  are independent given the document , i.e., (, ) ,"" (). It requires extra modeling effort for these models to overcome the assumption [4]. In contrast, our proposed discriminative models can easily get around it. For example,  (2 "","" 1, ) in Eqn. 6 can be replaced by  (2 "","" 1, , ) where no independence assumption is made on  (2 "","" 1, , ). Thirdly, the discriminative models directly and naturally characterize the notion of relevance. In Model 2 and many other language models, there""",1,corpora,True
188,686,0,,False
189,"is no explicit reference to the class variable that denotes whether an expert is relevant or not. We use  ( ,"" 1, ) instead of  () to make it explicit that the relevance of an expert is measured with respect to a query. This explicit notion of relevance can help quantify the extent to which a user's information need is satisfied.""",1,ad,True
190,5. EXPERIMENTS,0,,False
191,5.1 Data Collections,0,,False
192,"Our experiments are carried out in the setting of the Expert Search task of the TREC Enterprise tracks from 2005 to 2008. For TREC 2005 and 2006, the document collection was a crawl of the World Wide Web Consortium (W3C) [12, 32]. For TREC 2007 and 2008, a different and more realistic corpus was introduced, which is a crawl of the website of Commonwealth Scientific and Industrial Research Organization (CSIRO). The corpus is known as the CSIRO Enterprise Research Collection (CERC) [1, 7]. Table 1 gives detailed statistics of the collections and query sets. The W3C data is supplemented with a list of 1092 candidate experts represented by their full names and email addresses while the CERC data do not contain a predefined list of candidates. Based on the observation that most CSIRO employees have a CSIRO email address following the pattern ""firstname.lastname@csiro.au"", we extract a list of candidates with email addresses matching this pattern from text. We also use heuristic rules to filter non-personal addresses (e.g. education.act@csiro.au). The total number of candidates extracted is 3,482. In 2005, 50 queries were created based on the working groups in W3C (there were 10 training topics also available in 2005). In 2006, 49 queries were developed by the track participants collectively using the provided list of supporting documents for each candidate. The 50 queries used in 2007 were created with the help of CSIRO's Science Communicators, while the judgments of 77 queries in 2008 were made by participants.",1,TREC,True
193,"To evaluate the proposed models on W3C, we use the TREC 2006 topics plus the 10 available TREC 2005 training topics for training and test the models on the TREC 2005 topics. Similarly on CERC, we use TREC 2008 topics for training and TREC 2007 topics for testing. Although different years have different ways of topic assessments, we will see in the experiments that the discriminative models can still gain significant improvements from the training data. Our decision of choosing the training and testing configurations is mainly based on the number of relevance judgments available. We need a reasonable amount of training data for the discriminative models and there are relatively more relevance judgments in 2006 for W3C and in 2008 for CERC. Because the two test collections have very different characteristics, we do not evaluate the models across the corpora. To obtain a balanced training set, we randomly select the same number of negative instances with the number of positive instances for each training query, by following the undersampling method in [23]. To acquire negative instances for the queries without non-relevance judgments (i.e., 10 TREC 2005 training topics), we use the Base method introduced in Section 6.1 to identify a list of unjudged/irrelevant experts for each query. Evaluation measures are mean average precision (MAP), R-precision (R-Prec), mean reciprocal rank (MRR), and precision@5 (p@5) and precision@10 (p@10).",1,W3C,True
194,Table 1: Statistics of the W3C and CERC testbeds,1,W3C,True
195,W3C,1,W3C,True
196,CERC,1,CERC,True
197,# Documents,0,,False
198,"331,037",0,,False
199,"370,715",0,,False
200,# People,0,,False
201,"1,092",0,,False
202,"3,482",0,,False
203,Avg. Doc Length in Token,0,,False
204,983.4,0,,False
205,354.8,0,,False
206,Avg. # Rel Experts/Topic 51.5 (2006) 10.4 (2008),0,,False
207,(TREC Year),1,TREC,True
208,30.2 (2005),0,,False
209,3.0 (2007),0,,False
210,Training Queries,0,,False
211,2006 (49),0,,False
212,2008 (77),0,,False
213,2005 (10),0,,False
214,Testing Queries,0,,False
215,2005 (50),0,,False
216,2007 (50),0,,False
217,5.2 Research Questions,0,,False
218,An extensive set of experiments were designed to address the following questions of the proposed research:,1,ad,True
219, Can the discriminative trained model perform better than its generative counterpart when the same set of features are available for use? (Section 6.1),0,,False
220, Can integration of additional features into the discriminative model improve the performance? (Section 6.1),1,ad,True
221, What features are likely more important in terms of the relative values of the learned weights in the discriminative model? (Section 6.1),0,,False
222, What is the effect of only retrieving a subset of documents on the proposed model? (Section 6.2),0,,False
223, How robust is the proposed discriminative model with respect to the underlying document retrieval methods? (Section 6.3),0,,False
224, How robust is the proposed discriminative learning framework with respect to specific discriminative models? (Section 6.4),0,,False
225,"In all the sections except Section 6.4, we only use the arithmetic mean discriminative (AMD) model to assess the discriminative learning approach, since we care less about the difference between discriminative models than about the difference between generative and discriminative models.",0,,False
226,5.3 Experimental Setup,0,,False
227,"In all our experiments, we have done minimal preprocessing in which both queries and documents are stemmed using Krovetz stemmer. We only use the ""title"" or ""query"" fields in the topics without using extra information (e.g., ""narrative""). No query expansion nor external resource is utilized. As shown in Section 4, each query-expert pair is characterized by two feature vectors, i.e., document evidence (, ) and document-candidate associations (, ). Table 2 summarizes the features used in the discriminative models.",0,,False
228,"These features include the score from the standard document language model (1), document features (2 - 5), external document structure features (6 - 9), basic association features (1 -5), internal document structure features (6 - 9), and proximity features (10 - 13). Here the external document structure features are the boolean variables to represent whether a document (in W3C) comes from specific types of documents (e.g., 8 ,"" 1 means the document is either from """"www"""" or """"esw""""). The evaluations on W3C use all the features, while the features 6 - 9 and 6 - 9 are not applied to CERC, as the CERC dataset does not""",1,W3C,True
229,687,0,,False
230,Table 2: Features used in the discriminative models.,0,,False
231,B denotes the feature takes boolean values and,0,,False
232,N represents numerical values,0,,False
233,Feature,0,,False
234,Description,0,,False
235,Type References,0,,False
236,1,0,,False
237,LM,1,LM,True
238,N,0,,False
239,[37],0,,False
240,2,0,,False
241,PageRank,0,,False
242,N,0,,False
243,[38],0,,False
244,3,0,,False
245,URL length,0,,False
246,N,0,,False
247,[38],0,,False
248,4,0,,False
249,Anchor text,0,,False
250,N,0,,False
251,[38],0,,False
252,5,0,,False
253,Title,0,,False
254,N,0,,False
255,[38],0,,False
256,6,0,,False
257,From lists,0,,False
258,B,0,,False
259,[12],0,,False
260,7,0,,False
261,From people,0,,False
262,B,0,,False
263,[12],0,,False
264,8,0,,False
265,From www+esw,0,,False
266,B,0,,False
267,[12],0,,False
268,9,0,,False
269,From other+dev,0,,False
270,B,0,,False
271,[12],0,,False
272,1,0,,False
273,Exact name match,0,,False
274,B,0,,False
275,[3],0,,False
276,2,0,,False
277,Name match,0,,False
278,B,0,,False
279,[3],0,,False
280,3,0,,False
281,Last name match,0,,False
282,B,0,,False
283,[3],0,,False
284,4,0,,False
285,Email match,0,,False
286,B,0,,False
287,[3],0,,False
288,5,0,,False
289,LM score,1,LM,True
290,N,0,,False
291,[6],0,,False
292,6,0,,False
293,EMAIL FROM,0,,False
294,B,0,,False
295,[5],0,,False
296,7,0,,False
297,EMAIL TO,0,,False
298,B,0,,False
299,[5],0,,False
300,8,0,,False
301,EMAIL CC,0,,False
302,B,0,,False
303,[5],0,,False
304,9,0,,False
305,EMAIL CONTENT,0,,False
306,B,0,,False
307,[5],0,,False
308,10  13,0,,False
309,Proximity,0,,False
310,B,0,,False
311,-,0,,False
312,"contain explicit document types nor many emails with internal structure information useful for expert search [38]. The 1 feature is the document retrieval score by LM using the topic as the query. The smoothing method of LM is Jelinek-Mercer with the parameter  ,"" 0.5 (we use the same smoothing for other LMs). The 5 feature is the retrieval score by LM using the candidate identifier as the query [6]. The """"Proximity"""" features (6 - 9) are the boolean variables indicating whether the candidate identifier co-occurs with the query term in a window with various sizes. We use 20, 50, 100 and 250 as the window sizes (in number of words), approximated to the sizes of sentence, passage, paragraph and section, respectively. The details about these features can be found in the corresponding reference. To normalize the features, we use query-based normalization for each feature as suggested in [19].""",1,LM,True
313,"Many of these features have been shown useful for expert search. Because of the generative nature of language models, it is difficult for them to incorporate such heterogeneous features in a unified modeling framework, but discriminative models can effortless include all the features and many more. Since the focus of this study is on the probabilistic models rather than feature engineering, we do not intend to choose a complete set of features, but they are one of the most comprehensive and diverse feature sets in a single work among the existing expert search research.",1,corpora,True
314,6. RESULTS,0,,False
315,6.1 Discriminative Model vs. Model 2,0,,False
316,"In this section, we compare the proposed discriminative model with its generative counterpart: Model 2. The proposed model is evaluated on four different feature configurations, which are presented in Table 3. The Base method is the implementation of Model 2 by following [3], which includes 4 types of document-candidate associations. The R1 configuration uses these 4 association features plus 1 as document evidence. Thus, the identical information is",0,,False
317,Table 3: Experimental configurations,0,,False
318,Base Balog et al's Model 2 (candidate-centric) with 4,0,,False
319,"association features (i.e., 1 - 4) [3]",0,,False
320,R1,0,,False
321,Discriminative model with 4 association features,0,,False
322,(1 - 4) and LM document evidence feature (1),1,LM,True
323,R2,0,,False
324,Discriminative model with full document evi-,0,,False
325,dence features and 4 association features (1 -4),0,,False
326,R3,0,,False
327,Discriminative model with full association fea-,0,,False
328,tures and one document evidence feature (1),0,,False
329,R4,0,,False
330,Discriminative model with full document evi-,0,,False
331,dence features and full association features,0,,False
332,Table 4: Comparison of the discriminative model (AMD) with the Base mehod on W3C and CERC. Best results on each collection are highlighted. The symbol indicates statistical significance at 0.95 confidence interval against Base,1,W3C,True
333,MAP R-Prec MRR,1,MAP,True
334,P5,0,,False
335,P10,0,,False
336,W3C,1,W3C,True
337,Base R1 R2 R3 R4,0,,False
338,0.1909 0.2001 0.2282 0.2412 0.2598,0,,False
339,0.2445 0.2552 0.2764 0.2904 0.3035,0,,False
340,0.5081 0.5300 0.5624 0.6232 0.6196,0,,False
341,0.3760 0.3820 0.3960 0.4020 0.4130,0,,False
342,0.3120 0.3310 0.3370 0.3560 0.3680,0,,False
343,CERC,1,CERC,True
344,Base R1 R2 R3 R4,0,,False
345,0.4039 0.4123 0.4453 0.4569 0.4604,0,,False
346,0.3514 0.3569 0.3854 0.3879 0.3938,0,,False
347,0.5389 0.5593 0.5924 0.5886 0.6143,0,,False
348,0.2240 0.2280 0.2390 0.2610 0.2520,0,,False
349,0.1540 0.1540 0.1650 0.1660 0.1770,0,,False
350,"available for R1 and Base to use. The weights in Base are set by following the choice of the best run in [3]. R4 is the configuration with full applicable features for the discriminative model (the R4 configuration is the default setting in all the experiments except explicitly noted). Table 4 contains the evaluation results on the two test collections. We can see that the discriminative model consistently performs better than Base across all the feature configurations on all measures. With the full set of features (i.e., R4 vs Base), all the differences are statistically significant by two-tailed Student's t-test at 0.95 confidence level. In R1 vs Base, although their differences are not significant, the discriminative model outperforms the Base method on all the evaluation metrics.",0,,False
351,"Since all the features are normalized, the weight associated with each feature can reflect the importance of the feature in some degree. Table 5 reports the top 3 features with the largest weights in  and  respectively in the learned AMD model. These features are ordered alphabetically in the table since their weights are not very distinct from each other. We find that the features listed for the two testbeds are generally different with the exception of 1 and 2, showing the importance of these two features across the corpora. An interesting observation is that the 8 feature when used on W3C has a large weight among all the document-candidate association features. This is intuitive in the sense that the person who is in the email cc field is likely an authoritative of the topics of the email, which is also consistent with what was reported in [5]. Another observation is that the ""Proximity"" features have large weights for both testbeds (i.e., 13",1,corpora,True
352,688,0,,False
353,Table 5: The top 3 features with the largest weights,0,,False
354,in AMD (R4) learned from training data,0,,False
355,Doc evidence Doc-candidate associations,0,,False
356,W3C CERC,1,W3C,True
357,"1, 2, 6 1, 2, 5",0,,False
358,"1, 8, 13 4, 5, 11",0,,False
359,MAP,1,MAP,True
360,0.25 0.2,0,,False
361,0.15 0.1 102 0.5 0.4 0.3 0.2 101,0,,False
362,W3C,1,W3C,True
363,103 Number of Documents Retrieved,0,,False
364,CERC,1,CERC,True
365,102,0,,False
366,103,0,,False
367,Number of Documents Retrieved,0,,False
368,Base AMD,0,,False
369,104,0,,False
370,Base AMD,0,,False
371,104,0,,False
372,MAP,1,MAP,True
373,Figure 1: Impact of varying the number of documents retrieved ( ) on the discriminative model. Top: impact on W3C; Bottom: impact on CERC.,1,W3C,True
374,"for W3C and 11 for CERC), but with different window sizes: i.e., larger size on W3C. This may come from the fact that these two collections have very different average document lengths.",1,W3C,True
375,6.2 The Effect of the Size of Retrieved Documents,0,,False
376,"Similar to Model 2, the learned discriminative model can be efficiently used on top of an existing document search engine as follows: 1) Perform a standard document retrieval run using the topic as a query and retrieve the top  documents; 2) For each candidate associated with the relevant documents, calculate the probability of relevance using Eqn. 6 on these  documents. In this section, we aim to investigate the effect of the size of documents retrieved on the performance of the discriminative model. We use LM as the document retrieval run. Figure 1 shows the MAP results by varying  on the two test collections. Note that the scales on the x-axis and y-axis differ per plot. From the figure, we can see that as  increases, the discriminative model has a similar trend with the baseline: increasing, achieving a maximum, and then flattening. On W3C, the MAP value tops after 300 documents retrieved, fewer than what the baseline needs (i.e., 400). For CERC, both models need around 50 documents for best performance. Therefore, using a subset of documents could speed up the process of expert search as the best performers use much less documents than the whole set of relevant documents. At the same time, the retrieval performance can be improved although their differences are not found statistically significant.",1,LM,True
377,6.3 Experiments by Using Different Document Retrieval Methods,0,,False
378,"As shown in Section 6.1 as well as in prior work, the doc-",0,,False
379,Table 6: Evaluation of AMD with different docu-,0,,False
380,ment retrieval methods on W3C and CERC,1,W3C,True
381,MAP R-Prec MRR P5,1,MAP,True
382,P10,0,,False
383,W3C,1,W3C,True
384,LM BM25 Indri,1,LM,True
385,0.2598 0.2658 0.2562,0,,False
386,0.3035 0.3141 0.3066,0,,False
387,0.6196 0.6238 0.6149,0,,False
388,0.4130 0.4060 0.4090,0,,False
389,0.3680 0.3700 0.3640,0,,False
390,CERC,1,CERC,True
391,LM BM25 Indri,1,LM,True
392,0.4604 0.4551 0.4667,0,,False
393,0.3938 0.3895 0.4086,0,,False
394,0.6143 0.5877 0.6000,0,,False
395,0.2520 0.2470 0.2550,0,,False
396,0.1770 0.1740 0.1780,0,,False
397,Table 7: Comparison of the geometric mean discrim-,0,,False
398,inative model with Base and AMD (R4) on W3C,1,W3C,True
399,and CERC. The symbol indicates statistical signif-,1,CERC,True
400,icance at 0.95 confidence interval for GMD against,0,,False
401,Base,0,,False
402,MAP R-Prec MRR,1,MAP,True
403,P5,0,,False
404,P10,0,,False
405,W3C,1,W3C,True
406,Base AMD GMD,0,,False
407,0.1909 0.2598 0.2512,0,,False
408,0.2445 0.3035 0.3010,0,,False
409,0.5081 0.6196 0.6266,0,,False
410,0.3760 0.4130 0.4110,0,,False
411,0.3120 0.3680 0.3640,0,,False
412,CERC,1,CERC,True
413,Base AMD GMD,0,,False
414,0.4039 0.4604 0.4669,0,,False
415,0.3514 0.3938 0.4030,0,,False
416,0.5389 0.6143 0.6274,0,,False
417,0.2240 0.2520 0.2500,0,,False
418,0.1540 0.1770 0.1790,0,,False
419,"ument retrieval score 1 is an important feature to show document evidence for expert search. In this experiment, we assess the extent to which the performance of the discriminative model is affected by the choice of the underlying document retrieval model. Besides LM, another two different document retrieval methods are used (i.e., BM25 [28] and Indri [33]). Specifically, the 1 feature is replaced by these two retrieval scores respectively in the R4 configuration. Table 6 shows the MAP results of the proposed model across the three retrieval models. From the table, we can see that the results are quite similar and they are all significantly better than the baseline. This indicates that the discriminative model is robust to the underlying document retrieval method.",1,LM,True
420,6.4 The Alternative Discriminative Model vs. Base and AMD,0,,False
421,"In this section, we conduct the experiment to evaluate the alternative discriminative model (GMD). The aim is to investigate the robustness of the proposed discriminative framework with respect to the choice of specific discriminative models derived from the framework. Table 7 contains the results. From the table, we can see that all the results achieved by GMD significantly outperform the baseline. Furthermore, these results are quite similar with those achieved by the AMD (R4) model. In particular, the GMD model is generally better than AMD on CERC and worse on W3C, but the differences between GMD and AMD are not statistically significant. These results demonstrate that the proposed discriminative framework generates accurate and robust results with both types of discriminative models.",1,CERC,True
422,689,0,,False
423,7. CONCLUSIONS AND FUTURE WORK,0,,False
424,"In this work, we propose a discriminative learning framework and derive specific models for expert search. The main advantage of the proposed approaches is their ability to integrate a variety of document evidence and documentcandidate association features. The evaluations on two TREC Enterprise track testbeds have shown the effectiveness and robustness of the proposed framework.",1,ad,True
425,"There are several possibilities to extend the research in this paper. We chose ""out-of-order"" training in the experiments because more training data are available in 2006 and 2008. It would be interesting to perform the ""in-order"" experiments (i.e., training on 2005 or 2007), which would allow fair comparisons with the TREC submitted runs. The relevance judgments in 2005 and 2007 seem also more likely to be obtained in a real enterprise. In fact, lack of training data hinders the applicability of many discriminative models. On the other hand, generative models may be able to effectively utilize abundant unlabeled data. It is desirable to develop a hybrid of discriminative and generative models to obtain the best of both for expert search. In addition, in certain scenarios, pairwise comparisons between experts might be more easily collectible than the pointwise judgment for each expert. We will explore to extend the proposed discriminative learning framework to handle this type of training data.",1,TREC,True
426,8. ACKNOWLEDGMENTS,0,,False
427,"We thank the anonymous reviewers for many valuable comments. This research was partially supported by a grant from the Indiana Economic Development Company, the NSF research grant IIS-0749462, and a grant from Purdue University. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsor.",0,,False
428,9. REFERENCES,0,,False
429,"[1] P. Bailey, N. Craswell, A. De Vries, and I. Soboroff. Overview of the trec-2007 enterprise track. In TREC-15, 2007.",1,trec,True
430,"[2] K. Balog. Non-local evidence for expert finding. In CIKM, 2008.",0,,False
431,"[3] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert finding in enterprise corpora. In SIGIR, 2006.",1,corpora,True
432,"[4] K. Balog, L. Azzopardi, and M. de Rijke. A language modeling framework for expert finding. Information Processing & Management, 45(1):1­19, 2009.",0,,False
433,"[5] K. Balog and M. de Rijke. Finding experts and their details in e-mail corpora. In WWW, page 1036. ACM, 2006.",1,corpora,True
434,"[6] K. Balog and M. De Rijke. Associating people and documents. In ECIR, 2008.",0,,False
435,"[7] K. Balog, I. Soboroff, P. Thomas, N. Craswell, A. de Vries, and P. Bailey. Overview of the trec-2008 enterprise track. In TREC-16, 2008.",1,trec,True
436,"[8] Y. Cao, J. Liu, S. Bao, and H. Li. Research on expert search at enterprise track of TREC 2005. In TREC-13, 2005.",1,TREC,True
437,"[9] P. Carlile. Working knowledge: how organizations manage what they know. Human Resource Planning, 21(4):58­60, 1998.",0,,False
438,"[10] H. Chen, H. Shen, J. Xiong, S. Tan, and X. Cheng. Social network structure behind the mailing lists: Ict-iiis at trec 2006 expert finding track. In TREC-14, 2006.",1,trec,True
439,"[11] W. Cooper. Exploiting the maximum entropy principle to increase retrieval effectiveness. JASIST, 34(1):31­39.",0,,False
440,"[12] N. Craswell, A. de Vries, and I. Soboroff. Overview of the trec-2005 enterprise track. In TREC-13, 2005.",1,trec,True
441,"[13] J. Dennis and R. Schnabel. Numerical Methods for Unconstrained Optimization and Nonlinear Equations. Society for Industrial Mathematics, 1996.",0,,False
442,"[14] H. Fang and C. Zhai. Probabilistic models for expert finding. In ECIR, 2007.",0,,False
443,"[15] Y. Fang, L. Si, and A. Mathur. Ranking experts with discriminative probabilistic models. In SIGIR Workshop on Learning to Rank for Information Retrieval, 2009.",0,,False
444,"[16] Y. Fu, W. Yu, Y. Li, Y. Liu, M. Zhang, and S. Ma. THUIR at TREC 2005: Enterprise track. In TREC-14, 2006.",1,TREC,True
445,"[17] N. Fuhr. Probabilistic models in information retrieval. The Computer Journal, 35(3):243, 1992.",0,,False
446,"[18] T. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225­331, 2009.",0,,False
447,"[19] T. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR Workshop on Learning to Rank for Information Retrieval, 2007.",0,,False
448,"[20] C. Macdonald, D. Hannah, and I. Ounis. High quality expertise evidence for expert search. In ECIR, 2008.",0,,False
449,"[21] C. Macdonald and I. Ounis. Voting for candidates: adapting data fusion techniques for an expert search task. In CIKM, 2006.",1,ad,True
450,"[22] D. Metzler and W. Bruce Croft. Linear feature-based models for information retrieval. Information Retrieval, 10(3):257­274, 2007.",0,,False
451,"[23] R. Nallapati. Discriminative models for information retrieval. In SIGIR, 2004.",0,,False
452,"[24] A. Ng and M. Jordan. On discriminative vs. generative classifiers: a comparison of logistic regression and naive bayes. NIPS, 2002.",0,,False
453,"[25] D. Petkova and W. Croft. Proximity-based document representation for named entity retrieval. In CIKM, 2007.",0,,False
454,"[26] S. Robertson. The probability ranking principle in IR. Journal of documentation, 33(4):294­304, 1977.",0,,False
455,"[27] S. Robertson and K. Jones. Relevance weighting of search terms. JASIST, 27(3):129­146, 1976.",0,,False
456,"[28] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-4. In TREC-4, 1996.",1,TREC,True
457,"[29] P. Serdyukov and D. Hiemstra. Being omnipresent to be almighty: The importance of the global web evidence for organizational expert finding. In SIGIR Workshop on Future Challenges in Expertise Retrieval, 2008.",0,,False
458,"[30] P. Serdyukov and D. Hiemstra. Modeling documents as mixtures of persons for expert finding. In ECIR, 2008.",0,,False
459,"[31] P. Serdyukov, H. Rode, and D. Hiemstra. Modeling multi-step relevance propagation for expert finding. In CIKM, 2008.",0,,False
460,"[32] I. Soboroff, A. de Vries, and N. Craswell. Overview of the trec-2006 enterprise track. In TREC-14, 2006.",1,trec,True
461,"[33] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A language model-based search engine for complex queries. In International Conference on Intelligence Analysis, 2004.",0,,False
462,"[34] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: Extraction and mining of academic social networks. In SIGKDD, 2008.",1,ad,True
463,"[35] D. Tax, M. Van Breukelen, R. Duin, and J. Kittler. Combining multiple classifiers by averaging or by multiplying? Pattern recognition, 33(9):1475­1485, 2000.",0,,False
464,"[36] D. Yimam-Seid and A. Kobsa. Expert finding systems for organizations. Sharing Expertise: Beyond Knowledge Management, 2003.",0,,False
465,"[37] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. TOIS, 22(2):214, 2004.",0,,False
466,"[38] J. Zhu, X. Huang, D. Song, and S. Ruger. Integrating multiple document features in language models for expert finding. Knowledge and Information Systems, pages 1­26.",0,,False
467,690,0,,False
468,,0,,False

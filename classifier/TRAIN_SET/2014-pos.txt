Incorporating Query-Specific Feedback into Learning-to-Rank Models

Ethem F. Can, W. Bruce Croft, R. Manmatha
Center for Intelligent Information Retrieval (CIIR) School of Computer Science UMass Amherst
efcan, croft, manmatha@cs.umass.edu

ABSTRACT
Relevance feedback has been shown to improve retrieval for a broad range of retrieval models. It is the most common way of adapting a retrieval model for a specific query. In this work, we expand this common way by focusing on an approach that enables us to do query-specific modification of a retrieval model for learning-to-rank problems. Our approach is based on using feedback documents in two ways: 1) to improve the retrieval model directly and 2) to identify a subset of training queries that are more predictive than others. Experiments with the Gov2 collection show that this approach can obtain statistically significant improvements over two baselines; learning-to-rank (SVM-rank) with no feedback and learning-to-rank with standard relevance feedback.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms
Algorithms, Experimentation
Keywords
Learning-to-rank, query-specific feedback, relevance feedback
1. INTRODUCTION
Consider a case where there are navigational and history related queries available in the training set. For a history related test query, a retrieval model learned using all of the queries in the training set might not be as good as a model learned using queries only related to history. Training queries that are similar to a test query can rank that particular test query better than others.
Relevance feedback methods have been studied and used for some time in information retrieval. Feedback is either explicit (i.e., where the user provides the relevance information for some retrieved items of the test query) or implicit (i.e.,
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. ACM 978-1-4503-2257-7/14/07. http://dx.doi.org/10.1145/2600428.2609503 .

where the top ranked documents are assumed to be relevant). In both cases, the main idea is to use information from the initial search to improve retrieval performance. Relevance feedback is performed using the top retrieved documents to improve the retrieval by modifying the initial retrieval model. In this work we focus on situations where explicit feedback is available from the user for a few top documents. This typically happens when searching for patents or intelligence information where a user may be willing to spend additional time to label the documents from an initial search as relevant/non-relevant to improve the results for the rest of the retrievals. We consider a setting where documents in the top k are assessed, with the goal of improving the the other documents ranking.
In the learning-to-rank framework [9, 17], parameters for a retrieval model are learned based on training data consisting of queries and associated relevant and non-relevant documents. A common approach in learning-to-rank is to use machine learning techniques that optimize a set of weights on joint query-document features to maximize the number of pairs where a relevant document is ranked higher than a non-relevant document. Figure 1 illustrates the learning-torank framework. Q1, ..., Qk are the queries in the training set and Qt is the test query.

Quer y1 (Q1 ) . . . Queryk (Qk )
Quer ytest (Qt )

Q1, d11 Q1, d12 ... Q1, d1m ... Qk , dk1 Qk , dk2 ... Qk , dkm
Qt, dt1 Qt, dt2 ... Qt, dtm

M1
... Mk

M0

Figure 1: Illustration of learning-to-rank framework.
Typically a learning-to-rank approach estimates one retrieval model across all training queries Q1, ..., Qk (represented by feature vectors), after which the test query (Qt) is ranked upon the retrieval model and the output is presented to the user. We extend this approach by an additional step; we refer to the learning-to-rank model which is trained across all queries (Q1, ..., Qk) as the initial retrieval model (M0) and the induced ranking for the test query as initial ranking. We target a situation where partial relevance assessments are available on the initial ranking, for example in the top 10. Our goal is to leverage this user feedback to improve the ranking of unjudged documents in context to the initial rank-

1035

ing. An obvious way of doing so is to learn a learning-to-rank model on the partial judgments M and present a ranking of a combination of the initial model (M0) and partial model (M ) to the user. In this work, we evaluate an alternative approach: we estimate learning-to-rank models Mi for each individual training query Qi and ask "which of the Mi would have predicted the partial annotations the best?" We take this as a similarity measure between the test query Qt and each training query Qi. Coming back to the running example, we expect models of history queries to better predict the partial annotations if the test query is about history as well. Following this intuition, we derive a ranking for the test query as a combination of the most similar learning-to-rank models Mi.
Our approach is related in spirit to Malisiewicz et al. [12]. They create a number of object detectors each of which consists of a positive example and a number of negative ones. They conclude that their approach is better than creating a detector using all of the positive examples together. In another work, McCallum et al. [13] point out that the reduction in the computational cost obtained by dividing the data into overlapping subsets--called canopies--in the context of efficient clustering can be performed without any performance loss.
The main contributions of this paper are as follows: We focus on an approach in which a retrieval model is customized for a given test query by considering the similarity of the test query to the queries available in the training set in the context of learning-to-rank. Our aim here is to improve retrieval results for a given test query by exploiting partial information from an initial search for learning-to-rank problems. We show that our approach provides statistically significant improvements over two baselines; a learning-to-rank (support vector machine (SVM)-rank) baseline with no feedback and a learning-to-rank (SVM-rank) baseline with conventional relevance feedback.
2. RELATED WORK
Here we briefly summarize the related work about modifying learning-to-rank models to improve retrieval. Lv and Zhai [11] point out that the balance parameter of feedback to the original query is static i.e., not adaptive for each query. They propose the idea of adapting the balance variable for each query to increase the retrieval accuracy. When finding the optimal balance parameter for a query and a feedback model, they focus on the following heuristics: 1) discrimination of a query, 2) discrimination of feedback documents, and 3) divergence between a query and feedback documents. Cao et al. [3] indicate that in the training set, the number of relevant documents may vary by query so that the final model moves toward the queries having more relevant documents. They address this issue by adding another parameter to the objective function of the Support Vector Machines (SVM) that balances the effect of the queries on the final model so that they are less biased to the queries having more relevant documents. Zhang et al. [19] focus on a semi-supervised approach to capture the query-specific features such as the unique expansion terms based on the target query in the context of real-time Twitter search. Yue and Joachims [18] focus on finding a retrieval function that is close to the optimal one by formulating it as a bandit algorithm. Hofmann et al. [6] study an online learning-to-rank algorithm that works with implicit feedback as well as balancing exploration and

exploitation. Rather than modifying the objective function in the ranker, our approach develops a better retrieval model using queries from the training set that are similar to the test query.
There has also been work on selecting a portion of the training queries most similar to a test query to create better models in the absence of feedback. Geng et al. [4] essentially find the k-nearest neighbors for the test query and create a new model from these k-nearest neighbors. They also discuss modifications to speedup this process. Publicly available datasets such as OHSUMED [5, 9] only have a small number of training queries. On that dataset, we empirically show that this approach does no better than a baseline SVM-rank algorithm. Geng and colleagues in fact specifically use a private dataset with a large number of queries and mention that it does not do well on datasets such as Letor [16] with a small number of queries. Our technique, on the other hand, uses feedback documents to consider a subset of the training set. Peng et al. [15] and Banarjee at al. [1] use a principled component analysis (PCA) based approach to find the nearest training queries. Our approach is to tailor the retrieval model by selecting a number of queries to create a model. There have been some attempts to do this [1, 4]. The motivation in this case was to find queries of a similar type (e.g., navigational or information queries), but no improvements were observed with smaller training sets such as Letor.
3. APPROACH
In this paper, our main focus is to improve the retrieval performance for a given test query exploiting explicit relevance feedback information for that particular query. In addition to using feedback documents to modify the retrieval model, we also use the identified feedback documents to decide which of the training queries are most similar to the test query and consider those for retrieval. To decide which queries are most similar, we first create individual models M1, ..., Mk using the queries Q1, ..., Qk in the training set (e.g., one model per query in the training set). Each of these queries is tested against the feedback documents as measured in normalized discounted cumulative gain (NDCG). A model Mi that achieves a higher NDCG score on the partial judgments for the test query is likely to also provide a better ranking for the remaining documents. We assume that the query Qi that was used to train Mi with high NDCG is more similar to the test query than queries of models with low NDCG.
Having identified the most similar queries for a test query, we use these queries and the feedback documents to develop a better retrieval model. We make use of the prediction scores obtained from the individual models (a test query Qt is run against the individual models Mi) and partial model M is created using the feedback documents. We calculate the final scores to rank documents with Equation 1. Sfinal is the final score of a document in the test query for ranking. QS is a subset of the training set containing the queries that are most similar to that particular test query. SQi is the prediction score of a document in the test query against the individual model Mi of query Qi in the training set. |QS| is the number of queries in the QS set where S is the score obtained from a partial model M using the feedback documents.

1

Sfinal

=

|QS

|

+

1

((
Qi QS

SQi

)

+

S

)

(1)

1036

4. EXPERIMENTAL DESIGN

Here we provide information about the datasets used in

this study, how to perform feature normalization, and the

evaluation technique.

Datasets: We focus on the Gov2 dataset that has 25

million documents and 150 queries [14]. In Gov2 there are

three relevance ratings, which are 0, not relevant; 1, partially

relevant; and 2, relevant. In order to represent document-

query pairs, we first retrieve the documents for a given

query using the query likelihood model. Then, we focus on

the top ranked 1,000 documents for that particular query

and extract the features (e.g., low-level content features,

high-level content features, and document quality features)

defined by Bendersky et al. [2] for each document. The total

number of features extracted is 102 and these features are a

superset of the features defined by Liu [10]. In order to have

a fair comparison among the methods, we only focus on the

queries with more than zero and less than ten non-relevant

documents in the feedback documents.

We also remove queries without any relevant documents in

the test set. There are approximately 42 queries left in each

fold after removing such queries. We also analyze the results

of our approach on a different dataset; OHSUMED [5] which

is also available in Letor [16]. There are 106 queries in the

collection split into five folds.

Feature Normalization: Absolute values of a feature for

different queries might be in different ranges. For this reason,

we perform query-based normalization for each feature [9].

Each feature (xi) in each document is normalized considering

other documents in the same query:

xi -min(xi ) max(xi )-min(xi )

where

(min(xi)) and (max(xj)) are the minimum and maximum

values respectively of xi for all documents in the same query.

Evaluation and Learning: We split the dataset into

three folds each of which consists of a training, a validation,

and a test set. We use triple fold cross validation so that

each fold becomes a training, a validation, and a test set

once. For each fold, we use the validation set to tune the

parameters of the ranker. We focus on SVM-rank [7, 8]

with linear kernel as a learning-to-rank approach. In order

to compute the evaluation scores, we make use of the tools

provided in (research.microsoft.com/~letor/) [9]. When

we report the results for MAP and meanNDCG, we take the

average across the folds. The evaluation of our approach and

the baselines are based on the documents that are not part

of the feedback documents.

5. EXPERIMENTS
In this section, we first explain the baselines that we use to compare our approach. Then we detail the experimental results and discuss the efficiency of our approach.
5.1 Baselines
In order to compare the effectiveness of our approach, we focus on two baselines. The first one considers a learningto-rank model with no relevance feedback. We also consider another baseline where we apply standard relevance feedback to learning-to-rank models using partial ground truth in top 10 initial ranking. In order to perform relevance feedback, we combine the initial model M0 and another model M using the feedback documents (we assume that we know the relevance judgments of these documents) to improve the retrieval in the context of learning-to-rank. According to

our validation experiments, this linear combination provides better performance than methods in which the retrieval model is re-trained after inserting the feedback documents to the training set.
As discussed previously, Gent et al. [4] propose a k-nearest neighbor technique for selecting the most similar queries to the test set without feedback. We also consider their method yet another baseline to compare our approach.
5.2 Results and Discussion
We disregard a number of the most dissimilar queries to the test query and consider a subset from the training set so that it provides a better model while ranking the documents. In the experiments, we identify 5%, 10%, 25%, 50%, and 75% of the most dissimilar queries and consider the rest of the queries in the training set (i.e., 95%, 90%, 75%, 50%, and 25% of the most similar queries) to the test query. Consider an example where the relative similarities of queries Q1, Q2, Q3, and Q4 to the test query Qt are as follows; Q2 > Q1 > Q4 > Q3. In other words the NDCG result for M2 (a model created using only Q2 in the training set) is larger than the NDCG for M1. In the "50%" case, Q4 and Q3 are the most dissimilar queries. For the "25%" case, the query Q3 is the most dissimilar query to the test query.
We re-implement the approach of Geng et al. [4] and test on the OHSUMED dataset for different k values. According to the experimental results k = 20 provides the best MAP and meanNDCG results for the OHSUMED dataset. It turns out that their approach (MAP 44.11% and a meanNDCG score of 51.09%) is not better than our SVM-rank baseline (MAP 44.75% and meanNDCG 53.45%). (note in this particular case, since there is no feedback we use all documents ­including the top-10­ to compute the numbers). Given that their approach is not better than our SVM-rank (without relevance feedback) baseline, we only provide the SVM-rank baseline without relevance feedback and with relevance feedback in comparison to our approach.
Table 1 shows MAP and meanNDCG scores for the baselines as well as the scores of our approach. First two rows (SVM-rank w/o RF and SVM-rank w/ RF) provide the results for the baselines on the Gov2 dataset and the rest shows the results of our approach in the same dataset. We ignore 5%, 10%, 25%, 50%, and 75% of the most dissimilar queries from the training set. We report retrieval effectiveness in terms of MAP and NDCG in Table 1.

Table 1: Gov2 results on our approach (5%, 10%, 25%, 50%, and 75% ) as well as the baselines (SVMrank w/o RF and SVM-rank w/ RF). Method: 5%, 10%, 25%, 50%, and 75% of the most dissimilar queries are ignored from the training set based on the partial information out of the test query.

Method

MAP meanNDCG

SVM-rank w/o RF 0.3761 0.5875 SVM-rank w/ RF 0.3825 0.5965

5%

0.3910 0.5981

10%

0.3933 0.6005

25%

0.3984 0.6068

50%

0.4006 0.6116

75%

0.3960 0.6073

1037

Figure 2: MAP and meanNDCG scores for the OHSUMED dataset.
We obtain the best results when we ignore half of the most dissimilar queries from the training set (see Table 1). The results for the 25% case are very close to the 50% case. When more queries are ignored, some number of similar queries are most likely ignored as well. Likewise, when we ignore fewer queries, then we will leave some number of dissimilar queries in the training set. Our best scores (obtained for the 50% case) provide statistically significant improvements ( < 0.05) over the baseline without relevance feedback (SVM-rank w/o RF in Table) in all of the folds for MAP and meanNDCG, and for majority of the folds for MAP and meanNDCG when we consider the baseline with relevance feedback (SVM-rank w/ RF in Table). When we compare two baselines (i.e., a learning-to-rank (SVM-rank) baseline with no feedback, and a learning-to-rank baseline with relevance feedback), relevance feedback shows improvements over the case where no feedback is applied.
Analyzing the results on the OHSUMED [5] dataset; in Figure 2 with respect to MAP and meanNDCG, we also observe a very similar pattern to the Gov2 set. Ignoring 50% of the dissimilar queries from the training set yields the best results in terms of MAP and meanNDCG. We evaluate our approach for the case where different numbers of documents are available for training and test queries. The number of documents sub-sampled to create learning-to-rank representations are much lower in the OHSUMED case than the Gov2 case (1000 vs. approx. 200).
In our approach, we create individual models for the queries in the training set. However, this process can be completed offline since the training set is available ahead of time and the queries in the training set are independent of the test query. The operations such as creation of a model using the feedback documents and testing feedback documents against individual models should be performed online. However, these processes can be completed in a couple of milliseconds since we only use a small number of feedback documents (10 in our experiments).
6. CONCLUSION
In this work we explore a query-specific modification of learning-to-rank approaches. We make use of the feedback documents to improve a retrieval model using queries in the training set that are similar to a particular test query. We evaluate our approach on the Gov2 and OHSUMED dataset with two baselines; a learning-to-rank (SVM-rank) baseline with no relevance feedback and a learning-to-rank (SVMrank) baseline with standard relevance feedback using partial ground truth. In our experimental evaluation, we obtain statistically significant improvements using our approach over such baselines by exploiting the partial information of a particular test query.
Several future directions are promising. First, we would like to study the effect of the number of available feedback

judgments. Next, we target query-based similarity functions to avoid the need for feedback judgments.
7. ACKNOWLEDGEMENTS
This work was supported in the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. I would like to thank Laura Dietz for her valuable comments.
8. REFERENCES
[1] S. Banarjee, A. Dubey, J. Machchhbar, and S. Chakrabarti. Efficient and accurate local learning for ranking. In Learning-to-rank Worshop SIGIR, 2009.
[2] M. Bendersky, W. B. Croft, and Y. Diao. Quality biased ranking of web documents. In WSDM, 2011.
[3] Y. Cao, J. Xu, T.-Y. Liu, Y. Huang, and H.-W. Hon. Adapting ranking SVM to document retrieval. In SIGIR, 2006.
[4] X. Geng, T. Liu, T. Qin, A. Arnold, H. Li, and S. H. Query dependent ranking using k-nearest neighbor. In SIGIR, 2008.
[5] W. Hersh, C. Bukley, T. Leone, and D. Hickman. Ohsumed: An interactive retrieval evaluation and new large test collection for research. In SIGIR, 1994.
[6] K. Hoffmann, S. Whiteson, and M. Rijke. Balancing exploration and exploitation in learning to rank online. In ECIR, pages 251­263, 2011.
[7] T. Joachims. Optimizing search engines using clickthrough data. In KDD, 2002.
[8] T. Joachims. Training linear SVMs in linear time. In KDD, 2006.
[9] T. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmarking learning to rank for information retrieval. In SIGIR, 2007.
[10] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in IR, 3(3):225­331, 2009.
[11] Y. Lv and C. Zhai. Adaptive relevance feedback in information retrieval. In CIKM, pages 255­264, 2009.
[12] T. Malisiewicz, A. Gupta, and A. Efros. Ensemble of Exemplar-Svms for object detection and beyond. In ICCV, 2011.
[13] A. McCallum, K. Nigam, and L. H. Ungar. Efficient clustering of high-dimensional data sets with application to reference matching. In KDD, 2000.
[14] D. Metzler, T. Strohman, H. Turtle, and W. B. Croft. Indri at trec 2004: Terabyte track. Technical report, DTIC Document, 2004.
[15] J. Peng, C. Macdonald, and I. Ounis. Learning to select a ranking function. In ECIR, pages 114­126, 2010.
[16] T. Qin, T.-Y. Liu, J. Xu, and H. Li. Letor: A benchmark collection for research on learning to rank for IR. Information Retrieval, 13(4):346­374, 2010.
[17] A. Trotman. Learning to rank. Information Retrieval, 8(3):359­381, 2005.
[18] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In ICML, 2009.
[19] X. Zhang, B. He, T. Luo, and B. Li. Query-biased learning to rank for real-time twitter search. In CIKM, 2012.

1038

Assessing the Reliability and Reusability of an E-Discovery Privilege Test Collection

Jyothi K. Vinjumur
iSchool University of Maryland College Park, MD USA
jyothikv@umd.edu

Douglas W. Oard
iSchool and UMIACS University of Maryland College Park, MD USA
oard@umd.edu

Jiaul H. Paik
UMIACS University of Maryland College Park, MD USA
jiaul@umd.edu

ABSTRACT
In some jurisdictions, parties to a lawsuit can request documents from each other, but documents subject to a claim of privilege may be withheld. The TREC 2010 Legal Track developed what is presently the only public test collection for evaluating privilege classification. This paper examines the reliability and reusability of that collection. For reliability, the key question is the extent to which privilege judgments correctly reflect the opinion of the senior litigator whose judgment is authoritative. For reusability, the key question is the degree to which systems whose results contributed to creation of the test collection can be fairly compared with other systems that use those privilege judgments in the future. These correspond to measurement error and sampling error, respectively. The results indicate that measurement error is the larger problem.
Categories and Subject Descriptors
H.3.4 [Information Storage & Retrieval]: Systems & Software-performance evaluation
General Terms
Measurement, Performance, Experimentation.
Keywords
Evaluation, sampling, measurement error
1. INTRODUCTION
In civil litigation, the disclosure of documents that could have been withheld on the basis of attorney-client privilege or attorney work-product doctrine is an important concern for litigants. As a result, it is common for every document that is responsive to a counterparty request to be subjected to a manual review to determine whether a claim of privilege can be made. It is not uncommon for there to be tens of thousands of responsive documents, making the cost of this
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. .

manual review for privilege quite high. Lawyers are naturally reluctant to trust automated techniques because failing to withhold even a small set of privileged documents might be consequential. Thus evaluation techniques that make it possible to compare alternative approaches to privilege classification are needed.
Evaluation of information retrieval systems relies on test collections in which relevance judgments can affordably be created for only a small portion of the collection [4]. One way of selecting these documents is to focus on documents found by the systems that are to be compared. This approach, known as pooling, has been widely used in the Text Retrieval Conference (TREC) and elsewhere. Two concerns arise. The first, which we call reliability, is that different assessors may reach different judgments for the same document. Voorhees has shown that absolute measures of effectiveness are sensitive to this effect but that relative comparisons between systems are relatively insensitive to interassessor disagreement [5].
A second concern, reusability, is that new systems will generally find some documents that did not contribute to the pool, and assuming such documents not to be relevant might adversely affect even relative comparisons. Reusability is important because reusable test collections allow the cost of relevance judgments to be amortized over future uses of a test collection. Reusability of pooled judgments was examined by Zobel [10], who found that TREC pooling had likely found no more than half of the relevant documents, but that relative comparisons remained reliable. Buckley et al. [1] later highlighted a key limitation of that conclusion, finding that when distinctive systems had contributed to the pool, removing one such system could yield a substantial adverse effect on measurements of mean average precision. One way to partially address this concern, introduced by Yilmaz and Aslam, is to sample the documents to be judged from the full collection and then to estimate the evaluation measure from the sampled judgments [8, 9].
Random samples drawn from very large collections yield confidence intervals that are so large as to be uninformative, so in this paper we focus on the sampling design used in the interactive task of the TREC Legal Track, in which set intersections were used as a basis for stratification [3]. Between 2006 and 2011, the TREC Legal Track created relevance judgments for more than 100 topics (which in e-discovery are called "production requests"). In 2010, this was augmented by the world's first (and to date only) shared-task evaluation of privilege classification [2]. In this paper, we study the reliability and reusability of the resulting priv-

1047

ilege test collection. When working with Legal Track test collections we need to think a bit differently about reliability and reusability. For reliability, we are interested not just in relative comparisons, but also in the reliability of absolute measures of effectiveness, and most particularly in estimates of recall. Point estimates from samples are (in expectation) insensitive to sample size, so characterizing the reusability of stratified samples requires comparing confidence intervals for systems that did and didn't contribute to the stratification.1 What we call reliability thus corresponds to the statistical concept of measurement error, reusability to the statistical concept of sampling error.
2. A PRIVILEGE TEST COLLECTION
The privilege task in the 2010 TREC Legal Track2 requested "all documents or communications that are subject to a claim of attorney-client privilege, work-product, or any other applicable privilege or protection ..." Although privilege classification is normally performed as a second pass after classification for relevance, nothing in the definition of privilege is specific to any litigated matter. The collection to be searched was the EDRM Enron collection, version 2, which is a collection of Enron email messages for which text extracted from attachments is provided with the collection. Following the practice for privilege review in e-discovery, the items to be classified were "document families," in this case a family was defined as an email message together with all of its attachments.3
2.1 Stratified Sampling
Two teams (A and H)4 submitted system results (runs) for the TREC 2010 privilege classification task: Team A submitted four runs (a1, a2, a3, a4); Team H submitted one (h1). Each run was a binary assignment of families to one of two classes: privileged or not privileged. Following TREC convention, we refer to these five runs as participating systems; each run was produced by people and machines working together (TREC refers to this as interactive task). The collection was partitioned into 32 strata, each defined by a unique 5-bit vector (e.g., 01010 for the stratum containing families runs a1, a3, and h1 classified as not privileged and runs a2 and a4 classified as privileged) [2]. The 00000 stratum included 398,233 of the 455,249 families (87% of the collection), but only 3,275 of the 6,766 samples (48%) were allocated to that stratum. The resulting sampling rate for the 00000 stratum (0.8%) was far sparser than for any other stratum (which averaged 6.1%). The allocation of samples was a bit denser for smaller strata since a 6.1% sampling rate might otherwise result in very few samples being drawn. Few samples were allocated to these very small strata in aggregate, so the sampling rate remained above 6% for every stratum other than the 00000 stratum.
2.2 Privilege Assessment
First-tier privilege assessors (henceforth, assessors), who were lawyers employed by a firm whose business included provision of document review services for e-discovery, were
1Thanks to William Webber for pointing this out. 2For bookkeeping purposes, the (non-topical!) privilege task was "topic 304." 3Use of families is referred to as "message" evaluation in [2]. 4In [2] Team A was called CB, team H was called IN.

Table 1: TA adjudication rates.

Category

Assessed Adjudicated

Random sample

6,766

223

Team appeal

6,766

237

Assessor disagreement 730

76

Rate 3.3% 3.5% 10.8%

provided with detailed guidelines written by a senior attorney (the Topic Authority (TA)). Assessors recorded ternary judgments: privileged, not privileged, or unassessable (e.g., for display problems, foreign-language content, or length). As expected, assessors sometimes made judgments that disagreed with the TA's conception of privilege. For other tasks, differing judgments might be treated as equally valid, but in e-discovery the TA's judgments are authoritative (because the TA models the senior attorney who will certify that the review was performed correctly). Judgments that disagree with those of the TA are therefore considered incorrect. In TREC 2010, an assessor's judgment regarding whether a family should be classified as privileged could be escalated to the TA for adjudication in three ways. First, a team might appeal the decision of an assessor to the TA. A total of 237 such appeals were received. Of course, teams might not as easily notice, nor would they rationally appeal, assessor errors that tended to increase their estimated classification accuracy. In particular no team would rationally appeal an erroneous assessor judgment of privileged in the 11111 stratum, nor an assessment of not privileged in the 00000 stratum. The set of appealed judgments is thus biased [7]. To create an unbiased sample, 223 assessor judgments were thus independently drawn using simple random sampling. Since this is a random sample of a stratified sample, it results in a smaller stratified sample of the full collection. To facilitate symmetric comparisons among assessors, a second simple random sample containing 730 families was drawn and each family in that sample was duplicated in the set of families to be assessed. This was done in a manner that had been expected to result in the duplicated families being assigned to different assessors.5 When conflicting assessments for a family were received, the judgment was adjudicated. Table 1 summarizes the selection process.
2.3 Estimation
We focus separately on estimates of recall and precision. To estimate the precision of system Si we need to estimate the number of privileged families that were correctly found by Si. Let Nphr be the number of privileged documents in stratum h. Then

N^pir =

N^phr

(1)

h:Si S h

where Sh is the set of systems that retrieved documents in stratum h, N^phr is the unbiased estimator of Nphr and N^pir is the unbiased estimator of Npir. Let the total number of
families in stratum h be Nh and let the number of families
drawn from stratum h as a simple random sample without replacement be nh. Then an unbiased estimator of Nphr is

N^phr

=

Nh × nhpr nh

(2)

5Some pairs may have been judged by the same assessor.

1048

Recall with Confidence Intervals 0.0 0.2 0.4 0.6

where nhpr is the number of privileged families in the sample of stratum h. In TREC 2010, the TA judgment was used for each adjudicated family; the assessor's judgment was used for all other sampled families. To estimate recall, an estimate of Np (total number of privileged families in the collection), is also needed. An unbiased estimator of Np is

m

N^p = N^ph

(3)

h=1

where Nph is the number of privileged documents in stratum h and m is the number of strata. From these estimates, we can derive point estimates for recall and precision. Point estimates alone are of little use, however, since small samples can yield estimates with large variance. We therefore also calculate two-tailed confidence intervals for recall and precision. A two-tailed confidence interval is a range of values (Recalllower, Recallupper) within which the recall of system Si lies with 1 -  confidence. Webber observes that the beta-binomial with hyper-parameters  =  = 0.5 provides accurate estimates of confidence intervals [6]. We therefore compute distributions over the number of privileged families retrieved and not retrieved by Si in each stratum to obtain the beta-binomial posterior. Once the posterior for each stratum is obtained, the yield Nphr for each stratum is simulated numerically. A large number of beta-binomial Monte Carlo simulations with 40,000 draws are performed, and the confidence intervals are obtained.

3. RESULTS
Here we analyze the reliability and reusability of the TREC 2010 Legal Track privilege task test collection.
3.1 Analysis of Measurement Error
The use of assessor judgments for families that the TA had not adjudicated would be reasonable if the appeal process had identified most of the assessor errors. This is a testable hypothesis. Although the TA might also make errors, we ignore that factor because we believe its effect to be small. We therefore treat the TA's judgments as a gold standard. As a further simplification, we treat the small handful of unassessable documents (13 families) as not privileged in our analysis. One way of visualizing the effect of assessor errors is to use only some or all of the families that were selected for adjudication, plotting confidence intervals using TA judgments in one case and using assessor judgments in the other. The adjudicated sample is less than 8% of the size of the full set of official judgments, so this yields fairly large confidence intervals, but the comparison does offer useful insights.
Figure 1 compares the (95%) confidence intervals on recall for each participating system using only the families that were selected for adjudication by the simple random sample; Figure 2 shows a similar comparison using all of the adjudicated families. From Figure 1 we can observe that judgments from assessors yield somewhat higher recall estimates than does the TA, but Figure 2 shows the opposite effect. The difference results from some combination of sampling error, appeals that disproportionately benefit participating systems, or systematic biases in the families on which assessors disagree. As the size of the error bars illustrates, we cannot reject sampling error as an explanation. Nonethe-

Assessors Judgments as gold standard Topic Authority Judgments as gold standard

a1

a2

a3

a4

h1

System/Run

Figure 1: Recall, a4 ablated, random adjudication

Assessors Judgments as gold standard Topic Authority Judgments as gold standard

Recall with Confidence Intervals 0.0 0.2 0.4 0.6

a1

a2

a3

a4

h1

System/Run

Figure 2: Recall, a4 ablated, all adjudication

less, there is some evidence to support the hypothesis that appeals disproportionately benefit participating systems.
Table 2 shows how the overturn rate varies with the reason for adjudication and with the original judgment. As the random sampling results show, assessors are more likely to mistakenly judge a family as privileged than as not privileged. Specifically, a z-ratio test for independent proportions finds the 1  0 overturn to be significantly more likely than a 0  1 overturn (p < 0.05). The same is not true for documents appealed by participating teams, however, where the overturn rates in each direction are statistically indistinguishable. Said another way, the increase in total overturn rate from 23% to 36% between randomly sampled adjudications and appealed adjudications (a 58% relative increase) can be largely explained by participating teams being no better than chance at recognizing an assessor's false positive judgments, but by being much better than chance at recognizing an assessor's false negative judgments.
The implications of this for the reliability of the test collection are clear: estimating absolute measures, and particularly absolute estimates of recall, using assessor judgments that exhibit systematic errors results in estimates that are open to question. If uncorrected assessor judgments were a small fraction of the total judgments, this would be a relatively minor concern, but uncorrected judgments are being used for about 92% of the sampled families. On the positive side, the availability of adjudicated random samples offers the potential for modeling differential error rates conditioned on the first-tier assessor's judgment. On the negative side, the inability to associate judgments with individual assessors in TREC 2010 means that such corrections can only be applied on an aggregate basis. We note, however, that relative comparisons between participating systems can still be informative, so long as assessor errors penalize all participating systems similarly.

1049

Table 2: Overturn rates Assessor  Topic Authority

Adjudication Basis

01

10

Random sample 31 of 161 (19%) 20 of 62 (32%)

Team appeal

32 of 77 (42%) 54 of 160 (34%)

Disagreement

28 of 49 (57%) 9 of 27 (33%)

After ablation Before Ablation

Precision with Confidence Intervals 0.1 0.2 0.3 0.4 0.5 0.6

a1

a2

a3

a4

h1

System/Run

Figure 3: Precision, a4 ablated, all adjudication

3.2 Analysis of Sampling Error
To assess reusability, we need to assess the comparability of evaluation results for systems that did and did not contribute to the development of the test collection. A standard way of performing such analyses is through system ablation [10]: removing a system that in fact did participate in the stratification and then rescoring all systems, including the ablated system, and observing the effect on system comparisons. With pooling, ablation results in removing judgments for documents that were uniquely found by one system. With stratified sampling, by contrast, ablation results in re-stratification. For example, when system a4 (the participating system with the highest recall) is ablated, the 00000 stratum and the 00010 stratum become merged into a 000?0 stratum (where ? indicates a don't-care condition), the 11001 stratum gets merged with the 11011 stratum to form a 110?1 stratum, and similarly for each other stratum pair that is differentiated only by the ablated system. If we then reapply the process for deciding on the number of families to sample from each merged stratum, we will see little effect on the sampling rate for most strata. The one important exception is the 000?0 stratum (continuing with our example of ablating system a4), where we are merging large strata with quite different sampling rates (very small strata can also see substantial changes in their sampling rate, but their effect on the overall estimate will be small). We therefore model the effect of ablation by allocating all of the samples in each pair of strata to the corresponding merged stratum, adjusting the contributions of each sample to the estimate of the yield for the merged stratum to be equal.
To generalize, let a refer to the stratum in the pair including families classified as privileged by the ablated run, b to the corresponding stratum containing families classified as not privileged by the ablated run, and c to the merged stratum. We assume that the merged stratum would include the same number of samples that the two original strata contained separately; that is nc = na +nb and the sampling rate for merged stratum c is pc = nc/Nc, where Nc = Na + Nb.
We performed three ablation experiments, in each case ablating one system with high, medium or low recall and

then recalculating point estimates and confidence intervals for every system. Comparing post-ablation to pre-ablation results, we see that point estimates are unchanged, as expected, but as Figure 3 shows confidence intervals for precision increase for the ablated system (system a4 in this figure). We attribute this to the reductions in the sampling rate for the 00010 stratum (from merging with the 00000 stratum, which results in documents in the former 00010 stratum being sampled at a far lower rate), since we expect families classified uniquely by any reasonable system as privileged to more often actually be privileged than families that no system classified as privileged. The same pattern is evident in our other two ablation experiments (ablating systems a2 or h1; not shown). No similar effect was observed for confidence intervals on recall, however, perhaps because the estimates for the retrieved set contribute to both the numerator and the denominator of the recall computation.
4. CONCLUSION
We have explored set-based evaluation for privilege classification using stratified sampling, with strata defined by the overlapping classification results from different participating systems. We have characterized reliability by examining the impact of unmodeled assessor errors on evaluation results, and we have characterized reusability by showing that confidence intervals are affected when we reconstruct the test collection in a way that does not rely on the contributions of one participating system. We found that assessor errors do adversely affect absolute estimates of recall, and we have suggested future work on statistical correction for the effect of those errors. Confidence intervals for precision increased noticeably when we ablate a system, but no comparable effect was noted for recall. Since recall is the more important measure in e-discovery, this is a promising result.
5. ACKNOWLEDGMENTS
This work has been supported in part by NSF award 1065250. Opinions, findings, conclusions & recommendations are those of the authors and may not reflect NSF views.
6. REFERENCES
[1] C. Buckley et al. Bias and the limits of pooling for large collections. Information Retrieval, 10(6), 2007.
[2] G. Cormack et al. Overview of the TREC 2010 legal track. In TREC, 2010.
[3] D. Oard et al. Overview of the TREC 2008 legal track. In TREC, 2008.
[4] K. Sp¨arck Jones et al. Information retrieval test collections. Journal of Documentation, 32(1), 1976.
[5] E. Voorhees. Variations in relevance judgments & the measurement of retrieval effectiveness. IP&M, 2000.
[6] W. Webber. Approximate recall confidence intervals. Transactions on Information Systems, 31(1), 2013.
[7] W. Webber et al. Assessor error in stratified evaluation. In CIKM, 2010.
[8] E. Yilmaz et al. Estimating average precision with incomplete and imperfect judgments. In CIKM, 2006.
[9] E. Yilmaz et al. A simple and efficient sampling method for estimating AP & NDCG. In SIGIR, 2008.
[10] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In SIGIR, 1998.

1050

A Syntax-Aware Re-ranker for Microblog Retrieval

Aliaksei Severyn
DISI, University of Trento
severyn@disi.unitn.it

Alessandro Moschitti
QCRI, Qatar
amoschitti@qf.org.qa

Manos Tsagkias
University of Amsterdam
e.tsagkias@uva.nl

Richard Berendsen
University of Amsterdam
r.w.berendsen@uva.nl

Maarten de Rijke
University of Amsterdam
derijke@uva.nl

ABSTRACT
We tackle the problem of improving microblog retrieval algorithms by proposing a robust structural representation of (query, tweet) pairs. We employ these structures in a principled kernel learning framework that automatically extracts and learns highly discriminative features. We test the generalization power of our approach on the TREC Microblog 2011 and 2012 tasks. We find that relational syntactic features generated by structural kernels are effective for learning to rank (L2R) and can easily be combined with those of other existing systems to boost their accuracy. In particular, the results show that our L2R approach improves on almost all the participating systems at TREC, only using their raw scores as a single feature. Our method yields an average increase of 5% in retrieval effectiveness and 7 positions in system ranks.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval
Keywords
Microblog search; semantic modeling; re-ranking
1. INTRODUCTION
Social media has become part of our daily lives, and is increasingly growing into the main outlet for answering various information needs, e.g., the query, Facebook privacy, may be answered by the following tweet: Facebook Must Explain Privacy Practices to Congress http://sns.ly/2Qbry7. Such queries have proven difficult to answer with a single retrieval model, and lead to models that learn to combine a large number of rankers. Learning to rank (L2R) methods have been shown to improve retrieval effectiveness and they have recently been used for ranking short documents from social media. However, L2R suffers from an important drawback: different training data is needed for different applications. The required amount of training data critically depends on the task being tackled and the quality of the used text representations, e.g., lexical features are less powerful than search engine scores or other metafeatures. Optimal representations require considerable effort to be
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '14, July 06­11, 2014, Gold Coast, QLD, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ... $15.00. http://dx.doi.org/10.1145/2600428.2609511 .

designed and implemented. Hence, flexible and adaptable features can be valuable for rapid and effective designs of L2R systems.
Previous work has shown that one source of more adaptable features comes from structural relations between object pairs [6], which in the case of text mainly refers to its syntactic structure. Unfortunately, the latter is subject to errors when it is automatically generated. This problem is exacerbated when we deal with informal and unedited text typically prominent in social media. Most importantly, it is not clear which part of the structure should be considered to design effective features.
We tackle the problems noted above in the context of recent TREC Microblog retrieval tasks by proposing relational shallow syntactic structures to represent (query, tweet) pairs. Instead of trying to explicitly encode salient features from syntactic structures, we opt for a structural kernel learning framework, where the learning algorithm operates in rich feature spaces of tree fragments automatically generated by expressive tree kernel functions.
The following characterizes our approach: (i) it uses shallow syntactic parsers developed for social media, which are robust and shown to be accurate in such domains; (ii) tree kernels implicitly generate all possible tree fragments, thus all of them are used as features by the learning algorithm, solving the problem of engineering task-specific features. We design experiments using the 2011 and 2012 editions of the TREC Microblog track to verify the following: (i) relational syntactic features produced by a shallow syntactic parser are effective for L2R; (ii) our automatic feature engineering approach based on structural kernels is accurate and produces general features, which are complementary to those typically used in L2R models; and (iii) our structural representations can easily be combined with existing systems to boost their accuracy.
Our results show that employing relational syntactic structures improves on almost all the participating systems by only using their raw scores along with our L2R model based on relational syntactic structures. Our method boosts retrieval effectiveness by more than 5% on average and improves the rankings of participating systems by at least 7 positions on average.
2. A SYNTAX-AWARE RE-RANKER
Our syntax-aware re-ranker consists of two components: (i) a syntactic model that encodes tweets into shallow linguistic trees to ease feature extraction, and (ii) a tree kernel learning framework that computes similarities between (query, tweet) pairs. We also define a shallow tree kernel to enable efficient kernel computations.
2.1 A syntactic model for tweets
Our approach to extract features from (query, tweet) pairs goes beyond traditional feature vectors. We employ structural syntactic models (STRUCT) that encode each tweet into shallow syntactic

1067

Figure 1: Shallow tree representation for an example (query, tweet) pair: ("Facebook privacy", "Facebook Must Explain Privacy Practices to Congress http://sns.ly/2Qbry7") (in the original tree we use word stems). Part-of-speech tag ^ refers to a common noun. Note that an additional REL tag links the words (stems) common between the query and the candidate tweet, Facebook and privacy).
trees. The latter are input to tree kernel functions for generating structural features. Our structures are specifically adapted to the noisy tweets and encode important query/tweet relations.
In particular, our shallow tree structure (inspired by [6­8]) is a two-level syntactic hierarchy built from word lemmas (leaves) and part-of-speech tags that are grouped into chunks (Fig. 1). While full syntactic parsers would significantly degrade in performance on noisy texts such as tweets, our choice for shallow structure relies on simpler and more robust components: a part-of-speech (POS) tagger and a chunker. For POS tagging we use the CMU tagger [3] trained on Twitter data and an off-the-shelf OpenNLP chunker.
Fig. 1 provides an example of a candidate (query, tweet) pair each of which is encoded into a shallow linguistic structure. To upweight the tree fragments spanning words that are found in both the query and the tweet we introduce a special REL tag at the level of part-of-speech and chunk nodes. This step is important to generate syntactic patterns that carry additional semantics of sharing common terms between a query and a tweet. To find matching word pairs we lowercase and stem words and use plain string matching.
2.2 Learning
We employ a pointwise approach to re-ranking where a binary classifier is used to learn a model to discriminate between relevant and non-relevant (query, tweet) pairs. The prediction scores from a classifier are then used to re-rank candidates. We define a novel and efficient tree kernel function, namely, Shallow syntactic Tree Kernel (SHTK), which is as expressive as Partial Tree Kernel (PTK) [4] to handle feature engineering over the structural representations of the STRUCT model. For feature vectors we use a linear kernel.
Computing similarity between (query, tweet) pairs. A typical kernel machine classifies a test input example x using the following prediction function: h(x) = i iyiK(x, xi), where i are the model parameters estimated from the training data, yi are target variables, xi are support vectors, and K(·, ·) is a kernel function that computes the similarity between two input objects.
We represent each (query, tweet) pair x as a triple composed of a query tree Tq and a tweet tree Ttw together with a traditional feature vector v, i.e., x = Tq, Ttw, v . Given two (query, tweet) pairs xi and xj, we define the following similarity kernel:
K(xi, xj ) = KTK (Tiq, Tjq) + KTK (Tiq, Tjtw) + KTK (Titw, Tjtw) + KTK (Tjq, Titw) (1) + Kv(vi, vj ),
where KTK computes a tree kernel similarity between linguistic trees and Kv is a kernel over feature vectors. It computes an all-vsall tree kernel similarity between two (query, tweet) pairs.

Shallow syntactic tree kernel. Following the convolution kernel framework, we define the new SHTK function from Eq. 1 to compute the similarity between tree structures. It counts the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. The general equation for Convolution Tree Kernels is: KTK (T1, T2) =
n1NT1 n2NT2 (n1, n2), where NT1 and NT2 are the sets of the nodes in T1 and T2, respectively, and (n1, n2) is equal to the number of common fragments rooted in the n1 and n2 nodes, according to several possible definitions of the atomic fragments.
To speed up the computation of KTK , we consider pairs of nodes (n1, n2) belonging to the same tree level. Thus, given H, the height of the STRUCT trees, where each level h contains nodes of the same type, i.e., chunk, POS, and lexical nodes, we define SHTK as the following:

KSHTK (T1, T2) =

H h=1

n1NTh1 n2NTh2(n1, n2), (2)

where NTh1 and NTh2 are sets of nodes at height h. The above equation can be applied with any  function. To have
a more general and expressive kernel, we use  from PTK, which employs subsequence kernels, thus making it possible to generate child subsets of the two nodes, i.e., it also allows for gaps, which makes matching of syntactic patterns less rigid.
The resulting SHTK is a special case of PTK [4], adapted to the shallow structural representation STRUCT. When applied to STRUCT trees, SHTK computes the same feature space as PTK, but faster (on average). Unlike PTK, where all combinations of node pairs are considered, the kernel definition in (2) constrains the node pairs being considered to be from the same level, i.e., the matched nodes have to be of the same type--chunk, POS or lexicals. Hence, the number of node pairs considered for matching by SHTK is smaller, which results in faster kernel evaluation.
Finally, given its recursive definition in Eq. 2 and the use of subsequences (with gaps), SHTK can derive useful dependencies between its elements. E.g., it will generate the following subtree fragments (in nested parenthesis format):

1. [ROOT [REL-NP[REL-^ facebook]][VP V][REL-NP [REL-N privacy]]]
2. [ROOT [REL-NP[REL-^]][VP V][REL-NP [REL-N]]] 3. [ROOT [REL-NP][VP][REL-NP]] 4. [ROOT [VP[V explain]][NP[N privacy]]].

Subtree 3 generalizes Subtree 2, which, in turn, generalizes Subtree 1. These structures are interesting when paired with the query structure. E.g., given the query pattern, [REL-NP [REL-^][REL-N]], which means a famous proper noun (^) followed by a noun, if the tweet contains Subtree 2, i.e., a famous proper noun (matched in the query) followed by a verbal phrase (VP) and a common noun (also matched in the query), the candidate tweet may be relevant.

3. EXPERIMENTS AND EVALUATION
To evaluate the utility of our structural syntactic re-ranker for microblog search we focus on the 2011 and 2012 editions of the ad-hoc retrieval task at TREC microblog tracks [5, 9]. Our main research question is: Does the use of relational syntactic features produced by our shallow syntactic parser, and the automatic feature engineering approach based on structural kernels lead to improvements in state-of-the-art L2R and retrieval algorithms?
To answer this question, we test our model in two settings. In the first, we re-implement an accurate recent L2R-based approach and add our features alongside its features. This will allow us to see directly if our features are complementary to the other features. We

1068

opted for the L2R approach in [2] ("the UvA model"), because of its comprehensiveness. It uses pseudo-test collections [1] to learn to fuse ten well-established retrieval algorithms and implements a number of query, tweet, and query-tweet features. It is a strong baseline, its performance ranks sixth and 26th in the 2011 and 2012 editions of the microblog track, respectively. In the second setting, we use the participant systems in the TREC microblog task as a black-box, and implement our model on top of them using only using their raw scores (ranks) as a single feature in our model. This allows us to see whether our features add information to the approaches these retrieval algorithms use.
3.1 Experimental setup
Dataset. Our dataset is the tweet corpus used in the TREC Microblog track in both 2011 (TMB2011) and 2012 (TMB2012). It consists of 16M tweets spread over two weeks, and a set of 49 (TMB2011) and 60 (TMB2012) timestamped topics. We minimally preprocess the tweets--we normalize elongations (e.g., sooo  so) and normalize URLs and author ids. For the second set of experiments, we also use the system runs submitted at TMB2011 and TMB2012, which contain 184 and 120 models, respectively.
Training and testing an L2R algorithm. For learning to rank we use SVM-light-TK1 with no parameter tuning. In our first set of experiments, we train on TMB2011 topics, test on TMB2012 topics, and vice versa. In the second set, where we build upon the TREC participant runs, we train our system only on the runs submitted at TMB2011, and test on the TMB2012 runs. We focus on one direction only to avoid training bias, since TMB2011 topics were already used for learning systems in TMB2012.
Feature normalization. When combining our features with those of the UvA model, while training and testing we use the features of the latter model as v in Eq. 1; these features are already normalized. In contrast, we use the output of participant systems as follows. We use rank positions of each tweet rather than raw scores, since scores for each system are scaled differently, while ranks are uniform across systems. We apply the following transformation of the rank r: 1/ log (r + 1). In the training phase, we take the top {10, 20, or 30} systems from the TMB2011 track (in terms of P@30). For each (query, tweet) pair we average the transformed rank over the top systems to yield a single score. This score is then used as a single feature in v from Eq. 1. In the testing phase, for each participant system we want to improve, we use the transformed rank of the (query, tweet) pairs as the single feature in v.
Evaluation. We report on the official evaluation metric for the TREC 2012 Microblog track, i.e., precision at 30 (P@30), and also on mean average precision (MAP). Following [2, 5], we regard minimally and highly relevant documents as relevant and use the TMB2012 evaluation script. For significance testing, we use a pairwise t-test, where and denote significance at  = 0.05 and  = 0.01, respectively. Triangles point up for improvement over the baseline, and down otherwise. We also report the improvement in the absolute rank (R) in the official TMB2012 ranking.
3.2 Results
Table 1 lists the outcome of our first set of experiments, where we use our syntactic features alongside the features of the UvA model. It shows the obtained MAP and P@30 scores when we train on TMB2011 and test on TMB2012 topics, and vice versa. The STRUCT model yields a significant improvement in P@30 and MAP scores on TMB2012 pushing up the system by 15 positions in the official ranking, and making it second best in TMB2011. The
1http://disi.unitn.it/moschitti/Tree-Kernel.htm

Table 1: System performance (P@30, MAP; higher is better) and system rank (R; lower is better) for UvA's L2R system [2] (UvA), our re-implementation (UvA*), and a UVA* system using our STRUCT model (+STRUCT). We report on relative improvement (Impr) and statisical significance against UvA*.

Model

TMB2011

TMB2012

MAP P@30 R MAP P@30 R

UVA UVA

.3880 .4460 6 .2450 .3920 26 .3845 .4456 6 .2467 .3870 28

+ STRUCT .3991 .4571 2 .2683 .4277 13

Change +3.8% +2.6% +4 +8.8% +10.5% +15

result support our claim that learning useful syntactic patterns from noisy tweets is possible and that relational syntactic features generated by our shallow syntactic tree kernel improve over a strong feature-based L2R baseline.
Table 2 reports on the application of our syntax-aware re-ranker on participant systems. It has results for re-ranking runs of the best 30 systems from TMB2012 (based on their P@30 score) when we train our system using the top {10, 20, or 30} runs from TMB2011. Our re-ranker improves P@30 for all systems with a relative improvement ranging from several points up to 10%--about 5% on average. This is remarkable, given that the pool of participants in TMB2012 was large, and the top systems are therefore likely to be very strong baselines. We observe that our syntactic model has a precision-enhancing effect. In cases where MAP drops a bit it can be seen that our model sometimes lowers relevant documents in the runs. It is possible that our model favors tweets with a higher syntactic quality, and that it down-ranks tweets that contain less syntactic structure but are nonetheless relevant. This is an interesting direction for analysis in future work.
Looking at the improvement in absolute position in the official ranking (R), we see that, on average, using our re-ranker boosts the absolute position in the official ranking for top 30 systems by 7 positions. All in all, the results suggest that using syntactic features adds useful information to many state-of-the-art microblog search algorithms.
Finally, using aggregate scores from the best 10, 20 or 30 systems from TMB2011 does not reveal large differences, which suggests that our syntax-aware re-ranker is robust w.r.t. the exact retrieval models used in the training stage.
While improving the top systems from 2012 represents a challenging task, it is also interesting to assess the potential improvement for systems that ranked lower. For this purpose, we select 30 systems from the middle and the bottom of the official ranking. Table 3 summarizes the average improvement in P@30 for three groups of 30 systems each: top-30, middle-30, and bottom-30. We find that the improvement over underperforming systems is much larger than for stronger systems. In particular, for the bottom 30 systems, our approach achieves an average relative improvement of 20% in both MAP and P@30. These results further support our hypothesis that syntactic patterns automatically extracted and learned by our re-ranker can provide an additional benefit for learning to rank methods on microblog data.

4. CONCLUSIONS
To the best of our knowledge, this work is the first to study the utility of syntactic patterns for microblog retrieval. We propose an efficient way to encode tweets into linguistic structures and use kernels for automatic feature engineering and learning. Our experi-

1069

Table 2: System performance on the top 30 runs from TMB2012, using the top 10, 20 or 30 runs from TMB2011 for training.

TMB2012

TOP10

TOP20

TOP30

# runs

MAP P@30

MAP

P@30 R%

MAP

P@30 R%

MAP

P@30 R%

1 hitURLrun3

.3469 .4695 .3307 (-4.7%) .4831 (2.9%)

0 .3378 (-2.6%) .4864 (3.6%)

0 .3328 (-4.1%) .4774 (1.7%)

0

2 kobeMHC2

.3070 .4689 .3029 (-1.3%) .4740 (1.1%)

1 .3065 (-0.2%) .4768 (1.7%)

1 .3037 (-1.1%) .4768 (1.7%)

1

3 kobeMHC

.2986 .4616 .2956 (-1.0%) .4706 (2.0%)

2 .2989 (0.1%) .4734 (2.6%)

2 .2965 (-0.7%) .4718 (2.2%)

2

4 uwatgclrman .2836 .4571 .3010 (6.1%) .4729 (3.5%)

3 .3032 (6.9%) .4729 (3.5%)

3 .2995 (5.6%) .4712 (3.1%)

3

5 kobeL2R

.2767 .4429 .2734 (-1.2%) .4452 (0.5%)

0 .2785 (0.7%) .4514 (1.9%)

0 .2744 (-0.8%) .4463 (0.8%)

0

6 hitQryFBrun4 .3186 .4424 .3102 (-2.6%) .4554 (2.9%)

1 .3145 (-1.3%) .4582 (3.6%)

2 .3118 (-2.1%) .4554 (2.9%)

2

7 hitLRrun1

.3355 .4379 .3200 (-4.6%) .4508 (3.0%)

2 .3266 (-2.7%) .4542 (3.7%)

2 .3226 (-3.9%) .4525 (3.3%)

2

8 FASILKOM01 .2682 .4367 .2827 (5.4%) .4548 (4.1%)

3 .2841 (5.9%) .4525 (3.6%)

3 .2820 (5.2%) .4531 (3.8%)

3

9 hitDELMrun2 .3197 .4345 .3090 (-3.4%) .4446 (2.3%)

4 .3142 (-1.7%) .4458 (2.6%)

4 .3105 (-2.9%) .4424 (1.8%)

4

10 tsqe

.2843 .4339 .2832 (-0.4%) .4435 (2.2%)

5 .2865 (0.8%) .4458 (2.7%)

5 .2836 (-0.3%) .4441 (2.4%)

5

11 ICTWDSERUN1 .2715 .4299 .2873 (5.8%) .4610 (7.2%)

7 .2885 (6.3%) .4576 (6.4%)

7 .2862 (5.4%) .4582 (6.6%)

7

12 ICTWDSERUN2 .2671 .4266 .2809 (5.2%) .4503 (5.6%)

7 .2808 (5.1%) .4508 (5.7%)

7 .2785 (4.3%) .4475 (4.9%)

7

13 cmuPrfPhrE

.3179 .4254 .3159 (-0.6%) .4486 (5.5%)

8 .3190 (0.4%) .4452 (4.7%)

8 .3172 (-0.2%) .4469 (5.1%)

8

14 cmuPrfPhrENo .3198 .4249 .3167 (-1.0%) .4497 (5.8%)

9 .3201 (0.1%) .4480 (5.4%)

9 .3179 (-0.6%) .4486 (5.6%)

9

15 cmuPrfPhr

.3167 .4198 .3117 (-1.6%) .4441 (5.8%) 10 .3154 (-0.4%) .4407 (5.0%)

8 .3130 (-1.2%) .4379 (4.3%)

8

16 FASILKOM02 .2454 .4141 .2725 (11.0%) .4497 (8.6%) 11 .2721 (10.9%) .4497 (8.6%) 11 .2718 (10.8%) .4508 (8.9%) 11

17 IBMLTR

.2630 .4136 .2734 (4.0%) .4424 (7.0%) 10 .2758 (4.9%) .4412 (6.7%) 10 .2734 (4.0%) .4441 (7.4%) 10

18 otM12ihe

.2995 .4124 .2968 (-0.9%) .4333 (5.1%)

7 .3015 (0.7%) .4339 (5.2%)

7 .2969 (-0.9%) .4322 (4.8%)

7

19 FASILKOM03 .2716 .4124 .2861 (5.3%) .4407 (6.9%) 12 .2879 (6.0%) .4469 (8.4%) 14 .2859 (5.3%) .4452 (8.0%) 14

20 FASILKOM04 .2461 .4113 .2584 (5.0%) .4362 (6.1%) 11 .2596 (5.5%) .4322 (5.1%)

9 .2575 (4.6%) .4294 (4.4%)

9

21 IBMLTRFuture .2731 .4090 .2803 (2.6%) .4384 (7.2%) 14 .2830 (3.6%) .4328 (5.8%) 10 .2808 (2.8%) .4311 (5.4%) 10

22 uiucGSLIS01 .2445 .4073 .2574 (5.3%) .4271 (4.9%) 10 .2612 (6.8%) .4260 (4.6%)

9 .2575 (5.3%) .4260 (4.6%)

9

23 PKUICST4

.2786 .4062 .2913 (4.6%) .4537 (11.7%) 18 .2931 (5.2%) .4486 (10.4%) 18 .2909 (4.4%) .4514 (11.1%) 18

24 uogTrLsE

.2909 .4028 .2983 (2.5%) .4282 (6.3%) 12 .3015 (3.6%) .4243 (5.3%)

9 .2977 (2.3%) .4282 (6.3%)

9

25 otM12ih

.2777 .3989 .2807 (1.1%) .4260 (6.8%) 12 .2839 (2.2%) .4232 (6.1%) 10 .2810 (1.2%) .4175 (4.7%) 10

26 ICTWDSERUN4 .1877 .3887 .1995 (6.3%) .4136 (6.4%)

8 .1992 (6.1%) .4164 (7.1%) 10 .1985 (5.8%) .4164 (7.1%) 10

27 uwatrrfall

.2620 .3881 .2829 (8.0%) .4158 (7.1%) 11 .2841 (8.4%) .4136 (6.6%)

9 .2812 (7.3%) .4136 (6.6%)

9

28 cmuPhrE

.2731 .3842 .2792 (2.2%) .4130 (7.5%) 10 .2810 (2.9%) .4164 (8.4%) 12 .2797 (2.4%) .4136 (7.7%) 12

29 AIrun1

.2237 .3842 .2350 (5.1%) .4085 (6.3%)

7 .2359 (5.5%) .4056 (5.6%)

5 .2339 (4.6%) .4102 (6.8%)

5

30 PKUICST3

.2118 .3825 .2320 (9.5%) .4220 (10.3%) 15 .2324 (9.7%) .4181 (9.3%) 14 .2318 (9.4%) .4119 (7.7%) 14

Average

2.4%

5.4%

7.7

3.3%

5.3%

7.3

2.4%

5.0%

7.1

Table 3: System performance for top, middle (mid), and bottom (btm) 30 systems from TMB2012 system ranking and relative improvements using our method trained on top 20 (TOP20) performing systems in TMB2011.

TMB2012 band MAP P@30

TOP20

MAP

P@30

top .2794 .4209 .2876 (3.3%) .4430 (5.3%) mid .2193 .3460 .2461 (12.2%) .3906 (12.9%) btm .1332 .2636 .1626 (22.1%) .3298 (25.1%)

mental findings show that our model: (i) improves in both MAP and P@30 when coupled with the features from a strong L2R baseline; (ii) provides a complementary source of features general enough to improve the best 30 systems from TMB2012; (iii) the performance gains are stable when we use run scores from the top 10, 20 or 30 best systems for learning; and (iv) the improvement becomes larger for underperforming systems achieving an average 20% of relative improvement in MAP and P@30 for bottom 30 systems.
Acknowledgments. This research was partially supported by the Google Europe Doctoral Fellowship Award 2013, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreements nr 288024 (LiMoSINe) and nr 312827 (VOXPol), the Netherlands Organisation for Scientific Research under nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, the Center for Creation, Content and Technology (CCCT), the QuaMerdes project funded by the CLARIN-nl program, the TROVe project funded by the CLARIAH program, the Dutch national program COMMIT, the ESF Research Network Program ELIAS, the Elite

Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), the Netherlands eScience Center under number 027.012.105 the Yahoo! Faculty Research and Engagement Program, the Microsoft Research PhD program, and the HPC Fund.
REFERENCES
[1] L. Azzopardi, M. de Rijke, and K. Balog. Building simulated queries for known-item topics: An analysis using six European languages. In SIGIR, 2007.
[2] R. Berendsen, M. Tsagkias, W. Weerkamp, and M. de Rijke. Pseudo test collections for training and tuning microblog rankers. In SIGIR, 2013.
[3] K. Gimpel, N. Schneider, B. O'Connor, D. Das, D. Mills, J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan, and N. A. Smith. Part-of-speech tagging for twitter: Annotation, features, and experiments. In ACL, 2011.
[4] A. Moschitti. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML, 2006.
[5] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. Overview of the TREC-2011 microblog track. In TREC, 2011.
[6] A. Severyn and A. Moschitti. Structural relationships for large-scale learning of answer re-ranking. In SIGIR, 2012.
[7] A. Severyn, M. Nicosia, and A. Moschitti. Learning semantic textual similarity with structural representations. In ACL, 2013.
[8] A. Severyn, M. Nicosia, and A. Moschitti. Building structures from classifiers for passage reranking. In CIKM, 2013.
[9] I. Soboroff, I. Ounis, J. Lin, and I. Soboroff. Overview of the TREC-2012 microblog track. In TREC, 2012.

1070

Learning Sufficient Queries for Entity Filtering
Miles Efron, Craig Willis, Garrick Sherman
Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign {mefron, willis8, gsherma2}@illinois.edu

ABSTRACT
Entity-centric document filtering is the task of analyzing a time-ordered stream of documents and emitting those that are relevant to a specified set of entities (e.g., people, places, organizations). This task is exemplified by the TREC Knowledge Base Acceleration (KBA) track and has broad applicability in other modern IR settings. In this paper, we present a simple yet effective approach based on learning high-quality Boolean queries that can be applied deterministically during filtering. We call these Boolean statements sufficient queries. We argue that using deterministic queries for entity-centric filtering can reduce confounding factors seen in more familiar "score-then-threshold" filtering methods. Experiments on two standard datasets show significant improvements over state-of-the-art baseline models.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Retrieval models
General Terms
Algorithms, Experimentation
Keywords
Document filtering, Entity retrieval, Boolean models
1. INTRODUCTION
Though document filtering is well-studied in the information retrieval (IR) literature, filtering tasks are seeing renewed interest today. In recent years, the TREC knowledge base acceleration (KBA), temporal summarization and microblog tracks have all run filtering tasks. These tasks differ from the earlier TREC filtering tasks, which focused on topical information needs. Contemporary filtering often concerns entities such as people, organizations or places. In this paper, we argue that entity-related filtering presents
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609517.

different profile-representation challenges than topical filtering. We propose an approach that is tailored to contemporary domains and shows strong effectiveness on two TREC collections.
Our core argument is that using training data to craft high-quality Boolean queries which are applied deterministically during filtering can be more effective than methods based on estimating a dissemination threshold on document scores. We propose shifting the prediction problem in filtering from a document classification task (emit/ withold) to a feature classification task (include/exclude this feature from our query).
The paper's main contribution is an approach to query construction for entity filtering. We present an algorithm that includes terms in a Boolean query if they improve the ability to filter training data correctly. While this is a very simple approach, it shows strong experimental effectiveness on two TREC collections.
2. ENTITY-CENTRIC FILTERING
This paper is concerned with the problem of entity-centric filtering, as exemplified by the KBA track's cumulative citation recommendation (CCR) task [6, 12]. The goal of CCR is to monitor an incoming document stream and send appropriate documents to editors of a knowledge base such as Wikipedia. In this context, each entity corresponds to a Wikipedia page, say Phyllis_Lambert or Red_River_Zoo. CCR systems route documents containing "edit-worthy" information about the entity E to the editors of the E node in the knowledge base.
While KBA is our motivating example, the need for this type of filtering arises in other domains such as social media, where users might wish to follow information about particular people, or companies might like to track discussions of their brand [12].
But is entity-centric filtering qualitatively different from earlier TREC filtering tracks or related areas of topic detection and tracking (TDT)? One important difference is the availability of a surface-form match on the entity's name1. In TREC KBA 2012, out of 7,806 "central" (i.e. true-positive) documents in the training corpus, zero failed to include a surface-form match on the entity name [6]. This is in contrast to topics from earlier TREC filtering tasks such as falk-
1 Throughout this paper, the surface-form representation of an entity is simply the title of its Wikipedia page, with punctuation and disambiguation metadata removed. Several of the 2013 KBA entities were represented by Twitter accounts instead of Wikipedia pages. These entities' surface form is the name element associated with their Twitter account.

1091

land petroleum exploration, where relevant documents may contain only some query terms, with no guaranteed interterm proximity or ordering. It has been noted [7] that few CCR runs at TREC 2013 achieved an F1 score higher than one gets by running a simple egrep on each entity's surfaceform representation over the corpus. But looking for an exact match on queries in earlier filtering collections gives very poor performance. This disparity suggests that something is indeed different between older filtering tasks and CCR.
In the remainder of this paper, we propose a framework for exploiting this difference. We develop highly accurate Boolean queries for each entity in the CCR task. This is in contrast to more familiar filtering approaches where documents are scored against a topic profile, then emitted if their score exceeds an empirically determined threshold. Our approach capitalizes on what is easy about entity-centric filtering­the near-guarantee of surface-form matches in relevant documents­while avoiding what is hard about all filtering tasks: defining a document scoring function and an associated dissemination threshold.
2.1 Vocabulary and Notation
Let E be an entity that a user would like to track using a CCR system. We define the moment at which the user specifies E to be time t0. All documents available to the system prior to t0 are available for training. After t0 the CCR system runs without user feedback (test phase).
We assume that at time t0, the user labels m  0 documents with respect to their relevance to E. These labeled documents comprise the training set T = (T1, T2, . . . , Tm). The system may use T to inform subsequent decisions.
During the test phase, documents reach the system sequentially, as a stream. When the system encounters a document Di, it must immediately make a decision: emit the document, or do not emit. The decision to emit implies that Di is relevant to E.
3. APPROACHES TO CCR
Score-then-Threshold: The scenario described above is identical to the setup of the earlier TREC filtering tasks [11], particularly batch filtering. A strategy developed for the earlier tasks and still used in KBA today is what we call score-then-threshold (STT). An STT system estimates E, a profile for E. The system also relies on a scoring function (E, Di) (e.g. the KL divergence between language models, BM25, etc.) whose scores ostensibly correlate with document relevance. Finally, an STT system defines a threshold  . If (E, Di) >  the system emits Di, otherwise it does not. Typically  is estimated by optimizing some accuracy measure such as F1 over T. Discussions of STT approaches include [3, 10, 11].
We argue that much of the difficulty in STT-based filtering arises because systems must tackle at least three problems:
1. Profile estimation 2. Document scoring 3. Threshold estimation. Handling these three tasks simultaneously has proven very difficult, especially with respect to the CCR task, where sophisticated approaches failed to outperform a simple surfaceform match on entity names. Sufficient Queries: Given the difficulty and weak performance of STT-based strategies, we propose a novel approach to the CCR task: filtering via sufficient queries:

Definition 1: Sufficient Query. A Boolean query with enough breadth and nuance to identify documents relevant to an entity E without further analysis or estimation.

For an entity E, the "sufficient query application" method (SQA) to filtering involves defining a sufficient query QE, and then simply applying QE to all incoming documents, emitting those that evaluate to true with respect to QE. Because a sufficient query is expressed as a Boolean criterion, no document scoring or thresholding is necessary.
For an entity such as Phyllis Lambert, a sufficient query must cast a wide enough net to capture a large proportion of relevant documents. A simple match on the surface-form query S = #1(phyllis lambert) does this2.
But a sufficient query must also constrain the set of retrieved documents in order to reduce the number of false positives; since Phyllis Lambert is a relatively common name, a sufficient query must discriminate between the intended person (i.e. the architect from Montreal) and all other people with that name. Additionally, the query must filter documents that contain the bigram phyllis lambert but that do not rise to the level of relevance.
Our goal is to elaborate on the surface-form query S to improve effectiveness. Of course many changes to S might accomplish this. For simplicity, we rely on a single strategy. For entity E, we emit document D iff it:
· contains a match on S, the surface-form query for E. · matches any of k additional features, (f1, f2, . . . , fk),
where k  0. For example:

#band(#1(phyllis lambert)

#syn(architect montreal canada))

(Q1

#band(#1(phyllis lambert) #syn(#1(canadian architect) #1(public art))) (Q2

conform to this structure. Query Q1 requires a document to

match the quoted phrase "phyllis lambert" and to contain at

least one of the terms: architect, montreal or canada. Query

Q2 has the same structure, but relies on two bigrams to

refine the reach of S. More systematically, we build queries

that consist of two parts:

· Constraint Clause: The surface-form query, S.

· Refinement Clause: A set of 0 or more features (un-

igrams, bigrams, etc.).

The constraint and refinement are combined via a Boolean

AND. In general, the queries we propose using as determin-

istic document filters have this form:

#band( S #syn( f1, f2, . . . , fk))

(Q3

for the constraint S and refinement clause #syn( f1, f2, . . . , fk).

In entity retrieval S will usually accumulate many relevant

documents, but it is probably too broad. The refinement

clause shrinks the size of the overall retrieved set. Because

members of the refinement clause are treated as an equiv-

alence class, as their number grows the overall query will

tend to revert back to the breadth of S.

4. LEARNING SUFFICIENT QUERIES
As described above, for an entity E, the CCR task defines a set of m labeled training documents T. Let F be the set
2 We express example queries using the Indri query language. All queries generated during experiments are available at http://timer.lis.illinois.edu/sigir-2014.

1092

of "features" that we will consider adding to our constraint clause. In this paper we define F as the set of word bigrams in relevant training documents3. Thus, all of our estimated queries take the form of Q2 above.
When building a query for SQA-based filtering, we wish to find the best features in F to include in the refinement clause. For this, we use the Bayes decision rule:

P (T |f )P (f )

log-odds(f, T) = log

(1)

P (T |f )P (f )

where P (T |f ) is the probability that a training document in T is correctly classified, given that we add feature f to the refinement, and P (T |f ) is the probability of a correct classification if we exclude f . We include all features fi where log-odds(fi, T) > 1.
To estimate Eq. 1, we simply have:

P^(T |f ) = n(T +|f )

(2)

m

where n(T +|f ) is the number of correctly classified training documents if we include f in the query. P (T |f ) is calculated analogously, replacing the numerator in Eq. 2 with the number of correct classifications when f is omitted.
The factor P (f ) in Eq. 1 encodes the knowledge that some features are, a priori, better topical discriminators than others. However, without such knowledge readily available, we let P (f ) = P (f ). This yields the simple decision rule that we include a feature f if it improves classification accuracy on T over a query that lacks f .
To make estimation tractable, we treat each candidate feature fi  F in isolation when computing Eq. 1. Thus, P (T |f ) is calculated from the surface-form query, while P (T |f ) derives from a query requiring a surface-form match AND the presence of f . This is not globally optimal, as non-independencies among features surely exist. However, without this simplification, the search space during query building is intractably large.

5. EXPERIMENTAL EVALUATION
To test the effectiveness of the queries generated by the method described above, we performed the CCR task over two data sets: the TREC KBA collections from 2012 and 2013. Summary statistics about these collections appear in Table 1. Full details about pre-processing of the data is available in [5]. However, very little pre-processing was done: no stemming, and a stoplist applied at query time4.

Table 1: Collection Statistics for 2012 and 2013 KBA CCR Tasks.

Num. Docs Num. Entities Median Training Docs Median Rel. Training Docs.

KBA 2012 400 M 28 684 51

KBA 2013 1 B 141 45 12

We compared five approaches to the CCR task, which we enumerate in Table 2. Gray rows indicate that a method
3 The choice of bigram features was due to analysis that space constraints force us to omit. 4 A copy of the stoplist we used is available at http://timer.lis.illinois.edu/sigir-2014.

uses the score-then-threshold strategy. Blue indicates a Boolean approach. The row labeled SQ-2 (sufficient queries, 2-grams) is our sufficient query approach, and as such, is our main point of interest. The SF (surface-form) run simply emits any document that contains a surface-form match on the entity. The STT-Base run is a "pure" score-then-threshold run, where every document containing a unigram from the surface-form query was scored. We include STT-Base for completeness, but its effectiveness was low. For more realistic STT benchmarks, we include Base and RM3. These runs are similar to many TREC KBA submissions. In both approaches, documents are initially filtered on S. Matching documents are then scored and thresholded. So although we label them as STT, they are in fact hybrid methods. Base and RM3 differ only in entity profile representation. Base uses unigram features that comprise S to score and threshold. RM3 represents each entity by a relevance model [9] estimated from the training documents.

Table 3: Filtering Effectiveness Statistics.

KBA 2012

KBA 2013

F1

Prec. Recall F1

Prec. Recall

STT 0.182 0.162 0.387 0.062 0.073 0.198

Base 0.251 0.274 0.350 0.243 0.243 0.406

RM3 0.268 0.321 0.273 0.231 0.288 0.285

SF 0.261 0.199 0.666 0.309 0.238 0.820

SQ-2 0.280 0.222 0.596  0.316 0.252 0.737

Statistically significant outcomes are shown in Table 3 as follows. Improvements (declines) with respect to RM3 are shown with  (). Improvements (declines) with respect to SF are shown by ( ). Significant changes imply p < 0.05 using a paired, one-tailed t-test.
Though it is not surprising to see the shifting balance of precision and recall across different methods in Table 3, several noteworthy results are evident from the data.

1. With respect to F1 (the official metric of the KBA track), sufficient queries give very strong effectiveness, improving over all other methods.

2. Sufficient queries temper the decline in precision that a SF match incurs over STT approaches.

Overall, Table 3 suggests that sufficient queries perform well. Their F1 score on the 2013 data exceeds the best reported official TREC run, and the 2012 score is approximately the median among systems that use a large array of features instead of the limited text-only strategy used here.
However, a fair question is whether the benefit of SQ-2 is due to an unintended artifact; perhaps the low recall of the STT methods is due to a systematic over-estimation of the dissemination threshold, a defect that Boolean methods overcome not by better estimation but simply by virtue of being broad. Maybe instead of tuning Boolean queries, we can (using a loosely Bayesian flavor) simply lower STT's dissemination thresholds via:

^ = train + (1 - )min score

(3)

where train is the optimal cutoff given the training data, and min score is the lowest document score on the query among training documents, and   [0, 1]. As  decreases, the dissemination threshold drops, allowing more documents to pass through the filter. The parameter sweeps shown in

1093

Table 2: CCR Approaches used in Effectiveness Comparisons. Row colors correspond to two major types of run: score-then-threshold (STT, gray) or simple Boolean (blue).

Name STT
Base
RM3
SF SQ-2

Type STT
STT
STT
Boolean Boolean

Description Documents scored by negative KL divergence from a query model induced from the unigram in entity name (Dirichlet smoothing, µ = 2500). Threshold is cutoff that optimizes F1 over the training data. Identical to STT, except documents are only analyzed if they contain a surface-form match on the entity name. Relevance model. Identical to Base, except that each entity is represented by a linear combination of the original query and a relevance model estimated from its judged relevant documents in the training data . Simple egrep . The surface-form query is used as a Boolean filter. Sufficient queries estimated as in Section 4. Bigram refinement clause features.

0.260

F1 0.250

0.240

KBA 2012 KBA 2013
0.0 0.2 0.4 0.6 0.8 1.0
lambda
Figure 1: Parameter Sweeps of Mixture Model Coefficient for Lowering Emit Thresholds in Base Runs.
Figure 1 suggest that lowering the STT threshold does help, but not to the extent that we see with sufficient queries.
We hypothesize that the advantage here is due to the flexibility of SQ-2. The breadth of SQ-2 queries varies widely from entity-to-entity. The mean number of bigrams added by SQ-2 on the 2012 data was 88.3 (s.d. 22.6), with a mean of 46.345 (s.d. 46.2) in 2013. Occasionally, SQ-2 will add 0 features to a model. Figure 1 suggests that this flexibility is more effective than a wholesale decision to emit more documents.
6. CONCLUSION
The analysis presented here supports our core hypothesis: well-crafted Boolean queries can be effective filters for entitybased tasks such as CCR. A sufficient query is a Boolean query that is broad enough to allow the whole range of relevant documents to evaluate to true, while offering enough constraint to maintain reasonable precision. The method for building sufficient queries that we proposed in Section 4 yielded models that were highly effective in an experimental setting. This approach obviates the need to estimate dissemination thresholds common in other filtering approaches and shifts the prediction problem from a document classification task to a feature classification task, which we argue is more tractable in the presence of adequate training data.
Our approach gives an expanded query optimized over training data. Thus it is a form of relevance feedback. Feedback and expansion techniques in document filtering have a long history [1, 11]. But our work is closer in spirit to supervised methods that generate powerful query features from noisy data (e.g. [4, 8, 2]). We see our contribution as a natural extension of this research area.
In future work we plan to improve our ability to estimate sufficient queries. Several important directions include: adapting queries during the course of a filter's ex-

ecution, integrating sufficient query-based scores into stateof-the-art machine learning approaches, and extending the applicability of sufficient queries to tasks other than entitybased filtering.
7. ACKNOWLEDGEMENTS
This work was supported in part by the US National Science Foundation under Grant No. 1217279 Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the National Science Foundation.
8. REFERENCES
[1] J. Allan. Incremental relevance feedback for information filtering. In Proc. of SIGIR'96, pages 270­278, 1996.
[2] M. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In Proc. of SIGIR '08, pages 491­498, 2008.
[3] J. Callan. Learning while filtering documents. In Proc. of SIGIR '98, pages 224­231, 1998.
[4] G. Cao et al. Selecting good expansion terms for pseudo-relevance feedback. In Proc. SIGIR '08, pages 243­250, 2008.
[5] M. Efron et al. The Univ. of Illinois' Grad. School of Library and Information Science at TREC 2013. In The 22nd Text REtrieval Conference, 2013.
[6] J. R. Frank et al. Building an Entity-Centric Stream Filtering Test Collection. In TREC 2012, 2012.
[7] J. R. Frank et al. Evaluating stream filtering for entity profile updates for trec 2013. In TREC-2013, Forthcoming.
[8] G. Kumaran and V. R. Carvalho. Reducing long queries using query quality predictors. In Proc. of SIGIR '09, pages 564­571, 2009.
[9] V. Lavrenko and W. B. Croft. Relevance based language models. In Proc. of SIGIR '01, pages 120­127, 2001.
[10] S. E. Robertson. Threshold setting and performance optimization in adaptive filtering. Inf. Retr., 5(2-3):239­256, Apr. 2002.
[11] S. E. Robertson and I. Soboroff. The trec 2002 filtering track report. In TREC 2002, 2002.
[12] M. Zhou and K. Chang. Entity-centric document filtering: boosting feature mapping through meta-features. In Proc. of CIKM 2013, pages 119­128, 2013.

1094

The Effect of Sampling Strategy on Inferred Measures
Ellen M. Voorhees
National Institute of Standards and Technology
ellen.voorhees@nist.gov

ABSTRACT
Using the inferred measures framework is a popular choice for constructing test collections when the target document set is too large for pooling to be a viable option. Within the framework, different amounts of assessing effort is placed on different regions of the ranked lists as defined by a sampling strategy. The sampling strategy is critically important to the quality of the resultant collection, but there is little published guidance as to the important factors. This paper addresses this gap by examining the effect on collection quality of different sampling strategies within the inferred measures framework. The quality of a collection is measured by how accurately it distinguishes the set of significantly different system pairs. Top-K pooling is competitive, though not the best strategy because it cannot distinguish topics with large relevant set sizes. Incorporating a deep, very sparsely sampled stratum is a poor choice. Strategies that include a top-10 pool create better collections than those that do not, as well as allow Precision(10) scores to be directly computed.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation
Keywords
Test collections; Incomplete judgments; Sampling
1. INTRODUCTION
Test collections drive much of the research in information retrieval. Obtaining human judgments regarding document relevance is the most expensive direct cost of building a test collection, so recent research has focused on developing evaluation mechanisms to support fair comparison of retrieval results using relatively small amounts of judging effort. One such mechanism is the judgment-set-building process that supports the computation of extended inferred estimates of traditional evaluation measures using many fewer judgments
This paper is authored by an employee(s) of the United States Government and is in the public domain. Non-exclusive copying or redistribution is allowed, provided that the article citation is given and the authors and agency are clearly identified as its source. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. 2014 ACM 978-1-4503-2257-7/14/07 http://dx.doi.org/10.1145/2600428.2609524 .

than would be required to reliably compute the traditional scores directly [4, 5]. This framework has proved popular because it is convenient to use in practice and supports a variety of evaluation measures.
Empirical evaluation of the inferred measures shows the estimates can have high fidelity to their directly-computed counterparts, but the quality of the estimates depends on the sampling strategy used to select which documents to judge. Despite the importance of the sampling strategy on estimate quality, there is little guidance in the literature on how best to sample to obtain good estimates. This paper thus examines the question of how the sampling strategy used in creating judgment sets affects the quality of the resulting test collection when using extended inferred measures.
2. METHODOLOGY
A test collection is a triple containing a document set, a set of topics, and a set of relevance judgments which we will call a qrels. A run is the output of a retrieval system for each of the topics in the collection. This output is assumed to be a list of documents ranked by decreasing similarity to the topic. The quality of a run is evaluated using the mean over all topics in the collection of some metric that computes a per-topic score based on the ranks at which relevant documents are retrieved. This work considers three measures, mean average precision (MAP), normalized discounted cumulative gain (NDCG), and precision at 10 documents retrieved (P(10)) and their inferred counterparts.
Ideally, qrels would be complete meaning that all documents are judged for all topics. Complete judgments are infeasible for all but the tiniest of document sets, however, so instead some subset of documents is judged. In pooling [2], the top K results from each of a set of runs are combined to form the pool and only those documents in the pool are judged. Runs are subsequently evaluated by assuming that all unpooled (and hence unjudged) documents are not relevant. The inferred measures framework uses stratified sampling strategies and estimates the values of the measures based on the judgments obtained on sampled documents.
Both pooling and the inferred measures framework construct a judgment set based on a set of runs. The framework focuses different amounts of assessing effort on different regions of the ranked lists (the strata) by assigning a sampling rate to each stratum. The strata are defined by disjoint sets of contiguous ranks. A document belongs to the stratum defined by the smallest rank at which the document was retrieved across the set of runs. The sampling rate is the probability of selecting a document from the stra-

1119

tum to be judged. The inferred measures use the knowledge of a stratum's size, sampling rate, and number of relevant documents found from among the judged set to estimate the total number of relevant documents in the stratum, and combines the different strata's estimates to compute a final global estimate of the number of relevant documents for a topic. Similar local estimates are made for the number of relevant documents retrieved by an individual run based on the number of judged documents retrieved by that run in each of the strata.
A sampling strategy is the combination of strata definition and sampling rate per stratum. Different sampling strategies applied to the same run set create different test collections since the judged set of documents depends on the sampling strategy. The quality of a sampling strategy is thus measured by the quality of the test collections it induces. To measure the quality of a test collection, we use the accuracy of determining the set of significantly different run pairs as compared to a gold standard set of runs pairs (similar to the approach used by Bompada, et al. [1]).
In general, the larger the assessment budget (i.e., maximum number of human judgments that can be obtained), the better a test collection will be. Thus it is important to control for the total number of documents judged when comparing sampling strategies. In practice, this means that stratum size and sampling rate must be traded off against one another: small strata can have large sampling rates but large strata must be sampled more sparsely.
We consider the following strategies in this work. pool: A single, exhaustively judged (thus shallow) stra-
tum. The same documents as in pooling are judged, but subsequent evaluation uses the inferred measures.
1stratum: A single stratum drawn to a moderate depth and using a moderate sampling rate. In this study, the 1stratum strategy uses a depth of 100 and a sampling rate of 30%.
2strata: An exhaustively judged small initial stratum plus a moderate depth and sampling rate second stratum. This strategy is motivated by the fact that it allows P(depth) to be computed exactly, and it concentrates the most assessment effort at ranks that have a large effect on the measures' scores. In this study, the 2strata strategy consists of all documents in ranks 1­10 plus a 10% sample of documents in ranks 11­100.
3strata: An exhaustively judged small initial stratum, a moderate depth, moderate sampling rate second stratum, and a deep, very sparsely sampled final stratum. The addition of a large, sparsely sampled stratum is motivated by the hope that it will provide a better estimate of the total number of relevant documents. In this study, the 3strata strategy consists of all documents from ranks 1­10 in union with a 10% sample of documents from ranks 11­20 and a 1% sample of documents from ranks 21­1000.
To examine the quality of a sampling strategy, we use an existing test collection and create a set of sampled collections containing a subset of the original collection's judgments. We then compare the average behavior of the sampled collections to that of the original collection, which we treat as gold standard behavior. We use two existing TREC collections as the base collection in this work: the collection built in the the TREC-8 ad hoc retrieval task and the collection built in the TREC 2012 Medical Records track.

The TREC-8 ad hoc track collection contains about 528,000 mostly newswire documents and 50 topics. Binary relevance judgments were created through pooling with K = 100. This collection was chosen for this study because it is a high quality test collection that has been used in many similar studies; evaluation scores computed on this collection are as close to Truth as we are likely to ever get. We use traditional evaluation measures computed over the official qrels as the gold standard for this collection.
The original motivation for this study was unexpectedly noisy scores that resulted from using inferred measures in the TREC 2011 Medical Records track [3]. The 2012 Medical Records track collection is similar to the 2011 collection in that it uses the same document set and similar topics, but it contains a better set of relevance judgments to use as ground truth. The document set is approximately 17,000 medical visit reports and there are 47 topics. Judgment sets were created using a two strata strategy. The first stratum contains all documents retrieved between ranks 1­15 from 88 runs, and the second stratum is drawn from ranks 16­ 100 with a sampling rate of 25%. Documents were judged on a three-way scale of relevant, partially relevant and not relevant. For this collection, the gold standard scores are the inferred scores computed using the entire qrels created in the track. The computation of infNDCG uses a gain value of 1 for partially relevant documents and 2 for fully relevant documents.
Call a run that contributed to the judgment sets for the base collection a judged run1. To test a sampling strategy, we randomly split the set of judged runs in half, and apply the sampling strategy to only the runs in one half. Once a judgment set is produced, we evaluate all runs submitted to the original TREC track using it. The entire process is repeated 50 times, each time called a trial, with each trial using a different random split of judged runs. All sampling strategies use precisely the same split of judged runs in each trial, but necessarily produce different judgment sets from that split.
3. EXPERIMENTAL RESULTS
Using the set of sampled judgment sets, we can compare the sampling strategies along three dimensions: the number of judged documents required, the accuracy of determining significantly different run pairs, and the accuracy of estimates of the total number of relevant documents, R.
3.1 Judgment set size
As mentioned above, comparisons of sampling strategies must control for the total number of judgments a strategy requires, called the judgment set size. The mean judgment size over the 50 trials for each sampling strategy is given in Table 1.
Note that it is possible that a document selected to be judged might not actually have a judgment in the base collection's qrels. When this happens, we ignore the selection (i.e., we mark it as not selected). To account for this effect, the sampling rate actually used in constructing the sample judgment sets was greater than the target rate such that the number of judgments obtained approximated the number the target rate would produce (while always sampling
1While 129 runs were submitted to the TREC-8 ad hoc task, only 71 runs were judged. All submitted runs were judged for the TREC 2013 Medical Records collection.

1120

Table 1: Mean judgment set size per strategy

TREC-8

TREC 2012

Mean

Mean

ID

Size

ID

Size

pool15 179.7 pool15 197.9

pool25 296.0

1stratum 323.2 1stratum 255.9

2strata 221.6 2strata 227.8

3strata 199.4

from the appropriate stratum). The rates given in the definitions of the strata are the target rates, which are also the effective sampling rates. The strata sizes and sampling rates provide only coarse control of the resulting judgment set size, however, and there are sampling strategies the base collection cannot support because there are too many missing judgments in its qrels. The TREC 2012 collection cannot support a pooled strategy with a depth greater than 15, nor does it support the exploration of a 3strata strategy. We use a pool depth of both 15 and 25 for the TREC-8 collection since depth 25 is a better match for the other methods' judgment set sizes on that collection while depth 15 is used for the TREC 2012 collection.
The lack of fine control means there is a wider spread in the range of judgment set sizes across the strategies than is ideal. However, the 1stratum strategy is the strategy with the largest judgment set size, and as will be seen below, it is not a very good strategy even with the larger number of judgments.
3.2 Collection quality
We use the accuracy of detecting the set of significantly different run pairs as the measure of a sampled collection's quality. For a given measure, we first compute the set of significantly different run pairs in the base collection using a paired t-test with  = 0.05. Using the same test for a sampled collection, we calculate the number of pairs in each of the following five categories: true positive: the run pair is significantly different in both
collections and the collections agree as to which system is better; true negative: the run pair is not significantly different in either collection; miss: the run pair is significantly different in the base collection, but not the sampled collection; false alarm: the run pair is not significantly different in the base collection, but is in the sampled collection; inversion: the run pair is significantly different in both collections, but the collections disagree as to which is the better run. The accuracy of the significant pairs determination is then computed as the number of correct classifications over the sum of correct and incorrect classifications where inversions are counted as both a false alarm and a miss. Accuracy scores are given in Table 2, which shows the mean value as well as the minimum and maximum accuracy values observed across the 50 trials. The 1stratum strategy is consistently the worst strategy for the NDCG and P(10) measures, and is only somewhat more competitive for MAP. This strategy is the only strategy examined that does not judge all of the documents retrieved in the top 10 ranks, but instead samples uniformly from the top 100 ranks. Since all three measures strongly emphasize

Table 2: Accuracy of reproducing the set of signifi-

cantly different run pairs

a) TREC-8

infAP

infNDCG

infP10

pool15

0.906

0.922

0.959

[0.855, 0.926] [0.873, 0.945] [0.920, 0.975]

pool25

0.929

0.947

0.978

[0.889, 0.943] [0.901, 0.966] [0.950, 0.987]

1stratum

0.878

0.884

0.868

[0.839, 0.911] [0.836, 0.918] [0.834, 0.904]

2strata

0.918

0.924

0.953

[0.891, 0.939] [0.879, 0.943] [0.932, 0.962]

3strata

0.834

0.912

0.926

[0.786, 0.874] [0.880, 0.934] [0.912, 0.937]

b) TREC 2012

infAP

infNDCG

infP10

pool15

0.888

0.883

0.917

[0.862, 0.911] [0.855, 0.907] [0.885, 0.956]

1stratum

0.930

0.820

0.895

[0.903, 0.948] [0.783, 0.858] [0.853, 0.924]

2strata

0.935

0.904

0.967

[0.901, 0.953] [0.880, 0.926] [0.953, 0.978]

the quality of the top of the ranked list--though MAP less so than P(10) and NDCG--unjudged documents in these ranks increase the variability of the final estimated score much more than unjudged documents later in the ranked list do. Thus it is beneficial to concentrate more of the judging resources at the very top of the document ranked lists.
The 3strata strategy is the strategy used for the TREC 2011 Medical Records collection whose noisy score estimates prompted this study. Its behavior on the TREC-8 collection confirms that it is a poor choice of sampling strategy. The problem is that in this strategy the stratum for which the least information is known receives the greatest emphasis. The computation of the inferred measures uses as an estimate of a run's number of relevant retrieved in a particular stratum, S, a smoothed fraction of the number of judged relevant documents in S retrieved by the run over the number of judged documents in S retrieved by the run. When a run retrieves few (or none) of the documents that were selected to be judged in the stratum but relatively many documents from the stratum, the estimate can be far afield of the true value. For example, if a run retrieves only one document in S that was judged, and that document is relevant, the estimated number of relevant retrieved is .99998 times the number of documents in S that are retrieved by the run. Any strategy that includes a large stratum that is sampled very sparsely will be affected by this behavior.
The relative quality of the pool and 2strata strategies is more complex. The 2strata strategy is more accurate for the TREC 2012 collection. The pooling strategy is generally more accurate for the TREC-8 collection, but there is a clear dependency on the pool depth. The next section will show that the 2strata strategy produces better estimates of the number of relevant documents for both collections.
3.3 Estimating R
Part of the computation of extended inferred measures is creating an estimate of the total number of relevant documents for a topic. This estimate has a large impact on the quality of the final estimates of NDCG and MAP scores. Since NDCG is defined over a fixed set of ranks (100 in this work), while MAP considers the entire relevant set, the quality of the estimate of R has a bigger impact on infAP than it does on infNDCG.

1121

Pool25

2strata

Figure 1: Estimates of R on the TREC-8 collection.

Table 3: Mean correlation between estimated and

gold-standard R

TREC-8 TREC 2012

1stratum

0.97

0.98

2strata

0.94

0.99

pool25

0.91

--

pool15

0.87

0.94

Table 3 gives the mean over the 50 trials of the Pearson correlation between the per-topic estimate and goldstandard values of R, the number of relevant documents. The gold-standard value of R for the TREC 2012 collection is the estimate produced using the entire set of runs submitted to the Medical Records track. The estimates based on half of the runs are very highly correlated with the original estimates, though the pooling strategy produces less good estimates than the other strategies. The gold-standard value of R for the TREC-8 collection is the count of the the number of relevant in the original qrels. The estimated R values are again highly correlated with true R, but the correlations are weaker for the TREC-8 collection than the TREC 2012 collection. The pooling strategies exhibit the weakest correlations, and the correlations get weaker as the pool depth gets smaller.
Despite a good overall correlation, the pooling strategies underestimate R for all but the smallest of relevant sets, meaning that they cannot distinguish topics that truly have a small relevant set. Figure 1 shows the estimates of R per topic for the TREC-8 collection for the pool25 (left) and 2strata (right) sampling strategies. Individual topics are plotted on the x-axis and the number of documents on the y-axis. The minimum and maximum estimates of R across the 50 trials are plotted by a line connecting the two points (which runs off the top of the graph if the maximum estimate exceeds the graph's largest y-value). The mean across the 50 trials of the estimated R is plotted as an x on that line. The gold-standard value of R is plotted as a circle. For the pool25 strategy, nine topics have a mean estimated R

smaller than 20. The gold-standard and estimated R values for these topics are shown below, which has a correlation of just 0.42.
Gold: 13 22 16 6 28 13 17 17 65 Est: 12.7 17.8 15.0 6.0 13.7 9.8 17.0 17.0 16.0
4. CONCLUSION
The inferred measures framework can construct test collections that have high fidelity with their traditionallyconstructed counterparts using relatively few judgments provided an appropriate sampling strategy is used in the construction process. The framework does not work well with large, sparsely sampled strata nor with strategies that do not exhaustively judge a small top stratum. Restricting all judgments to the very top ranks (i.e., pooling) is also not a good strategy, though, because such samples are unable to accurately estimate R. Thus, what this paper called the 2strata strategy--using an exhaustively judged small initial stratum coupled with a moderate depth, moderate sampling rate stratum--is a practical and effective sampling strategy to use for inferred measures.
5. REFERENCES
[1] T. Bompada, C.-C. Chang, J. Chen, R. Kumar, and R. Shenoy. On the robustness of relevance measures with incomplete judgments. In Proceedings of SIGIR 2007, pages 359­366, 2007.
[2] K. Sparck Jones and C. van Rijsbergen. Report on the need for and provision of an "ideal" information retrieval test collection. British Library Research and Development Report 5266, 1975.
[3] E. M. Voorhees. The TREC Medical Records track. In Proceedings of ACM Conference on Bioinformatics, Computational Biology and Biomedical Informatics, pages 239­246, 2013.
[4] E. Yilmaz and J. A. Aslam. Estimating average precision when judgments are incomplete. Knowledge Information Systems, 16:173­211, 2008.
[5] E. Yilmaz, E. Kanoulas, and J. A. Aslam. A simple and efficient sampling method for estimating AP and NDCG. In Proceedings of SIGIR 2008, pages 603­610, 2008.

1122

Cache-Conscious Runtime Optimization for Ranking Ensembles

Xun Tang, Xin Jin, Tao Yang
Department of Computer Science, University of California Santa Barbara, CA 93106, USA
{xtang,xin_jin,tyang}@cs.ucsb.edu

ABSTRACT
Multi-tree ensemble models have been proven to be effective for document ranking. Using a large number of trees can improve accuracy, but it takes time to calculate ranking scores of matched documents. This paper investigates data traversal methods for fast score calculation with a large ensemble. We propose a 2D blocking scheme for better cache utilization with simpler code structure compared to previous work. The experiments with several benchmarks show significant acceleration in score calculation without loss of ranking accuracy.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models, Search Process
Keywords
Ensemble methods; query processing; cache locality
1. INTRODUCTION
Learning ensembles based on multiple trees are effective for web search and other complex data applications (e.g. [9, 8, 10]). It is not unusual that algorithm designers use thousands of trees to reach better accuracy and the number of trees becomes even larger with the integration of bagging. For example, winning teams in the Yahoo! learning-to-rank challenge [8] have all used boosted regression trees in one form or another and the total number of trees reported for scoring ranges from 3,000 to 20,000 [11, 6, 12], or even reaches 300,000 or more combined with bagging [13].
Computing scores from a large number of trees is timeconsuming. Access of irregular document attributes along with dynamic tree branching impairs the effectiveness of CPU cache and instruction branch prediction. Compiler optimization [5] cannot handle complex code such as rank scoring very well. For example, processing a 8,051-tree ensemble can take up to 3.04 milliseconds for a document with 519 features on an AMD 3.1 GHz core. Thus the scoring time
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609525.

per query exceeds 6 seconds to rank the top-2,000 results. It takes more time proportionally to score more documents with larger trees or more trees and this is too slow for interactive query performance. Multi-tree calculation can be parallelized; however, query processing throughput is not increased because less queries are handled in parallel. Tradeoff between ranking accuracy and performance can be played by using earlier exit based on document-ordered traversal (DOT) or scorer-ordered traversal (SOT) [7], and by tree trimming [3]. The work in [4] proposes an architectureconscious solution called VPred that converts control dependence of code to data dependence and employs loop unrolling with vectorization to reduce instruction branch misprediction and mask slow memory access latency. The weakness is that cache capacity is not fully exploited and maintaining the lengthy unrolled code is not convenient.
Unorchestrated slow memory access incurs significant costs since memory access latency can be up to 200 times slower than L1 cache latency. How can fast multi-tree ensemble ranking with simple code structure be accomplished via memory hierarchy optimization, without compromising ranking accuracy? This is the focus of this paper.
We propose a cache-conscious 2D blocking method to optimize data traversal for better temporal cache locality. Our experiments show that 2D blocking can be up to 620% faster than DOT, up to 245% faster than SOT, and up to 50% faster than VPred. After applying 2D blocking on top of VPred which shows advantage in reducing branch mis-prediction, the combined solution Block-VPred could be up to 100% faster than VPred. The proposed techniques are complementary to previous work and can be integrated with the tree trimming and early-exit approximation methods.
2. PROBLEM DEFINITION
Given a query, there are n documents matching this query and the ensemble model contains m trees. Each tree is called a scorer and contributes a subscore to the overall score for a document. Following the notation in [7], Algorithm 1 shows the program of DOT. At each loop iteration i, all tress are calculated to gather subscores for a document before moving to another document. In implementation, each document is represented as a feature vector and each tree can be stored in a compact array-based format [4]. The time and space cost of updating the overall score with a subscore is relatively insignificant. The dominating cost is slow memory accesses during tree traversal based on document feature values. By exchanging loops i and j in Algorithm 1, DOT becomes SOT. Their key difference is the traversal order.

1123

Algorithm 1: Ranking score calculation with DOT.
for i = 1 to n do for j = 1 to m do Compute a subscore for document i with tree j. Update document score with the above subscore.

(a)

(b)

Figure 1: Data access order in DOT (a) and SOT (b).

Figure 1(a) shows the data access sequence in DOT, marked on edges between documents and tree-based scorers. These edges represent data interaction during ranking score calculation. DOT first accesses a document and the first tree (marked as Step 1); it then visits the same document and the second tree. All m trees are traversed before accessing the next document. As m becomes large, the capacity constraint of CPU cache such as L1, L2, or even L3 does not allow all m trees to be kept in the cache before the next document is accessed. The temporal locality of a document is exploited in DOT since the cached copy can be re-accessed many times before being flushed; however, there is no or minimal temporal locality exploited for trees. Similarly, Figure 1(b) marks data interaction edges and their access order in SOT. SOT traverses all documents for a tree before accessing the next tree. Temporal locality of a tree is exploited in SOT; however, there is no or minimal temporal locality exploited for documents when n is large.
VPred [4] converts if-then-else branches to dynamic data accesses by unrolling the tree depth loop. The execution still follows DOT order, but it overlaps the score computation of several documents to mask memory latency. Such vectorization technique also increases the chance of these documents staying in a cache when processing the next tree. However, it has not fully exploited cache capacity for better temporal locality. Another weakness is that the length of the unrolled code is quadratic to the maximum tree depth in a ensemble, and linear to the vectorization degree v. For example, the header file with maximum tree depth 51 and vectorization degree 16 requires 22,651 lines of code. Long code causes inconvenience in debugging and code extension. In comparison, our 2D blocking code has a header file of 159 lines.

3. 2D BLOCK ALGORITHM

Algorithm 2 is a 2D blocking approach that partitions the

program in Algorithm 1 into four nested loops. The loop

structure is named SDSD because the first (outer-most) and

third levels iterate on tree-based Scorers while the second

and fourth levels iterate on Documents. The inner two loops

process d documents with s trees to compute subscores of

these documents. We choose d and s values so that these d

documents and s trees can be placed in the fast cache under

its capacity constraint. To simplify the presentation of this

paper, we assume

m s

and

n d

are integers.

The hierarchical

data access pattern is illustrated in Figure 2. The edges in

the left portion of this figure represent the interaction among

blocks of documents and blocks of trees with access sequence marked on edges. For each block-level edge, we demonstrate the data interaction inside blocks in the right portion of this figure. Note that there are other variations of 2D blocking structures: SDDS, DSDS and DSSD. Our evaluation finds that SDSD is the fastest for the tested benchmarks.

Algorithm 2: 2D blocking with SDSD structure.

for j = 0 to for i = 0

m
s
to

-
n d

1 do - 1 do

for jj = 1 to s do

for ii = 1 to d do

Compute subscore for document i × d + ii

with tree j × s + jj.

Update the score of this document.

Figure 2: Data access order in the SDSD blocking scheme.

There are two to three levels of cache in modern AMD or

Intel CPUs. For the tested datasets, L1 cache is typically

too small to fit multiple trees and multiple document vectors

for exploiting temporal locality. Thus L1 is used naturally

for spatial locality and more attention is on L2 and L3 cache.

2D blocking design allows the selection of s and d values so

that s trees and d documents fit in L2 cache.

Detailed cache performance analysis requires a study of

cache miss ratio estimation in multiple levels of cache. Due

to the length restriction of this paper, we use a simplified

cache-memory model to illustrate the benefits of the 2D

blocking scheme. This model assumes there is one level

of cache which can hold d document vectors and s tree-

based scorers, i.e. space usage for s and d do not exceed

cache capacity. Here we estimate the total slow memory

accesses during score calculation using the big O notation.

The inner-most loop ii in Algorithm 2 loads 1 tree and d

document vectors. Then loop jj loads another tree and still

accesses the same d document vectors. Thus there are a to-

tal of O(s) + O(d) slow memory accesses for loops jj and

ii. In loop level i, the s trees stay in the cache and every

document block causes slow memory accesses, so memory

access

overhead

is

O(s) + O(d) ×

n d

.

Now

looking

at

the

the

outer-most loop j, total memory access overhead per query

is

m s

(O(s)

+

O(n))

= O(m +

m×n s

).

From Figure 1, memory access overhead per query in DOT

can be estimated as O(m×n+n) while it is O(m×n+m) for

SOT. Since term m×n typically dominates, our 2D blocking

algorithm incurs s times less overhead in loading data from

slow memory to cache when compared with DOT or SOT.

Vectorization in VPred can be viewed as blocking a num-

ber of documents and the authors have reported [4] that a

larger vectorization degree does not improve latency mask-

ing and for Yahoo! dataset, 16 or more degree performs

about the same. The objective of 2D blocking scheme is to

1124

Dataset Yahoo! MSLR-30K MQ2007

Leaves 50 150 400 50 50 200

m 7,870 8,051 2,898 1,647 9,870 10,103

n 5,000 2,000 5,000 5,000 10,000 10,000

DOT 186.0 377.8 312.3 88.3 1.79 204.1

SOT 113.8 150.2 223.8 41.4 1.66 30.3

VPred [v] 47.4 [8] 123.0 [8] 136.2 [8] 32.6 [8] 2.02 [8] 43.1 [32]

2D blocking [s, d] 36.4 [300, 300] 81.9 [100, 400] 90.9 [100, 400] 26.6 [500, 1,000] 1.51 [300, 5,000]
28.3 [100, 10,000]

Block-VPred [s, d, v] 36.7 [300, 320, 8] 76.1 [100, 480, 8] 86.0 [100, 400, 8] 31.1 [500, 1,600, 8] 1.94 [300, 5,000, 8] 26.2 [100, 5,000, 32]

Latency 1.43 1.23 1.25 0.22 0.15 2.65

Table 1: Scoring time per document per tree in nanoseconds for five algorithms. Last column shows the average scoring latency per query in seconds under the fastest algorithm marked in gray.

400 DOT

SOT

350

VPred

2D blocking

300 Block-VPred

250

200

400

400

350

350

DOT

300

SOT

300

VPred

2D blocking

250

Block-VPred

250

200

200

Time (ns) Time (ns) Time (ns)

150 100
50100

500 1000 2000 4000 8000 16000 32000 Ensemble size m
(a)

150 100
5010

100

1000 2000 500010000

Number of matched documents n

(b)

100000

150 100
50 1

s=10 s=100 s=1,000 s=8,000

10

100

1000

10000

Number of documents per block d

(c)

100000

Figure 3: Scoring time per document per tree in nanoseconds when varying m (a) and n (b) for five algorithms, and varying

s and d for 2D blocking (c). Benchmark used is Yahoo! dataset with a 150-leaf multi-tree ensemble.

fully exploit cache locality. We can apply 2D blocking on top of VPred to exploit more cache locality while inheriting the advantages of VPred. We call this approach Block-VPred. The code length of Block-VPred is about the same as VPred.
4. EVALUATIONS
2D block and Block-VPred methods are implemented in C and VPred code is from [4]. Code is compiled with GCC using optimization flag -O3. Experiments are conducted on a Linux server with 8 cores of 3.1GHz AMD Bulldozer FX8120 and 16GB memory. FX8120 has 16KB of L1 data cache per core, 2MB of L2 cache shared by two cores, 8MB of L3 cache shared by eight cores. The cache line is of size 64 bytes. Experiments are also conducted in Intel X5650 2.66GHz sixcore dual processors and the conclusions are similar. This paper reports the results from AMD processors.
We use the following learning-to-rank datasets as the core test benchmarks. (1) Yahoo! dataset [8] with 709,877 documents and 519 features per document from its learning-torank challenge. (2) MSLR-30K dataset [2] with 3,771,125 documents and 136 features per document. (3) MQ2007 dataset [1] with 69,623 documents and 46 features per document. The tree ensembles are derived by the open-source jforests [10] package using LambdaMART [6]. To assess score computation in presence of a large number of trees, we have also used bagging methods to combine multiple ensembles and each ensemble contains additive boosting trees.
There are 23 to 120 documents per query labeled in these datasets. In practice, a search system with a large dataset ranks thousands or tens of thousands of top results after the preliminary selection. We synthetically generate more matched document vectors for each query. Among these synthetic vectors, we generate more vectors bear similarity to those with low labeled relevance scores, because typically the majority of matched results are less relevant.
Metrics. We mainly report the average time of computing a subscore for each matched document under one tree. This scoring time multiplied by n and m is the scoring latency per query for n matched documents ranked with an m-tree model. Each query is executed by a single core.

A comparison of scoring time. Table 1 lists scoring time under different settings. Column 2 is the maximum number of leaves per tree. Tuple [s,d,v] includes the parameters of 2D blocking and the vectorization degree of VPred that leads to the fastest scoring time. Choices of v for VPred are the best in the tested AMD architecture and are slightly different from the values reported in [4] with Intel processors. Last column is the average scoring latency per query in seconds after visiting all trees. For example, 2D blocking is 361% faster than DOT and is 50% faster than VPred for Row 3 with Yahoo! 150-leaf 8,051-tree benchmark. In this case, Block-VPred is 62% faster than VPred and each query takes 1.23 seconds to complete scoring with Block-VPred. For a smaller tree in Row 5 (MSLR-30K), Block-VPred is 17% slower than regular 2D blocking. In such cases, the benefit of converting control dependence as data dependence does not outweigh the overhead introduced.
Figure 3 shows the scoring time for Yahoo! dataset under different settings. In Figure 3(a), n is fixed as 2,000; DOT time rises dramatically when m increases because these trees do not fit in cache; SOT time keeps relatively flat as m increases. In Figure 3(b), m is fixed as 8,051 while n varies from 10 to 100,000. SOT time rises as n grows and 2D blocking is up to 245% faster. DOT time is relatively stable. 2D blocking time and its gap to VPred are barely affected by the change of m or n. Block-VPred is 90% faster than VPred when n=5,000, and 100% faster when n=100,000. Figure 3(c) shows the 2D blocking time when varying s and d. The lowest value is achieved with s=1,000 and d=100 when these trees and documents fit in L2 cache.
Cache behavior. Linux perf tool reports L1 and L3 cache miss ratios during execution. We observed no strong correlation between L1 miss ratio and scoring time. L1 cache allows program to exploit limited spatial locality, but is too small to exploit temporal locality in our problem context. L3 miss ratio does show a strong correlation with scoring time. In our design, 2D blocking sizes (s and d) are determined based on L2 cache size. Since L2 cache is about the same size as L3 per core in the tested AMD machine, reported L3 miss ratio reflects the characteristics of L2 miss ratio.

1125

L3 cache miss ratio (%) L3 cache miss ratio (%) L3 cache miss ratio (%)

60

80

80

DOT

70

SOT

70

50

VPred

DOT

60 2D blocking

60

40

SOT

VPred

50

50

2D blocking

30

40

40

30

30

20

20

20

s=10

10

10

s=100

10

s=1,000

s=8,000

0100

500 1000 2000 4000 8000 16000 32000

010

100

1000 2000 500010000

100000

0 1

10

100

1000

10000

100000

Ensemble size m

Number of matched documents n

Number of documents per block d

(a)

(b)

(c)

Figure 4: L3 miss ratio when varying n (a), varying m (b) for four algorithms, and when varying s and d for 2D blocking (c).

Figure 4 plots the L3 miss ratio under the same settings as Figure 3 for Yahoo! data. This ratio denotes among all the references to L3 cache, how many are missed and need to be fetched from memory. The ratios of Block-VPred, which are not listed, are very close to that of 2D blocking. In Figure 4(a) with n=2,000, SOT has a visibly higher miss ratio because it needs to bring back most of the documents from memory to L3 cache every time it evaluates them against a scorer; n is too big to fit all documents in cache. The miss ratio of DOT is low when all trees can be kept in L2 and L3 cache; this ratio grows dramatically after m=500. Figure 4(b) shows miss ratios when m=8,051 and n varies. The miss ratio of SOT is close to VPred and 2D blocking when n<100, but deteriorates significantly when n increases and these documents cannot fit in cache any more. The miss ratios of VPred in both Figure 4(a) and 4(b) are below 6% because vectorization improves cache hit ratio. Performance of 2D blocking is the best, maintaining miss ratio around 1% even when m or n is large.
Figure 4(c) plots L3 miss ratio of 2D blocking when varying s and d block sizes. The trends are strongly correlated with the scoring time curve in Figure 3(c). The optimal point is reached with s=1,000 and d=100 when these trees and documents fit in L2 cache. When s=1,000, miss ratio varies from 1.64% (d=100) to 78.1% (d=100,000). As a result, scoring time increases from 86.2ns to 281.5ns.
Branch mis-prediction. We have also collected instruction branch mis-prediction ratios during computation. For MQ2007 and 50-leaf trees, mis-prediction ratios of DOT, SOT, VPred, 2D blocking and Block-VPred are 1.9%, 3.0%, 1.1%, 2.9%, and 0.9% respectively. For 200-leaf trees, these ratios increase to 6.5%, 4.2%, 1.2%, 9.0%, and 1.1%. VPred's mis-prediction ratio is lower than 2D blocking while its scoring time is still longer, indicating the impact of cache locality on scoring time is bigger than branch mis-prediction. For smaller trees, mis-prediction ratios of 2D blocking and Block-VPred are close and this explains why Block-VPred does not outperform 2D blocking in Table 1 for 50-leaf trees. Adopting VPred's strategy of converting if-then-else instructions pays off for large trees. For such cases when n increases, Block-VPred outperforms 2D blocking with lower branch mis-prediction ratios. This is reflected in the Yahoo! 150-leaf 8,051-tree benchmark: mis-prediction ratios are 1.9%, 2.7%, 4.3%, and 6.1% for 2D blocking, 1.1%, 0.9%, 0.84%, and 0.44% for Block-VPred, corresponding to the cases of n=1,000, 5,000, 10,000 and 100,000 respectively.
5. CONCLUDING REMARKS
The main contribution of this work is cache-conscious design for computing ranking scores with a large number of trees and/or documents by exploiting memory hierarchy ca-

pacity for better temporal locality. Multi-tree score calculation of each query can be conducted in parallel on multiple cores to further reduce latency. Our experiments show that 2D blocking still maintains its advantage using multiple threads. In some applications, the number of top results (n) for each query is inherently small and can be much smaller than the optimal block size (d). In such cases, multiple queries could be combined and processed together to fully exploit cache capacity. Our experiments with Yahoo! dataset and 150-leaf 8,051-tree ensemble shows that combined processing could reduce scoring time per query by 12.0% when n=100, and by 48.7% when n=10.
Our 2D blocking technique is studied in the context of tree-based ranking ensembles and one of future work is to extend it for other types of ensembles by iteratively selecting a fixed number of the base rank models that can fit in the fast cache.
Acknowledgments. This work was supported in part by NSF IIS-1118106. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.
6. REFERENCES [1] Lector 4.0 datasets. http://research.microsoft.com/enus/um/beijing/projects/letor/letor4dataset.aspx. [2] Microsoft learning to rank datasets. http://research.microsoft.com/en-us/projects/mslr/. [3] N. Asadi and J. Lin. Training Efficient Tree-Based Models for Document Ranking. In ECIR, pages 146­157, 2013. [4] N. Asadi, J. Lin, and A. P. D. Vries. Runtime Optimizations for Tree-Based Machine Learning Models. IEEE TKDE, pages 1­13, 2013. [5] D. F. Bacon, S. L. Graham, and O. J. Sharp. Compiler transformations for high-performance computing. ACM Comput. Surv., 26(4):345­420, 1994. [6] C. J. C. Burges, K. M. Svore, P. N. Bennett, A. Pastusiak, and Q. Wu. Learning to rank using an ensemble of lambda-gradient models. In J. of Machine Learning Research, pages 25­35, 2011. [7] B. B. Cambazoglu, H. Zaragoza, O. Chapelle, and J. Chen. Early Exit Optimizations for Additive Machine Learned Ranking Systems Ranking in Additive Ensembles. In WSDM, pages 411­420, 2010. [8] O. Chapelle and Y. Chang. Yahoo! Learning to Rank Challenge Overview. J. of Machine Learning Research, pages 1­24, 2011. [9] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189­1232, 2000. [10] Y. Ganjisaffar, R. Caruana, and C. Lopes. Bagging
Gradient-Boosted Trees for High Precision, Low Variance Ranking Models. In SIGIR, pages 85­94, 2011. [11] P. Geurts and G. Louppe. Learning to rank with extremely randomized trees. J. of Machine Learning Research, 14:49­61, 2011. [12] A. Gulin, I. Kuralenok, and D. Pavlov. Winning the transfer learning track of yahoo!'s learning to rank challenge with yetirank. J. of Machine Learning Research, 14:63­76, 2011. [13] D. Y. Pavlov, A. Gorodilov, and C. A. Brunk. Bagboo: a scalable hybrid bagging-the-boosting model. In CIKM, pages 1897­1900, 2010.

1126

An Enhanced Context-sensitive Proximity Model for Probabilistic Information Retrieval

Jiashu Zhao1, Jimmy Xiangji Huang2
Information Retrieval and Knowledge Management Research Lab 1Department of Computer Science & Engineering, 2School of Information Technology
York University, Toronto, Canada
1jessie@cse.yorku.ca, 2jhuang@yorku.ca

ABSTRACT
We propose to enhance proximity-based probabilistic retrieval models with more contextual information. A term pair with higher contextual relevance of term proximity is assigned a higher weight. Several measures are proposed to estimate the contextual relevance of term proximity1. We assume the top ranked documents from a basic weighting model are more relevant to the query, and calculate the contextual relevance of term proximity using the top ranked documents. We propose a context-sensitive2 proximity model, and the experimental results on standard TREC data sets show the effectiveness of our proposed model.
Keywords
Context-Sensitive IR, Measure, Proximity, Probabilistic Model
1. INTRODUCTION AND MOTIVATION
The study of how to integrate the context information of queries and documents into retrieval process draw a lot of attention in recent years [3]. More specifically, many term proximity approaches [2, 9, 10, 11], which reward the documents where the query terms occurring closer to each other, show significant improvements over basic Information Retrieval (IR) models. In these proximity-based approaches, all the query term pairs are usually treated equally and the difference among various query pairs are not considered, although there is a need to distinguish the importance of term proximities. For example, given a query "recycle automobile tires", there is a stronger association between "automobile" and "tire" than the association between "recycle" and "automobile". In the top ranked documents, "automobile" and "tire" are expected to occur close to each other, while "recycle" and "automobile" do not necessarily have to occur close.
In this paper, we focus on the problem of differentiating the influence of associated query term pairs. We propose
1The contextual relevance of term proximity is the relevancy between the query term proximity and the topic of the query. 2Context-sensitive means that the contextual relevance of term proximity is considered.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609527.

a proximity enhancement approach to integrate the contextual relevance of term proximity into the retrieval process. We also propose four measures for estimating the contextual relevance of term proximity, and reward the query term pairs according to both the proximity and the contextual relevance of proximity. There are several studies that boost proximity retrieval models. A machine learning method is proposed to determine the "goodness" of a span in [8]. [1] learns the concept importance from several sources (e.g. google n-gram corpus, query logs and wikipedia titles). SVM is used to learn different weights for various term dependencies in [6]. The importance of the global statistics is examined for proximity weighting [4]. Phrases are treated as providing a context for the component query terms in [7]. The contribution of this paper is that we propose the contextual relevance of term proximity, which represents to what extent the corresponding term pair should be related to the topic of the query. The contextual relevancy of term proximity is combined with the value of term proximity to characterize how much a document should be boosted.
The remainder of this paper is organized as follows. In Section 2, we introduce four measures for estimating the contextual relevance of term proximity. In Section 3, we propose an enhanced context-sensitive proximity model using the proposed measures. Section 4 presents the experimental results and parameter sensitivities. Section 5 concludes the findings and discusses possible future directions.
2. CONTEXTUAL RELEVANCE OF TERM
PROXIMITY
In this section, we propose how to estimate the contextual relevance of term proximity. The contextual relevance of term proximity is defined as how much the corresponding term pair should be related to the topic of the query in the context. The notations used in this paper are shown as follows.
· Q = {q1, ..., qm} is a query · D is a relevant document · tf (qi, D) is the term frequency of qi in D · {pos1,i, pos2,i, ..., postfi,i} are the positions of qi in the
document D · dist(posk1,i, posk2,j ) is defined as dist(posk1,i, posk2,j ) =
|posk1,i - posk2,j |, which is the distance between two positions.
We measure the contextual relevance of term proximity base on the assumption that distributions of qi and qj in a relevant documents can represent the association between qi and qj. If qi and qj occur closely in relevant documents,

1131

RelCoOccur RelSqRecip RelM inDist RelKernel

q1, q2 1.0000 0.2331 0.0486 1.3200

q1, q3 1.0000 0.0408 0.0009 0.7200

q1, q4 1.0000 0.0434 0.0025 0.7200

q1, q5 0.0000 0.0000 0.0000 0.0000

q2, q3 1.0000 0.0523 0.0067 0.7200

q2, q4 1.0000 0.0434 0.0025 0.7200

q2, q5 0.0000 0.0000 0.0000 0.0000

q3, q4 1.0000 1.0000 0.3133 0.4800

q3, q5 0.0000 0.0000 0.0000 0.0000

Table 1: An example of the contextual relevance of term proximity

q4, q5 0.0000 0.0000 0.0000 0.0000

the contextual relevance of term proximity between qi and qj is high. On the contrary, if qi and qj do not co-occur or occur far away to each other, the contextual relevance of term proximity between qi and qj is low. Therefore we propose the following four methods for estimating the contextual relevance of term proximity between qi and qj in a relevant document D. For the extreme case when qi and qj do not co-occur in D, we consider the contextual relevance of term proximity equals 0. Otherwise, we define the following measures to generate a positive value for the contextual relevance of term proximity.

Definition 1. RelCoOccur(qi, qj , D) is defined to be 1, if qi and qj both occur in D.

RelCoOccur(qi, qj , D) = 1{qiDqj D}

(1)

Definition 2. The RelSqRecip is defined as the sum of squared reciprocal distances between qi and qj.

tf (qi,D) tf (qj ,D)

1

RelSqRecip(qi, qj , D) =
k1 =1

k2=1 dist(posk1,i, posk2,j )2

(2)

Definition 3. The RelMinDist is a defined as the following function of the minimum distance between qi and qj.
RelMinDist(qi, qj , D) = ln( + e-MinDist(qi,qj ,D)) (3)

where  is a parameter, and M inDist(qi, qj, D) is the minimum distance between all co-occurring qi and qj in D.
M inDist(qi, qj, D) = mink1{1..tf (qi,D)},k2{1..tf (qj ,D)}(dist(posk1,i, posk2,j ))

Definition 4. The RelKernel is defined as the sum of the kernel functions of distances between qi and qj.

RelKernel(qi, qj , D)

tf (qi,D) tf (qj ,D)

1

(4)

=

Kernel( 2 dist(posk1,i, posk2,j ))

k1=1 k2=1

where Kernel(·) is kernel function. Here we use the triangle

kernel function.

K ernel(u)

=

(1

-

u) 

·

1{u}

(5)

where u is an input value, and  is the kernel parameter.

These functions measure the contextual relevance from different perspectives. RelCoOccur measures whether qi and qj are co-occurring in D. RelSqRecip, RelMinDist and RelKernel considers the positions of qi and qj in D. In RelSqRecip, we generate a squared reciprocal function for the distances between all the occurrences of qi and qj, and accumulate the values over D. Then the query term pairs with terms occurring closer to each other and/or occurring more frequently

will have higher contextual relevance. RelMinDist is modified from [9], where the minimum distance is shown to be more effective than the other distance-based and span-based proximity approaches. RelKernel utilizes the term proximity approach proposed in [10], where a query term is simulated by the kernel function, where the triangle kernel function is recognized to be the most effective. Different types of information are incorporated in these measures.
To better analyze the contextual relevance measurements defined above, we present an example for a given query Q = {q1, q2, q3, q4, q5} and a relevant document D.
D = {xq1xq2xxxxq3q4xxxxxq1xq2}
where x represents a non-query term. By observing the query and the document, we find that the term q5 does not present in D, which means it does not related to D, and therefore do not have an association with other query terms. Since q3 and q4 are adjacent to each other and far apart from other query terms, q3 and q4 are more likely to have a stronger association than the combination of q2 and q4. We calculate the contextual relevance of term proximity between q2 and q4 as an instance, and the procedure will be the same for the rest of the query term pairs. The term frequency of terms q2 and q4 are tf (q2, D) = 2 and tf (q4, D) = 1. The positions of q2 and q4 in D are pos(q2, D) = {4, 18} and pos(q4, D) = {10} correspondingly. Therefore there are 2 co-occurrences of q2 and q4, and the corresponding distances between these co-occurrences are {6, 8}. Then we can calculate Rel(q2, q4, D) with these distances by formulae (1-4). Table 1 shows the values of the contextual relevance of term proximity in this example.
We can see that the contextual relevance measures defined above show different characteristics. RelCoOccur detects term pairs with or without an association. For example, q5 and other query terms do not have an association. The term pairs containing q5 are distinguished by RelCoOccur. On the other hand, RelCoOccur does not consider the term distributions in D. RelMinDist takes into account the closest occurrences between a pair of quay terms, and does not consider the frequency of occurrences. RelMinDist and RelKernel accumulates over all the occurrences of two query terms, with different functions and therefore generates different values.
3. CONTEXT-SENSITIVE PROXIMITY MODEL
In this section, we propose a context-sensitive proximity retrieval model, by integrating the proposed measures for contextual relevance of term proximity into retrieval process. Naturally we treat the values of contextual relevance as weights to reward the query term pairs with higher contextual relevance and to penalize the query term pairs with lower contextual relevance. In practice, we assume the top ranked documents returned by a basic retrieval model (for example, BM25) are more relevant than the rest of the documents. The averaged contextual relevance of term proximity over the top ranked documents is multiplied by the proximity part in the weighting function. A general form of the

1132

context-sensitive proximity model is

RelP rox(D )

= (1 - )  qi w(qi, D )

(6)

+   qi,qj AR(qi, qj , topDoc)  P rox(qi, qj , D )

where D is a given document, w(qi, D ) is the weight of qi in D by a basic probabilistic weighting function, P rox(qi, qj, D ) is a bigram proximity weighting approach,  is a balancing parameter, topDoc is the number of top ranked documents, AR(qi, qj, topDoc) is the average contextual relevance value of term proximity between qi and qj over the top ranked documents
1 AR(qi, qj, topDoc) = topDoc Rel(qi, qj, D) (7)
D
where Rel(qi, qj, D) is one of the measures defined in Section 2. Please note that AR(qi, qj, topDoc), P rox(qi, qj, D ) and w(qi, D ) need to be normalized to the same scale.
In formula (6), we use the probabilistic BM25 [5] as the basic weighting function. We adopt the proximity approach used in CRTER [10], since it is an effective pairwise proximity model for probabilistic IR. The BM25 weighting function has the following form.

w(qi, D )

= (k1 + 1)  tf (qi, D )  (k3 + 1)  qtf (qi)  log N - n(qi) + 0.5

K + tf (qi, D )

k3 + qtf (qi)

n(qi) + 0.5

(8)

where N is the number of documents in the collection, n(qi) is the number of documents containing qi, qtf (qi ) is the within-query term frequency, dl (D ) is the length D , avdl is the average document length, the kis are tuning constants, K equals k1  ((1 - b) + b  dl(D)/avdl). The proximity part of CRTER is shown as follows.

P rox(qi, qj , D ) = w(qi,j , D )

(9)

where qi,j represents the association between query terms qi and qj, w(qi,j, D ) is the BM25 weighting function with the following features of qi,j [10]

tf (qi,j ,

D

)

=

tf (qi,D

)

tf (qj ,D

)

K

er

nel(

1 2

dist(posk1

,i

,

posk2,j ))

k1 =1

k2 =1

qtf (qi,j )

=

K ernel(

1 2

)

·

min(qtf (qi),

qtf

(qj ))

n(qi,j ) =
D

tf (qi,j , D ) Occur(qi,j , D )

where Kernel(·) is a kernel function, and Occur(qi,j, D )

equals

tfi k1 =1

1 . tfj

k2 =1

{K

ernel(

1 2

dist(posk1

,i

,posk2

,j

))=0}

4. EXPERIMENTS

We evaluate the proposed approach on three standard

TREC data sets. They are AP88-89 with topics 51-100,

Web2G with topics 401-450, and TREC8 with topics 401-

450. AP88-89 contains articles published by Association

Press from the year of 1988 to 1989. The WT2G collec-

tion is a 2G size crawl of Web documents. The TREC8

contains newswire articles from various sources, such as Fi-

nancial Times (FT), the Federal Register (FR) etc. For all

the data sets used, each term is stemmed using Porter's En-

glish stemmer, and standard English stopwords are removed.

We have three baseline models, BM25, Dirichlet Language

Data Sets
BM25 Dirichlet LM
CRTER Improvement over BM25
RelCoOccur Improvement over BM25 Improvement over CRTER
RelSqRecip Improvement over BM25 Improvement over CRTER
RelM inDist Improvement over BM25 Improvement over CRTER
RelKernel Improvement over BM25 Improvement over CRTER

AP88-89
0.2708 0.2763
0.2744 1.329%
0.2768 2.216% 0.875%
0.2812* 3.840% 2.478%
0.2800 3.397% 2.041%
0.2812* 3.840% 2.478%

Web2G
0.3136 0.3060
0.3298* 5.166%
0.3375* 7.621% 2.335%
0.3444*  9.821% 4.427%
0.3444*  9.821% 4.427%
0.3425* 9.216% 3.851%

TREC8
0.2467 0.2552
0.2606 * 5.634%
0.2622* 6.283% 0.614%
0.2633* 6.729% 1.036%
0.2615* 5.999% 0.345%
0.2625* 6.405% 0.729%

Table 2: Overall MAP Performance ("*" indicates significant improvement over BM25, and "" indicates significant improvement over CRTER)

Model (LM) and CRTER. The best parameters are chosen in the baseline models for fair comparisons. In BM25, the values of k1, k2, k3 and b are set to be 1.2, 0, 8 and 0.35 respectively, since they are recognized with a good performance. In CRTER model, we use the recommended settings [10]., which are  = 25,  = 0.2, and triangle kernel function. In our proposed context-sensitive proximity model, we use the same parameters in the basic weighting model part (e.g. BM25) and the proximity part (e.g. CRTER). In RelKernel, we set the kernel parameter  = 25. In RelMinDist, we set  = 1, which has the best performance in [9]. We normalize AR(qi, qj, topDoc), P rox(qi, qj, D ) and w(qi, D ) in formula (6) to the scale of [0,1]. We use the Mean Average Precision (MAP) as our evaluation metric.
Table 2 shows the overall MAP performance. The proposed context-sensitive proximity model outperforms BM25, Language Model (LM) and CRTER with all of the contextual relevance measuring approaches on all the data sets. For the space limitation, we only include these comparisons. It shows that using the contextual relevance of term proximity can further boost the retrieval performance. We can see that the RelCoOccur, which measures whether two query terms are co-occurring in the relevant documents, reaches the lowest MAP among the contextual relevance measures, which indicates the necessity of considering the term location information in the term pair contextual relevance definition. RelSqRecip has the highest MAP over the other approaches on all the data sets. In general, considering both the closeness and frequency of two query terms in the contextual relevance definition benefits the contextual relevance estimation.
In Table 3, we investigate how the number of top relevant documents affects the retrieval performance. We take the topDoc = 5, 10, 20, 30, 40, 50 60, 70, and 80 documents as relevant, and calculate the average contextual relevance obtained from these documents. The bolded values are the best performance among different topDoc values. We can see that the best topDoc will be around 5 to 40. It means that selecting too many top documents as relevant will introduce noises to the model.
Figure ?? shows the sensitivity of  on all the data sets. We can see that with the growth of , MAP first increases

1133

AP88-89 Web2G TREC8

RelCoOccur RelSqRecip RelM inDist RelKernel
RelCoOccur RelSqRecip RelM inDist RelKernel
RelCoOccur RelSqRecip RelM inDist RelKernel

5
0.2757 0.281 0.2800 0.2812
0.3348 0.3401 0.3351 0.3358
0.2622 0.2633 0.2612 0.2625

10
0.2766 0.2812 0.2794 0.2796
0.3359 0.342 0.3444 0.3409
0.2612 0.263 0.2614 0.2622

20
0.2768 0.2801 0.2800 0.2801
0.3354 0.3409 0.3421 0.3425
0.2614 0.2627 0.2615 0.2619

30
0.2767 0.2801 0.2796 0.2801
0.3367 0.3401 0.3382 0.3406
0.2611 0.2623 0.2612 0.2615

40
0.2762 0.2789 0.2797 0.2789
0.3375 0.3444 0.3406 0.3423
0.2611 0.2617 0.2606 0.2607

50
0.2759 0.2781 0.2784 0.2781
0.3369 0.3435 0.3414 0.3418
0.261 0.2615 0.2605 0.2604

60
0.2758 0.278 0.279 0.2783
0.3367 0.3433 0.3427 0.3421
0.2608 0.2613 0.2606 0.2605

Table 3: Performance over the change of topDoc

70
0.2755 0.2777 0.2787 0.2781
0.3374 0.3424 0.3434 0.3414
0.2601 0.2614 0.2607 0.2605

80
0.2753 0.2774 0.2786 0.278
0.3363 0.3419 0.3433 0.3399
0.2599 0.2612 0.2605 0.2606

0.285

0.28

0.275

0.27

0.265

0.26

0.255

QualityCoOccur

0.25

QualitySqRecip

0.245

QualityMinDist QualityKernel

0.24

0

0.2

0.4

0.6

0.8

1



(a) AP88-89

0.36

0.35

0.34

0.33

0.32

0.31

0.3

QualityCoOccur

0.29

QualitySqRecip

0.28

QualityMinDist

QualityKernel

0.27

0

0.2

0.4

0.6

0.8

1



(b) Web2G Figure 1: Sensitivity of 

0.26

0.25

0.24

0.23

QualityCoOccur

QualitySqRecip

0.22

QualityMinDist

QualityKernel

0.21

0

0.2

0.4

0.6

0.8

1



(c) TREC8

MAP MAP MAP

and then decreases. Please note that when  = 0, there is no proximity utilized, which is our baseline BM25. When  = 1, only term proximity and the contextual relevance of term proximity are considered. In CRTER, the recommended setting for the balancing parameter is 0.2. After introducing the contextual relevance of term proximity, we can see that the balancing parameter  with a value of around 0.3 or 0.4 is better. The reason is that the contextual relevance is normalized to [0,1]. The value for the second part of formula (6) becomes smaller, therefore it requires a larger balancing parameter.
5. CONCLUSIONS AND FUTURE WORK
We propose a new approach to integrate the contextual relevance of term proximity in retrieval. The contextual relevance of term proximity evaluates how much we should focus on a pair of query terms. In particular, we propose four measures to estimate the contextual relevance of term proximity, namely RelOcOccur, RelSqRecip, RelMinDist, and RelKernel. They incorporate different types of information utilized in the contextual relevance of term proximity. We further propose a context-sensitive proximity model via multiplying the contextual relevance of term proximity by the proximity part in a retrieval model.
We evaluate our proposed context-sensitive proximity model on several TREC standard data sets, and the experimental results show the effectiveness of our proposed model over three baselines BM25, Dirichlet LM and CRTER with optimal parameter settings. In more detail, we discuss how many top documents should be selected for calculating the proximity contextual relevance, and how the balancing parameter  affects the retrieval performance.
In the future, we can extend the contextual relevance of term proximity to more query terms. In addition, the contextual relevance of term proximity can be adopted in other

basic weighting models (e.g. Language Model) and/or other
proximity approaches. We can also apply the proposed
model into relevance feedback.
6. ACKNOWLEDGEMENTS
This research is supported by the research grant from the
NSERC of Canada and the Early Researcher Award. We
thank four anonymous reviewers for their thorough review
comments on this paper.
7. REFERENCES [1] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In Proc. of WSDM, pages 31­40. ACM, 2010. [2] S. Buttcher, C. Clarke, and B. Lushman. Term proximity scoring for ad-hoc retrieval on very large text collections. In Proc. of SIGIR, page 622. ACM, 2006. [3] W. Croft, D. Metzler, and T. Strohman. Search engines: Information retrieval in practice. Addison-Wesley, 2010. [4] C. Macdonald and I. Ounis. Global statistics in proximity weighting models. In Web N-gram Workshop, pages 30­37, 2010. [5] S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, M. Gatford, et al. Okapi at TREC-3. In Proc. of TREC, pages 109­126. NIST, 1995. [6] L. Shi and J.-Y. Nie. Using various term dependencies according to their utilities. In Proc. of CIKM, pages 1493­1496. ACM, 2010. [7] R. Song, M. J. Taylor, J.-R. Wen, H.-W. Hon, and Y. Yu. Viewing term proximity from a different perspective. In Proc. of ECIR, pages 346­357. Springer, 2008. [8] K. M. Svore, P. H. Kanani, and N. Khan. How good is a span of terms? exploiting proximity to improve web retrieval. In Proc. of SIGIR, pages 154­161. ACM, 2010. [9] T. Tao and C. Zhai. An exploration of proximity measures in information retrieval. In Proc. of SIGIR, pages 259­302. ACM, 2007. [10] J. Zhao, J. Huang, and B. He. CRTER: using cross terms
to enhance probabilistic information retrieval. In Proc. of SIGIR, pages 155­164, 2011. [11] J. Zhao, J. X. Huang, and Z. Ye. Modeling term associations for probabilistic information retrieval. ACM Transactions on Information Systems (TOIS), 32(2):7, 2014.

1134

On the Information Difference between Standard Retrieval Models

Peter B. Golbus and Javed A. Aslam College of Computer and Information Science
Northeastern University Boston, MA, USA
pgolbus,jaa@ccs.neu.edu

ABSTRACT
Recent work introduced a probabilistic framework that measures search engine performance information-theoretically. This allows for novel meta-evaluation measures such as Information Difference, which measures the magnitude of the difference between search engines in their ranking of documents. for which we have relevance information. Using Information Difference we can compare the behavior of search engines--which documents the search engine prefers, as well as search engine performance--how likely the search engine is to satisfy a hypothetical user. In this work, we a) extend this probabilistic framework to precision-oriented contexts, b) show that Information Difference can be used to detect similar search engines at shallow ranks, and c) demonstrate the utility of the Information Difference methodology by showing that well-tuned search engines employing different retrieval models are more similar than a well-tuned and a poorly tuned implementation of the same retrieval model.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation (efficiency and effectiveness)
Keywords
Information Retrieval; Search Evaluation
1. INTRODUCTION
One of the ways search engines are compared is by computing the magnitude of the difference between their performance using some IR evaluation measure. These measures attempt to quantify the satisfaction of a hypothetical user of a search engine. This is crucial information about a search engine, but it does not necessarily provide a great deal of insight into search engine behavior --which documents does the search engine prefer and why? For example it is possible
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609528.

for two search engines to retrieve wildly different documents, or to rank similar documents in a very different order, and yet receive the same score from an evaluation metric, even a diversity metric such as ERR-IA [3]. It is equally possible for two ranked lists to be highly similar and yet for one system to have a much greater performance than the other.
Recently, Golbus and Aslam [2] introduced a probabilistic framework that measures performance informationtheoretically. The advantage of this approach is that it provides additional novel interpretations beyond simply estimating user satisfaction. For example, the authors defined Information Difference, which measures the magnitude of the difference between systems in their ranking of documents for which we have relevance information. The authors demonstrated that Information Difference can be used to detect similar search engines whereas performance deltas cannot.
However, the probabilistic framework underlying Information Difference required a recall-oriented approach and was evaluated at rank 1000. This leads to the concerns that Information Difference detected the similarities between systems based on the uninteresting long "tail" of the ranked lists. In this work, we a) adapt the probabilistic framework to precision-oriented tasks at shallow ranks. We demonstrate that b) even when evaluated at rank 20, Information Difference is still able to detect similar systems with high accuracy. Therefore, Information Difference relies on whether systems choose the same highly relevant documents at the top. Finally, we c) demonstrate the utility of the Information Difference methodology by showing that well-tuned search engines employing different retrieval models are more similar than a well-tuned and a poorly-tuned implementation of a retrieval model, i.e. that a well-tuned instantiation of BM25 is more similar to a well-tuned instantiation of a language model than it is to a poorly tuned instantiation of BM25.
2. INFORMATION DIFFERENCE
In this section, we provide an overview of the Information Difference methodology described in [2].
Mathematically, one can view a search system as providing a total ordering of the documents ranked and a partial ordering of the entire collection, where all ranked documents are preferred to unranked documents but the relative preference among the unranked documents is unknown. Similarly, one can view relevance assessments--commonly referred to as QRELs--as providing a partial ordering of the entire collection: in the case of binary relevance assessments, for exam-

1135

ple, all judged relevant documents are preferred to all judged non-relevant and unjudged documents, but the relative preferences among the relevant documents and among the nonrelevant and unjudged documents is unknown. Thus, mathematically, one can view retrieval evaluation as comparing the partial ordering of the collection induced by the search system with the partial ordering of the collection induced by the relevance assessments.
Golbus and Aslam described a probabilistic framework within which to compare two such orderings, defined in terms of three things: (1) a sample space of objects, (2) a distribution over this sample space, and (3) random variables over this sample space. For example, Golbus and Aslam defined a new evaluation measure, Relevance Information Correlation in the following way. Let the sample space,  = {(di, dj) | rel(di) = rel(dj)}, be the set of all ordered pairs of judged documents with different relevance grades. Let P = U be the uniform probability distribution over all such pairs of documents. We define a QREL variable Q over ordered pairs of documents as

Q [(di, dj )] =

1 0

if rel(gi) > rel(gj) otherwise.

(1)

The ranked list variable R is computed by truncating the list at the last retrieved relevant document. Let ri represent the rank of document di.

 1  R [(di, dj)] = 0
-1

if ri < rj if neither di nor djwere retrieved (2) otherwise.

Relevance Information Correlation is the mutual information between the QREL variable Q and the truncated ranked list variable R.

RIC(System) = I(RSystem; Q).

(3)

This quantity is estimated via Maximum Likelihood for a given QREL and system.
This definition is inherently recall-oriented. In this work, we propose a precision-oriented version, RIC@k. RIC@k differs from RIC in two ways. First, we normalize with respect to the maximum possible RIC@k of an ideal ranked list, as with nDCG. Second, we alter the probability distribution so as to give more weight to documents with higher relevance grades. To do so, we begin by observing that evaluation metrics can be viewed as inducing probability distributions over ranks. For example, Carterette [1] defines the probability of stopping at a rank k according to nDCG as

1

1

PDCG(k)

=

log2(k

+ 1)

-

log2(k

. + 2)

(4)

Imagine a QREL with Rgmax documents relevant at the highest grade. According to the QREL these documents are equally likely to appear at ranks one through Rgmax, but have zero probability of appearing anywhere else. Therefore, in any ideal ranked list, the probability associated with one of these documents will be PDCG(k) for some k with 1  k  Rgmax. Therefore, we define the probability of a document as the average probability of the ranks at which the document can appear in an ideal list. If Rg is the number of documents that are relevant at grade g, then for a document d with such that rel(d) = g, the minimum rank

System 1

System 2

QREL

Figure 1: Information Difference corresponds to the symmetric difference between the intersections of the systems with the QREL in information space.

for this document in an ideal list

gmax

kmin =

Ri,

(5)

i=g+1

i.e. after all of the documents with higher relevance grades, and the maximum rank is

kmax = kmin + Rg.

(6)

Then the probability associated with the document is

kmax

P (d)

=

 i=kmin

1 log2 (i+1)

-

1 log2 (i+2)

Rg

=



1 log2 (kmin +1)

-

1 log2 (kmax +2)

,

(7)

Rg

where  is a normalizing constant. Note that the probability of non-relevant documents is non-zero, and that this definition can also be used for binary relevance.
RIC requires us to define a probability distribution over document pairs, whereas Equation 7 defines a probability for documents. To create the appropriate distribution, we assume that each document in the pair is chosen independently,

P (di, dj ) = P (di)P (dj )

(8)

where  is a normalizing constant that ensures that P (di, dj) forms a distribution.
We define RIC@k by normalizing by the ideal ranked list, as in nDCG, and computing mutual information with respect to the probability distribution defined in 8.

RIC@k(S) = I(RS; Q)

(9)

I(Rideal; Q)

Information Difference is inspired by the Boolean Algebra symmetric difference operator as applied to information space, corresponding to the symmetric difference between the intersections of the systems with the QREL (see Figure 1). For two systems S1 and S2,

id(S1, S2) = I(RS1 ; Q | RS2 ) + I(RS2 ; Q | RS1 ), (10)
with Q and R defined as in Equations 1 and 2 respectively. We also define id@k by using the probability distribution described in Equation 8, and by normalizing with respect to an ideal system.

id@k(S1, S2)

=

I(RS1 ; Q

|

RS2 ) + I(RS2 ; Q I(Rideal; Q)

|

RS1 ) .

(11)

1136

Figure 2: ROC curve of RIC (left) and Information Difference (right) when used to predict whether systems with similar performance are in fact "similar" (as described in Section 3.1).

Figure 3: Performance as a function of retrieval model parameters

3. ANALYSIS
In this section, we employ the framework described in Section 2 to demonstrate the utility of the Information Difference framework. In Section 3.1, we show that Information Difference can be be used to determine whether systems are similar, even at shallow ranks, whereas performance deltas cannot. In Section 3.2, we demonstrate the use of Information Difference as a tool for meta-evaluation by performing a simplistic experiment concerning the similarity between search engines.
3.1 Detecting Similarity
We wish to detect whether two systems are similar. As a proxy for similarity, we will consider two systems submitted to TREC1 to be "similar" iff they were submitted by the same group. It is reasonable to assume that the majority of these systems were different instantiations of the same underlying technology, although there will be many instances where this is not the case at all. We repeat the experiment first performed by Golbus and Aslam [2] to show that the results also hold when performed in a precision-oriented fashion (at rank 20), and are therefore not dependent on the long and uninteresting "tail" of the ranked lists.
Using the same construction, we sorted all the systems submitted to TREC 8 and TREC 9 by RIC and RIC@20, and separated them into twenty equal-sized bins. By construction, each bin contained systems with small differences in performance. All systems within each bin are compared to one another using Information Difference and performance delta. Figure 2 shows the resulting ROC curves when Information Difference and performance delta are used to predict whether two systems meet our proxy for similarity described above. We also report the area under the ROC curves averaged over all four conditions (TREC 8 vs TREC 9; RIC
1The annual, NIST-sponsored Text REtrieval Conference (TREC) creates test collections commonly used in academic research.

vs RIC@20). It is quite evident that Information Difference, with an average AUC greater than 0.9, is quite capable of detecting similarities as reported by our proxy, even at shallow ranks. It is equally evident that with an average AUC less than 0.6, performance delta is not capable of detecting similarities. This result is obvious--Information Difference was constructed for just this purpose, whereas performance delta is not. However, we believe that this result is also important. Since performance delta has traditionally been used to measure the similarities between search engines, our notions of which systems are similar may be false.
3.2 Measuring Differences
In this section, we will attempt to determine whether the choice of retrieval model has a bigger impact on the behavior (rather than the performance) of a search engine than does parameter tuning. To perform this experiment, we use a standard, state-of-the-art search engine, in this case the Terrier search engine [4], to create highly simple search engines, i.e. without query expansion, pseudo-relevance feedback, etc., and analyze the results when our systems are run over TRECs 8 and 9. We compare 1) a query-prediction language model (LM) with Dirichlet smoothing, 2) BM25, and 3) Divergence from Randomness (DFR) models,2 across a range of 21 different, evenly spaced, "reasonable" parameter values (see Figure 3), and the "best" of these observed parameter values which achieves the maximum performance (see Table 1).3 As we can see from Table 1, the three models perform relatively consistently with one another. Figure 3 shows that, with the exception of BM25 and DFR at rank 20, each model has reasonably consistent performance with itself as parameters are tuned.
2We use IneB2 for RIC and PL2 for RIC@20. 3Our goal is to compare search engines during the evaluation phase, when relevance assessments have already been used. Therefore we employ the best parameters for these queries, rather than optimal parameters applicable to future queries.

1137

RIC
BM25 LM DFR

TREC8
0.292 0.314 0.323

TREC8@20
0.325 0.294 0.302

TREC9
0.344 0.358 0.349

TREC9@20
0.295 0.277 0.296

Table 1: Best observed performance of standard retrieval models.

ID
LM DFR LM BM25 BM25 DFR

TREC8
0.076 0.068 0.077

TREC8@20
0.110 0.149 0.091

TREC9
0.088 0.092 0.108

TREC9@20
0.122 0.127 0.056

Table 2: Information Difference between standard retrieval models with "best" parameters (Section 3.2).

Figure 4: Cumulative histograms of Information Difference between parameterizations of a standard retrieval model.
Using Information Difference we can now measure the similarity of these models are in terms of their behavior, rather than their performance. Recalling that a small Information Difference implies a high degree of similarity, consider Table 2, which shows the Information Difference between the best performing retrieval models. As a point of reference, using an Information Difference threshold of 0.1 would have achieved a roughly 92% average accuracy on the classification task described in Section 3.1. Therefore, it is quite likely that Information Difference would have failed in this case and considered these retrieval models to be the same system.
Now we consider the difference between instantiations of a single model. We instantiate each model with all 21 parameter values and compute the Information Difference between each pair of instantiations. Figure 4 shows cumulative histograms of the Information Difference between all 21 choose

2 pairs of these model instantiations. These histograms show that there is far more difference in behavior within a model across parameterizations than their is across models with the best parameterization. For example, consider the largest Information Difference between models, which is between our language model and BM25 on TREC 8 at rank 20. The Information Difference of 0.149 is smaller than roughly 40% of the pairs of BM25 models. The smallest Information Difference is between BM25 and DFR on TREC 9 at rank 20. The Information Difference of 0.056 is smaller than all but roughly 45% of the pairs of LM models.
4. CONCLUSION
The probabilist framework developed by Golbus and Aslam leads to interesting, novel, and highly interpretable metaevaluations within the same context as traditional evaluation. As originally introduced, this framework was only applicable in deeply judged, recall-oriented contexts, greatly reducing its practical value. In this work, we extended this probabilistic framework to precision-oriented contexts, which are far more prevalent. We also showed two novel applications of Information Difference, a tool developed within this framework. We showed that Information Difference can be used to detect similar search engines at shallow ranks. We also showed that Information Difference can be used as a tool for meta-evaluation by showing that well-tuned search engines employing different retrieval models are more similar than a well-tuned and a poorly-tuned implementation of the same retrieval model.
5. REFERENCES
[1] Ben Carterette. System effectiveness, user models, and user utility: A conceptual framework for investigation. In SIGIR `11.
[2] Peter B. Golbus and Javed A. Aslam. A mutual information-based framework for the analysis of information retrieval systems. In SIGIR `13.
[3] Peter B. Golbus, Javed A. Aslam, and Charles L.A. Clarke. Increasing evaluation sensitivity to diversity. Information Retrieval, 16(4), 2013.
[4] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A High Performance and Scalable Information Retrieval Platform. In OSIR `06.

1138

Using Score Differences for Search Result Diversification

Sadegh Kharazmi Mark Sanderson Falk Scholer
RMIT University & NICTA, Melbourne, Australia
{sadegh.kharazmi, mark.sanderson, falk.scholer}@rmit.edu.au

David Vallet
NICTA, Sydney, Australia
david.vallet@nicta.com.au

ABSTRACT
We investigate the application of a light-weight approach to result list clustering for the purposes of diversifying search results. We introduce a novel post-retrieval approach, which is independent of external information or even the full-text content of retrieved documents; only the retrieval score of a document is used. Our experiments show that this novel approach is beneficial to effectiveness, albeit only on certain baseline systems. The fact that the method works indicates that the retrieval score is potentially exploitable in diversity.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval­retrieval models, search process
General Terms
Algorithm, Theory, Experimentation
Keywords
Diversity; Score Difference; Clustering
1. INTRODUCTION
User queries submitted to an Information Retrieval (IR) system are often ambiguous at different levels [7]. To address such ambiguity, IR systems attempt to diversify search results, so that they cover a wide range of possible interpretations (aspects, intents or subtopics) of a query. Consequently, the number of redundant items in a search result list should be decreased, while the likelihood that a user will be satisfied with any of the displayed results should become higher.
In traditional IR, the estimated relevance of a document, which is used to determine the ranking of search results, depends primarily on query-document similarity. In diversified retrieval, search result rankings are based not only on querydocument similarity, but also on the other documents that
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609530.

have been retrieved prior to the current document under consideration (i.e., document-document similarity).
Many of the proposed diversification techniques take a greedy approach, comparing a document to all previously retrieved documents, or the subtopics of a query. Also, they may use additional information, such as past user interactions, to identify which of the possible subtopics of a query are more likely to be interesting to the user. Most effective diversification approaches in the literature use techniques that focus on coverage, favoring documents that cover as many novel subtopics of a query as possible. This is in contrast to earlier techniques that focus on novelty, estimating the newness of a document with respect to those already retrieved. Novelty-based techniques usually exploit implicit information, such as differences in document content.
One source of implicit information derived from search results that appears to have never been investigated are differences in retrieval scores: score differences. Retrieval systems will usually, in response to a query, return a list of documents sorted by a relevance score, indicating the degree to which a query and document match. When analyzing a retrieved document list, the differences between the scores of adjacent retrieved documents differ, and this variation might be exploitable. Two documents that receive similar relevance scores are likely to share similar features; they might therefore address the same subtopics of a user's query. Conversely, two adjacent documents that have a large score difference are likely to have fewer features in common, which suggests that the documents might cover different query subtopics.
Our research question is to ask if we can exploit score differences to help with search result diversification.
We develop a simple non-greedy diversification approach that uses differences between the scores of the initially retrieved documents. The approach is experimentally investigated using the TREC framework, comparing to baselines and state-of-the-art diversification approaches.
2. RELATED WORK
There are two key approaches to diversifying search results, based on explicit or implicit evidence [9]. The explicit approaches [1, 10] match retrieved documents to the subtopics of a query, which are "pre-derived" from external sources such as a query log or taxonomies. Implicit approaches attempt to diversify based on a representation of already-retrieved documents.
Maximal Marginal Relevance (MMR) [2] is perhaps the most widely-studied implicit approach. Here, a diverse set of

1143

results (S) is built incrementally from an initial retrieved list (R). The results are picked from R using a greedy approach where, in each iteration, the document that is most novel is selected. Novelty in this case is defined as the mean contentbased dissimilarity between the candidate document and the already selected documents in S. A tuning parameter  defines the trade-off between relevance and diversity.
Inspired by Modern Portfolio Theory (MPT) in finance, Wang et al. introduced a new implicit approach that analyses the expected mean and variance of the return of a portfolio [11]. Facility Location Analysis (FLA) was introduced by Zuccon et.al [12] to improve the MPT approach.
Explicit approaches are focused on query subtopics, which can be derived from a pre-defined taxonomy such as the Open Directory Project1 (ODP), internal document features, query logs, or online resources [1, 8, 9]. The two most effective explicit diversification approaches are xQuAD [10] and IASelect [1].
All of these approaches use an iterative greedy selection approach to rank the most diverse documents.
3. SCORE DIFFERENCES
We hypothesize that documents with similar features (e.g., content, aspects covered, length) will be allocated (by ranking functions) similarity scores that are close together. Conversely, documents with different features are likely to be allocated similarity scores that are further apart.
Let D1, D2, . . . , DN be an initial ranking of documents which are ordered by a ranking function s(D), and  be a difference threshold parameter. Then if
|s(Di) - s(Di+1)| <  s(Di+1)
we assume that the documents cover the same query subtopic. If the value is  , it indicates that the two documents belong to different subtopics.
To test our hypothesis, we set up an experiment to measure the score differences between pairs of adjacent ranked documents that either covered the same or different subtopics of a query. We used the documents, queries, and diversity relevance judgments from the TREC 2009­2011 Web Tracks [3, 4]. Documents were ranked using the Dirichletsmoothed language model from the Indri IR system2 with default parameter settings. The query subtopics covered by individual answer documents are defined in the TREC relevance judgments. All non-relevant documents were assigned to an extra "non-relevant" subtopic. In total, 148 topics (TREC Web Track 2009, 2010 and 2011 queries) were used and for each topic, and pairs of documents in the top 100 positions in a ranked list were examined.
We tested the Language Modeling (LM) and Okapi BM25 ranking functions, which are widely used in retrieval research, and have been shown to be effective ranking functions [6]. First, using language modelling as the ranking function to score documents, our analysis shows that for pairs of documents where there is no change in subtopic, the mean measured score difference is 0.065. Conversely, when the subtopic changes, the mean difference in scores is 0.073. Although the differences are small, a pairwise permutation test indicates that they are statistically significant (p
1http://www.dmoz.org/ 2Version 5.2, http://lemurproject.org/indri.php

< 0.01). This analysis suggests that score differences have the potential to be used as a technique to help with result diversification.
We repeated the same experiment using BM25. The mean measured score difference when there is no change in subtopic is 0.069, versus 0.071 when the subtopic changes. A pairwise permutation test indicates that these differences are not statistically significant. We therefore hypothesise that score differences are less likely to work well when the BM25 ranking function is used as a base run.

4. DIVERSIFYING RESULTS USING SCORE DIFFERENCES
To apply the score differences technique for result diversification, first, the score difference between each pair of documents, starting at rank position 1, was calculated. The top 100 documents were then re-ranked by decreasing size of the score difference between each document and the document above it. The documents with the biggest difference between the paired documents would now be top ranked, and they should be documents covering different subtopics.
After some experimentation with this simple approach, we found that it was better to re-rank documents based on a linear combination of the rank positions in the initial ranking and score differences, as shown in Algorithm 1. This approach to diversification, RankScoreDiff, does not use any information apart from the similarity scores from an initial retrieval run. It is therefore an implicit diversification approach.

Algorithm 1 RankScoreDiff(L)

L  ScoreDiff(L)

for 1 < i  |L| do

Score(L[i])



1 Rank(L[i])

+

1 Rank(L

[i])

end for

Sort L on Score(L[i])

5. EXPERIMENTAL SETUP
We investigated the effectiveness of RankScoreDiff as a diversification approach using the diversity task framework of the TREC Web Track from 2009­2011, which comprises 148 queries. Results are reported over the Clueweb category B collection. The Clueweb Online Services3 were used to retrieve the top 100 documents for each query, using the same ranking functions and version of Indri described in Section 3. The top 100 documents were diversified using the methods under test.
Effectiveness was measured with -nDCG, a widely-used metric that incorporates both relevance and diversity into a single score. The parameter , which sets the relative importance of these two evaluation considerations, was set to 0.5, as recommended by the creators of the measure [5]. In the subsequent presentation of results, two-tailed paired t-tests are used to evaluate statistical significance.
5.1 Implementation and Tuning
For diversity approaches that require explicitly defined subtopics as an input (xQuAD and IASelect), two sources of
3http://boston.lti.cs.cmu.edu/Services/

1144

Runs Initial Run (LM) MMR MPT FLA-MPT RankScoreDiff xQuAD RankScoreDiff + xQuAD IASelect RankScoreDiff + IASelect

-nDCG@5
0.235 0.233 0.235 0.240 0.246 * 0.246  0.258 * 
0.266 0.283 *  

ODP Subtopics -nDCG@10
0.276 0.274 0.277 0.280
0.274 0.286  0.291 
0.298 0.315 *  

-nDCG@20 0.315 0.317 0.316 0.320 0.324 * 0.326  0.332 0.337 0.347 *

-nDCG@5
0.235 0.233 0.232 0.240 0.246 * 0.318 **  0.318 **  0.321 **  0.323 ** 

TREC Subtopics

-nDCG@10 -nDCG@20

0.276

0.315

0.274

0.317

0.277

0.319

0.280

0.320

0.274 0.357 **  0.358 **  0.365 **  0.367 ** 

0.324 * 0.396 ** 0.397 **  0.400 ** 0.405 **

Table 2: Effectiveness of diversification approach using language modeling as baseline. For approaches that need explicit representation of subtopics, TREC official subtopics and ODP subtopics were used.

Implicit Explicit

Method
MMR MPT F LA - M P T
xQuADODP I AS electODP xQuADT rec IASelectT rec

Spearman's  0.92 0.86 0.83 0.78 0.75 0.67 0.71

Table 1: The Spearman correlation of RankScoreDiff with other diversification approaches in terms of effectiveness measured by -nDCG@20.

subtopic definitions were used: first, the TREC Web Track official subtopics; and second, subtopics derived from the ODP using TextWise4 services, with three levels of categorization to generate subtopics. These subtopics represent an upper-bound on effectiveness (perfect knowledge from the relevance judgments), and a reasonable but imperfect approach, respectively.
All approaches used in the experiments were trained to provide the best possible uniform diversification, to ensure that comparisons between the methods are fair. For approaches that required the tuning of parameters, this was carried out using 10-fold cross-validation to determine the best value on each collection. Parameters were tuned at increments of 0.1, and the best  value obtained as 0.8 for the ODP subtopics and 0.9 for the official TREC subtopics.
5.2 Experimental Results
We investigated the impact on effectiveness of the proposed approach as a diversification feature and compared it to existing approaches in the literature [1, 2, 10, 11, 12].
Table 1 shows the Spearman correlation between our proposed approach and other diversification approaches. The results show that, in general, RankScoreDiff is more strongly correlated with implicit diversification approaches than with explicit approaches; the difference with the latter becomes more pronounced when the TREC (perfect) subtopics are available.
The results of our effectiveness experiments are shown in Tables 2 and 3, for the LM and BM25 initial retrieval runs, respectively. To carry out a detailed analysis, different baselines were considered. For this reason the following comparisons were made:
· A significant difference between the measured technique and the initial run is shown using * (p < 0.05) and ** (p < 0.01).
4http://www.textwise.com/

· A significant difference between the measured technique and implicit approaches (MMR, MPT, FLAMPT and RankScoreDiff), which are independent of external knowledge such as subtopics, is shown using  (p < 0.05) and  (p < 0.01). (The symbol indicates that a technique is significantly better than all four of the implicit approaches at the specified level.)
· A significant difference between a state-of-the-art explicit diversification method (xQuAD or IASelect), compared to RankScoreDiff combined with that method, is shown using  (p < 0.05) and  (p < 0.01).
Language model as a baseline
Table 2 shows the results when using LM as an initial retrieval run. It can be seen that RankScoreDiff (row 5) significantly improves over the base run (row 1) for -nDCG@5 and -nDCG@20. Although there is some marginal improvement in comparison with other implicit approaches (rows 2-4), this improvement is only significant for -nDCG@5. The results suggest that RankScoreDiff is competitive in comparison with implicit approaches, but it is not as good as the explicit approaches (rows 6 and 8). However, RankScoreDiff can also be used in combination with the explicit approaches (rows 7 and 9 of the table).
Using ODP subtopics, the combination of RankScoreDiff with an explicit approach improves over using the explicit approach on its own in most cases. The improvement is significant for -nDCG@5 and -nDCG@10 when IASelect and RankScoreDiff are combined. Using the TREC (perfect) subtopics, marginal improvements are obtained when combining RankScoreDiff with the explicit approaches, however the combined approach is not significantly different compared with the original explicit approach. In addition, the combined approaches are always significantly better than the base run, and are usually significantly better than the implicit approaches.
OKAPI BM25 as a baseline
Table 3 shows results when using BM25 as a baseline run. The improvements in effectiveness over the base run are marginal for all implicit approaches, including RankScoreDiff.
Furthermore, Table 3 shows that for the ODP subtopics, even the explicit approaches do not lead to significant improvements over the base run. Similarly, combining RankScoreDiff with xQuAD and IASelect for the ODP subtopics does not improve significantly on the baseline, and in some cases reduces effectiveness. When using TREC (perfect) subtopics, all explicit approaches (on their own, or combined

1145

Runs Initial Run (BM25) MMR MPT FLA-MPT RankScoreDiff xQuAD RankScoreDiff + xQuAD IASelect RankScoreDiff + IASelect

-nDCG@5 0.268 0.266 0.270 0.275 0.270 0.273 0.275 0.263 0.256

ODP Subtopics -nDCG@10 0.300 0.300 0.301 0.305 0.298 0.309 0.309 0.294 0.288

-nDCG@20 0.336 0.337 0.336 0.340 0.335 0.344 0.339 0.331 0.323

-nDCG@5
0.268 0.266 0.272 0.275 0.270 0.335 **  0.341 **  0.348 ** 0.343 ** 

TREC Subtopics

-nDCG@10 -nDCG@20

0.300

0.336

0.300

0.337

0.302

0.337

0.305

0.340

0.298

0.335

0.377 ** 

0.407 **

0.378 ** 

0.409 **

0.389 ** 

0.420 **

0.384 ** 

0.413 **

Table 3: Effectiveness of diversification approach using Okapi (BM25) as baseline. For approaches that need explicit representation of subtopics, TREC official subtopics and ODP subtopics were used.

with RankScoreDiff) improve significantly over the base run and over the implicit approaches. The combination of RankScoreDiff and xQuAD could marginally improve over xQuAD on its own for -nDCG@5, while this is not the case for IASelect.
Overall, the results in Tables 2 and 3 show that RankScoreDiff is equivalent in effectiveness with other implicit approaches, although with no significant improvement over strong base runs. However, in the absence of perfect subtopics, RankScoreDiff can potentially be used in combination with explicit approaches to provide a boost in effectiveness. We note that a particular feature of RankScoreDiff is that it is computationally much less intensive than all other diversification approaches, being based only on information that is already available with a base run.
6. CONCLUSIONS
This paper examined a novel approach that uses the differences in original retrieval scores as evidence of diversity, based on the assumption that similar documents will receive similar retrieval scores with respect to a given query, and that similar documents could represent a similar subtopic. We experimentally evaluated the use of a score difference technique to diversify search results. In contrast with existing diversification techniques, which need additional document representations or external subtopics, our proposed approach only needs the relevance score provided by a ranking function. From the results, diversifying using score differences is competitive with other implicit diversification approaches. However, none of these approaches regularly lead to significant improvements over a base run. When perfect subtopic knowledge is not available, the RankScoreDiff approach can potentially boost the effectiveness of state-ofthe-art explicit diversification techniques.
Our analysis of the distribution of score differences showed that the approach is directly affected by the ranking function that generates the initial retrieval scores.
In future work, we plan to investigate how particular features of ranking functions interact with the score differences approach. For example, a parameterised ranking function such as BM25 allows individual effects such as length normalisation, or the relative emphasis of TF and IDF effects, to be isolated and explored.

8. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proc. WSDM, pages 5­14. ACM, 2009.
[2] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. SIGIR, pages 335­336. ACM, 1998.
[3] C. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 web track. In Proc. TREC, 2009.
[4] C. Clarke, N. Craswell, I. Soboroff, and G. Cormack. Preliminary overview of the trec 2010 web track. In Proc. TREC, volume 10, 2010.
[5] C. Clarke, M. Kolla, G. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proc. SIGIR, pages 659­666. ACM, 2008.
[6] W. B. Croft, D. Metzler, and T. Strohman. Search engines: Information retrieval in practice. Addison-Wesley Reading, 2010.
[7] F. Radlinski, P. Bennett, B. Carterette, and T. Joachims. Redundancy, diversity and interdependent document relevance. In Proc. SIGIR, volume 43, pages 46­52. ACM, 2009.
[8] F. Radlinski and S. Dumais. Improving personalized web search using result diversification. In Proc. SIGIR, pages 691­692. ACM, 2006.
[9] R. L. T. Santos, C. Macdonald, and I. Ounis. On the role of novelty for search result diversification. Information Retrieval, 2012.
[10] R. L. T. Santos, J. Peng, C. Macdonald, and I. Ounis. Explicit search result diversification through sub-queries. In Proc. ECIR, pages 87­99, Milton Keynes, UK, 2010. Springer.
[11] J. Wang and J. Zhu. Portfolio theory of information retrieval. In Proc. SIGIR, pages 115­122. ACM, 2009.
[12] G. Zuccon, L. Azzopardi, D. Zhang, and J. Wang. Top-k retrieval using facility location analysis. In Proc. ECIR, pages 305­316. Springer, 2012.

7. ACKNOWLEDGMENTS
This work was supported in part by the Australian Research Council (DP130104007), as well as NICTA Victoria which is funded by both the Federal and State governments.

1146

TREC: Topic engineeRing ExerCise

J Shane Culpepper Stefano Mizzaro Mark Sanderson Falk Scholer
School of Computer Science and Information Technology, RMIT University GPO Box 2476, Melbourne 3001, Victoria, Australia
{shane.culpepper,stefano.mizzaro,mark.sanderson,falk.scholer}@rmit.edu.au

ABSTRACT
In this work, we investigate approaches to engineer better topic sets in information retrieval test collections. By recasting the TREC evaluation exercise from one of building more effective systems to an exercise in building better topics, we present two possible approaches to quantify topic "goodness": topic ease and topic set predictivity. A novel interpretation of a well known result and a twofold analysis of data from several TREC editions lead to a result that has been neglected so far: both topic ease and topic set predictivity have changed significantly across the years, sometimes in a perhaps undesirable way.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Search and Retrieval
General Terms
Experimentation
Keywords
Evaluation; TREC; Topics
1. INTRODUCTION
Test collection-building conferences such as TREC, CLEF, or NTCIR aim at understanding and evaluating Information Retrieval (IR) system effectiveness, and are usually seen as exercises to develop better IR systems. We address a dual research question that to our knowledge has not been asked so far: Can these evaluation conferences be interpreted as a tool to develop better topics instead of better systems? Are the topics that have been produced over the years improving? We might even ask: When considering one of the best known of these conferences, could the acronym of TREC be recast from a system engineering exercise to a "Topic engineeRing ExerCise"? Permanent address: Dept. of Mathematics and Computer Science, University of Udine, Udine, Italy, mizzaro@uniud.it
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609531.

As part of an initial study, we present two possible approaches to quantify the "goodness" of a topic ­ topic ease and topic predictivity. Each of these approaches are explored in turn in Sects. 2 and 3. Sect. 4 summarizes our current progress, and outlines future work that our initial exploration of this area has generated.
2. TOPIC EASE
In the usual system-oriented approach, IR researchers try to understand if there is a trend in system effectiveness over multiple years using measures such as MAP.1 Instead, in this section we examine whether topic ease has changed over time.
2.1 Background
Mizzaro and Robertson [4] define easy topics as "topics to which runs tend to give high AP values", and topic ease can be quantified using AAP.2 However, this is not completely satisfying for our purposes: changes in the trend of AAP over the years of an evaluation exercise, such as TREC, could be caused by changes in the effectiveness of systems over those same years (a system effect). Fortunately there is a potential workaround, which we discuss next.
2.2 Experimental data
The workaround exploits a result that is now well-known in the TREC community: that ad hoc retrieval effectiveness on conventional TREC collections appears to have stabilized. Fig. 1 is adapted from Voorhees and Harman [7, Fig. 8] and derived from data published by Buckley and Walz [2, Table 3]: eight versions of the SMART IR system, developed over the first eight years of TREC, were run on the topics of eight editions of TREC, and their MAP was computed accordingly. This historical analysis of SMART was one of the primary reasons to discontinue the ad hoc track at TREC, the argument being that systems had reached a "plateau" in effectiveness gains, and that the effort of maintaining the track outweighed the knowledge being gained [6].
However, the data used to generate this graph can tell another story that has not been addressed as much. The SMART analysis takes topic variations into account implicitly (by running each system version on each topic set), but does not make them explicit. By focusing on topic variations, we can replot the same data in a dual way ­ for each version of the system, rather than for each set of topics ­ and produce Fig. 2. This shows that topic ease decreased over the first five years of TREC (i.e., in the years 1992­1996) and then increased in the last two. The trend over the eight SMART systems seems consistent.
1Mean Average Precision, the arithmetic mean of the average precision values for a system, or run, over all topics. 2Average Average Precision is the arithmetic mean of the Average Precision (AP) values for a topic, over all systems/runs.

1147

Figure 1: Effectiveness (MAP) of eight systems on the topics of eight TREC editions, adapted from Voorhees and Harman [7, Fig. 8].
Figure 2: A topic-oriented representation of Fig. 1, showing how effectiveness (MAP) varies across the topic sets of eight years of TREC for eight fixed versions of the SMART system.
In Fig. 3 we overlay Fig. 2 with AAP values computed for all runs across TRECs 2­11.3 Comparing the series in Fig. 3, one can see that despite quite different runs being measured, the AAP trend is consistent with the SMART trends: topics appear to get harder in the middle years of TREC ad hoc, easier in TRECs 7 and 8. Considering the later TRECs 9­11, the topics get harder again.
2.3 Discussion
While the data is old, we initially focus on the first eight years of TREC, since: (i) Figs. 1 and 2 can be drawn only for those years, and (ii) the goals of the tasks and the compositions of the document collections were largely the same.
The figures show that there is a substantial variation in topic ease across different years of TREC, with potentially undesirable effects. For example, a group participating in different TREC editions might be tempted to compare run effectiveness without taking topic ease changes into account. Also, the increased topic ease in TRECs 7 and 8 deserves particular attention since those are the TREC editions that have probably been used most frequently in data analysis and experiments (for example, it has been shown that to be effective in TREC 8, runs need to be effective on the easy topics [4]). As discussed at length by Mizzaro and Robertson [4], having harder topics is probably desirable as a track evolves: the 3We do not include TREC 1 data as it is not available on the TREC website, and we add TRECs 9­11, to be able to draw some general conclusions later.

Figure 3: AAP values for TRECs 2­11
overall decreasing trends in Figs. 2 and 3 seems reassuring in this respect, although with important variations that are not completely understood, and with a notable drop in TRECs 9­11. More generally, being cognizant of changes in topic ease when analyzing trends in system effectiveness is important.
It is also possible that the trends we describe are attributable to other effects, which we consider here.
2.3.1 System effects
The AAP analysis might suffer from a system effect: perhaps topics are not becoming more difficult, but rather, the IR systems participating in TREC are on average becoming less effective. To consider this question, we plot the frequency of runs after they are assigned into one of five buckets of AAP values. The frequency values are computed over all runs submitted to the different years of TREC in Fig. 4. This graph shows that the number of low AAP values increased over time, and also around TRECs 5­6. However, this effect is likely due to topics becoming more difficult and not due to more poor systems being submitted to TREC. We draw this conclusion, because Fig. 2 shows that fixed versions of the SMART system become worse over different years of TREC.
2.3.2 Topic variation
The way in which topics were specified was itself an evolving process in the early years of TREC. For example, the TREC 1 and 2 topics contained a concepts field which was used effectively by several groups as a surrogate to the summary and description fields. From TREC 3 onward, the assessors who made relevance judgements were also the people who created the topics, and the concepts field was removed. As a result, groups began to depend on the title field for simple keyword queries. In TREC 4, only a shortened summary field was distributed to participants, greatly increasing the difficulty in identifying the key concepts automatically. Title and Narrative fields were reintroduced in TREC 5, and the basic topic format stabilized for the remaining ad hoc tracks.
However, these changes to topics do not appear to affect the trends seen in Fig. 2 and 3. The removal of concepts in TREC 3 did not uniformly impact SMART and AAP, and despite an improvement in topic detail from TREC 4 to TREC 5, both SMART and AAP were decreasing.
2.3.3 Examining early web tracks
In Fig. 3 and 4 we also show data from TRECs 9­11, which behaves differently from the first eight TRECs. We conjecture that this is largely due to a shift to web-based search tasks. In particular, TREC 11 has much lower AAP values; this is unlikely to be a factor of topics only, and indeed the TREC 11 task was different

1148

Figure 4: Trend of low and high AAP values
from the classic ad hoc tasks, attempting to introduce topic distillation. This turned out to be quite difficult initially, as there was confusion regarding exactly what the goals of the task were [6]. More generally, TRECs 9­11 were largely a transition period between the text-based ad hoc search tasks and HTML-based search tasks which could also leverage link information in the ranking algorithms. Historically, the web-based ad hoc tasks stabilized once GOV2 was introduced in 2004. We leave a thorough exploration of the trends of topic evolution in web documents to future work.
2.3.4 Conclusions on topic ease
From our preliminary analysis of past TREC runs, it would appear that examination of a system versus topic effect is worth considering. There is reasonable evidence that topics in TREC got harder and that this was not due to weaker systems being submitted or to changes in the way topics were defined. Analysis of such past data is difficult, but we consider there to be value in examination of these issues. The best way forward is for further work to disentangle the effect of topics and systems over the years.
3. TOPIC SET PREDICTIVITY
A second possible aspect of topic goodness is that a good topic should be a good predictor of overall system effectiveness.
3.1 Background
This interpretation is inspired by the work of Guiver et al. [3], Robertson [5] and Berto et al. [1]. In these papers, the authors attempted to understand whether the system evaluations carried out in TREC-like exercises could be accomplished using fewer than fifty topics. While no strong conclusions were drawn by the authors, byproducts of the work are useful here.
The first byproduct is the notion of topic predictivity (our term), i.e., the capability of a topic to predict overall system effectiveness. This can be rephrased as follows: if a system is evaluated using a subset of a test collection's topics, then in general the effectiveness scores using MAP (or any other metric) would be different. If that system was measured along with other IR systems using that topic subset, it would result in a different ranking of systems. The question then is how well the MAP (or system ranking) computed using the topic subset correlates with the MAP (or system ranking) computed using all the topics.
Previous work has shown that some topic subsets are more predictive than others, and the differences can be quite high [1, 3, 5]. Coming back to our research question, one might interpret "better topics" as "topics with higher predictivity" ­ we seek to understand if topic predictivity has changed over the years. The second byproduct that we use for our evaluations is the BestSub software [1, 3, 5], which can compute various correlations between

effectiveness computed on a topic subset and on the full set of topics.
3.2 Experimental data
As with previous work [1, 3, 5], we use both linear correlation and Kendall's  as measures of predictivity: the former measures how close MAP computed on a topic subset is to MAP computed on the full topic set. The latter measures how close a system rank measured on topic subsets is to a rank measured on full topic sets.
Fig. 5 (left) shows the best linear correlations (i.e., what we would find when selecting the possible best subset) that can be obtained when using topic subsets of cardinalities 3­12 over the TREC ad hoc test collections from TREC 2 to TREC 11.4 Fig. 5 (right) shows similar data, but with Kendall's  . Fig. 6 shows the same data for a random subset (i.e., what we would expect to find when sampling randomly from the population of topics; the average results over 10, 000 repeated samples are shown).
The trend in all figures is quite consistent across cardinalities and it is overall decreasing. Fitting linear models leads to regression line gradients that are negative in all cases. The slopes for both the linear and  correlation "best" classes (Fig. 5) are statistically significant (t-test, p < 0.05) when at least four (eight) topics are used for the  (linear) correlations.
3.3 Discussion
The overall decreasing trend means that the topic sets have become less predictive over the years, for the cardinalities evaluated here. The trend is particularly clear for Kendall's  , indicating that the capability of topic subsets to predict the ranking of systems according to their effectiveness has fallen over time. At first sight, this is unlikely to be a desirable feature of topics. However, two corollaries should be considered.
First, the trend is more manifest for the best subsets than for the random subsets. This means that the theoretical potential predictivity (i.e., the predictivity that we would have if we were in some way able to select the best possible subset) is decreasing more than the practical effective predictivity (i.e., the predictivity that we might expect to find by selecting a random topic subset).
Second, the fact that subsets of topics appear to be becoming less able to predict system rankings of the full set of topics is not necessarily negative. If subsets of topics predict system rankings accurately, that tells us that there is some redundancy in the topics of old collections, and that is potentially a waste of effort. If newer collections do not have such redundancy, then this is positive: those collections include a more diverse set of topics, and are therefore presumably testing IR systems in a more complete way.
Coming back to the differences between TRECs 1­8 and TRECs 9­11, which are clear for topic ease (see Figs. 3 and 4), they become barely noticeable when considering topic predictivity (see Figs. 5 and 6). This means that whereas topics in TRECs 9­11 seem to be getting harder than in TRECs 1­8, that difference almost disappears when considering topic predictivity.
4. CONCLUSIONS AND FUTURE WORK
This paper proposes a dual approach to TREC data analysis: in place of trying to develop better IR systems, we try to understand how to develop better topics. While this work is a first step in this direction, there is much left for future work.
Although the trend in Fig. 1 is the same for each of the TREC topic sets (the eight lines), across the different versions of SMART
4The low cardinality topic sets (i.e., cardinalities 1­2) are noisy and prone to random effects; we do not take them into account.

1149

Figure 5: Topic set predictivity: best subsets, linear correlation (left) and Kendall's  (right).

Figure 6: Topic set predictivity: random subsets, linear correlation (left) and Kendall's  (right).

(the x-axis) the absolute magnitude of change (the y-axis) differs. Generally, the easier topics show larger variation, the harder topics show smaller. We will consider if statistical significance is more likely to be measured across topic sets with different topic ease.
In addition, there might be a "collection effect": over time the number of "possible" relevant results for each topic, on average, is increasing. Therefore, finding 1,000 documents in larger collections for each topic is easier now than in earlier collections. The role of the concept and title fields as a surrogate for a good keyword query is also not well understood. Note also that we ignore the shift from text to HTML in some of our analyses.
Other definitions of topic "goodness" can be proposed, and used to perform similar analyses. For example, a notion of "representativeness" of the topic space could be imagined for a topic set, although this would be difficult to quantify.
Our analysis is restricted to a limited number of TRECs, and should be extended to other collections, including other evaluation exercises such as NTCIR, INEX, FIRE, or CLEF. We have focused on MAP as the only effectiveness metric, but of course others can be used.
Finally, it would be interesting to repeat the work done by the SMART group in a broader fashion, i.e., with a set of systems working on the topics of all TREC tracks. Overall, we believe that there is a lot of work to be done, before the issue that we have started to address in this paper is fully understood.

Acknowledgments
This work was supported in part by the Australian Research Council (DP130104007) and also by a Google Faculty Research Award. Dr. Culpepper is the recipient of an ARC DECRA Research Fellowship (DE140100275).
References
[1] A. Berto, S. Mizzaro, and S. Robertson. On using fewer topics in information retrieval evaluations. In ICTIR, pages 30­37, 2013.
[2] C. Buckley and J. Walz. SMART in TREC8. In TREC-8, 2000.
[3] J. Guiver, S. Mizzaro, and S. Robertson. A few good topics: Experiments in topic set reduction for retrieval evaluation. ACM TOIS, 27(4):1­26, 2009.
[4] S. Mizzaro and S. Robertson. HITS hits TREC: exploring IR evaluation results with network analysis. In SIGIR, pages 479­486, 2007.
[5] S. Robertson. On the Contributions of Topics to System Evaluation. In Advances in Information Retrieval, volume 6611 of LNCS, pages 129­140, 2011.
[6] E. M. Voorhees and D. K. Harman, editors. TREC: Experiment and evaluation in information retrieval. MIT Press, London, 2005.
[7] E. M. Vorhees and D. K. Harman. Overview of the Eight Text REtrieval Conference (TREC-8). In TREC-8, 2000.

1150

The Correlation between Cluster Hypothesis Tests and the Effectiveness of Cluster-Based Retrieval

Fiana Raiber fiana@tx.technion.ac.il

Oren Kurland kurland@ie.technion.ac.il

Faculty of Industrial Engineering and Management, Technion Haifa 32000, Israel

ABSTRACT
We present a study of the correlation between the extent to which the cluster hypothesis holds, as measured by various tests, and the relative effectiveness of cluster-based retrieval with respect to document-based retrieval. We show that the correlation can be affected by several factors, such as the size of the result list of the most highly ranked documents that is analyzed. We further show that some cluster hypothesis tests are often negatively correlated with one another. Moreover, in several settings, some of the tests are also negatively correlated with the relative effectiveness of cluster-based retrieval.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
Keywords: cluster hypothesis, cluster-based retrieval
1. INTRODUCTION
The cluster hypothesis states that "closely associated documents tend to be relevant to the same requests" [19]. The hypothesis plays a central role in information retrieval. Various tests were devised for estimating the extent to which the hypothesis holds [5, 20, 3, 17]. Furthermore, inspired by the hypothesis, document retrieval methods that utilize document clusters were proposed (e.g., [10, 11, 6, 7, 15]).
There are, however, only a few reports regarding the correlation between the cluster hypothesis tests and the relative effectiveness of cluster-based retrieval with respect to document-based retrieval [20, 3, 13]. Some of these are contradictory: while it was initially argued that Voorhees' nearest neighbor cluster hypothesis test is not correlated with retrieval effectiveness [20], it was later shown that this test is actually a good indicator for the effectiveness of a specific cluster-based retrieval method [13].
The aforementioned reports focused on a single cluster hypothesis test (the nearest neighbor test), used a specific retrieval method which is not state-of-the-art and were evaluated using small documents collections which were mostly composed of news articles. Here, we analyze the correla-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609533.

tion between cluster hypothesis tests and the relative effectiveness of cluster-based retrieval with respect to documentbased retrieval using a variety of tests, state-of-the-art retrieval methods and collections.
We found that (i) in contrast to some previously reported results [3], cluster hypothesis tests are in many cases either negatively correlated with one another or not correlated at all; (ii) cluster hypothesis tests are often negatively correlated or not correlated at all with the relative effectiveness of cluster-based retrieval methods; (iii) the correlation between the tests and the relative effectiveness of the retrieval methods is affected by the number of documents in the result list of top-retrieved documents that is analyzed; and, (iv) the type of the collection (i.e., Web vs. newswire) is a strong indicator for the effectiveness of cluster-based retrieval when applied over short retrieved document lists.
2. RELATED WORK
The correlation between cluster hypothesis tests was studied using small document collections, most of which were composed of news articles [3]. We, on the other hand, use a variety of both (small scale) newswire and (large scale) Web collections. The correlation between cluster hypothesis tests and the effectiveness of cluster-based retrieval methods was studied using only a single test -- Voorhees' nearest neighbor test [20, 13]. Each study also focused on a different cluster-based retrieval method. This resulted in contradictory findings. In contrast, we use several cluster hypothesis tests and retrieval methods.
Document clusters can be created either in a query dependent manner, i.e., from the list of documents most highly ranked in response to a query [21] or in a query independent fashion from all the documents in a collection [5, 10]. In this paper we study the correlation between cluster hypothesis tests and the effectiveness of retrieval methods that utilize query dependent clusters [6, 7, 15]. The reason is threefold. First, these retrieval methods were shown to be highly effective. Second, we use for experiments large-scale document collections; clustering all the documents in these collections is computationally difficult. Third, the cluster hypothesis was shown to hold to a (much) larger extent when applied to relatively short retrieved lists than to longer ones or even to the entire corpus [18].
3. CLUSTER HYPOTHESIS TESTS AND
CLUSTER-BASED RETRIEVAL
To study the correlation between tests measuring the extent to which the cluster hypothesis holds and the effective-

1155

ness of cluster-based retrieval methods, we use several tests and (state-of-the-art) retrieval methods.
Let Dinit be an initial list of n documents retrieved in response to query q using some retrieval method. The retrieval method scores document d by score(q, d). (Details of the scoring function used in our experiments are provided in Section 4.) All the cluster hypothesis tests and the retrieval methods that we consider operate on the documents in Dinit. In what follows we provide a short description of these tests and methods.

Cluster hypothesis tests. The first test that we study con-

ceptually represents the Overlap test [5]. The test is based

on the premise that, on average, the similarity between two

relevant documents should be higher than the similarity be-

tween a relevant and a non-relevant document. Formally,

let R(Dinit) be the set of relevant documents in Dinit and

N (Dinit) the set of non-relevant documents; nR and nN de-

note the number of documents in R(Dinit) and N (Dinit), re-

spectively. The score assigned by the Overlap test to Dinit is

1

P

( ) nR(nR-1) di,dj R(Dinit),di=dj

sim(di,dj )+sim(dj ,di)

; sim(·, ·) is 1 ( ) nRnN

P di R(Dinit),dj N (Dinit)

sim(di,dj )+sim(dj ,di)

an inter-text similarity measure described in Section 4.1 This

score is averaged over all the tested queries for which nR and

nN are greater than 1 to produce the final test score.

We next consider Voorhees' Nearest Neighbor test (NN)

[20]. For each relevant document di ( R(Dinit)) we count the number of relevant documents among di's k - 1 nearest neighbors in Dinit; k is a free parameter. These counts are

averaged over all the relevant documents retrieved for all the

tested queries. The nearest neighbors of di are determined

based on sim(di, dj).

The Density test [3] is defined here as the ratio between

the average number of unique terms in the documents in

Dinit and the number of terms in the vocabulary. The un-

derlying assumption is, as for the tests from above, that rel-

evant documents are more similar to each other than they

are to non-relevant documents. Now, if the number of terms

that are shared by documents in the initial list is high, then

presumably relevant documents could be more easily distin-

guished from non-relevant ones.

We also explore the Normalized Mean Reciprocal Distance

test (nMRD) [17]. The test is based on using a complete

relevant documents graph. Each vertex in the graph rep-

resents a different document in R(Dinit); each pair of ver-

tices is connected with an edge. The edge weight repre-

sents the distance between the documents. The distance

between documents di and dj is defined as the rank of dj in a ranking of all the documents d  Dinit (d = di) that is created using sim(di, d); the rank of the highest

ranked document is 1. The score assigned by the nMRD test

to D is P ; init

1

nR

PnR i=1

1 log2 i+1

1 di,dj R(Dinit),di=dj spd(di,dj )

spd(di, dj) is the shortest path distance between di and dj

in the graph. This score is averaged over all tested queries

for which nR > 1 to produce the final nMRD score.

Cluster-based document retrieval methods. Let C l(Dinit)
be the set of clusters created from the documents in Dinit using some clustering algorithm. All the cluster-based re-
1We use both sim(di, dj) and sim(dj, di) as the similarity measure that was used for experiments is asymmetric. Further details are provided in Section 4.

trieval methods that we consider re-rank the documents in

Dinit using information induced from clusters in C l(Dinit).

The interpolation-f method (Interpf in short) [6] directly

ranks the documents in Dinit. The score assigned to docu-

ment

d

(

Dinit )

is

 score(q,d)

P di Dinit

score(q,di )

+

(1

-

)

; P cC

l(Dinit )

sim(q,c)sim(c,d)

P di Dinit

P cC l(Dinit)

sim(q,c)sim(c,di )



is

a

free

parameter.

The cluster-based retrieval methods that we consider next

are based on a two steps procedure. First, the clusters in

C l(Dinit) are ranked based on their presumed relevance to

the query. Then, the ranking of clusters is transformed to a

ranking over the documents in Dinit by replacing each cluster

with its constituent documents (and omitting repeats).

The AMean and GMean methods [12, 16] rank the clus-

ters based on the arithmetic and geometric mean of the orig-

inal retrieval scores of the documents in a cluster, respec-

tively. Specifically, AMean assigns cluster c with the score

1 |c|

P
dc

score(q,

d)

where

|c|

is

the

number

of

documents

in

1

c.

The

score

assigned

to

c

by

GMean

is

Q
dc

score(q,

d)

|c|

.

Another cluster ranking method that we use is Clus-

tRanker [7]. ClustRanker assigns cluster c with the score

 + cent(c)sim(q,c)

P ci C

l(Dinit )

cent(ci )sim(q,ci )

(1-)

P dc

score(q,d)sim(c,d)cent(d)

P ci C l(Dinit)

P dci

score(q,d)sim(ci ,d)cent(d)

;

cent(d)

and

cent(c) are estimates of the centrality of a document d in

Dinit and that of a cluster c in C l(Dinit), respectively. These

estimates are computed using a PageRank algorithm that

utilizes inter-document and inter-cluster similarities [9, 7].

We also use the recently proposed state-of-the-art

ClustMRF cluster ranking method [15]. ClustMRF uses

Markov Random Fields which enable to integrate various

types of cluster-relevance evidence.

4. EXPERIMENTAL SETUP
Experiments were conducted using the datasets specified in Table 1. WSJ, AP and ROBUST are small (mainly) newswire collections. WT10G is a small Web collection and GOV2 is a crawl of the .gov domain. CW09B is the Category B of the ClueWeb09 collection and CW09A is its Category A English part. We use two additional settings, CW09BF and CW09AF, for categories B and A [2], respectively. These settings are created by filtering out from the initial ranking documents that were assigned with a score below 50 and 70 by Waterloo's spam classifier for CW09B and CW09A, respectively. Thus, the initial lists, Dinit, used for these two settings presumably contain fewer spam documents.

corpus

# of docs # of unique terms data

queries

WSJ AP

173,252 242,918

ROBUST 528,155

WT10G GOV2 CW09B CW09BF CW09A CW09AF

1,692,096 25,205,179 50,220,423
503,903,810

186,689 259,501 663,700 4,999,228 39,251,404 87,262,413
507,500,897

Disks 1-2 Disks 1-3
Disks 4-5 (-CR)
WT10g GOV2

151-200 51-150 301-450, 600-700 451-550 701-850

ClueWeb09 Cat. B 1-200

ClueWeb09 Cat. A 1-200

Table 1: TREC data used for experiments.

1156

The Indri toolkit was used for experiments2. Titles of

topics served for queries. We applied Krovetz stemming to

documents and queries. Stopwords were removed only from

queries using INQUERY's list [1].

We use the nearest neighbor clustering algorithm to cre-

ate the set of clusters C l(Dinit) [4]. A cluster is created from each document di  Dinit. The cluster contains di and the k - 1 documents dj  Dinit (dj = di) with the high-

est sim(di, dj). We set k = 5. Recall that k is also the

number of nearest neighbors in the NN cluster hypothesis

test. Using such small overlapping clusters was shown to be

highly effective, with respect to other clustering schemes, for

cluster-based retrieval [4, 11, 7, 14, 15].

The similarity between texts x and y, sim(x, y), is defined

as

" exp -CE

"pDx ir[0](·)

  

pDy ir[µ]

"" (·) ;

CE

is

the

cross

en-

tropy measure and pDz ir[µ](·) is the Dirichlet-smoothed (with

the smoothing parameter µ) unigram language model in-

duced from text z [8]. We set µ = 1000 in our experi-

ments [22]. This similarity measure was found to be highly

effective, specifically for measuring inter-document similar-

ities, with respect to other measures [9]. The measure is

used to create Dinit -- i.e., score(q, d) d=ef sim(q, d) --3 and

to compute similarities between the query, documents and

clusters. We represent a cluster by the concatenation of its

constituent documents [10, 6, 8, 7]. Since we use unigram

language models the similarity measure is not affected by

the concatenation order.

To study the correlation between two cluster hypothesis

tests, we rank the nine experimental settings (WSJ, AP, RO-

BUST, WT10G, GOV2 and the ClueWeb09 settings) based

on the score assigned to them by each of the tests. Kendall's-

 correlation between the rankings of experimental settings

is the estimate for the correlation between the tests. We note

that Kendall's- is a rank correlation measure that does not

depend on the actual scores assigned to the settings by the

tests. Kendall's- ranges from -1 to +1 where -1 represents

perfect negative correlation, +1 represents perfect positive

correlation, and 0 means no correlation.

The correlation between a cluster hypothesis test and the

relative effectiveness of a cluster-based retrieval method is

also measured using Kendall's- . The experimental settings

are ranked with respect to a cluster-based retrieval method

by the performance improvement it posts over the original

document-based ranking. Specifically, the ratio between the

Mean Average Precision at cutoff n (MAP@n) of the rank-

ing induced by the method and the MAP@n of the initial

ranking is used; n is the number of documents in Dinit.

The free-parameter values of Interpf, ClustRanker and

ClustMRF were set using 10-fold cross validation. Query

IDs were used to create the folds; MAP@n served for the

optimization criterion in the learning phase. The value of

 which is used in Interpf and ClustRanker is selected from

{0, 0.1, . . . , 1}. To compute the document and cluster cen-

trality estimates in ClustRanker, the dumping factor and the

number of nearest neighbors that are used in the PageRank

algorithm were selected from {0.1, 0.2, . . . , 0.9} and {5, 10, 20,

30, 40, 50}, respectively. The implementation of ClustMRF

follows that in [15].

2www.lemurproject.org/indri 3Thus, the initial ranking is induced by a standard languagemodel-based approach.

n = 50 n = 100 n = 250 n = 500

Overlap NN Density nMRD
Overlap NN Density nMRD
Overlap NN Density nMRD
Overlap NN Density nMRD

Overlap
1.000 -0.171 -0.778
0.278
1.000 -0.354 -0.833 -0.222
1.000 -0.329 -0.611 -0.556
1.000 -0.588 -0.778 -0.611

NN
-0.171 1.000 0.229
-0.229
-0.354 1.000 0.412 0.000
-0.329 1.000 0.569 0.329
-0.588 1.000 0.650 0.402

Density
-0.778 0.229 1.000
-0.056
-0.833 0.412 1.000 0.389
-0.611 0.569 1.000 0.722
-0.778 0.650 1.000 0.722

nMRD
0.278 -0.229 -0.056
1.000
-0.222 0.000 0.389 1.000
-0.556 0.329 0.722 1.000
-0.611 0.402 0.722 1.000

Table 2: The correlation between cluster hypothesis tests (measured in terms of Kendall's- ). n is the number of documents in Dinit.

5. EXPERIMENTAL RESULTS
The correlations between the cluster hypothesis tests are presented in Table 2 for different values of n. With the exception of the Overlap test, we can see that the correlation between all other pairs of tests increases with increasing values of n, but can be negative or zero for low values of n. The Overlap test is negatively correlated with all the other tests across almost all values of n.
A decent positive correlation is attained between Density and NN for n  100. For n  250 a decent positive correlation is also attained between nMRD and NN. While nMRD is a global test that considers the relations between all the documents in Dinit, NN is a more local test that only considers the relations between a document and its nearest neighbors.
For n  {250, 500}, nMRD and Density are the most correlated tests. This finding is surprising since these tests are based on completely different properties of the initial list. While nMRD is based on directly measuring interdocument similarities, the Density test is based on the number of unique terms in the documents which presumably attests to the ability to differentiate between relevant and non-relevant documents.
Cluster-based document retrieval methods. We next
study the correlation between the cluster hypothesis tests and the relative effectiveness of cluster-based retrieval methods. For reference, we report the correlation numbers with respect to a ranking of the experimental settings induced by the size of the corresponding collections (Size). The results are presented in Table 3.
We observe a negative correlation between the Density test and all five cluster-based retrieval methods for n = 50. This finding can be explained as follows. First, the size of the collection is positively correlated with the number of terms in the vocabulary. (Refer back to Table 1.) Now, by definition, this number is negatively correlated with Density. Second, the size of the collection is positively correlated with the effectiveness of cluster-based retrieval methods as observed in Table 3 for the Size correlations for n = 50. We note that here, the Web collections are larger than the newswire collections and are in general noisier. Thus, we conclude that the type of the collection, i.e., Web vs. newswire, can have

1157

n = 50 n = 100 n = 250 n = 500

Overlap NN

Interpf AMean

0.761 -0.029 0.111 0.000

GMean

0.333 -0.229

ClustRanker 0.778 -0.057

ClustMRF

0.889 -0.171

Interpf

0.500 0.000

AMean

-0.222 -0.118

GMean

-0.333 -0.059

ClustRanker 0.611 0.059

ClustMRF

0.722 -0.295

Interpf

-0.254 0.486

AMean GMean ClustRanker ClustMRF

-0.611 -0.333 -0.056
0.500

0.150 0.210 0.150 -0.210

Interpf AMean GMean ClustRanker ClustMRF

-0.111 -0.444 -0.444 -0.222
0.389

0.155 0.340 0.279 0.402 -0.155

Density nMRD Size

-0.648 -0.333 -0.444 -0.667 -0.778

0.028 0.278 -0.056 0.167 0.167

0.725 0.286 0.457 0.743 0.857

-0.333 0.056 0.167
-0.444 -0.556

-0.167 0.556 0.333
-0.278 -0.056

0.400 -0.114 -0.229
0.514 0.629

0.254 0.333 0.389 0.000 -0.333
0.111 0.444 0.444 0.333 -0.167

0.028 0.500 0.556 -0.056 -0.278
0.167 0.500 0.611 0.278 0.000

-0.203 -0.400 -0.400
0.057 0.400
-0.057 -0.457 -0.457 -0.286
0.229

Table 3: The correlation between cluster hypothesis tests and the relative effectiveness of cluster-based retrieval methods. The Size "test" ranks experimental settings by the number of documents in the collections. n is the number of documents in Dinit.

an influence on the effectiveness of cluster-based retrieval methods for short retrieved lists.
Another observation that we make based on Table 3 is that for n = 50 the correlation attained for the nMRD and NN tests is often lower than that attained for Overlap and Size. The relatively high positive correlation attained for the Overlap and the Size tests for n = 50 suggests that these tests are very strong indicators for the relative effectiveness of cluster-based retrieval with respect to document-based retrieval when applied to short retrieved lists. For larger values of n, NN and nMRD, as well as Density, start to post more positive correlations while the reverse holds for Overlap and Size.
We can also see that none of the retrieval methods is correlated only positively or only negatively with all the tests for any fixed value of n. In addition, only in a few cases a test is either only positively or only negatively correlated with all the retrieval methods for a fixed value of n. Thus, we conclude that the correlation between the effectiveness of a retrieval method and a cluster hypothesis test can substantially vary (both positively and negatively) across retrieval methods and tests.
6. CONCLUSIONS
We studied the correlation between cluster hypothesis tests and cluster-based retrieval effectiveness. We showed that the correlation between the two depends on the specific tests and methods that are used, and on the number of documents in the result list that is analyzed. We also showed that the type of the collection, i.e., Web or newswire, can be a stronger indicator for the relative effectiveness of cluster-based retrieval with respect to document-based retrieval, for short retrieved lists, than tests designed for estimating the extent to which the cluster hypothesis holds.

Acknowledgments We thank the reviewers for their comments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center. This work has also been supported in part by Microsoft Research through its Ph.D. Scholarship Program.
7. REFERENCES
[1] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proceedings of TREC-9, pages 551­562, 2000.
[2] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Informaltiom Retrieval Journal, 14(5):441­465, 2011.
[3] A. El-Hamdouchi and P. Willett. Techniques for the measurement of clustering tendency in document retrieval systems. Journal of Information Science, 13:361­365, 1987.
[4] A. Griffiths, H. C. Luckhurst, and P. Willett. Using interdocument similarity information in document retrieval systems. Journal of the American Society for Information Science, 37(1):3­11, 1986.
[5] N. Jardine and C. J. van Rijsbergen. The use of hierarchic clustering in information retrieval. Information Storage and Retrieval, 7(5):217­240, 1971.
[6] O. Kurland. Re-ranking search results using language models of query-specific clusters. Journal of Information Retrieval, 12(4):437­460, August 2009.
[7] O. Kurland and E. Krikon. The opposite of smoothing: A language model approach to ranking query-specific document clusters. Journal of Artificial Intelligence Research, 41:367­395, 2011.
[8] O. Kurland and L. Lee. Clusters, language models, and ad hoc information retrieval. ACM Transactions on information systems, 27(3), 2009.
[9] O. Kurland and L. Lee. PageRank without hyperlinks: Structural reranking using links induced by language models. ACM Transactions on information systems, 28(4):18, 2010.
[10] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proceedings of SIGIR, pages 186­193, 2004.
[11] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.
[12] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proceedings of ECIR, pages 454­462, 2008.
[13] S.-H. Na, I.-S. Kang, and J.-H. Lee. Revisit of nearest neighbor test for direct evaluation of inter-document similarities. In Proceedings of ECIR, pages 674­678, 2008.
[14] F. Raiber and O. Kurland. Exploring the cluster hypothesis, and cluster-based retrieval, over the web. In Proceedings of CIKM, pages 2507­2510, 2012.
[15] F. Raiber and O. Kurland. Ranking document clusters using markov random fields. In Proceedings of SIGIR, pages 333­342, 2013.
[16] J. Seo and W. B. Croft. Geometric representations for multiple documents. In Proceedings of SIGIR, pages 251­258, 2010.
[17] M. D. Smucker and J. Allan. A new measure of the cluster hypothesis. In Proceedings of ICTIR, pages 281­288, 2009.
[18] A. Tombros, R. Villa, and C. van Rijsbergen. The effectiveness of query-specific hierarchic clustering in information retrieval. Information Processing and Management, 38(4):559­582, 2002.
[19] C. J. van Rijsbergen. Information Retrieval. Butterworths, second edition, 1979.
[20] E. M. Voorhees. The cluster hypothesis revisited. In Proceedings of SIGIR, pages 188­196, 1985.
[21] P. Willett. Query specific automatic document classification. International Forum on Information and Documentation, 10(2):28­32, 1985.
[22] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334­342, 2001.

1158

The Effect of Expanding Relevance Judgements with Duplicates

Gaurav Baruah
David R. Cheriton School of Computer Science
University of Waterloo, Canada
gbaruah@uwaterloo.ca

Adam Roegiest
David R. Cheriton School of Computer Science
University of Waterloo, Canada
aroegies@uwaterloo.ca

Mark D. Smucker
Department of Management Sciences
University of Waterloo, Canada
mark.smucker@uwaterloo.ca

ABSTRACT
We examine the effects of expanding a judged set of sentences with their duplicates from a corpus. Including new sentences that are exact duplicates of the previously judged sentences may allow for better estimation of performance metrics and enhance the reusability of a test collection. We perform experiments in context of the Temporal Summarization Track at TREC 2013. We find that adding duplicate sentences to the judged set does not significantly affect relative system performance. However, we do find statistically significant changes in the performance of nearly half the systems that participated in the Track. We recommend adding exact duplicate sentences to the set of relevance judgements in order to obtain a more accurate estimate of system performance.
Categories and Subject Descriptors: H.3.4 [Systems and Software Performance evaluation]: Efficiency and Effectiveness
Keywords: Duplicate Detection; Evaluation; Pooling
1. INTRODUCTION
The Temporal Summarization Track (TST) at TREC 2013 [2], required returning information relevant to topics, from a web-scale time-ordered document stream (the TREC KBA 2013 Stream Corpus [1]). The Sequential Update Summarization (SUS) task in the TST, called for participating systems (runs), to return updates (sentences) about events (topics), with the goal that new updates should contain information that is new to the user.
For the SUS task, the Stream corpus is effectively a collection of documents (along with their timestamps), spanning the time period between October 2011 to January 2013, with an average of 93,037 documents per hour. Each participating system was tasked to return updates from a duration spanning 240 hours for each topic. In all, the TST received 28 runs from the participants of the SUS task. The number of updates returned in the runs varied from 110 to 2,815,808.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609534.

For evaluation of runs, the pool of updates judged by track assessors contains 9,113 sentences for 9 task topics.
We found that there are a very large number of duplicate sentences in the corpus. Indeed, the track organizers found duplicates amongst the sentences sampled from runs, while constructing the pool for assessment. Accordingly, they created an evaluation framework designed to accommodate for duplicates within the judged set of sentences. In effect, the identifier of a duplicate sentence (within the judged set of sentences) is mapped to the identifier of the designated original sentence (prototype). However, the duplicates of judged sentences from the corpus, were not mapped to the prototypes. The track's evaluation is also designed to omit sentences that are not in the judged set. Such omission is consistent with similar approaches [7, 3], and works very well for evaluating relative system performance (Section 3.1).
Including exact duplicates of judged sentences from the corpus may have ramifications for a system's evaluation as well as for the re-usability of the TST test collection. For example, if sentence s is an exact duplicate of a sentence p in the judged set, a system would neither be rewarded nor penalized for returning s, since s is not in the judged set of sentences. This may lead to an unfair evaluation for the system.
In this work we investigate the effect of expanding the set of judged sentences with their exact duplicates from the corpus, where, an exact duplicate is a sentence that exactly matches the text of the judged sentence for each character in the sentence string. We find that:
· There exist an extremely large number of exact duplicate sentences in the corpus (Section 2.2).
· Adding duplicates to the judged set of sentences, does not change relative ranking of systems with respect to Kendall's  (Section 3.1).
· System performance is affected when the judged set of sentences is expanded with duplicates (Section 3.2), with 13 of 28 submitted runs showing statistically significant changes for a paired t-test with p-value  0.05.
2. OBSERVATIONS ON DUPLICATES
We briefly describe the original set of judged sentences of the SUS task and set a context for subsequent sections.
2.1 Original Judged Sentences
Each topic in the SUS task corresponds to an event (of type earthquake, storm, accident, bombing or shooting) that

1159

occurred within the corpus duration. The query for the topic included query terms, and query start and end times. The query was considered active for the time interval between query start and end times (the query duration, typically 240 hours).
The assessors initially identified a set of nuggets (short snippets of text containing information relevant to the topic), from the edit history of the Wikipedia1 article for an event. A pool of sentences for evaluation was created by sampling 60 updates per topic per submitted run with highest confidence scores (a confidence score was required for each update in a run). Some close duplicates were identified from this initial sample which allowed more updates to be included in the pool [2]. This created a pool of sentences totalling 9,113 for 9 topics. A pooled sentence was matched against nuggets and was considered relevant if it contained a nugget.
2.2 Expanded Set of Judged Sentences
Given the original set of judged sentences, for each topic, for each sentence in the judged set, we found the exact duplicates within the query duration of the topic, from the Stream corpus. For each duplicate found, we added it to the original judged set and mapped its identifier to the identifier of its prototype. Table 1 lists the number of judged sentences, the number of duplicates known to exist within the original judged set, the number of sentences for which the duplicates were found within the query duration. Also shown are the number of relevant sentences in the original judged set for each topic, and the number of relevant sentences for which duplicates were found in the query duration from the corpus.
We see that the original judged set expands nearly a 1000 times from 9,113 to 9,034,179, when duplicates are added. However, the number of relevant duplicates found is extremely less at 97,256 (about 10 times the size of the original set). Table 2 shows the top occurring duplicate sentences. In fact, the top 3 most occurring duplicate sentences account for 67% of the total duplicates found. We observed that most of the duplicates that occur with high frequency are duplicates of non relevant sentences. They appear to be boilerplate sentences found in web-site navigation menus or at the end of news articles. In contrast the most frequent relevant duplicate, "National Hurricane Center in Miami said Isaac became a Category 1 hurricane Tuesday with winds of 75 mph.", was found to occur just 5,403 times, in the query duration for topic 5 ("Hurricane Isaac").
We feel that the high number of duplicates found overall could be because of news/web syndication services. News wire documents form the second largest component of documents in the Stream corpus [1], with social documents (blogs and forums) forming the bulk of the corpus, and documents sourced from links submitted to bitly.com forming the remainder. One would expect a high number of news/web articles to be generated after the occurrence of a catastrophic event.
3. EFFECT OF ADDING DUPLICATES TO THE ORIGINAL SET OF JUDGEMENTS
The TST introduces new measures to evaluate temporal summarization. The measures are analogous to precision/gain and recall and they are also designed to account
1http://www.wikipedia.org/

for latency and verbosity of updates (sentences). Participant systems are tasked to return a timestamp along with each update about an event. Latency discounts are applied to sentences that are returned later than the first known occurrence of the nugget of information that they contain. The nuggets were identified, and their time of first occurrence noted, by the track's assessors, using the edit histories of the Wikipedia articles for the topics (Section 2.1). Verbosity discounts are applied based on the length of the returned sentences. Longer sentences are penalized more than shorter sentences by the verbosity discount, which essentially forms an aspect of "user friendliness" for a system.
The track introduces two precision-like measures, Expected Latency Gain (E[LG]) and Expected Gain (E[G]), as well as two recall-like measures, Latency Comprehensiveness (LC) and Comprehensiveness (C). The E[LG] and E[G] measures use the relevance score (0/1 for binary relevance) as a measure of gain and potentially discount the gain for updatelatency. The C and LC measures attempt to capture how well a system performed at returning all identified nuggets and how quickly it emitted updates containing these nuggets from the canonical time of first occurrence for the nuggets.
The track coordinators consider E[LG] and LC to be the official metrics of the track and we report our analyses for these metrics only. Detailed descriptions of all metrics can be found in the 2013 Temporal Summarization Track Overiew [2]. We note here that in the track's evaluation framework, a system returning sentences not present in the judged set of sentences is neither penalized nor rewarded. On the other hand, returning duplicate sentences present in the judged set will result in the verbosity discount being applied for each.
3.1 Effect on Systems' Ranking
Table 3 provides Kendall's  correlation between the rankings given by the original judged and expanded set for each of the four task measures. We can see that the correlation tends to be high indicating that relative performance is typically maintained regardless of assessment pool. This is consistent with other works conducting similar research [7, 10, 11, 3, 9]. Accordingly, we can see that not including all exact duplicates in the judged sentences was a reasonable and effective method of relative system performance evaluation. However, there may still be benefit to expanding the judged set because there are changes to the absolute performance scores of the systems (Section 3.2) which may be indicative of a change in the user experience.
3.2 Effect on System Performance
Overall, 13 submitted runs showed a statistically significant change in E[LG] and 12 submitted runs showed a statistically significant change in LC, for a paired t-test with p-value  0.05. The average difference across topics (and standard deviation) between the original judged set and the expanded set are presented in Tables 4 and 5 for E[LG] and LC, respectively. Runs for which there was no difference in score are not listed in the tables. Runs are listed in sorted order based upon the run names.
There exist several runs with no E[LG] change which is primarily due to the fact that they do not contain any newly identified duplicates and so would not be penalized for them. The majority of the the other runs, do experience a general decrease in the performance with respect to the expanded

1160

Topic
1 2 3 4 5 6 8 9 10 Total

#sentences
779 912 762 1463 1069 1517 1128 873 610 9113

Original Judged Set of Sentences

#known #with duplicates #relevant

duplicates found in corpus sentences

100

309

431

180

474

381

112

494

211

276

946

410

0

689

82

187

905

493

205

609

172

172

423

168

89

338

287

1321

5187

2635

#relevant with dup.s in corpus
146 202 154 260 63 270 102 97 143 1437

Expanded Judged Set

#sentences #relevant

sentences

833794

1445

2241589

6301

552145

25199

264474

22587

821897

17043

730296

18661

1057643

1741

2430455

2384

101886

1895

9034179

97256

Table 1: The number of sentences in the Original judged set vs. the Expanded set

Frequency 3376809 2013684 673876 529085 294662
... 166557 111503
... 81985

Topics 2,9,1 2,9,8
3 5 8
6,9 8,9
6

Duplicate Sentence All rights reserved./All rights reserved Yahoo! New User ? 3. This material may not be published, broadcast, rewritten or redistributed.
U.S. Register Sign In Help New Firefox
16 Optimised for Yahoo! Notifications Help Mail My Y!
Join Here .

Table 2: Examples of Duplicate Sentences with high number of occurrences across all topics

Judged Sentences expanded with exact duplicates lowercase duplicates

Kendall's  for Ranking Metric

E[LG] E[G] LC

C

0.899 0.894 0.942 0.937

0.899 0.894 0.942 0.937

Table 3: Rank correlation between Original Judged sentences vs Expanded set, for TST measures

set; though, the affect on such runs is not consistent across topics. Of particular note is run 8 in Table 4, which has a general increase in performance indicating that run 8 did return relevant sentences which were not in the original judged set, but were duplicates of relevant sentences from the original set. This increase was found not to be statistically significant with a p-value > 0.05 for a paired t-test. However, with more topics, we may find more statistically significant positive improvements [8].
Furthermore, we can see that the duplicate detection in the expanded set does not hurt but improves LC performance on average. This makes sense since systems may have returned relevant sentences duplicate to those in the original set of judgements. By expanding the original judged set with exact duplicates, we argue that a more accurate assessment of absolute performance is being achieved since runs are now being rewarded or penalized for new sentences which were not present in the original set.
3.3 Variations on Duplicate Detection
We also tried duplicate detection with simple transformations (to ensure minimal information loss) like lowercase-ing, whitespace-collapsing (reducing sequences of whitespace to a single space) and whitelower (lowercase + whitespace). The

Run

µ () Run

µ ()

1 0.0112 (0.0134) 2 0.0097 (0.0100)

4 0.0107 (0.0138) 8 -0.0037 (0.0089)

9

0.0391 (0.0559) 10 0.0363 (0.0388)

11 0.0006 (0.0030) 12 0.0001 (0.0028)

13 0.0014 (0.0019) 14 0.0013 (0.0016)

15 0.0007 (0.0020) 16 0.0014 (0.0019)

18 0.0151 (0.0163) 19 0.0160 (0.0181)

20 0.0162 (0.0152) 21 0.0171 (0.0192)

22 0.0008 (0.0016) 23 0.0008 (0.0016)

24 0.0054 (0.0067) 25 0.0054 (0.0061)

26 0.0052 (0.0048) 27 0.0036 (0.0030)

28 0.0007 (0.0010)

 denotes a p-value  0.05 for a paired t-test

Table 4: Average Difference and Standard Deviation of E[LG] between the Original Set and the Expanded Set of judged sentences

lowercase transformation is a common normalization technique employed in search engine indexing, and may introduce errors (e.g. US, the country, vs us, the pronoun). It did increase the total number of duplicates found to 10,872,223 but found only 44 new duplicates for relevant updates. Both whitespace transformations did not produce different sets of sentences than their basis transformations.
The Kendall's  between the original judged set and the expanded set, using exact duplicates and lowercase duplicates, are identical (Table 3), due to the fact they both produced identical rankings, and in fact scores, for all systems. The lowercase transformation found additional duplicates overall but very few relevant duplicates and hence there are insignificant changes in the scores when averaged across topics and runs. With more topics and more participant systems, we might expect to see the effect of such transformations to be more pronounced.
4. DISCUSSION
We believe that much of the negative effect of the original judged set expansion (on the gain-based measures) would be subsumed, in the majority of cases, if the verbosity penalization were applied to all sentences retrieved by a system. Currently, such penalization only occurs if a retrieved sentence is also present in the judged set. While beneficial for evaluating system performance for finding relevant sentences, ignoring unjudged sentences does not accurately reflect the user experience. One would expect a large difference in performance scores between submitting 1,000 sentences and 1

1161

Run

µ () Run

µ ()

1 -0.0594 (0.0446) 2 -0.0594 (0.0446)

4 -0.0809 (0.0571) 8

-0.0138 (0.0183)

9 -0.0325 (0.0402) 10 -0.0408 (0.0611)

11 -0.1247 (0.0550) 12 -0.1324 (0.0557)

18 -0.0915 (0.0604) 19 -0.1045 (0.0586)

20 -0.0669 (0.0551) 21 -0.0734 (0.0526)

24 -0.0192 (0.0247) 25 -0.0297 (0.0328)

26 -0.0038 (0.0061)

 denotes a p-value  0.05 for a paired t-test

Table 5: Average Difference and Standard Deviation of LC between the Original Set and the Expanded Set of judged sentences

million sentences; a difference which has the potential to overwhelm the user. The official set of judgements averages around 1,000 sentences per topic which results in an upper limit on verbosity penalization.
However, the current evaluation methology is consistent with that of [7], who found that removing unjudged documents from evaluation of ranked lists works well and does not affect the relative system performance. Indeed, the new TST metrics are stable for measuring relative system performance, even after processing an expanded set of judgements that is 1000 times the size of the original.
Nugget-based evaluation [4, 6] - where identified relevant material is representative of relevance - is aimed towards automatic identification of nuggets in the whole collection. However, tracking duplicates of the retrievable unit (e.g. documents, sentences) may be useful depending on the evaluation metrics for the specific task at hand (such as Temporal Summarization).
5. FUTURE WORKS
As an immediate future work, we plan to investigate the effect of including unjudged updates for verbosity discounts. Accounting for unjudged sentences in runs may not be a straightforward task due to the potential quantity of them. A simple but potentially inefficient mechanism would be to store information (e.g. length, timestamp, duplicates) about every sentence in the corpus which would facilitate applying the necessary discount. Alternatively, it may be possible to produce an estimate of unjudged sentence lengths. This may require the use of some form of sampling and the use of inclusion probabilities (e.g. [5]). Determining a reasonable method for applying verbosity penalization to unjudged sentences is an area of research that we intend to pursue.
The TST at TREC 2013 had only 9 topics. As per [8, 12], even though there are statistically significant changes in systems' performance, we cannot currently presume that the effects would be reproducible for a different/larger set of topics. We definitely need to test for the effect of duplicates on more number of topics.
6. CONCLUSIONS
We experimented with expanding the set of judged sentences with exact duplicates from the corpus and investigated its effects on the evaluation of temporal summarization. We found that adding exact duplicate sentences to the set of relevance judgements, does not affect relative ordering of temporal summarization systems. It does however,

induce a change in the performance scores of the systems. 13 out of 28 systems that participated in the Temporal Summarization Track at TREC 2013 experienced a statistically significant change in performance scores with respect to the track's metrics. With more topics from the TREC 2014 version of the track, we expect to get a more accurate estimate of changes in performance when evaluating with a large number of duplicates. Expansion of relevance judgements with exact duplicates is simple and not only does it help produce more accurate performance scores but also potentially aids in reusability of the test collection for the development of new temporal summarization systems.
7. ACKNOWLEDGMENTS
We thank Rakesh Guttikonda for helpful discussions and ideas regarding this work. This work was made possible by the facilities of SHARCNET (www.sharcnet.ca) and Compute/Calcul Canada, and was supported in part by NSERC, and in part by the Google Founders Grant, and in part by the University of Waterloo.
8. REFERENCES
[1] KBA Stream Corpus 2013. http: //trec-kba.org/kba-stream-corpus-2013.shtml.
[2] J. Aslam, F. Diaz, M. Ekstrand-Abueg, V. Pavlu, and T. Sakai. TREC 2013 Temporal Summarization. In TREC, 2013.
[3] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In SIGIR, pages 25­32, 2004.
[4] G. Marton and A. Radul. Nuggeteer: Automatic nugget-based evaluation using descriptions and judgements. In HLT-NAACL, pages 375­382, 2006.
[5] V. Pavlu and J. Aslam. A practical sampling strategy for efficient retrieval evaluation, Technical Report, College of Computer and Information Science, Northeastern University. 2007.
[6] V. Pavlu, S. Rajput, P. B. Golbus, and J. A. Aslam. IR system evaluation using nugget-based test collections. In WSDM, pages 393­402, 2012.
[7] T. Sakai and N. Kando. On information retrieval metrics designed for evaluation with incomplete relevance assessments. Information Retrieval, 11(5):447­470, 2008.
[8] M. Sanderson and J. Zobel. Information retrieval system evaluation: effort, sensitivity, and reliability. In SIGIR, pages 162­169, 2005.
[9] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In SIGIR, pages 66­73, 2001.
[10] A. Trotman and D. Jenkinson. IR evaluation using multiple assessors per topic. ADCS, 2007.
[11] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In SIGIR, pages 315­323, 1998.
[12] E. M. Voorhees and C. Buckley. The effect of topic set size on retrieval experiment error. In SIGIR, pages 316­323, 2002.

1162

Necessary and Frequent Terms in Queries

Jiepu Jiang
Center for Intelligent Information Retrieval School of Computer Science
University of Massachusetts Amherst
jpjiang@cs.umass.edu

James Allan
Center for Intelligent Information Retrieval School of Computer Science
University of Massachusetts Amherst
allan@cs.umass.edu

ABSTRACT
Vocabulary mismatch has long been recognized as one of the major issues affecting search effectiveness. Ineffective queries usually fail to incorporate important terms and/or incorrectly include inappropriate keywords. However, in this paper we show another cause of reduced search performance: sometimes users issue reasonable query terms, but systems cannot identify the correct properties of those terms and take advantages of the properties. Specifically, we study two distinct types of terms that exist in all search queries: (1) necessary terms, for which term occurrence alone is indicative of document relevance; and (2) frequent terms, for which the relative term frequency is indicative of document relevance within the set of documents where the term appears. We evaluate these two properties of query terms in a dataset. Results show that only 1/3 of the terms are both necessary and frequent, while another 1/3 only hold one of the properties and the final third do not hold any of the properties. However, existing retrieval models do not clearly distinguish terms with the two properties and consider them differently. We further show the great potential of improving retrieval models by treating terms with distinct properties differently.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ query formulation, retrieval models.
General Terms
Performance, Experimentation, Human Factors.
Keywords
Query; term frequency; term occurrence.
1. INTRODUCTION
Term frequency (TF) is widely used as an important heuristic in retrieval models [1­3]. The assumption is that documents with comparatively higher frequencies of query terms are more likely to be relevant. However, we suspect that in many cases this assumption does not hold. Instead, users may adopt some query terms to simply include or exclude documents regardless of the occurrences of the terms ­ that is, in those cases TF does not indicate the relevance of documents as long as the terms appear. In such cases, retrieval models that heavily exploit TF may
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'14, July 06­11, 2014, Gold Coast, QLD, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 $15.00. http://dx.doi.org/10.1145/2600428.2609536

incorrectly rank some non-relevant documents with high frequencies of the terms to the top.
We define the following two properties of query terms. We say that a term is necessary to a topic if most relevant documents contain the term. Documents with no occurrences of the necessary term are unlikely to be relevant. In comparison, we say that a term is frequent to a topic if relevant documents usually have relatively more occurrences of the term comparing to the non-relevant ones. Documents in which the frequent term appears many times are more likely relevant compared to those where the term appears less frequently. Note that the two properties do not conflict with each other: a term can be both necessary and frequent.
We hypothesize that both necessary and frequent terms exist in user queries, but some query terms may only conform to one of the two properties. We study the following research questions:
RQ1: Do query terms differ with respect to the two properties? We examine the two properties of query terms in a dataset based on term occurrences in relevant and non-relevant documents.
RQ2: How do users perceive the two properties of query terms? Do users' opinions agree with those learned from the dataset and do users agree with each other? We ask assessors to annotate query terms regarding the two properties and analyze the results.
RQ3: Assuming we know the properties of query terms, can we improve search performance by treating terms differently? We show a simple approach that can achieve 35% improvement in nDCG@10 compared to the query likelihood model, if it knows these properties of query terms. Results suggests great potential for improving search performance by identifying properties of query terms and treating them differently in retrieval models.

2. EVALUATION OF TERM PROPERTIES

In this section, we define indicators and examine query term properties in the TREC Robust 2004 dataset.

2.1 Indicators of Term Properties

We denote the degree to which a query term w is necessary to a

topic by P(X=1|R), the probability of observing w in the set of

relevant documents, R. X=1 refers to the occurrence of w in a

document regardless of its frequency. In a dataset with R being

judged, we can estimate P(X=1|R) by Equation (1), where: N is

the total number of documents in R; Nw is the number of documents in R where w appears at least once. The greater the

value of P(X=1|R), the more necessary the term to the topic.

P^ 

X

1|

R



Nw N

(1)

We evaluate to what degree a query term w is frequent to a

topic by comparing P(w|R) and P(w|NR), where NR is the set of

non-relevant documents. P(w|R) is the probability of the term w in

relevant documents, which is estimated by Equation (2), where:

P(w|d) is the probability of w in the multinomial document

language model of d; each document d in R has an equal weight

1/N to contribute to P(w|R). We estimate P(w|d) using maximum

likelihood estimation with Dirichlet smoothing [4]. The parameter

 is selected to optimize the nDCG@10 of query likelihood model

1167

1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0
0

P(X=1|R)
75 150 225 300 375 450 525 600
(a)

10000.000 1000.000
100.000 10.000 1.000 0 0.100 0.010
0.001

P(w|R)/P(w|NR)
75 150 225 300 375 450 525 600
(b)

16.000 8.000
4.000 2.000 1.000
0 0.500 0.250
0.125

P(w|R, X=1)/P(w|NR, X=1)
75 150 225 300 375 450 525 600
(c)

Figure 1. Distribution of P(X=1|R), P(X=1|R)/P(X=1|NR), P(w|R)/P(w|NR), and P(w|R, X=1)/P(w|NR, X=1) on 663 query terms.

in the dataset. We estimate P(w|NR) in a similar form but among the set of non-relevant documents. The greater the value of P(w|R) compared to P(w|NR), the more frequent is the term w.

P^ w |

R



1 N

 dR

Pw|

d



(2)

It should be noted that we can easily observe P(w|R) > P(w|NR)

when w is necessary for R but rarely appears in NR. Therefore, we

further examine a stronger form of the frequent term property:

within the set of documents where w appears at least once,

relatively higher frequency of the term indicates greater likelihood

of relevance. We quantify this stronger property by comparing

P(w|X=1,R) and P(w|X=1,NR). The two probabilities are estimated

similar to Equation (2), but within the set of relevant and non-

relevant documents where w appears at least once.

2.2 Evaluation

We calculate the indicators related to term properties in TREC Robust 2004 dataset. The dataset includes 250 queries and 663 query terms (counting multiple occurrences of the same term in different queries). We remove the Indri standard stopwords and stem using the Krovetz stemmer when processing documents and queries. Figure 1(a), 1(b), and 1(c) show the distribution of P(X=1|R), P(w|R)/P(w|NR), and P(w|R,X=1)/P(w|NR,X=1) for the 663 query terms.
Results show that it is very common to use query terms that do not hold the two properties. As shown in Fig. 1(a), among the 663 query terms, only 18.5% are fully necessary ­ i.e., P(X=1|R)=1 ­ and 44.8% roughly hold the necessary property ­ P(X=1|R)0.8. Moreover, 33% of the query terms do not hold the necessary property (P(X=1|R)<0.5), and 50% of the queries have at least one such term. Using query terms with the frequent term property is also very common in the dataset: Figures 1(b) and 1(c) show that 475 out of the 663 query terms (71.6%) hold the basic frequent term property, but only 373 (56.3%) hold the stronger form where P(w|R,X=1)/P(w|NR,X=1)>1. Among the 250 queries, 57.8% have at least one term that does not hold the frequent term property and 75.1% have at least one term that does not hold the stronger form of the frequent term property.
We further evaluate the relation between search effectiveness and using query terms that do not hold the two properties. Figure 2 shows the average nDCG@10 of queries in which at least one term's value of the three indicators is less than P, where P ranges from 0.1 to 1.0. Results suggest that queries with terms that do not hold either of the two properties are less effective. For example, for the set of queries with at least one term's value of P(X=1|R) < 0.5, the nDCG@10 of these queries is only 0.356, less effective than those of the 250 queries on average. For queries with terms that do not hold either of the two properties, search performance declined by a greater magnitude. However, we noticed that for queries with terms that have P(w|R)/P(w|NR) < P ranging from 0.2 to 0.6, there are no apparent differences in the queries' search

performance. This indicates that P(w|R)/P(w|NR) is less indicative of term's search effectiveness. In following discussions, we use the stronger form of frequent term property and adopt P(w|R,X=1)/P(w|NR,X=1) as the indicator.

0.50

nDCG@10 of queries P(X=1|R) < P

0.45

nDCG@10 of queries P(w|R)/P(w|NR) < P

nDCG@10 of queries P(w|R,X=1)/P(w|NR,X=1) < P

0.40

0.35

0.30

0.25

0.20

0.15
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 ALL P
Figure 2. nDCG@10 of queries with at least one term for which the three indicators < P. P ranges from 0.1 to 1.0. "ALL" shows the average nDCG@10 of ALL queries.

Necessary Terms P(X=1|R)0.8

Frequent Terms P(w|R,X=1)/P(w|NR,X=1)>1

74 terms

224 terms

149 terms

216 terms are neither necessary nor frequent.
Figure 3. Overlap of terms conforming to two properties.
We further show the overlap of query terms conforming to the two properties in Figure 3. Among the 663 terms, 224 (33.8%) are both necessary and frequent, but 223 (33.6%) only hold one of the two properties. The remaining 216 terms (32.6%) are neither necessary nor frequent. This suggests different strategies should be adopted to improve ineffective queries. The 216 terms that do not hold either property are not indicative of document relevance and would be better removed. For the 74 terms only having the necessary term property, we should prefer documents where the term appears but do not give further credit to high term frequency. For the 149 terms only having the frequent term property, we should prefer documents where the term appears many times over those where the term appears only once or twice, but it may be risky to filter out documents without any occurrence of the term.
To summarize, our results show that whether or not a term holds the two properties affects the search effectiveness of queries. In the dataset, only 1/3 of the query terms hold both properties. Another 1/3 hold only one of the two properties. The other 1/3 have neither property. This suggests that we may

1168

improve search systems in two different ways: identify terms with these properties and adopt different ranking criteria; predict terms without any of the two properties and discount the effects of such terms in ranking. In further sections, we explore such potentially by assuming we can correctly identify properties of terms.

3. USER JUDGMENTS OF PROPERTIES
In this section, we study whether or not users can make correct judgments of these two term properties. This is meaningful for two reasons. First, if the users make poor judgments on query properties, it provides a new explanation for ineffective queries. Second, it the users can make correct judgments, systems may benefit from providing query languages allowing users to express their sense of the properties.
We asked 10 users to annotate 100 TREC queries selected from the TREC Robust 2004 dataset (Topic 301-400). Each user annotated 15 queries, with 10 overlapping with another two users. For example, the first user annotated query 301-315, the second user on query 311-325, ... , and the last user on query 391-400 as well as 301-305. This resulted in 10 users' annotations on the 100 queries. For 50 queries, we have only one user's annotation, and for the other 50, we have two users' annotations, so that we can study users' agreements on the properties of query terms. For each query term, we asked users two yes/no questions as follows. We say that a user annotated a query term as necessary or frequent if the answer on Q1 or Q2 is yes, respectively.
Q1: I believe most of the relevant results should have this word. Results that do not contain this word are unlikely to be useful.
Q2: I believe this word should appear many times in relevant results. Results in which the word appears only once or twice are less likely to be useful.
We found that pairs of users have some agreement on whether or not a term is necessary, but their opinions are rather independent of each other on the frequent terms. Among the 126 query terms involving two users' annotations, users agreed in 67% of the cases regarding whether or not a term is necessary. However, they agreed only in 48% of the cases on whether a term is frequent.

Table 1. Correctness of user annotation of term properties.

Property

P

Num Y/N by P

Num Y/N by Users

User Acc / Prior

Class

Prec

Rec

0.8 88/164 Necessary
0.5 145/107

201/51

0.50/0.65

Y N

201/51

0.63/0.57

Y N

0.41 0.93 0.88 0.27 0.63 0.87 0.63 0.30

1.0 124/128 Frequent
0.8 156/96

165/87

0.60/0.51

Y N

165/87

0.61/0.62

Y N

0.57 0.76 0.66 0.45 0.67 0.71 0.48 0.44

Table 1 shows the accuracy of users' annotations of query term properties comparing to those evaluated by the values of P(X=1|R) and P(w|R,X=1)/P(w|NR,X=1). Results show that in general it is difficult for users to make correct judgments on the query terms' properties. If we use P(X=1|R)>0.5 as the criteria for necessary terms, users' judgments are slightly better than a classifier using prior probability of the classes (accuracy 0.63 versus 0.57). When we use P(w|R,X=1)/P(w|NR,X=1)>1.0 as the threshold for frequent terms, users did also only slightly better than a classifier using prior probabilities (accuracy 0.60 versus 0.51). The accuracy and precision of user judgments look not useful. Moreover, when we adopt different criteria for term properties, e.g. P(X=1|R)>0.8, users' judgments may even be worse than a classifier using the prior probability of classes.

To conclude, the results of user annotation on query term properties show that it is very difficult for users to select the properties of query terms prior to looking at search results. Users also agree only slightly with others on whether a term property applies. Specifically, users' judgments on frequent terms are completely independent of others.

4. SYSTEMS USING TERM PROPERTIES

In this section, we explore the potential of improving retrieval systems assuming we know the properties of terms correctly. The prediction of term properties is left for future work.

4.1 Approaches

Let q be a query. We assume we know the set of necessary terms qN and the set of frequent terms qF. Note that qN and qF can be empty set, and a term in q may be in neither qN nor qF. We rank a document d by Equation (3), where: we assume qN and qF are independent given d; each term in qN and qF are generated independently of other terms from d by different process PN(w|d) and PF(w|d).
P qN , qF | d 

 PqN | d  PqF | d 

(3)

  PN w | d    PF w | d 

wqN

wqF

We calculate PN(w|d) and PF(w|d) in Eq(4) and Eq(5). In Eq(4), we calculate PN(w|d) as the probability of selecting a term w from d's vocabulary Vd ignoring the frequency of terms in d. |Vd| is the size of d's vocabulary. PN(w) is the probability that w exists in a the vocabulary of a document in the whole corpus. In a corpus of

k documents, we estimate PN(w) as Eq(6). N is a parameter for smoothing. PF(w|d) is simply the probability of a term w from the multinomial document language model of d, estimated using

maximum likelihood estimation with Dirichlet smoothing. In our

experiments, we set F to the value that can maximize nDCG@10 of using all terms as qF and no term as qN for retrieval (equivalent to query likelihood model). In contrast, we set N to the value that can maximize nDCG@10 of using all terms as qN and no term as qF for retrieval.

P^N



w

|

d





1



N Vd

 PN 
 N

w

(4)

P^F



w

|

d





c



w,

d

  F 
d  F

PF



w

(5)

 P^N w 

1 k



d

1 Vd

(6)

For a necessary term w in qN, PN(w|d) totally ignores the frequency of w in d. Its value depends only on whether or not w

appears in d. In addition, it favors documents with a small

vocabulary. (This is intuitively correct because observing w in d is

less informative if d is very long and has a large vocabulary.)

When we put all the query terms into qF and none into qN, Equation (3) falls back to the query likelihood language model.

4.2 Search Effectiveness

In this section, we evaluate the approaches proposed above by assuming different sets of necessary and frequent terms. Table 2 shows the results. For "qN" and "qF" in Table 2, "none" means do not use any terms, "all" means using all query terms, and "best" means using the best possible combination of query terms (the set of query terms that leads to the best nDCG@10).
We first evaluate the effectiveness of PN(w|d) and PF(w|d) on different set of terms individually. Unsurprisingly, using all terms as necessary terms (N++) performs worse than using all terms as frequent terms (F++ and also Query Likelihood). However,

1169

simply ignoring term frequencies of all documents still achieved nDCG@10 as high as 0.293. This indicates that solely considering term occurrences is still useful in many cases. However, simply using all terms as both necessary and frequent terms (N++F++) did not result in any improvements.
We further examine whether removing inappropriate terms from qN or qF can lead to improved search performance. As shown in Table 2, removing inappropriate terms from qF can potentially improve nDCG@10 from 0.438 (F++) to 0.514 (F+), and from 0.436 (N++F++) to 0.528 (F++F+). Similarly, removing terms from qN can potentially improve nDCG@10 from 0.293 (N++) to 0.329 (N+), and from 0.436 (N++F++) to 0.503 (N+F++). When we remove inappropriate words from both qN and qF (N+F+), we can potentially improve nDCG@10 to 0.590, which is about 35% improvements comparing to QL and N++F++. This suggests that there is great potentiality of improving search performance if we can predict correctly the frequent and necessary words.
However, it should be noted that the best set of terms for qN and qF are dependent of each other. When we use the best set of qN in N+F++ and the best set of qF in N++F+ for retrieval (N+F+ local), there will be 10% decline of nDCG@10 comparing to N+F+. Besides, we found that a part of the improvement of search performance comes from removing inappropriate terms from both qN and qF. If we restrict that all the query terms should be in at least one of qN and qF (N+F+ (-rmv)), the nDCG@10 declined from 0.590 to 0.552, although still a substantial improvement comparing to F++ (QL).
We further examine whether using the indicators of properties in section 2, i.e., P(X=1|R) and P(w|R,X=1)/P(w|NR,X=1), can effectively select the appropriate set of terms for qN and qF to enhance search performance. We examined a simple rule-based approach as follows. We start with all query terms in qF and no terms in qN. We remove terms in qF if P(w|R,X=1)/P(w|NR,X=1) < 1.05. If the removed term has P(X=1|R)>0.2, we add the term into qN. Besides, we add all terms with P(X=1|R)>0.95 into qN. This simply rule-based approach (N+F+ P) improves nDCG@10 by 8.7% comparing to F++ (using all terms for qF). This suggests that the two indicators are effective criterion of selecting qN and qF. However, the performance of the selected qN and qF cannot be compared with the best possible qN and qF in N+F+. This indicates that the two indicators are not enough for selecting qF and qN. The exploration of predictors for qF and qN is left for future works.
Earlier, we showed that users made poor judgments on the properties of query terms. To further verify the quality of users' judgments, we select terms into qN and qF if users answered yes in Q1 and Q2. As shown in Table 2, this approach reduces search

Table 2. Potential improvements of search performance.

Label F++ (QL) F+ N++ N+ N++F++ N++F+ N+F++ N+F+ N+F+ local N+F+ (-rmv)
N+F+ P
N+F+ user
F+RM

qN none none all best all all best best best.L best
P(X|R)
user
none

qF all best none none all best all best best.L best P(w|R,X) / P(w|NR,X)
user
RM100

nDCG@10 0.438 0.514 0.293 0.329 0.436 0.528 0.503 0.590 0.541 0.552
0.476
0.416
0.644

Change / Baseline -
+17.4% / F++ -
+12.3% / N++ -
+21.1% / N++F++ +15.4% / N++F++ +35.3% / N++F++ +24.1% / N++F++ +26.6% / N++F++
+8.7% / QL
QL: nDCG@10 0.443 (100 queries)
-

* N/F in the run labels refers to qN/qF; ++ means using all terms; + means using selected query terms.

performance. The nDCG@10 is 0.416 (N+F+ user) versus 0.443 in QL on the same set of 100 queries. This further confirms that it is difficult for users to make useful judgments on term properties.
So far we limit the set of query terms among those being issued by the users, and the improvements of search performance mainly comes from correct identification of the necessary terms and the frequent terms. We compare our approach with query expansion on the potential of improving search performance. We estimate the true relevance model based on qrels, and use the top 100 terms ("RM100") as qF for search. As shown in Table 2, solely working on the set of query terms issued by users, N+F+ is not much worse than F+RM (true relevance model) on nDCG@10, which extensively exploits the representative terms in relevant results.
5. FUTURE WORK
In this preliminary study, we show that retrieval models that exploit term frequency can potentially be improved substantially by separately considering TF for some query terms and counting only occurrence or non-occurrence for some other query terms. This conclusion comes from our findings that query terms hold different properties. Specifically, sometimes the frequencies of terms do not indicate document relevance as long as the terms appear. In such cases, existing retrieval models may incorrectly rank documents with high term frequencies to the top. Queries with terms lacking either property are less effective in general.
Future work on this topic mainly focuses on the prediction of an appropriate set of terms in qN and qF. As discussed in section 4, though values of the two indicators can effectively predict qN and qF, it is far from perfect and the two indicators are also computed based on known relevance judgments.
Our study is closely related but different from the recent work of term necessity prediction by Zhao and Callan [5, 6]. Zhao et al. focused on predicting P(w|R) and aimed at solving term mismatch by selecting terms with highly predicted P(w|R) values for query expansion. In comparison, we do not expand the query but aim at recognizing the correct properties of query terms that are issued by the users. The two approaches follow different directions but may potentially be combined. As shown in Table 2, our approach may have substantial improvements on search performance that is comparable to those can be achieved by predicting P(w|R).
ACKNOWLEDGEMENT
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
6. REFERENCES
[1] Lafferty, J. and Zhai, C. 2001. Document language models, query models, and risk minimization for information retrieval. In Proc. SIGIR'01: 111-119.
[2] Ponte, J.M. and Croft, W.B. 1998. A language modeling approach to information retrieval. Proc. SIGIR'98: 275-281.
[3] Robertson, S.E. et al. 1995. Okapi at TREC-3. NIST Special Publication 500-226: Proceedings of the Third Text REtrieval Conference (TREC-3).
[4] Zhai, C. and Lafferty, J. 2001. A study of smoothing methods for language models applied to Ad Hoc information retrieval. In Proc. SIGIR'01: 334­342.
[5] Zhao, L. and Callan, J. 2012. Automatic term mismatch diagnosis for selective query expansion. In Proc. SIGIR'12: 515-524.
[6] Zhao, L. and Callan, J. 2010. Term necessity prediction. In Proc. CIKM'10: 259­268.

1170

Predicting Query Performance In Microblog Retrieval
Jesus A. Rodriguez Perez, Joemon M. Jose
School of Computing Science University of Glasgow Glasgow
(Jesus.RodriguezPerez; Joemon.Jose)@glasgow.ac.uk

ABSTRACT
Query Performance Prediction (QPP) is the estimation of the retrieval success for a query, without explicit knowledge about relevant documents. QPP is especially interesting in the context of Automatic Query Expansion (AQE) based on Pseudo Relevance Feedback (PRF). PRF-based AQE is known to produce unreliable results when the initial set of retrieved documents is poor. Theoretically, a good predictor would allow to selectively apply PRF-based AQE when performance of the initial result set is good enough, thus enhancing the overall robustness of the system. QPP would be of great benefit in the context of microblog retrieval, as AQE was the most widely deployed technique for enhancing retrieval performance at TREC. In this work we study the performance of the state of the art predictors under microblog retrieval conditions as well as introducing our own predictors. Our results show how our proposed predictors outperform the baselines significantly.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords
Ad-hoc Retrieval; Query Performance Prediction; Query Expansion
1. INTRODUCTION
Most information retrieval systems experience a high variability in retrieval performance across different queries. Whilst many queries are satisfied successfully, the system produces poor results for many others. Since a number of retrieval approaches rely on the initial set of results, it would be highly desirable to predict when retrieval results are not satisfactory enough.
This task is known as query performance prediction (QPP), and has been an active and challenging area of research over
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, QLD, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609540.

the last decade. Multiple predictors have been proposed in the literature with varying degrees of success. These predictors fall mainly into two categories: pre-retrieval and post-retrieval predictors. Pre-retrieval predictors are computed before retrieving any documents, thus relying solely on query term features. On the other hand, post-retrieval predictors rely on features extracted from the retrieved documents. Post-retrieval predictors mainly estimate how well a query is represented by the retrieved documents.
In this work, we study pre and post retrieval predictors for microblog retrieval tasks. Although much work has been done in predicting the performance of queries over web collections, to the best of our knowledge, no work has been done in the context of microblogs. Microblogging platforms such as Twitter have gained momentum over recent years providing a new way of sharing information and broadcasting short messages over a network of users. Microblogs present many differences with respect to web documents both in morphology and content [9]. Mainly, microblogs constitute a time ordered stream of very short documents as they are published. Moreover, microblogs contain community defined tags to refer to certain topics (hashtags), or people (mentions), which we intent to investigate in our QPP study.
The motivation behind studying QPP for microblogs resides in increasing the robustness of existing retrieval approaches. More specifically, QPP can be especially handy for selectively applying pseudo relevance feedback (PRF) based automatic query expansion (AQE) approaches [2]. PRFbased AQE approaches rely on the initially retrieved set of documents. Thus if these documents loosely represent the initial information need, PRF-based approaches most likely result in unexpected behaviour, and unreliable results.
Effective QPP represents an opportunity to estimate the performance of a system for a particular query, based on pre-retrieval and post-retrieval features. In turn, this would allow an IR system to selectively perform AQE, when the circumstances are most propitious, based on estimates given by the predictors.
Our work is driven by two research questions. (RQ1) To what extent, can we predict the performance of a retrieval model for microblog corpora?. (RQ2) To what extent, the combination of predictors can improve overall prediction performance, in the context of microblogs?.
In this work, we investigate the performance of previously proposed predictors [5], in the context of microblogs. We then show that they fail to perform effectively which prompts the need to develop better predictors. We propose a number of predictors, which take into consideration the

1183

characteristics of microblogs. Our evaluation findings show how our predictors outperform those found in the literature. Finally we further improve our performance by learning a prediction model, combining our predictors by means of a support vector machine for regression.
2. RELATED BACKGROUND
One of the main works in query performance prediction is that by [3]. In their work, they proposed a predictor based on the Kullback-Leiber divergence between the query's and the collection's language models. This predictor attempts to quantify the "clarity" of the query. In other words, the nonambiguity of the query which in turn should reflect on how well it represents a particular topic. Their evaluation shows good correlation of their predictor with average precision, using Spearman's ranking correlation tests.
Work by [7] suggests other predictors such as the standard deviation of IDF values within the query. They also defined a simplified version of the "Clarity Score" proposed by [3] namely Simplified Clarity Score (SCS). Finally, they proposed an alternative to SCS called query scope (QS). Their main objective was to investigate pre-retrieval predictors, as post-retrieval predictors are normally computationally more expensive to use.
To predict query difficulty [8] proposed a query coherence score (QC-1) which attempts to quantify how related are the query terms to the retrieved set of documents as well as measuring the differences between the language used in the retrieved set and the query, with respect to the collection. They found that their approach correlates well with average precision, using Spearmans's rank correlation test. Furthermore they also suggested two other versions of this score however their performance was poorer than their simpler first version. For their evaluation they used a number of retrieval models, including BM25 and TFIDF to retrieve documents from the TREC Robust track collection.
Work by [10] proposed a series of pre-retrieval performance predictors. One of the most succesful was SCQ. The aim of SCQ is to compute a similarity score between the queries and the collection. Moreover, they also proposed a variability measure relying on the standard deviations of TFIDF scores for the query terms. Furthermore, they also proposed a predictor using both previous approaches together. Their evaluation showed how the combined predictor outperformed all previous approaches. It is important to note that their joint approach is slightly better than their simple SCQ, only when the linear interpolation gives most of the weight to SCQ, albeit being much more complex, thus computationally much more expensive. In this work, we will evaluate the performance of SCQ in our particular context.
A short but comprehensive survey of performance predictors was produced by [6]. This study helped on deciding which predictors to include in our study in the context of microblogs, as it showed results for many state of the art predictors across multiple collections.
Previous evaluations. The "de facto" evaluation procedure in previous work has been the statistical correlations between the predictors and the evaluation metric results for a given system. More often than not, the evaluation metric used was Average Precision (AP). The better the predictor estimates the performance of the system in terms of an evaluation metric, the higher the correlation scores.

The most used correlation metrics are Kendall-Tau (K.Tau) and Spearman's (SP.Rho) rank correlation coefficients. The SP.Rho correlation coefficient is a measure of statistical dependence between two variables, by which it is estimated how well their relation is represented by a monotonic function (I.e.: grows/decreases always in the same direction). SP.Rho uses Pearson's correlation coefficient in such a way that is much less sensitive to outliers. Kendall-Tau's correlation coefficient is slightly different in that, it does not rely on the values of the variables themselves, but it rather measures the similarity in the ordering of the data provided when ranked by each of the variables.
State of the art prediction. The correlation coefficients obtained for AP in web collections vary wildly. The Kendalltau coefficients, with respect to AP, for the best performing pre-retrieval predictors range from 0.30 to 0.49 depending on the collection [1]. On the other hand, the Kendall-tau coefficients for post-retrieval predictors are generally higher.
It is important to note the high variability in terms or predicting performance, with respect to the collection. The collections used in the literature include "TREC Vol. 4+5"; "WT10g" and "GOV2", where it is often the case for a particular predictor to be the best for a particular collection and the worst for another.
Selective Query Expansion. One of the main applications of QPP is selective Query Expansion [1]. It refers to selectively applying automatic query expansion (AQE) whenever predicted performance is above a certain threshold. This serves as a warranty for PRF-based AQE approaches, as they rely on the top N retrieved documents to perform optimally.
3. PREDICTORS
In this section, we describe the state of the art predictors we will be considering in our evaluation, including our proposed predictors. Subsequently, we introduce the evaluation approach followed to benchmark and compare their performances.
Baseline Predictors: As a starting point we selected those predictors performing best from the survey by [6].
The QueryTermIdf predictor utilizes the IDF values of query terms for making estimations of retrieval performance. The intuition is that the higher the IDF value the more specific a term is, thus score variations across terms may indicate drifting concepts, negatively affecting the performance. We derive different predictors considering the mean, median, standard deviation (Std), max, min and diff(max - min) IDF scores from each query [6]. Moreover, Simplified Clarity Score (SCS) proposed by [7], attempts to model the clarity of a query, i.e. how well it targets a particular topic based on the collections metrics. An homologous predictor to SCS is Query Scope (QS), which was also proposed by [7].
Another predictor is Similarity of Collection w/ Query (SCQ) which was proposed by [10] to compute the similarity between the collection and the query at hand. In a similar context, Term Weight Variability (VAR) was proposed. This predictor measures the variability of weights for a term across the collection. [10] hypothesises that the higher standard deviation for a term, the more discriminative it is.

1184

Moreover, the work by [1] introduced four post retrieval predictors namely NQC, WIG, QF and Clarity. NQC measures the normalized standard deviation of the top scores. The intuition behind this predictor is that relevant documents are assumed to have a much higher score than that of the mean score. Similarly WIG measures the divergence of retrieval of the top-ranked results scores from that of the documents in the corpus. QF and Clarity are predictors that take into account the actual content of the documents. QF measures the divergence between the original top results for the query and the results that would be obtained for a query constructed from the top results. Finally, Clarity measures the KL divergence between a (language) model induced from the result list and the corpus model.
Proposed Predictors: The first two predictors we defined are to measure the coverage of terms over the documents retrieved. The CoveredQueryTerms (QTCov) predictor measures how well the query is being represented by the documents in the result list. For each document, we divide the number of query terms found in the document by the total number of query terms, which gives a normalized value between 1 and 0. (1 being a document that completely fits the query). Similarly we defined TopTermsCoverage (TTCov) which measures the coverage of the top N terms in the result list. Intuitively, the more times these terms appear the more likely documents are to revolve around a particular topic.
Another predictor is TimeCohesion (TimeCH), which taps into the distribution of retrieved tweets over time. We assume that the closer the documents appear with respect to time, the more likely they refer to the same event. To compute it, we take the differences between retrieved document timestamps. Differences are taken only between contiguous documents in the rank.
Also, exploiting microblog specific features we defined the Http predictor. This predictor measures how common is to find a Url in the retrieved set of documents. To this end we count the number of documents with Urls and divide by the total number of documents retrieved. (I.e the rate of Urls). This predictor relies in previous findings which suggest that the presence of Urls indicates the presence of relevant documents [4].
Finally, we defined HashTagCount as the rate of documents with hashtags in the retrieved results set. Hashtags are important in the context of Twitter as they refer to particular topics. Thus the presence of similar hashtags often indicates that users are dealing with the same topic.
3.1 Evaluation
Datasets. In this evaluation we utilize the Tweet2011, 2012 and 2013 collections with a total of contains 170 topics. The collections have been merged together to produce enough evidence for learned predictor models.
Retrieval Model Used. We utilized the DFRee for producing the runs, as it provides competitive performance. (P@10 = 0.527 & P@30= 0.409).
Predictor's Correlation. In the literature, evaluations have mainly taken into account either K.Tau or SP.Rho as correlation measures with respect to average precision (AP).

AP correlations

Predictor

K.Tau SP.Rho

post TTCov Mean

0.302 ** 0.447 **

post TTCov Median

0.253 ** 0.312 **

post TTCov upper 0.356 ** 0.463 **

post TTCov Lower

0.178

0.218 **

post TimeCH Lower

-0.202 ** -0.300 **

post TimeCH Median -0.200 ** -0.291 **

post TimeCH Upper

-0.122 * -0.188 *

post TimeCH Mean

-0.197 ** -0.288 **

pre SCQ Sum

0.094

0.138

pre QueryTermIdf Diff 0.140 ** 0.209 **

Pearson 0.403 0.274 0.434 0.197 -0.273 -0.310 -0.231 -0.281 0.254 0.200

Table 1: Correlations of DFRee runs with AP (**p < 0.01 & *p < 0.05)

In this work, we also pay attention to Pearson's correlation coefficient.
In microblog retrieval, it is most important to optimise performance for the first retrieved documents due to its realtime nature. It has been agreed in the literature that a user will not look further than the first 30 documents, thus AP might not be appropriate for this task. Moreover, to help in selectively applying PRF-based AQE, we focus on the very top retrieved documents, thus we also study prediction in terms of P@10.

4. RESULTS AND DISCUSSION
In this section we introduce and discuss the results we obtained during the evaluation of the above mentioned predictors. Tables 1 and 2 show the correlation coefficients in terms of K.Tau, SP.Rho and Pearson for a subset of predictors. Since it was not possible to show all the predictors in this paper, we have chosen to include only those achieving a Pearson coefficient higher than 0.19. The predictors are prefixed with either "pre " or "post " to indicate whether they are pre-retrieval or post-retrieval predictors. Furthermore, the suffixes: Mean, Median, Std, Max, Min, Lower and Upper; denote mean, median, Standard Deviation, maximum, minimum, lower percentile and upper percentile, of the predictor values respectively. Moreover, Sum refers to the Sum of all predictor values, whereas Diff is the difference between Max and Min.
Table 1 shows the correlations coefficients in terms of AP. In the survey done by [6] the maximum correlation achieved using K.Tau ranged from 0.30 to 0.49 depending on the collection.
State of the art predictors such as VAR, SCS, NQC or WIG (Described in Section 3) performed poorly in the context of microblogs, as their K.Tau coefficient values ranged between 0 and 0.16, thus are not shown in Tables 1 or 2. This demonstrates how challenging performance prediction is in the context of microblog retrieval, and the need for tailored predictors to this new task. On the other hand, the predictors we proposed achieved a much better correlation than that obtained by the state of the art in this context. post TTCov upper is one of such predictors, achieving a K.Tau coefficient of 0.356, being the best correlation with respect to AP. This predictor takes the upper percentile of the rate at which top terms appear in the retrieved set of documents.
These results are very promising, but our main focus is to enable selective PRF-based AQE, thus we present correlations in terms of P@10 in Table 2. As it can be observed, amongst the top performing predictors we find those rely-

1185

P@10 correlations

Predictor

K.Tau SP.Rho

post http

0.163 ** 0.206 **

post QTCov mean

0.291 ** 0.382 **

post QTCov median

0.305 ** 0.382 **

post QTCov upper

0.325 ** 0.404 **

post QTCov lower

0.266 ** 0.336 **

post TTCov mean

0.301 ** 0.416 **

post TTCov median 0.365 ** 0.456 **

post TTCov upper

0.264 ** 0.355 **

post TTCov lower

0.253 * 0.303 **

post TimeCH lower

-0.212 ** -0.286 **

post TimeCH median -0.145 ** -0.199 *

post TimeCH mean

-0.170 ** -0.233 **

post TimeCH diff

0.192 ** 0.269 **

Pearson 0.213 0.375 0.373 0.392 0.312 0.429 0.441 0.374 0.298 -0.236 -0.239 -0.212 0.198

Table 2: Correlations of DFRee runs with P@10 (**p < 0.01 & *p < 0.05)

ing on microblog specific features, namely post TimeCH which measures how close in time are the retrieved tweets and post http measuring the presence of URL's in documents. Additionally, the correlations achieved by these predictors with respect to P@10, are generally higher than that achieved for MAP, with post TTCov Median being the best performing predictor.
An interesting observation regarding post TTCov upper and post TTCov Median is that they may be referring to the same documents, as with AP a larger set of documents is considered compared to P@10.
Combining Predictors. Since our main objective is predicting performance with AQE in mind, we focus on P@10. To combine the predictors for P@10, we used a Support Vector Machine for regression. To avoid biasing we performed a ten-fold cross-validation. The learned prediction model is defined as follows:
P@10 = 0.3028 * TTCov_upper + 0.3494 * QTCov_median + 0.3701 * QTCov_upper - 0.4745 * twids_median - 0.2641 * TTCov_mean + 0.5014 * twids_mean + 0.3394 * TTCov_median + 0.2318 * TTCov_lower + 0.3122 * twids_diff + 0.2429 * http - 0.1651 * QTCov_lower - 0.2745
The correlation coefficients obtained for this model, are 0.412 (+12.88%), 0.559(+22.59%), and 0.539 (+22.22%), for K.Tau, SP.Rho and Pearson respectively.
Finally, the predictors proposed in this work outperform those in the literature, within this particular context. However, as in previous attempts, it is uncertain that it will be enough for enabling effective selective AQE. Nonetheless, these predictors may represent an important step towards that much sought after objective, within the context of microblogs.
5. CONCLUSIONS
In this work, we studied the performance of the state of the art predictors in the context of microblogs. The most sought after benefit from predicting query performance is increasing the robustness of PRF-based AQE approaches. To this end we paid special attention to the prediction in terms of the top retrieved documents, specifically Precision@10 (P@10).
Our evaluation suggests that predictors in the literature perform poorly in the context of microblogs, thus we need to come up with predictors that are better fit for purpose.

To this end, we defined a number of predictors relying on microblog features and characteristics. We benchmarked their performance and showed that most of them outperform those in the literature, with TTCov being the most correlated with MAP and P@10.
Finally, we used support vector machines for regression to learn a prediction model based on the best performing predictors. The resulting model further increased performance by a +22% in terms of the Pearson correlation coefficient, and +12.88% for K.Tau.
Future work will put these findings to a practical application for selective approaches to PRF-AQE, or in the selection of a baseline model to optimize a system's overall performance given the conditions of a particular query. Furthermore, we will study the performance of other predictors which will consider more microblog specific features.
6. ACKNOWLEDGMENT
This research is partially supported by the EU funded project LiMoSINe (288024).
7. REFERENCES
[1] D. Carmel and E. Yom-Tov. Estimating the query difficulty for information retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services, 2(1):1­89, 2010.
[2] C. Carpineto and G. Romano. A survey of automatic query expansion in information retrieval. ACM Computing Surveys (CSUR), 44(1):1, 2012.
[3] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 299­306. ACM, 2002.
[4] D. F. Gurini and F. Gasparetti. Trec microblog 2012 track: Real-time algorithm for microblog ranking systems. 2012.
[5] C. Hauff. Predicting the effectiveness of queries and retrieval systems. University of Twente, 2010.
[6] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 1419­1420. ACM, 2008.
[7] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In String Processing and Information Retrieval, pages 43­54. Springer, 2004.
[8] J. He, M. Larson, and M. De Rijke. Using coherence-based measures to predict query difficulty. In Advances in Information Retrieval, pages 689­694. Springer, 2008.
[9] J. Teevan, D. Ramage, and M. Morris. # twittersearch: a comparison of microblog search and web search. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 35­44. ACM, 2011.
[10] Y. Zhao, F. Scholer, and Y. Tsegay. Effective pre-retrieval query performance prediction using similarity and variability evidence. In Advances in Information Retrieval, pages 52­64. Springer, 2008.

1186

What Makes Data Robust: A Data Analysis in Learning to Rank
Shuzi Niu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Xiubo Geng
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, P. R. China
{niushuzi,gengxiubo}@software.ict.ac.cn, {lanyanyan,guojiafeng,cxq}@ict.ac.cn

ABSTRACT
When applying learning to rank algorithms in real search applications, noise in human labeled training data becomes an inevitable problem which will affect the performance of the algorithms. Previous work mainly focused on studying how noise affects ranking algorithms and how to design robust ranking algorithms. In our work, we investigate what inherent characteristics make training data robust to label noise. The motivation of our work comes from an interesting observation that a same ranking algorithm may show very different sensitivities to label noise over different data sets. We thus investigate the underlying reason for this observation based on two typical kinds of learning to rank algorithms (i.e. pairwise and listwise methods) and three different public data sets (i.e. OHSUMED, TD2003 and MSLR-WEB10K). We find that when label noise increases in training data, it is the document pair noise ratio (i.e. pNoise) rather than document noise ratio (i.e. dNoise) that can well explain the performance degradation of a ranking algorithm.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms
Experimentation
Keywords
Learning to Rank; Label Noise; Robust Data
1. INTRODUCTION
Learning to rank has gained much attention in recent years, especially in information retrieval [11]. When applying learning to rank algorithms in real Web search applications, a collection of training data is usually constructed, where human judges assign relevance labels to a document
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609542.

with respect to a query under some pre-defined relevance judgment guideline. In real scenario, the ambiguity of query intent, the lack of domain knowledge and the vague definition of relevance levels all make it difficult for human judges to give reliable relevance labels to some documents. Therefore, noise in human labeled training data becomes an inevitable issue that will affect the performance of learning to rank algorithms.
An interesting observation is that the performance degradation of ranking algorithms may vary largely over different data sets with the increase of label noise. On some data sets, the performances of ranking algorithms decrease quickly, while on other data sets they may hardly be affected with the increase of noise. This motivated us to investigate the underlying reason why different training data sets show different sensitivities to label noise. Previous work either only observed how noise in training data affects ranking algorithms [21, 2], or focused on how to design robust ranking algorithms to reduce the effect of label noise [17, 6]. So far as we know, this is the first work talking about data robustness to label noise in learning to rank.
To investigate the underlying reasons for our observations, we conducted data analysis in learning to rank based on three public data sets, i.e. the OHSUMED and TD2003 data sets in LETOR3.0 [13], and the MSLR-WEB10K data set. There are multiple types of judging errors [9] and they leads to different noise distributions [19, 10], but a recent study shows that the learning performance of ranking algorithms is dependant on label noise quantities and has little relationship with label noise distributions [10]. Therefore we randomly injected label errors (i.e. label noise) into training data (with fixed test data) to simulate human judgment errors, and investigated the performance variation of the same ranking algorithm over different data sets. In this study, we mainly focus on the widely used pairwise and listwise learning to rank algorithms.
We find that it is the document pair noise ratio (i.e. pNoise) rather than document noise ratio (i.e. dNoise) that can well explain the performance degradation of a ranking algorithm along with the increase of label noise. Here dNoise denotes the proportion of noisy documents (i.e. documents with error labels) to all the documents, while pNoise denotes the proportion of noisy document pairs (i.e. document pairs with wrong preference order) to all the document pairs. We show that the performance degradation of ranking algorithms over different data sets is quite consistent with respect to pNoise. It indicates that pNoise captures the intrinsic factor that determines the ranking performance. We also find the increase

1191

of pNoise w.r.t. dNoise varies largely over different data sets. This explains the original observation that the performance degradation of ranking algorithms varies largely over different data sets with the increase of label noise.
2. BACKGROUND
Before we conduct data analysis in learning to rank, we first introduce related work and our experimental settings.
2.1 Related work
How noise affects ranking algorithms has been widely studied in previous work. Voorhees [18] and Bailey et al. [2] show the relative order of ranking algorithms in terms of performance is actually quite stable despite of remarkable noise among human judgements. Recently various learning to rank algorithms have been proposed [7, 4, 22, 3]. Correspondingly, noise in training data has also attracted much attention in this area. Xu et al. [21] explored the effect of training data quality on learning to rank algorithms.
There are also various approaches proposed to learn robust ranking models [6, 20]. Another solution to deal with noisy training set in learning to rank is to employ pre-processing mechanisms, such as "pairwise preference consistency" [5] and repeated labeling techniques [16].
Similar to our work, there do exist a branch of studies on how to construct effective training data in learning to rank. They show us what characteristics of training data are needed to train a effective ranker [1, 8]. Especially, [12] makes an extensive analysis of the relation between the ranking performance and sample size. Different from existing work, our work focuses on investigating the inherent characteristics of training data related to noise sensitivity.
2.2 Experimental Setting
Here we present data sets and ranking algorithms used in our experiments.
Data Sets. In our experiments, we use three public data sets in learning to rank, i.e. OHSUMED, TD2003 and MSLR-WEB10K, for training and evaluation. Among the three data sets, OHSUMED and TD2003 in LETOR3.0 [13] are constructed based on two widely used data collections in information retrieval respectively, the OHSUMED collection and ".gov" collection for TREC 2003 topic distillation task respectively. MSLR-WEB10K is another data set released by Microsoft Research, where relevance judgments are obtained from a retired labeling set of a commercial Web search engine (i.e. Bing). We choose such three data sets for our experiments, since they have quite different properties described in Table 1. All the three data sets can be further divided into training set and test set with their "standard partition". In our experiments, all the evaluations are conducted using the 5-fold cross validation.
Ranking Algorithms. In our work, we mainly focus on the widely applied pairwise and listwise learning to rank algorithms. We employ two typical pairwise learning to rank algorithms, namely RankSVM [7] and RankBoost [4], and two typical listwise learning to rank algorithms, namely ListNet [3] and AdaRank [22]. These algorithms are chosen not only because they belong to different categories of learning to ranking approaches (i.e. pairwise and listwise), but also because they adopt different kinds of ranking functions (i.e. linear and non-linear). Specifically, RankSVM and ListNet use a linear function for scoring, which can be repre-

sented as f (x) = w · x. Meanwhile, RankBoost and AdaRank

are both ensemble methods that combine many weak rank-

ipnrgesfsuendctaisonfs(.x)T=heir nnt=o1n-litnheta(xr )r,awnkhienrge

function ht(x) is

can the

be exchosen

weak ranking function at the t-th iteration.

3. DOCUMENT LABEL NOISE
In this section, we first simulate the label noise in training data with the same noise injection method as [14, 21]. Then we show our basic observations on data sensitivities to label noise.
3.1 Noise Injection Method
We take the original data sets as our ground truth (i.e. noise free data) in our experiments. To simulate label noise in real data sets, we randomly inject label errors into training data, while the test data is fixed. We randomly change some of the relevance labels to other labels uniformly. Since the noise is introduced at the document level, here we define the document noise ratio as the proportion of noisy documents (i.e. documents with error labels) to all the documents, referred to as dNoise. Given a dNoise , each query-document pair keeps its relevance label with probability 1 - , and changes its relevance label with probability . Note that with the simple noise injection method, we assume that human judgment errors are independent and equally possible in different grades. In practice, the possibility of judgment errors in different grades are not equal [15], which can be considered in our future work.
3.2 Basic Observations
Here we show our basic observations which motivated our work. We consider the ranking performance variation in terms of MAP and NDCG@10 with dNoise from 0 to 0.5 with a step of 0.05. For each fold with a given dNoise, we will apply our noise injection method to a training set for 10 times, and obtain the average performance over the corresponding noise free test set. Each point in the performance curve is derived from the average over 5 folds in Figure 1.
The performance degradation of ranking algorithms may vary largely over different data sets with the increase of dNoise. Take RankSVM for instance in Figure 1(a), its performance on TD2003 in terms of MAP (i.e. orange curve with up triangles) decreases quickly as the dNoise increases. Its performance on OHSUMED (i.e. black curve with up triangles) keeps stable for a long range of dNoise, and then drops. Its performance on MSLR-WEB10K (i.e. blue curve with up triangles) is hardly affected even though the dNoise reaches 0.5. The results are consistent in terms of NDCG@10 corresponding to three curves with circles in Figure 1(a). Besides, we can also observe very similar performance degradation behavior with the other three algorithms in Figure 1(b), (c) and (d). In fact, similar results can also been found in previous work [21], but such observations are not the main concern in their work.
The above observations are actually contrary to the following two intuitions. 1)Degradation Intuition: For a machine learning algorithm, its performance usually would degrade along with the deterioration of the training data quality (i.e. increase of noise in the training data), no matter quickly or slowly. 2)Consistency Intuition: For a same machine learning algorithm, the performance degradation behavior against label noise usually would be similar across

1192

Data Sets OHSUMED
TD2003 MSLR-WEB10K

#queries 106 50
10,000

Table 1: Detailed Statistics of Three Data Sets

#docs Ave. #docs/query #features relevance judgments

16,140

152

45

0,1,2

49,058

981

64

0,1

1,200,192

120

136

0,1,2,3,4

label proportions(%) 70:16:14 99.2:0.8
51.7:32.5:13.3:1.7:0.8

0.6

0.6

0.5

0.5

MAP-OHSUMED

N@10-OHSUMED

MAP-TD2003

N@10-TD2003

0.5

MAP-MSWEB10K

N@10-MSWEB10K

0.5

0.4

0.4

0.4

0.4

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.0 0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.0 0.1 0.2 0.3 0.4 0.5

dNoise

dNoise

dNoise

dNoise

(a) RankSVM

(b) RankBoost

(c) ListNet

(d) AdaRank

Figure 1: Performance Evaluation against dNoise with four ranking algorithms in terms of MAP and NDCG@10

0.6

0.6

0.5

0.5

MAP-OHSUMED

N@10-OHSUMED

0.5

MAP-TD2003 MAP-MSWEB10K

N@10-TD2003 N@10-MSWEB10K

0.5

0.4

0.4

0.4

0.4

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.0 0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.0 0.1 0.2 0.3 0.4 0.5

pNoise

pNoise

pNoise

pNoise

(a) RankSVM

(b) RankBoost

(c) ListNet

(d) AdaRank

Figure 2: Performance Evaluation against pNoise with four ranking algorithms in terms of MAP and NDCG@10

.

the data sets. A possible reason for the above results is that the label noise (i.e. dNoise) cannot properly characterize the deterioration of the training data quality in learning to rank. This brings us the following question: what is the true noise that affects the performances of ranking algorithms?

Table 2: Proportions of Predicted Positive Document Pairs

with a tie

RankSVM RankBoost ListNet AdaRank

OHSUMED

0.49

0.45

0.48

0.44

TD2003

0.50

0.49

0.50

0.47

MSLR-WEB10K

0.50

0.49

0.49

0.44

4. DOCUMENT PAIR NOISE
To answer the above question, we need to briefly re-visit the learning to rank algorithms. As we know, the pairwise ranking algorithms transform the ranking problem to a binary classification problem, by constructing preference pairs of documents from human labeled data. The FIFA World Cup may help understand that a ranking list, a basic unit in listwise learning, is generated by pairwise contests. Thus, for both pairwise and listwise learning, the quality of pairs turns out to be the key to the ranking algorithms. In other words, errors (i.e. noise) in document pairs arise from the original document label noise might be the true reason for the performance degradation of ranking algorithms.
4.1 Definition
To verify our idea, we propose to evaluate the performance of ranking algorithms against the document pair noise. Similar to the definition of dNoise, here we define the document pair noise ratio as the proportion of noisy document pairs (i.e. document pairs with wrong preference order) to all the document pairs, referred to as pNoise. We would like to check whether the performance degradation of ranking algorithms against pNoise is consistent across the data sets.
For this purpose, we first investigate how document pair noise arises from document label noise and take a detailed look at pNoise. The document pairs di, dj (i.e. document di is more relevant than dj to the given query) generated from a noisy training data set can be divided into three

categories according to the original relationship of the two documents in the noise free data set. (1)Correct-Order Pairs. Document di is indeed more relevant than dj in the original noise free training data. It is clear that the correctorder pairs do not introduce any noise since they keep the same order as in the noise free case, even though the relevance labels of the two documents might have been altered. (2)Inverse-Order Pairs. Document dj is more relevant than di in the original noise free training data instead. Obviously, the inverse-order pairs are noisy pairs since they are opposite to the true preference order between two documents. (3)New-Come Pairs. Document di and dj are a tie in the original noise free training data instead, so we estimate its true preference order by the ranking model learned from the noise free training data. The probability being a noisy pair is estimated by the proportion that the prediction score of dj is higher than di through experiments1. The experimental results in Table 2 show that there is approximately half a chance of being a noisy pair.
According to the above analysis, the pNoise of a given data set with known noisy labels can be estimated as follows
pNoise = Ninverse + 0.5  Nnew , Nall
1We randomly sample a collection of document pairs (e.g. 10, 000 pairs) with a tie from the noise free training data, and compute the proportion of the predicted negative pairs. We denote a document pair, di and dj , as positive if the predicted relevance score of di is higher than dj , otherwise negative. Experiments are conducted over all the three data sets with both pairwise and listwise ranking algorithms and results are averaged over 5-fold training sets.

1193

where Ninverse, Nnew and Nall denote the number of inverseorder pairs, new-come pairs and all the document pairs, respectively.
With this definition, the performance variations of ranking algorithms against pNoise are depicted in Figure 2, where each sub figure corresponds to performances over three data sets using four ranking algorithms respectively. Surprisingly consistent behavior for the performance degradation is observed across different data sets. The performances (e.g. MAP and NDCG@10) of ranking algorithms keep quite stable as pNoise is low. When pNoise exceeds a certain point2 (around 0.5 as indicated in our experiment), performances drop quickly. The results w.r.t. pNoise are in accordance with the degradation and consistency intuitions mentioned before, which are violated in the case of dNoise.
Therefore, the results indicate that pNoise captures the intrinsic factor that determines the performance of a ranking algorithm, and thus can well explain the consistency of performance degradation of various ranking algorithms.

4.2 pNoise VS. dNoise
Now we know that dNoise is a natural measure of label noise in training data as label noise is often introduced at document level in practice, while pNoise is an intrinsic measure of noise that can reflect the true noise for ranking algorithms. Here we explore the variation of pNoise against dNoise across different data sets in Figure 3.
The increase of pNoise with respect to dNoise varies largely over different data sets, which actually well explains our basic observations. Given a small dNoise (e.g. 0.1) on TD2003, pNoise reaches a very high value (>0.4) in Figure 3, which is the turning point of the performance curve in Figure 2. This explains why the performances of ranking algorithms on TD2003 drop quickly along with the increase of dNoise. On the contrary, on OHSUMED and MSLR-WEB10K, the variations of pNoise with respect to dNoise are more gentle, and correspondingly the performances of ranking algorithms are quite stable. Even when dNoise reaches 0.5 on MSLRWEB10K, the corresponding pNoise is still below a threshold (about 0.4), which means the pNoise has not reached the turning point of the performance curve on MSLR-WEB10K according to blue curves in Figure 2. This explains why the ranking performance is hardly affected by dNoise on MSLRWEB10K in our basic observations.

0.6

OHSUMED

TD2003

MSLR-WEB10K 0.5

0.4

pNoise

0.3

0.2

0.1

0.0 0.0 0.1 0.2 0.3 0.4 0.5 dNoise
Figure 3: pNoise with respect to dNoise over Three Data
Sets

5. CONCLUSION
In this paper, we conducted data analysis to first address the data robustness problem in learning to rank algorithms. In our study, we find that document pair noise captures
2There is a threshold of pNoise, after which the performance will be affected by the label noise much heavily. This guess needs further theoretical guarantee, which are not included in our work.

the true noise of ranking algorithms, and can well explain the performance degradation of ranking algorithms. As a measure of labeling accuracy, it can be used for annotator filtering in a crowdsourced relevance labeling task in our next step. Additionally the current noise injection method is somehow simple and we may further improve the injection method for better analysis. In fact, the noise ratio could be different among queries due to various difficulty levels.
6. ACKNOWLEDGMENTS
This research work was funded by the 973 Program of China under Grants No. 2012CB316303, No. 2013CB329602, the 863 Program of China under Grants No. 2014AA015204, No. 2012AA011003, the National Natural Science of China under Grant No. 61232010, No. 61203298 and the National Key Technology R&D Program of China under Grants No. 2012BAH39B02.
7. REFERENCES [1] J. A. Aslam, E. Kanoulas, and et. al. Document selection methodologies for efficient and effective learning-to-rank. SIGIR '09, pages 468­475, 2009. [2] P. Bailey, N. Craswell, and et. al. Relevance assessment: are judges exchangeable and does it matter. SIGIR '08, pages 667­674, 2008. [3] Z. Cao, T. Qin, and et. al. Learning to rank: from pairwise approach to listwise approach. ICML '07, pages 129­136. [4] Y. Freund, R. Iyer, and et. al. An efficient boosting algorithm for combining preferences. JMLR, 4:933­969, 2003. [5] X. Geng, T. Qin, and et. al. Selecting optimal training data for learning to rank. IPM, 47:730­ 741, 2011. [6] V. Jain and M. Varma. Learning to re-rank: query-dependent image re-ranking using click data. WWW '11, pages 277­286. [7] T. Joachims. Optimizing search engines using clickthrough data. KDD '02, pages 133­142, 2002. [8] E. Kanoulas, S. Savev, and et. al. A large-scale study of the effect of training set characteristics over learning-to-rank algorithms. SIGIR '11, pages 1243­1244, 2011. [9] G. Kazai, N. Craswell, and et. al. An analysis of systematic judging errors in information retrieval. CIKM '12, pages 105­114. [10] A. Kumar and M. Lease. Learning to rank from a noisy crowd.
SIGIR '11, pages 1221­1222, 2011. [11] T.-Y. Liu. Introduction. In Learning to rank for information
retrieval, chapter 1, pages 3­30. 2011. [12] C. Macdonald, R. Santos, and I. Ounis. The whens and hows of
learning to rank for web search. Information Retrieval, 16(5):584­628, 2013. [13] T. Qin, T.-Y. Liu, and et. al. Letor: A benchmark collection for research on learning to rank for information retrieval. Information Retrieval Journal, 13:346­374, 2010. [14] U. Rebbapragada and C. E. Brodley. Class noise mitigation through instance weighting. ECML '07, pages 708­715, 2007. [15] F. Scholer, A. Turpin, and M. Sanderson. Quantifying test collection quality based on the consistency of relevance judgements. SIGIR '11, pages 1063­1072, 2011. [16] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get another label? improving data quality and data mining using multiple, noisy labelers. KDD '08, pages 614­622, 2008. [17] W. C. J. V. Carvalho, J. Elsas. A meta-learning approach for robust rank learning. In Proceedings of SIGIR 2008 LR4IR. [18] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. SIGIR '98, pages 315­323. [19] J. Vuurens, A. P. De Vries, and C. Eickho. How much spam can you take? an analysis of crowdsourcing results to increase accuracy. In Proceedings of SIGIR 2011 Workshop on CIR. [20] J. L. S.-F. C. Wei Liu, Yugang Jiang. Noise resistant graph ranking for improved web image search. In CVPR, 2011. [21] J. Xu, C. Chen, and et. al. Improving quality of training data for learning to rank using click-through data. WSDM '10, pages 171­180, 2010. [22] J. Xu and H. Li. Adarank: a boosting algorithm for information retrieval. SIGIR '07, pages 391­398, 2007.

1194

Wikipedia-Based Query Performance Prediction

Gilad Katz1 katzgila@bgu.ac.il

Anna Shtok2

Oren Kurland2

annabel@tx.technion.ac.il kurland@ie.technion.ac.il

Bracha Shapira1 bshapira@bgu.ac.il

Lior Rokach1 liorrk@bgu.ac.il

1. Department of Information Systems Engineering, Ben-Gurion University, Beer-Sheva, Israel 2. Faculty of Industrial Engineering and Management, Technion, Haifa 32000, Israel

ABSTRACT
The query-performance prediction task is to estimate retrieval effectiveness with no relevance judgments. Pre-retrieval prediction methods operate prior to retrieval time. Hence, these predictors are often based on analyzing the query and the corpus upon which retrieval is performed. We propose a corpus-independent approach to pre-retrieval prediction which relies on information extracted from Wikipedia. Specifically, we present Wikipedia-based features that can attest to the effectiveness of retrieval performed in response to a query regardless of the corpus upon which search is performed. Empirical evaluation demonstrates the merits of our approach. As a case in point, integrating the Wikipediabased features with state-of-the-art pre-retrieval predictors that analyze the corpus yields prediction quality that is consistently better than that of using the latter alone.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
Keywords: query-performance prediction, Wikipedia
1. INTRODUCTION
There is a large body of work on query-performance prediction [5]. The goal is to estimate the effectiveness of ad hoc (query-based) retrieval with no relevance judgments.
Pre-retrieval prediction methods operate prior to retrieval time [12, 15, 16, 11, 22, 10]. Most of these methods analyze the query using information induced from the corpus upon which search is performed [8, 12, 11, 22]. Post-retrieval prediction methods analyze also the result list of the documents most highly ranked [5]. Hence, their prediction quality transcends that of pre-retrieval predictors. However, preretrieval predictors are much more efficient as they do not rely on performing the search. Accordingly, here we focus on pre-retrieval prediction.
We present a novel corpus-independent pre-retrieval queryperformance prediction approach. Our approach is based on using information induced from Wikipedia so as to estimate
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609553.

the absolute query difficulty. That is, the Wikipedia-based features that we present attest to the resultant retrieval effectiveness of using the query regardless of the corpus on which search is performed. Therefore, a clear advantage of our approach is for non-cooperative search settings where corpus-based information is not available [4].
Empirical evaluation performed with TREC datasets attests to the merits of our approach. First, when used alone, our approach yields decent prediction quality. Furthermore, we integrate the approach with previously proposed state-ofthe-art pre-retrieval predictors that analyze the corpus used for retrieval. The resultant prediction quality is consistently better than that of using these predictors alone.
2. RELATED WORK
As noted, in contrast to our approach that is corpusindependent, most pre-retrieval prediction methods analyze the corpus that serves for retrieval [8, 12, 16, 11, 22, 10].
There is very little work on corpus-independent queryperformance prediction [15, 11]. Query-syntactic features and WordNet-based features were used for prediction. In contrast to our work, Wikipedia-based information was not used and corpus-independent predictors were not integrated with corpus-dependent ones. Integrating these predictors [15, 11] with ours is a future venue we intend to explore.
There is some work on pre-retrieval query-performance prediction for the entity retrieval task which was performed over Wikipedia [18]. In contrast, we present predictors for the standard ad hoc retrieval task, and are the first, to the best of our knowledge, to use Wikipedia to this end. Furthermore, almost all of the features we use for prediction here were not used in this work on entity retrieval [18]
Finally, we note that most of the Wikipedia-based information sources that we use -- titles, content, links and categories -- were used to improve retrieval effectiveness (e.g., [1, 19, 3, 13]) rather than to predict effectiveness.
3. OUR APPROACH
Let q and D be a query and a document corpus, respectively. The task we pursue, a.k.a. pre-retrieval queryperformance prediction [5], is estimating, prior to retrieval time, the effectiveness of a search that will be performed by some retrieval method over D in response to q.
The most effective pre-retrieval predictors utilize information induced from the corpus D [11]. In contrast, we devise predictors that use information induced from the Wikipedia

1235

corpus. We use the predictors as features in a regression framework to estimate the Average Precision (AP) of retrieval performed over D. Thus, the approach we present is independent of D and is, in essence, designed to estimate the absolute difficulty of q.
Notational conventions. Herein, we refer to a subset of q's
terms as a sub-query. A Wikipedia page p is defined to be associated with a set of terms if its title contains at least one of the terms in the set. Aq denotes the set of pages associated with q (i.e., with its set of terms {qi}). A page p is exact match to a sub-query, if there is a 1-1 match between the sub-query's terms and the page title. The maximal exact match length, M EM L, is the size of the largest sub-query (i.e., the number of its terms) for which an exact match holds. The set of Wikipedia pages for which the maximal match holds is referred to as MMEML. The set of pages that are an exact match to the entire query q is denoted Eq.
We use S1, S2 and S3 to denote the sets of sub-queries that contain one, two and three terms, respectively. Predictors marked with * are computed separately for each of the sets. Predictors that are not marked with * are computed only for single query terms (i.e., sub-queries of size 1).
Computing some of our proposed features requires retrieval over Wikipedia. However, we note that Wikipedia is a very small corpus (4.5 million documents), specifically, in comparison to large-scale Web corpora.
The predictors we propose can be categorized into three groups, based on the information provided by Wikipedia that they utilize: titles and content, links and categories and previously proposed corpus-based predictors adapted for Wikipedia. We next describe each of these groups.
3.1 Titles and Content
The following predictors use information in the titles and content of Wikipedia pages to (mainly) measure aspects of the Wikipedia coverage of sub-queries of q.
· T N W P * is the sum of the sizes of sets of pages associated with a sub-query in Si; i  {1, 2, 3}. This predictor can be thought of as quantifying the "scope" of q in Wikipedia and is conceptually similar a previously proposed corpus-based predictor [12].
· N W Pavg is the average size of a set of pages associated with a sub-query in Si; i  {1, 2, 3}.
· N W Pstdev is the standard deviation of the sizes of sets of pages that are associated with a sub-query in Si; i  {1, 2, 3}.
· N EM * is the number of pages that constitute an exact match for a sub-query in Si; i  {1, 2, 3}.
· W EM is the maximum size, denoted |q|max ( {1, 2, 3}), of a sub-query for which there is an exact match page.
· P W EM is the average size of a sub-query which has an exact match.
· LP EMavg is the average length (measured by the number of terms) of the pages in Eq  MMEML; i.e., those that are exact match to q or a maximal exact match.
· SP Tavg is a conceptual reminiscent of a previously proposed post-retrieval predictor [20]. For each pair of query terms we determine the number of pages associated with both. We normalize this number with

respect to the number of pages associated with each of the two terms and average the two resultant values. The values obtained for all pairs of query terms are then averaged. Thus, the prediction value presumably attests to the coherence of query aspects represented by Wikipedia pages associated with the query terms.
3.2 Links and Categories
The most common pages in Wikipedia are entity pages, which refer to people, objects, etc. Entity pages are assigned with human-generated categories which constitute a rich source of structured information. In addition, almost every page in Wikipedia contains links to other related pages. We next present predictors that extract information from links and categories.
· LAC* is the overall number of links that contain at least one of q's terms in their anchor text.
· P DCavg & P DCstdev are the average and standard deviation, respectively, computed over sub-queries in Si, of the number of categories that appear in at least one of the pages associated with a sub-query; i  {1, 2, 3}.
· N OLavg & N OLstdev are the average and standard deviation, respectively, of the number of outgoing links for a Wikipedia page in MMEML.
· N ILavg & N ILstdev are the average and standard deviation, respectively, of the number of incoming links for a Wikipedia page in MMEML.
· ROILavg & ROILstdev are the average and standard deviation, respectively, of the ratio between the number of incoming and outgoing links for a page in MMEML. Pages with no outgoing links are ignored.
· LBGavg, LBGstdev & LBGmax. We count the overall number of links that point from pages associated with a sub-query in S1 (i.e., query term) to pages associated with another sub-query in S1. We aggregate these counts over all ordered pairs of sub-queries in S1 using the average, standard deviation, and maximum functions, respectively. The resultant predictors presumably quantify the coherence of query aspects represented by Wikipedia pages associated with the query terms.
· IGCavg & IGCstdev. For a set of pages associated with a sub-query in S1 (i.e., a query term) we compute the percentage of outgoing links which point from a page in the set to another page in the set. The average and standard deviation of the percentages over all the sets that correspond to the different sub-queries in S1 serve for prediction. This is yet another method that quantifies the presumed coherence of the query's aspects as manifested in the Wikipedia pages associated with the query terms.
3.3 Adaptations of previously proposed corpusbased pre-retrieval predictors
We next adapt previously proposed pre-retrieval predictors to predict q's performance over Wikipedia. We hypothesize that this predicted performance can attest to q's performance over any corpus.
· W SCS (Wikipedia Simplified Clarity Score). This is our Wikipedia-based variant of the Simplified Clarity

1236

Collection ROBUST
WT10G GOV2 ClueWeb

Data Disks 4&5-CR
WT10G GOV2 ClueWeb'09 (Category B)

# of Docs 528,155
1,692,096 25,205,179 50,220,423

Topics 301-450, 601-700 451-550 701-850
1-150

Table 1: TREC data used in Section 4.1.

Score [12]. The prediction value is the KL divergence between a unigram language model induced from q and that induced from the document that results from concatenating the pages in Aq, i.e., the pages associated with at least one of q's terms.
· SDQTavg & SDQTmax quantify the semantic distance between query terms. For each sub-query in S1, i.e., a query term, we construct a language model from the concatenation of all pages associated with it. We then compute the KL divergence between each pair of language models that correspond to sub-queries, and use for prediction the average and maximal values computed over pairs.
· P M Iavg & P M Imax are the Pairwise Mutual Information predictors. We compute the average and maximal PMI between two query terms as described in [10]. The documents set used for computing PMI is Aq.

4. EVALUATION
We turn to evaluate the prediction quality of the proposed methods. In Section 4.1 we evaluate the prediction quality for a standard language-model-based retrieval. Then, in Section 4.2 we study the effectiveness of our methods in predicting the performance of runs submitted to TREC.
4.1 Predicting the performance of languagemodel-based retrieval
Table 1 presents the details of the TREC datasets used for evaluation. ROBUST consists of (mainly) newswire articles, WT10G contains Web pages that can be of low quality (e.g. spam), and GOV2 is a crawl of the .GOV domain; ClueWeb ('09) is a large scale noisy Web collection, which contains also Wikipedia pages.
We used the TREC topics titles as queries. Krovetz stemming and stopwords removal, using the INQUERY list, were applied to both documents and queries. The experiments were conducted using the Indri toolkit1. The query likelihood method [17] serves for the retrieval method. Document d's retrieval score with respect to query q is: log qiq p(qi|d); p(qi|d) is the probability assigned to qi by a Dirichlet smoothed unigram language model induced from d with the smoothing parameter set to 1000 [21]. The same smoothing approach was applied to the language models used by our Wikipediabased predictors, except for the query language model which was a simple maximum likelihood estimate.
As we integrate the proposed predictors in a regression framework, we follow previous recommendations [10]2, and past practice [10, 7, 2], and measure prediction quality by the root mean squared error (RMSE) between the predicted
1www.lemurproject.org 2It was shown that measuring prediction quality using Pearson correlation between the predicted and the actual value of average precision might not result in reliable results when integrating predictors using regression.

Collection ROBUST WT10G
GOV2 ClueWeb

Wikipedia 0.191 0.188 0.188 0.162

Corpus 0.188w 0.186W 0.184W
0.163

Integrated 0.186w 0.175W c 00..117565W Cw c

Table 2: Prediction quality for language-modelbased retrieval of the Wikipedia-based and corpusbased predictors and their integration. `W' ('w') and `C' ('c') mark statistically significant differences with the Wikipedia-based and corpus-based predictors, respectively, with p  0.05 (p  0.1). Boldface marks the best result in a row.

average precision (AP@1000) and the ground truth AP@1000 determined by using TREC's relevance judgments. Statistically significant differences of prediction quality are determined using the paired t-test.
We compared the prediction quality of using three sets of predictors. The first is our (42) proposed Wikipedia-based corpus-independent predictors. We also use the query length as a predictor which results in 43 predictors.
The second set of predictors is composed of three families of state-of-the-art pre-retrieval corpus-based predictors [11] applied to the corpus at hand (D). These use the following statistics for a single query term: (i) inverse-document frequency (IDF) [8, 12, 11], (ii) variability of its tf-idf value in documents in the corpus in which it appears (VAR.TF.IDF) [22], and (iii) its similarity to the corpus (SQC) [22]. To aggregate the term-based statistics across the query terms, for each of the three families of predictors, we used the average, standard deviation and maximum functions. Overall, 9 predictors were created for this set.
The third set of predictors that we consider is a merge of the first two sets (i.e., our Wikipedia-based predictors and the previously proposed corpus-based predictors).
We integrate the predictors in each set using Lasso regression [9] so as to predict AP@1000. Leave-one-out cross validation is performed over the query set. The prediction quality numbers (RMSE) are presented in Table 2.
We see in Table 2 that the corpus-based predictors outperform, to a statistically significant degree, the Wikipediabased predictors for three out of four collections. This finding should come as no surprise as the Wikipedia-based predictors do not analyze the corpus upon which search is performed. For ClueWeb, the Wikipedia-based predictors outperform the corpus-based predictors, but not to a statistically significant extent. (This could be the result of ClueWeb containing Wikipedia pages.) More importantly, the integrated set of Wikipedia-based and corpus-based predictors results in improved prediction quality. Specifically, for the Web collections, the integration of the two sets yields substantial and statistically significant improvements of prediction quality over using either set alone.
Analysis of the relative effectiveness of the Wikipedia predictors reveals interesting insights. The most effective predictor was T N W P , which was used by the regressor for all four collections. The other leading predictors were LBG, P DC and P M I, each used for three out of the four collections. We believe that the fact that all aspects of Wikipedia: page titles, links, categories and page text, were used for prediction attests to the merits of using Wikipedia for queryperformance prediction.

1237

track TREC07 TREC08 TREC12 Clue09 Clue10 Clue11

run
median top5 median top5 median top5
median top5 median top5
median top5

Wikipedia
0.179 0.32 0.171 0.337 0.157 0.277
0.114 0.173 0.103 0.173
0.115 0.146

Corpus 0.165W
0.32 0.164 0.337 0.151
0.277
0.114 0.156W
0.099 0.182w 0.141W 0.152

Integrated
0.169 0.315
0.156 0.337
0.144 0.258W C 0.104W C 0.162W
0.1 0.173c 0.11W C 0.145

Table 3: Prediction quality for TREC runs. `W' ('w') and `C' ('c') mark statistically significant differences with the Wikipedia-based and corpus-based predictors, respectively, with p  0.05 (p  0.1). Boldface marks the best result in a row.
4.2 Predicting the performance of TREC runs
Insofar, we predicted the performance of language-modelbased retrieval. To evaluate prediction quality for a variety of retrieval methods, we use TREC runs. Specifically, we perform the evaluation with runs submitted to "ROBUSTbased" tracks (TREC7, TREC8, TREC12) and ClueWebbased tracks from 2009-2011 (denoted Clue09, Clue10 and Clue11), as these represent two different types of corpora, namely, newswire and Web.
We focus on two "pseudo" runs: (i) a median run, where the AP@1000 value per query is the median of the AP@1000 values of all runs for that query; b) a top5 run, where the AP@1000 per query is the average AP@1000 values for that query posted by the five best (MAP) performing runs in a track. Thus, the prediction quality for these two pseudo runs reflects the ability to predict the performance of median and highly effective retrieval methods.
To predict the performance of the runs in each track, we trained the Lasso regressor over the queries in the other two tracks that use the same collection; e.g., to predict the performance for TREC7, we trained over the TREC8 and TREC12 queries. For the training phase, we used the querylikelihood retrieval model as was the case in Section 4.1.
Table 3 presents the prediction quality numbers. Overall, the results are consistent with those presented for the language-model-based retrieval. For the ROBUST-based tracks, the integration of Wikipedia-based and corpus-based predictors can be of merit, although the improvements are rarely statistically significant. For the ClueWeb tracks, the integration often posts statistically significant improvements. Furthermore, in some cases for ClueWeb, the Wikipediabased predictors outperform the corpus-based predictors.

5. CONCLUSIONS AND FUTURE WORK
We presented a novel corpus-independent approach to preretrieval prediction of query performance. That is, the effectiveness of retrieval performed in response to a query is predicted without analyzing the corpus upon which search is performed. Our approach is based on quantifying properties of the query using Wikipedia. Empirical evaluation demonstrated the merits of our approach.
For future work, we plan to integrate our prediction methods with state-of-the-art post-retrieval predictors [14]. In addition, we intend to evaluate the quality of our predictors in a cross-corpus experimental setting as in [6].

6. ACKNOWLEDGMENTS
We thank the reviewers for their comments. This paper is based on work supported in part by the Israel Science Foundation under grant no. 433/12, by Google's and Yahoo!'s faculty research awards, and by an IBM Ph.D. fellowship.
7. REFERENCES
[1] J. Arguello, J. L. Elsas, J. Callan, and J. G. Carbonell. Document representation and query expansion models for blog recommendation. In Proceedings of ICWSM, 2008.
[2] N. Balasubramanian, G. Kumaran, and V. R. Carvalho. Predicting query performance on the web. In Proceedings of SIGIR, pages 785­786, 2010.
[3] K. Balog, M. Bron, and M. De Rijke. Category-based query modeling for entity search. In Advances in Information Retrieval, pages 319­331. Springer, 2010.
[4] J. Callan. Distributed information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 5, pages 127­150. Kluwer Academic Publishers, 2000.
[5] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2010.
[6] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In Proceedings of SIGIR, pages 390­397, 2006.
[7] K. Collins-Thompson and P. N. Bennett. Predicting query performance via classification. In Proceedings of ECIR, pages 140­152, 2010.
[8] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proceedings of SIGIR, pages 299­306, 2002.
[9] C. Hans. Bayesian lasso regression. Biometrika, 96(4):835­845, 2009.
[10] C. Hauff, L. Azzopardi, and D. Hiemstra. The combination and evaluation of query performance prediction methods. In Proceedings of ECIR, pages 301­312, 2009.
[11] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In Proceedings of CIKM, pages 1419­1420, 2008.
[12] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In Proceedings of SPIRE, pages 43­54, 2004.
[13] E. Hoque, G. Strong, O. Hoeber, and M. Gong. Conceptual query expansion and visual search results exploration for web image retrieval. In Advances in Intelligent Web Mastering­3, pages 73­82. Springer, 2011.
[14] O. Kurland, A. Shtok, S. Hummel, F. Raiber, D. Carmel, and O. Rom. Back to the roots: a probabilistic framework for query-performance prediction. In Proceedings of CIKM, pages 823­832, 2012.
[15] J. Mothe and L. Tanguy. Linguistic features to predict query difficulty. In ACM SIGIR 2005 Workshop on Predicting Query Difficulty - Methods and Applications, 2005.
[16] F. Scholer, H. E. Williams, and A. Turpin. Query association surrogates for web search. JASIST, 55(7):637­650, 2004.
[17] F. Song and W. B. Croft. A general language model for information retrieval (poster abstract). In Proceedings of SIGIR, pages 279­280, 1999.
[18] A.-M. Vercoustre, J. Pehcevski, and V. Naumovski. Topic difficulty prediction in entity ranking. In Proceedings of INEX, pages 280­291, 2009.
[19] Y. Xu, G. J. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on wikipedia. In Proceedings of SIGIR, pages 59­66, 2009.
[20] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In Proceedings of SIGIR, pages 512­519, 2005.
[21] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334­342, 2001.
[22] Y. Zhao, F. Scholer, and Y. Tsegay. Effective pre-retrieval query performance prediction using similarity and variability evidence. In Proceedings of ECIR, pages 52­64, 2008.

1238

Relevation!: An Open Source System for Information
Retrieval Relevance Assessment

Bevan Koopman
Australian e-Health Research Centre, CSIRO Brisbane, Australia
bevan.koopman@csiro.au

Guido Zuccon
Queensland University of Technology Brisbane, Australia
g.zuccon@qut.edu.au

ABSTRACT
Relevation! is a system for performing relevance judgements for information retrieval evaluation. Relevation! is web-based, fully configurable and expandable; it allows researchers to effectively collect assessments and additional qualitative data. The system is easily deployed allowing assessors to smoothly perform their relevance judging tasks, even remotely. Relevation! is available as an open source project at: http://ielab.github.io/relevation.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval] General Terms: Measurement, Experimentation.
1. INTRODUCTION
An integral part of information retrieval (IR) evaluation is the use of standard test collections. Relevance assessments are critical to the quality of test collections and obtaining assessments is often a long and laborious task, which, in many cases, involves a large number of documents to be judged by multiple assessors. Performing relevance assessments is often a one-off task for the creators of test collections and is usually under tight time and budget constraints. As a result, collection creators often have little resources to invest in a high quality tool to aid judges in performing relevance assessments, even though such a tool could greatly aid this process. With this in mind, we have developed Relevation!, an adaptable, open source tool for performing relevance judgements.
Relevation! allows users to upload documents and queries that are then browsed by judges through a web-based interface; for each document, judges can assign a relevance label. The system also allows judges to provide qualitative feedback about the judgement process, both at query level and at document level. Relevation! is open source and uses the Model-View-Controller design pattern, making it customisable to specific user requirements. The system is written in Python using the Django Web framework, making it easily deployed and remotely available. Relevation! has already
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. ACM ACM 978-1-4503-2257-7/14/07. http://dx.doi.org/10.1145/2600428.2611175 .

been used to collect additional relevance assessments for the TREC Medical Records Track [1] and for the CLEF eHealth (2013 & 2014) evaluation campaigns [2].
Early work by Hawking et al. [3] developed a tool for collecting relevance assessment. However, this tool is outdated, does not provide web-based deployments and is no longer available. RAT is a more recent web-based system for obtaining relevance assessments [4] but is specific to web search engines and assumes documents are screen scraped from the web. In addition, RAT is not open source so cannot be extended or adapted to specific needs.
2. FEATURES OF Relevation!
The architecture of Relevation! is shown in Fig 1. The system comprises four main modules: (i) the setup module, (ii) the queries module, (iii) the documents module, and (iv) the judgements module.
Setup Module. This module allows users to upload their queries and the document judging pool to Relevation!. Two files are required: 1) Query file in tab separated format QueryId [tab] QueryText, and 2) Document judging pool using the standard TREC results file format. Note that the standard setup module can be extended with additional parsers for query files and document judging pool; for example, the deployment of Relevation! for the CLEF eHealth 2013 was extended by implementing a query file parser which accepted additional query fields like "Description", "Narrative" and "Profile".
Queries Module. Judges are presented with a list of queries currently in the system (screenshot of Fig 2(a)). For each query, the number of documents assigned to that query is displayed, along with the number of unjudged documents, giving an indication of the overall judging progress. In the screenshot, each query has a QueryId column which identi-
Judges

Judging Pool
Queries

HTML Presentation Layer (Twitter Bootstrap)

Setup Module

Document Module

Query Module

Relevation!

Django

Judging Module

Docs

SQL DB
Figure 1: Architecture of Relevation!

1243

(a) The list of queries currently in the system (query module).
(b) List of docs. assigned to single query (documents module).
(c) Assessing a document (judgements module). Figure 2: Screenshots from the CLEF eHealth 2013 deployment of Relevation!. fies it and an associated Text column, which is the actual keywords for that query.
Documents Module. Clicking on the Text field entry takes the judge to the next page: the list of documents associated with that query (Fig 2(b)). For each document, the assessment Status column shows the relevance label assigned to the documents (e.g., highly relevant, somewhat relevant, etc.). The status field provides the judge with a quick and easy overview of the progress of this query, as well as the collection creators with an overview of the relevance assessments for that query (e.g., distribution of relevant/irrelevant documents). The Documents# field is the filename of the particular document and is a link to the judgements page.

Once all the documents for a query are judged, the assessors can optionally provide some qualitative feedback on the particular query. A short questionnaire is presented at the bottom of the list of documents. The questionnaire can be removed or customised to suit the particular needs of the relevance assessment task.
Judgements Module. This is where judges can read a document and enter their assessment (Fig 2(c)). The top of the page gives the query keywords and description. The document contents are displayed on the lefthand side.1 On the right hand side panel is a choice for the relevance assessment ("Not Judged" is the default judgement). Judges can also select portions of the document content and the selected text will automatically be added to the Supporting Evidence field. This information can be used for qualitative analysis after judging is complete or for passage based retrieval evaluation. Once assessment is complete, judges have to press the Save&Next button (not shown in screenshot), which saves the assessment and loads the next document for judging. The judgement module is customisable and different deployments of Relevation! may implement different judgements modules. For example the judgement module for CLEF eHealth 2013 also displayed narrative and profiles for the queries and used a different relevance assessment scale.
Other functionality. A script is provided for exporting relevance judgments in the standard TREC qrel format from the SQL database of Relevation! In addition, the SQL database can easily be queried to export other data, e.g., the qualitative questionnaires provided by the judges.
3. CONCLUSIONS AND FUTURE WORK
In its current version, Relevation! provides an open-source, modular, customisable system for collecting relevance assessments. In future versions of Relevation!, we plan to integrate the qrel creation tools and database querying methods within the core modules. The Setup module will be extended to allow users to configure custom relevance grades and qualitative questionnaires. Additional document parsers for the main TREC collections will also be added to this module. The Judging module will be extend to allow configurable placement of documents within the judgement interface, e.g., to support the visualisation of two documents at the time for preference judgements. Finally, we plan to incorporate a new Crowdsourcing module that allows Relevation! to outsource relevance assessment collection to workforce platforms such as Amazon Mechanical Turk. This will be available at http://ielab.github.io/relevation
References
[1] Bevan Koopman. Semantic Search as Inference: Applications in Health Informatics. PhD thesis, QUT, 2014.
[2] L. Goeuriot et al. ShARe/CLEF eHealth Evaluation Lab 2013, Task 3: Information retrieval to address patients' questions when reading clinical reports. In CLEF, 2013.
[3] D. Hawking, N. Craswell, P. Bailey, and K. Griffihs. Measuring search engine quality. Inf. Ret., 4(1):33­59, 2001.
[4] D. Lewandowski and S. Su¨nkler. Designing search engine retrieval effectiveness tests with RAT. Information Services and Use, 33(1):53­59, 2013.
1In the example screenshot the documents are web pages from CLEF eHealth 2013; however, documents could also be plain text documents: Relevation! can be customised by adding document parsers for other formats, e.g. TREC or WARC formats.

1244

VIRLab: A Web-based Virtual Lab for Learning and Studying Information Retrieval Models

Hui Fang, Hao Wu, Peilin Yang
University of Delaware Newark, DE, USA
{hfang, haow, franklyn}@udel.edu

ChengXiang Zhai
University of Illinois at Urbana-Champaign Urbana, IL, USA
czhai@illinois.edu

ABSTRACT
In this paper, we describe VIRLab, a novel web-based virtual laboratory for Information Retrieval (IR). Unlike existing command line based IR toolkits, the VIRLab system provides a more interactive tool that enables easy implementation of retrieval functions with only a few lines of codes, simplified evaluation process over multiple data sets and parameter settings and straightforward result analysis interface through operational search engines and pair-wise comparisons. These features make VIRLab a unique and novel tool that can help teaching IR models, improving the productivity for doing IR model research, as well as promoting controlled experimental study of IR models.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation
Keywords: virtual lab; IR models; teaching
1. INTRODUCTION
Information Retrieval (IR) models determine how to compute the relevance score of a document for a given query, thus directly affecting the search accuracy. Developing optimal IR models has been one of the most important research problems in information retrieval. Over the past decades, many retrieval models have been proposed and studied. However, there is no single winner, and we end up having a few different retrieval models that all seem to perform equally well [2]. Moreover, it has proven very hard to further improve these state of the art retrieval models [1]. Unfortunately, experimenting with any new retrieval model is inevitably time consuming and requires significant amount of resources available since the new model needs to be empirically evaluated and validated over as many data sets as possible. Thus, it would be necessary to develop a tool that facilitates the development and study of IR models.
IR toolkits such as Lemur, Terrier and Lucene have been developed to enable IR research and successful technology
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. ACM 978-1-4503-2257-7/14/07. http://dx.doi.org/10.1145/2600428.2611178.

transfer of IR models to industry and various applications. They are often designed as off-the-shelf systems that allow users to take advantage of the implemented retrieval functions to their own applications. In particular, users can use command lines to build indexes, retrieve documents based on a specified retrieval function, and evaluate the retrieval results.
Unfortunately, existing IR toolkits do not offer an easy solution for users to implement, evaluate and analyze new retrieval models, which often pose unnecessary extra burdens to the users. For example, users need to learn how to access various term or document statistic information from the index, and read through source codes or API documentations to figure out how to implement a new retrieval function. After implementing the function, the users often need to write their own scripts to evaluate and analyze the search results over multiple collections. This kind of process adds unnecessary burden to the users and could discourage them from experimenting with more functions over more collections. Naturally, it is also very challenging to use such toolkits for a course assignment.
This paper describes our efforts on developing a web-based tool for IR students or researchers to study retrieval functions in a more interactive and cost-effective way. We will demonstrate that the developed system, i.e,. Virtual IR Lab (VIRLab), can offer the following new functionalities:
· Easy implementation of retrieval functions: Users only need to write a few lines of code through a Web form to combine statistics retrieved from the indexes without worrying about how to access the indexes. The code will be automatically checked for syntax errors and translated to an executable, which will be used for ranking documents, by a dynamic code generator.
· Flexible configuration of search engines: Users can configure a search engine by selecting a retrieval function and a test collection. Multiple search engines can be easily created at the same time. The users can either submit their own queries or select queries from a set of topics associated with the corresponding document collection. Moreover, the users can also compare the search results of two search engines side by side to figure out their ranking differences.
· Tight connections among implementation, evaluation and result analysis: After creating a retrieval function, the users can evaluate its effectiveness over a few provided test collections by simply clicking a button. If a retrieval function contains multiple parameter values, the users may select to evaluate all of them. If a

1249

Name the function

Implement the retrieval function by combining the
provided features

Display the selected function
Click here to generate evaluation results on the
selected collection
Display the evaluation results for TREC8

Side-by-side result comparison
Evaluation comparison for each query

Provide a list of available features
Screenshot for creating a function

Screenshot for evaluating a function

Screenshots for comparing two functions

Figure 1: Screenshots of function creation (left), function evaluation (center) and function comparison (right)

search engine is configured using an existing test collection with relevance judgments, the official queries and judgments will be displayed so that the users can easily analyze the search results to figure out when the search engine fails and why.
· Performance comparison through leader-boards: A leaderboard is created for each collection so that the most effective 10 retrieval functions are displayed. Users can see how their retrieval functions are compared with others, and they can also leverage the comparison functionality described earlier to figure out how to revise their retrieval functions to improve the performance.
Empowered by these new functionalities, the VIRLab system is a novel IR tool that can (1) help teaching IR models to students with limited programming experience; (2) improve the productivity for doing research on IR models; and (3) promote controlled experimental study of IR models by establishing baselines on various data collections.
Our prototype system is available at http://infolab.ece.udel.edu:8008. Please contact the authors to obtain the login information.
2. SYSTEM OVERVIEW
Figure 1 shows the screenshots of three major functionalities including creating a retrieval function, evaluating the function and comparing the results of two functions. We now provide more details about these functionalities.
The front end of the system is a Web interface that allows users to create retrieval functions. Specifically, a user can implement a retrieval function by simply combining multiple features (i.e., collection statistics) from a provided list based on C/C++ syntax. As an example, the left part of Figure 1 shows how the Dirichlet prior retrieval function [2] is implemented. Moreover, instead of specifying a single parameter value, the users can also specify a set of values for retrieval parameters, and then the system will automatically create a group of functions with these parameter settings. Once a retrieval function has been created, the user can select test collections and evaluate the effectiveness of the retrieval function over the collections (as shown in the middle part of Figure 1).
The front end also enables users to use or evaluate the retrieval function through a Web-based search interface. The user first needs to create a search engine by selecting a re-

trieval function and a document collection. After that, the user can either enter his or her own query or select a query from existing test collections when queries are available. If the query is from the test collections, we will display not only search results but also the relevance judgment of these results as well as the evaluation results for the query. This feature would allow users to easily see when their search engines fail or succeed and encourage them to identify the problems and try to fix them by changing the retrieval function. Moreover, we also empower users to compare the search results of two search engines side by side so that they could analyze them and identify how to revise one of the search engines accordingly. The right part of Figure 1 shows the screenshot of this functionality.
To promote controlled experimental study of IR, we generate a leader-board to report the best performed retrieval functions for each collection. This functionality is similar to the evaluatIR system 1. One key difference is that VIRLab enables users to conduct more result analysis such as sideby-side comparison between the results of the best system with those of their own retrieval functions.
The back end of the system includes several basic components such as indexer, ranker and evaluation script. The indexing process is done offline. Several standard TREC ad hoc collections have been indexed and ready for users to choose from. The ranker is determined by the retrieval function that the user provided through the front end.
3. DEMO PLAN AND FUTURE WORK
We expect to demonstrate all these functions of VIRLab, particularly, how one can easily (1) modify/implement a retrieval function and immediately evaluate its performance on the fly; (2) create an operational search engine with the implemented function with one click; and (3) compare the search results of two retrieval functions side by side to analyze their relative weaknesses and strengths.
We plan to extend the developed system by enabling flexible implementations of other system components.
4. REFERENCES
[1] T. G. Armstrong, A. Moffat, W. Webber, and J. Zobel. Improvements that don't add up: ad-hoc retrieval results since 1998. In Proceedings of CIKM'09, 2009.
[2] H. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. In Proceedings of SIGIR'04, 2004.
1http://wice.csse.unimelb.edu.au:15000/evalweb/ireval/

1250

FitYou: Integrating Health Profiles to Real-Time Contextual Suggestion

Christopher Wing
Georgetown University Washington, DC, USA
cpw26@georgetown.edu
ABSTRACT
Obesity and its associated health consequences such as high blood pressure and cardiac disease affect a significant proportion of the world's population. At the same time, the popularity of location-based services (LBS) and recommender systems is continually increasing with improvements in mobile technology. We observe that the health domain lacks a suggestion system that focuses on healthy lifestyle choices. We introduce the mobile application FitYou, which dynamically generates recommendations according to the user's current location and health condition as a real-time LBS. It utilizes preferences determined from user history and health information from a biometric profile. The system was developed upon a top performing contextual suggestion system in both TREC 2012 and 2013 Contextual Suggestion Tracks.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords
Contextual Suggestion; Location based service; Health IR
1. INTRODUCTION
A 2010 U.S. National survey found that more than onethird of U.S. adults were obese.1 Obesity greatly increases the risk of diabetes, heart disease, and strokes. In addition, the National Institutes of Health estimates obesity will cause approximately 500,000 additional cases of cancer by 2030 given the current obesity trends.2 There has never been a more important time for people to incorporate healthier dining options and some form of physical activity.
The Text REtrieval Conference (TREC) 2012-2013 Contextual Suggestion Tracks have identified technologies to retrieve and suggest venues to visit at a user's current location according to the user's rated preferences of past venues, current location's time, season, traffic, and temperature [2]. More general location-based recommender systems primarily use population interests, user interests, and friends' interests but often fail to address a health component. In FitYou, we add a health dimension on top of our TREC
1 http://www.cdc.gov/nchs/data/databriefs/db82.pdf 2 http://www.cancer.gov/cancertopics/factsheet/Risk/obesity
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. ACM 978-1-4503-2257-7/14/07. http://dx.doi.org/10.1145/2600428.2611185 .

Hui Yang
Georgetown University Washington, DC, USA
huiyang@cs.georgetown.edu
Figure 1: The FitYou System system and demonstrate the effectiveness of our system on the Foursquare platform as a real-time LBS.
Some health domain LBS can find health care providers [1], but no system has yet accounted for the need to help people live healthier lifestyles. We propose that healthy, personalized contextual suggestions can be suggested by considering their medical history and future health goals. By integrating users' health needs and their preferences, suggestions can be further personalized to help users live healthier and happier. Although users can consult with a nutritionist or physician, a specialist may not always be readily available. Thus, it is important to have technology that can generate personalized suggestions whenever necessary.
We separately suggest venues for dining and performing physical activity that burns at least 150 cal/hour. We selected 13 cuisine types3 to make dining suggestions and prepared a list of 45 activities4 with corresponding number of calories burned per hour as estimated for a 155 lb person.5
2. APPROACH
For testing purposes, we utilize interest profiles provided by TREC 2013 Contextual Suggestion Track and combine them with randomly sampled health profiles. First, we mapped each example venue to one of our categories. Past venues
3Italian, Indian, Japanese, Chinese, American, Korean, French, Ethiopian, Vegetarian, Vegan, Seafood, Salad, Greek 4golf, walk, kayaking, softball, baseball, swimming, tennis, running, bicycling, football, basketball, soccer, outdoors & recreation, archery, badminton, ballet, ballroom dancing, bird watching, bowling, boxing, canoeing, rowing, cricket, croquet, skiing, diving, fencing, fishing, lacrosse, paddleball, polo, racquetball, skateboarding, rollerblading, table tennis, yoga, hiking, rock climbing, mountain climbing, snorkeling, ice skating, painting, billiards, shopping, museum 5 www.nutristrategy.com/caloriesburned.htm

1263

are rated by users on a five-level scale Interest score: -0.9 for strongly disinterested, -0.3 for disinterested, 0 for neutral, 0.3 for interested, and 0.9 for strongly interested.
Similar to [5], we employ state-of-the-art matrix factorization approach. We operate Singular Value Decomposition (SVD) over a user-category matrix SM×N . Each entry Si,j is estimated by: S^i,j = cTj ui where cj presents category j and ui presents user i. These vectors are estimated given the entries in SM×N . The value of Si,j in the matrix is determined by the user's Interest as mentioned above. We calculate a user's average interest score across all categories x¯ui and all users' average interest score for a category y¯cj :

interest(ui, cj ) x¯ui = cj cat |cat|

interest(ui, cj )

y¯cj

=

ui users
|users|

One of the key factors in our success in TREC Contextual Suggestion evaluations [4] is our focus on satisfying users' major interests. We classify a category as a major interest if a user's score for the category is greater than the average of his score over all categories and if his score for the category is greater than the mean of all users for this category, that is, if Pinterest(ui|cj ) > x¯ui and Pinterest(ui|cj ) > y¯cj .

2.1 Integrating Health Profile

The user health profile contains: age, gender, height, weight,

neck, forearm, waist, hip, wrist, prevailing health conditions,

and exercise preference (light, medium, or intense). In pro-

duction, users will provide and update their health profile as

needed. In order to experiment using the TREC dataset, we

had to randomly sample health profiles. We assume health

profiles and Interest are independent.

We next calculate biometrics using the health profile. Body

mass index (BMI)

is

703×w(lb) h(in)2

or

w(kg) h(m)2

.

Body

fat

percentage

(BFP)

is

100×(w-(1.082×w+94.42)-4.15×waist)) w

for

male

and

100

×

(w

-

(.732

×

w

+

8.987

+

wrist 3.14

-

.157

×

waist

-

.249

×

hip+.434×f orearm)/w for female. Lastly, we provide a sug-

gested weight using the J. D. Robinson formula: 52kg+1.9kg

per inch over 5 feet for male and 49kg + 1.7kg per inch over

5 feet for female [3]. w is weight and h is height; other mea-

sures are circumferences of the body parts. Users can accept

the suggested weight or manually set a target weight.

2.2 Activity Suggestion
Although calorie burning varies with body weight, the change is proportional for all activities. Considering the TREC dataset, we added a few activities which are not often associated with calorie burning such as shopping, museum, and outdoors and recreation, and estimated the calorie burning for venues of these types to be half that of walking.
When suggesting activity venues, we consider user interest, variety, and exercise intensity. Given the user's current location, we issue separate queries for each activity type and collect the first fifty results. We determine each activity type's score (ATS) by combining health and interest:

CaloriesBurnedP erHour

AT S = 

+ (1 - )Interest (1)

1000

where we empirically set  = 0.4 and Interest was the interest score. If the user has a health condition such as high blood pressure or cardiac disease, greater bias is given to burning calories and  = 0.6. Calorie content is divided by 1000 so that it is similar in magnitude to Interest.

All activity types were sorted by their ATS. We first returned one venue corresponding to each major interest to increase the likelihood the user will find the first recommendations valuable. Next, we considered both major and non-major interests. One venue from the highest scoring activity was returned. After a venue of a given activity type was returned, the ATS score was discounted by 25% and the activity types are re-sorted. This ensures adequate variety in the recommendations. This process was continued until 50 recommendations were determined.

2.3 Dining Suggestion
We observe macronutrient information such as protein, fat, and carb content is not available for many restaurants; thus dining recommendations are optimized by considering calorie content. We estimated the typical calories in a meal for each cuisine type by randomly selecting several restaurants of each cuisine type that had caloric information available. We randomly selected entrees from each restaurant and computed the geometric mean for each cuisine type.
When recommending dining venues, we consider user interest, cuisine type variety, health conditions, and whether the user is trying to lose or gain weight. We determine each cuisine type's score (CTS), which differs from ATS by introducing  = -1 to penalize calories if the user needs to lose weight, else  = 1. The dining suggestion process follows the same logic as for activity suggestions.

CalP erM eal

CTS = 

+ (1 - )Interest (2)

1000

3. USER EXPERIENCE AND CONCLUSION
As obesity rates and their associated health concerns have prodigious effects upon a significant proportion of the world's population, we determine a new type of contextual suggestion system should exist to help users live healthier lifestyles. FitYou, developed upon our system in the 2012 and 2013 TREC Contextual Suggestion Tracks, integrates health profile and preference history to generate personalized suggestions according to the user's current location and its context.
Users have received high quality suggestions at different contexts ranging from Washington, DC to Vernon, CT. Based on initial user testing, we are confident that FitYou can supplement a physician's advice to help health-conscious users improve their lives by suggesting healthy recommendations that they enjoy. For future work, we would like to implement further personalized dining recommendations as restaurant data becomes increasingly available.

4. REFERENCES
[1] M. N. K. Boulos. Location-based health information services: a new paradigm in personalised information delivery. Int J Health Geogr. 2003.
[2] A. Dean-Hall, C. Clarke, J. Kamps, P. Thomas, N. Simone, and E. Voorhees. Overview of the trec 2013 contextual suggestion track. In TREC '13.
[3] R. JD, L. SM, P. L, L. M, and A. M. Determination of ideal body weight for drug dosage calculations. Am J Hosp Pharm, 1983.
[4] J. Luo and H. Yang. Boosting venue page rankings for contextual retrieval. In TREC '13.
[5] J. Wang and Y. Zhang. Utilizing marginal net utility for recommendation in e-commerce. In SIGIR '11.

1264

Query-Performance Prediction: Setting the Expectations Straight

Fiana Raiber fiana@tx.technion.ac.il

Oren Kurland kurland@ie.technion.ac.il

Faculty of Industrial Engineering and Management, Technion -- Israel Institute of Technology

ABSTRACT
The query-performance prediction task has been described as estimating retrieval effectiveness in the absence of relevance judgments. The expectations throughout the years were that improved prediction techniques would translate to improved retrieval approaches. However, this has not yet happened. Herein we provide an in-depth analysis of why this is the case. To this end, we formalize the prediction task in the most general probabilistic terms. Using this formalism we draw novel connections between tasks -- and methods used to address these tasks -- in federated search, fusion-based retrieval, and query-performance prediction. Furthermore, using formal arguments we show that the ability to estimate the probability of effective retrieval with no relevance judgments (i.e., to predict performance) implies knowledge of how to perform effective retrieval. We also explain why the expectation that using previously proposed query-performance predictors would help to improve retrieval effectiveness was not realized. This is due to a misalignment with the actual goal for which these predictors were devised: ranking queries based on the presumed effectiveness of using them for retrieval over a corpus with a specific retrieval method. Focusing on this specific prediction task, namely query ranking by presumed effectiveness, we present a novel learning-to-rank-based approach that uses Markov Random Fields. The resultant prediction quality substantially transcends that of state-of-the-art predictors.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
Keywords: query-performance prediction, learning-to-rank
1. INTRODUCTION
The query-performance prediction task has attracted much research attention [11]. The goal of the task has been stated as estimating the effectiveness of retrieval performed in response to a query with no relevance judgments [11]. An important claim put forth for motivating the development
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609581.

of query-performance predictors was that using them would help to improve retrieval effectiveness.
However, the expectation that improved prediction methods would translate to improved retrieval approaches has not been realized. One of our goals here is to explore why this is the case. To this end, we formalize the prediction challenge in the most general probabilistic terms. Using this formalism we draw novel connections between tasks and methods used to address them in federated search [10], fusion-based retrieval [14] and query-performance prediction [11].
We formally show that the ability to successfully estimate the probability of effective retrieval with no relevance judgments (i.e., to predict performance) directly implies knowledge of how to perform effective retrieval. Thus, at the conceptual level, it should come as no surprise that improved prediction methods do not lead to improved retrieval methods. On the practical level, we make the observation that previously proposed query-performance prediction methods -- specifically, those analyzing the retrieved list -- were designed as approaches to ranking queries based on presumed retrieval effectiveness rather than as direct estimates for the probability of effective retrieval. Nevertheless, we argue for why even in their capacity as query ranking techniques the most effective among the proposed predictors were not, and cannot, be used to improve retrieval effectiveness. The core issue is that these predictors are by design effective only in ranking queries for specific retrieval methods.
Given that previously proposed query-performance predictors are essentially query ranking methods, we present a learning-to-rank approach based on Markov Random Fields for query-performance prediction. The resultant prediction quality substantially transcends that of state-of-the-art predictors. We then use our approach in a fusion-based retrieval setting to further support our claim that the expectation that using query-performance predictors would help to improve retrieval effectiveness is somewhat unrealistic. These predictors should be examined and evaluated with respect to the task they were designed for: estimating for which queries a retrieval performed over a given corpus with a given retrieval method would result in better performance.
2. RELATED WORK
There are three main lines of work on query-performance prediction. The first is devising pre-retrieval prediction methods that are based only on the query and corpus-based statistics [15, 25, 42, 37, 23, 53, 41]. The second is that on post-retrieval prediction approaches that analyze also the retrieved list [11]. Specifically, some methods measure the

13

clarity of the list with respect to the corpus [15, 16, 2, 24]. Others quantify different notions of the robustness of the list [52, 51, 54, 4, 40]. There are post-retrieval predictors that rely on analysis of the retrieval scores distribution in the list [50, 9, 19, 55, 6, 38, 17, 18, 46].
The third line of work is on prediction frameworks. One framework is based on measuring distances between query representations, relevant documents, and the corpus [12]. In another framework the prediction challenge was casted as a utility estimation task [45]. A probabilistic framework was proposed to explain the common grounds of various (preand post- retrieval) prediction methods [29]. The basic probability used in this work to derive the framework emerges as a specific case of the formalism we present here. In a similar vein, various post-retrieval predictors were presented as instantiations of a simple prediction principle [28]; namely, comparing the retrieved list with a pseudo (in)effective list.
In contrast to all this previous work, we formally show that the query-performance prediction task which has been addressed is a specific instance of a much more general prediction challenge. Accordingly, we establish novel connections between various prediction tasks. We also study the reasons to why improved query-performance prediction has not translated to improved novel retrieval methods.
It was argued using a simulation [21] that the state-ofthe-art prediction quality should much improve to allow for effective selective query expansion [2, 16]. We provide here reasons to the potential difficulty in substantially improving prediction quality by connecting the ability to predict performance with the knowledge to perform effective retrieval.
Our learning-to-rank framework for query-performance prediction integrates previously proposed pre- and post- retrieval predictors. Pre-retrieval predictors were integrated using linear regression [22]. In contrast to our work, postretrieval predictors which (as we show) are more effective than pre-retrieval predictors were not integrated. More generally, the prediction task was not treated as a ranking problem. Post- and pre- retrieval predictors were also integrated in a probabilistic framework [29]. In contrast to our learningto-rank framework, only two post-retrieval predictors and one pre-retrieval predictor could be integrated; and, the integration was non parametric. Hence, the relative prediction quality of the predictors, as determined using a train set of queries, could not be utilized. Linear interpolation of only two post-retrieval predictors was also proposed [51, 55, 45, 28], but without applying a learning-to-rank approach.

3. THE PREDICTION TASK
The query performance prediction task was stated as "estimating the effectiveness of a search performed in response to a query in the absence of relevance judgements" [15, 11]. To formalize the task in the most general terms, we use the following notation. Q, C and M are random variables that take as values queries, document corpora, and retrieval methods, respectively. A retrieval method gets as input a query and a corpus of documents and produces a ranking over the corpus. The random variable R is assigned with a value 1 if the retrieval was effective and 0 otherwise. Accordingly, predicting retrieval effectiveness amounts to estimating

p(R = 1|Q, C, M ).

(1)

Different assignments to the random variables in Equation 1 result in different prediction tasks that we discuss below.

These tasks were not necessarily referred to in past work as prediction, nor derived from the same basic formalism. Table 1 summarizes these tasks.

3.1 Prediction over corpora
In the federated retrieval setting [10, 44], a set of corpora, {ci}, can be used for retrieval. The resource selection problem is to select a subset of these corpora to perform the retrieval on [10, 44]. This task can be performed by addressing a prediction challenge over corpora; specifically, estimating for each corpus c the probability that using a given query q in some retrieval method upon this corpus will result in effective retrieval. Formally, we fix Q = q, and for each c ( {ci}) estimate p(R = 1|Q = q, C = c, M ), for any assignment of M . We re-write this probability as

pq(R = 1|C = c),

(2)

to emphasize the fact that the query is fixed and that the estimation is performed across corpora and independently of a retrieval method.
In practice, the probability in Equation 2 is not estimated directly. Rather, the corpora are ranked in response to the query using some form of corpora representation [10, 44].

3.2 Prediction over retrieved lists
The fusion task is merging lists that were retrieved from the same corpus in response to a query [14]. The lists differ due to the retrieval methods employed to produce them; e.g., the document representation, the query representation, and the ranking function can be varied [14].
A long standing challenge in work on fusion is estimating the effectiveness of the lists to be fused in lack of relevance judgements [3]. Such estimates could be used, for example, to weigh the lists when linearly fusing them [3, 31, 43]; or, to select a single list as the final result list [2, 16, 34, 5].
Thus, estimating list effectiveness in the fusion setting is a prediction task performed over retrieval methods, and consequently, over retrieved lists. That is, the choice of a retrieval method for a given pair of a query and a corpus entails a ranking. Formally, we fix Q = q and C = c as retrieval is performed over a single corpus. Then, for each retrieval method m we follow Equation 1 and estimate p(R = 1|Q = q, C = c, M = m). Doing that amounts to estimating p(R = 1|Q = q, C = c, L = l) for each retrieved document list l, where L is a random variable that takes as values such lists. We write this probability as

pq;c(R = 1|L = l),

(3)

to emphasize the fact that both the query and the corpus are fixed, and the estimation is across retrieved lists.
In previous work on fusion, the probability in Equation 3 was not directly estimated. Rather, the estimation task was often replaced with a "ranking retrieved lists with respect to a query" task for a fixed query and corpus. List ranking was performed based on past performance of the retrieval methods (e.g., [3, 31]), features assumed to be correlated with list effectiveness [34, 5, 43], or the similarity between the list and the "average list" among those available [47, 19].

3.3 Prediction over queries
Heretofore, we used the term "prediction" to refer to the tasks described above as they are manifestations of the challenge of estimating Equation 1 with no relevance judgments.

14

prediction task

target probability

fixed

fundamental

p(R = 1|Q, C, M )

none

over corpora

pq (R = 1|C = c)

Q

over lists

pq;c(R = 1|L = l)

Q, C

(pre-) over queries pc(R = 1|Q = q)

C

(post-) over queries pc(R = 1|Q = q, L = l) C

varied no effect

none none

C

L

L

none

Q

L

Q, L none

Table 1: Summary of the prediction tasks. "target probability": the probability to be estimated using no relevance judgments; "fixed", "varied", "no effect": the random variables whose values are (i) fixed, (ii) varied (and consequently, over which prediction is performed), and (iii) has no effect on the estimation, respectively. A triplet of a query (Q), corpus (C) and retrieval method (M ) entails a retrieved list (L). "pre-" and "post-" stand for preretrieval and post-retrieval, respectively.

However, these tasks were not referred to as "prediction" in past literature. In fact, almost all methods that were proposed in past work on query-performance prediction address (either explicitly or implicitly) the challenge of prediction over queries. These methods can be categorized into the two classes that we discuss next [11].

Pre-retrieval prediction. The first class of prediction over
queries approaches is that of pre-retrieval prediction. Given a query and a fixed corpus of documents, retrieval effectiveness is estimated before retrieval is employed and independently of a retrieval method [15, 25, 42, 37, 24, 53, 41]. Then, the estimates are compared across queries to predict which queries will result in more effective retrieval. Formally, C is fixed to c, and following Equation 1 the task becomes estimating p(R = 1|Q = q, C = c, M ). To emphasize the fact that the corpus is fixed, and that the estimation is across queries and independent of a retrieval method (and therefore of a retrieved list), we write the probability as:

pc(R = 1|Q = q).

(4)

As was the case for the prediction tasks described in Sections 3.1 and 3.2, Equation 4 was not directly estimated in past work [15, 25, 42, 37, 24, 53, 41].1 Rather, methods for ranking queries with respect to a corpus have been devised.
We note the interesting connection between Equations 2 and 4. Both are specific instantiations of Equation 1. In the prediction over corpora case (Equation 2), the query is fixed and the corpora are varied. In the pre-retrieval prediction over queries case (Equation 4), the corpus is fixed and the queries are varied. In both cases, the estimation (prediction) is for any retrieval method. Perhaps not surprisingly, then, effective methods for ranking corpora and queries (in a pre-retrieval mode) bear much resemblance. For example, the CORI resource selection approach for federated search [10] ranks corpora with respect to the query using a tf.idf-based similarity measure. The SCQ pre-retrieval over queries predictor scores queries with respect to a corpus also using a tf.idf-based similarity measure [53]. This connection between the formal tasks of resource selection and pre-retrieval prediction over queries, and methods used to address them, is novel to this study.

1Pre-retrieval predictors were used in a linear regression to predict average precision [22], but the probability of effective retrieval was not directly estimated.

Post-retrieval prediction. Methods referred to as post-
retrieval query-performance predictors estimate retrieval effectiveness based on the query, the corpus, and the retrieved list [15, 2, 16, 50, 9, 52, 12, 51, 54, 4, 19, 55, 24, 6, 45, 17, 18, 46]. The goal is to compare estimates across queries as is the case for pre-retrieval prediction that was discussed above. We can formalize the task as follows. We fix a corpus, C = c. Then, following Equation 1 we estimate for each pair of a query (q) and a retrieval method (m) p(R = 1|Q = q, C = c, M = m). Given a corpus, a pair of a query and a retrieval method entails a retrieved list, l. Furthermore, most work on post-retrieval prediction has focused on analyzing the retrieved list, rather than the retrieval method itself.2 Thus, the probability of interest is:

pc(R = 1|Q = q, L = l).

(5)

Equation 5 served as the basis for deriving a probabilistic framework that was used to explain the commonalities between prediction over queries methods [29]. Virtually all previously proposed post-retrieval predictors do not directly estimate Equation 5. Instead, scoring methods are devised for pairs of a query and a retrieved list. The scores are used to rank the queries by presumed retrieval effectiveness.
We also note the fundamental difference between Equations 5 and 3. In post-retrieval prediction over queries (Equation 5) estimation is performed across pairs of query and retrieved list. In the fusion setting (Equation 3), the query is fixed and estimation is performed across retrieved lists. We come back to this point below.

3.4 Improved prediction  improved retrieval?
Work on query-performance prediction -- i.e., prediction over queries -- based the motivation for devising predictors on the claim that improved prediction would help to improve retrieval effectiveness [11, Chapter 1]. However, there was very little empirical support to this claim.3 We now explore the gap between the claim and the lack of its realization.
Consider Equation 1 that was used to derive the prediction tasks. Suppose that we fix the query, Q = q, and that the corpus contains a single document d which is the only one that can be retrieved by any retrieval method. Accordingly, the prediction task becomes estimating p(R = 1|Q = q, D = d); D is a random variable that takes documents as values. That is, the prediction task amounts to the core estimation challenge in probabilistic retrieval: what is the probability that this document is relevant to this query? [49]. Thus, we arrive to the following argument:

The ability to successfully estimate the probability of effective retrieval with no relevance judgments (i.e., predict performance) directly implies knowledge of how to perform effective retrieval.

2There is work on predicting retrieval effectiveness based on statistics of features used by the retrieval method [5, 6]. 3Small performance improvements were attained when using (i) post-retrieval predictors in federated search [52] and (ii) pre-retrieval predictors in a learning-to-rank framework [35]. Post-retrieval predictors were also integrated with many (ranker specific) features to select a ranking function [5] (with somewhat inconclusive findings about the resultant effectiveness) and for fusion [43] (with small relative contribution to overall performance). Selective query expansion using predictors was not shown to be of much merit [2, 16].

15

A potential criticism about the argument just posed is that prediction methods are presumably more successful in estimating retrieval effectiveness for a retrieved document set than for a single document, because they exploit properties of such sets. To address this criticism, we can set the corpus in Equation 1 to contain only a cluster of similar documents. Work on ranking document clusters in response to a query [33, 39] showed that effective estimates for cluster relevance translate to improved document ranking. Thus, we get further support to the argument that the ability to predict the effectiveness of a retrieved document set implies knowledge of performing effective document retrieval.4
Another potential attack on the argument is that the inability to accurately estimate the probability of effective retrieval does not necessarily imply inability to effectively rank the items of concern. Indeed, in Sections 3.1, 3.2 and 3.3 we made the observations that in most cases the probability for effective retrieval was not estimated directly, but rather ranking approaches have been employed.
The situation just described is reminiscent of that in ad hoc retrieval. In probabilistic retrieval methods [49], the probability of document relevance is rarely directly estimated. Instead, quantities which are presumably rank equivalent to this probability are used to rank documents.
Hence, we re-visit the above mentioned motivation for devising prediction over queries methods from a ranking point of view. A recurring example for using these predictors to improve retrieval effectiveness -- which was not backed up empirically -- was switching a retrieval method if retrieval was predicted to fail [11]. Presumably, there is a task mismatch: post-retrieval over queries predictors rank pairs of a query and a retrieved list, while the suggested task is ranking retrieved lists for a given query (prediction over retrieved lists).5 However, if we fix the query in post-retrieval over queries prediction methods we should presumably get retrieved-lists ranking methods. More generally, fixing the query in Equation 5 results in the post-retrieval over queries prediction task becoming the prediction over retrieved lists task (Equation 3).
Yet, as it turns out, the most effective previously proposed post-retrieval over queries predictors are (either explicitly or implicitly) coupled with, and therefore effective for, a specific retrieval method or family of retrieval methods. We explain why this is the case in Appendix A. Thus, it should not be expected that these prediction methods could help to select a retrieved list for a given query. In other words, these predictors are essentially query ranking functions which exploit information induced from the retrieved list and which are based on a specific retrieval approach. Indeed, the evaluation of the prediction quality of these methods in past work was based on fixing the corpus and the retrieval method -- often, to a standard document ranking approach that uses document-query surface-level similarities -- and measuring the effectiveness of the query ranking [11].
We draw the following conclusions. The ability to directly estimate, with no relevance judgments, the probability for effective retrieval implies the knowledge of how to perform effective retrieval. Thus, at this conceptual level, it should
4The connection between query-performance prediction and cluster ranking was also stated elsewhere [27]. 5Pre-retrieval query-performance predictors are independent of the retrieved list and therefore cannot be used alone to select a retrieved list.

not be expected that improved prediction would translate to novel improved retrieval methods. On the practical level, the prediction tasks discussed above have been addressed using ranking functions rather than by directly estimating the probability for effective retrieval. Specifically, the most effective post-retrieval over queries predictors are query ranking methods that are committed to specific retrieval approaches. Hence, it should not be expected that using these predictors would help in selecting a retrieved list for a query so as to improve retrieval performance.

3.5 Learning to rank queries using Markov Random Fields
With the realization that previously proposed prediction over queries methods (a.k.a. query-performance predictors [11]) -- both pre-retrieval and post-retrieval -- are essentially query ranking methods, we now turn to tackle the prediction over queries task using a learning-to-rank approach.
Inspired by Equation 1, and given that the choice of a query and a retrieval method entails a retrieved list for a given corpus, we set as a goal to estimate

p(R = 1, Q, C, L).

(6)

This is the joint probability for a relevance event and a triplet of a query (Q), a corpus (C), and a document list (L) retrieved for the query from the corpus.
We estimate Equation 6 using Markov Random Fields (MRFs). An MRF is defined over a graph G with nodes representing random variables and edges representing dependencies between the variables. The nodes of the graph here are the query (Q), the corpus (C), and the retrieved list (L). As is standard practice in using MRFs to rank documents [36], we omit the relevance random variable (R) from the graph to simplify the formulation. This has no effect on the resultant query ranking. Accordingly, we re-write Equation 6 as pR=1(Q, C, L). This is the joint probability over G's nodes which can be written as:

pR=1(Q, C, L)

=

1 Z

Y

x(x);

(7)

xX (G)

X(G) are the cliques in G; x(x) is a potential (positive)

function

defined

over

the

clique

x;

Z =PQ,C,L

Q
xX (G)

x(x)

is the normalization factor. Computing this factor is a very

hard task. However, since our goal is to rank queries, and

the normalization factor does not affect this ranking, we do

not compute it. This is yet another example for the transi-

tion mentioned in Section 3.4 from devising direct estimates

of probabilities to applying rank-equivalence manipulations

that yield ranking functions.

We use the standard instantiation of potential functions

[36]: x(x) d=ef exp(xfx(x)), where fx(x) is a feature func-

tion defined over the clique x and weighted by the parameter

x. Using the feature functions in Equation 7, omitting the

normalization factor, and applying a log transformation, we

arrive to a linear ranking function for queries:

Score(Q; C, L) d=ef

X xfx(x).

(8)

xX (G)

Most linear learning-to-rank methods are based on a linear combination of feature functions as in Equation 8 [32]. Our motivation to address the query ranking challenge using MRFs is the correspondence between the graph structure

16

and the types of information used by previously proposed query-performance prediction approaches that serve for feature functions in Section 3.5.1.
Each feature function fx(x) that we use in Equation 8 is defined as log(hx(x) + ), where h is one of the feature functions defined below;  = 10-10 is a smoothing factor. This results in a conjunction style integration of feature functions similarly to work on using MRFs for document [36] and cluster [39] ranking. In Section 4.1.1 we use SVMrank [26] to learn the values of the x's in Equation 8.

3.5.1 Cliques and feature functions
We consider four different cliques in the graph G. These are depicted in Figure 1. In what follows, we describe the cliques and the feature functions associated with them. All feature functions are previously proposed query-performance predictors. Using these enables us to (i) demonstrate their correspondence with cliques; and, (ii) study whether a learningto-rank method can effectively integrate them.
The xQC clique. This clique is composed only of the query
and the corpus. Thus, it corresponds to pre-retrieval queryperformance predictors that do not analyze the retrieved list. We use three different measures for a query term that utilize corpus-based information. Then, we apply three functions: Sum, Average and Max, to aggregate the measures across the query terms. The first measure is the tf.idf-based similarity between a term and the corpus computed using the SCQ predictor [53]. The second measure is the variance of the tf.idf values of a term over the documents in the corpus in which it appears [53], referred to as VAR. The third measure is the inverse document frequency, IDF, of the term [16]. Using the SCQ-based measure amounts to assuming that query terms similar to the corpus indicate effective retrieval. The VAR and IDF measures are estimates of the discriminative power of the query terms. All together, we have 9 feature functions of the form "AB" for the xQC clique, where A  {Sum,Avg,Max} and B  {SCQ,VAR,IDF}.
The xL clique. This clique contains only the retrieved list.
Feature functions associated with the clique reflect queryindependent properties of the list that potentially attest to retrieval effectiveness.
The first feature function measures the cohesion of the retrieved list [12, 51]. The premise is that a cohesive list is more likely to be effective than a diverse one. To measure cohesion, we compute for each document d in L its similarity with all documents in L, and average the resultant values over all the documents in L. The inter-document similarity measure is described in Section 4.1.1.
The next feature functions are adopted from recent work on estimating document-list quality [40]. Three measures are defined for each document in the list. These potentially attest to the breadth of the document content. Content breadth can be thought of as a prior for document relevance [8]. The measures are then averaged over the documents in the retrieved list to yield the feature functions.
High entropy of the term distribution in the document potentially indicates content breadth [8]. The entropy of document d is defined as -Pwd p(w|d) log p(w|d); w is a term and p(w|d) is the probability assigned to w by an unsmoothed unigram language model induced from d.

Figure 1: The four cliques considered for graph G. G is composed of a query node (Q), corpus node (C) and a retrieved list node (L).
Two additional feature functions are: (i) the ratio between the number of stopwords and non-stopwords in the document, sw1; and (ii) the fraction of stopwords in a stopword list (INQUERY's in our case [1]) that appear in the document, henceforth sw2. These functions were shown to be highly effective document quality measures, especially for noisy Web collections [8]. The reason is that the presence of stopwords presumably attests to the use of "rich" language and therefore to content breadth [8].
The xLC clique. This clique includes the retrieved list and
the corpus but not the query. Thus, it corresponds to queryindependent properties of the retrieved list which are quantified using corpus-based information. One such property is Clarity [15]. This is the KL divergence between a relevance language model [30] induced from the list and that induced from the corpus. The premise is that a retrieved list with a language model different from that of the corpus is focused, and hence reflects effective retrieval. We also use ImpClarity which is a variant of Clarity proposed for Web corpora [24]: only the terms that appear in less than t% of the documents in the corpus are used to induce a relevance language model from the list; t is a free parameter.
The xQLC clique. This clique contains all the nodes in the
graph. The first two feature functions that we consider are the highly effective post-retrieval query-performance predictors WIG [55] and NQC [46]. WIG is the difference between the mean retrieval score in the list and that of the corpus which represents a pseudo non-relevant document.6 The premise is that high retrieval scores with respect to that of the corpus attest to effective retrieval. NQC is defined as the standard deviation of retrieval scores in the list. High deviation was argued to correlate with potentially reduced query drift, and thus with improved effectiveness [46].7
The highly effective UEF prediction framework [45] is based on re-ranking the retrieved list L using a relevance
6The difference is normalized by the query length to ensure inter-query compatibility of prediction values. 7The standard deviation is normalized with respect to the corpus retrieval score to ensure inter-query compatibility of prediction values.

17

language model induced from L. We use the exponent of the Pearson correlation between the scores in L and those produced by the re-ranking as a basic prediction measure. The measure is scaled by the value assigned by some basic predictor -- in our case, Clarity, ImpClarity, WIG or NQC-- to produce the final prediction value. We get 4 feature functions: UEF(Clarity), UEF(ImpClarity), UEF(WIG) and UEF(NQC). The core idea, as mentioned in Appendix A, is that the relevance-model-based re-ranking is more effective than the original ranking and can therefore serve as a reference comparison. The basic predictors serve to estimate the presumed quality of the relevance model.

4. EVALUATION
We start in Section 4.1 by evaluating the effectiveness of our learning-to-rank method, henceforth LTRoq, for the prediction over queries task. Then, in Section 4.2 we address the claim made in Section 3.4 that previously-proposed postretrieval over queries predictors are not effective for prediction over retrieved lists.
4.1 Prediction over queries

4.1.1 Experimental setup
The TREC datasets used for the prediction over queries experiments are specified in Table 2. WSJ, AP and ROBUST are small collections, mostly composed of news articles. WT10G is a small Web collection, and GOV2 is a crawl of the .gov domain. For the ClueWeb09 collection we used both the Category B subset, CW09B, and the English part of Category A, CW09A. To study the effect of spam documents on prediction quality, we created an additional experimental setting for each category of ClueWeb09 [13]. Specifically, documents assigned with a score below 50 (70) by Waterloo's spam classifier [13] were filtered out from the initial ranking created over the documents in CW09B (CW09A); the residual corpus ranking was used for experiments. Accordingly, we get the additional CW09BF setting for Category B and the CW09AF setting for Category A.

corpus
WSJ AP
ROBUST
WT10G GOV2 CW09B CW09BF CW09A CW09AF

# of docs 173,252 242,918
528,155
1,692,096 25,205,179
50,220,423

data Disks 1-2 Disks 1-3
Disks 4-5 (-CR)
WT10g GOV2
ClueWeb09 Category B

queries
151-200 51-150 301-450, 600-700 451-550 701-850
1-200

503,903,810 ClueWeb09 Category A 1-200

Table 2: TREC data used to evaluate the quality of prediction over queries.

Titles of TREC topics served as queries. We applied Krovetz stemming via the Indri toolkit8, which was used for experiments. Stopwords on the INQUERY list [1] were removed from queries, but not from documents.
We used our approach to predict the effectiveness of the query likelihood (QL) retrieval model [48] which is often used in reports on query-performance prediction [15, 54, 24, 53, 45, 46, 29]. Dirichlet-smoothed unigram language models
8www.lemurproject.org/indri

are used with the smoothing parameter set to 1000. Log

query-likelihood values are used for retrieval scores.

The similarity between documents di and dj, used by the

cohesion feature function, is defined as the exponent of the

negative cross entropy between the unsmoothed language

model of di and the Dirichlet-smoothed (with the smoothing

parameter set to 1000) language model of dj.

Except for the pre-retrieval predictors, all the predictors

defined as feature functions in LTRoq analyze the n most

highly ranked documents in the retrieved list. To learn

which values of n are better than others for each of these pre-

dictors we do the following. For each feature function fx(x)

defined over a clique x, instead of using a single value of n we

use

a

linear

combination

of

feature

functions,

P
n

x;nfx;n(x);

fx;n(x) is a feature function defined exactly as fx(x), but an-

alyzes only the n highest ranked documents in the list. Each

linear combination per feature function, as that just defined,

is weighed by x following Equation 8.

All the predictors that were used as feature functions in

LTRoq serve as reference comparisons. As already men-

tioned, a few of these are state-of-the-art. We also use as

a baseline the state-of-the-art Query Feedback (QF) post-

retrieval predictor [55]. A relevance language model induced

from the n most highly ranked documents in the retrieved

list is used to rank the entire collection. The number of

documents that are among the top QF documents in both

the original retrieved list and that retrieved by using the

relevance model serves as the prediction value; QF is a free

parameter. QF was not used as a feature function in LTRoq

because it has two free-parameters, n and QF , and their

effective values do not generalize well across queries [46].

The reference comparison predictors are based on a single

value of n as is standard. This value is set via cross val-

idation as described below. Note that in contrast, LTRoq

integrates instantiations of the same predictor with various

values of n as feature functions.

Following common practice [11], prediction over queries

quality is measured by the Pearson correlation between the

values assigned to queries by a predictor and the actual av-

erage precision (AP@1000) computed for these queries using

TREC's relevance judgments.

The weights associated with feature functions in LTRoq

are learned in two separate phases. We use SVMrank [26],

applied with default free-parameter values, to learn weights

in each phase using the same query train set. Queries are

ranked for SVMrank by the AP(@1000) attained using the

QL retrieval method.

In the first phase, we learn independently for each feature

function fx(x) the values of the x;n weights in the linear

combination it forms:

P
n

x;nfx;n(x).

As the pre-retrieval

predictors are independent of n, we treat them as a sep-

arate linear combination of feature functions. The weight

associated with each feature function that corresponds to a

pre-retrieval predictor is learned, independently, in the first

phase. In the second phase, we learn the values of the x

weights of the linear combinations of the feature functions,

including that of the pre-retrieval predictors.

To learn and test the weights of the feature functions in

LTRoq, and the value of n for the reference comparison pre-

dictors, we randomly split the queries per each experimental

setting into two equal-sized sets and use two-fold cross vali-

dation. The train set is used to learn the feature-functions'

weights in LTRoq in two phases as described above. The

18

SumSCQ AvgSCQ MaxSCQ SumVAR AvgVAR MaxVAR SumIDF AvgIDF MaxIDF
entropy sw1 sw2 cohesion
Clarity ImpClarity
WIG NQC UEF(Clarity) UEF(ImpClarity) UEF(WIG) UEF(NQC)
QF
LTRoq

WSJ -.062
.391 .409 .194 .483 .395 .015 .271 .147 -.160 -.010 -.130 .140 .551 .309 .620 .654 .611 .543 .558 .639 .462
.695

AP -.182
.469 .338 .153 .645 .497 -.103 .415 .207 -.185 -.053 -.045 .279 .569 .565 .542 .526 .613 .636 .575 .552 .495
.692

ROBUST .072 .238 .341 .279 .407 .424 .267 .394 .429
-.120 -.049 -.110 -.004
.371 .453 .493 .496 .537
.563 .538 .510 .457
.557

WT10G .141 .320 .406 .256 .312 .406 .195 .180 .233
-.171 -.076 -.026 -.036
.153 .256 .399 .405 .327 .421 .413 .435 .378
.346

GOV2 .267 .311 .357 .360 .361 .384 .318 .268 .287 .248 .307 .318 .232 .104 .352 .466 .336 .437 .537 .502 .417 .374
.570

CW09B .229 .243 .362 .275 .255 .358 .278 .237 .330 .419 .435 .419
-.110 -.239
.166 .281 .234 -.023 .265 .361 .272 .221
.512

CW09BF .230 .220 .331 .267 .224 .322 .268 .211 .293 .492 .409 .439
-.040 -.197
.091 .319 .406 .133 .382 .466 .458 .273
.535

CW09A
.455 .151 .364
.455 .170 .360 .493 .173 .380 .388 .430 .281 -.140 -.363 .030 .256 .067 -.097 .028 .063 .069 .068
.457

CW09AF .314 .198 .317 .313 .190 .262 .345 .207 .306 .401 .360
.403 -.171 -.267 -.078
.328 .274 -.004 .217 .375 .253 .166
.422

Table 3: Prediction (over queries) quality of LTRoq. `' marks a statistically significant difference with LTRoq. The best result per experimental setting is boldfaced.

value of n for the reference comparisons is learned by optimizing Pearson's correlation over the train set. The evaluation score for a split of the query set is the average prediction quality over the two test sets defined by the split. We repeat this procedure 30 times, and report the average prediction quality over the 30 splits (i.e., Pearson's correlation). For fairness of comparison, the pre-retrieval predictors which serve as reference comparisons are evaluated using the same splits, although they do not incorporate free parameters. Statistically significant differences of prediction quality are determined using the two-tailed paired t-test with p < 0.05 computed over the query set splits.
To construct a relevance language model [30], which is used in Clarity, ImpClarity, UEF and QF, we used unsmoothed document language models and set the number of terms to 100 [46]; for ImpClarity we set t = 1. The value of n in the prediction methods that rely on it was selected from {5, 10, 25, 50, 100, 250, 500, 1000}. For QF, we set QF to a value in {5, 10, 25, 50, 100}.
4.1.2 Experimental results
The prediction quality of the various predictors is presented in Table 3. Our first observation based on Table 3 is that, in general, the post-retrieval predictors yield prediction quality that surpasses that of the pre-retrieval predictors. A case in point, the most effective pre-retrieval predictor in an experimental setting is outperformed by the most effective post-retrieval predictor for seven out of the nine settings. The inferiority of the pre-retrieval predictors could be attributed to the fact that they are independent of the retrieved list that is evaluated.
Another observation that we make based on Table 3 is that in most cases for the ClueWeb09 settings the pre-retrieval predictors are more effective when used to predict the effectiveness of the retrieved list before suspected spam documents were filtered out, i.e., for CW09B and CW09A, rather than after they were removed (CW09BF and CW09AF). On the other hand, post-retrieval predictors are more effective when predicting the effectiveness of the retrieved list after spam documents were filtered out. This finding could poten-

tially be attributed to the fact that post-retrieval predictors analyze the retrieved list, while pre-retrieval predictors utilize only corpus-based query-term statistics which does not change by removing spam documents (only) from the list.
Most importantly, we see in Table 3 that for six out of the nine experimental settings, the best prediction quality is attained by LTRoq. Furthermore, LTRoq outperforms each of the reference comparisons (often to a statistically significant degree) in a vast majority of the experimental settings. LTRoq can be (sometimes statistically significantly) outperformed by some of the predictors for ROBUST, WT10G and CW09A. However, none of the predictors statistically significantly outperforms LTRoq for more than two settings. As UEF, NQC, WIG and QF are state-of-the-art methods, we conclude that LTRoq is the most effective prediction-overqueries method reported in the literature.
Feature function analysis. We next study the importance
of the different feature functions used in LTRoq. To this end, we present in Table 4 for each experimental setting the five feature functions assigned with the highest x values by SVMrank in the second phase of training. Recall that the weights assigned to feature functions in LTRoq are learned in two phases. The weights of the pre-retrieval predictors are set in the first phase of training; the weight of the linear combination of these predictors is set in the second phase. Hence, the pre-retrieval predictors are treated here as a single group, denoted Pre.
We can see in Table 4 that, with the exception of WSJ and GOV2, the pre-retrieval predictors are always among the top-5 feature functions. In contrast, when not integrated in LTRoq, pre-retrieval predictors are often outperformed by post-retrieval predictors as shown in Table 3.
Another observation that we make based on Table 4 is that for the Web settings (WT10G, GOV2 and the four ClueWeb09 settings) at least one of the feature functions defined over the xL clique is always among the top-5 feature functions; for the newswire collections, these feature functions are never among the top-5 feature functions. This finding attests to the merits of using feature functions that

19

WSJ

AP

ROBUST

WT10G

GOV2

CW09B

CW09BF CW09A CW09AF

UEF(NQC)

UEF(Clarity) NQC

Pre

sw2

sw1

sw1

Pre

sw1

NQC

UEF(ImpClarity) UEF(ImpClarity) sw1

sw1

sw2

sw2

sw1

UEF(WIG)

UEF(Clarity) Clarity

Pre

UEF(NQC)

UEF(WIG) Pre

UEF(WIG) cohesion entropy

UEF(WIG)

UEF(NQC)

UEF(NQC)

UEF(ImpClarity) ImpClarity UEF(ImpClarity) Pre

UEF(WIG) Pre

UEF(ImpClarity) Pre

ImpClarity

UEF(WIG)

UEF(NQC) cohesion

NQC

NQC

sw2

Table 4: The top-5 feature functions used by LTRoq for prediction over queries. "Pre": pre-retrieval predictors.

analyze query-independent properties of the retrieved list for prediction over queries in Web collections, as was also demonstrated elsewhere [40].
We can also see that feature functions associated with all four types of cliques used by LTRoq are represented in the top-5 feature functions. As just noted, the feature functions defined over the xL clique are among the top-5 feature functions for all Web settings. The pre-retrieval predictors which are defined over the xQC clique are almost always among the top-5 feature functions. UEF, which is defined over the xQLC clique, is always among the top-5 feature functions when instantiated with either WIG or ImpClarity. The least represented clique in the top-5 feature functions is xQL which has representatives (Clarity or ImpClarity) for only three out of the nine settings.

4.2 Prediction over retrieved lists
We next study the effectiveness of utilizing the feature functions defined in Section 3.5.1, and which were used by LTRoq to rank queries, for the prediction over retrieved lists task. To this end, we use a learning-to-rank approach, denoted LTRol, for ranking lists. Recall that these feature functions are previously proposed predictors over queries. Our goal in using these feature functions here is to empirically examine our claim from Section 3.4. That is, these predictors, which are essentially query ranking functions, should not be expected to be effective for ranking retrieved lists (i.e., for the prediction over retrieved lists task).

4.2.1 Experimental setup

Let {li} be a set of lists retrieved in response to a query q

using some retrieval methods. S(d; q, l) is the (sum-normalized)

retrieval score of document d in l ( {li}); S(d; q, l) d=ef 0 if d

is not in l. Linear fusion methods [3] assign d with the score

P
l{li

}

l

S

(d;

q,

l),

where

l

is

a

(non-negative)

weight

as-

signed to list l. The prediction over retrieved lists task that

we focus on here is learning the l weights. These weights

should reflect the effectiveness of the lists with respect to q.

Runs submitted to TREC serve as the results of different

retrieval methods. Details of the TREC benchmarks are

provided in Table 5. We randomly sample 5 runs out of all

the runs submitted to a track; the runs sampling procedure

is repeated 30 times. The 100 most highly ranked documents for a query in a run serve as a retrieved list to be fused.9

We use the mean average precision at cutoff 100 (MAP),

and the precision of the top-5 documents (p@5), averaged

over the 30 samples, as evaluation measures. Statistically

significant differences of retrieval performance are computed

with respect to the average performance over the 30 samples.

The pre-retrieval predictors are not used here, neither

when applied alone nor when integrated in LTRol, as they

9Fusion methods were shown to be most effective when fusing relatively short lists [47, 7].

TREC
TREC7 TREC8 TREC9 TREC18 TREC19 TREC20 TREC21

track Ad hoc
Web
Web

# of docs 528,155 1,692,096
50,220,423

data Disks 4-5 (-CR) WT10g
ClueWeb09 Category B

queries
351-400 401-450 451-500
1-50 51-100 101-150 151-200

Table 5: Data used for the evaluation of fusion-based retrieval effectiveness when using prediction over retrieved lists to weigh lists.

are independent of a retrieved list and LTRol uses a linear combination of predictors. The remaining feature functions that are used in LTRol are computed for each retrieved list. While in Section 4.1 these feature functions were used to rank queries, here we use them to weigh the lists.
Since our focus here is on ranking lists rather than queries, the query-length and corpus normalization factors used in WIG and NQC to ensure across query compatibility need not be computed [46]. Thus, WIG is the mean retrieval score in a retrieved list; NQC is the standard deviation. Instead of using QL retrieval scores to weigh documents when constructing the relevance model in Clarity, ImpClarity and UEF, document retrieval scores in the lists are used [46].
For reference comparison we use several state-of-the-art predictors over queries as list weights, namely WIG, NQC, UEF(WIG) and UEF(NQC). These were described in Section 3.5.1. We also report the performance of using the following three list-weighting schemes: (i) optimal weighting, OPT, where the weight assigned to a list is its actual AP(@100); (ii) UNI, where all the lists are uniformly weighted; this amounts to using the CombSum linear fusion method [20]; and, (iii) cross validation, CV, where the weight assigned to a retrieved list for queries in a test set is the MAP computed based on the queries in the train set for the same retrieval method (i.e., the same run). CV is based on the premise that past performance of a retrieval method (i.e., over train queries) indicates its performance with new queries. In contrast, all other predictors that we consider (except for UNI) weigh a list by directly estimating its effectiveness.
The number of the highest ranked documents in a retrieved list (n) considered in the feature functions and reference comparisons, and the weights of the feature functions in LTRol, are set using 5-fold cross validation; query IDs are used to create the folds. The weights in LTRol are learned using SVMrank [26] in two phases as was the case for LTRoq10. AP@100 is used in the learning phase to rank the lists that are fused in the training set. Since each of the lists
10We apply min-max normalization to the l weights which are assigned by SVMrank to the retrieved lists to avoid using negative weights.

20

TREC7 TREC8 TREC9 TREC18 TREC19 TREC20 TREC21

MAP p@5 MAP p@5 MAP p@5 MAP p@5 MAP p@5 MAP p@5 MAP p@5

WorstRun 318..77ucuc 247..18ucuc 124..65ucuc 215..96ucuc 164..05ucuc 197..72ucuc 309..00ucuc

MedianRun
1455..48ucuc 2500..29ucuc 1235..06ucuc 1301..45ucuc 11.2uc 27.3c 1239..89ucuc 1341..66ucuc

BestRun
21.2 59.1uc 27.2u 64.9u 19.7u 38.7u
14.6 36.1 16.9u 38.8u
20.3 36.7
20.8 39.1

OPT 2655..22ucuc 3608..55ucuc 2435..45ucuc 1483..27ucuc 2402..74ucuc 2483..45ucuc 2465..45ucuc

UNI
20.2c 54.4c 25.8c 60.9c 17.1c 34.8c 14.3c 36.5
14.8c 28.6c 21.1c 37.7c 21.6c 37.7c

CV 21.4u 56.7u 27.6u 63.2u
19.0u 38.3u
14.9u
36.6 16.9u 34.4u 22.0u 38.5u
22.4u 38.7u

WIG 20.5uc 54.1c 26.6uc 61.3c
17.1c 34.5uc 14.2c
36.3 1248..71ucuc 21.2c
37.7c
21.8
37.2c

NQC 1580..97ucuc 26.3c
61.1c 1249..66ucuc 3122..45ucuc 218..55ucuc 1375..52ucuc 1384..27ucuc

UEF(WIG) 20.6uc 54.1c 2661..64ucuc 17.1c 34.0uc 14.2c 36.9
14.6c 28.6c
20.8c 37.5c
21.7c 37.4c

UEF(NQC)
1590..08ucuc 26.5uc 61.4c 1340..82ucuc 1322..35ucuc 228..05ucuc 1375..52ucuc 1384..97ucuc

LTRol
20.6c 57.3u 2662..55ucu
17.1c 34.1c 13.9uc 35.5 16.5u 36.4uc 19.9uc 37.2c
20.8c 42.3uc

Table 6: Retrieval effectiveness of using LTRol to weigh lists for linear fusion. WorstRun, MedianRun and BestRun: average performance of the worst, median and best run, among the five fused, respectively. `u' and `c' mark statistically significant differences with UNI and CV, respectively. The best result of a prediction method per experimental setting and evaluation metric is boldfaced.

to be fused is composed of 100 documents, the value of n is selected from {5, 10, 25, 50, 100}. All other implementation details are the same as those described in Section 4.1.1.
4.2.2 Experimental results
The results are presented in Table 6. WorstRun, MedianRun and BestRun denote the average performance, over the 30 samples, of the worst, median and best run among the 5 runs in a sample, respectively. The substantial performance differences among the three, along with the very high OPT performance numbers, attest to the substantial merits of assigning effective weights to the lists when fusing them.
We can see that LTRol is more effective than NQC and UEF(NQC) in most relevant comparisons (track × evaluation metric). LTRol is also more effective than WIG and UEF(WIG) in about half of the relevant comparisons. Furthermore, LTRol is the only method among those using predictors over queries which in most relevant comparisons outperforms UNI (i.e., uniform weighting of lists). Nonetheless, LTRol is outperformed in most cases by CV. This means that weighing a retrieved list based on the past performance of the retrieved method that produces the list is much more effective than applying prediction-over-queries methods to the list itself to set its weight. Thus, we get empirical support to the claim from Section 3.4 that using predictors over queries to rank retrieved lists is not highly effective.11
5. SUMMARY
We framed the query-performance prediction task in the most general probabilistic terms. This gave rise to novel connections between tasks, and methods used to address them, in federated search, fusion-based retrieval, and previous work on query-performance prediction (i.e., prediction over queries). We formally argued that successful prediction directly implies knowledge of how to perform effective retrieval. We also argued why using previously proposed query-performance predictors was not shown to improve retrieval effectiveness. These are query ranking methods that
11Learning-to-rank methods were used to fuse lists [43] and to rank them [5]. Both approaches use features specific to, and shared by, the rankers employed. In the TREC setting here, the features used by the rankers are not necessarily known, nor shared. We used predictors over queries that can be applied to any retrieved list.

are suited for specific retrieval methods. In addition, we devised a learning-to-rank approach for predicting performance over queries. The resultant prediction quality substantially transcends that of state-of-the-art predictors. We used our approach to show that predictors over queries are not very effective for prediction over retrieved lists.
Acknowledgments We thank the reviewers for their comments and Yun Zhou for a discussion about the connections between prediction tasks. This work was supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center. This work was also supported in part by Microsoft Research through its Ph.D. Scholarship Program.
6. REFERENCES
[1] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proc. of TREC-9, pages 551­562, 2000.
[2] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. of ECIR, pages 127­137, 2004.
[3] C. C. V. ant Garrison W. Cottrell. Fusion via linear combination of scores. Information Retrieval, 1(3):151­173, 1999.
[4] J. A. Aslam and V. Pavlu. Query hardness estimation using Jensen-Shannon divergence among multiple scoring functions. In Proc. of ECIR, pages 198­209, 2007.
[5] N. Balasubramanian and J. Allan. Learning to select rankers. In Proc. of SIGIR, pages 855­856, 2010.
[6] N. Balasubramanian, G. Kumaran, and V. R. Carvalho. Predicting query performance on the web. In Proc. of SIGIR, pages 785­786, 2010.
[7] S. M. Beitzel, E. C. Jensen, A. Chowdhury, O. Frieder, D. A. Grossman, and N. Goharian. Disproving the fusion hypothesis: An analysis of data fusion via effective information retrieval strategies. In Proc. of SAC, pages 823­827, 2003.
[8] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of web documents. In Proc. of WSDM, pages 95­104, 2011.
[9] Y. Bernstein, B. Billerbeck, S. Garcia, N. Lester, F. Scholer, and J. Zobel. RMIT university at trec 2005: Terabyte and robust track. In Proc. of TREC-14, 2005.
[10] J. Callan. Distributed information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 5, pages 127­150. Kluwer Academic Publishers, 2000.
[11] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool, 2010.
[12] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In Proc. of SIGIR, pages 390­397, 2006.
[13] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Informaltiom Retrieval Journal, 14(5):441­465, 2011.

21

[14] W. B. Croft. Combining approaches to information retrieval. In W. B. Croft, editor, Advances in information retrieval, chapter 1, pages 1­36. Kluwer Academic Publishers, 2000.
[15] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proc. of SIGIR, pages 299­306, 2002.
[16] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A language modeling framework for selective query expansion. Technical Report IR-338, Center for Intelligent Information Retrieval, University of Massachusetts, 2004.
[17] R. Cummins. Predicting query performance directly from score distributions. In Proc. of AIRS, pages 315­326, 2011.
[18] R. Cummins, J. M. Jose, and C. O'Riordan. Improved query performance prediction using standard deviation. In Proc. of SIGIR, pages 1089­1090, 2011.
[19] F. Diaz. Performance prediction using spatial autocorrelation. In Proc. of SIGIR, pages 583­590, 2007.
[20] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proc. of TREC-2, 1994.
[21] C. Hauff and L. Azzopardi. When is query performance prediction effective? In Proc. of SIGIR, pages 829­830, 2009.
[22] C. Hauff, L. Azzopardi, and D. Hiemstra. The combination and evaluation of query performance prediction methods. In Proc. of ECIR, pages 301­312, 2009.
[23] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In Proc. of CIKM, pages 1419­1420, 2008.
[24] C. Hauff, V. Murdock, and R. A. Baeza-Yates. Improved query difficulty prediction for the web. In Proc. of CIKM, pages 439­448, 2008.
[25] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In Proc. of SPIRE, pages 43­54, 2004.
[26] T. Joachims. Training linear svms in linear time. In Proc. of KDD, pages 217­226, 2006.
[27] O. Kurland, F. Raiber, and A. Shtok. Query-performance prediction and cluster ranking: Two sides of the same coin. In Proc. of CIKM, pages 2459­2462, 2012.
[28] O. Kurland, A. Shtok, D. Carmel, and S. Hummel. A unified framework for post-retrieval query-performance prediction. In Proc. of ICTIR, pages 15­26, 2011.
[29] O. Kurland, A. Shtok, S. Hummel, F. Raiber, D. Carmel, and O. Rom. Back to the roots: a probabilistic framework for query-performance prediction. In Proc. of CIKM, pages 823­832, 2012.
[30] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. of SIGIR, pages 120­127, 2001.
[31] D. Lillis, F. Toolan, R. W. Collier, and J. Dunnion. Probfuse: a probabilistic approach to data fusion. In Proc. of SIGIR, pages 139­146, 2006.
[32] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.
[33] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proc. of SIGIR, pages 186­193, 2004.
[34] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.
[35] C. Macdonald, R. L. T. Santos, and I. Ounis. On the usefulness of query features for learning to rank. In Proc. of CIKM, pages 2559­2562, 2012.
[36] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proc. of SIGIR, pages 472­479, 2005.
[37] J. Mothe and L. Tanguy. Linguistic features to predict query difficulty. In ACM SIGIR 2005 Workshop on Predicting Query Difficulty - Methods and Applications, 2005.
[38] J. P´erez-Iglesias and L. Araujo. Standard deviation as a query hardness estimator. In Proc. of SPIRE, pages 207­212, 2010.
[39] F. Raiber and O. Kurland. Ranking document clusters using markov random fields. In Proc. of SIGIR, pages 333­342, 2013.
[40] F. Raiber and O. Kurland. Using document-quality measures to predict web-search effectiveness. In Proc. of ECIR, pages 134­145, 2013.
[41] F. Scholer and S. Garcia. A case for improved evaluation of query difficulty prediction. In Proc. of SIGIR, pages 640­641, 2009.
[42] F. Scholer, H. E. Williams, and A. Turpin. Query association surrogates for web search. JASIST, 55(7):637­650, 2004.
[43] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. Lambdamerge: merging the results of query reformulations. In Proc. of WSDM, pages 795­804, 2011.

[44] M. Shokouhi and L. Si. Federated search. Foundations and Trends in Information Retrieval, 5(1):1­102, 2011.
[45] A. Shtok, O. Kurland, and D. Carmel. Using statistical decision theory and relevance models for query-performance prediction. In Proc. of SIGIR, 2010.
[46] A. Shtok, O. Kurland, D. Carmel, F. Raiber, and G. Markovits. Predicting query performance by query-drift estimation. ACM Transactions on Information Systems, 30(2):11, 2012.
[47] I. Soboroff, C. K. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In Proc. of SIGIR, pages 66­73, 2001.
[48] F. Song and W. B. Croft. A general language model for information retrieval (poster abstract). In Proc. of SIGIR, pages 279­280, 1999.
[49] K. Sparck Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval: development and comparative experiments - part 1. Information Processing and Management, 36(6):779­808, 2000.
[50] S. Tomlinson. Robust, Web and Terabyte Retrieval with Hummingbird Search Server at TREC 2004. In Proc. of TREC-13, 2004.
[51] V. Vinay, I. J. Cox, N. Milic-Frayling, and K. R. Wood. On ranking the effectiveness of searches. In Proc. of SIGIR, pages 398­404, 2006.
[52] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In Proc. of SIGIR, pages 512­519, 2005.
[53] Y. Zhao, F. Scholer, and Y. Tsegay. Effective pre-retrieval query performance prediction using similarity and variability evidence. In Proc. of ECIR, pages 52­64, 2008.
[54] Y. Zhou and B. Croft. Ranking robustness: a novel framework to predict query performance. In Proc. of CIKM, pages 567­574, 2006.
[55] Y. Zhou and B. Croft. Query performance prediction in web search environments. In Proc. of SIGIR, pages 543­550, 2007.
APPENDIX
A. POST-RETRIEVAL PREDICTION OVER
QUERIES
We argue that post-retrieval prediction over queries methods (henceforth prediction methods or predictors) were coupled with, and are hence effective for, specific retrieval methods; that is, for ranking queries by the presumed effectiveness of using these retrieval methods for them.
There are predictors that explicitly rely on features used by the retrieval method [6]. This reliance is implicit for other prediction methods as we discuss next.
As described in Section 2, there are three classes of postretrieval predictors. The Clarity predictor [15] was shown to be ineffective for a variety of retrieval methods [41].
Predictors that analyse the retrieval scores distribution [50, 9, 19, 55, 6, 38, 17, 18, 46] are by definition dependent on the retrieval method as noted in some work [55, 46]. In fact, almost all of these predictors are suited for, and were evaluated with, retrieval methods that use only documentquery surface-level similarities to rank documents.
Query feedback (QF) [55] and UEF [45] are among the most effective post-retrieval predictors that do not analyze retrieval scores. Both rely on the premise that retrieval using pseudo-feedback-based query expansion is more effective than that at hand. Indeed, in both reports [55, 45], the retrieval method used for evaluation was a language-modelbased approach with no query expansion. Comparison with query-expansion-based retrieval, or some other effective retrieval, need not necessarily indicate the effectiveness of retrieval methods that are much more effective [47].

22

Evaluation of Machine-Learning Protocols for Technology-Assisted Review in Electronic Discovery

Gordon V. Cormack
University of Waterloo
gvcormac@uwaterloo.ca

Maura R. Grossman
Wachtell, Lipton, Rosen & Katz
mrgrossman@wlrk.com

ABSTRACT
Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks ­ four derived from the TREC 2009 Legal Track and four derived from actual legal matters ­ recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P < 0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P < 0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" ­ determining when training is adequate, and therefore may stop.
Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Search process, relevance feedback.
Keywords: Technology-assisted review; predictive coding; electronic discovery; e-discovery.
The views expressed herein are solely those of the author and should not be attributed to her firm or its clients.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the authors. Copyright is held by the authors. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. ACM 978-1-4503-2257-7/14/07. http://dx.doi.org/10.1145/2600428.2609601.

1. INTRODUCTION
The objective of technology-assisted review ("TAR") in electronic discovery ("e-discovery")1 is to find as nearly all of the relevant documents in a collection as possible, with reasonable effort. While this study does not presume to interpret the common-law notion of what is reasonable, it serves to quantify the tradeoff between how nearly all of the relevant documents can be found (as measured by recall), and the human effort needed to find them (as measured by the number of documents that must be manually reviewed, which translates into time and cost).
In a typical review task, a "requesting party" prescribes relevance2 by way of a "request for production," while the "responding party," its adversary, is required to conduct a review and produce the responsive, non-privileged documents identified as a result of a reasonable search. A study by Grossman and Cormack [8] shows that two TAR methods can be both more effective and more efficient than traditional e-discovery practice, which typically consists of keyword or Boolean search, followed by manual review of the search results. One of these methods, due to Cormack and Mojdeh [7], employs machine learning in a protocol we refer to as Continuous Active Learning ("CAL"). The other method, due to H5 [13], does not employ machine learning, and therefore is not considered in this study.
Often relying on Grossman and Cormack for support, many legal service providers have advanced TAR tools and methods that employ machine learning, but not the CAL protocol. These tools and methods, often referred to in the legal marketplace as "predictive coding," follow one of two protocols which we denote Simple Active Learning ("SAL") and Simple Passive Learning ("SPL"). Some tools that employ SAL have achieved superior results at TREC, but have never, in a controlled study, been compared to CAL. Tools that use SPL, while widely deployed, have not achieved superior results at TREC, and have not, in a controlled study, been compared to traditional methods, to SAL, or to CAL.
1See Grossman and Cormack [10] for a glossary of terms pertaining to TAR. See Oard and Webber [16] for an overview of information retrieval for e-discovery. 2The IR term "relevant" generally describes a document sought by an information-retrieval effort, while the legal term "responsive" describes a document that satisfies the criteria set forth in a request for production. In this study, the terms are used interchangeably; however, in the context of litigation, relevance may take on a broader meaning than responsiveness.

153

This study compares CAL, SAL, and SPL, and makes available a TAR evaluation toolkit3 to facilitate further comparisons. The results show SPL to be the least effective TAR method, calling into question not only its utility, but also commonly held beliefs about TAR. The results also show that SAL, while substantially more effective than SPL, is generally less effective than CAL, and as effective as CAL only in a best-case scenario that is unlikely to be achieved in practice.
2. THE TAR PROCESS
The TAR process, in the abstract, proceeds as follows. Given a document collection and a request for production, a human operator uses one or more tools to identify documents to be shown to one or more human reviewers, who may or may not be the same individual as the operator. The reviewers examine these documents and label ("code") them each as responsive or not. More documents are identified using the tools, reviewed and coded by reviewers, and the process continues until "enough" of the responsive documents have been reviewed and coded. How many constitute "enough" is a legal question, which is informed by how much additional effort would likely be required to find more responsive documents, and how important those documents would likely be in resolving the legal dispute (i.e., "proportionality considerations"). For our purposes, we consider the process to continue indefinitely, and track the number of responsive documents found (i.e., recall) as a function of effort (i.e., the number of documents reviewed and coded by reviewers). Using this information, the reader can determine retrospectively, for any definition of "enough," how much effort would have sufficed to find enough documents.
For this study, the operator is assumed to follow a strict protocol. All choices, including what tools are used, and when and how they are used, are prescribed by the protocol. In addition to satisfying the requirements for a controlled comparison, the use of a strict protocol may be appealing in the e-discovery context because the requesting party may distrust, and therefore wish to prohibit discretionary choices made by the operator on behalf of the responding party. The reviewers are assumed to code the documents they review in good faith, to the best of their abilities. In light of Grossman and Cormack [8, 9], and others [4, 18, 25, 26], it is unrealistic to assume the reviewers to be infallible ­ they will necessarily, but inadvertently, code some responsive documents as non-responsive, and vice versa.
The CAL protocol involves two interactive tools: a keyword search system and a learning algorithm. At the outset of the TAR process, the operator typically uses a keyword search to identify an initial set of documents to be reviewed and coded. These coded documents (often referred to as the "seed set") are used to train a learning algorithm, which scores each document in the collection by the likelihood that it is responsive. The top-scoring documents that have not yet been coded are then reviewed and coded by reviewers. The set of all documents coded thus far (the "training set") is used to train the learning algorithm, and the process of selecting the highest-scoring documents, reviewing and coding them, and adding them to the training set continues until "enough" of the responsive documents have been found.
3Available at http://cormack.uwaterloo.ca/cormack/tartoolkit.

The SAL protocol, like CAL, begins with the creation of a seed set that is used to train a learning algorithm. The seed set may be selected using keywords, random section, or both, but, unlike CAL, the subsequent training documents to be reviewed and coded are selected using uncertainty sampling [15], a method that selects the documents about which the learning algorithm is least certain. These documents are added to the training set, and the process continues until the benefit of adding more training documents to the training set would be outweighed by the cost of reviewing and coding them (a point often referred to as "stabilization"). At this point, the learning algorithm is used for the last time to create either a set or a ranked list of likely relevant documents (the "review set"), which is subsequently reviewed and coded by reviewers.
The SPL protocol, unlike CAL or SAL, generally relies on the operator or random selection, and not the learning algorithm, to identify the training set. The process is typically iterative. Once a candidate training set is identified, the learning algorithm is then trained on these documents and used to create a candidate review set. If the review set is "inadequate," the operator creates a new candidate training set, generally by adding new documents that are found by the operator, or through random selection. The process continues until the review set is deemed "adequate," and is subsequently reviewed and coded by reviewers.
The TAR process addresses a novel problem in information retrieval, which we denote here as the "TAR Problem." The TAR Problem differs from well-studied problems in machine learning for text categorization [21] in that the TAR process typically begins with no knowledge of the dataset and continues until most of the relevant documents have been identified and reviewed. A classifier is used only incidentally for the purpose of identifying documents for review. Gain is the number of relevant documents presented to the human during training and review, while cost is the total number of relevant and non-relevant documents presented to the human during training and review.
3. SIMULATING REVIEW
To simulate the application of a TAR protocol to a review task, we require a realistic document collection and request for production, a keyword query ("seed query") to be used (if required by the protocol), and a simulated reviewer. To evaluate the result, we require a "gold standard" indicating the true responsiveness of all, or a statistical sample, of the documents in the collection.
Four review tasks, denoted Matters 201, 202, 203, and 207, were derived from Topics 201, 202, 203, and 207 of the TREC 2009 Legal Track Interactive Task ­ the same Topics that were used to evaluate Cormack and Mojdeh's CAL efforts at TREC [7, 12]. Four other review tasks, denoted Matters A, B, C, and D, were derived from actual reviews conducted in the course of legal proceedings. Statistics for the collections are provided in Table 1, and the requests for production are shown in Table 2.
The seed queries for the tasks derived from TREC, shown in Table 3, were composed by Open Text in the course of its participation in the TREC 2010 Legal Track Learning Task (which used the same topics as the TREC 2009 Legal Track Interactive Task), using a strategy that "attempted to quickly create [a] Boolean query for each topic" [24, page 5]. The seed queries for the tasks derived from legal proceedings,

154

Matter 201 202 203 207 A B C D

Collection Size 723,537 723,537 723,537 723,537
1,118,116 409,277 293,549 405,796

# Rel. Docs. 2,454 9,514 1,826 8,850 4,001 6,236 1,170
15,926

Prevalence (%) 0.34 1.31 0.25 1.22 0.36 1.52 0.48 3.92

Table 1: Collection statistics.

Matter 201
202
203
207
A B C D

Request for Production All documents or communications that describe, discuss, refer to, report on, or relate to the Company's engagement in structured commodity transactions known as prepay transactions. All documents or communications that describe, discuss, refer to, report on, or relate to the Company's engagement in transactions that the Company characterized as compliant with FAS 140 (or its predecessor FAS 125). All documents or communications that describe, discuss, refer to, report on, or relate to whether the Company had met, or could, would, or might meet its financial forecasts, models, projections, or plans at any time after January 1, 1999. All documents or communications that describe, discuss, refer to, report on, or relate to fantasy football, gambling on football, and related activities, including but not limited to, football teams, football players, football games, football statistics, and football performance. [Regulatory request] [Regulatory request] [Third-party subpoena] [Regulatory request]

Table 2: Requests for production.

described in Table 3, were composed by or negotiated with the requesting party prior to the review process. The recall and precision of each of the seed queries (as measured with respect to the gold standard, discussed below) are shown in Table 3.
To simulate a reviewer, we use a "training standard" that consists of a relevance assessment for each document in the collection. If, during the course of simulating a particular review protocol, the reviewer is called upon to code a document, the assessment from the training standard ­ responsive or not responsive ­ is used for this purpose. The training standard does not represent ground truth; instead, it represents the coding decision that a fallible reviewer might render when presented with the document for review. For all of the simulated tasks, all of the positive trainingstandard assessments, and some of the negative assessments, were rendered by a reviewer during the course of a prior review. For the TREC-derived tasks, we used Cormack and

Matter 201 202
203
207 A B C D

Seed Query "pre-pay" OR "swap" "FAS" OR "transaction" OR "swap" OR "trust" OR "Transferor" OR "Transferee" "forecast" OR "earnings" OR "profit" OR "quarter" OR "balance sheet" "football" OR "eric bass" [7-term Boolean query] [98-term Boolean query] [46-term Boolean query] [9-term Boolean query]

Recall Prec. 0.436 0.038 0.741 0.090

0.872 0.034

0.492 0.545 0.991 0.259 0.945

0.167 0.045 0.019 0.026 0.325

Table 3: Keyword seed queries and their associated recall and precision.

Matter
201 202 203 207 A B C D

Training Standard Recall Precision 0.843 0.911 0.844 0.903 0.860 0.610 0.896 0.967 1.000 0.307 0.942 0.974 1.000 0.429 0.961 1.000

Table 4: Recall and precision for the training standard used to simulate human review.

Mojdeh's TREC submissions;4 for the legal-matter-derived tasks, we used the coding rendered by the first-pass reviewer in the course of the review. Documents that were never seen by the first-pass reviewer (because they were never identified as potentially responsive) were deemed to be coded as non-responsive. Overall, as measured with respect to the gold standard, the recall and precision of the training standard (shown in Table 4) indicate that the simulated reviewer achieves a high-quality ­ but far from perfect ­ result, by human review standards.
In contrast to the training standard, the gold standard represents ground truth. For the TREC-derived tasks, the gold standard consists of a stratified random sample, assessed by TREC using a two-stage adjudication process [12]. For the legal-matter-derived tasks, the gold standard consists of the documents produced to the requesting party, after a second-pass review and quality-assurance efforts. Each document in the gold standard is associated with an inclusion probability ­ the prior probability that it would have been included in the gold standard. Following TREC practice, the recall of any simulated review is estimated using the Horvitz-Thompson estimator [14], which weights each gold-standard document by the reciprocal of its inclusion probability.
Evaluation results are presented in two ways: as gain curves and as 75% recall-effort ("75% RE") values. A gain curve plots recall as a function of the number of documents reviewed. For any level of effort (as measured by the number
4Available at http://trec.nist.gov/results.html, subject to a usage agreement.

155

of documents reviewed), one can determine at a glance the recall that would be achieved, using a particular protocol, for that level of effort (see Figures 1 and 2 below). Conversely, for any recall level it is possible to determine what level of effort would be required to achieve that recall level. For the purpose of quantitative comparison, we tabulate 75% RE for all protocols (see Tables 5 and 6 below).
4. TAR PROTOCOLS
In this study, we used the same feature engineering and learning algorithm for every protocol, without any collection- or task-specific tuning. Following Cormack and Mojdeh [7], the first 30,000 bytes of the ASCII text representation of each document (including a text representation of the sender, recipient, cc or bcc recipients, subject, and date and time sent) was shingled as overlapping 4-byte segments. The number of distinct possible segments was reduced, by hashing, from 232 = 4, 294, 967, 296 to 1, 000, 081 (an arbitrarily chosen prime number near one million). Each feature consisted of a binary value: "1" if the feature was present in the first 30,000 bytes of the document; "0" if it was absent. For the learning algorithm, we used the Sofia-ML implementation of Pegasos SVM,5 with the following parameters: "--iterations 2000000 --dimensionality 1100000."
For all protocols, we used a batch size of 1,000 documents. That is, the initial training set (the seed set) was 1,000 documents, and each iteration, whether CAL, SAL, or SPL, involved reviewing 1,000 documents and, if indicated by the protocol, adding them to the training set. Our primary experiments evaluated the specific formulations of CAL, SAL, and SPL described in Section 2; secondary experiments explored the effect of using keyword-selected versus randomly selected documents for the seed and training sets.
Our primary CAL implementation used, as the initial training set, 1,000 documents, randomly selected from the results of a search using the seed query. In each iteration, the training-set documents were coded according to the training standard, then used to train Sofia-ML, and hence to score the remaining documents in the collection. The 1,000 topscoring documents were added to the training set, and the process was repeated 100 times.
Our primary SAL implementation used exactly the same 1,000-document keyword-selected seed set as CAL. Like CAL, in each iteration, the training-set documents were coded according to the training standard, then used to train Sofia-ML, and hence to score the remaining documents in the collection. These documents, ranked by score, constitute a candidate review set. Rather than implementing the decision as to whether stabilization had occurred, we recorded the candidate review set for future retrospective evaluation, and continued the training process. Unlike CAL, the 1,000 documents with the least magnitude scores were coded and added to the training set, and the process was repeated 100 times. In the end, the simulation yielded 100 different candidate review sets, corresponding to stabilization having occurred with a training-set size of 1,000, 2,000, ..., 100,000 documents. Each training-set size, when evaluated, yields a different gain curve, and a different 75% RE. Due to space considerations, we show gain curves only for the representative training-set sizes of 2,000, 5,000, and 8,000 documents. We report 75% RE for these three training-set sizes, as well
5Available at http://code.google.com/p/sofia-ml.

as for the ideal training-set size, which in reality would not be known, since it requires the benefit of hindsight. The ideal training-set size is derived using the gold standard; 75% RE is calculated for every training-set size, and the lowest value is chosen.
Our primary SPL implementation used random selection throughout, as advocated by some SPL proponents. The initial training set (which we denote the "seed set," notwithstanding the fact that many SPL proponents use the same term to refer to the final training set) consisted of 1,000 randomly selected documents, and each iteration added 1,000 more randomly selected documents. As for SAL, we recorded the candidate review set after each iteration, and report gain curves for the representative training-set sizes of 2,000, 5,000, and 8,000 documents, as well as 75% RE for these training-set sizes, and for the ideal training-set size, as defined above.
Variants of these protocols, for which we report 75% RE, include using randomly selected documents as a seed set for CAL and SAL, using a keyword-selected seed set for SPL, and using an entirely keyword-selected training set for SPL.
5. PRIMARY RESULTS
As illustrated in Figure 1, the CAL protocol achieves higher recall than SPL, for less effort, for all of the representative training-set sizes. All eight graphs show the same basic result: After the first 1,000 documents (i.e., the seed set), the CAL curve shows a high slope that is sustained until the majority of relevant documents have been identified. At about 70% recall, the slope begins to fall off noticeably, and effectively plateaus between 80% and 100% recall. The SPL curve exhibits a low slope for the training phase, followed by a high slope, falloff, and then a plateau for the review phase. In general, the slope immediately following training is comparable to that of CAL, but the falloff and plateau occur at substantially lower recall levels. While the initial slope of the curve for the SPL review phase is similar for all training-set sizes, the falloff and plateau occur at higher recall levels for larger training sets. This advantage of larger training sets is offset by the greater effort required to review the training set: In general, the curves for different training sets cross, indicating that a larger training set is advantageous when high recall is desired.
75% recall effort, shown in Table 5, illustrates the superiority of CAL over SPL, even when SPL is afforded the benefit of hindsight to choose the ideal training-set size. A simple sign test shows with statistical significance (P < 0.01) that CAL is superior to SPL according to 75% RE, and also according to recall effort for any other level of recall.
Figure 2 shows that the CAL protocol generally achieves higher recall than SAL. However, the SAL gain curves, unlike the SPL gain curves, often touch the CAL curves at one specific inflection point. The strong inflection of the SAL curve at this point is explained by the nature of uncertainty sampling: Once stabilization occurs, the review set will include few documents with intermediate scores, because they will have previously been selected for training. Instead, the review set will include primarily high-scoring and low-scoring documents. The high-scoring documents account for the high slope before the inflection point; the low-scoring documents account for the low slope after the inflection point; the absence of documents with intermediate scores accounts for the sharp transition. The net effect

156

Matter 201 - Recall vs. Documents Reviewed 1

Matter 202 - Recall vs. Documents Reviewed 1

0.8

0.8

0.6

0.6

Recall

Recall

0.4 0.2
0 0
1 0.8 0.6

CAL SPL-8k SPL-5k SPL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter 203 - Recall vs. Documents Reviewed

CAL SPL-8k SPL-5k SPL-2k

0.4 0.2
0 0
1 0.8 0.6

CAL SPL-8k SPL-5k SPL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter 207 - Recall vs. Documents Reviewed

Recall

Recall

0.4 0.2
0 0
1 0.8 0.6

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter A - Recall vs. Documents Reviewed

CAL SPL-8k SPL-5k SPL-2k

0.4 0.2
0 0
1 0.8 0.6

CAL SPL-8k SPL-5k SPL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter B - Recall vs. Documents Reviewed

Recall

Recall

0.4 0.2
0 0
1

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter C - Recall vs. Documents Reviewed

0.4 0.2
0 0
1

CAL SPL-8k SPL-5k SPL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter D - Recall vs. Documents Reviewed

0.8

0.8

0.6

0.6

Recall

Recall

0.4
0.2
0 0

CAL SPL-8k SPL-5k SPL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

0.4
0.2
0 0

CAL SPL-8k SPL-5k SPL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Figure 1: Continuous Active Learning versus Simple Passive Learning using three different training-set sizes of randomly selected documents.

157

Matter 201 - Recall vs. Documents Reviewed 1

Matter 202 - Recall vs. Documents Reviewed 1

0.8

0.8

0.6

0.6

Recall

Recall

0.4 0.2
0 0
1

CAL SAL-8k SAL-5k SAL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter 203 - Recall vs. Documents Reviewed

0.4 0.2
0 0
1

CAL SAL-8k SAL-5k SAL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter 207 - Recall vs. Documents Reviewed

0.8

0.8

0.6

0.6

Recall

Recall

0.4 0.2
0 0
1

CAL SAL-8k SAL-5k SAL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter A - Recall vs. Documents Reviewed

0.4 0.2
0 0
1

CAL SAL-8k SAL-5k SAL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter B - Recall vs. Documents Reviewed

0.8

0.8

0.6

0.6

Recall

Recall

0.4 0.2
0 0
1

CAL SAL-8k SAL-5k SAL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter C - Recall vs. Documents Reviewed

0.4 0.2
0 0
1

CAL SAL-8k SAL-5k SAL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Matter D - Recall vs. Documents Reviewed

0.8

0.8

0.6

0.6

Recall

Recall

0.4
0.2
0 0

CAL SAL-8k SAL-5k SAL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

0.4
0.2
0 0

CAL SAL-8k SAL-5k SAL-2k

5

10

15

20

25

30

Thousands of Documents Reviewed

Figure 2: Continuous Active Learning versus Simple Active Learning using three different training-set sizes of uncertainty-sampled documents.

158

Matter
201 202 203 207
A B C D

CAL
6 11
6 11 11
8 4 18

SAL

Training Set Size

2K 5K 8K Ideal

237 7 10

7

34 12 14 12

43 7 10

6

55 23 13 13

210 42 12 12

119 10 11 10

5 8 10

5

60 54 53 18

SPL

Training Set Size

2K 5K 8K Ideal

284 331 164 56

47 29 26 26

521 331 154 99

103 50 36 35

502 326 204 85

142 41 21 20

9 8 10

7

55 38 37 37

Table 5: 75% Recall Effort for Primary Results (measured in terms of thousands of documents reviewed). Bold numbers reflect the least possible effort to achieve the target recall of 75%.

Matter
201 202 203 207
A B C D

CAL
6 11
6 11 11
8 4 18

CAL-seedran
6 12 637 12 15 10 4 19

SAL SAL-seedran

Training Set Size

Ideal

Ideal

7

8

12

12

6

614

13

13

12

15

10

10

5

4

18

19

SPL
Ideal 56 26 99 35 85 20 7 37

SPL-seedkey SPL-allkey

Training Set Size

Ideal

Ideal

36

43

23

20

26

16

26

16

79

66

19

39

6

9

28

34

Table 6: 75% Recall Effort for Primary and Supplemental Results (measured in terms of thousands of documents reviewed). Bold numbers reflect the least possible effort to achieve the target recall of 75%.

is that SAL achieves effort as low as CAL only for a specific recall value, which is easy to see in hindsight, but difficult to predict at the time of stabilization.
Table 5 illustrates the sensitivity of the SAL and SPL results to training-set size, and hence the difficulty of choosing the precise training-set size to achieve 75% recall with minimal effort.
6. SUPPLEMENTAL RESULTS
To assess the role of keyword versus random selection at various stages of the training process, we evaluated the following variants of the primary protocols: (i) CAL-seedran, in which the seed set was selected at random from the entire collection; (ii) SAL-seedran, in which the seed set was selected at random from the entire collection; (iii) SPLseedkey, in which the initial 1,000 training documents were the same keyword-selected seed set used for CAL and SAL in the primary protocols; and (iv) SPL-allkey, in which all training examples were selected at random from the results of the keyword seed query. 75% recall effort (with ideal training-set sizes, where applicable) for these variants, as well as the primary protocols, is shown in Table 6.
A comparison of the results for CAL and CAL-seedran shows that a random seed set generally yields the same or slightly inferior results to a keyword-selected seed set. In one case ­ Matter 203 ­ the random seed set fails spectacularly. The collection for this task has very low prevalence (0.25%), and the seed set of 1,000 random documents contained only two responsive documents, which were insufficient to "kick-start" the active-learning process. A comparison of the results for SAL and SAL-seedran shows the same

general effect, including the degraded performance caused by random seeding for Matter 203.
A comparison of the results for SPL and SPL-seedkey shows that, as for CAL and SAL, the use of keyword selection for the initial training set generally yields superior results to random selection. A comparison of the results for SPL and SPL-allkey shows that, with two exceptions, keyword selection for the entire training set is superior to random selection. However, a comparison of the results for SPL-seedkey and SPL-allkey shows neither to be consistently superior; in four cases, using keywords for only the initial training set was superior, and in four cases, using keywords for the entire training set was superior.
In summary, the use of a seed set selected using a simple keyword search, composed prior to the review, contributes to the effectiveness of all of the TAR protocols investigated in this study.
7. DISCUSSION
7.1 Random vs. Non-Random Training
The results presented here do not support the commonly advanced position that seed sets, or entire training sets, must be randomly selected [19, 28] [contra 11]. Our primary implementation of SPL, in which all training documents were randomly selected, yielded dramatically inferior results to our primary implementations of CAL and SAL, in which none of the training documents were randomly selected. While it is perhaps no surprise to the information retrieval community that active learning generally outperforms random training [22], this result has not previously

159

been demonstrated for the TAR Problem, and is neither well known nor well accepted within the legal community.
Perhaps more surprising is the fact that a simple keyword search, composed without prior knowledge of the collection, almost always yields a more effective seed set than random selection, whether for CAL, SAL, or SPL. Even when keyword search is used to select all training documents, the result is generally superior to that achieved when random selection is used. That said, even if passive learning is enhanced using a keyword-selected seed or training set, it is still dramatically inferior to active learning. It is possible, in theory, that a party could devise keywords that would render passive learning competitive with active learning, but until a formal protocol for constructing such a search can be established, it is impossible to subject the approach to a controlled scientific evaluation. Pending the establishment and scientific validation of such a protocol, reliance on keywords and passive learning remains a questionable practice. On the other hand, the results reported here indicate that it is quite easy for either party (or for the parties together) to construct a keyword search that yields an effective seed set for active learning.
The principal argument in favor of random selection appears to be the concern that non-randomly selected training examples are "less than representative of the entire population of relevant documents" [19, pages 260-261], and therefore might "bias" the learning method, resulting in the exclusion of certain classes of relevant documents. It is easy to imagine that such an effect might occur with SPL; however, it is more difficult to imagine how such a bias could persist through the CAL process.
There are situations in which a finite random sample used as a training set could exclude an identifiable population of relevant documents. By way of example, consider a collection consisting of 1,000,000 emails and 100,000 spreadsheets, of which 10,000 emails and 1,000 spreadsheets were relevant. A random training set consisting of 1,100 documents would contain about 1,000 emails, of which about 10 were relevant, and about 100 spreadsheets, of which, as likely as not, none would be relevant. A machine-learning method might well infer that spreadsheets generally were not relevant, thereby exhibiting a blind spot. Random training tends to be biased in favor of commonly occurring types of relevant documents, at the expense of rare types. Non-random training can counter this bias by uncovering relevant examples of rare types of documents that would be unlikely to appear in a random sample.
7.2 Continuous vs. Simple Active Learning
The differences between the CAL and SAL results arise, we believe, from differences in the design objectives underlying their training methods. The underlying objective of CAL is to find and review as many of the responsive documents as possible, as quickly as possible. The underlying objective of SAL, on the other hand, is to induce the best classifier possible, considering the level of training effort. Generally, the classifier is applied to the collection to produce a review set, which is then subject to manual review.6 The use of SAL raises the critical issues of (i) what is meant by the "best" classifier, and (ii) how to determine the point at which
6In some circumstances ­ which have not been considered in this study ­ the review set may be produced to the requesting party without any subsequent review.

the best classifier has been achieved (commonly referred to as "stabilization" in the context of TAR). In this study, we arbitrarily define "best" to minimize the total training and review effort necessary to achieve 75% recall, and sidestep the stabilization issue by affording SAL the luxury of an oracle that determines immediately, perfectly, and without cost, when stabilization occurs. In practice, defining and detecting stabilization for SAL (and also for SPL) is "[p]erhaps the most critical question attendant to the use of technologyassisted review for the production of documents" [19, page 263]. In practice, recall and precision of candidate review sets are typically estimated using sampling, and stabilization is deemed to occur when an aggregate measure, such as F1, appears to be maximized [17]. The choice of a suitable criterion for stabilization, and the cost and uncertainty of sampling to determine when that criterion has been met [3], are fundamental challenges inherent in the use of SAL and SPL that are not addressed in this study; instead, SAL and SPL have been given the benefit of the doubt.
With CAL, each successive classifier is used only to identify ­ from among those documents not yet reviewed ­ the next batch of documents for review. How well it would have classified documents that have already been reviewed, how well it would have classified documents beyond the batch selected for review, or how well it would have classified an independent, identically distributed sample of documents, is irrelevant to this purpose. Once it has served this narrow purpose, the classifier is discarded and a new one is created. Because the TAR process continues until as many as possible of the relevant documents are found, the nature of the of documents to which successive classifiers are applied drifts dramatically, as the easy-to-find relevant documents are exhausted and the harder-to-find ones are sought.
For SAL, where training is stopped well before the review is complete, we observed informally that uncertainty sampling was superior to relevance feedback, consistent with previously reported results in machine learning for text categorization [15]. For CAL, our results indicate relevance feedback to be superior.
7.3 When to Terminate the Review
Regardless of the TAR protocol used, the question remains: When to terminate the review? The answer hinges on the proportionality considerations outlined in (U.S.) Federal Rules of Civil Procedure 26(b)(2)(C) and 26(g)(1)(B)(iii), which, respectively, limit discovery if "the burden or expense of the proposed discovery outweighs its likely benefit, considering the needs of the case, the amount in controversy, the parties' resources, the importance of the issues at stake in the action, and the importance of the discovery in resolving the issues," and require that discovery be "neither unreasonable nor unduly burdensome or expensive, considering the needs of the case, prior discovery in the case, the amount in controversy, and the importance of the issues at stake in the action."
Whether the termination point is determined at stabilization (as for SAL and SPL), or deferred (as for CAL), eventually a legal decision must be made that a reasonable review has been conducted, and that the burden or expense of continuing the review would outweigh the benefit of any additional documents that might be found. The density of responsive documents discovered by CAL appears to fall off monotonically, thus informing the legal decision maker how

160

much effort would be necessary to find more documents; moreover, Cormack and Mojdeh [7] note that the scores of the responsive documents tend to a normal distribution, and that by fitting such a distribution to the scores, it is possible to estimate recall without resorting to sampling. That said, we leave to future research the issue of how best to determine when to stop.
7.4 Imperfect Training
It has been argued that the accuracy of the human review of the training set is critical, and that a "senior partner" [1, page 184], or even a bi-party committee, should review the training documents [2, page 7]. While the existing scientific literature indicates this concern to be overstated [6, 20, 27], our results further confirm that superior results can be achieved using a single, fallible reviewer. That said, a limitation of our evaluation toolkit is that our simulated reviewer always codes a given document the same way; a real reviewer would be influenced by factors such as the prevalence of responsive documents among those reviewed [23], the order in which the documents were reviewed, and any number of other human factors. We conjecture that these factors would tend to benefit CAL over the other protocols because: (i) the prevalence of responsive documents among those reviewed would be higher, especially at the outset of the review; (ii) similar documents would tend to be reviewed together by virtue of having similar scores; and (iii) the reviewer would gain early insight into the nature of responsive documents without having to wade through a haystack of random or marginal documents looking for an unfamiliar needle. Knowledge of the legally significant documents early in the review process is valuable in its own right. We leave it to future research to confirm or refute our conjecture.
7.5 Limitations
The prevalence of responsive documents in the eight review tasks varies from 0.25% to 3.92%, which is typical for the legal matters with which we have been involved. Others assert that these are examples of "low-prevalence" or "lowrichness" collections, for which TAR is unsuitable [19]. We suggest that such assertions may presuppose an SPL protocol [11], which is not as effective on low-prevalence datasets. It may be that SPL methods can achieve better results on higher-prevalence collections (i.e., 10% or more responsive documents). However, no such collections were included in this study because, for the few matters with which we have been involved where the prevalence exceeded 10%, the necessary training and gold-standard assessments were not available. We conjecture that the comparative advantage of CAL over SPL would be decreased, but not eliminated, for high-prevalence collections.
Our evaluation toolkit embodies a number of design choices, the effects of which remain to be explored. Our choices for feature engineering and learning algorithm are state of the art for text classification [5, chapter 11], and we have no indication that another choice would yield materially different results. We reprised most of the experiments in this study using logistic regression, instead of SVM, achieving similar results. A na¨ive Bayes classifier, on the other hand, achieved generally inferior results overall, but the same relative effectiveness among the protocols. A full exploration of feature engineering and classifier choices remains the subject of future research.

Finally, our use of a batch size of 1,000 was occasioned by efficiency considerations. In each of 100 iterations, we augmented the training set by 1,000 documents, trained the classifier, and scored every document in the collection. Each simulation required several hours of computation; the study required several weeks. For the CAL protocol only, we reran the simulations using a batch size of 100 ­ entailing ten times as much computation (i.e., several days per simulation) ­ and achieved slightly better results. The effect of even smaller batch sizes on the effectiveness of TAR protocols remains an open question.
8. CONCLUSION
While the mechanisms and efficacy of active machine learning are well known to the information retrieval community, the legal community has been slow to adopt such technologies, which could help address the growing volume of electronically stored information in (U.S.) legal proceedings. Much of the resistance, we submit, is due to lack of awareness of differences among TAR methods and protocols, and over generalization from one TAR method (typically, a variant of SPL) to all TAR.
Because SPL can be ineffective and inefficient, particularly with the low-prevalence collections that are common in ediscovery, disappointment with such tools may lead lawyers to be reluctant to embrace the use of all TAR. Moreover, a number of myths and misconceptions about TAR appear to be closely associated with SPL; notably, that seed and training sets must be randomly selected to avoid "biasing" the learning algorithm.
This study lends no support to the proposition that seed or training sets must be random; to the contrary, keyword seeding, uncertainty sampling, and, in particular, relevance feedback ­ all non-random methods ­ improve significantly (P < 0.01) upon random sampling.
While active-learning protocols employing uncertainty sampling are clearly more effective than passive-learning protocols, they tend to focus the reviewer's attention on marginal rather than legally significant documents. In addition, uncertainty sampling shares a fundamental weakness with passive learning: the need to define and detect when stabilization has occurred, so as to know when to stop training. In the legal context, this decision is fraught with risk, as premature stabilization could result in insufficient recall and undermine an attorney's certification of having conducted a reasonable search under (U.S.) Federal Rule of Civil Procedure 26(g)(1)(B).
This study highlights an alternative approach ­ continuous active learning with relevance feedback ­ that demonstrates superior performance, while avoiding certain problems associated with uncertainty sampling and passive learning. CAL also offers the reviewer the opportunity to quickly identify legally significant documents that can guide litigation strategy, and can readily adapt when new documents are added to the collection, or new issues or interpretations of relevance arise.
There is no reason to presume that the CAL results described here represent the best that can be achieved. Any number of feature engineering methods, learning algorithms, training protocols, and search strategies might yield substantive improvements in the future. The effect of review order and other human factors on training accuracy, and thus overall review effectiveness, may also be substantial.

161

Nevertheless, the experimental protocol, evaluation toolkit, and results presented here provide a foundation for further studies to investigate these and other possible approaches to improve the state of the art in TAR for e-discovery.
9. ACKNOWLEDGEMENT
Cormack's research is supported by a Discovery grant and a Research Tools and Instruments grant from the Natural Sciences and Engineering Research Council of Canada.
10. REFERENCES
[1] Da Silva Moore v. Publicis Groupe, 287 F.R.D. 182, S.D.N.Y., 2012.
[2] Case Management Order: Protocol Relating to the Production of Electronically Stored Information ("ESI"), In Re: Actos (Pioglitazone) Products Liability Litigation, MDL No. 6:11-md-2299, W.D. La., July 27, 2012.
[3] M. Bagdouri, W. Webber, D. D. Lewis, and D. W. Oard. Towards minimizing the annotation cost of certified text classification. In Proceedings of the 22nd ACM International Conference on Information and Knowledge Management, pages 989­998, 2013.
[4] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. de Vries, and E. Yilmaz. Relevance assessment: are judges exchangeable and does it matter? In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 667­674, 2008.
[5] S. Bu¨ttcher, C. L. A. Clarke, and G. V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, 2010.
[6] J. Cheng, A. Jones, C. Privault, and J.-M. Renders. Soft labeling for multi-pass document review. ICAIL 2013 DESI V Workshop, 2013.
[7] G. V. Cormack and M. Mojdeh. Machine learning for information retrieval: TREC 2009 Web, Relevance Feedback and Legal Tracks. The Eighteenth Text REtrieval Conference (TREC 2009), 2009.
[8] M. R. Grossman and G. V. Cormack. Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Richmond Journal of Law and Technology, 17(3):1­48, 2011.
[9] M. R. Grossman and G. V. Cormack. Inconsistent responsiveness determination in document review: Difference of opinion or human error? Pace Law Review, 32(2):267­288, 2012.
[10] M. R. Grossman and G. V. Cormack. The Grossman-Cormack glossary of technology-assisted review with foreword by John M. Facciola, U.S. Magistrate Judge. Federal Courts Law Review, 7(1):1­34, 2013.
[11] M. R. Grossman and G. V. Cormack. Comments on "The Implications of Rule 26(g) on the Use of Technology-Assisted Review." Federal Courts Law Review, 1, to appear 2014.
[12] B. Hedin, S. Tomlinson, J. R. Baron, and D. W. Oard. Overview of the TREC 2009 Legal Track. The Eighteenth Text REtrieval Conference (TREC 2009), 2009.

[13] C. Hogan, J. Reinhart, D. Brassil, M. Gerber, S. Rugani, and T. Jade. H5 at TREC 2008 Legal Interactive: User modeling, assessment & measurement. The Seventeenth Text REtrieval Conference (TREC 2008), 2008.
[14] D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a finite universe. Journal of the American Statistical Association, 47(260):663­685, 1952.
[15] D. D. Lewis and W. A. Gale. A sequential algorithm for training text classifiers. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3­12, 1994.
[16] D. W. Oard and W. Webber. Information retrieval for e-discovery. Information Retrieval, 6(1):1­140, 2012.
[17] Y. Ravid. System for Enhancing Expert-Based Computerized Analysis of a Set of Digital Documents and Methods Useful in Conjunction Therewith. United States Patent 8527523, 2013.
[18] H. L. Roitblat, A. Kershaw, and P. Oot. Document categorization in legal electronic discovery: Computer classification vs. manual review. Journal of the American Society for Information Science and Technology, 61(1):70­80, 2010.
[19] K. Schieneman and T. Gricks. The implications of Rule 26(g) on the use of technology-assisted review. Federal Courts Law Review, 7(1):239­274, 2013.
[20] J. C. Scholtes, T. van Cann, and M. Mack. The impact of incorrect training sets and rolling collections on technology-assisted review. ICAIL 2013 DESI V Workshop, 2013.
[21] F. Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1­47, 2002.
[22] B. Settles. Active learning literature survey. University of Wisconsin, Madison, 2010.
[23] M. D. Smucker and C. P. Jethani. Human performance and retrieval precision revisited. In Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 595­602, 2010.
[24] S. Tomlinson. Learning Task experiments in the TREC 2010 Legal Track. The Nineteenth Text REtrieval Conference (TREC 2010), 2010.
[25] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5):697­716, 2000.
[26] W. Webber, D. W. Oard, F. Scholer, and B. Hedin. Assessor error in stratified evaluation. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management, pages 623­632, 2010.
[27] W. Webber and J. Pickens. Assessor disagreement and text classifier accuracy. In Proceedings of the 36th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 929­932, 2013.
[28] C. Yablon and N. Landsman-Roos. Predictive coding: Emerging questions and concerns. South Carolina Law Review, 64(3):633­765, 2013.

162

ReQ-ReC: High Recall Retrieval with Query Pooling and Interactive Classification

Cheng Li,1, Yue Wang,2, Paul Resnick1, Qiaozhu Mei1,2
1School of Information, University of Michigan, Ann Arbor, MI, USA 2Department of EECS, University of Michigan, Ann Arbor, MI, USA
{lichengz, raywang, presnick, qmei}@umich.edu

ABSTRACT
We consider a scenario where a searcher requires both high precision and high recall from an interactive retrieval process. Such scenarios are very common in real life, exemplified by medical search, legal search, market research, and literature review. When access to the entire data set is available, an active learning loop could be used to ask for additional relevance feedback labels in order to refine a classifier. When data is accessed via search services, however, only limited subsets of the corpus can be considered--subsets defined by queries. In that setting, relevance feedback [17] has been used in a query enhancement loop that updates a query.
We describe and demonstrate the effectiveness of ReQReC (ReQuery-ReClassify), a double-loop retrieval system that combines iterative expansion of a query set with iterative refinements of a classifier. This permits a separation of concerns: the query selector's job is to enhance recall, while the classifier's job is to maximize precision on the items that have been retrieved by any of the queries so far. The overall process alternates between the query enhancement loop (to increase recall) and the classifier refinement loop (to increase precision). The separation allows the query enhancement process to explore larger parts of the query space. Our experiments show that this distribution of work significantly outperforms previous relevance feedback methods that rely on a single ranking function to balance precision and recall.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Relevance Feedback
General Terms
Algorithms, Experimentation
Keywords
Relevance Feedback; Query Expansion; Active Learning
Cheng Li and Yue Wang contributed equally to this work.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609618.

1. INTRODUCTION
We are witnessing an explosive growth of text data in many fields, including millions of scientific papers, billions of electronic health records, hundreds of billions of microblog posts, and trillions of Web pages. Such a large scale has created an unprecedented challenge for practitioners to collect information relevant to their daily tasks. Instead of keeping local collections of data related to these tasks, many users rely on centralized search services to retrieve relevant information. These services, such as Web search engines (e.g., Google), literature retrieval systems (e.g., PubMed), or microblog search services (e.g., Twitter search API, Topsy) typically return a limited number of documents that are the most relevant to a user-issued query. These existing retrieval systems are designed to maximize the precision of top-ranked documents; they are good at finding "something relevant," but not necessarily everything that is relevant.
We focus on scenarios where a user requires a high recall of relevant results in addition to high precision. Such scenarios are not uncommon in real life, exemplified by social search, medical search, legal search, market research, and literature review. For example: a social analyst needs to identify all the different posts in which a rumor spreads in order to reconstruct the diffusion process and measure the influence of the rumor; a physician needs to review all the patients that satisfy certain conditions to select cohorts for clinical trials; an attorney needs to find every piece of evidence related to her case from documents that are under legal hold; a scientist does not want to miss any piece of prior work that is related to his ongoing research. We denote all these tasks generically as "high-recall" retrieval tasks.
Finding a needle in a haystack is hard; finding all the needles in a haystack is much harder. Existing retrieval systems do not naturally meet this type of information need. To conduct a comprehensive literature review using a search engine, we have to submit many alternative queries and examine all the results returned by each query. Such a process requires tremendous effort of the user to both construct variations of queries and examine the documents returned.
This high-precision and high-recall task becomes substantially harder as the collection grows large, making it impossible for the user to examine and label all the documents in the collection, and impractical even to label all the documents retrieved by many alternative queries. In some contexts such as e-discovery, a computer-assisted review process has been used that utilizes machine learning techniques to help the user examine the documents. Such a process typically casts high-recall retrieval as a binary classification task. At the

163

beginning, the user is required to label a small sample of documents. A classifier trained using these labeled documents then takes over and predicts labels for other documents in the collection. An active learning loop can be used to ask for additional relevance labels in order to refine the classifier. These methods, however, require that the user has access to the full collection of documents and that it is feasible to execute her classifier on all the documents.
In other scenarios, the users either do not own the collection or it is too large, so they can only access documents in the collection through an external search service. This makes it unrealistic to either examine or classify the entire collection of documents. Instead, only limited subsets of the document corpus can be considered, subsets defined by queries.
Existing retrieval systems are not tuned for high-recall retrieval on the condition of limited access to the data via search services. In most cases, a system only aims to maximize the precision in the documents that are retrieved by the current query. Relevance feedback has been used in a query enhancement loop that updates a query. Many search engines provide services to collect explicit and/or implicit feedback from the users or to suggest alternative queries to the users. These practices typically generate a new query that replaces the old one, which is expected to improve both precision and recall. Once a new query is issued, the results retrieved by the old queries are forgotten, unless they are manually harvested by the user.
We study a novel framework of retrieval techniques that is particularly useful for high-recall retrieval. The new framework features a ReQ-ReC (ReQuery-ReClassify) process, a double-loop retrieval system that combines iterative expansion of a query set with iterative refinements of a classifier. This permits a separation of concerns, where the query generator's job is to enhance recall while the classifier's job is to maximize precision on the items that have been retrieved by any of the queries so far. The overall process alternates between the query expansion loop (to increase recall) and the classifier refinement loop (to increase precision). The separation of the two roles allows the query enhancement process to be more aggressive in exploring new parts of the document space: it can explore a non-overlapping portion of the corpus without worrying about losing the veins of good documents it had found with previous queries; it can also use queries that have lower precision because the classifier will weed out the misses in a later stage. Our experiments show that this distribution of work significantly outperforms previous relevance feedback methods that rely on a single ranking function to balance precision and recall. The new framework also introduces many opportunties to investigate more effective classifiers, query generators, and human-computer interactive algorithms for labeling subsets, and especially to investigate what combinations work best together.
Unlike Web search engines that target users who have real-time, ad hoc information needs, the ReQ-ReC process targets users who care about the completeness of results and who are willing to spend effort to interact with the system iteratively and judge many (but not all) retrieved documents. The process has considerable potential in applications like social media analysis, scientific literature review, e-discovery, patent search, medical record search, and market investigation, where such users can be commonly found.

The rest of the paper is organized as follows. We discuss related work in Section 2. Section 3 gives an overview of the ReQ-ReC double-loop framework and its key components. Section 4 describes several instantiations of the framework. Section 5 provides a systematic evaluation of the proposed methods. Finally, we conclude the paper in Section 6.
2. RELATED WORK
The ReQuery-ReClassify framework integrates and extends two well-established "human-in-the-loop" mechanisms: relevance feedback in information retrieval, and active learning in text classification.
Relevance feedback was shown long ago to be effective for improving retrieval performance [17]. In a feedback procedure, the retrieval system presents the top-ranked documents to the user and collects back either explicit judgments of these documents or implicit feedback implied by certain actions of the user [9, 19]. The system then learns from the collected feedback and updates the query. The new query reflects a refined understanding of the user's information need [15, 28], which improves both precision and recall in the next round of retrieval. Even without real user judgments, retrieval performance may still benefit from simply treating the top-ranked documents as relevant, which is known as a process of pseudo relevance-feedback [1].
In a search session, relevance feedback can be executed for multiple rounds. Harman [8] studied multiple iterations of relevance feedback, and found that retrieval performance is greatly improved by the first two to three iterations, after which the improvements became marginal. Multiple iterations of relevance feedback have received more attention in content-based image retrieval [3, 16, 30].
In complicated search tasks, the user is often involved in a search session consisting of a series of queries, clickthroughs, and navigation actions. Session-based retrieval aims at learning from these signals in order to better understand the user's information need, thus improving the relevance of results when the user issues the next query [19, 14]. Instead of improving the performance of the next query, ReQ-ReC aims to maximize the recall of the results collectively retrieved by all the queries in the search session.
Like traditional iterative relevance feedback, the ReQReC process also adopts multiple iterations of user interaction. Indeed, as shown in Section 3, iterative relevance feedback is a special case instantiation of the ReQ-ReC framework. Instead of replacing the old query with a new query, however, ReQ-ReC can accumulate documents retrieved by any of the queries issued so far. By doing this, rather than optimizing both precision and recall through the choice of a single query, we place the burden of maximizing precision on a classifier, and new queries can be dedicated to improving only recall.
When it is feasible to process the entire collection of documents, the problem of high-recall retrieval can be cast as a binary classification problem where the positive class captures documents that are relevant to the information need and the negative class captures the rest. The practice of relevance feedback essentially becomes an active learning process, in which the system iteratively accumulates training examples by selecting documents and asking the user for labels [18]. This strategy is commonly used in computer-assisted reviews for e-discovery, often referred to as the process of `predictive coding' [13]. Different active learning algorithms

164

use specific strategies for selecting the documents to label, many of which attempt to maximize the learning rate of a `base' classifier with limited supervision [18]. For text classification, a popular choice of such a `base' classifier is the support vector machine (SVM) [2]. Using SVM, a variety of document selection strategies have been explored. Tong and Koller [23] proposed to select documents closest to the decision hyperplane in order to rapidly shrink the version space and reduce model uncertainty. In contrast, Drucker et al. [4] selected documents with highest decision function values to avoid probing the user with too many non-relevant documents. Xu et al. [27] mixed these two strategies and achieved better retrieval performance.
Like active learning, the ReQ-ReC process also trains a binary classifier. The major difference is that ReQ-ReC does not require knowledge about the entire document collection and thus does not classify all documents. Instead, it starts from a limited subset defined by the original query and actively expands the space. This is a huge gain, as text classification and active learning are usually computationally prohibitive for modern IR collections containing a large number of documents. Indeed, previous studies that apply active learning to retrieval can only evaluate their approaches using moderate-scale collections (such as the 11,000-documents Reuters collections used in [4] and [27]), or only focus on the documents retrieved by one query (top 100 documents in [26] and top 200 in [22]). Given its big advantage in efficiency, the ReQ-ReC process could potentially provide a new treatment for active learning, especially when the data collection is large and the positive class is very rare.
The idea of active learning has also been applied to relevance feedback for retrieval. Shen and Zhai [20] studied active feedback, where the system actively selects documents and probes the user for feedback instead of passively presenting the top ranked documents. It is shown that selecting diverse top-ranked documents for labeling is desirable, since it avoids asking for labels on similar documents and thus accelerates learning. Xu et al. [26] improved this heuristic by jointly considering relevance, diversity, and density in selected documents. Both techniques exploit density information among top-ranked documents, and select representative ones for feedback. Recently, Tian and Lease [22] combined uncertainty sampling (Simple Margin) and density-based sampling (Local Structure) in iterative relevance feedback to minimize user effort in seeking several to many relevant documents. The difference between our work and theirs is articulated by the difference between the ReQReC process and relevance feedback described above: the addition of a classifier and use of results from all queries allows more aggressive exploration of alternative queries.
3. THE REQ-REC FRAMEWORK
In this section, we introduce the general ReQuery-ReClassify (ReQ-ReC) framework, including its key components. Specific instantiations of the framework will be discussed in the next section. The basic idea of the framework is to distribute the burden of maximizing both the precision and recall to a set of queries and a classifier, where the queries are responsible for increasing the recall of relevant documents retrieved and the classifier is responsible for maximizing the precision of documents retrieved collectively by all of the queries in the set. The framework features a double-loop mechanism: the inner-loop classifies the retrieved documents, actively

collects user feedback, and improves the classifier (ReClassify); the outer-loop generates new queries (ReQuery), issues API calls, and iteratively adds newly retrieved documents into the workset. In the rest of the paper, we refer to the framework as "ReQ-ReC" or "double-loop" interchangeably.
3.1 The Double Loop Process
Query set

Query Query Generator

outer-loop

Y

Is classifier

N

stable enough?

Classifier f : D {0, 1}

inner-loop

Search Engine
Retrieved documents D
Document Selector Documents to label

Classifier Learner

Labeled set

Figure 1: ReQ-ReC framework

The ReQ-ReC framework can be viewed as a double-loop review process, as illustrated in Figure 1. The process maintains a set of queries, a pool of retrieved documents, and a binary classifier. With an initial query composed by the user, the system retrieves an initial set of documents using a search service. An inner-loop starts from there, in which the system iteratively presents a small number of documents (e.g., 10) selected from the current pool of retrieved documents to the user and asks her to label them as either relevant or not. The classifier is consequently updated based on the accumulated judgments of the user, which is then used to reclassify the pool of documents. After a few iterations of the inner-loop, the the classifier's predictions stabilize. At this point, the inner-loop will suspend. The system then proposes to add a new query to the query set, aiming to retrieve more relevant documents from the collection. Upon the approval--and possible edits--of the user, the system will retrieve a new set of documents using the new query, and merge them into the pool of retrieved documents. The requery process makes up one iteration of the outer-loop of the framework. After new documents are retrieved and added into the pool, the system starts a new inner-loop and continues to update the classifier left from the last iteration. The whole review process will end when no more relevant documents can be retrieved by a new query or when the user is satisfied.
Another way to look at the framework is to imagine a search process in the information space (e.g. a vector space of documents and queries), as illustrated in Figure 2. The system interacts with the user as it navigates through the information space, aiming to delineate a manifold that contains as many relevant documents and as few non-relevant documents as possible. Each query can only reveal a small region of the information space that surrounds it. The "first guess" on such a manifold is, of course, the region surround-

165

q0!

qi!

(a) Initial retrieval

(b) Inner-loop

qi+1! qi!
(c) Outer-loop

query! retrieved unlabeled document ! labeled relevant document ! labeled non-relevant document ! query update path! retrieved document pool! classifier decision boundary! classifier decision boundary, updated !

Figure 2: A double-loop process of search in the information space. (a) Each query only retrieves its surrounding region under inspection. (b) The innerloop updates a classifier that refines the boundary between relevant and non-relevant documents. (c) The outer-loop expands the subspace which includes more relevant documents.

ing the initial query. A classifier clarifies the boundary of the manifold (to maximize precision), which is iteratively refined with newly labeled data points selected from the revealed regions. To explore other regions in the space so as to expand the relevant manifold (to maximize recall), the system will estimate a promising direction and will make a new query to move in that direction into the uncharted space. This new region and all previously unveiled regions are combined as the current search space, in which the system continues to refine the boundary of the relevant manifold. The search process will end if the relevant manifold stops expanding, or if the user decides to terminate early.
From this perspective, each query contributes a new region to the search space without giving up any already discovered regions. Such a pure "expansion" of the search space will include many non-relevant documents, but the classifier is able to filter the non-relevant documents at the end and recover the true boundary of the relevant manifold. By contrast, in a relevance feedback procedure, every new query will "redefine" the search space as the region surrounding the new query. Given a good query, this region indeed contains fewer non-relevant documents than our "expanded" search space (i.e., achieves a higher precision), but it is also likely to contain fewer new relevant documents. In relevance feedback, the challenge is to find a new query that both retrieves the relevant documents from the old query and also retrieves new ones. In ReQ-ReC, the challenge is simply to find a query that retrieves new relevant documents.
3.2 Anatomy of the ReQ-ReC Framework
Given the high-level intuitions of the ReQ-ReC framework, we now discuss the key components in the doubleloop. To facilitate the discussion, we introduce the notations in Table 1 and summarize the framework in Algorithm 1.

Table 1: Notations of the double-loop process

D

index of the document collection

qi

the i-th query submitted

Dq

the union of all unjudged documents

retrieved by the set of queries {qi}

Ds

documents selected for user judgments

Dl retrieve(D, qi)

set of documents labeled already a retrieval function that returns a subset of documents from index D by query qi

A R trainA(Dq, Dl)

model for document selection model for relevant/non-rel classification function to train/update A using labeled and unlabeled documents

trainR(Dq, Dl) function to train/update R using labeled and unlabeled documents

selectK(Dq, A) function to select K documents using the document selection model

label(Ds)

function to obtain relevance labels of Ds

predict(Dq, R) function to predict the relevance labels and rank unlabeled documents

query({qi}, ·) function to generate a new query

3.2.1 Search
The ReQ-ReC framework assumes neither ownership nor full access to the document collection, but instead relies on a standard search service to retrieve documents from the index. The retrieval service's ranking function can use any reasonable retrieval model that takes the input of a query qi and outputs a certain number of ranked documents from the index (e.g., using a vector space model, a language modeling approach, or a boolean retrieval model). In most cases, the user has no knowledge about the algorithm that is employed by the external search service. In that case, the retrieval function is treated as a black box in the framework.
After each search process the retrieved documents will be merged into the pool of unlabeled documents Dq, which expands the workset for document selection and classification.

Algorithm 1 The double-loop process
Input: Initial query q0, index of document collection D Output: A set of labeled documents Dl and a set of un-
judged documents in Dq with system predicted labels.

1: Dq  

2: Dl  

3: repeat // outer loop

4: Dq  retrieve(D, qi)  Dq

5: repeat // inner loop

6:

if Dl ==  then

7:

Ds  selectK(Dq)

8: else

9:

A  trainA(Dq, Dl)

10:

Ds  selectK(A, Dq)

11: end if

12:

Dl  Dl  label(Ds)

13:

Dq  Dq - Ds

14:

R  trainR(Dq, Dl)

15:

predict(R, Dq)

16: until meet stopping criteria for inner loop

17: qi+1  query({qi}, Dq, Dl, A, R)

18: until meet stop criteria for outer loop

166

3.2.2 Document Selection
In every iteration of the inner-loop, during steps 6-10 of the algorithm the system selects K (e.g., 10) documents Ds from the pool of retrieved documents that are yet unlabeled, Dq, and asks the user for judgments. At the beginning of the double-loop process, where there are no judged documents, this process can simply return the top documents ranked by the retrieval function, select a more diverse set of documents through an unsupervised approach, or even randomly sample from Dq. Once labeled documents have been accumulated, the process is able to select documents based on an active learning strategy. Such a process aims to maximize the learning rate of the classifier and thus reduce the user's effort on labeling documents.
3.2.3 Classification
Given an accumulated set of labeled documents, the classification component learns or updates a binary classifier (i.e., R) at step 14 and reclassifies documents from Dq at step 15. Any reasonable classifier can be applied here.
In many high-recall retrieval tasks such as medical record search, it is important to find all patients that "match" certain conditions, but it is not necessary to rank the records identified as relevant [7]. In those cases, the labels of documents in Dq can be directly predicted by the classifier. In cases where ranking is desired, documents in Dq and Dl can be ranked/reranked using either the confidence values or the posterior probabilities output by the classifier, or by using an alternative machine learning method such as a regression or learning-to-rank model.
3.2.4 Query Expansion
When the classifier appears to be achieving a stable precision on the current workset of documents Dq, the system proceeds to expand Dq in order to increase the recall. This is done through constructing a new query (step 17) and retrieving another set of documents through the search service. Any reasonable query expansion method can be applied here, including the classical relevance feedback methods such as Rocchio's [15] or model-based feedback [28]. Other query reformulation methods can also be applied, such as synonym expansion [24] and semantic term matching [5].
3.2.5 Stop Criteria
Stop criteria of the inner-loop: new labels stop being requested when either of the following conditions is met:
· The performance of the classifier converges. The system correctly predicts the user's labels of a new batch of documents Ds and, after adding those labels, there is no evident change in the classifier's predictions.
· The user runs out of energy or patience.
Stop criteria of the outer-loop: new queries stop being submitted when either of the following conditions is met:
· New queries no longer pick up new relevant documents. This can be assessed heuristically by running the existing classifier on a new result set, or can be verified by running the inner loop again to check whether any new positive documents are identified.
· The user runs out of energy or patience.

4. INSTANTIATIONS OF REC-REQ
The key components of the general ReQ-ReC framework, document selection, classification, and query expansion can be instantiated in many ways. To illustrate the power of the framework, we describe five instantiations, beginning with iterative relevance feedback as a degenerate form and progressively substituting elements that take greater advantage of the broader framework. Section 5 will provide performance comparisons of these instantiations.

4.1 Iterative Relevance Feedback
Interestingly, an iterative relevance feedback process can be interpreted as a special case of the ReQ-ReC framework, if both the classification component and the document selection component simply adopt a ranking function that is based on the current query, qi. More specifically, define R to classify a document as relevant if it is in retrieve(D, qi), and define A to always select the next highest ranked unlabeled item from retrieve(D, qi). There is no difference in whether the results retrieved by the previous queries are kept in the document pool Dq or not, if the results are eventually ranked based on the last query, qi.
Note that many query updating methods (in the context of relevance feedback) can be applied to generate the new query at each iteration. To establish a baseline for performance comparison, we choose Rocchio's method [15], by which the next query is selected according to Equation 1:

1

1

qi = q0 +  |Dr|

dj -  |Dnr|

dk, (1)

dj Dr

dk Dnr

where q0 is the original query vector, Dr and Dnr are the set of known relevant and nonrelevant documents, and , , and  are parameters. The basic idea of Rocchio's method is to learn a new query vector from documents labeled as positive or negative, and then interpolate it with the original query vector. When the parameters are well tuned, this achieves performance comparable to alternatives such as model-based feedback [28] and negative feedback [25].

4.2 Passive
The next two instantiations modify the relevance feedback process by introducing a separate classifier, R, rather than using the retrieval function as a degenerate classifier. This classifier is involved to maximize the precision of labels for Dq. Here, keeping the documents retrieved by previous queries does make a difference, because R will operate at the end to rank all of the results from all of the queries.
Any machine learning-based classifier, as well as any reasonable selection of features, can be used to identify relevant documents in Dq. We adopt the support vector machine (SVM) [2] with unigram features and linear kernel. In cases where a ranked list of documents is desired, documents in Dq are ranked by the score of the decision function wT x + b output by linear SVM.
We call this second instantiation of ReQ-ReC Passive. It is passive in the sense that the classifier is not used to control the interactive process with the user; we still choose the topranked documents for labeling and use Rocchio's method of query expansion, as in our iterative RF instantiation. By comparing the performance of passive and the Iterative RF baseline, we can determine the effect of the classifier acting solely as a post-hoc reranking function.

167

4.3 Unanchored Passive
Note that in Rocchio updating, the parameter that interpolates the new query vector with the original query is quite sensitive. This is because when one relies on the query to maximize both precision and recall, the expansion has to be conservative so that the new query does not drift too far from the original query. When the burden of maximizing precision is transferred from the query to the classifier, we anticipate that this interpolation should become less critical. To test this, we introduce another simple instantiation by removing the original query vector (i.e., the q0 component in Equation 1) from Rocchio, by setting  = 0. Note that this is a rather extreme case for test purposes. In reality, keeping closer to the original query may still be important even for the purpose of increasing recall. We call this instantiation Unanchored Passive, because the updated queries are no longer anchored to the initial query.
4.4 Active
Next, we consider an instantiation of RecQ-ReC that makes use of the classifier to select documents for labeling in the inner loop. As before, we train the classifier using SVM. We select documents for labeling using uncertainty sampling [23], a simple active learning algorithm that selects examples closest to the decision hyperplane learned by the classifier. In each inner-loop iteration, we present to the user ten documents that are the most uncertain by the current classifier. Specifically, five are chosen from each side of the hyperplane. We call this instantiation Active because the classifier is active in choosing which documents to label.
Note that after the very first search process, the system has no labeled documents in the pool. A classifier cannot be trained and thus the uncertainty sampling cannot be applied. At this cold start, we simply select the top 10 documents returned by the search service as the first batch of documents to request user judgments.
As uncertainty-based active learning gradually refines the decision boundary of the classifier, every new query to the search service may affect its performance. This is because a new query expands the pool of documents Dq with newly retrieved documents, which might dramatically change the distribution and the manifold of data in the search space. At this point, instead of gradually refining the old decision boundary, the classifier may need a bigger push to quickly adapt to the new distribution of data and approach the new decision boundary. In other words, it is important for the classifier to quickly explore the newly retrieved documents. Therefore, in the first inner-loop iteration after each new query brings back new documents, we select top ranked documents for labeling instead of the most uncertain ones. Uncertain ones are picked in the following inner-loop iterations.
4.5 Diverse Active
The final instantiation we consider modifies the query expansion algorithm used in the Active instantiation. Previously, we considered an unanchored version of Rocchio's method of selecting the next query. Here, we consider a different modification of Rocchio's method.
To maximize recall, we naturally want a new query to retrieve as many relevant documents as possible. Even more importantly, these relevant documents should overlap as little as possible with the documents retrieved by previous queries. In other words, a new query should retrieve as many new relevant documents as possible.

Our idea is inspired by the theory of "weak ties" in sociology [6]. While strong ties trigger social communication, weak ties can bring in novel information. If we think of the top-ranked documents in a retrieved list as "strong ties" to the query, we can think of the lower-ranked documents as "weak ties." We thus exploit documents that are judged as relevant, but ranked lower in the list returned by the search service. These documents are likely to act as bridges to expand the search space into other clusters of relevant documents.
Are there many such documents? In a relevance feedback process, there might be few, as the user always labels the top-ranked documents. In a ReQ-ReC process that actively selects documents, however, documents ranked lower by the retrieval function are more likely to be viewed and judged by the user.
In Equation 1, instead of using all relevant documents Dr, we use its subset Drl, which includes the documents that are judged as relevant but ranked low by the original retrieval function. We employ a simple criterion to determine which documents should be included in Drl. For each document d, we maintain its rank returned by the retrieval function, denoted as rd. If the document has been retrieved by multiple queries in the past, its highest rank in those retrieved lists is kept. Let rl be the lowest rank rd of all the documents in Dr. We include documents that are ranked lower than rl/2 in Drl. This leads to inclusion in the next query of terms from relevant documents that were not highly weighted in previous queries. Since this method aims to diversify new queries, while still using the classifier to actively choose documents for labeling, we refer to this method as Diverse Active.
5. EXPERIMENTS
In this section, we present empirical experiments to evaluate the effectiveness of the ReQ-ReC framework and its instantiations. We start with a description of the data sets, metrics, and methods included in the comparisons.
5.1 Data Sets
There are several criteria for selecting the right data sets for evaluating ReQ-ReC. Ideally, the data sets should be large enough and standard search APIs should exist. A representative set of queries should also exist, and each query should have a reasonable number of relevant documents in the data set. To avoid the high variance of real-time user judgments and to facilitate comprehensive and fair comparisons, we use existing judgments for each query to `automate' the actual user feedback in the process. The same approach is used in most existing work on relevance feedback (e.g., [8, 20, 25]). We therefore require that many relevant judgments exist for each query.
We first select four large scale TREC data sets, the data sets used in TREC-2012 Microblog Track (MB12) [21], TREC2013 Microblog Track (MB13)1, the TREC-2005 HARD Track (HARD), and the TREC-2009 Web Track (ClueWeb092, category A)3. These data sets normally provide 50­60 queries and 500­1,000 relevant judgments for a query.
1https://github.com/lintool/twitter-tools/wiki/ 2http://lemurproject.org/clueweb09/ 3http://trec.nist.gov/data/web09.html

168

Note that there is a natural deficiency of using TREC judgments for the evaluation of a high-recall task, simply because not all documents in a TREC data set have been judged. Instead, judgments are provided for only a pool of documents that consist of the top-ranked documents submitted by each participating team. In many cases, only a sample of the pool is judged. Therefore, it is likely that many relevant documents for a query are actually not labeled in the TREC provided judgments. This creates a problem for a `simulated' feedback process--when the system requests the label of a document, the label may not exist in the TREC judgments. It is risky to label that document either as relevant or as irrelevant, especially because mislabeling a relevant documents as irrelevant may seriously confuse a classifier. In such situations, we ignore that document and fetch the next document available. The same treatment has been used in the literature [20]. When measuring the performance of a retrieved list, however, we follow the norm in the literature and treat a document not judged by TREC as negative.

Table 2: Basic information of data sets

20NG HARD MB12 MB13 ClueWeb09

#docs 18,828 1,033,461 15,012,766 243,000,000 503,903,810

avg dl 225 353 19 14 1570

#topics(IDs) 20 categories 50 (303-689) 59 (51-110) 60 (111-170) 50 (1-50)

#qrels 18,828 37,798 69,045 71,279 23,601

* HARD has non-consecutive topic IDs. Topic 76 of MB12 has no judgment hence is removed.

To better understand the behavior of ReQ-ReC, it is desirable to include a data set that is fully judged, even though a large data set like that is rare. Therefore, we include the 20newsgroup data set (20NG) [11] for this purpose. As every document belongs to one of the 20 topics, we use the titles of 20 topics as the queries, following the practice in [4]. For words that are abbreviated in the topic titles, we manually expand them into the normal words. For example, "rec" is converted to "recreation," and "autos" to "automobiles." Although it is feasible to apply a classifier to the entire 20NG data set, we only access the data using rate-limited retrieval functions. The statistics of all five data sets in our experiments are presented in Table 2.
Both the 2013 Microblog Track4 and the ClueWeb095 provide official search APIs, which are implemented using the Dirichlet prior retrieval function (Dirichlet) [29]. For other data sets, we maintain a similar search service using Lucene,6 which also implements the Dirichlet prior function. Documents are tokenized with Lucene's StandardAnalyzer and stemmed by the Krovetz stemmer [10]. No stopwords are removed.
5.2 Metrics
Many popular metrics for retrieval performance, such as precision@K and NDCG, are not suitable for high-recall tasks. We use two standard retrieval metrics that depend more on recall, namely the mean average precision (MAP)
4https://github.com/lintool/twitter-tools/wiki/ TREC-2013-API-Specifications 5http://boston.lti.cs.cmu.edu/Services 6http://lucene.apache.org/

[12] and the R-precision (R-Prec) [12]. R-precision measures the precision at the R-th position for a query with R relevant judgments. The R-th position is where precision equals recall. To increase R-precision, a system has to simultaneously increase precision and recall. For each query, we use the top 1,000 relevant documents (either labeled or predicted) to compute the measures.
When measuring performance, we include documents that the user labeled during the process. This is because a highrecall retrieval task is successful when more relevant documents can be found, whether they are actually judged by the user or predicted by the system. If an interactive process does a good job of presenting more relevant documents to the user, it should not be punished by having those documents excluded from the evaluation. In all methods included in comparative evaluation, we put the documents judged as relevant at the top of the ranked list, followed by those predicted to be relevant using R.
5.3 Methods
We summarize all baseline methods and ReQ-ReC instantiations included in our evaluation in Table 3. The most important baseline we are comparing with is the iterative relevance feedback as described in Section 4.1, in which a new query is expected to maximize both precision and recall. We then include four instantiations of the ReQ-ReC framework, as described in Section 4.
In Passive and Unanchored Passive, we employed a negative form of pseudo-relevance feedback: the lowest ranked 1,000 documents retrieved by the final query are treated as negative examples to train the classifier. The positive examples for training came from the actual judgments.
5.4 Parameters
For the MP13 and ClueWeb09 datasets, we used the official search APIs, which returned, respectively, 10,000 and 1,000 documents per query. For the three data sets without official search APIs, the parameter of the Dirichlet prior µ for the base retrieval function was tuned to maximize the mean average precision and each query returned the top 2,000 matching documents.
To obtain the strongest baseline, we set the parameters of Rocchio to those that maximize the mean average precision of a relevance feedback process using 10 judgments. We fix  to be 1 and conduct a grid search on the other two. For ClueWeb09, we set the parameters according to the recommendation in [12] as the rate limits of the API prevent us from tuning the parameters. We do not further tune the parameters in the ReQ-ReC methods even though the optimal parameters for the baseline may be suboptimal for ReQReC. The values of all the parameters used are shown in Table 4. In all our experiments, we also use the default parameter of SVM (c = 1). We stop the inner-loops when SVM confidence value produces stable ranking of Dq, i.e., Spearman's rank correlation coefficient of previous and current rankings of Dq is above 0.8 for two consecutive inner-loops.
5.5 Overall Performance
Table 5 summarizes the performance of all included methods, with one additional criterion to stop the process when the "user" has judged 300 documents for a topic. Statistical significance of the results are provided by comparing to the baseline, iterative relevance feedback, and by comparing to another ReQ-ReC method. In general, methods developed

169

Table 3: Baselines and methods included in comparison.

Method

Doc. Selection Classification Query Expansion # outer loops # inner loops

Relevance Feedback (RF)

top

-

Rocchio

1

1

Iterative RF

top

-

Rocchio

M

1

Passive

top

SVM at end

Rocchio

M

1

Unanchored Passive (Unanchored)

top

SVM at end

Rocchio - q0

M

1

Active

uncertainty

SVM

Rocchio

M

M

Diverse Active (Diverse)

uncertainty

SVM

divRoc

M

M

* M: multiple iterations; top: select 10 top-ranked documents; uncertainty: uncertainty-based active document selection;

divRoc: diverse Rocchio; Rocchio - q0: Rocchio without interpolation of the original query.

Table 4: Parameter settings: µ in Dirichlet prior;  and  in Rocchio ( fixed as 1); Results per query: number of documents returned by a search API call.

µ   Results/query

MB12 2100 0.95 0.4 2,000

MB13 -
0.85 0.15 10,000

ClueWeb09 -
0.75 0.15 1,000

HARD 1100 0.6 0.05 2,000

20NG 3200 0.5 0.4 2,000

under the ReQ-ReC framework significantly outperform iterative relevance feedback. Diverse Active, which uses an active document selection strategy and a diverse query expansion, achieves the best performance. For most data sets, the improvement over iterative relevance feedback is as large as 20% ­ 30% of MAP and R-Precision. This is promising given the difficulty of improvements based on those two metrics. On the largest data set, ClueWeb09, the best ReQ-ReC algorithm achieves more than 120% improvement over iterative relevance feedback.
We make the following remarks:
· (Compare Relevance Feedback with Iterative RF ) Multiple iterations of relevance feedback indeed outperforms a single iteration of feedback, even if the same number of judgments (i.e., 300) are used in this single iteration. The only exception is the ClueWeb09 data, for which the collection is too large and the relevance judgments are very sparse. In this case, an iterative relevance feedback method may stop earlier if none of the top 10 results brought back by a new query are relevant. In that situation, presenting more documents to the user at once may be less risky.
· (Compare Iterative RF with Passive and UnanchoredPassive) Distributing the burden of maximizing precision to a classifier is effective, even if the classifier is only involved at the end of the process. Iterative relevance feedback relies on the new query to maximize both precision and recall. By simply keeping the results retrieved by all previous queries and classifying them at the end (by an SVM trained on accumulated judgments), the retrieval performance increases significantly on all the data sets (Passive). Since the involvement of the classifier releases the burden of the queries to maximize precision, we anticipate that the queries no longer have to be tied closely to the original one. Indeed, even if we strip the effect of the original query from every expanded query (Unanchored-Passive), the

ReQ-ReC process still yields results comparable to-- and sometimes even better than--anchored query expansion (Passive). The performance is further improved when the classifier is involved in all the iterations instead of being applied at the end (Active).
· (Active) A straightforward active document selection approach (which picks the documents that the classifier is the least certain about) outperforms picking documents from the top of the ranked list. This is consistent with the observations in literature [22]. By actively selecting documents to present to the user, her effort of labeling documents is significantly reduced.
· (Diverse Active) The diverse query expansion method inspired by the weak-tie theory is clearly the winner on all five data sets. By moving the burden of precision to a classifier, the objective of a new query is purely to bring new relevant documents into the pool of retrieved documents. This gives freedom to the queries to expand the search space aggressively, and provides a great opportunity to investigate new algorithms that are particularly suitable for this goal.
5.6 Learning Behavior Analysis
The previous section summarizes the performance of ReQReC methods when the stop criteria are met. To better understand the behavior of ReQ-ReC, we provide the following analysis and plot the intermediate performance of three methods (Iterative RF, Active, and Diverse Active) throughout the user-interaction process. Note that each topic may accumulate judgments at a different pace and meet stop criteria earlier or later. We interpolate a per-topic curve by a piecewise linear function, and extrapolate it by extending the end-point constantly to the right. These per-topic curves are then averaged to generate the aggregated curve.
Figure 3 plots the performance of each method against the number of documents the "user" has judged so far throughout the ReQ-ReC process, measured using R-precision. All three curves start at the same point where there is no user judgment. At that point the ranking is essentially based on the original retrieval function (i.e., Dirichlet prior). When user judgments are beginning to be collected, there is a significant gain by iterative relevance feedback. Performance increases rapidly at the first 2 runs (20 judgments), and the growth becomes much slower after that. This is consistent with the findings in literature.
Methods developed under the ReQ-ReC framework (Active and Diverse Active) do not really take off until we obtain a reasonable number of judgments (50 on the HARD data set and 90 on the microblog data set). This is ascribed to

170

Table 5: Retrieval performance of competing methods. At most 300 judgments per topic. ReQ-ReC methods significantly outperform iterative relevance feedback.

MB13

MB12

HARD

20NG

ClueWeb09

R-prec MAP

R-prec MAP

R-prec MAP

R-prec MAP

R-prec MAP

Dirichlet

0.268

0.203

0.233

0.183

0.247

0.174

0.327

0.107

0.101

0.058

RF

0.417

0.415

0.466

0.479

0.440

0.447

0.451

0.356

0.256

0.229

Iterative RF Passive Unanchored Active Diverse

0.532 0.568 0.603 0.653 0.675

0.552 0.585 0.618 0.661 0.692

0.633 0.646 0.667 0.727 0.760

0.649 0.661 0.673 0.740 0.771

0.592 0.615
0.609 0.729 0.789

0.597 0.637 0.624 0.737 0.799

0.474 0.548 0.527 0.595 0.620

0.421 0.490 0.464 0.562 0.580

0.237 0.275 0.268 0.493 0.533

0.216 0.247 0.236 0.493 0.533

(+27%) (+25%) (+20%) (+19%) (+33%) (+34%) (+31%) (+38%) (+125%) (+147%)

** and * indicate the improvement over Iterative Relevance Feedback is statistically significant according to Wilcoxon signed rank test

at the significance level of 0.01 and 0.05; : the improvement over Passive is significant at the level of 0.05; : the improvement over

Active is significant at the level of 0.05; (+x%) indicates the percentage of improvement over the baseline Iterative RF.

R-Prec R-Prec MAP MAP

0.7

0.6

0.5

0.4 0.3 0.20

Iterative RF Active Diverse Active
50 100 150 200 250 300 350 #Judgments

(a) MB13

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.10

Iterative RF Active Diverse Active
50 100 150 200 250 300 350 #Judgments

(b) HARD

Figure 3: R-Prec vs. labeling effort

0.25 0.20 0.15 0.10 0.05 0.000

Iterative RF Active Diverse Active
50 100 150 200 250 300 350 #Judgments

0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.050

Iterative RF Active Diverse Active
50 100 150 200 250 300 350 #Judgments

(a) MB13

(b) 20NG

Figure 4: Residual Analysis

the "cold start" problem of supervised classification. When few labeled documents are available, the performance of a classifier does not outperform a simple ranking function.
As stated before, a ReQ-ReC process targets users who truly seek a high recall of relevant documents and are therefore willing to spend more effort on interacting with the system and labeling more results. Indeed, after the first few iterations, the two methods developed under ReQ-ReC framework improve dramatically and become significantly better than iterative relevance feedback. For the users who are reluctant to label more than 50 documents, conventional relevance feedback may still be a better choice.
The cold start implies that there is considerable room for improving the performance of the ReQ-ReC. For example, a semi-supervised classifier may be used early on to achieve better precision with few training examples.
We also notice that the benefit of Diverse Active over Active kicks in later in the process, when there are around 150 judgments collected. At that point, getting new relevant documents becomes more challenging, as many documents retrieved by the new query may have already been retrieved by a previous query. At this stage, introducing some diversity to the query expansion brings in considerable benefit. Similar observations are made on the other three data sets.
Another interesting analysis is how well a method works with documents that have not been selected for labeling so far. We are particularly interested in this behavior because we have decided to include all judged documents when measuring the performance of the system (see Section 4).
We plot the residual MAP in Figure 4, which is the mean average precision computed purely based on documents that have not been presented to the user so far in the process.

In general, the two ReQ-ReC methods (Active and Diverse Active) do a much better job in finding the relevant documents and ranking them high, even if they are not judged by the user. On the microblog data set, we see that the residual MAP decreases when more documents are presented to and labeled by the user. This may be simply because there are fewer relevant documents remaining in the collection. However, it is also likely due to the fact that the TREC judgments are not complete. There might be many relevant documents that were not judged by TREC at all. If a method successfully finds those documents, its performance may be significantly undervalued simply because we have to treat these documents as negative in computing the metrics.
We are therefore interested in how ReQ-ReC behaves if the data set is fully judged. Looking at the curves on the 20NG, we observe a contrary pattern, where the two ReQReC methods actually enjoy a continuous growth of residual MAP, while the same metric for iterative feedback is still dropping. This is a promising finding that indicates the performance of ReQ-ReC may be underestimated on data sets with incomplete judgments (i.e., TREC data sets).
6. CONCLUSION
We present ReQ-ReC (ReQuery-ReClassify), a doubleloop retrieval framework that is suitable for high-recall retrieval tasks without sacrificing precision. The interactive process combines iterative expansion of a query set with iterative refinements of a classifier. The work of maximizing precision and recall is distributed so that the queries increase recall and the classifier handles precision.
The ReQ-ReC framework is general, which includes classi-

171

cal feedback methods as special cases, and also leads to many instantiations that use different combinations of document selection, classification, and query expansion methods. The framework is very effective. Some instantiations achieved a 20% ­ 30% improvement of mean average precision and Rprecision on most data sets, with the largest improvement up to 150% over classical iterative relevance feedback.
In order to clearly illustrate the power of the framework, we have intended to keep all the instantiations simple. It is a promising future direction to optimize the choices and combinations of the key components of the ReQ-ReC framework. Findings from our experiments also indicate possibilities for investigating new classification and query expansion algorithms that are particularly suited to this framework.
Acknowledgment. The authors thank Sam Carton, Kevyn Collins-Thompson, ChengXiang Zhai, and reviewers for their useful comments. This work is partially supported by the National Science Foundation under grant numbers IIS-0968489 and IIS-1054199, and partially supported by the DARPA under award number W911NF-12-1-0037.
7. REFERENCES
[1] C. Buckley, A. Singhal, M. Mitra, and G. Salton. New retrieval approaches using smart: Trec 4. In Proceedings of the Fourth Text REtrieval Conference (TREC-4), pages 25­48, 1995.
[2] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273­297, 1995.
[3] I. J. Cox, M. L. Miller, S. M. Omohundro, and P. N. Yianilos. Pichunter: Bayesian relevance feedback for image retrieval. In Pattern Recognition, 1996., Proceedings of the 13th International Conference on, volume 3, pages 361­369. IEEE, 1996.
[4] H. Drucker, B. Shahrary, and D. C. Gibbon. Support vector machines: relevance feedback and information retrieval. Information Processing and Management, 38(3):305­323, 2002.
[5] H. Fang and C. Zhai. Semantic term matching in axiomatic approaches to information retrieval. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 115­122. ACM, 2006.
[6] M. S. Granovetter. The strength of weak ties. American journal of sociology, pages 1360­1380, 1973.
[7] D. A. Hanauer. Emerse: the electronic medical record search engine. In AMIA Annual Symposium Proceedings, volume 2006, page 941. American Medical Informatics Association, 2006.
[8] D. Harman. Relevance feedback revisited. In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '92, pages 1­10, New York, NY, USA, 1992. ACM.
[9] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 154­161. ACM, 2005.
[10] R. Krovetz. Viewing morphology as an inference process. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, pages 191­202. ACM, 1993.
[11] K. Lang. Newsweeder: Learning to filter netnews. In In Proceedings of the Twelfth International Conference on Machine Learning, pages 331­339, 1995.
[12] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge, 2008.

[13] D. W. Oard and W. Webber. Information Retrieval for E-Discovery. Foundations and Trends in Information Retrieval. Now Publishers, 2013.
[14] K. Raman, P. N. Bennett, and K. Collins-Thompson. Toward whole-session relevance: Exploring intrinsic diversity in web search. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '13, pages 463­472. ACM, 2013.
[15] J. Rocchio. Relevance feedback in information retrieval. In The SMART retrieval system experiments in automatic document processing, pages 313­323. Prentice Hall, 1971.
[16] Y. Rui, T. S. Huang, M. Ortega, and S. Mehrotra. Relevance feedback: a power tool for interactive content-based image retrieval. Circ. and Sys. for Video Tech., IEEE Transactions on, 8(5):644­655, 1998.
[17] G. Salton and C. Buckley. Improving retrieval performance by relevance feedback. In Journal of the American Society for Information Science (1986-1998), volume 41, pages 288­297. ACM, 1990.
[18] B. Settles. Active learning literature survey. University of Wisconsin, Madison, 2010.
[19] X. Shen, B. Tan, and C. Zhai. Context-sensitive information retrieval using implicit feedback. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '05, pages 43­50. ACM, 2005.
[20] X. Shen and C. Zhai. Active feedback in ad hoc information retrieval. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 59­66. ACM, 2005.
[21] I. Soboroff, I. Ounis, C. Macdonald, and J. Lin. Overview of the trec-2012 microblog track. In Proceedings of the Twenty-First Text REtrieval Conference, 2012.
[22] A. Tian and M. Lease. Active learning to maximize accuracy vs. effort in interactive information retrieval. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 145­154. ACM, 2011.
[23] S. Tong and D. Koller. Support vector machine active learning with applications to text classification. J. Mach. Learn. Res., 2:45­66, Mar. 2002.
[24] E. M. Voorhees. Query expansion using lexical-semantic relations. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 61­69. ACM, 1994.
[25] X. Wang, H. Fang, and C. Zhai. A study of methods for negative relevance feedback. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 219­226. ACM, 2008.
[26] Z. Xu, R. Akella, and Y. Zhang. Incorporating diversity and density in active learning for relevance feedback. In Advances in Information Retrieval, pages 246­257. Springer, 2007.
[27] Z. Xu, X. Xu, K. Yu, and V. Tresp. A hybrid relevance feedback approach to text retrieval. In European Conference on Information Retrieval, Lecture Notes in Computer Science, pages 281­293. Springer, 2003.
[28] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In Proceedings of the tenth international conference on Information and knowledge management, pages 403­410. ACM, 2001.
[29] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 334­342. ACM, 2001.
[30] X. S. Zhou and T. S. Huang. Relevance feedback in image retrieval: A comprehensive review. Multimedia systems, 8(6):536­544, 2003.

172

Estimating Global Statistics for Unstructured P2P Search in the Presence of Adversarial Peers

Sami Richardson
Dept. of Computer Science University College London Gower St., London WC1E 6BT, UK
sami.richardson.10@ucl.ac.uk

Ingemar J. Cox
Dept. of Computer Science University College London Gower St., London WC1E 6BT, UK
i.cox@ucl.ac.uk

ABSTRACT
A common problem in unstructured peer-to-peer (P2P) information retrieval is the need to compute global statistics of the full collection, when only a small subset of the collection is visible to a peer. Without accurate estimates of these statistics, the effectiveness of modern retrieval models can be reduced. We show that for the case of a probably approximately correct P2P architecture, and using either the BM25 retrieval model or a language model with Dirichlet smoothing, very close approximations of the required global statistics can be estimated with very little overhead and a small extension to the protocol. However, through theoretical modeling and simulations we demonstrate this technique also greatly increases the ability for adversarial peers to manipulate search results. We show an adversary controlling fewer than 10% of peers can censor or increase the rank of documents, or disrupt overall search results. As a defense, we propose a simple modification to the extension, and show global statistics estimation is viable even when up to 40% of peers are adversarial.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software--Distributed systems
Keywords
P2P IR; adversarial IR
1. INTRODUCTION
Full-text search across peer-to-peer (P2P) networks has received considerable interest in recent years. P2P architectures can be classified as structured, where content is placed according to defined rules to allow for efficient retrieval, and unstructured, where there are no such rules. To guarantee
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609567.

finding content in an unstructured P2P network it is necessary to search all nodes. Communication costs typically make this infeasible, so search is probabilistic. The Probably Approximately Correct framework [10] was proposed to model the problem of probabilistic search in an unstructured distributed network. The PAC framework assumes that (i) nodes operate independently, (ii) each node indexes a subset of documents from the collection, (iii) the documents indexed are not disjoint across nodes, i.e. each document may be indexed on more than one node, and (iv) a query is performed by sampling a random subset of nodes and combining the results. The accuracy of a query is defined as the size of the intersection of the set of documents retrieved by a constrained, probabilistic search and the set that would have been retrieved by an exhaustive search, normalized by the size of the latter.
A PAC architecture gracefully handles the churn associated with nodes joining and leaving the network. This is because the addition of any node compensates for the loss of any other. P2P networks comprised of volunteer nodes typically experience high levels of churn [18], and therefore may be a good match for a PAC architecture. One example is a PAC P2P web search engine, as proposed by Asthana et al [1]. They demonstrated, from a communications bandwidth perspective, the feasibility of using a PAC architecture to store an index of the world wide web on one million volunteer nodes, and to handle a query load equivalent to that seen by the Google web search engine.
However, in a PAC architecture each node is only aware of the documents it indexes, and typically does not have access to the global statistics of the entire document collection that are often used by modern information retrieval models. Without these statistics, a node may not be able to correctly score and rank its documents when responding to a query. As a consequence, the accuracy of queries may not reach the level predicted by the PAC framework. In this paper we evaluate a solution that requires only a small modification to the PAC query procedure. When each node involved in a query returns a list of matching documents, we propose that it also returns information on statistics derived from its local index. After responses from all nodes have been received, this information is used to calculate an improved estimate of global statistics, and the retrieved documents are then scored and ranked again to form a new, potentially more accurate top-k result list. We test this technique with two examples of modern retrieval models, BM25 [17], and a language model with Dirichlet smoothing [22]. We show that accuracy approaches the theoretical value predicted by

203

the PAC model, thereby overcoming a previous limitation of the architecture.
It should be expected that a PAC P2P network comprised of volunteer nodes will be subject to malicious behavior. When a secure method is used to select the random set of nodes for each query, such as a gossip-based secure peer sampling service like Brahms [4], the random nature of a PAC architecture makes it relatively resilient to attack. Unfortunately, the global statistics estimation technique greatly increases vulnerability. We demonstrate this by first identifying how an adversary can introduce malicious nodes to perform three attacks: (i) censorship of a document, (ii) promotion (increasing the rank) of a document, and (iii) disruption of overall search results. We then develop theoretical models of these attacks, assuming the global statistics estimation technique is not used. This establishes a baseline of vulnerability. Next, we outline how an adversary can corrupt the global statistics estimation technique, and through simulations demonstrate the potential for manipulation of search results is much greater than for the PAC architecture baseline.
As a defense, we propose that the querying node measures the skewness of global statistics information returned from nodes, and filters out values that appear to be manipulated. We show the technique to be highly effective, withstanding up to 40% of malicious nodes before query results are significantly affected.
1.1 Paper Overview
In Sect. 2 we discuss related work. In Sect. 3 we review the PAC framework and provide details of BM25 and the language model. In Sect. 4 we modify the PAC query procedure to incorporate the estimation of global statistics, and evaluate its effectiveness. In Sect. 5 we investigate how this increases vulnerability to attack, and in Sect. 6 we propose a defense. Finally, in Sect. 7 overall conclusions are drawn.
2. RELATED WORK
The problem of estimating global statistics for P2P information retrieval (P2P IR) is similar to that of estimating corpus statistics for distributed information retrieval (DIR). The multi-database model of DIR assumes that (i) a query is sent to a subset of the most promising databases, (ii) each database returns matching documents, and (iii) results from each database are merged into a final ranked result list [5]. A PAC P2P architecture can be thought of as a special case of this model, where queries are sent to a random subset of databases (nodes), and where each database uses the same retrieval algorithm and contains documents randomly selected from the same central document collection.
In DIR, document scores assigned by different databases may be based on different corpus statistics and retrieval algorithms, and therefore may not be directly comparable. To correctly merge results from different databases, scores from each database can be normalized. When databases are uncooperative, and do not aid in this task, normalized scores can be estimated from a sample of documents obtained by submitting queries [6], but normalization is easier when databases are cooperative and share local database information, such as corpus statistics. Viles et al. proposed that databases periodically share corpus statistics, so that all databases use the same corpus statistics [19]. However, this may be impractical when there are a very large num-

ber of databases (or equivalently, nodes in a P2P network). Callan et al. suggested that corpus statistics be requested from databases before each query, and then passed along with the query [7]. All databases responding to the query can then use the same corpus statistics. However, the increase in query latency may be unacceptable. Kirsch et al. proposed that each queried database returns local corpus statistics, in addition to the result list [15]. The corpus statistics from all databases are then combined, and new, normalized scores are calculated for each returned document. This is similar to the technique we use in this paper. Our work differs, because instead of using a deterministic architecture, we specifically consider a PAC P2P architecture, where documents are randomly replicated across peers, and queries are directed to a random subset of peers.
P2P IR differs from DIR in that P2P networks are typically intended to scale to a much larger number of nodes, potentially thousands or tens of thousands, and are characterized by much higher levels of churn. A number of solutions have been proposed to overcome the lack of global statistics at each node in P2P networks. PlanetP [11] is a P2P information retrieval system that efficiently routes queries to nodes containing relevant documents by using a compact summary of the entire document index maintained at each node. The document frequency global statistic, which is the proportion of documents that index a given term, is not available at each node, so it is difficult to rank documents with the commonly used measure of term frequency-inverse document frequency (TF-IDF). However, the summary index at each node makes it possible to determine peer frequency, the proportion of peers that index at least one document with a given term, and this is used to calculate the measure of term frequency-inverse peer frequency (TF-IPF). The performance of TF-IPF is shown to be similar to that of TF-IDF. Unfortunately, for many other P2P architectures, including PAC, it is not substantially easier to calculate peer frequency than it is to calculate document frequency.
Lu et al. considered search of text based libraries in hierarchical P2P networks [16]. They assume some nodes act as top-level `hubs' and provide a directory service for low-level `leaf' nodes that contain text libraries. A query is routed to one or more hub nodes, which in turn route it to appropriate leaf nodes or pass it on to be handled by other hub nodes. The responses from leaf nodes are returned down the query path. Hub nodes maintain global statistics for all connected leaf nodes, and also share these global statistics with other hub nodes. This allows hub nodes to normalize document scores in query responses and merge them into a ranked list before passing the query response back down the query path. As a result, the user is provided with a correctly merged ranked result list. This can be a very effective solution, but it is only applicable to hierarchical P2P architectures.
Chen et al. proposed a hybrid structured/unstructured P2P system for full text-search [9]. The structured component efficiently handles multi-term queries, while the unstructured component gathers global statistics at each node using a gossip protocol. This allows each node to maintain up-to-date global statistics, but comes at the cost of extra inter-node communication traffic.
Witschel et al. [21] showed that reasonable estimates of global statistics can be derived by requesting statistics from random nodes. Our approach also amounts to receiving statistics from random nodes, but takes advantage of the

204

mechanism already in place to perform queries, whereas in theirs the random sampling is implemented alongside the mechanism to perform queries. Witschel et al. also showed the effectiveness of random sampling can be improved by combining it with a small reference corpus of global statistics on each node. However, this may be less effective with a dynamically changing document collection, and is unnecessary with our approach because global statistics are derived from a large proportion of the document collection.
A major part of our contribution is an analysis of the adversarial manipulation of global statistics. While the impact of malicious nodes in P2P networks has been widely studied [20], we are unaware of investigations into attacks against global statistics. Bender et al. [3] reduce bias of estimates of document frequency using hash sketches. Each node creates a hash sketch to provide a compact synopsis of documents that contain a query term, and hash sketches are combined to calculate document frequency. However, this is intended to reduce bias arising from the overlap of document collections across nodes, and not bias caused by adversarial nodes.

3. PRELIMINARIES

We first review the fundamental concepts of the PAC

framework [10]. This earlier work assumes there is no ad-

versarial behavior. In Sect. 5 we drop this assumption. Let

there be n homogenous nodes in the network, m unique

documents in the collection, and each node indexes  doc-

uments. Let the total index capacity of the network be R.

There are ri copies of each document di replicated across

the indexes of nodes, such that i ri = R. Documents are

uniformly

randomly

replicated,

so

ri

=

R m

.

Queries

are

sent

to z randomly selected nodes, and relevant documents are

combined and ranked to form a top-k result list. The prob-

ability of finding c copies of a document di is binomially distributed. It was shown [10] that the probability P (di) of finding at least one copy of document di is given by

z

P (di) = 1 -

1- m

.

(1)

For (1) to hold, the number of documents returned from each queried node, k , needs to be greater than or equal to k. In [10] this was implicitly assumed. In this paper, since we vary k , we explicitly state this requirement.
Using the property of exponential functions, (1) can be approximated with

P

(di)



1

-

e-

z m

.

(2)

As a consequence, for a fixed collection size m, the probability of finding di is determined by the product z.
In information retrieval, typically there are multiple documents that are relevant to a query. Let Dk(j) be the global top-k, the set of top-k documents retrieved for query j from the full document collection, and D k(j) be the local top-k, the set retrieved from a constrained search of z nodes. The accuracy aj for query j is then defined [10] as

aj

=

|Dk(j)  D k(j)| |Dk (j )|

.

(3)

It was shown [10] that each query is expected to retrieve k · P (di) documents out of the global top-k, and therefore expected average accuracy is given by

E(aj )

=

k

· P (di) k

=

P (di)

.

(4)

3.1 BM25 Ranking Function
The first ranking function we evaluate the global statistics estimation technique for is BM25. Let C be the set of documents in the collection and T be the set of terms in a query. The score, sBM25(d, T ), assigned to document d for query T is then given [17] by

sBM25(d, T ) = w(t) · s(t, d) ,

(5)

tT

where

s(t, d) =

T F (t, d) · (k1 + 1)

, (6)

T F (t, d) + k1

1

-

b

+

b

·

DL(d) AV GDL

w(t)

=

log

, 1
Pdoc (t)

k1

and

b

are

free

parameters,

T F (t, d)

is

the term frequency of term t in document d, DL(d) is the

number of terms in document d, i.e. its length, and AV GDL

is the average document length across all documents in C.

Pdoc(t) is the probability of a document in collection C con-

taining term t, and is calculated with

DF (t, C)

Pdoc(t) = |C| ,

(7)

where DF (t, C) is document frequency, the number of documents from collection C that contain the term t.
Each node has access to or can calculate all the parameters of (5), with the exception of Pdoc(t) and AV GDL. These are the global statistics for the collection, which we need to estimate.

3.2 Language Model with Dirichlet Smoothing
The second ranking function we consider is a language model with Dirichlet smoothing. For a language model, the score, slang(d, T ), assigned to each document d for query T is given by

slang(d, T ) = p(t|d) ,

(8)

tT

where p(t|d) is the probability of the language model of document d generating term t, and is given by

p(t|d) = T F (t, d) .

(9)

DL(d)

This does not require global statistics of the collection. However, to prevent a score of zero if a query term is not present in a document, it is common to use smoothing. Various techniques have been proposed [22] that assign a non-zero value to p(t|d) if the term is missing. In this paper we consider Dirichlet smoothing, for which p(t|d) is given by

p(t|d) = T F (t, d) + µ · Pcoll(t) ,

(10)

DL(d) + µ

where µ is a free parameter to control the amount of smoothing, and Pcoll(t) is the probability of term t being generated from the collection. Pcoll(t) is given by

Pcoll(t) =

dC T F (t, d) , dC DL(d)

(11)

and is the global statistic we need to estimate.

205

4. GLOBAL STATISTICS ESTIMATION
We now outline a modification to the PAC query procedure that allows the estimation of global statistics. Each node, u, uses its local collection of documents, Lu, to calculate an initial estimate of the retrieval model global statistics. Using these estimated statistics the node calculates the retrieval model score for each term from all documents in Lu. These scores are then added to an index for use when scoring documents for queries. The query procedure is as follows.
1. A querying node issues a query comprised of a set of terms, T , to z random nodes (including itself).
2. Each queried node, u, then: (a) Compiles a top-k result list of the highest ranked documents from Lu for query T , using the previously calculated scores. (b) The node returns to the querying node two sets of information: Ru and Gu. The former contains summary information for the top-k documents that the querying node needs to produce a final top-k result list (e.g. document id, parameters of the document required to calculate its score). The latter contains information to estimate the document collection global statistics, used by the retrieval model scoring algorithm.
3. On receiving responses from all z queried nodes, the querying node: (a) Calculates new, improved estimates of global statistics based on Gu returned from each node. (b) Calculates a score for each received document using summary information from Ru and the new global statistics. (c) Ranks documents by their new score, and presents a final top-k result list to the user.

For BM25, the estimate of the global statistic Pdoc(t) is calculated in Step 3(a) with

P^doc(t) =

uZ DF (t, Lu) , uZ |Lu|

(12)

and the estimate of the global statistic AV GDL is calculated with

AV G^ DL =

uZ dLu DL(d) . uZ |Lu|

(13)

Therefore, for BM25, Gu consists of DF (t, Lu) for t  T , |Lu|, and dLu DL(d).
For the language model, the estimate of the global statistic Pcoll(t) is calculated in Step 3(a) with

P^coll(t) =

uZ dLu T F (t, d) , uZ dLu DL(d)

(14)

requiring Gu to consist of dLu T F (t, d) for t  T , and dLu DL(d). For both BM25 and the language model, the summary
information Ru consists of T F (t, d) for t  T and DL(d), for each document d in the top-k result list.
The collections Lu on each node used by (12) to (14) are not disjoint across nodes, but because a PAC architecture distributes documents uniformly randomly, global statistics are, on average, unaffected.

4.1 Evaluation
We begin our evaluation of the above technique by first assuming there are no malicious nodes present. This demonstrates the maximum gain in query accuracy. In Sect. 5 we then consider the risk that malicious nodes may be able to manipulate search results by returning corrupt global statistics information.
4.1.1 Experimental Setup
A simulated network of n = 10, 000 nodes was used. The document collection, C, was comprised of m = 1, 692, 096 documents from the WT10g [2] web corpus. Documents were uniformly randomly distributed across nodes so that each node indexed  documents. Fifty queries were drawn randomly from the TREC 2009 Million Query track [8] and used as the query test set. Each query was performed using the technique described above, where each queried node returned the top k = 10 matching documents, and the accuracy of the final top-10 list calculated with (3). Each query was repeated for a total of ten repetitions, and the average accuracy across queries for a given value of z recorded. In our simulations we chose parameter z, the number of nodes a query is issued to, and , the number of documents indexed per node, such that the theoretical expected average accuracy given by (4) would be 0.9. For values of z = 1, 2000, 4000, 6000, 8000, 10000 this meant corresponding values of  = 1692096, 1946, 973, 649, 486, 389. Such large values of z, and correspondingly small values of , were chosen so that the global statistics technique was evaluated under the most challenging circumstances. These experiments were then repeated, first with the querying node using only its local documents to estimate collection global statistics in step (3a), and then again assuming each node had access to the global statistics of the document collection.
We performed the above experiments for both BM25 and the language model. For the former, the free parameters were k1 = 2.0, b = 0.75, and for the latter µ = AV GDL. These are typical choices [17].
4.1.2 Results
Figure 1(a) shows average accuracy of queries for different values of z, for BM25. There are curves for different combinations of the global statistics Pdoc(t) and AV GDL, derived either from assumed knowledge of the whole collection (coll ), or from only the collection on the querying node (node). As would be expected, when both Pdoc(t) and AV GDL were derived from the entire collection, average accuracy was about 0.9 for all values of z. However, when Pdoc(t) or AV GDL were estimated only from the index of the querying node, accuracy in general decreased as z increased, i.e. as the number of documents per node, , decreased and thus the number of documents from which global statistics could be estimated decreased. Deriving Pdoc(t) from only the index of the querying node caused a drop in accuracy of up to nearly 35%, whereas doing the same for AVGDL caused a less severe drop of up to about 10%. Figure 1(b) shows the results for the language model. Here the estimated global statistic is Pcoll(t), and using documents only from the querying node to derive the estimate resulted in a drop of up to about 20%.
Fig. 1(c),(d) show the results when the global statistics estimation technique is used. There are curves for different values of k , i.e. the maximum number of results returned from each of the z queried nodes. For BM25, the global

206

1

1

1

1

average accuracy average accuracy average accuracy average accuracy

0.5

0.5

0.5

0.5

0 2000 4000 6000 800010000 z
(a)

0 2000 4000 6000 800010000 z
(b)

0 2000 4000 6000 800010000 z
(c)

0 2000 4000 6000 800010000 z
(d)

Figure 1: Average accuracy of queries, where each pair of z,  values is chosen to achieve a theoretical expected average accuracy of 0.9. Global statistics are estimated from either the whole collection (coll ) or from just the querying node (node). (a) is for BM25, where the curves from top to bottom are for (coll Pdoc(t), coll AV GDL), (coll Pdoc(t), node AV GDL), (node Pdoc(t), coll AV GDL), and (node Pdoc(t), node AV GDL). Note the last two curves overlap. (b) is for the language model, where the curves from top to bottom are for (coll Pcoll(t)), and (node Pcoll(t)). (c) and (d) are for BM25 and the language model respectively, using the global statistics estimation technique with k =  (top) or k = 10 (bottom). Note the two curves in (c) overlap.

100

100

% queries > accuracy x % queries > accuracy x

50

50

0

0

0

0.5

1

0

0.5

1

x

x

(a)

(b)

Figure 2: The percentage of queries that achieve accuracy x when global statistics are estimated from either
the whole collection (coll ) or from just the querying node (node). (a) is for BM25, where the curves from right to left are for (coll Pdoc(t), coll AV GDL), (P^doc(t), AV G^ DL), and (node Pdoc(t), node AV GDL). Note the first two curves overlap. (b) is for the language model, where the curves from right to left are for (coll Pcoll(t)), (P^coll(t)), and (node Pcoll(t)). For z = 10, 000,  = 389, and k = 10.

statistics estimation technique achieves accuracy that is very close to the theoretical value of 0.9, for all values of z. The same is true for the language model for k = , but for k = 10 average accuracy drops to 0.8 for larger values of z, which is about 10% lower than the theoretical value.
We are also interested in how accuracy varies across different queries. For parameters z = 10, 000,  = 389, and k = 10, Fig. 2(a),(b) show the proportion of queries that achieve a given accuracy, for BM25 and the language model respectively. With only 389 documents per node these results represent performance under challenging circumstances. Nevertheless, for BM25, the proportion of queries achieving a given accuracy when using the global statistics estimation technique (P^doc(t), AV G^ DL) is almost identical to the case where collection global statistics are available at each node (coll Pdoc(t), coll AVGDL), e.g. about 95% of queries achieve an accuracy of at least 0.7. When global statistics are derived only from documents at the querying node (node Pdoc(t), node AVGDL), this figure falls to 15%. For the language model, the global statistics estimation technique does not perform quite as well, with about 65% of queries achieving an accuracy of at least 0.7, compared to about 90% for when global statistics are available at each node, and 60% when global statistics are estimated from only documents at the querying node. However, for the global statistics technique over 95% of queries achieve an accuracy of at least 0.3, compared to less than 80% when global statistics are estimated from only documents at the querying node.

4.1.3 Discussion
The experiments showed that the global statistics estimation technique can achieve an average query accuracy that is very close to what would be attained if global statistics had been available at each node, at least for larger values of k , even for extreme cases where each node indexes only a small proportion of the document collection. To understand why this is the case, we observe that for each query global statistics are estimated from z documents, a potentially very large sample. Of course these documents are unlikely to be distinct. The number of distinct documents, ndistinct, is between  and min(m, z). It is straightforward to show that the expected value of ndistinct is given by

E(ndistinct) = P (di)m ,

(15)

where P (di) is given by (1). It follows that the expected coverage for an estimate, i.e. the proportion of documents in the collection that the estimate is based on, is given by

E(Coverage)

=

ndistinct m

=

P (di)

.

(16)

The expected average accuracy for a top-k query, as given by (4), is also equal to P (di). Therefore, by choosing network parameters  and z to increase theoretical expected average accuracy, coverage is also increased for the global statistics estimates, and accuracy moves towards the upper bound predicted by the PAC framework. For example, a network could be designed to achieve high theoretical accuracy, such

207

as 0.9, which means that estimates of global statistics will be based on 90% of the document collection, and would be expected to be very close to the correct global statistics.
As was apparent for the language model with k = 10, the effectiveness of the global statistics estimation technique may be reduced when k < . Since the top-k result lists are calculated using global statistics estimated from just the local index of one node, ranking may be incorrect, and therefore relevant documents may not be returned to the querying node. Consequently, no matter how accurate the final global statistic estimate is, these documents will never appear in the top-k result list presented to the user. However, in practice it is likely that a large value of k can be used, since the communication cost associated with each result in the result list is small. For example, both BM25 and the language model require only a document id, and values for term frequency and document length to be returned for each result.
5. ADVERSARIAL ATTACKS
We now show that if an adversary can introduce malicious nodes, the global statistics estimation technique can be subverted to manipulate search results. In the analysis that follows, it is assumed an adversary controls the proportion f  [0, 1] of the n nodes in the network. To ensure no node has a greater influence on search results than any other, each node is restricted to indexing the same number of documents, . In practical systems the capacity of each node may differ, and can be dealt with by allowing nodes with higher capacities to operate multiple `virtual' nodes, each of which has capacity . This resembles the Sybil attack [12], where an adversary impersonates a large number of nodes to control the network. Therefore, the defensive techniques discussed in [12] to restrict the number of nodes operated by an individual need to be applied to both virtual and physical nodes. For example, a check can be made to verify that only a limited number of virtual or physical nodes are associated with an email address.
We consider the following attacks.
· Censorship. Reduce the likelihood of a target document appearing in the final top-k result list.
· Promotion. Increase the rank of a target document so that it is more likely to appear in the final top-k result list, and if it does appear, to rank higher.
· Disruption. Reduce the `correctness' of the final topk result list, i.e reduce accuracy, as given by (3).
A node responding to a query returns Ru for the top-k matching documents, and Gu. The former contains result summary information, such as document id, document length etc, and the latter contains information on global statistics. An adversary can perform the above attacks by using malicious nodes to return corrupt information for Ru and/or Gu. (We assume that malicious nodes cannot construct and return corrupt documents that will score highly for a query; this can be enforced by requiring documents to be digitally signed by a trusted third party.)
In our analysis we initially assume the global statistics estimation technique is not used, and attacks only corrupt Ru. This establishes the baseline vulnerability inherent to the PAC architecture. We then consider the increase in attack

effectiveness that arises when the global statistics estimation technique is used and an adversary can also corrupt Gu.

5.1 Baseline Vulnerability
Malicious nodes can perform the Censorship attack by returning corrupt summary information for k documents in Ru, such that each document would score higher than the target document and prevent it from appearing in the final top-k result list. For the Promotion attack, malicious nodes would always include the target document in Ru, along with corrupt summary information so that it will outrank any other. For the Disruption attack, malicious nodes would return k irrelevant documents, all with corrupt summary information that ensures they will outrank other documents. To have an effect on a query, these attacks require only a single malicious node to be one of the z nodes randomly sampled. If the proportion f of nodes are malicious, the probability P (mi) of a query visiting at least one malicious node is

P (mi) = 1 - (1 - f )z .

(17)

For z = 1, 000, it would require an adversary to control only f = 0.3% of nodes for there to be a 0.95 probability of a malicious node being visited by the query, and therefore allow the adversary to manipulate on average 95% of queries.
However, incorrect summary information in Ru can be detected by retrieving the documents. For example, a querying node, on receiving responses from all queried nodes, could retrieve the documents in the final top-k result list, calculate the summary information for each, and only display to the user results with correct scores. The extra latency and communication costs involved with this may be unacceptable, so an alternative is to display the top-k result list to the user, unchecked. Only when a user chooses to view a document is it retrieved and the score verified. If the score proves to be incorrect, then the document is not made available to the user. Since incorrect summary information can be easily detected, we assume an adversary does not perform attacks using this approach. A more subtle, and less easily detectable alternative, is for malicious nodes to return correct summary information for documents in Ru, but to exclude specific documents. Each node indexes random documents, so it is more difficult to determine if a node is not returning a given document because it is behaving maliciously, or because the document is simply not in its index. Attacks carried out by excluding documents form the baseline of vulnerability for a PAC architecture.

5.1.1 Censorship
The Censorship attack can be performed by malicious nodes excluding the target document. Let P (di) be the probability of retrieving document di when the proportion f of nodes are malicious and exclude it. From (1) it is straightforward to show that P (di) is given by

 z(1-f )

P (di) = 1 -

1- m

.

(18)

Using the property of exponential functions, this can be approximated with

P

(di)



1

-

e-(1-f )

z m

.

(19)

Equations (2) and (4) can be used to estimate the expected average accuracy for a query when there are no malicious

208

nodes present, E(aj). P (di), for a given proportion f of

malicious

nodes,

and

E(aj )

are

both

determined

by

z m

.

Fig-

ure 3 shows the effect on P (di) as f is varied. Each curve de-

picts

different

choices

of

z m

to

achieve

E(aj) = 0.3, 0.6, 0.9.

As E(aj) increases, resilience to censorship also increases.

Typically,

z m

would

be

chosen

to

achieve

high

average

ex-

pected accuracy, so resilience to censorship would be high.

For example, when E(aj) = 0.9, it would require about 70%

of nodes to be malicious to reduce the probability of finding

di by 50% from 0.9 to 0.45.

1

P'(d )
i

0.5

0

0

0.5

1

f

Figure 3: Probability P (di) of retrieving document di when the proportion f of nodes are performing the Censorship attack by excluding di. For E(aj) = 0.9 (top), 0.6 (middle), 0.3 (bottom).

5.1.2 Promotion
The Promotion attack can be carried out by censoring documents that rank higher than the target document, thus improving the rank of the target document. If there are u documents in the global top-k for a query that rank higher than the target document, and if malicious nodes never return them when queried, the probability P (u ) of retrieving u documents out of the total u is given by

P (u ) =

u u

P (di)u

1 - P (di) u-u ,

(20)

where P (di) is the probability of retrieving one of the excluded documents, as given by (18). Since this is a standard binomial distribution, the expected number of documents retrieved is

E(u ) = u · P (di) .

(21)

If document di is retrieved for a query, then its rank is one plus the number of other documents retrieved that rank higher. Therefore, (21) can be expressed in terms of the expectations of the rank of the target document before the attack, rbefore, and the rank after, rafter:

E(rafter) = (E(rbefore) - 1)P (di) + 1 .

(22)

As for the Censorship attack in Sect. 5.1.1, we consider the

effectiveness of this attack for an example system designed to

achieve expected average accuracy of 0.9 when no malicious

nodes are present.

This

requires

z m

=

2.3.

An adversary

would need to control over f = 50% of nodes to increase the

expected rank of a target document from 10 to 2.

5.1.3 Disruption
The Disruption attack can be performed in a similar manner to the Promotion attack, except rather than excluding the u documents that rank higher than a target document,

all k documents in the global top-k for a query are excluded. The probability P (u ) of retrieving u documents from the global top-k for the query is given by (20) (where u = k), and the expected number of documents retrieved, E(u ), is given by (21). If aj denotes accuracy for a query j when malicious nodes are censoring all global top-k documents, then E(aj) is given by

E(u )

E(aj) = k = P (di) .

(23)

As an

example,

for

z m

=

2.3,

expected accuracy for

queries

is

0.9 when no malicious nodes are present. If we assume users

find search results acceptable as long as expected accuracy

remains above 0.5, then an adversary would need to control

over f = 70% of nodes to reduce expected accuracy below

this threshold.

5.2 Increased Vulnerability - Global Statistics Estimation
Section 5.1 established a theoretical baseline for the vulnerability of a PAC architecture, which assumes attacks are performed by excluding documents. In this section we investigate the increased vulnerability that the global statistics estimation technique introduces. For the following analysis, the global statistic to be estimated for BM25 is Pdoc(t), calculated at the querying node with (12), and for the language model Pcoll(t), calculated at the querying node with (14). BM25 also requires the global statistic AVGDL, but since AV GDL is a single value, it is feasible for every node to store it. For a fixed document collection size, this is trivial to implement; for a collection size that varies, a gossip protocol can be used to compute it [14].
When estimating Pdoc(t) and Pcoll(t) with (12) and (14), the numerator and denominator of both equations are values returned from queried nodes. If no limits are placed on these, even a single malicious node can dominate the result. This is prevented for Pdoc(t), however, since we restrict the capacity of each node to . Pdoc(t) is then estimated with

P^doc(t) =

uZ min (, DF (t, Lu)) . ·z

(24)

To prevent a single node dominating the estimate of Pcoll(t), it is assumed that the sum of document lengths on a node is AV GDL · . Approximate estimates of Pcoll(t) can then be calculated with

P^coll(t) 

uZ min , dLu T F (t, d) ·z

,

where

(25)

 = AV GDL ·  .

(26)

We shall see that this approximation can still yield very good results.
Equations (24) and (25) are more succinctly expressed as

1 g^t = c

x(tu) ,

uZ

(27)

where g^t is an estimate of the global statistic gt. For BM25, we have gt = Pdoc(t), and x(tu) equal to the numerator of (24) and c equal to the denominator. For the language model, we have gt = Pcoll(t), and x(tu) equal to the numerator of

209

(25) and c equal to the denominator. The Gu information returned from each node then consists of x(tu) : t  T .
We now investigate how an adversary may attempt the
attacks from Sect. 5. Experiments were performed for both
BM25 and the language model, but since findings for both
are similar, for brevity we only present results for BM25.

5.2.1 Censorship/Promotion

An adversary can decrease/increase the score of a target document for query T by using malicious nodes to manipulate g^t : t  T . However, this will not necessarily decrease/increase the rank, since the scores of other documents may also be decreased/increased. A more effective approach is to iterate through different values of gt : t  T , calculate the rank of the target document for each, and select the values that minimize/maximize rank. We denote these optimal values as gt : t  T . An adversary then uses malicious nodes to return a corrupt value, xt, for x(tu), and manipulates g^t to be gt. To find the required value of xt, we observe that during the attack g^t can be estimated with

z g^t = c

(1 - f )x(tu) + f xt

.

(28)

The value of xt is then selected so that g^t = gt. If the proportion of nodes an adversary controls, f , is too small to select a value of xt that will satisfy (28), then gt : t  T are discarded and the iteration repeated until values of gt : t  T are found that allow (28) to be satisfied.
Figure 4 illustrates the potential effect of these attacks for the queries T1 =`small dog' and T2 =`brown dog' on the rank of two documents selected from the WT10g corpus, D1 and D2. Each curve is calculated with (5) by assuming full access to the document collection, and using global statistic g1 when scoring the first term and g2 for the second. The range of ranks achieved by varying g1 and g2, and therefore the potential for manipulation, is considerable, but depends heavily on the query-document combination.
We simulated these attacks using the experimental setup from Sect. 4.1.1, but with a proportion f of nodes behaving maliciously. Malicious nodes performed the attacks by returning corrupt global statistics, as described above, and by excluding specific documents, as described in Sects. 5.1.1 and 5.1.2. In order to observe the full range of ranks a document may achieve when under attack, the final result list for each query was not restricted to just the top-k, i.e. all retrieved documents were treated as important, and each queried node returned results for all documents it indexes, i.e. k = .
Figure 5 shows results for the query T =`small dog', when performing the Censorship attack on D2, and when performing the Promotion attack on D1. There are z = 2, 000 nodes involved in the query, and each node indexes  = 1, 946 documents. Considering first the Censorship attack (top left), when varying the proportion of malicious nodes from 0 to 10% to 20% to 30%, rank decreases from 5 to 9 to 582 to 2166. Therefore, for top-k queries, when k = 10 it would require an adversary to control less than 20% of nodes for the target document to not appear in the final top-10. Compared to the PAC architecture baseline, where correct global statistics are available at each node, and the attack is performed only by excluding D2, then from (18), with 20% of malicious nodes there is still an expected 84% probability of D2 being retrieved and appearing in the final top-10.

0 10

rank (log scale)

5

10

1 0.5 g

00

0.5 1 g

2

1

Figure 4: Effect of global statistics on rank of documents D1 and D2 for queries T1 = `small dog' and T2 = `brown dog', where g1 is the global statistic for the first term, and g2 is for the second. For querydocument combinations T1, D1(top left), T1, D2(top middle), T2, D1(bottom), T2, D2(bottom left).

0 10

rank (log scale)

5 10

10

10

0

0.5

1

f

Figure 5: Censorship attack on D2 by manipulating global statistics (top left). Promotion attack on D1 by manipulating global statistics and excluding documents (top), or just by excluding documents (bottom). For query T = `small dog', and z = 2, 000,  = 1, 946.

From the Promotion attack curve (top), it can be seen that varying the proportion of malicious nodes from 0 to 10% to 20% to 30% increases the rank of D1 from 20778 to 84 to 11 to 9, demonstrating that fewer than 30% of malicious nodes are required to bring D1 into the top-10. The Promotion attack baseline curve (bottom) shows the PAC architecture baseline, which is the theoretical expected rank calculated with (22), when correct global statistics are available at each node, and nodes perform the Promotion attack by only excluding documents. In this case, over 95% of malicious nodes are required to promote D1 into the top-10. Clearly, for both the Promotion and Censorship attacks, the global statistics estimation technique can greatly increase vulnerability to manipulation.
We repeated the simulations with k = k = 10, i.e. only the final top-10 documents were considered important, and each queried node returned the top-10 matching documents. Results were very similar to the previous simulations for documents at ranks 1 to 10.
5.2.2 Disruption
An adversary can achieve maximum disruption of a query by using malicious nodes to return responses that make estimates of global statistics at the querying node, g^t : t  T , as `wrong' as possible. For example a global statistic would be assigned a high value, even though it should be low, and vice-versa. The Disruption attack can be represented as an optimization problem, where g^t : t  T are manipulated to maximize the squared difference, , between the true global

210

average accuracy

1

0.5

0

0

0.5

1

f

Figure 6: Disruption attack by manipulating global statistics and excluding documents (bottom), or just by excluding documents (top). For z = 2, 000,  = 1, 946.

0 10

rank (log scale)

5 10

10 10

0

0.5

1

f

Figure 7: Censorship and Promotion attacks. As Fig. 5, but with skewness defense in operation.

1

average accuracy

statistic gt and the estimated value g^t for each term t in the

0.5

query, as given by

 = (gt - g^t)2 ,
tT

(29)

with g^t : t  T constrained according to the global statistics they represent, e.g. for gt = Pdoc(t) or gt = Pcoll(t), we require 0  g^t  1. Standard non-linear optimization techniques can be used to find values of g^t : t  T that maximize (29). As for the Censorship/Promotion attacks in Sect. 5.2.1, we denote these optimal values as gt : t  T . Again, an adversary can use malicious nodes to return xt, so that each value of g^t, as calculated with (28), becomes gt.
Simulations were performed using the setup from Sect. 4.1.1, with fifty randomly selected queries and k = k = 10. Malicious nodes returned corrupt global statistics information, Gu, to maximize (29). In addition, they also excluded the global top-k documents for the query, as described in Sect. 5.1.3. Figure 6 shows the results for z = 2, 000 and  = 1, 946. The bottom curve depicts average accuracy across the fifty queries for different proportions of malicious nodes. Also, as a baseline, the top curve depicts theoretical expected average accuracy, calculated with (23), that assumes correct global statistics are available at each node and that malicious nodes perform the Disruption attack by excluding documents. Clearly, the potential for attack is much greater when the adversary is able to corrupt global statistics. With only 10% of malicious nodes, average accuracy drops from a theoretical baseline of about 0.9 to about 0.6, a nearly 35% fall.

6. ROBUST GLOBAL STATISTICS ESTIMATION
Section 5.2 showed that when the global statistics estimation technique is used, even a small proportion of malicious nodes can significantly affect query results. We now propose a defense. The querying node calculates estimates of the global statistics, g^t : t  T , using (27) with values of x(tu) : t  T returned from each queried node u. Values of x(tu) from non-malicious nodes are expected to be normally distributed, since each node determines the value from its local collection of random documents. If an adversary attempts to bias g^t by using malicious nodes to return skewed values of x(tu), then this normal distribution will be skewed in one direction. We propose that the querying node measures this skewness, Kt, using a standard measure [13] given

0

0

0.5

1

f

Figure 8: Disruption attack. As Fig. 6 but with skewness defense in operation.

by

Kt =



z(z - 1) 

1 z

z-2


1

z



uZ (x(tu) - x¯(tu))3

 3

,

uZ (x(tu) - x¯(tu))2 2

(30)

where

x¯(tu)

=

1 z

uZ x(tu). If Kt is greater than a thresh-

old  , the querying node sorts values of x(tu), and repeatedly

discards the largest value until skew is within the limit. Sim-

ilarly, if Kt < - , the smallest value is repeatedly discarded.

We reran the attack simulations from Sect. 5.2, but with

the querying node reducing skewness to within the threshold

 = ±0.1. Figures 7 and 8 show the results for the Censor-

ship/Promotion, and Disruption attacks respectively. For a

proportion of malicious nodes f < 40%, the attacks have

very little effect. This is because malicious values of x(tu) are being removed, and the only impact of the attacks is to

reduce the number of non-malicious values available for com-

putation of global statistics. When f rises above 40%, the

defense rapidly breaks down due to the proportion of mali-

cious nodes nearing that of the proportion of non-malicious

nodes, and therefore it is no longer possible to distinguish

between malicious and non-malicious nodes.

The defense is effective because in order to manipulate

global statistics, an adversary needs to introduce skew, but

the defense directly measures skew and limits it. With this

defense the global statistics estimation technique can be

safely used to improve query accuracy when fewer than 40%

of nodes are malicious. For many situations this may be suf-

ficient. For example, to select a random subset of nodes for

each query, a gossip-based secure peer sampling service like

Brahms [4] can be used. Brahms can withstand up to 20% of

nodes behaving maliciously before sampling becomes signif-

icantly biassed. Consequently, it would be Brahms that im-

poses the limit on the maximum number of malicious nodes

tolerated, and not the technique to estimate global statistics.

211

7. CONCLUSIONS AND FUTURE WORK
In unstructured P2P information retrieval, performance can be severely degraded by poor estimates of the global statistics of the collection. For the case of unstructured P2P PAC search, we proposed that a querying node estimates the global statistics of the collection using information derived from the local statistics of the responding nodes. We showed, both theoretically, and experimentally with BM25 and a language model, that such an approach can provide accurate estimates of global statistics and significantly improve retrieval performance. The solution is well suited to a PAC architecture because it requires only a minimal amount of extra information to be returned from queried nodes. Unfortunately, it greatly increases the ability for an adversary to manipulate search results. We identified attacks where an adversary may attempt to (i) censor a document, (ii) promote a document, or (iii) disrupt overall search results. Through theoretical modeling and simulations we showed that while a PAC architecture is resilient to even a large proportion of malicious nodes, when the global statistics estimation technique is used, an adversary would need to control only 10% of nodes to have a significant impact. To protect against this, we proposed that the querying node filters out the most skewed responses, and showed that more than 40% of nodes would need to be malicious before these attacks become effective.
Our work assumed that a peer's local collection consists of a uniform random sample from the global collection. Future work is needed to analyze the case where document sampling is non-uniform, such as when based on document popularity. In this case, we believe that hash sketches, described in Section 2, may form the basis of a solution.
8. ACKNOWLEDGEMENTS
Sami Richardson was supported by EPSRC grant no. EPG037264-1 (Security Science Doctoral Training Centre).
9. REFERENCES
[1] H. Asthana, R. Fu, and I. J. Cox. On the feasibility of unstructured peer-to-peer information retrieval. In Advances in Information Retrieval Theory, pages 125­138. Springer, 2011.
[2] P. Bailey, N. Craswell, and D. Hawking. Engineering a multi-purpose test collection for web retrieval experiments. Information Processing & Management, 39(6):853­871, 2003.
[3] M. Bender, S. Michel, P. Triantafillou, and G. Weikum. Global document frequency estimation in peer-to-peer web search. In Proc. of the 9th Int. Workshop on the web and databases, 2006.
[4] E. Bortnikov, M. Gurevich, I. Keidar, G. Kliot, and A. Shraer. Brahms: Byzantine resilient random membership sampling. Computer Networks, 53(13):2340­2359, 2009.
[5] J. Callan. Distributed information retrieval. In Advances in Information Retrieval, pages 127­150, 2000.
[6] J. Callan and M. Connell. Query-based sampling of text databases. ACM Transactions on Information Systems (TOIS), 19(2):97­130, 2001.
[7] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. In

Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 21­28. ACM, 1995.
[8] B. Carterette, V. Pavlu, H. Fang, and E. Kanoulas. Million query track 2009 overview. In Proceedings of TREC, volume 9, 2009.
[9] H. Chen, J. Yan, H. Jin, Y. Liu, and L. M. Ni. TSS: Efficient term set search in large peer-to-peer textual collections. Computers, IEEE Transactions on, 59(7):969­980, 2010.
[10] I. J. Cox, R. Fu, and L. K. Hansen. Probably approximately correct search. In Advances in Information Retrieval Theory, pages 2­16. Springer, 2009.
[11] F. M. Cuenca-Acuna, C. Peery, R. P. Martin, and T. D. Nguyen. Planetp: Using gossiping to build content addressable peer-to-peer information sharing communities. In HPDC'03: Proceedings of the 12th International Symposium on High Performance Distributed Computing, Seattle, WA, USA, 2003.
[12] J. Douceur. The Sybil attack. Peer-to-peer Systems, pages 251­260, 2002.
[13] R. A. Groeneveld and G. Meeden. Measuring skewness and kurtosis. The Statistician, pages 391­399, 1984.
[14] D. Kempe, A. Dobra, and J. Gehrke. Gossip-based computation of aggregate information. In Foundations of Computer Science, 2003. Proceedings. 44th Annual IEEE Symposium on, pages 482­491. IEEE, 2003.
[15] S. T. Kirsch. Document retrieval over networks wherein ranking and relevance scores are computed at the client for multiple database documents, Aug. 19 1997. US Patent 5,659,732.
[16] J. Lu and J. Callan. Federated search of text-based digital libraries in hierarchical peer-to-peer networks. In ECIR'05: Proceedings of the 27th European conference on IR Research, Santiago de Compostela, Spain, 2005.
[17] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge, 2008.
[18] D. Stutzbach and R. Rejaie. Understanding churn in peer-to-peer networks. In Proceedings of the 6th ACM SIGCOMM conference on Internet measurement, pages 189­202. ACM, 2006.
[19] C. L. Viles and J. C. French. Dissemination of collection wide information in a distributed information retrieval system. In Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 12­20. ACM, 1995.
[20] D. Wallach. A survey of peer-to-peer security issues. Software Security--Theories and Systems, pages 253­258, 2003.
[21] H. F. Witschel. Global term weights in distributed environments. Information Processing & Management, 44(3):1049­1061, 2008.
[22] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems (TOIS), 22(2):179­214, 2004.

212

Hypothesis Testing for the Risk-Sensitive Evaluation of Retrieval Systems

B. Taner Dinçer
Dept of Statistics & Computer Engineering Mugla University Mugla, Turkey
dtaner@mu.edu.tr

Craig Macdonald and Iadh Ounis
School of Computing Science University of Glasgow Glasgow, UK
{firstname.lastname}@glasgow.ac.uk

ABSTRACT
The aim of risk-sensitive evaluation is to measure when a given information retrieval (IR) system does not perform worse than a corresponding baseline system for any topic. This paper argues that risk-sensitive evaluation is akin to the underlying methodology of the Student's t test for matched pairs. Hence, we introduce a risk-reward tradeoff measure TRisk that generalises the existing URisk measure (as used in the TREC 2013 Web track's risk-sensitive task) while being theoretically grounded in statistical hypothesis testing and easily interpretable. In particular, we show that TRisk is a linear transformation of the t statistic, which is the test statistic used in the Student's t test. This inherent relationship between TRisk and the t statistic, turns risk-sensitive evaluation from a descriptive analysis to a fully-fledged inferential analysis. Specifically, we demonstrate using past TREC data, that by using the inferential analysis techniques introduced in this paper, we can (1) decide whether an observed level of risk for an IR system is statistically significant, and thereby infer whether the system exhibits a real risk, and (2) determine the topics that individually lead to a significant level of risk. Indeed, we show that the latter permits a state-of-the-art learning to rank algorithm (LambdaMART) to focus on those topics in order to learn effective yet risk-averse ranking systems.
Categories and Subject Descriptors: H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval; G3.3 [Probability and Statistics]: Experimental design
Keywords: Risk-Sensitive Evaluation, Student's t Test
1. INTRODUCTION
Various paradigms for the evaluation of information retrieval (IR) systems rely on many topics to produce reliable estimates of their effectiveness. For instance, in the TREC series of evaluation forums, 50 topics is generally seen as the minimum for producing a reliable test collection [2, 25]. However, in more recent times, the evaluation of systems has increasingly focused upon their robustness - ensuring that a given IR system performs well on difficult topics (as
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609625 .

investigated by the TREC Robust track [24]), or at least as well as a baseline system (which is known as risk-sensitive evaluation [26]). Recently, the TREC 2013 Web track introduced a risk-sensitive task, which assessed how systems could perform effectively yet without exhibiting large losses compared to a pre-determined baseline system [10].
In such a risk-sensitive evaluation, the risk associated with an IR system is defined as the risk of performing a given particular topic less effectively than a given baseline system [8, 9, 26]. In particular, the URisk risk-sensitive evaluation measure [26] calculates the absolute difference of an effectiveness measure (e.g. NDCG) between a given retrieval system and the baseline system, in a manner that more strongly emphasises decreases with respect to the baseline (known as risk) than gains (reward). A parameter   0 controls the riskreward tradeoff towards losses in effectiveness compared to the baseline, where  = 0 weights risk and rewards equally.
In this paper, we argue that in the current practice of risk-sensitive evaluation based on URisk, any amount of loss in an IR system's average effectiveness, observed on a particular set of topics, is considered enough in magnitude to infer that the system exhibits a "real risk". However, from a statistical viewpoint, such an inferential decision may be said to be valid only if the observed amount of loss cannot be attributed to chance fluctuation. Otherwise, it will be equally likely that the corresponding system may or may not be under a real risk, meaning that it is possible that the system can perform every topic with a score higher than that of the baseline system on another set of topics that could be drawn from the population of topics. On the other hand, it is also possible that the observed amount of loss in a particular system's average effectiveness can be attributed to a chance fluctuation, while the corresponding performance losses for some individual topics are statistically significant in magnitude. In other words, significant performance losses for a few topics may not result in a significant total loss on average, given a relatively large set of topics.
Hence, we advocate that risk-sensitive evaluation can actually provide the necessary basis for (i) testing the significance of the observed amount of loss in a given IR system's average effectiveness, called inferential risk analysis in this paper, and (ii) testing the significance of the corresponding losses for individual topics, called exploratory risk analysis.
Indeed, we show that the URisk risk-reward tradeoff measure is actually a linear transformation of the t statistic, as used in the Student's t test. Therefore, using this statistical interpretation of URisk based upon hypothesis testing, this paper proposes a new risk-reward tradeoff measure, TRisk, which is a linear transformation of the existing URisk measure, yet is theoretically grounded upon the Student's t test

23

for testing the significance of the observed amount of loss in a given IR system's average effectiveness. For  = 0, TRisk is equivalent to the standard t statistic used typically in the Student's t test for testing the null hypothesis of equality in the population mean effectiveness for two IR systems. However, for  > 0, the URisk measure emphasises performance losses compared to the baseline effectiveness. This raises challenges in the estimation of the standard error of the calculated URisk scores. For this reason, we propose the use of the Jackknife technique (or leave-one-out) [11], which is a re-sampling technique for estimating the bias and the standard error of any estimate. The Jackknife technique serves two purposes: firstly, to allow the empirical verification of the estimation of the standard error of URisk as valid; and secondly, for testing the significance of the corresponding performance losses for individual topics.
From a practical perspective, a risk-sensitive evaluation serves two objectives: firstly, as a step further than the classical evaluation of IR systems, which takes into account the stability or variance of retrieval results across queries as well as for the average retrieval effectiveness [8, 9]; and secondly, as a technique for jointly optimising the retrieval effectiveness and robustness of retrieval frameworks such as learning to rank [26]. Indeed, compared to the existing URisk measure, this paper contributes to both objectives, by exploiting the theory of statistical hypothesis testing for allowing meaningful interpretation of risk-sensitive evaluation scores, and also by allowing a learning to rank technique, namely LambdaMART, to focus on those topics that lead to a significant level of risk, in order to learn effective yet risk-averse ranking systems. The remainder of this paper is structured as follows: Section 2 provides an overview of risk-sensitive evaluation practices, including URisk; Section 3 relates the URisk measure to the t statistic, and hence proposes the new TRisk risk-sensitive evaluation measure, and discusses the estimation of the standard error. Section 4 and Section 5 describe new forms of analysis, inferential and exploratory respectively, that arise from the TRisk measure, and demonstrate their application upon the TREC 2012 Web track. Next, Section 6 shows how TRisk can improve the robustness of the LambdaMART state-of-the-art learning to rank technique. Finally, we review some related work and provide concluding remarks in Sections 7 & 8, respectively.
2. RISK-SENSITIVE EVALUATION
Different approaches in IR such as query expansion [1, 5] and learning to rank [17] behave differently across topics, often improving the effectiveness for some of the topics while degrading performance for others. This results in a high variation in effectiveness across the topics. To address such variation, there has been an increasing focus on the effective tackling of difficult topics in particular (e.g. through the TREC Robust track [23]), or more recently, on the risksensitive evaluation of systems across many topics [8, 9, 26].
Originally, the aim of risk-sensitive evaluation [9] was to provide new analysis techniques for quantifying and visualising the risk-reward tradeoff of any retrieval strategy that requires a balance between risk and reward. Hence, it facilitates the quest for ranking strategies that are more robust in retrieval effectiveness compared to a baseline retrieval strategy ­ robust in the sense of the stability or variance of the retrieval results across topics, while achieving good average performance over all topics.
The variance with respect to a given baseline system b over a given set of topics Q with c topics can then be measured as

a risk function FRisk, which takes into account the downsiderisk of a new system r (i.e. performing a topic worse than the baseline) is defined in [26] as follows:

FRisk

=

1 c

c

max [0, (bi - ri)] ,

(1)

i=1

where ri and bi are respectively the score of the system r and the score of the baseline system b on topic i, as measured by a retrieval effectiveness measure (e.g. NDCG@20, ERR@20 [6]). Similarly, a reward function FReward, which takes into account the upside-risk (i.e. performing a topic better than the baseline) is defined as:

FReward

=

1 c

c

max [0, (ri - bi)] .

(2)

i=1

Thereby, the overall gain in the retrieval effectiveness of r with respect to b can be expressed as:

UGain = FReward - FRisk.

(3)

Next, a single measure, URisk [26], which allows the riskreward tradeoff to be adjusted, was defined:

URisk = UGain -  · FRisk





=

1 c

q + (1 + )

q , (4)

qQ+

qQ -

where q = rq - bq. The left summand in the square brackets, which is the sum of the score differences q for all q where rq > bq (i.e., q  Q+), gives the total win (or upsiderisk) with respect to the baseline. Orthogonally, the right summand, which is the sum of the score differences q for all q where rq < bq, gives the total loss (or downside-risk). The risk sensitivity parameter   0 controls the tradeoff between reward and risk (or win and loss):  = 0 results in a pure gain model, while for higher , the penalty for under-performing with respect to the baseline is increased: typically  = 1, 5, 10 [10].
In this paper, we extend the original aforementioned aim of risk-sensitive evaluation with the following contributions: 1. A well-established statistical hypothesis testing theory for risk-sensitive evaluations from which arises a new risk measure TRisk (Section 3), to turn risk-sensitive evaluation from a descriptive analysis to a fully-fledged inferential analysis (Section 4). 2. A method for exploratory risk analysis that can identify the topics that commit real levels of risk (Section 5). 3. Adaptations of the proposed TRisk measure that can enhance the robustness of the state-of-the-art LambdaMART learning to rank technique, compared to URisk, without degradations in overall effectiveness, where the learned model adaptively adjusts with respect to the risk level committed by individual topics (Section 6).

3. THE NEW TRISK MEASURE
Without loss of generality, at  = 0, the risk-reward tradeoff measure URisk reduces to the UGain formula in Eq. (3), which can be expressed as the average gain over c topics:

UGain

=

1 c

c

i

=

1 c

c
(ri - bi).

(5)

i=1

i=1

In the context of statistics, UGain refers to the sample mean of paired score differences, d¯, for two IR systems (the system under evaluation r and the baseline system b):

24

d¯ =

r¯ - ¯b

=

1 c

c
(ri - bi) = UGain

(6)

i=1

and in the context of evaluating IR systems, this refers to the difference in average effectiveness between two IR systems, r¯-¯b, where r¯ and ¯b are respectively the average effectiveness of system r and the average effectiveness of the baseline system b over c topics.
On the other hand, the Student's t statistic for matched pairs, as is commonly applied when testing the significance of results between two systems, can be expressed as:

t

=

d¯ SE(d¯)

=

r¯ - ¯b SE(d¯)

,

(7)

Within Eq. (7), the standard error of paired sample mean, SE(d¯), can be estimated as follows:

SE(d¯)

=

sd c

,

(8)

where sd = c-1 (i - d¯)2 is the paired sample standard

deviation. Hence, we argue that the Student's t statistic of

Eq. (7) is actually a linear transformation of UGain from

Eq. (3), which we call TGain:



TGain

=

UGain S E (UGain )

=

c sd

×

UGain .

(9)

This transformation can be referred to as studentisation (c.f., t-scores) [14], which in fact is a type of standardisation (i.e., z-scores). Standardisation is a monotonic linear transformation, which transforms any given set of data to a set with zero mean and unit variance, while preserving the original data distribution in shape.
The t-score of a raw UGain measurement, TGain, differs from the raw measurement in two important aspects. First, given a set of IR systems, a test collection, and a baseline system, the systems' ranking to be obtained on the basis of TGain will not necessarily be concordant with the systems' ranking to be obtained on the basis of UGain, since the t statistic takes into account the inherent variation in the observed paired score differences ri - bi across the topics, i.e., SE(UGain). Second, given a particular baseline system, the two TGain scores to be obtained on two different test collections for the same IR system are comparable with each other in magnitude, at least in theory [7], while the two UGain scores are not, as typical in the case of the two raw effectiveness scores to be yielded from a standard effectiveness measure, such as mean average precision [28].
Having shown how TGain can be defined as a linear transformation of UGain, based upon the t statistic, we now examine URisk, which allows the risk-reward tradeoff to be controlled by the  parameter. For   0, the t statistic based on URisk, which we call TRisk, can be expressed as follows:

TRisk

=

URisk S E (URisk

)

.

(10)

Although both the TGain formula in Eq. (9) and the TRisk formula in Eq. (10) stem from the classical t statistic in Eq. (7), the estimation of the standard error in URisk, the estimation of SE(URisk) within TRisk, is not as straightforward as in the case of SE(UGain), for the reason that the URisk formula reweighs the score differences i in averaging, proportionally to , for each topic i where ri < bi, as opposed to UGain. Hence, in the remainder of this section, we propose two methods to estimate SE(URisk): A

speculative parametric estimator SEx¯ that is an analogy to the paired sample standard deviation sd (Section 3.1); and a nonparametric Jackknife Estimator SEJ , based on the leaveone-out Jackknife technique (Section 3.2). Indeed, later in Section 3.3, we use the Jackknife Estimator SEJ to show the validity of the speculative SEx¯ estimator.
On the other hand, TRisk has several advantages over URisk. Firstly, it can be easily interpreted for an inferential analysis of risk. Indeed, we will later show in Section 4 that in order to test the significance of an observed riskreward tradeoff score between a particular IR system and a provided baseline system, one can use TRisk as the test statistic of the Student's t test for matched pairs.
Secondly, TRisk permits the identification of topics that commit significant risk or not ­ we call this exploratory risk analysis ­ which we present later in Section 5.
Finally, this exploratory risk analysis leads to new risksensitive measures that can be directly integrated into the LambdaMART learning to rank technique, to produce learned models that exhibit less risk than those obtained from URisk whilst not degrading effectiveness, as explained in Section 6.

3.1 Parametric Estimator of SE(URisk)
Let the random variable Xi denote the risk-reward tradeoff score between system r and baseline b for topic i:

Xi =

i (1 + )i

if ri > bi if ri < bi

(11)

for i = 1, 2, . . . , c and a predefined value of   0. Then, the

standard error of URisk, SE(URisk) can be approximated by the standard error of the sample mean x¯:

SEx¯

=

sx c

,

(12)

where s2x = c-1 (xi - x¯)2. Here, the sample mean x¯ corresponds to the URisk score considered as the arithmetic mean of the sample of the observed individual topic risk-reward
tradeoff scores x1, x2, . . . , xc at a predefined value of :

x¯

=

URisk

=

1 c

c

xi.

i=1

(13)

This parametric estimator of SE(URisk), SEx¯, is speculative and hence its validity might be compromised to some extent. Therefore, we empirically verify the validity of SEx¯ in estimating SE(URisk) by means of comparing it with a nonparametric re-sampling technique, called the Jackknife [21], which we present in Section 3.2. Indeed, by comparing the two estimates of SE(URisk) (i.e., the parametric estimate SEx¯ of Eq. (12) and the nonparametric Jackknife estimate of SE(URisk)), one can decide whether an inference to be made on the basis of the TRisk statistic is valid. If the two estimates agree with each other, such an inference may be said to be valid, otherwise its validity is compromised.

3.2 Jackknife Estimate of SE(URisk)
In this paper, the Jackknife technique is employed for a purpose which serves two different aims: 1) as a mechanism of the empirical verification of the validity of an inference to be made based on the TRisk statistic in Eq. (10), and 2) as a mechanism for exploratory risk analysis.
Jackknife, which is also known as the Quenouille-Tukey Jackknife or leave-one-out, was first introduced by Quenouille [18] and then developed by Tukey [21]. Tukey used the Jackknife technique to determine how an estimate is affected by the subsets of observations when discordant values

25

(i.e., outlier data) are present. In the presence of discordant values, it is expected that the Jackknife technique could reduce the bias in the estimate. Although the original objective of Jackknife is to detect outliers, in principle it is a re-sampling technique for estimating the bias and the standard error of any estimate [11]. In Jackknife, the same test is repeated by leaving one subject out each time: this explains why this technique is also referred to as leave-one-out.
Let the random variables X1, X2, . . . , Xc denote a random sample of size c, such that Xi is drawn identically and independently from a distribution F for i = 1, 2, . . . , c. Suppose that the goal is to estimate an unknown parameter  of F . It can be shown that  can be estimated by a statistic ^, which is derived from an observed sample x1, x2 . . . , xc from F , with a measurable amount of sampling error [15].
An unbiased estimator ^ is a statistic whose expected value E(^) is equal to the true value of the population parameter of interest , i.e., E(^) = . The amount of bias associated with an estimator is therefore given by:

bias(^) = E(^ - ) = E(^) - .

(14)

We denote as X(i) the sub-sample without the datum Xi. There are in total c sub-samples of size c-1 for i = 1, 2, . . . , c:
X(i) = X1, X2, . . . , Xi-1, Xi+1, . . . , Xc. Next, let the estimate derived from the ith sub-sample X(i)
be denoted as ^(i), and the mean over c sub-samples be:

^(.)

=

1 c

c

^(i).

i=1

(15)

The Jackknife estimate of bias, which is actually a nonparametric estimate of E(^ - ), is defined as follows [21]:

biasJ (^)

=

(c -

1)(^(.)

-

^)

=

(c - 1) c

c

(^(i) - ^).

i=1

and, in accordance, the bias-reduced Jackknife estimate of  is defined as ~ = ^ - biasJ (^) = c^ - (c - 1)^(.).
Tukey [21] showed that the Jackknife technique can also
be used to estimate the variance of ^ by introducing the so-called pseudo-values, ~(i) = c^ - (c - 1)^(i), such that

varJ (^)

=

1 c(c - 1)

c

~(i)

- ~

2

=

(c - 1) c

c

^(i) - ^(.)

2
.

i=1

i=1

This nonparametric Jackknife estimate of variance gives the empirical estimate of the standard error of ^:

SE(^) = varJ (^).

(16)

For the TRisk statistic in Eq. (10), the standard error of
URisk, SE(URisk), can hence be estimated by substituting URisk into Eq. (16) as ^:

SEJ = varJ (URisk).

(17)

3.3 Empirical Validation of SE(URisk)
The nonparametric estimator SEJ is an alternative to the parametric estimator SEx¯ (Eq. (12)). In this section, we empirically compare these estimates of SE(URisk) with each other, to assess the validity of the result of a hypothesis test to be performed using TRisk as the test statistic. In general, if the two estimates agree, the test result may be said to be valid, and otherwise its validity will be compromised. As a result, nonparametric methods can help to alleviate doubts about the validity of the analysis performed [14].

In the following, we compare the estimates using the submitted runs to the TREC Web track. In particular, the provided baseline run for the TREC 2013 Web track risksensitive task is based on the Indri retrieval platform. However, as the submitted runs and results for the TREC 2013 campaign were not yet publicly available at the time of writing, in the following we perform an empirical study based on runs submitted to the TREC 2012 Web track. Indeed, the 2013 track coordinators have made available a set of Indri runs on the TREC 2012 Web track topics1 that correspond to the TREC 2013 baseline runs - in our results, we use the 2012 equivalent run to the 2013 pre-determined baseline, the so-called indriCASP. We report the URisk values obtained using the official TREC 2012 evaluation measure, ERR@20.
Table 1 reports the parametric estimates (SEx¯) and the nonparametric Jackknife estimates (SEJ ) of the standard errors associated with the average risk-reward tradeoff scores (URisk), calculated for each of the TREC 2012 Web track top 8 ad-hoc runs over c = 50 topics, with respect to the indriCASP baseline, applying several risk-sensitivity parameter values of  = 0, 1, 5, 10. From the results, it can be observed that the two estimates, SEx¯ and SEJ agree with each other for each of the 8 runs. In fact, over all of the 48 runs submitted to the TREC 2012 Web track, we observe a Root Mean Square Error (RMSE) of 0.000 between SEx¯ and SEJ . Thus, we conclude that it is highly likely that it would be valid to conduct an inferential risk analysis upon those TREC 2012 runs based on the new risk-reward tradeoff measure TRisk (Eq. (10)), regardless of how SE(URisk) is estimated. An example of inferential risk analysis based on TRisk follows in the next section.

4. INFERENTIAL RISK ANALYSIS
The goal of the classical evaluation of IR systems is to decide whether one IR system is better in retrieval effectiveness than another on the population of topics. This goal can be formulated into a (two-sided) null hypothesis, as given by:

H0 : µr = µb or H0 : µr - µb = 0,

(18)

against the alternative hypothesis H1 : µr = µb, where µr and µb represent respectively the population mean performance of the system r and the population mean performance of the baseline system b. The test statistic for this null hypothesis is the t statistic (Eq. (7)), since the larger values of t are evidence against the null hypothesis H0 : µr - µb = 0. Below, we describe the hypothesis testing of H0 in abstract terms, before explaining how it can be applied to TRisk (Section 4.1) and illustrating its application upon the TREC 2012 Web track runs (Section 4.2).
In order to decide how much difference between the two sample means r¯ and ¯b is assumed to be large enough to reject the null hypothesis, we should first determine how much difference can be attributed to a chance fluctuation. It can be shown that, under the null hypothesis H0, the sampling (or null) distribution of the test statistic t can be approximated by a Student's t distribution with df = c - 1 degrees of freedom for any population distribution with finite mean µ and variance 2 > 0, because of the central limit theorem [12]. Thus, at a predefined significance level of  (typically  = 0.05 for 95% confidence), two standard deviations (±t(/2,df) × SE(d¯)) determine the maximum difference that can be attributed to chance fluctuation, where in between the critical values ±t(/2,df) the area under the Student's t
1https://github.com/trec-web/trec-web-2013

26

Table 1: Calculated risk-reward tradeoff scores, URisk for the TREC 2012 Web track top 8 ad-hoc runs at the risk-sensitivity parameter values of  = 0, 1, 5, 10, along with the parametric estimates SEx¯ and the nonparametric Jackknife estimates SEJ of the associated standard errors SE(URisk). indriCASP is the baseline.

=0

=1

=5

 = 10

ERR@20 URisk SEx¯ SEJ URisk SEx¯ SEJ URisk SEx¯ SEJ URisk SEx¯ SEJ

uogTrA44xi

0.3132 0.1185 0.0528 0.0528 0.0556 0.0739 0.0739 -0.1959 0.1755 0.1755 -0.5104 0.3091 0.3091

srchvrs12c09

0.3049 0.1102 0.0479 0.0479 0.0679 0.0644 0.0644 -0.1015 0.1489 0.1489 -0.3133 0.2619 0.2619

DFalah121A

0.2920 0.0974 0.0425 0.0425 0.0467 0.0632 0.0632 -0.1558 0.1588 0.1588 -0.4089 0.2827 0.2827

QUTparaBline

0.2901 0.0954 0.0448 0.0448 0.0385 0.0672 0.0672 -0.1893 0.1703 0.1703 -0.4740 0.3033 0.3033

utw2012fc1

0.2195 0.0248 0.0449 0.0449 -0.0558 0.0705 0.0705 -0.3782 0.1849 0.1849 -0.7813 0.3314 0.3314

ICTNET12ADR2 0.2149 0.0203 0.0416 0.0416 -0.0495 0.0637 0.0637 -0.3286 0.1648 0.1648 -0.6774 0.2950 0.2950

indriCASP

0.1947

*

*

*

*

*

*

*

*

*

*

*

*

irra12c

0.1723 -0.0223 0.0410 0.0410 -0.1182 0.0693 0.0693 -0.5014 0.1904 0.1904 -0.9805 0.3437 0.3437

qutwb

0.1659 -0.0287 0.0462 0.0462 -0.1342 0.0791 0.0791 -0.5560 0.2194 0.2194 -1.0832 0.3969 0.3969

distribution sums up to (1 - ). If an observed t-score is greater than t(/2,df), or less than -t(/2,df), one can reject H0 with 100%(1 - ) confidence, denoted as the p-value.
4.1 Inference Based on TGain and TRisk
The above protocol of hypothesis testing is referred to as the Student's t test for matched pairs, or paired t test for short, in statistics. Hence, in the context of risk-sensitive evaluation, the TGain formula in Eq. (9) stands for the test statistic t. In fact, at  = 0, testing the significance of an observed risk-reward tradeoff score between r and b (i.e. an observed UGain score) is akin to testing the significance of the observed difference between r¯ and ¯b.
To test the significance of an observed UGain score, one can therefore compare the corresponding TGain score with the two-sided critical ±t(/2,df) values at a desired level of significance . If -t(/2,df)  TGain  t(/2,c-1), the observed UGain score can be attributed to chance fluctuation, meaning that the observed gain in the performance of the system r with respect to the baseline system b is not statistically significant. In such a case, it is equally likely that the observed UGain score may or may not occur on another topic sample drawn from the population. Otherwise, if TGain  -t(/2,c-1) or TGain  t , (/2,c-1) one can however be sure that a UGain score at least as extreme as the observed score would occur on 100(1 - )% of the topic samples that could be drawn from the population.
Both TGain and TRisk stem from the t statistic. Indeed, for  = 0, TGain = TRisk, while for  > 0, SE(URisk) was shown to be valid in Section 3.3. Hence, we argue that an equivalent inferential analysis can be conducted upon the TRisk scores that have been calculated based on URisk. In the following, we provide an illustration of such inferential analysis upon runs submitted to the TREC 2012 Web track, but the same inferential analysis methodology could be applied for any risk-sensitive evaluation scenario.
4.2 Inferential Analysis of Web Track Runs
Given a particular IR system, a baseline system, and a set of c topics, one can use the paired t test for testing the significance of the calculated average tradeoff score between risk and reward over the c topics, URisk, by comparing the corresponding t-score, TRisk, with the critical values ±t(/2,df) at a desired level of significance . To illustrate such an analysis, Table 2 reports the URisk risk-reward tradeoff scores based on ERR@20, and the corresponding TRisk scores for the 8 highest performing TREC 2012 ad-hoc runs, given the baseline run indriCASP (we omit other submitted runs for brevity, however the following analysis would be equally applicable to them). As the TREC 2012 Web track has 50

topics, for a significance level of  = 0.05, the critical values for TRisk are ±t(0.025,49) = ±2.
In Table 2, the URisk scores to which a two-sided paired t test gives significance are those that have a corresponding TRisk score less than -2 or greater than +2. For example, at  = 0, the calculated URisk scores of the top 4 runs are significant with a p-value less than 0.05. This means that, under the null hypothesis H0 : µr = µb, given another sample of 50 topics from the population, the probability of observing a risk-reward tradeoff score, between any one of these 4 runs and the baseline run indriCASP, that is as extreme or more extreme than the one that was observed is less than 0.05, i.e. the associated p-values. Since TRisk > 0, for those runs, the declared significance counts in favour of "reward" against "risk". Thus, one can conclude, with 95% confidence, that the expected per topic effectiveness of each of the top 4 runs is, on average, higher than the expected per topic effectiveness of the baseline run indriCASP on the population of topics. In other words, given a topic from the population, it is highly likely that any one of the top 4 runs will not perform worse for that topic than indriCASP. This suggests, as a result, that those top runs do not exhibit a real risk that is generalisable to the population of topics.
On the other hand, a run with TRisk < -2 at  = 0 will be under a real risk, though among the shown top 8 TREC 2012 runs there is no such run. For those runs with -2  TRisk < +2, such as utw2012fc1 and qutwb, the risk analysis performed here is inconclusive, since the associated URisk scores can be attributed to chance fluctuation, i.e. it is equally likely that they may or may not be under a real risk.
Next, we observe from Table 2 that as  increases, the observed tradeoffs between risk and reward for each run changes in favour of risk compared to reward, hence the runs exhibiting significant URisk scores change. For example, each of the runs with significant URisk scores at  = 0 (i.e., the top 4 runs) have a URisk score that can be attributed to a chance fluctuation at  = 10, while, in contrast, those runs whose URisk scores can be attributed to chance fluctuation at  = 0 (i.e., the last 4 runs) have a significant URisk score at  = 10.
Figure 1 shows the change in the TRisk scores of the TREC 2012 top 8 ad-hoc runs for several risk-sensitivity  parameter values from 0 to 15. From the figure, we observe that for  > 5 the TRisk scores for all runs are negative in sign, and for the last 4 runs the calculated URisk scores can be considered statistically significant (i.e., TRisk > -2.0 for  > 5). It is also observed that, even for  = 15, the calculated URisk scores of the top 4 TREC runs can still be attributed to chance fluctuation.
As a result, the inferential analysis performed so far suggests that, in general, none of the 8 top TREC 2012 ad-hoc

27

Table 2: URisk and TRisk scores risk-reward tradeoff scores for the top 8 TREC 2012 ad-hoc runs at  = 0, 1, 5, 10,

where the baseline is indriCASP. The underlined URisk scores are those for which a two-tailed paired t test

gives significance with p < 0.05 - i.e. exhibit a TRisk score greater than +2 or less than -2.

=0

=1

=5

 = 10

URisk TRisk p-value URisk TRisk p-value URisk TRisk p-value URisk TRisk p-value

uogTrA44xi

0.1185 2.2440 0.029 0.0556 0.7528 0.455 -0.1959 -1.1163 0.270 -0.5104 -1.6512 0.105

srchvrs12c09

0.1102 2.3034 0.026 0.0679 1.0541 0.297 -0.1015 -0.6817 0.499 -0.3133 -1.1961 0.237

DFalah121A

0.0974 2.2899 0.026 0.0467 0.7401 0.463 -0.1558 -0.9808 0.332 -0.4089 -1.4466 0.154

QUTparaBline 0.0954 2.1305 0.038 0.0385 0.5723 0.570 -0.1893 -1.1116 0.272 -0.4740 -1.5626 0.125

utw2012fc1

0.0248 0.5526 0.583 -0.0558 -0.7914 0.432 -0.3782 -2.0457 0.046 -0.7813 -2.3574 0.022

ICTNET12ADR2 0.0203 0.4869 0.629 -0.0495 -0.7775 0.441 -0.3286 -1.9942 0.052 -0.6774 -2.2960 0.026

irra12c

-0.0223 -0.5446 0.588 -0.1182 -1.7038 0.095 -0.5014 -2.6335 0.011 -0.9805 -2.8525 0.006

qutwb

-0.0287 -0.6226 0.536 -0.1342 -1.6956 0.096 -0.5560 -2.5342 0.015 -1.0832 -2.7295 0.009

Figure 1: The change in standardised TRisk scores for the top TREC 2012 ad-hoc runs for 0    15.

3 Upper Critical Value
2

1

Null Hypothesis H0:µr = µb

uogTrA44xi srchvrs12c09 DFalah121A QUTparaBline utw2012fc1 ICTNET12ADR2 indriCASP irra12c qutwb

0

TRisk score

-1 Lower Critical Value
-2

-3 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
 value
runs are under a real risk of performing any given topic from the population worse than the baseline run indriCASP, on average. In particular, there can be no significant reduction in risk that could be attained for the top 4 systems, given a baseline system with the average retrieval effectiveness of indriCASP. On the other hand, a significant reduction in risk could be attained, on average, for the last 4 systems, particularly for  > 5.
Lastly, in Table 2, it is notable that the high URisk scores do not necessarily imply high TRisk scores, because of the fact that each system would in general have a different inherent variation in ri -bi across topics (i.e. SE(URisk)) from that of the other systems. For example, consider the runs uogTrA44xi and srchvrs12c09. At  = 0, uogTrA44xi has a URisk score (0.1185) higher than the URisk score (0.1102) of srchvrs12c09, while srchvrs12c09 has a higher TRisk score than uogTrA44xi, i.e. 2.3034 vs. 2.2440. This shows that a ranking of retrieval systems obtained based on TRisk will not necessarily be concordant with the ranking of systems obtained based on URisk.
5. EXPLORATORY RISK ANALYSIS
In the previous section, the risk analysis that we performed could hide significant performance losses on individual topics. Nevertheless, one can perform an exploratory risk analysis to determine those individual topics on which the observed risk-reward tradeoff score between a given IR system and the baseline system (i.e., xi) is statistically significant. In the following, we provide a definition for exploratory risk analysis (Section 5.1), which we later illustrate upon the TREC 2012 Web track runs (Section 5.2).
5.1 Definition
The TRisk measure permits the topic-by-topic analysis of risk-reward tradeoff measurements, which we refer to as ex-

ploratory risk analysis. Such an analysis is implicitly sug-

gested by the t statistic itself. The t statistic in Eq. (7) can

be rewritten as follows:

t=

d¯ SE(d¯)

=

1 c

c i=1

(ri

-

bi)

sd/ c

=

 c c

c i=1

ri

- sd

bi

.

(19)

In here,

each component of the sum ti

=

: ri -bi
sd

gives the

standardised score of the observed difference in effectiveness

between the system r and the baseline system b on topic i,

for i = 1, 2, . . . , c.

In analogy, the TRisk measure, which stems from the t statistic, can be rewritten as:

TRisk =

URisk SEx¯

=

1 c

c
i=1

xi

sx/ c

=

 c c

c i=1

xi sx

,

(20)

where each component of the sum, in this case, gives the

standardised score of the individual topic risk-reward trade-

off measurements x1, x2, . . . , xc:

TRi

=

xi sx

.

(21)

In a similar manner that we compare the calculated TRisk score of a given IR system with the two-sided critical values ±t(/2,df) to decide whether the system exhibits a significant level of risk on average (Section 4), to decide whether an observed loss (or gain) on a particular topic i is significant, we can compare the component TRi score with the same critical values ±t(/2,df) , at a desired significance level of . If -t(/2,df)  TRi  t(/2,df), the observed loss (or gain) can be attributed to chance fluctuation, and otherwise it can be considered statistically significant.
Indeed, this is one of the typical methods of outlier detection in statistics [14]. Recall that the original objective of Jackknife is to detect outliers [21]. The TRisk measure can also be expressed in terms of the Jackknife estimate of bias, following Wu [29]:

TRisk

=

URisk SEJ

=

1 c

c i=1

(c

-

1) (^(i) SEJ

-

^)

.

(22)

Here, each component of the sum:

TJi =

(c - 1) (^(i) SEJ

- ^)

=

(c - 1) (x¯(i) - x¯) , (23) varJ (x¯)

gives the standardised Jackknife estimate of bias in URisk due to leaving the topic risk-reward score xi out of the sam-
ple x1, x2, . . . , xc, where x¯ = URisk and x¯(i) is the URisk score to be obtained when the ith topic is leaved out of the
topic set in use, for i = 1, 2, . . . , c.
In general, both the TRi statistic in Eq. (21) and the TJi statistic in Eq. (23) can be used for the purpose of exploratory risk analysis. However, there is a certain difference

28

between them in theory. Using TRi , we can decide whether an observed performance loss on topic i is significant, by comparing the topic risk-reward score xi with the maximum score that can be attributed to chance fluctuation, but as if the single datum xi is the whole sample. In contrast, using TJi , we can make the same decision by comparing the observed difference between two URisk scores, x¯(i) - x¯, with the maximum difference that can be attributed to chance fluctuation. Since we showed in Section 3.3 that the two estimates of the standard error for each TREC run are in perfect agreement (i.e. SEx¯  SEJ ), we argue that this theoretical difference has no practical consequences. Hence, in the following, we provide an illustration of exploratory risk analysis on the TREC 2012 Web track runs, based on TJi alone. However, initial experiments showed no differences between TRi and TJi .
5.2 Exploratory Analysis of Web Track Runs
Figure 2 shows the standardised Jackknife estimate of bias in the URisk scores calculated for two TREC runs, namely uogTrA44xi and qutwb at  = 0, 5, 10, 15 for the 50 TREC 2012 Web track topics, where indriCASP is the baseline. This standardised Jackknife estimate of bias, TJi is estimated by leaving one TREC 2012 Web track topic out of the set of topics {151, 152, . . . , 200} in turn. In the figure, the topics that result in a significant performance loss (gain) for the corresponding systems with respect to indriCASP, at the significance level of  = 0.05, are those which have a TJi score less than -2 (greater than 2, respectively). Horizontal lines at -2 and +2 are shown to aid clarity.
From Figure 2, at  = 0 it can be observed that uogTrA44xi has more significant wins in number than qutwb, and less significant losses. This shows in detail why the declared significance for uogTrA44xi in Section 4 counts in favour of reward against risk, while the observed tradeoff between risk and reward can be attributed to chance fluctuation for qutwb, with respect to the baseline indriCASP.
In general, both of the runs uogTrA44xi and qutwb exhibit considerable performance losses with respect to indriCASP on the same topics, including 166, 172, 174, 175, and 191, out of which 2 are significant for uogTrA44xi (i.e., 166 and 175) and 4 are significant for qutwb (i.e., 166, 172, 175, and 191), at  = 0. In particular, consider the topic 166, on which the magnitude of the TJi score is nearly the same for both runs. It is notable here that, as  increases, the significance of that topic relatively doubles for uogTrA44xi, while for qutwb it nearly remains the same. The situation is also similar for topic 175, though the TJi score of uogTrA44xi at  = 0 is small in magnitude compared to that of qutwb.
This is one of the important differences between TRisk and URisk in assessing the risk associated with IR systems. Given a particular topic i, the same amount of performance loss with respect to a provided baseline effectiveness can lead to different TJi (and TRi = xi/SEx¯) scores for different IR systems, depending on the variation in the observed riskreward tradeoff across the topics (i.e., different SEx¯ for different systems), while leading to the same topic risk-reward score, xi, for i = 1, 2, . . . , c. As  increases, the topic riskreward score xi increases proportionally for both of the runs uogTrA44xi and qutwb. However, the tradeoff counts, on average, significantly in favour of reward against risk for uogTrA44xi, whereas, it counts neither in favour of reward nor against risk for qutwb, as shown in Section 4. Thus, the same margin of increase in topic risk-reward tradeoff score xi in

favour of risk should lead to a relatively higher level of risk
for uogTrA44xi than that for qutwb, in a way that TJi did. Assessing the level of risk that a topic commits for a given
IR system relative to the level of risk associated with the sys-
tem on average is a property unique to the measures TJi and TRi . Besides the use of these measures for exploratory risk analysis, this property also enables adaptive risk-sensitive
optimisation within a learning to rank technique, as we ex-
plain in the next section.

6. ADAPTIVE RISK OPTIMISATION
In this section, we describe how to exploit the new riskreward tradeoff measure TRisk (Eq. (10)) in learning robust ranking models that maximises average retrieval effectiveness while minimising risk-reward ratio, in the context of the state-of-the-art LambdaMART learning to rank technique [30]. As discussed below, Wang et al. [26] proposed to integrate URisk (Eq.(4)) within LambdaMART to achieve risk sensitive optimisation, by using  to penalise risk during the learning process. However, URisk considers topics equally regardless of the level of risk they commit. In contrast, we propose to adaptively change the level of risksensitivity, so that the total risk-sensitivity is distributed across the topics proportionally to the level of risk each topic commits. In the following: Section 6.1 provides an overview of the LambdaMART objective function, while Section 6.3 describes the integration of URisk within LambdaMART; Section 6.3 explains our proposed adaptive risk-sensitive optimisation approaches, with the experimental setup & results following in Sections 6.4 & 6.5, respectively.

6.1 LambdaMART
LambdaMART [30] is a state-of-the-art learning to rank technique, which won the 2011 Yahoo! learning to rank challenge. It can be described as a tree-based technique, in that its resulting learned model takes the form of an ensemble of regression trees, which is used to predict the score of each document given the document's feature values. During learning, LambdaMART creates a sequence of gradient boosted regression trees that improve an effectiveness metric. In general, for our purposes2, it is sufficient to state that LambdaMART's objective function is based upon the product of two components: (i) the derivative of a crossentropy that originates from the RankNet learning to rank technique [3] calculated between the scores of two documents a and b, and (ii) the absolute change M in an evaluation measure M due to the swapping of documents a and b [4]. Therefore the final gradient naew of a document a within the objective function is obtained over all pairs of documents that a participates in for query q:

naew =

ab · |Mab|

b=a

where ab is RankNet's cross-entropy derivative, and Mab is the change in an evaluation measure M by swapping documents a and b. Various IR evaluation measures are suitable for use as M , including NDCG and MAP, as they have been shown to satisfy a consistency property [4]: for a pair of documents a and b where a is ranked higher than b, if the relevance label of a is higher than b, then a "degrading" swap of a and b must result in a decrease in M (i.e. M  0), and orthogonally M  0 for "improving" swaps.

2Further details on LambdaMART can be found in [4, 26].

29

Figure 2: and qutwb

Bar graph showing the at  = 0, 5, 10, 15, where

standardised indriCASP is

Jackknife estimate the baseline.

of

bias

in

the

URisk ,

TJi ,

for

uogTrA44xi

uogTrA44xi

qutwb

TJ at =0 i

2 0 -2

TJ at =5 i

TJ at =10 i

4 2 0 -2 -4
4 2 0 -2 -4
4 2 0 -2 -4
151 156 161 166 171 176 181 186 191 196

151 156 161 166 171 176 181 186 191 196

TJ at =15 i

6.2 Risk-Sensitive Optimisation
Wang et al. [26] demonstrated that a more robust learned model could be obtained from LambdaMART if the M is replaced by the difference in URisk for a given swap of two documents, denoted T . In doing so, their implementation weights the value of M by  + 1 only for the topics with down-side risk, while for the topics with up-side risk it leaves M as is, T = M . T was shown to exhibit the consistency property iff the underlying evaluation measure M is consistent (e.g. as obtained from NDCG).
6.3 Adaptive Risk-Sensitive Optimisation
Compared to URisk, TRisk is grounded in the theory of hypothesis testing and produces values that are easily interpretable ­ as shown in Section 4. However, as a linear transformation of URisk, the direct application of TRisk as T within LambdaMART to attain risk-sensitive optimisation cannot offer marked improvements on the resulting learned models. On the other hand, the exploratory risk analysis of Section 5 offers a promising direction, as it permits the learning to rank process to adaptively focus on topics depending upon the level of risk that they commit. In this section, we propose two new models of adaptive risk-sensitive optimisation that exploit the standardised topic risk-reward tradeoff scores (TRi , Eq. (21)), but which differ on which individual topics they operate on. In particular, the first model, SemiAdaptive Risk-sensitive Optimisation (SARO), focuses only on the topics with down-side risk and augments only the corresponding M values. In contrast, the Fully Adaptive Risk-sensitive Optimisation (FARO) model operates on all topics and augments every M value. Hence, compared to URisk as used in [26], FARO and SARO both alter the importance of riskier topics within the learning process.
In URisk, M is multiplied by  + 1 if the topic commits a downside risk3. This amounts to a static level of sensitivity for each topic, irrespective of the level of risk that the topic commits. In contrast, based on the standardised topic
3This follows directly from the definition of Eq. (4), however the consistency proof in Section 4.3.2 of [26] defines T for different scenarios.

risk-reward tradeoff scores, TRi (Eq. (21)), we propose to adaptively adjust  so that the total level of sensitivity can
be distributed across the topics proportional to the levels
of risk that they commit. In order to achieve this, for each
topic we must estimate the probability of observing a risk-
reward score greater than the actual observed TRi score. Technically speaking, we need to estimate the cumulative
probability P r (Z  TRi ), where TRi is the observed riskreward tradeoff score and Z is the corresponding standard
normal variable of TRi for all topics i = 1, 2, .., c. For large sample sizes (generally agreed to be  30), the distribu-
tion of the t statistic in Eq. (7) can be approximated by
the standard normal probability distribution function, with
zero mean and unit variance [15]. Thus, the probability
P r (Z  TRi ), which is the probability of a topic risk-reward score greater than TRi , can be estimated by the standard normal cumulative distribution function (·), as follows:

P r (Z  TRi )  1 -  (TRi ) ,

(24)

for i = 1, 2, . . . , c. (Z) is a monotonically increasing func-
tion of the standard normal random variable Z, where 0 
(Z = z)  1 for -  z  , and at Z = 0, (Z) = 0.5. Hence, we can replace the original  in T as  as follows:

 = [1 - (TRi )] · .

(25)

where 0    . As the level of risk TRi committed by topic i increases,  also increases. By substituting  into T (as defined by Wang et al. [26]), this augments the M values for every topic with a weight proportional to the level of risk that each topic commits.
The application of  differs between the SARO and FARO models. In particular, SARO only addresses the down-side risk, as in the case of URisk. Indeed, under the null hypothesis H0 : µr = µb, the higher the level of down-side risk (i.e. the larger the size of the difference ri - bi < 0), the higher the probability of observing a topic risk-reward tradeoff score greater than the observed score (P r (Z  TRi )). Hence, SARO varies  from 0 to , according to the downside risk of each topic.
On the other hand, FARO operates on all topics. Indeed, for the topics with up-side risk, FARO gives lower weights

30

to the topics that more strongly outperform the baseline
system (i.e. as the difference ri - bi > 0 increases). At the extreme, if topic i exhibits maximal improvements over the baseline (i.e. ri - bi = 1), then (TRi ) = 1, and hence topic i has minimal emphasis on the learner. In other words, the learner focuses on improving the riskier topics. FARO operates on all topics, by redefining T as follows:

T  = (1 + ) × M,

(26)

Moreover, for  = 0,  = 0, hence T  = M , i.e. the gain-only LambdaMART, as for URisk.
Finally, we informally comment on the consistency of SARO and FARO: For both models, we calculate SE(URisk) after the first iteration of boosting within LambdaMART, and
not for each considered swap ­ we found this to be sufficient to obtain accurate estimates of SE(URisk); Next, the consistency of SARO follows from URisk, as our replacement of  with , as 0    . For FARO, T  only changes sign with M , again as 0    . Hence, as long as M is consistent, both SARO and FARO are also consistent.

6.4 Experimental Setup
We implement the URisk, SARO and FARO models within the Jforests implementation [13] of LambdaMART4. Experiments are conducted using the large MSLR-Web10k learning to rank dataset5, as used by Wang et al. [26]. This dataset encompasses 9,685 queries with labelled documents obtained from a commercial web search engine. For each ranked document for each query, a range of 136 typical query-independent, query-dependent and query features are provided.
We use identical hyper-parameters for LambdaMART to those described by Wang et al. [26], namely: the minimum number of documents in each leaf m = 500, 1000, the number of leaves l = 50, the number of trees in the ensemble nt = 800 and the learning rate r = 0.075. The best m value is chosen for each of the five folds using the validation topic set, based on the NDCG@10 performance of the original LambdaMART algorithm, and used for all experiments for that fold thereafter. For the calculation of risk measures, like [26], we use the ranking obtained from the BM25.whole.document feature as the baseline system. The NDCG@10 performance of this baseline is 0.309.
The performances obtained for LambdaMART upon the MSLR-Web10k in terms of NDCG@1 and NDCG@10 are similar in magnitude to those reported by Wang et al. [26], however we note some differences in the risk profile. Such differences are expected given the different implementations: Wang et al. [26] used a private implementation of LambdaMART, while we use and adapt an open source machine learning toolkit for URisk, SARO and FARO. Nevertheless, the reported results allow valid conclusions to be drawn, including identical conclusions to [26] on the impact of using URisk within LambdaMART.
6.5 Results for SARO and FARO
Table 3 reports the effectiveness and robustness results for FARO and SARO along with URisk, for  = 1, 5, 10, 206. In the table, the gain over the baseline effectiveness is ex-
4All of our code has been integrated to Jforests, available at https://code.google.com/p/jforests/ 5http://research.microsoft.com/en-us/projects/mslr/ 6=0 is equivalent to the normal LambdaMART algorithm.

Table 3: Results for SARO, FARO and URisk.
 = 0  = 1  = 5  = 10  = 20

NDCG@1 (URisk) NDCG@1 (SARO)

0.472 0.468 0.458 0.442 0.423 - 0.470 0.463 0.455 0.439

NDCG@1 (FARO)

- 0.468 0.467 0.470 0.469

NDCG@10 (URisk) NDCG@10 (SARO)

0.480 0.478 0.470 0.458 0.448 - 0.479 0.474 0.468 0.458

NDCG@10 (FARO)

- 0.479 0.477 0.479 0.478

Risk/Reward (URisk) 0.172 0.168 0.164 0.176 0.185 Risk/Reward (SARO) - 0.167 0.164 0.169 0.177

Risk/Reward (FARO) - 0.170 0.171 0.171 0.172

Loss/Win (URisk) Loss/Win (SARO)

0.281 0.278 0.267 0.272 0.275 - 0.267 0.266 0.272 0.270

Loss/Win (FARO)

- 0.272 0.274 0.271 0.277

Loss (URisk ) Loss (SARO) Loss (FARO)
Win (URisk ) Win (SARO) Win (FARO)

2080 -
7400 -

2059 1996
2025 7417 7470
7451

1992
1992 2040 7468 7476 7452

2019
2024 2040 7427 7437 7469

2040 2010
2060 7406 7441
7429

Loss > 20% (URisk) Loss > 20% (SARO)
Loss > 20% (FARO)

1180 -

1130 1036 1124 1124 1152 1145

1036
1046 1155

1042 1032 1172

pressed as the risk (Eq. (1)) to reward (Eq. (2)) ratio (i.e., the "Risk/Reward" rows). Similarly, the number of topics that the risk-sensitive optimisation contributed to reward against risk is expressed as the loss to win ratio (i.e., the "Loss/Win" rows). Raw numbers of losses and wins associated with each  value for each model are also shown. Finally the "Loss > 20%" rows show, for each model, the number of topics on which the relative loss in performance over the BM25 baseline was higher than 20%7.
As expected, since the semi-adaptive risk-sensitive optimisation (SARO) and the risk-sensitive optimisation based on URisk focus on only those topics with down-side risk, there is a steady decrease in average retrieval effectiveness (i.e., NDCG@1 and NDCG@10), as the risk-sensitivity parameter value of  increases. Nevertheless, SARO results in a decrease in average retrieval effectiveness that is less than URisk, for all  values. In contrast, the fully adaptive risk-sensitive optimisation (FARO) maintains the average retrieval effectiveness nearly constant across all  values, as well as the values of the quality and robustness measures, namely the risk-reward ratio and the loss-win ratio.
For SARO, the observed values of the two quality and robustness metrics (risk-reward ratio and loss-win ratio) are better than for URisk across the  values. For the metric "Loss > 20%", they are comparable between SARO and URisk, given a topic sample as large as 9685 in size.
Next, for FARO, the observed values of the two quality and robustness metrics are comparable with that of the risksensitive optimisation based on URisk across  values, and for the metric, "Loss > 20%" the observed values for FARO are slightly worse than that of both URisk and SARO.
To summarise, the empirical evidence in Table 3 suggest that (i) FARO is best suited for retrieval tasks that are not tolerant to any loss in average effectiveness but also require robustness in effectiveness across the topics, and (ii) SARO suits retrieval tasks that require primarily robustness but are tolerant to some loss in the achievable average effectiveness.
7Similar measures are reported in [26]. With 9685 topics, all NDCG differences are statistically significant.

31

7. RELATED WORK
To the best of our knowledge, this paper is the first work examining risk-sensitive evaluation from the perspective of statistical inference. Indeed, while there has been some investigation into measures of robustness in the literature, such as Geometric-Mean Average Precision [24], developed within the context of the TREC 2004 Robust track, this paper advances upon the URisk measure, first proposed in [26] in 2012. The TRisk measure is the test statistic counterpart of URisk, which enables hypothesis testing on the level of risk associated with a given IR system. As a result, it facilitates adaptive risk-sensitive optimisation within learning to rank.
Outside of risk-sensitive evaluation, statistical hypothesis testing has a long history within IR. Van Rijsbergen [22] noted that "there are no known statistical tests applicable to IR". However, later, Hull [32] recommended various hypothesis tests for the evaluation of retrieval experiments, including the Student's t test for matched pairs. Zobel [31] was the first to apply re-sampling techniques in IR, by using a leaveone-out technique for assessing the effect of pooling on the effectiveness measurements and the significance of hypothesis tests, including the paired t test and the Wilcoxon signed rank test. Later, Smucker et al. [19, 20] and also Urbano et al. [27] investigated nonparametric re-sampling techniques, such as the bootstrapping and permutation tests, for the purposes of the evaluation of retrieval experiments.
Finally, much work in developing effective learning to rank techniques has occurred in the last few years, as reviewed by Liu [16]. Macdonald et al. [17] examined how the choice of evaluation measure encoded within their loss functions impacted upon the effectiveness of various learning to rank techniques. In particular, it is notable that the AdaRank technique [16, Ch. 4] focuses on hard queries using boosting. Taking a different approach, Wang et al. [26] proposed a risk-sensitive optimisation for the state-of-the-art LambdaMART technique, based on their URisk measure. We further extend URisk to the new TRisk measure within this paper, which is both theoretically founded, and results in more effective and less risky learning to rank.
8. CONCLUSIONS
This paper proposed the new TRisk measure for risk-sensitive evaluation, which is theoretically grounded within hypothesis testing. It easily allows inferential hypothesis testing of risk, as well as the exploratory identification of topics that commit significant levels of risk. In particular, we showed how TRisk could be integrated within the state-of-the-art LambdaMART learning to rank technique, to permit effective yet risk-averse retrieval. Indeed, compared to the existing URisk measure, we attain higher effectiveness with comparable or better risk/reward tradeoffs. For future work, we believe that there is a huge scope to build further effective and risk-averse adaptations for learning to rank upon TRisk, other than SARO and FARO, and beyond LambdaMART.
9. REFERENCES
[1] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. ECIR, 2004.
[2] C. Buckley and E. M. Voorhees. Evaluating evaluation measure stability. In Proc. SIGIR, 2000.

[3] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proc. ICML, 2005.
[4] C. J. C. Burges. From RankNet to Lambdarank to LambdaMART: An overview. Technical Report MSR-TR-2010-82, 2010.
[5] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. Automatic query refinement using lexical affinities with maximal information gain. In Proc. SIGIR, 2002.
[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proc. CIKM, 2009.
[7] J. Cohen. Statistical Power Analysis for the Behavioral Sciences. 2nd edition, 1988.
[8] K. Collins-Thompson. Accounting for stability of retrieval algorithms using risk-reward curves. In Proc. Future of Evaluation in IR Workshop. SIGIR. 2009.
[9] K. Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In Proc. CIKM, 2009.
[10] K. Collins-Thompson, P. N. Bennett, F. Diaz, C. Clarke, and E. Voorhees. TREC 2013 Web Track Guidelines.
[11] B. Efron. Bootstrap methods: Another look at the jackknife. Annals of Statistics, (7):1­26, 1979.
[12] H. Fisher. A History of the Central Limit Theorem: From Classical to Modern Probability Theory. Springer, 2010.
[13] Y. Ganjisaffar, R. Caruana and C. Lopes. Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models. In Proc. SIGIR, 2011.
[14] D. C. Hoaglin, F. Mosteller, and J. W. Tukey, editors. Understanding robust and exploratory data analysis. 1983.
[15] R. V. Hogg, A. T. Craig, and J. W. McKean. Introduction to Mathematical Statistics. 6th edition, 2004.
[16] T.-Y. Liu. Learning to rank for information retrieval. Foundations & Trends in IR, 3(3):225­331, 2009.
[17] C. Macdonald, R. Santos, and I. Ounis. The whens and hows of learning to rank for web search. Information Retrieval, 16(5):584-628, 2013.
[18] M. Quenouille. Approximate tests of correlation in time series. J. Royal Statistical Society, (11):18­84, 1949.
[19] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proc. CIKM, 2007.
[20] M. D. Smucker, J. Allan, and B. Carterette. Agreement among statistical significance tests for information retrieval evaluation at varying sample sizes. In Proc. SIGIR, 2009.
[21] J. W. Tukey. Bias and confidence in not quite large samples. Annals of Mathematical Statistics, 29(2):614, 1958.
[22] C. van Rijsbergen. Information Retrieval. 2nd edition, 1979.
[23] E. M. Voorhees. Overview of the TREC 2003 robust retrieval track. In Proc. TREC, 2003.
[24] E. M. Voorhees. The TREC Robust retrieval track. SIGIR Forum, 39(1):11­20, 2005.
[25] E. M. Voorhees and C. Buckley. The effect of topic set size on retrieval experiment error. In Proc. SIGIR, 2002.
[26] L. Wang, P. N. Bennett, and K. Collins-Thompson. Robust ranking models via risk-sensitive optimization. In Proc. SIGIR, 2012.
[27] J. Urbano, M. Marrero, and D. Mart´in. On the measurement of test collection reliability. In Proc. SIGIR, 2013.
[28] W. Webber, A. Moffat, and J. Zobel. Score standardization for inter-collection comparison of retrieval systems. In Proc. SIGIR, 2008.
[29] C. F. J. Wu. Jackknife, bootstrap and other resampling methods in regression analysis. Annuals of Statistics, 14:1261­1350, 1986.
[30] Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. Ranking, boosting, and model adaptation. Technical Report MSR-TR-2008-109, 2008.
[31] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In Proc. SIGIR, 1998.
[32] J. Zobel. Using statistical testing in the evaluation of retrieval experiments. In Proc. SIGIR, 1993.

32

Skewed Partial Bitvectors for List Intersection

Andrew Kane arkane@cs.uwaterloo.ca

Frank Wm. Tompa fwtompa@cs.uwaterloo.ca

David R. Cheriton School of Computer Science University of Waterloo
Waterloo, Ontario, Canada

ABSTRACT
This paper examines the space-time performance of in-memory conjunctive list intersection algorithms, as used in search engines, where integers represent document identifiers. We demonstrate that the combination of bitvectors, large skips, delta compressed lists and URL ordering produces superior results to using skips or bitvectors alone.
We define semi-bitvectors, a new partial bitvector data structure that stores the front of the list using a bitvector and the remainder using skips and delta compression. To make it particularly eective, we propose that documents be ordered so as to skew the postings lists to have dense regions at the front. This can be accomplished by grouping documents by their size in a descending manner and then reordering within each group using URL ordering. In each list, the division point between bitvector and delta compression can occur at any group boundary. We explore the performance of semi-bitvectors using the GOV2 dataset for various numbers of groups, resulting in significant space-time improvements over existing approaches.
Semi-bitvectors do not directly support ranking. Indeed, bitvectors are not believed to be useful for ranking based search systems, because frequencies and osets cannot be included in their structure. To refute this belief, we propose several approaches to improve the performance of rankingbased search systems using bitvectors, and leave their verification for future work. These proposals suggest that bitvectors, and more particularly semi-bitvectors, warrant closer examination by the research community.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation (e ciency and effectiveness); H.2.4 [Database Management]: Systems-- Query Processing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise,
or republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14,
July 6­11, 2014, Gold Coast, Queensland, Australia.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609609.

General Terms
Algorithms; Performance
Keywords
Information Retrieval; Algorithms; Database Index; Performance; E ciency; Optimization; Compression; Intersection
1. INTRODUCTION
We examine the space-time performance for algorithms that perform in-memory intersection of ordered integer lists. These algorithms are used in search engines where the integers are document identifiers and in databases where the integers are row identifiers. We assume that the lists are stored in integer order, which allows for fast merging and for compression using deltas.
Intersecting multiple lists can be implemented by intersecting the two smallest lists, then repeatedly intersecting the result with the next smallest list in order. This setversus-set (svs) or term-at-a-time (TAAT) approach is fast because of its sequential memory access, but it requires extra memory for intermediate results. Alternatively, a non-svs, document-at-a-time (DAAT) approach can be used, requiring less space for intermediate results, but having a slower random access pattern. We use the faster svs approach, but our work can also be applied to non-svs intersection.
Our experiments use data and queries from the search engine domain, so the lists of integers are usually encoded using bitvectors or compressed delta encodings such as variable byte (vbyte), PForDelta (PFD), and simple16 (S16), often with list index structures (skips) that allow jumping over portions of the compressed lists. Using skips results in significant performance gains, and so does using bitvectors for large lists. We have found that combining bitvectors with large skips gives the best results, regardless of the encoding, so we use this combination in our performance tests.
The integers in our lists are document identifiers assigned by the search engine. Typically, these values are assigned based on the order that the documents were indexed, referred to as the original order. Changing the order of the documents can produce smaller and faster systems. Many orderings have been examined in the literature, such as document size, content clustering, TSP, URL, and hybrid orderings. A detailed review of such reordering techniques is presented in Section 2.2. The URL ordering is easy to compute and produces comparable performance to the best approaches [23], so we use this as our basis of comparison.

263

The goal in this paper is to improve on the space-time performance of the combined bitvectors+skips algorithm executed on a URL ordered index. This is accomplished using partial bitvectors via the following contributions:
· We introduce a new hybrid ordering approach that groups by the terms-in-document size, and then sorts by the URL ordering within each group, resulting in better document clustering and delta compression.
· We introduce semi-bitvectors, which allows the front portion of a list to be a bitvector while the rest uses normal compression and skips, with cut points aligned to the group boundaries of our hybrid ordering.
We apply these techniques to the TREC GOV2 dataset and queries, and the end result is a significant improvement in runtime together with a small improvement in space. When compared to algorithms using only skips, the improvement is very significant, on top of the benefits from using URL ordering.
The remainder of the paper describes related work in Section 2, experimental setup in Section 3, partial bitvectors in Section 4, ranking in Section 5, partitioning in Section 6, extensions in Section 7, and conclusions in Section 8.
2. RELATED WORK
In this section, we present related work in list intersection and illustrate various algorithms' performance using space-time graphs, for which the experimental setup uses the GOV2 corpus and is described in Section 3. We also present related work exploring the eect of document reordering on the performance of search algorithms.
2.1 Intersection Approaches
There are many algorithms available for intersecting uncompressed integer lists. For a broad performance comparison see Barbay et al. [4]. Many of these algorithms are fast, but their memory use is very large (e.g., storing each integer in 32 bits) and probes into the list can produce wasted or ine cient memory access.
There are a large variety of compression algorithms available for sorted integer lists. They first convert the lists into dierences minus one (deltas or d-gaps) to get smaller values, removing the ability to randomly access elements in the list. Next, a variable length encoding is used to reduce the number of bits needed to store the values, often grouping multiple values together to get word or byte alignment.
We examine the standard variable byte (vbyte) encoding and some of the top performers: PForDelta (PFD) [32] and simple16 (S16) [30]. The S16 encoding uses a variable block size, but we combine these small blocks into larger fixed sized blocks [30] for a clearer comparison with PFD. The vbyte encoding is byte aligned, storing seven bits of a delta with one bit indicating additional data. As with the simple9 [1] encoding it is based upon, the S16 encoding is word (4-byte) aligned, using 4 bits to allocate the remaining 28 bits to fit a few deltas. The PFD encoding combines multiples of 32 deltas into a block (padding lists with zeros to fill out the blocks) to become word aligned. The block of deltas are stored in the same number of bits, with any that cannot fit (up to 10%) stored separately as exceptions using a linked list to indicate their location and then storing the values at the end of the encoding. Lists smaller than 100 use normal

vbyte encoding (to avoid the expense of padding lists with zeros to fit into blocks), thus producing a hybrid algorithm. We do not examine other variations of these encodings, such as the VSEncoding (VSE) algorithm [26] which has dynamically varying block sizes, storing deltas in each block using the same number of bits without exceptions. Recent work has improved decoding and delta restore speeds for many algorithms using vectorization [18], with some optimizations using more space. Another recent approach first acts on the values as monotone sequences, then incorporates some delta encoding more deeply into the compression algorithm [28]. While such approaches can be combined with our work, we do not explore this here.
List indexes are included to jump over values and thus avoid decoding, or even accessing, portions of the lists. A simple list index algorithm groups by a fixed number of elements storing every Xth element in an array [20], where X is a constant, and we refer to it as skips. Variable length skips are possible, but the dierences are not important here. Another approach groups by a fixed size document identifier range into segments [20], allowing array lookups into the list index. This segment approach has similar performance to skips for the large jump points we are using and it conflicts with block based encodings, so it is not examined here. List index algorithms can be used with compressed lists by storing the deltas of the jump points, but the block based structure causes complications if the jump points are not byte or word aligned. To prevent non-aligned jump points, skips over block based encodings choose the block size to be equal to the skip size X [16]. Similarly, using compression algorithms that act on large blocks can obstruct the runtime performance of skips.
For list intersection, the compression algorithms produce much smaller encodings, but they are slow. Uncompressed lists are larger, but random access makes them fast. Combining list indexes with the compressed algorithms adds little space, and this targeted access into the lists allows them to be faster than the uncompressed algorithms.
When using a compact domain of integers, as we are, the lists can be stored as bitvectors, where the bit number is the integer value and the bit is set if the integer is in the list. For our dataset, such an encoding uses large amounts of space but can have very good runtime performance. To alleviate the space costs, lists with document frequency less than F can be stored using vbyte compression, resulting in a hybrid bitvector algorithm [11]. This hybrid algorithm, vbyte+bitvectors, is faster than non-bitvector algorithms, and some settings result in smaller configurations, since very large lists can be more compactly stored as bitvectors. We found similar improvements in runtime, with smaller improvements in space, when combining bitvectors with more compact compression algorithms such as PFD and S16.
We have found that bitvectors combined with large skips perform better than either skips or bitvectors separately, regardless of the compression algorithm. Large skips of size 256 give a good space-time tradeo when combined with bitvectors, even though that size is not the fastest when skips are used by themselves. Since the best algorithms in terms of space and runtime use bitvectors for large lists, future work in this area should remove large lists from consideration when comparing compression algorithms.
We compare performance of various algorithms using a graph of space (bits per posting when encoding the entire

264

dataset of lists) versus time (milliseconds per query when running the entire workload sequentially). The space-time performance of various configurations of an algorithm are connected to form a performance curve. These configurations are from the X settings for the skips algorithm and the F settings for the bitvector algorithm. The bitvector+skips algorithm uses a fixed X value of 256 and various F settings.
A comparison of skips, bitvectors and bitvectors+skips using PFD encoding under the original document ordering is shown in Figure 1 (top), where points further left and down use less space and less time, respectively. Clearly, the combination of PFD+bitvectors+skips is much faster than PFD+bitvectors or PFD+skips, especially when the configurations use small amounts of space.
For a more detailed background of integer list compression and list intersection, we recommend the survey of search engine techniques written by Zobel and Moat [31]. The performance of many of these algorithms have previously been compared in other experimental settings [4, 11, 20, 30].
2.2 Reordering
Intersecting lists of integers applies to search engines when the integers are document identifiers assigned by the system, giving a compact domain of values and small deltas that are compressible. This assignment of identifiers can be changed to produce space and/or runtime benefits, and we refer to this process as reordering the documents.
Reordering can improve space usage by placing documents with similar terms close together in the ordering, thus reducing the deltas, which can then be stored using smaller amounts of space. This reduces the index space as well as the amount of data being accessed per query. We have found that a reduction in data access per query does not improve the runtime of in-memory intersection, because data transfer is not the bottleneck in such systems. Note, reordering can also improve compression in other areas, such as the term frequencies embedded in the lists [29].
Reordering can improve runtime performance by producing data clustering within the lists [29], as well as query result clustering within the document identifier domain [17]. This gives larger gaps in the document domain during query processing, which causes list indexes (skips) to work better and the optimal skip size to increase. This also causes fewer cache line loads within bitvectors, making them more e cient. These runtime improvements are seen not just in list intersection performance, but with frequency, ranking, and even dynamic pruning, where knowledge of the ranking algorithm is used to avoid processing some parts of the lists [27]. Tuning the compression algorithms to an ordering can also give a better space-time tradeo [29].
The runtime benefits of reordering come from using skips and bitvectors to avoid accessing portions of the lists, rather than from reading more compressed data. Clearly, space improvement and decoding time are not the only metrics that should be considered when comparing document orderings.
Below we present various document ordering techniques:
Random: If the documents are ordered randomly (rand), there are no trends for the encoding schemes or the intersection algorithms to exploit, so this is a base of comparison for the other orderings.
Original: The dataset comes in an original order (orig), which may be the order in which the data was crawled. This

time (ms/query)

8

PFD+skips(orig)

F = 1/8 

X = 256 128

 PFD+bitvectors(orig) PFD+bitvectors+skips(orig)

6

32 64

F = 1/4

 1/16

4

1/8

 1/24

1/16

 1/32

2

1/24 1/32

 1/48

1/48

0 4
8
6

6

8

10

12

PFD+bitvectors+skips(rand) PFD+bitvectors+skips(orig)  PFD+bitvectors+skips(td) PFD+bitvectors+skips(url)

F = 1/4

4

1/8



1/16 

2

F = 1/4 1/8

 

1/24 1/32

1/48

1/16

 

1/24

1/32

1/48

0

4

6

8

10

12

8

S16+bitvectors+skips(url)

PFD+bitvectors+skips(url)

6

time (ms/query)

time (ms/query)

4

F = 1/2

1/4

F = 1/4

2

1/8

1/8

1/16

1/16

1/24

1/32

1/48

0

4

6

8

10

12

space (bits/posting)

Figure 1: Space vs. time graphs for intersection algorithms with skip size X and bitvector cuto frequency F .

ordering could be anything, so it may not a good base. We have found that using the original order of the GOV2 dataset gives only small improvement over random ordering.
Rank: Reordering to approximate ranking allows the engine to terminate early when su ciently good results are found. A global document order [19], such as PageRank or result occurrence count in a training set [15], can be used. Individual lists could be ordered independently, as done in impact ordering [2], increasing space usage and requiring accumulators to process queries. These techniques essentially prune portions of the lists from the calculation. Thus they sacrifice space in order to improve runtime performance, while the remaining types of ordering exploit information from the dataset to improve both space and runtime performance.
Matrix: Reordering by manipulating the document vs. term matrix can produce improvements in space by grouping documents with high frequency terms [21], producing a block

265

Blandford 2002 Long 2003 Shieh 2003 Garcia 2004 Silvestri 2004 Blanco 2006 Silvestri 2007 Baykan 2008 Yan 2009 Bu¨ttcher 2010 Ding 2010 Silvestri 2010 Tonellotto 2011 Shi 2012 Arroyuelo 2013

ref [7] [19] [22] [15] [24, 25] [6] [23] [5] [29] [9] [13] [26] [27] [21] [3]

type clustering global page ordering TSP query results clustering SVD·TSP·clustering URL; URL·clustering block-diagonal matrix URL·local tweeks doc terms; URL TSP·LSH; URL·doc size URL doc size; URL term frequency run-length optimize

data collection and size TREC4-5(1GB); WT2g(2GB) Web crawl 2002(1.2TB) PC(small); FBIS(470MB); LATimes(475MB) WT10g(10GB) GPC(2GB) FBIS(470MB); LATimes(475MB) WBR99(22GB) GPC-plus(3GB) GOV2(426GB) GOV2(426GB) Wiki08(50GB); Ireland(160GB); GOV2(426GB) WT10g(10GB); WBR99(22GB); GOV2(426GB) ClueWeb09-CatB(1.5TB) FBIS(470MB); LATimes(475MB); WT2g(2GB); Wiki12(17GB) GOV2(426GB)

space Y N Y N Y Y Y Y Y Y Y Y Y Y Y

runtime N Y N Y N N N N Y N Y Y Y N Y

Table 1: Details of reordering papers.

diagonal matrix [5], or creating run-length encodable portions of the matrix [3]. Manipulating the matrix for large datasets can be expensive, and merging subindexes can be di cult, so these techniques have not been widely used.
Document Size: The simple method of ordering documents by decreasing number of unique terms in the document (td) produces index compression [9] and runtime performance improvements [27], while requiring no additional information about the documents and very little processing at indexing time. Ordering by terms-in-document is approximately the same as ordering by the number of tokens in the document or by document size. The improvements obtained from terms-in-document ordering are not as large as occurs with other orderings, so it has been mostly ignored.
Content Similarity: Ordering by content similarity uses some similarity metric that is applied in various ways to produce an order. Ordering using normal content clustering techniques [7] or a travelling salesman problem (TSP) [22] formulation can produce space improvements. However, even with various improvements [6, 13, 24, 25], these approaches are too slow to be used in practice. In addition, these techniques must start from scratch when subindexes are merged, although not for subindex compaction.
Metadata: Ordering lexicographically by URL provides similar improvements in space usage as obtained from ordering by content similarity [23], and it improves runtime substantially when using skips [29]. URL ordering is essentially using the human-based organization of website authors, which often groups the documents by topic, to produce content similarity in the ordering. Using other metadata makes this technique broadly applicable, but the eectiveness can vary greatly based on the dataset and density distribution of the data within the chosen domain. This approach is simple to compute at indexing time and can support fast merging of subindexes.
Hybrid: Ordering by terms-in-document is not as eective as ordering by URL, but these two can be combined to get a slightly better result. For example, one hybrid approach groups the documents by URL server name, then subdivides each into 5 document size ranges, and finally orders by URL within each subdivided group [13]. This approach is similar to what we present later in this paper, but the reasoning and final result are quite dierent.

Ordering documents lexicographically by URL is fast to calculate, just as good as any of the other approaches [23], and especially eective for the GOV2 dataset. URL ordering achieves this performance by placing documents with similar terms close together. Such tight clustering reduces the delta sizes substantially, with approximately 69.8% of the deltas having the value one in URL ordering vs. approximately 20.4% for random ordering. This suggests that there is limited room for additional improvement from new ordering methods.
A summary of the papers on ordering documents is shown in Table 1, where the last two columns indicate if the paper is examining the space and/or runtime benefits from ordering the documents.
2.3 Improvements from Reordering
The runtime performance of skips has been shown to improve by approximately 50% when the index is ordered by URL [29]. Our bitvectors+skips algorithm is, however, still superior to bitvectors or skips separately, regardless of the compression algorithm or document ordering. As a result, we use the bitvectors+skips algorithm for the remainder of this paper. The performance of the bitvectors+skips algorithm using the PFD encoding under the random, original, terms-in-document and URL orderings is shown in Figure 1 (middle). Furthermore, experiments using vbyte and S16 encodings produce similar runtime performance improvements. The terms-in-document ordering produces benefits over original and random orderings, but URL ordering is clearly much better than the others in terms of both space and time.
The amount of compression from ordering by URL has been shown for various datasets, and most uses produce significant improvements in space. The rate of improvement varies considerably for dierent encodings, for example, vbyte compression improves space by 8.1%, PFD improves space by 24.7%, and S16 improves space by 43.1% for our GOV2 index. This dierence in improvement rate makes S16 much smaller than PFD under the URL ordering, as shown in Figure 1 (bottom). (The performance using the S16 and PFD encodings is presented, but the vbyte encoding is omitted because it is dominated by PFD.) The URL ordering and resultant compression does not, however, produce runtime improvements for list intersection until skips or bitvectors are added. In fact, these runtime improvements are not proportional to the space savings, suggesting that memory transfer time is not the bottleneck.

266

Clearly, the URL ordering is better than the others and the bitvectors+skips algorithm is fast. While the S16 encoding gives smaller results, the PFD encoding is faster and still requires small amounts of space. As a result, in the next few sections we use the faster PFD+bitvectors+skips(url) algorithm as our basis of comparison, and we show how to improve upon this base by skewing the distribution of postings and creating partial bitvectors over certain dense regions. Please note that the more compact S16 based algorithm can also be similarly improved.
3. EXPERIMENTAL SETUP
We use the TREC GOV2 corpus, indexed by Wumpus1 without stemming to extract document postings. The corpus size is 426GB from 25.5 million documents, giving 9 billion document postings and 49 million terms.
Our workload is a random sample of 5000 queries chosen by Barbay et al. [4] from the 100,000 corpus queries, which we have found to produce very stable results. These queries are tokenized by Wumpus, giving an average of 4.1 terms per query. Query statistics are summarized in Table 2, including averages of the smallest list size, the sum of all list sizes, and the result list size over all queries with the indicated number of terms for the entire corpus. For our runtime calculations, we remove the single term queries.

terms 1 2 3 4 5 6 7 8 9
total

queries 92
741 1270 1227
803 428 206
98 135 5000

% 1.8 14.8 25.4 24.5 16.1 8.6 4.1 2.0 2.7 100.0

smallest 131023 122036 194761 199732 204093 192445 205029 206277 198117 186070

all 131023 1520110 6203147 13213388 20361435 29367581 36346235 46198187 63406170 14944683

result 131023
39903 31730 17879 13087 15004
8240 5726 3308 24699

Table 2: Query information.

Our experiments simulate a full index: we load the postings lists for query batches, encode them, flush the CPU cache by scanning a large array, then execute the conjunctive intersection of terms to produce the results. Each query has its own copy of its encoded postings lists, so performance is independent of query order and shared terms. Intersection runtimes per step are recorded, and overall runtimes are the sums over all steps for all queries. Space and time values ignore the dictionary, positional information, and ranking.
Our code was run on an AMD Phenom II X6 1090T 3.6Ghz Processor with 6GB of memory, 6mb L3, 512k L2 and 64k L1 caches running Ubuntu Linux 2.6.32-43-server with a single thread executing the queries. The gcc compiler was used with the -O3 optimizations to produce high performance code. The query results were visually verified to be plausible and automatically verified to be consistent for all algorithms.
We used the C++ language and classes for readability, but the core algorithms use only non-virtual inline functions, allowing a large range of compiler optimizations. We encode directly into a byte array for each list, and then include
1http://www.wumpus-search.org/

3000

2500

Terms-in-document

2000

1500

1000 500

x=10.9%,y=608


x=38.7%,y=313


0

0.0

0.2

0.4

0.6

0.8

1.0

Fraction of Documents

Figure 2: Terms-in-document distribution for the GOV2 dataset, cutos for three groups are marked.

Postings Lists

Bitvector Compressed
Large

Medium

Small

Documents by Group

Figure 3: Schematic of a three-group semi-bitvector index.

decode time in our runtimes to produce more realistic and repeatable measurements. The code was tuned to minimize memory access and cache line loads.
4. PARTIAL BITVECTORS
We develop our approach to representing postings lists in two steps. First, we introduce partial bitvectors over grouped lists in terms-in-document ordering. After that, we show that ordering by URL within groups outperforms other representations.
4.1 Grouped Terms-in-Document Ordering
The URL and clustering based orderings place documents with similar terms close together, producing tight clustering within the postings lists. The terms-in-document ordering, however, does not place documents with similar terms together in the ordering. Instead, the ordering by decreasing number of terms-in-document packs more postings into lower document identifiers, meaning that the density of the postings lists tends to decrease throughout the lists. This front-packing results in many smaller deltas, which can be more easily compressed. The front-packing also means that values in the postings lists are denser for lower document identifiers, giving skewed clustering with more eective skips at the end of the lists. This skewing of postings to lower document identifiers can be clearly seen in the distribution of terms-in-document values, as shown in Figure 2. The dotted lines split the index into three groups containing equal numbers of postings, meaning that the largest 10.9% of documents contain 33.3% of the postings.
In addition, the likelihood of a document occurring in the intersection of multiple lists increases as the number of lists

267

containing the document identifier increases, which is exactly the number of terms in the document. This causes the result list to be even more skewed towards lower document identifiers than the input lists. The result lists are indeed skewed in our query workload: the largest 10.9% of the documents contain 58.2% of the intersection results. Such a skew of the result list is similar to what would be expected from ordering by the document's usage rate in a set of training queries, where the usage rate could be measured by the number of times a document occurs in the postings lists or the result lists of the queries [15]. Preliminary experiments indicate that the terms-in-document ordering has similar performance to such ordering by usage, but these results are not presented here.
We can exploit the skewed nature of the terms-in-document ordering by using partial bitvectors. In particular, we use bitvectors for the denser front portion of a postings list, and then normal delta compression and skips for the rest of the list. We call this front partial bitvector structure a semibitvector, and the highest document identifier in the bitvector portion of a postings list is called the cut point. The semi-bitvector intersection algorithm must first order the lists ascending by their cut points, then execute in a pairwise set-versus-set manner. Each pairwise list intersection has three (possibly empty) parts that are executed in order: bitvector-to-bitvector, sequence-to-bitvector, and sequenceto-sequence. In general, the end result contains a partial bitvector that must be converted to values, followed by a sequence of values. The intersection of two semi-bitvectors is defined in Algorithm 1 using basic intersection subroutines acting on bitvectors and sequences of integers.

Algorithm 1 intersect semi-bitvector

1: function

(M,N)

SemiBV

2: r {}

3: s M.bitvSize

4: t N.bitvSize

5: b bvand (M.bitv , N.bitv , s)

6: if N.isLastList then

7:

r r [ bvconvert(b, s)

. assert(s  t)

8: r r[bvcontains(M.seq.select(value < t), N.bitv , t)

9: r r [ merge(M.seq.select(value t), N.seq)

10: if N.isLastList then

11:

return r

12: return new semi-bitvector (b, r, s)

Our implementation of semi-bitvector intersection applies various optimizations: The bvand and bvconvert algorithms (lines 5 and 7) are executed in a single pass on the last intersection and the bitvector b is not created if the query contains only two lists. The restrictions on M.seq applied by the select calls (lines 8 and 9) are executed as a single pass on M . Also the two conditionals from the select call (value < t) and the loop through M.seq (line 8) are combined when possible (i.e., first find the end point t in an uncompressed sequence or the nearest skip point before t in a compressed sequence, then use that location as a single conditional check). The result set r can be reused between pairwise steps (i.e., as input from the last step and output of the current step), except on the final step where the bitvector-to-bitvector portion is added (line 7).

time (ms/query)

4

 F = 1/4

PFD+bitvectors+skips(url)  PFD+bitvectors+skips(td)

PFD+semi-bitvectors(td-g1024-td)

3

 1/8

F = 1/4

F = 1/4

 1/16

2

1/8

1/8

 1/24

1/16 1/24
1/16

 1/32 1/32

 1/48 1/48

1

1/24

1/32

1/48

0 4

6

8

10

12

space (bits/posting)

Figure 4: Space vs. time graph for semi-bitvectors using cuto frequency F and td-g1024-td ordering compared to the bitvectors+skips algorithm using cuto frequency F , skip size 256 and either URL or terms-in-document ordering.

Our semi-bitvector structure allows more postings to be stored in bitvectors for the same amount of memory used. Since the performance of bitvectors is much faster than other approaches, better use of bitvectors can produce a significant improvement in runtime performance, allowing the overall system to be more e cient.
We pick the semi-bitvector cut points so that the bitvector portion of each list will have at least frequency F . We make the cut point calculation faster by splitting the document domain into groups and only allowing semi-bitvectors to have cut points at group boundaries. A schematic of a semi-bitvector index using three groups (and thus four potential cut points) with lists ordered by their cut points is shown in Figure 3. The cut point for a list is the highest group boundary where the group itself is above the density threshold F , and the bitvector portion (from the start of the list to the end of the group) is also above F . This definition allows a large number of groups to be used without degrading the index with too few or too many bitvector regions.
We choose group boundaries so that each group contains the same number of postings. (Other approaches could be used to determine the group boundaries, but this is not relevant here.) When we run this semi-bitvector structure with many groups using the terms-in-document ordering, we see significant performance improvement. The performance of semi-bitvectors for terms-in-document ordering using 1024 groups (td-g1024-td) is shown in Figure 4. The improvement means that PFD+semi-bitvectors(td-g1024-td) dominates PFD+bitvectors+skips(td), and it is faster than our previously best URL based approach for configurations using larger amounts of memory. However, it is still slower for small configurations, and no configuration is as small as what can be achieved with URL ordering.
4.2 Grouped URL Ordering
The terms-in-document ordering gives skewed clustering towards the front of the lists, while URL ordering and the other approaches give tight clustering throughout the lists. We would like to combine these two orderings into a hybrid ordering to produce the benefits of both skewed clustering and tight clustering.

268

url Portion of Deltas

0.12

0.10

0.08

0.06

0.04

0.02

0.00

0

0.2

0.4

0.6

0.8

1

td Portion of Deltas

0.12

0.10

0.08

0.06

0.04

0.02

0.00

0

0.2

0.4

0.6

0.8

1

td-g3-url Portion of Deltas

0.12

0.10

0.08

0.06

0.04

0.02

0.00

0

0.2

0.4

0.6

0.8

1

Fraction of Documents

Figure 5: Plots of delta counts across the document domain for various document orderings.

Terms-in-document ordering was previously combined with URL ordering by Ding et al. [13] in the form of url.server-tdurl, which splits into chunks by url.server, then groups into five parts by terms-in-document, then orders by URL. This hybrid ordering gives slight benefits in terms of space, but the eect on runtime performance was not tested. While the method of determining group separations (i.e., the boundaries for each terms-in-document group) was not specified, the url.server portion of the hybrid ordering will split the index into many small pieces. As a result, the skew from the subsequent td ordering is spread out across the entire document range. This means that the skew cannot be easily exploited through grouping as we did in Section 4.1.
For our hybrid combination of terms-in-document and URL ordering, we first group the documents by their termsin-document value, then reorder within each group using the URL ordering. We will refer to it as td-g#-url, where # is the number of groups. (Since the GOV2 dataset covers only one section of the Web, we can relate this new approach to the previous hybrid approach as being in the form of url.server.su x-td-url.) Increasing the number of groups of documents will reduce the tight clustering from the URL ordering, but increase the skewed clustering of the data. This means that as the number of groups increases, the performance will trend towards the grouped terms-in-document performance and thus degrade `coherence.'
The td-g3-url ordering results in some skewing of deltas towards the front of the lists, as shown in Figure 5 (bottom). It also reduces the delta sizes as compared to URL ordering, with approximately 71.9% of the deltas having the value one for this ordering.

3.4

3.2

Entropy

3.0

2.8

2.6 0

20

40

60

80

100

Terms-in-document Groups

Figure 6: Entropy vs. number of terms-in-document groups.

We measure the compressibility of the data using zero

order Shannon entropy H on the deltas d (which assumes

deltas are independent and generated with the same proba-

bility distribution), where pi is the probability of delta i in the data:

P

H(d) =

pilog2(pi)

i2d

Lower values of entropy indicate that more compression is

possible. The td-g-url approach can improve entropy com-

pared to the URL ordering, as shown in Figure 6, where four

groups is the optimal setting. Surprisingly, even with one

hundred groups, the entropy has not significantly degraded,

even though the entropy of the (pure) terms-in-document

ordering is 5.07, which is much higher, and we expect that

splitting the index into many groups will degrade perfor-

mance towards terms-in-document ordering.

The actual space-time performance for dierent numbers

of groups and dierent F values is shown in Table 3. Using

four

groups

produces

the

smallest

configuration

with

F

=

1 8

,

but for other F values, using eight groups is better than

using four groups in both space and time. As a result, we

use td-g8-url as our optimal configuration.

td-g2-url td-g4-url td-g8-url td-g12-url td-g16-url

F

=

1 8

space time

5.32 1.77

5.16 1.65

5.23 1.56

5.28 1.54

5.34 1.55

F

=

1 16

space time

6.28 1.33

6.15 1.22

6.14 1.17

6.23 1.16

6.28 1.15

F

=

1 32

space time

8.11 1.08

7.92 0.98

7.79 0.96

7.94 0.96

7.98 0.96

Table 3: Space (bits/posting) and time (ms/q) performance of PFD+semi-bitvectors for various numbers of groups and cuto values F .

When we compare our semi-bitvector approach using tdg8-url ordering to the bitvectors+skips algorithm using URL ordering, we see a significant improvement in performance. For the same amount of memory, the semi-bitvectors produce a speedup of at least 1.4x compared to the best URL ordering approach, and a small reduction in space usage is also possible, as shown in Figure 7. Interestingly, running the bitvectors+skips algorithm using the new td-g8-url ordering produces very little improvement, and running semibitvectors without grouping by terms-in-document also produces little improvement. Clearly, both the grouping and

269

time (ms/query)

4 PFD+bitvectors+skips(url) PFD+semi-bitvectors(td-g8-url)

3
F = 1/4

1/8

2

F = 1/4

1/16

1/8

1/24 1/32

1/48

1

1/16 1/24

1/32

1/48

0 1.8
td-g8-url vs. url
1.6

speedup

1.4

1.2

1.0

4

6

8

10

12

space (bits/posting)

Figure 7: Space vs. time graph and improvement graph for semi-bitvectors using cuto frequency F and td-g8-url ordering compared to the bitvectors+skips algorithm using cuto frequency F , URL ordering and skip size 256.

4

X = 256

PFD+skips(url) PFD+semi-bitvectors(td-g8-url)

128

32

3

64

time (ms/query)

2

F = 1/4

1/8

1

1/16 1/24

1/32

1/48

speedup

0 3.5
td-g8-url vs. url 3.0

2.5

2.0

1.5

1.0

4

6

8

10

12

space (bits/posting)

Figure 8: Space vs. time graph and improvement graph for semi-bitvectors using cuto frequency F and td-g8-url ordering compared to the skips algorithm using URL ordering and skip size X.

semi-bitvectors are needed to produce the performance improvements of our approach.
In addition, the space-time benefits of semi-bitvectors for the terms-in-document ordering (td-g1024-td vs. td) are similar to the benefits of semi-bitvectors for the URL ordering (td-g8-url vs. url). This suggests that our td-g-url approach is combining the benefits of tight clustering found in the URL ordering with the benefits of skewed clustering found in the terms-in-document ordering.
Most of the publicly available search systems do not use bitvectors or combine bitvectors with skips. As a result, a more appropriate comparison is between our semi-bitvectors and simple skips, where we found a speedup of at least 2.4x, as shown in Figure 8. This comparison also shows that a significant space improvement is possible. These benefits are in addition to the performance gains from using URL ordering rather than some other ordering. Such ine cient orderings may be common in existing installations. Incredibly, our semi-bitvector approach has a speedup of at least 6.0x compared to skips using the original ordering, while using the same amount of space, although significant improvements to space are possible.
Similar types of runtime improvements would occur with any compression algorithm, since the benefits come from using bitvectors in dense regions where they are much faster than any compression algorithm.
5. RANKING BASED SYSTEMS
For comparison, we provide performance numbers from various papers using the GOV2 dataset (Table 4). Clearly, our approach using bitvectors can answer a conjunctive query much faster than the existing ranking based systems, while using much less space than a full index. The runtime performance dierences are much bigger than any hardware or

query dierences could produce. Indeed, our results are at a disadvantage, because we have indexed much more data, including the HTTP header information (6.8 billion [13] vs. 9.0 billion document level postings).
We have demonstrated that executing conjunctive queries with semi-bitvectors can be done using small amounts of space to produce extremely fast runtimes compared to ranking based search systems. These characteristics suggest that ranking-based systems can benefit from judiciously incorporating semi-bitvector data structures. We introduce five possible approaches below:
Pre-filter: Use semi-bitvectors to produce the conjunctive results, then process the ranking structures restricted to these results, as suggested in previous work [11]. This may require reordering of conjunctive results if the ranking structures use a dierent document ordering. The ranking structures could use non-query based information, such as PageRank, or normal ranking structures, thus duplicating some postings information. Having the conjunctive results can make the ranking process more e cient by exploiting skips in the first list, or limiting the number of accumulators. Using conjunctive results to pre-filter proximity or phrase queries is the natural implementation approach. Using this type of pre-filtering, however, prevents the use of non-AND based processing such as Weak-AND [8].
Sub-document pre-filter: Use semi-bitvectors in a prefiltering step as a heuristic to limit the results to high quality or highly ranked documents by exploiting the correlation of query term proximity to query relevance [10]. This is accomplished by splitting the documents into (potentially overlapping) sub-document windows, then building the semibitvector structures over these windows. This reduces the number of results that must be ranked, while the results being ranked will be highly relevant because the query terms

270

Algorithm/System

time (ms/q) space (GB) data

structures

type

reorder ref

Lucene (vbyte)

26.0

42.1

text

docID+osets AND+counts N [28]

Quasi-succinct indices (QS*)

11.9

36.9

text

docID+osets AND+counts N [28]

Exhaustive AND

6.56

4.5

text

docID+freq. BM25

Y [14]

Hierarchical Block-Max (HIER 10G)

4.29

14.5

text

docID+freq. BM25

Y [12]

PFD+semi-bitvectors(

1 32

,

td-g8-url)

0.96

8.8

text+meta docID

AND

Y

-

Table 4: Published performance numbers from various papers using the GOV2 dataset.

appear close together. The conjunctive results will need to be mapped from window IDs to document IDs before executing the ranking step. The windows could be implemented as half-overlapping windows to guarantee proximity of query terms within half the window size. Clearly, this approach needs more examination to determine if significant filtering can be achieved without adversely aecting ranking eectiveness. If this approach can produce significant filtering, the ranking step could be implemented by directly storing the tokens of each window for quick ranking/proximity/phrase processing.

High density filters: High density terms have low value

for ranking, with the extreme case being stopwords. How-

ever, they can still act as a filter and be processed more

e ciently using semi-bitvectors. In fact, high density re-

gions of a postings lists may act similarly, but a constant

ranking value may be needed to smoothly integrate filtering

regions with ranking regions in a single postings list. Based

on our results, even using semi-bitvectors for postings lists

with document frequency F performance benefits.

1 8

can

result

in

significant

Query specific filter: The terms that could be implemented as filters may be query specific. To improve the processing e ciency of these filtering terms, duplicate structures can be introduced: a semi-bitvector structure for filtering and a separate structure suitable for ranking. In fact, additional information about the user, such as topics of interest, can be included in the ranking algorithm. This may reduce the eect of query terms in the ranking, allowing more query terms to be executed as filters using semi-bitvectors.

Guided processing: Semi-bitvector structures can be used to produce conjunctive results that will provide statistics on the query, and these statistics can guide subsequent processing of the query. For example, the statistics can indicate whether ranking should be done using conjunctive processing or some form of non-conjunctive processing, such as a Weak-AND implementation. These statistics can also indicate how to adapt this processing to the specific query terms, perhaps by identifying the specific query term that causes the conjunctive processing to be overly restrictive. Processing the conjunctive results for a subset of the documents may be enough to produce eective statistics. Such adaptive query processing techniques deserve close examination.

6. PARTITIONING BY DOCUMENT SIZE
Previously, we were able to capitalize on the postings list skew resulting from terms-in-document ordering by using partitioning [17]. Like semi-bitvectors, this partitioning mechanism, in conjunction with URL ordering, allows eective use of bitvectors and skips. It was argued that partitioning by document size would be valuable in a distributed environment. When we ran experiments using three partitions, URL ordering within each partition, and the bitvec-

tor+skips algorithm, the overall space and runtime performance was similar to our semi-bitvectors with td-g8-url.
To gain these benefits in a single machine environment, all of the partitions must run on the same machine. The multi-partition approach, however, has several limitations: the costs to manage multiple partitions, the overheads per query for each partition, and the wasted space in duplicated dictionary entries for the terms found in multiple partitions. Determining when the benefits of partitioning outweigh the limitations of running multiple partitions on a single machine may be highly specific to the situation.
7. EXTENSIONS
Our hybrid td-g-url ordering has been described as grouping by terms-in-document, followed by ordering within each group by URL, but it is equivalent to sieving documents from the URL ordering based on their terms-in-document values. A more detailed combination of URL's tight clustering and terms-in-document's skewed clustering could provide a better combined ordering, and we leave such exploration for future work.
Alternative hybrid orderings could be computed by combining groups using terms-in-document ordering with some other second ordering. This allows the exploitation of better general ordering techniques or orderings tuned to the workload and dataset. As such, our grouping by terms-indocument approach acts to boost the performance of another document ordering technique.
In addition, the combination of grouping by terms-indocument with a second ordering could reduce the amount of time needed to calculate the second ordering, because the secondary ordering acts only on the documents within each group, rather than on the documents in the entire index. This could be a big advantage for ordering techniques that do not scale well, such as content similarity based algorithms.
If a search system is unable to use document ordering techniques, perhaps because the system has a very high update rate, the documents could still be grouped (or partitioned) by their terms-in-document size to produce some benefits. Indeed, any partial ordering that can exploit some amount of tight clustering or skewed clustering may have large benefits for such systems.
8. CONCLUSIONS
We have shown how groups of documents defined by the skewed terms-in-document ordering, when combined with URL ordering and partial bitvectors, can be used to make list intersection more e cient. This is accomplished by forming varying densities within grouped portions of the postings lists, reordering within the groups by URL ordering, and then storing them as semi-bitvectors, which encode dense front portions of the lists as bitvectors. Essentially, this al-

271

lows us to store more postings in bitvectors for a given space budget, and these bitvectors are much faster than other approaches. This combination gives most of the benefits of tight clustering in URL ordering, while also gaining the benefits of skewed clustering for eective use of semi-bitvectors.
This multi-ordered configuration (td-g-url) gives significant space-time improvements, when combined with semibitvectors. When compared to a fast and compact configuration that combines bitvectors, large skips and URL ordering, we get a speedup of at least 1.4x. When compared to using only skips with URL ordering, we get a speedup of at least 2.4x. While the overall improvement will depend on the size and type of the data, as well as the number of groups used, we expect significant benefits for most large datasets.
To expand the applicability of semi-bitvectors, we have described various methods for using them to improve ranking based search systems. These proposals warrant further investigation.
9. ACKNOWLEDGEMENTS
This research was supported by the University of Waterloo and by the Natural Sciences and Engineering Research Council of Canada. We thank the researchers at WestLab, Polytechnic Institute of NYU for providing their block based compression code [30].
10. REFERENCES
[1] V. N. Anh and A. Moat. Inverted index compression using word-aligned binary codes. Information Retrieval, 8(1):151­166, 2005.
[2] V. N. Anh and A. Moat. Simplified similarity scoring using term ranks. In SIGIR, pages 226­233, 2005.
[3] D. Arroyuelo, S. Gonz´alez, M. Oyarzu´n, and V. Sepulveda. Document identifier reassignment and run-length-compressed inverted indexes for improved search performance. In SIGIR, pages 173­182, 2013.
[4] J. Barbay, A. L´opez-Ortiz, T. Lu, and A. Salinger. An experimental investigation of set intersection algorithms for text searching. JEA, 14, 2009.
[5] I. C. Baykan. Inverted index compression based on term and document identifier reassignment. PhD thesis, Bilkent University, 2008.
[6] R. Blanco and A. Barreiro. TSP and cluster-based solutions to the reassignment of document identifiers. Information Retrieval, 9(4):499­517, 2006.
[7] D. Blandford and G. Blelloch. Index compression through document reordering. In DCC, pages 342­351, 2002.
[8] A. Z. Broder, D. Carmel, M. Herscovici, A. Soer, and J. Zien. E cient query evaluation using a two-level retrieval process. In CIKM, pages 426­434, 2003.
[9] S. Bu¨ttcher, C. Clarke, and G. V. Cormack. Information retrieval: Implementing and evaluating search engines. The MIT Press, 2010.
[10] S. Bu¨ttcher, C. L. Clarke, and B. Lushman. Term proximity scoring for ad-hoc retrieval on very large text collections. In SIGIR, pages 621­622, 2006.
[11] J. S. Culpepper and A. Moat. E cient set intersection for inverted indexing. TOIS, 29(1), 2010.
[12] C. Dimopoulos, S. Nepomnyachiy, and T. Suel. Optimizing top-k document retrieval strategies for block-max indexes. In WSDM, pages 113­122, 2013.

[13] S. Ding, J. Attenberg, and T. Suel. Scalable techniques for document identifier assignment in inverted indexes. In WWW, pages 311­320, 2010.
[14] S. Ding and T. Suel. Faster top-k document retrieval using block-max indexes. In SIGIR, pages 993­1002, 2011.
[15] S. Garcia, H. E. Williams, and A. Cannane. Access-ordered indexes. In Proc. of the 27th Australasian Conf. on Computer Science, pages 7­14, 2004.
[16] S. Jonassen and S. E. Bratsberg. E cient compressed inverted index skipping for disjunctive text-queries. In Advances in Information Retrieval, pages 530­542. 2011.
[17] A. Kane and F. W. Tompa. Distribution by document size. In LSDS-IR, 2014.
[18] D. Lemire and L. Boytsov. Decoding billions of integers per second through vectorization. SPE, 2013.
[19] X. Long and T. Suel. Optimized query execution in large search engines with global page ordering. In VLDB, pages 129­140, 2003.
[20] P. Sanders and F. Transier. Intersection in integer inverted indices. In ALENEX, 2007.
[21] L. Shi and B. Wang. Yet another sorting-based solution to the reassignment of document identifiers. In Information Retrieval Technology, pages 238­249. 2012.
[22] W.-Y. Shieh, T.-F. Chen, J. J.-J. Shann, and C.-P. Chung. Inverted file compression through document identifier reassignment. Information Processing & Management, 39(1):117­131, 2003.
[23] F. Silvestri. Sorting out the document identifier assignment problem. Advances in Information Retrieval, pages 101­112, 2007.
[24] F. Silvestri, S. Orlando, and R. Perego. Assigning identifiers to documents to enhance the clustering property of fulltext indexes. In SIGIR, pages 305­312, 2004.
[25] F. Silvestri, R. Perego, and S. Orlando. Assigning document identifiers to enhance compressibility of Web search engines indexes. In SAC, pages 600­605, 2004.
[26] F. Silvestri and R. Venturini. VSEncoding: e cient coding and fast decoding of integer lists via dynamic programming. In CIKM, pages 1219­1228, 2010.
[27] N. Tonellotto, C. Macdonald, and I. Ounis. Eect of dierent docid orderings on dynamic pruning retrieval strategies. In SIGIR, pages 1179­1180, 2011.
[28] S. Vigna. Quasi-succinct indices. In WSDM, pages 83­92, 2013.
[29] H. Yan, S. Ding, and T. Suel. Inverted index compression and query processing with optimized document ordering. In WWW, pages 401­410, 2009.
[30] J. Zhang, X. Long, and T. Suel. Performance of compressed inverted list caching in search engines. In WWW, pages 387­396, 2008.
[31] J. Zobel and A. Moat. Inverted files for text search engines. ACM Computing Surveys, 38(2):6, 2006.
[32] M. Zukowski, S. Heman, N. Nes, and P. Boncz. Super-scalar RAM-CPU cache compression. In ICDE, pages 59­59, 2006.

272

Partitioned Elias-Fano Indexes

Giuseppe Ottaviano
ISTI-CNR, Pisa
giuseppe.ottaviano@isti.cnr.it

Rossano Venturini
Dept. of Computer Science, University of Pisa
rossano@di.unipi.it

ABSTRACT
The Elias-Fano representation of monotone sequences has been recently applied to the compression of inverted indexes, showing excellent query performance thanks to its efficient random access and search operations. While its space occupancy is competitive with some state-of-the-art methods such as --Golomb codes and PForDelta, it fails to exploit the local clustering that inverted lists usually exhibit, namely the presence of long subsequences of close identifiers.
In this paper we describe a new representation based on partitioning the list into chunks and encoding both the chunks and their endpoints with Elias-Fano, hence forming a twolevel data structure. This partitioning enables the encoding to better adapt to the local statistics of the chunk, thus exploiting clustering and improving compression. We present two partition strategies, respectively with fixed and variablelength chunks. For the latter case we introduce a linear-time optimization algorithm which identifies the minimum-space partition up to an arbitrarily small approximation factor.
We show that our partitioned Elias-Fano indexes offer significantly better compression than plain Elias-Fano, while preserving their query time efficiency. Furthermore, compared with other state-of-the-art compressed encodings, our indexes exhibit the best compression ratio/query time trade-off.
Categories and Subject Descriptors
H.3.2 [Information Storage and Retrieval]: Information Storage; E.4 [Coding and Information Theory]: Data Compaction and Compression
Keywords
Compression; Dynamic Programming; Inverted Indexes
1. INTRODUCTION
The inverted index is the data structure at the core of most large-scale search systems for text, (semi-)structured data, and graphs, with web search engines, XML and RDF
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609615.

databases, and graph search engines in social networks as the most notable examples. The huge size of the corpora involved and the stringent query efficiency requirements of these applications have driven a large amount of research with the ultimate goal of minimizing the space occupancy of the index and maximizing its query processing speed. These are two conflicting objectives: a high level of compression is obtained by removing the redundancy in the dataset, high speed is obtained by keeping the data easily accessible and by augmenting the dataset with auxiliary information that drive the query processing algorithm. The effects of space reduction on the memory hierarchy partially mitigate this dichotomy. Indeed, memory transfers are an important bottleneck in query processing, and, thus, fitting more data into higher and faster levels of the hierarchy reduces the transfer time from the slow levels, and, hence, speeds up the algorithm [5]. In fact, in the last few years the focus has shifted from disk-based indexes to in-memory indexes, as in many scenarios it is not possible to afford a single disk seek. However, these beneficial effects can be nullified by slow decoding algorithms. Thus, research has focused its attention on designing solutions that best balance decoding time and space occupancy.
In its most basic and popular form, an inverted index is a collection of sorted sequences of integers [6, 16, 27]. Compressing such sequences is a crucial problem which has been studied since the 1950s; the literature presents several approaches, each of which introduces its own trade-off between space occupancy and decompression speed [15, 17, 19, 22, 26]. Most of these methods only allow sequential decoding, so the lists involved while processing a query need to be entirely decoded. This can be avoided by using the standard technique of splitting each sequence into blocks of fixed size (say, 128 elements) and encoding each block independently, so that it is possible to avoid the decompression of portions which are not necessary for answering the query. Still, a block must be fully decoded even if just one of its values is required.
Recently Vigna [23] overcame this drawback by introducing a new data structure called quasi-succinct index. This index hinges on the Elias-Fano representation of monotone sequences [10, 11], a conceptually simple and elegant data structure that supports fast random access and search operations, combining strong theoretical guarantees and excellent practical performance. Vigna showed that quasi-succinct indexes can compete in speed with mature and highly engineered implementations of state-of-the-art inverted indexes. In particular, Elias-Fano shines when the values accessed are scattered in the list; this is very common in conjunctive queries when the number of results is significantly smaller

273

than the sequences being intersected. A typical scenario,

for example, is intersecting edge sets in large social net-

work graphs, as Facebook's Graph Search, which has in fact

adopted an implementationof Elias-Fano indexes [8].

Experiments show however that Elias-Fano indexes can be

significantly larger than those obtained using state-of-the-art

encoders. This inefficiency is caused by its inability to exploit

any characteristics of the sequence other than two global

statistics: the number of elements in the list, and the value of

the largest element. More precisely, Elias-Fano represents a

monotone sequence of m integers smaller than u with roughly

m

log

u m

+ 2m bits of space, regardless of any regularities

in the sequence. As an extreme toy example consider the

sequence S = [0, 1, 2, . . . , m - 2, u - 1] of length m. This

sequence is highly compressible since the length of the first

run and the value of u, which can be encoded in O(log u)

bits, are sufficient to describe S. Elias-Fano requires instead

log

u m

+ 2 bits for every element in the sequence, which is

not one single bit less than it would require to represent a

random sequence of m sorted elements smaller than u. Even

if this is just a toy example, it highlights an issue that occurs

frequently in compressing posting lists, whose compressibility

is caused by large clusters of very close values.

In this paper we tackle this issue by partitioning the se-

quences into contiguous chunks and encoding each chunk

independently with Elias-Fano, so that the encoding of each

chunk can better adapt to the local statistics. To perform ran-

dom access and search operations, the lists of chunk endpoints

and boundary elements are in turn encoded with Elias-Fano,

thus forming a two-level data structure which supports fast

queries on the original sequence. The chunk endpoints can be

completely arbitrary, so we propose two strategies to define

the partition. The first is a straightforward uniform partition,

meaning that each chunk (except possibly the last) has the

same fixed size. The second aims at minimizing the space

occupancy by setting up the partitioning as an instance of

an optimization problem, for which we present a linear-time

algorithm that is guaranteed to find a solution at most (1 + )

times larger than the optimal one, for any given  (0, 1).

We perform an extensive experimental analysis on two

large text corpora, namely Gov2 and ClueWeb09 (Category

B), with several query operations, and compare our indexes

with the plain quasi-succinct indexes as described in [23], and

with three state-of-the-art list encodings, namely Binary In-

terpolative Coding [17], the PForDelta variant OptPFD [26],

and Varint-G8IU [22], a SIMD-optimized Variable Byte code.

Respectively, they are representative of best compression

ratio, best compression ratio/processing speed trade-off, and

highest speed in the literature.

In our experiments, we show that our indexes are signifi-

cantly smaller than the original quasi-succinct indexes, with

a small query time overhead. Compared to the others, they

are slightly larger but significantly faster than Interpolative,

both faster and smaller than OptPFD, and slightly slower

but significantly smaller than Varint-G8IU.

Our contributions. We list here our main contributions.
1. We introduce a two-level representation of monotone sequences which, given a partition of a sequence into chunks, represents each chunk with Elias-Fano and stores the endpoints of the chunks and their boundary values in separate Elias-Fano sequences, in order to support fast random access and search operations.

2. We describe two partitioning strategies: an uniform strategy, which divides the sequence into chunks with a fixed size, and a strategy with variable-length chunks, whose endpoints are chosen by solving an optimization problem in order to minimize the overall space occupancy. More precisely, we introduce a linear-time dynamic programming algorithm which finds a partition whose cost is at most (1 + ) times larger than the optimal one, for any given  (0, 1). In the experiments we show that this -optimal strategy gives significantly smaller indexes than the uniform strategy, which in turn are significantly smaller than the indexes obtained with non-partitioned Elias-Fano. Indeed, the latter are more than 23% larger on ClueWeb09 and more than 64% larger on Gov2.
3. We show with an extensive experimental analysis that the partitioned indexes are only slightly slower than non-partitioned indexes. Furthermore, in comparison with other indexes from the literature, the -optimal indexes dominate in both space and time methods which are in the same region of the space-time tradeoff curve, and obtain spaces very close to the methods which give the highest compression ratio. More precisely, Binary Interpolative Coding is only 2%-8% smaller but up to 5.5 times slower; OptPFD is roughly 12% larger and almost always slower; Varint-G8IU is 10%-40% faster but more than 2.5 times larger.
2. BACKGROUND AND NOTATION
Given a collection D of documents, the posting list of a term t is the list of all the document identifiers, or docIds, that contain the term t. The collection of posting lists for all the terms in the collection is called the inverted index of D; the set of such terms is usually called the dictionary. Posting lists are often augmented with additional information about each docId, such as the number of occurrences of the term in the document (the frequency), and the set of positions where the term occurs. Since the inverted index is a fundamental data structure in virtually all modern search systems, there is a vast amount of literature describing several variants and query processing strategies; we refer the reader to the excellent surveys and books on the subject [6, 16, 27].
In the following, we adopt the docId-sorted variant, meaning that each posting list is sorted by docId; this enables fast query processing and efficient compression. In our experiments we focus our attention on posting lists storing docIds and frequencies; we do not store positions or other additional data, since they have different nature and often require specialized compression techniques [25], thus they are outside of the scope of this paper. We also ignore additional per-document or per-term information, such as the mappings between docIds and URLs, or between termIds and actual terms, as their space is negligible compared to the index size.
Query Processing. Given a term query as a (multi-)set of
terms, the basic query operations are the boolean conjunctive (AND) and disjunctive (OR) queries, retrieving the documents that contain respectively all the terms or at least one of them. In many scenarios the query-document pairs can be associated with a relevance score which is usually a function of the term frequencies in the query and in the document, and other global statistics. Instead of the full set of matches,

274

for scored queries it is often sufficient to retrieve the k highest scored documents for a given k. A widely used relevance score is BM25 [18], which we will use in our experiments.
There exist two popular query processing strategies, dual in nature, namely Term-at-a-Time (TAAT) and Documentat-a-Time (DAAT). The former scans the posting list of each query term separately to build the result set, while the latter scans them concurrently, keeping them aligned by docId. We will focus on the DAAT strategy as it is the most natural for docId-sorted indexes.
The alignment of the lists during DAAT scanning can be achieved by means of the NextGEQt(d) operator, which returns the smallest docId in the list of t that is greater than or equal to d. A fast implementation of the function NextGEQt(d) is crucial for the efficiency of this process. The trivial implementation that scans sequentially the whole posting lists is usually too slow; a common solution resorts to skipping strategies. The basic idea is to divide the lists in small blocks that are compressed independently, and to store additional information about each block, in particular the maximum docId present in the block. This allows to find and decode only the block that may contain the sought docId by scanning the list of maxima, thus skipping a potentially large number of useless blocks. In the following we call block-based the indexes that adopt this technique.
Solving scored queries can be achieved with DAAT by computing the relevance score for the matching documents as they are found, and maintaining a priority queue with the top-k matches. This can be very inefficient for scored disjunctive queries, as the whole lists need to be scanned. Several query processing strategies have been introduced to alleviate this problem. Among them, one the most popular is WAND [3], which augments the index by storing for each term its maximum impact to the score, thus allowing to skip large segments of docIds if they only contain terms whose sum of maximum impacts is smaller than the top-k documents found up to that point. Again, WAND can be efficiently implemented in terms of NextGEQt(d).
3. RELATED WORK
Index compression ultimately reduces to the problem of representing sequences, specifically strictly monotone sequences for docIds, and positive sequences for the frequencies. The two are equivalent: a strictly monotone sequence can be turned into a positive sequence by subtracting from each element the one that precedes it (also known as delta encoding), the other direction can be achieved by computing prefix sums. For this reason most of the work assumes that the posting lists are delta-encoded and focuses on the representation of sequences of positive integers.
Representing such sequences of integers in compressed space is a crucial problem which has been studied since the 1950s with applications going beyond inverted indexes. A classical solution is to assign to each integer an uniquelydecodable variable length code; if the codes do not depend on the input they are called universal codes. The most basic example is the unary code, which encodes a non-negative integer x as the bit sequence 0x1. The unary code is efficient only if the input distribution is concentrated on very small integers. More sophisticated codes, such as Elias Gamma/Delta codes and Golomb/Rice codes build on the unary code to efficiently compress a broader class of distributions. We refer to Salomon [19] for an in-depth discussion on this topic.

Bit-aligned codes can be inefficient to decode as they require several bitwise operations, so byte-aligned or wordaligned codes are usually preferred if speed is a main concern. Variable byte [19] or VByte is the most popular byte-aligned code. In VByte the binary representation of a non-negative integer x is split into groups of 7 bits which are represented as a sequence of bytes. The lower 7 bits of each byte store the data, whereas the eighth bit, called the continuation bit, is equal to 1 only for the last byte of the sequence. Stepanov et al. [22] present a variant of variable byte (called Varint-G8IU) which exploits SIMD operations of modern CPUs to further speed up decoding.
A different approach is to encode simultaneously blocks of integers in order to improve both compression ratio and decoding speed. This line of work has seen in the last few years a proliferation of encoding approaches which find their common roots in frame-of-reference (For) [14]. Their underlying idea is to partition the sequence of integers into blocks of fixed or variable length and to encode each block separately. The integers in each block are encoded by resorting to codewords of fixed length. A basic application of this technique (called also binary packing or packed binary [2,15]) partitions the sequence of integers into blocks of b consecutive integers (e.g., b = 128 integers); for each block, the algorithm encodes the range enclosing the values in the block, say [l, r], then each value is subtracted l and represented with h = log(r - l + 1) bits. There are several variants of this approach which differentiate themselves for their encoding or partitioning strategies [9, 15, 21]. For example, Simple-9 and Simple-16 [1, 2, 26] are two popular variants of this approach.
A major space inefficiency of For is the fact that the presence of few large values in the block forces the algorithm to encode all its integers with a large h, thus affecting the overall compression performance. To address this issue, PForDelta (PFD) [28] introduces the concept of patching. In PFD the value of h is chosen so that h bits are sufficient to encode a large fraction of the integers in the block, say 90%. Those integers that do not fit within h bits are called exceptions and encoded separately with a different encoder (e.g., Simple-9 or Simple-16). Yan et al.'s introduce the OptPFD variant [26], which selects for each block the value of h that minimizes the space occupancy. OptPFD is more compact and only slightly slower than the original PFD [15, 26].
A completely different approach is taken by Binary Interpolative Coding [17], which skips the delta-encoding step and directly encodes strictly monotone sequences. This method recursively splits the sequence of integers in two halves, encoding at each split the middle element and recursively the two halves. At each recursive step the range that encloses the middle element is reduced, and so is the number of bits needed to encode it. Experiments [21,24,26] have shown that Binary Interpolative Coding is the best encoding method for highly clustered sequence of integers. However, this space efficiency is paid at the cost of a very slow decoding algorithm.
Recently, Vigna [23] introduced quasi-succinct indexes, based on the Elias-Fano representation of monotone sequences, showing that it is competitive with delta-encoded block-based indexes. Our paper aims at making this representation more space-efficient.
Optimal partitioning algorithms. The idea of partition-
ing a sequence to improve compression dates back to Buchsbaum et al. [4], who address the problem of partitioning the

275

input of a compressor C so that compressing the chunks individually yields a smaller space than compressing the whole input at once. Their paper discusses only the case of compressing a large table of records with gzip but their solution can be adapted to solve the more general problem stated above. Their approach is to reduce this optimization problem to a dynamic programming recurrence which is solved in (m3) time and (m2) space, where m is the input size.
Silvestri and Venturini [21] resort to a similar dynamic programming recurrence to optimize their encoder for posting lists. They obtain an O(mh) construction time by limiting the length of the longest part to h. Ferragina et al. [12] significantly improve the result in [4] by computing in O(m log1+ m) time and O(m) space a partition whose compressed size is guaranteed to be at most (1 + ) times the optimal one, for any given > 0, provided that the compression ratio of C on any portion of the input can be estimated in constant time.
In this paper we apply the same ideas in [12] to the EliasFano representation and we exploit some of its properties to compute exactly, as opposed to estimating, its encoding cost in constant time. Then, we improve the optimization algorithm reducing its time to O(m) while preserving the same approximation guarantees.

4. SEARCHABLE SEQUENCES

The Elias-Fano representation [10, 11] is an elegant encod-

ing for monotone sequences, which provides good compression

ratio and efficient access and searching operations. Consider

a monotonically increasing sequence S[0, m - 1] of m non-

negative integers (i.e., S[i]  S[i + 1], for any 0  i < m - 1)

drawn from an universe [u] = {0, 1, . . . , u - 1}.

Given an integer , the elements of S are conceptually

grouped into buckets according to their log u - higher bits. The number of buckets is thus u . The cardinalities of
2
these buckets (including the empty ones) are written into a

bitvector H with negated unary codes; it follows that H has

length

at

most

m

+

u 2

bits. The remaining

lower bits of

each integer are concatenated into a bitvector L, which thus

requires m bits. It is easy to see that H and L are sufficient

to recover S[i] for every i: its log u - higher bits are equal

to the number of 0s preceding the ith 1 in H, and the lower

bits can be retrieved directly from L.

While can be chosen arbitrarily between 0 and log u ,

it can be shown that the value

=

log

u m

minimizes the

overall space. Summing up the lengths of H and L, it follows

that

the

representation

requires

at

most

m

log

u m

+ 2m bits.

Despite its simplicity, it is possible to support efficiently

powerful operations on S. For our purposes we are interested

in the following.

· Access(i) which, given i  [m], returns S[i];

· NextGEQ(x) which, given x  [u], returns the smallest element in S which is greater than or equal to x.

The support for these operations requires to augment H with an auxiliary data structure to efficiently answer Select0 and Select1 operations, which, given an index i, return the position of respectively the ith 1 or the ith 0 in H. See [23] and references therein for more details on the implementation
of these standard operations. To access the ith element of S we have to retrieve and
concatenate its higher and lower bits. The value of the higher bits is obtained by computing Select1(i) - i in H, which

represents the number of 0s (thus, buckets) ending before

the ith occurrence of 1. The lower bits are directly accessed

by reading consecutive bits in L starting from position i .

The operation NextGEQ(x) is supported by observing that

p = Select0(hx) - hx is the number of elements of S whose higher bits are smaller than hx, where hx are the higher bits of x. Thus, p is the starting position in S of the elements whose

higher bits are equal to hx (if any) or larger. NextGEQ(x) is identified by scanning the elements starting from p.

Several implementations of Elias-Fano supporting efficiently these operations are available1.

All the operations can be implemented in a stateful cursor,

in order to exploit locality of access by optimizing short

forward skips, which are very frequent in query processing.

An additional convenient cursor operation is Next, which

advances the cursor from position i to i + 1.

Note that Elias-Fano requires just weak monotonicity of S,

so if only the Access operation is needed, the space occupancy

can

be

reduced

to

m

log

u-m+1 m

+ 2m bits by turning S into

a weakly monotone sequence S [i] = S[i] - i and encoding S

with Elias-Fano. At query time we can recover the ith entry

of S by computing AccessS (i) + i. The quasi-succinct indexes of Vigna [23] are a direct ap-

plication of Elias-Fano; the posting lists are immediately

representable with Elias-Fano as the docIds are monoton-

ically sorted. Since Elias-Fano is not efficient for dense se-

quences, a bitvector is used instead when it is convenient to

do so; the Access and NextGEQ can be supported efficiently

with small additional data structures. The frequencies can

be turned into a strictly monotone sequence by computing

their prefix sums, and the ith frequency can be recovered as

Access(i)-Access(i-1). Moreover, since the query processing

needs only Access on the frequencies, we can use the trick

mentioned above to reduce the space occupancy of the rep-

resentation. In positional indexes, a similar transformation

can be used to represent the term positions.

4.1 Uniform partitioning

As we argued above Elias-Fano uses roughly

log

u m

+2

bits per element regardless the sequence being compressed.

Notice

that

log

u m

is

the

logarithm

of

the

average

distance

among consecutive elements in the sequence. Apart from this

average distance, Elias-Fano does not exploit in any way the

nature of the underlying sequence and, thus, for example it

does not make any distinction between the two extreme cases

of a randomly generated sequence and a sequence formed by

only a long run of consecutive integers. While paying

log

u m

bits per element is the best we can hope for in the former

case, we would expect to achieve a better space occupancy in

the latter. Indeed, a sequence of integers is intuitively more

compressible than a random one when it contains regions of

integers which are very close to each other. The presence of

these regions is typical in posting lists. Consider for example

a term which is present only in few domains. If the docIds are

assigned by sorting the documents by their URLs, then the

posting list of this term is, apart from few outliers, formed

by clusters of integers in correspondence of those domains.

Observe that, since the elements within each region are very

close to each other, the average distance intra-region is much

smaller than the global average distance.

1Such as the open-source http://github.com/facebook/ folly, http://github.com/simongog/sdsl, http:// github.com/ot/succinct, and http://sux.di.unimi.it.

276

The above observation is the main motivation for intro-

ducing a two-level Elias-Fano. The basic idea is to parti-

tion the sequence S into m/b chunks of b consecutive in-

tegers each, except possibly the last one. The first level is

an Elias-Fano representation of the sequence L obtained

by juxtaposing the last element of each chunk (i.e., L =

S[b - 1], S[2b - 1], . . . , S[m - 1]). The second level is the col-

lection of the chunks of S, each represented with Elias-Fano.

The main advantage of having this first level is that the ele-

ments of the jth chunk can be rewritten in a smaller universe

of size uj = L[j] - L[j - 1] - 1 by subtracting L[j - 1] + 1

from each element. Thus, the Elias-Fano representation of

the chunk requires

quantity

uj b

is the

log

uj b

average

+ 2 bits distance

per element. Since the of the elements within

the chunk, we expect that this space occupancy is much

smaller that the one obtained by representing the sequence

in its entirety, especially for highly compressible sequences.

Observe that part of this gain vanishes due to the cost of

the first level which is

log

u m/b

+ 2 bits every b original

elements. Indeed, the first level stores m/b integers drawn

from a universe of size u.

This two-level representation introduces a level of indi-

rection in solving the operations Access and NextGEQ. The

operation Access(i) is solved as follows. Let j be the index

of the chunk containing the ith element of S (i.e., j = i/b )

and k be its offset within this chunk (i.e., k = i mod b).

We access L[j - 1] and L[j] on the first level to compute

the size of the universe uj of the chunk as L[j] - L[j - 1] (or L[j] if j is the first chunk). Knowing uj and b suffices for accessing the kth element of the jth chunk. If e is the

value at this position, then we conclude that the value S[i]

is equal to L[j] + 1 + e. The operation NextGEQ(x) is solved

as follows. We first compute the successor of x, say L[j], on

the first level. This implies that the successor of x in S is

within the jth chunk and, thus, it can be identified by solving

NextGEQ(x - L[j] - 1) on this chunk.

An important distinction with block-based indexes is that

the choice of b does not affect significantly the efficiency

of the operations: while block-based indexes may need to

scan the full block to retrieve one element, the chunks in our

representation are searchable sequences themselves, so the

performance does not degrade as b gets larger; it actually

gets better, as fewer block boundaries have to be crossed

during a query.

In our implementation we use different encodings to over-

come the space inefficiencies of Elias-Fano in representing

dense chunks. The jth chunk is dense if the chunk covers

a large fraction of the elements in the universe [uj] (or,

in other words, b is close to uj). Indeed, the space bound

b

log

uj b

+ 2b bits becomes close to 2uj bits whenever b

approaches uj. However, we can always represent the chunk

within uj bits by writing the characteristic vector of the

set of its elements as a bitvector. Note that Vigna [23] also

uses this technique but only for whole lists, which are very

unlikely to be so dense except for very few terms such as

stop-words. In our case, instead, we expect dense chunks

to occur frequently in representing posting lists, because

they can be contained inside dense clusters of docIds. Hence,

besides Elias-Fano, we adopt two other encodings chosen de-

pending on the relation between uj and b. The first encoding

addresses the extreme case in which the chunk covers the

whole universe (i.e., whenever uj is equal to b). The first level gives us the values of uj and b which are enough by

themselves to derive all the elements in the chunk without

the need of encoding further information. Operations Access

and NextGEQ become trivial: both Access(i) and NextGEQ(i)

are equal to i. The second encoding is used whenever the

size of the Elias-Fano representation of the chunk is larger

than

uj

bits

(i.e.,

whenever

b

>

uj 4

).

In

this

case

we

encode

the set of elements in the chunk by writing its characteristic

vector in uj bits. The Access and NextGEQ operations can be reduced to the standard Rank and Select operations on

bitvectors; their implementation is described in detail in [23].

4.2 Optimal partitioning

Splitting S into chunks of fixed size is likely to be subopti-

mal, since we cannot expect the dense clusters in S to appear

aligned with the uniform partition. Intuitively it would be

better to allow S to be partitioned freely, with chunks of vari-

able size. It is not obvious however how to find the partition

that minimizes the overall space occupancy: on one hand, the

chunks should be as large as possible to minimize the number

of entries in the first level of the representation and, thus, its

space occupancy; on the other hand, the chunks should be as

small as possible to minimize the average distances between

their elements, and, thus, the space occupancy of the second

level of the representation. An optimal partition can be computed in (n2) time

and space by solving a variant of dynamic programming

recurrence introduced in [4]. However, these prohibitive com-

plexities make this solution unfeasible for inputs larger than

few thousands of integers. This is the main motivation for de-

signing an approximation algorithm which reduces the time

and space complexities to linear at the cost of finding slightly

suboptimal solutions. More precisely, in this subsection we

present an algorithm that identifies in O(m log1+ 1 ) time and linear space a partition whose cost is only 1 + times

larger than the optimal one, for any given  (0, 1). Observe

that the time complexity is linear as soon as is constant.

Before entering into the technical details of our solution, it

is convenient to fix precisely the space costs involved in our

representation. The space occupancy of a given partition P

of k chunks S[i0, i1 - 1] S[i1, i2 - 1] . . . S[ik-1, ik], with i0 = 0

and ik = m-1, is C(P ) =

k-1 h=0

C (S [ih ,

ih+1

-1])

bits,

where

C(S[i, j]) is the cost of representing the chunk S[i, j]. Each

of these costs C(S[i, j]) is the sum of two terms: a fixed cost

F to store information regarding the chunk in the first level

and the cost of representing its elements in the second level.

Concerning the fixed cost F , for each chunk we store three

integers in the first level: the largest integer within the chunk,

the size of the chunk, and the pointer to its second-level Elias-

Fano representation. Thus, we can safely upper bound this

cost F with the quantity 2 log u + log m bits. Instead, the

cost of representing the elements in S[i, j] is computed by

taking the minimum between the costs of the three possible

encodings introduced in the previous subsection. Depending

on the size of the universe u = S[j]-S[i-1] (or, u = S[j], if

i = 0) and the number of elements m = j - i + 1, these three

costs are i) m

+m

+

u 2

bits with

=

log

u m

, if S[i, j]

is encoded with Elias-Fano; ii) m bits, if S[i, j] is encoded

with its characteristic vector; iii) 0 bits, if m = u and,

thus, S[i, j] covers the whole universe. A crucial property

to devise our approximation algorithm is the monotonicity

of the cost function C, namely, for any i, j and k with

0  i < j < k  m, we have C(S[i, j])  C(S[i, k]).

In the following we first use the algorithm in [12] to ob-

277

tain a solution which finds a (1 + )-approximation in time

O(m log1+

U F

),

where

U

is

the

cost

in

bits

of

representing

S as a single partition. Then, we improve the algorithm to

obtain a linear time solution with the same approximation

guarantees. Following [12], it is convenient to recast our

optimization problem to the problem of finding a shortest

path in a particular directed acyclic graph (DAG) G. Given

the sequence S of m integers, the graph G has a vertex

v0, v1, . . . , vm-1 for each position of S plus a dummy vertex vm marking the end of the sequence. The DAG G is complete in the sense that, for any i and j with i < j  m, there

exists the edge from vi to vj, denoted as (vi, vj). Notice

that there is a one-to-one correspondence between paths

from v0 to vm in G and partitions of S. Indeed, a path  = (v0, vi1 )(vi1 , vi2 ) . . . (vik-1 , vim ) crossing k edges corresponds to the partition S[0, i1-1] S[i1, i2-1] . . . S[ik-1, m-1] of k chunks. Hence, by assigning the weight w(vi, vj) = C(S[i, j - 1]) to each edge (vi, vj), the weight of a path is

equal to the cost in bits of the corresponding partition. Thus,

a shortest path on G corresponds to an optimal partition of S.

Computing a shortest path on a DAG has a time complexity

proportional to the number of edges in the DAG. This is

done with a classical elegant algorithm which processes the

vertices from left to right [7]. The goal is to compute the

value M [v] for each vertex v which is equal to the cost of

a shortest path which starts at v0 and ends at v. Initially, M [v0] is set to 0, while M [v] is set to + for any other vertex v. When the algorithm reaches the vertex v in its

left-to-right scan, it assumes that M [v] has been already

correctly computed and extends this shortest path with any

edge outgoing from v. This is done by visiting each edge

(v, v ) outgoing from v and by computing M [v] + w(v, v ).

If this quantity is smaller than M [v ], the path that follows

the shortest path from v0 to v and then the edge (v, v ) is currently the best way to reach v . Thus, M [v ] is updated

to M [v] + w(v, v ). The correctness of this algorithm can be

proved by induction and, since each edge is relaxed exactly

once, its time complexity is proportional to the number of

edges in the DAG.

Unfortunately our DAG G is complete and, thus, it has (n2) edges, so this algorithm by itself does not suffice to

obtain an efficient solution for our problem. However, it

can be used as the last step of a solution which performs

a non-trivial pruning of G. This pruning produces another

DAG G with two crucial properties: i) its number of edges

is substantially reduced from (n2) to O(m log1+

U F

),

for

any given  (0, 1); ii) its shortest path distance is (almost)

preserved since it increases by no more than a factor 1 + .

The pruned graph G is constructed as the subgraph of G

consisting of all the edges (vi, vj) such that at least one of

the following two conditions holds: i) there exists an integer h  0 such that w(vi, vj)  F (1 + )h < w(vi, vj+1); ii) (vi, vj) is the last outgoing edge from vi (i.e., j = m).
Since w is monotone, these conditions correspond of keep-

ing, for each integer h, the edge of G that better approximates the value F (1 + )h from below. The edges of G are called

(1 + )-maximal edges. We point out that, since there exist at

most log1+

U F

possible values for h, each vertex of G

has at

most log1+

U F

outgoing (1 +

)-maximal edges. Thus, the to-

tal size of G

is O(m log1+

U F

).

Theorem

3

in

[12]

proves

that

the shortest path distance on G is at most 1 + times larger

than the one in G. Thus, given G , a (1 + )-approximated

partition can be computed in O(m log1+

U F

)

time

with

the

above algorithm.

We show now how to further reduce this time complexity

to O(m log1+ 1 ) time without altering the approximation guarantees. Let 1  (0, 1] and 2  (0, 1] be two parameters to be fixed later. We first obtain the graph G¯ from G by

keeping only edges whose weight is no more than L = F + 2F 1
plus the first edge outgoing from every vertex whose cost is larger than L. Then, we apply the pruning above to G¯ by

fixing the approximation parameter to 2. The only difference

here is that we force the pruning to retain the m edges in G¯ of

cost larger than L. In this way we obtain a graph G¯ 2 having

O(m log1+ 2

L F

)

=

O(m log1+

2

1 ) edges. We can prove that
1

the shortest path distance in G¯ 2 is at most (1 + 1)(1 + 2)

times larger than the one in G. This implies that the partition

computed with G¯ is a (1 + )-approximation of the optimal

one, by setting 1 = 2 = 3 so that (1 + 1)(1 + 2)  1 + . This result is stated in the following lemma.

Lemma 1. For any 1 > 0 and 2 > 0, the shortest path distance in G¯ 2 is at most (1 + 1)(1 + 2) times larger than the one in G.

Proof. Let G, G¯ and G¯ 2 denote the shortest paths in G, G¯ and G¯ 2 , respectively. We know that the weight w(G¯ 2 ) of G¯ 2 satisfies w(G¯ 2 )  (1 + 2)w(G¯). It remains to prove that w(G¯)  (1 + 1)w(G) which allows us to conclude that
w(G¯ 2 )  (1 + 1)(1 + 2)w(G ). We do so by showing that there exists a path  in G¯ such

that its weight is no more than (1 + 1) times the weight of
the shortest path G of G. The thesis follows immediately because G¯ is a shortest path, so by definition w(G¯)  w( ).
The path  is obtained by transforming G so that its edges are all contained in G¯. Note that this transformation is

not actually performed by the algorithm but it only serves

for proving the lemma.

This transformation is done as follows. For each edge

(ui, vj) in G¯, either w(vi, vj)  L, so there is nothing to do, or w(vi, vj) > L, in which case we need to substitute it with a subpath from vi to vj whose edges are all in G¯. This subpath can be found greedily, starting from vi and traversing always the longest edge until we reach vj. It can be proved that the weighting function w satisfies w(vi, vk) + w(vk, vj)  w(vi, vj) + F + 1 for any 0  i < k < j  m; intuitively, this

means that by breaking an edge into two shorter edges we

lose at most F + 1 bits. By combining these properties with

the fact that all the edges in the subpath except possibly

the last have cost at least L, it follows that the number of

edges

in

this

subpath

cannot

be

larger

than

w(vi,vj )-F L-F

+1

and its cost is at most w(vi, vj) +

w(vi,vj )-F L-F

+1

(F + 1) 

w(vi, vj) + 1w(vi, vj), thus proving the thesis.

It remains to describe how to generate the pruned graph G¯ 2 in O(n log1+ 1 ) time directly without explicitly constructing G which, otherwise, would require quadratic time. This is done by keeping k = O(log1+ 1 ) windows W0, . . . , Wk sliding over the sequence S, one for each possible exponent h such that F  F (1 + )h  L. These sliding windows cover potentially different portions of S that start at the same position i but have different ending positions. Armed with
these sliding windows, we generate the (1 + )-maximal edges
outgoing from any vertex vi on-the-fly as soon as the shortest
path algorithm visits this vertex. Initially, each sliding win-
dow Wj starts and ends at position 0. During the execution,

278

every time the shortest path algorithm visits the next vertex vi, we advance the starting position of each Wj by one position and its ending position until the cost of representing the currently covered portion of S is larger than F (1 + )j. It is easy to prove that if at the end of these moves the window Wj covers S[i, r], then (vi, vr) is the (1 + )-maximal edge outgoing from the vertex vi for the weight bound F (1 + )j. Notice that with this approach we generate all the maximal edges of G¯ 2 by performing a scan of S for each sliding window. Every time we move the starting or the ending position of a window we need to evaluate the cost of representing its covered portion of S. This can be done in constant time with simple arithmetic operations. Thus, it follows that generating the pruned graph requires constant time per edge, hence O(n log1+ 1 ) time overall.
We conclude the section by describing how to modify the first-level data structure to support arbitrary partitions: together with the first-level sequence L with the last element of each block, we write a second sequence E which contains the positions of the endpoints of the partition. This sequence can be again represented with Elias-Fano. The NextGEQ operation can be supported as before, while for Access(i) we can find the chunk of i with NextGEQ on E. Both L and E have as many elements as the number of the chunks in the partition.
5. EXPERIMENTAL ANALYSIS
We performed our experiments on the following datasets.
· ClueWeb09 is the ClueWeb 2009 TREC Category B test collection, consisting of 50 million English web pages crawled between January and February 2009.
· Gov2 is the TREC 2004 Terabyte Track test collection, consisting of 25 million .gov sites crawled in early 2004; the documents are truncated to 256 kB.

Documents Terms Postings

Gov2
24, 622, 347 35, 636, 425 5, 742, 630, 292

ClueWeb09
50, 131, 015 92, 094, 694 15, 857, 983, 641

Table 1: Basic statistics for the test collections

For each document in the collection the body text was extracted using Apache Tika2, and the words lowercased and stemmed using the Porter2 stemmer; no stopwords were removed. The docIds were assigned according to the lexicographic order of their URLs [20]. Table 1 reports the basic statistics for the two collections.

Indexes tested. We compare three versions of Elias-Fano
indexes, namely EF single, EF uniform, and EF -optimal, that are respectively the original single-partition representation, a partitioned representation with uniform partitions, and a variable-length partitioned representation optimized with the algorithm of Section 4.2. The implementation of the base Elias-Fano sequences is mostly faithful to the original description and source code [23].
To validate Elias-Fano indexes against the more widespread block-based indexes, we tested three more indexes, namely
2http://tika.apache.org/

Interpolative, OptPFD, and Varint-G8IU, which are representative of best compression ratio, best compression ratio/processing speed trade-off, and highest speed in the literature [15, 21]. For the last two use the code made available by the authors of [15]. Actually, recent experiments [15] show that there exist methods faster than Varint-G8IU by roughly 50% in sequential decoding. However, they require very large blocks (from 211 to 216) making block decoding prohibitively expensive if the lists are sparsely accessed.
For these three indexes we encoded the lists in blocks of 128 postings; we keep in a separate array the maximum docId of each block to perform fast skipping through linear search (we experimented with falling back to binary search after a small lookahead, but got worse results). Blocks of postings and blocks of frequencies are interleaved, and the frequencies blocks are decoded lazily as needed. The last block of each list is encoded with variable bytes if it is smaller than 128 postings to avoid any block padding overhead.
All the implementations of posting lists expose the same interface, namely the Access, NextGEQ, and Next operations described in Section 4. The query algorithms are C++ templates specialized for each posting list implementation to avoid the virtual method call overhead, which can be significant for operations that take just a handful of CPU cycles. For the same reason, we made sure that frequent code paths are inlined in the query processing code.
Testing details. All the algorithms were implemented in
C++11 and compiled with GCC 4.9 with the highest optimization settings. We do not use any SIMD instructions or special processor features, except for the instructions to count the 1 bits in a word and to find the position of the least significant bit; both are available in modern x86-64 CPUs and exposed by the compiler through portable builtins. Apart from this, all the code is standard C++11. The tests were performed on a machine with 24 Intel Xeon E5-2697 Ivy Bridge cores (48 threads) clocked at 2.70Ghz, with 64GiB RAM, running Linux 3.12.7.
The data structures were saved to disk after construction, and memory-mapped to perform the queries. Before timing the queries we ensure that the index is fully loaded in memory.
The source code is available at http://github.com/ot/ partitioned_elias_fano/tree/sigir14 for the reader interested in replicating the experiments.
5.1 Space and construction time
Before comparing the spaces obtained with the different indexes, we have to set the parameters needed by the partitioned EF indexes, namely the chunk size for EF uniform and the approximation parameters 1, 2 for EF -optimal.
For the former, we show in Figure 1 the space obtained by EF uniform on Gov2 for different values of the chunk size. The minimum space is obtained at 128, which is also a widely used block size for block-based indexes; in all the following experiments we will use this chunk size.
A little more care has to be put in choosing the parameters 1 and 2 since they significantly affect the construction time, as shown in Figure 2. First we fix 1 at 0, thus posing no upper bound on the chunk costs, and let 2 vary. Notice that the (1 + 2) approximation bound is very pessimistic: even setting 2 as high as 0.5, meaning a potential 50% overhead, the actual overhead is never higher than 1.5%. Hence, we set 2 = 0.3 to control the construction time.

279

EF single EF uniform EF -optimal
Interpolative OptPFD Varint-G8IU

Gov2

space

doc

freq

GB

bpi

bpi

7.66 (+64.7%) 5.17 (+11.2%) 4.65

7.53 (+83.4%) 3.14 (+32.4%)

4.63 (+12.9%) 2.58 (+8.4%)

4.10

2.38

4.57 (-1.8%) 4.03 (-1.8%) 2.33 (-1.8%) 5.22 (+12.3%) 4.72 (+15.1%) 2.55 (+7.4%) 14.06 (+202.2%) 10.60 (+158.2%) 8.98 (+278.3%)

ClueWeb09

space

doc

freq

GB

bpi

bpi

19.63 (+23.1%) 17.78 (+11.5%) 15.94

7.46 (+27.7%) 2.44 (+11.0%)

6.58 (+12.6%) 2.39 (+8.8%)

5.85

2.20

14.62 (-8.3%) 5.33 (-8.8%) 2.04 (-7.1%) 17.80 (+11.6%) 6.42 (+9.8%) 2.56 (+16.4%) 39.59 (+148.3%) 10.99 (+88.1%) 8.98 (+308.8%)

Table 2: Overall space in gigabytes, and average bits per docId and frequency

8.00 7.50 7.00 6.50 6.00 5.50 5.00 4.50
32

EF single EF uniform EF -opt

64

128

256

512

1024

Figure 1: Index size in gigabytes for Gov2 with EF uniform at different chunk sizes

After fixing 2, we let 1 vary. Notice the sharp drop in running time without a noticeable increase in space as soon
as 1 is non-zero: it is a direct consequence of the algorithm going from O(m log m)-time to O(m)-time. Again, the spread between the worse and best solutions found is smaller than 1%. In the following, we set 1 = 0.03. We found that with these parameters, the average chunk length on Gov2 for docId sequences is 231 and for frequencies 466. On ClueWeb09 they are respectively 142 and 512. For brevity we omit the plots for ClueWeb09, which are very similar to the ones for Gov2.

4.66 4.65 4.64 4.63 4.62 4.61 4.60 4.59
0.0
4.85
4.80
4.75
4.70
4.65
4.60 0.00

0.1

0.2

0.3

0.4

(a) 1 = 0, varying 2 from 0.025 to 0.5

0.02

0.04

0.06

0.08

(b) 2 = 0.3, varying 1 from 0 to 0.1

3500 3000 2500 2000 1500 1000 500 0 0.5
350 300 250 200 150 100 50 0.10

Figure 2: Influence of the parameters 1 and 2 on the EF -optimal indexes for Gov2. Solid line is the overall size in gigabytes (left scale), dashed line is
the construction time in minutes (right scale).

Table 2 shows the index space, both as overall size in gigabytes and broken down in bits per integers for docIds and frequencies. Next to each value is shown the relative loss/gain (in percentage) compared to EF -optimal. The results confirm that partitioning the indexes indeed pays off: compared to EF -optimal, EF single is 64.7% larger on Gov2 and 23.1% larger on ClueWeb09. The optimization strategy also produces significant savings: EF uniform is about 11% larger on both Gov2 and ClueWeb09, but still significantly smaller than EF single.
Compared to the other indexes, Varint-G8IU is by far the largest, 2.5 to 3 times larger than EF -optimal; it is particularly inefficient on the frequencies, as it needs at least 8 bits to encode an integer. On the other end of the spectrum, Interpolative confirms its high compression ratio and produces the smallest indexes, but the edge with EF -optimal is surprisingly small: only 1.8% on Gov2 and 8.3% on ClueWeb09. To conclude the comparison, we observe that OptPFD loses more than 10% w.r.t. EF -optimal: 12.3% on Gov2 and 11.6% on ClueWeb09. Since, as we will show in the following, EF -optimal is also faster than OptPFD, this implies that our solution dominates OptPFD on the trade-off curve.
Regarding construction time, for all the indexes except EF -optimal, Gov2 can be processed on a single thread in 8 to 10 minutes, while ClueWeb09 in 24 to 30 minutes; in both cases the construction is essentially I/O-bound. For EF -optimal, instead, the algorithm that computes the optimal partition, despite linear-time, has a noticeable CPU cost, raising the time for Gov2 to 95 minutes and for ClueWeb09 to 284 minutes. However, since the encoding of each list can be performed independently, using all the 24 cores of the test machine makes EF -optimal construction I/O-bound as well. Parallelization does not improve the construction time of the other encoders.
5.2 Query processing
To evaluate the speed of query processing we randomly sampled two sets of 1000 queries respectively from TREC 2005 and 2006 Efficiency Track topics, drawing only queries whose terms are all in the collection dictionary. The two dataset have quite different statistics, as will be apparent in the results.
In his experimental analysis, Vigna [23] provides some examples showing that Elias-Fano indexes are particularly efficient in conjunctive queries that have sparse results. To make the analysis more systematic, we define as selective any query such that the fraction of the documents that contain all its terms over those that contain at least one of them is small (we set this threshold to 0.5%). For both datasets,

280

selective queries among TREC 2005 queries are at least 58%, and among TREC 2006 queries at least 78%, hence making up the most part of the samples.
The query times were measured by running each query set 3 times, and averaging the results. All the times are reported in milliseconds. In the timings tables, next to each timing is reported in parentheses the relative percentage against EF -optimal. Not very surprisingly, Interpolative is always 50% to 500% slower than the others, and Varint-G8IU is 10% to 40% faster, so for the sake of brevity we will focus the following analysis on the Elias-Fano indexes and OptPFD.

Boolean queries. We first analyze the basic disjunctive
(OR) and conjunctive (AND) queries. Note that these queries do not need the term frequencies, so only the docId lists are accessed. For these types of operations, we measure the time needed to count the number of results matching the query.

Gov2

ClueWeb09

TREC 05 TREC 06 TREC 05 TREC 06

EF single

80.7 (+8%) 175.0 (+10%) 261.0 (+0%) 444.0 (-2%)

EF uniform 72.1 (-3%) 154.0 (-3%) 254.0 (-3%) 435.0 (-4%)

EF -optimal 74.5

159.0

261.0

451.0

Interpolative 121.0 (+62%) 257.0 (+62%) 399.0 (+53%) 680.0 (+51%)

OptPFD

69.5 (-7%) 148.0 (-7%) 235.0 (-10%) 398.0 (-12%)

Varint-G8IU 67.4 (-10%) 143.0 (-10%) 222.0 (-15%) 375.0 (-17%)

Table 3: Times for OR queries

Times for OR queries are reported in Table 3. Unsurprisingly, as OR needs to scan the whole lists, block-based indexes perform better than Elias-Fano indexes, since they are optimized for raw decoding speed. However, the edge is not as high as one could expect, ranging from 7% to 17%. In sequential decoding Varint-G8IU can be even double as fast as OptPFD [15], however in this task it is not even 10% faster. The reason can be most likely traced back to branch misprediction penalties: the cost of decoding an integer is in the order of 2-5 CPU cycles; at each decoded docId, the CPU has to decide whether it is equal to the current candidate docId, or if it must be considered as a candidate for the next docId. The resulting jump is basically unpredictable, and the branch misprediction penalty on modern CPUs can be as high as 10-20 cycles (specifically at least 15 for the CPU we used [13]), thus becoming the main bottleneck of query processing.
Among Elias-Fano indexes, EF -optimal and EF uniform are either negligibly slower or slightly faster than EF single; we believe that the additional complexity is balanced by the higher memory throughput caused by the smaller sizes.
Table 4 reports the times for AND queries. Again, EF -optimal is competitive with EF single, except for selective queries where the overhead is slightly higher, touching 14%. EF uniform is slightly slower, which is likely caused by the smaller chunk sizes compared to EF -optimal. This will also be the case in the other queries. Compared to OptPFD, EF -optimal is 14% to 26% faster in all cases on general queries. The gap becomes even higher for selective queries, ranging from 34% to 40%, confirming the observations made in [23].

Ranked queries. In order to analyze the impact of accessing
the frequencies in query time, we measured the time required to find the top-10 results for AND and WAND [3] queries

Gov2

ClueWeb09

TREC 05 TREC 06 TREC 05 TREC 06

EF single

2.1 (+10%) 4.7 (+1%) 13.6 (-5%) 15.8 (-9%)

EF uniform 2.1 (+9%) 5.1 (+10%) 15.5 (+8%) 18.9 (+9%)

EF -optimal 1.9

4.6

14.3

17.4

Interpolative 7.5 (+291%) 20.4 (+343%) 55.7 (+289%) 76.5 (+341%)

OptPFD

2.2 (+14%) 5.7 (+24%) 16.6 (+16%) 21.9 (+26%)

Varint-G8IU 1.5 (-20%) 4.0 (-13%) 11.1 (-23%) 14.8 (-15%)

(a) All queries

Gov2

ClueWeb09

TREC 05 TREC 06 TREC 05 TREC 06

EF single

1.1 (-11%) 2.5 (-9%) 9.2 (-14%) 11.1 (-13%)

EF uniform 1.3 (+8%) 3.0 (+11%) 11.3 (+6%) 13.0 (+2%)

EF -optimal 1.2

2.7

10.7

12.7

Interpolative 6.0 (+399%) 14.3 (+430%) 49.9 (+368%) 61.0 (+379%)

OptPFD

1.6 (+34%) 3.8 (+40%) 13.0 (+22%) 17.0 (+33%)

Varint-G8IU 1.1 (-11%) 2.5 (-6%) 8.8 (-18%) 11.3 (-12%)

(b) Selective queries

Table 4: Times for AND queries

using BM25 [18] scoring. Results for scored AND, reported in Table 5, and for
WAND, reported in Table 6, are actually very similar. As before, we note that the overhead of EF -optimal against EF single is small, ranging between 2% and 13% for all queries, and between 8% and 18% for selective queries. Also as before, EF uniform is about 10% slower than EF -optimal; this is likely caused by the optimal partitioning algorithm placing chunk endpoints around dense clusters of docId, hence making the query algorithms crossing fewer chunk boundaries. Compared to OptPFD, EF -optimal is slightly slower on Gov2 and slightly faster on the larger ClueWeb09 on all queries. On selective queries, however, it is never slower, and up to 23% faster, thus providing further evidence that Elias-Fano indexes benefit significantly from selectiveness, even for non-conjunctive queries.

Gov2

ClueWeb09

TREC 05 TREC 06 TREC 05 TREC 06

EF single EF uniform EF -optimal

4.0 (-2%) 8.4 (-7%) 22.8 (-9%) 24.6 (-13%)

4.4 (+7%) 9.8 (+8%) 27.3 (+9%) 31.0 (+9%)

4.1

9.0

25.1

28.4

Interpolative 14.1 (+242%) 38.6 (+327%) 99.1 (+295%) 132.0 (+365%)

OptPFD

3.9 (-7%) 9.2 (+1%) 25.8 (+3%) 31.6 (+11%)

Varint-G8IU 2.6 (-38%) 5.5 (-39%) 15.8 (-37%) 18.0 (-37%)

(a) All queries

Gov2

ClueWeb09

TREC 05 TREC 06 TREC 05 TREC 06

EF single EF uniform EF -optimal

1.7 (-16%) 2.1 (+7%) 2.0

4.1 (-14%) 12.5 (-18%) 16.2 (-17%)

5.2 (+9%) 16.3 (+7%) 21.1 (+8%)

4.8

15.2

19.4

Interpolative 10.2 (+412%) 25.9 (+439%) 80.7 (+430%) 99.7 (+412%)

OptPFD

2.3 (+18%) 5.7 (+18%) 18.8 (+23%) 23.1 (+19%)

Varint-G8IU 1.4 (-32%) 3.3 (-32%) 10.6 (-30%) 13.6 (-30%)

(b) Selective queries

Table 5: Times for AND top-10 BM25 queries

281

Gov2

ClueWeb09

TREC 05 TREC 06 TREC 05 TREC 06

EF single EF uniform EF -optimal

8.8 (-4%) 15.8 (-7%)

9.7 (+6%) 18.2 (+7%)

9.2

17.1

31.5 (-7%) 41.2 (-13%)

36.9 (+9%) 51.2 (+8%)

34.0

47.4

Interpolative 28.0 (+203%) 62.6 (+267%) 123.0 (+262%) 200.0 (+322%)

OptPFD

8.7 (-6%) 16.7 (-2%) 35.6 (+5%) 52.1 (+10%)

Varint-G8IU 6.1 (-34%) 11.1 (-35%) 24.0 (-30%) 34.3 (-28%)

(a) All queries

Gov2

ClueWeb09

TREC 05 TREC 06 TREC 05 TREC 06

EF single EF uniform EF -optimal

8.5 (-8%) 12.2 (-9%)

9.8 (+6%) 14.4 (+7%)

9.2

13.5

21.0 (-19%) 34.2 (-15%)

27.3 (+6%) 43.0 (+7%)

25.8

40.1

Interpolative 33.8 (+265%) 54.7 (+307%) 115.0 (+345%) 179.0 (+346%)

OptPFD

9.3 (+0%) 14.1 (+4%) 29.9 (+16%) 45.7 (+14%)

Varint-G8IU 6.3 (-32%) 9.2 (-32%) 19.2 (-26%) 30.0 (-25%)

(b) Selective queries

Table 6: Times for WAND top-10 BM25 queries

6. CONCLUSION AND FUTURE WORK
We introduced two new index representations, EF uniform and EF -optimal, which significantly improve the compression ratio of Elias-Fano indexes at a small query performance cost. EF -optimal is always convenient except when construction time is an issue, in which case EF uniform offers a  12% worse compression ratio without this additional construction overhead. Furthermore, EF -optimal offers a better spacetime trade-off than the state-of-the-art OptPFD.
Future work will focus on making partitioned Elias-Fano indexes even faster; in particular it may be worth exploring the different space-time trade-offs that can be obtained by varying the average chunk size. It would also be interesting to devise faster algorithms to compute optimal partitions preserving the same approximation guarantees.
Acknowledgements
This work was supported by Midas EU Project (318786), by MIUR of Italy project PRIN ARS Technomedia 2012 and by eCloud EU Project (325091).
We would like to thank Sebastiano Vigna for sharing with us his C++ implementation of Elias-Fano indexes, which we used to validate our implementation.
7. REFERENCES
[1] V. N. Anh and A. Moffat. Inverted index compression using word-aligned binary codes. Inf. Retr., 8(1), 2005.
[2] V. N. Anh and A. Moffat. Index compression using 64-bit words. Softw., Pract. Exper., 40(2):131­147, 2010.
[3] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Y. Zien. Efficient query evaluation using a two-level retrieval process. In CIKM, pages 426­434, 2003.
[4] A. Buchsbaum, G. Fowler, and R. Giancarlo. Improving table compression with combinatorial optimization. Journal of the ACM, 50(6):825­851, 2003.
[5] S. Bu¨ttcher and C. L. A. Clarke. Index compression is good, especially for random access. In CIKM, 2007.

[6] S. Bu¨ttcher, C. L. A. Clarke, and G. V. Cormack. Information retrieval: implementing and evaluating search engines. MIT Press, Cambridge, Mass., 2010.
[7] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms. The MIT Press, 2009.
[8] M. Curtiss and et al. Unicorn: A system for searching the social graph. VLDB, 6(11):1150­1161, Aug. 2013.
[9] R. Delbru, S. Campinas, and G. Tummarello. Searching web data: An entity retrieval and high-performance indexing model. J. Web Sem., 10:33­58, 2012.
[10] P. Elias. Efficient storage and retrieval by content and address of static files. J. ACM, 21(2):246­260, 1974.
[11] R. M. Fano. On the number of bits required to implement an associative memory. Memorandum 61, Computer Structures Group, MIT, Cambridge, MA, 1971.
[12] P. Ferragina, I. Nitto, and R. Venturini. On optimally partitioning a text to improve its compression. Algorithmica, 61(1):51­74, 2011.
[13] A. Fog. The microarchitecture of Intel, AMD and VIA CPUs. http: //www.agner.org/optimize/microarchitecture.pdf.
[14] J. Goldstein, R. Ramakrishnan, and U. Shaft. Compressing relations and indexes. In ICDE, 1998.
[15] D. Lemire and L. Boytsov. Decoding billions of integers per second through vectorization. Software: Practice & Experience, 2013.
[16] C. D. Manning, P. Raghavan, and H. Schu¨lze. Introduction to Information Retrieval. Cambridge University Press, 2008.
[17] A. Moffat and L. Stuiver. Binary interpolative coding for effective index compression. Inf. Retr., 3(1), 2000.
[18] S. E. Robertson and K. S. Jones. Relevance weighting of search terms. Journal of the American Society for Information science, 27(3):129­146, 1976.
[19] D. Salomon. Variable-length Codes for Data Compression. Springer, 2007.
[20] F. Silvestri. Sorting out the document identifier assignment problem. In ECIR, pages 101­112, 2007.
[21] F. Silvestri and R. Venturini. VSEncoding: Efficient coding and fast decoding of integer lists via dynamic programming. In CIKM, pages 1219­1228, 2010.
[22] A. A. Stepanov, A. R. Gangolli, D. E. Rose, R. J. Ernst, and P. S. Oberoi. Simd-based decoding of posting lists. In CIKM, pages 317­326, 2011.
[23] S. Vigna. Quasi-succinct indices. In WSDM, 2013. [24] I. H. Witten, A. Moffat, and T. C. Bell. Managing
gigabytes (2nd ed.): compressing and indexing documents and images. Morgan Kaufmann Publishers Inc., 1999. [25] H. Yan, S. Ding, and T. Suel. Compressing term positions in web indexes. In SIGIR, pages 147­154, 2009. [26] H. Yan, S. Ding, and T. Suel. Inverted index compression and query processing with optimized document ordering. In WWW, pages 401­410, 2009. [27] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Comput. Surv., 38(2), 2006. [28] M. Zukowski, S. Heman, N. Nes, and P. Boncz. Super-scalar RAM-CPU cache compression. In ICDE, 2006.

282

Principled Dictionary Pruning for Low-Memory Corpus Compression

Jiancong Tong

Anthony Wirth

Justin Zobel

jctong@nbjl.nankai.edu.cn awirth@unimelb.edu.au jzobel@unimelb.edu.au

College of Computer and Control Engineering, Nankai University, China Department of Computing and Information Systems, The University of Melbourne, Australia

ABSTRACT
Compression of collections, such as text databases, can both reduce space consumption and increase retrieval efficiency, through better caching and better exploitation of the memory hierarchy. A promising technique is relative Lempel-Ziv coding, in which a sample of material from the collection serves as a static dictionary; in previous work, this method demonstrated extremely fast decoding and good compression ratios, while allowing random access to individual items. However, there is a trade-off between dictionary size and compression ratio, motivating the search for a compact, yet similarly effective, dictionary.
In previous work it was observed that, since the dictionary is generated by sampling, some of it (selected substrings) may be discarded with little loss in compression. Unfortunately, simple dictionary pruning approaches are ineffective. We develop a formal model of our approach, based on generating an optimal dictionary for a given collection within a memory bound. We generate measures for identification of low-value substrings in the dictionary, and show on a variety of sizes of text collection that halving the dictionary size leads to only marginal loss in compression ratio. This is a dramatic improvement on previous approaches.
Categories and Subject Descriptors
E.4 [Coding and Information Theory]: [Data compaction and compression]; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Search process
Keywords
Corpus compression; string algorithms; retrieval efficiency; optimization
1. INTRODUCTION
Compression plays a key role in the efficiency of large-scale information retrieval systems such as web search engines [1, 2, 7, 19, 28, 31]. In particular, compression of stored data
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609576.

can enable both reduced storage demands and improved retrieval speed, through lower data transfer costs and better caching. For web-scale collections, a repository compression scheme must meet several constraints: that documents can be retrieved and decompressed in any order; that memory requirements are reasonable, regardless of the size of the collection; and that new material can be added to the repository. Underlying this, good compression effectiveness must be achieved and decompression speed must be high.
An approach to compression that meets these goals is relative Lempel-Ziv factorization (RLZ) [9, 15, 30]. In RLZ, the collection text is parsed into a contiguous sequence of fragments, where each fragment is sourced from an external static dictionary. RLZ dramatically outperforms repository adaptations of general-purpose compression approaches, for example based on the Lempel-Ziv (LZ) family [29], as it can exploit global properties of the collection.
RLZ uses a portion of the to-be-compressed data as the external dictionary [9, 15]. In the dictionary-generation method proposed by Hoobin et al. [9], fixed-size blocks of data (say 1 KB) are sampled from the repository and then concatenated to form the dictionary. In this way, a dictionary of any given size can be generated by simply varying the number and size of samples. With a sufficient number of samples ­ say, a million ­ there is an extremely high likelihood that all common strings are present in the dictionary, and (as we confirm in our experiments reported here) excellent compression can be achieved, easily outperforming, for example, Huffman-based methods [28].
However, it is also the case that some strings are sampled many times (as would be expected, statistically), meaning that there is extensive redundancy in the dictionary and it is larger than required. Hoobin et al. [10] observed that some parts of the dictionary were rarely, or even never, used. As an illustration, with the first 1 GB of documents in GOV2 [6] as the test collection, and 5% of the collection sampled (with a sample block size of 1 KB) as the test dictionary, we compress the test collection relative to the test dictionary. The reference frequency of each byte in the dictionary is the number of times that byte is referred to by an LZ factor in the compressed collection. Figure 1 visualizes the reference frequency for 32 randomly chosen blocks, and shows that some of the dictionary is indeed little used.
Pruning. We wish to obtain the best possible compression
performance for a given dictionary size. As shown in previous work [9], and as we show here, larger dictionaries give better compression. But to achieve fast decoding and ran-

283

ref = [0, 5)

ref = [5, 20) ref = [20, Inf)

n-th byte vector
8 12 16 20 24 28 32

14

1 128 256 384 512 640 768 896 1024
n-th byte in the byte vector
Figure 1: Reference dictionary frequency heat-map for 32 1-KB blocks randomly extracted from a 50 MB RLZ dictionary. The darker the point, the higher the frequency of use in the factorization.
dom access, RLZ also requires the dictionary to be small enough to reside in memory. In a mobile environment, the limits on transmission speed and storage space make it valuable to keep the dictionary as small as possible, while still maintaining a good compression ratio. Excessive dictionary size may also be a disadvantage when data is compressed and mirrored in two remote servers.
However, optimal dictionary pruning is a difficult problem. In a dictionary D, there are (|D|2) candidate segments to remove. Removal of a single segment has an unpredictable effect on the remainder of the dictionary. A substring of length may be identical in the first - 1 bytes to some other substring, or may be entirely different to any other material; several low-usage substrings may be identical but for one byte; deletion of a substring creates new substrings by concatenation of the material to the left and right; and so on. We have found in our experiments that na¨ive approaches to pruning do not give good results.
In this paper, we formulate dictionary pruning as an optimization problem. Pruning is similar to known NP-hard compression problems, so we propose a heuristic measure to measure the `value' (in terms of contribution to compression performance) of each byte in the dictionary. This measure guides our iterative scheme for pruning the dictionary in a principled way.
As we do not alter the decompression-time costs, there is no impact on the impressive retrieval speed that was originally reported. The results for compression show that we can substantially reduce the dictionary size with only a small fraction of the compression degradation of other methods. For example, on the 426 GB GOV2 collection and a dictionary of 1000 MB, the data is reduced to 10.271% of its original size; with the existing method [10], halving the dictionary to 500 MB increased compressed size by 0.276% (in ab-

solute terms), whereas with our method it increases by only 0.005%. Halving again to 250 MB gives increases of 3.181% (previous method) and 0.636% (our method), respectively, compared to the 1000 MB dictionary. Our method shows how to halve dictionary size with virtually no impact on compression, and thus has the potential to yield substantial gains in practice for data retrieval, storage, and transmission.
2. RELATED WORK
Compression has been extensively employed and studied in the area of text retrieval systems. In this paper, we focus on compression of the text collection [8], a very different problem to inverted index compression [32] and full-text index compression [20].
The general-purpose LZ family [29] can be viewed as an on-line dictionary-based compression scheme. A sliding window captures repetition in the data; these previously observed strings act as an internal dynamic dictionary. The compression observed on a single document tends to be poor, since insufficient material is available to build a representative dictionary; to adapt these approaches to repositories, typically one concatenates and compresses blocks of documents together. While this can provide good compression, it means that a whole block must be transmitted and decompressed to access a single document, greatly reducing the value and applicability of the method.
There are several approaches based on off-line or static dictionaries. Word-based methods using Huffman codes have attracted considerable interest in information retrieval research [28], but have shortcomings. In particular, they are limited to cases where the characteristics of the data (for example, that it consists of ASCII text that can be parsed into words) are known in advance; and compression performance is relatively poor. Another family is based on dictionary inference [3, 4, 18, 24]. These methods use the data (or a large part of it) to infer a dictionary represented as a simple hierarchical grammar, and then replace the bytes or words with references to tokens in the dictionary. They have the general strong disadvantage that the data must be held in memory during dictionary inference. An alternative, proposed by Kuruppu et al. [14] for genomic data, is to construct the grammar iteratively in an offline manner, which can yield reasonable compression performance, but is extremely slow and the resulting dictionary tends to be large.
A further class of methods is based on delta compression, which is primarily designed for sharing of versions of material and requires that the whole repository be used as a dictionary of long strings [11, 13, 21]. While it has superficial similarities to our problem, such methods do not by themselves constitute a solution for repository compression; moreover, these methods require an underlying compression method of the kind being explored here.
RLZ. In this paper, we examine how to improve, subject to
a bound on the dictionary size, the compression efficiency of RLZ [9]. This off-line dictionary compression algorithm has been applied to genomes [15, 16, 17] as well as text. In RLZ, a fixed number, say k, of blocks (that is, substrings) of fixed length, say 1 KB, are sampled from the original byte sequences of the collection and concatenated, in their original order, to form the dictionary (Figure 2). This dictionary remains unchanged during compression and decompression.

284

The data is then factorized with respect to the dictionary using a greedy strategy inspired by the LZ77 factorization algorithm of Chen et al. [5], with the help of a suffix array.

l1

l2

l3

2

p1 l1

p4 l4 p2 l2

l4 p3 l3

RLZ Factorization Routine

p1 l1

p2 l2

p3 l3

p4 l4

Factor Encoding

2

Routine

... ...

Figure 2: Overview of RLZ. (a) The collection is sampled: blocks are concatenated (in collection order) to form a dictionary. (b) Each document is factorized relative to the dictionary. These factors are then encoded and concatenated (in document order) to constitute the compressed representation of the collection.
Each factor is represented as a pair (p, l): p is the position in dictionary D where the factor starts, and l is the length of the factor. To represent a character c that does not appear in the dictionary, (p, l) = (c, 0). These factors are then encoded with standard simple methods such as byte-oriented codes.
A particular attraction of RLZ is the speed of decompression. Hoobin et al. [9] report experiments where in the best case RLZ decompresses at around 18,000 GOV2 [6] documents per second (or around 120 per second, with random access), compared to around 2600 documents per second (or around 70­100 per second, with random access) with previous methods. They show that RLZ is consistently superior in decompression speed to previous methods, including retrieval of uncompressed text.
The work of Hoobin et al. [10] is the only previous examination of the dictionary pruning problem. In their approach, they removed blocks from the original dictionary based on the statistics derived from the initial compression and then reconstructed a smaller dictionary with less redundancy. This strategy serves as our major baseline and is described in detail in Section 3.2. We show that this approach to redundancy elimination does not provide satisfactory results when the pruned amount is large.
3. PRUNING THE DICTIONARY
In this section, we first present a formulation of the dictionary pruning problem. We describe and discuss straightforward redundancy elimination approaches, as baselines. We then explain our method, which like the other approaches is heuristic but is based on observations that arise from formal analysis of the problem.

Notation. We let [x..y] be a representation of the sequence {x, x + 1, . . . , y - 1}, and A[x..y] stand for the array, or substring, A[x], A[x + 1], . . . , A[y - 1].

3.1 Problem formulation
Let C represent the collection of files, indeed a concatenation of the files. Samples are generated from the text, that is, we choose blocks (substrings) from C, and concatenate these, in C order, to form a dictionary D of (initial) specified size. The aim is to prune the dictionary D, while maximizing the compression effectiveness.
In general, a factorization F of C with respect to a dictionary D (however obtained) is a sequence of M factors {(p1, l1), (p2, l2), . . . , (pM , lM )} that expand to C, as defined by requirement (1) below. Let Li be the length of the text collection represented by the first i factors. That is L0 = 0 and Li = Li-1 + max(li, 1), acknowledging that li = 0 is `code' for a single character. The requirement is that

LM = |C|

and

C[Li-1..Li] =

D[pi..pi + li] ci

li > 0 (1) li = 0

(In our implementations, we do not allow factors to span document boundaries. For simplicity, here, we consider the collection to be one contiguous string.)
The encoding of the factors varies in size due to the properties of variable-byte representations of the lengths {li}. For the purpose of the optimization, the variation is unlikely to be important because, for each of the great majority of factors, a single byte suffices to encode its length. We therefore make the simplifying, and established [25], assumption that all non-zero-length factors are encoded with the same number of bytes, f , and that each zero-length factor consumes one byte. Hence the compression effectiveness (which we call `cost') of F, with m non-zero-length factors, is f m + (M - m), leading to this characterization of the pruning problem:

Given a collection C, a dictionary D, and a required reduction in dictionary size , remove at least  bytes from D (to get D ) so that the cost of the factorization F of C with respect to D is minimized.

Unless otherwise specified, all pruning algorithms in this paper leave the remaining parts of the dictionary in the same order as prior to the pruning.
This problem is similar to the dictionary selection problem of Storer and Szymanski [25]. In their model, however, the dictionary can itself be factorized and the aim is to minimize the total compressed representation of both the dictionary and the collection. This is very similar to one of the compression effectiveness measures we describe in Section 4, the archived ratio. Although there are differences--our model has a bound on the uncompressed dictionary size, and is generated from samples--we expect that their NP-hardness results can be applied to our formulation.
Given this formulation, we could annotate each byte in the dictionary by the number of times it is referenced, as calculated in Algorithm 1.
Intuitively, the dictionary can be pruned by removing bytes, or strings of bytes, that have low numbers of references; and indeed this is the method we pursue below. However, we note that in general this may not be optimal.

285

Algorithm 1 Calculating byte reference frequencies.

Input: Factorization F, Dictionary size d

Output: Vector of byte reference frequencies r[0..d]

1: procedure Freq(F, d)

2: r[0..d]  0

3: for (p, l)  F do

4:

for j  [p..p + l] do

5:

r[j]  r[j] + 1

6: return r

For example, if we remove the unreferenced substring abc but also remove b from abcd elsewhere in the dictionary, the removal of abc can imply increased costs. That is, where abc could previously be encoded with a single reference, it might now require three. Given that we are beginning with an effective dictionary, our new statement of the problem implies that we need to reduce the dictionary without destroying useful factors or unnecessarily increasing the number of factors. Heuristics for identifying the material to delete from a dictionary should attempt to minimize fragmentation of factors, as well as to remove material that has a relatively low reference count.
Both our in-principle analysis of the problem and our experiments highlight the degree to which deletions from a dictionary have unpredictable effects on the factorization. Removal of a single character from a factor used early in a document can cause the entire remaining factorization to change. In our investigation, we discovered no simple measure that reliably quantified the impact of deletions from a dictionary.
3.2 Previous and preliminary methods

Block-level redundancy elimination. As discussed above,
Hoobin et al. [10] proposed an elimination method based on block removal, which we show as Algorithm 2 and refer to as REM . A frequency counter is maintained for each block in the dictionary. Each time a factor is generated, the frequency counter of the corresponding block in which the factor occurs is incremented. The blocks with the lowest counters are then removed, until the size bound on the dictionary is achieved. In this algorithm, Factorize(C, D) in line 2 corresponds to the RLZ algorithm [9, 15].

Algorithm 2 Pruning the dictionary by removing least frequently referred-to blocks [10].

Input: Text Collection C, Original Dictionary D, Block size

, Target dictionary reduction 

Output: Pruned dictionary D

1: R[0..|D|/]  0

Block reference counts

2: F  Factorize(C, D)

3: for each (p, l) in F do

4: for i  [p/..(p + l - 1)/ + 1] do blocks involved

5:

R[i]  R[i] + 1

6: Based on R[] counts, discard / least-frequently used blocks
7: return D  remaining blocks

Algorithm REM of Hoobin et al. [10] compresses better, and is faster, than LZMA when the dictionary size is halved,

while it outperforms ZLIB even with a ten-fold reduction in dictionary size. However, this strategy is far from optimal. Figure 1 shows that there are striking differences between the reference frequency of various parts of a sample block. As REM treats the block as the unit of elimination, it may remove a highly useful substring from the dictionary, should it happen to reside in an otherwise useless block. Elimination at a finer granularity is needed, prompting the following techniques.
Chunk-level redundancy elimination. Hash-based finger-
print technology has been widely used for the tasks of duplicate identification and redundancy elimination [12, 13, 21, 22, 23, 26]. Algorithm 3 identifies redundant chunks using the Rabin-Karp rolling hash function [12].
Algorithm 3 Pruning the dictionary by removing redundant fixed-length substrings.
Input: Original dictionary D, Chunk size c Output: Pruned dictionary D 1: Using a rolling hash, calculate hash of each c-gram in D 2: Identify the identical length-c substrings 3: Discard the redundant substrings 4: return D  remainder of dictionary
If c is large, then Algorithm 3 may not reduce the dictionary much, as only repeated strings of length c or greater are pruned. There are more chunk candidates to remove for small c, but then long, useful strings tend to be broken up in unpredictable ways. In preliminary experiments, we found that this chunk-level reduction does lead to slightly better compression than REM. It is, however, considerably less effective than our subsequent techniques; space constraints compel us to omit the details.
Byte-level filtering. Instead of identifying redundant parts
of the dictionary at a chunk level, individual redundant bytes could be removed. Based on the factorization of the collection, the least-frequently used bytes are removed from the dictionary, until the dictionary is sufficiently small. The remaining bytes are kept in original order. Preliminary results show that such filtering does outperform chunk-level elimination (Algorithm 3). However, performance degrades drastically when too much material is removed, because such filtering makes the pruned dictionary too fragmented, which dramatically increases the number of factors. Again, we omit the details of these preliminary experiments.
3.3 Contribution-aware pruning method
A major challenge in dictionary pruning is how to choose the segments to remove. In other words, it is essential to estimate as precisely as possible the consequences of removing a particular substring. In principle, one method is to compress the text collection against a version of the dictionary from which a particular candidate segment has been excluded. Then, based on the resulting compression ratio, we can tell which segments have the least effect on the compression. However, in practice, re-evaluation of the compression for each candidate segment is out of the question.
Instead, we propose a measure to estimate a segment's `contribution' to the compression if it is kept in the pruned dictionary. To calculate this measure, only the dictionary it-

286

self is required, not the collection. The segment is factorized against the `pruned' dictionary, that is, against a notional version of the dictionary in which the candidate segment is absent (Figure 3). With the constraint that the factors of a candidate segment should not overlap the segment itself, the standard Factorize routine of RLZ suffices.
Figure 3: Estimating the value of a candidate segment by factorizing it against the rest of the dictionary.
In Figure 3, the candidate segment s (in a dotted bubble) can be described by three factors that appear in the rest of the dictionary. Thus each reference to this segment of the dictionary will produce three factors when being compressed using the pruned dictionary.
New measure. Here, we develop an estimate of the effect
of removing a segment s from dictionary D. Suppose s is exactly the target of some string t in the factorization of C against D. If the dictionary D - s were used instead of D, then t would be factorized the same way that s is factorized against D - s. To estimate this effect, we calculate nfac(s, D), which is the number of factors that s generates when factorized against D-s. For the second candidate segment in Figure 3, this value is three. If a notional t's target were s, then it would now require nfac(s, D) factors.
More generally, (part of) s may be (part of) the target of some string t. Now, when the collection is factorized against D - s, that string t might instead be factorized differently. However, it is possible that the part of t whose target is part of s is factorized in the same way as the common subsegment of s is against D - s. On average, if t targets only the subsegment s in s, then it would incur nfac(s, D) × |s |/|s| new factors when factorized against D - s. Counting this from the point of view of s, we consider the average number of times each byte of s is a target, denoted by Fre(s, r). This is is r[i]/|s|, where r = Freq(F , |D|). When multiplied by nfac(s, D), this is a rough estimate of the number of new factors appearing in the factorization of C when s is removed from D. We refer to this measure as FF (frequency & factor), and propose the removal of segments that have the lowest FF values.
Were it included in a dictionary, segment s would consume |s| space. Therefore we also introduce the per-byte measure FFL(s, D), which is FF(s, D)/|s|.
Though these two measures are only an approximation, this `factorizing and counting' strategy provides us with an estimate of the effect of removing a segment. Importantly, it is relatively cheap to calculate.
CARE algorithm. Our contribution-aware reduction algo-
rithm (CARE) may be applied as a one-off procedure, or iteratively. We start by describing the core of the process, in

Algorithm 4. It removes from the dictionary those segments that have low FFL (or FF) values. Importantly, to control the number of candidate segments, we consider only segments of length at least , containing no byte with reference frequency greater than . These candidates, S, are, in practice, found via a greedy heuristic, Candidates(D, r, , ), based on the reference-frequency vector r = Freq(F, |D|). Given a starting point in the string, a segment of bytes with frequency at most  and of maximal length is found. Should this segment's length be at least , it is added to S, otherwise it is ignored. The search for candidates resumes with the next byte whose frequency is at most . By design, the candidate segments do not overlap.

Algorithm 4 Pruning the dictionary using CARE.

Input: Text collection C, Original dictionary D, Byte-

frequency threshold , Length threshold , Desired dic-

tionary size reduction 

Output: Pruned dictionary D

1: procedure O-Pruning(, , )

One-off pruning

2: F  Factorize(C, D)

3: r  Freq(F, |D|)

4: S  Candidates(D, r, , )

5: if |s|   then

Out of candidates

sS

6:

return D

7: for each s in S do

8:

Calculate Fre(s, r) and

9:

Execute Factorize(s, D) and calculate nfac(s, D)

10:

FFL(s, D)  Fre(s, r) × nfac(s, D)/|s|

11: Based on the FFL-values, discard segments in S from D until length discarded is at least .
12: return D  remainder of dictionary

Algorithm 4 is called O-Pruning as it is a one-off process. However, it can be applied iteratively: at each step the dictionary size is reduced by a specified amount. Our results show that iterating this procedure with a `small' amount removed from the dictionary each time results in different outcomes to those of pruning the dictionary in a single step.
4. PERFORMANCE EVALUATION
In most of our experiments we use subsets of GOV2 [6]. The collections small, medium, large, and full correspond to the first 1 GB, 10 GB, 100 GB, and all (426 GB) of the documents, respectively. We first study in detail the effectiveness of our method by carrying out experiments with various settings on both the small and medium datasets, then we repeat the experiments on the large and full datasets to demonstrate the scalability of our method. For each experiment, an original dictionary is generated by the sampling technology described in [9, 10]; we then prune each dictionary to a variety of fixed sizes.
The baselines we use are the plain sampling strategy [9], which we call ORI , and the previous redundancy elimination (or pruning) method REM [10]. Unless indicated otherwise, 1 KB is the default for both the sample block size used during original RLZ dictionaries generation and reduced unit size in REM. The coding schemes used to compress the position and length of factors are Z (ZLIB, zlib.net) and V (VBYTE [27]), respectively. This combination achieves the fastest compression time and is only marginally worse than

287

the best, but much slower, combination (ZZ) reported in previous work [9] in terms of compression ratio.
Since the superiority of RLZ over the compression libraries ZLIB and LZMA has already been established [9], we do not examine these latter two further. CARE is only concerned with the construction of the dictionary, so it does not affect the decompression process or the compressed data layout, and thus does not affect the retrieval time of RLZ. Therefore our evaluations do not include retrieval speed.
As shown by Hoobin et al. [9], we can achieve a compression ratio of less than 10% for a large collection (GOV2, 426 GB), where compression ratio is the final compressed size as a percentage of the original size. This is achieved with a dictionary ratio of only 0.5% or less (the ratio of dictionary size to the collection size). However, to maintain the same compression ratio as an unpruned dictionary, for a small collection (say, 4 GB), we have to increase the dictionary ratio to 20%­30%. Since the dictionary ratio and the compression ratio are both relative to the uncompressed data size, we can introduce a new performance measure (AR, for archived ratio) that covers them both. The AR is the compressed size of the dictionary and collection together, as a percentage of the uncompressed collection size. We use a standard tool (7zip) to compress the dictionary, to represent the size it would occupy during archiving. In other words, AR is the size required for storage or transmission of a repository that has been compressed with RLZ.
5. EXPERIMENTAL RESULTS
Our experiments involve parameters that trade against each other in complex ways. For example, as can be observed in these experiments, the dictionary size is not a function of the collection size, for a given compression ratio. As another example, a given dictionary size can be achieved by sampling; or can be achieved by pruning from a larger dictionary. The pruning can be achieved directly, or iteratively. We thus need to report IDS , the initial dictionary size; the PDS , or pruned dictionary size; the ICR, or initial compression ratio; and the step, which is the amount the dictionary size is reduced in each pruning iteration. Ultimately, we wish to discover the best compression ratio available for a given dictionary size which, as we show, is given by our new method CARE.
For a fair comparison, we first generate equal-sized dictionaries with each method. That is, given an IDS and a sequences of pruned sizes, we compare directly sampling (ORI) to achieve the pruned size to sampling to the original size then applying REM and CARE to achieve the pruned size. In this first experiment, the CARE algorithm uses FFL as the measure for the removal of candidate segments from the dictionary, and the pruning strategy is one-off (noniterative). Only the segments with maximum byte frequency at most  and length at least  may be removed. Sensitivity to these parameters is discussed later.
Figure 4(a) shows that, with the small dataset, CARE is consistently better than ORI in terms of compression ratio for same-sized dictionaries. The ORI line represents the result of using a range of initial dictionary sizes; the REM and CARE lines are the result of using a specified initial dictionary size (250 MB) and then pruning. As the pruning continues, CARE outperforms REM when the dictionaries are reduced by a a third or more. Meanwhile REM gets dramatically worse, and after a two-fold reduction in dictio-

Compression Ratio (%)

22

ORI

REM

20 CARE

18

16

14

12

10

250

200

150

100

50

Dictionary Size (MB)

(a) dataset: small (1 GB),  = 10,  = 20

19

18

ORI REM

17 CARE

16

15

14

13

12

11

10 2000 1750 1500 1250 1000 750 500 Dictionary Size (MB)

(b) dataset: medium (10 GB),  = 10,  = 20

Compression Ratio (%)

Figure 4: Compression ratio achieved by different construction strategies. The initial dictionary size is 250 MB and 2000 MB for the small and medium datasets, respectively. Pruning in CARE is applied one-off; FFL is used as the measure.

nary size becomes poorer than the commensurate directlysampled dictionary. The same patterns are also observed in Figure 4(b) for the medium dataset.
We also investigate the impact of pruning on AR for each method. As depicted in Figures 5(a) and 5(b), in contrast to the compression ratio, AR does not change monotonically as the dictionary is pruned. At first AR slightly decreases, because the saving in dictionary size is greater than the loss in compression ratio. For example, when the dictionary for medium dataset is pruned from 2000 MB to 1500 MB (310 MB to 268 MB, in terms of compressed size), we save 0.4% in compressed dictionary size while losing around 0.1% in compression ratio. However, with the dictionary size further reduced, AR increases instead. The 7zip utility compresses the dictionary so well (around 15%-16% of original) that the gap between the sizes of the compressed dictionaries is dramatically narrowed. Therefore, the saving in dictionary size is eventually overwhelmed by the increase in compression ratio.
Though pruning the dictionary will eventually lead to a poorer archived ratio and compression ratio than was avail-

288

Archived Ratio (%)

24

ORI

22

REM CARE

20

18

16

14

250

200

150

100

50

Dictionary Size (MB)

(a) dataset: small (1 GB),  = 10,  = 20

21

20

ORI REM

19 CARE

18

17

16

15

14

13

12 2000 1750 1500 1000 750 500 250 Dictionary Size (MB)

(b) dataset: medium (10 GB),  = 10,  = 20

Archived Ratio (%)

Figure 5: Archived ratio achieved by different construction strategies. The initial dictionary size is 250 MB and 2000 MB for the small and medium datasets, respectively. Pruning in CARE is applied one-off; FFL is used as the measure.

able with the original dictionary size, for a given (uncompressed) dictionary size, it can give a much better AR. For example, on medium, compare ORI at 1000 MB (AR = 12.4%) to CARE at 1000 MB (AR = 10.9%), having pruned a 2000 MB dictionary to 1000 MB. That is, it is better to build a large dictionary and prune than to directly create a small dictionary.
We next compare the two CARE measures proposed in Section 3.3, with regard to evaluation of candidate segments. Table 1 shows that FFL is superior to FF in every setting. Note that the data in the ORI, REM, and FFL (CARE) columns is identical to that illustrated in Figure 4. The results here also reveal that each setting of CARE is consistently better than REM when there is significant pruning. Table 1 shows that the loss of compression ratio caused by CARE pruning (compare to the IDS) is less than 1% even when the dictionary size is reduced by half.
Varying removal candidates. So far, we have examined
only one-off pruning. We investigate the choice of the two

Table 1: One-off pruning using CARE with different measures (FF & FFL). The values of  and  are the same as those in Figure 4.

Data Set and
IDS /ICR
small (1GB) (IDS=250MB) (ICR=10.68%)
medium (10GB)
(IDS=2000MB) (ICR=10.03%)

PDS (MB)

Compression Ratio (%)

ORI REM

CARE FF FFL

200 150 100
50
1750 1500 1250 1000
750 500

11.85 13.09 14.65 17.09
10.52 11.07 11.68 12.39 13.23 14.35

10.78 12.95 16.81 22.22
10.03 10.12 10.85 12.65 15.47 18.77

11.26 12.53 14.12 16.43
10.09 10.46 11.05 11.79 12.67 13.78

10.83 11.39 12.74 15.28
10.09 10.17 10.39 10.86 11.75 13.15

Table 2: Iterative pruning using CARE with different combinations of (, ) for the FF & FFL measures on small dataset (1 GB), IDS=250 MB, ICR=10.68%.

PDS REM

Compression Ratio (%)

(20, 20)

(10, 20)

(10, 50)

(MB)

FF FFL FF FFL FF FFL

200 10.78 11.57 11.10 11.13 10.83 11.26 10.83 150 12.95 12.71 11.79 12.13 11.32 12.40 11.33 100 16.81 14.23 12.98 13.41 12.48 13.85 12.51
50 22.22 16.49 15.05 15.25 14.88 15.82 14.92

arguments--the upper limit of frequency  and the lower limit of length --in the context of iterative pruning. Table 2 presents the results of three different combinations of these two arguments on the small dataset. The differences in compression ratio among the combinations are small. Optimizing the parameter selection is a research question we leave for the future, but these results suggest that, no matter which combinations we use, CARE is consistently better than REM. The results on the medium dataset are much the same. In the following evaluations, the combinations (, ) = (10, 20) and (10, 50), are set as the default values in experiments for the small and medium dataset, respectively. These were chosen based on initial experiments.
As described in Section 3.3, there are two ways to progress the pruning. One is to prune the dictionary to a fixed volume in an one-off manner, while the other is to iteratively reduce the dictionary multiple times by a fixed step. Figures 6(a) and 6(b) show that the iterative strategy is consistently better than the one-off method. And as the reduction continues, the advantage of the iterative strategy increases. The results also demonstrate that FFL remains superior to FF as a measurement of the value of segments. The iterative FF CARE algorithm (I-FF) in Figure 6(b) runs out of candidates after being pruned to 750 MB, so there is no corresponding result for a dictionary of 500 MB.
Varying dictionary sizes. Next we studied the impact of
the step size  on the effectiveness of different strategies. The results in Table 3 show that we can achieve better compression by choosing smaller step sizes, though dictionary construction is slower. However, the improvement achieved

289

Compression Ratio (%)

17

16

O-FF I-FF

O-FFL

15

I-FFL

14

13

12

11

10

250

200

150

100

50

Dictionary Size (MB)

(a) dataset: small (1 GB),  = 10,  = 20

14.0

13.5

O-FF I-FF

13.0

O-FFL I-FFL

12.5

12.0

11.5

11.0

10.5

10.0 2000 1750 1500 1250 1000 750 500
Dictionary Size (MB)

(b) dataset: medium (10 GB),  = 10,  = 20

Compression Ratio (%)

Figure 6: Compression ratio of fixed-size dictionaries generated by CARE with one-off (O) and iterative (I) strategies. FFL is used as the measure.

by fine granularity is very small. Thus, we continue to use the previous settings in our remaining experiments. For the purpose of this table, we ignored settings where the gap between the IDS and PDS was not a multiple of the step size.
Table 4 shows the results of CARE dictionary reduction with different initial dictionary sizes (IDS). The results suggest that, by starting from a larger IDS, we end up with a better dictionary for each specified final size. The reason is that the larger dictionary represents the collection better, while the multiple rounds of pruning reduce the dictionary to the most valuable substrings. The results on the small dataset (not shown) support this observation.
We also present results for ORI and REM on the medium dataset in Table 5. By comparing Tables 4 and 5, we observe that, for REM, a smaller IDS leads to a better pruned dictionary. When REM starts with a larger dictionary, more blocks must be removed, which makes the pruned dictionary less representative of the collection. More significantly, even the worst case in CARE is better than the best case in REM (except when the pruned volume is small). The results on the small dataset (not shown) are similar.

Table 3: Iterative pruning using CARE with various progressive pruning differences.

Data Set
small (1GB) (IDS=250)
medium (10GB) (IDS=2000)

PDS (MB)
200 150 100
50
PDS (MB) 1500 1000
500

Compression Ratio (%)

Difference per step ()

50 MB 20 MB 10 MB

10.83

­

10.81

11.32 11.27 11.25

12.48

­

12.36

14.88 14.77 14.69

Difference per step ()

500 MB 250 MB 100 MB

10.17 10.16 10.15

10.80 10.76 10.74

12.99 12.96 12.86

Table 4: Iterative pruning using CARE with different IDSs on medium dataset (10 GB), step  = 250 MB.

PDS
(MB) 1250 1000
500

Compression Ratio (%)

IDS=2000MB IDS=1500MB IDS=1000MB

ICR=10.03% ICR=11.07% ICR=12.39%

10.36

11.02

­

10.76

11.26

­

12.98

12.99

13.05

Table 5: Performance of dictionary construction methods (ORI and REM) on medium dataset (10 GB).

Constructed Dictionary Size (MB)
1250 1000 500

ORI
11.68 12.39 14.35

Compression Ratio (%)

REM with IDS (MB)

IDS=2000 IDS=1500 IDS=1000

10.85

11.07

­

12.65

11.65

­

18.77

17.82

15.72

The impact of the block size on the effectiveness of RLZ is not studied in previous work [9, 10]. Table 6 shows that the sample size has little effect on CARE, and only very limited impact on the other methods.
Larger collections. After our extensive studies of CARE
on both the small and medium datasets, we repeat the experiments on the large dataset as well as the full GOV2 collection. Table 7 demonstrates that CARE significantly outperforms REM as expected. Halving the dictionary size using CARE causes less than 0.3% loss in compression ratio, while the compression ratio is 1% better than that of the commensurate size of the dictionary constructed by ORI. When reducing the dictionary to only a quarter of its original size, compression loss is only around 1.4%, while REM suffers around 6.5%­7%.
In Table 7 we also report the compressibility of different pruned dictionaries (7zip is used here). The results show that the dictionaries are less compressible after pruning, which means they now contain less redundancy. For commensurable size of pruned dictionaries, the number of factors generated by REM is far larger than that by CARE, explaining why CARE outperforms REM in compression ratio. For example, with a 250 MB dictionary (IDS=1000 MB), REM produces 5.64 billion factors, CARE 3.55 billion factors.

290

Table 6: Performance of dictionary pruning methods using various sample block sizes on the small and medium datasets. CARE uses FFL with  = 10,  = 20, iteratively.

PDS (MB)
200 150 100
50

Compression Ratio(%)

small (IDS = 250 MB, step = 50 MB) medium (IDS = 2000 MB, step = 250 MB)

sample = 1 KB

sample = 8 KB

sample = 1 KB

sample = 8 KB

ICR = 10.68%

ICR = 10.81%

ICR = 10.13%

ICR = 9.96%

REM CARE REM CARE REM CARE REM CARE

10.78

10.83

11.46

10.90

10.85

10.36

11.25

10.25

12.95

11.32

13.01

11.33

12.65

10.76

12.38

10.68

16.81

12.48

15.94

12.48

15.47

11.56

14.13

11.50

22.22

14.88

21.64

14.86

18.77

12.98

17.01

12.94

PDS (MB)
1250 1000
750 500

Table 7: Performance of dictionary construction methods on large dataset (100 GB). CARE uses FFL,  = 100,  = 20, iteratively.

PDS

IDS = 2000 MB ICR = 11.71%

IDS = 1000 MB ICR = 12.96%

(MB) ORI REM CARE ORI REM CARE

Compression Ratio (%)

1500 12.23 11.76 11.73 ­

­

­

1000 12.98 13.43 11.98 ­

­

­

750 ­

­

­

13.45 13.06 12.97

500 14.26 18.20 13.12 14.26 15.12 13.25

250 ­

­

­

15.60 20.11 14.40§

Compressed Dictionary Size (MB)

1500 215 256 256

­

­

­

1000 146 206 210

­

­

­

750 ­

­

­

111 132 133

500 75 127 115

75 106 109

250 ­

­

­

38 64

59

§ In order to obtain complete results, we changed  from 100 to 200 here, as the process run out of candidates at this point.

Table 8 shows that all the conclusions drawn above also hold on the whole GOV2 corpus (426 GB). For example, a CARE-based dictionary of 500 MB can give compression as good as that originally available with 1000 MB. Table 9 shows results for the Wikipedia dataset, which is very different from GOV2. The Wikipedia data is highly structured, with many common elements repeated from page to page, whereas GOV2 contains highly diverse material from every branch of the US government. However, the compression results are very similar, as is the relative behavior of the different algorithms.
In both tables, reducing the dictionary by a quarter with REM or CARE has almost no impact on compression ratio; the tiny changes (improvements in a couple of cases!) are due to the effect of different but nearly equivalent factors being chosen. For greater reductions, however, the CARE method again exhibits much better performance, with very slow degradation in compression ratio compared to the alternatives. These results show that our CARE method has proved much the most effective way of reducing dictionary size, and also show the benefit of starting with a large dictionary which is then progressively reduced.
6. CONCLUSIONS AND FUTURE WORK
Relative Lempel-Ziv factorization is an efficient compression algorithm that provides both good compression ratio

Table 8: Performance of dictionary construction methods on GOV2 (426 GB). CARE uses FFL,  = 200,  = 20, iteratively.

PDS

IDS = 2000 MB ICR = 9.419%

IDS = 1000 MB ICR = 10.271%

(MB) ORI REM CARE ORI REM CARE

Compression Ratio (%)

1500 9.789 9.414 9.425

­

­

­

1000 10.271 9.588 9.437

­

­

­

750

­

­

­ 10.645 10.262 10.272

500 11.083 12.371 10.036 11.083 10.547 10.276

250

­

­

­ 11.987 13.452 10.907§

 These numbers are slightly different from those reported in [9]. This is caused by the slight difference between the size of our dictionaries (e.g., 2000 MB versus 2 GB).
§  adjusted from 200 to 800 here. (In fact, we found that by simply setting  as the ratio of collection size over dictionary size will always guarantee the sufficiency of the candidates.)

Table 9: Performance of different dictionary construction methods on Wikipedia dataset (251 GB). CARE uses FFL  = 200,  = 20, iteratively.

PDS

IDS = 2000 MB ICR = 8.688%

IDS = 1000 MB ICR = 9.900%

(MB) ORI REM CARE ORI REM CARE

Compression Ratio (%)

1500 9.202 8.708 8.738

­

­

­

1000 9.900 9.342 8.898

­

­

­

750

­

­

­ 10.383 9.909 9.901

500 11.096 11.369 9.787 11.096 10.517 10.052

250

­

­

­ 12.226 12.561 11.066

and fast retrieval. Though it only requires a relatively small dictionary, compared with the size of the collection to be compressed, the dictionary size is still an essential concern as it must be maintained in memory.
We first formulate the dictionary pruning problem as an optimization problem and then propose heuristic strategies for pruning the dictionary while maintaining compression effectiveness. Our main heuristic can be calculated efficiently by factoring segments of the dictionary against the dictionary itself. By identifying and eliminating low-value segments, we can markedly reduce the volume of the dictionary without significant loss of compression performance.
RLZ may be deployed on mobile devices, where a fixed dictionary can be used to reduce download requirements. In

291

such a context, dictionary size must be kept small, and the value of these kinds of savings is accentuated.
In our view, we should next refine our understanding of the dictionary optimization problem. The consequent pruning algorithms could start with much larger dictionaries, which are then progressively reduced, and we hypothesize that compression will be even more effective. However, the existing results are already significantly superior to any current alternative, and provide a practical method for largescale corpus compression.
7. ACKNOWLEDGMENTS
We thank Christopher Hoobin for providing the source code of RLZ. This work is partially supported by The Australian Research Council, NSF of China (61373018, 11301288), Program for New Century Excellent Talents in University (NCET-13-0301) and Fundamental Research Funds for the Central Universities(65141021). Jiancong would also like to thank the China Scholarship Council (CSC) for the State Scholarship Fund.
8. REFERENCES
[1] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern Information Retrieval - The Concepts and Technology Behind Search, Second edition. Addison-Wesley, 2011.
[2] S. Bu¨ttcher, C. L. A. Clarke, and G. V. Cormack. Information Retrieval - Implementing and Evaluating Search Engines. MIT Press, 2010.
[3] A. Cannane and H. E. Williams. General-purpose compression for efficient retrieval. JASIST, 52(5):430­437, 2001.
[4] A. Cannane and H. E. Williams. A general-purpose compression scheme for large collections. ACM Transactions on Information Systems, 20(3):329­355, 2002.
[5] G. Chen, S. J. Puglisi, and W. F. Smyth. Lempel-Ziv factorization using less time & space. Mathematics in Computer Science, 1(4):605­623, 2008.
[6] C. Clarke, N. Craswell, and I. Soboroff. Overview of the trec 2004 terabyte track. In TREC, 2004.
[7] W. B. Croft, D. Metzler, and T. Strohman. Search Engines - Information Retrieval in Practice. Addison-Wesley, 2009.
[8] P. Ferragina and G. Manzini. On compressing the textual web. In WSDM, pages 391­400, 2010.
[9] C. Hoobin, S. J. Puglisi, and J. Zobel. Relative Lempel-Ziv factorization for efficient storage and retrieval of web collections. PVLDB, 5(3):265­273, 2011.
[10] C. Hoobin, S. J. Puglisi, and J. Zobel. Sample selection for dictionary-based corpus compression. In SIGIR, pages 1137­1138, 2011.
[11] J. J. Hunt, K.-P. Vo, and W. F. Tichy. Delta algorithms: An empirical analysis. ACM Transactions on Software Engineering and Methodology, 7(2):192­214, 1998.
[12] R. M. Karp and M. O. Rabin. Efficient randomized pattern-matching algorithms. IBM Journal of Research and Development, 31(2):249­260, 1987.
[13] P. Kulkarni, F. Douglis, J. D. LaVoie, and J. M. Tracey. Redundancy elimination within large collections of files. In USENIX Annual Technical Conference, General Track, pages 59­72, 2004.

[14] S. Kuruppu, B. Beresford-Smith, T. C. Conway, and J. Zobel. Iterative dictionary construction for compression of large dna data sets. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 9(1):137­149, 2012.
[15] S. Kuruppu, S. J. Puglisi, and J. Zobel. Relative Lempel-Ziv compression of genomes for large-scale storage and retrieval. In SPIRE, pages 201­206, 2010.
[16] S. Kuruppu, S. J. Puglisi, and J. Zobel. Optimized relative Lempel-Ziv compression of genomes. In ACSC, pages 91­98, 2011.
[17] S. Kuruppu, S. J. Puglisi, and J. Zobel. Reference sequence construction for relative compression of genomes. In SPIRE, pages 420­425, 2011.
[18] N. J. Larsson and A. Moffat. Offline dictionary-based compression. In DCC, pages 296­305, 1999.
[19] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, 2008.
[20] G. Navarro and V. M¨akinen. Compressed full-text indexes. ACM Computing Surveys, 39(1), 2007.
[21] Z. Ouyang, N. D. Memon, T. Suel, and D. Trendafilov. Cluster-based delta compression of a collection of files. In WISE, pages 257­268, 2002.
[22] A. Peel, A. Wirth, and J. Zobel. Collection-based compression using discovered long matching strings. In CIKM, pages 2361­2364, 2011.
[23] P. Shilane, M. Huang, G. Wallace, and W. Hsu. WAN-optimized replication of backup datasets using stream-informed delta compression. ACM Transations on Storage, 8(4):1­26, 2012.
[24] P. Skibinski, S. Grabowski, and S. Deorowicz. Revisiting dictionary-based compression. Software: Practice and Experience, 35(15):1455­1476, 2005.
[25] J. A. Storer and T. G. Szymanski. Data compression via textual substitution. Journal of the ACM, 29(4):928­951, 1982.
[26] T. Suel, P. Noel, and D. Trendafilov. Improved file synchronization techniques for maintaining large replicated collections over slow networks. In ICDE, pages 153­164, 2004.
[27] H. E. Williams and J. Zobel. Compressing integers for fast file access. The Computer Journal, 42(3):193­201, 1999.
[28] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images, Second Edition. Morgan Kaufmann, 1999.
[29] J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Transactions on Information Theory, 23(3):337­343, 1977.
[30] J. Ziv and N. Merhav. A measure of relative entropy between individual sequences with application to universal classification. IEEE Transactions on Information Theory, 39(4):1270­1279, 1993.
[31] N. Ziviani, E. Silva de Moura, G. Navarro, and R. A. Baeza-Yates. Compression: A key for next-generation text retrieval systems. IEEE Computer, 33(11):37­44, 2000.
[32] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Computing Surveys, 38(2), 2006.

292

Learning for Search Result Diversification
Yadong Zhu Yanyan Lan Jiafeng Guo Xueqi Cheng Shuzi Niu
Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China
{zhuyadong, niushuzi}@software.ict.ac.cn {lanyanyan, guojiafeng, cxq}@ict.ac.cn

ABSTRACT
Search result diversification has gained attention as a way to tackle the ambiguous or multi-faceted information needs of users. Most existing methods on this problem utilize a heuristic predefined ranking function, where limited features can be incorporated and extensive tuning is required for different settings. In this paper, we address search result diversification as a learning problem, and introduce a novel relational learning-to-rank approach to formulate the task. However, the definitions of ranking function and loss function for the diversification problem are challenging. In our work, we firstly show that diverse ranking is in general a sequential selection process from both empirical and theoretical aspects. On this basis, we define ranking function as the combination of relevance score and diversity score between the current document and those previously selected, and loss function as the likelihood loss of ground truth based on Plackett-Luce model, which can naturally model the sequential generation of a diverse ranking list. Stochastic gradient descent is then employed to conduct the unconstrained optimization, and the prediction of a diverse ranking list is provided by a sequential selection process based on the learned ranking function. The experimental results on the public TREC datasets demonstrate the effectiveness and robustness of our approach.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Search and Retrieval ­ Retrieval Models
General Terms
Algorithms, Experimentation, Performance, Theory
Keywords
Diversity, Relational Learning-to-Rank, Sequential Selection, Plackett-Luce Model
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609634 .

1. INTRODUCTION
Most users leverage Web search engine as a predominant tool to fulfill their information needs. Users' information needs, typically described by keyword based queries, are often ambiguous or multi-faceted. On the one hand, for some ambiguous queries, there are multiple interpretations of the underlying needs (e.g., query "band" may refer to the rock band, frequency band or rubber band). On the other hand, queries even with clear definition might still be multi-faceted (e.g., "britney spears"), in the sense that there are many aspects of the information needs (e.g., news, videos, photos of britney spears). Therefore, search result diversification has attracted considerable attention as a means to tackle the above problem [1]. The key idea is to provide a diversified result list, in the hope that different users will find some results that can cover their information needs.
Different methods on search result diversification have been proposed in literature, which are mainly non-learning methods, and can be divided into two categories: implicit methods and explicit methods. Implicit methods [3] assume that similar documents cover similar aspects, and rely on inter-document similarity for selecting diverse documents. While explicit methods [29] directly model the aspects of user queries and select documents that cover different aspects for diversification. However, most existing methods utilize a heuristic predefined utility function, and thus limited features can be incorporated and extensive tuning is required for different retrieval settings.
In this paper, we address search result diversification as a learning problem where a ranking function is learned for diverse ranking. Different from traditional relevance ranking based on the assumption of independent document relevance [17], diverse ranking typically considers the relevance of a document in light of the other retrieved documents [29]. Therefore, we introduce a novel Relational Learningto-Rank framework (R-LTR for short) to formulate the task of search result diversification. R-LTR considers the interrelationships between documents in the ranking process, besides the content information of individual documents used in traditional learning-to-rank framework. However, the definitions of ranking function and loss function for the diversification problem are challenging.
From the top-down user browsing behavior and the ubiquitous greedy approximation for diverse ranking, we find that search result diversification is in general a sequential ranking process. Therefore, we propose to define the ranking function and loss function in a sequential way: (1) The ranking function is defined as the combination of relevance

293

score and diversity score, where the relevance score only depends on the content of the document, and the diversity score depends on the relationship between the current document and those previously selected. We describe different ways to represent the diversity score. (2) The loss function is defined as the likelihood loss of ground truth based on Plackett-Luce model [18], which can naturally model the sequential generation of a diverse ranking list. On this basis, stochastic gradient descent is employed to conduct the unconstrained optimization, and the prediction of diverse ranking list is provided by a sequential selection process based on the learned ranking function.
To evaluate the effectiveness of the proposed approach, we conduct extensive experiments on the public TREC datasets. The experimental results show that our methods can significantly outperform the state-of-the-art diversification approaches, with Official Diversity Metrics (ODM for short) of TREC diversity task including ERR-IA[1, 6], -N DCG[11] and N RBP [12]. Furthermore, our methods also achieve best in the evaluations of traditional intent-aware measures such as Precision-IA [1] and Subtopic Recall [37]. In addition, we give some discussions on the robustness of our methods and the importance of the proposed diversity features. Finally, we also study the efficiency of our approach based on the analysis of running time.
The main contributions of this paper lie in:
1. the proposal of a novel R-LTR framework to formulate search result diversification as a learning problem, where both content information and relationship among documents are considered;
2. the new definitions of ranking function and loss function based on the foundation of sequential selection process for diverse ranking;
3. an empirical verification of the effectiveness of the proposed approach based on public datasets.
The rest of the paper is organized as follows. We first review some related work in Section 2. We then introduce the R-LTR framework in Section 3, and describe the specific definitions of ranking function and loss function, learning and prediction procedures in Section 4. Section 5 presents the experimental results. Section 6 concludes the paper.
2. RELATED WORK
Most existing diversification methods are non-learning methods, which can be mainly divided into two categories: implicit approaches and explicit approaches.
The implicit methods assume that similar documents cover similar aspects and model inter-document dependencies. For example, Maximal Marginal Relevance (MMR) method [3] proposes to iteratively select a candidate document with the highest similarity to the user query and the lowest similarity to the already selected documents, in order to promote novelty. In fact, most of the existing approaches are somehow inspired by the MMR method. Zhai et al. [37] select documents with high divergence from one language model to another based on the risk minimization consideration.
The explicit methods explicitly model aspects of a query and then select documents that cover different aspects. The aspects of a user query can be achieved with a taxonomy [1, 32], top retrieved documents [5], query reformulations [24,

29], or multiple external resources [15]. Overall, the explicit methods have shown better experimental performances comparing with implicit methods.
There are also some other methods which attempt to borrow theories from economical or political domains. The work in [26, 33] applies economical portfolio theory for search result ranking, which views search diversification as a means of risk minimization. The approach in [13] treats the problem of finding a diverse search result as finding a proportional representation for the document ranking, which is like a critical part of most electoral processes.
The authors of [2, 27] try to construct a dynamic rankedretrieval model, while our paper focuses on the common static ranking scenario. There are also some on-line learning methods that try to learn retrieval models by exploiting users' online feedback [25, 31, 35, 30, 28]. These research work can tackle diversity problem to some extent, but they focus on an `on-line' or `coactive' scenario, which is different from our work (i.e. offline supervised learning scenario).
Recently, some researchers have proposed to utilize machine learning techniques to solve the diversification problem. Yue et al. [36] propose to optimize subtopic coverage as the loss function, and formulate a discriminant function based on maximizing word coverage. However, their work only focuses on diversity, and discards the requirements of relevance. They claim that modeling both relevance and diversity simultaneously is a more challenging problem, which is exactly what we try to tackle in this paper. In this paper, we propose a novel R-LTR approach to conduct search result diversification, which is different from traditional approaches and shows promising experimental performance.
3. RELATIONAL LEARNING-TO-RANK
Traditional relevance ranking has been well formulated as a learning-to-rank (LTR for short) problem [17], where a ranking function is defined on the content of each individual document and learned toward some loss functions. However, in diverse ranking scenario, the overall relevance of a document ranking for a given query, should depend not only on the individual ranked documents, but also on how they related to each other [29]. Therefore, in this paper, we introduce a novel R-LTR framework to formulate the diverse ranking problem. The difference between LTR and R-LTR is that the latter considers both contents of individual document and relations between documents. In the following paper, we use superscript to denote the id of a query and subscript to denote the id of a document.
Formally, let X = {x1, · · · , xn}, where xi denotes the d dimensional feature vector of a candidate document xi for query q; Let R  Rn×n×l denote a 3-way tensor representing relationships between the n documents, where Rijk stands for the k-th feature of relation between documents xi and xj. Let y be a ground-truth of the query q, in the form of a vector of ranking scores or a ranking list. Supposing that f(X, R) is a ranking function, and the goal of R-LTR is to output the best ranking function from a function space F .
In training procedure, given the labeled data with N queries as: (X(1), R(1), y(1)), (X(2), R(2), y(2)), · · · , (X(N), R(N), y(N)). A loss function L is defined, and the learning process is conducted by minimizing the total loss with respect to the given

294

training data.

 N

^f = arg min L(f(X(i), R(i)), y(i)).

(1)

fF

i=1

In prediction, given X(t) and R(t) of nt documents for query qt, we output y^(t) based on the learned ranking function ^f(X(t), R(t)).
In fact, the proposed R-LTR framework is very general, in the sense that many traditional ranking problems are its special cases.
(1) It is obvious to see that the conventional LTR framework is a special case of R-LTR. Specifically, if we ignore the relation tensor R, then we get the same function as that in traditional LTR, i.e. f(X, R) = f(X).
(2) The `learning to rank relational objects' framework [22, 23] is also a special case of R-LTR. Specifically, if we restrict the relation tensor R to be a matrix, with Rij representing the relation between document xi and xj, then we get the same function as that in the problem of learning to rank relational objects.
The above framework gives a formulation of ranking problems involving relationship. When solving the specific problem, one needs to define the corresponding ranking function and loss function according to the task.

4. SEARCH RESULT DIVERSIFICATION VIA R-LTR FRAMEWORK
As mentioned in the previous section, it is natural to formulate search result diversification under R-LTR framework. In this paper, we mainly focus on the diverse ranking scenario. To apply the above framework to this specific task, the most challenging problem is the definition of ranking function and loss function.
4.1 Motivation
In order to properly define the ranking function and loss function, we first look into the diverse ranking problem.
(1) Empirically, users usually browse the Web search results in a top-down manner, and perceive diverse information from each individual document based on what he/she have obtained in the preceding results [8].
(2) Theoretically, diverse ranking can be naturally stated as a bi-criterion optimization problem, and it is NP-hard [1, 4]. Therefore, in practice, most previous approaches on search result diversification are based on greedy approximation, which sequentially select a `local-best' document from the remanent candidate set [29].
From both empirical and theoretical analysis above, we can see that it is better to view diverse ranking as a sequential selection process, in the sense that the ranking list is generated in a sequential order, with each individual document ranked according to its relevance to the query and the relation between all the documents ranked before it.
4.2 Definition of Ranking Function
As discussed above, diverse ranking is in general a sequential selection process, where each individual document is ranked according to its relevance to the query and the relation between all the documents ranked before it. The intuitive idea is illustrated in Figure 1, when ranking documents in X\S given the already ranked results S, both content-based relevance and diversity relation between this

X

d1

S

d2

d3

Diversity

Diversity

d7 d6 d5 X\S

d8

Relevance d4

Relevance

Figure 1: An illustration of the sequential way to define ranking function. All the rectangles represent candidate documents of a user query, and different colors represent different subtopics. The solid rectangle is relevant to the query, and the hollow rectangle is irrelevant to the query, and larger size means more relevance. X denotes all the candidate document collection. S denotes previously selected documents, and X\S denotes the remanent documents.

document and the previously selected documents in S should be considered. Noting that larger size of the rectangle means the document is more relevant to the query, and different colors represent different subtopics. Therefore, the document 8 may be more preferred than document 4 given S, since it is relevant to the query, and also provides different aspects additionally comparing with the selected set S.
Based on this ranking process, here we give the precise definition of ranking function. Given a query q, we assume that a set of documents have been selected, denoted as S, the scoring function on the candidate document in X\S, is then defined as the combination of the relevance score and the diversity score between the current document and those previously selected, shown as follows.

fS (xi, Ri) = rT xi + dT hS (Ri), xi  X\S,

(2)

where xi denotes the relevance feature vector of the candidate document xi, Ri stands for the matrix of relationships between document xi and other selected documents, with each Rij stands for the relationship vector between document xi and xj, represented by the feature vector of (Rij1, · · · , Rijl), xj  S, and Rijk stands for the k-th relation feature between documents xi and xj. hS(Ri) stands for the relational function on Ri, rT and dT stands for the corresponding relevance and diversity weight vector. When S = , fS(xi, Ri) is directly represented as rT xi. Then the ranking function can be represented as the set of scoring function:
f(X, R) = (fS , fS1 , · · · , fSn-1 )
where Si, denotes the previously selected document collection with i documents. From the above definition, we can see that if we do not consider diversity relation, our ranking

295

function reduce to f(X) = (f (x1), · · · , f (xn)), which is the traditional ranking function in learning-to-rank.

4.2.1 Relational function hS(Ri)
Please note that the relational function hS(Ri) denotes the way of representing the diversity relationship between the current document xi and the previously selected documents in S. If we treat diversity relation as distance, hS(Ri) can be viewed as the distance of xi to the set S. According to different definitions of the distance between an item and a set of items, hS(Ri) can be defined as the following three ways.
Minimal Distance. The distance between a document xi and a set S is defined as the minimal distance of all the document pairs (xi, xj), xj  S.

hS (Ri) = ( min Rij1, · · · , min Rijl).

xj S

xj S

Average Distance. The distance between a document

xi and a set S is defined as the average distance of all the

document pairs (xi, xj), xj  S.

1

1

hS(Ri) = ( |S|

Rij1, · · · , |S|

Rijl).

xj S

xj S

Maximal Distance. The distance between a document xi and a set S is defined as the maximal distance of all the document pairs (xi, xj), xj  S.

hS (Ri) = (max Rij1, · · · , max Rijl).

xj S

xj S

4.2.2 Diversity Feature Vector Rij
How to define discriminative features that can well capture diversity relation is critical for the success of R-LTR. In this work, we provides several representative features for the learning process, including semantic diversity features (i.e. subtopic diversity, text diversity, title diversity, anchor text diversity and ODP-based diversity) and structural diversity features (i.e. link-based diversity and url-based diversity).
Subtopic Diversity. Different documents may associate with different aspects of the given topic. We use Probabilistic Latent Semantic Analysis (PLSA) [16] to model implicit subtopics distribution of candidate objects, which is important for the diversification task as mentioned before. Therefore, we define the diversity feature based on implicit subtopics as follows.

Rij1 =

 m (p(zk|xi) - p(zk|xj ))2
k=1

Text Diversity. Text dissimilarity is also meaningful for diversity. We propose to represent it as the cosine dissimilarity based on weighted term vector representations, and define the feature as follows.

Rij2

=

1

-

di · dj , didj 

where di, dj are the weighted document vectors based on tf  idf , and tf denotes the term frequencies, idf denotes inverse document frequencies. There also exists other computing ways such as the work in [14], which is based on sketching algorithm and Jaccard similarity.

Title Diversity. The way of computing title diversity feature is similar as that for text diversity feature, which is denoted as Rij3.
Anchor Text Diversity. The anchor text can accurately describe the content of corresponding page and is important. This type of feature is computed similarly as text and title diversity features, denoted as Rij4.
ODP-Based Diversity. The existing ODP taxonomy1 offers a succinct encoding of distances between documents. Usually, the distance between documents on similar topics in the taxonomy is likely to be small. For two categories u and v, we define the categorical distance between them as following:

c

dis(u, v)

=

1

-

|l(u, v)| max{|u|, |v|}

where l(u, v) is the length of their longest common prefix.

|u| and |v| is the length of category u and v. Then given two

documents xi and xj and their category information sets

Ci and Cj respectively, we define the ODP-based diversity

feature as:



Rij5 =

uCi vCj c dis(u, v) |Ci| · |Cj |

where |Ci| and |Cj| are the number of categories in corre-

sponding category sets.

Link-Based Diversity. By constructing a web link graph,

we can calculate the link similarity of any document pair

based on direct inlink or outlink information. The link-based

diversity feature is then defined as follows.

{

Rij6 =

0 1

if xi  inlink(xj)  outlink(xj), otherwise

URL-Based Diversity. Given the url information of

two documents, we can judge whether they belong to the

same domain or the same site. The url-based diversity fea-

ture is then defined as follows.  0 if one url is another's prefix

Rij7 = 01.5

if they belong to the same site or domain otherwise

Based on these diversity features, we can obtain the diversity feature vector Rij = (Rij1, Rij2, · · · , Rij7). All the feature values are normalized to the range of [0,1]. Please note that there might be some other useful resources for the definition of diversity features, e.g., clickthrough logs, which will be further considered in our future work.

4.3 Definition of Loss Function
Motivated by the analysis that the process for diverse ranking is in general a sequential selection process, we propose to model the generation of a diverse ranking list in a sequential way, and define the loss function as the likelihood loss of the generation probability.

L(f(X, R), y) = - log P (y|X).

(3)

Intuitively, the generation probability of a ranking list can be viewed as a process to iteratively select the top ranked

1http://www.dmoz.org/

296

documents from the remaining documents. The precise definition is given as follows.

P (y|X) = P (xy(1), xy(2), · · · , xy(n)|X)

(4)

= P (xy(1)|X)P (xy(2)|X\S1) · · · P (xy(n-1)|X\Sn-2),

where y(i) stands for the index of document which is ranked
in position i in the ranking list y, X denotes all the candidate
documents, Si = {xy(1), · · · , xy(i)}, denotes the previously selected document collection with i documents, P (xy(1)|X) stands for the probability that xy(1) is ranked first among the documents in X, and P (xy(j)|X\Sj-1) stands for the probability that document xy(j) is ranked first among the documents in X\Sj-1.

4.3.1 Plackett-Luce based Probability P (y|X)
The above sequential definition approach can be well captured by the Plackett-Luce Model [18]. Therefore, we propose to define P (xy(1)|X) and P (xy(j)|X\Sj-1) in a similar way, shown as follows, j  2.

P (xy(1)|X) = nke=x1pe{xfp{(fxy((1x)y)(}k))} ,

(5)

P (xy(j)|X\Sj-1) = nke=xjpe{xfpS{jf-S1k(-x1y((xj)y,(Rk)y,(Rj)y)(}k))} . (6)

Incorporating Eq.(5) and Eq.(6) into Eq.(4), the generation probability of a diverse ranking list is formulated as follows.

P (y|X)

=

n
j=1

nke=xjpe{xfpS{jf-S1k(-x1y((xj)y,(Rk)y,(Rj)y)(}k))} ,

(7)

where S0 = , f(x, R) = rT x.

4.3.2 Relation to ListMLE in Learning-to-Rank

Incorporating Eq.(7) into the definition of the loss func-

tion Eq.(3), we can obtain the precise definition of the loss

function as follows.

{

}

n L(f(X,R), y) = - log
j=1

nke=xpj e{xfpS{j-fS1 k(x-y1((xj)y,(Rk)y,(Rj)y)(}k))}

(8)

We can see that our loss function is similar to that in

ListMLE [34], which is formulated as follows.

{

}

n L(f(X), y) = - log
j=1

nke=xjpe{xfp({xfy((xj)y)(}k))}

,

where f (x) is the score function in traditional learning-torank, i.e. f (x) = T x.
Therefore, if we do not consider diversity relation in our framework, our loss function will reduce to the same form of that in ListMLE. That is to say, ListMLE is a special case of our loss function.

4.4 Learning and Prediction
Based on the definitions of ranking function and loss function, we present the learning and prediction process in this section. Specifically, we first describe how to construct the training data, and then introduce the optimization procedure. Finally, we show how to make predictions based on the learned ranking function.

Algorithm 1 Construction of Approximate Ideal
Ranking List
Input: (qi, X(i), Ti, P (x(ji)|t)), t  Ti, x(ji)  X(i)
Output: y(i) 1: Initialize S0  , y(i) = (1, · · · , ni) 2: for k = 1, ..., ni do
3: bestDoc  argmaxxX(i)\Sk-1 ODM (Sk-1  x) 4: Sk  Sk-1  bestDoc 5: y(i)(k) = the index of bestDoc
6: end for 7: return y(i) = (y(i)(1), · · · , y(i)(ni)).

Algorithm 2 Optimization Algorithm

Input: training data {(X(i), R(i), y(i))}Ni=1, parameter: learning rate , tolerance rate 

Output: model vector: r, d

1: Initialize parameter value r, d

2: repeat

3: Shuffle the training data

4: for i = 1, ..., N do

5:

Compute gradient r(i) and d(i)

6:

Update model: r = r -  × r(i),

d = d -  × d(i)

7: end for

8: Calculate likelihood loss on the training set

9: until the change of likelihood loss is below 

4.4.1 Training Data
The labeled data in search result diversification such as
TREC diversity task are usually provided in the form of (qi, X(i), Ti, P (x(ji)|t)), t  Ti, x(ji)  X(i), where X(i) is a candidate document set of query qi, Ti is the subtopics of query qi, t is a specific subtopic in Ti, and P (x(ji)|t) describes the relevance of document x(ji) to subtopic t. We can see that the above form of labeled data deviates the formulation of y(i) in our R-LTR framework, which requires a ranking list
of candidate documents. In order to apply R-LTR, we need to construct y(i) from the provided form of labeled data.
We propose to construct an approximate ideal ranking list
by maximizing the ODM measures (e.g., ERR-IA), and use
the approximate ideal ranking list as the training groundtruth y(i) for query qi, as described in Algorithm 1.
According to the results in [20], if a submodular func-
tion is monotonic (i.e., f (S)  f (T ), whenever S  T ) and normalized (i.e., f () = 0), greedily constructing gives an (1 - 1/e)-approximation to the optimal. Since any member of ODM is a submodular function, we can easily prove that
Algorithm 1 is (1 - 1/e)-approximation to the optimal (We omit the proof here). And the quality of training ground-
truth can be guaranteed.

4.4.2 Learning

Given the training data {(X(i), R(i), y(i))}Ni=1, the total loss is represented as follows.

 N  ni -
i=1 j=1

 

exp{rT x(yi()j) + dT

logni
k=j

exp{rT

x(yi()k)

+

hSj(i-)1 (Ry(i()j))} dT hSk(i-) 1 (Ry(i()k)

  )}

(9)

297

Algorithm 3 Ranking Prediction via Sequential Se-
lection
Input: X(t), R(t), r, d Output: y(t) 1: Initialize S0  , y(t) = (1, · · · , nt)
2: for k = 1, ..., nt do
3: bestDoc  argmaxxXt fSk-1 (x, R) 4: Sk  Sk-1  bestDoc 5: y(t)(k)  the index of bestDoc
6: end for 7: return y(t) = (y(t)(1), · · · , y(t)(nt))

For such a unconstrained optimization problem, we employ Stochastic Gradient Descent (SGD) to conduct optimization as shown in Algorithm 2. According to Eq.(9), the

gradient at training sample X(i) is computed as follows.

r(i)

=

 ni
j=1

 

ni
k=j

x(yi()k)

exp{rT

x(yi()k)

+

dT

hSk(i-) 1

(Ry(i()k) )}



ni
k=j

exp{rT

x(yi()k)

+

dT

hSk(i-) 1

(Ry(i()k) )}

-

x(yi()j) exp{rT x(yi()j) exp{rT x(yi()j) +

+ dT hSj(i-)1 (Ry(i()j) dT hSj(i-)1 (Ry(i()j))}

)}

  

,

d(i)=j n=i1nk=i j

hSk(i-) 1 (Ry(i()k)) exp{rT x(yi()k) + dT hSk(i-) 1 (Ry(i()k)

ni
k=j

exp{rT

x(yi()k)

+

dT

hSk(i-) 1 (Ry(i()k))}

- hSj(i-)1 (Ry(i()j)) exp{rT x(yi()j) + dT hSj(i-)1 (Ry(i()j))} exp{rT x(yi()j) + dT hSj(i-)1 (Ry(i()j))}

)}
  

.

diversity methods and the importance of our proposed diversity features. Finally, we study the efficiency of our approach based on the analysis of running time.
5.1 Experimental Setup
Here we give some introductions on the experimental setup, including data collections, evaluation metrics, baseline models and detailed implementation.
5.1.1 Data Collections
Our evaluation was conducted in the context of the diversity tasks of the TREC2009 Web Track (WT2009), TREC2010 Web Track (WT2010), and TREC2011 Web Track (WT2011), which contain 50, 48 and 50 test queries (or topics), respectively. Each topic includes several subtopics identified by TREC assessors, with binary relevance judgements provided at the subtopic level2. All the experiments were carried out on the ClueWeb09 Category B data collection3, which comprises a total of 50 million English Web documents.
5.1.2 Evaluation Metrics
The current official evaluation metrics of the diversity task include ERR-IA [6], -N DCG [11] and N RBP [12]. They measure the diversity of a result list by explicitly rewarding novelty and penalizing redundancy observed at every rank. We also use traditional diversity measures for evaluation: Precision-IA [1] and Subtopic Recall [37].They measure the precision across all subtopics of the query and the ratio of the subtopics covered in the results, respectively. All the measures are computed over the top-k search results (k = 20). Moreover, the associated parameters  and  are all set to be 0.5, which is consistent with the default settings in official TREC evaluation program.

4.4.3 Prediction
As the ranking function is defined sequentially, traditional prediction approach (i.e., calculating the ranking score of each independent document simultaneously and sorting them in descending order to obtain a ranking list) fails in our framework. According to the sequential selection essence of diverse ranking, we propose a sequential prediction process, as described in Algorithm 3. Specifically, in the first step, the most relevant document with maximal relevance score will be selected and ranked first. If the top k items have been selected, then the document in position k + 1 should be with maximum fSk . At last, all the documents are ranked accordingly, and we obtain the final ranking list.
Assuming that the size of output ranking is K, the size of candidate set is n, then this type of sequential selection algorithm 3 will have time complexity of O(n  K). Usually, the original value of n is large, therefore, an initial retrieval can be applied to provide a filtered candidate set with relatively small size (e.g., top 1000 or 3000 retrieved documents). With a small K, the prediction time is linear.
5. EXPERIMENTS
In this section, we evaluate the effectiveness of our approach empirically. We first introduce the experimental setup. We then compare our approach with baseline methods under different diversity evaluation measures. Furthermore, we analyze the performance robustness of different

5.1.3 Baseline Models
To evaluate the performance of our approach, we compare our approach with the state-of-the-art approaches, which are introduced as follows.
QL. The standard Query-likelihood language model is used for the initial retrieval, which provides the top 1000 retrieved documents as a candidate set for all the diversification approaches. It is also used as a basic baseline method in our experiment.
MMR. MMR is a classical implicit diversity method in the diversity research. It employs a linear combination of relevance and diversity as the metric called "marginal relevance" [3]. MMR will iteratively select document with the largest "marginal relevance".
xQuAD. The explicit diversification approaches are popular in current research field, in which xQuAD is the most representative and used as a baseline model in our experiments [29].
PM-2. PM-2 is also a explicit method that proposes to optimize proportionality for search result diversification [13]. It has been proved to achieve promising performance in their work, and is also chosen as baseline in our experiment.
2For WT2011 task, assessors made graded judgements. While in the official TREC evaluation program, it mapped these graded judgements to binary judgements by treating values > 0 as relevant and values  0 as not relevant. 3http://boston.lti.cs.cmu.edu/Data/clueweb09/

298

Table 1: Relevance Features for learning on

ClueWeb09-B collection [21, 19].

Category Feature Description Total

Q-D

TF-IDF

5

Q-D

BM25

5

Q-D

QL.DIR

5

Q-D

MRF

10

D

PageRank

1

D

#Inlinks

1

D

#Outlinks

1

ListMLE. ListMLE is a plain learning-to-rank approach without diversification considerations, and is a representative listwise relevance approach in LTR field [17].
SVMDIV. SVMDIV is a representative supervised approach for search result diversification [36]. It proposes to optimize subtopic coverage by maximizing word coverage. It formulates the learning problem and derives a training method based on structural SVMs. However, SVMDIV only models diversity and discards the requirement of relevance. For fair performance comparison, we will firstly apply ListMLE to do the initial ranking to capture relevance, and then use SVMDIV to re-rank top-K retrieved documents to capture diversity.
The above three diversity baselines: MMR, xQuAD and PM-2, all require a prior relevance function to implement their diversification steps. In our experiment, we choose ListMLE as the relevance function to implement them, and denote them as: MMRlist, xQuADlist and PM-2list, respectively.
According to the different ways in defining the relational function hS(Ri) in section 4.2.1, our R-LTR diversification approach has three variants, denoted as R-LTRmin, R-LTRavg and R-LTRmax, respectively.
5.1.4 Implementation
In our experiments, we use Indri toolkit (version 5.2)4 as the retrieval platform. For the test query set on each dataset, we use a 5-fold cross validation with a ratio of 3:1:1, for training, validation and testing. The final test performance is reported as the average over all the folds.
For data preprocessing, we apply porter stemmer and stopwords removing for both indexing and query processing. We then extract features for each dataset as follows. For relevance, we use several standard features in LTR research [21], such as typical weighting models (e.g., TF-IDF, BM25, LM), and term dependency model [19, 38], as summarized in Table 1, where Q-D means that the feature is dependent on both query and document, and D means that the feature only depends on the document. For all the Q-D features, they are applied in five fields: body, anchor, title, URL and the whole document, resulting in 5 features in total, respectively. Additionally, the MRF feature has two types of values: ordered phrase and unordered phrase [19], so the total feature number is 10.
For three baseline models: MMR, xQuAD and PM-2, they all have a single parameter  to tune. We perform a 5fold cross validation to train  through optimizing ERR-IA. Additionally, for xQuAD and PM-2, the official subtopics are used as a representation of taxonomy classes to simu-
4http://lemurproject.org/indri

late their best-case scenarios, and uniform probability for all subtopics is assumed as [29, 13].
For ListMLE and SVMDIV, we utilize the same training data generated by Algorithm 1 to train their model, and also conduct 5-fold cross validation. ListMLE adopts the relevance features summarized in Table 1. SVMDIV adopts the representative word level features with different importance criterion, as listed in their paper and released code [36]. As described in above subsection, SVMDIV will rerank top-K retrieved documents returned by ListMLE. We test K  {30, 50, 100}, and find it performs best at K = 30. Therefore, the following results of SVMDIV are achieved with K = 30.
For our approach, the learning rate  parameter in Algorithm 2 is chosen from 10-7 to 10-1, and the best learning rate is obtained based on the performance of validation set.
5.2 Performance Comparison
5.2.1 Evaluation on Official Diversity Metrics
We now compare our approaches to the baseline methods on search result diversification. The results of performance comparison are shown in Table 2, 3, and 4. We also present the performance of top performing systems on CategoryB reported by TREC [7, 10, 9], which are just taken as indicative references. The number in the parentheses are the relative improvements compared with the baseline method QL. Boldface indicates the highest scores among all runs.
From the results we an see that, our R-LTR outperform the plain LTR approach without diversification consideration, i.e. ListMLE, which can be viewed as a special case of our approach. Specifically, the relative improvement of R-LTRmin over ListMLE is up to 41.87%, 49.71%, 29.17%, in terms of ERR-IA on WT2009, WT2010, and WT2011, respectively. It indicates that our approach can tackle multicriteria ranking problem effectively, with the consideration of both content-based information and diversity relationship among candidate objects.
Regarding the comparison among representative implicit and explicit diversification approaches, explicit methods (i.e. xQuAD and PM-2) show better performance than the implicit method (i.e. MMR) in terms of all the evaluation measures. MMR is the least effective due to its simple predefined "marginal relevance". The two explicit methods achieve comparable performance: PM-2list wins on WT2010 and WT2011, while xQuADlist wins on WT2009, but their overall performance differences are small.
Furthermore, our approach outperforms the state-of-theart explicit methods in terms of all the evaluation measures. For example, with the evaluation of ERR-IA, the relative improvement of R-LTRmin over the xQuADlist is up to 17.18%, 11.26%, 13.38%, on WT2009, WT2010, WT2011, respectively, and the relative improvement of R-LTRmin over the PM-2list is up to 18.31%, 10.65%, 10.59% on WT2009, WT2010, WT2011, respectively. Although xQuADlist and PM-2list all utilize the official subtopics as explicit query aspects to simulate their best-case scenarios, their performances are still much lower than our learning-based approaches, which indicates that there might be certain gap between their heuristic predefined utility functions and the final evaluation measures.
Comparing with the learning-based diversification baseline method, our R-LTR approach also show better per-

299

Table 2: Performance comparison of all methods in official TREC diversity measures for WT2009.

Method

ERR-IA

-NDCG

NRBP

QL

0.1637

0.2691

0.1382

ListMLE

0.1913 (+16.86%)

0.3074 (+14.23%)

0.1681 (+21.64%)

MMRlist xQuADlist
PM-2list SVMDIV

0.2022 (+23.52%) 0.2316 (+41.48%) 0.2294 (+40.13%) 0.2408 (+47.10%)

0.3083 (+14.57%) 0.3437 (+27.72%) 0.3369 (+25.20%) 0.3526 (+31.03%)

0.1715 (+24.09%) 0.1956 (+41.53%) 0.1788 (+29.38%) 0.2073 (+50.00%)

R-LTRmin R-LTRavg R-LTRmax

0.2714 (+65.79%) 0.2671 (+63.16%) 0.2683 (+63.90%)

0.3915 (+45.48%) 0.3964 (+47.31%) 0.3933 (+46.15%)

0.2339 (+69.25%) 0.2268 (+64.11%) 0.2281 (+65.05%)

TREC-Best

0.1922

0.3081

0.1617

Table 3: Performance comparison of all methods in official TREC diversity measures for WT2010.

Method

ERR-IA

-NDCG

NRBP

QL

0.1980

0.3024

0.1549

ListMLE

0.2436 (+23.03%)

0.3755 (+24.17%)

0.1949 (+25.82%)

MMRlist xQuADlist
PM-2list SVMDIV

0.2735 (+38.13%) 0.3278 (+65.56%) 0.3296 (+66.46%) 0.3331 (+68.23%)

0.4036 (+33.47%) 0.4445 (+46.99%) 0.4478 (+48.08%) 0.4593 (+51.88%)

0.2252 (+45.38%) 0.2872 (+85.41%) 0.2901 (+87.28%) 0.2934 (+89.41%)

R-LTRmin R-LTRavg R-LTRmax

0.3647 (+84.19%) 0.3587 (+81.16%) 0.3639 (+83.79%)

0.4924 (+62.83%) 0.4781 (+58.10%) 0.4836 (+59.92%)

0.3293 (+112.59%) 0.3125 (+101.74%) 0.3218 (+107.74%)

TREC-Best

0.2981

0.4178

0.2616

formance than the SVMDIV approach. The relative improvement of R-LTRmin over the SVMDIV is up to 12.71%, 9.49%, 10.02%, in terms of ERR-IA on WT2009, WT2010, WT2011, respectively. SVMDIV simply uses weighted word coverage as a proxy for explicitly covering subtopics, while our R-LTR approach directly models the generation probability of the diverse ranking based on the sequential ranking formulation. Therefore, our R-LTR approach shows deeper understanding and better formulation of diverse ranking, and leads to better performance. We further conduct statistical tests on the results, which indicates that all these improvements are statistically significant (p-value < 0.01).
Among the R-LTR approaches, R-LTRmin obtains better performance than the other two variants especially on WT2010 and WT2011 data collection, although their performance difference is small. It indicates that when defining the diversity relation between a document and a set of documents, the minimal distance would be a better choice.
5.2.2 Evaluation on Traditional Diversity Metrics.
Additionally, we also evaluate all the methods under traditional diversity measures, i.e. Precision-IA and Subtopics Recall. The experimental results are shown in Figure 2 and 3. We can see that our approaches outperform all the baseline models on all the data collections in terms of both metrics, which is consistent with the evaluation results in Table 2, 3, and 4. It can further demonstrate the effectiveness of our approach on search result diversification from different aspects. When comparing the three variants of our R-LTR approaches, they all show similar performance and none obtains consistent better performance than the others under these two measures.
5.3 Robustness Analysis
In this section we analyze the robustness of these diversification methods, i.e., whether the performance improvement is consistent as compared with the basic relevance baseline

Table 5: The robustness of the performance of all

diversity methods in Win/Loss ratio

WT2009 WT2010 WT2011 Total

ListMLE 20/18 27/16 26/11 73/45

MMRlist xQuADlist PM-2list SVMDIV

22/15 28/11 26/15 30/12

29/13 31/12 32/12 32/11

29/10 31/12 32/11 32/11

80/38 90/35 90/38 94/34

R-LTRmin R-LTRavg R-LTRmax

34/9 33/9 33/10

35/10 34/11 35/10

35/9 34/10 34/10

104/28 101/30 102/30

QL. Specifically, we define the robustness as the Win/Loss ratio [36, 13] - the ratio of queries whose performance improves or hurts as compared with the original results from QL in terms of of ERR-IA.
From results in Table 5, we find that our R-LTR methods achieve best as compared with all the baseline methods, with the total Win/Loss ratio around 3.49. Among the three variants of R-LTR methods, R-LTRmin performs better than the others, with the Win/Loss ratio as 3.71.
Based on the robustness results, we can see that the performance of our R-LTR approach is more stable than all the baseline methods. It demonstrates that the overall performance gains of our approach not only come from some small subset of queries. In other words, the result diversification for different queries could be well addressed under our approach.
5.4 Feature Importance Analysis
In this subsection, we analyze the relative importance of the proposed diversity features. Table 6 shows an ordered list of diversity features used in our R-LTRmin model according to the learned weights (average on three datasets). From the results, we can see that the subtopic diversity Rij1(topic) is with the maximal weight, which is in accordance with

300

Table 4: Performance comparison of all methods in official TREC diversity measures for WT2011.

Method

ERR-IA

-NDCG

NRBP

QL

0.3520

0.4531

0.3123

ListMLE

0.4172 (+18.52%)

0.5169 (+14.08%)

0.3887 (+24.46%)

MMRlist xQuADlist
PM-2list SVMDIV

0.4284 (+21.70%) 0.4753 (+35.03%) 0.4873 (+38.44%) 0.4898 (+39.15%)

0.5302 (+17.02%) 0.5645 (+24.59%) 0.5786 (+27.70%) 0.5910 (+30.43%)

0.3913 (+25.30%) 0.4274 (+36.86%) 0.4318 (+38.26%) 0.4475 (+43.29%)

R-LTRmin R-LTRavg R-LTRmax

0.5389 (+53.10%) 0.5276 (+49.89%) 0.5285 (+50.14%)

0.6297 (+38.98%) 0.6219 (+37.25%) 0.6223 (+37.34%)

0.4982 (+59.53%) 0.4724 (+51.26%) 0.4741 (+51.81%)

TREC-Best

0.4380

0.5220

0.4070

0.06

WT2009

SVMDIV PM-2_list
xQuAD_list MMR_list ListMLE QL

R-LTR_max R-LTR_avg R-LTR_min

WT2010
R-LTR_max R-LTR_avg R-LTR_min SVMDIV PM-2_list xQuAD_list MMR_list ListMLE QL

WT2011
R-LTR_max R-LTR_avg
R-LTR_min SVMDIV PM-2_list xQuAD_list MMR_list ListMLE QL

0.08 0.1 0.12 0.14 0.16 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42

Figure 2: Performance comparison of all methods in Precision-IA for WT2009, WT2010, WT2011.

WT2009
R-LTR_max R-LTR_avg R-LTR_min
SVMDIV PM-2_list
xQuAD_list MMR_list ListMLE QL

WT2010
R-LTR_max R-LTR_avg R-LTR_min
SVMDIV PM-2_list xQuAD_list MMR_list ListMLE QL

WT2011
R-LTR_max R-LTR_avg R-LTR_min SVMDIV PM-2_list xQuAD_list MMR_list ListMLE QL

0.39 0.43 0.47 0.51 0.55 0.59 0.63 0.52 0.56 0.6 0.64 0.68 0.72 0.76 0.64 0.68 0.72 0.76 0.8 0.84 0.88

Figure 3: Performance comparison of all methods in Subtopic Recall for WT2009, WT2010, WT2011.

Table 6: Order list of diversity features with corre-

sponding weight value.

feature

weight

Rij1(topic) Rij3(title) Rij4(anchor) Rij2(text) Rij5(ODP) Rij6(Link) Rij7(URL)

3.71635 1.53026 1.34293 0.98912 0.52627 0.04683 0.01514

our intuition that diversity mainly lies in the rich semantic information. Meanwhile, the title and anchor text diversity Rij3(title) and Rij4(anchor) also work well, since these fields typically provide a precise summary of the content of the document. Finally, The Link and URL based diversity Rij6(Link) and Rij7(URL) seem to be the least important features, which may be due to the sparsity of such types of features in the data.
As a learning-based method, our model is flexible to incorporate different types of features for capturing both the relevance and diversity. Therefore, it would be interesting

to explore other useful features under our R-LTR framework to further improve the performance of diverse ranking. We will investigate this issue in future.
5.5 Running Time Analysis
We further study the efficiency of our approach and the baseline models. All of the diversity methods associate with a sequential selection process, which is time-consuming due to the consideration of the dependency relations of document pairs. While as discussed before, this type of algorithms all have time complexity of O(n  K), With a small K, the prediction time is linear.
All the learning-based methods (i.e. ListMLE, SVMDIV and R-LTR) need additional offline training time due to the supervised learning process. We compare the average training time of different learning-based methods, and the result is shown as following (unit: hour):
ListMLE ( 1.5h)  SVMDIV ( 2h)  R-LTR ( 3h)
We can observe that our approach takes longer but comparable offline training time among different learning-based methods. Besides, in our experiments, we also found that the three variants of our R-LTR approach are with nearly the same training time. We will attempt to optimize our

301

code to provide much faster training speed via parallelization technique in the following work.
6. CONCLUSIONS
In this paper, we propose to solve the search result diversification problem within a novel R-LTR framework. However, the specific definitions of ranking function and loss function are challenging. Motivated by the top-down user browsing behavior and the ubiquitous greedy approximation for diverse ranking, we firstly define the ranking function as the combination of relevance score and diversity score between the current item and those previously selected. Then the loss function is defined as the likelihood loss of ground truth based on Plackett-Luce model, which can naturally model the sequential generation of a diverse ranking list. On this basis, we utilize stochastic gradient descent to conduct the unconstrained optimization. The prediction of a diverse ranking list is then provided by iteratively maximizing the learned ranking function. Finally the experimental results on public TREC data collections demonstrate the effectiveness and robustness of our approach.
The proposed R-LTR framework is quite general that can be used in other applications, such as pseudo relevance feedback and topic distillation. Therefore, it would be interesting to apply our R-LTR framework in different applications in our future work.
7. ACKNOWLEDGEMENTS
This research work was funded by the 973 Program of China under Grants No.2012CB316303 and No.2013CB329602, 863 program of China under Grants No.2012AA011003, National Natural Science Foundation of China under Grant No.61232010 and No.61203298, and National Key Technology R&D Program under Grants No.2012BAH39B02 and No.2012BAH46B04.
8. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of the 2th ACM WSDM, pages 5­14, 2009.
[2] C. Brandt, T. Joachims, Y. Yue, and J. Bank. Dynamic ranked retrieval. In Proceedings of the 4th ACM WSDM, pages 247­256, 2011.
[3] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st ACM SIGIR, pages 335­336, 1998.
[4] B. Carterette. An analysis of np-completeness in novelty and diversity ranking. In Proceedings of the 2nd ICTIR, 2009.
[5] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In Proceedings of the 18th ACM CIKM, pages 1287­1296, 2009.
[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM CIKM, pages 621­630, 2009.
[7] C. L. Clarke, N. Craswell, and I. Soboroff. Overview of the trec 2009 web track. In TREC, 2009.
[8] C. L. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In Proceedings of the 4th ACM WSDM, pages 75­84, 2011.
[9] C. L. Clarke, N. Craswell, I. Soboroff, and E. M.Voorhees. Overview of the trec 2011 web track. In TREC, 2011.
[10] C. L. Clarke, N. Craswell, I. Soboroff, and G. V.Cormack. Overview of the trec 2010 web track. In TREC, 2010.
[11] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of the 31st ACM SIGIR, pages 659­666, 2008.

[12] C. L. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. In Proceedings of the 2nd ICTIR, pages 188­199, 2009.
[13] V. Dang and W. B. Croft. Diversity by proportionality: an election-based approach to search result diversification. In Proceedings of the 35th ACM SIGIR, pages 65­74, 2012.
[14] S. Gollapudi and A. Sharma. An axiomatic approach for result diversification. In Proceedings of the 18th WWW, pages 381­390, 2009.
[15] J. He, V. Hollink, and A. de Vries. Combining implicit and explicit topic representations for result diversification. In Proceedings of the 35th ACM SIGIR, pages 851­860, 2012.
[16] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd ACM SIGIR, pages 50­57, 1999.
[17] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.
[18] J. I. Marden. Analyzing and Modeling Rank Data. Chapman and Hall, 1995.
[19] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proceedings of the 28th ACM SIGIR, pages 472­479, 2005.
[20] G. Nemhauser, L. Wolsey, and M. Fisher. An analysis of approximations for maximizing submodular set functions­i. Mathematical Programming, 14(1):265­294, 1978.
[21] T. Qin, T.-Y. Liu, J. Xu, and H. Li. Letor: A benchmark collection for research on learning to rank for information retrieval. Inf. Retr., pages 346­374, 2010.
[22] T. Qin, T.-Y. Liu, X.-D. Zhang, D.-S. Wang, and H. Li. Global ranking using continuous conditional random fields. In Proceedings of the 22th NIPS, Vancouver, British Columbia, Canada, December 8-11, 2008, pages 1281­1288, 2008.
[23] T. Qin, T.-Y. Liu, X.-D. Zhang, D.-S. Wang, W.-Y. Xiong, and H. Li. Learning to rank relational objects and its application to web search. In Proceedings of the 17th WWW, pages 407­416, 2008.
[24] F. Radlinski and S. Dumais. Improving personalized web search using result diversification. In Proceedings of the 29th ACM SIGIR, 2006.
[25] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse rankings with multi-armed bandits. In Proceedings of the 25th ICML, pages 784­791, 2008.
[26] D. Rafiei, K. Bharat, and A. Shukla. Diversifying web search results. In Proceedings of the 19th WWW, pages 781­790, 2010.
[27] K. Raman, T. Joachims, and P. Shivaswamy. Structured learning of two-level dynamic rankings. In Proceedings of the 20th ACM CIKM, pages 291­296, 2011.
[28] K. Raman, P. Shivaswamy, and T. Joachims. Online learning to diversify from implicit feedback. In Proceedings of the 18th ACM SIGKDD, pages 705­713, 2012.
[29] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proceedings of the 19th WWW, pages 881­890, 2010.
[30] P. Shivaswamy and T. Joachims. Online structured prediction via coactive learning. In ICML'12, 2012.
[31] A. Slivkins, F. Radlinski, and S. Gollapudi. Learning optimally diverse rankings over large document collections. In Proceedings of the 27th ICML, pages 983­990, 2010.
[32] S. Vargas, P. Castells, and D. Vallet. Explicit relevance models in intent-oriented information retrieval diversification. In Proceedings of the 35th ACM SIGIR, pages 75­84, 2012.
[33] J. Wang and J. Zhu. Portfolio theory of information retrieval. In Proceedings of the 32nd ACM SIGIR, pages 115­122, 2009.
[34] F. Xia, T.-Y. Liu, J. Wang, W. Zhang, and H. Li. Listwise approach to learning to rank: theory and algorithm. In Proceedings of the 25th ICML, pages 1192­1199, 2008.
[35] Y. Yue and C. Guestrin. Linear submodular bandits and their application to diversified retrieval. In NIPS, pages 2483­2491, 2011.
[36] Y. Yue and T. Joachims. Predicting diverse subsets using structural svms. In Proceedings of the 25th ICML, pages 1224­1231, 2008.
[37] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proc. of the 26th ACM SIGIR, pages 10­17, 2003.
[38] Y. Zhu, Y. Xue, J. Guo, Y. Lan, X. Cheng, and X. Yu. Exploring and exploiting proximity statistic for information retrieval model. In Proceedings of the 8th Asia Information Retrieval Societies Conference, volume 7675 of Lecture Notes in Computer Science, pages 1­13, 2012.

302

Fusion Helps Diversification

Shangsong Liang
University of Amsterdam Amsterdam, The Netherlands
s.liang@uva.nl

Zhaochun Ren
University of Amsterdam Amsterdam, The Netherlands
z.ren@uva.nl

Maarten de Rijke
University of Amsterdam Amsterdam, The Netherlands
derijke@uva.nl

ABSTRACT
A popular strategy for search result diversification is to first retrieve a set of documents utilizing a standard retrieval method and then rerank the results. We adopt a different perspective on the problem, based on data fusion. Starting from the hypothesis that data fusion can improve performance in terms of diversity metrics, we examine the impact of standard data fusion methods on result diversification. We take the output of a set of rankers, optimized for diversity or not, and find that data fusion can significantly improve state-of-the art diversification methods. We also introduce a new data fusion method, called diversified data fusion, which infers latent topics of a query using topic modeling, without leveraging outside information. Our experiments show that data fusion methods can enhance the performance of diversification and DDF significantly outperforms existing data fusion methods in terms of diversity metrics.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--retrieval models
Keywords
Data fusion; rank aggregation; diversification; ad hoc retrieval
1. INTRODUCTION
Search result diversification is widely being studied as a way of tackling query ambiguity. Instead of trying to identify the "correct" interpretation behind a query, the idea is to make the search results diversified so that users with different backgrounds will find at least one of these results to be relevant to their information need [2]. In contrast to the traditional assumption of independent document relevance, search result diversification approaches typically consider the relevance of a document in light of other retrieved documents [40]. Diversification models try to identify the probable "aspects" of the query and return documents for each aspect, thereby making the result list more diverse.
Data fusion approaches, also called rank aggregation approaches, consist in combining result lists in order to produce a new and hopefully better ranking [16, 42]. Here, results lists can be produced by
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '14, July 06­11, 2014, Gold Coast, QLD, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ... $15.00. http://dx.doi.org/10.1145/2600428.2609561

a wide range of ranking approaches, based, e.g., on different query or document representations. Data fusion methods can improve retrieval performance in terms of traditional relevance-oriented metrics like MAP and precision@k over the methods used to generate the individual result lists being fused [17, 26, 27, 49]. One reason is that retrieval approaches often return very different non-relevant documents, but many of the same relevant documents [49].
We examine the hypothesis that data fusion can improve performance in terms of diversity metrics by promoting aspects that are found in disparate ranked lists to the top of the fused list. Our first step in testing this hypothesis is to examine the impact of existing data fusion methods in terms of diversity scores when fusing ranked lists. We find that they tend to improve over individual component runs on nearly all of the diversity metrics that we consider: Prec-IA, MAP-IA, -NDCG, ERR-IA (all at rank 20).
Building on these findings we propose a new data fusion method, called diversified data fusion (DDF). Based on latent Dirichlet allocation (LDA), it operates on documents in the result lists to be fused, whether the result lists have been diversified or not. DDF infers latent topics, their probabilities of being relevant and a multinomial distribution of topics over the documents being fused. Thus, it integrates topic structure and rank information. DDF does not assume the explicit availability of query aspects, but infers these as well as the latent prior for a given query via the documents being fused. Experimental results show that DDF can aggregate result lists--whether produced by diversification or ad hoc retrieval models--and boost the diversity of the final fused list, outperforming state-of-the-art diversification methods and established data fusion methods, especially in terms of intent-aware precision metrics.
Our contributions in this paper can be summarized as follows:
i. We tackle the challenge of search result diversification in a novel way by using data fusion methods.
ii. We propose a novel data fusion method that aims at optimizing diversification measures and that proves to be especially effective in terms of intent-aware precision metrics.
iii. We analyze the effectiveness of data fusion for result diversification and find that our fusion method as well as other fusion methods can significantly outperform state-of-the-art diversification methods.
§2 discusses related work. §3 describes the fusion models that we use (old and new). §4 describes our experimental setup. §5 is devoted to our experimental results and we conclude in §6.
2. RELATED WORK
We distinguish between three directions of related work: search result diversification, data fusion, and latent topic modeling.

303

2.1 Search result diversification
Search result diversification is similar to ad hoc search, but differs in its judging criteria and evaluation measures [8, 12]. The basic premise in search result diversification is that the relevance of a set of documents depends not only on the individual relevance of its members, but also on how they relate to one another [2]. Ideally, users can find at least one relevant document to the underlying information need. Most previous work on search result diversification can be classified as either implicit or explicit [39, 41].
Implicit approaches to result diversification promote diversity by selecting a document that differs from the documents appearing before it in terms of vocabulary, as captured by a notion of document similarity, such as cosine similarity or Kullback-Leibler divergence. Carbonell and Goldstein [6] propose the maximal marginal relevance (MMR) method, which reduces redundancy while maintaining query relevance when selecting a document. Chen and Karger [7] describe a retrieval method incorporating negative feedback in which documents are assumed to be non-relevant once they are included in the result list, with the goal of maximizing diversity. Zhai et al. [51] present a subtopic retrieval model where the utility of a document in a ranking is dependent on other documents in the ranking and documents that cover many different subtopics of a query topic are found. Other implicit work includes, e.g., [1] where set-based recommendation of diverse articles is proposed. We also tackle the problem of search result diversification implicitly, but in a different way, i.e., by data fusion.
Explicit approaches to diversification assume that a set of query aspects is available and return documents for each of them. Past work has shown that explicit approaches are usually somewhat superior to implicit diversification techniques. Well-known examples include xQuAD [39], RxQuAD [45], IA-select [2], PM-2 [13], and, more recently, DSPApprox [14]. Instead of modeling a set of aspects implicitly, these algorithms obtain the set of aspects either manually, e.g., from aspect descriptions [8, 12], or they create them directly from, e.g., suggested queries generated by commercial search engines [13, 39] or predefined aspect categories [44]. We propose an implicit fusion-based diversification model where we do not assume that the aspects of the query are available but do assume that we can infer the underlying topics and the prior relevance of each topic for search result diversification.
2.2 Data fusion
A core concern in data fusion is how to assign a score to a document that appears in one of the lists to be fused [17, 19, 42, 49]. Most previous work on data fusion focuses on optimizing a traditional evaluation metric, like MAP, p@k and nDCG. Fusion approaches can be categorized into supervised or unsupervised: Supervised data fusion approaches, like -Merge [43], first extract a number of features, either from documents or lists, and then utilize a machine learning algorithm to train the fusion model [15, 17, 49].
In contrast, unsupervised data fusion methods mainly use either retrieval scores or ranks of documents in the lists to be merged, with the CombSUM family of fusion methods being the oldest and one of the most successful ones in many information retrieval tasks [26, 42]. State-of-the-art data fusion methods ClustFuseCombSUM and ClustFuseCombMNZ (both cluster-based methods) are proposed in [23]. Methods utilizing retrieval scores take score information from the lists to be fused as input, while those utilizing rank information only use order information of the documents appearing in the lists to be fused as input. Data fusion methods utilizing rank information have many uses and applications in information retrieval, including, e.g., expert search [30, 35], query reformulations [43], meta-search [4, 17] and microblog search [31, 32].

We do not make the assumption that labeled data is available but integrate standard unsupervised data fusion information into our diversified fusion model for search result diversification via a latent topic model.
2.3 Topic modeling
Topic models have been proposed for reducing the high dimensionality of words appearing in documents into low-dimensional "latent topics." From the first work on topic models [21], the Probablistic LSI model, topic models have received significant attention [5, 18, 22] and have proved to be effective in many information retrieval tasks [24, 47, 50]. Latent dirichlet allocation (LDA) [5] represents each document as a finite mixture over "latent" topics where each topic is represented as a finite mixture over words existing in that document. Based on LDA, many extensions have been proposed, e.g., to handle users' connections with particular documents and topics [37], to learn relations among different topics [25, 29], for topic over time [46], for dynamic mixture model [48], or tweet summarization [36]. LDA has also been extended to sentiment analysis [28]. We propose a novel topic model where fusion scores of each document appearing in lists to be fused are used to boost the performance of state-of-the-art diversification methods.
Our work adds the following to the work discussed above. We propose a fusion-based approach to the search result diversification task. We find that existing unsupervised fusion methods significantly outperform state-of-the-art diversification methods. In addition, we propose a novel fusion method, diversified data fusion, that uses the output of a fusion step and a topic modeling step as input to a diversification step. To the best of our knowledge, ours is the first attempt to utilize data fusion for diversification.
3. FUSION METHODS
We first review our notation and terminology. Then we introduce the task to be addressed, as well as the baseline fusion methods that we use in this paper plus a new fusion method.
3.1 Notation and terminology
We summarize the main notation used in this paper in Table 1. In the remainder, we distinguish between queries, aspects and topics. A query is an expression of an information need; in our experimental evaluation below, queries are provided as part of a TREC test collection. An aspect (sometimes called subtopic at the TREC Web track) is an interpretation of an information need. We use topic to refer to latent topics as identified by a topic modeling method, in our case LDA. A component list is a ranked list that serves as input for a data fusion method. A fused list is a list that is the result of applying a fusion method to component lists.
3.2 The diversified data fusion task
The diversified data fusion task that we address is this: given a query, an index of documents, and a set of ranked lists of documents produced in response to a query, aggregate the lists into a final result list where documents should be diversified. The component lists may or may not have been diversified themselves or ranked by relevance only.
The underlying data fusion problem consists of running a ranking function FX that satisfies:
L = {L1, L2, . . . , Lm}, q, C -FX Lf ,
where L is a set of components lists, m = |L| is their number, C the document corpus, q a query, and Lf the final fused list.

304

Table 1: Basic notation used in the paper.

Notation Gloss

C

document corpus

q

query

z

topic

d

document

w

a token

Nd

number of tokens in d

Li

i-th ranked list of documents

L

set of ranked lists to be fused

m

number of ranked lists to be fused, i.e., m = |L|

CL

set of documents that appear in the lists L

|CL|

number of documents in CL

FX

a data fusion method

FX(d; q) score of document d for query q according to a data fusion

method FX

RLi d

rank-based score of d in list Li

rank(d, Li) rank of d in list Li

|Li|

length of list Li

R

set of top ranked documents

qt[z|q]

quotient score for z given q in PM-2 algorithm [13]

vz|q

probability of z given q

sz|q

"portion" of seat occupied by z given q in PM-2



a free trade-off parameter in PM-2



the parameter of topic Dirichlet prior



the parameter of token Dirichlet prior

T

number of topics

V

number of unique tokens in CL

d

multinomial distribution of topics specific to d

z

multinomial distribution of tokens specific to topic z

µz

mean of Log-normal distribution of fusion scores for topic z

z

deviation of Log-normal distribution of fusion scores for z

zdi

topic associated with the i-th token in the document d

wdi

i-th token in document d

fdi

fusion score for token wdi

3.3 Baseline data fusion methods

Let RLid denote the score of document d based on the rank

of d in list Li; in the literature on data fusion, one often finds

RLid = 0 if d / Li (d still in the combined set of documents

CL :=

m i=1

Li).

In both CombSUM and CombMNZ, RLid is

often defined as:

RLid =

(1+|Li |)-rank(d,Li ) |Li |
0

d  Li d / Li,

(1)

where |Li| is the length of Li and rank(d, Li)  {1, . . . , |Li|} is the rank of d in Li. The well-known CombSUM fusion method [17, 49], for instance, scores d by the sum of its rank scores in the lists:

FCombSUM(d; q) := Li RLid, while CombMNZ [17, 49] rewards d that ranks high in many lists:

FCombMNZ(d; q) := |{Li : d  Li}| · FCombSUM(d; q),

where |{Li : d  Li}| is the number of lists in which d appears. We consider CombSUM, CombMNZ and two state-of-the-art
data fusion methods, ClustFuseCombSUM and ClustFuseCombMNZ [23], that integrate cluster information into CombSUM and CombMNZ, respectively, as baseline fusion methods.
In addition, a natural and direct way of diversifying a result list in the setting of data fusion is this: first rank the documents in the component lists by their estimated relevance to the query through a standard data fusion method, such as CombSUM, and then diversify the ranking through effective search result diversification models, such as MMR [6] and PM-2 [13]. In our experiments, we implement two more baselines, called CombSUMMMR and

CombSUMPM-2. They first use CombSUM to obtain a fused list and then use MMR and PM-2, respectively, to diversify the list.
3.4 Diversified data fusion
We propose a diversified data fusion (DDF) method that not only inherits the merits of traditional data fusion methods, i.e., it can improve the performance on relevance orientated metrics, but also considers a query as a compound rather than a single representation of an underlying information need, and regards documents appearing in the component lists as mixtures of latent topics.
3.4.1 Overview of DDF
DDF consists of three main parts: (I) perform standard data fusion; (II) infer latent topics; (III) perform diversification; see Algorithm 1. In the first part ("Part I" in Algorithm 1), DDF computes the fusion scores of the documents in the component lists based on an existing unsupervised data fusion method (steps 1 and 2 in Algorithm 1); in this paper we use CombSUM, as our experimental results in §5.1 and §5.2 show that CombSUM outperforms other plain fusion methods in most cases. In the second part ("Part II" in Algorithm 1), DDF integrates fusion scores into an LDA topic model such that latent topics of the documents, their corresponding estimated relevance scores, and the multinomial distribution of the topics specific to each document can be inferred (steps 3­15 in Algorithm 1). In the last part ("Part III" in Algorithm 1), DDF uses the outputs of Parts I and II as input for an existing diversification method; in this paper, we use PM-2 [13] because it is a the state-ofthe-art search result diversification model. Some concepts in PM-2, such as "quotient" and "seat," play important roles in the definition of the diversification step; they will be discussed in §3.4.3.
Below we describe how to infer latent topics ("Part II" in Algorithm 1) in §3.4.2 and how we utilize the information generated from latent topics and fusion scores ("Part III") in §3.4.3.
3.4.2 Part II: Inferring latent topics
Previous work on search result diversification shows that explicitly computing the probabilities of aspects of a query can improve diversification performance [1, 20, 39]. We do not assume that aspect information is explicitly available; we infer latent topics and their probabilities of being relevant using topic modeling.
Topic discovery in DDF is influenced not only by token co-occurrences, but also by the fusion scores of documents in the component lists. To avoid normalization and because fusion scores of the documents theoretically belong to (0, +), we employ a lognormal distribution for fusion scores to infer latent topics of the query via the documents and their relevance probabilities.
The latent topic model used in DDF is a generative model of relevance and the tokens in the documents that appear in the component individual lists. The generative process used in Gibbs sampling [34] for parameter estimation, is as follows:
i. Draw T multinomials z from a Dirichlet prior , one for each topic z;
ii. For each document d  CL, draw a multinomial d from a Dirichlet prior ; then for each token wdi in document d:
(a) Draw a topic zdi from multinomial d; (b) Draw a token wdi from multinomial zdi ; (c) Draw a fusion score fdi for wdi from Log-normal N (µzdi ,
zdi ).
Fig. 1 shows a graphical representation of our model. In the generative process, the fusion scores of tokens observed in the same document are the same and computed by a data fusion method, like

305

Algorithm 1: Diversified data fusion

Input : A query q

Ranked lists to be fused, L1, L2, . . . , Lm

The combined set of documents CL :=

m i=1

Li

A standard fusion method X

A tradeoff parameter 

Number of latent topics T

Hyperparameters , 

Output: A final fused diversified list of documents Lf .

/* Part I: Perform standard data fusion

*/

1 for d = 1, 2, . . . , |CL| do 2 Initialize FX(d|L, q) using a standard fusion method X

/* Part II: Infer latent topics

*/

3 Randomly initialize topic assignment for all tokens in w

4 for z = 1, 2, . . . , T do

5 Initialize µz and z randomly for topic z

6 for iter = 1, 2, . . . , Niter do

7 for d = 1, 2, . . . , |CL| do

8

for i = 1, 2, . . . , Nd do

9

draw zdi from P (zdi|w, r, z-di, , , µ, , L, q)

10

update nzdiwdi and mdzdi

11 for z = 1, 2, . . . , T do

12

update µz and z

13 Compute the posterior estimate of 

14 for z = 1, 2, . . . , T do

15

vz|q 

exp{uz

+

1 2

z2

}

T z

=1 exp{uz

+

1 2

2 z

}

/* Part III: Perform diversification

*/

16 Lf  

17 R  CL

18 for z = 1, 2, . . . , T do

19

sz|q  0

20 for all positions in the ranked list Lf do

21 for z = 1, 2, . . . , T do

22

qt[z|q]

=

vz|q 2sz|q +1

23

z  arg maxz qt[z|q]

24

d  arg maxdR  × qt[z|q] × P (d|z, q)+

25

(1 - ) z=z qt[z|q] × P (d|z, q)

26

Lf  Lf  {d}

27

R  R\{d}

/* append d to Lf */

28 for z = 1, 2, . . . , T do

29

sz|q  sz|q +

P (d|z,q) z P (d|z ,q)

CombSUM, for the document, although a fusion score is generated for each token from the log-normal distribution. We use a fixed number of latent topics, T , although a non-parametric Bayes version of DDF that automatically integrates over the number of topics would certainly be possible. The posterior distribution of topics depends on the information from two modalities--both tokens and the fusion scores of the documents.
Inference is intractable in this model. Following [18, 24, 34, 36, 46, 47, 50], we employ Gibbs sampling to perform approximate inference. We adopt a conjugate prior (Dirichlet) for the multinomial distributions, and thus we can easily integrate out  and , analytically capturing the uncertainty associated with them. In this way we facilitate the sampling, i.e., we need not sample  and  at all. Because we use the continuous log-normal distribution rather than discretizing fusion scores, sparsity is not a big concern in fitting the model. For simplicity and speed we estimate these log-normal distributions µ and  by the method of moments, once per iteration of Gibbs sampling (see the Appendix). We find that the sensitivity of the hyper-parameters  and  is limited. Thus, for simplicity,



q



L



z



T



w

f

µ

T

Nd

T

|CL|

Figure 1: DDF graphical model for Gibbs sampling.

we use fixed symmetric Dirichlet distributions ( = 50/T and  = 0.1) in all our experiments.
In the Gibbs sampling procedure above, we need to calculate the conditional distribution P (zdi|w, r, z-di, , , µ, , L, q) (step 9 in Algorithm 1), where z-di represents the topic assignments for all tokens except wdi. We begin with the joint probability of documents to be fused, and using the chain rule, we can obtain the conditional probability conveniently as

P (zdi|w, r, z-di, , , µ, , L, q) 

(mdzdi + zdi - 1) ×

nzdiwdi + wdi - 1

V v=1

(nzdi

v

+

v )

-

1

×

1  exp{- (ln FX(d|L, q) - µzdi )2 },

FX(d|L, q)zdi 2

2z2di

where nzv is the total number of tokens v that are assigned to topic

z, mdz represents the number of tokens in document d that are

assigned to topic z. An overview of the Gibbs sampling procedure

we use is shown from step 3 to step 12 in Algorithm 1; details are

provided in the Appendix.

One merit of our generative model for DDF is that we can predict

a fusion score for any document once the tokens in the document

have been observed. Given a document, we predict its fusion score

by choosing the discretized fusion score that maximizes the poste-

rior which is calculated by multiplying the fusion score probability

of all tokens from their corresponding topic-wise log-normal dis-

tributions, i.e., arg maxf

Nd i=1

p(f

|µzi

,

zi

).

More importantly, after the Gibbs sampling procedure, we can

easily infer the multinomial distribution of topics specific to each

document d  CL as (step 13 in Algorithm 1):

d,z =

nd,z + z

T z=1

(nd,z

+

z

)

,

(2)

where nd,z is the number of tokens assigned to latent topic z in document d; we can also conveniently estimate the probability of a topic being relevant to the query, denoted as vz|q, by (step 15 in Algorithm 1):

vz|q :=

E[f |z]

T z

=1

E[f

|z

]

=

T z

exp{uz + =1 exp{uz

1 2

z2}

+

1 2

z2

, }

(3)

where E denotes the expectation.

3.4.3 Part III: Diversification
In Part III of our DDF model we propose a modification of PM2. Before we discuss the details of this modification, we briefly describe PM-2. PM-2 is a probabilistic adaptation of the SainteLaguë method for assigning seats (positions in the ranked list) to

306

members of competing political parties (aspects) such that the number of seats for each party is proportional to the votes (aspect popularity, also called aspect probabilities, i.e., p(z|q)) they receive. PM-2 starts with a ranked list Lf with k empty seats. For each of these seats, it computes the quotient qt[z|q] for each topic z given q following the Sainte-Laguë formula:

qt[z|q] = vz|q ,

(4)

2sz|q + 1

where vz|q is the probability of topic z given q, i.e., the weight of topic z. According to the Sainte-Laguë method, this seat should
be awarded to the topic with the largest quotient in order to best
maintain the proportionality of the list. Therefore, PM-2 assigns the current seat to the topic z with the largest quotient. The document to fill this seat is the one that is not only relevant to z but to other
topics as well:

d = arg max  × qt[z|q] × P (d|z, q) +

(5)

dR

(1 - ) z=z qt[z|q] × P (d|z, q) ,

where P (d|z, q) is the probability of d talking about topic z for a given q. After the document d is selected, PM-2 increases the

"portion" of seats occupied by each of the topics z by its normalized relevance to d:

sz|q  sz|q +

P (d|z, q)

z

P (d|z

,

. q)

This process repeats until we get k documents for Lf or we are out of candidate documents. The order in which a document is appended to Lf determines its ranking.
We face two challenges in PM-2: it is non-trivial to get the aspect probability vz|q (i.e., p(z|q)), which is often set to be uniform, and it is non-trivial to compute p(d|z, q), which usually requires explicit access to additional information. To address the first challenge, we compute vz|q by (3), such that (4) can be modified as:

qt[z|q] =

p(z|q) =
2sz|q + 1

exp{uz

+

1 2

z2

}

(2sz|q + 1)

T z

=1

exp{uz

.

+

1 2

z2

}

For the second challenge, instead of computing P (d|z, q) explicitly, we modify P (d|z, q) and apply Bayes' Theorem so that

p(z|d, q)p(d|q) p(z|d, q)p(d|q)

P (d|z, q) =

=

. (6)

p(z|q)

vz|q

Then we integrate the fused score generated by CombSUM into our model, i.e., we set

p(d|q) ra=nk FCombSUM(d; q)

in (6). As a result, after applying (6) to (5), DDF selects a candidate document by:

d = arg max  · qt[z|q] · p(z|d, q) · FCombSUM(d; q) +

dR

vz |q

(7)

(1 - )

z=z

qt[z|q]

·

, p(z|d,q)·FCombSUM (d;q)
vz|q

where p(z|d; q) is the probability of document d belonging to topic z, which can easily be inferred in our DDF model by (2) (i.e., p(z|d, q) = d,z). Therefore, after applying (2) and (3), (7) can be rewritten as:

d

= arg max
dR

 · qt[z|q] ·

d,z · FCombSUM(d;

exp{µz

+

1 2

z2

}

q

)

+

(8)

(1 - )

z=z

qt[z|q]

·

, d,z ·FCombSUM(d;q)

exp{µz

+

1 2

z2

}

where it should be noted that we ignore the constant term

T z=1

exp{µz

+

1 2

z2

},

as it has no impact on selecting the candidate document d.

4. EXPERIMENTAL SETUP
In this section, we describe our experimental setup; §4.1 lists our research questions; §4.2 describes our data set; §4.3 lists the metrics and the baselines; §4.4 details the settings of the experiments.
4.1 Research questions
The research questions guiding the remainder of the paper are:
RQ1 Do fusion methods help improve state-of-the-art search diversification methods? Do they help in terms of intent-aware precision, as our main metric? Does DDF beat standard and state-of-the-art fusion methods? (See §5.1 and §5.2.)
RQ2 What is the effect on the diversification performance of DDF and fusion methods of the number of component lists? Does the contribution of fusion to diversification performance depend on the quality of the component lists? (See §5.3)
RQ3 Does DDF outperform the best diversification and fusion methods on each query? (See §5.4.)
RQ4 How do the rankings of DDF differ from those produced by other fusion methods? (See §5.5.)
RQ5 What is the effect on the diversification performance of DDF of the number of latent topics used by DDF? (See §5.6.)
4.2 Data set
In order to answer our research questions we work with the runs submitted to the TREC 2009, 2010, 2011 and 2012 Web tracks, and the billion-page ClueWeb09 collection.1 There are two tasks in these tracks: an ad hoc search task and a search result diversification task [8, 10­12]. We only focus on the diversification task, where the top-k documents returned should not only be relevant but also cover as many aspects as possible in response to a given query. In total, we have 200 ambiguous queries from the four years, with 2 queries (#95 and #100 in the 2010 edition) not having relevant documents. Typically, each query has 2 to 5 aspects, and some relevant documents are relevant to more than 2 aspects of the query.
Many of the runs submitted to these four years of the Web track for the diversification task were generated by state-of-the-art diversification methods. In total, we have 119, 88, 62 and 48 runs from the 2009, 2010, 2011 and 2012 editions, respectively.2
4.3 Evaluation metrics and baselines
We evaluate our component runs and fused runs using several standard metrics that are official evaluation metrics in the diversification tasks at TREC Web tracks [8, 10­12] and are widely used in the literature on search result diversification [2, 3, 13, 14, 38, 40]: Prec-IA@k [2], MAP-IA@k [2], ERR-IA@k [2] and nDCG@k [9]. The former two are set-based and indicate, respectively, the precision and mean average precision across all aspects of the query in the search results, whereas the remaining ones are cascade measures that penalize redundancy at each position in the ranked list based on how much of that information the user has already seen from documents at earlier ranks.
1Available from http://boston.lti.cs.cmu.edu/ Data/clueweb09.
2All runs are available from http://trec.nist.gov.

307

We follow published work on search result diversification and mainly compute the metric scores at depth 20. Statistical significance of observed differences between the performance of two runs is tested using a two-tailed paired t-test and is denoted using (or
) for significant differences for  = .01, or (and ) for  = .05. When assessing a fusion method X we will prefer fusion methods that are safe, where we say that X is safe for metric M if applying X to a set of component runs always yields a fused run that scores at least as high as the highest scoring component run in the set (according to M ). We consider several baselines. Two standard fusion methods [26], CombSUM and CombMNZ; two state-of-the-art fusion methods [23], ClustFuseCombSUM and ClustFuseCombMNZ; each year's best performing runs in the diversification tasks at the TREC Web track [8, 10­12], and state-of-the-art plain diversification methods, xQuAD [39] and PM-2 [13]. As DDF builds on both fusion and diversification methods, we also consider two fusion methods, CombSUMMMR and CombSUMPM-2, that integrate plain diversification methods MMR [6] and PM-2 into CombSUM for diversification, respectively.
4.4 Experiments
We report on five main experiments aimed at answering the research questions listed in §4.1. In our first experiment, aimed at determining whether fusion methods help diversification, we fuse the five top performing diversification result lists from the TREC Web 2009, 2010, 2011 and 2012 submitted runs (some lists are generated by the implementation of PM-2) by our baselines, viz., CombSUM, CombMNZ, ClustFuseCombSUM, ClustFuseCombMNZ, CombSUMMMR and CombSUMPM-2 (see §4.3). The performance of the baselines is compared against that of DDF.
Our second experiment is aimed at understanding the effect on the diversification performance of DDF and fusion methods of the number of component lists; we randomly sample k  {2, 4, . . . , 26} component runs from the submitted runs in the TREC Web 2012 track and fuse them. We repeat the experiments 20 times and report the average results and the standard deviations. We also show one sample's result when fusing 4 runs.
Next, in order to understand how DDF outperforms the best component run and the fusion methods per query, our third experiment provides a query-level analysis. Our fourth experiment is aimed at understanding how the runs generated by DDF differ from those produced by other fusion methods; we zoom in on the differences between DDF and the next best performing fusion method, CombSUMPM-2, in terms of the documents (and aspects) retrieved by one, but not the other, or by both.
Finally, to understand the influence of the number of latent topics used in DDF, we vary the number of latent topics and assess the performance of DDF. We also use an oracle variant of DDF, called DDF2, where for every test query we consider as many latent topics as there are aspects according to the ground truth. The number of topics used in DDF is set to 10, unless stated otherwise.
5. RESULTS
In §5.1 we examine the performance of baseline fusion methods on the diversification task, which we follow with a section on the performance of DDF in §5.2. §5.3 details the effect of the number of lists; §5.4 provides a query-level analysis; §5.5 zooms in on the effect on ranking of DDF compared to the next best fusion method; §5.6 examines the effect of the number of latent topics on DDF.
5.1 Performance of baseline fusion methods
In Table 2 we list the diversity scores of the baseline fusion

methods on the diversity task: CombSUM, CombMNZ, ClustFuseCombSUM, ClustFuseCombMNZ, CombSUMMMR, CombSUMPM-2, with the 5 best performing component lists from the TREC Web 2009, 2010, 2011 and 2012 tracks, respectively.3 For all metrics and in all years, almost all baseline fusion methods outperform the state-of-the-art diversification methods, and in many cases significantly so. Note, however, that none of the baseline methods is safe in the sense defined in §4.3. Additionally, Table 3 shows the diversity scores of the baseline fusion methods when we fuse 4 randomly sampled runs from the 2012 data set, which confirms that fusion does help diversification.
5.2 The performance of DDF
Inspired by the success of baseline fusion methods on the diversification task, we now consider our newly proposed fusion method, DDF. Returning to Tables 2 and 3, two types of conclusion emerge. First, DDF outperforms all component runs (note that component runs in Table 2 are the best runs in the tracks), on all metrics, for all years. In other words, it is safe in the sense defined in Section 4.3. The difference between DDF and the best performing component run is always significant. We believe that the strong performance of DDF is due to the fact that DDF not only focuses on improving the relevance score of fused run but also explicitly tries to diversify the fused run.
Second, DDF outperforms all baseline fusion methods, on all metrics. In many cases, CombSUMPM-2 and CombSUM yield the second and third best performance, respectively, but DDF outperforms them in every case, and often significantly so. DDF can beat CombSUMPM-2 as it tackles two main challenges in PM-2 (see §3.4.3), although they build on the same framework. CombSUMMMR follows a similar strategy as DDF but its performance is worse than that of DDF. This is due to the fact that MMR models documents as if they are centered around a single topic only. It is clear from Tables 2 and 3 that cluster-based data fusion methods (ClustFuseCombSUM, ClustFuseCombMNZ) sometimes perform a little worse than the standard fusion method they build on (CombSUM, CombMNZ). This is because cluster-based fusion focuses on relevance of the documents rather than on diversification.
5.3 Effect of the number of component lists
Next, we zoom in on DDF. In particular, we explore the effect of varying the number of lists to be fused on its performance. Fig. 2 shows the fusion results of randomly sampling k  {2, 4, . . . , 26} lists from the 48 runs submitted to the TREC Web 2012 track plus the PM-2 runs (due to space limitations, we only report results using the 2012 runs; the findings on other years are qualitatively similar). For each k, we repeat the experiment 20 times and report on the average scores and the corresponding standard deviations indicated by the error bars in the figure. We use CombSUM as a representative example for comparison with DDF, as the results of other baseline fusion methods are worse or have qualitatively similar results to those of CombSUM. As shown in Fig. 2, DDF always outperforms CombSUM in terms of the Prec-IA, -nDCG and ERRIA evaluation metrics and the performance gaps remain almost unchanged, in absolute terms, no matter how many component lists are fused. One reason for this is that as DDF builds on CombSUM, it inherits the merits of the fusion method, and more importantly, at the same time it tries to infer latent topics and rerank the high
3The run "PM-2 (TREC)" is the run that utilizes aspect information from the ground truth in the PM-2 model and the run "PM-2 (engine)" is produced using information from a commercial search engine. The run "xQuAD (uogTrX)" is a uogTrX TREC edition run generated using the xQuAD algorithm; see [33].

308

Table 2: Performance obtained using the 2009­2012 editions of the TREC Web tracks. The best performing run per metric per year is in boldface. Statistically significant differences between fusion method and the best component run, between DDF and CombSUM, and between DDF and CombSUMPM-2, are marked in the upper right hand corner of the fusion method score, in the upper left hand corner of DDF's score, and in the lower left hand corner of DDF's score, respectively.

Prec-IA MAP-IA -nDCG ERR-IA

2012 DFalah120A DFalah120D xQuAD (uogTrA44xi) xQuAD (uogTrA44xu) xQuAD (uogTrB44xu) ClustFuseCombMNZ ClustFuseCombSUM CombSUMMMR CombSUMPM-2 CombMNZ CombSUM DDF

.3241 .3241 .3349 .3504 .3389 .3533 .3545 .3558 .3718 .3663 .3592 .3904

.0990 .0990 .1345 .1360 .1339 .1488 .1495 .1544 .1826 .1785 .1767 .1910

.5291 .5291 .5917 .6061 .5795 .6010 .5965 .6106 .6228 .6154 .6114 .6334

.4259 .4259 .4873 .5048 .4785 .5105 .5049 .5115 .5179 .5153 .5126 .5266

2011 ICTNET11ADR2 umassGQdist xQuAD (uogTrA45Nmx2) xQuAD (uogTrA45Vmx) UWatMDSdm ClustFuseCombMNZ ClustFuseCombSUM CombSUMMMR CombSUMPM-2 CombMNZ CombSUM DDF

.2993 .3003 .3039 .3030 .3214 .3303 .3296 .3395 .3450 .3413 .3376 .3596

.1328 .1313 .1365 .1323 .1350 .1757 .1775 .1830 .2024 .1943 .1966 .2102

.5725 .5513 .6298 .6304 .5979 .6221 .6307 .6341 .6448 .6430 .6423 .6496

.4658 .4530 .5284 .5238 .4875 .5001 .5110 .5107 .5196 .5209 .5216 .5295

2010 CSE.pm2.run cmuWi10D xQuAD (uogTrA42x) PM-2 (engine) PM-2 (TREC) ClustFuseCombMNZ ClustFuseCombSUM CombSUMMMR CombSUMPM-2 CombMNZ CombSUM DDF

.1832 .1879 .1845 .2009 .2026 .2105 .2072 .2115 .2129 .2177 .2159 .2285

.0351 .0599 .0529 .0414 .0430 .0845 .0825 .0836 .0839 .0899 .0875 .0910

.4165 .3452 .3558 .3660 .4449 .4313 .4257 .4366 .4379 .4471 .4454 .4627

.3052 .2484 .2454 .2581 .3320 .3221 .3148 .3189 .3193 .3411 .3350 .3406

2009 NeuDiv1 NeuDivW75 xQuAD(uogTrDPCQcdB) xQuAD (uogTrDYCcsB) uwgym ClustFuseCombMNZ ClustFuseCombSUM CombSUMMMR CombSUMPM-2 CombMNZ CombSUM DDF

.1343 .1239 .1302 .1268 .1224 .1381 .1379 .1424 .1588 .1400 .1400 .1631

.0458 .0397 .0463 .0444 .0456 .0681 .0680 .0682 .0754 .0666 .0664 .0731

.2781 .2501 .2968 .3081 .2798 .3076 .3223 .3343 .3887 .3343 .3482 .4005

.1705 .1598 .1848 .1922 .1701 .1937 .2005 .2028 .2674 .2033 .2080 .2713

ranked documents in terms of novelty of the documents. For the MAP-IA metric, however, the gaps increase with more component lists being fused. The performance of both DDF and CombSUM increases faster when the number of component lists increases but is  10 than when the number of component lists is > 10, for all the metrics. This seems to be inherent to the underlying CombSUM method and is due to the fact that with smaller numbers of component lists, there is simply more space available at depth 20 to obtain improvements than with larger numbers of component lists.

Table 3: Performance obtained using the 2012 editions of the TREC Web track. The best performing run per metric is in boldface. Other notational conventions as in Table 2.
Prec-IA MAP-IA -nDCG ERR-IA

2012 QUTparaBline xQuAD (uogTrA44xl) utw2012c1 PM-2 (TREC) ClustFuseCombMNZ ClustFuseCombSUM CombSUMMMR CombSUMPM-2 CombMNZ CombSUM DDF

.2261 .2957 .1637 .2631 .2735 .2752 .2783 .2934 .2864 .2884 .3193

.0639 .1077 .0439 .0601 .1155 .1172 .1189 .1305 .1267 .1275 .1409

.5270 .5161 .5075 .5245 .5717 .5726 .5799 .6013 .5851 .5944 .6107

.4185 .4009 .4046 .4155 .4608 .4674 .4633 .4877 .4708 .4803 .4919

5.4 Query-level analysis
We take a closer look at per test query improvements of DDF over the best baseline fusion run when fusing the best 5 runs in 2012, viz., CombSUMPM-2, which outperforms the best component list. Fig. 3 shows the per query performance differences in terms of Prec-IA, MAP-IA, -nDCG and ERR-IA, respectively, of DDF against CombSUMPM-2. DDF achieves performance improvements for many queries when compared against CombSUMPM-2, although the differences are sometimes relatively small.
In a very small number of cases, DDF performs poorer than CombSUMPM-2. This appears to be due to the fact that DDF "over-diversifies" documents in runs produced by CombSUM that have very few relevant document to start with, so that DDF ends up promoting different but non-relevant documents.
5.5 Zooming in on Prec-IA@k
Next, we zoom in on one of the metrics that shows the biggest relative differences between DDF and the next best performing fusion method, Prec-IA, so as to understand how the runs generated by DDF differ from those by other fusion-based methods. Here, again, we use CombSUMPM-2 as a representative, as it tends to outperform or equal the other fusion methods. Specifically, we report changes in the number of relevant documents for DDF against CombSUMPM-2 when fusing the 2012 runs in Table 2 in 2012; see Fig. 4. Red bars indicate the number of relevant documents that appear in the run of DDF but not the run of CombSUMPM-2, white bars indicate the number of relevant documents in both runs, whereas blue bars indicate the number of relevant documents that appear not in DDF but in CombSUMPM-2; topics are ordered first by the size of the red bar, then the size of the white bar, and finally the size of the blue bar.
Clearly, the differences between DDF and CombSUMPM-2 in the top 5 and 10 are more limited than the differences in the top-15 and 20, but in all cases DDF outperforms CombSUMPM-2. E.g., in total there are 45 more relevant documents in the top 20 of the run produced by DDF than those in the CombSUMPM-2 run (49 relevant documents in DDF but not in CombSUMPM-2, 4 relevant documents in CombSUMPM-2 but not in DDF). We examine the matter further by comparing the Prec-AI@5, 10, 15, 20 scores of the DDF and CombSUMPM-2 runs for the 2012 data; see Table 4. The differences at small depths (5, 10) are weakly statistically significant while those at bigger depths are significant, confirming our observations in Fig. 4; we also find that DDF statistically significantly outperforms CombSUMPM-2 in terms of Prec-IA scores at depth 5, 10, 15 and 20, which again confirms the above observations based on Fig. 4.

309

Prec-IA

0.35 0.3
0.25 0.2 0

DDF CombSUM

10

20

30

Number of runs to be fused

MAP-IA

0.3 0.25
0.2 0.15
0.1 0.05
0

0.59

0.55

-nDCG

0.51

DDF CombSUM

10

20

30

Number of runs to be fused

0.47 0.43
0

0.5

0.45

ERR-IA

0.4

DDF CombSUM

10

20

30

Number of runs to be fused

0.35 0

DDF CombSUM

10

20

30

Number of runs to be fused

Figure 2: Effect on performance (in terms of Prec-IA, MAP-IA, -nDCG and ERR-IA) of the number of component lists, using runs sampled from the TREC 2012 Web track. We plot averages and standard deviations. Note: the figures are not to the same scale.

0.2

0.2

0.5

0.2

ERR-IA

-nDCG

MAP-IA

Prec-IA

0

0

0

0

-0.2 queries

-0.2 queries

-0.5 queries

-0.2 queries

Figure 3: Per query performance differences of DDF against CombSUMPM-2 (second row). The figures shown are for fusing the runs in TREC Web 2012 track, for Prec-IA@20, MAP-IA@20, -nDCG@20 and ERR-IA@20 (from left to right). A bar extending above the center of a plot indicates that DDF outperforms CombSUMPM-2, and vice versa for bars below the center.

Table 4: Prec-IA@5, 10, 15, 20 performance comparison between CombSUMPM-2 and DDF. A statistically significant difference between DDF and CombSUMPM-2 is marked in the upper left hand corner of the DDF score.

Prec-IA@

5

10 15 20

CombSUMPM-2 .4367 .4066 .3887 .3718

DDF

.4555 .4194 .4060 .3904

5.6 Effect of the number of topics
Finally, we examine the effect on the overall performance of the number of latent topics used in DDF, and contrast the performance of DDF with varying number of latent topics against DDF2, CombSUM and CombSUMPM-2. Here, DDF2 is the same algorithm as DDF except that for every test query it considers as many latent topics as there are aspects according to the ground truth. We use DDF2, DDF, CombSUM and CombSUMPM-2 to fuse the component result runs listed in Table 2 in 2012 as an example. We vary the number of latent topics in DDF from 2 to 16. See Fig. 5.
When the number of latent topics used in DDF increases from 2 to 6, the performance of DDF increases dramatically. When only 2 latent topics are used, the performance is worse than that of CombSUM and CombSUMPM-2; e.g., Prec-IA@20 for DDF is 0.3404, while the scores of CombSUM and CombSUMPM-2 are 0.3592 and 0.3718, respectively. In contrast, when the number of latent topics varies between 8 to 16, the performance of DDF seems to level off. This demonstrates another merit of our fusion model, DDF: it is robust and not sensitive to the number of latent topics once the number of latent topics is "large enough." Another important finding from Fig. 5 is that DDF2 always enhances the performance of DDF, CombSUM and CombSUMPM-2, for all metrics, which demonstrates the fact that latent topics can enhance the performance. The performance differences between DDF2 and DDF are quite marginal and not statistically significant. We leave it as future work to dynamically estimate the number of aspects (and latent topics) of an incoming query and to use this estimate in DDF.

6. CONCLUSION
Most previous work on search result diversification focuses on

the content of the documents returned by an ad hoc algorithm to diversify the results implicitly or explicitly, i.e., using implicit or explicit representations of aspects. In this paper we have adopted a different perspective on the search result diversification problem, based on data fusion. We proposed to use traditional unsupervised and state-of-the-art data fusion methods, CombSUM, CombMNZ, ClustFuseCombSUM, ClustFuseCombMNZ, CombSUMMMR and CombSUMPM-2 to diversify result lists. This led to the insight that fusion does aid diversification. We also proposed a fusion-based diversification method, DDF, which infers latent topics from ranked lists of documents produced by a standard fusion method, and combines this with a state-of-the-art result diversification model. We found that data fusion approaches outperform state-of-the-art search result diversification algorithms, with DDF invariably giving rise to the highest scores on all of the metrics that we have considered in this paper. DDF was shown to behave well with different numbers of component lists. We also found that DDF is insensitive to the number of latent topics of a query, once a sufficiently large number was chosen, e.g., 10.
As to future work, we aim to incorporate into DDF methods for automatically estimating the number of aspects, which will be used to set the number of latent topics. The last and third part of DDF is based on a particular choice of method, viz. PM-2, and we only apply rank-based fusion methods for diversification. In future work we plan to compare these choices with alternative choices, and apply other fusion alternatives, e.g., score-based fusion methods.
Acknowledgements. We thank Van Dang for generating the PM-2 runs for us. This research was partially supported by the China Scholarship Council, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreements nrs 288024 and 312827, the Netherlands Organisation for Scientific Research (NWO) under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, the Center for Creation, Content and Technology (CCCT), the QuaMerdes project funded by the CLARIN-nl program, the TROVe project funded by the CLARIAH program, the Dutch national program COMMIT, the ESF Research Network Program ELIAS, the Elite Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), the Netherlands eScience Center under project number 027.012.105, the Yahoo! Faculty Research and Engagement Program, the Microsoft Research PhD program, and the HPC Fund.
7. REFERENCES
[1] S. Abbar, S. Amer-Yahia, P. Indyk, and S. Mahabadi.

310

number

Top 5 documents 6
4
2

number

Top 10 documents 10
5

number

Top 15 documents 15
10
5

number

Top 20 documents 20 15 10
5

00

10 20 30 40 50

queries

00

10 20 30 40 50

queries

00

10 20 30 40 50

queries

00

10 20 30 40 50

queries

Figure 4: How runs produced by DDF and CombSUMPM-2 differ. Red, white, blue bars indicate the number of relevant documents that appear in DDF but not in CombSUMPM-2, in both runs and not in DDF but in CombSUMPM-2, respectively, at corresponding depth k (for k = 5, 10, 15, 20). Figures should be viewed in color.

Prec-IA

0.4

0.35 0.3 0

DDF2 DDF CombSUMPM-2 CombSUM

5

10

15

Number of topics

MAP-IA

0.19 0.18 0.17 0.16
0

DDF2 DDF CombSUMPM-2 CombSUM

5

10

15

Number of topics

-nDCG

0.64

0.62 0.6
0.58 0

DDF2 DDF CombSUMPM-2 CombSUM

5

10 15

Number of topics

ERR-IA

0.54 0.52
0.5 0.48
0

DDF2 DDF CombSUMPM-2 CombSUM

5

10

15

Number of topics

Figure 5: Performance comparison between DDF2, DDF, CombSUMPM-2 and CombSUM when varying the number of latent topics used in DDF. Note: the figures are not to be the same scale.

Real-time recommendation of diverse related articles. In WWW, pages 1­12, 2013. [2] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In WSDM, pages 5­14, 2009. [3] E. Aktolga and J. Allan. Sentiment diversification with different biases. In SIGIR, pages 593­600, 2013. [4] J. A. Aslam and M. Montague. Models for metasearch. In SIGIR'01, pages 276­284, 2001. [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993­1022, 2003. [6] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR, pages 335­336, 1998. [7] H. Chen and D. R. Karger. Less is more: probabilistic models for retrieving fewer relevant documents. In SIGIR, pages 429­436, 2006. [8] C. L. A. Clarke and N. Craswell. Overview of the TREC 2011 web track. In TREC, pages 1­9, 2011. [9] C. L. A. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Büttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR, pages 659­666, 2008. [10] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 web track. In TREC, pages 1­9, 2009. [11] C. L. A. Clarke, N. Craswell, I. Soboroff, and G. V. Cormack. Overview of the TREC 2010 web track. In TREC, pages 1­9, 2010. [12] C. L. A. Clarke, N. Craswell, and E. M. Voorhees. Overview of the TREC 2012 web track. In TREC, pages 1­8, 2012. [13] V. Dang and W. B. Croft. Diversity by proportionality: An election-based approach to search result diversification. In SIGIR, pages 65­74, 2012. [14] V. Dang and W. B. Croft. Term level search result diversification. In SIGIR, pages 603­612, 2013. [15] M. Efron. Information search and retrieval in microblogs. J. Am. Soc. for Inform. Sci. and Techn., 62(6):996­1008, 2011. [16] M. Farah and D. Vanderpooten. An outranking approach for rank aggregation in information retrieval. In SIGIR'07, 2007.

[17] E. A. Fox and J. A. Shaw. Combination of multiple searches. In TREC-2, 1994.
[18] T. L. Griffiths and M. Steyvers. Finding scientific topics. PNAS, 101:5228­5235, 2004.
[19] D. He and D. Wu. Toward a robust data fusion for document retrieval. In IEEE NLP-KE'08, 2008.
[20] J. He, V. Hollink, and A. de Vries. Combining implicit and explicit topic representations for result diversification. In SIGIR, pages 851­860, 2012.
[21] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR, pages 50­57, 1999.
[22] O. Jin, N. N. Liu, K. Zhao, Y. Yu, and Q. Yang. Transferring topical knowledge from auxiliary long texts for short text clustering. In CIKM, pages 775­784, 2011.
[23] A. K. Kozorovitsky and O. Kurland. Cluster-based fusion of retrieved lists. In SIGIR'11, pages 893­902, 2011.
[24] T. Kurashima, T. Iwata, T. Hoshide, N. Takaya, and K. Fujimura. Geo topic model: joint modeling of user's activity area and interests for location recommendation. In WSDM, pages 375­384, 2013.
[25] J. D. Lafferty and D. M. Blei. Correlated topic models. In NIPS'05, pages 147­154, 2005.
[26] J. H. Lee. Combining multiple evidence from different properties of weighting schemes. In SIGIR'95, pages 180­188, 1995.
[27] J. H. Lee. Analyses of multiple evidence combination. In SIGIR, 1997.
[28] F. Li, M. Huang, and X. Zhu. Sentiment analysis with global topics and local dependency. In AAAI, 2010.
[29] W. Li and A. McCallum. Pachinko allocation: Dag-structured mixture models of topic correlations. In ICML, pages 577­584. ACM, 2006.
[30] S. Liang and M. de Rijke. Finding knowledgeable groups in enterprise corpora. In SIGIR'13, pages 1005­1008, 2013.
[31] S. Liang, M. de Rijke, and M. Tsagkias. Late data fusion for microblog search. In ECIR'13, pages 743­746, 2013.
[32] S. Liang, Z. Ren, and M. de Rijke. The impact of semantic document expansion on cluster-based fusion for microblog

311

search. In ECIR'14, pages 493­499, 2014. [33] N. Limsopatham, R. McCreadie, and M.-D. Albakour.
University of Glasgow at TREC 2012: Experiments with Terrier in medical records, microblog, and web tracks. In TREC, 2012. [34] J. S. Liu. The collapsed gibbs sampler in bayesian computations with applications to a gene regulation problem. J. Am. Stat. Assoc., 89(427):958­966, 1994. [35] C. Macdonald and I. Ounis. Voting for candidates: Adapting data fusion techniques for an expert search task. In CIKM, 2006. [36] Z. Ren, S. Liang, E. Meij, and M. de Rijke. Personalized time-aware tweets summarization. In SIGIR, 2013. [37] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. The author-topic model for authors and documents. In UAI, pages 487­494, 2004. [38] T. Sakai, Z. Dou, and C. L. A. Clarke. The impact of intent selection on diversified search result. In SIGIR, 2013. [39] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In WWW, pages 881­890, 2010. [40] R. L. Santos, C. Macdonald, and I. Ounis. Intent-aware search result diversification. In SIGIR, pages 595­604, 2011. [41] R. L. T. Santos, J. Peng, C. Macdonald, and I. Ounis. Explicit search result diversification through sub-queries. In ECIR, 2010. [42] J. A. Shaw and E. A. Fox. Combination of multiple searches. In TREC 1992, pages 243­252. NIST, 1993. [43] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. LambdaMerge: merging the results of query reformulations. In WSDM, pages 795­804, 2011. [44] I. Szpektor, Y. Maarek, and D. Pelleg. When relevance is not enough: promoting diversity and freshness in personalized question recommendation. In WWW '13, 2013. [45] S. Vargas, P. Castells, and D. Vallet. Explicit relevance models in intent-oriented information retrieval diversification. In SIGIR, pages 75­84, 2012. [46] X. Wang and A. McCallum. Topics over time: a non-markov continuous-time model of topical trends. In KDD'06, pages 424­433, 2006. [47] X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In SIGIR, pages 178­185, 2006. [48] X. Wei, J. Sun, and X. Wang. Dynamic mixture models for multiple time-series. In IJCAI, pages 2909­2914, 2007. [49] S. Wu. Data fusion in information retrieval, volume 13 of Adaptation, Learning and Optimization. Springer, 2012. [50] Z. Xu, Y. Zhang, Y. Wu, and Q. Yang. Modeling user posting behavior on social media. In SIGIR, pages 545­554, 2012. [51] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR, pages 10­17, 2003.

APPENDIX Gibbs sampling derivation for DDF model
We begin with the joint distribution P (w, f , z|, , µ, , L) and use conjugate priors to simplify the integrals. Notation defined in §3.
P (w, f , z|, , µ, , L, q) = P (w|z, )p(f |µ, , z, L)P (z|)
= P (w|, z)p(|)d × p(f |µ, , z, L, q) P (z|)P (|)d

|CL| Nd

T

=

P (wdi|zdi ) p(z |)d

d=1 i=1

z=1

|CL| Nd

×

p(fdi|µzdi , zdi , L, q)

d=1 i=1

|CL|  Nd



×

 P (zdi|d)p(d|) d

d=1 i=1

=

TV

T

nzvzv

z=1 v=1

z=1

(

V v=1

v

)

V v=1

(v )

V v=1

zvv -1

d

|CL| Nd

×

p(fdi|µzdi , zdi , L, q)

d=1 i=1

×

|CL| T

|CL |

dmzdz

d=1 z=1

d=1

(

T z=1

z

)

T z=1

(z

)

T z=1

dzz -1

d

=

(

V v=1

v

)

T

V v=1

(v

)

(

T z=1

z )

T z=1

(z

)

|CL |

|CL| Nd

×

p(fdi|µzdi , zdi , L, q)

d=1 i=1

×

T z=1

(

V v=1

(nzv

V v=1

(nzv

+ +

v ) v ))

|CL | d=1

(

T z=1

(mdz

T z=1

(mzd

+ +

z ) z ))

Using the chain rule, we can obtain the conditional probability conveniently,

P (zdi|w, f , z-di, , , µ, , L, q)

= P (zdi, wdi, fdi|w-di, f-di, z-di, , , µ, , L, q) P (wdi, fdi|w-di, f-di, z-di, , , µ, , L, q)

P (w, f , z|, , µ, , L, q) =
P (w, f , z-di|, , µ, , L, q)

because zdi depends only on wdi and fdi

P (w, f , z|, , µ, , L, q) 
P (w-di, f-di, z-di, |, , µ, , L, q)



(mdzdi + zdi - 1)

nzdiwdi + wdi - 1

V v=1

(nzdi

v

+

v )

-

1

×

1 

exp{- (ln fdi - µzdi )2 }

fdizdi 2

2z2di



(mdzdi + zdi - 1)

nzdiwdi + wdi - 1

V v=1

(nzdi

v

+

v )

-

1

×

1

 exp{- (ln FX(d|L, q) - µzdi )2 },

FX(d|L, q)zdi 2

2z2di

where FX(d|L, q)  (0, +) is a fusion score generated by a standard fusion method FX for document d  CL given the observation of lists L to be merged and query q. We use FCombSUM(d|L, q).
Since the data fusion score of a token that appears in d when fusing all
the lists in L given a query q and the latent topics of which is zdi, is drawn from log-normal distributions, sparsity is not a big problem for parameter
estimation of both µzdi and zdi . For simplicity, we update both µzdi and zdi after each Gibbs sample iteration by maximum likelihood estimation:

µ^zdi = =
^z2di = =

|CL | d =1
|CL | d =1
|CL | d =1
|CL | d =1

Nd i (zd

i

=zdi) ln fd

i

nzdi

Nd i (zd

i

=zdi )

ln FX(d

|L, q)

nzdi

Nd i (zd

i

=zdi)(ln fd

i

- µ^)2

nzdi

Nd i (zd

i

=zdi)(ln FX(d

|L, q) - µ^)2

nzdi

312

Utilizing Relevance Feedback in Fusion-Based Retrieval

Ella Rabinovich
IBM Research Labs, Haifa
Israel
ellak@il.ibm.com

Ofri Rom
Thomson Reuters
ofri.rom@ thomsonreuters.com

Oren Kurland
Faculty of IE&M, Technion
Israel
kurland@ie.technion.ac.il

ABSTRACT
Work on using relevance feedback for retrieval has focused on the single retrieved list setting. That is, an initial document list is retrieved in response to the query and feedback for the most highly ranked documents is used to perform a second search. We address a setting wherein the list for which feedback is provided results from fusing several intermediate retrieved lists. Accordingly, we devise methods that utilize the feedback while exploiting the special characteristics of the fusion setting. Specifically, the feedback serves two different, yet complementary, purposes. The first is to directly rank the pool of documents in the intermediate lists. The second is to estimate the effectiveness of the intermediate lists for improved re-fusion. In addition, we present a meta fusion method that uses the feedback for these two purposes simultaneously. Empirical evaluation demonstrates the merits of our approach. As a case in point, the retrieval performance is substantially better than that of using the relevance feedback as in the single list setting. The performance also substantially transcends that of a previously proposed approach to utilizing relevance feedback in fusion-based retrieval.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Relevance feedback, Retrieval models
Keywords: fusion, relevance feedback
1. INTRODUCTION
It is a well-established fact that using (positive) relevance feedback in ad hoc retrieval helps to substantially improve retrieval effectiveness [29, 30]. Usually, relevance feedback, if available, is provided for the documents most highly ranked by some initial search performed in response to the query. Then, information induced from the feedback documents is used for a second retrieval.
Here we address the challenge of utilizing relevance feedback in fusion-based retrieval [12]. That is, the document list for which feedback is provided results from merging (fusing) several intermediate lists that were produced using dif-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609573.

ferent retrievals from the same corpus in response to the query. Thus, our main goal is to address the questions of whether and how relevance feedback can be effectively utilized while accounting for the special characteristics of the fusion setting. Furthermore, we opt for an approach that is not committed to a specific retrieval framework (e.g., vector space, language modeling) as we operate in a fusion setting.
We present retrieval methods that use the relevance feedback for two different, yet complementary, purposes. The first is to directly rank the pool of documents in the intermediate lists. The second is to estimate the effectiveness of the intermediate lists so as to re-fuse them. Several listeffectiveness estimates are proposed based on the observation that this is essentially an evaluation task with minimal (incomplete) relevance judgments. To simultaneously leverage both purposes just described, we present a meta fusion method. The method fuses the direct ranking induced over the pool with that created by the re-fusion of the intermediate lists.
Empirical evaluation performed with TREC corpora attests to the merits of our approach. Specifically, the retrieval performance is substantially better than that attained by treating the relevance feedback as in the standard single retrieved list setting; i.e., disregarding the fact that the list for which feedback is provided results from fusing intermediate lists. Furthermore, our approach is substantially more effective than the only previously proposed method to utilizing relevance feedback in fusion-based retrieval [4].
2. RELATED WORK
There is a large body of work on using relevance feedback for retrieval [29, 30, 9]. In contrast to our work, the fusionbased retrieval setting has not been specifically addressed, with the exception of some work which is discussed below. As already noted, we show that leveraging the special characteristics of the fusion setting when utilizing the relevance feedback is of much merit.
There has been much work on fusing document lists that were retrieved from the same corpus in response to a query (e.g., [7, 14, 21, 22, 28, 36, 2, 12, 3, 10, 26, 39, 5, 23, 32, 6, 31, 38]). However, to the best of our knowledge, there has only been a single report on using relevance feedback in a fusion-based retrieval setting [4]. A fusion method served for active feedback, that is, selecting documents to be judged by the user through an iterative process. We do not address the active feedback task. However, as a fusion approach that utilizes a feedback set of documents was proposed, specifically, to estimate list effectiveness for re-fusion [4], we use

313

this work for reference comparison. The list-effectiveness estimate proposed in this work [4] is different than those we present here. Furthermore, in contrast to our approach, the feedback was not used to directly rank documents in the intermediate lists. Accordingly, the integration of the two purposes that the feedback can be used for -- direct ranking and list-effectiveness estimation for re-fusion -- is not proposed in contrast to our work. In Section 4.2.4 we empirically show that our approach is substantially more effective in utilizing the relevance feedback.
In work on fusion, the intermediate retrieved lists (or segments thereof) were weighted (i) uniformly (which is the most common case), (ii) using unsupervised approaches [39, 38], or (iii) based on past performance of the retrieval method as determined using a train set of queries [7, 2, 23, 32, 6, 31]. In Section 4.2.5 we demonstrate the relative merits of using our proposed list-effectiveness estimates which utilize relevance feedback to weight the lists.

3. RETRIEVAL FRAMEWORK

Let q and D be a query and a corpus of documents, re-

spectively. Suppose that the documents lists L1, . . . , Lm,

each composed of n documents, were retrieved from D in

response to q by m different retrievals. These can be based,

for example, on different query representations, document

representations, and ranking functions [12]. In what follows

we use the notation d  L to indicate that document d is in

the list L; SL(d) is d's normalized (non negative) score in

L; if d  L, we set SL(d) d=ef 0. Details regarding the score

normalization approach are provided in Section 4.1.

The goal of fusion methods is to merge the lists into one

result list, Lfuse. For example, the CombSUM method [14]

scores

d

by

SCombSUM (d)

d=ef

P
Li :dLi

SLi (d).

Thus,

docu-

ments that are ranked high in many of the lists are rewarded.

The CombMNZ method [14, 22] further rewards documents

that appear in many of the lists: SCombMNZ (d) d=ef |{Li :

d  Li}|SCombSUM (d).

3.1 Using Relevance Feedback
As in previous work on using relevance feedback [30, 9], we assume that a user scans the list she is presented with, Lfuse in our case, top down until she encounters r documents that are relevant to the information need she expressed using the query q. We use Rq[r](Lfuse) (henceforth Rq) to denote the set of these relevant documents, and F (Lfuse) to denote the set of all documents she scanned and therefore judged; i.e., F (Lfuse) \ Rq are the non-relevant documents the user encountered. We note that the user need not be aware of the fact that the result list she scans (Lfuse) was produced by fusing intermediate lists. Our goal is to devise retrieval methods that use information induced from F (Lfuse).
Several of the approaches that we present use some query expansion method. The method takes as input several documents -- the relevant ones (Rq) in our case -- the query q, and some corpus-based term statistics. The method then produces a query model, Mq;Rq , that can be used to rank documents; the score assigned to document d is S(d; Mq;Rq ). For example, in Rocchio's method [29], the query model is a tf.idf-based vector where cosine is often used as the scoring function. In the mixture model [41] and relevance model [20] approaches, the query model is a unigram language model; documents are ranked by the cross entropy between

the query model and their language models. In Section 4.1 we provide the details of the query expansion method used for experiments. We hasten to point out that our methods are not committed to a specific query expansion approach.
Following standard practice in work on utilizing relevance feedback [30], we can use Mq;Rq to rank the entire corpus; CorpusRank denotes this approach. We also study a method, FusedListReRank, which uses Mq;Rq to re-rank Lfuse rather than to rank the entire corpus.
However, CorpusRank and FusedListReRank do not account for the special characteristics of the retrieval setting we address here. That is, the fact that the list Lfuse, for which relevance feedback is provided, results from fusing intermediate retrieved lists. The methods we present below do exploit this fact.

3.2 Exploiting the Special Characteristics of the Fusion Setting

We start with the simple observation that fusion-based re-

trieval is a two steps procedure. First, a pool of documents,

Dpool

d=ef

S
i

Li

,

is

created

by

the

different

retrievals.

Then,

list-specific properties of documents in the pool (e.g., docu-

ment scores in the lists) are used to rank the pool. Accord-

ingly, we devise methods that rank Dpool using the relevance

feedback. Following common practice in work on fusion [12],

documents not in the pool are assigned with a zero score in

all methods.

Our first method, PoolRank, ranks Dpool using the query

model Mq;Rq which was induced from the relevant documents and the query:

SP oolRank(d) d=ef S(d; Mq;Rq ).

(1)

The pool contains documents "considered" relevant by retrievals which were based only on the query. Thus, using information induced from relevant documents to re-rank it can potentially improve retrieval effectiveness.
Our second method, ReFuse, uses the relevance feedback to weight the intermediate lists Li so as to re-fuse them. The development of ReFuse is guided by probabilistic retrieval principles as described next.
The goal of probabilistic retrieval methods is to estimate the probability p(d|Iq) that d is relevant to the information need Iq expressed by q. Relevance is determined with respect to the information need rather than with respect to the query which is a signal about the information need.
To estimate p(d|Iq) using information induced from the intermediate lists, and inspired by some recent work on predicting query-performance for fusion [25], we can write

p(d|Iq) = X p(d|Iq, Li)p(Li|Iq),

(2)

Li

if we assume that p(Li|Iq) is a probability distribution over the intermediate lists.
Following common practice in work on aspect and mixture models [16], we first make the assumption that d is independent of Iq given Li. Then, we get the estimate

p^(d|Iq) d=ef X p^(d|Li)p^(Li|Iq),

(3)

Li

where p^ denotes an estimate for p. That is, the probability that d is relevant to Iq is estimated based on estimates for

314

its association with the intermediate lists (p^(d|Li)); the impact of a list Li depends on its effectiveness (relevance) with respect to Iq (p^(Li|Iq)). Thus, Equation 3 reflects a transition from using q as an explicit signal about Iq to using the intermediate lists that were retrieved in response to q as a pseudo signal about Iq.
The mixture model just described is the conceptual basis of linear fusion methods [2].1 For example, the CombSUM method [14], which was mentioned above, is a linear fusion method: normalized document scores in the lists serve for list-association measures (p^(d|Li)); and, uniform list-effectiveness estimates (p^(Li|Iq)) are used.
In the absence of relevance feedback, estimating the effectiveness of the intermediate lists is a difficult task [2, 39]. However, here, some relevance feedback is provided, although for the fused list Lfuse and not for the intermediate lists. In Section 3.2.1 we present a few measures that use this relevance feedback to estimate the effectiveness of the intermediate lists. Using these estimates in the linear fusion framework results in our ReFuse method that scores d ( Dpool) by:

SReF use(d) d=ef X wIq (Li; F (Lfuse))SLi (d); (4)
Li :dLi

wIq (Li; F (Lfuse)) is Li's estimated effectiveness with re-

spect to Iq; and, d's score in list association measure as

Li in

,CSoLmib(dS)U, sMer.2ves

as

the

document-

Meta Fusion. The relevance feedback served two different
purposes in the PoolRank and ReFuse methods; namely, to directly rank the pool and to estimate the effectiveness of the intermediate lists so as to re-fuse them, respectively. We next integrate these approaches.
Instead of making the independence assumption that led from Equation 2 to Equation 3 -- i.e., that d is independent of Iq given Li -- we estimate p(d|Iq, Li) using p^^(d|Iq)+(1- )p^(d|Li); p^^(d|Iq) is some estimate for p(d|Iq);  is a free parameter. Using the estimate for p(d|Iq, Li) in Equation 2, applying some algebra, and using the assumption from above that p(Li|Iq) is a distribution over the intermediate lists, we derive a new estimate for p(d|Iq):

p^^(d|Iq) + (1 - ) X p^(d|Li)p^(Li|Iq).

(5)

Li

This estimate "backs off" from some direct estimate (p^^(d|Iq))

to the mixture-based estimate from Equation 3.

For the direct estimate, p^^(d|Iq), we use the normalized

score assigned by PoolRank to d:

. SP oolRank (d)

P d Dpool

SP oolRank (d)

This is a probability distribution over the entire corpus as

documents not in Dpool are assigned with a 0 score. The resultant estimate is based on using the query model Mq;Rq , which was induced from the relevant documents and used in

PoolRank for ranking, as a representation for Iq.

Then, following Equation 5 we interpolate the direct esti-

mate just described with the normalized score assigned by

1We write "conceptual" to emphasize the fact that the actual measures that are used in work on linear fusion methods are not necessarily valid probability distributions [2]. 2Document d is associated only with lists in which it ap-
pears, because SLi (d) d=ef 0 if d  Li.

ReFuse to d:

. SReF use(d)

P d Dpool

SReF use(d)

This is a distribution

over the entire corpus which is based on the linear mixture

model described in Equation 3. Accordingly, we arrive to our MetaFuse method that scores d by:3

SMetaF use(d)

d=ef



P
d

SP oolRank(d) S Dpool P oolRank

(d

)

(6)

+

(1

-

)

SReF use(d)

P
d Dpool

SReF use(d)

.

The name MetaFuse is coined based on the following observation. The PoolRank method induces a ranking over Dpool using Mq;Rq . This ranking is essentially linearly fused, using Equation 6, with a second ranking of Dpool which was created by ReFuse. The ReFuse ranking is by itself the result of linearly fusing the rankings of the intermediate lists L1, . . . , Lm using list-effectiveness estimates in Equation 4.
For  = 1 and  = 0, MetaFuse becomes PoolRank and ReFuse, respectively. More generally, the higher the value of , the more weight is put on the ranking produced by using the query model that was induced from the relevant documents. Lower values of  result in more emphasis on the re-fusion of the intermediate lists that are weighted using information induced from the feedback documents.

3.2.1 Estimating list effectiveness
We now turn to address the task of estimating the effectiveness of an intermediate retrieved list Li with respect to Iq using the feedback document set, F (Lfuse). The estimate, denoted wIq (Li; F (Lfuse)), is used in Equation 4 for the ReFuse method, which is then used in MetaFuse in Equation 6.
It is important to note that documents in Li, even if are relevant, might not be among those for which relevance feedback is available. Recall that the relevance feedback was provided for the documents most highly ranked in the fused list Lfuse. Thus, the challenge is estimating retrieval effectiveness with incomplete (minimal) relevance judgments.
The first list-effectiveness estimate that we consider is the standard average precision measure, AP. AP is computed using the feedback set, F (Lfuse), and treats unjudged documents -- i.e., those not in F (Lfuse) -- as non relevant.
To address the scarcity of relevance judgments in our setting, we also consider infAP [40]. This is a state-of-the-art retrieval effectiveness measure that was designed as an approximation to average precision (AP); specifically, for cases of incomplete relevance judgments. An important difference between infAP and AP is that the former differentiates between unjudged and non-relevant documents and the latter does not. We compute infAP based on the feedback set F (Lfuse). Documents not in F (Lfuse) are treated as unjudged. For comparison purposes, we also consider a variant of infAP, termed infAPonlyRel, which is computed using only the set of relevant documents, Rq; i.e., all other documents are treated as unjudged.4
3Experiments -- actual numbers are omitted as they convey no additional insight -- showed that using a weighted geometric mean of the normalized scores of PoolRank and ReFuse yields performance that is very similar to that of using the weighted arithmetic mean from Equation 6. 4We note that in contrast to the case for the original setting in which infAP was introduced [40], here infAP is not

315

It is worth noting at this point that we could have potentially used information induced from the non-relevant documents (F (Lfuse) \ Rq) to also improve the query model, Mq;Rq , which is used in PoolRank. However, utilizing negative feedback to improve retrieval performance has long been known as an extremely hard task [17, 29, 37] with the potential merits confined to very difficult queries [37, 18].
The development of the third and fourth list-effectiveness estimates, referred to as Kendall- and Pearson, is inspired by work on query-performance prediction [33]. The query model, Mq;Rq , which was induced from the relevant documents, is used to re-rank Li; the re-ranked list is denoted ReRank(Li). As Li was not created using relevance feedback, it is presumably less effective than ReRank(Li). Consequently, the latter can serve as a positive reference comparison to the former for estimating effectiveness [33]; i.e., we assume that the more similar the rankings of Li and ReRank(Li), the higher the effectiveness of Li. We use Kendall's- and Pearson's correlation coefficient to measure the similarity between Li and ReRank(Li). While Kendall's- is based on document ranks, Pearson's correlation coefficient depends on document scores. As the values assigned by the two measures are in [-1, 1] we shift and re-scale them to [0, 1] for consistency with the estimates described above. The resultant values serve as Li's effectiveness estimates.
4. EVALUATION
4.1 Experimental Setup
The methods we presented in Section 3 utilize relevance feedback. The feedback is provided for the documents at the top ranks of a result list which is produced by fusing several intermediate retrieved lists. Thus, to evaluate the effectiveness of the methods, we use runs submitted to different tracks of TREC as the intermediate lists.
Table 1 provides the details of the TREC tracks used for experiments. We used the ad hoc tracks of TREC3, TREC7 and TREC8, and the Web track of TREC9. These were also used in prior work on fusion [3, 26, 4, 5, 23, 32, 19]. We randomly sample 5 runs from all those submitted to a track and which contain at least 100 documents as a result for every query. (We refer to these runs as candidates in Table 1.) We use 30 such random samples; each sample constitutes an experimental setting. The retrieval effectiveness numbers that we report are averages over these 30 samples (settings). The n = 100 most highly ranked documents in a run per query serve for an intermediate retrieved list.5 Retrieval scores in the lists are min-max normalized [22, 27, 24]. Then, the five lists for each query are fused using the CombMNZ method [14, 22], which was described in Section 3. CombMNZ is a highly effective fusion approach that commonly serves as a baseline in work on fusion [3, 26, 23, 32, 19].
To create the set of feedback documents, F (Lfuse), we scan the list Lfuse which was produced by CombMNZ top down until r relevant documents are accumulated or the
necessarily a statistical estimate for AP. Yet, the empirical results presented in Section 4.2 attest to the merits of using infAP as a list effectiveness estimate in our setting. 5It was argued, based on the "skimming effect" principle [2], and empirically demonstrated [34, 35, 8, 19], that there are clear merits in fusing relatively short lists.

TREC
TREC3 TREC7 TREC8 TREC9

Data
Disks 1&2 Disks 4&5-CR Disks 4&5-CR WT10G

Queries
151-200 351-400 401-450 451-500

# of candidate runs
38 86 113 64

Table 1: TREC data used for experiments. Candidate runs are those that contain at least 100 results for every query.

end of the list is reached.6 F (Lfuse) is the set of documents scanned. Documents in F (Lfuse) are either relevant or non-relevant as determined by using TREC's qrels files. (Documents in F (Lfuse) with no judgement in the qrels file are considered not relevant as is standard.) The set of relevant documents in F (Lfuse) was denoted Rq in Section 3.1. We present results for r  {1, . . . , 5}.
Titles of TREC topics serve for queries. Tokenization and Porter stemming were applied to queries and documents using the Lemur toolkit7 which was used for experiments.

Query expansion method. As described in Section 3, a

few of our approaches use some query expansion method.

The method produces a query model Mq;Rq that can be used for ranking. For experiments we use the effective rele-

vance model number 3 (RM3) [20, 1] as the query expansion

method. When using relevant documents to construct RM3,

which is a unigram language model, the probability assigned

to

term

w

is

[20,

1]:

(1 - )p(w|q) +

 r

P
dRq

p(w|d);



is

a free parameter; p(w|q) is the maximum likelihood esti-

mate of term w with respect to q; p(w|d) is the probability

assigned to w by a Jelinek-Mercer smoothed unigram lan-

guage model induced from document d with smoothing pa-

rameter  [20];  = 0.1 following previous recommendations

[20]. It is common practice [1] to clip the relevance model

by using only the  terms to which it assigns the highest

probability; these terms' probabilities are sum-normalized

to yield a valid probability distribution. To rank documents

using RM3, we use the cross entropy between the relevance

model and their (smoothed) unigram language models [20].

To this end, we use Dirichlet smoothed document language

models with the smoothing parameter set to 1000 [41]. We

note that RM3 interpolates the query language model with

a linear mixture of the language models of the given relevant

documents. Therefore, it is the language-model-based ana-

logue of Rochhio's method [29, 20]. The latter interpolates

the query vector with the centroid (i.e., linear mixture) of

the vectors of relevant documents.

Evaluation metrics. To evaluate retrieval effectiveness, we
use the mean average precision at cutoff n = 100 (MAP@100), which is the size of the intermediate lists that are fused, and the precision of the top 10 document (p@10). Statistically significant differences of performance are determined using the two-tailed paired t-test computed at a 95% confidence level based on the average performance per query over the 30 samples of runs.

6In the very few cases (specifically, 0.8% of the cases for TREC9) that fusing a sample of 5 lists (for a specific query) results in a list with no relevant documents, we omit this sample. 7www.lemurproject.org

316

CombMNZ
CorpusRank FusedListReRank PoolRank ReFuse MetaFuse

TREC3

r=1

r=2

MAP p@10 MAP p@10

20.3 69.3
22.6 72.8 22.5 76.1c 24.1cf 75.1c 20.2cpf 68.4cpf 24.9cpfr 76.2cr

20.3 69.3
24.9 77.5 23.4c 78.8 25.6cf 78.2 20.8cpf 70.3cpf 26.0cpfr 79.0r

TREC7

r=1

r=2

MAP p@10 MAP p@10

21.1 53.7
22.8 56.9 22.8 57.3 23.4c 57.1 22.1 56.5 25.1cpfr 59.3cpfr

21.1 53.7
25.1 60.9 24.2 60.6 25.4f 60.8 22.7fp 57.7 27.3cpfr 63.3cpfr

TREC8

r=1

r=2

MAP p@10 MAP p@10

23.9 54.3
25.2 58.8 25.2 59.6 25.9c 59.4 25.4 56.9f 27.8cpfr 60.9r

23.9 54.3
27.2 61.6 26.5 61.6 27.6f 61.8 26.0p 58.2cpf 29.2cpfr 63.8cpfr

TREC9

r=1

r=2

MAP p@10 MAP p@10

19.8 35.2
28.0 42.2 27.3 43.6 28.8f 43.9 22.6cpf 38.1fp 29.3crf 44.8r

19.8 35.2
33.2 47.8 31.5c 47.8 33.5f 48.4 23.4cpf 39.5cpf 33.6fr 48.5r

Table 2: Main result table. Boldface marks the best result in a column. Italics marks performance that is statistically significantly better than that of CombMNZ. 'c', 'f ', 'p' and 'r' mark statistically significant differences with CorpusRank, FusedListReRank, PoolRank and ReFuse, respectively.

An important question in evaluating the retrieval effectiveness of methods that utilize relevance feedback is whether to consider for evaluation the given set of relevant documents [9]. To compare our methods to each other with respect to various aspects (e.g., the number relevant documents), we consider the given relevant documents in the evaluation presented in Sections 4.2.1, 4.2.2 and 4.2.3. We note that our methods do not directly position the given relevant documents at the highest ranks of the final result list. Thus, this evaluation also enables to study their ability to rank high the given relevant documents. When comparing our methods with reference comparisons in Sections 4.2.4 and 4.2.5, we use a residual corpus approach for evaluation [9]: the given relevant documents (and in Section 4.2.4 also the given non-relevant documents) are not considered in the evaluation. For completeness, we re-visit the comparison of our methods using the residual corpus evaluation approach in Section 4.2.6. We note that in all cases the final result list that is evaluated contains n = 100 documents.
Free-parameter values. Our methods that use RM3 de-
pend on its free parameters. The MetaFuse method has an additional free parameter, . The free-parameter values, in all methods, are set using leave-one-out cross validation performed over queries. MAP serves as the optimization criterion for the train set. The parameter  in MetaFuse is set to values in {0, 0.1, . . . , 1}. The values of  and , which are used by RM3, are selected from {0.5, 0.8, 0.9, 1} and {10, 50, 75}, respectively8.
4.2 Experimental results
4.2.1 Main result
We show in Section 4.2.3 that the infAP list-effectiveness estimate is more effective than the others we consider. Hence, unless otherwise stated, the results presented for ReFuse and MetaFuse are based on using infAP. In practical scenarios, very few relevant documents are available as feedback (if at all). Thus, in Table 2 we present results for r  {1, 2}. Below we study the effect of further varying r.
We see in Table 2 that, as is expected, all methods that use the relevance feedback are more effective -- almost always to a statistically significant degree ­ than CombMNZ which
8The optimal value of  as determined over the train sets of queries was in most cases  0.9; that of  was in most cases in {50, 75}. As a case in point for performance sensitivity analysis, setting  = 0.9 and  = 50 in MetaFuse often results in MAP performance very similar to that attained using leave-one-out cross validation, except for TREC9.

does not utilize it. In Section 4.2.6 we show that the same finding holds with the residual-corpus evaluation approach.
We can also see in Table 2 that CorpusRank, which ranks the entire corpus using the relevance model, is somewhat more effective in terms of MAP than FusedListReRank; FusedListReRank uses the relevance model to re-rank the list produced by CombMNZ (Lfuse). In terms of p@10, the reverse often holds. However, the performance differences are statistically significant in very few cases.
We now turn to analyze the performance of the methods that leverage the special characteristics of the fusion setting when exploiting the relevance feedback. PoolRank uses the relevance model to rank the pool of documents in the intermediate retrieved lists. Its performance is better in most relevant comparisons (track × evaluation measure) than that of CorpusRank and FusedListReRank; in quite a few cases the performance improvements are statistically significant.
The ReFuse method uses the relevance feedback to estimate the effectiveness of the intermediate lists so as to refuse them. Its performance is worse than that of the CorpusRank, FusedListReRank and PoolRank methods; these use the relevance model induced from the relevant documents to directly rank documents. Yet, we see that the performance of ReFuse is superior to that of CombMNZ in a vast majority of the relevant comparisons. CombMNZ uses uniform list-effectiveness estimates, while ReFuse utilizes infAP (here) with the given feedback to estimate list effectiveness.
The most effective method in Table 2 is MetaFuse. Specifically, MetaFuse always outperforms -- often substantially and to a statistically significant degree -- CorpusRank and FusedListReRank. These two methods use the relevance feedback as in the single retrieved list case; i.e., they do not leverage the fact that feedback is provided for a list which results from fusion. MetaFuse does leverage this fact by integrating PoolRank and ReFuse. Thus, we conclude that there is much merit in exploiting the special characteristics of the fusion setting when using the relevance feedback.
We also see in Table 2 that although PoolRank is always more effective than ReFuse, MetaFuse that integrates the two yields performance that transcends that of both; the improvements are statistically significant in most relevant comparisons. This finding attests to the fact that the two purposes for which the relevance feedback is used in MetaFuse -- direct ranking of documents and list-effectiveness estimation for re-fusion -- are complementary.
Varying the number of relevant documents. Figure 1 ex-
hibits the effect of varying r, the number of given relevant documents, on the performance of the various methods.

317

MAP MAP MAP MAP

0.32

CombMNZ

CorpusRank

0.3

FusedListReRank PoolRank

ReFuse

0.28

MetaFuse

TREC3

CombMNZ

0.34

CorpusRank

FusedListReRank

0.32

PoolRank ReFuse

MetaFuse

0.3

TREC7

0.34

CombMNZ

CorpusRank

FusedListReRank

0.32

PoolRank ReFuse

MetaFuse

0.3

TREC8

0.45

CombMNZ

CorpusRank

0.4

FusedListReRank PoolRank

ReFuse

MetaFuse

0.35

TREC9

0.26

0.28

0.28

0.3

0.24

0.26

0.22

0.24

0.26

0.25

0.2

0.22

0.24

0.2

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

r

r

r

r

Figure 1: The effect of varying the number of relevant documents (r) on MAP performance. Note: figures are not to the same scale.

Our first observation is that the performance of all methods increases with increasing values of r. The CorpusRank, FusedListReRank and PoolRank methods directly rank documents using the relevance model which is constructed from the r relevant documents. Thus, these methods benefit from the improved quality of the relevance model when constructed from more relevant documents. The ReFuse method uses the feedback (relevant and non-relevant documents) with infAP to estimate the effectiveness of the intermediate lists so as to re-fuse them. Thus, increasing r, which means more feedback, results in more reliable estimates. Naturally then, the effectiveness of MetaFuse, which integrates PoolRank and ReFuse, improves with increasing values of r.
We also see in Figure 1 that the relative performance patterns of the methods across the values of r are consistent with those exhibited in Table 2 for r  {1, 2}. For example, PoolRank is in many cases more effective than CorpusRank and FusedListReRank. This provides further support to the relative merits of using the relevance model to rank the pool of documents in the intermediate lists with respect to using it to (re-)rank the entire corpus or the final fused list.
We observe in Figure 1 that although PoolRank is consistently more effective than ReFuse, MetaFuse which integrates them is in most cases more effective than both. We note that most of the improvements over ReFuse, and many of the improvements over PoolRank, specifically for TREC7 and TREC8, were found to be statistically significant.
With increasing values of r the performance difference between MetaFuse and PoolRank become smaller. (For r > 2 in TREC9 the performance is almost identical.) The reason is that the performance differences between PoolRank and ReFuse become larger when increasing r. Indeed, the relative performance improvements of PoolRank with increasing values of r are larger than those of ReFuse. This finding means that the improvements in the quality of the relevance model used in PoolRank have relatively more impact on the resultant retrieval effectiveness than those of the list effectiveness estimates used in ReFuse for re-fusion.
4.2.2 Balancing the roles of the relevance feedback
The parameter  in MetaFuse controls the balance between the two purposes (roles) that the relevance feedback serves. Higher values of  result in more reliance on using the relevance model in PoolRank to rank the documents in the pool; for  = 1, MetaFuse becomes PoolRank. Lower values of  result in more reliance on using the relevance feedback to estimate list effectiveness for re-fusion in ReFuse; specifi-

cally,  = 0 amounts to ReFuse. In Figure 2 we present the effect of varying  on the performance of MetaFuse. The other free parameters of the methods (i.e., those of the relevance model) are set using leave-one-out cross validation.
We see in Figure 2 that for  > 0 MetaFuse outperforms ReFuse (MetaFuse with  = 0). The reason is that MetaFuse integrates ReFuse with PoolRank (MetaFuse with  = 1) and the latter outperforms the former. Yet, often, the best performance of MetaFuse is attained for  < 1. This finding attests to the merit in using the relevance feedback simultaneously to directly rank documents in the pool (PoolRank) and to estimate list effectiveness for re-fusion (ReFuse).
A general trend observed in Figure 2 is that the optimal value of  rises when increasing the number of relevant documents (r). This finding can be explained by the fact that the performance of PoolRank improves to a much larger extent than that of ReFuse when increasing the value of r as discussed above for Figure 1.

4.2.3 Comparing list-effectiveness estimates

One of the two purposes for which relevance feedback is

used in our methods -- specifically, in ReFuse (Equation 4)

that is used by MetaFuse -- is to estimate the effectiveness

of the intermediate lists. Insofar, infAP was used in the eval-

uation. We now turn to study the performance of ReFuse

when using all the list-effectiveness estimates proposed in

Section 3.2.1. Recall that we are provided with relevance

feedback for the set F (Lfuse) of the documents most highly

ranked in the fused list Lfuse. Thus, for many documents

in the intermediate lists there are no relevance judgments.

For comparison, we study two additional list-effectiveness

estimates which are applied in ReFuse and which do not use

the relevance feedback. The first is uniform that considers

all intermediate lists to be of the same effectiveness. Us-

ing ReFuse with uniform amounts to the CombSUM fusion

method [14] mentioned in Section 3.

The second estimate is overlap [38]. For a list Li, the

overlap

is

defined

as

P
j=i

2|Li T Lj | |Li|+|Lj |

--

i.e.,

the

normalized

sum of its overlap with all other lists. Conceptually similar

list-effectiveness estimates were used in other work on fusion

[39] and in work on evaluating the effectiveness of search

systems without relevance judgments [34]. The premise is

that inter-list similarity (in terms of shared documents) in-

dicates effective retrieval. The performance of ReFuse using

the list-effectiveness estimates is presented in Figure 3.

318

MAP MAP MAP MAP

MAP MAP MAP MAP

TREC3 0.28

0.27

0.26

0.25

0.24

0.23

0.22

r=1

r=2

0.21

r=3 r=4

0.2

r=5

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1



TREC7

0.3

0.28

0.26

0.24

r=1

0.22

r=2 r=3

r=4

0.2

r=5

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1



TREC8

0.31

0.3

0.29

0.28

0.27

0.26

0.25

r=1

r=2

0.24

r=3

r=4

0.23

r=5

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 

TREC9

0.38

0.36

0.34

0.32

0.3

0.28

0.26

r=1 r=2

r=3

0.24

r=4

r=5

0.22

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1



Figure 2: The performance of MetaFuse for different values of the number of relevant documents (r) when varying .  = 1 amounts to PoolRank and  = 0 amounts to ReFuse. Note: figures are not to the same scale.

TREC3

TREC7

TREC8

TREC9

0.23

AP

infAP 0.225 infAPonlyRel

Kendall

0.22

Pearson

uniform

0.215

overlap

0.21

0.205

0.2

AP

0.25

infAP

infAPonlyRel

Kendall

0.24

Pearson

uniform

overlap

0.23

0.22

0.29

AP infAP

infAPonlyRel

0.28

Kendall

Pearson

uniform

0.27

overlap

0.26

0.25

0.28

AP

infAP

infAPonlyRel

0.26

Kendall

Pearson

uniform

overlap

0.24

0.22

0.195

0.21

0.24

0.2

0.19

0.23

0.2

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

r

r

r

r

Figure 3: The performance of ReFuse with various list-effectiveness estimates. Note: figures are not to the same scale

Our first observation based on Figure 3 is that all listeffectiveness estimates are almost always more effective than the uniform estimate. We also see that the overlap measure, which does not use the relevance feedback, is more effective than the Pearson and Kendall- estimates that do use it. However, overlap is often substantially less effective than the other estimates that use the relevance feedback, namely, infAP, infAPonlyRel and AP.
We see in Figure 3 that, in general, the performance of ReFuse when employed with the list-effectiveness estimates that use the relevance feedback tends to increase when the number of relevant documents (r) increases. The increase for Pearson and Kendall- is, however, extremely small. Recall that these two estimates, in contrast to infAP, infAPonlyRel and AP, do not estimate list effectiveness directly; rather, via the comparison of the list with its re-ranked version attained by using the relevance model. Thus, it may come as no surprise that using Pearson and Kendall- in ReFuse yields performance that is inferior (often substantially) to that of using infAP, infAPonlyRel and AP.
It is evident in Figure 3 that using in ReFuse the infAP estimate, which was used insofar in the experiments reported above, results in the best overall performance. infAP is the only estimate that directly exploits the non-relevant documents in F (Lfuse) by differentiating them from unjudged documents -- i.e., documents not in F (Lfuse). Using infAPonlyRel, which treats non-relevant documents as unjudged, and AP which treats non-relevant and unjudged documents the same, result in performance that is often somewhat inferior to that of using infAP.

With increased number of relevant documents (r) the performance of using AP becomes closer to that of using infAP. (For almost all values of r for TREC9 the performance is almost identical.) The reason is that AP becomes more robust when more relevant documents are available. In contrast, the performance of using infAPonlyRel with increased r becomes more inferior to that of using infAP, because infAPonlyRel treats the non-relevant documents as unjudged. Note that increased r is likely to result in increased number of non-relevant documents in F (Lfuse) by the virtue of the experimental setting described in Section 4.1.

4.2.4 Comparison with the Hedge method
As noted in Section 2, there is a single previous report on using relevance feedback in the context of fusion-based retrieval [4]. TREC runs were fused using relevance judgments obtained through an iterative process of active relevance feedback based on the Hedge method [15]. Here, we use the approach as a reference comparison that fuses the intermediate lists using the feedback documents (F (Lfuse)).
The loss of document d in F (Lfuse) with respect to an intermediate list Li in which it appears is:

l(d; Li)

d=ef

1

(-1)rel(d)

tmax
X

2

1; j

(7)

j=tk

tk is the rank of d in Li; rel(d) is 1 if d is relevant and 0 if it is not; tmax = |Dpool| is the size of the pool of documents in the intermediate lists. Then, Li's weight is defined as:

wIq (Li; F (Lfuse))

d=ef

 , P d:dLi F

(Lf use )

l(d;Li )

(8)

319

MAP MAP MAP MAP

TREC3

TREC7

TREC8

TREC9

0.24 0.22

0.2

0.23

0.21

0.24

0.18

0.22

0.2

0.22

0.16

0.21

0.19

0.14

0.2

0.18

0.2

0.19

0.17

0.12

0.18

0.17

Hedge ReFuse(Hedge)

0.16

ReFuse PoolRank

0.15

MetaFuse

0.16 Hedge

0.15 ReFuse(Hedge) ReFuse

0.14

PoolRank

MetaFuse

0.13

0.18

Hedge

0.16

ReFuse(Hedge) ReFuse

PoolRank

MetaFuse

0.14

0.1

0.08

Hedge ReFuse(Hedge)

ReFuse

0.06

PoolRank

MetaFuse

0.04

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

r

r

r

r

Figure 4: Comparison with the Hedge method [4]. In contrast to the case in previous figures, a special residual corpus approach is used for evaluation wherein the given relevant and non-relevant documents are not considered in the evaluation. Note: figures are not to the same scale.

where  is a free parameter with a value in {0.1, . . . , 0.9}. If
Li  F (Lfuse) =  then wIq (Li; F (Lfuse)) d=ef 0. The fusion score of d ( Dpool) is its average weighted loss
over all lists, computed as if it is non-relevant (rel(d) d=ef 0):

SHedge(d)

d=ef

X l(d; Li)wIq (Li;

F (Lfuse)).

(9)

Li

If d  Li, then l(d; Li) is set in Equation 9 to the average loss of all the documents in Dpool \ Li, where these are treated as if they are not relevant and positioned below the documents in Li (i.e., at ranks 101 to tmax = |Dpool|).
The original implementation of Hedge as an iterative active feedback approach positioned the given feedback documents at the highest ranks of the final result list [4]. Such direct positioning calls for a residual-corpus approach for evaluation [9]. Specifically, here, the documents in F (Lfuse) are removed from all evaluated rankings and the residual rankings serve for evaluation.
Our ReFuse and MetaFuse methods use the infAP listeffectiveness estimate which was shown above to be the most effective among those considered. In addition, we also study an instance of our ReFuse method, ReFuse(Hedge), in which the list-effectiveness estimate is that defined in Equation 8 and used by Hedge. The parameter  used by Hedge and ReFuse(Hedge) is set using leave-one-out cross validation performed over the queries in a track. Recall that the free-parameter values of our methods are also set using leaveone-out cross validation. Figure 4 presents the results.
We first see in Figure 4 that in contrast to Figures 1, 2 and 3 the curves are (almost always) monotonically decreasing with increasing values of r. The reason is that we use here a residual corpus approach for evaluation wherein all feedback documents are not considered for evaluation.
Figure 4 shows that ReFuse outperforms ReFuse(Hedge). This means that infAP is a more effective list-effectiveness estimate than that used by Hedge (Equation 8). Furthermore, in all tracks, except for TREC3, ReFuse outperforms Hedge. Both are linear fusion methods that differ in the listweighting function, and in the scores assigned to documents; in ReFuse the retrieval scores of a document in the lists are used, and in Hedge Equation 9 is used.
We also see in Figure 4 that PoolRank and MetaFuse substantially outperform Hedge. A vast majority of the performance improvements for PoolRank, and all of them for MetaFuse, were found to be statistically significant.

4.2.5 Comparison with past-performance-based estimation of list effectiveness

ReFuse, and therefore MetaFuse, use the relevance feed-

back to estimate the effectiveness of the intermediate lists

so as to re-fuse them. We now turn to compare them with

methods that estimate list effectiveness based on the past

performance of the retrieval method used to create the list.

Past performance is determined using a train set of queries.

In our experimental setting, the intermediate lists are de-

rived from TREC runs. Each run contains the results of a

retrieval method for the queries in a track. We used a leave-

one-out cross validation procedure across queries throughout

the evaluation. Thus, all queries in a run except for the one

at hand serve as the train set. Based on this set, the past

performance of the retrieval method is determined.

The Learning method [2] is a linear fusion method that

uses Equation 4 as is the case for ReFuse. For a list effec-

tiveness estimate it uses the MAP (mean average precision)

of the run computed over the train set of queries.

ProbFuse is a highly effective fusion method [23]. It uses

the train query set (henceforth Q) to estimate the effective-

ness of segments of the intermediate lists retrieved for a test

query.

Specifically, p^(dk|L) d=ef

P 1

|Rk,q |

is |Q|

qQ |Rk,q |+|Nk,q |

an estimate for the probability that a document, denoted

dk, in the k'th segment of an intermediate list L, will be relevant to some query. |Rk,q | and |Nk,q | are the number

of documents marked as relevant and non-relevant, respectively, to a train query q; these documents appear in the

k'th segment of an intermediate list in the train set which was retrieved for q in the same run that L belongs to. That is, Rk,q and Nk,q are documents retrieved in response to q

by the same retrieval method that produced L.

A document d in the pool (Dpool) of documents retrieved

for a test

query

is

scored by

SP robF use(d)

d=ef

P
Li

1 k

p^(dk

|Li

),

where k is the number of the segment of Li in which d ap-

pears; if d  Li then p^(dk|Li) d=ef 0 for all k. We use the

train set of queries (with MAP serving as the optimization

criterion) to also set the number of segments in the inter-

mediate lists to a value in {2, 4, 6, 8, 10, 15, 20, 25, 30, 40, 50}.

(Recall that the lists are composed of 100 documents.)

For an additional reference comparison we use CombMNZ

[14, 22] which essentially utilizes uniform list-effectiveness

estimates. (Refer back to Section 3 for details.) As Learn-

ing, ProbFuse and CombMNZ do not use the relevance feed-

320

MAP MAP MAP MAP

TREC3

TREC7

TREC8

TREC9

0.24

0.22

0.24

0.2

0.18

0.22

0.2

0.22

0.16

0.2

0.18

0.2

0.14

0.12

0.16

0.18

0.18

0.1

CombMNZ 0.16 Learning
ProbFuse ReFuse
0.14 MetaFuse

0.14 CombMNZ Learning ProbFuse
0.12 ReFuse MetaFuse

0.16 CombMNZ Learning
0.14 ProbFuse ReFuse
0.12 MetaFuse

0.08 CombMNZ Learning
0.06 ProbFuse ReFuse
0.04 MetaFuse

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

r

r

r

r

Figure 5: Comparison with fusion methods that estimate list effectiveness based on past performance of the retrieval method. A residual corpus evaluation approach is used where the given relevant documents are not considered in the evaluation. Note: figures are not to the same scale.

back, specifically, the given relevant documents, we use a residual-corpus approach to evaluation [9]: the given relevant documents are removed from all evaluated rankings and the residual rankings serve for evaluation. Figure 5 presents the performance numbers. All curves are monotonically decreasing due to the residual-corpus evaluation approach.
We see in Figure 5 that all methods outperform CombMNZ in almost all cases. We also see that in many cases ReFuse outperforms Learning. The main exceptions are for a small number of relevant documents (r) for TREC3 and TREC7. It is important to recall that the feedback is provided for the fused list (Lfuse) and not for the intermediate lists. Thus, the feedback available for the intermediate lists is scarce as was discussed in Section 3.2.1. Thus, we conclude that estimating list effectiveness with minimal relevance feedback can often result in better fusion performance than that of estimating list effectiveness using (much) information about the past performance of the retrieval method.
We also see in Figure 5 that ProbFuse outperforms ReFuse and Learning. This comes as no surprise because ProbFuse uses estimates for the effectiveness of segments of the retrieved lists (and higher segments are rewarded) while ReFuse and Learning use estimates for the entire lists. Thus, a future direction is integrating the feedback-based list effectiveness estimates of ReFuse with the segment-based ones of ProbFuse. Yet, as shown in Figure 5, our MetaFuse method that utilizes the relevance feedback, but does not rely on past performance of the retrieval method to estimate list effectiveness, consistently outperforms ProbFuse; many of the improvements are substantial and statistically significant.
4.2.6 Further evaluation using the residual corpus approach
The main comparison of our approaches which was presented in Section 4.2.1 was based on considering the given relevant documents for the evaluation. Here we compare the methods' performance with the residual corpus approach [9]: the given relevant documents are removed from any ranking that is evaluated and the residual ranking serves for evaluation. Figure 6 presents the performance curves.
We observe in Figure 6 the same relative performance patterns observed in Figure 1; the latter was based on evaluation that considers the given relevant documents. Specifically, (i) all methods that use the relevance feedback perform (almost always) better than CombMNZ which does not use it; we note that almost all of these improvements are sta-

tistically significant; (ii) using the relevance model induced from the relevant documents to rank only the the pool of documents in the intermediate retrieved lists (PoolRank) is often more effective than using it to rank the entire corpus (CorpusRank) or to re-rank the final fused list (FusedListReRank); (iii) the methods that use the relevance model to directly rank documents (CorpusRank, FusedListReRank, PoolRank, MetaFuse) are more effective than the ReFuse method that uses the relevance feedback to estimate the effectiveness of the intermediate lists so as to re-fuse them; and, (iv) our MetaFuse method, which uses the relevance feedback to both directly rank documents and re-fuse the intermediate lists, is the most effective. Many of the improvements it posts over the other methods are substantial and were found to be statistically significant.
5. CONCLUSIONS AND FUTURE WORK
We addressed the challenge of using relevance feedback in the fusion-based retrieval setting. That is, the feedback is provided for the documents most highly ranked in a list that results from fusing several intermediate retrieved lists.
We devised methods that use the relevance feedback for two different, yet complementary, purposes. The first is directly ranking the pool of documents in the intermediate lists. The second is estimating the effectiveness of the intermediate lists so as to re-fuse them. We presented a meta fusion approach that uses the feedback for these two purposes simultaneously.
Empirical evaluation demonstrated the merits of our approach. For example, the resultant retrieval performance is much better than that of using the feedback as in the single retrieved list setting; i.e., ignoring the fact that the feedback is provided for a list that results from fusion.
We plan to explore additional list-effectiveness estimates to be used in our approach. Adapting our methods to use pseudo feedback, rather than true feedback, is another interesting future venue.
6. ACKNOWLEDGMENTS
We thank the reviewers for their comments. This paper is based upon work done in part while Ofri Rom was at the Technion. Oren Kurland's work is supported in part by the Israel Science Foundation under grant no. 433/12 and by Google's and Yahoo!'s faculty research awards.

321

MAP MAP MAP MAP

0.28 0.26 0.24 0.22 0.2 0.18 0.16
1

TREC3

CombMNZ

0.26

CorpusRank

FusedListReRank

0.24

PoolRank

ReFuse MetaFuse

0.22

0.2

0.18

0.16

0.14

0.12

2

3

4

5

1

r

TREC7

CombMNZ

CorpusRank

0.26

FusedListReRank

PoolRank ReFuse

0.24

MetaFuse

0.22

0.2

0.18

0.16

0.14

0.12

2

3

4

5

1

r

TREC8

CombMNZ CorpusRank

0.2

FusedListReRank

PoolRank

0.18

ReFuse

MetaFuse

0.16

0.14

0.12

0.1

0.08

0.06

2

3

4

5

1

r

TREC9
CombMNZ CorpusRank FusedListReRank
PoolRank ReFuse
MetaFuse

2

3

4

5

r

Figure 6: Performance comparison of our methods using the residual corpus evaluation approach where the given relevant documents are not considered for evaluation. Note: figures are not to the same scale.

7. REFERENCES
[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 -- novelty and hard. In Proc. of TREC-13, pages 715­725, 2004.
[2] C. C. V. ant Garrison W. Cottrell. Fusion via linear combination of scores. Information Retrieval, 1(3):151­173, 1999.
[3] J. A. Aslam and M. Montague. Models for metasearch. In Proc. of SIGIR, pages 276­284, 2001.
[4] J. A. Aslam, V. Pavlu, and R. Savell. A unified model for metasearch, pooling, and system evaluation. In Proc. of CIKM, pages 484­491, 2003.
[5] J. A. Aslam, V. Pavlu, and E. Yilmaz. Measure-based metasearch. In Proc. of SIGIR, pages 571­572, 2005.
[6] N. Balasubramanian and J. Allan. Learning to select rankers. In Proc. of SIGIR, pages 855­856, 2010.
[7] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked retrieval systems. In Proc. of SIGIR, pages 173­181, 1994.
[8] S. M. Beitzel, E. C. Jensen, A. Chowdhury, O. Frieder, D. A. Grossman, and N. Goharian. Disproving the fusion hypothesis: An analysis of data fusion via effective information retrieval strategies. In Proc. of SAC, pages 823­827, 2003.
[9] C. Buckley and S. Robertson. Relevance feedback track overview: TREC 2008. In Proc. of TREC-17, 2008.
[10] A. Chowdhury, O. Frieder, D. A. Grossman, and M. C. McCabe. Analyses of multiple-evidence combinations for retrieval strategies. In Proc. of SIGIR, pages 394­395, 2001.
[11] W. B. Croft, editor. Advances in Information Retrieval: Recent Research from the Center for Intelligent Information Retrieval. Number 7 in The Kluwer International Series on Information Retrieval. Kluwer, 2000.
[12] W. B. Croft. Combining approaches to information retrieval. In Croft [11], chapter 1, pages 1­36.
[13] W. B. Croft and J. Lafferty, editors. Language Modeling for Information Retrieval. Number 13 in Information Retrieval Book Series. Kluwer, 2003.
[14] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proc. of TREC-2, 1994.
[15] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computing Systems Science, 55(1):119­139, 1997.
[16] T. Hofmann, J. Puzicha, and M. I. Jordan. Learning from dyadic data. In Proc. of NIPS, pages 466­472, 1998.
[17] E. Ide. New experiments in relevance feedback. In Salton G. (Ed.), The SMART Retrieval System (pp. 337-354). Englewood Cliffs, N. J.: Prentice-Hall, Inc, 1971.
[18] M. Karimzadehgan and C. Zhai. Improving retrieval accuracy of difficult queries through generalizing negative document language models. In Proc. of CIKM, pages 27­36, 2011.
[19] A. K. Kozorovitzky and O. Kurland. Cluster-based fusion of retrieved lists. In Proc. of SIGIR, pages 893­902, 2011.
[20] V. Lavrenko and W. B. Croft. Relevance models in information retrieval. In Croft and Lafferty [13], pages 11­56.

[21] J. H. Lee. Combining multiple evidence from different properties of weighting schemes. In Proc. of SIGIR, pages 180­188, 1995.
[22] J. H. Lee. Analyses of multiple evidence combination. In Proc. of SIGIR, pages 267­276, 1997.
[23] D. Lillis, F. Toolan, R. W. Collier, and J. Dunnion. Probfuse: a probabilistic approach to data fusion. In Proc. of SIGIR, pages 139­146, 2006.
[24] I. Markov, A. Arampatzis, and F. Crestani. Unsupervised linear score normalization revisited. In Proc. of SIGIR, pages 1161­1162, 2012.
[25] G. Markovits, A. Shtok, O. Kurland, and D. Carmel. Predicting query performance for fusion-based retrieval. In Proc. of CIKM, pages 813­822, 2012.
[26] M. Montague and J. A. Aslam. Condorcet fusion for improved retrieval. In Proc. of CIKM, pages 538­548, 2002.
[27] M. H. Montague and J. A. Aslam. Relevance score normalization for metasearch. In Proc. of CIKM, pages 427­433, 2001.
[28] K. B. Ng and P. P. Kantor. An investigation of the preconditions for effective data fusion in information retrieval: A pilot study, 1998.
[29] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, editor, The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313­323. Prentice Hall, 1971.
[30] I. Ruthven and M. Lalmas. A survey on the use of relevance feedback for information access systems. Knowledge Engineering Review, 18(2):95­145, 2003.
[31] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. Lambdamerge: merging the results of query reformulations. In Proc. of WSDM, pages 795­804, 2011.
[32] M. Shokouhi. Segmentation of search engine results for effective data-fusion. In Proc. of ECIR, pages 185­197, 2007.
[33] A. Shtok, O. Kurland, and D. Carmel. Using statistical decision theory and relevance models for query-performance prediction. In Proc. of SIGIR, 2010.
[34] I. Soboroff, C. K. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In Proc. of SIGIR, pages 66­73, 2001.
[35] T. Tsikrika and M. Lalmas. Merging techniques for performing data fusion on the web. In Proc. of CIKM, pages 127­134, 2001.
[36] C. C. Vogt and G. W. Cottrell. Predicting the performance of linearly combined IR systems. In Proc. of SIGIR, pages 190­196, 1998.
[37] X. Wang, H. Fang, and C. Zhai. A study of methods for negative relevance feedback. In Proc. of SIGIR, pages 219­226, 2008.
[38] S. Wu. Data fusion in information retrieval. Springer, 2012.
[39] S. Wu and F. Crestani. Data fusion with estimated weights. In Proc. of CIKM, pages 648­651, 2002.
[40] E. Yilmaz and J. A. Aslam. Estimating average precision with incomplete and imperfect judgments. In Proc. of CIKM, pages 102­111, 2006.
[41] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. of SIGIR, pages 334­342, 2001.

322

A Simple Term Frequency Transformation Model for Effective Pseudo Relevance Feedback

Zheng Ye, Jimmy Xiangji Huang
Information Retrieval and Knowledge Management Research Lab School of Information Technology York University, Toronto, Canada
{yezheng, jhuang}@yorku.ca

ABSTRACT
Pseudo Relevance Feedback is an effective technique to improve the performance of ad-hoc information retrieval. Traditionally, the expansion terms are extracted either according to the term distributions in the feedback documents; or according to both the term distributions in the feedback documents and in the whole document collection. However, most of the existing models employ a single term frequency normalization mechanism or criteria that cannot take into account various aspects of a term's saliency in the feedback documents. In this paper, we propose a simple and heuristic, but effective model, in which three term frequency transformation techniques are integrated to capture the saliency of a candidate term associated with the original query terms in the feedback documents. Through evaluations and comparisons on six TREC collections, we show that our proposed model is effective and generally superior to the recent progress of relevance feedback models.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models, Relevance feedback
General Terms
Algorithms, Performance, Experimentation
Keywords
Term Frequency Transformation; Pseudo Relevance Feedback
1. INTRODUCTION AND MOTIVATION
Users often issue very short queries to describe their information need, which leads to the absence of some important terms from the queries. Thus, users could get a poor coverage of relevant documents. To solve this problem, pseudo relevance feedback (PRF) via query expansion (QE) is an effective technique for boosting the overall performance in
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609636 .

Information Retrieval (IR). It assumes that top-ranked documents in the first-pass retrieval are relevant, and then used as feedback documents in order to refine the representation of original queries by adding potentially related terms or adjusting the weights of query terms. PRF has been shown to be effective in improving IR performance [6, 10, 14, 17, 29, 32, 34, 35, 42, 44, 46] in a number of IR tasks.
In general, the expansion terms are weighted and extracted either according to the term distributions in the feedback documents (i.e. one tries to extract the most frequent terms); or according to both the term distributions in the feedback documents and in the whole document collection (i.e. to extract the most specific terms in the feedback documents). Normally, the term frequency in a document determines its importance in that document, while inverse document frequency in the whole collections is used to estimate its importance globally. The term frequency is always normalized according to the length of the document that contains it. However, most of the existing models employ a single term frequency normalization mechanism or criteria that cannot take into account various aspects of a term's saliency in the feedback documents. When estimating the weight of a candidate expansion term, how the other terms are distributed are largely unexplored. First, for example, the original query itself is usually ignored in the process of expansion term selection. In other words, the term associations between candidate terms and the query terms have been ignored in most of traditional PRF models. Term proximity is an effective measure for term associations, which has been studied extensively in the past few years. Most of these studies focus on the term proximity within the original query and adapt this in ranking documents [5, 9, 15, 20, 30, 38, 40], which has proven to be useful in discriminating between the relevant and non-relevant documents. So it is promising to take into account the distribution of candidate expansion terms in combination with that of the original query terms. Second, although document length-based normalization can balance the weight of a term in different feedback documents well, the importance of the documents are not well utilized. Third, when one estimates the importance of a term in different documents, normal term frequency normalization methods only consider the frequency of itself and the document length. The distributions of other terms are always ignored, while we believe that it will affect the importance of the current term in a given document.
In this paper, we propose a uniform and heuristic model, in which three term frequency transformation techniques are used to capture the local saliency of a candidate term asso-

323

ciated in the feedback documents. In particular, three kinds of term frequency transformation techniques are then integrated to capture the overall saliency. First, besides the traditional term frequency normalization to overcome the length difference of documents, we take into account the weights of feedback documents to get a weighted and normalized term frequency. Second, we propose to use a relative term frequency transformation to capture the relative importance of a term in a given document. Third, we use a kernel-base term frequency transformation to capture the closeness to the original query.
The main contributions can be summarized as follows: 1) expansion terms are no longer selected merely based on term distributions regardless of original query and other terms in the same feedback documents. We can expect the selected terms to be more closely related to the original query, and thus have a higher impact on the effectiveness; 2) three different term frequency transformation techniques are combined in a heuristic way. 3) our model is simple, yet effective since we only have to transform the term frequency based on information in the feedback documents.
We evaluate our model on six TREC collections and compare it to the traditional PRF models. The experimental results show that the retrieval effectiveness can be improved significantly and empirical parameter settings are suggested while no training data is available.
The remainder of this paper is organized as follows: in Section 2 we review the related work. In Section 3, three transformation methods for capturing different aspects of term frequency (TF) and our proposed model are presented in details. In Section 4, we introduce the settings of the experiments. In Section 5, the experimental results are presented and discussed. Finally, we conclude our work briefly and present future research directions in Section 6.
2. RELATED WORK
PRF via query expansion is referred to as the techniques or algorithms that reformulate the original query by adding new terms and adjust their weights, in order to obtain a better query. With the refined query, usually better retrieval performance can be expected. PRF has been shown to be effective with various retrieval models [6, 10, 14, 17, 29, 32, 34, 35, 42, 44, 46, 43, 25]. There are a large number of studies on the topic of PRF. Here we mainly review the work about PRF which is the most related to our research.
The Rocchio's model [34] is one of the earliest work of PRF models, which was developed in 1971 for the Smart retrieval system. It provides a framework for implementing (pseudo) relevance feedback via improving the query representation in vector space retrieval model. In the following decades, a number of PRF models were developed, mostly derived from Rocchio's framework. For example, Carpineto et al. proposed an information-theoretic approach to automatic query expansion evaluated under the vector space model. Another popular and successful automatic PRF model was proposed by Robertson et al. [32, 31]. Amati et al. [3] proposed a query expansion algorithm in the divergence from randomness (DFR) retrieval framework.
In addition, with the development of language model [27] in IR, a number of PRF (e.g. [17, 39, 46]) have been developed to fit in the language modeling framework. For example, the model based feedback approach [46] is not only theoretically sound, but also performs well empirically. The

essence of model based feedback is to update the probability of a term in the query language model by making use of the feedback information. Much like model-based feedback, relevance models [17] also estimate an improved query model. The difference between the two approaches is that relevance models do not explicitly model the relevant or pseudo-relevant document. Instead, they model a more generalized notion of relevance [22]. Lv et al. [19] have conducted a comparable study of five representative state-of-the-art methods for estimating improved query language models in ad hoc information retrieval, including RM3 (a variant of the relevance language model), RM4, DMM, SMM (a variant of model-based feedback approach), and RMM [17, 39, 46]. They found that SMM and RM3 are the most effective in their experiments, and RM3 is more robust to the setting of feedback parameters.
Most of these PRF approaches estimate a value of the importance (or probability) of a candidate expansion term based on its own distribution or statistics (e.g. term frequency, collection term frequency and document frequency). However, when estimating the value of a term, how the original query terms and other terms in the same document are distributed was not considered together. Unlike previous work, we not only use the raw term frequency to capture the saliency of a term, but also take advantage of the distribution information of other terms. One of the information to utilize is the distributions of original query terms in combination with that of expansion terms. In particular, we model the closeness in terms of term proximity. In the following, we review related work of term proximity in IR.
Term proximity is the co-occurrences of terms within a specified distance, which could measure the closeness of terms. A large amount of work has been done to integrate term proximity into both probabilistic and language models, which are characterized by the distance of the original query terms in documents. For example, Allan and Ballesteros [1] proposed phrases indexing instead of terms, and obtained some improvements on TREC datasets. However, this approach cannot handle the scenario in which the query terms are not adjacent to each other. A more relaxed approach [15, 16, 8] attempted to introduce "NEAR" operator to quantify the proximity of query terms. Hawking and Thistlewaite [12] proposed a similar one, which evaluated text segments containing all query terms. In addition, Song et al. [38] grouped query terms into phrases and the contribution of a term is determined by how many query terms appear in the context phrases. Under the language modeling framework, Zhao et al. [50] used a query term's proximate centrality as a hyper parameter in the Dirichlet language model. Lv et al. [20] integrated the positional and proximity information into the language model by a different way. They defined a positional language model at each position in documents by creating virtual documents based on term propagation. With probabilistic models, Zhao et al.[48, 49] introduced a pseudo term, called cross term, to measure the association of multiple query terms.
Although there have been plenty of efforts in integrating proximity heuristic into existing retrieval models, work on how to utilize this information for PRF is still limited. Lv et al. [21] presented two methods to estimate the joint probability of a term w with the query Q at every position in each feedback document, which extended the relevance model [17], and significant improvements were obtained on

324

two large collections. Miao et al. [24] employed proximity heuristic in a formalistic framework which extensively differs from the language modeling framework. Unlike previous work, we propose a TF transformation method to capture this feature, which is then integrated into the PRF procedure.
Another aspect that could affect the performance of PRF is the quality of feedback documents. In most of traditional PRF approaches, there is a very strong assumption that top ranked documents from the first-pass retrieval are all relevant. In fact, the top ranked documents are not necessarily to be good for PRF since they are not evaluated by real users. Therefore, the candidate expansion terms should be assigned different weights. Several studies ([13, 18]) have investigated this problem by detecting the right documents for PRF, from which expansion terms are extracted. In ([13]), He et al. proposed to detect good feedback documents by classifying all feedback documents using a variety of features such as the distribution of query terms in the feedback document, the similarity between a single feedback document and all top-ranked documents, or the proximity between the expansion terms and the original query terms in the feedback document. In addition, Lee et al. ([18]) proposed a re-sampling method using clusters to select better documents for PRF. The main idea is to use document clusters to find dominant documents for the initial retrieval set, and to repeatedly feed the documents to emphasize the core topics of a query. In this study, we model this problem by transforming the TF of the candidate terms to consider the importance of different feedback documents.
3. A TERM FREQUENCY TRANSFORMATION MODEL
We first give the notations and conventions used in this paper. Then, a term frequency (TF) transformation model for PRF is proposed.
3.1 Notation and Conventions
Given a query Q and a document collection C, a list of ranked documents in descending order, denoted as D, is returned by an information retrieval system. This step is always call first-pass retrieval in the process of PRF. We use di to denote the i-th ranked document in D. After the firstpass retrieval, the top-k documents in D will be used as feedback documents in PRF, which is denoted as Df .
In traditional PRF models, each di in Df will all be treated as relevant. The goal is to utilize these feedback documents to expand the original queries and adjust their weights in order to derive a refined query Q1. With Q1, we could expect better retrieval performance.
3.2 Enhancement of Rocchio's Model
In this study, we explore the techniques of term frequency transformation in the classic Rocchio's model. Although the Rocchio's model has been introduced in the information retrieval field for many years, it is still very effective in obtaining relevant documents and most of the state-ofthe-art PRF approaches are derived from Rocchio's model. According to ([45]), "BM25 ([33]) term weighting coupled with Rocchio feedback remains a strong baseline which is at least as competitive as any language modeling approach for many tasks". This observation is also supported in ([45,

24]) as well as in our preliminary experiments of this paper. In the following, we revisit the traditional Rocchio's models and enhance it with three TF transformation techniques.
The Rocchio's model provides a way of incorporating (pseudo) relevance feedback information into the retrieval process. In case of pseudo relevance feedback, Rocchio's method has the following steps:

1. All documents are ranked for the given query using a particular Information Retrieval model (for example the BM25 model [33]). The |Df | highest ranked documents are identified as the pseudo relevance set Df .
2. An expansion weight w(t, Df ) is assigned to each term appearing in the set of the Df highest ranked documents. In general, w(t, Df ) is the mean of the weights provided by a weighting model (for example the TFIDF weighting model [36] and the KL-Divergence weighting model [7]).

3. The vector of query terms weight is finally modified by taking a linear combination of the initial query term weights with the expansion weight w(t, Df ) as follows:

Q1

=

  Q0

+



ri Df

ri |Df |

(1)

where Q0 and Q1 represent the original and first iteration query vectors, ri is the expansion term weight vector for the i-th feedback document, and  and  are tuning constants controlling how much we rely on the original query and the feedback information. We enhance the Rocchio's model by refining the estimation of ri described in the following section. In practice, we can always fix  to 1, and only study  in order to get better performance.

3.3 Our Proposed Model
As we can see from the previous section, the most important part within this framework is to calculate the vector ri for di. Namely, how to weight the candidate terms in a feedback document. The traditional approach uses the socalled TF-IDF weighting function to address this problem. However, most of these approaches only normalize the term frequency according to the length of feedback documents. Other aspects failed to be captured in this simple, yet effective framework as discussed in the introduction. So in this paper, we propose different transformation techniques and investigate how to integrate them to obtain a still simple and efficient, but more performing PRF approach. The resulting weighting framework is as follows:

n

w(t, di) = (j  tfj(t))  IDF (t)

(2)

j=0

where tfj(t) is the j-th transformation technique and j is the importance of j-th transformation technique. IDF (t) is the inverse document frequency of term t in the collection. In this study, we mainly focus on term frequency transformation techniques. So we simple use the IDF formula from BM25 as follows:

IDF (t) = log N - n(t) + 0.5

(3)

n(t) + 0.5

325

where N is the total number of documents in the collection, and n(t) is the number of documents containing t. It is of note that other variants of IDF can also be here, and performance improvement may be expected.
In the following, we present three methods for term frequency transformation, which are used for pseudo relevance feedback.

3.3.1 Weighted TF Transformation
In traditional TF-IDF model and its variants, the term frequency is always normalized according to the length of the document [37, 33, 4]. All these methods have shown to be simple, yet effective to make term frequency comparable in different documents for the first-pass retrieval.
However, feedback documents play different roles in the process of PRF. More specifically, some feedback document may be much more important that other ones. So it is necessary to take into account the quality or importance of a candidate feedback to PRF. In order to address this problem, we define a weighted term frequency by integrating the importance of a candidate document and the normalized term frequency according to the length of this document. The resulting formula, denoted as T F 1, is as follows:

T F 1(t, d) = lntf (t, d)  imp(d)

(4)

where lntf (t) denotes the traditional length-based normalized term frequency, and imp(d) is the importance of document d. For lntf (t), we use the following formula proposed in [4]:

avdl

lntf (t, d)

=

tf (t, D)



log2(1

+

) len(d)

(5)

where len(d) is the length of document d and avdl is the average document length in the collection. Other TF normalization functions, such as the Robertson TF [33], are also viable. Here, the reason we choose formula 5 is simply because it is not only effective but also parameter-free such that we can focus on evaluating main framework of our proposed PRF model.
For imp(d), without extra knowledge about the candidate feedback document, the best bet is to believe scores of the documents returned in the first-pass retrieval. In particular, we use the normalized scores returned the BM25 model as follows:

imp(d) = (k1 + 1)  tf (t, d)  (k3 + 1)  qtf (t)  IDF (t)

K + tf (t, d)
tQ

k3 + qtf (t)

(6)

where k1 and k3 are tuning constants which depend on the dataset used and possibly on the nature of the queries. K equals k1  ((1 - b) + b  dl/avdl), and dl is the length of the document. In our experiments, the values of k1, k3 are default to 1.2 and 8, respectively, which is the recommended setting in [32].

3.3.2 Kernel-based TF Transformation
In the process of PRF, the raw frequency of a term or its length-based normalization variants could be used to estimate its importance in a feedback document. However, it cannot capture the characteristic that whether a candidate term occurs near or far away from the query, which may cause the selected expansion terms not relevant to the

query topic. Thus, we propose a Kernel-based term frequency (ktf ) transformation method, which models the frequency of a term as well as the closeness to the query in terms of proximity.
In [21, 48], a pseudo term, namely Cross Term, is introduced to model term proximity within original query for boosting retrieval performance. kernel-based method to count the term frequency in a document. There are a number of kernel functions (e.g. Gaussian, Triangle, Cosine, and Circle [48]) which were used for measuring the proximity. The Gaussian kernel has been shown to be effective in most cases. So, in this paper, we adapt the concept of Cross Term with the Gaussian kernel by proposing a kernel-based TF transformation method, which captures the saliency of a candidate term brought not only by its occurrences but also the closeness to the original query. The resulting kernelbased TF between a candidate expansion term t and a query term q is as follows:

K (t,

q)

=

exp[

-(pt - pq)2 22

]

(7)

where pt and pq are respectively the positions of candidate term t and query term q in a document,  is a tuning parameter which controls the scale of Gaussian distribution.
In this method, beside the average proximity to the query, we also take into account the importance of different query terms. Therefore, we build a representational vector for the query, in which each dimension is the weight of a query term by the inverse document frequency formula below, and then the kernel-based term frequency, denoted as T F 2, in the Kernel-based method is computed as follows:

|Q|

T F 2(t) = K(t, qi)IDF (qi)

(8)

i=1

where qi is a query term, |Q| is the number of query terms, and IDF (qi) is the same as in Equation 6. N is the number of documents in the collection, and Nt is the number of documents that contain qi.

3.3.3 Relative TF Transformation
When comparing the importance of a term in difference documents, normal term frequency methods only consider the frequency of a term itself and the document length. The distributions of other terms are always ignored, while we believe that it will affect the importance of the current term. Similar to [26], let d1 and d2 be two documents of equal lengths, the frequency values of t in d1 and d2 are the same; but d2 has more distinct terms and even some other terms have higher frequency that t. One could imagine an extreme case that all other terms in di occur only once. So should the weight values of t in these two documents be the same?
With traditional TF normalization techniques, these two documents will be assigned with the same weights, which makes the ranking infeasible in this case. Intuitively, however, d2 mentions the query term t more frequently than d1, so t in d2 is more likely to be most important term than in d1. In other words, for d1 it is possible that it talks about a topic not related to the query term t since other terms occur more frequently, while d2 has a higher chance to talk about the query term t.
In order to capture the saliency of a term in this aspect, we use a relative TF transformation method, denoted as T F 3

326

as follows:

T F 3(t) = log2(1 + tf (t, d))

(9)

log2(1 + atf (d))

where tf (t, d) is the raw term frequency of term t in document d, and atf (d) is the average term frequency of document d. This formula was also used in [37, 26] to normalize the tf values. Defferent from previous work, here we use it to transform the raw TF in the scenario of PRF.

3.3.4 Normalization and Combining
As we can see from Equation 2, the three different TF transformation methods are linearly combined in our proposed model. In order to make the tuning of parameter simple, we need to normalize these three aspects, and the normalization method f (x) is suggested to meet the following property: 1) when the T F = 0, f (T F ) = 0; 2) f (T F + 1) > f (T F ), but f (T F + 2) - f (T F + 1) > f (T F + 1) - f (T F ); it means the weight of a term increases as the increase of T F , but the improvement has a diminishing effect; 3) f (x) maps T F into a specified scale.
One of the possible functions that satisfy the above requirements is as follows:

T F (t) = T F (t)

(10)

1 + T F (t)

This popular sub-linear TF normalization method has an upper-bound of 1, and puts TF into a range of 0 to 1. It also has the effect of reducing the influence of extreme values or outliers in the data without removing them from the data set. Other normalization methods may also be viable, and we leave this issue for further study in future work.

4. EXPERIMENTAL SETTINGS
4.1 Test Collections and Evaluation Metrics
In this section, we describe six representative test collections used in our experiments: Disk1&2, Disk4&5, WT2G, WT10G, GOV2 and Robust04, which are different in size and genre. The Disk1&2, Disk4&5 collection contains newswire articles from various sources, such as Association Press (AP), Wall Street Journal (WSJ), Financial Times (FT), etc., which are usually considered as high-quality text data with little noise. The WT2G collection is a general Web crawl of Web documents, which has 2 Gigabytes of uncompressed data. This collection was used in the TREC 8 Web track. The WT10G collection is a medium size crawl of Web documents, which was used in the TREC 9 and 10 Web tracks. It contains 10 Gigabytes of uncompressed data. GOV2 is a very large crawl of the .gov domain, which has more than 25 million documents with an uncompressed size of 423 Gigabytes. This collection has been employed in the TREC 2004, 2005 and 2006 Terabyte tracks. There are 150 ad-hoc query topics, from TREC 2004 - 2006 Terabyte tracks, associated to GOV2. In our experiments, we use 100 topics in TREC 2005 - 2006. The TREC tasks and topic numbers associated with each collection are presented in Table 1.
In all the experiments, we only use the title field of the TREC queries for retrieval. It is closer to the actual queries used in the real application and feedback is expected to be the most useful for short queries [46].
In the process of indexing and querying, each term is stemmed using Porter's English stemmer [28], and stopwords

Table 1: the TREC tasks and topic numbers associ-

ated with each collection.

Collection

Task

Queries

Docs

disk1&2 TREC1,2,3

51-200

741,856

Disk4&5 TREC 2004

301-450

528,155

WT2G

TREC8

401-450

247,491

WT10G TREC9,10

451-550

1,692,096

GOV2 TREC04-06

701-850

25,178,548

Robust04 Robust04 301-450,601-700 528,155

from InQuery's standard stoplist [2] with 418 stopwords are removed. The MAP (Mean Average Precision) performance measure for the top 1000 documents is used as evaluation metric, as is commonly done in TREC evaluations. The MAP metric reflects the overall accuracy and the detailed descriptions for MAP can be found in [41]. We take this metric as the primary single summary performance for the experiments, which is also the main official metric in the corresponding TREC evaluations. To emphasize on the top retrieved documents, we also include P@k in the evaluation measures, which measures precision at fixed low levels of retrieved results, such as 10 or 20 documents. This is referred to as "Precision at k", for example "Precision at 20". It has the advantage of not requiring any estimate of the size of the set of relevant documents but the disadvantages that it is the least stable of the commonly used evaluation measures and that it does not average well, since the total number of relevant documents for a query has a strong influence on precision at k.

4.2 Baseline Models
In our experiments, we compare our model with a informationtheoretic approach [7] (denoted as RocchioKL) developed under the Rocchio's framework in combination with the basic model BM25 as shown in Equation 6 and Rocchio's feedback model. According to ([45]), "BM25 term weighting coupled with Rocchio feedback remains a strong baseline which is at least as competitive as any language modeling approach for many tasks". In addition, we also compare the proposed models with the state-of-the-art feedback models in language modeling (LM) retrieval framework. In particular, for the basic language model, we use a Dirichlet prior (with a hyperparameter of µ) for smoothing the document language model, which can achieve good performance generally [47].
For PRF in language modeling framework, we first compare our proposed model with the relevance language model [17, 19], which is a representative and state-of-the-art approach for re-estimating query language models for PRF [19]. Relevance language models do not explicitly model the relevant or pseudo-relevant document. Instead, they model a more generalized notion of relevance R. The formula of RM1 is:

p(w|R)  p(w|D)p(D)P (Q|D)

(11)

D

The relevance model p(w|R) is often used to estimate the feedback language model F , and then interpolated with the original query model Q in order to improve its estimation as follows: Q = (1-)Q +F . This interpolated version of relevance model is called RM3. Lv et al. [19] systematically compared five state-of-the-art approaches for estimating query language models in ad-hoc retrieval, in which RM3

327

not only yields impressive retrieval performance in both precision and recall metric, but also performs steadily. In particular, we apply Dirichlet prior for smoothing document language models [46].
4.3 Parameter Settings
As we can see from all the PRF retrieval models in our experiments, there are several controlling parameters to tune. In order to build strong baseline, the parameter b in BM25 and µ in LM are optimized as follows. For the smoothing parameter µ in LM with Dirichlet prior, we sweep over values from 500 to 2000 with an interval of 50. Meanwhile, we sweep the values of b for BM25 from 0 to 1.0 with an interval of 0.05. In order to find the optimal parameter setting for fair comparisons, we use the training method presented in [11] for all the PRF baselines and our models, which is popular in the IR domain for building strong baselines. To evaluate the baselines and our proposed approach, we use 2-fold cross-validation, in which the TREC queries are partitioned into two sets by the parity of their numbers on each collection. Then, the parameters learned on the training set are applied to the test set for evaluation purpose as in [23].
Specifically, for parameters in PRF models, we evaluate all PRF models with different settings of feedback document size (|Df |  {5, 10, 15, 20, 30, 50}). We sweep the number of expansion terms over (k  {10, 15, 20, 25, 30, 35, 50}), and the interpolation parameter (, 1, 2, 3,  {0.0, 0.1, . . . , 1.0}).
5. EXPERIMENTS AND ANALYSIS
5.1 Comparison of Basic Retrieval Models
As we mentioned in the previous section, the results of both basic models are optimized using the same method. Therefore, it is fair to compare them on these six collections.
As we can see from Table 2, BM25 slightly outperforms LM with Dirichlet prior on the Disk1&2, WT10G and WT2G collections, while LM is superior on the other three collections. The performance of these two basic models are generally comparative, and no significant difference is observed. So it is reasonable to use them as the basic models of the PRF baselines and our proposed model.
5.2 Comparison with PRF Baseline Models
In Table 3, we present the results of the baseline PRF models and our proposed PRF model with different settings of feedback documents. We denote our PRF model as TFPRF. The last row in each of these tables is the average performance of each PRF model with different settings. We calculate the average MAP scores of each query with different number of Df , and then conduct significant test. In particular, a "*" and a "+" indicate a statistically significant improvement over RocchioKL and RM3 respectively, according to the Wilcoxon matched-pairs signed-ranks test at the 0.05 level. The bold phase style in a row means that it is the best result. As we mentioned in Section 4.2, the basic retrieval models of RocchioKL is using BM25 for fair comparison.
First, both of RocchioKL and RM3 have proven effective and been considered as strong baselines in previous studies. The RocchioKL model outperforms the RM3 model on the disk4&5, Robust04, WT2G, GOV2 and Robust04 collections, but defeated by the latter on the WT10G collection in terms of average MAP. In terms of average MAP and

P@20, our proposed TF-PRF model is generally better than the two baseline PRF models on all collections, and achieve significant better results in most cases. The maximum average improvement is as high as 8.00% and 8.25 in terms of MAP and P@20, respectively. Thus it is fair to conclude that our proposed models can outperform RocchioKL and RM3 generally.
Second, with the increase of feedback document size |Df |, the performance of our proposed TF-PRF model is much stabler than the RocchioKL model and the RM3 model. When |Df | is 50, both the baseline models obtain the worst results while our proposed TF-PRF model can still get very good performance on all the six collections. Besides, the optimal |Df | for our proposed TF-PRF model is always larger than that for the baseline models. To some extent, this phenomenon proves the effectiveness of the the weighted term frequency transformation method. Usually, when |Df | is very small, the top-k documents are more likely to be relevant, and it is reasonable to treat them equally and obtain good performance. However, when |Df | increases, the ratio of relevant documents will decrease. Since we have already taken this factor into account, the weights of feedback documents will be adjusted so that we can still obtain satisfying results. On the contrary, the baseline models which still treat the feedback documents equally may fail in this case.
Third, it is also interesting to note on Disk1&2 that as the increase of |Df |, the performance of all the PRF models increases. When |Df | increases to 50, there is only a slight decrease. It means most of the feedback documents are at least not harmful to PRF, which differs from all other collections. This is the only case that our TF-PRF model is slightly inferior to RocchioKL, though it still performs significantly better that RM3. The main reason is that the T F 1 transformation method down-weight the term frequency of terms in lower-ranked documents.
To summarize, all the PRF models generally outperform the basic models (BM25 and LM). Meanwhile, the performance of the baseline PRF models, namely the RocchioKL model and the RM3 model, is generally comparable on all the five collections, except Disk1&2. Moreover, our proposed model, TF-PRF, makes significant improvements over the baseline models and extensive experiments have shown the effectiveness of TF-PRF especially when a relatively larger value of |Df | is used.
5.3 Robustness Analysis
As we can see from the above experiments, the number of feedback documents |Df | can greatly impact the performance of PRF models, and the choice of |Df | turns out to be a challenge problem since it is hard to determine the optimal number of feedback documents. In this section, we further analyze their robustness of our proposed PRF models with respect to |Df |.
From Figure 5.3, it is clear to have a picture about the robustness of each method. Generally, the performance of all methods increases at the beginning when the number of feedback documents |Df | grows up. However, there is no unique optimal value of |Df | for all of them. The performance of each method starts to continuously drop after a peak. For example, TF-PRF obtains the best value when |Df | is 30 while RM3 performs the best when |Df | is 5 on the disk4&5 collection. Meanwhile, the best performance of TF-PRF is much better than that of RM3 on this collection.

328

Figure 1: Robustness Comparison in terms of MAP
Figure 2: Robustness Comparison in terms of P@20 329

Table 2: Performance of basic retrieval models in terms of MAP. Basic Models disk1&2 disk4&5 WT2G WT10G GOV2 Robust04

BM25

0.2378 0.2251 0.3132 0.2068 0.2994 0.2492

LM

0.2320 0.2274 0.3002 0.2056 0.3040 0.2511

Table 3: Comparison of the performance of PRF methods in terms of MAP and P@20. The values in the

parentheses are the improvements over RocchioKL and RM3 respectively. "Ave" in the last column means the average performance of each PRF model with different |Df |. A "*" indicates a statistically significant improvement over the classic Rocchio's model, and a "+" indicates a statistically significant improvement

over the RM3 model according to the Wilcoxon matched-pairs signed-ranks test at the 0.05 level. The bold

phase style means that it is the best result.

Models/docs

5

10

15

20

30

50

Average

Disk1&2 Disk4&5
WT2G WT10G GOV2 Robust04

RocchioKL RM3
TF-PRF RocchioKL
RM3 TF-PRF RocchioKL
RM3 TF-PRF RocchioKL
RM3 TF-PRF RocchioKL
RM3 TF-PRF RocchioKL
RM3 TF-PRF

0.2904 0.2772 0.2918+ (0.48%, 5.27%)
0.2576 0.2538 0.2659+ (3.22%, 4.77%) 0.3389 0.3243
0.3388 (-0.03%, 4.47%)
0.2308 0.2271 0.2351+ (1.86%, 3.52%)
0.3276 0.3255
0.3291 (0.46%, 1.11%)
0.2887 0.2870 0.2950+ (2.18%, 2.79%)

0.3002 0.2781 0.2978+ (-0.80%, 7.08%)
0.2645 0.2527 0.2699+ (2.04%, 6.81%)
0.3402 0.3268 0.3415+ (0.38%, 4.50%)
0.2153 0.2203 0.2350+ (9.15%, 6.67%)
0.3274 0.3241 0.3348+ (2.26%, 3.30%)
0.2960 0.2873 0.2982+ (0.74%, 3.79%)

MAP
0.3033 0.2780 0.3026+ (-0.23%, 8.85%)
0.2660 0.2518 0.2703+ (1.62%, 7.35%)
0.3393 0.3257 0.3448+ (1.62%, 5.86%)
0.2119 0.2206 0.2320+ (9.49%, 5.17%)
0.3288 0.3245 0.3344+ (1.70%,3.05%)
0.2990 0.2862 0.3028+ (1.27%, 5.80%)

0.3073 0.2777 0.3025+ (-1.56%, 8.93%)
0.2677 0.2504 0.2738+ (2.28%, 9.35%)
0.3390 0.3260 0.3454+ (1.89%, 5.95%)
0.2175 0.2197 0.2310+ (6.21%, 5.14%)
0.3283 0.3243 0.3366+ (2.53%, 3.79%)
0.2980 0.2863 0.3057+ (2.58%, 6.78%)

0.3086 0.2770 0.3044+ (-1.36%, 9.89%)
0.2567 0.2493 0.2747+ (7.01%, 10.19%)
0.3293 0.3242 0.3510+ (6.59%, 8.27%)
0.2105 0.2188 0.2255+ (7.13%, 3.06%)
0.3269 0.3242 0.3393+ (3.79%, 4.66%)
0.2986 0.2842 0.3082+ (3.22%, 8.44%)

0.3037 0.2746 0.3042+ (0.16%, 10.78%)
0.2521 0.2465 0.2703+ (7.22%, 9.66%)
0.3058 0.3233 0.3503+ (14.55%, 8.35%)
0.2037 0.2182 0.2215+ (8.74%, 1.51%)
0.3213 0.3221 0.3394+ (5.63%, 5.37%)
0.2953 0.2822 0.3074+ (4.10%, 8.93%)

0.3023 0.2771 0.3006+ (-0.56%, 8.46%)
0.2608 0.2508 0.2708+ (3.85%, 8.00%)
0.3321 0.3251 0.3453+ (3.98%, 6.23%)
0.2150 0.2208 0.2300+ (7.01%, 4.18%)
0.3267 0.3241 0.3356+ (2.72%, 3.54%)
0.2959 0.2855 0.3029+ (2.35%, 6.08%)

Disk1&2 Disk4&5
WT2G WT10G GOV2 Robust04

RocchioKL RM3
TF-PRF RocchioKL
RM3 TF-PRF RocchioKL
RM3 TF-PRF RocchioKL
RM3 TF-PRF RocchioKL
RM3 TF-PRF RocchioKL
RM3 TF-PRF

0.5217 0.5100 0.5227+ (0.19%, 2.49%)
0.3860 0.3620 0.3787+ (-1.89%, 4.61%)
0.4130 0.3910 0.4000+ (-3.14%, 2.30%)
0.2787 0.2720 0.2835+ (1.72%, 4.23%)
0.5691 0.5587 0.5909+ (3.83%, 5.76%)
0.3797 0.3767 0.3843+ (1.21%, 2.10%)

0.5327 0.5017 0.5413+ (1.61%, 7.89%)
0.3867 0.3610 0.3917+ (1.29%, 8.50%)
0.4030 0.3950 0.3970+ (-1.49%, 0.51%)
0.2770 0.2720 0.2925+ (5.60%, 7.54%)
0.5785 0.5520 0.5997+ (3.66%, 8.64%)
0.3815 0.3781 0.3890+ (1.97%, 2.88%)

P@20
0.5417 0.5037 0.5450+ (0.61%, 8.20%)
0.3867 0.3647 0.3953+ (2.22%, 8.39%)
0.4050 0.3920 0.4050+ (0.00%, 3.32%)
0.2830 0.2735 0.2920+ (3.18%, 6.76%)
0.5842 0.5587 0.5953+ (1.90%, 6.55%)
0.3809 0.3731 0.3940+ (3.44%, 5.60%)

0.5453 0.5023 0.5440+ (-0.24%, 8.30%)
0.3757 0.3637 0.3963+ (5.48%, 8.96%)
0.4060 0.3910 0.4080+ (0.49%, 5.35%)
0.2820 0.2735 0.2945+ (4.43%, 7.68%)
0.5849 0.5574 0.5997+ (2.53%, 7.59%)
0.3725 0.3715 0.3956+ (6.20%, 6.48%)

0.5417 0.5030 0.5430+ (0.24%, 7.95%)
0.3717 0.3627 0.3960+ (6.54%, 9.18%)
0.4020 0.3870 0.4110+ (2.24%, 6.20%)
0.2760 0.2685 0.2935+ (6.34%, 9.31%)
0.5919 0.5564 0.6040+ (2.04%, 8.55%)
0.3711 0.3701 0.3914+ (5.47%, 5.76%)

0.5423 0.5013 0.5467+ (0.81%, 9.06%)
0.3643 0.3593 0.3947+ (8.34%, 9.85%)
0.3730 0.3900 0.4040+ (8.31%, 3.59% )
0.2710 0.2665 0.2905+ (7.20%, 9.01%)
0.5789 0.5510 0.6044+ (4.40%, 9.69%)
0.3629 0.3733 0.3843+ (5.90%, 2.95%)

0.5376 0.5037 0.5405+ (0.54%, 7.30%)
0.3785 0.3622 0.3921+ (3.59%, 8.25%)
0.4008 0.3910 0.4042+ (0.85%, 3.38%)
0.2780 0.2710 0.2911+ (4.73%, 7.41%)
0.5813 0.5557 0.5990+ (3.05%, 7.79%)
0.3748 0.3738 0.3898+ (4.00%, 4.28%)

In addition, the best values of |Df | of TF-PRF are larger than those for RocchioKL and RM3 in most cases. This indicates that our proposed methods can make better use of feedback documents. Furthermore, after the peak point, the curve of TF-PRF falls down much smoother than those of the baselines. When |Df | is 50, which means 50 feedback documents are selected, the performance of our proposed methods are much better than RocchioKL and RM3 on all most collections. Thus, it is clear that our proposed method performs more robustly with respect to |Df |. We also observe that our proposed model can obtain the best performance on all the six collections which are of different sizes and quality. This is a solid evidence that the TF-PRF model can make better use of the feedback documents to improve the overall performance and constantly get good results.
In addition, it is also interesting to note that the performance of TF-PRF first increases with the increase of |Df |

(this is especially obvious on the disk4&5, WT2G and GOV2 collections), and then after the peak point it stays relatively stable on all collections. The reasons are two-fold. First, the increase in the first phase is due to the utilization of more useful feedback documents, though the optimal values of |Df | are different on different collections. In other words, the performance of PRF models can be increased by using more useful feedback documents. Second, since TF-PRF uses a weighted term frequency of candidate feedback documents, it is possible to reduce the negative impact of feedback documents with lower qualities. This leads to the stability of our proposed models after the peak points. So it is always safe to choose a relatively larger value of |Df |. This feature of our proposed models makes it viable to address the problem of the selection of the number of feedback documents, while still keep good performance.

330

Table 4: Comparison with PRM1, PRM2 and PRoc on Tera06 dataset. The bold phase style means that it is the best result.

MAP P@10 P@30 P@100

PRoc3 0.3283 0.5800 0.5260 0.3756

PRM1 0.3322 0.5306 0.4884 0.3671

PRM2 0.3319 0.5490 0.4871 0.3741

TF-PRF 0.3371 (2.68%, 1.48%, 1.57%) 0.6248 (7.72%, 17.8%, 13.8%) 0.5678 (7.95%, 16.3%, 16.5%) 0.4326 (15.2%, 17.8%, 15.6%)

In summary, the proposed model can utilize sufficient number of useful feedback documents, and can also reduce the negative impact of feedback documents with lower qualities. Generally, TF-PRF can reach their best performance when |Df | is in the range of [20, 30]. These values can be used as empirical optimal when no training data is available, although even larger values of |Df | do not necessarily harm the retrieval performance in terms of MAP and P@20. We also evaluate our proposed model on other data sets and similar results are observed. Due to the limit of space, we did not include all the results here.
5.4 Comparison with the Recent Progress
In this section, we compare our model with the recent progress related to this paper, namely the proximity-based Rocchio's model (PRoc) [24] and the position relevance model (PRM) [21]. PRoc extends the classic Rocchio's model by integrating proximity information between expansion term and original query such that terms that occur closer to the original query will be given more weight in the process of relevance feedback. Unlike PRoc, PRM is developed under the language modeling framework. It extends the relevance model, which takes into account term positions and proximity with the similar intuition that words closer to query words are more likely to be related to the query topic, and assigns more weights to candidate expansion terms closer to the query.
To make the comparison fair, we train our parameters on the Terabyte05 topics and use Terabyte06 1 topics on the GOV2 collection for testing as Lv. et al. did in [21]. Since we do not give results for the Million Query Track so far, we do not compare our method with PRM on the ClueWeb collection with the topics of this track. In [21], parameter µ in the Dirichlet smoothing is set to an optimal value of 1500, and b in our basic model, BM25, is set to 0.3 as PRoc in [24]. As we mentioned previously, the performance of BM25 and LM with Dirichlet smoothing does not differ significantly on the GOV2 collection. Therefore, this setting will not affect the comparison. Since PRoc3 is the most robust and performs the best generally among PRoc's three variants, it is selected to make this comparison. There are two versions of PRM, PRM1 and PRM2, which behave differently with different evaluation measures. So the results of both PRM1 and PRM2 are obtained directly from [21] for fair comparison.
1http://trec.nist.gov/data/terabyte.html

First, as we can see from Table 4, the TF-PRF model outperforms PRoc3 and PRM1 in terms of all metrics, which indicates the general effectiveness of our model. Second, it is also interesting to notice that TF-PRF is markedly superior to PRoc3, PRM2 and PRM1 by up to 17.8% improvement in terms of P@10, P@30 and P@100 which is more significant than on MAP. It shows that our model has more advantages in applications that emphasize the top results. In summary, our model is at least comparable to the recent progress in both probabilistic model and language model framework in MAP, and significantly better in P@10, P@30 and P@100.
6. CONCLUSIONS AND FUTURE WORK
In this paper, a new feedback model, TF-PRF, is proposed by incorporating three different term frequency transformation methods into the classic Rocchio's model. Specifically, we present three term frequency transformation methods to capture the saliency of an expansion terms from different local aspects. Then, three frequency measures, namely weighted term frequency, relative term frequency and kernelbased term frequency, are integrated for capturing the overall saliency of expansion terms.
Experiment results on six standard TREC data sets show that the proposed TF-PRF model is very effective and robust, and significantly outperforms strong baseline PRF models in different retrieval frameworks. Meanwhile, our proposed TFPRF is at least competitive to the most recent work, the PRoc model and the PRM model. Compared to the RocchioKL model, the proposed model is also less sensitive to the setting of parameter |Df |, the number of feedback documents. Additionally, we carefully analyze the robustness of our model with respect to the number of feedback documents, and an empirical rule to set this parameter is suggested.
There are several interesting future research directions to further explore. We would like to study more term frequency transformation techniques and try to use machine learning models to further optimize the PRF procedure. Another possible research direction is to study how the parameters in our model can be set automatically. It is also interesting to study different normalization and combination methods for integrating the three TF transformation techniques proposed in this paper, and to evaluate our models on more collections (e.g. ClueWeb).
7. ACKNOWLEDGMENTS
This research is supported by the research grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada and the Early Researcher Award/ Premier's Research Excellence Award. We thank five anonymous reviewers for their thorough review comments on this paper.
8. REFERENCES
[1] J. Allan, L. Ballesteros, J. Callan, W. Croft, and Z. Lu. Recent experiments with inquery. In Proceedings of the 4th Text Retrieval Conference, pages 49­64, 1995.
[2] J. Allan, M. E. Connell, W. B. Croft, F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proceedings of TREC, 2000.
[3] G. Amati. Probabilistic models for information retrieval based on divergence from randomness. Ph.D. thesis, Department of Computing Science, University of Glasgow, 2003.
[4] G. Amati and C. J. van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.
[5] S. Bu¨ttcher, C. L. A. Clarke, and B. Lushman. Term proximity scoring for ad-hoc retrieval on very large text collections. In

331

Proceedings of SIGIR '06, pages 621­622, New York, NY, USA, 2006. ACM.
[6] G. R. C. Carpineto, R. de Mori and B. Bigi. An information-theoretic approach to automatic query expansion. ACM Transactions on Information Systems (TOIS), 19(1):1­27, 2001.
[7] C. Carpineto, R. de Mori, G. Romano, and B. Bigi. An information-theoretic approach to automatic query expansion. ACM Trans. Inf. Syst., 19(1):1­27, 2001.
[8] C. Clarke, G. Cormack, and F. Burkowski. Shortest substring ranking (MultiText experiments for TREC-4). In Proceedings of the 4th Text REtrieval Conference, pages 295­304, 1996.
[9] C. L. Clarke, G. V. Cormack, and E. A. Tudhope. Relevance ranking for one to three term queries. Information Processing Management, 36(2):291 ­ 311, 2000.
[10] K. Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In Proceedings of CIKM '09, pages 837­846, New York, NY, USA, 2009. ACM.
[11] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '06, pages 154­161, New York, NY, USA, 2006. ACM.
[12] D. Hawking and P. Thistlewaite. Proximity operators - So near and yet so far. In Proceedings of the 4th Text Retrieval Conference, pages 131­143, 1995.
[13] B. He and I. Ounis. Finding good feedback documents. In Proceedings of CIKM '09, pages 2011­2014, 2009.
[14] X. Huang, Y. R. Huang, M. Wen, A. An, Y. Liu, and J. Poon. Applying data mining to pseudo-relevance feedback for high performance text retrieval. In Proceedings of ICDM '06, pages 295­306. IEEE Computer Society, 2006.
[15] E. M. Keen. The use of term position devices in ranked output experiments. J. Doc., 47:1­22, 1991.
[16] E. M. Keen. Some aspects of proximity searching in text retrieval systems. J. Inf. Sci., 18:89­98, 1992.
[17] V. Lavrenko and W. B. Croft. Relevance based language models. In Proceedings of SIGIR '01, pages 120­127, New York, USA, 2001. ACM.
[18] K. S. Lee, W. B. Croft, and J. Allan. A cluster-based resampling method for pseudo-relevance feedback. In Proceedings of SIGIR '08, pages 235­242, New York, NY, USA, 2008. ACM.
[19] Y. Lv and C. Zhai. A comparative study of methods for estimating query language models with pseudo feedback. In Proceedings of CIKM '09, pages 1895­1898, New York, NY, USA, 2009. ACM.
[20] Y. Lv and C. Zhai. Positional language models for information retrieval. In Proceedings of SIGIR '09, pages 299­306, New York, NY, USA, 2009. ACM.
[21] Y. Lv and C. Zhai. Positional relevance model for pseudo-relevance feedback. In Proceeding of SIGIR '10, pages 579­586. ACM, 2010.
[22] D. Metzler and W. B. Croft. Latent concept expansion using markov random fields. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 311­318, New York, NY, USA, 2007. ACM.
[23] D. Metzler, J. Novak, H. Cui, and S. Reddy. Building enriched document representations using aggregated anchor text. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 219­226, New York, NY, USA, 2009. ACM.
[24] J. Miao, J. X. Huang, and Z. Ye. Proximity-based rocchio's model for pseudo relevance. In Proceedings of SIGIR '12, pages 535­544, 2012.
[25] T. Miyanishi, K. Seki, and K. Uehara. Improving pseudo-relevance feedback via tweet selection. In Proceedings of the 22Nd ACM International Conference on Conference on Information Knowledge Management, CIKM '13, pages 439­448, New York, NY, USA, 2013. ACM.
[26] J. H. Paik. A novel tf-idf weighting scheme for effective ranking. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '13, pages 343­352, New York, NY, USA, 2013. ACM.
[27] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of SIGIR '98, pages 275­281, New York, NY, USA, 1998. ACM.

[28] M. Porter. An algorithm for suffix stripping. Program: electronic library and information systems, 14:130­137, 1980.
[29] K. Raman, R. Udupa, P. Bhattacharyya, and A. Bhole. On improving pseudo-relevance feedback using pseudo-irrelevant documents. In Proceedings of ECIR '10, pages 573­576, 2010.
[30] Y. Rasolofo and J. Savoy. Term proximity scoring for keyword-based retrieval systems. In F. Sebastiani, editor, Advances in Information Retrieval, volume 2633 of Lecture Notes in Computer Science, pages 79­79. Springer Berlin / Heidelberg, 2003.
[31] S. E. Robertson and K. Sparck-Jones. Relevance weighting of search terms. Journal of the American Society for Information Science, 27:129­146, 1977.
[32] S. E. Robertson, S. Walker, M. Hancock-Beaulieu, M. Gatford, and A. Payne. Okapi at TREC-4. In The Forth Text REtrieval Conference (TREC-4), 1995.
[33] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In Proceedings of TREC-3, pages 109­126, 1994.
[34] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, The SMART retrieval system: Experiments in automatic document, pages 313­323, 1971.
[35] G. Salton and C. Buckley. Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 41:288­297, 1990.
[36] G. Salton, A. Wong, and C. Yang. A vector space model for information retrieval. Journal of American Society for Information Retrieval, 18(11):613­620, November 1975.
[37] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 21­29, Zurich, Switzerland, 1996.
[38] R. Song, M. Taylor, J.-R. Wen, H.-W. Hon, and Y. Yu. Viewing term proximity from a different perspective. In Advances in Information Retrieval, volume 4956 of Lecture Notes in Computer Science, pages 346­357. Springer Berlin / Heidelberg, 2008.
[39] T. Tao and C. Zhai. Regularized estimation of mixture models for robust pseudo-relevance feedback. In Proceedings of SIGIR '06, pages 162­169, New York, NY, USA, 2006. ACM.
[40] T. Tao and C. Zhai. An exploration of proximity measures in information retrieval. In Proceedings of SIGIR '07, pages 295­302, New York, USA, 2007. ACM.
[41] E. M. Voorhees and D. Harman. Overview of the sixth text retrieval conference. Information Processing and Management: an International Journal, 36:3­35, July 2000.
[42] R. W. White and G. Marchionini. Examining the effectiveness of real-time query expansion. Inf. Process. Manage., 43(3):685­704, 2007.
[43] H. Wu and H. Fang. An incremental approach to efficient pseudo-relevance feedback. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '13, pages 553­562, New York, NY, USA, 2013. ACM.
[44] J. Xu and W. B. Croft. Improving the effectiveness of information retrieval with local context analysis. ACM Trans. Inf. Syst., 18(1):79­112, 2000.
[45] C. Zhai. Statistical language models for information retrieval a critical review. Found. Trends Inf. Retr., 2:137­213, March 2008.
[46] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In Proceedings of CIKM '01, pages 403­410. ACM, 2001.
[47] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, 2004.
[48] J. Zhao, J. X. Huang, and B. He. CRTER: using cross terms to enhance probabilistic information retrieval. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '11, pages 155­164, New York, USA, 2011. ACM.
[49] J. Zhao, J. X. Huang, and Z. Ye. Modeling term associations for probabilistic information retrieval. ACM Trans. Inf. Syst., 32(2):7, 2014.
[50] J. Zhao and Y. Yun. A proximity language model for information retrieval. In Proceedings of the 32th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '09, pages 291­298, New York, USA, 2009. ACM.

332

Entity Query Feature Expansion using Knowledge Base Links

Jeffrey Dalton, Laura Dietz, James Allan
Center for Intelligent Information Retrieval School of Computer Science
University of Massachusetts Amherst Amherst, Massachusetts
{jdalton, dietz, allan}@cs.umass.edu

ABSTRACT
Recent advances in automatic entity linking and knowledge base construction have resulted in entity annotations for document and query collections. For example, annotations of entities from large general purpose knowledge bases, such as Freebase and the Google Knowledge Graph. Understanding how to leverage these entity annotations of text to improve ad hoc document retrieval is an open research area. Query expansion is a commonly used technique to improve retrieval effectiveness. Most previous query expansion approaches focus on text, mainly using unigram concepts. In this paper, we propose a new technique, called entity query feature expansion (EQFE) which enriches the query with features from entities and their links to knowledge bases, including structured attributes and text. We experiment using both explicit query entity annotations and latent entities. We evaluate our technique on TREC text collections automatically annotated with knowledge base entity links, including the Google Freebase Annotations (FACC1) data. We find that entity-based feature expansion results in significant improvements in retrieval effectiveness over state-of-the-art text expansion approaches.
Categories and Subject Descriptors
H.3.3 [Selection Process]: [Information Search and Retrieval]
Keywords
Entities; Ontologies; Information Retrieval; Information Extraction
1. INTRODUCTION
Today's commercial web search engines are increasingly incorporating entity data from structured knowledge bases into search results. Google uses data from their Knowledge Graph and Google Plus, Yahoo! has Web Of Objects, Bing incorporates Facebook and Satori entities, and Facebook searches over entities with Graph Search. However, the majority of content created on the web remains unstructured text in the form of web pages, blogs, and microblog posts. For many search tasks, these documents will continue to be the main
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609628.

source of content for users. In this work, we address the task of ad hoc document retrieval leveraging entity links to knowledge bases in order to improve the understanding and representation of text documents and queries. We demonstrate that this gain in semantic understanding results in significant improvements in retrieval effectiveness.
We bridge the gap between entities and text using automatic information extraction to identify entities and link them to a knowledge base. The task of `entity linking' to a knowledge base has received significant attention, with one major venue being the Text Analysis Conference (TAC) Knowledge Base Population (KBP) Entity Linking Task [17]. In this task traditional named entities (people, geo-political entities, and organizations) are linked to a knowledge base derived from Wikipedia. Beyond TAC, there is increasing interest in more general concept entities, with the task of `wikifying' [28, 16, 19] documents by linking them to Wikipedia. Beyond information extraction, content owners are augmenting HTML markup with embedded structured data through standardized markup efforts such as schema.org. A study from 2012 showed that 30% of web documents contain embedded structured data in RDFa or Microformats [26].
Google recently released the FACC1 dataset [15] for the TREC ClueWeb09 and ClueWeb12 web collections. The dataset contains automatically extracted entity mentions from web documents that are linkable to the Freebase knowledge base [6]. Freebase is a publicly available general purpose knowledge base with over 42 million entities and over 2.3 billion facts.1 The FACC1 dataset is the first publicly available web-scale collection of entity linked documents. In addition to annotated documents, the FACC1 data also contains explicit manual annotations for the TREC web track queries. We present one of the first published experiments using this data for retrieval.
For this work, we define an entity broadly to be a thing or concept that exists in the world or fiction, such as a person, a battle, a film, or a color. We focus primarily on entities that are linked to two existing publicly available knowledge bases, Wikipedia and Freebase. We use a combination of both of these knowledge bases because they provide complementary information. Wikipedia provides rich text and link associations. Freebase provides a significantly larger database of concepts, many of whom may not meet Wikipedia's standards for notability, with structured data in RDF, including categories and types.
Our work addresses two fundamental research areas using entity annotations for ad hoc retrieval. The first is the representation of both queries and documents with linked entities. What features, if any, improve retrieval effectiveness? The second is inferring latent
1As of January 27, 2014 according to Freebase.com

365

entities (and more importantly, features of entities and terms) for an information need.
The FACC1 annotations include entity annotations for queries. However, these annotations are limited to entities that are explicitly mentioned, where we hypothesize that many more latent entities are relevant to the users' information need. For example, consider the TREC query about [Barack Obama family tree]. There are explicit query entities, [Barack_Obama] and [Family_Tree]. There are also relevant latent entities such as [Ann_Dunham], [Michelle_Obama], [Barack_Obama_Sr], [Ireland], [Kenya], [DNA], and others.
One issue is that explicit entity mentions have the same fundamental problems of query-document mismatch as words. For example, a document on the topic of Obama's family history may not explicitly refer to a [Family_Tree], but may refer to other related entities, such as a [Family_Crest] and [Genealogy]. In addition, for many existing collections, no explicit entity annotations for queries exist. In both cases, it is important to infer related entities and expand the query representation.
Entities provide a wealth of rich features that can be used for representation. These include text as well as structured data. Some of the important attributes that we highlight for these experiments include: fine-grained type information (athlete, museum, restaurant), category classifications, and associations to other entities. Although we do not explore them in detail in this work we also observe that the knowledge base contains rich relational data with attributes and relations to other entities. These attributes include: gender, nationality, profession, geographical information (latitude, longitude), and temporal attributes (such as birth and death), and many more depending on the type of entity.
We hypothesize that the language in the document contexts of entity mentions differs from that found in Wikipedia or in the knowledge base description. But, mentions of the entity are also contained in text documents across the entire corpus. To address this, we propose new query-specific entity context models extracted from snippets in the feedback documents surrounding the entity's annotations. We further hypothesize that this context information will allow us to identify entities that are relevant to the query and use their presence as signals of document relevance.
To summarize, the main contributions of this work are:
· Introducing new query expansion techniques with featurebased enrichment using entity links to a knowledge base
· Demonstrating significant improvements in retrieval effectiveness when entity features are combined with existing text approaches
· Proposing a new entity modeling technique for building queryspecific context models that incorporate evidence from uncertainty inherent in automatic information extraction
· Performing the first published experiments using the FACC1 Freebase annotations for ad hoc document retrieval
· Analyzing the ClueWeb09 FACC1 annotations for their use in retrieval applications
· Providing new entity-annotated query datasets for the TREC web track queries that substantially improve entity recall
The remainder of the paper is structured as follows. Section 2 provides retrieval model background. In Section 3, we introduce the new feature expansion approach and introduce the entity context feedback model (3.4). We experimentally evaluate our approach in Section 4 on standard TREC test collections including: Robust'04, ClueWeb09B, and ClueWeb12B. Connections to related work are discussed in Section 6 before concluding.

2. BACKGROUND
2.1 Notation
We distinguish notationally between random variables in upper case (e.g. E) and possible assignments (e) in lower case. We denote count statistics of a configuration e in a sequence of as ei.
2.2 Log-linear Models
Graphical models [20], such as Markov Random Fields (MRF), are a popular tool in both information extraction and information retrieval. Dependencies between two (or more) variables (e.g. Queries and Documents) are encoded by factor functions that assign a nonnegative score to each combination of variable settings. Regarding the factor function in log space allows for arbitrary scores.
Factor functions (or similarity functions) between two variables are indicated by  (e.g. (Q, W )) which is assumed to be of loglinear form. This means that  is determined by an inner product of weight vector  and feature vector f in log-space.
2.3 Retrieval Models
The query likelihood (QL) retrieval model can be represented as a factor between a multi-word query, and a document represented as a bag of words as (Q, D) = wiQ (wi, D).
Within this framework, we adopt both the QL model and the widely used Sequential Dependence Model (SDM) [24], which incorporates word unigrams, adjacent word bigrams, and adjacent word proximity. The feature function used to match words, W to a document is a Dirichlet smoothed probability:

(W,

D)

=

log

#(W,

D)

+

µ

#(W,C) |C|

(1)

|D| + µ

This approach generalizes to bigrams "W1, W2" and unordered term proximity. Furthermore, we can apply it to different kinds of vocabularies, such as entity identifiers or categories with appropriate redefinition of the document length |D| and collection statistics.

2.4 Query Expansion
One commonly used query expansion model is the Relevance Model (RM) [22]. It is a pseudo-relevance feedback approach that uses retrieved documents to estimate the query topic. Relevant expansion terms are extracted and used in combination with the original query (the RM3 variant). We use this as our baseline text-based expansion model. Beyond unigrams, Metzler and Croft propose a generalized model, Latent Concept Expansion (LCE) [25], which models arbitrary expansion vocabularies such as words, entity identifiers, types or categories [25].
In both relevance modeling and LCE, the formulation is similar. Assuming that the retrieval score represents the probability of the document under the query, e.g. p(D|Q), document-wise multinomial distributions over a vocabulary p(V |D) are combined via a mixture model.

p(V |Q) = p(V |d)p(D = d|Q)

(2)

d

Hyperparameters of this approach are the number of expansion documents, number of expansion features, and a balance parameter for weighting the original query against the expanded query, which are further weighted according to P (V |Q).
The document probability, p(D = d|Q) is typically derived from the retrieval score s(d) by exponentiation and re-normalization over the domain of expansion documents. The document specific

366

Table 1: Example expansion terms for the query "Obama Family Tree"

Words family tree genealogy surname history crest

Entity ID Barack_Obama Michelle_Obama Family_Tree Family_Crest Barack_Hussein_Obama_Sr Family_History

Wiki Categories cat:first_families_u.s. cat:political_families_u.s. cat:bush_family cat:american_families_english cat:american_families_german cat:business_families_u.s.

Freebase Type /people/family /book/book_subject /location/country /film/film_subject /base/presidentialpets/first_family /base/webisphere/topic

#combine( #sdm( obama family tree ) #sdm( [Barack_Obama] [Family_Tree] ) #sdm( {US President} {Politician}) #sdm( [Michelle_Obama] [Ireland] [Kenya])
)
Figure 1: Example expansion of query C09-1 with entities [] and Freebase types {}.

a) Annotated Query

W Q

M

E

b) Knowledge Base

E

A

C

W

T

distribution of features is derived under the multinomial assumption

by p(V |d) =

#(V d) V #(V d)

.

3. ENTITY QUERY FEATURE MODEL
In this section we introduce the representation of queries and documents using linked entities and provide background on the models we use throughout this work.
In a preprocessing step, documents in the collection are annotated with entity links. Entity links establish a bidirectional reference from words to entities in the KB, and indirectly to Freebase types and Wikipedia categories and further related entities in the knowledge base (called neighbors, henceforth). We index these different vocabulary representations for each document in different fields. Our retrieval engine supports proximity and approximate matches with respect to each of the vocabularies.
The goal is to derive expansions across the different kinds of vocabularies such as words W , entities E, types T , and categories C to retrieve annotated documents with the goal of maximizing document retrieval effectiveness.
Figure 1 details expansions for the ClueWeb09B query 1 "obama family tree" for the words, entities and Freebase types. The first three entries constitute words, entities and types directly mentioned in the query, where the last entry includes other relevant entities. A sample of the expansion terms from our system on this query are given in Table 1.
Expansions in different vocabularies can be derived through multiple options. Entity linking the query provides very precise indicators, but may also miss many of the relevant entities. Alternative expansion entities can be found using pseudo-relevance feedback on the document collection containing entity annotations or alternatively by issuing the query against an index of the knowledge base and considering top-ranked entries. Figure 2 gives an overview of all possibilities studied in this work, which we detail in this section.

3.1 Annotated Query
The query, Q, is given as a sequence of keywords w1w2, ...w|Q|. Aside from representing the query Q by their query word representation W , we can annotate the query in the same way we preprocess

c) KB Feedback

Q

E

d) Corpus Feedback

M

E

Q

D

W

e) Entity Context

E

W

D

Q User query

D Collection document

E Knowledge base entity W Words

A Entity alias

T Freebase type

C Wikipedia Category M Entity mention

Figure 2: Overview over feature sources.

the documents before indexing. This provides annotations for all entity mentions M in the query terms together with a link to the KB entity E (cf. Figure 2a)
Resolution through the entity provides further information about its type, category and name alias information. We can additionally infer indirectly related entities by following hyperlinks on Wikipedia articles or exploiting Freebase relations (cf. Figure 2b).
For instance, terms on the entity's Wikipedia article provide a resource for related words W . These are derived through a hierarchical multinomial model by integrating over mentions and entities

f ExplWiki(Q, W ) =

p(W |E)p(E|M ) p(M |Q)

M

E

367

In the equation, p(M |Q) is a uniform distribution over annotated mentions and p(E|M ) is the entity disambiguation confidence and p(W |E) refers to the language model of the entity's article.
In addition to words, we access different alternative names A in the knowledge base through the entity link. Our knowledge base contains name aliases of different confidences, e.g. title versus anchor text, which we take into account through the multinomial distribution p(A|E).
3.2 KB Feedback
An alternative route can be taken by issuing the query against a search index containing all knowledge base entries (cf. Figure 2c). The ranking of articles can be interpreted as a distribution over entities, encoded in the feature f KB(Q, E) which is obtained by exponentiating and renormalizing the retrieval score sQ(E) of the entity under the query.
1 f KB(Q, E) = Z exp sQ(E)
Here, Z ensures normalization across the feedback entities. For instance we can derive a distribution over words W from the
highest ranked entities. This has been found to be effective in the related work [3, 34]. Further vocabularies over name aliases, related entities, types and categories can be derived as explained above.
3.3 Corpus Feedback
We can also apply a variation on pseudo-relevance feedback which we extend to document annotations (cf. Figure 2d). The unaltered relevance model provides feature f RM(Q, W ) by integrating p(D|Q) and p(W |D) over documents in the feedback set.
In a similar fashion we can derive a distribution over all mentions M , denoted by the feature f RM(Q, M ). Mentions include both the string that is linked to an entity as well as unlinked Named Entity Spans (NERs). Even if these mentions M cannot be linked to the knowledge base, they provide useful entity-like information for expansion, as used by Callan et al. [7].
For linked mentions, the entity link disambiguation probability gives an alternative indicator for relevant entities E.

f RM(Q, E) =

p(E|M )p(M |D) p(D|Q)

DM

The disambiguation confidence distribution p(E|M ) has an effect in cases where multiple entities have a high disambiguation probability for a given mention. In the experimental section we explore options ranging from noisy indicators to high-precision links, such as using only the highest ranked entity or additionally applying a NIL classification. In these conservative options we define p(E|M ) = 1 for the most confident (and non-NIL) linked entity E and 0 otherwise.
From the distribution over entities, we can follow the connection to the knowledge base (cf. Figure 2b) and derive distribution over name aliases, types, categories, and neighbor entities.

3.4 Entity Context Model
The corpus feedback provides distributions over entities. However, it is likely that relevant entities are referred to in a query-specific way which differs from the global term distribution in the knowledge base. For instance in the query "obama family tree" we expect the entity [Barack_Obama] to be referred to by personal names and less via his political function. Also, some related entities (in particular family members) are more important than others.

Our goal is to develop query-specific distributions over name aliases and related entities by inspecting the local context surrounding entity annotations for co-occurrences of entities with words and pairs of entities. In our experiments, we create three versions of each entity's query-specific context model, varying the size of the context snippets: 8 words on either side of a mention, 50 words on either side, or one sentence, where sentence boundaries are determined by a sentence-splitter.
From each feedback document D and each annotated entity mention, M , we build entity context snippets using the only contextual window around the annotation. For each entity, E, we aggregate all snippets by weighting them by the document retrieval probability p(D|Q).
The entity context model for a given entity, E, provides a distribution over words, W , which is used for the context model feature f ECM(E, W ). Likewise, the entity context model provides a distribution over co-occurring neighboring entities E as f ECM(E, E ). And by following the link to the knowledge base, features over co-occurring name aliases, types, and categories.
3.5 Learning Feature Weights
So far we introduced several features f for query expansion with words, entities, mentions, types, categories, and neighbors using various options to traverse available information sources, each representing a path in Figure 2.
The large number of features renders grid-tuning approaches infeasible. We exploit that our model falls into the family of loglinear models and can therefore be efficiently estimated with a learning-to-rank approach.
For every feature, f , we build the expansion model induced by this feature only. For example from f RM(Q, E) we build an expansion model over entities pRM (E) by normalizing across all entity candidates E .

pRM (E) =

f RM(Q, E) E f RM(Q, E )

For every document d in our training set, we compute the retrieval score under the RM1 expansion model pRM (E) using only the k highest ranked entities weighted by their respective probability under the expansion model.
Following this procedure, each feature function f (Q, V ) is converted for all vocabularies V into a feature vector for documents d in the training set. We use a log-linear learning-to-rank approach to optimize the retrieval effectiveness for the target metric under this feature function. This provides the parameter vector  which corresponds to the weights for each expansion model, when retrieving rankings for test queries.
By incorporating the retrieval score from the original query Q as a special feature function, this also determines the RM3 balance weight on the original query with respect to all expansion models.

4. EXPERIMENTAL SETUP
This section details the tools and datasets used for our experiments. Results are available online.2 The retrieval experiments described in this section are implemented using Galago,3 an open source search engine. The structured query language supports exact matching, phrases, and proximity matches needed for our retrieval models. A summary of the document collections used in these experiments is presented in Table 2. The corpora include both newswire (Robust04)
2http://ciir.cs.umass.edu/downloads/eqfe/ 3http://www.lemurproject.org/galago.php

368

Table 2: Test Collections Statistics.

Name Robust04 ClueWeb09-B ClueWeb12-B

Documents 528,155
50,220,423 52,343,021

Topic Numbers 301-450, 601-700
1-200 1-50

and web pages (ClueWeb). During indexing and retrieval, both documents and query words are stemmed using the Krovetz stemmer [21]. Stopword removal is performed on word features using the INQUERY 418 word stop list. For the web collections, the stopword list is augmented with a small collection of web-specific terms, including "com", "html", and "www". We use title queries which contain only a few keywords.
Across all collections retrieval and feedback model parameters are learned or tuned using 5-fold cross-validation. Instead of selecting a single number of feedback documents or entities, we include expansion feature models with different hyperparameters and learn a weighted combination of these along with other features. We include expansion features from one, ten, and twenty feedback entities and documents. We optimize parameters  with a coordinate-ascent learning algorithm provided in the open source learning-to-rankframework RankLib.4 Parameters are optimized for mean average precision (MAP) effectiveness directly.
Retrieval effectiveness is evaluated with standard measures, including mean average precision (MAP) at 1000. Because several of our collections focus on web search, where precision at the early ranks is important, we also report normalized discounted cumulative gain (NCGD@20) and expected reciprocal rank (ERR@20).
We now describe the aspects of the entities in documents and queries for each collection in more detail.
4.1 Knowledge Base
We use a combination of Wikipedia and Freebase as knowledge bases in these experiments. Many of the Freebase entities are contained in Wikipedia. We use the Freebase schema to map between the two knowledge bases (using attributes: /wikipedia/en_title and /wikipedia/en). These knowledge resources provide the entity features used for query expansion from linked entities described in Section 3. Our Wikipedia collection is derived from a Freebasegenerated dump of the English Wikipedia from January 2012, which contains over 3.8 million articles. For each Wikipedia entity we extract an entity representation consisting of its article text, canonical name, categories, and a distribution over aliases from redirects, Wikipedia-internal anchor text, and web anchor text from the Google cross-lingual dictionary [31]. In these experiments we also use a subset of the Freebase data: machine identifiers (MIDs), types, and aliases.
4.2 Robust'04
No publicly available entity annotations exist for Robust04 queries or documents. We do not exploit explicit entity annotations in queries, reducing the model in 3.1 to only the words in the title query. For document analysis, we use the extraction tools in the Factorie [23] NLP library. We use Factorie to perform tokenization, sentence segmentation, named entity recognition, part-of-speech tagging, dependency parsing, and entity mention finding. The entity mentions detected by Factorie are linked to the knowledge base
4http://people.cs.umass.edu/~vdang/ranklib. html

using our state-of-the-art entity linking system, KB Bridge [11], which is trained on the TAC KBP entity linking data from 20092012. For each mention, the entity linker provides a distribution over the top fifty most probable entities. Based on the TAC evaluation data, the linker has an F1 score of approximately 80-85%. We note that this entity linker is trained to detect and link traditional named entities (people, organization, and geo-political entities) and may not detect or link conceptual entities. Because of limited resources we do not entity link all documents in the Robust04 collection. Instead, we pool the top one hundred documents from all of the baseline text retrieval runs. For our resulting experiments we perform re-ranking on this pooled set of documents using the entity linking features. We use the top documents as sources for extracting both text and entity features.
4.3 ClueWeb09 and ClueWeb12
We perform experiments using two web datasets from the TREC web track. The first is the ClueWeb09 dataset. For the queries, we use the title queries, but some entity annotations are derived from the descriptions. The Google FACC1 data provides explicit entity annotations for the web track queries (2009-2012) queries, created by automatically entity linking and manually correcting entities in the text of the topic descriptions. We found these to be missing significant numbers of entities and so manually revised these annotations to improve recall and fix several annotation errors. We discuss these revisions in Section 5.3. For the documents, we use the ClueWeb09 Category-B collection, which consists of 50 million pages, including Wikipedia. For ClueWeb09-B, we apply spam filtering with a threshold of 60, using the Waterloo spam scores [9].
We use the ClueWeb12 collection with the TREC web tack 2013 queries, using only the titles. Similar to Robust04, there are no explicit entity annotations. We do not apply spam filtering on the ClueWeb12 documents because hard spam filtering was shown to hurt all the baseline retrieval runs.
5. EXPERIMENTAL EVALUATION
The effectiveness of our query feature expansion is compared with state-of-the-art word-based retrieval and expansion models. Our baseline retrieval model is the Sequential Dependence Model (SDM) [24]. We also compare to two baseline expansion models. The first is an external feedback model, which uses the Wikipedia knowledge base as a text collection and extracts terms from the top ranked article, which we call WikiRM1. Models similar to WikiRM1 were shown to be effective for these collections in previous work [3, 34]. The second baseline uses collection ranking from the SDM model and builds a collection relevance model, which we call SDM-RM3. For ClueWeb12 we also report an official baseline using Indri's query likelihood model (Indri-QL).
5.1 Overall Performance of EQFE
The overall retrieval effectiveness across different methods and collections is presented in Table 3 and Figure 3. Our EQFE model is the best performer on MAP for Robust04 and best on NDCG@20, ERR@20 and MAP on the ClueWeb12B collection. A paired t-test with -level 5% indicates that the improvement of EFQE over SDM is statistically significant for both. For ClueWeb09B, the EQFE numbers are slightly worse, but no significant difference is detected among the competing methods. The helps/hurts analysis reveals that EQFE helps a few more times than it hurts in ClueWeb09B. (cf. 4).
In order to analyze whether the EQFE method particularly improves difficult or easy queries, we sub-divide each set into percentiles according to the SDM baseline. In Figure 4 the queries are organized from most difficult to easiest. The 5% of the hardest

369

Table 3: Summary of results comparing EQFE for <title> queries across the three test collections.

Model SDM WikiRM1 SDM-RM3 EQFE

MAP 26.15 27.41 29.38 32.77

Robust04 P@20 NDCG@20 37.52 42.37 37.71 42.81 38.82 43.44 38.00 42.40

MAP 11.43 11.39 11.43 11.00

ClueWeb09B

ERR@20 NDCG@20

13.63

21.40

15.29

22.56

13.63

21.40

14.00

21.12

MAP 4.18 4.00 3.53 4.67

ClueWeb12B

ERR@20 NDCG@20

9.15

12.61

9.31

12.80

7.61

11.00

10.00

14.61

map ndcg20 ndcg20

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00

sdm

rm

wikiRm1 EQFE

(a) Robust04

0.25

0.20

0.15

0.10

0.05

0.00

sdm

rm

wikiRm1 EQFE

(b) ClueWeb09B

0.18

0.16

0.14

0.12

0.10

0.08

0.06

0.04

0.02

0.00 sdm

rm wikiRm1 EQFE indri-ql

(c) ClueWeb12B

Figure 3: Mean retrieval effectiveness with standard error bars.

0.9

sdm

0.8

rm

wikiRm1

0.7

EQFE

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0%-5%

5%-25% 25%-50% 50%-75% 75%-95% 95%-100%

difficulty percentile according to sdm

(a) Robust04

0.8

sdm

0.7

rm

wikiRm1

0.6

EQFE

0.5

0.4

0.3

0.2

0.1

0.0

0%-5%

5%-25% 25%-50% 50%-75% 75%-95% 95%-100%

difficulty percentile according to sdm

(b) ClueWeb09B

0.45

sdm

0.40

rm

wikiRm1

0.35

EQFE

indri-ql
0.30

0.25

0.20

0.15

0.10

0.05

0.00

0%-5%

5%-25% 25%-50% 50%-75% 75%-95% 95%-100%

difficulty percentile according to sdm

(c) ClueWeb12B

Figure 4: Mean retrieval effectiveness across different query-difficulties, measured according to the percentile of the SDM method.

370

map ndcg20 ndcg20

W doc 20 W sdm
E doc-ent 20 t1nn E doc-ent 20 all E doc-ent 20 t1 A kb 20 A kb 10 W kb 5 E ecm 50 A kb 5 E ecm sent W kb 1 W ecm 50 W ecm sent E ecm 8 C doc-ent 20 W ecm 8 A kb 1 E kb 50 E kb 20 A doc-ent 20 all A doc-ent 20 t1 E kb 10
M doc-ent 20 ner A doc-ent 20 t1nn
T doc-ent 20 A ecm 8 E kb 1
M doc-ent 10 ner C kb 1
A ecm sent A ecm 50
W doc-ent all 20 W doc-ent t1 20 W doc-ent t1 10 W doc-ent all 10 W doc-ent t1nn 20 W doc-ent t1nn 10
T kb 1 W doc-ent t1 1 W doc-ent all 1 W doc-ent t1nn 1

Table 4: Queries EFQE helped versus hurt over SDM baseline.

Robust04 ClueWeb09B ClueWeb12B

Queries Helped 173 68 26

Queries Hurt 47 65 8

queries are represented by the left-most cluster of columns, the 5% of the easiest queries in the right-most cluster of columns, the middle half is represented in two middle clusters (labeled "25%-50%" and "50%-75%").
This analysis shows that EQFE especially improves hard queries. For Robust04 and ClueWeb12B EQFE outperforms all methods, except for the top 5% of the easiest queries (cf. 4a and 4c). For ClueWeb09B all queries in the difficult bottom half (cf. 4b) are improved. We want to point out that we achieve this result despite having on average 7 unjudged documents in the top 20 and 2.5 unjudged documents in the top 10 (in both the "5%-25%" and "25%50%" cluster), which are counted as negatives in the analysis.
The WikiRM1 method, which is the most similar expansion method to EQFE, demonstrates the opposite characteristic, outperforming EQFE only on "easiest" percentiles.
5.2 Feature-by-Feature Study
We study the contribution of each of the features by re-ranking the pooled documents according to the feature score alone and measuring the retrieval effectiveness in MAP. The results for each collection are shown in Figure 5. It shows a subset of the top expansion features. The label on the x-axis has three attributes of the entity features: the vocabulary type, feedback source, and number of expansion terms. The vocabulary types are (A,E,C,W ,M , and T from Figure 2). The source is the original query (Q), query annotation (query ann), corpus feedback (doc), corpus feedback using entity features (doc - ent), knowledge base feedback (kb), and entity context model feedback (ecm). The last number in the description usually indicates the number of feedback terms (1, 5, 10, 20, and 50). For ecm it indicates the size of the context model window. We note that for several classes of features have similar names. These are variations of the same expansion feature. For example, the most confident entity (t1), the most confident entity whose score is above the NIL threshold (t1nn), or any entity above the NIL threshold (all).
Entity identifiers E are top features across all collections, but every collection prefers entities expanded by a different paradigm: For Robust from corpus feedback, for ClueWeb09B from the entity context model, and for ClueWeb12B from knowledge base expansion with five entities.
For the Robust04 collection, our study confirms that query keywords are highly indicative of relevance and accordingly words from corpus feedback are strong features. This is in not the case for the ClueWeb collections.
For both ClueWeb collections, the entity context model with window size 8 performs well. Further, name aliases from both corpus feedback and from entity context models are highly effective, even where the entity identifiers themselves are not. We believe this is because the recall of the entity identifiers in the FACC1 data is limited. Here the name aliases bridge this annotation gap.
We note that certain vocabularies such as categories and types do not perform well on their own, but likely help in combination with other features.

map

map

0.30 0.25 0.20 0.15 0.10 0.05 0.00
0.12 0.10 0.08 0.06 0.04 0.02 0.00
0.040 0.035 0.030 0.025 0.020 0.015 0.010 0.005 0.000

robust clueweb09b clueweb12b

map

Figure 5: Features sorted by retrieval effectiveness on its own.

E ecm 8 A doc-ent 20 all
W ecm 8 A kb 10 W kb 1
W ecm 50 W sdm E kb 1
A ecm 50 A kb 1
A doc-ent 20 t1nn A doc-ent 20 t1
E doc-ent 20 t1nn E kb 20
W doc-ent t1 10 W doc-ent all 10
A kb 20 A kb 5
W doc-ent all 20 W doc-ent t1nn 10
W doc-ent all 1 W doc-ent t1nn 1
W doc-ent t1 1 W doc 20
E doc-ent 20 all E ecm 50
W doc-ent t1 20 C doc-ent 20 A ecm 8 C kb 1 T doc-ent 20 T kb 1
W doc-ent t1nn 20 W kb 5 E kb 50 E kb 10
E doc-ent 20 t1

W sdm W kb 5 W kb 1 W ecm 8 W ecm 50 A kb 20 A ecm 8 A kb 10 A kb 5 A ecm 50 W doc 20 E doc-ent 20 t1 E doc-ent 20 all M doc-ent 20 ner A kb 1 E doc-ent 20 t1nn M doc-ent 10 ner A doc-ent 20 t1nn A doc-ent 20 t1 A doc-ent 20 all E ecm 50 E ecm 8 W doc-ent t1 20 W doc-ent all 20 W doc-ent t1nn 20 W doc-ent t1nn 10 W doc-ent t1 10 W doc-ent all 10 C kb 1 C doc-ent 20 W doc-ent t1nn 1 W doc-ent t1 1 W doc-ent all 1 T doc-ent 20 T kb 1 E kb 50 E kb 20 E kb 1 E kb 10

371

Table 5: Different classes of entities are more prevalent in different data set. Number of queries that mention each entity class.

Dataset

Overall Freebase NER PER/ORG/LOC

Robust04

249

243 85

49

Clueweb09

200

191 108

80

ClueWeb12

50

48 26

16

Table 6: Mean Average Precision over subsets of Robust04 queries that mention entities of respective classes.

Method Overall Freebase NER PER/ORG/LOC

SDM MSE EQFE

26.15 30.49 32.77

26.61 31.11 31.02 36.45 33.33 38.28

27.72 31.98 33.31

5.3 Error Analysis of ClueWeb09
We now perform an analysis of the ClueWeb09 results to better understand why EQFE using entity feature expansion does not significantly improve the results. This case is particularly surprising because it is the only dataset where explicit entity query annotations are available.
We first examine the FACC1 query annotations. The FACC1 dataset contains entity annotations for 94 of the 200 queries. Upon inspecting the annotations, we found that despite manual labeling, many entities were not annotated. The queries were manually re-annotated, resulting in 191 of the 200 queries containing an entity. The revised annotations are available on our website.5 We used the revised query entity annotations for our experiments on ClueWeb09B.
The remaining queries without entity links are interesting. Several contain entities that are not noteworthy enough to be included in existing public knowledge bases, including:"jax chemical company", "fickle creek farm", "sit and reach test", and "universal animal cuts". The remaining are not entity-centric without clearly defined concepts: "getting organized" and "interview thank you".
However, even after coverage of queries is improved, the feature does not appear to help overall. To better understand the contribution (or lack thereof) of explicit entities in the query, we evaluated a query model that uses only entity identifiers to retrieve documents. Surprisingly, the entities alone have poor effectiveness, with a MAP of 0.048, an NDCG@20 of 0.162, and an ERR@20 of 0.123. This is less than half the effectiveness of the SDM baseline. We observe that 72.5% of the documents returned using the entity model are unjudged. The retrieved results differ significantly from the pool of judged documents. Further assessments are required to assess the model effectiveness. Beyond unjudged documents, we also examine the potential for explicit entities by analyzing the relevance judgments.
We analyze the potential for explicit entities using all of the judged documents for the queries. We find that 37.4% of the relevant documents in ClueWeb09B do not contain an explicit query entity. The largest source of missing entities in documents are those in Wikipedia. Missing entity links for Wikipedia accounts for 24.6% of the documents. The FACC1 annotations do not contain annotations for the majority of Wikipedia articles in ClueWeb09B. Of the relevant documents that contain at least one entity, 43% of these contain at least one mention of an explicit query entity. This indicates that 57% of the remaining relevant documents do not contain the explicit query entity and cannot be matched using this feature alone. The reasons for the mismatch is an area for future work. It is caused by both missing entity links as well as fundamental query-document mismatch.
5.4 Entity Analysis of queries
In this section we further study the entity characteristics of these datasets. How common are different classes of entities in the
5http://ciir.cs.umass.edu/downloads/eqfe/

queries? We manually classify the presence of entities in the queries for all of the datasets.
The queries are labeled with three classes of entities. The first is the most general, whether the entity occurs in Freebase. The second whether it contains a named entity that would be detected by a typical entity recognition (NER) system. The last class of entities is narrower and is restricted to people, organizations, and locations. For each query we classify whether or not an entity of a particular class appears in the query. We do not examine the number of entities in the query or the centrality of the entity to the query.
The entity classification statistics are shown in Table 5. We observe that between 95% and 98% of the queries contain at least one mention of a Freebase entity. Many of the entities in the queries are general concepts, such as `mammals', `birth rates', `organized crime', and `dentistry'. For the web queries, approximately half the queries (54% and 52%) contain a named entity. The distribution of the types in the web queries is similar. A smaller percentage of queries for Robust04 contain named entities, only 34%. One reason for this is that web queries are more likely to contain brand names, actors, songs, and movies. Examples of these include `Ron Howard,', `I will survive', `Nicolas Cage', `Atari', `Discovery Channel', `ESPN', and `Brooks Brothers'.
When the entities are restricted to people, organizations, and locations the fraction of queries containing entities decreases further. The fraction of entities that fall into this limited class is between 59% and 74% of the queries containing named entities overall. These entities belong to the "MISC" category and include diseases, songs, movies, naval vessels, drugs, nationalities, buildings, names of government projects, products, treaties, monetary currencies, and others. These appear to be common in queries and more emphasis should be placed on finer grained entity type classification.
5.5 Effectiveness on Robust04
In this section we describe an analysis of the effectiveness of the previously described entity query classes for the Robust04 dataset. We study the behavior of three retrieval models: sequential dependence model (SDM), multiple source expansion (MSE) [3], and entity-based feature expansion (EQFE). The results are shown in Table 6.
We observe that the EQFE expansion model is the best performing model across all classes of queries. We also note that queries that contain entities perform better for all retrieval models. The differences with queries containing Freebase entities are small, which is not surprising because most of the queries contain at least one entity. EQFE performs consistently better than the other models for all classes of queries.
The most interesting finding is the comparison of queries with named entities (NER). Queries containing named entities, but not restricted to PER/ORG/LOC show a difference over the other classes of queries. It demonstrates that the queries with `MISC' entities perform better than other classes of entity queries for all models. The gains are the largest for this class of queries for EQFE compared with the baseline SDM retrieval model.

372

6. RELATED WORK
6.1 Query Expansion
Query expansion techniques have been well studied for many models [22, 29]. Unlike most models, our approach goes beyond words or even features of words and includes features from entity links. The mostly closely related work is Latent Concept Expansion (LCE) model proposed by Metzler and Croft [25]. It builds upon the sequential dependence retrieval framework and introduces the idea of using arbitrary features for expansion. However, although a general framework is proposed they find improvements using only unigram features. Another well-known expansion model is Latent Concept Analysis from Xu and Croft [33], which selects `concepts', limited to unigram and phrase features that co-occur near query terms in top ranked documents. The contribution of words versus phrases was not tested. In contrast, we use words, phrases, and structured entity attributes in EQFE to improve retrieval effectiveness.
6.2 Entity Retrieval
Using entities in retrieval is an area that has been well studied. In particular, the research area of retrieving entities has received significant recent attention. Entity retrieval was studied at the TREC entity retrieval track [2], at INEX with the entity ranking [12] and linked data tracks [32], the workshop on Entity Oriented and Semantic Search [1, 5], and other venues. In contrast, we focus on document retrieval leveraging entity annotations. Exploiting entity links and other types of semantic annotations is an area of open research. The workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR) [4, 18] has run over the last five years, and highlights the need for continued research in this area.
6.3 World Knowledge
Using Wikipedia as a source of world knowledge has been demonstrated to improve a variety of tasks, including retrieval. It is a common source of external query expansion [13, 3, 34]. Wikipedia entities as a basis for semantic representation demonstrated significant gains for a variety of NLP tasks. These tasks include semantic relatedness [14], document clustering [14], and entity linking [10]. We demonstrate that leveraging structured attributes of knowledge base entities similarly provides substantial gains in effectiveness for retrieval.
6.4 Entity Context Model
Building entity context models from their surrounding representation has been studied in the past. In 1994, Conrad and Utt [8] used all paragraphs in the corpus surrounding named entity mentions to represent the entity, allowing free text queries to find names associated with a query. Ten years later, Raghavan et al. [27] extended that idea to use language modeling as a representation and showed that these models could successfully be used to cluster, classify, or answer questions about entities. In these cases, the entity's context was a paragraph or a fixed number of words surrounding all mentions of the entity in the corpus. More recently, the work of Schlaefer et al. [30] expanded the representation of a Wikipedia entity using extracted "text nuggets" from the web for use in the Watson question answering system. Nuggets that were scored as relevant to the entity were used as its context, even if the nugget did not contain an actual mention.
Our entity context model (ECM) differs from existing work in three key ways. First, it uses state-of-the-art disambiguated entity links. If there are multiple ambiguous mentions of the same name, the contexts are separated based on their linked entity. Also, we do

this for all types of concepts that exist in the knowledge base rather than just traditional named entities (person, organization, location).
Second, our context models are query focused. We construct an entity context model from documents retrieved in response to the query. This change is important for large corpora because for entities with multiple diverse topics a representation across the entire collection will blend these topics together and lose their distinguishing characteristics. For example, the ClueWeb09 query [obama family tree] focuses on aspects of Obama's family life and relationships to relatives, which is a relatively obscure topic when compared with more popular aspects such as "obamacare."
Finally, our approach captures not just words and phrases surrounding the mention, but structured annotations from co-occurring entities: their mentions and features of them, including types and categories. We also incorporate the uncertainty of extracted features, both the source relevance and entity link probability.
7. CONCLUSION
We have shown that features derived from linked entities can be used to improve the effectiveness of document retrieval. In qualitatively different collections (Robust04 and ClueWeb12B), the EQFE method was on average the strongest performer compared to several state-of-the-art baselines. These are some of the first reported results using the FACC1 Freebase annotations for ad hoc retrieval.
One limitation of this work is that it depends upon the success and accuracy of the entity annotations and linking. It would be useful to understand the accuracy and utility more robust detection of entities such as `poverty', or `term limits' rather than focusing primarily on people, organizations, and locations.
Our results are also affected by entities that are detectable but that are not in the knowledge base ­ e.g., `fickle creek farms'. For these entities, there is no knowledge base entry to leverage, so the simplest solution is to consider only the unstructured word features. Lastly, we also described and successfully incorporated an entity context model that represents an entity by the language surrounding its mentions in the context of the query.
This work presents a first step leveraging large-scale knowledge resources that have become available in the last several years. We expect that as these knowledge resources mature that entity-based representations of both queries and documents will grow in importance, supporting increasingly complex information needs.
Acknowledgements
This work was supported in part by the Center for Intelligent Information Retrieval, in part by IBM subcontract #4913003298 under DARPA prime contract #HR001-12-C-0015, and in part by NSF grant #IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
8. REFERENCES
[1] K. Balog, D. Carmel, A. P. de Vries, D. M. Herzig, P. Mika, H. Roitman, R. Schenkel, P. Serdyukov, and T. T. Duc. The first joint international workshop on entity-oriented and semantic search (JIWES). In ACM SIGIR Forum, volume 46, pages 87­94. ACM, 2012.
[2] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the TREC 2011 entity track. In Proceedings of the Text REtrieval Conference (TREC), 2011.
[3] M. Bendersky, D. Metzler, and W. B. Croft. Effective query formulation with multiple information sources. In Proceedings of the fifth ACM international conference on Web

373

search and data mining, WSDM '12, pages 443­452, New York, NY, USA, 2012. ACM.
[4] P. Bennett, E. Gabrilovich, J. Kamps, and J. Karlgren. Sixth Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'13). In In Proceedings of CIKM '13, pages 2543­2544, New York, NY, USA, 2013. ACM.
[5] R. Blanco, H. Halpin, D. M. Herzig, P. Mika, J. Pound, H. S. Thompson, and T. T. Duc. Entity search evaluation over structured web data. In Proceedings of the 1st international workshop on entity-oriented search workshop (SIGIR 2011), ACM, New York, 2011.
[6] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In In Proceedings of SIGMOD '08, pages 1247­1250, New York, NY, USA, 2008. ACM.
[7] J. P. Callan, W. B. Croft, and J. Broglio. Trec and tipster experiments with inquery. Information Processing & Management, 31(3):327­343, 1995.
[8] J. G. Conrad and M. H. Utt. A system for discovering relationships by feature extraction from text databases. In SIGIR'94, pages 260­270. Springer, 1994.
[9] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and Effective Spam Filtering and Re-ranking for Large Web Datasets, Apr. 2010.
[10] S. Cucerzan. TAC Entity Linking by Performing Full-document Entity Extraction and Disambiguation. In Proceedings of the Text Analysis Conference 2011, 2011.
[11] J. Dalton and L. Dietz. A Neighborhood Relevance Model for Entity Linking. In Proceedings of the 10th International Conference in the RIAO series (OAIR), RIAO '13, New York, NY, USA, May 2013. ACM.
[12] G. Demartini, T. Iofciu, and A. P. De Vries. Overview of the INEX 2009 entity ranking track. In Focused Retrieval and Evaluation, pages 254­264. Springer, 2010.
[13] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '06, pages 154­161, New York, NY, USA, 2006. ACM.
[14] E. Gabrilovich and S. Markovitch. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI'07, pages 1606­1611, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.
[15] E. Gabrilovich, M. Ringgaard, and A. Subramanya. FACC1: Freebase annotation of ClueWeb corpora, version 1 (release date 2013-06-26, format version 1, correction level 0), June 2013.
[16] D. W. Huang, Y. Xu, A. Trotman, and S. Geva. Focused access to XML documents. chapter Overview of INEX 2007 Link the Wiki Track, pages 373­387. Springer-Verlag, Berlin, Heidelberg, 2008.
[17] H. Ji, R. Grishman, and H. Dang. Overview of the TAC2011 knowledge base population track. In Text Analysis Conference, 2011.
[18] J. Kamps, J. Karlgren, and R. Schenkel. Report on the Third Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR). SIGIR Forum, 45(1):33­41, May 2011.
[19] R. Kaptein, P. Serdyukov, and J. Kamps. Linking wikipedia to the web. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information

retrieval, SIGIR '10, pages 839­840, New York, NY, USA, 2010. ACM. [20] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
[21] R. Krovetz. Viewing morphology as an inference process. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, pages 191­202. ACM, 1993.
[22] V. Lavrenko and W. B. Croft. Relevance-Based Language Models. In Proceedings of the ACM SIGIR 01 conference, pages 120­127, 2001.
[23] A. Mccallum, K. Schultz, and S. Singh. Factorie: Probabilistic programming via imperatively defined factor graphs. In In Advances in Neural Information Processing Systems 22, pages 1249­1257, 2009.
[24] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '05, pages 472­479, New York, NY, USA, 2005. ACM.
[25] D. Metzler and W. B. Croft. Latent concept expansion using markov random fields. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 311­318, New York, NY, USA, 2007. ACM.
[26] P. Mika and T. Potter. Metadata statistics for a large web corpus. In Proceedings of the Linked Data Workshop (LDOW) at the International World Wide Web Conference, 2012.
[27] H. Raghavan, J. Allan, and A. McCallum. An exploration of entity models, collective classification and relation description. In KDD Workshop on Link Analysis and Group Detection, pages 1­10, 2004.
[28] L. Ratinov, D. Roth, D. Downey, and M. Anderson. Local and global algorithms for disambiguation to wikipedia. In ACL, 2011.
[29] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, editor, The SMART Retrieval System: Experiments in Automatic Document Processing, Prentice-Hall Series in Automatic Computation, chapter 14, pages 313­323. Prentice-Hall, Englewood Cliffs NJ, 1971.
[30] N. Schlaefer, J. C. Carroll, E. Nyberg, J. Fan, W. Zadrozny, and D. Ferrucci. Statistical source expansion for question answering. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM '11, pages 345­354, New York, NY, USA, 2011. ACM.
[31] V. I. Spitkovsky and A. X. Chang. A Cross-Lingual dictionary for english wikipedia concepts. In Conference on Language Resources and Evaluation, 2012.
[32] Q. Wang, J. Kamps, G. R. Camps, M. Marx, A. Schuth, M. Theobald, S. Gurajada, and A. Mishra. Overview of the INEX 2012 linked data track. In INitiative for the Evaluation of XML Retrieval (INEX), 2011.
[33] J. Xu and W. B. Croft. Improving the effectiveness of information retrieval with local context analysis. ACM Trans. Inf. Syst., 18(1):79­112, Jan. 2000.
[34] Y. Xu, G. J. F. Jones, and B. Wang. Query Dependent Pseudo-relevance Feedback Based on Wikipedia. In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '09, pages 59­66, New York, NY, USA, 2009. ACM.

374

Leveraging Knowledge across Media for Spammer Detection in Microblogging

Xia Hu, Jiliang Tang, and Huan Liu
Computer Science and Engineering Arizona State University Tempe, AZ 85287, USA
{xia.hu, jiliang.tang, huan.liu}@asu.edu

ABSTRACT
While microblogging has emerged as an important information sharing and communication platform, it has also become a convenient venue for spammers to overwhelm other users with unwanted content. Currently, spammer detection in microblogging focuses on using social networking information, but little on content analysis due to the distinct nature of microblogging messages. First, label information is hard to obtain. Second, the texts in microblogging are short and noisy. As we know, spammer detection has been extensively studied for years in various media, e.g., emails, SMS and the web. Motivated by abundant resources available in the other media, we investigate whether we can take advantage of the existing resources for spammer detection in microblogging. While people accept that texts in microblogging are different from those in other media, there is no quantitative analysis to show how different they are. In this paper, we first perform a comprehensive linguistic study to compare spam across different media. Inspired by the findings, we present an optimization formulation that enables the design of spammer detection in microblogging using knowledge from external media. We conduct experiments on real-world Twitter datasets to verify (1) whether email, SMS and web spam resources help and (2) how different media help for spammer detection in microblogging.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Classification; I.2.7 [Artificial Intelligence]: Natural Language Processing
General Terms
Algorithm, Performance, Experimentation
Keywords
Spammer Detection, Twitter, Emails, SMS, Web, CrossMedia Mining, Social Media
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609632 .

1. INTRODUCTION
Microblogging ­ a style of communicating through shortform content ­ has emerged as a popular social networking platform. Microblogging systems have been increasingly used for large-scale information dissemination and sharing in various fields such as marketing, journalism or public relations. With microblogging's growing popularity, activities of spamming have become rampant in launching various attacks in the medium. For example, the spammers spread ads to generate sales, disseminate pornography, viruses, phishing, or simply to compromise a system's reputation [3]. To improve user experience and the overall value of a system, it is essential to detect spammers in microblogging.
Existing methods for spammer detection in social media [26] focus on using social networking information. These network-based methods characterize the spammers by analyzing the network features, e.g., social status. The assumption behind this strategy is that it is difficult for the spammers to establish a large number of social relations with legitimate users. Different from other social media sites, in microblogging, users can follow anyone without prior consent from the followee. Many users just follow back when they are followed by someone for the sake of courtesy [20]. So, the spammers can easily enhance their influence score to fool the system. In this case, content analysis could complement network-based methods in spammer detection; thus, we explore the use of content information in this work.
A straightforward way to perform content-based spammer detection [22] is to model this task as a supervised learning problem. These methods extract effective textual features from the messages and build a classifier or a regressor based on the features. Given a new user, the built model can output a class label or score to determine whether it is a spammer based on microblogging messages the user posted. Content-based methods become difficult to be directly applied due to the distinct features of microblogging data. First, in microblogging, it is time-consuming and labor intensive to obtain labeled data, which is essential in building an effective supervised spammer detection model. Given the size and dynamic nature of microblogging, a manual labeling process is neither scalable nor sensible. Second, the texts in microblogging are short and noisy; thus, we lack sufficient aggregated information to evaluate the given messages. These present great challenges to directly making use of existing content-based methods for effective spammer detection in microblogging.
While the problem of spamming in microblogging is relatively new, it has been extensively studied for years in other

547

platforms, e.g., email communication [4], SMS [14] and the web [31]. Similarly, the spammers in these platforms unfairly overwhelm other users by spreading unwanted information, which leads to phishing, malware, and scams [20]. Also, it has been reported in Natural Language Processing (NLP) literature that microblogging is not as noisy as was expected [2]. Although microblogging is an informal communication medium, it has been shown to be similar to other platforms [21] and it is seemingly possible to employ NLP tools to "clean" it [11]. Motivated by the previous findings, we explore the possibility of using knowledge learned from other platforms to facilitate spammer detection in the context of microblogging.
In this paper, we explore the use of resources available in other media to help spammer detection in microblogging. To study this problem, we need to answer the following questions: Are the resources from other media potentially helpful for spammer detection in microblogging? How do we explicitly model and make use of the resources from other media for spammer detection? Is the knowledge learned from other media helpful for microblogging spammer detection? By answering the above questions, this paper presents the following contributions:
· Conducting a quantitative analysis of linguistic variation of spam resources from different media;
· Formally defining the problem of leveraging knowledge across media for spammer detection in microblogging;
· Presenting a novel framework of leveraging knowledge from existing corpora to help spammer detection in microblogging; and
· Systematically evaluating the proposed method on realworld Twitter, email, SMS and web datasets and elaborating the effects of the knowledge learned from different media on spammer detection in Twitter.
The remainder of this paper is organized as follows. In Section 2, we conduct a quantitative study to examine the differences between spam corpora in different media from a linguistic perspective. In Section 3, we formally define the problem of leveraging knowledge across media for spammer detection in microblogging. In Section 4, we propose a novel framework for the problem we study. In Section 5, we report empirical results on real-world datasets. In Section 6, we review existing literature related to our work. In Section 7, we conclude this work and present some future work.
2. LINGUISTIC VARIATION ANALYSIS
This work is motivated by numerous spam resources available in other well-studied media, e.g., email, SMS and web. A natural question could be, given the short and noisy form of microblogging messages, how different are the texts in microblogging when compared to those in other media? Before proceeding further, we also examine whether the textual information from other media is potentially useful in the problem we study.
2.1 Datasets
Two Twitter datasets are used in our study for experiment purposes, i.e., TAMU Social Honeypots and Twitter Suspended Spammers. In addition, three representative datasets from different types of media, including Enron

Email Dataset, SMS Dataset and Web Dataset, are used in the analysis. The statistics of the datasets are presented in Table 1. Now we introduce the datasets in detail.
TAMU Social Honeypots Dataset (TweetH): Lee et al. [22] created a collection of 41,499 Twitter users with identity labels: spammers and legitimate users. The dataset was collected from December 30, 2009 to August 2, 2010 on Twitter. It consists of users, their number of followers and tweets. We filtered the non-English tweets and users with less than two tweets.
Twitter Suspended Spammers Dataset (TweetS): We employed a data crawling process, which is similar to [32, 34], to construct this dataset. We first crawled a Twitter dataset from July to September 2012 via the Twitter Search API. The users that were suspended by Twitter during this period are considered as the gold standard [32] of spammers in the experiment. We then randomly sampled the legitimate users from a publicly available Twitter dataset provided by TREC 2011.1 We filtered the non-English tweets and users with less than two tweets.
The first dataset TweetH has balanced number of spammers and legitimate users. To avoid effects brought by different class distribution, according to the literature of spammer detection [22], we made the two classes in TweetS imbalanced, i.e., the number of legitimate users is much greater than that of spammers in the dataset.
Enron Email Dataset (Email): We used a subset of a widely used Enron email dataset,2 which is collected during the investigation of Enron corporation and contains more than 200,000 emails between its employees. The emails in this dataset are preprocessed and used as a testbed in [25] for experiments. Each email in the dataset is labeled as either "spam" or "ham".
SMS Dataset(SMS): We used the SMS spam collection provided by Almeida et al. [1] for analysis. This dataset is constructed based on two sources, Grumbletext web site3 and NUS SMS Corpus.4 The spam messages were manually labeled, and the ham messages were randomly sampled from the NUS SMS Corpus. To the best of our knowledge, this is the largest public SMS spam dataset.
Web Dataset (Web): Web spam is a key challenge for internet users. Web pages which are created to deceive other users by manipulating search engine. Webb et al. [31] constructed the Web Dataset. This is the largest publicly available dataset to the best of our knowledge. We removed the web pages that have no textual content or only contain http request error information.
2.2 Lexical Analysis
To evaluate the style of a language, many metrics have been proposed in literature of linguistics and communication [2, 30]. In this subsection, we first introduce the metrics used in our study and then discuss lexical analysis results on the datasets from different media.
Basic Statistics: average Word Length (WL, in characters) and average Sentence Length (SL, in words) are used to evaluate the basic style of different datasets. In addition to those, we further employ other widely used lexical metrics in the analysis. We list the metrics below.
1http://trec.nist.gov/data/tweets/ 2http://www.isi.edu/~adibi/Enron/Enron.htm 3http://www.grumbletext.co.uk/ 4http://wing.comp.nus.edu.sg/SMSCorpus/

548

Table 1: Statistics of the Datasets TweetH TweetS Email

# of Spam Messages # of Legitimate Messages # of Messages Avg. # of Words per Document

1,310,318 1,220,198 2,530,516
18.64

71,842 308,957 380,799
17.88

10,582 13,990 24,572 168.87

SMS
747 4827 5574 14.59

Web
22,386 N.A. 82,386 57.67

Table 2: Lexical Analysis Results

Basics

Lexical Analysis

WL SL TTR LD OOV

TweetH TweetS Email SMS Web

4.12 12.95 5.42 0.48 0.32 3.95 12.38 5.65 0.50 0.31 4.52 17.88 5.46 0.53 0.29 3.99 12.60 6.54 0.45 0.34 4.81 18.66 6.13 0.48 0.32

Type-Token Ratio (TTR): This is a widely used metric

to evaluate the difficulty (or readability) of words, sentences

and documents by measuring their lexical variety [7, 33].

The basic assumption of using TTR is that difficult words

are those that appear least often in a document. Given a

corpus D, TTR is calculated as T T R(D) =

wD

F req(w) Size(D)

,

where w means a word (token) in the corpus, F req(w) means

word frequency of w in D, and Size(D) means the number

of distinct words (types) in D. In practice, a higher TTR

indicates a larger amount of lexical variation and a lower

score indicates relatively less lexical variation [33].

Lexical Density (LD): We employ lexical density to

further analyze the stylistic difference between different cor-

pora. Lexical words [15], also known as content or infor-

mation carrying words, refer to verbs, nouns, adjectives and

adverbs. Similarly, given a document D, LD is defined as

LD(D) =

wLex

F req(w) Size(D)

,

where

Lex

means

the

whole

lex-

ical words dictionary. In general, a higher lexical density

indicates that it is a more formal document, and a lower

lexical density represents a more conversational one.

Out-of-Vocabulary (OOV): This metric is to measure

the ratio of out-of-vocabulary words in the corpora. We use

a list of top 10,000 words with highest frequency provided

by the Project Gutenberg [16] in our study. In general, a

higher OOV rate indicates that the language is more infor-

mal. Many NLP and IR models suffer from high OOV rates.

Experimental results of the lexical analysis are presented

in Table 2. By comparing the results of different metrics, we

observe the following: (1) The word lengths of different cor-

pora are very similar, and the sentence lengths of TweetH,

TweetS and SMS are smaller than those of more formal me-

dia Email and Web. This indicates that the textual form of

microblogging data is similar to SMS, and relatively different

from email and web. (2) In most of the tests, microblogging

data is similar to the datasets from the other media. It

demonstrates that, although microblogging is considered an

informal media, the language use is similar to that in other

media, especially in email and SMS. We observe that the

type-token ratios of microblogging are smaller than those of

SMS and web. It suggests that the language used in mi-

croblogging is easier than that in the other two platforms.

We further employ hypothesis testing to examine the lex-

ical differences between microblogging datasets and other

Table 3: Hypothesis Testing Results (P-Values)

TweetH

TweetS

TTR LD OOV TTR LD OOV

Email 0.318 0.108 0.442 0.234 0.267 0.308 SMS <0.01 0.205 0.350 <0.01 0.082 0.163 Web <0.01 0.623 0.398 0.108 0.551 0.462

datasets. For each lexical metric, we form a null hypothesis for a microblogging dataset and a dataset from the other media. The null hypothesis is: in terms of the specific lexical metric, there is no difference between microblogging data and data from the other media. We test the hypotheses on all pairs of the datasets for all the three lexical metrics.
In particular, to verify the difference between TweetH and Email datasets on the TTR, we construct two vectors ttrth and ttrem. Each element of the first vector ttrth is obtained by calculating the TTR score of a subset sampled with bootstrapping from TweetH dataset. Similarly, each element in the second vector corresponds to the TTR score of a subset sampled with bootstrapping from Email dataset. In the experiment, the two vectors contain equal number of elements.5 Each element in the vectors corresponds to 100 data instances. We formulate a two-sample two-tail t-test on the two constructed vectors ttrth and ttrem. We examine whether there is sufficient statistical evidence to support the hypothesis that the two datasets have the same sample mean, and it is defined as follows:

H0 : th - em = 0

(1)

H1 : th - em = 0

where H0 is the null hypothesis, H1 is the alternative hypothesis, and c and r represent the sample means of the two vectors, respectively. Similarly, we form the hypothesis testings for other pairs of datasets with other lexical metrics.
The t-test results, p-values, are summarized in Table 3. From the table, we can observe the following: (1) With few exceptions, the results are much greater than the significance level  = 0.05. It demonstrates that there is no statistical evidence to reject the null hypothesis in the tests on the two datasets. In other words, the results suggest that microblogging data is not significantly different from the datasets in other media. (2) In some tests, microblogging data appears more similar to Email than the other datasets.
In conclusion, while characteristics of different datasets appear different, there are no statistically significant lexical differences between them. The resources from other media are potentially useful in the task we study. Next, we formally define the problem we study and introduce the proposed learning framework for spammer detection.

5Note this is the setting used for experiment purposes, and it is not a mandatory setting for a two-sample t-test.

549

3. PROBLEM STATEMENT

In this section, we first present the notations and then

formally define the problem we study.

Notation: lower-case bold Roman letters (e.g., a) denote

column vectors, upper-case letters (e.g., A) denote matri-

ces, and lowercase letters (e.g., a) denote scalars. A(i, j) denotes the entry at the ith row and jth column of a ma-

trix A. Let A denote the Euclidean norm, and A F

the Frobenius norm of the matrix A. Specifically, A F =

m i=1

n j=1

A(i,

j)2.

Let AT

and

T r(A) denote the trans-

pose and trace of A, respectively.

Let S = [X, Y] be available resources from other media,

with the content information X and identity label matrix Y. We use term-user matrix X  Rm×d to denote con-

tent information, i.e., posts written by the users, where

m is the number of textual features, and d is the number

of users in the other media. X = {X1, X2, ..., Xr} means

the combination of content information from multiple media, and Y  Rd×c = {Y1, Y2, ..., Yr} means the combi-
nation of label information from the media. For each user (xi, yi)  Rm+c consists of message content and identity label, where xi  Rm is the message feature vector and yi  Rc is the spammer label vector. In this paper, we con-

sider the task we study as a two-class classification problem,

i.e., c = 2. For example, yi = (1, 0) means this user is a spammer. yiT yi = 1 constrains that yi has to have one la-
bel and cannot be (0, 0) or (1, 1). It is practical to extend

this setting to a multi-class or regression problem. We use T  Rm×n to denote the content information of microblog-

ging users, where m is the number of textual features, and

n is the number of users in microblogging. The texts from

microblogging and other media share the same feature space.

We now formally define the problem as follows:

We have a set of resources S from different media, with

the content information X = {X1, X2, ..., Xr} and identity label information Y = {Y1, Y2, ..., Yr}. Given the content

information T from microblogging, our goal is to automat-

ically infer the identity labels for unknown users in T as

spammers or legitimate users.

4. LEVERAGING KNOWLEDGE ACROSS MEDIA FOR SPAMMER DETECTION
We plot the work flow of our proposed framework in Figure 1. From the figure, we see that there are two constraints on the learned model for spammer detection. As shown in the upper right part of the figure, the first constraint is from the lexicon information U, which is learned from the other media sources S. As shown in the lower right part of the figure, the second constraint is a Laplacian regularization M learned from microblogging content information. We now introduce each part of the proposed framework in detail.
4.1 Modeling Knowledge across Media
As we discussed in the last section, from a linguistic perspective, it does not show significant difference between microblogging data and other types of data. A straightforward method to make use of external information is to learn a supervised model based on data from the other media, and apply the learned classifier on microblogging data for spammer detection. However, this method yields two problems to be directly applied to our task. First, text representation models, like n-gram model, often lead to a high-dimensional

T =

Ut Ht

Vt

Spammer Detection

Modeling Knowledge across Media
UH V =
S = {Email, SMS, Web}

M

Microblogging

Modeling Content Information

Figure 1: Illustration of the Proposed Spammer Detection Framework

feature space because of the large size of data and vocabulary. Second, texts in the media are short, thus making the data representation very sparse [21].
To tackle the problems, instead of learning knowledge at word-level, we propose to capture the external knowledge from topic-level. In particular, the proposed method is built on the orthogonal nonnegative matrix tri-factorization model (ONMTF) [9]. The basic idea of the ONMTF model is to cluster data instances based on distribution of features, and cluster features according to the distribution of data instances. The principle of ONMTF is consistent with PLSI [17], in which each document is a mixture of latent topics that each word can be generated from. The ONMTF can be formulated by optimizing:

min
U,H,V0

X - UHVT

2 F

,

(2)

s.t. UT U = I, VT V = I,

where X is the content matrix, and U  Rm + ×c and V  Rd+×c are nonnegative matrices indicating low-dimensional representations of words and users, respectively. m is the size of vocabulary, c is the number of classes, d is the number of users. H  Rc+×c provides a condensed view of X. The orthogonal and nonnegative conditions of U and V provide
a hard assignment of class label to the words and users.
With the ONMTF model, we project the original content
information from the other media into a latent topic space.
By adding a topic-level least squares penalty to the ONMTF,
our proposed framework can be mathematically formulated
as solving the following optimization problem:

min
U,H,V,W0

J=

X - UHVT

2 F

+

VW - Y

2 F

,

(3)

s.t. UT U = I, VT V = I,

where W represents the weights and Y is the label matrix. In the formulation, the first term is the basic factorization model, and the second introduces label information from the other media by using a linear penalty.  is to control the effect of external information to the learned lexicon U, in which each row represents the predicted label of a word.
As the problem in Eq. (3) is not convex with respect to the four variables together, there is no closed-form solution for the problem. Next, we introduce an alternative scheme to solve the optimization problem.

550

4.1.1 Optimization Algorithm
Following [9], we propose to optimize the objective with respect to one variable, while fixing others. The algorithm will keep updating the variables until convergence.
Computation of H: Optimizing the objective function in Eq. (3) with respect to H is equivalent to solving

min
H0

JH =

X - UHVT

2 F

.

(4)

Let H be the Lagrange multiplier for constraint H  0; the Lagrange function L(H) is defined as follows:

L(H) =

X - UHVT

2 F

-

T r(H HT ).

(5)

By setting the derivative HL(H) = 0, we get

H = -2UT XV + 2UT UHVT V.

(6)

The Karush-Kuhn-Tucker complementary condition [6] for the nonnegativity constraint of H gives

H (i, j)H(i, j) = 0 ;

(7)

thus, we obtain

[-UT XV + UT UHVT V](i, j)H(i, j) = 0.

(8)

Similar to [9], it leads to the updating rule of H,

H(i, j)  H(i, j)

[UT XV](i, j) [UT UHVT V](i, j) .

(9)

Computation of U: Optimizing the objective function in Eq. (3) with respect to U is equivalent to solving

min
U0

JU =

X - UHVT

2 F

s.t. UT U = I.

(10)

Let U and U be the Lagrange multipliers for constraints U  0 and UT U = I, respectively; the Lagrange function
L(U) is defined as follows:

L(U) =

X - UHVT

2 F

- T r(U UT )

+

T r(U (UT U

-

I))

(11)

By setting the derivative UL(U) = 0, we get

U = -2XVHT + 2UHVT VHT + 2UU .

(12)

With the KKT complementary condition for the nonnegativity constraint of U, we have

U (i, j)U(i, j) = 0;

(13)

thus, we obtain

[-XVHT + UHVT VHT + UU ](i, j)U(i, j) = 0, (14)
where

U =UT XVHT - HVT VHT .

(15)

anLdet -U(Ui,=j)=+U

--U , where (|U (i, j)| -

+U (i, j) = (|U U (i, j))/2 [9];

(i, j)|+U we get

(i,

j))/2

[-(XVHT + U-U ) + (UHVT VHT + U+U )](i, j)U(i, j) = 0, (16)
which leads to the updating rule of U,

U(i, j)  U(i, j)

[XVHT + [UHVT VHT

U-U ](i, j) + U+U ](i,

j

)

.

(17)

Algorithm 1: Modeling Knowledge across Media

Input: {X, Y, , I} Output: V

1: Initialize U, V, H, W  0 2: while Not convergent and iter  I do

3:

Update H(i, j)  H(i, j)

[UT XV](i,j) [UT UHVT V](i,j)

4:

Update U(i, j)  U(i, j)

[XVHT +U- U ](i,j) [UHVT VHT +U+ U ](i,j)

5:

Update V(i, j)  V(i, j)

[XT UH+YWT +V- V ](i,j) [VHT UT UH+VWWT +V+ V ](i,j)

6:

Update W(i, j)  W(i, j)

[VT Y](i,j) [VT VW](i,j)

7: iter = iter + 1

8: end while

Computation of V: Optimizing the objective function in Eq. (3) with respect to V is equivalent to solving

min
V0

J=

X - UHVT

2 F

+

VW - Y

2 F

(18)

s.t. VT V = I.

Similar to the computation of U, by introducing two Lagrange multipliers V and V for the constraints, we get
[-(XT UH + YWT + V-V ) + (VHT UT UH + VWWT + V+V )](i, j)V (i, j) = 0,
(19) which leads to the updating rule of V,

V(i, j)  V(i, j)

[XT UH + YWT + V-V ](i, j) [VHT UT UH + VWWT + V+V ](i, j)

(20)

Computation of W: Optimizing the objective function

in Eq. (3) with respect to W is equivalent to solving

min
W0

J=

VW - Y

2 F

.

(21)

Similar to the computation of U, by introducing a Lagrange multiplier and satisfying KKT condition, we obtain

[VT VW - VT Y](i, j)W(i, j) = 0,

(22)

which leads to the updating rule of W,

W(i, j)  W(i, j)

[VT Y](i, j) [VT VW](i, j) .

(23)

We summarize the algorithm of optimizing Eq. (3) in Algorithm 1, where I is the number of maximum iterations. In line 1, we conduct initialization for the variables. From lines 2 to 8, the four variables are updated with the updating rules until convergence or until they reach the number of maximum iterations. The correctness and convergence of the updating rules can be proven with the standard auxiliary function approach [28].
4.2 Modeling Content Information
In this subsection, as shown in the lower right part of Figure 1, we introduce how to model content information of microblogging data in the proposed model.

551

To make use of the content information of microblogging
messages, we introduce a graph Laplacian [8] in the proposed model. We construct a graph based on content information of the users. In the graph, each node represents a user and each edge represents the affinity between two users. The adjacency matrix M  Rn×n of the graph is defined as

1 if u  N (v) or v  N (u)

M(u, v) =

(24)

0 otherwise

where u and v are nodes, and N (u) represents the k-nearest neighbor of the user. Content similarity is adopted to obtain the k-nearest neighbor in this work. Since we aim to model the mutual content similarity between two users, the adjacency matrix is symmetric.
The basic idea of of using the graph Laplacian to model the content information is that if two nodes are close in the graph, i.e., they posted similar messages, their identity labels should be close to each other. It can be mathematically formulated as minimizing the following loss function:

nn

R

=

1 2

Vt(i, ) - Vt(j, )

2 2

M(i,

j).

i=1 j=1

(25)

This loss function will incur a penalty if two users have dif-

ferent predicted labels when they are close to each other in the graph. Let D  Rn×n denote a diagonal matrix, and

its diagonal element is the degree of a user in the adjacency

matrix M, i.e., D(i, i) =

n j=1

M(i,

j

).

Theorem 1. The formulation in Eq. (25) is equivalent to the following objective function:

R = T r(VtT LVt),

(26)

where the Laplacian matrix [8] L is defined as L = D - M.

Proof. It is easy to verify that Eq. (25) can be rewritten as

R=
= =

nn c
Vt(i, k)M(i, j)VtT (i, k)

i=1 j=1 k=1

nn c

-

Vt(i, k)M(i, j)VtT (j, k)

i=1 j=1 k=1

T r(VtT (D - M)Vt)

T r(VtT LVt),

(27)

which completes the proof. 2

4.3 Spammer Detection Framework

As illustrated in Figure 1, we employ two types of information to formulate two kinds of constraints on the learned model. By integrating knowledge learned from other media and content information from microblogging, we can perform spammer detection by optimizing

min
Ut ,Ht ,Vt 0

J=

T - UtHtVtT

2 F

+

T r(VtT LVt)

+

GU (Ut - U)

2 F

),

(28)

s.t. UTt Ut = I, VtT Vt = I,

where the first term is to factorize the microblogging data into three variables, which are similar to the idea discussed in Section 4.1. The second term is to introduce content information and the third is to introduce knowledge learned from the other media. U is the lexicon learned from the other

Algorithm 2: Spammer Detection in Microblogging

Input: {T, U, , , I }

Output: Vt

1: Construct matrices L in Eq. (26)

2: Initialize Ut = U, V, H  0 3: while Not convergent and iter  I do

4:

Update Ht(i, j)  Ht(i, j)

[UTt XVt ](i,j) [UTt UtHt VtT Vt](i,j)

5: Update

Ut(i, j)  Ut(i, j)

[XVt HTt +GU U+Ut - U ](i,j) [Ut HtVtT Vt HTt +GU Ut+Ut+ U ](i,j)

6: Update

7:

Vt(i, j)  Vt(i, j)

[XT Ut Ht+MVt+Vt - V ](i,j) [Vt HTt UTt UtHt +DVt+Vt + V ](i,j)

8: iter = iter + 1

9: end while

media by solving the problem in Eq. (3). GU  {0, 1}m×m is a diagonal indicator matrix to control the impact of the learned lexicon, i.e., GU (i, i) = 1 represents that the i-th word contains identity information, GU (i, i) = 0 otherwise.
This optimization problem is not convex with respect to the three parameters together. Following the optimization procedure to solve Eq. (3), we propose an algorithm to solve the problem in Eq. (28) and summarize it in Algorithm 2. In line 1, we construct the Laplacian matrix L. In line 2, we initialize the variables. From lines 3 to 9, we keep updating the variables with the updating rules until convergence or until the number of maximum iterations is reached.
5. EXPERIMENTS
In this section, we empirically evaluate the proposed learning framework and the factors that could bring in effects to the framework. Through the experiments, we aim to answer the following two questions:
· How effective is the proposed framework compared with other possible solutions of using external information across media in real-world spammer detection tasks?
· What impact do the other resources have on the performance of spammer detection in microblogging?
5.1 Experimental Setup
We follow a standard experiment setup used in spammer detection literature [34] to evaluate the effectiveness of our proposed framework for leveraging knowledge aCross media for Spammer Detection (CSD). In particular, we compare the proposed framework CSD with different baseline methods for spammer detection. To avoid bias, both TweetH and TweetS, introduced in Section 2.1, are used in the experiments. For email data, we consider each sender a user; For SMS and web data, we do not have user information and consider each message as sent from a distinct user. In the experiment, precision, recall and F1-measure are used as the performance metrics.
To evaluate the general performance of the proposed framework, we use all of the three datasets from different media, i.e., Email, SMS and Web datasets. In the first set of experiments, to be discussed in Section 5.2, we simply combine them together and consider them as homogeneous data sources. In the second set of experiments, to be discussed in

552

Least Squares Lasso MFTr MFSD CSD

Table 4: Spammer Detection Results on TweetH Dataset

External Data I (50%)

External Data II (100%)

Precision Recall F1-measure (gain) Precision Recall F1-measure (gain)

0.823 0.834

0.828 (N.A.)

0.839 0.852

0.845 (N.A.)

0.865 0.891 0.878 (+5.96%)

0.873 0.905 0.889 (+5.12%)

0.866 0.899 0.882 (+6.49%)

0.887 0.918 0.902 (+6.72%)

0.644 0.703 0.672 (-18.7%)

0.650 0.715 0.681 (-19.5%)

0.906 0.939 0.922 (+11.3%)

0.913 0.944 0.928 (+9.79%)

Least Squares Lasso MFTr MFSD CSD

Table 5: Spammer Detection Results on TweetS Dataset

External Data I (50%)

External Data II (100%)

Precision Recall F1-measure (gain) Precision Recall F1-measure (gain)

0.766 0.813

0.789 (N.A.)

0.793 0.820

0.806 (N.A.)

0.801 0.849 0.824 (+4.50%)

0.814 0.848 0.831 (+3.02%)

0.810 0.857 0.833 (+5.58%)

0.833 0.878 0.855 (+6.03%)

0.621 0.69 0.654 (-17.1%)

0.642 0.681 0.661 (-18.0%)

0.832 0.875 0.853 (+8.13%)

0.848 0.919 0.882 (+9.40%)

Section 5.3, we consider their individual impact on the performance of spammer detection. A standard procedure for data preprocessing is used in our experiments. The unigram model is employed to construct the feature space, tf-idf is used as the feature weight.
As we discussed in Sections 4.1 and 4.3, three positive parameters are involved in the experiments, including  in Eq. (3), and  and  in Eq. (28).  is to control the effect of knowledge from other media to the learned lexicon,  is to control the contribution of Laplacian regularization, and  is to control the contribution of lexicon to the spammer detection model. Since all the parameters can be tuned via crossvalidation with a set of validation data, in the experiment, we empirically set  = 0.1,  = 0.1 and  = 0.1 for general experiment purposes. The effects of the parameters on the learning model will be further discussed in Section 5.4.
5.2 Performance Evaluation
We compare the proposed method CSD with other methods for spammer detection, accordingly answer the first question asked above. The baseline methods are listed below.
· Least Squares: One possible solution for our task is to consider it as a supervised learning problem. We simply train a classification model with the available external data and apply the learned model on microblogging data for spammer detection. The widely used classifier, Least Squares [12], is used for comparison.
· Lasso: Sparse learning methods are effective for highdimensional data in social media. We further include Lasso [29] as the baseline method, which performs continuous shrinkage and automatic feature selection by adding l1 norm regularization to the Least Squares.
· MFTr : Although we first present a quantitative linguistic variation analysis and provide a unified model for spammer detection across different media, domain adaption and transfer learning have received great attention in various applications [27]. We apply a widely used transfer learning method [23], which transfers the knowledge directly from labeled data in the source do-

main to the target domain for classification, to test its performance on spammer detection in the experiment.
· MFSD: We test the performance of the unsupervised learning method by employing the basic matrix factorization model MFSD. This is a variant of our proposed method without introducing any knowledge learned from external sources. As a common initialization for clustering methods, we randomly assign initial centroids and an initial class indicator matrix for MFSD.
Experimental results of the methods on the two datasets, TweetH and TweetS, are respectively reported in Table 4 and 5. To avoid bias brought by the sizes of the training data,6 we conduct two sets of experiments with different numbers of training instances. In the experiments, "External Data I (50%)" means that we randomly chose 50% from the whole training data. "External Data II (100%)" means that we use all the data for training. Also, "gain" represents the percentage improvement of the methods in comparison with the first baseline method Least Squares. In the experiment, each result denotes an average of 10 test runs. By comparing the spammer detection performance of different methods, we observe the following:
(1) From the results in the tables, we can observe that our proposed method CSD consistently outperforms other baseline methods on both datasets with different sizes of training data. Our method achieves better results than the state-of-the-art method MFTr on both datasets. We apply two-sample one-tail t-tests to compare CSD to the four baseline methods. The experiment results demonstrate that the proposed model performs significantly better (with significance level  = 0.01) than the four methods.
(2) The performance of our proposed method CSD is better than the first three baselines, which are based on different strategies of using resources from the other media. This demonstrates the excellent use of cross-media knowledge in the proposed framework for spammer detection.
6Similar to the definitions in machine learning literature, training data here refers to the labeled data from the external sources, and testing data represents the unlabeled microblogging data.

553

Performance

1 0.95
0.9 0.85
0.8 0.75
0.7 Precision

CSD_Email CSD_SMS CSD_Web CSD

Recall
Metrics

F1-measure

Figure 2: Results on TweetH Dataset

Performance

1 0.95
0.9 0.85
0.8 0.75
0.7 Precision

CSD_Email CSD_SMS CSD_Web CSD

Recall
Metrics

F1-measure

Figure 3: Results on TweetS Dataset

(3) Among the baseline methods, MFTr achieves the best results. It demonstrates that the knowledge transferred from other media help the task of spammer detection in microblogging. Lasso performs better than Least Squares. This shows that, for high-dimensional textual data from email, SMS and web, feature selection is necessary for a supervised learning method for this task we study.
(4) The method MFSD achieves the worst performance among all the baseline methods. It shows that learning based on microblogging data itself can not discriminant well between spammers and legitimate users. It further demonstrates that the knowledge learned from external sources is helpful to build an effective model to tackle the problem.
In summary, with the effective use of data from the other media, our proposed framework outperforms the baseline methods in spammer detection. Next, we investigate the effects of different resources on the spammer detection task.
5.3 Effects of External Information
In this subsection, we study the effects of the external information from the other media on our proposed framework, accordingly answering the second question asked in the beginning of Section 5.
We first evaluate the performance of the proposed framework with data from only one of the three media. In particular, we learn a lexicon based on one of the three types of media, i.e., email, SMS and web, and perform spammer detection on the microblogging datasets. We do not have legitimate web pages in the original Web dataset. To build a classifier CSD Web, following the data construction procedure proposed in [18], we randomly sample 20,100 web

snippets with BingAPI as legitimate data. The experimental results of the methods on the two microblogging datasets are plotted in Figures 2 and 3, respectively. In the figures, the first three bars represent the performance of the baselines with one type of external information. The last is the method with all three types of external information. From the figures, we observe the following:
(1) With the integration of all three types of external information, CSD consistently achieves better performance than the three baselines with only one type of information. It demonstrates that the proposed method uses beneficial information to perform effective spammer detection.
(2) Among the three baseline methods, CSD Email and CSD SMS achieve better performance than CSD Web. It shows that, as external resources, email and SMS data are more suitable to be used for the spammer detection in microblogging than the web data. This result is consistent with the linguistic variation analysis in Section 2.
To further explore the effects of different media sources on the performance of spammer detection in microblogging, we employ a "knockout" technique in the experiment. Knockout has been widely used in many fields, e.g., gene function analysis, to test the performance variance brought by one component when it is made inoperative in the framework [10]. We conduct the experiments by knocking out one type of the external information from the proposed framework. The results are summarized in Table 6. In the table, "loss" represents the performance decrease of the methods as compared to the setting "Default" which is learned based on data from all three media sources. The three columns in the middle are experimental settings, in which "0" means this resource is knocked out. The last two columns are the F1measure results under different experimental settings. From the table, we observe the following:
(1) By knocking out one of the external sources, performance of the proposed framework decreases. This suggests that all the three types of external information are useful for spammer detection in microblogging.
(2) Knocking out email from the resources incurs the most performance decrease among all the experimental settings. This demonstrates that email is the most effective source among the three types of information. This finding is consistent with our discussion above.
In summary, the use of data from the other media shows the effectiveness in spammer detection task. The superior performance of the proposed method CSD validates its excellent use of knowledge from the other media.
5.4 Parameter Analysis
As discussed in Section 5.1, three positive parameters, i.e., ,  and , are involved in the proposed framework. We first examine the effects brought by , which is to control the contribution of knowledge from other media to the learned lexicon. In previous subsections, for general experimental purposes, we empirically set  = 0.1. We now conduct experiments to compare the spammer detection performance of the four methods introduced in Section 5.3 with different settings of . The experiment results on the TweetH dataset are plotted in Figure 4. From the figure, we observe the following: (1) The general trends of the four methods are similar with the variation of different parameter settings. They achieve relatively good performance when setting  in the range of [0.1, 10]. (2) In most cases, performance of

554

Table 6: Learning from Different Media for Spammer Detection in Microblogging

Email SMS

Web TweetH (loss)

TweetS (loss)

Default

1

1

1

0.928 (N.A.)

0.882 (N.A.)

Knock Out One Term

0 1 1

1

1

0.881 (-5.09%)

0.843 (-4.43%)

0

1

0.911 (-1.86%)

0.856 (-2.96%)

1

0

0.923 (-0.57%)

0.860 (-2.50%)

Spammer Detection Performance

1 0.95
0.9 0.85
0.8 0.75
0.7 0.65
0.6 1e-4

1e-3

0.01

0.1

1

Parameter 

CSD_Email CSD_SMS CSD_Web CSD

10

100

Figure 4: Performance with Different  Settings

Spammer Detection Performance

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65 10

5

10

1

5

0.1 0.01

1 0.1 0.01

Parameter -- 

1e-3 1e-3

Parameter -- 

Figure 5: Impact of Content Information () and External Information ()

the proposed CSD is better than the other three methods. It demonstrates that the combination of the three resources improve the spammer detection performance.
We further examine the effects of the parameters  and  discussed in Eq. (28) on the proposed framework.  is to control the contribution of content information and  is to control the effects of external information from the other media. To understand the effects brought by the parameters, we compare the spammer detection performance of the proposed CSD on the Twitter datasets with different parameter settings. The results on the TweetH dataset are plotted in Figure 5. From the figure, we observe that the proposed method CSD performs well when   [0.1, 5] and   [0.1, 1]. Generally, the performance of CSD is not quite sensitive to the parameters. The proposed framework can perform well when choosing parameter settings in a reasonable range. Similar results have been observed for the two sets of experiments on the TweetS dataset; we omit the results owing to lack of space.

6. RELATED WORK
Significant efforts have been devoted to detecting spammers in various online social networks, including Facebook [5], Twitter [19, 20, 22], Renren [32], Blogosphere [24], etc. One effective way to perform spammer detection is to use the social network information. The assumption is that spammers cannot establish a large number of social trust relations with normal users. This assumption might not hold in many social networks. Yang et al. [32] studied the spammers in Renren, and found that spammers can have their friend requests accepted by many other users and thus blend into the Renren social graph. Different from Facebook-like OSNs, microblogging systems feature unidirectional user bindings because anyone can follow anyone else without prior consent from the followee. Ghosh et al. [13] show that spammers can acquire many legitimate followers. Besides the methods based on social networks, some efforts [22] have also been devoted to study characteristics related to tweet content and user social behavior. By understanding spammer activities in social networks, features are extracted to perform effective spammer detection. These methods need a large amount of labeled data, which is hard to obtain in social media.
Spammer detection on emails [4], SMS [14] and the web [31] has been a hot topic for quite a few years. The spams are designed to corrupt the user experience by spreading ads or driving traffic to particular web sites [31]. A popular and well-developed approach for anti-spam applications is learning-based filtering. The basic idea is that we extract effective features from the labeled data and build a classifier. We then classify new users / messages as either spam or ham according to their content information for filtering. The attempts have been done in these areas and the abundant labeled resources are the major motivation of our work.
Some efforts have been made to employ domain adaption and transfer learning in various applications, e.g. sentiment analysis [23] and text classification [27]. Our work started the investigation of leveraging knowledge from other media for spammer detection in microblogging. Different from traditional methods, based on the quantitatively linguistic variation analysis, our proposed framework naturally combines knowledge learned from internal and external data sources in a unified model. In addition, some work has been done to study the linguistic challenges of social media texts. It is accepted that texts in social media are noisy, but it is also reported by researchers that the texts are not as noisy as what people expected [2]. The language used in Twitter is more like a projection of the language of formal media like news and blogs with shorter form [21], and it is possible to make use of normalization and domain adaption to "clean" it [11]. The evidence provided by linguists also motivate us to explore the language differences of spams across different media, and make use of resources from other media to help spammer detection in microblogging.

555

7. CONCLUSIONS AND FUTURE WORK
Texts in microblogging are short, noisy, and labeling processing is time-consuming and labor-intensive, which presents great challenges for spammer detection. In this paper, we first conduct a quantitative analysis to study how noisy the microblogging texts are by comparing them with spam messages from other media. The results suggest that microblogging data is not significantly different from data from the other media. Based on the observations, a matrix factorization model is employed to learn lexicon information from external spam resources. By incorporating external information from other media and content information from microblogging, we propose a novel framework for spammer detection. The experimental results demonstrate the effectiveness of our proposed model as well as the roles of different types of information in spammer detection.
This work suggests some interesting future directions. Different types of medium resources have different effects on the spammer detection performance. It would be interesting to quantify the contributions of different types of sources to spammer detection in microblogging. This could be an important support for source selection in spammer detection.
Acknowledgments
We truly thank the anonymous reviewers for their pertinent comments. This work is, in part, supported by ONR (N000141410095) and ARO (#025071).
8. REFERENCES
[1] T. A. Almeida, J. M. G. Hidalgo, and A. Yamakami. Contributions to the study of sms spam filtering: new collection and results. In Proceedings of DocEng, 2011.
[2] T. Baldwin, P. Cook, M. Lui, A. MacKinlay, and L. Wang. How noisy social media text, how diffrnt social media sources? In Proceedings of IJCNLP, 2013.
[3] L. Bilge, T. Strufe, D. Balzarotti, and E. Kirda. All your contacts are belong to us: automated identity theft attacks on social networks. In WWW, 2009.
[4] E. Blanzieri and A. Bryl. A survey of learning-based techniques of email spam filtering. Artificial Intelligence Review, 29(1):63­92, 2008.
[5] Y. Boshmaf, I. Muslukhov, K. Beznosov, and M. Ripeanu. The socialbot network: when bots socialize for fame and money. In ACSAC, 2011.
[6] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.
[7] H. M. Breland. Word frequency and word difficulty: A comparison of counts in four corpora. PSS, 1996.
[8] F. Chung. Spectral graph theory. Number 92. Amer Mathematical Society, 1997.
[9] C. Ding, T. Li, and M. Jordan. Convex and semi-nonnegative matrix factorizations. TPAMI, 2010.
[10] T. Egener, J. Granado, and M. Guitton. High frequency of phenotypic deviations in physcomitrella patens plants transformed with a gene-disruption library. BMC Plant Biology, 2:6, 2002.
[11] J. Eisenstein. What to do about bad language on the internet. In Proceedings of NAACL-HLT, 2013.
[12] J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning, 2008.

[13] S. Ghosh, B. Viswanath, F. Kooti, N. Sharma, G. Korlam, F. Benevenuto, N. Ganguly, and K. Gummadi. Understanding and combating link farming in the twitter social network. In WWW, 2012.
[14] J. M. G´omez Hidalgo, G. C. Bringas, E. P. Sa´nz, and F. C. Garc´ia. Content based sms spam filtering. In Proceedings of DocEng, 2006.
[15] M. A. Halliday and C. M. Matthiessen. An introduction to functional grammar. 2004.
[16] M. Hart. Project gutenberg. Project Gutenberg, 1971. [17] T. Hofmann. Probabilistic latent semantic indexing. In
Proceedings of SIGIR, 1999. [18] X. Hu, N. Sun, C. Zhang, and T.-S. Chua. Exploiting
internal and external semantics for the clustering of short texts using world knowledge. In CIKM, 2009. [19] X. Hu, J. Tang, and H. Liu. Online social spammer detection. In AAAI, 2014. [20] X. Hu, J. Tang, Y. Zhang, and H. Liu. Social spammer detection in microblogging. In IJCAI, 2013. [21] X. Hu, L. Tang, J. Tang, and H. Liu. Exploiting social relations for sentiment analysis in microblogging. In WSDM, 2013. [22] K. Lee, J. Caverlee, and S. Webb. Uncovering social spammers: social honeypots + machine learning. In Proceedings of SIGIR, 2010. [23] T. Li, Y. Zhang, and V. Sindhwani. A non-negative matrix tri-factorization approach to sentiment classification with lexical prior knowledge. In Proceedings of ACL, 2009. [24] Y.-R. Lin, H. Sundaram, Y. Chi, J. Tatemura, and B. L. Tseng. Splog detection using self-similarity analysis on blog temporal dynamics. In AirWeb, 2007. [25] V. Metsis, I. Androutsopoulos, and G. Paliouras. Spam filtering with naive bayes-which naive bayes? In Proceedings of CEAS, 2006. [26] D. O'Callaghan, M. Harrigan, J. Carthy, and P. Cunningham. Network analysis of recurring youtube spam campaigns. In Proceedings of ICWSM, 2012. [27] S. J. Pan and Q. Yang. A survey on transfer learning. TKDE, pages 1345­1359, 2010. [28] D. Seung and L. Lee. Algorithms for non-negative matrix factorization. NIPS, pages 556­562, 2001. [29] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267­288, 1996. [30] R. Wardhaugh. An introduction to sociolinguistics, volume 28. Wiley. com, 2011. [31] S. Webb, J. Caverlee, and C. Pu. Introducing the webb spam corpus: Using email spam to identify web spam automatically. In CEAS, 2006. [32] Z. Yang, C. Wilson, X. Wang, T. Gao, B. Zhao, and Y. Dai. Uncovering social network sybils in the wild. In Proceedings of IMC, 2011. [33] S. J. Yates. Oral and written linguistic aspects of computer conferencing. Pragmatics and beyond New Series, 1996. [34] Y. Zhu, X. Wang, E. Zhong, N. Liu, H. Li, and Q. Yang. Discovering spammers in social networks. In Proceedings of AAAI, 2012.

556

Win-Win Search: Dual-Agent Stochastic Game in Session Search
Jiyun Luo, Sicong Zhang, Hui Yang
Department of Computer Science, Georgetown University
{jl1749,sz303}@georgetown.edu, huiyang@cs.georgetown.edu

ABSTRACT
Session search is a complex search task that involves multiple search iterations triggered by query reformulations. We observe a Markov chain in session search: user's judgment of retrieved documents in the previous search iteration affects user's actions in the next iteration. We thus propose to model session search as a dual-agent stochastic game: the user agent and the search engine agent work together to jointly maximize their long term rewards. The framework, which we term "win-win search", is based on Partially Observable Markov Decision Process. We mathematically model dynamics in session search, including decision states, query changes, clicks, and rewards, as a cooperative game between the user and the search engine. The experiments on TREC 2012 and 2013 Session datasets show a statistically significant improvement over the state-of-the-art interactive search and session search algorithms.
Categories and Subject Descriptors
H.3.3 [Information Systems ]: Information Storage and Retrieval--Information Search and Retrieval
Keywords
Dynamic Information Retrieval Modeling; POMDP; Stochastic Game; Session Search
1. INTRODUCTION
Users often need a multi-query session to accomplish a complex search task. A session usually starts with the user writing a query, sending it to the search engine, receiving a list of ranked documents ordered by decreasing relevance, then examining the snippets, clicking on the interesting ones, and spending more time reading them; we call one such sequence a "search iteration." In the next iteration, the user modifies the query or issues a new query to start the search again. As a result, a series of search iterations form, which include a series of queries q1, ..., qn, a series of returned documents D1, ..., Dn, and a series of clicks C1, ..., Cn, some of which are SAT clicks (satisfactory clicked documents [9]). The session stops when the user's information need is satisfied or the user abandons the search [6]. The information retrieval (IR) task in this setting is called session search [7,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609629.

Figure 1: A Markov chain of decision states in session search. (S: decision states; q: queries; A: user actions such as query changes; D: documents).
11, 17, 22, 24, 25, 30, 31]. Table 1 lists example information needs and queries in session search.
We are often puzzled about what drives a user's search in a session and why they make certain moves. We observe that sometimes the same user behavior, such as a drift from one subtopic to another, can be explained by opposite reasons: either the user is satisfied with the search results and moves to another sub information need, or the user is not satisfied with the search results and leaves the previous search path. The complexity of users' decision making patterns makes session search quite challenging [4, 29].
Researchers have attempted to find out the causes of topic drifting in session search. The causes under study include personalization [29], task types [20, 24], and previous documents' relevance [11]. A user study is usually needed to draw conclusions about user intent. However, the focus of this paper is not on identifying user intent. Instead, we simplify the complexity of users' decision states into a cross product of only two dimensions: whether previously retrieved documents are relevant and whether the user would like to explore the next sub information need. Our work differs from existing work in that we consider that a session goes through a series of hidden decision states, with which we design a statistical retrieval model for session search. Our emphasis is an effective retrieval model, not a user study to identify the query intent.
The hidden decision states form a Markov chain in session search. A Markov chain is a memoryless random process where the next state depends only on the current state [18]. Figure 1 illustrates a Markov chain of hidden decision states for TREC 2013 Session 9. In a session, a user's judgment of the retrieved documents in the previous iteration affects or even decides the user's actions in the next iteration. A user's actions could include clicks, query changes, reading the documents, etc. The user's gain, which we call reward, is the amount of relevant information that he or she obtains in the retrieved documents. The reward motives the later user actions. If the decision states are known, we can use a Markov Decision Process (MDP) to model the process. However, in session search, users' decision states are hidden. We therefore model session search as a Partially Observable Markov Decision Process (POMDP) [18].

587

Table 1: Information needs and queries (examples are from TREC 2013 Session Track).

Session 2: Information Need

Session 2: Queries

You want to buy a scooter. So you're inter- q1=scooter brands

q2=scooter brands reliable q3=scooter

ested in learning more facts about scooters q4=scooter cheap

q5=scooter review

q6=scooter price

including:what brands of scooters are out

q7=scooter price

q8=scooter stores

q9=where to buy scooters

there? What brands of scooters are reliable? Which scooters are cheap? Which stores sell scooters? which stores sell the best scooters?

Session 9: Information Need

Session 9: Queries

You want to know more about old US coins. q1=old us coins

q2=collecting old us coins q3=selling old us coins

Relevant information to you includes value of old US coins, types of old US coins, old US silver dollar, q4=selling old "usa coins"

how to start collecting old US coins, how to sell old US coins and how to buy them, where to buy those coins.

Session 87: Information Need

Session 87: Queries

Suppose you're planning a trip to the United States.You will be there for a month and able to travel within a 150-mile radius of your destination.With that constraint, what are the best cities to consider as possible destinations?

q1=best us destinations

q2=distance new york boston

q3=maps.bing.com

q4=maps

q5=bing maps

q6=hartford tourism

q7=bing maps

q8=hartford visitors

q9=hartford connecticut tourism

q10=hartford boston travel

q11=boston tourism

q12=nyc tourism

q13=philadelphia nyc distance

q14=bing maps

q15=philadelphia washington dc distance

q16=bing maps

q17=philadelphia tourism q18=washington dc tourism

q19=philadelphia nyc travel q20=philadelphia nyc train q21=philadelphia nyc bus

In fact, not only the user, but also the search engine, makes decisions in a Markov process. A search engine takes in a user's feedback and improves its retrieval algorithm iteration after iteration to achieve a better reward too. The search engine actions could include term weighting, turning on or turning off one or more of its search techniques, or adjusting parameters for the techniques. For instance, based on the reward, the search engine can select p in deciding the top p documents used in pseudo relevance feedback.
We propose to model session search as a dual-agent stochastic game. When there is more than one agent in a POMDP, the POMDP becomes a stochastic game (SG). The two agents in session search are the user agent and the search engine agent. In contrast to most two-player scenarios such as chess games in game theory, the two agents in session search are not opponents to each other; instead, they cooperate: they share the decision states and work together to jointly maximize their goals. We term the framework "win-win search" for its efforts in ensuring that both agents arrive at a winwin situation. One may argue that in reality a commercial search engine and a user may have different goals and that is why some commercial search engines put their sponsors high in the returned results. However, this paper focuses on the win-win setting and assume a common interest ­ fulfilling the information needs ­ for both agents.
The challenges of modeling session search as a stochastic game lie in how to design and determine the decision states and actions of each agent, how to observe their behaviors, and how to measure the rewards and set the optimization goals. We present the details in Sections 4 and 5. As a retrieval framework, we pay more attention to the search engine agent. When the search engine makes decisions, it picks a decision that jointly optimizes the common interest.
We evaluate the win-win search framework on TREC 2012 & 2013 Session data. TREC (Text REtrieval Conference) 2010 - 2013 Session Tracks [20, 21] have spurred a great deal of research in session search [3, 10, 11, 14, 24]. The tracks provide interaction data within a session and aim to retrieve relevant documents for the last query qn in the session. The interaction data include queries, top returned documents, user clicks, and other relevant information such as dwell time. Document relevance is judged based on information need for the entire session, not just the last query. In this paper, all examples are from TREC 2013. Our experiments show that the proposed framework achieves statisti-

cally significant improvements over state-of-the-art interactive search and session search algorithms.
The remainder of this paper is organized as follows: Section 2 presents the related work, Section 3 provides preliminaries for POMDP. Section 4 details the win-win search framework, Section 5 elaborates the optimization, Section 6 evaluates the framework and Section 7 concludes the paper.
2. RELATED WORK
2.1 Session search
Session search has attracted a great amount of research from a variety of approaches [3, 11, 22, 25, 33]. They can be grouped into log-based methods and content-based methods.
There is a large body of work using query logs to study queries and sessions. Feild and Allan [8] proposed a taskaware model for query recommendation using random walk over a term-query graph formed from logs. Song and He's work [27] on optimal rare query suggestion also used random walk, with implicit feedback in logs. Wang et al. [30] utilized the latent structural SVM to extract cross-session search tasks from logs. Recent log-based approaches also appear in the Web Search Click Data (WCSD) workshop series.1
Content-based methods directly study the content of the query and the document. For instance, Raman et al. [25] studied a particular case in session search where the search topics are intrinsically diversified. Content-based session search also include most research generated from the recent TREC Session Tracks [20, 21]. Guan et al. [10] organized phrase structure in queries within a session to improve retrieval effectiveness. Jiang et al. [14] proposed an adaptive browsing model that handles novelty in session search. Jiang and He [13] further analyzed the effects of past queries and click-through data on whole-session search effectiveness.
Others study even more complex search ­ search across multiple sessions [22, 24, 30]. Kotov et al. [22] proposed methods for modeling and analyzing users' search behaviors in multiple sessions. Wang et al. [30] identified cross-session search by investigating inter-query dependencies learned from user behaviors.
Our approach is a content-based approach. However, it uniquely differs from other approaches by taking a Markov process point of view to study session search.
1http://research.microsoft.com/en-us/um/people/ nickcr/wscd2014

588

2.2 Relevance feedback
Session search is closely related to relevance feedback, a traditional IR research field. Classic relevance feedback methods include Rocchio [16], pseudo relevance feedback [2], and implicit relevance feedback [27] based on user behaviors such as clicks and dwell time. Recently, researchers have investigated new forms of relevance feedback. Jin et al. [15] employed a special type of click ­ "go to the next page" ­ as relevance feedback to maximize retrieval effectiveness over multi-page results. Zhang et al. [33] modeled query changes between adjacent queries as relevance feedback to improve retrieval accuracy in session search.
These relevance feedback approaches only considers oneway communication from the user to the search engine [15, 33]. On the contrary, this paper explicitly sets up a two-way feedback channel where both parties transmit information.
2.3 MDP and POMDP in IR
Markov Decision Process (MDP) is an important topic in Artificial Intelligence (AI). An MDP can be solved by a family of reinforcement learning algorithms. Kaelbling et al. [18] brought techniques from operational research to choose the optimal actions in partially observable problems, and designed algorithms for solving Partially Observable Markov Decision Processes (POMDPs). IR researchers have just begun showing interests in MDP and POMDP [11, 30, 32] in finding solutions for IR problems.
Early work on interactive search modeling by Shen et al. [26] used a Bayesian decision-theoretic framework, which is closely related to the MDP approaches. The QCM model proposed by Guan et al. [11] models session search as an MDP and effectively improves the retrieval accuracy. However, [11] used queries as states while we use a set of welldesigned hidden decision states. In addition, we explicitly model the stochastic game played between two agents, the user and the search engine, while [11] focused on just the search engine. Another difference is that we model a wide range of actions including query changes, clicks, and document content while [11] only used query changes.
Yuan and Wang [32] applied POMDP for sequential selection of online advertisement recommendation. Their mathematical derivation shows that belief states of correlated ads can be updated using a formula similar to collaborative filtering. Jin et al. [15] modeled Web search as a sequential search for re-ranking documents in multi-page results. Their hidden states are document relevance and the belief states are given by a multivariate Gaussian distribution. They consider "ranking" as actions and "clicking-on-the-next-page" as observations. In win-win search, we present a different set of actions, observations, and messages between two agents. The fundamental difference between our approach and theirs is that we model the retrieval task as a dual-agent cooperative game while [15] uses a single agent.
3. PRELIMINARIES: MDP AND POMDP
Markov Decision Process provides the basics for the winwin search framework. An MDP is composed by agents, states, actions, reward, policy, and transitions [19]. An agent takes inputs from the environment and outputs actions. The actions in turn influences the states of the environment. An MDP can be represented by a tuple < S, A, T, R >:
States S is a discrete set of states. In session search, they can be queries [11] or hidden decision states (Section 4.2).

Actions A is a discrete set of actions that an agent can

take. For instance, user actions include query changes, clicks,

and reading the returned documents or snippets.

Transition T is the state transition function T (si, a, sj) = P (sj|si, a). It is the probability of starting in state si, taking action a, and ending in state sj. The sum over all actions gives the total state transition probability between si and sj T (si, sj) = P (sj|si); which is similar to the state transition probability in the Hidden Markov Model (HMM) [1].

Reward r = R(s, a) is the immediate reward, also known

as reinforcement. It gives the expected immediate reward of

taking action a at state s.

Policy  describes the behaviors of an agent. A non-

stationary policy is a sequence of mapping from states to

actions.  is usually optimized to decide how to move around

in the state space to optimize the long term reward

 t=1

r.

Value function and Q-function Given a policy  at

time t, a value function V calculates the expected long term reward starting from state s inductively: V,t(s) = R(s, t(s))+  s T (s, a = t(s), s )V,t+1(s ), where the initial value V,t=1(s) = R(s, a = t=1(s)), s is the current state, s
is the next state, a = t(s) is any valid action for s at t,
 is a future discount factor. Usually an auxiliary func-

tion, called the Q-function, is used for a pair of (s, a): Q(st, a) = R(st, a) +  a P (st|st+1, a) maxa Q(st+1, a), where R(st, a) is the immediate reward at t, a is any valid action at t + 1. Note that V (s) = maxa Q(s, a).
Q-Learning Reinforcement learning (RL) algorithms pro-

vides solutions to MDPs [19]. The most influential RL al-

gorithm is Q-learning. Given a Q-function and a starting

state s, the solution can be a greedy policy that at each

step, it takes the action that maximizes the Q-function:

(s) = arg maxa R(s, a) +  a T (s, a, s )Q(s , a) , where

the base case maximizes R(s1, a). Partially Observable MDP (POMDP) When states

are unknown and can only be guessed through a probabilistic

distribution, an MDP becomes a POMDP [18]. POMDP is

represented by a tuple < S, A, T, , O, B, R >, where S, A, R

are the same as in MDP. Since the states are unknown, the

transition function T models transitions between beliefs, not

transitions between states any more: T : B × A × B 

[0, 1]. Belief B is the set of beliefs defined over S, which

indicates the probability that an agent is at a state s. It

is also known as belief state. Observations  is a discrete

set of observations that an agent makes about the states. O

is the observation function which represents a probabilistic

distribution for making observation  given action a and

landing in the next state s . A major difference between an

HMM and a POMDP is that POMDP considers actions and

rewards while HMM does not.

4. THE WIN-WIN SEARCH FRAMEWORK
A session can be viewed as a Markov chain of evolving states (Figure 1). Every time when a new query is issued, both the user and the search engine transition into a new state. In our setting, the two agents work together to achieve a win-win goal.
4.1 Model Illustration
Figure 2 shows the proposed dual-agent SG, which is represented as a tuple < S, Au, Ase, u, se, u, se, O, B, T, R >.
S is the decision states that we will present in Section 4.2.

589

Table 2: Symbols in the dual-agent stochastic game.

Name

Symbol Meanings

State

S the four hidden decision states in Figure 3

User action

Au add query terms, remove query terms, keep query terms

Search engine action

Ase increase/decrease/keep term weights, adjust search techniques, etc

Message from user to search engine u clicked and SAT clicked documents

Message from search engine to user se top k returned documents

User's observation

u observations that the user makes from the world

Search engine's observation

se observations that the search engine makes from the world and from the user

User reward

Ru relevant information the user gains from reading the documents

Search engine reward

Rse nDCG that the search gains by returning documents

Belief state

B belief states generated from the belief updater and shared by both agents

Figure 2: Dual-agent stochastic game.
Au, Ase, u, and se are the actions. We divide the actions into two types: domain-level actions A, what an agent acts on the world directly, and communication-level actions , also known as messages, which only go between the agents. User actions Au are mainly query changes [11] while search engine actions Ase are term weighting schemes and adjustments to search techniques. Both u and se are sets of relevant documents, that an agent uses to inform the other agent about what they consider as relevant. (Section 4.3)
 is the observation that an agent can draw from the world or from the other agent. O is the observation function that maps states and actions to observations: O : S × A   or O : S ×   . Note that the actions can be domain-level actions A or messages , or a combination of both. (S. 4.4)
B is the set of belief states that shared by both agents. The beliefs are updated every time when an observation happens. There are two types of belief: B·, beliefs before the messages and B·, beliefs after the messages. (Section 4.5)
The reward function R is defined over B×A  R. It is the amount of document relevance that an agent obtains from the world. Rse is the nDCG score (normalized Discounted Cumulative Gain [20]) that the search engine gains for the documents it returns. Ru is the relevance that the user gains from reading the documents. Our retrieval algorithm jointly optimize both Ru and Rse. (Section 5)
Table 2 lists the symbols and their meanings in the dualagent SG. The two agents share the decision states and beliefs but differ in actions, messages, and policies. Although they also make different observations, both contribute to the belief updater; the difference is thus absorbed. As a retrieval model, we only pay attention to the search engine policy se : B  A. The following describes their interactions in the stochastic game:
1. At search iteration t = 0, both agents begin in the same initial state S0.

2. t increases by one: t = t + 1. The user agent writes a query qt and takes the tth user agent action atu, which is a query change from the previous query.
3. (*) The search engine agent makes observations se from the world and updates its before-message-beliefstate bt·se based on O(St, atu, se).
4. The search engine runs its optimization algorithm and picks the best policy se, which maximizes the joint long term rewards for both agents. Following the policy, it takes actions atse. This is where the model performs retrieval.
5. Search engine action atse results in a set of documents Dt, which are returned as message tse sent from the search engine agent to the user agent through the world.
6. (*) The user agent receives message tse and observes u. If the user would like to stop the search, the process ends. Otherwise, the user updates the aftermessage-belief-state bt·u based on O(St, tse, u).
7. Based on the current beliefs, the user agent sends its feedback messages tu to inform the search engine agent. tu are clicks, some of which are SAT clicks. It contains a set of documents Dclicked.
8. (*) The search engine agent observes se from the world and updates its after-message-belief-state bt·se based on O(St, tu, se).
9. The user agent picks a policy u, which we don't study here, and continues to send out actions atu+1 in the form of query changes. The world moves into a new state st+1. t = t + 1. The process repeats from step 3.
Steps 3, 6, and 8 happen after making an observation from the world or from the other agent. They then all involve a belief update. In the remainder of this section, we present the details of states (Section 4.2), actions (Section 4.3), observation functions (Section 4.4), and belief updates (Section 4.5) for win-win search.
4.2 States
We often observe that the same user behavior in session search may be motivated by different reasons. For instance, in TREC 2013 Session 2 (Table 1), a user searches for "scooter brands" as q1 and finds that the 6th returned document with title "Scooter Brands - The Scooter Review - The Scooter Review" is relevant. The user clicks this document and reads it for 48 seconds, which we identify as a SAT click since it lasts more than 30 seconds [12]. Next, the user adds a new term `reliable' into q1 to get q2 ="scooter brands reliable". We notice that `reliability' does not appear in any previously retrieved documents D1. It suggests that the user is inspired to add `reliability' from somewhere else, such as from personal background knowledge or the in-

590

formation need. In this case, she finds relevant documents from the previously retrieved documents but still decides to explore other aspects about the search target.
On the contrary, in TREC 2013 Session 9 q1 (Table 1), another user searches for "old US coins" and also finds relevant documents, such as a document about "... We buy collectible U.S.A. coins for our existing coin collector clients...". He adds a new term `collecting' to get the next query q2 "collecting old us coins". After reducing the query terms and document terms into their stemmed forms, the added term `collecting' does appear in this document as we can see. It suggests that the user selects a term from the retrieval results and hones into the specifics. In this case, he finds relevant documents in the previously retrieved documents and decides to exploit the same sub information need and investigate it more.
We observe that even if both users show the same search behavior, e.g. adding terms, the reasons vary: one is adding the new search term because the original search results are not satisfactory, while the other is because the user wants to look into more specifics. This makes us realize that document relevance and users' desire to explore are two independent dimensions in deciding how to form the next query.
Inspired by earlier research on user intent and task types [24, 28] and our own observations, we propose four hidden decision making states for session search. They are identified based on two dimensions: 1) "relevant dimension" ­ whether the user thinks the returned documents are relevant, and 2) "exploration dimension" ­ whether the user would like to explore another subtopic. The two dimensions greatly simplify the complexity of user modeling in session search. The relatively small number of discrete states enables us to proceed with POMDP and its optimization at low cost.
The cross-product of the two dimensions result in four states: i) user finds a relevant document from the returned documents and decides to explore the next sub information need (relevant and exploration, e.g. scooter price  scooter stores), ii) user finds relevant information and decides to stay in the current sub information need to look into more relevant information (relevant and exploitation, e.g. hartford visitors  hartford connecticut tourism), iii) user finds out that the returned documents are not relevant and decides to stay and try out different expressions for the same sub information need (non-relevant and exploitation, e.g. philadelphia nyc travel  philadelphia nyc train), iv) user finds out that documents are not relevant and decides to give up and move on to another sub information need (nonrelevant and exploration, e.g. distance new york boston  maps.bing.com ).
Figure 3 shows the decision state diagram for win-win search. The subscriptions stand for {RT = Relevantexploi T ation, RR = RelevantexploRation, N RT = N onRelevant exploiT ation, N RR = N onRelevantexploRation}. We insert a dummy starting query q0 before any real query and it always goes to SNRR. The series of search iterations in a session move in the decision states from one to the next. A sequence of states can be time stamped and presented as st = Sm, where t = 1, 2, ..., n and m = {RT, RR, N RT, N RR}.
4.3 Actions
There are two types of actions in our framework, domainlevel actions and communications-level actions.

Figure 3: States.
4.3.1 Domain-Level Actions
The domain-level actions Au and Ase represent the actions directly performed on the world (document collection) by the user agent and by the search engine agent, respectively.
The common user actions include writing a query, clicking a document, SAT clicking a document, reading a snippet, reading a document, changing a query, and eye-tracking the documents. In this paper, we only study query changes and clicks as user actions. However, the framework can be easily adopted for other types of user actions.
Query changes q [11] consist of added query terms +qt = qt\qt-1, removed query terms -qt = qt-1\qt, and theme terms qtheme = LongestCommon Subsequence(qt, qt-1). For example, in Session 87 , given q19="philadelphia nyc travel" and q20="philadelphia nyc train", we obtain the following query changes: qtheme = LCS(q19, q20) = "philadelphia", -q20 = "travel", and +q20 = "train". All stopwords and function words are removed.
The search engine domain-level actions Ase include increasing, decreasing, and maintaining the term weights, as well as adjusting parameters in one or more search techniques. We present the details in Sections 5 and 6.
4.3.2 Communications-Level Actions (Messages)
The second type of actions are communication-level actions (messages) u and se. They are actions that only performed between agents.
In our framework, the messages are essentially documents that an agent thinks are relevant. u is the set of documents that the user sends out; we define them as the clicked documents Dclicked. In TREC 2013 Session, 31% search iterations contain SAT clicked documents. 23.9% sessions contain 1 to 4 SAT clicked documents, and a few sessions, for instance Sessions 45, 57 and 72, contain around 10 SAT clicked documents. 88.7% SAT clicked documents appear in the top 10 retrieved results.
Similarly, se is the set of documents that the search engine sends out. They are the top k returned documents (k ranges from 0 to 55 in the TREC setting). They demonstrate what documents the search engine thinks are the most relevant. In TREC 2013, 2.8% (10) search iterations return less than 10 documents, 90.7% (322) return exactly 10, 5.1% (18) return 1020, and 1.4% (5) return 2055 documents.
4.4 Observations
Section 4.1 illustrates the win-win search framework and the interactions between agents. This section shows how we calculate the observation functions.
The observation function O(sj, at, t), defined as P (t|sj, at), is the probability of observing t   when agents take action at and land on state sj. The first type of observation

591

is related to relevance. In Section 4.1 Step 8, after the user sends the message u (user clicks) out at Step 7, the search engine updates its after-message-belief-state b·se based on its observation of user clicks. The observation function for `Relevant' states is:

O(st=Rel, u, t=Rel) ==de=f= P (t = Rel|st = Rel, u)

(1)

It can be written as

. P (t=Rel,st=Rel,u)
P (st=Rel,u)

By taking P (st =

Rel, u) as a constant, we can approximate it by P (t =

Rel, st = Rel, u) = P (st = Rel|t = Rel, u)P (t =

Rel, u). Given that user clicks u are highly correlated

to t, we can approximate P (st = Rel|t = Rel, u) by

P (st = Rel|t = Rel). Further, by taking P () as a con-

stant, we have

O(st=Rel, u, t=Rel)  P (st = Rel|t = Rel)P (t = Rel, u)  P (st = Rel|t = Rel)P (t = Rel|u)
(2) Similarly, we have

O(st=Non-Rel, u, t=Non-Rel)  P (st = Non-Rel|t = Non-Rel)P (t = Non-Rel|u)
(3) as well as O(st=Non-Rel, u, t=Rel) and O(st=Rel, u, t=Non-Rel).
Based on whether a SATClick exists or not, we calculate the probability of the SG landing at the "Relevant" states or the "Non-Relevant" states (the first dimension of hidden decision states). At search iteration t, if the set of previously returned documents leads to one or more SAT clicks, the current state is likely to be relevant, otherwise non-relevant. That is to say,

st is likely to be

Relevant

if  d  Dt-1 and

d is SATClicked

Non-Relevant otherwise.

Based on this intuition, we calculate P (t = Rel|u) and P (t = Non-Rel|u) as:

P (t = Rel|u) = P ( SATClicks  Dct-lic1ked) (4)

P (t = Non-Rel|u) = P ( SATClicks  Dct-lic1ked) (5)

The conditional probability of observations P (st = Rel|t =

Rel) and P (st = Non-Rel|t = Non-Rel) can be calculated

by maximum likelihood estimation (MLE). For instance,

P (st

=

Rel|

=

Rel)

=

#

of #

observed true relevant of observed relevant

,

where "#

of observed true relevant" is the number of times where the

previously returned document set Dt-1 contain at least one

SAT clicks and those SAT clicked documents are indeed rel-

evant documents in the ground truth. "# of observed rele-

vant" is the number of times where Dt-1 contains at least

one SAT clicks. The ground truth of whether the SG lands

on a "Relevant" state is generated by documents whose rele-

vance grades  3 (relevant to highly relevant). The relevance

are judged by NIST assessors [21].

The second type of observation is related to exploitation

vs. exploration. This corresponds to a combined observa-

tion at Step 3 and the previous Step 6 (Section 4.1), where

the SG update the before-message-belief-state b·se for a

user action au (query change) and a search engine message

se=Dt-1, the top returned documents at the previous it-

eration. The search engine agent makes observations about

exploitation vs. exploration (the second dimension of hidden

decision states) by:

O(st=Exploitation, au=qt, se=Dt-1, t=Exploitation)  P (st = Exploitation|t = Exploitation) ×P (t = Exploitation|qt, Dt-1)
(6) O(st=Exploration,au=qt, se=Dt-1, t=Exploration)  P (st = Exploration|t = Exploration) ×P (t = Exploration|qt, Dt-1)
(7)
The search engine can guess the hidden states based on
the following intuition:

 Exploration  
st is likely to be Exploitation  

if (+qt =  and +qt / Dt-1 ) or (+qt =  and -qt =  ) if (+qt =  and +qt  Dt-1 ) or (+qt =  and -qt =  )

The idea is that given that Dt-1 is the message from search engine and au = q is the message from user, if added query terms +q appear in Dt-1, it is likely that the user stays at the same sub information need from iteration t - 1 to t for `exploitation'. On the other hand, if the added terms +q do not appear in Dt-1, it is likely that the user moves to the next sub information need from iteration t - 1 to t for `exploration'. In addition, if there is no added terms (+qt is empty) but there are deleted terms ( -qt is not empty), it is likely that the user goes to a broader topic to explore. If +qt and -qt are both empty, it means there is no change to the query, it is likely to fall into exploitation.
Hence, P (t|qt, Dt-1) can be calculated as:

P (t = Exploration|qt, Dt-1) = P (+qt =   +qt / Dt-1)

+P (+qt =   -qt = )

(8) P (t = Exploitation|qt, Dt-1) = P (+qt =   +qt  Dt-1)

+P (+qt =   -qt = )

(9)

where Dt-1 include all clicked documents and all snippets

that are ranked higher than the last clicked document at

iteration t - 1. User actions au include the current query

changes +qt and -qt. In fact, P (t|qt, Dt-1) needs to

be calculated for each specific case. For instance, P (t =

Exploration|a = `delete term', qt, Dt-1) =

#

of #

observed true explorations due of observed explorations due to

to deleting terms deleting terms

.

Here we only

calculate for the actions with "deleted terms". "# of ob-

served explorations" is the number of observed explorations

suggesting that the user is likely to explore another subtopic

based on Eq. 8, while "# of observed true explorations" is

the number of observed explorations judged positive by hu-

man accessors in a ground truth. The annotations can be found online.2

The conditional probability P (st = Exploitation|t =

Exploitation) is

calculated

as

# of #

observed true exploitations of observed exploitations

,

where

"# of observed exploitations" is the number of observed ex-

ploitations suggesting that the user is likely to exploit the

same subtopic (based on Eq. 9), and "# of observed true ex-

ploitations" is the number of observed exploitations that are

judged positive in the ground truth. P (st = Exploration|t =

Exploration) is calculated in a similar way.

4.5 Belief Updates
At every search iteration the belief state b is updated twice; once at Step 3, another at Step 8. It reflects the interaction and cooperative game between the two agents. 2The manual annotations for "exploration" transitions can be found at www.cs.georgetown.edu/~huiyang/win-win.

592

A belief bt(si) is defined as P (si|at, bt). The initial belief states can be calculated as: b0(si = Sz) = P (si = Sx)P (si = Sy), where x  {R = Rel, N R = N on-Rel}, y  {R = exploRation, T = exploiT ation}, z is the cross-product of

Table 3: Dataset statistics.

TREC 2012 TREC 2013

#Sessions

98

87

#Search topics

48

49

x and y and z  {RR, RT, N RR, N RT }. In addition, 0 

#Queries

297

442

b(si)  1 and si b(si) = 1. The belief update function is bt+1(sj) = P (sj|t, at, bt)

Avg. session length

3.03

Max session length

11

Avg. #sessions per topic

2.04

5.08 21 1.78

by taking into account new observations t. It is updated

from iteration t to iteration t + 1:

The formula matches well with common search scenarios

bt+1(sj ) =d=e=f= P (sj |t, at, bt)

O(sj , at, t) =

siS T (si, at, sj )bt(si)

P (t|at, bt)

where the user makes decisions about their next actions

based on the most relevant document(s) they examined in

(10)

the previous run of retrieval. Such a document we call it

maximum rewarding document(s). We use document with

where si and sj are two states, i, j  {RR, RT, N RR, N RT }. t indices the search iterations, and O(sj, at, t) = P (t|sj, at) is calculated based on Section 4.4. P (t|at, bt) is the normalization factor to keep siS b(si) = 1. For notation simplicity, we will only use a to represent actions from now

the largest P (qt-1|dt-1) as the maximum rewarding docu-

ment. P (qt-1|dt-1) is calculated as 1- tqt-1 {1 - P (t|dt-1)},

where

P (t|dt-1)

=

, #(t,dt-1 )
|dt-1 |

#(t, dt-1) is

the

number

of

occurrences of term t in document dt-1, and |dt-1| is the

document length.

on. However, it is worthy noting that actions can be both

By optimizing both long term rewards for the user and

domain-level actions a and messages .

for the search engine, we learn the best policy  and use it

Transition probability T (si, at, sj) is defined as P (sj|si, at, bt).

It

is

can

be

calculated

as

T (si, at, sj )

=

, #T ransition(si,at,sj )
#T ransition(si,at,s)

where Transition (si, at, sj) is the sum of all transitions that

starts at state si, takes action at, and lands at state sj.

to predict the next action for the search engine. The joint optimization for the dual-agent SG can be represented as:

ase

=

arg max
a

Qse(b, a) + Qu(b, au)

(14)

T ransition (si, at, s) is the sum of all transitions that starts at state si and lands at any state by action at.

where ase  Ase at t = n and n is the number of search iterations in a session, i.e., the session length.

Finally, equals to P

taking (t|sj ,

O(sj , at, t) = P (t|sj , at), which at, bt) when we consider beliefs, and T

also (si, at,

sj )

In win-win search, Ase can include many search engine actions. One type of actions is adjusting a query's term weight.

= P (sj|si, at, bt), the updated belief can be written as:

Assuming the query is reformulated from the previous query

bt+1(sj )

=

P (t|sj , at, bt)

siS P (sj |si, at, bt)bt(si) P (t|at, bt)

P (t|sj , at, bt) =

siS P (sj |si, at, bt)bt(si)

skS P (t|sk, at) siS P (sk|si, at)bt(si)

(11)

where bt(si) is P (si|at, bt), whose initial value is b0(si).

by adding +q or deleting -q. That is to say, Ase = {increasing, decreasing, or keeping term weights}. The term weights are increased or decreased by multiplying a factor. We also use a range of search techniques/algorithms as action options for the search engine agent. They are reported in Section 6. Based on Eq. 14, the win-win search framework picks the optimal search engine action.

5. JOINT OPTIMIZATION AND RETRIEVAL 6. EVALUATION

After every search iteration, we decide the actions for the search engine agent. We employ Q-learning [18] to find out the optimal action. For all a  Ase, we write the search engine's Q-function, which represents the search engine agent's long term reward, as:

We evaluate the proposed framework on TREC 2012 and TREC 2013 Session Tracks [20, 21]. The session logs are collected from users through a search system by the track organizers. The topics, i.e., information need (Table 1), are provided to users. The session logs record all URLs dis-

played to the user, snippets, clicks, and dwell time. Table 3

Qse(b, a) = (b, a)+

P

(|b,

au,

se)P (|b,

u)

max
a

Qse(b

,

a)

shows the dataset statistics. The task is to retrieve a ranked



(12)

list of 2,000 documents for the last query in a session. Doc-

where the reward for a belief state b is (b, a) = sS b(s)R(s, a). P (|b, au, se) corresponds to Eq. 8 and Eq. 9 and P (|b, u)
corresponds to Eq. 4 and Eq. 5. b is the belief state up-

ument relevance is judged based on the whole-session relevance. We use the official TREC evaluation metrics in our experiments. They include nDCG@10, nERR@10, nDCG,

dated by Eq. 11.

and MAP [21]. The ground truth relevant documents are

In win-win search, we take into account both the search

provided by TREC.

engine reward and the user reward. As in [11], we have Qu calculated as the long term reward for the user agent:

The corpora used in our experiments are ClueWeb09 CatB (50 million English web pages crawled in 2009, used in TREC

2012), and ClueWeb12 CatB (50 million English web pages

Qu(b, au) = R(s, au) +  au T (st|st-1, Dt-1) maxst-1 Qu(st-1, au) crawled in 2012, used in TREC 2013). Documents with the

= P (qt|d) +  a P (qt|qt-1, Dt-1, a) maxDt-1 P (qt-1|Dt-1)

Waterloo spam scores [5] less than 70 are filtered out. All

(13)

duplicated documents are removed.

which recursively calculates the reward starting from q1 and continues with the policy until qt. P (qt|d) is the current reward that the user gains through reading the documents.

We compare our system with the following systems: Lemur [23] (language modeling + Dirichlet smoothing), PRF (Pseudo Relevance Feedback in Lemur assuming the top 20 docu-

maxDt-1 P (qt-1|Dt-1) is the maximum of the past rewards.

ments are relevant), Rocchio (relevance feedback that as-

593

Table 4: Search accuracy on TREC 2012 Session (

indicates a statistical significant improvement over

Rocchio at p < 0.05 (t-test, one-sided));  indicates a

statistical significant improvement over QCM+DUP

at p < 0.05 (t-test, one-sided)).

Approach

nDCG@10 nDCG MAP nERR@10

Lemur

0.2474 0.2627 0.1274 0.2857

TREC median 0.2608 0.2468 0.1440 0.2626

TREC best

0.3221 0.2865 0.1559 0.3595

PRF

0.2074 0.2335 0.1065 0.2415

Rocchio

0.2446 0.2714 0.1281 0.2950

Rocchio-CLK

0.2916 0.2866 0.1449

0.3366

Rocchio-SAT

0.2889 0.2836 0.1467 0.3254

QCM+DUP

0.2742 0.2560 0.1537 0.3221

QCM SAT

0.3350 0.3054 0.1534 0.1534

Win-Win

0.2941 0.2691 0.1346 0.3403

Table 5: Search accuracy on TREC 2013 Session (

indicates a statistical significant improvement over

Rocchio at p < 0.01 (t-test, one-sided));  indicates a

statistical significant improvement over QCM+DUP

at p < 0.01 (t-test, one-sided)).

Approach

nDCG@10 nDCG MAP nERR@10

Lemur

0.1147

0.1758 0.0926

0.1314

TREC median 0.1531

­

­

­

TREC best

0.1952

­

­

­

PRF

0.1061

0.1701 0.0787

0.1245

Rocchio

0.1320

0.1924 0.1060

0.1549

Rocchio-CLK

0.1315

0.1929 0.1060

0.1546

Rocchio-SAT

0.1147

0.1758 0.0926

0.1314

QCM+DUP

0.1316

0.1929 0.1060

0.1547

QCM SAT

0.1186

0.1754 0.0939

0.1425

Win-Win

0.2026 0.2609 0.1290 0.2328

sumes the top 10 previous retrieved documents are relevant), Rocchio-CLK (implicit relevance feedback that assumes only previous clicked documents are relevant), Rocchio-SAT (implicit relevance feedback that assumes only previous SATclicked documents are relevant), QCM+DUP (the QCM approach proposed by [11]), and QCM SAT (a variation of QCM by [33]). We choose Rocchio (a state-of-the-art interactive search algorithm) and QCM+DUP (a state-of-the-art session search algorithm) as two baseline systems and all other systems are compared against them. TREC median and TREC best scores are also included for reference. Note that TREC best are an aggregation from the best scores of each individual submitted TREC runs; it is not a single search system.
6.1 Search Accuracy
Our run, win-win, implements six retrieval technologies. They are: (1) increasing weights of the added terms (+q) by a factor of x={1.05, 1.10, 1.15, 1.20, 1.25, 1.5, 1.75 or 2}; (2) decreasing weights of the added terms by a factor of y ={ 0.5, 0.57, 0.67, 0.8, 0.83, 0.87, 0.9 or 0.95}; (3) the term weighting scheme proposed in [11] with parameters , , ,  set as 2.2, 1.8, 0.4, 0.92; (4) a PRF (Pseudo Relevance Feedback) algorithm which assumes the top p retrieved documents are relevant while p ranges from 1 to 20; (5) an adhoc variation of win-win, which directly uses the last query in a session to perform retrieval; and (6) a brute-force variation of win-win, which combines all queries in a session, extracts all unique query terms from them, and weights them equally. Win-win examine 21 search engine action options in total to

find out the optimal action that maximizes the joint long term reward Qse(b, a) + Qu(b, au) for both agents.
Table 4 shows the search accuracy of all systems under comparison for TREC 2012 Session Track. We can see that win-win search is better than most systems except QCM SAT. It statistically significantly outperforms Rocchio by 20%, Lemur by 18.9%, and PRF by 41.8% in nDCG@10 (p-value<.05, one-side t-test). It also outperforms RocchioCLK, Rocchio-SAT and QCM+DUP, but the results are not statistically significant. The trends for other evaluation metrics are similar to nDCG@10.
Table 5 shows the search accuracy of all systems for TREC 2013 Session Track. Since we only indexed ClueWeb12 CatB, after spam reduction, many relevant CatA documents are not included in the CatB collection. To evaluate the systems fairly, we created a filtered ground truth which only consists of relevant documents in CatB. The results are shown in Table 5. We can see that win-win is the best run among all systems. It shows statistically significant gain (p-value<.01, one-sided t-test) over all other systems across all evaluation metrics. Particularly, the proposed approach achieves a significant 54% improvement of nDCG@10 comparing to QCM+DUP. The experimental results support that our approach is highly effective.
6.2 Immediate Search Accuracy
TREC Session tasks request for retrieval results for the last query in a session. Theoretically, however, win-win search can optimize at every search iteration throughout a session. We hence compare our approach (the Win-Win run) with the top returned documents provided by TREC (the Original run) in terms of immediate search accuracy. We define immediate search accuracy at i as an evaluation score that measures search accuracy at search iteration i. The evaluation scores used are nDCG@10 and nERR@10.
We report the averaged immediate search accuracy for all sessions. It is worthy noting that session lengths vary. To average across sessions with different lengths, we make all sessions equals to the maximum session length in a dataset. TREC 2012 and 2013 Session have different maximum session lengths; they are 11 and 21, respectively. When a session is shorter than the maximum session length, we use the retrieval results from its own last iteration as the retrieval results for iterations beyond its own last iteration. In addition, since TREC did not provide any retrieval results for the last query, the Original runs has no value at the last iteration.
Figures 4 and 5 plot the immediate search accuracy for TREC 2012 & 2013 Session Tracks averaged over all sessions. We observe that win-win search's immediate search accuracy is statistically significantly better than the Original run at every iteration. In Figure 4, win-win outperforms Original since iteration 2 in nDCG@10 and outperforms it since iteration 3 in nERR@10. At the last iteration, winwin outperforms Original by a statistically significant 27.1% in nDCG@10 (p-value<.05, one-sided t-test). We observe similar trends in Figure 5. Another interesting finding is that win-win search's immediate search accuracy increases while the number of search iterations increases. In Figure 4, the nDCG@10 starts at 0.2145 at the first iteration and increases dramatically 37.1% to 0.2941 at the last iteration. It suggests that by involving more search iterations, i.e., learning from more interactions between the user and the search

594

Figure 4: TREC 2012 Im- Figure 5: TREC 2013 Immediate Search Accuracy. mediate Search Accuracy.

Figure 7: Factual and Specific sessions.

Figure 8: Factual and Amorphous sessions.

Figure 6: Long sessions (length >= 4). Transition probabilities are listed with actions: Add (A), Remove (R), and Keep (K).
engine, win-win is able to monotonically improve its search accuracy.
6.3 State Transitions
This experiment investigates how legitimate the proposed states are in presenting the hidden mental states of users.
First, we use examples to demonstrate the state transitions in sessions. Session 87 (Table 1) is a long session with 21 queries. The chain of decision states identified for this session based on techniques presented in Sections 4.2 and 4.4 is: SNRR(q1=best us destination)  SRT (q2=distance new york boston)  SNRT  SNRR  SNRR  SRR  SRR  SNRR  SRT  SRT  SRR(q11=boston tourism)  SNRR(q12=nyc tourism)  SNRR(q13=philadelphia nyc distance)  SNRR  SRT  SNRR  SRR  SNRT  SNRT (q19=philadelphia nyc travel)  SNRT (q20=philadelphia nyc train)  SNRT (q21 =philadelphia nyc bus). Our states correctly suggests that the user is in the exploration states (RR, NRR, NRR) from q11 to q13, while he keeps changing queries to explore from city to city (boston, new york city, and philadelphia). The user eventually finds the cities, philadelphia and nyc, that fulfill the information need ­ "best US destinations within a 150-mile radius". During the last 3 queries, the user exploits the current subtopic (philadelphia and nyc) to find out more specifics on transportations (travel, train, bus) about them. Our system correctly recognizes the last three states as exploitation states (NRT, NRT, NRT). This example suggests that the proposed states are able to reflect the real user decision states quite accurately.
Second, we examine state transition patterns in long sessions since they contain enough transitions for us to study. Figure 6 plots state probabilities, state transition probabilities, and that under different user actions for long sessions (sessions with 4 or more queries). The data are combined from both TREC 2012 & 2013 Session Tracks. We notice that NRR (non-relevant and exploration) is the most com-

mon state (42.4%). This reflects that a user spend may a long time to explore while receiving non-relevant documents. On the contrary, the RR state (relevant and exploration) is the least common state (11.3%).Moreover, we see that state transitions are not uniformly distributed. For instance, the transition from NRT to both relevant states (RT and RR) are very rare (in total 5.65%). In addition, we notice that actions are related to state transition probabilities. There are 90.8% transitions generated by adding terms and among all the transitions with removing terms, 84.8% of them lead to exploitation states (RT or NRT).
Third, we find that state probability distribution and state probability transitions differ among different session types. We plot the state probabilities and transition probabilities in Figures 7 to 10 for four different TREC session types, which were created along two aspects: search target (factual or intellectual ) and goal quality (specific or amorphous). Suggested by [24], the difficulty levels of the session types usually are FactualSpecific < IntellectualSpecific < FactualAmorphous < IntellectualAmorphous. An interesting finding is that as the session difficult level increases, the transition probability from state NRT (non-relevant and exploitation) to state RT (relevant and exploitation) becomes lower: FactualSpecific (0.25), IntellectualSpecific (0.2), FactualAmorphous (0.12), IntellectualAmorphous(0.1). It suggests that the harder the task is, the greater the necessity to explore rather than to exploit, when the user is not satisfied with the current retrieval results. In addition, we observe that Intellectual sessions (Figures 9 and 10) have a larger probability, 0.1548, to be in the RR (relevant and exploration) state than the other session types (on average 0.1018).
7. CONCLUSION
This paper presents a novel session search framework, winwin search, that uses a dual-agent stochastic game to model the interactions between user and search engine. With a careful design of states, actions, and observations, the new framework is able to perform efficient optimization over a finite discrete set of options. The experiments on TREC Ses-

595

Figure 9: Intellectual and Specific sessions.
Figure 10: Intellectual and Amorphous sessions.
sion 2012 and 2013 datasets show that the proposed framework is highly effective for session search.
Session search is a complex IR task. The complexity comes from the involvement of many more factors other than just terms, queries and documents in most existing retrieval algorithms. The factors include query reformulations, clicks, time spent to examine the documents, personalization, query intent, feedback, etc. Most existing work on sessions and task-based search focuses on diving into one aspect. Through significantly simplifying the factors, we realize the integration of all the factors in a unified framework. For example, we simplify users' decision states into only four states, and discretize user actions and search engine actions into a finite number of options. Such simplification is necessary in creating practical search systems.
This paper views the search engine as an autonomous agent, that works together with user, another autonomous agent, to collaborate on a shared task ­ fulfilling the information needs. This view assumes that the search engine is more like a "decision engine". Session search can be imagined as two agents exploring in a world full of information, searching for the goal in a trial-and-error manner. Here we assume a cooperative game between the two agents. However, as we mentioned in the introduction, the search engine agent can of course choose a different goal. It will be very interesting to see how to still satisfy the user to achieve winwin. We hope our work calls for future adventures in the fields of POMDP in IR and game theory in IR.
8. ACKNOWLEDGMENT
This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.
9. REFERENCES
[1] L. E. Baum and T. Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6), 1966.
[2] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good expansion terms for pseudo-relevance feedback. In SIGIR '08.

[3] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In CIKM '11.
[4] M.-A. Cartright, R. W. White, and E. Horvitz. Intentions and attention in exploratory health search. In SIGIR '11.
[5] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5), Oct. 2011.
[6] A. Diriye, R. White, G. Buscher, and S. Dumais. Leaving so soon?: Understanding and predicting web search abandonment rationales. In CIKM '12.
[7] C. Eickhoff, K. Collins-Thompson, P. N. Bennett, and S. Dumais. Personalizing atypical web search sessions. In WSDM '13.
[8] H. Feild and J. Allan. Task-aware query recommendation. In SIGIR '13.
[9] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White. Evaluating implicit measures to improve web search. ACM Trans. Inf. Syst., 23(2), Apr. 2005.
[10] D. Guan, H. Yang, and N. Goharian. Effective structured query formulation for session search. In TREC '12.
[11] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In SIGIR '13.
[12] Q. Guo and E. Agichtein. Ready to buy or just browsing?: detecting web searcher goals from interaction data. In SIGIR '10.
[13] J. Jiang and D. He. Different effects of click-through and past queries on whole-session search performance. In TREC '13.
[14] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track. In TREC '12.
[15] X. Jin, M. Sloan, and J. Wang. Interactive exploratory search for multi page search results. In WWW '13.
[16] T. Joachims. A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. 1997.
[17] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM '08.
[18] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1):99­134, 1998.
[19] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: a survey. J. Artif. Int. Res., 4(1), May 1996.
[20] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2012 session track. In TREC'12.
[21] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2013 session track. In TREC'13.
[22] A. Kotov, P. N. Bennett, R. W. White, S. T. Dumais, and J. Teevan. Modeling and analysis of cross-session search tasks. In SIGIR '11.
[23] Lemur Search Engine. http://www.lemurproject.org/. [24] J. Liu and N. J. Belkin. Personalizing information retrieval for
multi-session tasks: The roles of task stage and task type. In SIGIR '10.
[25] K. Raman, P. N. Bennett, and K. Collins-Thompson. Toward whole-session relevance: Exploring intrinsic diversity in web search. In SIGIR '13.
[26] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for personalized search. In CIKM '05.
[27] Y. Song and L.-w. He. Optimal rare query suggestion with implicit user feedback. In WWW '10.
[28] A. R. Taylor, C. Cool, N. J. Belkin, and W. J. Amadio. Relationships between categories of relevance criteria and stage in task completion. Information Processing & Management, 43(4), 2007.
[29] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize or not to personalize: Modeling queries with variation in user intent. In SIGIR '08.
[30] H. Wang, Y. Song, M.-W. Chang, X. He, R. W. White, and W. Chu. Learning to extract cross-session search tasks. In WWW '13.
[31] R. W. White, I. Ruthven, J. M. Jose, and C. J. V. Rijsbergen. Evaluating implicit feedback models using searcher simulations. ACM Trans. Inf. Syst., 23(3), July 2005.
[32] S. Yuan and J. Wang. Sequential selection of correlated ads by pomdps. In CIKM '12.
[33] S. Zhang, D. Guan, and H. Yang. Query change as relevance feedback in session search. In SIGIR '13.

596

Injecting User Models and Time into Precision via Markov Chains

Marco Ferrante
Dept. Mathematics University of Padua, Italy
ferrante@math.unipd.it

Nicola Ferro
Dept. Information Engineering University of Padua, Italy
ferro@dei.unipd.it

Maria Maistro
Dept. Information Engineering University of Padua, Italy
maistro@dei.unipd.it

ABSTRACT
We propose a family of new evaluation measures, called Markov Precision (MP), which exploits continuous-time and discrete-time Markov chains in order to inject user models into precision. Continuous-time MP behaves like timecalibrated measures, bringing the time spent by the user into the evaluation of a system; discrete-time MP behaves like traditional evaluation measures. Being part of the same Markovian framework, the time-based and rank-based versions of MP produce values that are directly comparable.
We show that it is possible to re-create average precision using specific user models and this helps in providing an explanation of Average Precision (AP) in terms of user models more realistic than the ones currently used to justify it. We also propose several alternative models that take into account different possible behaviors in scanning a ranked result list.
Finally, we conduct a thorough experimental evaluation of MP on standard TREC collections in order to show that MP is as reliable as other measures and we provide an example of calibration of its time parameters based on click logs from Yandex.
Categories and Subject Descriptors
H.3.4 [Information Search and Retrieval]: Systems and Software--Performance evaluation (efficiency and effectiveness)
General Terms
Experimentation, Measurement, Performance
Keywords
Evaluation; Markov Precision; User Model; Time
1. INTRODUCTION
Experimental evaluation has been central to Information Retrieval (IR) since its beginning [15] and Cranfield is the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609637.

predominant paradigm for carrying out system-oriented experimentation [11]. Over the decades, several measures have been proposed to evaluate retrieval effectiveness.
AP [5] represents the "gold standard" measure in IR [35], known to be stable [3] and informative [1], with a natural top-heavy bias and an underlying theoretical basis as approximation of the area under the precision/recall curve. Nevertheless, due to its dependence on the recall base, it assumes a perfect knowledge of the relevance of each document in the collection, which is an approximation when pooling is adopted and not assessed documents are assumed to be not relevant [14], and is even more exacerbated in the case of large scale or dynamic collections [4, 35].
However, the strongest criticism to AP comes from the absence of a convincing user model for it, a feature which is deemed extremely important in order to make the interpretation of a measure meaningful and to bridge the gap between system-oriented and user-oriented studies [7, 21, 31]. In this respect, [22] argued that the model behind AP is abstract, complex, and far from the real behavior of users interacting with an IR system, especially when it comes to its dependence on the recall base which is something actually unknown to real users. As a consequence, [25] proposed a simple but moderately plausibile user model for AP, which allows for a mix of different behaviors in the population of users.
In this paper, we take up from the final considerations of [25], at page 690: "this argument could provide the basis for a more elaborate model, by for example basing the set of ps(n) on some more sophisticated view of stopping behaviour", where ps(n) is the probability that the user satisfaction point is the document at rank n.
We propose a family of measures of retrieval effectiveness, called Markov Precision (MP), where we exploit Markov chains [23] to inject different user models into precision and which does not depend on the recall base. We represent each position in a ranked result list with a state in a Markov chain and the different topologies and transition probabilities among the states of the Markov chain allow us to model the different and perhaps complex user behaviors and paths in scanning a ranked result list. The invariant distribution of the Markov chain provides us with the probability of the user being in a given state/rank position in stationary conditions and we use these probabilities to compute a weighted average of precision at those rank positions.
The framework we propose is actually more general and it is based on continuous-time Markov chains in order to take into account also the time a user spends in visiting a sin-

597

gle document. It is then possible to extract a discrete-time Markov chain, when considering only the transitions among rank positions and not the time spent in each document. This gives us a two-fold opportunity: when we consider the discrete-time Markov chain, we are basically reasoning as traditional evaluation measures which assess the utility for the user in scanning the ranked result list; when we consider the continuous-time Markov chain,we also embed the information about the time spent by the user in visiting a document and we have a single measure including both aspects. This represents a valuable contribution of the paper since, up to now, rank and time have been two separate variables according to which retrieval effectiveness is evaluated [31].
The Markov chain approach relies on some assumptions ­ e.g. no long-term memory and exponentially distributed holding times ­ which may seem oversimplifications of the reality, e.g. a user who considers the whole history of visited documents to decide whether to stop or not. However, other measures, such as Rank-Biased Precision (RBP) [22] where transitioning to the next document or stopping is a step-bystep decision based just on the persistence parameter, are memory-less in this sense. Moreover, a Markovian model is simple enough to be easily dealt with while still being quite powerful and this work intends to be a first step towards a richer world of models that we will explore in the future.
We then propose some basic models for the transition matrix of the Markov chain. Clearly, this is not intended to be an exhaustive list of all the possible models but more of an exemplification of how it is possible to plug different user models into the framework. Still, these basic models provide a second valuable contribution of the paper. Indeed, we will show how some of these models, when provided with the same level of information about the recall base as AP, actually are AP, thus giving an explanation of it in terms of a slightly richer user model than the one of [25]. We will also show how some of them are extremely highly correlated to AP, thus suggesting how AP can be considered a very good approximation of more complex user strategies. This helps in shedding some light on why AP is the de-facto "gold standard" in IR, even though it has been so often criticized.
Finally, we conduct a thorough experimental evaluation of the MP measure both using standard Text REtrieval Conference (TREC)1 collections and click-logs with assessed queries made available by Yandex [29]. The results show that MP is comparable to other measures for some desirable properties like robustness to pool downsampling while the Yandex click-logs allow us to estimate the time spent by the users on the documents and apply the continous-time Markov chain.
The paper is organized as follows: Section 2 presents the related works; Section 3 discusses other pertinent measures to which MP will be compared; Section 4 fully introduces MP; Section 5 reports the conducted experimental evaluation of MP; and Section 6 draws some conclusion and provides an outlook for future work.
2. RELATED WORKS
Markov-based approaches have been previously exploited in IR, for example: Markov chains have been used to generate query models [19], for query expansion [12, 20], and for document ranking [13]. However, to the best of our knowl-
1http://trec.nist.gov/

edge, Markov chains have not been applied to the definition of a fully-fledged measure for retrieval effectiveness.
[8] uses Markov chains to address the placement problem in the case of two-dimensional results presentation: they have to allocate images on a grid to maximize the expected total utility of the user, according to some evaluation measure, and the Markov chain models how the user moves in the grid. Their approach differs from ours since they are not defining a measure of effectiveness which embeds a Markov chain but they rather solve an optimization problem via a Markov chain; moreover, they only use discrete-time Markov chains and limit transitions only to adjacent states. What we share is the idea that a Markov chain can be used to model how a user scans a result list, mono dimensional in our case, two-dimensional in their case.
When it comes to other evaluation measures, the focus of the paper is on lab-style evaluation, search tasks with informational intents [2], and binary relevance. So, for example, measures for novelty and diversity are out of the scope of the present paper [10] as are measures for graded relevance like Discounted Cumulated Gain (DCG) [16], Expected Reciprocal Rank (ERR) [6], or Q-measure [26].
With regard to the time dimension brought in by the continuous-time Markov chain, the most relevant work is Time-Biased Gain (TBG) [30, 31]. We share the idea of getting time into evaluation measures but we adopted a different approach. While TBG substitutes traditional evaluation measures, MP provides a single framework for keeping both aspects depending on which Markov chain you use. With respect to the user model adopted in TBG, there are some relevant differences: first, we use full Markov models while [30] at page 2014 points out that "our model can be viewed as a semi-Markov model"; then, TBG assumes a sequential scanning of the result lists where MP allows the user to move and jump backward and forward in the results list. What TBG addressed and is not in the scope of the present work is how to calibrate the measure with respect to time: [31] proposed a procedure to calibrate time with respect to document length and [30] extended it to stochastic simulation. In the present work, we provide a basic example of calibration based on the estimation of average time spent per document from click logs, just to show how the parameters of the framework could be tuned. However, in the future, nothing prevents us (or others) from investigating more advanced calibration strategies or applying those proposed by [30, 31].
Previous work on click logs [17] has reported that, on average, users scan ranked list in a forward linear fashion while MP allow users to move forward and backward in a ranked list. As reported in Section 5.5, from Yandex logs, we found that 21% of the users move backward in the ranked list, thus supporting our assumption, even if more exploration on this is left for future work. Moreover, U-measure [28] is a recent proposal which shares with MP the idea of removing the constraint of the linear scan but it does not adopt Markov models and has also somewhat different goals, such as evaluating complex tasks like multi-query sessions and diversified IR.
When it comes to other ways of modelling user behaviour into evaluation measures, [7] proposes relying on three components: a browsing model, a model of document utility, and a utility accumulation model. Even if we took up from [25], MP can also be framed in the light of the work of [7]. Indeed,

598

the Markovian model provides us with the browsing model, precision account for the model of document utility, and the weighted average of precision by the invariant distribution of the Markov chain supplies the utility accumulation model.
Thus, evaluation measures of direct comparison, which will be detailed in Section 3, are those built around the concept of precision, namely AP, P@10, and Rprec [5]. RBP [22] comes into play as a binary evaluation measure not dependent on the recall base, even though it is not built around the concept of precision despite its name. Finally, we are also interested in Binary Preference (bpref) [4], just to have a comparison point when testing MP with respect to reducedsize pools. In this last respect, we are not interested in infAP [35], since we are neither looking for an estimator of AP nor investigating alternative strategies for pool downsampling. For the same reason, we are not interested here in experimenting with respect to condensed-list measures [27].

3. OTHER EVALUATION MEASURES
Let us consider a ranked list of T documents in response to a given topic, let dn be the document retrieved at position n  T whose relevance is denoted by an, equal to 1 if the document is considered relevant and 0 otherwise. The ranked list of documents is denoted with D = {di, i  T } and R = {ij : j = 1, . . . , T and aij = 1} is the set of the ranks of the relevant documents, whose cardinality is r = |R| and which indicate the total number of relevant retrieved documents by the system for the given topic. Let RB be the recall base of the topic, i.e. the total number of judged relevant documents for a given topic, and N RB the total number of judged not relevant documents for a given topic.
The precision at rank n is thus defined as

1n

Prec(n) = n

am

(1)

m=1

which corresponds to the percentage or "density" of relevant

documents present among the first n, n included, in the

list. Note that Rprec is Prec(RB), which makes clear its

dependence on the recall base.

The recall at rank T is defined as

Rec(T )

=

r RB

(2)

which corresponds to the fraction of relevant documents of the specific run with respect to the total number of judged relevant documents.

3.1 Average Precision (AP)
The original definition of Average Precision (AP) [5] is the average over all RB judged relevant documents of the precision at their ranks, considering zero the precision at the not retrieved relevant documents:

AP = 1

Prec(i) = r · 1 Prec(i) (3)

RB

RB r

iR

iR

where, in the last equation, the first operand is the recall and the second one is the arithmetic mean of the precisions at each relevant retrieved document. This formulation further highlights the dependence of AP on the recall base and the recall itself.

As previously discussed, [25] proposed a simple, probabilistic user model measure of effectiveness called Normalized Cumulative Precision (NCP), which includes AP as a particular case. The author assumes that any given user will stop his search at a given document in the ranked list, that we call its satisfaction point, according to a common probability law.
Furthermore, he considers that a user will stop his search only at relevant documents and that the probability that he stops at any given relevant documents is fixed and independent from the specific run he is considering, while it is 0 at any non relevant document. So, he defines a probability distribution ps on the set of all the documents available for a given topic.
Given a specific run and the set of its retrieved documents D, the definition of the NCP is then the expectation (average) of the precision at the ranks of the retrieved, relevant documents, accordingly to a distribution ps(·), i.e.

+
N CP (ps) = Eps [Prec(n)] = ps(dn)Prec(n) .
n=1

It is easy to see that the above definition of AP is in this con-

text equal to the NCP measure when we choose the uniform

law pU over all the relevant documents for the topic

 

1

pU (dn) = 

RB 0

if dn is relevant otherwise

The previous user model is simple and it can be considered as a starting point for more sophisticated models, as also suggested by [25] itself. As in the case of AP, the assumption that the user knows the recall base of a given topic is a weakness of this model. Furthermore, the probability that a user stops their search at a given document on a specific run depends on a probability distribution defined on the whole set of relevant documents available for a given topic.
The choice of the uniform distribution to determine the stopping point in a given search is itself of difficult interpretation, since this means that any relevant document in a ranked list of retrieved documents has the same probability.
We will see in the next section how, stepping from the intuition behind NCP, we can define, thanks to simple Markov chains, a more realistic user model, how AP can be still considered as a good approximation in many cases and how to generalize AP to a whole new class of Markovian models.

3.2 Rank-Biased Precision (RBP)
Rank-Biased Precision (RBP) [22] assumes a user model where the user starts from the top ranked document and with probability p, called persistence, goes to the next document or with probability 1 - p stops. RBP is defined as follows:

RBP = (1 - p) pi-1

(4)

iR

It can be noted that, despite its name, RBP does not depend on the notion of precision. Nevertheless, it represents a measure for binary relevance which does not depend on the recall base and thus gives a comparison point in this last respect for MP.

599

3.3 Binary Preference (bpref)
Binary Preference (bpref) [4, 32] is a measure based on binary preferences and it evaluates systems using only the judged documents. It can be thought of as the inverse of the fraction of judged irrelevant documents that are retrieved before relevant ones:

bpref = 1

1 - |j ranked higher than i|

(5)

RB

min(RB, N RB)

iR

where j is a member of the first RB not relevant retrieved documents. bpref has proved to be quite robust in the case of incomplete and imperfect relevance judgements. Here, for us, it represents a comparison point when evaluating MP with respect to reduced-size pools.
It can be noted how heavily bpref depends on the recall base RB. This is not only a scale factor as in the case of AP but it also determines the cardinality of the set from which the not relevant documents j are taken. Moreover, it makes use also of N RB, the total number of judged not relevant documents, a kind of information which is hard to imagine available to any real user. So, in a sense, it seems much more a "pool-oriented" than a system-oriented measure since, for determining its score, it uses much more information about the pool than about the system under examination and this could be an explanation of its robustness to the pool reduction.

4. A MARKOVIAN USER MODEL
4.1 General framework
We will assume that each user starts from a chosen document in the ranked list and considers this document for a random time, that is distributed according to a known positive random variable. Then they decides, according to a probability law that we will specify in the sequel and independent from the random time spent in the first document, to move to another document in the list. Then, they considers this new document for a random time and moves, independently, to a third relevant document and so on.
After a random number of forward and backward movements along the ranked list, the user will end their search and we will evaluate the total utility provided by the system to them by taking the average of the precision of the judged relevant documents they has considered during their search. According to this construction when we compute this average, the precision of a document visited k times will contribute to the mean with a k/n weight.
We mathematically model the user behavior in the framework of the Markovian processes [23]. To fix the notation, we will denote by X0, X1, X2, . . . the (random) sequence of document ranks visited by the user and by T0, T1, T2 the random times spent, respectively, visiting the first document considered, the second one and so on. Therefore, X0 = i means that the user starts from the first document at rank i and T0 = t0 means that they spends t0 units of time visiting this first document, then X1 = j means that they visits the document at rank j as the second one, and so on.
First of all, we will assume that X0 is a random variable on T = {1, 2, . . . , T } with a given distribution  = (1, . . . , T ); so for any i  T , P[X0 = i] = i. Then, we will assume that the probability to pass from the document at rank i to the

document at rank j will only depend on the starting rank i and not on the whole list of documents visited before.
This can be formalized as follows:
P[Xn+1 = j|Xn = i, Xn-1 = in-1, . . . , X0 = i0] = (6)
= P[Xn+1 = j|Xn = i] = pi,j
for any n  N and i, j, i0, . . . , in-1  T .

p1,T

p1,2

p1,T-1 p1,3
p2,T-1 p2,3

p2,T

d1

d2

d3

dT-1

dT

pT-1,1

pT-1,2

pT-1,3

pT,T-1 pT,3

pT,1

pT,2

Figure 1: Structure of the Markov chain (Xn)nN.

Thanks to the condition (6) and fixing a starting distribu-

tion , the random variables (Xn)nN define a time homoge-

nous discrete time Markov Chain, shown in Figure 1, with

state space T , initial distribution  and transition matrix

P = (pi,j )i,jT (Markov(,P) in the sequel).

To obtain a continuous-time Markov Chain, we have to

assume that the holding times Tn have all exponential dis-

tribution, i.e.  0

t<0

P[Tn  t] =  1 - exp(-t) t  0

Furthermore, conditioned on the fact that Xn = i, the law of Tn will be exponential with parameter i, where i is a positive real number that may depend on the specific state i of the chain the user is visiting at that time.
When our interest is only on the jump chain (Xn)nN, i.e. when we are interested in extracting the corresponding discrete-time Markov chain to act as a traditional evaluation measure, we simply assume that all these variables are exponential with parameter  = 1. When we are also interested in the time dimension, we have to provide a calibration for these exponential variables. We report a very simple example in Section 5 using click logs from Yandex.
The reason for choosing such a model will be immediately clear. Let us assume hereafter that the matrix P will be irreducible. This means that we can move in a finite number of steps from any document to any other document with positive probability. Thanks to (6) and the multiplication rule, the probability to pass in n steps from the document i to the document j is equal to p(i,nj), the (i, j) entry of the matrix P n and the irreducibility means that given any pair (i, j) there exists n > 0 such that p(i,nj) > 0. Furthermore, the probability distribution of any random variable Xn, which denotes the rank of the document visited after n movements, is completely determined by  and P , since
P[Xn = j] = (P n)j .
Given such a model, we assume that a user will visit a number n of documents in the list and then they will stop their search. In order to measure their satisfaction, we will evaluate the average of the precision of the ranks of the judged

600

relevant documents visited by the user during their search as
1 n-1 n Prec(Yk) .
k=0
where (Yn)nN denotes the sub-chain of (Xn)nN that considers just the visits to the judged relevant documents at ranks R, and shown in Figure 2.

p1,2

p1,T p2,T

d1

d2

d3

dT-1

dT

pT,1

pT,2

Figure 2: Structure of the sub-Markov chain (Yn)nN (relevant documents are shown in grey; not relevant ones in white).

Note that this sub-chain has in general a transition ma-
trix different form P . The new transition matrix P can be computed easily from P by solving a linear system as detailed in [23] and discussed in Section 4.3.1. Note that P computed in this way somehow "absorbs" and takes into account also the probabilities of passing through not relevant documents (which are basically redistributed over the relevant ones) and makes it different from the transition matrix that you would have obtained by using only the relevant documents since the beginning.
Clearly the previous quantity is of little use if evaluated at an unknown finite step n. However, the Ergodic Theorem of the theory of the Markov processes is perfect for approximating this quantity:

Theorem 1. Let P be irreducible,  be any distribution
and R finite. If (Yn)n0 is Markov(,P ), then for any function f : R  R we have

P

1 n

n-1
f (Yk) 

f

as

n



=1

k=0

where f = iR if (i) and  is the invariant distribution of P .

The importance of this class of theorems is clear: almost surely and independently of the initial distribution , we can approximate, for n large, the average over the time by the (much simpler) average over the states of the Markov chain. Indeed, under the previous assumptions it is possible to prove that the matrix P admits a unique invariant distribution, i.e a probability distribution  such that if (Yn)n0 is Markov(,P ), then for any n
P[Yn = j] = j .
Moreover, the invariant distribution in this case is the unique left eigenvector of the eigenvalue 1 of the matrix P , i.e. the unique solution of the linear equation

 = P .

Remark 1. Under additional hypotheses, it can be proved that the invariant distribution itself is the limit of any row of the matrix P n, as n  , useful result in order to evaluate in practice the invariant distribution. The convergence is generally very fast and for n = 10 we already have a reasonable approximation of the true value of . This justifies the use of MP to approximate the mean precision of the usually few documents visited by a user.

We can now define a new family of user oriented retrieval effectiveness measures, called Markov Precision (MP), which depends on the specific user model and the invariant distribution derived.

Definition 1. Given a ranked list of retrieved documents, defined by R the ranks of its judged relevant documents and defined a Markov (,P) user model, the Markov Precision metric will be defined as

M P = iPrec(i).
iR

where Prec(n) represent the Precision at n and  the (unique) invariant distribution of the Markov chain (Yn)nN.

MP is defined without knowing the recall base RB of a given topic, but just the ranks of the judged relevant documents in a given run for this topic. As pointed out, for example in [22], the need to know the value of RB represents a weakness in AP that is overcome here.
In order to include the time dimension and thanks to the Ergodic Theorem for the continuous time Markov chains, we can replicate the previous computations and define a new measure

M P cont = iPrec(i).
iR

where i =

, i (i )-1
jR j (j )-1



denotes

again

the

(unique)

distribution of the Markov chain (Yn)nN and i is the pa-

rameter of the holding time in state i. To use this alternative

measure, we have to provide a calibration for the coefficients

i and we will compare MP with MPcont in a very simple

example in Section 5 using click logs from Yandex.

4.2 Average Precision

In order to define a simple Markovian user model, whose MP value will be AP, let us consider the following transition probabilities among the documents in a given ranked list:

P[Xn+1

=

j|Xn

=

i]

=

1 T -1

(7)

for any i, j  T , i = j, and where, again, T denotes the

cardinality of the set T .

In this model we assume that a user moves from a docu-

ment to another document with a fixed, constant probability,

the value of which depends on the total number of relevant

documents present in the specific run.

Since the invariant distribution is

1 T

,

1 T

,

.

.

.

,

1 T

we obtain

that

1

MP =

P rec(i)

T

iR

which is equal to AP

once multiplied by

T RB

.

Note that if

we create the Markov chain starting directly from the rele-

vant documents R we have to multiply MP by Rec(T ) as in

601

equation 3. In this way, we explain AP with a slightly richer user model, where the user can move forward and backward among any document and is not forced to visit only the relevant ones. It is also clear from the equation above that MP is not AP unless you provide it with the same amount of information AP knows about the recall base, namely rescaling MP by the recall base.
Looking at this the other way around, this instantiation of MP (without the rescaling) can be considered a kind of AP where the artificial knowledge of the recall base has been removed and so, it tells us how AP might look like if you remove the dependency on the recall base and insert an explicit user model. This consideration will turn out to be useful in the experimental part when we will find other user models, highly correlated to AP, which may give a richer explanation of it.
Moreover, the previous constant invariant distribution is common to many others user models. For example, if the transition matrix is irreducible and symmetric or even just bistochastic, meaning that the sum of the entries on each column is equal to 1, the invariant distribution is again the above constant vector. In this sense, if the validity of the present Markovian user model is accepted, it shows once more why AP has become a reference point, since it represents a good approximation for a wide class of models that we can define.
4.3 Other models
We will analyze three possible choices:
· state space choice: the Markov chain (Xn)nN is on the whole set T , indicated with AD (all documents model), or on the set R, indicated with OR (only relevant documents model);
· connectedness: the nonzero transition probabilities are among all the documents, indicated with GL (global model), or only among adjacent documents, indicated with LO (local model);
· transition probabilities: the transition probabilities are proportional to the inverse of the distance, indicated with ID (inverse distance model), or to the inverse of the logarithm of the distance, indicated with LID (logarithmic inverse distance model).
We will obtain eight models that we will call after the possible three choices. So, for example, MP GL AD ID is an effectiveness measure with transition probabilities among all the retrieved documents, based on a model on the whole set T , and with transition probabilities proportional to the inverse of the distance of the documents in the ranked list and so on for the other combinations of the parameters.
4.3.1 State space choice
In the AD case, we consider the whole Markov chain (Xn)nN on the whole set T with a given initial distribution  and a transition matrix P = (pi,j )i,jT and then we derive the subchain (Yn)nN on the set R. In order to obtain the invariant distribution of the subchain, we will have to derive its transition matrix P . It can be proved (see [23]) that this matrix can be defined as follows
pi,j = hji for i, j  R

Table 1: Main features of the adopted data sets.

Topics Runs Min. Rel Avg. Rel Max. Rel

TREC 7

50

103

7

93.48

361

TREC 8

50

129

6

94.56

347

TREC 10 50

97

2

67.26

372

TREC 14 50

74

9

131.22

376

where the vector (hji , i  T ) is the minimal non-negative solution to the linear system

hji = pi,j +

pikhjk .

(8)

k=R

So, once this linear system is solved, we obtain the transition
matrix P needed to compute the Markov Precision for the given model.
In the OR model, we create the Markov Chain (Xn)nN directly on the set R.

4.3.2 Connectedness
In the GL model, we assume that the transition probabilities pi,j > 0 for any choice of i = j. In this case we will assume that there will be a positive, even if very small, probability to pass from any document in the ranked list to any other. For example, the previous model for Average precision is a GL model
By contrast, in LO we will assume that there exist transition probabilities only among adjacent nodes. This is the same kind of logic behind RBP, even though RBP allows only for forward transitions, and is similar to the strategy of [8] for the two-dimensional placement problem.

4.3.3 Transition probabilities

In the ID model, we assume that the probability to pass

from one document to another one in the ranked list is pro-

portional to the inverse of the relative distance of these two

documents:

 

1 |i-j|+1

if i = j

(i, j) =  0

if i = j

(9)

Denoting by (s1, . . . , sm) the states of the Markov chain, we thus have the following transition probabilities:

psi,sj =

(si, sj ) (si, sk)
k

(10)

It is immediately clear that the probabilities (10) define an irreducible transition matrix P of a discrete time Markov Chain on the state space and therefore we can define Markov precision for this model.
In the LID model, we smooth the distance by using the base 10 logarithm so that that transition probabilities do not decrease not too fast. The choice of the base 10 for the logarithm is due to a typical Web scenario focused on the page of the first 10 results.

5. EVALUATION
5.1 Experimental Setup
In order to assess MP and compare it to the other pertinent evaluation measures (AP, P@10, Rprec, RBP, and

602

Table 2: Kendall  correlation between AP and the other comparison measures using complete judgments (high correlations marked with *).
AP P@10 Rprec bpref RBP TREC 7 1.000 0.8018 0.9261* 0.9275 0.7886 TREC 8 1.000 0.8264 0.9219* 0.9361* 0.8090 TREC 10 1.000 0.7551 0.8730 0.8896 0.7401 TREC 14 1.000 0.7295 0.9377 0.8394 0.7229
bpref), we conducted a correlation analysis and we studied its robustness to pool downsampling. As far as RBP is concerned, we set p = 0.8, which indicates a medium persistence of the user.
We used the following data sets: TREC 7 Ad Hoc, TREC 8 Ad Hoc, TREC 10 Web, and TREC 14 Robust, whose features are summarized in Table 1. We used all the topics and all the runs that retrieved at least one document per topic. In the case of collections with graded relevance assessment (TREC 10 and 14), we mapped them to binary relevance with a lenient strategy, i.e. both relevant and highly relevant documents have been mapped to relevant ones.
As far as pool downsampling is concerned, we used the same strategy of [4]: it basically creates separate random lists of relevant/not relevant documents and select a given fraction R% of them, ensuring that at least 1 relevant and 10 not relevant documents are in the pool. We used R% = [90, 70, 50, 30, 10].
As far as the calibration of time is concerned, we used click logs made available by Yandex [29] in the context of the Relevance Prediction Challenge2. The logs consist of 340,796,067 records with 30,717,251 unique queries, retrieving 10 URLs each. We used the training set where there are 5,191 assessed queries which correspond to 30,741,907 records and we selected those queries which appear at least in 100 sessions each to calibrate the time.
The full source code of the software used to conduct the experiments is available for download3 in order to ease comparison and verification of the results.
5.2 Correlation Analysis
Table 2 reports the Kendall  correlation [18] between AP and the other comparison measures, using complete judgements, for all the collections. Previous work [33, 34] considered correlations greater than 0.9 as equivalent rankings and correlations less than 0.8 as rankings containing noticeable differences. Table 2 is consistent with previous findings, with a high correlation between AP, Rprec, and bpref and lower correlation values for P@10 and RBP.
Table 3 reports the Kendall  correlation between the different models for MP, discussed in Section 4.3 and whose notation (GL/LO, AD/OR, ID/LID) is used here as well, and the performance measures of direct comparison, for all the considered collections4. For each variant of MP, the table reports its actual value and also a second row labelled with the suffix @Rec(T ) to indicate a rescaled version of
2http://imat-relpred.yandex.ru/en/ 3http://matters.dei.unipd.it/ 4The fact that the values for the LO AD ID and LO AD LID models are the same is not due to a copy&paste error but to the fact that the two chains, in the local model, are the same apart from a constant and so they produce equal rankings.

MP by recall. Indeed, this is the same operation needed to make MP equal to AP in the case of the model with constant transition probabilities discussed in Section 4.2 and corresponds to providing MP with the same level of information about the recall base that also AP uses. This has a twofold purpose: (i) to determine if there are other models beyond the ones of Section 4.2 which can give us an additional interpretation of AP; (ii) to get a general feeling of what is the impact of injecting information about the recall into an evaluation measure. In the table, we have marked high correlations, those above 0.90, with a star and we have marked extremely high correlations, those above 0.97, with two stars.
As a general trend MP tends not to have high correlations with the other evaluation measures, indicating that it takes a different angle from them. This can be accounted for by the effect of the user model explicitly embedded in MP which, for example, allows the user to move forward and backward in the result list while other measures allow only for sequential scans. On the other hand, the proposed models keep it not too far away from the other measures, especially those around precision (AP, P@10, Rprec), since the correlation never drops below 0.70. This is coherent with the fact that both MP and the other measures (AP, P@10, Rprec) are all around the concept of precision and so they have a common denominator.
Moreover, it can be noted that MP tends to be more correlated with P@10 and then with Rprec and AP. This is consistent with the fact that MP does not depend on the recall base, as P@10 does, while Rprec implicitly and AP explicitly depend on it.
Finally, the results show a moderate correlation with bpref and a slightly lower one with RBP, whose only common denominator is to not depend on the recall base.
Whit regard to @Rec(T ), we can note how they greatly boost the correlation with AP in almost all cases, often moving MP from low to high correlations, and, in turn, increase the correlation with Rprec and bpref (more correlated by themselves to AP) with respect to the one with RBP which tends to decrease.
In particular, there are some cases, like MP GL AD LID or MP LO AD ID, where it jumps between 0.97 and 1.00. We consider this a case in which MP is providing us with an alternative interpretation of AP, in the sense discussed in Section 4.2. For example, MP GL AD LID provided with information about recall tells us that we can look at AP as a measure that also models a user who can move backward and forward among all the documents in the list and who prefers smaller jumps to bigger ones. The fact that we have found a few models so highly correlated with AP suggests that AP has become a gold standard also because it represents some articulated user models.
5.3 Effect of Incompleteness on Absolute Performances
Figure 3 shows the effect of reducing the pool size on the absolute average performances, over all the topics and runs. For space reasons, we do not report figures for all the possible combinations reported in Table 3 but just some to give the reader an idea of the behavior of MP; the considerations made here are however valid also for the not reported figures.
It can be noted how MP shows consistent behavior over all the collections and for various models: its absolute aver-

603

Table 3: Kendall  correlation between different instantiations of MP and the other comparison measures

using complete judgments (high correlations marked with *; extremely high correlations marked with **).

MP GL AD ID MP GL AD ID@Rec(T ) MP GL AD LID MP GL AD LID@Rec(T ) MP GL OR ID MP GL OR ID@Rec(T ) MP GL OR LID MP GL OR LID@Rec(T ) MP LO AD ID MP LO AD ID@Rec(T ) MP LO AD ID MP LO AD ID@Rec(T ) MP LO OR ID MP LO OR ID@Rec(T ) MP LO OR LID MP LO OR LID@Rec(T )

AP 0.7381 0.9823** 0.7378 0.9954** 0.7322 0.9117* 0.7379 0.9726** 0.7435 0.9946** 0.7435 0.9946** 0.7271 0.9130* 0.7386 0.9552*

TREC 7 P@10 Rprec 0.7522 0.7703 0.7916 0.9243* 0.7638 0.7712 0.7994 0.9252* 0.8311 0.7797 0.8316 0.8937 0.7853 0.7782 0.8158 0.9238* 0.7706 0.7706 0.7994 0.9225* 0.7706 0.7706 0.7994 0.9225* 0.8229 0.7754 0.8283 0.8958 0.8065 0.7826 0.8278 0.9166*

bpref 0.7827 0.9322* 0.7802 0.9277* 0.7689 0.8848 0.7788 0.9232* 0.7874 0.9265* 0.7874 0.9265* 0.7634 0.8853 0.7787 0.9142*

RBP 0.7490 0.7799 0.7632 0.7858 0.7689 0.8243 0.7858 0.8029 0.7685 0.7858 0.7685 0.7858 0.8393 0.8211 0.8058 0.8164

AP 0.8997 0.9815** 0.8912 0.9953** 0.8162 0.9208* 0.8664 0.9722** 0.8931 0.9953** 0.8931 0.9953** 0.8138 0.9195* 0.8534 0.9506*

P@10 0.8510 0.8128 0.8641 0.8221 0.9081* 0.8756 0.8884 0.8477 0.8642 0.8248 0.8642 0.8248 0.9013* 0.9195* 0.8982 0.8623

TREC 8 Rprec 0.9074* 0.9217* 0.9033* 0.9209* 0.8349 0.9024* 0.8853 0.9281* 0.9011* 0.9219* 0.9011* 0.9219* 0.8305 0.8714 0.8708 0.9186*

bpref 0.9222* 0.9299* 0.9173* 0.9337* 0.8402 0.9145* 0.8947 0.9390* 0.9174* 0.9343* 0.9174* 0.9343* 0.8354 0.8987 0.8810 0.9319*

RBP 0.8382 0.7938 0.8551 0.8041 0.9152* 0.8637 0.8858 0.8324 0.8537 0.8066 0.8537 0.8066 0.9176* 0.9127* 0.8995 0.8466

MP GL AD ID MP GL AD ID@Rec(T ) MP GL AD LID MP GL AD LID@Rec(T ) MP GL OR ID MP GL OR ID@Rec(T ) MP GL OR LID MP GL OR LID@Rec(T ) MP LO AD ID MP LO AD ID@Rec(T ) MP LO AD ID MP LO AD ID@Rec(T ) MP LO OR ID MP LO OR ID@Rec(T ) MP LO OR LID MP LO OR LID@Rec(T )

AP 0.7264 0.9726** 0.7125 0.9941** 0.7034 0.9117* 0.7052 0.9738** 0.7240 0.9742** 0.7240 0.9742** 0.7035 0.9326* 0.7114 0.9579*

TREC 10 P@10 Rprec 0.7832 0.7727 0.7340 0.8631 0.7971 0.7633 0.7512 0.8707 0.8269 0.7663 0.8316 0.8937 0.8077 0.7672 0.7575 0.8740 0.7969 0.7703 0.7376 0.8654 0.7969 0.7703 0.7376 0.8654 0.8300 0.7646 0.7726 0.8767 0.8172 0.7676 0.7601 0.8747

bpref 0.7611 0.8771 0.7494 0.8878 0.7470 0.8848 0.7466 0.8916 0.7614 0.8802 0.7614 0.8802 0.7449 0.8960 0.7533 0.8949

RBP 0.8013 0.8771 0.8187 0.7360 0.8590 0.8243 0.8396 0.7448 0.8159 0.7218 0.8159 0.7218 0.8618 0.7618 0.8472 0.7477

AP 0.8351 0.9896** 0.8294 0.9977** 0.7968 0.9601* 0.8140 0.9924** 0.8297 0.9970** 0.8297 0.9970** 0.7997 0.9674* 0.8084 0.9877**

TREC 14 P@10 Rprec 0.8078 0.8566 0.7221 0.9333* 0.8185 0.8501 0.7303 0.9385 0.8461 0.8206 0.7526 0.9327* 0.8291 0.8348 0.7375 0.9398* 0.8180 0.8504 0.7295 0.9363* 0.8180 0.8504 0.7295 0.9363* 0.8348 0.8234 0.7429* 0.9348* 0.8324 0.8306 0.7372 0.9381*

bpref 0.7778 0.8360 0.7751 0.8397 0.7677 0.8650 0.7716 0.8432 0.7783 0.8405 0.7783 0.8405 0.7714 0.8597 0.7689 0.8489

RBP 0.7980 0.7140 0.8071 0.8397 0.8302 0.7444 0.8155 0.7293 0.8089 0.7214 0.8089 0.7214 0.8220 0.7377 0.8180 0.7306

age values decrease as the pool reduction rate increases in a manner similare to AP and Rprec. Consistently with previous results, P@10 and RBP exhibit a more marked decrease while bpref tends to stay constant. This positive property of bpref is an indicator that it is not very sensible or it does not fully exploit the additional information which is provided when the pool increases.
5.4 Effect of Incompleteness on Rank Correlation
Figure 4 shows the effect of reducing the pool size on the Kendall  correlation between each measure on the full pool and the pool at a given reduction rate. The results shown are consistent with previous findings as far as the measures of direct comparison are concerned, showing that bpref is almost always the more robust measure to pool reduction. It is indeed plausible that, keeping bpref the absolute average performances almost constant, also the ranking of the systems does not change much.
As far as MP is concerned, we can note that global models [GL], shown in the case of TREC 7, 8 and 10, tend to perform comparably to AP and, when provided with the same information about the recall base, which both AP and bpref exploit, they consistently improve their performances and, in the case of TREC 8, they outperform AP and perform closely to bpref. This is an interesting result since, unlike

bpref, the absolute average performances of MP vary at different pool reduction rates, indicating that MP is able to exploit the variable amount of information available at different pool reduction rates, still not affecting too much the overall ranking of the systems.
The global models [GL] on only relevant documents [OR] behave consistently with the global ones on all documents [AD], shown in the case of TREC 7 and TREC 10, even if they are a little bit more resilient to the pool reduction. This is consistent with the fact that they use less information than the AD ones and so they are less sensitive to the pool size. The TREC 7 also shows the effect of using the inverse of the distance [ID] or the log of the inverse of the distance [LID], which provides more robustness to pool reduction.
When it comes to local models [LO], these tend to behave comparably to the global ones in the case of all documents [AD], as can be noted in the case of TREC 8, while they are more affected by the pool reduction in the case of only relevant documents [OR], as can be noted in the case of TREC 14.
5.5 Time Calibration
On the basis of the click logs, 21% of the observed transitions are backward, a fact that validates our assumption that a user moves forward and backward along the ranked list.

604

Performances averaged over topics and runs

0.45 0.4
0.35 0.3
0.25 0.2
0.15 0.1
0.05 0
100%

90%

TREC 07, 1998, Ad Hoc

GL_OR_ID GL_OR_ID@Rec(T) GL_OR_LID GL_OR_LID@Rec(T) AP P@10 Rprec bpref RBP

70%

50%

Pool reduction rate

30%

10%

Performances averaged over topics and runs

0.45 0.4
0.35 0.3
0.25 0.2
0.15 0.1
0.05 0
100%

90%

TREC 08, 1999, Ad Hoc

70%

50%

Pool reduction rate

GL_AD_ID GL_AD_ID@Rec(T) LO_AD_ID LO_AD_ID@Rec(T) AP P@10 Rprec bpref RBP

30%

10%

Performances averaged over topics and runs

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0
100%

90%

TREC 10, 2001, Web

70%

50%

Pool reduction rate

GL_AD_LID GL_AD_LID@Rec(T) GL_OR_LID GL_OR_LID@Rec(T) AP P@10 Rprec bpref RBP

30%

10%

Performances averaged over topics and runs

0.45 0.4
0.35 0.3
0.25 0.2
0.15 0.1
0.05 0
100%

90%

TREC 14, 2005, Robust

LO_AD_ID LO_AD_ID@Rec(T) LO_OR_ID LO_OR_ID@Rec(T) AP P@10 Rprec bpref RBP

70%

50%

Pool reduction rate

30%

10%

Figure 3: Pool reduction rate (x axis) vs. performance averaged over topics and runs (y axis)

TREC 07, 1998, Ad Hoc 1

0.9

0.8

0.7

0.6 0.5 0.4 100%

GL_OR_ID GL_OR_ID@Rec(T) GL_OR_LID GL_OR_LID@Rec(T) AP P@10 Rprec bpref RBP
90%

70%

50%

Pool reduction rate

30%

10%

Kendalls  correlation

1

0.95

0.9

0.85

0.8

0.75

0.7 0.65
0.6 0.55
0.5 100%

GL_AD_ID GL_AD_ID@Rec(T) LO_AD_ID LO_AD_ID@Rec(T) AP P@10 Rprec bpref RBP
90%

TREC 08, 1999, Ad Hoc

70%

50%

Pool reduction rate

30%

10%

Kendalls  correlation

TREC 10, 2001, Web 1

0.9

0.8

0.7

0.6 0.5 0.4 100%

GL_AD_LID GL_AD_LID@Rec(T) GL_OR_LID GL_OR_LID@Rec(T) AP P@10 Rprec bpref RBP
90%

70%

50%

Pool reduction rate

30%

10%

Kendalls  correlation

TREC 14, 2005, Robust 1

0.9

0.8

0.7

0.6 0.5 0.4 100%

LO_AD_ID LO_AD_ID@Rec(T) LO_OR_ID LO_OR_ID@Rec(T) AP P@10 Rprec bpref RBP
90%

70%

50%

Pool reduction rate

30%

10%

Figure 4: Pool reduction rate (x axis) vs. Kendall's rank correlation (y axis)

Kendalls  correlation

To compare the discrete-time version of MP with the continuous-time one, we have considered 3 runs with 5 relevant documents and estimated the parameters of the exponential holding times by the inverse of the sample mean of the time spent by the users visiting these states, multiplied by (n - 1)/n. We used the GL AD ID model and the values of discrete-time MP and continuous-time MP are reported in Table 4.
Note that the precisions at each fixed rank n of the first, second and third runs are decreasing and as one expects MP of the three runs is decreasing. However, since the (estimated) holding times of the first documents in the first run are very low, continuos-time MP is smaller for the first run. This clearly shows that the use of continuous-time MP depends heavily on the calibration of the holding times.
6. CONCLUSIONS AND FUTURE WORK
We introduced a new family of measures, called MP, which exploit Markov chains in order to inject different user models and time into precision and which is not dependent on the recall base. This permitted us to overcome some of the traditional criticisms of AP (lack of a clear user model, dependence on the recall base) while still offering a measure which is AP when provided with the same amount of information about the recall base that AP exploits. Moreover, MP goes beyond almost all the evaluation measures allowing for non sequential scanning of the result lists.
We have proposed some basic user interaction models and validated their properties, in terms of correlation to other measures and robustness to pool reduction, thus showing it is as reliable as them. We have also found that some of these models have an extremely high correlation with AP and this can help in providing alternative interpretations of AP in the light of more complex user models and in explaining why AP is a "gold standard" in IR.
MP also bridges the gap between "rank-oriented"and "timeoriented" measures, providing a single unified framework where both viewpoints can co-exist and allowing for direct

comparison among the values of the "rank-oriented"(discretetime Markov chain) and "time-oriented" (continuous-time Markov chain) versions. We have also provided an example of how time can be calibrated using click logs from Yandex.
Future works concern the investigation of alternative user models able to account also for the number of relevant/not relevant documents visited so far ­ a kind of information which is actually available to a real user ­ by employing a multidimensional Markov chains to not violate the memoryless assumption. A further interesting option would also be to investigate whether click model-based IR measures [9] can be represented via the Markov chain and thus embedded in MP, i.e. whether the transition probabilities of the Markov chain can be learned directly from click-logs, thus leveraging models fully induced by user behaviour.
Another area of interest concerns how to calibrate time into MP: work on click model-based measures can shed some light in this respect and the techniques proposed by [30, 31] for calibrating time with respect to document length can link MP not only to click logs but also to document collections.
An interesting question for the future is whether MP could fit search tasks other than informational ones, such as fact, entity, or attributes focused searches or whether it could also work with other kinds of test collections, such as nuggetbased ones [24].
Finally, the robustness of MP could be further investigated, for example evaluating how it performs on condensedlists [27].
Acknowledgements
We wish to thank the anonymous reviewers and meta-reviewers whose comments and discussions helped us in improving the paper and better clarifying some angles of it.
The PREFORMA project5 (contract no. 619568), as part of the 7th Framework Program of the European Commission, has partially supported the reported work.
5http://www.preforma-project.eu/

605

Table 4: Estimated parameters of the exponential holding times for three runs and values of the discrete-time

and continuous-time MP.

Run

µ1

µ2

µ3

µ4

µ5

µ6

µ7

µ8

µ9

µ10 disc MP cont MP

(1,1,1,1,0,0,0,1,0,0) 0.2000 0.0357 0.2000 0.0400 0.0056 0.0005 0.0035 0.0017 0.0034 0.0024 0.9205 0.6603

(1,1,1,0,1,0,0,0,1,0) 0.0177 0.0047 0.0037 0.0015 0.0041 0.0031 0.0057 0.0022 0.0061 0.0045 0.8668 0.8710

(1,1,0,1,1,0,0,0,0,1) 0.0056 0.0051 0.0062 0.0031 0.0046 0.0025 0.005 0.0022 0.007 0.005 0.8120 0.8001

7. REFERENCES
[1] J. A. Aslam, E. Yilmaz, and V. Pavlu. The Maximum Entropy Method for Analyzing Retrieval Measures. In SIGIR, pages 27­34, ACM, 2005.
[2] A. Broder. A Taxonomy of Web Search. SIGIR Forum, 36(2):3­10, 2002.
[3] C. Buckley and E. M. Voorhees. Evaluating Evaluation Measure Stability. In SIGIR, pages 33­40, ACM, 2000.
[4] C. Buckley and E. M. Voorhees. Retrieval Evaluation with Incomplete Information. In SIGIR, pages 25­32. ACM, 2004.
[5] C. Buckley and E. M. Voorhees. Retrieval System Evaluation. In TREC. Experiment and Evaluation in Information Retrieval, pages 53­78. MIT Press, USA, 2005.
[6] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. Expected Reciprocal Rank for Graded Relevance. In CIKM, pages 621­630. ACM, 2009.
[7] B. Carterette. System Effectiveness, User Models, and User Utility: A Conceptual Framework for Investigation. In SIGIR, pages 903­912. ACM, 2011.
[8] F. Chierichetti, R. Kumar, and P. Raghavan. Optimizing Two-Dimensional Search Results Presentation. In WSDM, pages 257­266, ACM, 2011.
[9] A. Chuklin, P. Serdyukov, and M. de Rijke. Click Model-Based Information Retrieval Metrics. In SIGIR, pages 493­502, ACM, 2013.
[10] C. L. A. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A Comparative Analysis of Cascade Measures for Novelty and Diversity. In WSDM, pages 84­75, ACM 2011.
[11] C. W. Cleverdon. The Cranfield Tests on Index Languages Devices. In Readings in Information Retrieval, pages 47­60. Morgan Kaufmann Publisher, Inc., USA, 1997.
[12] K. Collins-Thompson and J. Callan. Query Expansion Using Random Walk Models. In CIKM, pages 704­711. ACM, 2005.
[13] C. Danilowicz and J. Balin´ski. Document ranking based upon Markov chains. IPM, 37(4):623--637, July 2001.
[14] D. K. Harman. Overview of the Third Text REtrieval Conference (TREC-3). In D. K. Harman, editor, Overview of the Third Text REtrieval Conference (TREC-3) , pages 1­19. NIST, Special Pubblication 500-225, Washington, USA., 1994.
[15] D. K. Harman. Information Retrieval Evaluation. Morgan & Claypool Publishers, USA, 2011.
[16] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated Gain-Based Evaluation of IR Techniques. ACM TOIS, 20(4):422­446, 2002.
[17] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately Interpreting Clickthrough Data as

Implicit Feedback. In SIGIR, pages 154­161, ACM, 2005. [18] M. G. Kendall. The Treatment of Ties in Ranking Problems. Biometrika, 33(3):239­251, 1945. [19] J. Lafferty and C. Zhai. Document Language Models, Query Models, and Risk Minimization for Information Retrieval. In SIGIR, pages 111­119, ACM, 2001. [20] K. T. Maxwell and W. B. Croft. Compact Query Term Selection Using Topically Related Text. In SIGIR, pages 583­592, ACM 2013. [21] A. Moffat, P. Thomas, and F. Scholer. Users Versus Models: What Observation Tells Us About Effectiveness Metrics. In CIKM, pages 659­668. ACM, 2013. [22] A. Moffat and J. Zobel. Rank-biased Precision for Measurement of Retrieval Effectiveness. ACM TOIS, 27(1):2:1­2:27, 2008. [23] J. R. Norris. Markov chains. Cambridge University Press, UK, 1998. [24] V. Pavlu, S. Rajput, P. B. Golbus, and J. A. Aslam. IR System Evaluation using Nugget-based Test Collections. In WSDM, pages 393­402, ACM, 2012. [25] S. Robertson. A New Interpretation of Average Precision. In SIGIR, pages 689­690. ACM, 2008. [26] T. Sakai. Ranking the NTCIR Systems Based on Multigrade Relevance. In AIRS 2004, pages 251­262. LNCS 3411, Springer, 2005. [27] T. Sakai. Alternatives to Bpref. In SIGIR, pages 71­78, ACM, 2007. [28] T. Sakai and Z. Dou. Summaries, Ranked Retrieval and Sessions: A Unified Framework for Information Access Evaluation. In SIGIR, pages 473­482, ACM, 2013. [29] P. Serdyukov, N. Craswell, and G. Dupret. WSCD2012: Workshop on Web Search Click Data 2012. In WSDM, pages 771­772. ACM, 2012. [30] M. D. Smucker and C. L. A. Clarke. Stochastic Simulation of Time-Biased Gain. In CIKM, pages 2040­2044. ACM, 2012. [31] M. D. Smucker and C. L. A. Clarke. Time-Based Calibration of Effectiveness Measures. In SIGIR, pages 95­104. ACM, 2012. [32] I. Soboroff. Dynamic Test Collections: Measuring Search Effectiveness on the Live Web. In SIGIR, pages 276­283. ACM, 2006. [33] E. Voorhees. Evaluation by Highly Relevant Documents. In SIGIR, pages 74­82, ACM, 2001. [34] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. IPM, 36(5):697­716, 2000. [35] E. Yilmaz and J. A. Aslam. Estimating Average Precision With Incomplete and Imperfect Judgments. In CIKM, pages 102­111. ACM, 2006.

606

Searching, Browsing, and Clicking in a Search Session: Changes in User Behavior by Task and Over Time
Jiepu Jiang1, Daqing He2, James Allan1
1 Center for Intelligent Information Retrieval, School of Computer Science, University of Massachusetts Amherst
2 School of Information Sciences, University of Pittsburgh
jpjiang@cs.umass.edu, dah44@pitt.edu, allan@cs.umass.edu

ABSTRACT
There are many existing studies of user behavior in simple tasks (e.g., navigational and informational search) within a short duration of 1­2 queries. However, we know relatively little about user behavior, especially browsing and clicking behavior, for longer search session solving complex search tasks. In this paper, we characterize and compare user behavior in relatively long search sessions (10 minutes; about 5 queries) for search tasks of four different types. The tasks differ in two dimensions: (1) the user is locating facts or is pursuing intellectual understanding of a topic; (2) the user has a specific task goal or has an ill-defined and undeveloped goal. We analyze how search behavior as well as browsing and clicking patterns change during a search session in these different tasks. Our results indicate that user behavior in the four types of tasks differ in various aspects, including search activeness, browsing style, clicking strategy, and query reformulation. As a search session progresses, we note that users shift their interests to focus less on the top results but more on results ranked at lower positions in browsing. We also found that results eventually become less and less attractive for the users. The reasons vary and include downgraded search performance of query, decreased novelty of search results, and decaying persistence of users in browsing. Our study highlights the lack of long session support in existing search engines and suggests different strategies of supporting longer sessions according to different task types.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process. H.1.2 [Models and Principles]: User/Machine Systems ­ human factors.
General Terms
Experimentation, Human Factors.
Keywords
Session; task; search behavior; browsing; clicking; eye-tracking.
1. INTRODUCTION
Although some simple search problems (e.g., finding a specific homepage and locating specific facts with known keywords) can be
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. Copyright © 2014 ACM 978-1-4503-2257-7/14/07...$15.00. http://dx.doi.org/10.1145/2600428.2609633

satisfied through a single query and one click, it usually takes multiple searches to solve more complex tasks. The reasons vary. Sometimes it is the user who adopts a divide and conquer strategy, using each query to deal with a part of the task's goal [1]. Also sometimes it may be the complexity of the solution that makes it difficult to find all the answers with one query. Moreover, the user usually does not start with a clear goal and needs to figure out a specific information need after many searches [30]. For whichever reason, a search process that solves a complex problem usually spans more than one query and includes rich user interaction.
Studies of users' search behavior provide guidance to system design and evaluation. With many studies of user behavior in simple search tasks (1­2 queries), we know relatively well how to tailor a system for these tasks. For example, after Joachims et al. [15] showed that users' visual attention and clicks are biased to the top ranked results, we know systems achieving high precision are more preferable in web search than systems with high recall.
In comparison, we know relatively little about user behavior in long sessions of complex task types, especially those that can provide guidance to the design and evaluation of systems supporting long session and complex tasks. For example, do users examine more result snippets and go to deeper ranks in complex tasks and long sessions? Are users looking for factual information more accurate in clicking given short result snippets? Do users become less persistent in viewing the search engine result page (SERP) after long durations of search? Without knowing answers to these questions, we do not know how to design and evaluate systems to better support complex tasks and long search sessions.
To address that gap, we conducted an experiment with users working on complex tasks for relatively long search session (10 minutes; about 5 queries) and recorded search behaviors including eye movement data. We study the following research questions:
RQ1: How do users' search behaviors, especially browsing and clicking behaviors, vary in complex tasks of different types?
Studying this question helps us understand the effects of tasks on personalization and suggests how to design systems supporting complex tasks. To the best of our knowledge, among web search user studies focusing on SERP browsing patterns, our experiments involve the most complex tasks and the longest sessions. Joachims et al.'s experiments [15, 23] dealt with only navigational and informational tasks, with on average 1.6 queries issued per session. Moffat et al. [24, 29] did not report session length, but according to Wu et al. [31] the most complex tasks adopted by Moffat et al. involve 2.42 queries and 3.46 clicks. Cole et al. [6, 7, 21] included tasks comparably complex to our study, but they focused on how users shift between scanning and reading.
We will show that there are very noticeable differences of behavior depending on the type of task driving the user's search. Differences are present in how active users are, how they browse and click result abstracts in a SERP, and how they issue queries.

607

RQ2: How do users' search behaviors change over time in a search session?
Our study is also the first study with analysis of changes in SERP browsing and clicking patterns over time in relatively long search sessions (10 minutes). Answers to RQ2 may provide insights on how to support users in long sessions. We will show that user engagement with a search system changes substantially between the start and the end of these search sessions. The changes appear to largely reflect a loss of confidence in the results, along with shifted patterns in browsing SERP results.
The rest of this paper introduces our experiments and findings.
2. RELATED WORK
Our study is related to three areas of existing work: web search user behavior with eye-tracking data; search task and its effects to user behavior; and search session. We review each area below.
Early studies (before 2004) of web search user behavior are mostly based on the analysis of large-scale query logs, such as [13, 28]. These studies described what real life web searches are like and how users interact with search engines at the query level, but they do not provide details of user behavior on a SERP, such as how users examine result abstracts. The first work using eyetracking for web search user behavior [11, 15, 23] discovered how users browse a SERP, examine abstracts, and click results. They found decayed visual attention on results as the rank of the result increases and biased clicks on the top ranked results. Lorigo et al. showed that task type and gender may result in differences in search behavior and browsing style [23]. Later studies with eye-tracking [3, 8, 10, 29] further confirmed that users behave diversely in different tasks. They also showed that users may react distinctly to different outlooks of search result abstracts [5, 8] and SERP elements other than result abstracts, such as ads and related searches [3, 10]. More recently [24] used eye tracking studies to verify models and hypotheses in IR evaluation metrics. However, due to the limited accessibility of devices, user behavior studies with eye-tracking data are limited.
Although currently lots of work using eye movement data for user behavior studies exist, the search tasks being studied in [3, 8, 10, 11, 15, 23] are simple, e.g., the "navigational" and "informational" tasks defined in [2]. As reported in [23], on average 1.6 queries were issued in that work. Recently Moffat et al. [24, 29] used more complex tasks of different cognitive complexity, i.e., "remember", "understand", and "analyze" defined in [31]. However, even the most complex task type ("analyze") only involves search sessions of 2.42 queries and 3.46 clicks on average [31]. To the best of our knowledge, among existing search behavior user studies with eye-tracking data, only [6, 7, 21] conducted experiments based on tasks of comparable complexity to the tasks adopted in our paper. However, they did not study how users read result abstracts in a SERP, but focused on how they shift between scanning and reading [6, 7, 21]. Therefore, it is unclear how users behave--especially how they browse the result abstracts in a SERP and click results--in long sessions of complex tasks.
When tasks are complex, it usually requires relatively longer search sessions to finish. Previous studies using web search logs [13, 28] discussed search sessions as multiple searches across certain duration of time in search logs. However, from this aspect, the multiple searches within a session are not necessarily related to a consistent topic or search task. Spink et al. [27] found that multitasking is very common in search sessions derived using this definition. In our study, a search session refers to consecutive searches that aim at solving a consistent task, which is similar to the search sessions studied in [6, 7, 20­22].

3. EXPERIMENTS
We conducted an experiment to collect user behavior data in search sessions for completing complex tasks. We collected users' queries, their browsing of Search Engine Result Pages (SERPs) and their clicks of search results. In addition, we deliberately asked users to perform different types of search tasks.
3.1 Search Tasks
The search tasks involved in previous related studies mainly focused on short search sessions and mostly dealt with navigational and simple informational needs. For example, in Joachims et al. [15] and Lorigo et al.'s studies [23], users on average only issued 1.6 queries in each task, and the tasks in Cutrell and Guan's studies [8, 12] were simplified so that both navigational and informational search tasks were considered to be successful once a best result page was found. Our study of search behaviors, particularly the examination of changes over time, needs to work on relatively longer search sessions, which allow a long exploration process and complex user interactions. We therefore adopted search tasks from the TREC 2012 session track [17], which were categorized into four types using Li and Belkin's faceted classification approach [19].
We considered two facets of search tasks identified by Li and Belkin [19]: product and goal. The product of a search task can be either factual (to locate facts) or intellectual (to enhance the user's understanding of a topic). The goal of a search task can be either specific (well-defined and fully developed) or amorphous (illdefined or unclear goals that may evolve along with the user's exploration). This yields four types of tasks: known item search (KI), known subject search (KS), interpretive search (IN), and exploratory search (EX). Some examples of tasks are:
Known Item (factual + specific): Where is Bollywood located? From what foreign city did Bollywood derive its name? What is the Bollywood equivalent of Beverly Hills? What is Bollywood's equivalent of the Oscars? Where does Bollywood rank in the world's film industries? Who are some of the Bollywood stars?
Known Subject (factual + amorphous): You think that one of your friends may have depression, and you want to search information about the depression symptoms and possible treatments.
Interpretive (intellectual + specific): You would like to buy a dehumidifier. You want to know what makes a dehumidifier good value for money.
Exploratory (intellectual + amorphous): You would like to buy a dehumidifier. On what basis should you compare different dehumidifiers?
Although these four types of tasks appear to be different from those tasks presented in previous works [8, 12, 15, 23] (navigational and information search tasks), we believe that the two classification schemes do not conflict with each other, but are defined at different levels. Broder defined navigational and informational search tasks [2] based on a classification of individual web search queries. Therefore, each task in this case is intrinsically only indicative of what people can finish within a single query. In comparison, Li and Belkin's classification scheme [19] is defined regarding the nature of people's information needs and problems, and allows multiple queries in a search session. Of course, each query in the session may still fit into Broder's scheme [2]. For example, in IN and EX tasks, users may issue a navigational query "amazon" to know about the different types of dehumidifiers sold on amazon.com.
3.2 System
We built an experimental search system providing modified Google search results. First, all ads and sponsors' links were removed. Second, we showed 9 results each page (rather than the usual 10) to make sure that users do not need to scroll down to see

608

all of the result items. This change made it much simpler to analyze eye-tracking data. However, previous studies [15] also reported that scrolling down affected browsing patterns on results shown below the screen cutoff of a search result page. This change will miss such effects. We adopted this change because Joachims et al. [15] showed that most of the users' attention is still focused on the top ranked results which are visible before scrolling down. Third, if Google provided query suggestions (i.e., "related searches") for a query (usually shown below the search results), we moved them to the right side of the search results, again, to eliminate scrolling pages.
The system looks very similar to existing search engines except a few places specifically designed for our search tasks. It shows the task descriptions at the top of the search result page. This is because we found in our pilot study that, without showing the task description, users might constantly switch between search result pages and another page showing the task description, because they forgot details of the task. We believe this would cause greater issues to the collected data (e.g., more constantly switching of pages) than showing task description on the search result page. In addition, the system has a highlighted "finish task" link if the session exceeds the time limit (but not before the limit is reached). Although many systems for user studies (e.g. Liu et al.'s systems [21]) allow users to bookmark relevant results at search time, we did not adopt such settings because it may affect users' browsing behaviors. Instead, relevance judgments were completed after search.
3.3 Eye-Tracking
A Tobbi 1750 eye-tracker was used to collect eye movement data. Among the various types of eye movement data, we only focus on analyzing fixation: stably gazing at an area of the screen. Studies have shown that fixation on an area of the screen usually indicates that users are reading information displayed on the area of interest (AOI) [26]. The AOIs in our study include each search result abstract (snippet), query suggestion, and task description. We assume that fixation on these AOIs indicate that the participant has examined the corresponding result abstract, query suggestion, and task description. ClearView, a software accompanying the eyetracker, was used to analyze fixations on the defined AOIs. We set the minimum duration of fixation to 100ms, a common value adopted in many previous studies of web search behaviors using the same series of eye-tracker [8, 12].
In the following discussion, we say that the participant examined a result abstract if we observed fixations on its corresponding AOI when the participant was browsing search result pages. Similarly, we say that the participant examined the query suggestions or topic description if we observed fixations on the corresponding AOIs.
3.4 Participants
We recruited 20 English native speakers (13 female and 7 male) through flyers in the campus of University of Pittsburgh. We required the participants to be English native speakers and current students in a college or university. Considering previous studies [9] reported increased error rates of eye-tracking for participants wearing glasses or lens, we also specified in our ads that the participants should have perfect eyesight (20/25). 13 participants were aged between 25­30, 6 between 18­24, and one over 30. For the highest degree earned or expected, 9 reported bachelor degree, 9 master, and 2 doctoral. Eight participants were studying information related majors, while others' majors ranged from anthropology to microbiology. The participants rated their expertise of using web search engines by a 5 point Likert scale and the mean score is 3.75 (5 means the highest proficiency).

3.5 Experiment Procedure

We randomly sampled five groups of search tasks developed by the TREC 2012 session track [17]. Each group has four unique tasks of different types. For each task group, four participants worked on the tasks. We rotated the sequence of tasks in each group for different participants. Table 1 shows the topics.
Table 1. Search task assignments.

Group
1 2 3 4 5

TREC Topic No. & Task Type
32 (KI), 40 (KS), 07 (IN), 05 (EX) 11 (KI), 22 (KS), 02 (IN), 29 (EX) 15 (KI), 03 (KS), 39 (IN), 10 (EX) 30 (KI), 33 (KS), 41 (IN), 37 (EX) 23 (KI), 04 (KS), 48 (IN), 46 (EX)

Participants
S01 ­ S04 S05 ­ S08 S09 ­ S12 S13 ­ S16 S17 ­ S20

The total experiment duration for a participant is about 2 hours. The participants were reimbursed by the rate of $15 per hour. At the beginning of the experiment, participants were introduced to the system and a training task (with all the three stages but shorter time limits). Then the participants worked on four formal tasks. After two formal tasks, they took a 10-minute break. We interviewed the participants for their search behaviors at the end of the experiment. For each task, the participants finished the following stages:
1. Search (10 minutes). In the search stage, the participants were introduced to the search task and were asked to use the experimental system to find information in order to solve the task. They were instructed to use the experiment system as if they were using public search engines such as Google and Bing--e.g. they could search using textual queries, browse search result pages, click and view results. However, they were specifically instructed not to use other search engines. We set a limit of 10 minutes for each task. After 10 minutes, the system showed a highlighted link notifying them to terminate the search stage. However, we also allowed participants to finish the task before 10 minutes if they reported they had already learned enough to solve the task.
2. Report (5 minutes). In the report stage, the participants were asked to rate the difficulty of the task, their familiarity with the topic of the task prior to search, and their search performance using a 5 point Likert scale. Then they were asked to write a paragraph reporting their outcomes of the search task. During this stage, the system showed a countdown of 5 minutes to help the participants to finish in about that time. The system did not freeze after 5 minutes. We instructed the participants to make full use of the time instead of finishing as soon as possible.
3. Relevance Judgments (5 minutes). In this stage, the participants were asked to judge and rate results regarding their relevance to the search task. Due to the time limit of the experiment, it was usually impossible to judge all returned results of all queries in a session. Therefore we generated a pool of results for relevance judgments. The priority of selecting results (from high to low) is: clicked results, probably examined results, other results.
The pool size was about 25 results. First, we included all the results that the user clicked on. If less than 25 results (say, Nc results) were clicked, we continued to consider some "probably examined" results. We assume the participants looked through each search result page from top to bottom. Therefore, we located the deepest position of the clicked results in each search result page and considered unclicked results ranked higher than the deepest position as "probably examined results". If there were no more than 25-Nc "probably examined results", we included all into the pool; or, we randomly sampled 25-Nc. If summing up all clicked results and probably examined results did not total 25, we further included a random sample of other results into the pool.
This pooling procedure is to maximize the number of judged results among those were clicked and examined throughout the session. The participants rated each result as "highly relevant",

609

"somewhat relevant", or "non-relevant". The system forced the participants to click each result at least once before submitting judgments. Again, the system showed a countdown of 5 minutes and the participants were instructed to make full use of the time. Later, an external annotator judged the rest of the results.
4. DATA
We collected user behaviors from 80 search sessions on 20 unique tasks. During a search session, the participants on average issued 4.9 queries, examined 16.1 unique result abstracts, and clicked 9.3 unique results. The average length of a query was 3.96 words (without removing stopwords). As with most search engines, if the participant clicks a result, the experiment system left the current search engine result page (SERP) and switched to a new tab of the browser showing the result webpage. The participants needed to switch between the SERP and result webpages. This resulted in multiple views for a SERP. We refer to the duration from showing a SERP to switching to other webpages as a "SERP view". In our experiment, participants had 3.6 views for a SERP on average.
For each session, the participant on average judged 20.1 results and left 13.3 unjudged. In total this resulted in 992 unique unjudged task-URL pairs. In order to evaluate search performance of sessions, we asked an annotator (not an author of this paper) to assess the relevance of the unjudged results. To evaluate the agreement between the annotator and the participants on relevance judgments, we also sampled 100 unique judged results for the annotator to assess. The annotator was not aware of which result has been judged by the participants. If we merge "highly relevant" and "somewhat relevant" into one class, the annotator agreed with the participants on 77% of the cases.
To evaluate the correctness of the fixation data, we calculated the percentage of clicked results with observed fixations. Intuitively, the user should have examined a result abstract before clicking it. Therefore, we should observe fixations on the clicked results if the data is accurate. In our experiment, the percentage is 87%, comparable to those reported by Joachims et al. [15].
5. SEARCH BEHAVIORS
Users interact with a search engine mainly in two ways. First, they proceed through the search process by issuing queries. Second, for each query, they examine result abstracts on the SERP and may click on results. Therefore, we first compare the "search activeness" of users in terms of how frequently they search and how often they examine and click results in Section 5.1. This helps us understand the diverse weight of the two types of interactions in different tasks. Then we look into details of SERP browsing in Section 5.2 and results clicking in Section 5.3. Finally, we compare users' querying reformulation behaviors in Section 5.4.
5.1 Search Activeness
Search activeness examines how active users are in terms of the frequency of query and result level interactions. Specifically, we compare: search frequency (# queries); the frequency of viewing SERPs (# SERP views); the number of examined result abstracts (# unique fixations) and clicked results (# unique clicks); total time of viewing a SERP or SERPs (% or # time view SERP). Table 2 shows results for a session and for an individual query (labeled with "/ q").
It should be noted that the length of a task session is not strictly 10 minutes. A session can be shorter if the user chooses to finish before the time limit is reached. It may also be longer than 10 minutes it the user does not realize the time is up. This is because the system only shows the notification on the search page, but the users may be reading result webpages when the time limit expires. As shown in Table 2 ("Total task time"), users spent about 10% longer in KS tasks, while the time of other tasks does not differ

greatly. This is probably due to the fact that fewer users chose to finish a KS task before 10 minutes ("# sessions end by user").
As shown in the table, users in different tasks can be active in diverse ways. For example, users in KI and EX sessions tend to search more frequently but interact less actively in each search, while KS and IN sessions involve fewer searches in total but more activities during each search. Data in Table 2 shows that users issued 5.5 and 6.2 queries in KI and EX sessions, which is significantly more often than those in KS (4.2 queries) and IN tasks (3.6 queries) within roughly the same amount of time. However, during each individual search, users in KS and IN sessions viewed the SERP more frequently (2.96 and 3.41 SERP views) and clicked more search results (2.32 and 2.58 unique clicked results), which are significantly more active than they did in KI and EX sessions (2.17 and 2.38 SERP views; 1.58 and 1.96 unique clicks). In addition, users in KI, KS and IN sessions spent longer total duration (12.4s­13.9s) viewing a SERP than they did in EX sessions (10.7s).
Table 2. Users search activeness in different types of tasks.

Statistics

KI KS IN EX

Total task time (s)

599 651 600 581 KI<KS*, KS>IN*, KS > EX**

# sessions end by user 4/20 1/20 2/20 3/20

# queries

5.5 4.2 3.6 6.2 KI>IN *, KS<EX +, IN<EX **

# SERP views

12.0 12.5 12.1 14.7

# unique fixations

16.6 13.6 10.7 18.6 KI>IN *, KS<EX +, IN<EX **

# unique clicks

8.3 9.5 8.2 10.7 KI<EX +, IN<EX +

% time view SERP

22.0 13.2 13.8 21.2 KI>KS **, KI>IN **, KS<EX **, IN<EX **

# SERP views / q

2.17 2.96 3.41 2.38 KI<KS *, KI<IN **, IN>EX **

# unique fixations / q 3.37 3.48 3.75 3.61

# unique clicks / q

1.58 2.32 2.58 1.96 KI<KS **, KI<IN **, IN>EX *

Time view SERP / q 13.1 12.4 13.9 10.7

+, *, **: difference is significant at 0.1, 0.05, and 0.01 level.

According to Table 2, EX sessions are the most active among

the four tasks in terms of the frequency of search and SERP views

and the number of examined/clicked results in a search session. In

comparison, users in KS and IN sessions are less active. They spent

significantly shorter time on SERP views (13.2% and 13.8% of the

session) than they did in KI and EX sessions (22.0% and 21.2%).

They also examined fewer abstracts during the session (13.6 and

10.7 unique fixations. KI sessions are less active than EX in that

users clicked fewer results, but KI sessions are more active than KS

and IN because of more examined result abstracts and longer

durations of viewing SERPs.

The diverse styles of search activeness suggest that we can

support a task according to the popularity of query and result level

interactions in the task (once we know what types of tasks users are

dealing with). For example, in KI and EX tasks, we may support

search sessions by assisting with query reformulation (because they

search more often in a session). In comparison, with less query level

interaction and more result level actions, KS and IN tasks should

be supported by focusing on enhancing result level interaction. For

example, search results for KS and IN tasks can be optimized for

precision at lower ranks or of a whole page.

5.2 SERP Browsing

This section compares different tasks based on users' browsing styles. Results are compared from four aspects: the effort of a SERP view, the chances of examining results at different ranks, the sequence of examining result abstracts on a SERP (scan path), and the users' attentions on visited results.

5.2.1 Effort of SERP Browsing
We found that users in KI and EX sessions spent greater effort on examining result abstracts in a SERP view. As shown in Table 3, users examined significantly more unique abstracts in KI and EX tasks (2.48 and 2.59 unique fixations) than they did in KS and IN tasks (2.16 and 1.93). We further aggregated the durations of all the fixations on result abstracts in a SERP view ("Fix time on results").

610

Table 4. Statistics for users' scan path in a SERP view.

Statistics

KI KS IN EX

P(moving up) P(sequential) Breadth Gap

0.31 0.28 3.09 1.38

0.29
0.41 3.44 1.47

0.45 0.07 3.17 1.54

0.37 0.21
4.02 1.63

KI<IN **, KS<IN ** KI>IN **, KS>IN **, KS>EX *, IN<EX * KI<EX *

*, **: difference is significant at 0.05 and 0.01 level.

It shows that in a SERP view, users in KI and EX tasks also used

longer durations in total on examining result abstracts (1.94s and

1.91s) comparing to those in KS and IN tasks (1.35s and 1.46s). In

addition, users in KI and EX tasks examined each single result

abstract for longer periods than they did in KS tasks ("Fix time on

a result").

In comparison, users in KS and IN sessions spent more time on

reading result webpages. Between each SERP view, users can read

and explore result webpages. Although users may follow links on

the result webpage and visit new webpages, the time interval of two

SERP views to some degree tells the amount of time the user spent

on each result webpage. As shown in Table 3 ("SERP view

interval"), it took a significantly longer time for users in KS and IN

tasks to switch back from result webpages to SERP, probably

indicating that they spent more time reading each result webpage.

Table 2 also supports our conjecture. The percent of time users

spent on viewing SERPs ("% time view SERP") is significantly less

in KS and IN tasks.

Table 3. Browsing behavior statistics for a SERP view.

Statistics

KI KS IN EX

# unique fixations

2.48 2.16 1.93 2.59 KI>KS *, KI>IN **, KS<EX **, IN<EX **

Fix time on results 1.94 1.35 1.46 1.91 KI>KS **, KI>IN **, KS<EX **, IN<EX **

Fix time on a result 0.75 0.60 0.70 0.72 KI>KS **, KS<IN *, KS<EX **

Length a SERP view 6.04 4.17 4.07 4.50 KI>KS **, KI>IN **, KI>EX **

SERP view interval 28.6 38.5 36.1 21.9 KI<KS **, KI<IN *, KI>EX *, KS>EX **, IN>EX **

Avg examined rank 3.18 3.78 3.95 3.96 KI<KS **, KI<IN **, KI<EX **

Max examined rank 4.35 5.06 5.23 5.36 KI<KS **, KI<IN **, KI<EX **

% fixations on visited 21.6 22.4 22.7 21.7

*, **: difference is significant at 0.05 and 0.01 level.

As users spent more effort examining result abstracts in KI and

EX tasks, systems supporting these tasks should consider how to

generate more informative result abstracts. In the same way that

Cutrell et al. found that different lengths of result abstracts can

affect performance of navigational and informational search

differently [8], KI and EX tasks may also benefit from customized

styles of result abstracts specifically trimmed for the tasks. In

comparison, KS and IN tasks may benefit from various reading

supports for the result webpages (e.g., highlighting query terms in

a result webpage when users are redirected from a SERP).

5.2.2 Attention on Results at Different Ranks
Previous studies [15] showed that users focus more on top ranked results when examining a SERP. Does such tendency exist in a search session and is it different in the four types of tasks? Figure 1 shows the chances of examining results (fixation rates) at different ranks (we refer to the result at rank n as Rn). The left figure counts the fixation rates only for the first view of a SERP, while the right one aggregates all SERP views in a session. Both figures show that users still examined more on results at higher ranks in a search session, but vary slightly in patterns.
The results show that during the first view of a SERP, users in EX tasks were more willing to examine results at the bottom of a page comparing to other tasks. As shown in Figure 1 (left), users in EX sessions had 10%­20% chance of examining R7­R9, while this happened in less than 10% of the cases in other three tasks. However, when considering all SERP views of a query, KS, IN, and EX sessions have comparable fixation rates on lower ranked results (about 40%­50%), but KI sessions still showed observably lower tendency to examine results at the bottom (about 35%). This may be caused by the fact that users viewed a SERP more times in

0.9

% fixation (counting 1st view)

0.9

% fixation (counting all views)

0.8

% fixation (KI)

% fixation (KS) 0.8

% fixation (KI)

% fixation (KS)

0.7

% fixation (IN)

% fixation (EX) 0.7

% fixation (IN)

% fixation (EX)

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0.0

0.0

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

Figure 1. Fixation rates on results at each rank (R1­R9),

counting the first view (left) or all views (right) of each SERP.

KS and IN sessions than they did in KI sessions (see Table 2). As found by Lorigo et al. [23], when viewing a SERP multiple times, users shift attention to focus more on results at the bottom. Therefore, if counting all SERP views, it is not surprising that users in KS and IN sessions may increase their fixation rates on R7­R9 after viewing a SERP multiple times.
Results in Figure 1 shows that, unlike simple search that may be satisfied with one click (and therefore one SERP view if the click is accurate), in complex search tasks, results at lower rank positions of a SERP can still get substantial visual attention (about 30%­50% fixation rate) after multiple SERP views. This suggests that it is unnecessary to rigorously optimize results for precision at the very top positions in long sessions of complex tasks. In addition, results suggest that tasks do affect fixation rates, and therefore systems can be tailored for different browsing styles.

5.2.3 Scan Path

Solely looking into fixation rates is often not indicative of how users consecutively examine result abstracts in a SERP view. Therefore, we study the users' "scan paths" in a SERP view. As Lorigo et al. did in their studies [23], we aggregate the examined results in a SERP view as a "scan path". If the users examined the same result abstract repeatedly, we count the result only once in the scan path. For example, for an observed sequence "R1 R3 R3 R1 R4 R4", its corresponding scan path is "R1 R3 R1 R4".
We refer to two adjacent examined abstracts in a scan path as a move. For example, R1­R3, R3­R1, and R1­R4 are three moves in "R1 R3 R1 R4". The distance of the two results in a move is referred to as a "gap", and we define the gap of a scan path as the average gap of all its moves (e.g., "R1 R3 R1 R4" has gap 2, 2, 3 and its average gap is 2.33). The gap of a scan path can indicate how many results are skipped in browsing. A scan path with gap 1 means that each move is going to an adjacent result. We define the "breadth" of a scan path as the maximum gap of two examined result abstracts in the scan path (e.g., the breadth of "R1 R3 R1 R4" is 3, the distance of R1 and R4). The breadth of a scan path can indicate the magnitude of the area being browsed in a SERP. If the users examine results from top to bottom sequentially, each move in the scan path would be "moving down" (to the results at lower ranks). If all the moves in a scan path are "moving down", we say that the scan path is "sequential" (from top to bottom). We estimate the chances of "moving up" and the chances of a scan path being sequential. Table 4 shows the results.
Table 4 suggests that instead of scanning the whole page, users focus on an area of 3­4 results in a SERP view ("breadth") and usually skip results in browsing ("gap" ranges from 1.38 to 1.63). Although Table 3 shows a comparable amount of fixations in KI and EX tasks, Table 4 explains the difference between KI and EX. In EX sessions, users' scan paths have wider breadths (4.02) than those in KI sessions (3.09). This indicates that users in EX tasks browsed larger areas and skipped more results in a SERP view, while users focused on smaller areas in KI sessions.
As shown in Table 4, in all tasks the chance of moving up is lower than 50%, showing that users tend to scan results in a SERP

611

from top to bottom in general. However, the chances of going up is significantly lower in KI and KS tasks (about 30%) compared to those in IN tasks (45%). Note that 45% chance of moving up means that users in IN sessions are almost randomly moving toward either the top or the bottom. Thus it is not surprising that only 7% of the scan paths in IN tasks are sequential.
Table 4 also show strong dimensional characteristics. Tasks looking for factual products (KI and KS) show significantly stronger tendencies of sequential browsing (p<0.01). Tasks with amorphous goals (KS and EX) have significantly greater gap and breadth in a SERP view (p<0.01). This indicates that in both dimensions, more complex tasks (e.g. informational product and amorphous goal) lead to more complex browsing behaviors ­ e.g. non-linear browsing and scanning larger areas.

5.3 Results Clicking

We further compare the four types of tasks based on the users' clicking behaviors. Whether or not a result is clicked depends on two factors. First, whether the user examined the result abstract or not (though possible, it is very unlikely that users blindly open a result without examining it). The previous section examined that factor. This section focuses on the second one: after examining a result, what is the chance a user clicks on it? The results show that users do not click every result abstract they examined. The chance of clicking varies by task, by relevance of results, and by whether the result has been visited previously.

5.3.1 Chances of Clicking Examined Results
We calculate the probability of clicking a result provided that we observe a fixation on the result abstract during a SERP view. Table 5 shows the results ­ "P(click | examine)". It shows that users are significantly more likely to click a result after examining it in KS and IN sessions (61% and 59%), whereas the chances are lower in KI and EX sessions (45% and 52%). This is not surprising considering the fact that users in KI and EX tasks also retrieved fewer relevant results. As shown in Table 6 (analyzed in greater detail in Section 5.4), P@10 and nDCG@10 in KI and EX sessions are significantly lower than those in KS and IN sessions. With fewer relevant results retrieved, the examined results are less likely to be relevant and therefore less likely being viewed as promising and so worth clicking by the users.
We also noticed that users do not always click results during a SERP view. Sometimes they switch from a result webpage to the SERP and then switch back to the result webpage again, probably because they did not find any interesting results after examining the SERP. The chance of viewing a SERP without clicking result ("% SERP views w/o clicks") is lower in EX sessions. Users also clicked significantly more results in EX tasks during a SERP view (0.87 unique clicks) comparing to other tasks (0.77­0.80).

5.3.2 Relevance of Results and Clicking
To evaluate how relevance of results affects a user's decision to click in the four types of tasks, we further calculate the chance of clicking an examined result abstract when the result is judged as relevant (either "highly relevant" or "somewhat relevant"). As shown in Table 5, P(click | examine, relevant), the chance of
Table 5. Clicking behavior statistics in a search session.

Statistics

KI KS IN EX

# unique clicks / SERP view 0.77 0.80 0.79 0.87 KI<EX **, KS<EX *, IN<EX **

% SERP views w/o clicks 17.0 21.0 19.0 13.5 KS>EX *

P(click | examine) P(click | examine, relevant) P(click | examine, visited)

0.45 0.56 0.36

0.61
0.70 0.33

0.59 0.65 0.39

0.52 0.68 0.44

KI<KS **, KI<IN ** KI<KS **, KI<IN **, KI<EX **

% clicks relevant

87.7 87.2 79.6 73.2 KI>IN *, KI>EX **, KS>IN *, KS>EX **

% clicks visited

9.2 2.7 11.1 14.6 KI>KS **, KI<EX *, KS<IN **, KS<EX **

Avg clicked rank

2.94 3.51 3.72 3.46 KI<KS *, KI<IN **, KI<EX *

Deepest clicked rank

4.15 5.41 5.58 4.78 KI<KS **, KI<IN **, IN>EX *

*, **: difference is significant at 0.05 and 0.01 level.

clicking increases by 6%­16% if the examined result abstract is relevant. When a relevant result abstract has been examined, users in KS, IN, and EX sessions have comparable chances of clicking the result (65%­70%). However, users in KI sessions still have significantly lower chances of clicking (56%). This may indicate that users intrinsically click more selectively in KI tasks.
Unsurprisingly, users cannot perfectly predict whether a result is useful or not purely based on the abstract returned by a search engine. As shown in Table 5 ("% click relevant"), the percentage of relevant results among all clicked results varies from task to task: over 87% clicked results in KI and KS tasks are relevant, which is a significantly higher percentage than those in IN and EX tasks. This may also indicate that it is easier for users to judge the usefulness of documents if they are searching with a factual goal.
The lower click accuracy in tasks looking for intellectual products (IN and EX tasks) indicates that the result abstracts provided in current search engines are probably optimized for factual search only, which is difficult to satisfy users searching for other types of information. Users may substantially benefit from systems providing customized result abstracts for different tasks.
5.3.3 Clicks on Previously Visited Results
Similar to the default settings of web search, we show visited and unvisited URLs in different colors (purple and blue) in the experimental search system. Therefore, the users could quickly distinguish visited URLs from unvisited ones by color. We found that about 20% of the total fixations were on previously visited result abstracts (Table 3 "% fixations on visited") and there were 30%­40% chances that users will revisit a clicked result (Table 5 "P(click | examine, visited)"). This suggests that users still paid certain attention to the visited result abstract in SERP browsing rather than completely ignoring them, indicating that users may still expect to use the visited results when necessary.
However, results show that the chance of clicking an examined result is indeed lower than normal if the result has been previously visited by the users within the same session. In all types of tasks, the probability of clicking an examined result reduces if the result is previously visited by the users (comparing "P(click|examine)" and "P(click|examine,visited)"). However, the changes are more significant in the KS and IN tasks (decreased by 28% and 20%) compared to KI and EX tasks (by 9% and 8%). This suggests that whether the result has been visited or not has greater effects on users' clicking decisions in KS and IN tasks. Among four types of tasks, users in EX sessions are slightly more willing to re-open visited results (44%) comparing to other tasks (33%­39%), probably due to the complex nature of exploratory search tasks.
This section provides suggestions on how to deal with previously visited results in a search session. It seems risky to completely remove them because there are substantial needs to reopen previously visited results. However, the reduced chance of clicking suggests we may demote the ranking of the visited results in a new SERP. Besides, the results also show that we can customize systems for different tasks ­ e.g., for EX search, we may demote the rank of previously visited results less.
5.3.4 Clicks on Results at Different Ranks
Finally, we show click rates of results at rank R1­R9 in Figure 2. As before, we separately examine the click rates in the first view of each SERP and those in all SERP views. The click rate decays with the increase of result rank, but more quickly than the drop of fixation rate on result ranks shown in Figure 1.
As shown in Figure 2 (left), among the four tasks, we found that users in KS tasks have observably higher chance (about 10% more that on other tasks) of clicking the top one result but apparently lower chance of clicking the second top result in the first SERP

612

0.5

% click (counting 1st view)

0.5

% click (counting all views)

% click (KI)

% click (KS)

% click (KI)

% click (KS)

0.4

% click (IN)

% click (EX) 0.4

% click (IN)

% click (EX)

0.3

0.3

0.2

0.2

0.1

0.1

0.0

0.0

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

Figure 2. Click rates on results at each rank (R1 ­ R9),

counting the first view (left) or all views (right) of each query.

view of a query. The reason is unclear, but this results in overall higher rates of clicking R1 in KS sessions compared with other tasks (as shown in the right figure). Similar to Figure 1, we also found that the chances of clicking results at lower ranks are significantly lower in KI tasks (counting all SERP views). In fact, users in KI tasks have the lowest chance to click R3­R9 among the four types of tasks, showing that in KI tasks users mainly focus their attention on the top ranked few results. The average and deepest rank of the clicked results in Table 7 also support this finding. This again suggests that we may tailor search systems by the types of tasks, e.g., generate best top few results in KI tasks.
5.4 Query Reformulation
Finally, we compare the four types of tasks by the way users issue and reformulate search queries, which indicates how users proceed through their search session.
5.4.1 Characteristics of Queries
Table 6 shows statistics of user queries and user behaviors for query reformulation. We notice that in different tasks, user queries vary in length, search effectiveness, and novelty.
As shown in the table ("# terms"), users issued significantly shorter queries (3.54 and 3.55 words) in tasks looking for factual information (i.e., KI and KS) comparing to those with intellectual search goals such as IN and EX tasks (4.39 and 4.38 words). The queries of the four types of tasks also vary in terms of effectiveness of retrieving relevant information. We calculate P@10, nDCG@10, and Reciprocal-rank for queries of different tasks and report the mean values in Table 7: users issued queries with better search effectiveness in KS and IN tasks.
However, it should be noted that the effectiveness of queries in IN tasks may be over-estimated. We further analyze the number of common results in multiple queries of the same session. For each query except the initial one of a session, we calculate the number of results retrieved by both this query and the previous query ("# overlap results") and Jaccard similarity between this query and its previous query's first page of results ("Jaccard similarity"). We can see that queries in IN sessions have significantly greater overlap of results (2.61 in common and 0.23 Jaccard similarity) than other tasks (0.81­1.33 results in common and 0.07­0.1 Jaccard similarity). Therefore, it is unclear whether queries in IN tasks are

Table 6. Average user behavior statistics for a search query.

Statistics

KI KS IN EX

# terms

3.54

# overlap results

1.15

Jaccard similarity

0.09

P@10

0.34

nDCG@10

0.33

Recip-rank

0.70

Fix time results (transit) 1.06

Fix time results (normal) 1.94

Fix time task (transit) 1.67

Fix time task (normal) 0.75

# use qsug / session

0.40

3.55 0.81 0.07 0.45 0.42 0.76 0.93 1.35 1.52 0.39 0.45

4.39 2.61 0.23 0.47 0.48 0.79
1.83 1.46 0.87 0.29 0.50

4.38 1.33 0.10 0.28 0.33 0.68 1.39 1.91 0.82 0.11
0.60

KI<IN **, KI<EX **, KS<IN **, KS<EX ** KI<IN **, KS<IN **, KS<EX *, IN>EX ** KI<IN **, KS<IN **, IN>EX ** KI<KS **, KI<IN **, KI>EX *, KS>EX **, IN>EX ** KI<KS *, KI<IN **, KS>EX **, IN>EX ** KI<IN *, IN>EX * KI<IN *, KS<IN * KI>KS **, KI>IN **, KS<EX **, IN<EX ** KI>IN *, KI>EX **, KS>IN *, KS>EX * KI>KS **, KI>IN **, KI>EX **, KS>EX **, IN>EX *

Fix time qsug (transit) 0.33 0.75 0.62 0.88 KI<KS *, KI<EX * Fix time qsug (normal) 0.03 0.06 0.05 0.06

*, **: difference is significant at 0.05 and 0.01 level.

truly more effective because users may not be interested in some of the previously visited relevant results.
5.4.2 Source of Knowledge for Query Reformulation
Where do users acquire the knowledge for formulating new queries? To study this question, we look into user attention within the SERP views where users reformulated a search query (referred to as "transit SERP views"). We assume that if a user's attention on an area of the transit SERP view is apparently higher than those of a normal SERP view, they probably acquired knowledge from that area for query reformulation.
In Table 6, we label statistics in a transit SERP view by "(transit)" and those in a normal SERP view by "(normal)". For all the tasks, we observed increased attention of users on the task description and query suggestion in a transit SERP view. Additionally, users spent substantial time examining result abstracts in a transit SERP view. This indicates that task information, query suggestion, and result abstracts are possible sources of knowledge for users' query reformulation. Note that users do not necessarily need to adopt a query suggestion to be helped by one: they can get useful terms from the suggested queries (as suggested by Kelly et al. [18]). Results show that users examined diverse areas of the transit SERP in different tasks, indicating distinct source of knowledge for reformulation in different tasks.
We found that in tasks with factual goals (KI and KS), users rely mostly on task information itself for reformulating queries. As shown in Table 7, users in KI and KS tasks spent 1.67s and 1.52s in total examining task description in a transit SERP ("Fix time task info (transit)"), while in a normal SERP they spent only 0.75s and 0.39s. In addition, we noticed that users spent twice as much of the time on task description in tasks with factual goals compared with those with intellectual goals during a transit SERP view (0.87s and 0.82s). Also, in KI and KS tasks, the attention users put on task descriptions surpasses that on other areas of the transit SERP, such as the result abstracts (1.06s) and query suggestions (0.33s). All these results indicate that users in KI and KS sessions mainly focus on the task itself in query reformulation.
In comparison, we noticed that users in IN sessions probably reformulated queries mostly based on what they learned from the result abstracts. In KI, KS, and IN sessions, the total fixation duration on the result abstracts is shorter in a transit SERP view than those in a normal SERP view. However, in IN sessions, there is increased attention on result abstracts when reformulating queries (from 1.46s to 1.83s). Further, the amount of time users spent on examining result abstracts (1.83s) is also longer than they spent on task description (0.87s) and query suggestions (0.62s) in a transit SERP view.
Users in EX tasks are distinguished by the longest fixation duration on query suggestions in a transit SERP view (0.88s) among the four types of tasks. We also found increased attention of EX task users on task description during a transit SERP view (0.82s) compared with those in a normal SERP view (0.11s), indicating that task information may still constitute an important source of knowledge in EX tasks for query reformulation.
5.4.3 Use of Query Suggestion
Throughout the whole session, we found that users have limited direct use of query suggestion (i.e., adopting a query suggestion for search). As shown in Table 7, the number of times a query suggestion was used for search ranges from 0.4 to 0.6 in a session. Although we observed relatively higher frequencies of using query suggestion in exploratory search tasks, the differences are not significant and the usage frequency is still low (0.6). This may indicate the limited support of query suggestion for long search sessions in existing search engines.

613

To conclude, results suggest distinct strategies of supporting query reformulation in different tasks. For example, as users focus a lot on result abstracts in IN tasks, it may be preferable to generate suggestions based on frequent terms in result abstracts.

6. CHANGE OF BEHAVIOR IN A SESSION

How do search behaviors of users change in a search session? To answer this question, we compare users' search behavior in the initial query of a session with that in subsequent query reformulations. It should be noted that in our experiment we set a 10-minute limit for task completion. Therefore, the last query of each session was usually interrupted, and the behavior statistics may be inaccurate (e.g., with less SERP views, examined results and clicks). We did not consider this issue in the previous section because it does not introduce bias when we compare the differences between tasks. However, when comparing different queries in a search session, the last query of a session should be removed. Therefore, in this section, we selected 48 sessions with at least 3 queries and compare behaviors in the initial query with those in subsequent query reformulations excluding the last query of the session. The 48 sessions include 13 KI sessions, 9 KS sessions, 11 IN sessions and 15 EX sessions. Due to the limited sample size, we report significance at 0.1 level when necessary.

6.1 Decreased Interests on Search Results

We noticed that as a search session progresses, search results apparently attract less of the user's interest. As shown in Table 7, the number of unique clicks per query ("# uniq click / q") dropped significantly by 40%­60% in all tasks. The number of unique fixations per query ("# uniq fix / q") also decreased significantly, though by a smaller magnitude (about 20%­30%), except in IN sessions. Also, the number of SERP views per query ("# SERP views / q") reduced significantly by about 1­2 views per query. These all indicate that users became less and less interested in the results after a few searches.
We hypothesize three possible reasons for the reduced interests of users in a search session: 1) less relevant results are retrieved in subsequent query reformulations comparing to the initial query; 2) although as many as relevant results are returned, users lose their interests to the results because they are either highly overlap with results of previous queries or include very similar information; 3) users are becoming less persistent in SERP browsing.
We verify the first reason by comparing search effectiveness of query reformulations with those of the initial query in a session. We found that downgraded search performance may be one of the major reasons for KI and EX tasks resulting in decreased interests of users on results. Table 7 shows that the search performance of queries in KI and EX sessions are indeed decreasing, but there is no significant change of search effectiveness in KS and IN tasks. Both Reciprocal-rank and nDCG@10 declined significantly in KI and EX sessions. Due to the reduced search performance, it is not

Table 7. Changes of search behaviors in query reformulations (excluding the last query) compared with the initial query.

Statistics

KI

KS

IN

EX

# SERP views / q

2.92 1.70  3.78 1.89  3.45 2.80 3.93 1.80 

# uniq fix / q

3.69 3.05  3.56 2.88  2.64 3.35 4.47 3.17 

# uniq click /q

1.92 1.31  3.44 1.43  3.00 1.83  3.20 1.54 

P(click | examine) 0.52 0.47 0.67 0.46  0.74 0.46  0.58 0.47

P(click | examine, rel) 0.64 0.64 0.95 0.62  0.77 0.57  0.70 0.65

Avg examine rank 2.66 3.19  4.20 3.75 3.66 3.79 4.05 3.74

Deepest examine rank 3.57 4.39  5.04 5.01 4.95 5.11 5.21 5.25

Avg click rank

3.34 3.43 4.50 3.88 4.09 3.95 4.40 4.04

Deepest click rank 3.43 3.43 4.50 3.88 4.09 3.96 4.45 4.05

Time view a SERP 14.9 9.7  12.7 7.9  8.7 11.4 16.6 8.7 

Time a SERP view 4.95 5.82 3.35 4.15 2.51 4.51  4.23 4.90

Recip-rank

0.92 0.63  0.69 0.73 0.69 0.79 0.81 0.64 

nDCG@10

0.46 0.26  0.40 0.32 0.46 0.41 0.37 0.29 

/, /, /: difference is significant at 0.1, 0.05, and 0.01 level.

surprising that users may quickly feel that search results are not worth clicking and it is unnecessary to continue browsing a SERP after just one or two SERP views and clicks.
Further, we examine the validity of the second reason by the chances of clicking an examined result ("P(click | examine)") and an examined relevant result ("P(click | examine, rel)"). As shown in Table 7, as the session progresses, users in KS and IN sessions are less likely to click an examined result no matter whether it is relevant or not. For KS and IN sessions, we also did not find significant changes of queries' search performance in a session. Therefore, this indicates that it is probably the users themselves who believed that the search results, even the relevant results, are becoming less useful and worth clicking in a search session. One reasonable explanation could be that either the results are exactly previously retrieved ones, or similar information of the results appeared in previous results and users already knew relatively enough about it. Therefore, we conclude that declined novelty of search results may be one of the reasons resulting in decayed interest of users on search results in KS and IN tasks.
Finally, we examine whether users become less persistent in SERP browsing in a search session. Results indicate that users' persistence of browsing probably decreased in the tasks with unclear goals (KS and EX), but no evidence supports that users become less persistent in tasks with specific goals (KI and IN). As shown in Table 7, we found that the examined results in KS and EX tasks moved to higher ranked positions. The average rank of the examined results ("Avg examine rank") decreased from 4.20 to 3.75 in KS tasks and from 4.05 to 3.74 in EX tasks. Figure 5 also shows that, in KS and EX tasks, the chance of examining results decreased on every rank position without any exception. These all indicates that users in KS and EX tasks become less persistent and are more likely to stop browsing a SERP earlier than they did at the beginning of a search session. In comparison, the examined results in KI and IN tasks moved to lower ranked positions (the difference is significant in KI tasks). Also, Figure 5 shows that there are increased fixation rates on the results at lower ranked positions in the SERP. None of the evidences support decreased persistence of users in KI and IN sessions.
The decreased interests of users on search results indicate that users encountered difficulties as the search session progresses, but existing search systems did not provide supports for long sessions. It also partly confirms a hypothesis in search session performance evaluation that more weights should be put on the relevant results found at the early stage of a session [14]. Our studies of the three reasons also suggest different ways of supporting search sessions. For KI and EX sessions, the strategy is straightforward, i.e., it may help simply by improving search performance of queries. For KS and IN sessions, however, it requires systems that can retrieve novel search results without downgraded performance. For tasks with unclear search goals (KS and EX), we can optimize results for precision at higher ranked positions because users are less persistent to read lower ranked results of a SERP.
6.2 Changes of Browsing and Clicking
Figure 3­6 shows changes of fixation and click rates in the four types of tasks, counting the first view or all SERP views. Results show different changes of browsing patterns in the four tasks.
Figure 3 shows the changes of fixation rates in initial query and query reformulations, counting only the first view of each SERP. We noticed that throughout a search session, users shifted their attentions to focus less on the top 1 or 2 results but more on lower ranked results such as R3­R5. For example, in KI sessions, the chances of examining R4 and R5 increased, with less fixations on R1 to R3. Similarly, users moved their attentions from R1­R2 to

614

1.0 0.9

% fixation in KI tasks (first view)

1.0 0.9

% fixation in KS tasks (first view)

1.0 0.9

% fixation in IN tasks (first view)

1.0 0.9

% fixation in EX tasks (first view)

0.8

% fixation in initial query (KI) 0.8

% fixation in initial query (KS)

0.8

% fixation in initial query (IN) 0.8

% fixation in initial query (EX)

0.7

% fixation in reformulations (KI) 0.7

% fixation in reformulations (KS) 0.7

% fixation in reformulations (IN) 0.7

% fixation in reformulations (EX)

0.6

0.6

0.6

0.6

0.5

0.5

0.5

0.5

0.4

0.4

0.4

0.4

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.0

0.0

0.0

0.0

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

Figure 3. Changes of fixation rates in different tasks (initial query vs. query reformulations, counting 1st view of each SERP).

0.7

% click in KI tasks (first view)

0.7

% click in KS tasks (first view)

0.7

% click in IN tasks (first view)

0.7

% click in EX tasks (first view)

0.6

% click in initial query (KI)

0.6

% click in initial query (KS)

0.6

% click in initial query (IN)

0.6

% click in initial query (EX)

0.5

% click in reformulations (KI) 0.5

% click in reformulations (KS) 0.5

% click in reformulations (IN) 0.5

% click in reformulations (EX)

0.4

0.4

0.4

0.4

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.0

0.0

0.0

0.0

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

Figure 4. Changes of click rates in different tasks (initial query vs. query reformulations, counting 1st view of each SERP).

1.0 0.9

% fixation in KI tasks (all views)

1.0 0.9

% fixation in KS tasks (all views)

1.0 0.9

% fixation in IN tasks (all views)

1.0 0.9

% fixation in EX tasks (all views)
% fixation in initial query (EX)

0.8

% fixation in initial query (KI) 0.8

% fixation in initial query (KS) 0.8

% fixation in initial query (IN) 0.8

% fixation in reformulations (EX)

0.7

% fixation in reformulations (KI) 0.7

% fixation in reformulations (KS) 0.7

% fixation in reformulations (IN) 0.7

0.6

0.6

0.6

0.6

0.5

0.5

0.5

0.5

0.4

0.4

0.4

0.4

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.0

0.0

0.0

0.0

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

Figure 5. Changes of fixation rates in different tasks (initial query vs. query reformulations, counting all views of each SERP).

0.7

% click in KI tasks (all views)

0.7

0.6

% click in initial query (KI)

0.6

0.5

% click in reformulations (KI) 0.5

% click in KS tasks (all views) 0.7

% click in initial query (KS)

0.6

% click in reformulations (KS) 0.5

% click in IN tasks (all views)

0.7

% click in initial query (IN)

0.6

% click in reformulations (IN) 0.5

% click in EX tasks (all views)
% click in initial query (EX) % click in reformulations (EX)

0.4

0.4

0.4

0.4

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.0

0.0

0.0

0.0

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

R1 R2 R3 R4 R5 R6 R7 R8 R9

Figure 6. Changes of click rates in different tasks (initial query vs. query reformulations, counting all views of each SERP).

R3­R5 in KS tasks, from R1 to R2­R3 in IN sessions, and from R1­R2 to R3­R4 in EX sessions. However, users still mainly focused on the top results than others.
Figure 5 further shows the changes of fixation rates counting all SERP views of a query. As we discussed in the last section, the chance of examining a result decreased at every position in KS and EX tasks, but users moved their attentions from the top half of the SERP to the bottom results in KI and IN tasks. In addition, we note that there are some overall changes of browsing patterns in Figure 5. For KI and IN sessions, the slope of decreasing fixation rate by result rank is steep at the initial query of a session but less apparent in further query reformulations (this is due to decreased fixations on the top ranked results and increased attentions on the results at the bottom). In contrast, in KS and EX sessions, users' attentions are increasingly biased to the top ranked results. This is probably related to whether the goal is specific or amorphous.
As shown in Figure 4 and 6, the chances of clicking dropped significantly in almost all positions in four types of tasks, supporting our findings in the previous section. Though the chances of examining the top one result, as shown in Figure 3 and 5, did not drop by a large magnitude, the chances of clicking the top one result in query reformulations decreased to only about 2/3 to 1/2 of chances in the initial query.
The changing of browsing and clicking patterns indicate that, even during the session of the same tasks, we should customize the systems to support users at different time point of the search session. For example, due to the shifted pattern of fixation, in KI and IN sessions, systems may need to optimize search results for precision at very top positions at the beginning of a search session but shift to consider more on results at lower ranked positions after a few searches.

7. CONCLUSION
In this paper, we studied users' search behavior in long sessions of four different types of complex tasks. We found that search behavior varies distinctly by task and changes significantly after time. Table 8 summarizes our findings by four tasks and two dimensions.
Although it is confirmed that users' search behaviors will vary in different tasks, it is unexpected that only a small part of the differences show connections with the two task dimensions. In some cases, one type of task shows unique characteristics that are different from the other three. Sometimes we observed similarity between tasks that are different in both dimensions (e.g., KI and EX tasks, and KS and IN tasks). In addition, some characteristics exist in all types of tasks. This indicates that the two dimensions (product and goal) are probably still not enough to fully explain the differences of tasks and the underlying mechanisms that make user behavior different. Currently it remains unclear what the other possible factors are and how they might be identified.
One unique contribution of our work is that the results provide suggestions for systems to support sessions of complex search tasks. Specifically, our analysis of browsing and clicking patterns on the basis of eye-tracking data suggests that systems should be tailored for the search task at hand and the specific time point in the session. This advocates for futures systems that can automatically detect types of search tasks and optimize systems for the corresponding tasks, with dedicated supports during the search session. It also challenges existing evaluation metrics with fixed parameters in browsing and clicking models [4, 25] during a search session [14, 16].
ACKNOWLEDGEMENTS

615

Table 8. Summary of findings by tasks and dimensions.

Known Item (KI)  More searches, fewer examined and clicked results (5.1)  Greater efforts examining SERPs (5.2.1)  Most biased to top results in browsing (5.2.2) and clicking
(5.3.4)  Click selectively (5.3.1 & 5.3.2)  Less fixations per query (6.1)  Downgraded query search performance (6.1)
Interpretive (IN)  Fewer searches, more examined and clicked results (5.1)  More time reading result webpages (5.2.1)  Least likely sequential browsing (5.2.3)  Better query search performance (5.4.1)  Highest overlap of results (5.4.1)  Reformulate based on results (5.4.2)  Less willing to click examined results (6.1)
Goal: Specific
 Increased fixations on lower ranked results after time (6.2)

Known Subject (KS)  Fewer searches, more examined and clicked results
(5.1)  More time reading result webpages (5.2.1)  Better query search performance (5.4.1)  Less fixations per query (6.1)  Less willing to click examined results (6.1)
Exploratory (EX)  More searches, fewer examined and clicked results
(5.1)  Greater efforts examining SERPs (5.2.1)  Lowest click accuracy (5.3.2)  Willing to re-open visited results (5.3.3)  Less fixations per query (6.1)  Downgraded query search performance (6.1) Goal: Amorphous  Wider breadth of a SERP view (5.2.3)
 Less persistent in browsing (6.1)  Decrease of fixations (6.2)

Product: Factual  Higher click accuracy (5.3.2)  Shorter queries (5.4.1)  Focus on task information for query reformulation
(5.4.2)
Product: Intellectual  Less likely sequential scanning (5.2.3)
All Tasks  Substantial attentions on lower ranked results (5.2.2)  Over 20% total fixations on visited results (5.2.4)  More clicks on examined relevant result (5.3.2)  Limited use of "related searches" (5.4.3)  Less SERP views and clicks per query (6.1)

This work was supported in part by the School of Information Sciences at the University of Pittsburgh and the Center for Intelligent Information Retrieval at the University of Massachusetts Amherst. Part of the work was done when the first author was at the University of Pittsburgh. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
8. REFERENCES
[1] Bates, M.J. 1989. The design of browsing and berrypicking techniques for the online search interface. Online Information Review. 13(5): 407-424.
[2] Broder, A. 2002. A taxonomy of web search. SIGIR Forum. 36(2).
[3] Buscher, G. et al. 2010. The good, the bad, and the random. In Proc. SIGIR'10: 42-49.
[4] Chapelle, O. et al. 2009. Expected reciprocal rank for graded relevance. In Proc. CIKM'09: 621-630.
[5] Clarke, C.L.A. et al. 2007. The influence of caption features on clickthrough patterns in web search. In Proc. SIGIR'07, 135-142.
[6] Cole, M.J. et al. 2010. Linking search tasks with low-level eye movement patterns. In Proceedings of the 28th Annual European Conference on Cognitive Ergonomics.
[7] Cole, M.J. et al. 2011. Task and user effects on reading patterns in information search. Interacting with Computers. 23(4): 346­362.
[8] Cutrell, E. and Guan, Z. 2007. What are you looking for? In Proc. CHI'07: 407-416.
[9] Dahlberg, J. 2010. Eye Tracking with Eye Glasses. Master Thesis, Umea University.
[10] Dumais, S.T. et al. 2010. Individual differences in gaze patterns for web search. In Proc. IIiX'10: 185-194.
[11] Granka, L.A. et al. 2004. Eye-tracking analysis of user behavior in WWW search. In Proc. SIGIR'04: 478-479.
[12] Guan, Z. et al. 2007. An eye tracking study of the effect of target rank on web search. In Proc. CHI'07: 417-420.
[13] Jansen, B.J. et al. 2000. Real life, real users, and real needs: a study and analysis of user queries on the web. Information Processing & Management. 36(2): 207-227.
[14] Järvelin, K. et al. 2008. Discounted Cumulated Gain Based Evaluation of Multiple-Query IR Sessions. In LNCS 4956: Proc. ECIR'08: 4-15.

[15] Joachims, T. et al. 2005. Accurately interpreting clickthrough data as implicit feedback. In Proc. SIGIR'05: 154-161.
[16] Kanoulas, E. et al. 2011. Evaluating multi-query sessions. In Proc. SIGIR'11: 1053-1062.
[17] Kanoulas, E. et al. 2012. Overview of the TREC 2012 Session Track. In Proc. TREC 2012.
[18] Kelly, D. et al. 2009. A comparison of query and term suggestion features for interactive searching. In Proc. SIGIR'09: 371-378.
[19] Li, Y. and Belkin, N.J. 2008. A faceted approach to conceptualizing tasks in information seeking. Information Processing & Management. 44(6): 1822­1837.
[20] Liu, J. et al. 2012. Exploring and predicting search task difficulty. In Proc. CIKM'12: 1313-1322.
[21] Liu, J. et al. 2010. Search behaviors in different task types. In Proc. JCDL'10: 69-78.
[22] Liu, J. and Belkin, N.J. 2010. Personalizing information retrieval for multi-session tasks. In Proc. SIGIR'10: 26-33.
[23] Lorigo, L. et al. 2006. The influence of task and gender on search and evaluation behavior using Google. Information Processing & Management. 42(4): 1123-1131.
[24] Moffat, A. et al. 2013. Users Versus Models: What Observation Tells Us About Effectiveness Metrics. In Proc. CIKM'13.
[25] Moffat, A. and Zobel, J. 2008. Rank-biased precision for measurement of retrieval effectiveness. ACM TOIS 27(1): 2:1­2:27.
[26] Rayner, K. 1998. Eye movements in reading and information processing: 20 years of research. Psychological bulletin. 124(3).
[27] Spink, A. et al. 2002. Multitasking information seeking and searching processes. Journal of the American Society for Information Science and Technology. 53(8): 639­652.
[28] Spink, A. et al. 2001. Searching the web: The public and their queries. Journal of the American Society for Information Science and Technology. 52(3): 226­234.
[29] Thomas, P. et al. 2013. What Users Do: The Eyes Have It. In Proc. AIRS'13: 416­427.
[30] White, R.W. and Roth, R.A. 2009. Exploratory search: Beyond the query-response paradigm. Synthesis Lectures on Information Concepts, Retrieval, and Services. 1(1): 1­98.
[31] Wu, W.-C. et al. 2012. Grannies, tanning beds, tattoos and NASCAR. In Proc. IIIX'12: 254-257.

616

Query Log Driven Web Search Results Clustering

Jose G. Moreno
Normandie University UNICAEN, GREYC CNRS
F-14032 Caen, France
jose.moreno@unicaen.fr

Gaël Dias
Normandie University UNICAEN, GREYC CNRS
F-14032 Caen, France
gael.dias@unicaen.fr

Guillaume Cleuziou
University of Orléans LIFO
F-45067 Orléans, France
cleuziou@univ-orleans.fr

ABSTRACT
Different important studies in Web search results clustering have recently shown increasing performances motivated by the use of external resources. Following this trend, we present a new algorithm called Dual C-Means, which provides a theoretical background for clustering in different representation spaces. Its originality relies on the fact that external resources can drive the clustering process as well as the labeling task in a single step. To validate our hypotheses, a series of experiments are conducted over different standard datasets and in particular over a new dataset built from the TREC Web Track 2012 to take into account query logs information. The comprehensive empirical evaluation of the proposed approach demonstrates its significant advantages over traditional clustering and labeling techniques.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information search and retrieval--clustering
General Terms
Algorithms, Experimentation
Keywords
Web Search Results Clustering, Dual C-means, Automatic Labeling, Evaluation
1. INTRODUCTION
Web search results clustering (SRC), also known as post-retrieval clustering, multifaceted clustering or ephemeral clustering has received much attention for the past twenty years. SRC systems return meaningful labeled clusters from a set of Web snippets retrieved from any Web search engine for a given user's query. So far, most works have focused on the study of topical clustering [9] although some studies have been appearing in temporal clustering [1] and geospatial clustering [39]. As a consequence, SRC systems
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609583.

can be particularly useful to understand query intents (topical clustering) and query diversity (temporal/geospatial clustering). In this paper, we particularly focus on topical SRC.
As opposed to classical text clustering, SRC must deal with small text fragments (Web snippets) and be processed in run-time. As a consequence, it is hard to implement efficiently and effectively [9]. So, most successful methodologies follow a monothetic approach [40, 10, 12, 35]. The underlying idea is to discover the most discriminant topical words in the collection and group together Web snippets containing these relevant terms. On the other hand, the polythetic approach, in which the main idea is to represent Web snippets based on the Vector Space Model (VSM) has received less attention [18, 22, 41, 30]. The main reason is the fact that the labeling process is a surprisingly hard extra task [9].
Our research is motivated by the fact that the adequate combination of the polythetic and monothetic approaches in a single algorithm should lead to improved performance over three important factors in SRC: clustering accuracy, labeling quality and partitionning shape. For that purpose, we present a new algorithm called Dual C-Means, which provides a theoretical background for dualrepresentation clustering. Its originality relies on the fact that different representation spaces can drive the clustering process as well as the labeling task in a single step.
We evaluated the proposed algorithm over different metrics (e.g. F1N [13], Fb3 [2], ARI [37], D#-nDCG [34]), well-studied datasets (e.g. ODP-239 [10], SEMEVAL [28]) and different representation spaces (e.g. text and query logs). The results show that the combination of the VSM representation of Web snippets and a querylog-based representation of cluster centroids achieves the best configuration for the SRC task. In particular, increased performance is shown against most SRC solutions (e.g. STC [40], LINGO [30], TOPICAL [35], LDA [7]). Our main contributions are :
· A new algorithm (Dual C-Means), which can be seen as an extension of K-means [21] for dual-representation spaces;
· An instantiation of the Dual C-Means for SRC, which takes advantage of external resources such as query logs to improve clustering accuracy, labeling quality and partitioning shape;
· A new annotated dataset (WEBSRC401) based on the TREC Web Track 2012 for full SRC evaluation over the Web.
In the next section, we present the most important recent studies for SRC. In the third section, we present the general model of the Dual C-Means algorithm and its instantiation in the context of SRC. In the fourth section, we explain the construction of the WEBSRC401 dataset. In the fifth and sixth sections, we present the experimental setups and show the results obatined for different strategies over an exhaustive set of well-known evaluation metrics,

777

datasets and state-of-the-art algorithms. Finally, we draw some conclusions about our experiments and propose new perspectives.
2. RELATED WORK
A good survey of SRC methodologies can be found in [9]. As a consequence, we give a brief overview of older methodologies and focus on more recent works. The first important work in SRC is certainly proposed by [18]. They define a polythetic approach based on the VSM representation where similarity between documents is computed with cosine similarity measure. Then, a nonhierarchical partitioning strategy called fractionation is performed to discover the number of clusters suggested by the user. Initial results show that their "approach to document clustering is one which can produce significant improvements over similarity search ranking alone". Although they present the foundations of SRC, labeling is not tackled and evaluation is based on a small dataset and a limited user study.
In order to propose a more realistic solution, which includes labeling, [40] defined the Suffix Tree Clustering (STC) algorithm. They propose a monothetic clustering technique, which merges base clusters with high string overlap. Instead of using the VSM representation, they propose to represent Web snippets as compact tries. Their evaluation over a small set of 10 queries shows that STC outperforms group-average agglomerative hierarchical clustering, K-Means, buckshot, fractionation and single-pass algorithms. STC is still considered as a hard baseline to compete with.
Later, [30] proposed a polythetic approach called LINGO, which takes into account the string representation proposed by [40]. They first extract frequent phrases based on suffix-arrays. Then, they reduce the term-document matrix (defined as a VSM) using Single Value Decomposition to discover latent structures. Finally, they match group descriptions with the extracted topics and assign relevant documents to them. LINGO is evaluated with 7 users over a set of 4 search results and as such, no conclusive remarks can be drawn. However, their publicly available implementations of LINGO, STC and BiKM (Bi-section K-means) provide researchers with useful tools to build SRC systems.
More recently, [10] showed that the characteristics of the outputs of SRC algorithms suggest the adoption of a meta clustering approach. For that purpose, they introduce a novel criterion to measure the concordance of two partitions of Web snippets into different clusters based on the information content associated with the decisions made by the partitions on single pairs of Web snippets. A meta clustering phase is then casted to an optimization problem of the concordance between the clustering combination and the given set of clusterings. The results of their OPTIMSRC system demonstrate that meta clustering is superior over individual clustering techniques. In particular, they propose a dataset called ODP-239, which is widely used in the community.
Another polythetic methodology is proposed in [26]. Their underlying idea is that informative text similarity measures can improve SRC by adequately capturing the latent semantics conveyed by Web snippets. They propose a K-means based algorithm called GK-means within which a new objective function defined for a third-order similarity measure must be maximized. As different partitions are possible depending on the K value, they propose an automatic stopping criterion to retrieve one "optimal" clustering solution. Their main contribution is the fact that labels are built during the clustering process thus avoiding an extra processing step. Their results show improvements for ODP-239 in terms of Fb3 over all text-based SRC algorithms.
While all studies mentioned so far treat the task of SRC as a textbased problem, some other works propose to introduce external re-

sources. The first relevant work is presented in [16] where Web snippets are enriched with anchor text information and high quality indexes extracted from DMOZ. The underlying idea of their monothetic approach called SNAKET is that better labeling and clustering can be obtained from these external resources. Results over a non-standard dataset show that the introduction of external information improves Precision at different clustering levels. [16] certainly proposed a new trend in SRC.
Following the same idea, [35] proposed TOPICAL, a top performing SRC system over ODP-239 dataset. They propose to move away from the bag of words representation towards a graph of topics paradigm derived from TAGME, a wikification algorithm [38]. Each Web snippet is annotated with a set of topics, which are represented by Wikipedia articles. A bipartite-like graph structure is built where nodes are either Web snippets or topics and edges are either topic-to-topic or topic-to-snippet. Then, a spectral-like clustering algorithm is run over the graph to discover relevant clusters and meaningful labels. TOPICAL is an interesting approach as clustering is driven by the presence of Wikipedia titles in Web snippets and inderectly assures the quality of the labeling.
Another idea has recently been proposed in [13], which relies on Web n-grams. In order to better capture the similarity between Web snippets, a first step consists in building a co-occurence graph based on Dice coefficient calculated over the Google Web1T corpus [8] from which senses are discovered by word sense induction algorithms. Each Web snippet is represented as a bag-of-words (polythetic approach) but Similarity is computed over discovered word senses. Their experiments show that enhanced diversification and clustering performance results can be obtained based on the adjusted RandIndex [37] for a specific dataset built for ambiguous queries (MORESQUE). Recently, researchers from the same team proposed a new dataset within the context of the SEMEVAL task 11 [28], in which the goal is to provide an evaluation framework for the objective comparison of word sense disambiguation and induction algorithms in SRC for ambiguous queries.
All works propose interesting issues for SRC. On one hand, the monothetic approach mainly focuses on the identification of strong meaningful labels. The underlying idea is that good labels are a key factor for the success of user experience in Web search. On the other hand, the polythetic approach concentrates on discovering high quality clusters and the labeling task is usually treated as a separate process. The subjacent motivation is that good clustering should be provided to improve user experience in search for information. Moreover, recent studies show that the introduction of external resources improves overall results.
In this paper, we propose that both monothetic and polythetic approaches should be combined in a single algorithm capable of accepting external resources. For that purpose, we present the Dual C-Means algorithm, which extends the well-known K-Means for dual representation spaces. It particularity relies on the fact that different representation spaces compete to reach high clustering quality and meaningful labeling. In particular, we propose that query logs are introduced as external information to ensure quality labeling and drive the clustering process. The main characteritics of our proposal are as follows:
· New combination of polythetic and monothetic approaches in one single algorithm;
· Introduction of dual representations for Web snippets allowing the introduction of external resources;
· Theoretical framework based on an extension of K-means;
· First proposal with query logs as external resource for SRC.

778

3. DUAL C-MEANS ALGORITHM
This section is devoted to the presentation of the Dual C-Means algorithm that extends the classical K-means [21] for dual representation spaces. In the first subsection, we present the general model and in the second one we propose its instantiation for the specific task of SRC.
3.1 General Model
Let S be a dataset to partition where each data si  S is described on a representation space E1 and additionally, E2 denotes another space supporting cluster representation. We hypothesize the existence of a function d : E1 × E2  R+ quantifying the dissimilarity between any data from E1 and any cluster representative (cluster centroid) from E2. The new proposed clustering model (Dual CMeans) is driven by the objective criterion defined in Equation 1, which must be minimized.

c

Jdcm(, M) =   d(si, mk)

(1)

k=1 sik

As illustrated in Figure 1, the aim of the minimization of Jdcm(, M) is to find a partition of S into c clusters ( = {1, . . . , c}) such that in each cluster k any object is as closed as possible to a common
cluster representative mk (M = {m1, . . . , mc}).

E2 mk

d(si, mk)

E1

k

si

Figure 1: Dual C-Means aims to discover clusters of objects in E1 closed to a common cluster representative in E2.

as dissimilarity d(., .), the Dual C-Means algorithm comes down exactly to the usual K-means algorithm (mtk+1 = sikt si/|kt |). Finally, such as K-means, Dual C-Means is sensitive to random initialization and requires the number of expected clusters (C) as parameter1.
3.2 Instantiation in the SRC Context
In the context of SRC, objects are naturally Web snippets represented in the E1 space (si  S) and cluster representatives are labels represented in the E2 space (mk  M).
The crucial hypothesis of the Dual C-Means algorithm is the existence of a dissimilarity metric d(., .) capable of comparing objects from different feature spaces. For that purpose, a matching process between the two feature sets is required that can be formalized as a transition matrix P (p1 × p2) quantifying this matching for each of the p1 features defined in E1 with each of the p2 features from E2.
Without loss of generality, we define a generic dissimilarity measure considering such a transition matrix in Equation 2 where mTk is the transposed label vector, siPmTk quantifies a similarity between a Web snippet si and a label mk, and  is a constant to adjust in order to ensure dissimilarity values in R+.

d(si, mk) =  - siPmTk

(2)

Such a dissimilarity form allows us to rewrite the Dual C-Means algorithm as a maximization problem defined in Equation 3.

c

c

    min
,M

k=1

si

k

d

(si

,

mk

)



max
,M

k=1

si

k

si

PmTk

(3)

Let us notice that when the label space E2 is unconstrained (e.g. E2 = Rp2 ), the resolution of Equation 3 has no sense (M = +).

But in the SRC context, a small set of words (i.e. the labels)

are usually chosen to help the user in his search for information.

Thus, we consider two vocabularies V1 and V2 defining the two

feature spaces E1 and E2 respectively. We constrain Web snip-

pet descriptions to be word distributions over V1 (si, j  [0, 1] i, j

and



p1 j=1

si, j

=

1)

and

cluster

labels

to

subsets

of

p

words

from

V2

(E2 = {mk  {0, 1}p2 | lp=2 1 mk,l = p}).

Within that context, the computation of optimal cluster labels is

a discrete optimization process solved for each cluster k indepen-

dently, by first sorting the vocabulary V2 from the most relevant word (l1k) to the less relevant one (lkp2 ) using the relevance function defined in Equation 4

The optimization process can be achieved by an usual dynamic reallocation algorithm starting with a random initial clustering 0 and then iterating the following two steps (Update and Assignment) until convergence:
1. Update: compute new optimal cluster representatives Mt+1 considering a fixed partition t ,
2. Assignment: compute new optimal assignments t+1 considering fixed cluster representatives Mt+1 and use the following rule to assign each object to its closest representative:
si, si  k  k = arg minl=1,...,c d(si, ml ).
Note that the update of cluster representatives has to be defined depending on both the dissimilarity measure d(., .) and the representative space E2 in order to ensure that the objective criterion Jdcm(., .) decreases. Let us also notice that in the specific case where E1 = E2 = Rn and the squared euclidean distance is chosen

l, k k(l) =  siP.,l

(4)

si k

and then defining a cluster label vector mk as the combination of the p most relevant words from V2 for the snippets in k as proposed in
Equation 5.

{

mk,l =

1 if l  lkp 0 otherwise

(5)

It is interesting to notice that the GK-means, recently proposed by [26], falls into such an SRC instantiation of the Dual C-Means algorithm if the following constraints are true:

· Web snippet and label representation spaces are not dissociated (i.e. V1 = V2) thus not taking benefit from the duality of the clustering algorithm;
1These issues will be tackled in the Evaluation section.

779

· The transition matrix P is computed with the Symmetric Conditional Probability (SCP [36]) or the Pointwise Mutual Information (PMI [11]) on the unique vocabulary V1 = V2.
To make use of the duality concept from the new proposed algorithm in the SRC context, we suggest differentiating the two vocabularies V1 and V2. First, V1 is defined as the bag of words occurring in all Web snippets retrieved for a given query. Second, if we consider a set Y of query logs, the vocabulary V2 is defined by the bag of words occurring in Y and E2 is restricted to the set of query logs defined as distributions in the vector space model induced by V2. This situation is formalized in Equation 6 with i denoting the size of the query log yi.

 E2

=

{yi



{0,

1 i

}p2 |

p2
yi,
j=1

j

=

1

and

yi



Y

}

(6)

As such clustering is polythetic but query log driven. Figure 2 illustrates the instantiation of the Dual C-Means algorithm in the SRC context where the restricted set of available query logs guides the cluster formation process.

E2 = {query - logs}
y1 = (jaguar, animal, black) y2 = (jaguar, car)

s1 = "The black jaguar is one of three called panther..."
E1 = {snippets}
Figure 2: Example of the Dual C-Means instantiated for the SRC context with query logs as cluster label space.
4. THE WEBSRC401 DATASET
Different gold standards have been used for the evaluation of SRC algorithms among which the most cited are: AMBIENT [6], ODP-239 [10], MORESQUE [27] and SEMEVAL [28]. As ODP239 is an evolution of AMBIENT and SEMEVAL is the next generation of MORESQUE, we will only give an overview of the most recent datasets.
In ODP-239, each document is represented by a title and a Web snippet and the subtopics are chosen from the top levels of DMOZ2. However, this dataset does not represent the typical kind of results obtained through querying a given search engine as the number of possible subtopics is always equal to 10. It is clear that this structure clearly differs from a typical Web results set. Moreover, queries are not extracted from query logs but rather chosen based on the categories present in DMOZ. However, it is a publicly available dataset that allows us to conduct experiments to evaluate clustering accuracy. 2http://www.dmoz.org [Last access: 27/01/2014].

On the other hand, the subtopics in SEMEVAL follow a more natural distribution as they are defined based on the disambiguation pages of Wikipedia. As such, these subtopics are likely to cover most of the senses present in the Web for the 100 evaluated queries. However, SEMEVAL is built to specifically deal with ambiguous queries, which are self-contained in Wikipedia. But, it is clear that not all queries in general are Wikipedia articles or ambiguous. For example, many queries are multifaceted but not ambiguous [19]. Let us take "Olympic Games". Its Wikipedia entry is not ambiguous but it presents many different facets such as History, Logos, Year dates or Cities, to name but a few.
As a consequence, it is clear that different results can be obtained from one dataset to another. A quick summary of both datasets is presented in Table 1.

Dataset
ODP-239 SEMEVAL WEBSRC401

# of queries
239 100 50

# of Subtopics Avg / Min / Max
10 / 10 / 10 7.7 / 2 / 19 3.9 / 3 / 6

# of Web snippets
25580 6400 5560

Table 1: Description of the SRC gold standard datasets.

To afford a more realistic situation in the context of Web search results, we propose a new SRC dataset based on the ClueWeb09 Category B text collection (CCB)3, which comprises about 50 million English-language pages, including the entirety of the Englishlanguage Wikipedia and task descriptions of the TREC Web Track 2012. The goal of TREC Web Track 2012 is to return a ranked list of Web pages that together provide complete topical coverage of a given query, while avoiding excessive redundancy of the subtopics in the result list. In particular, each topic contains a query field, a description field and several subtopic fields which can be ambiguous or multifaceted. And for each topic, a judgement file (i.e. qrel) includes the list of relevant Web pages from CCB and the manually attributed grade of the Web page subtopic.
Instead of retrieving relevant Web pages, we are interested in obtaining relevant clusters (i.e. Web pages with the same subtopic) with high coverage of all the subtopics. So, we propose transforming the data available in the TREC Web Track 2012 in a typical SRC format [10], which result in the WEBSRC401 dataset4. First, for each Web page considered as query-relevant, its Web snippet is retrieved using the SnippetGenerator function of ChatNoir5. By default, a Web snippet composed of a maximum of 500 characters found around the query words is provided.
Secondly, for each query, its subtopics are defined as in the TREC Web Track 2012 and each qrel is encoded in a new format, which contains the Web page id, the subtopic id and the query6. Additionally, it is important to notice that the WEBSRC401 dataset facilitates the evaluation of new techniques based on more complex resources provided by researchers as it is based on the well-studied ClueWeb09. For example, cluster ranking or spam cluster filtering studies could be endeavored with the PageRank scores and the spam rankings of ClueWeb09 dataset which are publicly available.

5. CLUSTERING EVALUATION
As mentioned in [9], evaluating SRC systems is a hard task. Indeed, the evaluation process is three-fold. A successful SRC sys-
3http://lemurproject.org/clueweb09/ [Last access: 27/01/2014] 4http://websrc401.greyc.fr/ [Last access: 10/05/2014]. 5http://chatnoir.webis.de/ [Last access: 27/01/2014]. 6Note that these steps could be used to extend the dataset with the TREC Web tracks of the years 2009, 2010 and 2011.

780

tem must discover relevant topical clusters (clustering accuracy) and propose meaningful labels at the same time (labeling quality). We will also see in our experiments that partition shape is also an important factor to study.
5.1 Evaluation of SRC
Firstly, a successful SRC system must evidence high quality level clustering. Ideally, each query subtopic should be represented by a unique cluster containing all the relevant Web pages inside. However, this task is far from being achievable. As such, this constraint can be reformulated as for the TREC Web Track 2012: the task of SRC systems is to provide complete topical cluster coverage of a given query, while avoiding excessive redundancy of the subtopics in the result list of clusters.
Secondly, SRC systems should present meaningful labels to the user to ease their search for information. As such, the evaluation of the labeling task is of the utmost importance. As far as we know, only [10, 35] propose the evaluation of both dimensions. However, their experiments are not reproducible as they rely on manually annotated datasets, which are not publicly available.
Thirdly, SRC differs from classical text clustering as the partitioning shape, more precisely the distribution of the Web snippets into clusters, shows evidence of some particularity. Indeed, it is well-known that subtopics on the Web are not equally distributed. For example, for the query "Apple", it is much more likely to find Web snippets related to the company than the concept of fruit. In particular, we will see in our experiments that not all evaluation metrics cover this situation.
In the next sections, we propose a complete set of repeatable experiments to give an exhaustive overview of the SRC field. We start by focusing on the experimental setups.
5.2 Experimental Setups
In this section, we propose the comparison of different configurations of the Dual C-Means to several state-of-the-art algorithms using well-studied evaluation metrics.
Dual C-Means Configurations.
The originality of the Dual C-Means is to embody a great number of possible configurations due to the expressiveness of its model. In this paper, we will particularly focus on two main issues. The first one deals with using different similarity measures to compute the transition matrix P. The underlying idea is supported by the fact that different word similarity measures produce different results [31]. As a consequence, we aim to understand their impact on the SRC task. The second one aims to test our initial hypothesis stating that the introduction of external resources can improve SRC. As a consequence, we propose two different space representations: text-text (i.e. V1 = V2) and text-query logs (i.e. V1 = V2).
Word Similarity Measures.
The use of word similarity metrics is an important and interchangeable component of our algorithm encoded in the transition matrix. In this study, we propose the comparison of a total of five collocation metrics7. In particular, we used the Symmetric Conditional Probability (SCP) [36], the Pointwise Mutual Information (PMI) [11], the Dice coefficient [14], the LogLikelihood ratio (LogLike) [15] and 2 [17]. Each metric is defined in Table 2. The expressiveness of the Dual C-means permits the definition of different types of word similarity measures. As a consequence, we
7It is clear that a great deal of association measures that could be tested exist. However, we selected the ones which best complement themselves.

also compute word-word similarity based on the VSM representation. In particular, for each snippet si  S, a simple word-word similarity measure is ST S where ST is the transposed of the snippetterm matrix S. In this case, P = ST S. Another interesting similarity measure is LSA [20], which can be formulated as follows: P = U e UT where U  UT is the eigen decomposition of ST S, and e is the number of highest eigen values selected to represent the latent space8.
SRC Algorithms.
We aim to compare our algorithm to the most competitive strategies proposed so far in the SRC literature. For that purpose, we show the results of STC [40], LINGO [30], TOPICAL [35] and LDA [7]. It is worth noticing that for evaluation purposes, we developed an open source implementation9 of TOPICAL using the Wikipedia Miner API proposed by [24] and the spectral algorithm proposed by [29] included in SCIKit learning tool10. For LINGO and STC algorithms, we used the Carrot2 API11. And for LDA, we used the topic modeling package included in MALLET toolkit [23]. The parameters were set following the toolkit instructions (i.e. stop-words removal, t = 0.01, w = 0.01 and limited to 1000 iterations) and the cluster membership is assigned taken the maximum topic probability value.
Evaluation Metrics.
Different metrics have been proposed to evaluate text clustering. Within this paper, we present the results for the most relevant metrics. The first complete study in terms of evaluation has certainly been proposed by [10]. In the specific case of SRC, the authors propose the F1C metric, which is a specific implementation of the more general F measure. Other metrics have also been proposed. For example, the Fb3 measure [2] addresses many important problems in clustering such as cluster homogeneity, completeness, rag-bag and size-vs-quantity constraints, and has shown interesting properties for the SRC task as formulated in [26]. Two other important metrics have been studied in [13]: F1N and the Adjusted RandIndex (ARI) [37]. In particular, F1N can be seen as a complementary metric of F1C as it is also based on the classical F measure but computed in a different manner12, while ARI evidences an interesting property for SRC. While it measures clustering accuracy, it also takes into account the fact that a given partition shows a similar partitioning shape compared with the reference gold standard. The underlying idea is that the number of clusters and the average number of Web snippets in each cluster approximate as much as possible the reference clustering. An illustration of this situation can be seen in [25] although the authors do not refer to this issue as an important one for SRC. In terms of implementation, we used the Java evaluator13 to compute both F1N and ARI evaluation metrics, and the implementation provided by [3]14 to compute Fb3 . In Table 3, we defined all the metrics used for our experiments.
8In our experiments, this value was set to the minimum which guarantees that ei=1 i  0.9 ip=11 i. 9This implementation is publicly available upon request. 10http://scikit-learn.org/stable/ [Last access: 27/01/2014]. 11http://search.carrot2.org/stable/search [Last access: 27/01/2014]. 12Let us notice that these are two F1 measures, which computation is defined differently in [10] and [13]. 13http://www.cs.york.ac.uk/semeval2013/task11/index.php?id=data [Last access: 27/01/2014]. 14http://nlp.uned.es/~enrique/software/RS.zip [Last access: 27/01/2014].

781

Collocation Metric SCP(wi, w j) PMI(wi, w j) DICE(wi, w j)
LogLike(wi, w j) 2(wi, w j)

Formula

P(wi,w j)2

P(wi)P(w j)

log2

P(wi,w j) P(wi)P(w j)

2 f (wi,w j)

f (wi)+ f (w j)

-2  logLike( f (wi, w j),

f (wi),

f

(w N

j

)

)

+

l

ogLike(

f

(w

j

)

-

f (wi, w j), N

-

f (wi),

f

(w N

j

)

)

-logLike( f (wi, w j),

f (wi),

f

(wi,w j f (wi)

)

)

-

l

ogLike(

f

(w

j

)

-

f (wi, w j), N

-

f (wi),

f (w j)- f (wi,w j) N- f (wi))

where logLike(a, b, c) = (a  Log(c)) + ((b - a)  Log(1 - c))

P(wi,w j)-P(wi)P(w j) P(wi)P(w j)(1-P(wi))(1-P(w j))

Table 2: Collocation metrics used in our framework where P(wi, w j) is the joint probability of words wi and w j, P(wi) is the marginal probability of the word wi, f (wi, w j) is the frequency of word pairs (wi, w j), f (wi) is the frequency of the word wi and N is the number of retrieved Web snippets.

Evaluation Metric

F1C

=

2PR P+R

Fb3

=

2Pb3 Rb3 Pb3 +Rb3

F1N

=

2PR P+R

ARI(, ) and

where

k

k

P

=

T

TP P+F P

,

R

=

TP T P+FN

,TP

=

   g0(x j, xl), FP
z=1x jzxl z,l= j

=


i=1x


j i

xl


i ,l =

(1
j

-

g0

(x

j

,

xl

)),

k

FN =    (1 - g0(x j, xl))

z=1x jzxl z,l= j

Pb3

=

1 N

k

i=1

1 |i

|

x


j i


xl i

g0

(x

j

,

xl

),

Rb3

=

1 N

k

z=1

1 |z

|

x

j


z

xl


z

g0

(x

j

,

xl

)

k

k

P

=

1 ki=1 |i

|

i=1maxz

(

f

(z

,

i

)),

R

=

1 kz= 1 |z

|


z=1

f

(z

,

i


z

i

)

b  z



z = arg maxa( f (a, b)),

f

(a

,

b

)

=

x

j


a

xl


b

g1

(x

j

,

xl

)

ARI(, ) =

RI (, )-E (RI (, )) maxRI (, )-E (RI (, ))

where

{

RI(, )

=

T

T P+T P+F P+F

N N+T

N

,

T

N

=

N

-{T P - FP - FN

g0(xi, x j) =

1  l : xi  l  x j  l 0, otherwise

g0(xi, x j) =

1  l : 0, otherwise

xi  l  x j  l

where i is the cluster solution i ( = i) and{i is the gold standard of the category i ( = i).

g1(xi, x j) =

1  xi = x j 0, otherwise

Table 3: Clustering Evaluation Metrics.

Text Processing and Implementation.
All Web snippets were tokenized with the GATE platform15 but we did not apply stop-words removal so that we can propose a language-independent solution. In terms of dynamic reallocation algorithm, we used the optimized version of K-means++ proposed in [4] as the intialization process is semi-deterministic16 and there exists an efficient implementation called Scalable K-means++ [5].
5.3 Clustering Results
A great deal of experiments have been performed to achieve conclusive results. We first propose evaluating the clustering accuracy of the Dual C-Means against different state-of-the-art algorithms. For that purpose, we propose an exhaustive search as in [35], whose underlying idea is to evaluate the behavior of a given algorithm along with the increasing number of output partitions. In this first set of experiments, we pretend to understand the clustering quality of our approach when only text information is taken into account
15http://gate.ac.uk/ [Last access: 27/01/2014]. 16Note that for our experiments, the first seed Web snippet is selected as the one, which is most similar to all other ones in S.

(i.e. V1 = V2 and the number of p words composing the centroids is set to 2) and compare it to state-of-the-art algorithms. In particular,
we present the results for 20 runs (K = 2..20) and illustrate the Fb3 values over ODP-239 and WEBSRC401 in Figure 3. Indeed, recent
studies in [2][3] show that Fb3 is a superior metric to the classical F measures to compute clustering accuracy.
The obtained results show interesting situations. In all cases,
Dual C-means outperforms state-of-the-art algorithms in terms of
clustering accuracy. In particular, SCP, DICE and LogLike show
improved results and outperform other word-word similarity metrics. It is interesting to notice that PMI and 2, which are known
to give less importance to more frequent events show less relevant
results. As for the state-of-the-art algorithms, best results are ob-
tained by STC improving over TOPICAL and LDA.
These results only give a small idea of the overall phenomena.
In Tables 4, 5 and 6, results for 10 cluster outputs are given for
all metrics and all datasets. These new results show interesting
properties of evaluation metrics. Although Dual C-means shows improvements over all competitors in terms of Fb3 or F1C (except in one case) for ODP-239, SEMEVAL and WEBSRC401, this situation does not stand for the other metrics, ARI or F1N . For ODP-239,

782

Fbcubed-measure Fbcubed-measure

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 k

SCP PMI DICE LogLike PHISquared TOPICAL STC Lingo

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 k

Figure 3: Impact of K for Fb3 against ODP-239 (left) and WEBSRC401 (right) datasets.

SCP PMI DICE LogLike PHISquared STS TOPICAL STC Lingo LDA

the best results are obtained by LDA in terms of ARI and LINGO in terms of F1N . For SEMEVAL, the best performances are provided by STC in terms of ARI and LINGO in terms of F1N . Deep analysis shows that ARI embodies an interesting property for the SRC task
as it is well-known that the sizes of the clusters are not distributed
equally on the Web. Indeed, ARI tends to favor solutions, which
show similar partitioning shapes to the gold standard. As a con-
sequence, a good SRC system should be performant both in terms of ARI and Fb3 . On the other hand, F1N shows inconsistent results when compared to all other metrics. In particular, it tends to give
high results when the other metrics decrease.
Although different results are obtained for SEMEVAL and ODP-
239, steady results are obtained for WEBSRC401 by the Dual CMeans configured with the ST S word-word similarity metric. In-
deed, it clearly outperforms all other algorithms in terms of Fb3 , F1C and ARI. At this stage of our experiements, we can conclude that this configuration provides the best performance both in terms
of clustering accuracy and partitioning shape.

ODP-239 SEMEVAL WEBSRC

LDA LINGO
STC TOPICAL
LDA LINGO
STC TOPICAL
LDA LINGO
STC TOPICAL

F1N 0.5978 0.6636 0.5499 0.5760
0.7159 0.7742 0.7223 0.6791
0.7020 0.7123 0.6779 0.6932

ARI 0.2571 0.0920 0.1597 0.1505
0.1313 0.0783 0.1704 0.0621
0.0268 0.0247 0.0220 0.0203

Fb3 0.4370 0.3461 0.4027 0.3799
0.3966 0.3662 0.4632 0.3998
0.3214 0.3095 0.4293 0.3083

F1C 0.3900 0.2029 0.3238 0.2839
0.2840 0.2072 0.3682 0.2723
0.2613 0.2502 0.3905 0.2522

Table 4: Results of state-of-the-art algorithms for the ODP-239, SEMEVAL, WEBSRC401. K fixed to 10 Clusters for LDA and TOPICAL.
The second set of our experiments aims to analyse the behavior of Dual C-Means when external resources are included. In this case, we use the set of query logs provided by the NTCIR-10 Intent2 task [32] and propose to drive the clustering process by this external information. As such, a cluster centroid is represented by its most representative query log. Results are presented in Table 6 where V1 = V2 for WEBSRC401. Let us notice that this is the only dataset for which experiments with query logs can be performed and easily reproduced.

SEMEVAL ODP239

SCP PMI DICE LOGLIKE 2 ST S LSA
SCP PMI DICE LOGLIKE 2 ST S LSA

F1N 0.6114 0.6634 0.6245 0.5753 0.6797 0.6225 0.6219
0.4961 0.5671 0.5181 0.5078 0.5479 0.5294 0.5482

ARI 0.0435 0.1072 0.0545 0.0209 0.1055 0.0319 0.0240
0.0865 0.1741 0.1213 0.1388 0.1618 0.1304 0.1490

Fb3 0.5632 0.4198 0.5763 0.5416 0.3972 0.5722 0.5645
0.4845 0.4041 0.4939 0.4285 0.3759 0.4852 0.4712

F1C 0.4856 0.3297 0.4914 0.4934 0.2932 0.4808 0.4684
0.3785 0.3231 0.3885 0.3650 0.3059 0.3822 0.3731

Table 5: Results of the Dual C-Means algorithm for ODP-239 and SEMEVAL. K fixed to 10 Clusters. Let us notice that for all experiments, the number of p words composing the centroids was set to 2 and the vocabulary is the set of words appearing in the retrieved Web snippets.

V1 = V2 (Text)
V1 = V2 (QL)

SCP PMI DICE LOGLIKE 2 ST S LSA
SCP PMI DICE LOGLIKE 2 ST S LSA

F1N 0.6698 0.6788 0.6718  0.6566 0.6841 0.6713 0.6706
0.6580 0.6866 0.6593 0.6636 0.6783 0.6645 0.6719

ARI 0.0317 0.0280 0.0341 0.0242 0.0213 0.0343 0.0170
0.0418 0.0366 0.0320 0.0219 0.0267 0.0470 0.0403 

Fb3 0.6597 0.3981 0.6575 0.5499 0.4299 0.6666  0.6327 
0.6572 0.3806 0.6343 0.5728 0.4333 0.6160 0.5577

F1C 0.6217 0.3514 0.6202 0.5131 0.3836 0.6260  0.5884 
0.6239 0.3338 0.6023 0.5394 0.3926 0.5847 0.5264

Table 6: Results of the Dual C-Means algorithm for WEBSRC401. K fixed to 10 Clusters. Let us notice that for all experiments where V1 = V2, the number of p words composing the centroids was set to 2 and the vocabulary is the set of words appearing in the retrieved Web snippets. Note that  means paired student's t-test statistical relevance for p - value < 0.05 between a given metric in V1 = V2 and its counterpart in V1 = V2.

783

Not surprinsingly, the introduction of external information decreases clustering accuracy. But, this is true only for a glimpse when comparing ST S for V1 = V2 and SCP for V1 = V2 (statistical relevance is not true in this case). However, the difference in terms of ARI is higher in favor of the dual representation space, although not with statistical relevance. In this case, we can conclude that while clustering accuracy slightly drops, partitioning shape seems to be put in advance by the query log driven approach. The other benefit of this new dual approach may be embodied by the expressiveness of the query logs as meaningful labels. This is the objective of the next section.
6. LABELING EVALUATION
As mentioned in [9], the labeling process plays an important role in the success of SRC systems. As a consequence, a clear objective evaluation is needed. However, this has not yet been the case. Indeed, [18][16] proposed user studies, which are difficult to replicate. In order to solve reproducibility problems, [10][35] proposed to evaluate the kSSL metric but their datasets are defined in two different ways and they are not publicly available. So, in order to propose a conclusive evaluation of the labeling process, we propose to use a new gold standard dataset provided by the Subtopic Mining subtask of the NTCIR-10 Intent-2 [32] and apply recent evaluation metrics proposed by [34]: I-rec@10, D-nDCG@10 and D#-nDCG@10.
These metrics aim to measure Precision and Recall of the users' intents. Within our context, we can use the labels provided by the SRC algorithms as the users' intents candidates. If so, we can directly apply the given metrics. In particular, I-rec measures the number of intents discovered by the algorithm over the total different intents of the query. This metric can simply be viewed as an intent Recall. Then, D-nDCG is obtained by sorting all relevant intents by the global gain, which is defined as the sum of all the individual intent gains. Finally, the D#-nDCG metric is the linear combination of I-rec and D-nDCG, using  and 1 -  factors. Note that defining the probabilities of each intent as well as the relevant intents can be a hard task. However, as our experiments are realized over WEBSRC401 based on ClueWeb09, these values are known and publicly available [32]. In particular, the NTCIREVAL toolkit17 was used for the calculation of these metrics. Let us notice that for the specific task of SRC, we propose to use I-rec@10, D-nDCG@10 and D#-nDCG@10 as for most queries the number of intents is limited. These metrics are defined in the Equations 7, 8 and 9.

|I |

I-rec@N =

(7)

|I|

where I is the set of known intents for a query q and I is the set of intents covered by the returned labels at level N.

D-nDCG@N

=

Nr=1 i Pr(i|q)gi(r)/log(r + 1) Nr=1 i Pr(i|q)gi (r)/log(r + 1)

(8)

where Pr(i|q) (resp. Pr(i|q)) denotes the intent probability ob-

tained for the discovered labels (resp. for the reference labels) and gi(r) (resp. gi (r)) is the gain value of the label at rank r with respect to i for the output of the labeling (resp. for the reference

labeling).

17http://research.nii.ac.jp/ntcir/tools/ntcireval-en.html [Last access: 27/01/2014].

D#-nDCG@N = I-rec@N + (1 - )D-nDCG@N (9)
where  was set to 0.5 following the framework evaluation proposed in the Subtopic Mining subtask of the NTCIR-10 Intent-2.
The results provided by [33] for different query completions (BingC, GoogleC and YahooC), query suggestions (BingS) services and a simple merging strategy (Merge) are reported in Table 7 as well as the results of our approach. In particular, we show the results when clustering is query log driven (V1 = V2) and when labeling is performed a posteriori (V1 = V2). By a posteriori, we mean that clustering is first performed on the exclusive text representation. Then, as a usual second step, the label is computed by any heuristic. In our experiments, the query log that best represents each text-based cluster is computed using one iteration of the update function defined in section 3, which allows direct comparison results.

V1 = V2 V1 = V2 Baselines

SCP PMI DICE LOGLIKE 2 ST S LSA
SCP PMI DICE LOGLIKE 2 ST S LSA
BingS BingC GoogleC YahooC Merge

I - rec@10 0.2804 0.3136 0.2952 0.2269 0.3390 0.2837 0.3238
0.3669  0.4136  0.3761  0.3937  0.4249  0.4033  0.3946
0.3068 0.3231 0.3735 0.3829 0.3365

nDCG@10 0.3195 0.3444 0.3242 0.2885 0.3642 0.3063 0.3694
0.3932  0.4257  0.3884  0.4146  0.4221  0.4273  0.4197 
0.2787 0.3268 0.3841 0.3815 0.3181

D# - nDCG@10 0.2959 0.3250 0.3093 0.2550 0.3523 0.2935 0.3456
0.3793  0.4203  0.3814  0.4046  0.4225  0.4119  0.4050 
0.2928 0.3250 0.3788 0.3822 0.3273

Table 7: Evaluation results of the labeling process with query logs over the NTCIR-10 Intent-2 dataset. Note that  means paired student's t-test statistical relevance for p - value < 0.05 between a given metric in V1 = V2 and its counterpart in V1 = V2.
The results of the query driven Dual C-Means outperform all baselines and a posteriori labeling. Moreover, all the differences between a given metric in V1 = V2 and its counterpart in V1 = V2 are statistically relevant. These results also show interesting behaviors. Indeed, while PMI and 2 collocation metrics previously showed worst clustering accuracy results compared to other configurations, they show improved results in terms of labeling. The fact that these metrics tend to favour less frequent associations is an interesting characteristic for labeling purposes and a conclusive remark. Moreover, the ST S word-word similarity measure shows high nDCG@10 value and competitive overall D# - nDCG@10. These results clearly point at this last configuration as the best compromise for clustering accuracy, labeling quality and partitioning shape.
7. CONCLUSIONS AND PERSPECTIVES
In this paper, we proposed a new algorithm called Dual C-Means, which can be seen as an extension of the classical K-Means for dual representation spaces. Its originality relies in the fact that the clustering process can be driven by external resources by defining two distinct representation spaces. In particular, we proposed

784

that query logs are used as external information to guide clustering and afford meaningful labels to users in their search for information. We also built a new publicly available dataset called WEBSRC401 based on ClueWeb09, which affords a more realistic situation for Web SRC. A complete and reproducible evaluation was performed over different gold standard datasets (ODP-239 and SEMEVAL) based on different publicly available evaluation tools. In particular, a great deal of evaluation metrics have been applied over diffferent configurations of the Dual C-Means integrating distinct word-word similarity measures. Results showed that our approach steadily outperforms all existing state-of-the-art SRC algorithms in terms of clustering accuracy (Fb3 ) but is less competitive in terms of ARI. This situation is handled by the introduction of query logs, which allows high labeling quality with outperforming values of I - rec@10, D - nDCG@10 and D# - nDCG@10 and adequate partitioning shape with high values of ARI.
The final findings that show that collocation metrics sensitive to high frequency events tend to produce high quality clusters and low frequency sensitive ones give rise to quality labels, is an interesting issue. Indeed, like the dual representation space, it suggests a multiobjective implementation of the dynamic reallocation algorithm to the problem of SRC. Moreover, the next steps that are being carried out are the introduction of different resources to drive the clustering process, the definition of new P transition matices taking into account recent developments in word-word similarity and the definition of powerful instantiation functions provided by the introduced general model.
8. REFERENCES
[1] O. Alonso, M. Gertz, and R. Baeza-Yates. Clustering and exploring search results using timeline constructions. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM), pages 97­106, 2009.
[2] E. Amigó, J. Gonzalo, J. Artiles, and F. Verdejo. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval, 12(4):461­486, 2009.
[3] E. Amigó, J. Gonzalo, and F. Verdejo. A general evaluation measure for document organization tasks. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 643­652, 2013.
[4] D. Arthur and S. Vassilvitskii. K-means++: the advantages of careful seeding. In Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1027­1035, 2007.
[5] B. Bahmani, B. Moseley, A. Vattani, R. Kumar, and S. Vassilvitskii. Scalable k-means++. Proceedings of the Very Large Data Base Endowment (PVLDB), 5(7):622­633, 2012.
[6] A. Bernardini, C. Carpineto, and M. D'Amico. Full-subtopic retrieval with keyphrase-based search results clustering. In Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT), pages 206­213, 2009.
[7] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993­1022, 2003.
[8] T. Brants and A. F. Web 1t 5-gram, 2006. [9] C. Carpineto, S. Osinski, G. Romano, and D. Weiss. A
survey of web clustering engines. ACM Computer Survey, 41(3):1­38, 2009. [10] C. Carpineto and G. Romano. Optimal meta search results clustering. In 33rd International ACM SIGIR Conference on

Research and Development in Information Retrieval (SIGIR), pages 170­177, 2010. [11] K. Church and P. Hanks. Word association norms mutual information and lexicography. Computational Linguistics, 16(1):23­29, 1990. [12] A. Di Marco and R. Navigli. Clustering web search results with maximum spanning trees. In Proceedings of the 12th International Conference on Artificial Intelligence Around Man and Beyond (AI*AI), pages 201­212, 2011. [13] A. Di Marco and R. Navigli. Clustering and diversifying web search results with graph-based word sense induction. Computational Linguistics, 39(4):1­43, 2013. [14] L. Dice. Measures of the amount of ecologic association between species. Journal of Ecology, 26:297­302, 1945. [15] T. Dunning. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61­74, 1993. [16] P. Ferragina and A. Gulli. A personalized search engine based on web-snippet hierarchical clustering. Software: Practice and Experience, 38(2):189­225, 2008. [17] W. Gale and K. Church. Concordances for parallel texts. In Proceedings of the 7th Annual Conference of the UW Center for the New OED and Text Research, Using Corpora, pages 40­62, 1991. [18] M. Hearst and J. Pedersen. Re-examining the cluster hypothesis: Scatter/gather on retrieval results. In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 76­84, 1996. [19] W. Kong and J. Allan. Extracting query facets from search results. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 93­102, 2013. [20] T. Landauer and S. Dumais. A solution to plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, pages 211­240, 1997. [21] S. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28(2):129­137, 1982. [22] Y. Maarek, R. Fagin, I. Ben-Shaul, and D. Pelleg. Ephemeral document clustering for web applications. Technical report, IBM, 2000. [23] A. K. McCallum. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu, 2002. [24] D. Milne and I. Witten. An open-source toolkit for mining wikipedia. Journal of Artificial Intelligence, 194:222­239, 2013. [25] J. Moreno and G. Dias. Using text-based web image search results clustering to minimize mobile devices wasted space-interface. In Proceedings of 35th European Conference on Information Retrieval (ECIR), pages 532­544, 2013. [26] J. Moreno, G. Dias, and G. Cleuziou. Post-retrieval clustering using third-order similarity measures. In Proceedings of the 51st Annual Meeting of the Association of Computational Linguisitcs (ACL), pages 153­158, 2013. [27] R. Navigli and G. Crisafulli. Inducing word senses to improve web search result clustering. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 116­126, 2010. [28] R. Navigli and D. Vannella. Semeval-2013 task 11: Word sense induction & disambiguation within an end-user

785

application. In Proceedings of the International Workshop on Semantic Evaluation (SEMEVAL), pages 1­9, 2013.
[29] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In Proceedings of the 15th Neural Information Processing Systems Conference (NIPS), pages 849­856, 2001.
[30] S. Osinski and D. Weiss. A concept-driven algorithm for clustering search results. IEEE Intelligent Systems, 20(3):48­54, 2005.
[31] P. Pecina and P. Schlesinger. Combining association measures for collocation extraction. In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL), pages 651­658, 2006.
[32] T. Sakai, Z. Dou, T. Yamamoto, Y. Liu, M. Zhang, M. Kato, R. Song, and M. Iwata. Summary of the ntcir-10 intent-2 task: Subtopic mining and search result diversification. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 761­764, 2013.
[33] T. Sakai, Z. Dou, T. Yamamoto, M. Lui, Y. Zhang, and R. Song. Overview of the ntcir-10 intent-2 task. In Proccedings of the Research Infrastructure for Comparative Evaluation of Information Retrieval and Access Technologies (NTCIR), 2013.
[34] T. Sakai and R. Song. Evaluating diversified search results using per-intent graded relevance. In Proceedings of the 34th international ACM conference on Research and development in Information Retrieval (SIGIR), pages 1043­1052, 2011.

[35] U. Scaiella, P. Ferragina, A. Marino, and M. Ciaramita. Topical clustering of search results. In 5th ACM International Conference on Web Search and Data Mining (WSDM), pages 223­232, 2012.
[36] J. Silva, G. Dias, S. Guilloré, and J. Lopes. Using localmaxs algorithm for the extraction of contiguous and non-contiguous multiword lexical units. In Proceedings of 9th Portuguese Conference in Artificial Intelligence (EPIA), pages 113­132, 1999.
[37] N. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clusterings comparison: Is a correction for chance necessary? In Proceedings of the 26th Annual International Conference on Machine Learning (ICML), pages 1073­1080, 2009.
[38] D. Vitale, P. Ferragina, and U. Scaiella. Classification of short texts by deploying topical annotations. In 34th European Conference on Advances in Information Retrieval (ECIR), pages 376­387, 2012.
[39] X. Wang, W. Gu, D. Ziebelin, and H. Hamilton. An ontology-based framework for geospatial clustering. International Journal of Geogaphical Information Science, 24(11):1601­1630, 2010.
[40] O. Zamir and O. Etzioni. Web document clustering: A feasibility demonstration. In 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 46­54, 1998.
[41] D. Zhang and Y. Dong. Semantic, hierarchical, online clustering of web search results. In Proceedings of the 6th Asia Pacific Web Conference (APWEB), pages 69­78, 2004.

786

Search Result Diversification via Data Fusion
Shengli Wu and Chunlan Huang
School of Computer Science and Telecommunication Engineering Jiangsu University, Zhenjiang, China 212013
swu@ujs.edu.cn, palaceo77@163.com

ABSTRACT
In recent years, researchers have investigated search result diversification through a variety of approaches. In such situations, information retrieval systems need to consider both aspects of relevance and diversity for those retrieved documents. On the other hand, previous research has demonstrated that data fusion is useful for improving performance when we are only concerned with relevance. However, it is not clear if it helps when both relevance and diversity are both taken into consideration. In this short paper, we propose a few data fusion methods to try to improve performance when both relevance and diversity are concerned. Experiments are carried out with 3 groups of top-ranked results submitted to the TREC web diversity task. We find that data fusion is still a useful approach to performance improvement for diversity as for relevance previously.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ retrieval models
General Terms
Algorithms, Experimentation, Measurement, Performance
Keywords
Search result diversification, data fusion, linear combination, weight assignment
1. INTRODUCTION
In recent years, researchers have taken various approaches to investigate search result diversification [3, 1]. In such situations, information retrieval systems need to consider both relevance and diversity for those retrieved documents. In this short paper, we aim to find out if and how data fusion can help with this.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609451.

Previous research on data fusion (such as in [2, 6, 8]) demonstrates that it is possible to improve retrieval performance when we only consider relevance. Now with the new dimension of diversification, we need to re-evaluate the technology. In particular, some fusion methods need to be modified to accommodate for the new situation.
We may divide data fusion methods into two broad categories, according to how they deal with component results: equal-treatment and biased methods. As their names would suggest, the former treats all component results equally, while the latter does not. CombSum, CombMNZ, and the Condorcet method belong to the first category, while the linear combination method is a representative of the second category. Equal-treatment methods can likely be used in the new situation without modification, but the linear combination method needs more consideration.
In linear combination, weight assignment is a key issue for achieving good fusion performance and a considerable number of weight assignment methods have been proposed. Generally speaking, we need to consider two factors for weight assignment. One is the performance of every component retrieval system involved, and the other is the dissimilarity (or distance) between those component systems/results. For the information retrieval systems involved, well-performing systems should be given greater weights , while systems performing poorly should be assigned smaller weights. On the other hand, smaller weights should be assigned to those results that are similar to the others, while greater weights should be assigned to those results that are more different to the others. When assigning weights, we may take into consideration performance or dissimilarity, or even both together. It is also possible to use some machine learning techniques, known as "learning to rank", to train weights using some training data. This is especially popular for combining results at the feature level. These methods are aimed at optimizing a goal that is related to retrieval effectiveness measured by a given metric such as average precision. Because the metrics used for result diversification (e.g., ERR-IA@20) are very different from metrics such as average precision, almost all methods in this category cannot be directly used for result diversification.
In this paper, we are going to investigate data fusion methods, especially linear combination, for result diversification. Experiments are carried out to evaluate them with 3 groups of results submitted to the TREC web diversity task between 2009 and 2011. Experiments show that the proposed methods perform well and have the potential to be used for this purpose in practice.

827

Table 1: Information of 3 groups of results submitted to the web diversity task in TREC

TREC 2009

TREC 2010

TREC 2011

Run

ERR-IA@20 Run

ERR-IA@20 Run

ERR-IA@20

MSRAACSF

0.2144 qirdcsuog3

0.2051 ICTNET11DVR3 0.4764

MSDiv3

0.2048 THUIR10DvNov

0.3355 liaQEWikiAnA

0.2287

uogTrDYCcsB

0.1922 UAMSD10aSRfu

0.2423 msrsv2011d1

0.4994

UamsDancTFb1 0.1774 UCDSIFTDiv

0.2100 UAmsM705tFLS

0.4378

mudvimp

0.1746 UMd10IASF

0.2546 uogTrA45Nmx2

0.5284

UCDSIFTdiv

0.1733 uogTrB67xS

0.2981 UWatMDSqltsr

0.4939

NeuDiv1

0.1705 cmuWi10D

0.2484 uwBA

0.3986

THUIR03AbClu 0.1665 ICTNETDV10R2 0.3222 CWIcIA2t5b1

0.3487

Average

0.1842 Average

0.2645 Average

0.4265

Variance

0.0003 Variance

0.0024 Variance

0.0098

2. SEVERAL METHODS FOR RESULT

DIVERSIFICATION

As aforementioned, weight assignment is a key issue for

the linear combination method. In this section, we look at

different ways of dealing with this issue. Firstly, we may

consider the performance of the retrieval system in question

and its similarity with other retrieval systems separately, so

that we may obtain two types of weights. Then a combi-

nation of these two types of weights can be used to fuse

results. Note that performance and dissimilarity are two in-

dependent factors. On the one hand, performance of a result

concerns the ranking positions of relevant documents and

how diversified those relevant documents are in the ranked

list of documents; on the other hand, the dissimilarity of two

or more results is concerned with how different the ranking

positions of the same documents are in two or more indi-

vidual results, making no distinction between relevant and

non-relevant documents.

Suppose there are a group of information retrieval sys-

tems ir1, ir2,...,irt, with some training data available for us

to measure their performance and the dissimilarity between

them. We further assume that their performances are p1,

p2,..., pt, respectively, as measured by a given metric (e.g.,

ERR-IA@20), so that we may then assign the value of a function of pi (such as pi, pi2, and so on) to wi for (1  i  t),

as the performance-related weight of iri.

Different approaches are possible for calculating the dis-

similarity (or similarity) between two results. One approach

is to refer to each result as a set of documents and calculate

the overlap rate between two results (sets). It is also possi-

ble to calculate the correlation (such as Spearman's ranking

coefficient or Kendall's tau coefficient) of two ranked lists of

documents. If all the documents in the two results are as-

sociated with proper scoring information, then score-based

methods such as the Euclidean distance or city block dis-

tance can be used. In the following we discuss two different

ways of doing it.

Let us consider the top-n documents in all component re-

sults. Suppose that document dij, in result ri, appears or is referred to in cij of the other t - 1 results, then all n

top-ranked documents of ri are referred in all other results

refi =

n j=1

cij

times.

For each document dij, the maxi-

mum times it can appear in the other t - 1 results is t - 1,

This fact can be used to define the dissimilarity of ri to other

results as

1 disi = n

n

(t - 1 - cij ) t-1

(1)

j=1

disi are always in the range of 0 and 1. We may define disi or a function of them as the dissimilarity-related weights. Methods using this definition are referred to as reference-based methods later in this paper. One advantage of using such methods for dissimilarity is that we can obtain the weights for component systems by considering all the documents of them together.
An alternative of calculating the dissimilarity between results is to compare documents' ranking difference for each pair of them. Let us consider the n top-ranked documents in both results rA and rB. Suppose that m (m  n) documents appear in both rA and rB, and (n - m) of them appear in only one of them. For those n - m documents that only appear in one of the results, we simply assume that they occupy the places from rank n + 1 to rank 2n - m whilst retaining the same relative orders in the other result. Thus we can calculate the average rank difference of all the documents in both results and use it to measure the dissimilarity of rA and rB. To summarize, we have

v(rA, rB)

=

1

di
{

rA

di

rB

|pA(di) - pB(di)|

n

m

i=1,2,...,m

+

dirAdi/rB |pA(di) - (n + i)| n-m

i=1,2,...,n-m

+

di / rA di rB

|pB

(di) n

- -

(n m

+

i)|

}

(2)

i=1,2,...,n-m

Here pA(di) and pB(di) denote the rank position of di in

rA and rB, respectively.

1 n

is

the

normalization

coefficient

which guarantees that v(rA, rB) is in the range of 0 and 1.

Based on Equation 2, the dissimilarity weight of ri (1  i 

t) is defined as

1

j=i

disi = t - 1

v(ri, rj )

(3)

j=1,2,...,t

Methods that use this definition are referred to as ranking difference based methods. No matter how we obtain

828

the weights for dissimilarity, we may combine dissimilarity-
related weights with performance-related weights. Different options, such as pi disi, pi2 disi, pi disi2, and so on, might be used to obtain weight wi for information retrieval system iri. At the fusion stage, the linear combination method uses the following equation to calculate scores:

t

g(d) = wi  si(d)

(4)

i=1

where g(d) is the global score that document d obtains dur-
ing data fusion, si(d) is the (normalized) score that document d obtains from information retrieval system iri (1  i  t), and wi is the weight assigned to system iri. All the documents can be ranked according to the global scores they
obtain.

3. EXPERIMENTS
In the 3 successive years from 2009 to 2011, the web track of TREC used the collection of "ClueWeb09". The collection consists of roughly 1 billion web pages crawled from the Web.
3 groups of results are chosen for the experiment. They are 8 top-ranked results 1 (measured by ERR-IA@20) submitted to the diversity task in the TREC 2009, 2010, and 2011 web track. The information about all the selected results is summarized in Table 1.
As we know, it is harder to get improvement over better component results through data fusion. However, the purpose of the experiments is going to see if we can obtain even better results by fusing a number of top-ranked results submitted.
In the 3 aforementioned groups of results, the 2009 group has the lowest average effectiveness (.1842), the lowest best effectiveness (.2144), and the smallest variance (.0003); the 2011 group has the highest average effectiveness (.4265), the highest best effectiveness (.5284), and the largest variance (.0098); for all three metrics the 2010 group comes second (average is .2645, best is .3356, variance is .0024). We will see that the effectiveness of the fused results is affected by these factors.
In the 2009 group, M SRAACSF [4] is the best performer (ERR IA@20: 0.2144). This run is submitted by Microsoft Research Asia in Beijing. For a given query, sub-topics are mined from different sources including anchor texts, search result clusters, and web sites at which search results are located; and documents are ranked by considering both relevance and diversity of mined sub-topics.
In the 2010 group, T HU IR10DvN ov is the best among all 8 runs selected for the experiment. Its performance is 0.3355 when measured by ERR IA@20. Two other runs msrsv3div and uwgym (baseline) are slightly better than T HU IR10DvN ov. The technical details of this run are not known because we cannot find the corresponding report for this. We speculate that this run is submitted by a research group in Tsinghua University.
In the 2011 group, uogT rA45N mx2 [7] is the best performer (ERR IA@20: 0.5284). This run is submitted by the
1msrsv3div and uwgym in 2010 and UDCombine2 in 2011 are not chosen because they include much fewer documents than the others and using them would cause problems in calculating weights for the linear combination method and in the fusion process as well.

Table 2: Performance (measured by ERR-IA@20) of a group of data fusion methods (p denotes performance-related weight and dis denotes dissimilarity-related weight; dis is calculated using either Equation 1 or Equation 3; the figures in parentheses indicate the improvement rate of each method over the best component result; the figures in bold indicate the highest value in the column)

Group best result p p2
dis  p(Eq.1) dis  p2(Eq.1) dis2  p(Eq.1)
dis  p(Eq.3) dis  p2(Eq.3) dis2  p(Eq.3)

2009 0.2144 0.2544 (18.66%) 0.2499 (16.56%) 0.2552 (19.03%) 0.2492 (16.23%) 0.2548 (18.84%) 0.2553 (19.08%) 0.2503 (16.74%) 0.2534 (18.19%)

2010 0.3355 0.3567 (6.32%) 0.3684 (9.81%) 0.3548 (5.75%) 0.3705 (10.43%) 0.3533 (5.31%) 0.3531 (5.25%) 0.3658 (9.03%) 0.3562 (6.17%)

2011 0.5284 0.5398 (2.16%) 0.5343 (1.12%) 0.5398 (2.16%) 0.5355 (1.34%) 0.5410 (2.38%) 0.5388 (1.97%) 0.5347 (1.19%) 0.5330 (0.87%)

Ave. 0.3551 0.3836 9.05% 0.3842 9.16% 0.3833 8.60% 0.3851 9.33% 0.3830 8.84% 0.3824 8.77% 0.3836 8.99% 0.3809 8.41%

IR research group at Glasgow University. It uses Terrier

with a component xQuAD for search result diversification.

The primary idea is to find useful information of sub-topics

by sending the initial query to three commercial web search

engines.

In each year group, 50 queries are divided into 5 groups:

1-10, 11-20, 21-30, 31-40, and 41-50. 4 arbitrary groups

of them are used as training queries, while the remaining

one group is used for fusion test. This is referred to as the

five-fold cross validation method in statistics and machine

learning [5]. Every result is evaluated using ERR-IA@20

over training queries to obtain the performance weight pi.

On the other hand, either Equation 1 or Equations 2 and 3

are used with training data to obtain disi for the dissimilar-

ity weight. After that, we try 5 different ways of combining the weights: pi, pi2, pi  disi, pi2  disi, and pi  disi2.

In order to fuse component results by linear combination,

reliable scores are required for all the documents required.

In this study, we use the reciprocal function [2]. According

to [2], the reciprocal function is very good for converting

rankings into scores. For any resultant list r = <d1, d2,...,

dn>,

a

score

of

1 i+60

is

assigned

to

document

di

at

rank

i.

Experimental results are shown in Tables 2 and 3. Two

metrics, ERR-IA@20 and -nDCG@20, are used to evaluate

all the fusion methods. The best component result is used

as the baseline. When calculating dissimilarity weights by

reference based method, or Equation 1, we use the top 100

documents in all component results. We have also tried

some other options, including the top 50 and the top 200,

though the experimental results are omitted here since they

are so similar to what we observed for the top 100. When

using rank difference based method, or Equations 2 and 3,

829

Table 3: Performance (measured by -nDCG@20) of a group of data fusion methods (p denotes performance-related weight and dis denotes dissimilarity-related weight; dis is calculated using either Equation 1 or Equation 3; the figures in parentheses indicate the improvement rate of each method over the best component result; the figures in bold indicate the highest value in the column)

Group best result p p2
dis  p(Eq.1) dis  p2(Eq.1) dis2  p(Eq.1)
dis  p(Eq.3) dis  p2(Eq.3) dis2  p(Eq.3)

2009 0.3653 0.4130 (13.06%) 0.4108 (12.46%) 0.4141 (13.36%) 0.4100 (12.24%) 0.4150 (13.61%) 0.4138 (13.28%) 0.4108 (12.46%) 0.4126 (12.95%)

2010 0.4745 0.5071 (6.87%) 0.5226 (10.14%) 0.5057 (6.58%) 0.5241 (10.45%) 0.5045 (6.32%) 0.5054 (6.51%) 0.5202 (9.63%) 0.5084 (7.14%)

2011 0.6298 0.6510 (3.37%) 0.6468 (2.70%) 0.6513 (3.41%) 0.6477 (2.84%) 0.6522 (3.56%) 0.6506 (3.30%) 0.6478 (3.00%) 0.6488 (3.17%)

Ave. 0.4869 0.5237 7.77% 0.5267 8.43% 0.5237 7.78% 0.5273 8.51% 0.5239 7.83% 0.5233 7.70% 0.5263 8.36% 0.5233 7.75%

to calculate dissimilarity weights, we use all the documents in each component result.
From Tables 2 and 3, we can see that all the data fusion methods involved perform better than the best component result. However, improvement rates vary from one year group to another. For all the data fusion methods involved, the largest improvement of over 10% occurs in the 2009 year group, which is followed by the 2010 year group with improvement between 5% and 11%, while the smallest improvement of less than 4% occurs in the 2011 year group. According to [8], the target variable of performance improvement of the fused result over the best component result is affected by a few factors. Among other factors, the variance of performance of all the component results and the performance of the best component result (see Table 1) have negative effect on the target variable. This can partially explain what we observe: all data fusion methods do the best in the 2009 data set, the worst in the 2011 data set, and the medium in the 2010 data set.
Intuitively, such a phenomenon is understandable. If a component result is very good and a large percentage of relevant documents in multiple categories are retrieved and top-ranked, then it must be very difficult to make any further improvements over this result; on the other hand, if some of the results are much poorer than the others, then it is very difficult for the fused result to outperform the best component result. Anyway, in all 3 data sets, all of the fused results exhibit improvements over the best component result.
If we compare performance-related weights to combined weights, it is not always the case that combined weights can achieve greater improvement. However, if we examine the greatest improvement in each case, it always happens

when some form of combined weights is used. On average over three year groups, dis  p2(Eq.1) performs the best no matter if ERR-IA@20 or -nDCG@20 is used for evaluation. This suggests that dis  p2(Eq.1) is a very good option for the combined weight.
4. CONCLUSIONS
In this short paper we have reported our investigation on the search result diversification problem via data fusion. Especially we focus on the linear combination method. Two options of calculating dissimilarity weights and several options of combining performance-related weights and dissimilarity-related weights have been proposed. Experiments with 3 groups of results submitted to the TREC web diversity task show that all the data fusion methods perform well and better than the best component result. Among those methods proposed, a combined weight of square performance and dissimilarity (calculated by comparing ranking difference of pair-wise results) outperforms the others on average.
In summary, the experiments demonstrate that data fusion is still a useful technique for performance improvement when addressing search result diversification.
5. REFERENCES
[1] E. Aktolga and J. Allan. Sentiment diversification with different biases. In Proceedings of the 36th Annual International ACM SIGIR Conference, pages 593­602, Dublin, Ireland, July 2013.
[2] G. V. Cormack, C. L. A. Clarke, and S. Bu¨ttcher. Reciprocal rank fusion outperforms Condorcet and individual rank learning mthods. In Proceedings of the 32nd Annual International ACM SIGIR Conference, pages 758­759, Boston, MA, USA, July 2009.
[3] V. Dang and W. B. Croft. Term level search result diversification. In Proceedings of the 36th Annual International ACM SIGIR Conference, pages 603­612, Dublin, Ireland, July 2013.
[4] Z. Dou, K. Chen, R. Song, Y. Ma, S. Shi, and J. Wen. Microsoft Research Asia at the web track of TREC 2009. In Proceedings of The Eighteenth Text REtrieval Conference, Gaithersburg, Maryland, USA, November 2009.
[5] R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (Volumn 2), pages 1137­1145, Montreal, Canada, August 1995.
[6] J. H. Lee. Analysis of multiple evidence combination. In Proceedings of the 20th Annual International ACM SIGIR Conference, pages 267­275, Philadelphia, Pennsylvania, USA, July 1997.
[7] R. McCreadie, C. Macdonald, R. Santos, and I. Ounis. University of Glasgow at TREC 2011: Experiments with terrier in crowdsourcing, microblog, and web tracks. In Proceedings of The Twentieth Text REtrieval Conference, Gaithersburg, Maryland, USA, November 2011.
[8] S. Wu and S. McClean. Performance prediction of data fusion for information retrieval. Information Processing & Management, 42(4):899­915, July 2006.

830

Using the Cross-Entropy Method to Re-Rank Search Results

Haggai Roitman, Shay Hummel
IBM Research - Haifa Haifa 31905, Israel
{haggai,shayh}@il.ibm.com

Oren Kurland
Faculty of Industrial Engineering and Management, Technion Haifa 32000, Israel
kurland@ie.technion.ac.il

ABSTRACT
We present a novel unsupervised approach to re-ranking an initially retrieved list. The approach is based on the Cross Entropy method applied to permutations of the list, and relies on performance prediction. Using pseudo predictors we establish a lower bound on the prediction quality that is required so as to have our approach significantly outperform the original retrieval. Our experiments serve as a proof of concept demonstrating the considerable potential of the proposed approach. A case in point, only a tiny fraction of the huge space of permutations needs to be explored to attain significant improvements over the original retrieval.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval] Retrieval Models
General Terms: Algorithms, Experimentation
Keywords: Re-ranking, Optimization, Performance Prediction
1. INTRODUCTION
We present a novel unsupervised approach to the challenge of re-ranking a document list that was retrieved in response to a query so as to improve retrieval effectiveness. The approach utilizes a Monte-Carlo-based optimization method, named the Cross Entropy (CE) method [13], which is applied to permutations of the list. The approach relies on a retrieval performance predictor that can be applied to any ranking of the list.
Given the reliance on performance prediction, we present a novel pseudo predictor that enables to fully control the prediction quality. We use the pseudo predictor in our approach to set a lower bound on the prediction quality that is needed so as to have our approach significantly outperform the initial ranking. Further empirical evaluation provides a proof of concept for our approach. Specifically, via the exploration of a tiny fraction of the huge space of all possible permutations, our approach finds highly effective permutations. The retrieval effectiveness of these permutations is substantially, and statistically significantly better, than that of the original ranking of the list.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609454.

2. RELATED WORK
The Cross Entropy (CE) method [13] that is used in our approach is a Monte Carlo framework for rare event estimation and combinatorial optimization. The CE method has been previously applied in many domains such as machine learning, simulation, networks, etc. [13]. To the best of our knowledge, our work is the first to use the CE method in the information retrieval domain.
Our approach relies on predicting the retrieval performance of permutations of a document list. Applying performance prediction to select one of two retrieved lists was explored in some work [2, 6, 3, 11]. However, the conclusions regarding the resultant effectiveness of using the proposed predictors were inconclusive. In contrast, we do not present a concrete predictor. Rather, we devise a pseudo predictor that enables to control prediction quality, and accordingly set a lower bound on the prediction quality required so as to have our approach outperform the initial ranking.
Using a simulation study, a lower bound on the prediction quality required for effective selective query expansion was set [8]. While this work focused on performance prediction over queries, our approach relies on prediction over rankings for the same query. Furthermore, our approach is not committed to any ranking paradigm. In addition, rather than use a simulation, we propose a novel pseudo predictor that allows to control prediction quality.
Finally, we note that some list-wise learning to rank approaches [10] are also based on finding effective permutations of the same list, although not with the Cross Entropy method that we employ. Permutations are explored during the training phase and a ranker is induced. In contrast, our approach employs optimization over permutations as an unsupervised re-ranking mechanism.
3. FRAMEWORK
3.1 Problem Definition
Let Dqk denote the list of the k documents in a corpus D that are the most highly ranked by some initial search performed in response to query q. Let Dqk denote the set of all k! possible permutations (rankings) of the documents in Dqk. Let   Dqk denote a single permutation of Dqk and let (d) further denote the position (rank) of document d ( Dqk) in . Let Q() denote the retrieval performance (effectiveness) of the permutation  ( Dqk ).
The goal is to find a permutation  ( Dqk ) such that Q() is maximized. Unfortunately, finding an optimal per-

839

mutation is NP-Complete [1]. In addition, with no prior relevance judgement on the documents in Dqk, the true performance Q() for any given permutation   Dqk is unknown. Hence, the performance of any given permutation can only be predicted, and the task becomes even more challenging.
Let Q() denote the predicted performance of the permutation  ( Dqk ). Potential predictors may utilize any available pre-retrieval features [9] (e.g., induced from the query q and the corpus D), post-retrieval features (e.g., induced from the result list Dqk or the permutation ) or their combination [4].
3.2 An Optimization Approach
We next propose an optimization approach that effectively finds "promising" permutations which have the best
predicted performance according to a given predictor Q(). Since the resultant retrieval effectiveness of the optimiza-
tion procedure depends on the prediction quality of the predictor employed, we empirically derive a lower bound for the prediction quality of "effective" predictors. That is, if a predictor with a prediction quality higher than the lower bound is used in our approach then the approach is guaranteed to find -- as determined based on the benchmarks we have used -- as a solution a permutation  whose retrieval performance is better than that of the initial ranking.
3.2.1 Optimization using the Cross Entropy method
We propose a Monte-Carlo optimization approach to our permutation-based re-ranking task. The approach, which we term Cross Entropy Re-ranking Optimization (CERO), uses the Cross Entropy (CE) method [13]. Within the CE method, optimal solutions to hard problems (such as that we try to solve) are modeled as rare events whose occurrence probability is effectively estimated [13]. Given such estimates, optimal solutions can then be efficiently generated. Under a certain condition, the CE method is expected to converge to the optimal solution [12].
We now describe our algorithm and provide its pseudo code in Algorithm 1. The algorithm gets as an input the
initial ranked list Dqk , a performance predictor Q () and several tuning parameters (mentioned below) that control the learning rate and convergence of the algorithm. The algorithm iteratively explores permutations in Dqk using random sampling. To this end, the algorithm induces a probability space of "promising" permutations over Dqk using the feedback it gets about the relative performance of permutations that were explored in previous iterations. It is easy to show that for a given k, a unique bijection exists between the permutation set Dqk and the set of all k! possible Hamiltonian paths in a complete graph with k nodes. Therefore, random permutations can be efficiently drawn by sampling Hamiltonian paths using a simple constrained random walk method [13]. Let P(ti,j) denote the the probability for a single step transition between node i and node j in the graph, which corresponds to the event (dj) = (di) + 1, i.e., document di is ranked in  one position before document dj. With no prior knowledge on the permutations probability space, the algorithm is initialized with the uniform probability (having the maximum entropy) and Dqk is considered as the current best performing permutation. The algorithm's

Algorithm 1 Cross Entropy Re-ranking Optimization

1: input: Dqk , Q () , N, , 

2: initialize:

3: P(0i,j) =

1 k-1

,

i=j

0,

i=j

4:  = Dqk 5: t = 1 6: loop 7: Randomly draw N permutations l  Dqk using P t-1

8: if Q () < maxl=1,...,N Q (l) then

9:

 = arg maxl=1,...,N Q(l)

10: end if

11: Sort permutations l according to Q (l) 12: Let t be the sample (1-)-quantile of the performances:

Q (l);l = 1, .., N
13: for i = 1, . . . , k; j = 1, . . . , k do

14:

P(ti,j ) =

N l=1

I {l (dj

)=l (di )+1}I {Q(l )t }

N l=1

I {Q(l )t }

15:

P(ti,j ) =P(ti-,j1) +(1-)P(ti,j )

16: end for

17: if t converged then

18:

stop and return 

19: else

20:

t=t+1

21: end if

22: end loop

goal is to converge (via cross entropy minimization) to the unknown probability space of optimal permutations, from which optimal solutions can be generated [13].
On each iteration t, N random permutations are sampled based on the last induced promising permutations probability space P t-1. Next, the predicted performance of each sampled permutation is calculated and the performance of the current best performing permutation  is updated accordingly. "New" promising permutations are then explored by first sorting the sampled permutations according to their predicted performance and then updating the transition probabilities based on the top-N  performing samples, whose minimum performance is t. Given t, each transition probability P(ti,j) is induced according to the relative number of permutations out of the top-N  permutations (which have predicted performance equal to or higher than t) that also ranked document di one position above document dj. A fixed smoothing scheme, controlled by parameter , further allows to trade between the exploration (given by P(ti,j)) and the exploitation (given by P(ti-,j1)) of the algorithm.
The algorithm continuous until some convergence criteria is met. In this work we follow the convergence criteria suggested in [13] and stop the algorithm if the sample (1 - )-performance quantile t does not change within several consecutive iterations.
3.2.2 A criterion for effective prediction
The CERO algorithm is generic and can employ any performance predictor. Naturally, however, the prediction quality of the predictor has significant impact on the retrieval effectiveness of the ranking produced by the algorithm.
Thus, we turn to devise a method for determining the lower bound of prediction quality that will result in the CERO algorithm outperforming the initial ranking of Dqk. The bound is independent of a prediction approach.
Herein, we measure retrieval performance using average precision (AP@k); i.e., Q() in our case is the AP of the per-

840

corpus
GOV2 WT10G TREC8

# of documents
25,205,179 1,692,096 528,155

queries
701-850 451-550 401-450

disks
GOV2 WT10g 4&5-{CR}

Table 1: TREC data used for experiments.

mutation . Following standard practice in work on queryperformance prediction [4], prediction quality is measured by the Pearson correlation between the true AP of permu-
tations (Q()) and their predicted performance (Q()). To derive a lower bound on prediction quality, we next
present an approach for generating pseudo AP predictors, whose prediction quality can be controlled. Following previous observations [5], we assume that true AP values follow a normal distribution1.
We first normalize the AP of the permutation  ( Dqk ):

Qnorm()

=

Q() - EDqk

(AP ) ;

(1)

V arDqk (AP )

EDqk (AP ) and V arDqk (AP ) are the expectation and the variance of the true AP values of the permutations in Dqk , respectively. The two statistics can be estimated using max-
imum likelihood estimation for normal distribution, by sam-
pling a large enough random (uniform) sample of permuta-
tions in Dqk (e.g., N = 1000). Since AP follows a normal distribution, we get that as k  , for any permutation
  Dqk : Qnorm()  N (0, 1) [5]. Proposition 1 defines a -correlated pseudo AP predictor;
that is, a predictor with a  prediction quality (i.e., Pearson
correlation with true AP). The proof is quite straightforward
and is ommitted due to space considerations.

Proposition 1. Given a query q, initial result list Dqk, and permutation   Dqk , a -correlated pseudo AP predic-
tor, denoted Q(), is obtained as follows:

Q() = Qnorm() + 1 - 2X

(2)

where Qnorm() is the normalized true AP value according to Eq. 1 and X  N (0, 1).

4. EVALUATION
4.1 Setup
The TREC corpora and queries used for experiments are specified in Table 1. Titles of TREC topics served for queries. The Apache Lucene2 search library (version 4.3) was used for the experiments. Documents and queries were processed using Lucene's default analysis (i.e., tokenization, stemming, stopwords, etc). For each query, 100 documents were retrieved using Lucene's implementation, employed with default free-parameter values, of each of the following retrieval methods: vector space model (TF-IDF), query-likelihood (QL with Dirichlet smoothing) and Okapi BM25. Thus, we
1The assumption was further verified in our experiments using the 2 goodness-of-fit test. Details are omitted due to space considerations. 2http://lucene.apache.org

obtained three initial lists, Dqk, composed of k = 100 documents, for each query. Mean average precision (MAP@k) serves as the evaluation measure. Statistically significant differences of performance are measured using the paired t-test with a 95% confidence level.
Each initial list was re-ranked using the CERO algorithm employed with pseudo AP predictors of varying prediction quality levels. To control prediction quality, the pseudo predictors were generated according to Eq. 2; the prediction quality was varied from  = 0.05 (worst predictor) to  = 1.0 (best predictor). Following previous recommendations [13], the algorithm's learning parameters were set as follows: N = 1000,  = 0.01 and  = 0.7.
Eff ciency considerations. To implement CERO, we used
a parallelized version of the Cross Entropy method [7]. On average, CERO converged in 21.32 iterations (with a 15.6 standard deviation). CERO explores at each iteration a maximum of N = 1000 permutations. Thus, all together, CERO considered only about 21k - 36k permutations out of the 100! possible permutations of the initial list.
4.2 Results
CERO's effectiveness. We first study the potential of our
permutation-based optimization approach; specifically, in finding highly effective permutations in the huge space (100!) of permutations. To this end, we neturilize the effect of prediction quality by applying CERO with a "predictor" which reports the true AP of the considered permutations (i.e.,  = 1.0). The resultant MAP performance is presented in Table 2. As reference comparisons we use the initial ranking and an optimal re-ranking where all relevant documents from the initial list are positioned at the highest ranks.
We can see in Table 2 that, overall, CERO results in very good approximations. Specifically, CERO's MAP is at least as 91% as good as that of the optimal MAP. Recall from above that CERO explores only a tiny fraction of all permutations of the documents in the result list. It is worth noting that an even better approximation may be obtained by finer tuning of the algorithm (e.g., following [12]). We leave this exploration for future work, and view the results presented in Table 2 as a solid proof of concept for the optimization approach we have employed.
The effect of prediction quality. In Table 3 we present
the effect of the prediction quality of the pseudo AP predictors used in CERO on its MAP retrieval performance. Evidently, and as should be expected, the higher the prediction quality (), the better the performance. Furthermore, as from  = 0.3 CERO improves over the initial ranking for all the retrieval methods and across all corpora; for   0.35 the improvements are always statistically significant. With higher prediction quality, the improvements over the initial ranking become very substantial.
5. CONCLUSIONS AND FUTURE WORK
We presented a novel approach to re-ranking an initially retrieved list. The approach is based on applying the Cross Entropy optimization method [13] upon permutations of the list. Query-performance predictors are used to evaluate the performance of permutations. Empirical evaluation pro-

841

Initial Optimal CERO ( = 1.0)

BM25
.151 .274 .251 (92%)

GOV2 QL
.172 .296 .275 (93%)

TF-IDF
.137 .253 .232 (91%)

BM25
.156 .352 .320 (91%)

WT10G QL
.164 .391 .367 (94%)

TF-IDF
.158 .349 .322 (92%)

BM25
.198 .400 .367 (92%)

TREC8 QL
.206 .407 .373 (92%)

TF-IDF
.190 .388 .356 (92%)

Table 2: The MAP of the initial retrieval, optimal re-ranking and the CERO algorithm when employed with the true AP as the "predictor". The percentages are with respect to Optimal.

Initial  = 1.0  = .95  = .90  = .85  = .80  = .75  = .70  = .65  = .60  = .55  = .50  = .45  = .40  = .35  = .30  = .25  = .20  = .15  = .10  = .05

BM25
.151 .251 (66%) .249 (65%) .245 (62%) .242 (60%) .237 (57%) .232 (54%) .222 (47%) .222 (47%) .215 (43%) .207 (37%) .199 (32%) .192 (27%) .182 (20%) .170 (12%) .161 (6%)
.154 (2%) .143 (-5%) .134 (-11%) .120 (-20%) .119 (-21%)

GOV2
QL
.172 .275 (60%) .272 (58%) .267 (56%) .262 (53%) .255 (49%) .248 (44%) .245 (42%) .231 (35%) .229 (33%) .219 (28%) .214 (24%) .202 (18%) .194 (13%) .180 (5%)
.173 (1%) .165 (-4%) .150 (-13%) .144 (-16%) .141 (-18%) .137 (-20%)

TF-IDF
.137 .232 (69%) .231 (69%) .225 (64%) .225 (64%) .218 (59%) .212 (55%) .208 (52%) .203 (48%) .198 (45%) .194 (42%) .181 (32%) .173 (27%) .172 (25%) .157 (15%)
.144 (5%) .135 (-2%) .125 (-8%) .120 (-12%) .117 (-14%) .110 (-20%)

BM25
.156 .320 (105%) .320 (105%) .320 (104%) .319 (104%) .311 (99%) .309 (97%) .301 (93%) .301 (92%) .292 (87%) .290 (85%) .280 (79%) .260 (66%) .257 (64%) .211 (35%) .199 (27%) .172 (10%)
.151 (-3%) .134 (-14%) .110 (-30%) .79 (-50%)

WT10G
QL
.164 .367 (124%) .362 (121%) .361 (120%) .354 (115%) .355 (116%) .342 (108%) .345 (110%) .333 (103%) .328 (100%) .312 (90%) .303 (85%) .272 (66%) .258 (57%) .246 (50%) .222 (35%)
.177 (8%) .139 (-15%) .124 (-24%) .119 (-28%) .93 (-43%)

TF-IDF
.158 .322 (104%) .321 (104%) .317 (101%) .317 (101%) .313 (99%) .309 (96%) .303 (93%) .296 (88%) .293 (86%) .285 (81%) .284 (81%) .260 (65%) .249 (58%) .225 (43%) .213 (35%)
.172 (9%) .130 (-18%) .112 (-29%) .105 (-33%) .103 (-35%)

BM25
.198 .367 (85%) .364 (83%) .361 (82%) .358 (81%) .354 (79%) .348 (76%) .342 (72%) .332 (67%) .325 (64%) .311 (57%) .303 (53%) .285 (44%) .267 (35%) .244 (23%) .221 (11%)
.201 (1%) .167 (-16%) .145 (-27%) .123 (-38%) .115 (-42%)

TREC8
QL
.206 .373 (81%) .371 (80%) .370 (80%) .364 (77%) .362 (76%) .358 (74%) .345 (68%) .344 (67%) .332 (62%) .321 (56%) .310 (51%) .293 (42%) .268 (30%) .248 (21%) .227 (10%) .194 (-6%) .159 (-22%) .148 (-28%) .133 (-35%) .122 (-41%)

TF-IDF
.190 .356 (87%) .356 (87%) .354 (86%) .348 (83%) .342 (80%) .339 (78%) .330 (73%) .322 (70%) .317 (67%) .304 (60%) .294 (54%) .278 (46%) .259 (36%) .237 (24%) .212 (12%) .179 (-6%) .165 (-13%) .139 (-27%) .120 (-37%) .109 (-43%)

Table 3: The MAP of the initial retrieval and of CERO when using different -correlated pseudo AP predictors. marks a statistically significant improvement over the initial ranking. Numbers in italics are lower than those for the initial ranking. The reported percentages are with respect to the initial ranking.

vided a proof of concept for our approach. That is, the optimization procedure finds highly effective permutations by exploring only a tiny fraction of the space of all possible permutations. In addition, we devised novel pseudo predictors that allow to carefully control prediction quality and to infer the minimal prediction quality required for our approach to (significantly) outperform the original ranking.
Our main plan for future work is devising query-performance predictors that yield a prediction quality that is higher than that we established as a lower bound for effective application of our approach. We note that almost all previously proposed query-performance predictors [4] are not suited for this task as they operate over different queries rather than over different lists retrieved for the same query.
Acknowledgment
We would like to thank David Carmel for discussions on an earlier version of this work. Oren Kurland's work is supported in part by the Israel Science Foundation under grant no. 433/12 and by a Google faculty research award.
6. REFERENCES
[1] N. Alon. Ranking tournaments. SIAM Journal on Discrete Mathematics, 20(1):137­142, 2006.
[2] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. of ECIR, pages 127­137, 2004.
[3] N. Balasubramanian and J. Allan. Learning to select rankers. In Proc. of SIGIR, pages 855­856, 2010.
[4] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2010.

[5] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In Proc. of SIGIR, pages 268­275, 2006.
[6] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A language modeling framework for selective query expansion. Technical Report IR-338, Center for Intelligent Information Retrieval, University of Massachusetts, 2004.
[7] G. E. Evans, J. M. Keith, and D. P. Kroese. Parallel cross-entropy optimization. In Proc. of WSC, pages 2196­2202, 2007.
[8] C. Hauff and L. Azzopardi. When is query performance prediction effective? In Proc. of SIGIR, pages 829­830, 2009.
[9] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In Proc. of CIKM, pages 1419­1420, 2008.
[10] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.
[11] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.
[12] L. Margolin. On the convergence of the cross-entropy method. Annals of Operations Research, 134(1):201­214, 2005.
[13] R. Y. Rubinstein and D. P. Kroese. The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning. Springer, 2004.

842

Old Dogs Are Great at New Tricks: Column Stores for IR Prototyping

Hannes Mühleisen,1 Thaer Samar,1 Jimmy Lin,2 and Arjen de Vries1
1 Centrum Wiskunde & Informatica, Amsterdam, The Netherlands 2 University of Maryland, College Park, Maryland, USA
{hannes|samar}@cwi.nl, jimmylin@umd.edu, arjen@acm.org

ABSTRACT
We make the suggestion that instead of implementing custom index structures and query evaluation algorithms, IR researchers should simply store document representations in a column-oriented relational database and implement ranking models using SQL. For rapid prototyping, this is particularly advantageous since researchers can explore new scoring functions and features by simply issuing SQL queries, without needing to write imperative code. We demonstrate the feasibility of this approach by an implementation of conjunctive BM25 using two modern column stores. Experiments on a web collection show that a retrieval engine built in this manner achieves effectiveness and efficiency on par with custom-built retrieval engines, but provides many additional advantages, including cleaner query semantics, a simpler architecture, built-in support for error analysis, and the ability to exploit advances in database technology "for free".
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation
Keywords: Relational Databases; BM25
1. INTRODUCTION
Information retrieval researchers and practitioners have long implemented specialized, custom-built data structures and query evaluation algorithms for document ranking [15]. Today, these techniques can be quite complex, especially with "structured queries" that span multiple nested clauses with a panoply of query operators [11]. We revisit the idea that the information retrieval community can (and should!) make use of general-purpose data management solutions, instead of building specialized backend technology. We demonstrate that storing posting lists as relations and expressing ranking models as SQL queries is now a viable alternative to custom inverted indexes if we leverage modern column-oriented relational databases (or `column stores').
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609460.

The contribution of this work is to demonstrate empirically that we can now safely delegate the data management needs of IR engines to modern column stores without concerns regarding efficiency. We show experimentally that an IR engine built in this manner achieves not only effectiveness and efficiency on par with custom-built retrieval engines, but provides many additional advantages. This suggests a somewhat radical message: IR researchers should stop writing their own retrieval engines and just use column stores. We advocate this approach especially for rapid prototyping-- when developing new scoring models, features functions, etc.--but such an architecture has additional advantages. Although we are not the first to make this claim, there have been a number of developments since previous work that make our design more attractive than before.
2. BACKGROUND AND RELATED WORK
What advantages does using a database for IR have? We see many, beginning with a precise formal framework. As more complex query operators are introduced for document ranking, it sometimes becomes unclear how to properly score documents, particularly in corner cases. Developers often resort to heuristics and other shortcuts. Relational databases provide a formal and theoretically-sound framework in which to express any query evaluation algorithm--namely, relational calculus (or, practically, SQL). This forces IR researchers to be precise about query semantics, which may be especially useful when complex query operators are introduced in document ranking.
Second, taking advantage of relational databases yields a cleaner architecture. Almost all IR systems today are monolithic agglomerations of components for text processing, document inversion, integer compression, memory/disk management, query evaluation, etc. By offloading the storage management to a relational database, we introduce a clean abstraction (via SQL) between the "low-level" components of the engine and the IR-specific components (e.g., learning to rank). This (hopefully) reduces overall system complexity and may allow different IR engines to inter-operate.
Third, retrieval systems can benefit from advances in data management. Performance is a dominant preoccupation of database researchers, who make regular breakthroughs that propagate to the IR community (for example, PForDelta [17] compression originated from database researchers). By using relational databases, IR systems can benefit from future advances more rapidly, and "for free".
Fourth, a database provides integrated analytical tools useful for error analysis. Consider a simple example, where

863

we wish to examine whether our scoring model has a length bias. This might be accomplished by examining scatterplots of length vs. retrieval scores. With a database, generating these data can be accomplished with a straightforward join between qrels (easy to store in the database), document representations, and the results set. In contrast, with a custom-built retrieval engine, one would need to write additional code to dump out document lengths from internal data structures and perform the join in an ad hoc fashion. Furthermore, there is a rich ecosystem of external analytical toolkits that are able to interface directly with relational databases, which could also be helpful for IR researchers.
Finally, databases form a flexible rapid prototyping tool. Many IR researchers do not really care about index structures and query evaluation per se--they are merely means to an end, such as assessing the effectiveness of a particular ranking model or feature. In this case, forcing researchers to design data structures and query evaluation algorithms is a burden. Using a relational database, researchers can rapidly experiment by issuing declarative SQL queries without needing to write (error-prone) imperative or object-oriented code.
Since retrieval operates over collections of documents, analytics-optimized (OLAP) relational databases are more appropriate than transaction-optimized (OLTP) databases. There are many similarities between query evaluation in document retrieval and online analytical processing (OLAP) tasks in modern data warehouses. Both frequently involve scans, aggregations, and sorting. Thus, we believe that column-oriented databases, which excel at OLAP queries, are amenable to retrieval tasks. An overview of such databases is beyond the scope of this work, but the basic insight is to decompose relations into columns for storage and processing [4]. This storage model allows us to mask random access memory latencies and take full advantage of modern hardware. We use two different column stores, MonetDB [2, 9] and VectorWise [16], to illustrate document ranking on a portion of the ClueWeb12 collection.
We are not the first to claim that databases may have something to offer for IR. Examples of early work include [13, 10, 6]; key references on runtime efficiency are discussed in a 2005 survey [3]. Perhaps the first `real' SQL results for IR queries were presented in [7], but at excessively high costs in terms of the hardware required. More viable results were presented in the TREC 2006 terabyte track, but the approach required hand-written query plans [8]. Follow-up research by the same group allowed the retrieval model to be expressed at the conceptual level [5], however, using a rather `exotic' query language based on array comprehensions. After all these years, we have finally reached the state where database engines can take on IR workloads expressed in standard SQL, without forcing the database admin to resort to low-level tuning. As far as we know, this paper is the first report of experimental work where competitive IR results (in terms of both efficiency and effectiveness) have been obtained using standard relational database technology.
3. SYSTEM ARCHITECTURE
In a custom-built IR engine, a document collection must first be processed (e.g., tokenized) and indexed before retrieval can be performed. In an architecture based on column stores, there are equivalent steps: the collection must be processed and loaded into the database prior to query evaluation. This section provides details on our system architecture.

table: dict

table: terms

termid term df

termid docid pos

1

put 1

1

1

2

2

robe 1

2

1

5

3

wizard 1

3

1

7

4

hat 1

4

1

8

table: docs

docid name len

1 doc1 8

Figure 1: IR data structures in a relational database.

3.1 Document Processing and Loading
We take advantage of the massive scale-out capabilities of Hadoop MapReduce to convert a document collection into a collection of relational tables. Document processing includes tokenization, stemming using the Krovetz stemmer, and stopword removal. The stemmed and filtered terms are mapped to integer ids and stored in a dictionary table. In the main terms table, we store all terms in a document (by term id), along with the position in which they occur. To give a concrete example, consider the document doc1 with the content "I put on my robe and wizard hat". After stopword removal, the relational tables generated from this document are as shown in Figure 1. In more detail, these tables are first generated as flat text files using a two-pass approach on Hadoop; the first pass builds the term to term id mapping, and the second pass builds the terms and the docs tables. These flat text files are then bulk loaded into the database.
3.2 Query Evaluation
We implemented Okapi BM25 as an SQL query, but our approach can be easily extended to other ranking functions. Our experiments focused on conjunctive query evaluation, where the document must contain all query terms; previous work [1] has shown that this approach yields comparable end-to-end effectiveness to disjunctive query evaluation, but is faster. When scoring documents based on BM25, the only score component that depends on the query is the term frequency f (qi, D). An obvious opportunity for optimization here would be to precalculate the term frequencies for each term/document combination, since the positions of the terms do not influence the ranking score for conjunctive queries. By also sorting the terms table by term id (in effect performing document inversion), the database is able to avoid scanning the entire table and instead use binary search, with greatly improved efficiency. The complete ranking function can be expressed in SQL, shown in Figure 2.
We map conjunctive BM25 ranking to SQL in three parts: First, we find the entries in the terms table for the query terms (Lines 1 and 2). In this case, the query terms have ids 10575, 1285, and 191.1 The second step calculates the individual scores for all term/document combinations (Lines 4-13). To express the conjunctivity in the query, we filter this intermediate result to include only combinations with exactly three different term ids. (Lines 9-11). We collect information about document ids and lengths (Line 12) as well as the document frequencies of the terms (Line 13). We calculate the individual BM25 scores for each term/document combination (Lines 5 and 6), sum the results and sort (Lines 14 to 16). With the term frequency precalculation optimization,
1It would be simple to "join in" the dictionary by term, but the purpose of the shown query is to explain the basic concept.

864

1 WITH qterms AS (SELECT termid, docid FROM terms

2

WHERE termid IN (10575, 1285, 191)),

3 subscores AS (

4 SELECT docs.docid, len, term_tf.termid,

5

tf, df, (log((45174549-df+0.5)/(df+0.5))* ((tf*(1.2+1)/(tf+1.2*(1-

6

0.75+0.75*(len/513.67)))))) AS subscore

7

FROM (SELECT termid, docid, COUNT(*) AS tf FROM qterms

8

GROUP BY docid, termid) AS term_tf

9

JOIN (SELECT docid FROM qterms

10

GROUP BY docid HAVING COUNT(DISTINCT termid) = 3)

11

AS cdocs ON term_tf.docid=cdocs.docid

12

JOIN docs ON term_tf.docid=docs.docid

13

JOIN dict ON term_tf.termid=dict.termid)

14 SELECT name, score FROM (SELECT docid, sum(subscore) AS score

15

FROM subscores GROUP BY docid) AS scores JOIN docs ON

16

scores.docid=docs.docid ORDER BY score DESC LIMIT 1000;

Figure 2: Conjunctive BM25 in SQL. The numbers printed in bold are the only parts of the SQL query that depend on the document collection and query terms.

the grouping of the qterms CTE (Line 8) would be replaced by a straight selection from the term frequencies table. Note that this approach can be extended to any scoring function that is a sum of matching query terms. Other modifications are straightforward: disjunctive query evaluation can be implemented by replacing the number of matching terms in Line 10. Phrase queries can be performed by arithmetic over term positions and enforcing distance constraints.

4. EXPERIMENTS
The main point of this paper is that relational database technology ­ in particular, columnar storage ­ is suitable for rapid prototyping for IR. So far, we have shown how a retrieval model can be implemented without writing any imperative code. In further support of this claim, we present experimental results demonstrating that our approach achieves effectiveness and efficiency on par with custom-built retrieval engines. We compare two different relational backends to three open-source IR engines: the open-source columnar database MonetDB [9] (v11.17.13), the commercial database VectorWise [16] (v3.0.1); and Lucene (v4.3), Indri [14] (v5.5), and Terrier [12] (v3.5). Comparing MonetDB and VectorWise, the latter combines columnar storage with lightweight compression and a pipelining execution model, which makes it the current top performer on the well-known TPC-H benchmark for OLAP databases, and especially suited for very large workloads.
Our experiments used the first segment on the first disk of ClueWeb12 (45 million documents) with queries 201­250 from the TREC 2013 web track. This setup is realistic, since production search engines usually adopt a partitioned architecture (and we focus on a single partition). The qrels from the TREC topics were filtered to only include documents that are contained in the segment we used. To ensure that all the IR engines work on the same text, we used Hadoop to

System Indri MonetDB/VectorWise Lucene Terrier

MAP 0.246 0.225 0.216 0.215

P5 0.304 0.276 0.265 0.272

Table 1: MAP and P5 effectiveness scores.

pre-process the documents in the same manner as in the relational setup: this was accomplished by dumping the processed collection as plain text, turning off stemming/stopword removal in the IR engine, and tokenizing by whitespace. In all cases we retrieved the top 1000 results. All experiments were run on a Linux desktop computer (Fedora 20, Kernel 3.12.10, 64 Bit) with 8 cores (Intel Core i7-2600K, 3.4 GHz) and 16 GB of main memory.
Effectiveness results are shown in Table 1. MonetDB and VectorWise produce exactly the same rankings and scores, which is of course to be expected, given that both execute the same SQL queries. However, the effectiveness differences between Indri and Terrier do come as a surprise, since both systems implement BM25--these differences cannot be attributed to tokenization and stopword differences, given the unified document processing step described above.
From these results we draw two conclusions: First, our architecture yields effectiveness that is at least on par with existing custom-built IR engines. Second, these results highlight the advantage of SQL in providing concise yet precise semantics. While we do not claim to have the "correct" BM25 implementation, at the very least our model is concisely specified in a few lines (the SQL query) and thus easy to inspect, not buried in code. Furthermore, different backends produce exactly the same results.
In terms of efficiency, we measured query latency. For each system, queries were run sequentially in isolation. As mentioned previously, pre-calculating the term frequency for conjunctive queries is an obvious optimization in the relational representation, and halves the size of the terms table. In our experiments we examined this optimization independently ("Precalc" vs. "Full"). As an additional optimization, we have placed the MonetDB database on a compressed file system (BTRFS with zlib compression, marked as "C"), since MonetDB does not natively compress data. This reduced the disk footprint of the MonetDB database to 1/3 of the original size. In addition, we consulted with the developers of the Terrier system after observing pathologically slow response times. They made two suggestions: 1) changing the index storage model from on-disk to in-memory, which effectively leads to parts of the index being copied into memory on startup, and 2) changing the retrieval model from term-at-atime (TAAT) to document-at-a-time (DAAT). We did not include the first optimization here, as it makes the cold cache runs pointless (see below). However, the second optimization led to greatly improved performance. For fairness, we report on both the original configuration for Terrier (Orig) as well as on the optimized configuration (DAAT).
Figure 3 shows the query latency distribution across topics as box plots; the boxes contain the observations between the 25th and 75th percentiles. In addition, the plots are annotated with the median query latency. Since the indexes are stored on a hard disk, we expect disk I/O to have a large impact on performance. To test this, we ran the query set two times in succession. Before the first run, we asked the operating system to empty all caches and restarted the

865

Indri VectorWise/Precalc
VectorWise/Full Lucene
MonetDB/C+Precalc MonetDB/C+Full Terrier/DAAT Terrier/Orig MonetDB/Precalc MonetDB/Full

0.56

O

O

0.94

O

O

1.58
OO

1.69
O

4.08

6.19

6.57
O
8.56
O

O

O

O

OO

O O

O
17.82
19.25

0

10

20

Query Time (sec)

(a) Cold Runs

0.03
Lucene OO

0.2

Indri

O

0.34

Terrier/DAAT

O

OO

O

0.59

VectorWise/Precalc

O

0.74

VectorWise/Full

O

OO

1.73

MonetDB/Precalc

OO

1.84
MonetDB/C+Precalc

O

O

2

OO

Terrier/Orig

OO

2.5
MonetDB/Full

O

O

O

3

MonetDB/C+Full

OO

30

0

10

20

30

Query Time (sec)

(b) Hot Runs

Figure 3: Query latencies for TREC 2013 web queries on the first segment of ClueWeb12.

system under test. The two runs are presented in separate plots, the first ("cold" caches) in (a) and the second in (b).
Considering cold runs, we see that Indri was the fastest system, taking a median of 0.56 seconds to complete a query. Second and third were VectorWise, with the precaclulated term frequencies clearly faster, followed by Lucene with a comparable time. MonetDB on a compressed file system and Terrier are comparable. The uncompressed MonetDB experiments clearly show the necessity of compressing an IR index. The vastly greater index size increased the cold response times substantially.
For the hot runs, Lucene leads the field by a large margin, which we suspect to be due to result caching. For Terrier, we see vastly improved timings for the DAAT setting when parts of the index are available in memory and do not have to be loaded from disk. We see that MonetDB (uncompressed) is able to take advantage of caching by the operating system, improving performance by about an order of magnitude compared to the cold runs.
Summarizing these results, it is clear that our database architecture achieves query latencies that are at least on par with existing custom-built IR engines.
5. CONCLUDING REMARKS
This paper argues for the use of column stores for prototyping IR systems. The well-defined relational model allows for the precise expression of retrieval models independent of system implementations. Furthermore, clear abstraction via the declarative SQL interface allows the backend database engine to be swapped, allowing IR systems to quickly benefit from advances in the database field. We have described how to express a standard ranking function using an SQL query and experiments show that using a column store yields effectiveness and efficiency that are at least on par with custom-built IR engines. Hopefully, this will inspire additional IR researchers to think about using columnar databases for building future systems, as doing so would let us focus on the questions on how to best satisfy information needs.
6. ACKNOWLEDGMENTS
This research was supported by the Netherlands Organization for Scientific Research (NWO project 640.005.001), the Dutch national program COMMIT/, and the U.S. National Science Foundation (IIS-1144034 and IIS-1218043). Any

opinions, findings, or recommendations expressed are the authors' and do not necessarily reflect those of the sponsors.
7. REFERENCES
[1] N. Asadi and J. Lin. Effectiveness/efficiency tradeoffs for candidate generation in multi-stage retrieval architectures. SIGIR, 2013.
[2] P. A. Boncz. Monet: A Next-Generation DBMS Kernel For Query-Intensive Applications. PhD thesis, Universiteit van Amsterdam, May 2002.
[3] S. Chaudhuri, R. Ramakrishnan, and G. Weikum. Integrating DB and IR technologies: What is the sound of one hand clapping? CIDR, 2005.
[4] G. Copeland and S. Khoshafian. A decomposition storage model. SIGMOD, 1985.
[5] R. Cornacchia, S. H´eman, M. Zukowski, A. de Vries, and P. Boncz. Flexible and efficient IR using array databases. VLDB, 2008.
[6] N. Fuhr. Models for integrated information retrieval and database systems. IEEE Data Eng. Bull., 19(1):3­13, 1996.
[7] T. Grabs, K. Bhoem, and H.-J. Schek. PowerDB-IR: scalable information retrieval and storage with a cluster of databases. Knowledge and Information Systems, 6(4):465­505, 2004.
[8] S. H´eman, M. Zukowski, A. de Vries, and P. A. Boncz. MonetDB/X100 at the 2006 TREC terabyte track. TREC, 2006.
[9] S. Idreos, F. Groffen, N. Nes, S. Manegold, K. S. Mullender, and M. L. Kersten. MonetDB: Two decades of research in column-oriented database architectures. IEEE Data Eng. Bull., 35(1):40­45, 2012.
[10] I. Macleod. Text retrieval and the relational model. JASIS, 42(3):155­165, 1991.
[11] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. IP&M, 40(5):735­750, 2004.
[12] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A high performance and scalable information retrieval platform. OSIR, 2006.
[13] H.-J. Schek and P. Pistor. Data structures for an integrated data base management and information retrieval system. VLDB, 1982.
[14] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: a language-model based search engine for complex queries. International Conference on Intelligent Analysis, 2005.
[15] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Computing Surveys, 38(6):1­56, 2006.
[16] M. Zukowski and P. A. Boncz. Vectorwise: Beyond column stores. IEEE Data Eng. Bull., 35(1):21­27, 2012.
[17] M. Zukowski, S. Heman, N. Nes, and P. Boncz. Super-Scalar RAM-CPU Cache Compression. ICDE, 2006.

866

Score-Safe Term Dependency Processing With Hybrid Indexes

Matthias Petri
RMIT University & The University of Melbourne
Melbourne, Australia
matthias.petri@gmail.com

Alistair Moffat
The University of Melbourne Melbourne, Australia
ammoffat@unimelb.edu.au

J. Shane Culpepper
RMIT University Melbourne, Australia
shane.culpepper@rmit.edu.au

ABSTRACT
Score-safe index processing has received a great deal of attention over the last two decades. By pre-calculating maximum term impacts during indexing, the number of scoring operations can be minimized, and the top-k documents for a query can be located efficiently. However, these methods often ignore the importance of the effectiveness gains possible when using sequential dependency models. We present a hybrid approach which leverages score-safe processing and suffix-based self-indexing structures in order to provide efficient and effective top-k document retrieval.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing--indexing methods; H.3.2 [Information Storage and Retrieval]: Information Storage--file organization; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-- query formulation, retrieval models, search process; I.7.3 [Document and Text Processing]: Text Processing--index generation
General Terms
Text indexing; text compression; experimentation; performance
1. INTRODUCTION
Search engines rely on fast evaluation of ranking computations. Formulations based on bag-of-word queries and TF×IDF-type scoring mechanisms have been in use for several decades, and users are now accustomed to expressing their information needs via short keyword-based queries. One promising approach to improving the effectiveness of bag-of-words querying is term dependency modeling [12], which include statistics based on phrase combinations into the retrieval mechanism, to favor documents in which query terms appear consecutively, or near each other in unordered windows. The objective is to create a more favorable ordering than is achieved by a bag-of-words computation.
To process term-dependency queries, standard inverted indexbased techniques can be augmented in two different ways. The first pre-defines queryable phrases at index construction time and
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609469.

includes information about them in the index, so that those phrases are automatically supported during query evaluation. The second approach includes word positions within the inverted index, so that arbitrary phrases can be identified during query evaluation. Compared to a document-level inverted index suited for bag-of-words retrieval, the first option has the disadvantage of either requiring a substantially enlarged inverted index or limiting the set of phrases that are supported; while the second option uses a smaller index, but needs additional processing when phrases are part of the query. Compromises between these two implementation options are also possible. Zobel and Moffat [20] give an overview of indexing and searching using inverted files.
The last decade has seen the emergence of other techniques, including self-indexing methods derived from suffix arrays and the Burrows-Wheeler transformation (BWT), following initial work by Muthukrishnan [13]. Until recently, these techniques were "single term" and based solely on occurrence frequency, reporting a ranked list of documents in decreasing frequency order in which any single fixed string of characters or words appears. That is, they report "top-k" for one-term queries, with "top" based on frequency alone. More recent work has explored the use of bag-of-word similarity computations, and stepped closer to the functionality that can be supported using document-level inverted indexes [3].
Our contribution. We implement term-dependency similarity mod-
els using suffix-based indexes, including combining single terms with multi-term phrases, and present an efficient safe-to-k approach for processing bag-of-word and phrase-based queries. Our method combines both pre-calculation and on-the-fly processing in order to achieve attractive time/space trade-offs, and demonstrates that multi-term queries can be processed using a suffix-based index more quickly than is possible using an inverted index alone.
2. PHRASE INDEXING AND QUERYING
Metzler and Croft [12] describe the use of query term dependencies to increase the effectiveness of similarity-based retrieval techniques. They make use of two operators, "ordered window" phrases, in which two or more of the query terms appear in a document in the sequence in which they appear in the query, and "unordered windows" in which two or more of the query terms appear within some specified distance of each other, but not necessarily adjacent, and not necessarily in the order in which they appear in the query. Metzler and Croft assign 80% of the final score of the document to a bag of words computation, 10% to ordered phrases, and 10% to unordered windows.
In the approach used here, scores are based on terms and a complete set of ordered phrases from the query. For example, the query "arthur conan doyle" is processed as a total of six components: the

899

three terms, plus two 2-grams "arthur conan"; and "conan doyle" plus the 3-gram "arthur conan doyle". Including the t singletons, a query of t terms has t(t + 1)/2 different components, each of which is scored across the collection as if it were a separate term.
Intersection-based phrase discovery. The simplest way of query-
ing term m-grams is to store and process position offsets as part of an inverted index [20]. At query time, the various terms' postings lists are combined, and when terms co-occur in a document, a more detailed intersection operation is carried out to check for positional adjacencies. We call this the intersect method; for example, to create a query-time postings list for a phrase of m terms t1 to tm, the postings lists for the phrases t1 . . . tm-1 and t2 . . . tm are intersected, with the base case supplied by the stored postings lists in the term-level inverted index.
Adding position offsets to the term-level inverted index of a typical document collection approximately doubles its size, a reasonable overhead; and when terms are infrequent, intersection of list components is also reasonably fast. But when long lists for common terms are involved, for example, in "the president of the united states", query processing can be costly.
Score-safe processing. A technique is score-safe if it guarantees
that the top-k documents are the same as an exhaustive processing regime would generate. Score-safe methods include MaxScore [17], WAND [1], and BlockMax WAND (BMW) [4, 5]. All rely on a key insight: if the ranking metric is summative over terms, and if the maximum "contribution to any document" score of each term across the collection (or in the case of BMW, over a part of the collection) is known, then an evolving subset of the terms can be identified such that any document that might be added to the top k must contain at least one term in that set. Hence, attention can be restricted to documents containing one or more of those terms.
3. SUFFIX-BASED SELF-INDEXING
The FM-INDEX, introduced by Ferragina and Manzini [6], is a data structure used for pattern matching. Building on the suffix array, it also incorporates ideas embedded in the Burrows-Wheeler transform. A character-level FM-INDEX for a text can be stored in a fraction of the space occupied by the text itself, and provides pattern search and (with small overhead) random-access decoding from any location in the text. To build a word-level FM-INDEX, the input T is parsed into case-folded stemmed words exactly as for any other type of index, and retained as a sequence of word identifiers, W, relative to a vocabulary. An end-of-document sentinel is inserted after each document in W.
The sequence of integers is provided as input to a suffix-sorting algorithm, after which the FM-INDEX is constructed and stored. The FM-INDEX stores the symbols of W in a suffix-permuted ordering, W BWT, which allows efficient access, search and extraction of W using space equivalent to the compressed representation of the input. A third structure used during querying is the n-element document array, or D-array, which notes, for each element of W BWT[i], the document number from which that word originated [13]. Each value D[i] requires log d bits, where d is the number of documents in the collection; D takes n log d bits in total.
Single term query processing. Single term queries are processed
as follows. First, the query is mapped to a sequence of integers, Q, using the vocabulary. Using the FM-INDEX, a range W BWT[sp..ep] is determined which corresponds to all suffixes in W prefixed by Q.

The corresponding range in the D-array, D[sp..ep] now contains all document identifiers containing Q.
The interval D[sp..ep] corresponds to the total number of occurrences of Q in W, which can be large; and hence processing D[sp..ep] efficiently is critical to overall query performance. Various schemes have been proposed to identify the top-k most frequent document identifiers in D[sp..ep]. Culpepper et al. [2] show that if D is stored as a wavelet tree, the top-k documents can be found quickly in practice, but without a worst-case guarantee. Hon et al. [8] present a data structure that augments the D-array in order to provide worst-case bounds on both execution time and space. Their HSV mechanism pre-computes top-k result lists for a set of pre-determined [sp..ep] intervals, guaranteeing that only relatively small ranges are processed exhaustively at query time. Recently, Konow and Navarro [10] presented a compact data structure which efficiently retrieves the top-k most frequent documents independently of the size of the range [sp..ep].
Beyond single term top-k frequency retrieval. In the methods sum-
marized so far, "top-k" has the semantics "return the k documents which contain the most occurrences of the pattern". These approaches cannot be easily adapted to process more complex queries. For example, the formulation of the LMDS similarity computation includes a range of factors, meaning that the top-k answer set to a query over two or more terms is not constrained to lie within the union of the terms' independent top-k answer sets.
Sadakane [15] described a support data structure which computes the number of unique document identifiers in D[sp..ep] in constant time. Sadakane used this structure to support exhaustive processing over [sp..ep], and hence calculate a simple TF×IDF based similarity metric. Culpepper et al. [3] adopt the HSV data structure [8] in combination with a wavelet tree-based representation [2] to compute similarity scores for multi-term queries. Their scheme retrieves a fixed number of most frequent document identifiers for each query term, and combines them to retrieve a ranked document listing, an approach that gives competitive runtime performance for k  100, but is not score-safe.
4. A BLEND OF TECHNIQUES
We build on the previous work, and explore ways of implementing more complex score-safe query semantics ­ in particular, the sequential dependency model outlined earlier. The approach described shortly is a hybrid, consisting of a pruned suffix tree of document-level postings lists to efficiently handle large intervals, and fast sequential exhaustive processing of smaller sections to create document-level postings lists on-the-fly.
Pruned suffix tree. The set of intervals in the D-array that can be
generated by the FM-INDEX correspond to the internal nodes of a suffix tree over T . The HSV mechanism [8] attaches partial precomputed postings lists to nodes within that suffix tree, in order to compute the top-k (by frequency) values at any suffix tree node using a controlled amount of time. The length of each posting list is determined by the size of the corresponding [sp..ep] interval, the number of sampled points within it, and its location in the suffix tree over W; and is strategically balanced so as to achieve attractive time and space bounds.
We return to a simpler scheme which embeds several observations that apply to similarity metrics. Instead of the differing-length lists of the HSV structure, we store full postings lists every node in the suffix tree that corresponds to an interval wider than Tmin, and compute postings lists on-the-fly for [sp..ep] intervals that are

900

smaller. That is, we store a pruned suffix tree (PST) of full postings lists, for a subset of the intervals. A second threshold Tmax is also defined ­ postings lists are not stored for intervals greater than Tmax = d (the number of documents in the collection), since such terms have negligible bearing on the LMDS similarity computation.
Query evaluation. When a query identifies a small [sp..ep] inter-
val, a posting list is computed from the D-array via a sequential cache-friendly process in O(ep - sp) time. The longer the phrase and more precise the interval, the faster this computation is.
The thresholds Tmin and Tmax allow index space and execution cost to be traded ­ the broader the band between the thresholds, the larger the index, and the faster that query evaluation can be carried out. Our query processing scheme is summarized as follows: (1) determine for each query component (including all phrase components) the corresponding [sp..ep] interval using the vocabulary and the FM-INDEX; then either (2a) retrieve the posting list from the PST if Tmin  ep - sp  Tmax, or (2b) create the postings list on the fly by processing D[sp..ep] if ep - sp < Tmin; then (3) apply DAAT or MaxScore or WAND or BMW to the complete set of postings lists to identify the top-k documents.
5. EXPERIMENTS
We took the TREC GOV2 collection, containing 426 GB of web pages crawled from the .gov domain, and applied the Boilerpipe software package1 to generate a plain text version occupying90 GB. A total of 150 test topics and corresponding relevance judgments are available for this collection (topics 701­850). Algorithms were implemented using C++ and compiled with gcc 4.8.1 with ­O3 optimizations; and experiments were run on a 24 core Intel Xeon E52630 running at 2.3 GHz using 256 GB of RAM. All efficiency runs are reported as the mean or median of 5 consecutive runs of a sequence of at least 100 queries for a given query length, and correspond to fully in-memory evaluation, under optimal conditions. All results shown in this short paper are for identification of the top k = 100 documents, and with Tmin = 64,000. Other combinations of parameters will be explored in a full paper.
Each postings list is stored and compressed in blocks of 128 entries using the FastPFOR library [11]. All methods can take advantage of block representatives to allow faster skipping of list entries. Position offsets are stored using the succinct storage scheme of Vigna [18]. The FM-INDEX, the position offset representation, and the PST are implemented using the sdsl library [7].
Baseline performance. Table 1 shows the effectiveness of NewSys
relative to Indri2 and Atire [16]. The FDM model uses both phrase expansion and unordered-window proximity. Our approach uses only the phrase expansion component, and provides a compromise between the lesser effectiveness achievable using the efficient BOW approach, and the more complex FDM mechanism.
Figure 1 shows the effect that length has on query execution time for score-safe processing methods, using queries drawn randomly from the TREC million query set. The times reported in Figure 1 do not include phrase postings list creation cost, and cover only raw posting list evaluation cost after all required lists have been generated. The sequence of improvements arising from score-safe heuristics ­ WAND and BlockMax WAND (BMW) ­ is clear.
Improvements. The bars in Figure 2 show the relative cost of dif-
ferent approaches to on-the-fly construction for index lists that are
1https://code.google.com/p/boilerpipe/ 2http://www.lemurproject.org/indri.php

System
NewSys Atire Indri
NewSys Indri Best Known

Factors
BOW BOW BOW
Phrase FDM Unknown

MAP
0.278 0.279 0.280
0.298 0.317 0.334

Table 1: Effectiveness (MAP) on GOV2 for topics 701­850. The "Best Known" system is uwmtFadTPFB from the TREC 2006 Terabyte Track. All systems use Krovetz stemming, the LMDS similarity formulation with µ = 2500, and Boilerpipe preprocessing. A  represents p < 0.05 using a paired t-test against the BOW runs.
104


Time in msec per Query

103 102 101



 
 
 

 

 

  

   
 

 
 
  

 

 




  
  




 











 








  

DAAT


 

 

 


MAXSCORE WAND











BMW

100

1


2

3

4

5

6

7

8

9 10

Query Length

Figure 1: Query time distribution for different processing approaches using GOV2 and query lengths from 1 to 10. Note the logarithmic vertical scale.

not in the pruned index, normalized (at each query length) to the cost of processing the query using the exhaustive DAAT approach. The horizontal lines show the relative costs of different processing strategies, as already shown (in absolute terms) in Figure 1, normalized against the same reference costs. That is, the bars correspond to different ways of generating a full set of component postings lists, and the horizontal lines to different ways of then computing (the same top-k) document scores from those postings lists. A retrieval mechanism requires both steps. In Figure 2 the method labeled intersect is the baseline intersection-based approach; darray is our hybrid approach with phrases below the threshold Tmin generated on the fly from the stored D-array; and intersect-ppst is described shortly.
Figure 2 shows very clearly that sequential processing of the D-array is far more efficient than generating postings lists from an inverted index via iterated intersections. The longer the query, the greater the advantage. Also worth noting is that iterative intersection dominates the similarity computation: intersect+BMW is only marginally faster than intersect+DAAT. Using the proposed darray approach the full benefits of BMW can be realized.
Table 2 lists timings for the queries shown in Figure 2. These times should be regarded as being indicative rather than precise: Indri and Atire were run with standard out-of-the-box configurations and each query was measured after a pre-execution to bring the required data in to memory; but the timings might still not be exactly like-for-like. Even so, the new approach provides clear benefits.

901

Cost in Percent Relative to DAAT

10,000% 1,000% 100% 10% 1%

MAXSCORE WAND BMW

intersect intersect-ppst darray

 

 



 




       

 

2

4

8

Initial Number of Query Terms

Figure 2: Relative query costs, normalized so that the list processing time using the DAAT strategy is rated at 100.0%.

System
Atire Indri NewSys

Factors
BOW BOW Phrase

t=2
501 270 57

t=4
1814 1580 292

t=8
5920 9844 1084

Table 2: Average query processing time (milliseconds) on GOV2. The NewSys implementation uses the darray+BMW combination.

The performance advantages of the darray method come at a cost. It uses a total of 97 GB for the FM-INDEX, the PST, and the D-array; whereas the intersect method requires only 33 GB for a positional inverted index over terms. An obvious question is: can this space difference be used instead to index a subset of important phrases? Several authors have suggested just such a trade-off approach in which a subset of important phrases is added to the index (see Section 6). The intersect-ppst method shown in Figure 2 offers a pragmatic compromise between using position offsets for each term to generate the phrase components, and indexing all possible phrases. Since the PST already captures all possible phrases of frequency between Tmax and Tmin, we can add positional offsets to those inverted lists, covering the intermediate lists needed to construct postings lists for long phrases. Any remaining lists required are still generated on demand using iterated intersection. With the same value of Tmin, this approach requires 101 GB, and t = 5 term (15 component) queries take an average of 1,700 millisec, compared to 400 millisec for the darray, and 5,800 millisec for intersect. That is, for our chosen query semantics the darray offers superior performance even when compared to a similar-sized inverted index that includes positional postings lists for all phrases (of any length) that occur more than Tmin times.
6. RELATED WORK
Williams et al. [19] propose a modified inverted index, in which each posting list includes a record of what the next terms in the document are. Williams et al. also index common phrases as a single term within an inverted index in order to increase the efficiency of n-gram processing. Other researchers have also explored indexing common phrases, for example, Huston et al. [9] and Ozcan et al. [14]. These approaches provide efficient querying, but also have disadvantages: the phrase length is usually fixed at indexing time;

index sizes can grow quickly; and infrequent phrases might not be recorded in the index at all.
7. CONCLUSION
We have explored algorithmic components that can be used to support efficient phrase querying as a contributing factor in document similarity ranking using term dependency models. While the complete system is still not as effective as the best available systems (which make use of non-consecutive term pairs, and/or query expansion), the efficiency advantages of the BWT-based index are appealing. The BWT-based systems can also support arbitrarily long exact match phrase queries with no additional space costs beyond the fixed D array overhead. Indeed, as the phrases get longer, the processing time gets faster, since the corresponding [sp..ep] interval gets smaller.
Acknowledgments. This work was supported in part by the Aus-
tralian Research Council (DP110101743). Shane Culpepper is the recipient of an ARC DECRA Research Fellowship (DE140100275). Simon Gog designed and implemented the pruned suffix tree used in the experiments and integrated it into the IR framework.
References
[1] A. Z. Broder, D. Carmel, H. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In Proc. CIKM, pages 426­434, 2003.
[2] J. S. Culpepper, G. Navarro, S. J. Puglisi, and A. Turpin. Top-k ranked document search in general text databases. In Proc. ESA, pages 194­ 205, 2010.
[3] J. S. Culpepper, M. Petri, and F. Scholer. Efficient in-memory top-k document retrieval. In Proc. SIGIR, pages 225­234, 2012.
[4] C. Dimopoulos, S. Nepomnyachiy, and T. Suel. Optimizing top-k document retrieval strategies for block-max indexes. In Proc. WSDM, pages 113­122, 2013.
[5] S. Ding and T. Suel. Faster top-k retrieval using block-max indexes. In Proc. SIGIR, pages 993­1002, 2011.
[6] P. Ferragina and G. Manzini. Indexing compressed text. J. ACM, 52 (4):552­581, 2005.
[7] S. Gog, T. Beller, A. Moffat, and M. Petri. From theory to practice: Plug and play with succinct data structures. In Proc. SEA, 2014.
[8] W.-K. Hon, R. Shah, and J. S. Vitter. Space-efficient framework for top-k string retrieval problems. In Proc. FOCS, pages 713­722, 2009.
[9] S. Huston, J. S Culpepper, and W. B. Croft. Sketch-based indexing of n-words. In Proc. CIKM, pages 1864­1868, 2012.
[10] R. Konow and G. Navarro. Faster compact top-k document retrieval. In Proc. DCC, pages 5­17, 2013.
[11] D. Lemire and L. Boytsov. Decoding billions of integers per second through vectorization. Soft. Prac. & Exp., 2014. To appear.
[12] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proc. SIGIR, pages 472­479, 2005.
[13] S. Muthukrishnan. Efficient algorithms for document retrieval problems. In Proc. SODA, pages 657­666, 2002.
[14] R. Ozcan, I. S. Altingovde, B. B. Cambazoglu, F. P. Junqueira, and O. Ulusoy. A five-level static cache architecture for web search engines. Inf. Proc. & Man., 48(5):828­840, 2011.
[15] K. Sadakane. Succinct data structures for flexible text retrieval systems. J. Discr. Alg., 5(1):12­22, 2007.
[16] A. Trotman, X.-F. Jia, and M. Crane. Towards an efficient and effective search engine. In Wkshp. Open Source IR, pages 40­47, 2012.
[17] H. Turtle and J. Flood. Query evaluation: strategies and optimizations. Inf. Proc. & Man., 31(6):831­850, 1995.
[18] S. Vigna. Quasi-succinct indices. In Proc. WSDM, pages 83­92, 2013. [19] H. E. Williams, J. Zobel, and P. Anderson. Fast phrase querying with
combined indexes. ACM TOIS, 22(4):573­594, 2004. [20] J. Zobel and A. Moffat. Inverted files for text search engines. ACM
Comp. Surv., 38(2):6­1 ­ 6­56, 2006.

902

Evaluating Non-Deterministic Retrieval Systems

Gaya K. Jayasinghe
RMIT University Melbourne, Australia gaya.jayasinghe@rmit.edu.au

William Webber
William Webber Consulting Melbourne, Australia
william@williamwebber.com

Mark Sanderson
RMIT University Melbourne, Australia mark.sanderson@rmit.edu.au

Lasitha S. Dharmasena
Deakin University Burwood, Australia lasitha.dharmasena@deakin.edu.au

J. Shane Culpepper
RMIT University Melbourne, Australia shane.culpepper@rmit.edu.au

ABSTRACT
The use of sampling, randomized algorithms, or training based on the unpredictable inputs of users in Information Retrieval often leads to non-deterministic outputs. Evaluating the effectiveness of systems incorporating these methods can be challenging since each run may produce different effectiveness scores. Current IR evaluation techniques do not address this problem. Using the context of distributed information retrieval as a case study for our investigation, we propose a solution based on multivariate linear modeling. We show that the approach provides a consistent and reliable method to compare the effectiveness of non-deterministic IR algorithms, and explain how statistics can safely be used to show that two IR algorithms have equivalent effectiveness.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software-- Performance Evaluation
General Terms
Experimentation, measurement, information retrieval, effectiveness evaluation, experimental design, statistical analysis
1. INTRODUCTION
How should we measure the effectiveness of an IR system if each run might produce a different output? The obvious solution is to generate several system instances and make statistically grounded statements about the overall average effectiveness.
Experiments in IR add another layer of complexity to our problem as retrieval effectiveness varies by topic. The effectiveness of a system is characterized and compared using average effectiveness under whatever evaluation metric is employed across a set of topics. Differences are tested for statistical significance across a hypothetical population of topics using a significance test such as the t­test or bootstrapping, but these standard tests only support one source of variability (here, in choice of topics). The use of algorithms
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609472.

with non-deterministic output introduces an additional dimension of variability into this scenario. In this paper, we contribute the following · We present a methodology to solve the two-dimensional signifi-
cance testing problem (Section 3); · We explore the properties of our solution on a case study of com-
mon sampling-based algorithms ­ shard construction and centralized resource allocation in distributed IR [3, 6]. We examine the variability that can occur in this environment, observing that an apparently significant result on one instance of a samplebased algorithm can be contradicted by another, and we demonstrate the use of our two-dimensional significance testing methods to handle the variability and provide sound statistical inferences (Section 4). · We clarify statistical best-practices on parameter selection when comparing algorithms for equivalent effectiveness (Section 5).

2. BACKGROUND
For many years, researchers in the IR community have benefited from shared test collections. A collection includes documents, test topics, and relevance judgments. Using these shared collections, IR systems can be compared by calculating the effectiveness of each using a common metric such as MAP or NDCG. However, arguing that one system is "better" than another simply on the basis of achieving a higher effectiveness score on a collection is not as straightforward as it might initially seem. The subtle differences in average score might just be coincidental to the particular set of topics selected, and may not hold across the full set (or population) of possible queries and topics. In response, IR research has adopted the practice of statistical significance testing in order to estimate the likelihood of two systems being hypothetically equivalent (p value). A sufficiently low p value implies a systematic difference that cannot be attributed purely to chance in the selection of topics. Common approaches to significance testing include Student's paired t-test, ANOVA, the sign test, the Wilcoxon signed rank test, Fisher's randomization test, and bootstrapping.
Another approach to testing the significance of differences in system effectiveness on a sample set of topics is through linear modeling. Where variance comes solely through the selection of topics, the following linear model (equivalent to the t-test and ANOVA) is applied:

Eij =  + si + tj + ij .

(1)

Here, Eij is the effectiveness of system i on topic j. We model this effectiveness as due to two factors or effects: the system effect

911

si (essentially, the effectiveness of the system, relative to other systems), and the topic effect ti (essentially, the difficulty of the topic, relative to other topics). The  term, the intercept in the model, is the average effectiveness of all systems across all topics, while ij is the residual or error term, accounting for the deviation of the observed effectiveness from that explained by the model. The p value for the system effect can either be computed with si / 2/n or t-statistic from ANOVA, which follows Student's t distribution with n - 1 degrees of freedom. Here si is the difference between the two system effects, 2 is the variance of the residuals, and n is the number of topics.
Alternatively, the same can be modeled using a linear mixed effect (LME) model, consisting of fixed (non-random) and random effects [2, 5]. For this scenario, sampled topics produce a random effect, and systems produce a fixed effect. Though, not apparent at this stage, as we will see later LME can be used to build complex models that capture repeated measurements and hierarchical grouping. For large samples, the p value can be computed from a t-statistic obtained from the LME with n - f degrees of freedom, where f is the number of fixed effect parameters (f = 1 for the above scenario) [1].
Another way to compute the p value is to generate the posterior distribution for the system factor using Markov Chain Monte Carlo (MCMC) simulations. An MCMC simulation repeatedly samples from the conditional distributions of parameter subsets (, parameters defining the variance-covariance for random effects, and fixed and random effects) of the linear mixed effect model cyclically, thus making variance of all other parameter subsets reflect the variance for each parameter subset. The posterior distribution of the system effect parameter is expected to follow a normal distribution which can be used to compute the p value [1]. Deriving the posterior distribution also allows us to obtain the highest posterior density (HPD) interval which is analogous to the standard confidence interval. A  % HPD interval represents the shortest interval enclosing (1 - ) % of the posterior probability mass of the distribution. Therefore, the HPD interval is considered a better representation than the standard error interval.
All of the significance testing approaches above assume an IR system with a deterministic output, where only one observation exists per topic. However, topical variance can be thought of as combining two components: first, measurement or model error; and second, the fact that some systems do better on some topics than others (both systems and topics). If we only have one observation per topic, we cannot separate these two factors; but if we have repeated observations, the two factors can be separately estimated. Taking system effect as fixed, and topic-system interaction effects as random, the above can be modeled with LME, as follows:

Eijk =  + si + tj + tsij + ijk.

(2)

Here Eijk is the effectiveness on the k·th observation of system i on topic j, tsij is the topic-system interaction effect, and ijk represents the (random) error of a single observation. However, the model only makes sense if different observations on the same topicsystem pair lead to different scores. In Robertson and Kanoulas [5], this variability in topic-system scores is observed over different document sets, whereas in Carterette et al. [2] the variability is in different user types. Our focus in this paper is on non-determinism in IR system output due to variability introduced by, for example, a randomized sampling-based algorithm or an algorithm that exploits logs of (unpredictable) user input. Hence, variability exists in two dimensions (system instances and topics), with one (topical variation) grouped within the other (system instances).

3. OUR APPROACH
Our approach uses the following LME model:

Elmn =  + al + sm + tn + atln + lmn.

(3)

Here Elmn is the effectiveness observed for topic n on system instance m generated using algorithm l, and  represents the model intercept. The factors al, sm, and tn represent effects for the IR algorithm l, (non-deterministic) system instance m, and topic n respectively. The topic-algorithm interaction effect is captured by atln. The unallocated portion of effectiveness Elmn is what resides in lmn.
The algorithmic effect is fixed in the above model, where sampled topics, system instances and topic-algorithm interaction provide the non-deterministic effects. The data for constructing the above model contains effectiveness (E) and three factors: algorithm (a), system instance (s), and topic (t). If each level of one factor occurs in every level of another factor, we say the two factors are crossed. We say they are nested if the levels of one factor occurring within the levels of another factor are different.
Crossed factors generally result in an interaction effect in the LME model if they contain repeated measures. The effectiveness for the same set of topics is measured on each system instance and each algorithm. Hence, the algorithm and system instance factors are crossed with the factor topic which results in a topic-algorithm interaction effect, but not a topic-system interaction effect as we do not have repeated measurements.
When two non-deterministic algorithms are compared, the system instances used for evaluation are different for each algorithm, which naturally nests the system instance factor within the algorithm factor. However, if one of the algorithms in the comparison is fully deterministic, a crossed design can be used whereby each level of the system instance factor for the deterministic algorithm is a replicate. The p value for the algorithm factor can be computed with n - 1 degrees of freedom in the same manner described in Section 2.
Note that our model in Equation 3 differs from the model in Equation 2, as used by Robertson and Kanoulas [5] and Carterette et al. [2]. In the latter model, they regard the observations on the non-topic factor (respectively, document sets, and user type) simply as providing repeat observations of the topic-system interaction, and not as having a systematically grouped effect.

4. CASE STUDY
We now turn our attention to a concrete example. Sharding is a well-known technique to divide very large document collections. The technique is used in distributed IR to allocate and index shards of the collection on the different nodes of a cluster. In such a configuration, retrieval is performed across multiple indexes, one per node. Efficiency can be improved if the query is only sent to a subset of the indexes. The question then becomes how many indexes should be queried without causing a measurable loss in retrieval effectiveness. To efficiently and effectively select the best subset for each query, a widely-used approach is to create a centrally held index composed of documents sampled from each shard. This central sample index (CSI) is used to represent the true collection statistics [6].
Recent research has focused on reducing the search cost per query without hurting overall effectiveness by reordering the documents in each shard by topic or similarity [3]. These systems are able to achieve effectiveness close to a search over the entire collection (exhaustive search) while using only a few shards for each

912

CSI sample rate (%)
0.01 0.05 0.1 0.5 1.0 2.0 3.0 4.0

t­test p value < 0.05
100% 59% 21% 3% 0% 1% 1% 0%

t­test p value
< 0.1
100% 76% 31% 10% 2% 2% 1% 2%

Comparing with exhaustive search
(p value)
0.0000 0.0007 0.0601 0.8990 0.9325 0.8748 0.8407 0.7456

Comparing two non-deterministic algorithms (p value)
0.0000 0.0000 0.0000 0.0270 0.0991 0.2141 0.3483 1.0000

Table 1: The proportion of system instances that demonstrated a significant difference using a paired t-test, and the p values when comparing the sample-based IR algorithm proposed by Kulkarni and Callan [3] at varying CSI sample rates with a deterministic exhaustive search, and with itself (a nondeterministic algorithm) with a CSI sample rate of 4% using the TREC GOV2 dataset and TREC topics 701 ­ 850.

Effectiveness (NDCG@10)
0.0 0.2 0.4 0.6 0.8 1.0

Mean effectiveness
826 829 832 835 838 841 844 847 850
Topic

query. However, many clustering and classification algorithms are not able to scale to the typical size of a modern IR collection and therefore resort to sampling and k-means clustering to reorder the shards. Each time such an algorithm is run (i.e. there is a new system instance) distinct shards will be formed.
For the purpose of this study, we reexamine the problem of determining the optimal CSI sampling rate, originally presented by Si and Callan [6]. To select a subset of shards for a given query, the CSI is searched first. The proportion of documents from each shard in the CSI search results are then used to rank shards for a given query. Therefore, the time spent on searching the CSI is a key factor determining query response time. The CSI search time is correlated to the sampling rate used to construct the CSI and the query difficulty. The sampling rate used for constructing the CSI must be sufficiently high to represent the shard in order to avoid poor retrieval effectiveness. A high sampling rate can also minimize the likelihood of encountering out-of-vocabulary (OOV) terms in the mapping of the CSI to the shards. This is a classic effectiveness and efficiency trade-off problem, whereby the best query response time is achieved when the sample rate is set to the smallest level that still achieves similar effectiveness to that resulting from exhaustive search.
4.1 Experimental testbed
Experiments are performed on the TREC GOV2 dataset, using topics 701­850. Using two independent 1% samples of the dataset, 5 topically-partitioned distributed IR system instances are formed for each sample using a k­means clustering algorithm. Thus, we have 10 different instances of the sharded index. As with the original experiments [3], 50 shards were formed per instance, and the full dependency model (FDM) is used to rank the queries [4]. Selecting a subset of 5 shards produced equivalent retrieval results at depth 10 to exhaustive search [3]. Therefore, only the top 5 ranked shards are searched for each query. For each of the sharded versions, 10 CSI instances are formed for each sampling rate, giving 100 instances in total for each sample rate. NDCG@10 is used as the evaluation metric in all experiments.
4.2 Results
Each individual system instance is compared with the exhaustive search baseline using a paired t­test, and the overall comparison with the proposed approaches are analyzed in Table 1. As can be seen, the number of IR system instances that were significantly different to exhaustive search increases as CSI sampling rates are reduced. Some individual comparisons show a significant differ-

Figure 1: Topical variance for TREC topics 826 ­ 850 observed with IR system instances produced with the topical sharding algorithm proposed by Kulkarni and Callan [3] on TREC GOV2 dataset with a CSI sampling rate of 4%.
ence between two IR system instances while the rest agree on no such difference. For example, at CSI sampling rate of 0.5%, 3% of the comparisons show a significant difference at  = 0.05, and 10% at  = 0.1. This exemplifies the potential of drawing an inaccurate conclusion, and the difficulty of confidently comparing a single IR system instance produced by a non-deterministic algorithm. We could report the mean effectiveness for each topic across a large number of non-deterministic system instances to reduce the likelihood of producing conflicting results. But this may not result in a fair comparison, as variance due to non-determinism is not explicitly captured in such an evaluation framework.
The variance in effectiveness for TREC topics 826 ­ 850 across 100 topically partitioned distributed IR system instantiations are illustrated in Figure 1. While effectiveness for some topics are consistent, others are clearly not. A shift in mean effectiveness due to outliers is also observed for several topics. Our proposed approach for significance testing can be used to eliminate such ambiguity and help researchers derive more accurate conclusions.
A comparison of the non-deterministic algorithm at varying CSI sample rates with deterministic exhaustive search and with the same algorithm at a CSI sample rate of 4% is also presented in Table 1. The results verify the suitability of the proposed approach for evaluating non-deterministic algorithms.
We have now discussed the value in using two dimensional statistical significance tests when comparing sample-based algorithms. Assessing similarity when uncertainty is involved even with one source of variability is a concept that has not been broadly addressed by the IR community. Building on the case study, we now outline one possible statistical approach that can be used to compare non-deterministic systems for equivalence.
5. PROMOTING BEST PRACTICE
A statistical significance test cannot be used to "prove" a null hypothesis. The null hypothesis is, in reality, a statistical straw man. Even if we were to believe that two systems have identical population mean scores, we cannot use the methods of inferential statistics directly to perform the test. Statistical inference works probabilistically from the evidence of a sample; exact identity can be determined only by exhaustive examination of the population.

913

Type
Bad Good

H0
MA = MB |MA - MB |  0

Desired outcome
Fail to reject Reject

Inference
Systems "statistically indistinguishable" Difference in systems is significantly less than consequential

Encouraged behavior
Reduce sample size Increase sample size

Table 2: Two ways of testing for significance the "equivalence" of system A against system B.

MCMC Mean effectiiveness (Randomised - Exhastive)
-0.15 -0.10 -0.05 0.00

MCMC mean = +0.01 MCMC mean = -0.01
0.01 0.05 0.1 0.2 0.3 0.4 0.5 1 2 3 4
CSI sampling rate (%)
Figure 2: The 95% highest posterior density (HPD) interval for comparison of the topical sharding algorithm proposed by Kulkarni and Callan [3] with exhaustive search on TREC GOV2 dataset and TREC topics 701 ­ 850.
Even if we believed the null hypothesis, and even if we could establish it statistically, using the failure of a significance test to do so is bad practice. A researcher desires to find no statistical significance (that is, to fail to reject the null hypothesis) in order to confirm an experimental objective. The way to increase the likelihood of this happening is straightforward: decrease the sample size (for example the number of topics or for randomized techniques the number of randomizations), and you decrease the likelihood of finding statistical significance.
If we wish to use hypothesis testing to establish statistically significant "equivalence", a better practice is to proffer the smallest difference in mean performance, 0, that we would regard as being consequential. Here, |MA - MB|  0, becomes the null hypothesis; and |MA - MB| < 0 is the alternative hypothesis. We then test against the null hypothesis; if the test rejects it, we accept the alternative, and conclude that the difference between system A and system B is no more than 0: the systems are effectively equivalent. This process not only makes statistical sense, it also drives good practice: the experimentalist is incentivized to increase the size of the test set and thus the accuracy of the measurements.
The pros and cons of these two methods of testing for statistically significant equivalence are summarized in Table 2.
5.1 Non-deterministic significant equivalence
We now examine our case study. A possible goal is to find the minimum CSI sampling rate that is still able to achieve "equivalent" effectiveness to the exhaustive solution, namely a complete central index. We set 0 = 0.01. If a sampling rate with the CSI method causes an absolute difference less than 0 in mean NDCG@10 scores, we will regard it as equivalently effective to the full index. Therefore, we test the null hypothesis of |MA -MB|  0 for each

sampling rate, and choose the smallest sampling rate for which this null hypothesis is rejected.
The null hypothesis can also be tested indirectly by computing the highest posterior density (HPD) interval using the posterior distribution for the algorithm factor of the LME model. The HPD interval of the algorithm factor gives a 95% confidence interval on the true difference between the mean performance of the sampled and exhaustive indexes. If the confidence interval is within the lines ±0 then we reject the null hypothesis and conclude that the system MB is equivalently effective to system MA. However, if the confidence interval covers values, below or equal to -0, or above or equal to +0, then the null hypothesis cannot be rejected.
We show the HPD interval for different CSI sample rates in Figure 2. Sampling rates below 0.05% are clearly worse than exhaustive search. For sample rates above 0.1%, a portion of the confidence interval is greater than -0. Therefore, the sampled index may not be consequentially worse than the exhaustive one, but we can not draw a conclusion with any confidence. It is not until the sampling rate reaches 1% that the confidence interval is above -0. But, for these sampling rates part of the confidence interval is above +0. Therefore, we cannot conclude with confidence that the sampled method is equivalently effective to exhaustive method. But the fact that the confidence interval is above -0 allows us to say that the method is greater than or equivalently effective (that is, not consequentially worse than) than the exhaustive method for CSI sampling rates of greater than 1%.
6. CONCLUSION
In this paper we have explored the potential pitfalls of depending on a single instance of a non-deterministic system for evaluation. In order to alleviate this problem, we introduce the notion of two-dimensional significance and describe a sound methodology to compare non-deterministic systems. In future work, we will explore how best to safely compare these systems for equivalence directly.
Acknowledgments
This work was supported in part by the Australian Research Council (DP130104007). Dr. Culpepper is the recipient of an ARC DECRA Research Fellowship (DE140100275).
References
[1] R. H. Baayen, D. J. Davidson, and D. M. Bates. Mixed-effects modeling with crossed random effects for subjects and items. Journal of memory and language, 59(4):390­412, 2008.
[2] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In CIKM, pages 611­620, 2011.
[3] A. Kulkarni and J. Callan. Document allocation policies for selective searching of distributed indexes. In CIKM, pages 449­458, 2010.
[4] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In SIGIR, pages 472­479, 2005.
[5] S. E. Robertson and E. Kanoulas. On per-topic variance in IR evaluation. In SIGIR, pages 891­900, 2012.
[6] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. In SIGIR, pages 298­305, 2003.

914

Extending Test Collection Pools Without Manual Runs

Gaya K. Jayasinghe
RMIT University Melbourne, Australia gaya.jayasinghe@rmit.edu.au

William Webber
William Webber Consulting Melbourne, Australia
william@williamwebber.com
J. Shane Culpepper
RMIT University Melbourne, Australia shane.culpepper@rmit.edu.au

Mark Sanderson
RMIT University Melbourne, Australia mark.sanderson@rmit.edu.au

ABSTRACT
Information retrieval test collections traditionally use a combination of automatic and manual runs to create a pool of documents to be judged. The quality of the final judgments produced for a collection is a product of the variety across each of the runs submitted and the pool depth. In this work, we explore fully automated approaches to generating a pool. By combining a simple voting approach with machine learning from documents retrieved by automatic runs, we are able to identify a large portion of relevant documents that would normally only be found through manual runs. Our initial results are promising and can be extended in future studies to help test collection curators ensure proper judgment coverage is maintained across complete document collections.
Categories and Subject Descriptors
H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval--clustering, retrieval models, search & selection process
General Terms
Information retrieval, Evaluation, Test collection construction
1. INTRODUCTION
Successful evaluation and reproducibility of experiments in information retrieval (IR) depends on building reusable test collections composed of documents, topics, and relevance judgments. Ideally every document in a collection would be assessed against each topic, but this approach does not scale. So judgments are normally produced for a sample of the corpus, known as a pool, all other documents are assumed to be not relevant. This sample needs to be representative of the entire collection and robust enough to evaluate entirely new search algorithms. The genesis of pooling dates back to the 1970s [12].
To produce relevance judgments, the organizers of TREC, CLEF, NTCIR, and other such conferences invite researchers to submit the top-i documents retrieved for a set of topics from a specified corpus
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609473.

[14, 15] (typically i = 1, 000). The sets of documents are known as automatic runs. Across the runs, the top-j ranked documents for each topic are gathered for relevance assessment (typically j is set to 50 or 100). Such a practice seems to consistently identify most of the relevant documents, but provides no guarantee on the judgment coverage for documents retrieved by new IR approaches [4, 9, 16]. Test collections tend to have a bias towards the systems contributing to the pool, and may not reliably evaluate novel IR systems that retrieve unjudged but relevant documents.
In an attempt to "future proof" test collections, the organizers of the evaluation conferences commonly encourage submissions of manual runs, where humans can reformulate queries and/or merge results from multiple queries [1] before a final set of top-i documents is submitted. Such runs are generally highly effective and contribute many unique relevant documents to the judgment pool. However, manual runs are not always available when building a collection, so in this short paper we ask:
Research question: Can we construct reliable IR test collections using only automatic retrieval runs?
Our contribution: We describe a methodology that can be used to construct reusable test collections in the absence of manual retrieval runs. We evaluate a simple voting approach combined with machine learning to show that we can achieve collection coverage similar to pooling generated with manual runs.
2. BACKGROUND
Efficiently building test collections for evaluation of IR systems is a well-studied problem [10]. Early research concentrated on more efficient ways for assessors to scan pools, with the objective of judging more documents with a given budget or identifying a sufficient number of relevant documents as quickly as possible. Zobel [16] showed that the number of relevant documents in a collection varies from topic to topic. He suggested that assessors should focus their effort on judging topics with more relevant documents. For each topic, the number of relevant documents found so far were used to estimate the expected ratio of relevant documents in the remaining unjudged block. Each topic was assessed until relevant documents were depleted beyond an economically viable limit to assess the block.
The idea of focusing assessor effort on the most fruitful sources of relevant documents was also applied to IR systems that contribute to a pool. Just as some topics have more relevant documents than others, some systems retrieve more relevant documents than others. Using this insight, Cormack et al. [5] described a move-tofront pooling approach which ensured that documents from the IR systems producing the most relevant documents were moved to the

915

916

We also use Kendall's  to measure pairwise inversions between two rankings of runs, the first using full TREC relevance assessments and the second using relevance assessments generated from the union of the first and second pools formed by each of our methods. Using a convention from Voorhees [13], if the Kendall's  correlation is  0.9, the rankings are considered equivalent.

Metric
MAP P@10 P@20 P@30 P@100

Borda count
0.0778 0.1306 0.1122 0.1020 0.0743

ML
0.0268 0.0531 0.0378 0.0361 0.0167

Combined
0.1507 0.1714 0.1500 0.1367 0.0916

Table 1: Effectiveness on finding relevant documents in MRJ. A : significant improvement (p < 0.01) compared to Borda count.

Depth (k) Borda count ML Combined

50

15.22 4.52

19.00

100

24.19 5.45

29.83

150

29.30 6.71

37.28

171

31.36 7.11

39.53

200

33.75 7.97

42.33

Table 2: Percentage of MRJ documents found in top (k) of the proposed rankings. implies a similar assessment effort to traditional pooling method. A : significant improvement (p < 0.05) compared to Borda count.

5. RESULTS
The analysis is presented in Table 1. The combined method is significantly better than the other two when evaluated with MAP. The same trend is observed when measuring using precision, but none of the differences are significant. Using only the ML method produces worse results than either Borda count or combined.
Note that the relatively low reported effectiveness numbers in Table 1 are largely a byproduct of evaluating using only the unique relevant documents in MRJ and not the entire second pool. We cannot make any claims about new documents retrieved by the ML method since a large portion of retrieved documents using this method are not judged, compared to other two approaches. In fact, 9, 817 of the top-200 documents returned across all 50 topics using only ML (98.17%) are currently unjudged. Therefore, we have to assume that these documents are not relevant until all of the documents returned are judged. In future work, we hope to investigate the full impact unjudged documents have on our classifier method in more detail.
In Table 2 we measure the proportion of documents that were found to be relevant in the second pool. Again a similar trend of differences are seen, but with significant improvements across all measurements up to k = 200 for the combined method.
5.1 Discussion
As indicated in Figure 1, the majority of documents uniquely judged in the manual runs (MRJ) are also retrieved by the automatic runs (ARU+ARJ). However, few appear in the first pool as they (i.e. ARJ) are not ranked highly enough to be judged. In fact, 88% of the documents judged as relevant that are uniquely pooled by manual runs could be found in the first pool, if a pool depth of 1, 000 was used.
If there were no manual runs in a test collection (i.e. no MRJ), the effectiveness of IR systems producing results similar to such

runs would be underestimated and any improvements would go unnoticed. It would appear that manual retrieval runs still play a critical role in improving the re-usability of test collections.
Relevant documents exclusively pooled by manual retrieval runs

10 1000

Number of Relevant documents

0

Combined Estimates for combined

0

50

100

150

200

Depth of ranking (K)

Figure 2: The number of MRJ documents, and estimated number of relevant documents in the top-k of the combined ranked list on TREC GOV2 dataset and TREC topics 801 ­ 850.

Metric
MAP P@10 P@20 P@30 P@100

Borda count
0.3415 0.3571 0.3551 0.3401 0.2337

ML
0.4872 0.5082 0.4684 0.4299 0.2624

Combined
0.5049 0.5694 0.4959 0.4497 0.2555·

Table 3: Just considering the documents in MRJ, how effective are ranking algorithms on retrieving relevant documents? Significant improvements (p < 0.01 and p < 0.05) compared to Borda count are denoted with a  and ·.

Judging the ranked lists of the combined method up to a depth k identifies a subset of the relevant documents uniquely pooled by manual retrieval runs. However, we still know little about the large number of unjudged documents in the ranked lists produced by the combined method. If we assume the proportion of relevant documents among unjudged documents in these ranked lists is the same as the proportion found among judged documents in the same ranked list up to the same depth, we can estimate the total number of relevant documents that would have been found in the same depth of the ranking. Figure 2 illustrates the estimated number of relevant documents, along with the number of known relevant documents found.
Missing judgments for a large portion of the ranked lists from the proposed methods is one potential reason for the low retrieval effectiveness of those methods. Therefore, we calculate retrieval effectiveness on the intersection of the second pool with MRJ, Table 3. (Note, the first pool and the ranking functions remains the same.) The ML method now re-ranks a subset of unique documents top-j ranked by manual runs. The ranking produced by ML show significant improvements for all considered evaluation metrics compared to Borda count. The combined method achieves a better effectiveness than ML for all evaluation metrics considered, except p@100. This is due to ranking only the subset of documents top ranked by the Borda count. Re-ranking a carefully retrieved

917

1

1

MAP P@10 P@20 P@30 P@100

0.9

0.8

Kendall's Tau

0.9

Kendall's Tau

0.8

0

50

100

150

200

Depth

0.7

MAP P@10 P@20 P@30 P@100

5

10

15

20

25

30

Automatic systems

Figure 3: Kendall's  correlation of IR system rankings for varying depths of assessing documents using combined method.
subset of documents for topics with ML is an effective approach to locate new documents to be pooled and judged.
Whenever a new approach for pool composition is proposed, we would like to be able to quantify how well the approach ranks IR systems compared to the original method. A Kendall's  ranking correlation for varying depths of assessing documents with the proposed approach for various evaluation metric are shown in Figure 3. Here, we consider all 80 submitted runs rather than only the subset originally used for pooling. Manual retrieval runs are viewed as novel approaches to retrieval. The Kendall's  correlation for MAP is above 0.9 beyond a depth of 100. A budget similar to original assessment permits processing up to a depth of 171 documents, which demonstrates the validity of the proposed approach in the absence of manual retrieval runs.
Another question of interest is how small the automatic runs pool can be when there are no manual runs. In Figure 4 we introduce runs incrementally in order starting with the run contributing the fewest relevant documents. When 20 or more automatic retrieval runs are pooled the Kendall  correlation for MAP exceeds 0.9.
6. CONCLUSION
In this paper, we present a methodology for building reusable evaluation pools in the absence of manual retrieval runs. Our approach can discover many relevant documents that were previously only found by manual retrieval runs. The approach demonstrates the potential of finding relevant documents that are not currently possible using current pooling approaches. However, the true efficacy of our approach cannot be properly assessed until all of the newly retrieved documents are judged. We plan to investigate this in future work. Nonetheless, our initial results are promising as we are already able to achieve a similar IR system ranking to previous approaches which depended heavily on manual runs to add the necessary diversity to the assessment pool.
Acknowledgments
This work was supported in part by the Australian Research Council (DP130104007). Dr. Culpepper is the recipient of an ARC DECRA Research Fellowship (DE140100275).

Figure 4: Kendall's  correlation of IR system rankings with varying number of automatic systems in the pool. Automatic systems are added in the order least contributing system to most, and the ranking produced by the combined method is processed to a depth of 200.
References
[1] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Information Retrieval, 10(6): 491­508, 2007.
[2] S. Büttcher, C. L. A. Clarke, and I. Soboroff. The TREC 2006 terabyte track. In TREC-2006, volume 6, page 39, 2006.
[3] S. Büttcher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboroff. Reliable information retrieval evaluation with incomplete and biased judgements. In SIGIR, pages 63­70, 2007.
[4] B. Carterette, E. Gabrilovich, V. Josifovski, and D. Metzler. Measuring the reusability of test collections. In WSDM, pages 231­240, 2010.
[5] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In SIGIR, pages 282­289, 1998.
[6] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871­1874, June 2008.
[7] R. Krovetz. Viewing morphology as an inference process. In SIGIR, pages 191­202, Pittsburgh, Pennsylvania, USA, 1993.
[8] A. Moffat, W. Webber, and J. Zobel. Strategic system comparisons via targeted relevance judgments. In SIGIR, pages 375­382, 2007.
[9] T. Sakai. The unreusability of diversified search test collections. In EVIA, June 2013.
[10] M. Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4 (4):247­375, 2010.
[11] I. Soboroff and S. Robertson. Building a filtering test collection for TREC 2002. In SIGIR, pages 243­250, 2003.
[12] K. Spärck Jones and C. J. Van Rijsbergen. Report on the need for and provision of an "ideal" information retrieval test collection. Technical report, British Library Research and Development Report 5266, 1975.
[13] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information processing & management, 36(5):697­716, 2000.
[14] E. M. Voorhees. The philosophy of information retrieval evaluation. In Evaluation of cross-language information retrieval systems, pages 355­370. Springer, 2002.
[15] E. M. Voorhees and D. K. Harman. TREC: Experiment and evaluation in information retrieval, volume 63. MIT press Cambridge, 2005.
[16] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In SIGIR, pages 307­314, 1998.

918

The Search Duel: A Response to a Strong Ranker

Peter Izsak
Technion, Israel
peteriz@tx.technion.ac.il

Fiana Raiber
Technion, Israel
fiana@tx.technion.ac.il

Oren Kurland

Moshe Tennenholtz

Technion, Israel

Microsoft Research and

kurland@ie.technion.ac.il

Technion, Israel

moshet@ie.technion.ac.il

ABSTRACT
How can a search engine with a relatively weak relevance ranking function compete with a search engine that has a much stronger ranking function? This dual challenge, which to the best of our knowledge has not been addressed in previous work, entails an interesting bi-modal utility function for the weak search engine. That is, the goal is to produce in response to a query a document result list whose effectiveness does not fall much behind that of the strong search engine; and, which is quite different than that of the strong engine. We present a per-query algorithmic approach that leverages fundamental retrieval principles such as pseudofeedback-based relevance modeling. We demonstrate the merits of our approach using TREC data.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
Keywords: search engine competition, dueling algorithms
1. INTRODUCTION
We revisit the classic ad hoc relevance ranking problem from a competition perspective. Rather than addressing a single ranking system, we consider a duel in which a search problem -- i.e., a query representing an information need -- is presented to two players. One of the players has a relevance ranking function that is considerably "weaker" (i.e., less effective) than that of the other player. Yet, his goal is to produce a ranking that is competitive with that created by the player with the stronger ranking function.
On the theory side, this type of interaction is modeled in the context of the recently introduced dueling algorithms [9]. Our goal is to introduce a pragmatic manifestation in the context of an adversarial retrieval setting (e.g., the Web).
The ranking functions employed by leading Web search engines are remarkably effective. However, there is still much room for improving retrieval effectiveness due to various reasons. Many of these are related to the adversarial
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609474.

nature of the retrieval setting (e.g., search engine optimization efforts). Furthermore, it is impossible for a search engine to index all possible documents in a large-scale and dynamically changing collection such as the Web. This reality provides some hope for a search engine with a relatively weak ranking function to compete with a search engine that has a much stronger ranking function.
A potential approach to addressing the duel challenge is to try to explicitly learn the ranking function of the strong search engine. However, this approach falls short in our competitive setting. That is, some of the most important information types utilized by the strong ranker may not be available to the weak ranker; e.g., those based on user engagement information such as clickthrough data.
We propose a per-query competitive approach. We let the weak search engine observe the output (i.e., the result list of the most highly ranked documents) of the strong search engine for a query. Using this list, which is treated as a pseudo feedback set, we induce a relevance model [10]. The model is used to modify the ranking of the weak search engine. The modification is based on a bi-modal criteria: retrieval effectiveness (in terms of relevance) and diversification with respect to the results presented by the strong search engine. The motivation for diversification is based on the following realization. Users of the strong search engine would have no incentive to switch to (or consider) the weak engine if they are presented with the same results.
Empirical evaluation performed with TREC data attests to the effectiveness of our approach. For example, we show that the approach can be used to boost the retrieval effectiveness of weak rankers to a level competitive with that of strong rankers, while maintaining relatively low overlap with the strong rankers' result lists. The approach also substantially outperforms a highly effective fusion method that merges the results of the strong and weak search engines.
This paper describes a preliminary, and the first (to the best of our knowledge), attempt to address the interesting and practical challenge of a search engine duel. Naturally, an abundance of research challenges, in addition to those we address here, arise. We discuss some of these in Section 5.
2. RELATED WORK
As mentioned, from a theory perspective, the work on dueling algorithms [9] deals with a setting similar to ours. Although the emphasis is on computing minimax strategies, the basic building block is computing the response of one

919

agent to another. However, the model is stylized and does not refer to the realistic search duel we discuss here.
There is a large body of work on merging document lists that were retrieved in response to a query from the same corpus [4] or from different corpora [1]. Our use of a relevance model induced from one list to re-rank another list is conceptually reminiscent of work on using inter-document similarities between two lists for re-ranking [11]. However, in contrast to all these results merging approaches which aim to maximize only relevance, our methods are designed to also minimize overlap with the strong engine's result list.
There is work on training a ranker for one domain (language) and applying it to another [7, 8]. In contrast, we do not assume different domains or languages and we do not train a ranker but rather use a pseudo-feedback-based relevance model.
We note that existing methods for diversifying search results (e.g., [2, 12]) focus on a single retrieved list and on a notion of diversification -- i.e., coverage of query aspects -- which is different than that we address here; that is, the overlap with another result list.
3. SEARCH ENGINE RESPONSES
FRAMEWORK
Let q be a query which is fixed here and after. Let C be a corpus of documents upon which two search engines perform a search in response to q. One of the search engines, henceforth referred to as strong, is assumed to have a more effective relevance ranking function than that of the other -- the so called weak engine. Specifically, the ranking induced by the strong engine in response to q is assumed to be of higher effectiveness than that induced by the weak engine.
We use L[sntr] ong and L[wne]ak to refer to the lists of the n documents that are the most highly ranked by the strong and weak engines, respectively. The goal we pursue is devising an effective response strategy for the weak engine, given that it has access to the list L[sntr] ong of the strong engine.1 By response we mean producing a new result list, L[wne]ak;response, composed of n documents, that will replace the original list, L[wne]ak; yet, L[wne]ak can be used to produce the response.
A key question is what makes a response "effective". Obviously, L[wne]ak;response should be of the highest possible effectiveness, preferably, not significantly lower than that of L[sntr] ong. Supposedly, then, a highly effective response is setting L[wne]ak;response d=ef L[sntr] ong; that is, having the weak engine replicate the result list produced by the strong engine. However, assuming that the strong search engine already has a well established, and wide, base of users, such a response is not likely to result in any incentive for users to switch engines. Therefore, the second criterion we set for an effective response is that the overlap, in terms of shared documents, between L[wne]ak;response and L[sntr] ong will be minimal; i.e., the goal is to differentiate the weak engine from the strong engine.
In summary, the weak engine should produce a result list that is as diverse as possible, and competitive in terms of effectiveness, with respect to the result list produced by the
1In practical settings, the weak engine can potentially record, from time to time, the results produced by the strong search engine, specifically, in response to common queries.

strong engine. In doing so, the weak engine can use its original list L[wne]ak and that of the strong engine, L[sntr] ong.
3.1 Relevance Modeling as a Basis for Response Strategies
The basic assumption underlying the strategies that we employ below is that there are relevant documents in L[wne]ak that are not in L[sntr] ong. The reason could be, for example, a different coverage of the indexes of the two engines.
Yet, it is not necessarily the case that these documents are
ranked high enough due to the relatively weak relevance
ranking function of the weak engine. Thus, we use a relevance language model R [10] induced from L[skt]rong -- the k ( n) documents that are the highest ranked in L[sntr] ong -- to re-rank L[wne]ak; L[wne]ak;re-rank denotes the resultant ranked list. As R reflects a model of relevance of the strong engine
with respect to the query q, we assume that the ranking of L[wne]ak;re-rank is of higher effectiveness than that of L[wne]ak. We provide empirical support to this assumption in Section 4. The re-ranking of L[wne]ak using R is based on the cross entropy between R and the language models induced from the
documents. Details regarding (relevance) language model
induction are provided in Section 4.1.
In what follows we present several strategies of producing the final result list of the weak engine, L[wne]ak;response, using the lists L[wne]ak;re-rank and L[sntr] ong.
3.2 Response Strategies
The first response strategy, WeakReRank, is simply using L[wne]ak;re-rank for L[wne]ak;response. The source for diversity with the strong list L[sntr] ong is the assumed existence of documents in L[wne]ak that are not in L[sntr] ong. The presumably improved effectiveness of L[wne]ak;re-rank is due to the way it was created; that is, re-ranking L[wne]ak using a relevance model induced from L[sntr] ong.
The second response strategy is a probabilistic round robin
procedure, henceforth referred to as ProbRR. We create L[wne]ak;response top down by scanning the lists L[wne]ak;re-rank and L[sntr] ong also top down. The next document selected for L[wne]ak;response is taken from L[wne]ak;re-rank with probability p and from L[sntr] ong with probability 1-p. (Documents that were already selected are skipped.) Smaller values of p result in more documents taken from L[sntr] ong. Accordingly, the overlap of the final result list L[wne]ak;response with L[sntr] ong can increase and the effectiveness is potentially maintained at the same level as that of L[sntr] ong. Hence, ProbRR enables a certain degree of control over the effectiveness and diversification of L[wne]ak;re-rank with respect to L[sntr] ong using different values of p. However, L[wne]ak;re-rank can contain documents that are also in L[sntr] ong. Thus, higher values of p do not necessarily directly translate to increased diversity with respect to L[sntr] ong.
To directly control the level of diversity of L[wne]ak;response with respect to L[sntr] ong, we examine a variant of ProbRR termed ProbResRR -- the third response strategy we propose. Instead of using L[wne]ak;re-rank and L[sntr] ong we use

920

L[wne]ak;re-rank \ L[sntr] ong and L[sntr] ong; i.e., we remove from L[wne]ak;re-rank documents that are in L[sntr] ong and maintain the ranking of the residual documents. We then apply the same procedure described above for ProbRR to create the final result list L[wne]ak;response. Using larger values of p results in increased diversity with respect to L[sntr] ong.
4. EVALUATION
4.1 Experimental Setup
We used the Web tracks of TREC 2009­2011, henceforth TREC-2009, TREC-2010, and TREC-2011, to create the strong vs. weak search engine setting. We focused on runs submitted for the ClueWeb09 category B collection which is composed of around 50 million documents.
We randomly selected 30 pairs of runs from all those submitted and which contain at least 1000 documents as results for each query in the track. In each pair of runs, the result lists of the run whose MAP (@1000) is higher serve for the lists of the strong engine, L[sntr] ong, and those of the run with the lower MAP serve for the lists of the weak engine, L[wne]ak. Each result list contains n = 1000 documents. We report average performance over the 30 samples. Thus, here, the strong and weak engines are represented by "averages" over runs of which one is on average more effective than the other.
Titles of TREC topics serve for queries. Stopwords on the INQUERY list were removed from queries but not from documents. The Indri toolkit (www.lemurproject.org/indri) was used for experiments.
To evaluate retrieval performance, we use MAP (@1000) and NDCG (@20). Statistically significant differences of performance, computed over the 30 pairs of runs, were determined using the two-tailed paired t-test at a 95% confidence level. To measure the diversity of the result list of the weak engine with respect to that of the strong engine, we use the overlap (i.e., number of shared documents) at the top ten (OV@10) and twenty (OV@20) ranks of the two lists.
We use Dirichlet-smoothed document language models with the smoothing parameter set to 1000. For the relevance model R, we use the rank-based RM3 model [5] which is constructed from the top k (= 10) documents in the strong engine's result list. (We use ranks rather than retrieval scores as the latter are not assumed to be known.) The number of terms used by RM3, and its query anchoring parameter, are set to the default values of 50 and 0.5, respectively. Using this (under optimized) default parameter setting allows to demonstrate the potential of using the relevance modeling idea as a basis for producing responses.
As a reference comparison response strategy we use the highly effective CombMNZ fusion method [6] to merge the result lists of the weak and strong engines. Document scores induced from ranks, as suggested in [3], are used in CombMNZ. As all other fusion methods, CombMNZ addresses (explicitly) only one of the two criteria for effective response strategy -- i.e., retrieval effectiveness.
4.2 Experimental Results
The performance numbers of all methods are presented in Table 1. We use ProbRR(p) and ProbResRR(p) to indicate that the ProbRR and ProbResRR response strategies were used with the probability parameter p.

Table 1 shows that Strong is much more effective than Weak for both MAP and NDCG; the differences are substantial and statistically significant for all experimental settings. Furthermore, the overlap between Strong and Weak, as measured by OV@10 and OV@20, is low. Thus, the experimental setting we used adheres to the problem definition: (i) the strong search engine is (much) stronger in terms of retrieval effectiveness, and (ii) the overlap between the result lists of the strong and weak search engines is not high.
We also see in Table 1 that WeakReRank is quite an effective response strategy; specifically, in comparison to a highly effective fusion method (CombMNZ) in terms of both retrieval effectiveness and overlap at top ranks with Strong. WeakReRank's retrieval effectiveness can be statistically significantly worse than that of Strong. However, WeakReRank outperforms Weak for MAP and NDCG, with all the improvements being statistically significant. Although the overlap at top ranks of WeakReRank with Strong is larger than that of Weak, it is still quite low with respect to that of the other strategies considered. These findings attest to the effectiveness of using relevance modeling based on the result list of the strong search engine so as to re-rank the result list of the weak search engine.
Table 1 also shows that ProbRR is a highly effective response strategy in many cases. Evidently, the balance between retrieval effectiveness and overlap with Strong can be effectively controlled via the parameter p. Although in terms of overlap with Strong, ProbRR is somewhat less effective than WeakReRank and CombMNZ, in terms of retrieval effectiveness it is substantially better than the two and often better than Strong.
It is also evident in Table 1 that ProbResRR is less effective than ProbRR for MAP and NDCG. However, the overlap numbers for ProbResRR are lower than those for ProbRR. This finding is not surprising because ProbRR uses the re-ranked list of the weak engine while ProbResRR uses the residual documents in the same list that remain after the removal of documents that also appear in the result list of the strong engine.
The effectiveness-diversity tradeoff. We next study the
effect of the parameter p used in ProbRR and ProbResRR on the tradeoff between the retrieval effectiveness of the response list of the weak search engine, as measured using MAP, and its overlap with the result list produced by the strong search engine, measured using OV@10.
Figure 1 presents the results of setting p to values in {0, 0.1, . . . , 1}; p = 0 amounts to using only the result list of the strong search engine in ProbRR and ProbResRR, while p = 1 amounts to using the result list L[wne]ak;re-rank.
We can see that for ProbResRR, MAP decreases with increasing values of p. The reason is that fewer documents that appear in the result list of the strong engine are used. Furthermore, ProbRR outperforms ProbResRR for all p > 0. In addition, we see that ProbRR can attain its optimal performance for 0 < p < 1 (i.e., outperform both Strong and WeakReRank) which echoes findings in work on fusion [4].
For both ProbRR and ProbResRR, OV@10 decreases with increasing values of p, as fewer documents are selected from the result list of the strong engine. As expected, for the same value of p (> 0), the OV@10 of ProbRR is higher than that of ProbResRR. Yet, for the same value of overlap, the MAP of ProbRR is higher than that of ProbResRR.

921

Strong
Weak WeakReRank
CombMNZ
ProbRR(0.2) ProbRR(0.5) ProbRR(0.7) ProbResRR(0.2) ProbResRR(0.5) ProbResRR(0.7)

MAP
17.1w
11.4s 16.7w
13.4sw
18.3sw 18 7s
.w 18.6sw 15.4sw 11.2s 8.2sw

TREC-2009 NDCG OV@10 OV@20

26.5w -

-

19.5s 30 3s
.w
21.6sw
27.8sw 29.2sw 29.7sw 23.7sw 18.4s 13.9sw

17 4 .
35.0
40.9
87.2 67.5 55.4 81.0 52.2 32.6

19 4 .
34.5
43.6
87.1 67.6 54.8 80.5 52.4 33.2

MAP
19.9w
13.6s 16.5sw
16.4sw
20 6s .w
20.2w 19.4w 17.9sw 12.8s 9.0sw

TREC-2010 NDCG OV@10 OV@20

25 2 .w

-

-

16.9s 19.8sw
18.9sw
24.1sw 22.6sw 21.5sw 21.8sw 16.0s 11.7sw

19 7 .
29.3
38.3
85.2 63.3 49.7 79.4 50.0 29.9

23 3 .
30.5
42.1
85.0 64.4 50.6 79.8 50.5 29.8

MAP
19.3w
13.1s 17.6sw
17.2sw
21.7sw 22 3s
.w 22.0sw 19.2w 15.4sw 12.0s

TREC-2011 NDCG OV@10 OV@20

28.0w -

-

21.6s 27.9w
24.0sw
28.9sw 29.6sw 29 7s
.w 26.2sw 21.7s 17.6sw

18 6 .
30.3
45.1
86.0 65.6 51.4 80.3 50.4 30.4

20 7 .
31.1
47.5
86.3 66.1 52.8 80.0 50.2 30.6

Table 1: Main result table. The numbers in the parentheses for the ProbRR and ProbResRR strategies are the value of the p parameter. The highest result in a column for MAP and NDCG, and the lowest for OV@10 and OV@20, is boldfaced. `s' and `w' mark statistically significant differences, for MAP and NDCG, with Strong and Weak, respectively.

MAP OV@10
MAP OV@10
MAP OV@10

TREC-2009 20

18

16

14

12

10

8

6

4

ProbRR (MAP)

ProbRR (OV@10)

2

ProbResRR (MAP)

ProbResRR (OV@10) 0

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

p

100 80 60 40 20 0 1

TREC-2010 22

20

18

16

14

12

10

8

6

4

ProbRR (MAP) ProbRR (OV@10)

2

ProbResRR (MAP)

ProbResRR (OV@10) 0

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

p

100 80 60 40 20 0 1

TREC-2011 24

22

20

18

16

14

12

10

8

ProbRR (MAP)

ProbRR (OV@10)

6

ProbResRR (MAP)

ProbResRR (OV@10) 4

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

p

100 80 60 40 20 0 1

Figure 1: The effect of the parameter p on MAP and OV@10. The y-axis on the right of a figure is the range of OV@10. The y-axis on the left of a figure is the range of MAP.

Thus, we arrive to the conclusion that tuning the parameter p in ProbRR and ProbResRR helps to effectively control the balance between retrieval effectiveness and overlap (diversity) with the strong engine. Furthermore, ProbRR is a more effective response strategy than ProbResRR as it allows to attain improved level of retrieval effectiveness for the same level of overlap with the strong search engine.
5. CONCLUSIONS AND FUTURE WORK
We presented the first (preliminary) attempt to address the search engine duel problem; namely, how can a search engine with a relatively weak relevance ranking function compete with a search engine with a much stronger relevance ranking function? We devised an algorithmic response framework which consists of several strategies that can be used by the weak search engine. We consider a response to be effective if it results in improved search effectiveness and the search results are different than those presented by the strong engine. Empirical evaluation demonstrated the merits of our response strategies and shed some light on the (relevance) effectiveness-diversity tradeoff embodied in our bi-modal criteria for response effectiveness.
Devising additional criteria for response effectiveness, along with developing additional corresponding response strategies, is the first future venue we intend to explore. We also plan on devising response strategies for the strong engine.
Acknowledgments We thank the reviewers for their comments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center. This work has also been supported in part by Microsoft Research through its Ph.D. Scholarship Program.

6. REFERENCES
[1] J. Callan. Distributed information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 5, pages 127­150. Kluwer Academic Publishers, 2000.
[2] J. G. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR, pages 335­336, 1998.
[3] G. V. Cormack, C. L. A. Clarke, and S. Bu¨ttcher. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proc. of SIGIR, pages 758­759, 2009.
[4] W. B. Croft. Combining approaches to information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 1, pages 1­36. Kluwer Academic Publishers, 2000.
[5] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A language modeling framework for selective query expansion. Technical Report IR-338, Center for Intelligent Information Retrieval, University of Massachusetts, 2004.
[6] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proc. of TREC-2, 1994.
[7] W. Gao, J. Blitzer, and M. Zhou. Using english information in non-english web search. In Proc. of the 2nd ACM workshop on Improving Non English Web Searching, iNEWS, pages 17­24, 2008.
[8] W. Gao, P. Cai, K.-F. Wong, and A. Zhou. Learning to rank only using training data from related domain. In Proc. of SIGIR, pages 162­169, 2010.
[9] N. Immorlica, A. T. Kalai, B. Lucier, A. Moitra, A. Postlewaite, and M. Tennenholtz. Dueling algorithms. In Proc. of STOC, pages 215­224, 2011.
[10] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. of SIGIR, pages 120­127, 2001.
[11] L. Meister, O. Kurland, and I. G. Kalmanovich. Re-ranking search results using an additional retrieved list. Information Retrieval, 14(4):413­437, 2011.
[12] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proc. of WWW, pages 881­890, 2010.

922

On Run Diversity in "Evaluation as a Service"

Ellen M. Voorhees,1 Jimmy Lin,2 and Miles Efron3
1 National Institute of Standards and Technology 2 The iSchool, University of Maryland, College Park 3 Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign
ellen.voorhees@nist.gov, jimmylin@umd.edu, mefron@illinois.edu

ABSTRACT
"Evaluation as a service" (EaaS) is a new methodology that enables community-wide evaluations and the construction of test collections on documents that cannot be distributed. The basic idea is that evaluation organizers provide a service API through which the evaluation task can be completed. However, this concept violates some of the premises of traditional pool-based collection building and thus calls into question the quality of the resulting test collection. In particular, the service API might restrict the diversity of runs that contribute to the pool: this might hamper innovation by researchers and lead to incomplete judgment pools that affect the reusability of the collection. This paper shows that the distinctiveness of the retrieval runs used to construct the first test collection built using EaaS, the TREC 2013 Microblog collection, is not substantially different from that of the TREC-8 ad hoc collection, a high-quality collection built using traditional pooling. Further analysis using the `leave out uniques' test suggests that pools from the Microblog 2013 collection are less complete than those from TREC-8, although both collections benefit from the presence of distinctive and effective manual runs. Although we cannot yet generalize to all EaaS implementations, our analyses reveal no obvious flaws in the test collection built using the methodology in the TREC 2013 Microblog track.
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation
Keywords: test collections; reusability; meta-evaluation
1. INTRODUCTION
Large-scale community-wide evaluations such as TREC operationalize the Cranfield paradigm by building retrieval test collections that support IR research [2]. A test collection consists of a corpus of documents, a set of information needs called topics, and relevance judgments that specify which documents are relevant for which topics. A funda-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609484.

mental assumption in the paradigm is that researchers can acquire a test collection's document corpus. But what if this is not possible? This was the challenge in the TREC Microblog track evaluation, where the corpus is comprised of tweets posted on Twitter. Since the company's terms of service forbid redistribution of tweets, a test collection built on tweets cannot be distributed in a traditional manner.
The solution developed by the track organizers, termed "evaluation as a service" (EaaS), was to provide an API through which the tweet collection can be accessed for completing the evaluation task [4, 3]. This API, consisting of a basic keyword search interface, was the only method through which track participants could access the collection. The operational deployment of the EaaS model was successful in attracting participation at TREC 2013, which provides encouraging evidence that EaaS could be generalized to other evaluation scenarios involving sensitive data (e.g., medical or desktop search). Before expanding the use of the approach, however, we need to know whether EaaS has any substantive impact on the quality of the collections produced.
One concern about the EaaS model is a potential lack of diversity in the retrieval runs used to build the test collection. The criticism is this: if everyone must use the API, won't they all end up doing basically the same thing? Diversity is important from at least two perspectives. Previous work has tied the reusability of a collection built using pooling to the diversity of the retrieval runs that form the pools [1]. Diversity in participant submissions also serves as a proxy indicator that researchers are trying different (novel) retrieval techniques. The contribution of this paper is an analysis of the diversity of the pools in the TREC 2013 Microblog track (using EaaS) compared to previous evaluations--including the first two years of the Microblog track in TRECs 2011 and 2012--where participants had access to the entire corpus. We first propose a novel metric, called Run Average Overlap (RAO), that measures the overall distinctiveness of one retrieval run with respect to a given set of runs. In a second analysis, we apply another test of reusability, the leave out uniques (LOU) test. Analyses show no obvious differences in the characteristics of the pools, suggesting that the EaaS model can be used to build reusable collections.
2. BUILDING POOLED COLLECTIONS
Pooling [8] is a method for building test collections when the target document corpus is too large to make exhaustive relevance judgments for every document for every topic. A pooled collection is built from a set of retrieval runs sub-

959

Table 1: Test collection statistics.

Name Docs Topics Depth Runs Groups

TREC-8 528 K

50 100 71

40

MB2012 16 M

60 100 121

33

MB2013 243 M

60

90 71

20

mitted by participants of the process, typically as part of a community-wide evaluation such as TREC.1 A run is the output of a retrieval system for each topic in the collection; documents are assumed to be ordered by decreasing likelihood of relevance to the topic. The pool for a topic is comprised of the union of the top K (called the pool depth) documents retrieved for that topic by each run. Only documents in the pool are judged for relevance by a human; all other documents are assumed to be not relevant.
Test collections are research tools that are most useful when they are reusable, that is, when they fairly evaluate retrieval systems that did not contribute to their construction [1]. Thus, a reusable test collection can be used to evaluate systems even after the original evaluation that created the collection has concluded. For a pooled collection to be reusable, the pool must contain enough of the relevant documents in the collection to be an unbiased sample of the relevant documents. In practice, the completeness of a pool is a function of (at least) the pool depth, the effectiveness of the runs that comprise it, the diversity of the runs, and the true number of relevant documents [10, 9].
Many of the test collections built in TREC, including the ad hoc collections and the Microblog collections, were built through pooling. The TREC ad hoc collections, especially the TREC-8 collection, have been subject to a number of analyses and have been used in hundreds of retrieval experiments whose outcomes have proven to generalize. Thus, these collections are generally accepted as high-quality test collections [5]. Our basic premise is that the TREC 2013 Microblog collection is reusable if it displays characteristics similar to the TREC ad hoc collections. The particular characteristics of interest are the distinctiveness of the runs over which the pools are formed and the number of relevant documents found by a single participant.
More details about the three collections used in this paper are given in Table 1. (Our experiments also considered the ad hoc test collections from TREC-6 and TREC-7, but they yielded similar results, and so for brevity we only present results from TREC-8 here.) The first column of the table gives the name of the collection as used in the remainder of this paper. The corpus for TREC-8 consists of approximately half a million (mostly) newswire articles. The pools were predominantly constructed from the 71 ad hoc runs that were judged (from a total of 129 submitted runs), but runs from some other tracks contributed small numbers of documents to the pools as well.2 The other two collections were built as part of the TREC Microblog tracks in 2012 and 2013, whose document corpora consist of tweets posted to Twitter. The document corpus used for the MB2012 track (called Tweets11) contains approximately 16 million tweets, which were distributed by publishing a list of tweet ids that participants then fetched [7, 6]. The EaaS model enabled the MB2013 corpus to be much larger, about 243 million
1http://trec.nist.gov 2Values reported in Table 1 include only the ad hoc runs.

tweets [4]. Relevance judgments for the MB collections were made on a 3-point scale ("not relevant", "relevant", "highly relevant"), but in this work we ignore the different degrees of relevance and use both higher grades as "relevant".
The final column of Table 1 gives the number of participants that contributed runs. TREC generally allows participating groups to submit more than a single run to a given task. In practice, runs submitted by the same participant tend to be much more similar to each other than they are to runs from other participants. Analyses of run diversity need to account for this phenomenon.

3. RUN AVERAGE OVERLAP

Our biggest concern regarding the EaaS model is whether the common API imposed on participants restricts their retrieval approaches to the extent that different participants are compelled to submit essentially similar runs. In order to answer this question, we introduce a novel metric, Run Average Overlap (RAO), which is a measure of the tendency of a set of runs to retrieve documents in common.
Run Average Overlap is computed for a given run with respect to a specific set of runs. In what follows we compute the RAO score for each run in a pool set with respect to the pool set. Say that a run O retrieves document d for a topic t if d occurs in the top K ranks for t in O. When K is set equal to the pool depth, as done here, O retrieves d if it contributes d to the pool. Let P be the total number of participants that have runs in the run set, and Pd be the number of distinct participants that retrieve d. Finally, let T be the total number of topics and n be the number of documents retrieved by O for t. Then:

1X1X 1 RAO(O) =
T t n d Pd

In words, the RAO score for a run is the mean over all topics

of the score computed for individual topics, where the per-

topic score is the mean over the documents retrieved by O of

the reciprocal of the number of participants (including itself)

that retrieve that document. The greater the RAO score the

more distinctive the result set. The minimum RAO score is

1 P

which signifies that O retrieved only documents that all

other participants also retrieved. The maximum score is 1.0,

which means that no other participant retrieved any docu-

ment that O retrieved. Note that document distinctiveness

for the purposes of computing RAO is computed over partic-

ipants, and thus the number of runs submitted by any given

participant is not an issue.

While RAO scores close to

1 P

for all runs in a pool set

indicate that the runs are not diverse, the presence of runs

with high scores is not sufficient to conclude that the pool

is appropriately diverse. The RAO score is computed over

all retrieved documents, not relevant documents. Distinc-

tive but ineffective runs are easy to produce (for example,

by selecting random documents from the collection) but un-

helpful for pool building. Thus, the RAO score must be

used in conjunction with an effectiveness score to ensure

the presence of distinctive and effective runs in the pool set.

We use R-precision as the effectiveness measure here since it

was the primary metric for the TREC 2013 Microblog track.

Figure 1 shows plots of RAO vs. R-precision for the three

collections. Each point in a graph represents one pool run.

Manual runs (runs for which there was some human pro-

cessing in the construction of the run) are plotted as open

960

TREC-8 1

MB2012 1

MB2013 1

0.8

0.8

0.8

0.6

0.6

0.6

RAO RAO RAO

0.4

0.4

0.4

0.2

0.2

0.2

0

0

0.2 0.4 0.6 0.8

1

R-Precision

0

0

0.2 0.4 0.6 0.8

1

R-Precision

0

0

0.2 0.4 0.6 0.8

1

R-Precision

Figure 1: Run Average Overlap (y-axis) vs. mean R-precision (x-axis) for pool runs. Manual runs are plotted as open squares and automatic runs as filled circles.

squares while automatic (i.e., non-manual) runs are plotted as filled circles. A highly distinctive and highly effective run would fall in the upper right-hand corner of the graph.
The plots for the MB2013 and the TREC-8 collections are very similar. These collections exhibit a general inverse relationship between effectiveness and distinctiveness (points run roughly from upper left to lower right). This is simply a manifestation of the fact that there are many fewer relevant documents than non-relevant documents, and thus many fewer effective retrieved sets than ineffective retrieved sets. But, importantly, we see that both the TREC-8 and MB2013 collections contain outlier runs that are both distinctive and effective--these represent high-quality manual runs that contributed to the pools.
The graph for the MB2012 collection, which was not constructed using the EaaS model, is different from the others. The pool runs for MB2012 have a much narrower range of effectiveness and have a much greater RAO score on average. Note, however, that this is the one collection that does not have any runs that are outlier effective runs, and overall effectiveness is relatively poor. Thus, the large RAO scores are probably another manifestation of the tendency for ineffective runs to be different.
4. UNIQUELY RETRIEVED RELEVANT
In our second analysis, we examined uniquely retrieved relevant documents, which are relevant documents that were contributed to the pool by only a single participant. The leave out uniques test (LOU) [1] gauges the reusability of a test collection by examining the effect on evaluation scores of a participant's runs when the participant's uniquely retrieved relevant documents are removed from the judgment set. The test computes the evaluation scores that would have resulted had the participant not been involved in the collection building process.
Figure 2 plots the results of the LOU test for each of the three collections. Each run is plotted as a point, with manual runs shown as empty boxes and automatic runs as filled circles. The x-axis is the MAP score of the run as computed using the official relevance judgment set for that collection, and the y-axis is the MAP score as computed using the judgment set with the participant's unique relevants removed. When both scores are the same, the point falls on the diagonal line. A run whose MAP score decreases

Table 2: Collection characteristics on uniquely retrieved relevant documents and LOU test results.

% of total relevant that are unique max % of total uniques retrieved by a single participant

TREC-8 24.0
42.2

MB2012 30.7
19.7

MB2013 39.6
43.2

mean % diff in LOU scores (auto runs only) max % diff in LOU scores (auto runs only) # runs with diff in scores > 1% (auto only) / total

0.14 0.84 0/54

0.64 8.36 16/112

1.12 7.95 15/57

when unique relevants are removed falls below the line, and a run whose MAP score improves when unique relevants are removed lies above the line.
The presence of distinctive and effective manual runs in the TREC-8 and MB2013 collections is obvious in the LOU test results. These runs fall well below the line of equal scores, and are the only runs to do so. The degradation in MAP scores is not surprising given that the participants who submitted these runs contributed large numbers of unique relevant documents to the pools. As shown in the second row of Table 2, for these two collections the participant who submitted the most effective manual runs contributed more than 40% of the unique relevant documents. In contrast, the largest percentage of unique relevant documents contributed by a single participant in the MB2012 collection is just 20%. The percentage of total relevant documents that are uniquely retrieved relevants is given in the first row of the table. Here the TREC-8 and MB2013 collections differ: the MB2013 collection has a much larger percentage of total relevant documents that are unique.
A large percentage of total relevant documents that are uniquely retrieved relevant documents is one indication that a test collection may be less reusable, and the LOU test results are consistent with this observation. In Figure 2, all of the automatic runs are on the line of equal scores for the TREC-8 collection, but there is more movement away from the line for the two Microblog collections. The final three rows of Table 2 quantify this movement. The table gives statistics regarding the distribution of the percentage difference in MAP scores in the LOU test when considering

961

TREC-8 1

MB2012 1

MB2013 1

0.8

0.8

0.8

LOU MAP LOU MAP LOU MAP

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

0

0.2 0.4 0.6 0.8

1

Original MAP

0

0

0.2 0.4 0.6 0.8

1

Original MAP

0

0

0.2 0.4 0.6 0.8

1

Original MAP

Figure 2: Results of "leave out uniques" test. Original MAP scores are plotted on the x-axis and LOU MAP scores on the y-axis. Manual runs are plotted as open squares and automatic runs as filled circles.

only automatic runs whose original MAP scores were at least 0.1 (because percentage differences are artificially magnified for ineffective runs). The statistics computed are the mean of the percentage difference over these automatic runs, the maximum percentage difference observed in the automatic runs, and the count of the number of runs that have a percentage difference greater than 1% (out of the total). No TREC-8 automatic run has a percentage difference greater than 1%, and since differences that small are within the level of evaluation noise [9], the collection is regarded as highly reusable. The MB2013 collection has a mean percentage difference of about 1%, with 15 runs (out of 57) having at least a 1% difference and a maximum difference of about 8%.
Note that this amount of change in LOU test results is not a consequence of using the EaaS model. In fact, the large percentage of relevant documents that are unique is actually confirmation that the evaluation method accommodated distinctive runs (i.e., the API did not hamper researchers' ability to generate runs that are both effective and distinctive), and the MB2012 collection, which was not built using EaaS, has similar characteristics. The likely explanation is the size of pools relative to the size of the corpus [1]. Corpora used in the two Microblog collections are much larger than the TREC-8 collection, so the judgment pools represent a much smaller percentage of the collection than for the TREC-8 collection. Also note that this level of change is unlikely to have much impact in the utility of the collection as a research tool. As can be seen in Figure 2, the relative ordering of runs is largely stable even in the presence of MAP score differences. At worst, researchers who encounter many unjudged tweets in top ranks in new runs should be more cautious in their conclusions.
5. CONCLUSION
The evaluation as a service model for constructing retrieval test collections can offer significant advantages over traditional construction techniques, but only if it does not fundamentally hamper innovation and it leads to highquality resources. This paper examined the one collection that has been built using the EaaS model to date, the TREC 2013 Microblog collection, and found its run diversity to be similar to the high-quality TREC-8 ad hoc collection. The results of the leave out uniques test suggest that pools from the Microblog 2013 collection are less complete than pools

from TREC-8, but both collections strongly benefit from the presence of distinctive and effective manual runs.
Of course, these findings are for a single collection and a specific API. This implementation provides a generous number of documents in response to a query and a generous allotment of queries per participant. These choices contribute to good design, as a more restrictive API would likely have impacted submitted runs and the resulting test collection negatively. Although we cannot yet make statements about the EaaS model in general, the TREC 2013 Microblog collection does offer an existence proof that a high-quality retrieval test collection can be constructed using this new method.
6. ACKNOWLEDGMENTS
This work has been supported in part by NSF under awards IIS-1217279 and IIS-1218043. Any opinions, findings, conclusions, or recommendations expressed are the authors' and do not necessarily reflect those of the sponsor.
7. REFERENCES
[1] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Information Retrieval, 10:491­508, 2007.
[2] D. Harman. Information Retrieval Evaluation. Morgan & Claypool Publishers, 2011.
[3] J. Lin and M. Efron. Evaluation as a service for information retrieval. ACM SIGIR Forum, 47(2):8­14, 2013.
[4] J. Lin and M. Efron. Overview of the TREC-2013 microblog track. In TREC, 2013.
[5] C. D. Manning, P. Raghavan, and H. Schu¨tze. Evaluation in information retrieval. In Introduction to Information Retrieval, chapter 8, pages 153­154. Cambridge University Press, 2009.
[6] R. McCreadie, I. Soboroff, J. Lin, C. Macdonald, I. Ounis, and D. McCullough. On building a reusable Twitter corpus. In SIGIR, 2012.
[7] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. Overview of the TREC-2011 microblog track. In TREC, 2011.
[8] K. Sparck Jones and C. van Rijsbergen. Report on the need for and provision of an "ideal" information retrieval test collection. British Library Research and Development Report 5266, 1975.
[9] E. M. Voorhees. The philosophy of information retrieval evaluation. In CLEF, 2002.
[10] J. Zobel. How reliable are the results of large-sacle information retrieval experiments? In SIGIR, 1998.

962

Evaluating Answer Passages using Summarization Measures
Mostafa Keikha, Jae Hyun Park, W. Bruce Croft
CIIR, University of Massachusetts Amherst, Amherst, MA
{keikham, jhpark, croft } @cs.umass.edu

ABSTRACT
Passage-based retrieval models have been studied for some time and have been shown to have some benefits for document ranking. Finding passages that are not only topically relevant, but are also answers to the users' questions would have a significant impact in applications such as mobile search. To develop models for answer passage retrieval, we need to have appropriate test collections and evaluation measures. Making annotations at the passage level is, however, expensive and can have poor coverage. In this paper, we describe the advantages of document summarization measures for evaluating answer passage retrieval and show that these measures have high correlation with existing measures and human judgments.
1. INTRODUCTION
Some information retrieval (IR) queries can be best answered with a web page, others can be answered with a single fact or named entity. These types of queries, known as navigational and factoid questions, have been well-studied in the literature and the techniques for generating answers for them form the basis of many search engine result pages in both web and mobile environments. The category of informational queries is, however, very broad and many queries could potentially best be answered with a text passage that is longer than a factoid, but considerably shorter than a full web page. Passage retrieval has also been studied previously [3, 8, 2], but the main aim of this research was to improve the document ranking for a query by using passage-level evidence in combination with other features. Instead, our hypothesis is that there are queries for which a passage-level answer can be superior to a document-level answer and, for those queries, result lists that include passages will be more effective than documents alone.
In this paper, we focus on the critical issue of how to evaluate passage retrieval systems. Traditional document-level evaluation measures are not directly applicable to passage retrieval, because each variation of a passage retrieval model
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609485 .

retrieves different passages. In order to use those documentlevel measures one would need to manually assess all possible passages, which is not practical. As an alternative, character-based measures have been developed that treat each character as a document and evaluate them using existing precision and recall measures [1, 5]. These characterbased measures also have limitations and require the annotation of exact characters in order to consider it relevant. Annotating all the relevant passages in the large collections used currently is also not practical. On top of their limitations, these measures have been rarely studied for answer retrieval. Evaluating answer passages, as opposed to relevant passages, is even more difficult. Finding proper answers is usually more challenging and more ambiguous than finding relevant passages which makes answer annotation even more time consuming.
We employ summarization evaluation metrics for evaluating answer passage retrieval methods. These measures capture similarity of candidate passages to a sample of known or "ideal" answer passages. They do not require exhaustive annotation of passages, which makes them reasonable candidates for passage retrieval evaluation. Summarization measures address some important aspects of evaluation such as the amount of noise in a passage, size of a passage and the coverage over the ideal answers that are all crucial for a proper evaluation measure. We show that these measures are reasonably correlated with existing measures and human assessments. Further, we describe a cross-collection evaluation scenario as a new application of these measures that is not possible with existing metrics.
2. RELATED WORK
Passage analysis has been studied in the information retrieval community from different perspectives. Incorporating passages into document retrieval systems is the most common use of passage-level information [3, 8, 2]. Most of the work on passage retrieval uses passages as an intermediate representation for retrieving other types of objects such as documents. Less attention has been paid to directly retrieving passages instead of documents as final answers to a query. This problem was partly addressed in the HARD track in TREC, INEX ad hoc track and TREC Genomics track [1, 5, 4]. As part of these tasks, new evaluation measures were proposed. The proposed metrics, e.g., R-precision, use characters from annotated relevant passages that are found in the top retrieved passages for evaluating systems [1, 4]. The proposed character-level measures are generally similar to traditional document-level measures but

963

use characters as opposed to documents. The 1-click task from the NTCIR workshop is similar but is more focused on factual queries and requires an extra human annotation phase to extract the most important pieces of text (nuggets) for evaluation [6].
While we do not limit our study to any specific type of queries, we are interested in the situation where we have one or more ideal answers for the query and would like to compare the retrieved passages to the ideal answer. Thus we do not require any nugget annotation and the whole annotated answer is considered as a unit for evaluation. Content-based similarity measures were part of the HARD track initial evaluation proposal [1]. However, they were not employed in the official track evaluation and were not studied afterwards.
In order to compare a retrieved passage with an ideal annotated answer, we study the feasibility of using summarization measures. Previous studies show that summarization metrics are highly correlated with human judgments and can capture the quality of summaries [7]. In this paper, we study the behavior of those measures for evaluating answer passage retrieval and compare them to human judgments and existing measures. Further, we study the sensitivity of those measures to noisy judgments. Finally, we discuss an interesting application of these measures where we evaluate passages retrieved from one collection using judgments developed for another collection. This scenario that we call cross-collection evaluation is not possible using existing measures and has the potential to facilitate the creation of evaluation benchmarks.
3. ROUGE EVALUATION METRICS
Our evaluation is based on summarization evaluation metrics that are implemented in the ROUGE package [7]. Originally, the metrics were used to compare an automatically generated summary or translation against a reference or a set of reference (human-generated) summaries or translations. Essentially, any summarization metric compares two pieces of text based on their overlapping concepts. Depending on the type of concept, one can define different measures. We explore the following measures that are the most successful ones for the summarization task:
· ROUGE-N: a measure based on the N-gram co-occurrence statistics. We use N with values 1 and 2 that gives us unigram and bigram instances.
· ROUGE-S : this measure considers the number of overlapping skip-bigrams in the evaluation. A skip-bigram is any pair of words in the same order with a limited distance between them. We use ROUGE-S4 that considers any bigram with a distance less than 4.
· ROUGE-SU: this measure considers both unigrams and skip-bigrams in the evaluation. We use ROUGE-SU4 that considers any bigram with a distance less than 4.
ROUGE-1, ROUGE-2 and ROUGE-SU4 are the official evaluation metrics used in the Document Understanding Conference (DUC) for evaluating summaries [7]. We perform stemming and stop word removal before comparing the two passages. In any of the ROUGE measures, the number of overlapping concepts can be compared to the total number of concepts in the retrieved passage or in the ideal answer or combination of the two that results in a precision, recall and F1 variation of each measure [7]:

ROUGE-xprecision

=

|IdealAnswerx RetrievedP assagex| |RetrievedP assagex|

ROUGE-xrecall

=

|IdealAnswerx RetrievedP assagex| |I dealAnswerx |

ROUGE-xF 1

=

2 × ROUGE-xprecision × ROUGE-xrecall ROUGE-xprecision + ROUGE-xrecall

x shows the concept type and has values 1, 2, S4 and SU 4 representing four different concept types. P assagex shows the set of x concepts extracted from the P assage. As we can see, each of these measures calculates one value for each passage.
The recall variation captures what portion of the relevant concepts (concepts in the ideal answer) is present in the retrieved passage. On the other hand, the precision variation of each measure captures what portion of the retrieved concepts are among the relevant concepts. The F1 measure combines the precision and recall values and gives a single evaluation value for each passage [7]. Analogous to the size of the ranked list in the traditional IR metrics, with increasing size of the retrieved passage we generally expect recall to increase and precision to decrease.
When there are multiple ideal answers for a query, each retrieved passage will have multiple evaluation values; one value for each ideal answer. We explored two options for aggregating these values that includes averaging and maximum value. Our experiments showed that maximum value is a better choice for passage evaluation. Due to lack of space we only show the results of maximum function here.
After evaluating each single retrieved passage with respect to the query, we need to aggregate the the results in order to evaluate the ranked list as a whole. The simplest option would be to average the passage-level evaluation values over all the retrieved passages. We also explored other options in which we give more importance to the higher ranks, similar to the nDCG measure in document retrieval. While the nDCG variation improves the evaluation performance, the difference is not significant and for the sake of clarity and space we do not report those results in this paper.
Finally we average the evaluation values over all the queries to get the performance evaluation of a system.

4. COMPARISON TO EXISTING MEASURES
Character-level measures are the most comparable existing measures to the ROUGE measures. These measures have been studied before in the context of HARD track and Genomics track in TREC and the ad-hoc track in INEX [1, 5, 4]. They use a set of highlighted passages as relevant answers and treat each of the highlighted characters as a relevant item. Then they calculate existing evaluation measures such as precision and recall over characters.
In our first experiments, we assume that character-based measures are good evaluation measures and examine the correlation between them and ROUGE measures. We get a set of submitted runs to INEX and evaluate them using the official INEX evaluation toolkit. Then for all the submitted runs we extract their real text and evaluate them using ROUGE measures. Finally, we estimate the correlation between system performance using INEX measure and

964

Table 1: Correlation between ROUGE measures and

character-based MAP

Measure

Precision Recall F1

ROUGE-1

-0.01

0.69 0.61

ROUGE-2

0.57

0.70 0.69

ROUGE-S4

0.51

0.33 0.61

ROUGE-SU4 0.53

0.65 0.59

ROUGE measures. After cleaning and removing systems with non-valid passages, we have 39 systems to evaluate.
Table 1 shows the correlation between ROUGE measures and the character-based MAP measure. As we can see, except for ROUGE-1 that is based on the unigram overlap, the rest of the measures have quite high correlation with the character-based measure. The best measure is ROUGE2 that is based on bigram overlap and we can see that all its variations have high correlation.
These results show that ROUGE measures, while having other benefits that we will discuss later, can provide a similar ranking of systems to existing character-based measures.
5. COMPARISON TO HUMAN JUDGMENT
In the previous section, we assumed that character-based MAP is an ideal measures and having high correlation with it is a desired requirement. Given that this assumption is not necessarily true, in the next experiments we compare the ROUGE measures directly to human judgments.
For this analysis, we built a data set using the GOV2 collection and the corresponding TREC queries. Three human annotators including one graduate student and two undergraduate students were involved in the annotation process. The undergraduate students performed the main annotation task and the graduate student controlled the annotation results to make sure they are in a proper format and contain meaningful passages. We divided topics randomly in two different groups, one for each annotator. For each topic, we retrieved the top 50 documents using the Sequential Dependence Model (SDM), a state-of-the-art retrieval model. From the retrieved documents, we selected the relevant documents, based on the TREC relevance judgments, for the passage annotation phase. Each assessor annotated all the documents related to the topics assigned to him. Further, in order to study agreement between annotators, they also annotated the top five documents for the rest of the topics.
Annotators were asked to use our annotation toolkit and highlight all the answer passages in the document set. An answer passage is defined as a piece of text in a document that can answer the user information need. Our annotation guideline considers different properties of passages including how complete is the answer with respect to the query and how much non-relevant information it contains. Based on these criteria, we defined four level of answers as "perfect", "excellent", "good" and "fair". A perfect answer means the passage provides all the necessary information to answer the query and does not contain any non-relevant information. A fair answer provides some information regarding the query but it does not completely answer the query or it contains noise. Good and excellent answers are better that fair and worse than perfect answers. It is worth noting that all these answers are reasonable and the difference between them is generally marginal.
Our assessor found answer passages for 82 TREC queries and highlighted 8,027 passages, which is about 97 passages

Table 2: Correlation between ROUGE measures and

human judgment

Measure

Precision Recall F1

ROUGE-1

0.38

0.47 0.43

ROUGE-2 ROUGE-S4

0.43

0.47 0.45

0.42

0.46 0.44

ROUGE-SU4 0.42

0.47 0.45

per query on average. Among all the annotated passages 43% of them are perfect answers, 44% are excellent, 10% are good and the rest are fair answers.
Our annotators highlighted 84,381 words in the passage answers. Among these words, 59,693 of them are highlighted by one annotator, 46,660 of them by other annotator and 21,972 of them are highlighted by both annotators. Considering non-highlighted words in the judged documents as negative examples, the term-level kappa ratio between our annotators is about 0.38. This is comparable to answer-level agreement in previous studies where kappa value is reported about 0.3 [9].
In the next analysis, our goal is to test if ROUGE measures can distinguish between passage answers with different relevance levels. Since we have a graded relevance level for each passage, we can compare those grades to their scores assigned by ROUGE measures. For each query, we select some of the "perfect" passages with probability 0.5 as our ideal set of answers and we end up with about 20 ideal answers per query on average. We then evaluate the rest of the annotated passages using ROUGE measures by comparing them to the ideal answers. To this end, we assign a numeric value of 1,2,3,4 to "fair", "good", "excellent" and "perfect" grades respectively. We then calculate the correlation between these values and ROUGE outputs.
Table 2 shows the correlation values. As we can see, there is no significant difference between measures in this experiment. All the measures are comparably correlated with human judgments and all the correlation are at a statistically significant level with p-values less than 0.05. This shows that ROUGE measures can reasonably indicate the quality of the passages. It is worth noting that, the difference between different levels of relevance in our annotated passages, e.g. perfect and excellent, are very minimal. We believe that distinguishing between non-relevant and relevant passages would be even easier and ROUGE measures would perform better in a general evaluation scenario.
When we compare the content of a retrieved passage to a set of ideal answers, the quality of ideal answers is very important factor. In the previous experiment, we sampled the ideal answers only from the "perfect"-labeled answers. In the next experiments, we study the sensitivity of ROUGE measures to noisy judgments where we have also non-perfect answers, e.g. excellent answers, as part of the ideal set.
To this end, we randomly select a subset of the excellent answers with probability 0.5 and add them to the ideal set. Again we evaluate the rest of the passages by comparing them to the ideal set using ROUGE measures. Table 3 shows the correlations values when we have noisy judgments. We can see that all the correlation values are decreased, which shows the ROUGE measures are in fact sensitive to the quality of the ideal answers. However, the correlations are reasonably high and all of them are at statistical significant levels with p-values less than 0.05.

965

0.40

0.35

0.30

Table 3: Correlation between ROUGE measures and

human judgment with noisy judgments

Measure

Precision Recall F1

ROUGE-1

0.30

0.41 0.35

ROUGE-2

0.35

0.40 0.37

ROUGE-S4

0.34

0.40 0.36

ROUGE-SU4 0.34

0.40 0.36

0.25

0.20

Table 4: Correlation between ROUGE measure and

manual evaluation on CQA data

Measure

Precision Recall F1

ROUGE-1 0.41

0.30 0.28

ROUGE-2 0.56

0.32 0.31

ROUGE-SU4 0.51

0.32 0.30

6. CROSS-COLLECTION EVALUATION
As opposed to existing character-based measures, ROUGE measures do not compare exact positions in the documents for evaluation. This property enable us to use annotation from one document collection to evaluate passages that are retrieved from another collection.
In order to explore the feasibility of this option, we study the question answering problem. In a Community Question Answering (CQA) environment, questions and answers are provided by the users. Some users ask a question and other users answer the question or vote to the already provided answers. One of the interesting tasks in such environment is to automatically answer new questions. Evaluating the output of such system is a challenging task that might need a lot of annotations. In this section, we study if ROUGE measures can eliminate the need for annotation by directly comparing retrieved passages with the best human answers.
We use the Yahoo CQA data and manually select a set of fifty questions whose best answer is a coherent piece of text and has a chance to be found in our web collection. We generate a query based on each question by stemming and removing stop words. We use the resulted queries to retrieve passages from the Clueweb-B collection using the built-in passage retrieval functionality in Indri. We retrieve fixed size passages with length 50 terms and overlap 25 as shown to be effective choice [2].
We then use the best human-provided answer for each question as our ideal answer and evaluate the retrieved passages using ROUGE measures. Further we selected 200 top retrieved passages and manually assigned a relevance score to them between 0 and 4, where 0 is a non-relevant answer and 4 means a perfect answer. We then calculate the correlation between the manual assessments and the ROUGE values. The results are shown in table 4. In all the cases the correlation is at a statistically significant level (p-value less than 0.05). Again we can see that ROUGE measures can reasonably capture the quality of answer passages. As a more detailed inspection, Figure 1 shows the distribution of ROUGE-SU4 precision values for different levels of human judgments. We can clearly see those passages with high value of human assessment (level 3 or 4) have higher values for the ROUGE measures as well.

0

1

2

3

4

Figure 1: Distribution of REUGE-SU4 precision values for different levels of manual assessment.

7. CONCLUSION AND FUTURE WORK
In this paper, we investigated the evaluation of the answer passage retrieval task where the goal is to retrieve small passages, as opposed to full documents, in response to a user query. We employed text summarization evaluation metrics and showed that they are reasonably correlated with existing measures and human judgments. This suggests that ROUGE measures are reasonable measures to use, in addition to the existing measures, for evaluating passages.
Based on this new evaluation framework, our future work will be more focused on the passage-specific retrieval models. Due to the short length of passages, incorporating NLP features and translation models in the retrieval system seems to be a promising direction.
8. ACKNOWLEDGMENT
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-1160894. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
9. REFERENCES
[1] J. Allan. Hard track overview in trec 2004 - high accuracy retrieval from documents. In proceedings of TREC, 2004.
[2] M. Bendersky and O. Kurland. Utilizing passage-based language models for document retrieval. In proceedings of ECIR, pages 162­174, 2008.
[3] J. Callan. Passage-level evidence in document retrieval. In proceedings of SIGIR, pages 302­310, 1994.
[4] W. R. Hersh, A. M. Cohen, P. M. Roberts, and H. K. Rekapalli. Trec 2006 genomics track overview. In Proceedings of TREC'06, 2006.
[5] J. Kamps, J. Pehcevski, G. Kazai, M. Lalmas, and S. Robertson. Inex 2007 evaluation measures. In proceedings of INEX workshop, pages 24­33, 2007.
[6] M. P. Kato, T. Sakai, T. Yamamoto, and M. Iwata. Report from the ntcir-10 1click-2 japanese subtask: baselines, upperbounds and evaluation robustness. In proceedings of SIGIR, pages 753­756, 2013.
[7] C.-Y. Lin and F. J. Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In proceedings of ACL, pages 605­612, 2004.
[8] X. Liu and W. Croft. Passage retrieval based on language models. In proceedings of CIKM, pages 375­382, 2002.
[9] T. Sakai, D. Ishikawa, N. Kando, Y. Seki, K. Kuriyama, and C.-Y. Lin. Using graded-relevance metrics for evaluating community qa answer selection. In proceedings of WSDM, pages 187­196, 2011.

966

Understanding Negation and Family History to Improve Clinical Information Retrieval

Bevan Koopman1,2, Guido Zuccon2
1Australian e-Health Research Centre, CSIRO, Brisbane, Australia 2Queensland University of Technology, Brisbane, Australia
bevan.koopman@csiro.au, g.zuccon@qut.edu.au

ABSTRACT
We present a study to understand the effect that negated terms (e.g., "no fever") and family history (e.g., "family history of diabetes") have on searching clinical records. Our analysis is aimed at devising the most effective means of handling negation and family history. In doing so, we explicitly represent a clinical record according to its different content types: negated, family history and normal content; the retrieval model weights each of these separately. Empirical evaluation shows that overall the presence of negation harms retrieval effectiveness while family history has little effect. We show negation is best handled by weighting negated content (rather than the common practise of removing or replacing it). However, we also show that many queries benefit from the inclusion of negated content and that negation is optimally handled on a per-query basis. Additional evaluation shows that adaptive handing of negated and family history content can have significant benefits.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval] General Terms: Measurement, Experimentation.
1. INTRODUCTION
Negation and reference to family history are two unique characteristics of clinical records that affect natural language processing of clinical text [3]. Commonly mentioned conditions in a patient record (e.g., "fever" or "fracture") often appear in negated form (e.g., "denies fever" or "no fracture") [3]. Previous research has largely focused on identifying the negated portions of text, or reference to family history content (e.g.,"family history of heart disease") [3, 1]. From an information retrieval (IR) perspective, previous studies have considered how negation may adversely affect retrieval [5, 6]. For example, when searching patient records using the query "patients with heart murmour", the retrieval system might return a large number of irrelevant documents containing "no heart murmur". Traditional keyword-matching IR systems denote the presence of the query terms in a document
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00 http://dx.doi.org/10.1145/2600428.2609487.

as an indicator of relevance. Empirical analysis of the effect of negation on IR system effectiveness shows mixed results: Koopman et al. [5] found IR term-weighting methods naturally accounted for negation, while Limsopatham et al. [6] developed a technique that showed handling negation improved effectiveness.
In this paper, we provide a specific analysis of how and why negation affects retrieval effectiveness. In doing so, we uncover why previous studies of negation in IR produced differing results. In addition to negation, we also consider how the reference to family history also influences IR effectiveness. We explicitly represent, within the language modeling framework, a clinical record according to its different content: negated, family history and normal (i.e., all other normal content); the importance of each of these content types is then weighted separately. Relevance of a particular document to a query is estimated based on the mix of the three content types within the document.
An evaluation using the TREC Medical Records Track shows that handling negation does improve retrieval performance. However, further analysis revealed that many queries benefit from the inclusion of negated content. An outcome of this finding is that the common approach [8] of removing or replacing negated content from the document representation is sub-optimal; rather, negated content should be weighted separately, ideally on a per-query basis. The significant potential benefits of adaptive per-query handling of negated, family history and normal content are presented in further retrieval experiments.
2. RELATED WORK
Accounting for negated terms in clinical text has been an important topic in health informatics, with much of the focus being within the computational linguistics and natural language processing (NLP) fields. The main focus of these efforts is on negation detection and negated scope detection. Chapman et al.[2] developed NegEx, an algorithm which is effective in determining negated findings or diseases from clinical text. NegEx has become a common tool for identifying negated content; the tool was extended as the ConText algorithm, which in addition to negation also identifies hypothetical, or historical references in clinical text [4]. ConText was also extended to identify references to family history. Previous studies reported the effectiveness of NegEx to be at least 90% F-measure [2, 4].
Less research has been performed on the effect of negation on searching clinical text. Previous studies in this area have mainly considered how the negated content of a document can be removed or separated prior to indexing the docu-

971

ments; the assumption being that the presence of negated content always harms retrieval effectiveness [1, 6]. This assumption was pervasive amongst teams participating in the TREC Medical Record Track: many participants dealt with negation by pre-processing clinical records with the NegEx algorithm to remove negated content [8].
In this paper, we firstly empirically investigate the assumption that negation always harms retrieval performance. This is important to understand as previous studies differ in their findings on the effect of negation: Koopman et al. [5] found IR term-weighting methods naturally accounted for negation, while Limsopatham et al.[6] developed a technique that showed penalising negation improved effectiveness. In addition to the effect of negation, we also consider the less studied effect of family history references on clinical IR.
The previous work described here, and the focus of this study, is on explicitly negated terms found in documents, which differs from other work concerned with negation in queries (e.g., the Boolean query "hypertension NOT obesity"). Dealing with negation in queries presents its own set of challenges but is out of the scope of this study.

3. RETRIEVAL MODEL

We model retrieval as a language modeling process, where the standard document representation is enhanced to handle three different types of content within the document: negated, family history and normal. To this aim, we separate these different contents such that a document D can be represented by D^ = Dnor  Dneg  Dfh.
Separating the different contents allows weighting each content type differently. For example, instead of removing negated content from the document, a negative weight can be assigned to the negated content when estimating the probability of a document being relevant to a query Q:

P (Q|D^ )  P (Q|Dnor) - P (Q|Dneg)

(1)

This approach of subtracting the score contribution of negated content is similar to Limsopatham et al. [6], who reported improvement in retrieval effectiveness using this approach. However, we further enhance this by mixing the estimates of negated, family history and normal content:

P (Q|D^ )  norP (Q|Dnor) + negP (Q|Dneg) + fhP (Q|Dfh) (2)
where nor,fh,neg are the mixing parameters that control the weights for normal, negated and family history content respectively. Weights of normal and family history are bounded by 0  nor, fh  1, however negation weights are instead bounded by -1  neg  1; this is done to handle negative weighting for negation (making it equivalent to Eq. 1). Eq. 2 explicitly scores a clinical record according to its different types of content; the effect of negation and family history can then be investigated by varying .
The Indri toolkit1 was used to implement Eq. 2. For negation and family history detection we used the standard ConText algorithm [4]. Spans of text in a document are annotated with XML elements <negated> or <fhistory>, all other content is annotated <normal> (spans may overlap). Documents are then indexed using Indri's XML indexer, which stores the normal, negated and family history content in separate fields, thus providing the three representations of a document, Dnor, Dneg and Dfh. For re-
1Lemur Project, http://www.lemurproject.org

trieval with mixed weights we used the Indri Query Language #wsum method to assign weights to specific fields, e.g., if a query text is Dementia, and the mixing parameters are nor = 1.0, neg = -1.0 and fh = 0.5, we generate the query #wsum(1.0 dementia.normal -1.0 dementia.negated 0.5 dementia.fhistory). This query can be interpreted as requesting medical records than mention dementia but not in a negated form; while, the mention of dementia in the family history is weighted half that of an affirmative mention in the normal content.
4. EMPIRICAL EVALUATION
Our experimental evaluation is conducted to answer the following research questions:
RQ1: What effect does negation have on overall retrieval effectiveness?
RQ2: What effect does family history have on overall retrieval effectiveness?
RQ3: Does negation always harm effectiveness (and therefore should we always exclude or negatively weight negated content)?
RQ4: Can retrieval effectiveness be significantly improved by finding an optimal mix of normal, negated and family history content?
The test collection used in our experiments was the TREC 2011 & 2012 Medical Records Track (MedTrack) [8]. The unit of retrieval was a patient record rather than an individual report; thus, reports belonging to a single patient's record were concatenated into a single document called a patient visit document.2 The resulting corpus contained 17,198 patient visit documents.
The evaluation measures used in MedTrack 2011 were bpref and precision @ 10 (P@10). However, in MedTrack 2012 inferred measures and P@10 were used. Inferred measures required specific relevance assessments (prels) not available for 2011, but bpref and P@10 could be used for 2012 as qrels were available. While it is possible to separate the evaluation into two parts (34 queries for 2011 and 47 for 2012), it is more desirable to have a single, larger query set for more powerful statistical analysis. Therefore, we combine the query sets and use bpref and P@10.
To evaluate RQ1 (the overall effect of negation), we adjust the weight of negated content while fixing the weights for normal and family history. This is done by varying neg in Eq. 2 from -1 to 1 in 0.1 increments. Negation removal equates to neg = 0: this is the approach most systems at TREC Medtrack subscribe to, including the system that achieved the highest results (Udel) [8]. Note that in our experiments we do not explicitly compare with TREC systems because they mix negation handling (if any) with other techniques and engineering solutions (such as discriminating between type of reports). Instead, we consider two baselines to handle negation: that with neg = 0 (negation removal), which is used by most TREC system, and that with neg = -1 (negation penalisation), which resembles the strategy by Limsopatham et al. [6].
To evaluate RQ2 (the overall effect of family history) we adjust the weight of family history content while fixing the weights for normal and negated by varying fh in Eq. 2 from 0 to 1 in 0.1 increments.
To evaluate RQ3 (does negation always harm performance) we have a two-fold approach. Firstly, we investigate in-
2This is a common practise among MedTrack participants [8].

972

 P@10 -0.2 0.0 0.2 0.4 0.6

Method Baseline (include all) Negation removal
Family history removal
Negation & family history removal

nor neg f h 111 101
110
100

bpref
0.3644 0.3811 (+5%) 0.3652 (+0.2%) 0.3813 (+5%)

P@10
0.4469 0.4901 (+10%) 0.4519 (+1.1%) 0.4914 (+10%)

Table 1: TREC MedTrack retrieval results for nega-

tion removal, family history removal and both nega-

tion and family history removal.  = statistical sig-

nificance (paired t-test, p < 0.01) over the baseline.

Negated

Family History

0.50

0.50

bpref P@10

bpref P@10

0.45

0.45

0.40

0.40

0.35

0.35

-1.0 -0.5 0.0

0.5

1.0

0.0 0.2 0.4 0.6 0.8 1.0

neg

fh

Figure 1: The effect on retrieval effectiveness (y-

axis) with different weightings (x-axis) of negation

(left) and family history (right).

dividual query performance for the best settings of neg. Secondly, we perform an exploration of the full parameter space of nor, neg, fh to uncover how the weighting mix differs between queries and whether negation always harms retrieval. The sweep of the parameter space also informs RQ4 (the possible improvements from selecting an optimal mix of content).

4.1 Results & Analysis
RQ1 & RQ2: What effect does negation and family history have on overall retrieval effectiveness?
Table 1 presents the retrieval results for negation removal, family history removal and both negation and family history removal. Removing negated content does indeed improve overall retrieval performance, especially in P@10. The greater improvement in P@10 shows that negation can more adversely affect the top-ranked results. The results confirm that negation has an adverse effect on retrieval effectiveness and that removal of negated content (neg = 0) can improve the overall effectiveness. RQ2 considers the effect of family history content on retrieval effectiveness. In this regard, removing family history content did not affect effectiveness in a statistically significant way: Table 1 shows only minor changes in P@10 and bpref for family history removal.
The results in Table 1 report the effect of simply removing negation and family history; in our method, this corresponded to assigning a weight of 0 to neg and fh. However, Eq. 2 also allows assigning different weights to the different types of content. The effect on retrieval for different weighting values is illustrated in Figure 1. The x-axis refers to different values of neg from -1.0 to 1.0 and fh from 0.0 to 1.0. In both cases the weight for normal content is fixed, nor = 1.0. The y-axis shows the retrieval effectiveness (bpref and P@10).
For negation weighting, Figure 1 shows that negative weights

Query
Figure 2: Per-query effectiveness change in P@10 after applying negation weighting (neg = -0.7).
of neg are more effective (for both bpref and P@10) than negation removal (neg = 0.0). This finding is consistent with Limsopatham et al. [6] and supports their negative weighting method. It also highlights that common approaches of simply removing negated content [8] are sub-optimal; a more appropriate method is to negatively weight such content. The optimal effectiveness differs depending on the evaluation metric -- bpref is maximised with neg = -1.0, while P@10 is maximised with neg = -0.7. This difference again highlights that negation affects the top-ranked documents (measured by P@10) in a different way to the longer result set (measured by bpref).
In contrast to negation, weighting the contribution of family history content has little effect on effectiveness. However, it is important to note that removing the document portions that dealt with family history (i.e., fh = 0) did not actually degrade performance. Analysis on individual queries showed that family history did indeed have little effect on every query -- it was not the case that family history harmed and improved an equal number of queries, leading to an overall performance comparable to the baseline.
In summary, these results show that negation has overall a detrimental effect on retrieval effectiveness; while, family history does not affect retrieval in a significant way.
RQ3: Does negation always harm (should we always exclude or negatively weight negated content)?
The overall retrieval results motivate always negatively weighting negated content; the best P@10 settings being neg = -0.7. To verify whether this holds for each of the 81 queries, we analyse the effect of negation weighting on individual queries. Figure 2 shows the change in P@10 for each query when using the best setting of neg (-0.7) compared to the baseline. In the figure, positive values indicate queries that benefit from negation handling, while negative values indicate queries harmed by negation handling. The results confirm that overall negation handling has positive effects; however, there were a number of queries that were harmed or not affected (P @10 = 0). This finding, i.e., the difference between overall effectiveness and individual query effectiveness, explains the mixed results of previous studies. Studies that considered overall results concluded that negation harms effectiveness [6], while other studies that used a different set of individual queries found that negation had less of an adverse effect [5].
The analysis on individual queries suggests that a single parameter setting across all queries is sub-optimal. We thus perform a full exploration of the parameter space to determine the optimal parameter value on a per-query basis. Figure 3 presents the optimal  value for each of query.
For normal content we observe that most queries are optimised with nor = 1.0, although there are still a number

973

nor
0.2 0.4 0.6 0.8 1.0
neg
-1.0 -0.5 0.0 0.5 1.0
fh
0.0 0.2 0.4 0.6 0.8 1.0

Normal

Negated

Family History

100 120 140 160 180

100 120 140 160 180

100 120 140 160 180

QueryId

QueryId

QueryId

Figure 3: Optimal  (based on P@10) for each query. Optimal weighting of negated content is polarised (between -1.0 and 1.0), showing that negated content can sometimes aid effectiveness.

Method

bpref

P@10

Best fixed overall weighting Optimal per-query weighting

0.4017
0.4526 (+13%)

0.4975
0.6271 (+26%)

Table 2: Potential improvements for optimal perquery weighting of different content, compared to the best fixed overall weighting baseline (nor = 1.0, neg = -1.0, fh = 0.8).  = paired t-test, p < 0.01.

of queries for which the best effectiveness is achieved by reducing the importance of normal content.
For negated content, results are mixed: a number of queries are most effective with negative weighting, while only a few queries benefit from negation removal (neg = 0.0), and a large number of queries ( 30%) are most effective with positive negation handling (neg = 1.0). The overall results presented in the previous sections highlighted the preference for negative weighting over negation removal. However, the exploration of the optimal parameters illustrated here shows that there is a large number of queries that benefit from including negation via positive weighting. These might be documents that contained both affirmed and negated references to a query term where the overall status was affirmed. For example, a patient who tested positive to a condition at the beginning of their hospital admission, were treated, and a further test showed the condition was no longer present. Additionally, these could be examples where the NegEx algorithm incorrectly annotated a portion of text as negated when it should have been marked as normal.
For family history, the optimal parameter settings show that generally this can be treated like normal content.
The results show that the optimal parameter settings vary significantly between queries. Negation does not always harm performance: in many queries a positive weight should be assigned to negated content. The optimal weights also vary for both normal and family history content.
RQ4: Can retrieval effectiveness be significantly improved by finding an optimal mix of normal, negated and family history content?
Given the variability in optimal parameter values, a perquery approach may be beneficial. To quantify the possible advantage of an adaptive strategy that optimally mixes content types, we adjust weighting parameters on a per-query basis using the best settings determined in the previous section. Table 2 shows the retrieval results using this optimal per-query weighting as compared to the best fixed overall weighting baseline (nor = 1.0, neg = -1.0, fh = 0.8).
Optimal per-query weighting can significantly improve retrieval effectiveness, more so in the top-ranked results (P@10).

This again highlights how negated content does not always harm effectiveness and may be important to include for certain queries. It also motivates further research into an adaptive per-query estimation of the weighing parameters. Methods for estimating per-query parameters are often investigated in IR [7]. A supervised machine learning method may be employed, with the optimal parameter settings identified here supplied as training data. In addition, a set of features must be selected which might indicate which content type is most important given the query; some useful features to choose in this regard might include statistics related to frequencies of occurrences of terms both in normal and negated content, and collection-level statistics, for example, how rare is a term and how often does it occur negated.
5. CONCLUSIONS
This study provides an understanding of how and why negation affects clinical IR: overall, negation harms retrieval, family history has little effect. We show that assigning a negative weight to negated content is more effective than the common practise [8] of removing or ignoring this content. However, on an individual query level, negated content can be beneficial and therefore negated content within a document should not be ignored. The difference between overall retrieval effectiveness and individual query effectiveness explains the mixed results of previous studies in this area [5, 6]. Considering negated, family history and normal content separately is flexible and effective for handling these different content types. This approach can easily be applied to other content types beyond negation and family history. An analysis of the optimal weighting showed that significant improvements are possible if the right content mix is chosen. The analysis also reveled possible limitations of our study in that some errors could come from the NegEx and ConText algorithms (while NegEx does have F-measure 90% [2, 4], ConText has not been robustly evaluated). This may explain why no significant improvements were found when tuning the family history weights, although this may also indicate that family history is less important that negation. Future work will investigate an adaptive per-query method to automatically derive the importance of different content type in a clinical document to improve retrieval.
References
[1] M. Averbuch, T. H. Karson, B. Ben-Ami, O. Maimond, and L. Rokachd. Context-sensitive Medical Information Retrieval. In Proc. of MEDINFO, pages 282­285, 2004.
[2] W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper, and B. G. Buchanan. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics, 34(5):301­310, 2001.
[3] W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper, and B. G. Buchanan. Evaluation of Negation Phrases in Narrative Clinical Reports. In Proc. of AMIA, page 105, 2001.
[4] H. Harkema, J. N. Dowling, T. Thornblade, and W. W. Chapman. ConText: An Algorithm for Determining Negation, Experiencer, and Temporal Status from Clinical Reports. Journal of Biomedical Informatics, 42(5):839­851, 2009.
[5] B. Koopman, P. Bruza, L. Sitbon, and M. Lawley. Analysis of the Effect of Negation on Information Retrieval of Medical Data. In Proc. of ADCS, pages 89­92, 2010.
[6] N. Limsopatham, C. Macdonald, R. McCreadie, and I. Ounis. Exploiting Term Dependence while Handling Negation in Medical Search. In Proc. of SIGIR, pages 1065­1066, 2012.
[7] D. Metzler. Estimation, Sensitivity, and Generalization in Parameterized Retrieval Models. In CIKM, pages 812­813, 2006.
[8] E. M. Voorhees and W. Hersh. Overview of the TREC 2012 Medical Records Track. In Proc. of TREC, 2012.

974


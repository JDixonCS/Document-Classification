Sub-document Timestamping of Web Documents
Yue Zhao and Claudia Hauff
Web Information Systems, Delft University of Technology, The Netherlands
{y.zhao-1,c.hauff@tudelft.nl}

ABSTRACT
Knowledge about a (Web) document's creation time has been shown to be an important factor in various temporal information retrieval settings. Commonly, it is assumed that such documents were created at a single point in time. While this assumption may hold for news articles and similar document types, it is a clear oversimplification for general Web documents. In this paper, we investigate to what extent (i) this simplifying assumption is violated for a corpus of Web documents, and, (ii) it is possible to accurately estimate the creation time of individual Web documents' components (socalled sub-documents).
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval Keywords: timestamping; sub-documents; Web archiving
1. INTRODUCTION
Accurately estimating at what point in time a (Web) document has originally been created is of importance for a number of applications, including the tracking of ideas over time, the detection of copied content, and temporal information retrieval (IR) -- for some topics users might prefer to be served older Web documents, while for others users may prefer more recently created content. Current research in Web-document based temporal IR usually considers either the documents' creation timestamp (i.e. when the document first appeared on the Web) or the extracted content timestamps (i.e. which time periods the document contains information about) as a raw signal to be included in retrieval models [2]. In this work we focus on the creation time of Web documents. Previous work, e.g. [9, 5, 8], has made the simplifying assumption that each Web document di has been created at one moment in time ti and ti can either be approximated by the first time the document (its URL) was crawled or by the first/oldest timestamp appearing in the document content. On the Web this is a highly unrealistic assumption -- documents are constantly
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767803.

altered and updated, a classic example being blogs, which contain many different "sub-documents" (blog entries) created at different points in time. While the different subdocuments of a blog page may be easy to timestamp, for many other types of Web documents this is harder. Thus, in this work, we aim to arrive at a first understanding of sub-document timestamping. Specifically, we empirically investigate the following two research themes:
RT1: To what extent do Web documents consist of subdocuments created at different times? What kind of documents contain two or more sub-documents? What is the timespan between the oldest and most recent sub-document of a document?
RT2: To what extent are we able to classify each subdocument as either having been created within the past month (relative to the document crawl time), within the past year or more than m years ago? What document features are most useful in the classification? Which type of sub-documents can we most accurately identify?
We investigate a subset of documents from the ClueWeb12 corpus1 and date each document's paragraphs (a paragraph is a sub-document) individually based on historic Web crawl data collected from the Internet Archive2 (IA).
Having dated all sub-documents, we first analyse this corpus of sub-documents before turning towards estimating the creation time of each sub-document with a standard machine learning pipeline. We find that two thirds of the investigated Web documents (66.5%) do indeed contain sub-documents created at different points in time. More importantly, we also find a large gap between the oldest and most recently created subdocument (1052 days on average), indicating that relying on a single creation timestamp per document provides at best a very distorted picture of the true creation times. Classifying sub-documents according to their creation time using only sub-document internal features is possible with more than 66% of instances correctly classified.
2. RELATED WORK
Document creation timestamps are used in different temporal IR settings, such as timeline construction [11, 5], improving retrieval relevance [10, 8] and the estimation of a document's focus time [7]. A few existing works aim to infer the creation timestamps of document. De Jong et al. [4] built temporal language mod-
1http://www.lemurproject.org/clueweb12.php/ 2https://archive.org/

1023

Figure 1: Overview of our processing pipeline for sub-document timestamping.
els from existing newspaper articles across a range of years and tagged non-timestamped articles based on the likelihood of being generated by a particular model. Kanhabua et al. [9] improve the temporal language model by using word interpolation, temporal entropy and external search statistics. They rely on documents recorded on the IA for their experiments, with the document's creation time being the first recorded crawl within the IA. Chambers et al. [3] use machine learning to infer documents' cration timestamps based on the temporal expressions by leveraging the MaxEnt model and additional time constraints. Ge et al. [6] propose an event-based time label propagation model by using the relationship between events and documents (exploiting the fact that news articles are often about events). All these works infer a single creation time per document. In fact, most works [4, 9, 3, 6] rely on news corpora, which by design are rarely (or never) updated and usually contain an easily accessible creation timestamp. For general Web documents, there is little research work on inferring the subdocument creation timestamps. We attempt to fill this gap with our work.
3. APPROACH
To investigate our research questions we require a set of Web documents for which to determine the sub-document creation times. Instead of randomly sampling Web documents, we rely on the 11, 075 relevant documents Drel available for the ClueWeb12 corpus (topics topics 201-300), which consists of more than 700 million English Web documents and was crawled between 02/2012 and 05/2012. We thus investigate documents that are at least relevant to some information needs based on their textual content, avoiding Web spam documents and Web documents that contain very little to no text in the process.
Historical Versions Extraction In Fig. 1 we present an overview of our pipeline. For each document in Drel (identified through its URL), we retrieve all available historic versions from the IA, which began archiving Web documents in 1996. 7118 of the documents in Drel contain at least one historic version. We continue our processing with those documents only (Draerlchived). On average we are able to identify 17 historic versions per document in Draerlchived.
Sub-document Extraction In the second step we identify the different sub-documents of each document di  Draerlchived as well as the sub-documents of di's m historic versions Histi = {dhi 1 , dhi 2 , ..., dhi m } where h1 is the most recent archived version of di (most recent but older than di's

crawl date) and hm is the oldest available version. In order to split a Web document di into k sub-documents di = {s1,i, s2,i, .., sk,i}, we parse di's HTML. A sub-document is then a fraction split by tags <p> or <div>, which contains at least 50 non-markup characters. We empirically found this process to be a simple but robust mechanism to identify subdocuments. The number of sub-documents identified are on average 39 per document (median 21).
Sub-document Timestamping Let Histsiubdocs be the set of all sub-documents created across all historic versions of document di. Then, for each sub-document si,j of di we determine all matching (using approximate string matching) elements in Histsi ubdocs and assign to si,j the creation timestamp of the oldest historic sub-document we found.

F1 F2 F3 F4
F5
F6 F7 F8 F9
F10 F11 F12 F13 F14 F15
F16
F17
F18
F19
F20
F21

Starting position of sk,i within di

Number of terms in sk,i

Relative length of sk,i :

length of sk,i length of di

Character distance between last position of sk-1,i and starting

position of sk,i

Character distance between last position of sk,i and starting posi-

tion of sk+1,i

Number of sentences in sk,i

Number of terms in the longest sentence in sk,i

Number of terms in the shortest sentence in sk,i

Average sentence length in sk,i

Number of temporal expressions in sk,i Number of temporal expressions appearing before sk,i Number of Dates in sk,i Number of Durations in sk,i Number of Times in sk,i Number of Sets in sk,i

Difference in days between 1/1/1996 and the earliest temporal ex-
pression in sk,i Difference in days between 1/1/1996 and the most recent temporal
expression in sk,i Difference in days between 1/1/1996 and the temporal expression
in sk,i being closest to di 's crawl time Difference in days between the earliest and most recent temporal
expressions in sk,i Average number of characters between the appearance of temporal
expressions in sk,i Longest character distance between the appearance of temporal ex-
pressions in sk,i

Table 1: Features derived for sub-document sk,i  di. All features are based on the non-markup content.

Model Training Having identified for each sub-document its creation time, we now derive a set of 21 features in order to investigate RQ2. We restrict ourselves to documentinternal features only. The features are listed in Tab. 1. All features are based on the non-markup content extracted for a particular subdocument. While features F1 to F9 gather basic paragraph and sentence statistics, features F10 to F21 are based on the temporal expressions (TEs) we extract from a sub-document3. TEs can be classified into four different categories, depending on the specificity of the information: [F12] Date (e.g. Feb. 18, 2015 ), [F13] Duration (e.g. from 1996 to 2012 ), [F14] Time (e.g. 1pm) and [F15] Set (e.g. every weekend ). Since the focus of our work is an exploratory analysis of sub-document timestamping, we chose an established classifier with fixed parameter settings (Random Forest [1] with 5 features per tree and 100 trees in total) instead of experimenting with different algorithms and configurations. We train & test the classifier on the 277K pairs of (sub-document, sub-document creation timestamp). We distinguish 5 classes and annotate each pair accordingly depending on the difference between a sub-document sk,i's creation time and the
3TEs are extracted with the SUTime tagger: http://nlp. stanford.edu/software/sutime.shtml.

1024

Number of Pages 0 500 1000 1500 2000 2500

1

2

3

4

5

6

7

8

9 10 >10

Number of Timestamps

Figure 2: Overview of the number of documents containing content created at different points in time.

page crawl time of di4. We use the following five intervals: A = [0, 20.5], B = (20.5, 311.5], C = (311.5, 973.5], D = (973.5, 2183.5] and E = (2183.5, ). That is, class A contains those sub-documents created within the first 20 days of the page crawl time, while class E contains those subdocuments created more than 6 years before the page was actually crawled. We chose these interval settings to create a balanced data-set: each class has 55K instances. In a second set of experiments we consider a subset of all instances, namely those 120K in which each sub-document contains at least one TE, as we aim to investigate the effect TEs have on the accuracy of the classification. We employed the classifier to predict into which class a particular sub-document falls in a 10-fold cross-validation setup.
4. RESULTS
Sub-document timestamps.
Let us first consider RT1 and the question to what extent sub-document timestamping is actually an issue on the Web. In Fig. 2 we plot the number of documents within Draerlchived and the number of different timestamps we assigned to their respective sub-documents. Overall, 62.5% of documents have between 2 and 8 creation timestamps; very few documents contain content created at eight or more different times (4%). Since not only the number of different creation timestamps a document possesses, but also the time interval between the timestamps is important, in Fig. 3 we present the average difference (in days) between the oldest and most recent creation timestamp of a document, with the document set partitioned according to the total number of creation timestamps found in a document. For documents with two creation timestamps, the median difference is 400 days, i.e. 50% of those documents contain content created more than one year apart. Considering these numbers we next investigate how much content is created at different points in time. For each document di = {s1,i, s2,i, .., sk,i}  Draerlchived with 2, 3 or 4 creation timestamps we determined what fraction of document content was created when. The results are shown in Fig. 4. Here, we consider all sub-documents (i.e. the nonmarkup text) of di as 100% of the content and compute what percentage of text was existing at each creation timestamp.
4We assume that in practice a page's crawl time is usually available (as is the case for the ClueWeb12 corpus)

This is a simplification of how Web documents are maintained (content might also be updated, deleted and added again over time). However, since we use the content of di as our starting point, we are only interested in the time a particular sub-document of di was first created. The graph shows that most content is created initially -- for documents with 2 creation timestamps, on average 78% of the content is available after the first version of the document. For documents with 3 and 4 creation timestamps, 68% and 55% of content are created initially. Interestingly, the amount of content added in subsequent creation timestamps is roughly the same.

5000

3000

Timespan (days)

0 1000

2

3

4

5

6

7

8

9

10 >10

Number of Timestamps

Figure 3: The document set Draerlchived is partitioned according to the number of creation timestamps (documents with a single creation timestamp are ignored). Shown is the difference (in days) between the oldest and most recent creation timestamp.

Ver. 1 Ver. 2 Ver. 3 Ver. 4

Number of Timestamps

0.0

0.2

0.4

0.6

0.8

1.0

Mean Fraction of Page Content

Figure 4: The document set Draerlchived is partitioned according to the number of creation timestamps (documents with a single creation timestamp are ignored). A bar shows the mean fraction of content available at each creation timestamp for documents with 2, 3 and 4 creation timestamps. Ver. 1 indicates the content created at the oldest timestamp, Ver. 2 the content created at the second oldest timestamp and so on.

Finally, we consider whether or not different information needs (topics) attract different kinds of documents, i.e. documents with few or many creation timestamps. Fig. 5 shows the distribution of documents with differing creation times for the 25 ClueWeb12 TREC adhoc topics with the largest number of relevant documents (the median number of relevant documents is 126). The results show that for most topics a relatively large percentage of relevant documents contain two or more creation timestamps. If we were able to predict what type of topics favour what kind of documents (a single creation time vs. several) we could employ these

1025

50 100 150 200 250

4+ timestamps 3 timestamps 2 timestamps 1 timestamp

Number of Pages

0

211 214 216 217 221 223 234 240 251 262 263 266 272 273 279 280 282 284 285 288 289 292 294 296 297
Topic ID
Figure 5: Overview of the relevant documents per TREC topic and the amount of creation timestamps.

Entire Data Set
Data Set with TEs only
Data Set with TEs only Data Set with TEs only

#Instances 277,973 120,620 120,620 120,620

Method RF RF
BL: earliest TE BL: latest TE

Misclassified 33.73% 33.12% 60.90% 63.76%

A 0.68 0.69 0.42 0.33

F-Measure / Class

B

C

D

0.61 0.60 0.64

0.59 0.58 0.63

0.28 0.29

0.21 0.19

0.36 0.36

E 0.76 0.79 0.56 0.50

Table 2: Effectiveness of our sub-document timestamp classification pipeline. RF refers to the Random Forest setup, while BL indicates the baseline, using a single feature only (oldest/most recent temporal expression appearing in the sub-document).

creation time-based signals in a retrieval ranking function (a direction of future work).
Predicting Sub-document Timestamps.
Our vision is to eventually develop techniques that are reliably able to tag any Web page's sub-documents with an accurate estimate of their creation time. To answer the questions raised in RT2, we consider the results of the creation timestamp classification experiments in Tab. 2. The Random Forest (RF) classifier classifies 65% of the instances correctly, independent of the existence of TEs in a subdocument (rows 1 & 2). Instances of class E (i.e. those sub-documents created 6+ years before the page crawl time) can be classified with highest accuracy. We also present the results of two baselines for those instances that contain one or more TEs: using as single feature either the oldest or most recent TE for classification purposes only. About two thirds of the instances are not correctly classified showing that TEs alone are not sufficient in this setup and additional features (which on first sight may not always be pertinent to creation timestamps) are required.
5. CONCLUSIONS
Our work shows that sub-document timestamping is an issue which should be considered when employing document creation timestamps in IR applications. Not only the amount of documents containing content created at several points in time is significant, but also the interval between the changes is considerable. One of the limitations of our work is the fact that we relied on the Internet Archive and its historic versions of a document to determine each sub-document's creation time. While this approach yields very precise results for documents archived often by the Internet Archive, for less well-archived documents the temporal resolution is limited5. For this rea-
5Note though, that this has only a very limited effect on the number of creation timestamps. Correlating the num-

son we resorted to a classification setup with five classes instead of estimating the exact creation time. In future work we will (i) investigate the impact of subdocument timestamps on retrieval applications, and (ii) experiment with document-external features to increase the classification accuracy.
6. REFERENCES
[1] L. Breiman. Random forests. Machine learning, 45(1):5­32, 2001.
[2] R. Campos, G. Dias, A. M. Jorge, and A. Jatowt. Survey of temporal information retrieval and related applications. ACM Computing Surveys (CSUR), 47(2):15, 2014.
[3] N. Chambers. Labeling documents with timestamps: Learning from their time expressions. In ACL '12, pages 98­106, 2012.
[4] F. de Jong, H. Rode, and D. Hiemstra. Temporal language models for the disclosure of historical text. Royal Netherlands Academy of Arts and Sciences, 2005.
[5] L. D¨ohling and U. Leser. Extracting and aggregating temporal events from text. In WWW '14, pages 839­844, 2014.
[6] T. Ge, B. Chang, S. Li, and Z. Sui. Event-based time label propagation for automatic dating of news articles. In EMNLP '13, pages 1­11, 2013.
[7] A. Jatowt, C.-M. Au Yeung, and K. Tanaka. Estimating document focus time. In CIKM '13, pages 2273­2278, 2013.
[8] R. Jones and F. Diaz. Temporal profiles of queries. ACM Transactions on Information Systems, 25(3):14, 2007.
[9] N. Kanhabua and K. Nørv°ag. Using temporal language models for document dating. In Machine Learning and Knowledge Discovery in Databases, pages 738­741. 2009.
[10] X. Li and W. B. Croft. Time-based language models. In CIKM '03, pages 469­475. ACM, 2003.
[11] R. Swan and D. Jensen. Timemines: Constructing timelines with statistical models of word usage. In KDD Workshop on Text Mining, pages 73­80, 2000.
ber of records of documents with the number of creation timestamps found in them, yields r = 0.37.

1026

Splitting Water: Precision and Anti-Precision to Reduce Pool Bias

Aldo Lipani
Inst. of Software Technology & Interactive Systems Vienna University of Technology Vienna, Austria
lipani@ifs.tuwien.ac.at

Mihai Lupu
Inst. of Software Technology & Interactive Systems Vienna University of Technology Vienna, Austria
lupu@ifs.tuwien.ac.at

Allan Hanbury
Inst. of Software Technology & Interactive Systems Vienna University of Technology Vienna, Austria
hanbury@ifs.tuwien.ac.at

ABSTRACT
For many tasks in evaluation campaigns, especially those modeling narrow domain-specific challenges, lack of participation leads to a potential pooling bias due to the scarce number of pooled runs. It is well known that the reliability of a test collection is proportional to the number of topics and relevance assessments provided for each topic, but also to same extent to the diversity in participation in the challenges. Hence, in this paper we present a new perspective in reducing the pool bias by studying the effect of merging an unpooled run with the pooled runs. We also introduce an indicator used by the bias correction method to decide whether the correction needs to be applied or not. This indicator gives strong clues about the potential of a "good" run tested on an "unfriendly" test collection (i.e. a collection where the pool was contributed to by runs very different from the one at hand). We demonstrate the correctness of our method on a set of fifteen test collections from the Text REtrieval Conference (TREC). We observe a reduction in system ranking error and absolute score difference error.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation
General Terms
Experimentation, measurement, performance
Keywords
Evaluation, bias, pool, test collection, TREC
1. INTRODUCTION
A test collection is a valuable resource for Information Retrieval (IR) researchers because it gives the IR community
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767749 .

a common ground to facilitate the development of search models. Numerous test collections have been developed in the field since the first Cranfield experiments in the 1960s. Since the start of TREC in the 1990s, this creation happens at a rate of approximately 25 test collections per year. A test collection is composed of: a set of documents, a set of topics and a set of relevance assessments for each topic, derived from the collection of documents. The number of documents in the collection generally makes the full judgment of the document set for every topic infeasible. Therefore, the relevance assessment process is generally optimized by pooling the top N documents for each run. The pool is constructed from systems taking part in the challenge for which the collection was made, at a specific point in time, after which the collection is generally frozen in terms of relevance judgments. The pooling technique aims to identify an unbiased sample of relevant documents. Nevertheless, pool bias negatively affects the score of unpooled runs--those of systems not present at the time of test collection creation. This is a drawback that ultimately affects the reliability of the test collection. The variables controlling this reliability are [14]: the number of topics and their representativeness of the information needs of the target user, the number of documents assessed per run, and, last but not least, the diversity of the pooled systems (often however only assessed as the cardinality of the set of runs).
In the last decades the IR community has branched out significantly in a variety of domains and applications, with the creation of specific IR test collections focusing on specific problems. At the same time, benchmarking techniques developed in the IR community are being implemented in industry. Information aware companies request measures to quantify the quality of their information access systems in general, and search systems in particular. With a narrower focus however, the effort to successfully solve the challenges facing the creators of test collections takes on new significance. Most notably, it is often difficult to acquire a sufficient number of participants and diverse systems in order to fulfill the required run diversity to guarantee a reliable test collection.
In this paper, we estimate the pool bias by studying the effect of an unpooled run on the set of pooled runs, when a fixed-depth pooling strategy is used. We do this through the estimation of an average unjudged rate, which we then normalize with its potential growth interval, in order to adjust the pool bias. Additionally, we introduce an indicator that

103

provides strong clues about the quality of a new, unpooled run.
We do this based for Precision at cut-off P @n. There are two reasons to consider such a "simple" metric. First, it is a cornerstone for many other metrics developed for the most popular of user models these days: the web user [12]. Second, it is easy to understand by all users. This "understandability" of the IR metrics has drawn moderate attention from our community recently [10]. Our own experience in the industry leads us to believe that, when results are not presented as simply precision and recall, any numbers are just assumed to be precision or recall. Decision makers at lower or higher levels, trying to make sense of MAP, or any other commonly used metric in our community, will most often read 0.12 as 12% and simply assume that either 12% of documents are relevant or 12% of relevant documents have been returned on average. Of course, we do not forget why all the other metrics have been invented to replace, or complement, precision at cut-off: 1) for an ideal run, if the topic has less relevant documents then n, P @n does not reach 1; it is not normalized by the number of relevant documents, therefore it is difficult to average over topics, 2) it partially neglects the position of the documents. Nevertheless, there are many cases where P @n is useful (most often, but not only, for the user modeled as considering blocks of 10 documents at a time on the web). This is also demonstrated by its continued use and reporting throughout a majority of evaluation tracks at TREC, CLEF, or NTCIR.
We propose a new bias correction method and demonstrate its effectiveness through leave-one-out experiments, at different levels and combinations, organizations and systems, all the pooled runs, or only the 75% of the top runs as done in previous papers [22, 19, 2, 21, 20]. We then evaluate the results using the mean absolute error (MAE) and the system rank error (SRE), comparing it against the results obtained with the reduced pool, and with the method of correcting pool bias introduced by Webber and Park [23]. We do this on fifteen test collections from TREC, five of which are domain specific test collections.
In short, the contributions of this study are as follows: 1. a new perspective on P @n, based on the effect of a
new run on the set of runs which contributed to the creation of the pool;
2. an indicator to trigger bias correction only when it is indeed necessary;
3. a bias correction method for P @n, including extensive experimental results to show that it outperforms existing bias correction methods,
The remainder of the paper is structured as follows: in Section 2 we provide a brief summary of the extensive work already done to assess and correct pool bias. Section 3 provides the intuition of our method and introduces the required concepts, followed in Section 4 by the method itself. In Section 5 we present and discuss our experimental results. We conclude in Section 6.
2. RELATED WORK
Work related to pool bias can be grouped in three categories: first, that aiming to fundamentally change the way assessment is done, by, instead of pooling, choosing assessment documents in order to maximize some evaluation goal. For instance, Cormack et al. [11] suggest to boost the proportional of relevant documents, Moffat et al. [15] to fo-

cus on the score accuracy of the best-performing systems, Carterette et al [7, 6] to maximize confidence that one system has or has not a better score then another one, using different models of probability of relevancy. While important, this is not the focus of the current paper, which instead addresses the problem of evaluating against existing test collections built using pooling.
Second, there are those studies aiming to assess the reliability of a test collection. This reliability (or lack thereof) can be often traced back to the pooling procedure. Most recently Urbano et al. [20] proposed an estimation of reliability of a test collection using Generalization Theory. We shall use this in our study to better understand the observations made in our experiments.
Finally, and most related to this study, are those works that address the problem of pool bias directly. Here, three different strategies have been studied: removing the bias from the onset, at test collection creation time; creating new metrics to better handle unjudged documents; or estimating the score error to make an adjustment in the metric.
The first strategy is the most desirable: enforcing a diverse set of runs through the efforts of the test collection creators themselves. Especially early test collections have, for instance, created manual runs to increase the likelihood of relevant documents appearing in the pool. The benefit of such efforts has been demonstrated, among others by Kuriyama et al. [13]. Nevertheless, not all test collections have this advantage, and adding such runs a posteriori, after an initial set of systems have been evaluated, is not done because it breaks the comparability of the runs evaluated across the years.
A lot more can be done and has been done for the second strategy: new metrics. In 2004, Buckley and Voorhees [4] introduced BPref as a metric specifically designed to handle incomplete information, which, as pointed out by Sakai in 2007 [17], is a restricted form of Average Precision (AP) on a so called `condensed list'. These are condensed versions of the runs where unjudged documents are filtered out. Sakai introduces a new metric (the Q-measure) and shows that it is possible to obtain better performance then BPref even applying already well-known metrics to the condensed list. The concept of condensed list, first denoted as such by Sakai, was however already explored in relation to AP with the measure Induced AP, introduced by Yilmaz and Aslam [24] the year before, in 2006. Induced AP is Average Precision calculated on condensed lists. The methods explored by these three contributions do not simulate the effect of shallow pooling or of comparing unpooled runs against pooled ones, because they remove the effect of bias sampling from the query relevance (qrel) set, ending up with an unrealistic use case. This was later addressed by Sakai [18], who demonstrated that the condensed list approach leads in favor of new systems, the effect of these metrics instead creating incomplete relevance information by playing with the depth of the pool. Also in their 2006 report, Yilmaz and Aslam [24] introduce the Inferred AP, a more complex metric which is a closer approximation of AP but requires knowledge about the documents down to depth 100. Inferred AP adjusts the score sampling uniformly from the pooled documents and then estimates the true mean of the sample to adjust AP.
Later, Aslam and colleagues address the issue of the uniform sampling used in the 2006 version of Inferred AP [25]. In this later version they use a stratified sampling scheme.

104

However, their finding that the method is not subject to pooling bias is not confirmed in practice by Carterette et al. in 2008 [8]. As they write, this is possibly because aggregating probability of inclusion across multiple runs by taking the mean of the per run probabilities may not properly account for reinforcement by similar systems.
The problem of incomplete judgments leads to the definition of a completely new metric, defined by Moffat and Zobel [16]--called Rank-Biased Precision--expressed by a single value and a residual. The Residual quantifies the uncertainty introduced by the unjudged documents. Its value is computable thanks to the fact that it is not normalized by the number of relevant documents. This implies that the computation of the metric defines a lower bound for the given run. Moffat and Zobel attempted to make a measure that is naturally convergent, where the contribution of each rank has a fixed weight. This would have both benefits of a normalized metric and those of a metric averageable over topics with different numbers of relevant documents. This attempt was unsuccessful, as pointed out by Sakai [18], who proved this to be inferior with respect to the condensed list.
At this point we have the transition to the third category of approaches to solve the pool bias: metric error estimation and correction.
In their presentation of Rank-Biased Precision (RBP), Moffat and Zobel had already introduced the discussion concerning the fact that the residual can be used to estimate and correct pool bias. Webber and Park [23] continue their work on RBP by adding to the score the average residual calculated against the pool proceeding with a leave-one-runout approach. To estimate it they span two dimensions: the topics and the systems. They used Rank-Biased Precision at ten (RBP@10) and Precision at ten (P@10) although the results for this last metric were not reported in the 2009 paper, the authors only mentioned that they were similar to RBP. In the present study we return to precision at cut-off and look not only at coefficients to correct pool bias, but also at whether there is something to correct in the first place.
3. PRECISION AND ANTI-PRECISION
The intuition at the base of the proposed method is that we can observe how a new, unpooled run impacts the existing, pooled runs. Given such an existing run, we can imagine reranking it based on the ranks of its documents in the unpooled run. A "bad" new run will tend to bring down known relevant documents and push up non-relevant ones. Quantifying these changes we create a measure of the potential quality of the new run.
In the following we describe theoretically the measures later used to reduce the pool bias. In evaluating IR systems, Precision (P ) is one of the two fundamental measures. We recall its definition: given D a set of documents, Dr a subset of D (the documents in a run r), q a topic, and  a function of relevancy returning the level of relevancy of the document d for the topic q, P is defined as:
P = |d  Dr : (d, q) > 0| |Dr |
Precision represents the proportion of relevant and retrieved documents against the retrieved ones. From P we derive the definition of Precision at cut-off n (P @n), used to better handle ranked retrieval systems: given  a function that

returns the rank of a document d in a run r, we have:
P @n = |d  Dr : (d, q) > 0, (d, r)  n| n
The measure takes into account only the relevant documents because it is supposed to be used when there is a complete knowledge of the relevance function over the documents in the run. When we consider the problem of missing relevance assessments this assumption is not true, ending up considering unjudged documents as non-relevant. To overcome this problem and take into account the missing information about the run, we define the complement of Precision, called Anti-Precision (P ). Anti-Precision measures the proportion of non-relevant and retrieved documents against the retrieved documents. In statistics, a similarly defined quantity is referred to as the False Discovery Rate (FDR) [1]. It is used in quantifying the results of multiple hypothesis testing experiments. However, given the very different use of it here, we continue to refer to it as Anti-Precision in this study, and define it as:
P = |d  Dr : (d, q) = 0| |Dr |
As well as for Precision, we define also the cut-off version (P @n):
P @n = |d  Dr : (d, q) = 0, (d, r)  n| n
Indeed, when a run is fully judged the following equation holds:
P +P =1
When it is not, and unjudged documents are present in the run, the sum of P and P is lower than 1, reduced by a quantity that represents the proportion of retrieved and unjudged documents against the retrieved documents. We refer to this as k bar (k¯).
P + P = 1 - ¯k
This quantity represents the uncertainty of the measurement. Just as P and P , ¯k can be also defined at cut-off (¯k@n).
3.1 Analysis of a run shuffle
Before going on to the details of our proposed method, let us perform an imagination exercise in order to better understand the information content of a partially judged run. We want to analyze which kind of information precision and anti-precision expose if a given run r gets shuffled. As in a deck of cards a shuffling changes the order of the documents of a run and produces a new run that we will indicate as r . This run has the same set of documents as before. We want to observe the variation in score the run obtains in the two states, original and shuffled. If we would use P , since there is no information about the position of the documents in the formula, we would measure a change of 0. Therefore, let us observe P @n. Given a run r and its shuffled version r we define:
P @n(r ) = P @n(r ) - P @n(r)
P @n has domain [-1, 1] and is the variation in precision of the run after a shuffle. Its increase in value is the result of the combination of the following two related effects: the shuffle

105

moved up relevant documents, placing them in the top n, or moved down non-relevant or unjudged documents with the consequential moving up of potential relevant documents in the run. It decreases if the opposite happens. We also define P @n as following:
P @n(r ) = P @n(r ) - P @n(r)
P @n has domain [-1, 1] and is the variation in anti-precision of the run after a shuffle. Its increase in value is the result of the combination of the following two related effects: the shuffle moved up non-relevant documents, placing them in the top n, or moved down relevant or unjudged documents with the consequential moving up of potential non-relevant documents in the run. It decreases if the opposite happens.
Finally, ¯k@n that is derived as following:

¯k@n(r ) = ¯k@n(r ) - ¯k@n(r)

= 1 - (P @n(r ) + P @n(r ))

(1)

- [1 - (P @n(r) + P @n(r))]

= -P @n(r ) - P @n(r)

¯k@n has domain [-1, 1] and is the variation of unjudged documents on a given run. Its increase in value is the result of the combination of the following effects: the shuffle moved up unjudged documents or moved down relevant and non-relevant documents with the consequential moving up of potential unjudged documents in the run. An interesting property of this function, which is possible to prove, is that if r has been judged to depth d : d  n, then the domain of the function ¯k@n is [0, 1]. This property always holds for pooled runs because they verify the condition (provided of course that no mistakes occurred in the pooling process).
In summary, when a run changes the order of its documents, P , P , and ¯k are indicators of the direction of the judged relevant, judged non-relevant, and unjudged documents in the list.

3.2 Effect of a run on a pooled run
Now let us make a step further and consider not the relationship between a run and a random shuffle of itself, but between a run and another run. In the particular case where each run ranks completely the entire collection, this is the same as above. In general however, the systems only provide runs down to a certain limit (say 1000). To study this effect, we need to define a merging function between the two runs. The unpooled run will have an effect on the pooled run, measured by the quantities described above.
Such a merging function can simply be based on the rank of the documents in the run. The aim here is not to add or remove documents from a run, so although the word "merging" could imply the transfer of documents between the two runs to make a new one, we must keep in mind that all we need to do here is transfer only the information about the rank of the documents. We do this by linearly combining the ranks if the two runs share the same document.
In the following formula, by ru we denote the new, previously unseen and unpooled run, whose effect on rp an existing run, we want to study. This effect we represent as a new, synthetic run r , which consists exclusively of documents present in rp, potentially re-ordered.

r = rp  ru = {d  rp : (d, r ) = µ(d, rp, ru)} (2)

where

µ(d, rp, ru) =

(d, rp)(1 - ) + (d, ru) if d  ru

(d, rp)

if else

µ is the weighted arithmetic mean between the rank of the document in rp and the rank of the document in ru, with 0    1. When the same rank is assigned by µ to two different documents, which can happen in some cases for a pair of documents of which one is also in ru and the other one is not, the common document is inserted after the rpexclusive document. In other words, the original run rank has priority.
As any functional composition operator, our merging operator  is not commutative and always represents the effect of its right member on its left member.
In this context, P @n and P @n can be used to analyze the quality of an unpooled run against the pooled one. An increase in P @n is the result of two forces, one direct and one indirect: 1) direct, if the relevant documents in the top n of ru are the same documents found at the bottom of rp, they will be pushed up; 2) indirect, if the ru has non-relevant or unjudged documents in the bottom that are in the top n documents of rp, they will be pushed down. The contribution decreases if the contrary happens. As well for P @n the contribution is: 1) direct, if the non-relevant documents in the top n of ru are shared with documents in the bottom of rp; 2) indirect, if the ru has relevant or unjudged documents in the bottom that are in the top n documents of rp. If the run rp would be judged in its totality, these two effects would be perfectly correlated and it would be possible to calculate one just knowing the other from the following equation:

P @n + P @n = 0

However, when rp contains unjudged documents at ranks below n, their sum becomes -¯k@n, as shown in Eq. 1.
As explained above, ¯k@n represents the ratio of unjudged documents brought to the top n of the run rp by the run ru. Moreover, it is possible to prove that P = 0 and P = 0 if and only if one of the following two conditions occurs: 1) the two runs rp and ru do not share any documents with each other in their top n documents, or 2) the two runs are identical in the top n. These are the two cases where our method will not say anything about the new run ru just by using the existing run rp (but we might based on other pooled runs).
Let us now take an example to illustrate how this indicator could be useful to understand the behavior of a run and predict its quality. We use the test collection Robust 2005 and in particular we focus our attention on a special run that presents an unusual effect, the routing run sab05ror1. It has the peculiarity of being strongly discounted when it is not in the pool. Buckley et al. [3] studied it at length, pointing out that the reason for its behavior was related to the size of the test collection. For this run let us calculate P @10 and k¯@10. Let us also consider the average of P @10 and P @10, which we denote as follows:

1

P @10

=

|Rp| 1

P @10(rp
rRp

 ru)

P @10

=

|Rp|

P @10(rp
rRp

 ru)

where Rp is the set of runs used in the creation of the test collection.

106

P @10

-P @10



0.02 0.05

0.08 0.01
0.00
0.04

0.00 -0.05
0.00

-0.01

0.00 0.05 0.10 0.15 0.20

0.00 0.05 0.10 0.15 0.20

0.00 0.05 0.10 0.15 0.20

^

Figure 1: Plot of P @10, P @10 and  against the residual (^) in a leave-one-organization-out experiment,

for the Robust 2005 test collection. The run indicated as is the unusual run sab05ror1.

Table 1: Measures computed for the run sab05ror1 when it is not part of the pool
P @10 k¯@10 P @10 P @10 0.4220 0.444 0.0065 -0.1053

Table 1 shows these values for this particular run. When the run is not part of the pool, P @10 assigns it the 11th position in 18th runs. ¯k@10 says that there are many documents that are unjudged and that therefore there is a high potential to grow. P @10 indicates a low average positive contribution to the pooled runs, and shows that among the relevant documents there is little intersection. P @10 instead is negative which suggests that many non-relevant documents have been ranked lower than before, therefore suggesting a good ability of this special run to discriminate relevant documents from non-relevant ones. In Figure 1 we show the resulted P @10, P @10 against the residual error (^, the difference between the true score and the unpooled score), generated with a leave-one-organization-out approach. Here we can observe that just using P @10 is not enough because it takes into account only one of the two positive contributions of the run, the other one being the reduction in P @10. Let us now return to the general case. When the average negative contribution of the unpooled run to other runs is reduced (i.e. P < 0) and the run has a positive contribution (i.e. P > 0), the run suffers from pool bias and its score should be adjusted. More problematic is the case when P and P have the same sign (i.e. the run has both a negative and a positive contribution, on average). Indeed, if we have P > 0 and P > 0 we would improve the P @n score of the run only if their ratio is greater then the ratio of P to P , because it means that there is a chance to improve the existing score. On the other hand, if we have P < 0 and P < 0 we would improve only if their ratio is lower then the ratio of P to P because it means that the contribution of the run is more able to discriminate the non-relevant documents. From these observations we derive a single value indicator that merges the information of all the indicators defined:

 = k¯@n(P @n · P @n - P @n · P @n)

(3)

For all runs where  > 0 we apply our correction method. The sign of the difference in the brackets is equivalent to
the ratios discussion above. The ¯k factor has no impact on the sign (as k¯  0), but removes those special cases where

Algorithm 1 Adjustment based on pooled runs

ru  unpooled run

Rp  set of pooled runs

T  set of topics

Q  qrels on T derived from Rp

sru  P @n(ru) sru  P @n(ru) ¯kru  1 - (sru + sru ) for all rp  Rp do

rp  rp  ru Prp  (P @n(rp) - P @n(rp))

P rp  (P @n(rp) - P @n(rp))

¯krp  (-P rp - P rp)

end for

Pru



1 |Rp |

rpRp Prp

P ru



1 |Rp |

rpRp P rp

  ¯kru (Pru sru - P ru sru )

if  > 0 then

k¯ru



1 |Rp |

rpRp ¯krp

a  ¯kru max(¯kru , 0)

else

a0

end if

return sru + a

¯k = 0, since in these cases there is no possibility to improve the score of the run (i.e. to get it closer to what we would have obtained if the run had been contributing to the pool).
Returning briefly to the example of the sab05ror1 run, we can now see in Figure 1 that  clearly distinguishes this run from the rest.
4. ADJUSTING SCORE FOR BIAS
Now that we have an understanding of which runs are suffering from pool bias, with respect to precision at cut-off, we proceed by presenting our method to adjust the score (Algorithm 1). As hinted at before, the method is to adjust the pool bias suffered by a system that has not been pooled by measuring the effect of the system on the pooled runs. The only information that is needed is the relevance assessments for each topic and the pooled runs, normally available for most existing test collections. As we presented earlier, P @n as calculated with the incomplete pool is a lower bound for the score of ru. To correct the pool bias we want to add a

107

quantity that stays within its uncertainty limit ¯kru . In other words, our growth potential in terms of P @n is bounded by ¯kru . We are interested in estimating the missing precision of the unjudged documents in the run ru.
The question is then: Where in this interval do we find our correction value? In the absence of any other external information, we will take the average effect of this run ru on the existing runs, in terms of ¯k.
We do this by computing the ¯krp produced by ru on a pooled run rp via the run composition function defined in Eq. 2. This measures the aggregated change in precision and anti-precision, as described by Eq. 1. We do this for each run in the pool, and average these values. This average, denoted k¯ru , when positive, acts as a maximum likelihood estimator for our position in [0,¯kru ]. Therefore, the correction quantity is the product between ¯kru and k¯ru . The following section will therefore add ¯kru max(¯kru , 0) to the P @n for those runs with  > 0, as shown in the last seven lines of Algorithm 1.
5. EXPERIMENTS
To test the pool bias adjustment developed in the previous section we used 15 test collections sampled from TREC: 7 test collections from the Ad Hoc track, 3 from the Web track, and 5 from more domain specific IR tracks: Genomics, Robust, Legal, Medical and Microblog. We tested the algorithm1 through a leave-one-out approach comparing our method with that of Webber and Park [23]. As the baseline we consider the traditional evaluation against the reduced pool. We call this the reduced pool to distinguish it from the ground truth pool--the one also containing documents exclusively contributed by the removed runs or organizations. We performed the leave-one-out at two different levels: 1) leave-one-run-out: as firstly described by Zobel [26], one run at a time is exited from the pool. This is done by removing all the documents uniquely introduced by it from the relevance assessments; 2) leave-one-organization-out: as introduced by Bu¨ttcher [5], it is similar to the leave-one-runout, with the difference that not only is one run removed from the pool, but also all the runs generated by the same organization. This is done by removing all the documents uniquely introduced by the organization's runs from the relevance assessments. This second approach simulates better the testing of a new run, since in most cases it has been observed that the runs produced by the same organization come from the same system, with only some parameter variation. Therefore, they often bring to the pool the same relevant documents. Finally, as in previous studies [2, 19, 20, 21, 22], to avoid buggy implementations of some of the systems that took part in the challenges, we tested again with only the top 75% of runs of each test collection.
The results for the leave-one-run-out, in addition to being less realistic as a model of real life, are also more conservative than those for leave-one-organization-out, such that in the following we shall discuss only the latter.
5.1 Correction results
In these settings, we explored the different value of the weight of the merging function . In which we observe that as expected, as  goes from 1 to 0, the role of the new run on the runs in the pool decreases, to the point where, for
1The software is available on the website of the first author.

 = 0, the method no longer makes any change to the existing runs. This degree of change in  also affects the variability of P and P which decreases as well due to the lower variation between the synthetic and the pooled runs. This effect grows linearly with  and in the following we shall report the maximum effects, obtained for  = 1. Moreover, we tested the algorithm with different bias indicators (i.e. replacing  with P or P , as discussed in Section 3.2), thus testing the presence and absence of information about relevant or non-relevant documents in the relevance assessment as potential flags to trigger bias correction. We consistently observe lower performance compared with .
Figure 3 shows the comparison, per test collection, of the three different approaches in the leave-one-organizationout experiment as a function of the Mean Absolute Error (MAE). As defined before in [23], the Mean Absolute Error is computed as the absolute difference between the scores of two runs, averaged over the set of topics. In addition to observing the error in the scores, it is also of interest to see how many rank reversals occur. The System Rank Error (SRE) is the sum of all the variation on rank of the system given the true rank. Figure 4 shows the SRE. For the experiments with the 75% top runs, actual values are reported in Table 2. In addition to these two measures, in Table 2, we also reported the SRE*, which only counts the variation on system rank when the difference among them is statistically significant in the ground truth (Tukey's test, p < 0.05 [9]).
In the table and plots we observe that our method, in a majority of cases outperforms the reduced pool and the Webber method. The last lines of Table 2 show how often each method outperformed both other methods (ties are not counted). It also shows how often it obtained the absolute worst score. It can be observed that, of the 675 tests summarized in Table 2, the proposed method obtains the worst performer mark exactly three times. On the other hand, the competing method is significantly more aggressive in its bias correction. In the majority of the cases it obtains worse system scores and rankings when compared to the simpler method of not doing anything (i.e. the reduced pool). This happens in particular in Ad Hoc 6, 7, 8 and Robust 2005. The proposed method is shown to be stable. Particularly important, it gets worse scores exactly once on SRE*, the metric measuring reversals among systems identified to be statistically significantly different. And, it happens with Medical 2011 and P @100, increasing the SRE* from 0 (for reduced pool) to 10, which reason should be found in the used shallow pool depth of 10.
5.2 Relation to test collection stability
The results observed in Figures 3 and 4, as well as in Table 2 lead us to question whether or not there is a connection between the effect of our method and the quality of the test collection. We therefore compare the percentage MAE change for each test collection and for each n of P @n, with the two coefficients of stability recently adapted by Urbano et al. [20] from Generalizability Theory: the Generalizability Coefficient (E2) and Dependability (). E2 measures the stability based on system variance and the relative differences between systems;  measures the stability based on system variance and the absolute effectiveness scores. To infer that a test collection is reliable, both measures must tend to 1. Figure 2 shows the relation between these two factors and the change in MAE for P @10 and P @100 for

108

MAE Percentage Change

P @10, E2 -60
-40
-20
0
20 P @100, E2
-60
-40
-20
0
20

P @10,  P @100, 

Ad Hoc 2 Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Genomics 2005 Legal 2006 Medical 2011 Microblog 2011 Robust 2005 Web 2001 Web 2002 Web 9

1.00

0.75

0.50 1.00

0.75

0.50

Figure 2: Plots of the percentage change of Mean Absolute Error for P @10 and P @100 against the coefficients of stability, Generalizability Coefficient (E2) and Dependability (). Spearman's rank correlation for P@10: E2 is 0.48 (p>0.07) and for  is 0.36 (p>0.18). For P@100: E2 is 0.17 (p>0.54) and for  is 0.09 (p>0.74).

the 15 test collections studied here. This change in MAE is calculated between our method and the traditional, reduced pool method. In general, we observe a weak correlation with E2 and  (i.e. less error for more unstable test collections). With P @10 our method has a stronger effect with more unstable test collections. An interesting case happens at P @100, where for some test collection the MAE percentage change is positive, that is resulting in a lack of correlation, with which we get more ambiguous results.
To understand this, it is needed to understand that when the cut-off of P @n is greater then the depth of the pool, we are essentially comparing with an uncertain ground truth, since also the large pool (the one with the runs of the organization we removed for testing) is affected by the presence of unjudged documents. This uncertainty in comparing the result when the depth of the pool is less then the considered P @n needs to be considered when looking at these results, as well as those of all other proposed methods. The depth of each test collection is available for reference in Table 2.
6. CONCLUSION
The primary focus of this paper is an insight that information about the quality of an unpooled run can be obtained by observing its effect on existing, pooled runs. Such an effect is modeled by the creation of a synthetic run, obtained by merging the two runs--the pooled and the unpooled--in a very simple way, by linearly combining the ranks of each document in the old run (i.e. we do not want to add new documents to the old run, just observed how its own documents shift as a function of the information provided by the new run). The effect is measured with essentially three quantities: the change in the position of the judged relevant documents (measured via precision), the change in the position of the judged non-relevant documents (measured via anti-precision), and the change in the position of the unjudged documents (measured via a measure ¯k we define). Observing these changes across the set of pooled runs--the effect of the new run on the existing runs--we identify a

coefficient  whose sign allows us to decide whether a bias correction should be made or not. We then proceed with the provision of a bias correction procedure based on the above three quantities, which we show to be conservative in the sense that it never damages significant rank orders, and only very rarely affects changes in system rankings. This is opposed to previous methods which are too aggressive in the bias correction, and in so being, add another level of uncertainty to the system rankings.
The proposed method addresses a significant concern coming from research but also from practice: the necessity to have a reliable, yet understandable metric, which we can communicate to partners outside of our community. This last condition significantly restricts our possible choices. Precision at cut-off is by far the most easily understood quantity to communicate and with this study we have shown that we can correct pool bias when considering a run that has not participated in the creation of the pool.
Acknowledgements
This research was supported by the Austrian Science Fund (FWF) project number P25905-N23 (ADmIRE).
7. REFERENCES
[1] Y. Benjamini and Y. Hochberg. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society, B57(1), 1995.
[2] D. Bodoff and P. Li. Test theory for assessing ir test collections. In Proc. of SIGIR, 2007.
[3] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Inf. Ret., 10(6), 2007.
[4] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In Proc. of SIGIR, 2004.
[5] S. Bu¨ttcher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboroff. Reliable information retrieval evaluation

109

Table 2: Summary of the results per test collection generate trough a leave-one-organization-out using the top 75% of the pooled runs. With: |R| number of runs submitted, |O| number of organizations involved, |Rp| number of pooled runs, d depth of the pool and |T | number of topics.

Track

P@n

MAE

Ours SRE

SRE*

Webber MAE SRE SRE*

Reduced MAE SRE

Pool SRE*

|R|: 38

Ad

Hoc

2

|O|: 22 |Rp|: 36 d : 100

|T |: 50

5 0.0063 19 10 0.0082 32 20 0.0132 50 30 0.0149 51 100 0.0216 88

0 0.0069 18 0 0.0084 27 0 0.0119 48 0 0.0129 42 2 0.0169 61

0 0.0065 19 0 0 0.0085 32 0 0 0.0138 52 0 0 0.0164 56 0 2 0.0293 122 4

|R|: 40 5 0.0025 3 0 0.0038 3 0 0.0025 3 0

Ad

Hoc

3

|O|: 22 |Rp|: 26 d : 200

10 0.0024 5 20 0.0042 5 30 0.0049 13

0 0.0029 0 0 0.0041 6 0 0.0051 12

0 0.0025 5 0 0.0042 5 0 0.0051 13

0 0 0

|T |: 50 100 0.0067 21 0 0.0071 28 0 0.0085 25 0

|R|: 33 5 0.0065 17 0 0.0069 20 0 0.0073 18 0

Ad

Hoc

4

|O|: 19 |Rp|: 32 d : 100

10 0.0081 23 20 0.0089 29 30 0.0089 27

0 0.0081 22 0 0.0096 33 0 0.0098 31

0 0.0093 25 0 0.0107 33 0 0.0117 32

0 0 0

|T |: 50 100 0.0084 26 0 0.0115 35 0 0.0161 52 0

|R|: 61

Ad

Hoc

5

|O|: 21 |Rp|: 61 d : 100

|T |: 50

5 0.0067 39 10 0.0070 50 20 0.0071 59 30 0.0068 61 100 0.0048 66

0 0.0073 39 0 0.0069 39 0 0 0.0072 50 0 0.0074 50 0 0 0.0077 61 0 0.0080 66 0 0 0.0080 73 0 0.0085 77 0 0 0.0086 116 0 0.0104 136 0

|R|: 74

Ad

Hoc

6

|O|: 17 |Rp|: 19 d : 55

|T |: 50

5 0.0179 16 10 0.0224 15 20 0.0253 18 30 0.0263 20 100 0.0136 25

3 0.0308 28 5 0.0234 20 5 6 0.0354 29 8 0.0283 22 11 6 0.0355 35 12 0.0336 31 12 6 0.0365 38 11 0.0389 31 11 0 0.0186 34 4 0.0288 41 4

|R|: 103 5 0.0011 4 0 0.0018 4 0 0.0012 4 0

Ad

Hoc

7

|O|: 42 |Rp|: 79 d : 100

10 0.0017 8 20 0.0021 13 30 0.0022 24

0 0.0022 8 0 0.0027 18 0 0.0029 28

0 0.0017 8 0 0.0022 13 0 0.0025 27

0 0 0

|T |: 50 100 0.0025 40 0 0.0038 58 0 0.0038 53 0

|R|: 129 5 0.0036 11 8 0.0042 11 8 0.0038 11 8

Ad

Hoc

8

|O|: 41 |Rp|: 80 d : 100

10 0.0036 7 20 0.0035 6 30 0.0035 7

5 0.0043 9 1 0.0042 14 1 0.0042 10

7 0.0038 9 2 0.0038 7 1 0.0038 8

7 2 2

|T |: 50 100 0.0030 26 6 0.0049 42 8 0.0043 38 7

|R|: 104 5 0.0023 15 0 0.0031 15 0 0.0023 15 0

|O|: 23 10 0.0022 17 0 0.0027 19 0 0.0025 19 0

Web

9

|Rp|: 64 d : 100

20 0.0027 25 30 0.0027 41

0 0.0027 21 0 0.0026 34

0 0.0030 26 0 0.0034 43

0 0

|T |: 50 100 0.0043 105 0 0.0036 109 1 0.0052 147 3

|R|: 97 5 0.0010 5 0 0.0015 5 0 0.0010 5 0

Web

2001

|O|: 29 |Rp|: 61 d : 100

10 0.0016 9 20 0.0017 14 30 0.0018 26

0 0.0021 9 0 0.0021 17 0 0.0021 15

0 0.0016 9 0 0.0019 14 0 0.0022 27

0 0 0

|T |: 50 100 0.0040 96 0 0.0026 63 0 0.0041 87 0

|R|: 69

Web

2002

|O|: 16 |Rp|: 69 d : 50

|T |: 50

5 0.0038 54 10 0.0045 76 20 0.0041 78 30 0.0044 106 100 0.0039 138

0 0.0038 54 0 0.0040 80 0 0.0039 78 0 0.0036 87 0 0.0024 92

0 0.0038 54 0 0 0.0049 80 0 0 0.0051 95 0 0 0.0052 120 0 0 0.0038 136 0

|R|: 62 5 0.0052 64 0 0.0054 69 0 0.0057 69 0

Genomics

2005

|O|: 32 |Rp|: 58 d : 60 |T |: 49

10 0.0063 111 20 0.0077 89 30 0.0084 106 100 0.0055 93

0 0.0061 110 0 0.0066 80 0 0.0069 81 0 0.0049 93

0 0.0073 117 0 0.0088 106 0 0.0100 139 0 0.0091 197

0 0 0 0

|R|: 74

Robust

2005

|O|: 17 |Rp|: 19 d : 55

|T |: 50

5 0.0179 16 10 0.0224 15 20 0.0253 18 30 0.0263 20 100 0.0136 25

3 0.0308 28 5 0.0234 20 5 6 0.0354 29 8 0.0283 22 11 6 0.0355 35 12 0.0336 31 12 6 0.0365 38 11 0.0389 31 11 0 0.0186 34 4 0.0288 41 4

|R|: 34

Legal

2006

|O|: 8 |Rp|: 23 d : 10

|T |: 39

5 0.0593 80 10 0.0572 94 20 0.0306 64 30 0.0219 63 100 0.0063 54

0 0.1146 135 11 0.1327 136 11 15 0.1097 138 25 0.1440 139 25 12 0.0813 128 24 0.0984 139 33 1 0.0636 117 17 0.0750 130 30 17 0.0224 101 33 0.0250 107 39

|R|: 127 5 0.0267 142 0 0.0309 159 0 0.0464 261 0

Medical

2011

|O|: 29 |Rp|: 56 d : 10

10 0.0265 157 20 0.0224 152 30 0.0201 153

0 0.0377 219 0 0.0209 142 0 0.0153 121

1 0.0586 336 0 0.0326 206 0 0.0229 174

5 2 0

|T |: 34 100 0.0092 176 8 0.0054 97 0 0.0078 149 0

|R|: 184 5 0.0053 101 14 0.0056 101 14 0.0053 101 14

Microblog

2011

|O|: 58 |Rp|: 119 d : 30

10 20 30

0.0060 183 0.0065 217 0.0069 272

57 0.0063 183 68 0.0065 217 77 0.0070 256

57 0.0062 183 64 0.0067 217 76 0.0076 294

57 68 95

|T |: 49 100 0.0022 156 12 0.0022 143 13 0.0024 159 14

top performer 46 35 20 18 23 2 worst performer 2 3 1 26 18 1

000 44 33 12

with incomplete and biased judgements. In Proc. of SIGIR, 2007. [6] B. Carterette. Robust test collections for retrieval evaluation. In Proc. of SIGIR, 2007. [7] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In Proc. of SIGIR, 2006. [8] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over thousands of queries. In Proc. of SIGIR, 2008. [9] B. A. Carterette. Multiple testing in statistical analysis of systems-based information retrieval experiments. ACM Trans. Inf. Syst., 30(1), Mar. 2012. [10] C. L. A. Clarke and M. D. Smucker. Time well spent. In Proc. of IIiX, 2014. [11] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In Proc. of SIGIR, 1998. [12] C. Hauff and F. de Jong. Retrieval system evaluation: Automatic evaluation versus incomplete judgments. In Proc. of SIGIR, 2010. [13] K. Kuriyama, N. Kando, T. Nozue, and K. Eguchi. Pooling for a large-scale test collection: An analysis of the search results from the first NTCIR workshop. Inf. Ret., 5(1), 2002. [14] W.-H. Lin and A. Hauptmann. Revisiting the Effect of Topic Set Size on Retrieval Error. In Proc. of SIGIR, 2005. [15] A. Moffat, W. Webber, and J. Zobel. Strategic system comparisons via targeted relevance judgments. In Proc. of SIGIR, 2007. [16] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1), Dec. 2008. [17] T. Sakai. Alternatives to bpref. In Proc. of SIGIR, 2007. [18] T. Sakai and N. Kando. On information retrieval metrics designed for evaluation with incomplete relevance assessments. Inf. Ret., 11(5), 2008. [19] M. Sanderson and J. Zobel. Information retrieval system evaluation: Effort, sensitivity, and reliability. In Proc. of SIGIR, 2005. [20] J. Urbano, M. Marrero, and D. Mart´in. On the measurement of test collection reliability. In Proc. of SIGIR, 2013. [21] E. M. Voorhees. Topic set size redux. In Proc. of SIGIR, 2009. [22] E. M. Voorhees and C. Buckley. The effect of topic set size on retrieval experiment error. In Proc. of SIGIR, 2002. [23] W. Webber and L. A. F. Park. Score adjustment for correction of pooling bias. In Proc. of SIGIR, 2009. [24] E. Yilmaz and J. A. Aslam. Estimating average precision with incomplete and imperfect judgments. In Proc. of SIGIR, 2006. [25] E. Yilmaz, E. Kanoulas, and J. A. Aslam. A simple and efficient sampling method for estimating AP and NDCG. In Proc. of SIGIR, 2008. [26] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In Proc. of SIGIR, 1998.

110

0.030 0.025 0.020 0.015 0.010 0.005 0.010
0.008
0.006

Ad Hoc 2 Ours Reduced Pool Webber
Ad Hoc 5

0.0050 0.0045 0.0040 0.0035 0.0030 0.0025
0.005
0.004
0.003

Ad Hoc 8 Web 2002

Legal 2006 0.15

0.008 0.006 0.004 0.002 0.012 0.010 0.008 0.006 0.004 0.005 0.004 0.003 0.002
0.010 0.008 0.006 0.004
0.06

Ad Hoc 3 Ad Hoc 6
Web 9 Genomics 2005 Medical 2011

0.0150 0.0125 0.0100 0.0075 0.0050
0.003 0.002 0.001 0.004 0.003 0.002 0.001
0.04
0.03
0.02

Ad Hoc 4 Ad Hoc 7 Web 2001 Robust 2005 Microblog 2011

M AE

0.10

0.04

0.006

0.004

0.05

0.02

0.002 0.00

5 10 20 30

100

5 10 20 30

100

P @n

5 10 20 30

100

Figure 3: Plots per test collection of the Mean Absolute Error against the P @n of the Reduced Pool and the two approaches, Ours and Webber, to correct pool bias. Generated using a leave-one-organization-out,

using all the runs for the continuous lines and only the top 75% for the dashed lines. Our approach uses as

indicator  and  = 1.

111

Ad Hoc 2 125
Ours

Ad Hoc 3

Ad Hoc 4

50

100

Reduced Pool

20

40

75

Webber

50

10

30

25

20

0

Ad Hoc 5

Ad Hoc 6

Ad Hoc 7

150

25

60

120

20

90

15

10 60
5

Ad Hoc 8
160 40

120 30

80 20

Web 9

40
20
Web 2001 100
75 50

SRE

10

40

25

Web 2002

0
Genomics 2005 200
40

Robust 2005

150

35

150

30

100

100

25

20

50 Legal 2006

15

Medical 2011

Microblog 2011

300

200

300

250

150

200

200

100

150

50

100

100

5 10 20 30

100

5 10 20 30

100

5 10 20 30

100

P @n

Figure 4: Plots per test collection of the System Rank Error against the P @n of the Reduced Pool and the two approaches, Ours and Webber, to correct pool bias. Generated using a leave-one-organization-out, using all the runs for the continuous lines and only the top 75% for the dashed lines. Our approach uses as indicator  and  = 1.

112

Shiny on Your Crazy Diagonal
Giorgio Maria Di Nunzio
Department of Information Engineering University of Padua
Via Gradenigo 6/a, 35131 Padua, Italy
dinunzio@dei.unipd.it

ABSTRACT
In this demo, we present a web application which allows users to interact with two retrieval models, namely the Binary Independence Model (BIM) and the BM25 model, on a standard TREC collection. The goal of this demo is to give students deeper insight into the consequences of modeling assumptions (BIM vs. BM25) and the consequences of tuning parameter values by means of a two-dimensional representation of probabilities. The application was developed in R, and it is accessible at the following link: http://gmdn.shinyapps.io/shinyRF04.
Categories and Subject Descriptors
H.3.3 [[Information Search and Retrieval]: Retrieval Models; D.2.8 [Software Engineering]: Metrics--complexity measures, performance measures
General Terms
Theory, Algorithms, Experimentation
Keywords
Probabilistic Models, Bayesian Inference, Text Retrieval
1. INTRODUCTION
The Binary Independence Model (BIM) has been one of the most influential models in the history of Information Retrieval [3]. It is a probabilistic model that considers documents as binary vectors and ranks them in order of their probability of relevance given a query according to the Probability Ranking Principle [2]. The Okapi BM25 model goes one step further by taking into account the frequencies of the terms and the length of the documents. These two models are easy to train and reach satisfactory results even with standard default parameters. The optimisation of the parameters can be performed by means of machine learning approaches; nevertheless, this process may be computationally demanding and only lead to partial improvements [6].
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author(s). Copyright is held by the owner/author(s). SIGIR'15, August 09-13, 2015, Santiago, Chile. ACM 978-1-4503-3621-5/15/08. DOI: http://dx.doi.org/10.1145/2766462.2767867 .

In this demo, we want to study the problem of the optimisation of the parameters of the two BIM and BM25 models by means of an interactive visualisation approach based on the idea of Likelihood Spaces [5], a two-dimensional representation of probabilities. We have developed a web application which allows users to be directly involved into the process of the optimisation of the retrieval function in a real machine learning setting. The goal of this demo is to give students deeper insight in the consequences of modeling assumptions (BIM vs. BM25) and the consequences of tuning parameter values. As a showcase, we used the TREC2004 Robust test collection (528,155 documents and 250 topics). 1 We added to the original collection a pseudo-relevance feedback set constituted by the top 100 documents obtained by a `standard' BM25 approach for each query. For the online version of this demo, we used a sample of the collection in order to make the application usable. In particular, we built the training/validation sets by using all the pseudo-relevant feedback documents of the BM25 and the test set with all the relevant documents of the pool.

2. MATHEMATICAL BACKGROUND
The BIM ranks documents according to the probability of relevance (R = 1) given a document d and a query q, P (R = 1|d, q). The logarithm of this probability can be approximated by (see [4] for the details of all the passages):

log (P (R = 1|d, q)) 

wi

(1)

ti dq

where wi is the relevance score of the term ti, defined as

wi

= log pi (1 - qi) (1 - pi) qi

,

(2)

where pi is the probability that a relevant document contains the term ti, while qi is the probability that a non-relevant document contains the term ti. The estimates of pi and qi are smoothed with two parameters  and  to avoid zero probabilities (by default,  =  = 0.5, see [1]). The BM25 model has the same form of Eq.1 by replacing wi with wi:

wi

=

tfi

+

k1



tfi ((1 - b)

+

b



dl/)

·

wi

(3)

where tfi is the frequency of the term ti in the document, k1 and b are two parameters (usually set to 1.2 and 0.75, respectively), dl is the length of document d, and  is the

1http://trec.nist.gov/data/t13_robust.html

1031

average document length. Eq. 3 derives from [4] (in this formulation, (k1 + 1) is not present in the numerator).
In the two-dimensional representation of probabilities, we keep P (R = 1|d, q) distinct from the probability of a document being not relevant P (R = 0|d, q), and we order documents according to the difference:

log pi - log qi

(4)

ti

1 - pi

ti

1 - qi

With two more parameters M and Q, we define a decision line y = M x + Q (see the details in [1]):

M

pi +Q -

qi

(5)

ti 1 - pi

ti 1 - qi

x

y

This formulation allows us to study the problem on a twodimensional space where documents are represented by two coordinates x and y and ranking is performed by the line Mx + Q - y.

3. SHINY WEB APPLICATION
The application has been developed in R using the Shiny package, an R package that makes it easy to build interactive web applications straight from R. 2 The main window of the application is split into two parts, see Fig. 1: on the left side, the user can interact with the retrieval models and see the results on the right side in terms of both the performance of the retrieval and the visualisation of the coordinates. Interaction: (1) The user chooses the topic of interest from the drop-down menu, and the query is shown in the text area. (2) The user selects the retrieval model (if BM25 is not selected, the BIM is on), the pseudo-relevance feedback is used (default is on), and whether the sum of Eq. 5 of the probabilities is computed over the selected features (4) or over the query terms. If BM25 is selected, the parameters k1 and b can be adjusted. (3) The parameters  and  are used to smooth the probabilities pi and qi. (4) The fourth part focuses on the machine learning approach to the problem: select the number of folds k and the number of features we need to train the retrieval model. By default we have a fivecross validation and the top 50 features selected according to the difference pi - qi. The user can change the ranking line by adjusting the two sliders Angular coefficient M and Intercept Q. The default values are 1 and 0, respectively, which correspond to a zero-one loss function. When these parameters are changed, a green line remain fixed in the position of the zero-one loss function for comparison. Visualization: The right panel is divided into two columns: (6) shows the results on the validation set, (7) the results on the test set. Both columns contain the following pieces of information (from top to bottom): (i) The text box shows the total number of objects used for validation and the number of positive examples (red points, the pseudo-relevant documents of the chosen topic). The box in the validation column also tells the user in what fold we are validating. (ii) The table shows performance measures in terms of precision-atj (j = 5, 10, 20, 100, 500, 1000). (iii) The two-dimensional plot shows in red the relevant documents of the chosen topic (pseudo-relevant for the validation fold, true relevant for test set) and in black all the other documents of the collection.
2http://shiny.rstudio.com/

6

7

1

2 3

4

5

Figure 1: Web application developed in Shiny.

The blue line changes according to the parameters selected in (5) while the green line remains fixed to the bisecting line of the third quadrant.
4. CONCLUSIONS
In this demo, we have presented a Web application developed in R which allows users to interact with two retrieval models, the BIM and the BM25 models, on a standard TREC collection. The probabilistic models are visualised on a two-dimensional space based on the idea of Likelihood spaces. The interactive application shows, in a real machine learning setting, how the human pattern recognition capabilities can immediately detect whether the model is close to the optimal solution or not. In theory, a classical fullyautomatic machine learning approach that searches the best combination of parameters can find the optimal solution [6]. However, the number of possible combinations of the values of the parameters grows exponentially with the number of the parameters; consequently, this interactive approach may be a crucial step in setting the initial parameters of the function that optimises these parameters automatically.
5. REFERENCES
[1] Giorgio Maria Di Nunzio. A new decision to take for cost-sensitive na¨ive bayes classifiers. Information Processing & Management, 50(5):653 ­ 674, 2014.
[2] S. E. Robertson. The Probability Ranking Principle in IR. Journal of Documentation, 33(4):294­304, 1977.
[3] Stephen E. Robertson and Karen Sparck Jones. Relevance weighting of search terms. In Peter Willett, editor, Document retrieval systems, pages 143­160. Taylor Graham Publishing, London, UK" 1988.
[4] Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333­389, 2009.
[5] Rita Singh and Bhiksha Raj. Classification in likelihood spaces. Technometrics, 46(3):318­329, 2004.
[6] Andrew Trotman, Antti Puurula, and Blake Burgess. Improvements to bm25 and language models examined. In Proceedings of the 2014 Australasian Document Computing Symposium, ADCS '14, pages 58:58­58:65, New York, NY, USA, 2014. ACM.

1032

Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures

Long Xia Jun Xu Yanyan Lan Jiafeng Guo Xueqi Cheng
CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences
xialong@software.ict.ac.cn, {junxu, lanyanyan, guojiafeng, cxq}@ict.ac.cn

ABSTRACT
In this paper we address the issue of learning a ranking model for search result diversification. In the task, a model concerns with both query-document relevance and document diversity is automatically created with training data. Ideally a diverse ranking model would be designed to meet the criterion of maximal marginal relevance, for selecting documents that have the least similarity to previously selected documents. Also, an ideal learning algorithm for diverse ranking would train a ranking model that could directly optimize the diversity evaluation measures with respect to the training data. Existing methods, however, either fail to model the marginal relevance, or train ranking models by minimizing loss functions that loosely related to the evaluation measures. To deal with the problem, we propose a novel learning algorithm under the framework of Perceptron, which adopts the ranking model that maximizes marginal relevance at ranking and can optimize any diversity evaluation measure in training. The algorithm, referred to as PAMM (Perceptron Algorithm using Measures as Margins), first constructs positive and negative diverse rankings for each training query, and then repeatedly adjusts the model parameters so that the margins between the positive and negative rankings are maximized. Experimental results on three benchmark datasets show that PAMM significantly outperforms the state-of-the-art baseline methods.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Search and Retrieval ­ Retrieval Models
General Terms
Algorithms
Keywords
search result diversification; maximal marginal relevance; directly optimizing evaluation measures
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767710.

1. INTRODUCTION
It has been widely observed that users' information needs, described by keyword based queries, are often ambiguous or multi-faceted. It is important for commercial search engines to provide search results which balance query-document relevance and document diversity, called search result diversification [1, 30]. One of the key problems in search result diversification is ranking, specifically, how to develop a ranking model that can sort documents based on their relevance to the given query as well as the novelty of the information in the documents.
Methods for search result diversification can be categorized into heuristic approaches and learning approaches. The heuristic approaches construct diverse rankings with handcrafted ranking rules. As a representative method in the category, Carbonell and Goldstein [2] propose the maximal marginal relevance (MMR) criterion for guiding the construction ranking models. In MMR, constructing of a diverse ranking is formulated as a process of sequential document selection. At each iteration, the document with the highest marginal relevance is selected. The marginal relevance can be defined as, for example, a linear combination of the querydocument relevance and the maximum distance of the document to the selected document set. A number of approaches have been proposed [8, 23, 24, 25] on the basis of the criterion and promising results have been achieved. User studies also shows that the user browsing behavior matches very well with the maximal marginal relevance criterion: usually users browse the web search results in a top-down manner, and perceive diverse information from each individual document based on what they have obtained in the preceding results [5]. Therefore, in a certain sense, we can say that maximal marginal relevance has been widely accepted as a criterion for guiding the construction of diverse ranking models.
Recently, machine learning approaches have been proposed for the task of search result diversification [14, 20, 22, 29, 31], especially the methods that can directly optimize evaluation measures on training data [16, 28]. Yue and Joachims [28] propose SVM-DIV which formulates the task as a problem of structured output prediction. In the model, the measure of subtopic diversity is directly optimized under the structural SVM framework. Liang et al. [16] propose to conduct personalized search result diversification via directly optimizing the measure of -NDCG, also under the structural SVM framework. All of these methods try to resolve the mismatch between the objective function used in training and the final evaluation measure used in testing. Experimen-

113

tal results also showed that directly optimizing the diversity evaluation measures can indeed improve the diverse ranking performances [16, 28]. One problem with the direct optimization approaches is that it is hard, if not impossible, to define a ranking model that can meet the maximal marginal relevance criterion under the direct optimization framework.
In this paper, we aim to develop a new learning algorithm that utilizes the maximal marginal relevance model for ranking as well as can directly optimize any diversity evaluation measure in training. Inspired by the work of R-LTR [31] and Perceptron variations [7, 15], we propose a new algorithm for search result diversification, referred to as PAMM (Perceptron Algorithm using Measures as Margins). PAMM utilizes a sequential document selection process as its ranking model. In learning, it first generates positive rankings (ground truth rankings) and negative rankings for the training queries. It then repeats the process of estimating the probabilities for the rankings, calculating the margins between the positive rankings and negative rankings in terms of the ranking probabilities, and updating the model parameters so that the margins are maximized. We show that PAMM algorithm minimizes an upper bound of the loss function that directly defined over the diversity evaluation measures.
PAMM offers several advantages: 1) adopting the ranking model that meets the maximal marginal relevance criterion; 2) ability to directly optimize any diversity evaluation measure in training; 3) ability to use both positive rankings and negative rankings in training.
To evaluate the effectiveness of PAMM, we conducted extensive experiments on three public TREC benchmark datasets. The experimental results showed that our methods significantly outperform the state-of-the-art diverse ranking approaches including MMR, SVM-DIV, and R-LTR. We analyzed the results and showed that PAMM makes a good balance between the relevance and diversity via maximizing marginal relevance in ranking. We also showed that by directly optimizing a measure in training, PAMM can indeed enhance the ranking performances in terms of the measure.
The rest of the paper is organized as follows. After a summary of related work in Section 2, we describe the general framework of learning maximal marginal relevance model in Section 3. In Section 4 we discuss the proposed PAMM algorithm. Experimental results and discussions are given in Section 5. Section 6 concludes this paper and gives future work.
2. RELATED WORK
Methods of search result diversification can be categorized into heuristic approaches and learning approaches.
2.1 Heuristic approaches
It is a common practice to use heuristic rules to construct a diverse ranking list in search. Usually, the rules are created based on the observation that in diverse ranking a document's novelty depends on not only the document itself but also the documents ranked in previous positions. Carbonell and Goldstein [2] propose the maximal marginal relevance criterion to guide the design of diverse ranking models. The criterion is implemented with a process of iteratively selecting the documents from the candidate document set. At each iteration, the document with the highest marginal relevance score is selected, where the score is a linear combination of the query-document relevance and the maximum

distance of the document to the documents in current result set. The marginal relevance score is then updated in the next iteration as the number of documents in the result set increases by one. More methods have been developed under the criterion. PM-2 [8] treats the problem of finding a diverse search result as finding a proportional representation for the document ranking. xQuAD [25] directly models different aspects underlying the original query in the form of sub-queries, and estimates the relevance of the retrieved documents to each identified sub-query. See also [3, 9, 10, 11, 21]
Heuristic approaches rely on the utility functions that can only use a limited number of ranking signals. Also, the parameter tuning cost is high, especially in complex search settings. In this paper, we propose a learning approach to construct diverse ranking models that can meet the maximal marginal relevance criterion.
2.2 Learning approaches
Methods of machine learning have been applied to search result diversification. In the approaches, rich features can be utilized and the parameters are automatically estimated from the training data. Some promising results have been obtained. For example, Zhu et al. [31] proposed the relational learning to rank model (R-LTR) in which the diverse ranking is constructed with a process of sequential document selection. The training of R-LTR amounts to optimizing the likelihood of ground truth rankings. More work please refer to [14, 20, 22, 29]. All these methods, however, formulate the learning problem as optimizing loss function that loosely related to diversity evaluation measures.
Recently methods that can directly optimize evaluation measures have been proposed and applied to search result diversification. Yue and Joachims [28] formulate the task of constructing a diverse ranking as a problem of predicting diverse subsets. Structural SVM framework is adopted to perform the training. Liang et al. [16] propose to conduct personalized search result diversification, also under the structural SVM framework. In the model, the loss function is defined based on the diversity evaluation measure of -NDCG. Thus, the algorithm can be considered as directly optimizing -NDCG in training. One issue with the approach is that it is hard to learn a maximal marginal relevance model under the structural SVM framework.
In this paper, we propose a Perceptron algorithm that can learn a maximal marginal relevance model, at the same time directly optimizing diversity evaluation measures.
3. LEARNING MAXIMAL MARGINAL RELEVANCE MODEL
We first describe the general framework of learning maximal marginal relevance model for search result diversification.
3.1 Maximal marginal relevance model
Suppose that we are given a query q, which is associated with a set of retrieved documents X = {x1, · · · , xM }, where each document xi is represented as a D-dimensional relevance feature vector. Let R = RM×M×K denotes a 3-way tensor representing relationship among the M documents, where Rijk stands for the k-th relationship feature of document xi and document xj.

114

Algorithm 1 Ranking via maximizing marginal relevance
Input: documents X, document relation R, and ranking model parameters r and d
Output: ranking y 1: S0   2: for r = 1, · · · , M do 3: y(r)  arg maxj:xj X\Sr-1 fSr-1 (xj , Rj ) 4: Sr  Sr-1  {xy(r)} 5: end for 6: return y

The maximal marginal relevance model creates a diverse ranking over X with a process of sequential document selection. At each step, the document with the highest marginal relevance is selected and added to the tail of the list [31]. Specifically, let S  X be the set of documents have been selected for query q at one of the document selection step. Given S, the marginal relevance score of each document xi  X\S at current step is defined as a linear combination of the query-document relevance and diversity of the document to the documents in S:

fS (xi, Ri) = rT xi + dT hS (Ri),

(1)

where xi denotes the relevance feature vector of the document, Ri  RM×K is the matrix representation of the relationship between document xi and the other documents (note that Rij  RK denotes the relationship feature vector of document pair (xi, xj)), and r and d are the weights for the relevance features and diversity features, respectively.
The first term in Equation (1) represents the relevance of
document xi to the query and the second term represents the diversity of xi w.r.t. documents in S. Following the practice in [31], the relational function hS(Ri) is defined as the minimal distance:

hS (Ri) = min Rij1, · · · , min RijK .

xj S

xj S

According to the maximal marginal relevance criterion, sequential document selection process can be used to create a diverse ranking, as shown in Algorithm 1. Specifically, given a query q, the retrieved documents X, and document relationship R, the algorithm initializes S0 as an empty set. It then iteratively selects the documents from the candidate set. At iteration r (r = 1, · · · , M ), the document with the maximal marginal relevance score fSr-1 is selected and ranked at position r. At the same time, the selected document is inserted to Sr-1.

3.2 Learning the ranking model

Machine learning approaches can be used to learn the
maximal marginal relevance model. Suppose we are given N labeled training queries {(X(n), R(n), J (n))}Nn=1, where J (n) denotes the human labels on the documents, in the form of a binary matrix. J(n)(i, s) = 1 if document x(in) contains the s-th subtopic of qn and 0 otherwise1. The learning process, thus, amounts to minimize the loss over all of the training
queries:

N

min

L y^(n), J (n) ,

(2)

r ,d n=1

1Some datasets also use graded judgements. In this paper,

we assume that all labels are binary.

Table 1: Summary of notations.

Notations

Explanations

q
X = {x1, · · · , xM } xi  RD R  RM×M×K
Y
yY
y(t)  {1, · · · , M }
Sr  X fS (xi, Ri) hS (Ri) d r J
E(X, y, J)  [0, 1]

query list of documents for q
document relevant feature vector
relationship tensor among M documents set of rankings over documents the ranking of documents index of the document ranked at t selected documents before iteration r the scoring function at each step the relational function on Ri weights for relevance features weights for diversity features human labels on document subtopics diversity evaluation measure

where y^(n) is the ranking constructed by the maximal marginal relevance model (Algorithm 1) for documents X(n), and L(y^(n), J(n)) is the function for judging the `loss' of the predicted ranking y(n) compared with the human labels J(n).

3.3 Diversity evaluation measures
In search result diversification, query level evaluation measures are used to evaluate the `goodness' of a ranking model. These measures include -NDCG [5], ERR-IA [4], and NRBP [6] etc. We utilize a general function E(X, y, J)  [0, 1] to represent the evaluation measures. The first argument of E is the set of candidate documents, the second argument is a ranking y over documents in X, and the third argument is the human judgements. E measures the agreement between y and J.
As an example of diversity evaluation measures, -NDCG [5] is a variation of NDCG [13] in which the newly found subtopics are rewarded and redundant subtopics are penalized. The -NDCG score at rank k can be defined by replacing the raw gain values in standard NDCG@k with novelty-baised gains:

-NDCG@k =

k r=1

N

G(r)/

log(r

+

1)

k r=1

N

G

(r)/

log(r

+

1)

,

(3)

where N G(r) = s J (y(r), s)(1 - )Cs(r-1) is the novelty-

biased gain at rank r in ranking y, Cs(r-1) =

r-1 k=1

J

(y(k),

s)

denotes the number of documents observed within top r - 1

that contain the s-th subtopic, N G(r) is the novelty-biased

gain at rank r in a positive ranking, and y(k) denotes the

index of the document ranked at k. Usually the parameter

 is set to 0.5.

ERR-IA [4] is another popular used diversity evaluation

measure. Given a query with several different subtopics

s, the probability of each intent Pr(s|q) can be estimated,

where s Pr(s|q) = 1. The intent-aware ERR at rank k can be computed as:

ERR-IA@k = Pr(s|q)ERR@k(s),

(4)

s

where ERR@k(s) is the expected reciprocal rank score at k in terms of subtopic s.
Table 1 gives a summary of the notations described above.

115

4. OUR APPROACH: PAMM

4.1 Evaluation measure as loss function
We aim to maximize the diverse ranking accuracy in terms of a diversity evaluation measure on the training data. Thus, the loss function in Equation (2) becomes

N

1 - E(X(n), y^(n), J (n)) .

(5)

n=1

It is difficult to directly optimize the loss as E is a nonconvex function.
We resort to optimize the upper bound of the loss function under the framework of structured output prediction. According to Theorem (2) in [27], we know that the loss function defined in Equation (5) can be upper bounded by the function defined over the ranking pairs:

N

max E(X(n),y+,J (n))-E(X(n),y-,J (n)) ·

n=1

y+ Y+(n) ; y- Y-(n)

F (y+, X(n), R(n))  F (y-, X(n), R(n)) , (6)

where Y+(n) is the set of all possible `positive' rankings (rankings whose -NDCG/ERR-IA equals to one) for the n-th query, Y-(n) is the set of all possible `negative' rankings (rankings whose -NDCG/ERR-IA is less than one) for the n-the query, · is one if the condition is satisfied otherwise zero, and F (X, R, y) is the query level ranking model. F takes the document set X, document relationship R, and ranking over the document y as inputs. The output of F is the confidence score of the ranking y. The predicted y^(n) in Equation (5) can be considered as the ranking that maximizes F :

y^(n) = arg max F (X(n), R(n), y),

(7)

yY (n)

where Y(n) is the set of all possible rankings over X(n). Here F is defined as the probability of generating the ranking list y with a process of iteratively selecting the top ranked documents from the remaining documents, and using the marginal relevance function fS in Equation (1) as the selection criterion:

F (X, R, y) =Pr(y|X, R)

=Pr(xy(1) · · · xy(M)|X, R)

M -1

=

Pr(xy(r)|X, Sr-1, R)

(8)

r=1

M -1
=
r=1

exp{fSr-1 (xi, Ry(r))}

M k=r

exp{fSr-1

(xi,

Ry(k))}

where y(r) denotes the index of the document ranked at the r-th position in y, Sr-1 = {xy(k)}rk-=11 is the documents ranked at the top r - 1 positions in y, fSr-1 (xi, Ri) is the marginal relevance score of document xi w.r.t. the selected documents in Sr-1, and S0 =  is an empty set. With the definition of F , it is obvious that the maximal marginal
relevance process of Algorithm 1 actually greedily searches
the solution for optimizing the problem of Equation (7).
To conduct the optimization under the Perceptron frame-
work, the upper bound of Equation (6) is further relaxed,
by replacing the max with sum and moving the term

(E(X(n), y+, J (n)) - E(X(n), y-, J (n))) into · as margin. The upper bound of Equation (6) becomes:
N
F (X(n), R(n), y+) - F (X(n), R(n), y-) 
n=1y+ ;y-
E(X(n),y+,J (n))-E(X(n),y-,J (n)) . (9)
This is because i xi  maxi xi if xi  0 holds for all i, and x - y  z  z · x  y holds if z  [0, 1]. Please note that we assume E(X, y+, J)  [0, 1] and thus we have (E(X(n), y+, J (n)) - E(X(n), y-, J (n)))  [0, 1] because E(X(n), y+, J (n)) > E(X(n), y-, J (n)).
4.2 Direct optimization with Perceptron
The loss function in Equation (9) can be optimized under the framework of Perceptron. In this paper, inspired by the work of structured Perceptron [7] and Perceptron algorithm with uneven margins [15], we have developed a novel learning algorithm to optimize the loss function in Equation (9). The algorithm is referred to as PAMM and is shown in Algorithm 2.
PAMM takes a training set {(X(n), R(n), J (n))}Nn=1 as input and takes the diversity evaluation measure E, learning rate , number of positive rankings per query  +, and number of negative rankings per query  - as parameters. For each query qn, PAMM first generates  + positive rankings P R(n) and  - negative rankings N R(n) (line (2) and line (3)). P R(n) and N R(n) play as the random samples of Y+(n) and Y-(n), respectively. PAMM then optimizes the model parameters r and d iteratively in a stochastic manner over the ranking pairs: at each round, for each pair between a positive ranking and a negative ranking (y+, y-), the gap of these two rankings in terms of the query level ranking model F = F (X, R, y+) - F (X, R, y-) is calculated based on current parameters r and d (line (9)). If F is smaller than the margin in terms of evaluation measure E = E(X, y+, J)-E(X, y-, J) (line (10)), the model parameters will be updated so that F will be enlarged (line (11) and line (12)). The iteration continues until convergence. Finally, PAMM outputs the optimized model parameters (r, d).
Next, we will explain the key steps of PAMM in detail.
4.2.1 Generating positive and negative rankings
In PAMM, it is hard to directly conduct the optimization over the sets of positive rankings Y+(n) and negative rankings Y-(n), because in total these two sets have M ! rankings if the candidate set contains M documents. Thus, PAMM samples the rankings to reduce the training time.
For each training query, PAMM first samples a set of positive rankings. Algorithm 3 illustrates the procedure. Similar to the online ranking algorithm shown in Algorithm 1, the positive rankings are generated with a sequential document selection process and the selection criteria is the diversity evaluation measure E. After generating the first positive ranking y(1), the algorithm constructs other positive rankings based on y(1), by randomly swapping the positions of two documents whose subtopic coverage are identical.
For each training query, PAMM also samples a set of negative rankings. Algorithm 4 shows the procedure. The algorithm simply generates random rankings iteratively. If the generated ranking is not a positive ranking and satisfies the

116

Algorithm 2 The PAMM Algorithm

Input: training data {(X(n), R(n), J (n))}Nn=1, learning rate
, diversity evaluation measure E, number of positive rankings per query  +, number of negative rankings per query  -.

Output: model parameters (r, d)

1: for n = 1 to N do 2: P R(n) PositiveRankings(X(n), J (n), E,  +) {Algo-

rithm 3}

3: N R(n) NegativeRankings(X(n), J (n), E,  -) {Algo-

rithm 4}

4: end for

5: initialize {r, d}  random values in [0, 1]

6: repeat

7: for n = 1 to N do

8:

for all {y+, y-}  P R(n) × N R(n) do

9:

F  F (X(n), R(n), y+) - F (X(n), R(n), y-)

{F (X, R, y) is defined in Equation (8)}

10:

if F  E(X(n), y+, J (n)) - E(X(n), y-, J (n))

then

11:

calculate r(n) and d(n) {Equation (10)

and Equation (11)}

12:

(r, d)  (r, d) +  × (r(n), d(n))

13:

end if

14: end for

15: end for

16: until convergence

17: return (r, d)

user predefined constraints (e.g, -NDCG@20  0.8), the ranking will be added into the ranking set N R.
Please note that in some extreme cases Algorithm 3 and Algorithm 4 cannot create enough rankings. In our implementations, the algorithms are forced to return after running enough iterations.

4.2.2 Updating r and d
Given a ranking pair (y+, y-)  P R(n) × N R(n), PAMM updates r and d as

r  r +  × r and d  d +  × d,
if F (X, R, y+) - F (X, R, y-)  E(X, y+, J) - E(X, y-, J). The goal of the update is to enlarge the margin between y+ and y- in terms of query level model: F = F (X, R, y+) - F (X, R, y-). For convenience of calculation, we resort to the problem of

F (X, R, y+)

max log

,

r,d F (X, R, y-)

because F (X, R, y) > 0 and log(·) is a monotonous increasing function. Thus, r can be calculated as the gradient:

r

 =

log

F (X,R,y+) F (X,R,y-)
r

 log F (X, R, y+)  log F (X, R, y-)

=

-

,

r

r

(10)

Algorithm 3 PositiveRankings
Input: documents X, diversity labels J, evaluation measure E, and the number of positive rankings  +
Output: positive rankings P R 1: for r = 1 to |X| do 2: y(1)(r)  arg maxj:xj X\Sr-1
E Sr-1  {xj }, y(1)(1), · · · , y(1)(r - 1), j , J
3: Sr  Sr-1  {xy(1)(r)} 4: end for 5: P R  {y(1)} 6: while |P R| <  + do 7: y  y(1) 8: (k, l)  randomly choose two documents whose hu-
man labels are identical, i.e., J(y(k)) = J(y(1)(l)) 9: y(k)  y(l) {swap documents at rank k and l} 10: if y / P R then 11: P R  P R  {y} 12: end if 13: end while 14: return P R
Algorithm 4 NegativeRankings
Input: documents X, diversity labels J, evaluation measure E, and number of negative rankings  -
Output: N R 1: N R =  2: while |N R| <  - do 3: y  random shuffle (1, · · · , |X|) 4: if y / N R and E(X, y, J) is as expected then 5: N R  N R  {y} 6: end if 7: end while 8: return N R

where

 log F (X, R, y)  =

|X |-1 j=1

log

Pr(xy(j)

|X

\Sj-1

,

R)

r

r

|X |-1
=
j=1

xy(j) -

|X | k=j

xy(k)

exp{fSj-1 (xy(k),

Ry(k))}

.

|X | k=j

exp{fSj-1 (xy(k),

Ry(k))}

Similarly, d can be calculated as

 log F (X, R, y+)  log F (X, R, y-)

d =

d

-

, d

(11)

where

|X |-1

 log F (X, R, y)

=

d

j=1

hSj-1 (Ry(j))-

|X | k=j

hSj-1 (Ry(k))

exp{fSj-1 (xy(k),

Ry(k))}

.

|X | k=j

exp{fSj-1 (xy(k),

Ry(k))}

Intuitively, the gradients r and d are calculated so that the line 12 of Algorithm 2 will increase F (X, R, y+)
and decrease F (X, R, y-).

4.3 Analysis
We analyzed time complexity of PAMM. The learning process of PAMM (Algorithm 2) is of order O(T · N ·  + ·  - ·

117

Table 2: Statistics on WT2009, WT2010 and

WT2011. Dataset #queries #labeled docs #subtopics per query

WT2009

50

WT2010

48

WT2011

50

5149 6554 5000

38 37 26

M 2 · (D + K)), where T denotes the number of iterations, N the number of queries in training data,  + the number of positive rankings per query,  - the number of negative rankings per query, M the maximum number of documents for queries in training data, D the number of relevance features, and K the number of diversity features. The time complexity of online ranking prediction (Algorithm 1) is of order O(M 2(D + K)).
PAMM is a simple yet powerful learning algorithm for search result diversification. It has several advantages compared with the existing learning methods such as R-LTR [31], SVM-DIV [28], and structural SVM [26].
First, PAMM employs a more reasonable ranking model. The model follows the maximal marginal relevance criterion and can be implemented with a process of sequential document selection. In contrast, structural SVM approaches [26] calculate all of the ranking scores within a single step, as that of in relevance ranking. The marginal relevance of each document cannot be taken into consideration at ranking time.
Second, PAMM can incorporate any diversity evaluation measure in training, which makes the algorithm focus on the specified measure when updating the model parameters. In contrast, R-LTR only minimizes loss function that is loosely related to diversity evaluation measures and SVM-DIV is trained to optimize the subtopic coverage.
Third, PAMM utilizes the pairs between the positive rankings and the negative rankings in training, which makes it possible to leverage more information in training. Specifically, it enables PAMM algorithm to enlarge the margins between the positive rankings and negative rankings when updating the parameters. In contrast, R-LTR only uses the information in the positive rankings and the training is aimed to maximizing the likelihood.
5. EXPERIMENTAL RESULTS
5.1 Experiment setting
We conducted experiments to test the performances of PAMM using three TREC benchmark datasets for diversity tasks: TREC 2009 Web Track (WT2009), TREC 2010 Web Track (WT2010), and TREC 2011 Web Track (WT2011). Each dataset consists of queries, corresponding retrieved documents, and human judged labels. Each query includes several subtopics identified by TREC assessors. The document relevance labels were made at the subtopic level and the labels are binary2. Statistics on the datasets are given in Table 2.
All the experiments were carried out on the ClueWeb09 Category B data collection3, which comprises of 50 million English web documents. Porter stemming, tokenization, and stop-words removal (using the INQUERY list) were applied
2WT2011 has graded judgements. In this paper we treat them as binary. 3http://boston.lti.cs.cmu.edu/data/clueweb09

to the documents as preprocessing. We conducted 5-fold cross-validation experiments on the three datasets. For each dataset, we randomly split the queries into five even subsets. At each fold three subsets were used for training, one was used for validation, and one was used for testing. The results reported were the average over the five trials.
As for evaluation measures, -NDCG@k (Equation (3)) with  = 0.5 and k = 20 is used. We also used ERR-IA@k (Equation (4)) with k = 20 to evaluate the performances.
We compared PAMM with several types of baselines. The baselines include the conventional relevance ranking models in which document diversity is not taken into consideration. Query likelihood (QL) [18] language models for informa-
tion retrieval. ListMLE [17] a representative learning-to-rank model for
information retrieval. We also compared PAMM with three heuristic approaches to search result diversification in the experiments. MMR [2] a heuristic approach to search result diversifica-
tion in which the document ranking is constructed via iteratively selecting the document with the maximal marginal relevance. xQuAD [25] a representative heuristic approach to search result diversification. PM-2 [8] another widely used heuristic approach to search result diversification. Please note that these three baselines require a prior relevance function to implement their diversification steps. In our experiments, ListMLE was chosen as the relevance function. Learning approaches to search result diversification are also used as baselines in the experiments. SVM-DIV [28] a representative learning approach to search result diversification. It utilizes structural SVMs to optimize the subtopic coverage. SVM-DIV does not consider relevance. For fair performance comparison, in the baseline, we first apply ListMLE to capture relevance, and then apply SVM-DIV to re-rank the top-K retrieved documents. Structural SVM [26] Structural SVM can be configured to directly optimize diversity evaluation measures, as shown in [16]. In the paper, we used structural SVM to optimize -NDCG@20 and ERR-IA@20, denoted as StructSVM(-NDCG) and StructSVM(ERR-IA), respectively. R-LTR [31] a state-of-the-art learning approach to search result diversification. The ranking function is a linear combination of the relevance score and diversity score between the current document and those previously selected. Following the practice in [31], in our experiments we used the results of R-LTRmin which defines the relation function hS(R) as the minimal distance.
5.2 Features
As for features, we adopted the features used in the work of R-LTR [31]. There are two types of features: the relevance features which capture the relevance information of a query with respect to a document, and the diversity features which represent the relation information among documents. Table 3 and Table 4 list the relevance features and diversity features used in the experiments, respectively.
4http://www.dmoz.org

118

Table 3: Relevance features used in the experiments.

The first 4 lines are query-document matching features, each applied to the fields of body, anchor, title, URL, and the whole documents. The latter 3

lines are document quality features. [31]

Name Description

# Features

TF-IDF The tf-idf model

5

BM25 BM25 with default parameters

5

LMIR LMIR with Dirichlet smoothing

5

MRF[19] MRF with ordered/unordered phrase

10

PageRank PageRank score

1

#inlinks number of inlinks

1

#outlinks number of outlinks

1

Table 4: The seven diversity features used in the experiments. Each feature is extracted over two documents. [31]

Name

Description

Subtopic Diversity Text Diversity Title Diversity Anchor Text Diversity ODP-Based Diversity Link-Based Diversity URL-Based Diversity

Euclidean distance based on PLSA[12] Cosine-based distance on term vectors Text diversity on title Text diversity on anchor ODP4 taxonomy-based distance Link similarity of document pair URL similarity of document pair

5.3 Experiments with TREC datasets
In the experiments, we made use of the benchmark datasets of WT2009, WT2010, and WT2011 from the TREC Web Track, to test the performances of PAMM.
PAMM has to tune some parameters. The learning rate parameter  was tuned based on the validation set during each experiment. In all of the experiments in this subsection, we set the number of positive rankings per query  + = 5, and number of negative rankings per query  - = 20. As for the parameter E of PAMM, -NDCG@20 and ERR-IA@20 were utilized. The results for PAMM using -NDCG@20 in training are denoted as PAMM(-NDCG). The PAMM results using ERR-IA@20 as measures are denoted as PAMM(ERR-IA).
The experimental results on WT2009, WT2010, and WT2011 are reported in Table 5, Table 6, and Table 7, respectively. Numbers in parentheses are the relative improvements compared with the baseline method of query likelihood (QL). Boldface indicates the highest score among all runs. From the results, we can see that PAMM(-NDCG) and PAMM(ERRIA) outperform all of the baselines on all of the three datasets in terms of both -NDCG@20 and ERR-IA@20. We conducted significant testing (t-test) on the improvements of PAMM(-NDCG) over the baselines in terms of -NDCG@20 and ERR-IA@20. The results indicate that all of the improvements are statistically significant (p-value < 0.05). We also conducted t-test on the improvements of PAMM(ERRIA) over the baselines in terms of -NDCG@20 and ERRIA@20. The improvements are also statistically significant. All of the results show that PAMM is effective for the task of search result diversification.
We observed that on all of the three datasets, PAMM(NDCG) trained with -NDCG@20 performed best in terms of -NDCG@20 while PAMM(ERR-IA) trained with ERRIA@20 performed best in terms of ERR-IA@20. The results indicate that PAMM can enhance diverse ranking perfor-

mances in terms of a measure by using the measure in training. We will further discuss the phenomenon in next section.

Table 5: Performance comparison of all methods in

official TREC diversity measures for WT2009.

Method

ERR-IA@20

-NDCG@20

QL ListMLE
MMR xQuAD
PM-2 SVM-DIV StructSVM(-NDCG) StructSVM(ERR-IA)
R-LTR PAMM(-NDCG) PAMM(ERR-IA)

0.164 0.191(+16.46%) 0.202(+23.17%) 0.232(+41.46%) 0.229(+39.63%) 0.241(+46.95%) 0.260(+58.54%) 0.261(+59.15%) 0.271(+65.24%) 0.284(+73.17%) 0.294(+79.26%)

0.269 0.307(+14.13%) 0.308(+14.50%) 0.344(+27.88%) 0.337(+25.28%) 0.353(+31.23%) 0.377(+40.15%) 0.373(+38.66%) 0.396(+47.21%) 0.427(+58.74%) 0.422(+56.88%)

Table 6: Performance comparison of all methods in

official TREC diversity measures for WT2010.

Method

ERR-IA@20

-NDCG@20

QL ListMLE
MMR xQuAD
PM-2 SVM-DIV StructSVM(-NDCG) StructSVM(ERR-IA)
R-LTR PAMM(-NDCG) PAMM(ERR-IA)

0.198 0.244(+23.23%) 0.274(+38.38%) 0.328(+65.66%) 0.330(+66.67%) 0.333(+68.18%) 0.352(+77.78%) 0.355(+79.29%) 0.365(+84.34%) 0.380(+91.92%) 0.387(+95.45%)

0.302 0.376(+24.50%) 0.404(+33.77%) 0.445(+47.35%) 0.448(+48.34%) 0.459(+51.99%) 0.476(+57.62%) 0.472(+56.29%) 0.492(+62.91%) 0.524(+73.51%) 0.511(+69.21%)

Table 7: Performance comparison of all methods in

official TREC diversity measures for WT2011.

Method

ERR-IA@20

-NDCG@20

QL ListMLE
MMR xQuAD
PM-2 SVM-DIV StructSVM(-NDCG) StructSVM(ERR-IA)
R-LTR PAMM(-NDCG) PAMM(ERR-IA)

0.352 0.417(+18.47%) 0.428(+21.59%) 0.475(+34.94%) 0.487(+38.35%) 0.490(+39.20%) 0.512(+45.45%) 0.513(+45.74%) 0.539(+53.13%) 0.541(+53.70%) 0.548(+55.68%)

0.453 0.517(+14.13%) 0.530(+17.00%) 0.565(+24.72%) 0.579(+27.81%) 0.591(+30.46%) 0.617(+36.20%) 0.613(+35.32%) 0.630(+39.07%) 0.643(+41.94%) 0.637(+40.62%)

5.4 Discussions
We conducted experiments to show the reasons that PAMM outperforms the baselines, using the results of the WT2009 dataset as examples.
5.4.1 Effect of maximizing marginal relevance
We found that PAMM makes a good tradeoff between the query-document relevance and document diversity via maximizing marginal relevance. Here we use the result with regard to query number 24 ("diversity" which contains 4 subtopics), to illustrate why our method is superior to the baseline method of Structural SVM trained with -NDCG@20 (denoted as StructSVM(-NDCG)). Note that structural

119

PAMM intermediate rankings

1

StructSVM 2, 4



2, 4



2, 4



2, 4



2, 4

ranking positions

2

3

4

1, 4

2

1, 3

2

4

1, 3

1, 3

2

4

1, 3

1, 4

4

1, 3

1, 4

2

5 -NDCG@5 4 0.788 1, 4 0.744 1, 4 0.803 2 0.812 4 0.815

Figure 1: Example rankings from WT2009. Each shaded block represents a document and the number(s) in the block represent the subtopic(s) covered by the document.

SVM cannot leverage the marginal relevance in its ranking model. Figure 1 shows the top five ranked documents by StructSVM(-NDCG), as well as four intermediate rankings generated by PAMM(-NDCG) (denoted as fS0 , fS1 , fS2 , and fS3 ). The ranking denoted as fSr is generated as: first sequentially selecting the documents for ranking positions of 1, 2, · · · , r - 1 with models fS0 , fS1 , · · · , fSr-2 , respectively; then ranking the remaining documents with fSr-1 . For example, the intermediate ranking denoted as fS2 is generated as: selecting one document with fS0 and setting it to rank 1, then selecting one document with fS1 and set it to rank 2, and finally ranking the remaining documents with fS2 and putting them to the tail of the list. Each of the shaded block indicates a document and the number(s) in the block indicates the subtopic(s) assigned to the document by the human annotators. The performances in terms of -NDCG@5 are also shown in the last column. Here we used -NDCG@5 because only the top 5 documents are shown.
The results in Figure 1 indicate the effectiveness of the maximal marginal relevance criterion. We can see that the -NDCG@5 increases steadily with the increasing rounds of document selection iterations. In the first iteration, fS0 selects the most relevant document and puts it to the first position, without considering the diversity. Thus, the NDCG@5 of the ranking generated by fS0 is lower than that of by StructSVM(-NDCG). In the second iteration, the ranking function fS1 selects the document associated with subtopics 1 and 3 and ranks it to the second position, according to the maximal marginal relevance criterion. From the view point of diverse ranking, this is obviously a better choice than StructSVM(-NDCG) made, which selects the document with subtopics 1 and 4. (Note that both Structural SVM and PAMM select the document with subtopics 2 and 4 for the first position.) In the following steps, fS2 and fS3 select documents for ranking positions of 3 and 4, also following the maximal marginal relevance criterion. As a result, fS1 , fS2 , and fS3 outperforms StructSVM(-NDCG).
5.4.2 Ability to improve the evaluation measures
We conducted experiments to see whether PAMM has the ability to improve the diverse ranking quality in terms of a measure by using the measure in training. Specifically, we trained models using -NDCG@20 and ERR-IA@20 and

ERR-IA@20

-NDCG@20

0.43
0.42
0.41
0.4
0.39
fold1 fold2 fold3 fold4 fold5 PAMM(-NDCG) PAMM(ERR-IA)
Figure 2: Performance in terms of -NDCG@20 when model is trained with -NDCG@20 or ERRIA@20.
0.3
0.28
0.26
0.24
fold1 fold2 fold3 fold4 fold5 PAMM(-NDCG) PAMM(ERR-IA)
Figure 3: Performance in terms of ERR-IA@20 when model is trained with -NDCG@20 or ERRIA@20.
evaluated their accuracies on the test dataset in terms of both -NDCG@20 and ERR-IA@20. The experiments were conducted for each fold of the cross validation and performances on each fold are reported. Figure 2 and Figure 3 show the results in terms of -NDCG@20 and ERR-IA@20, respectively. From Figure 2, we can see that on all of the 5 folds (except fold 1), PAMM(-NDCG) trained with NDCG@20 performs better in terms of -NDCG@20. Similarly, from Figure 3, we can see that on all of the 5 folds (except fold 4), PAMM(ERR-IA) trained with ERR-IA@20 performs better in terms of ERR-IA@20. Similar results have also been observed in experiments on other datasets (see the results in Table 5, Table 6, and Table 7). All of the results indicate that PAMM can indeed enhance the diverse ranking quality in terms of a measure by using the measure in training.
5.4.3 Effects of positive and negative rankings
We examined the effects of the number of positive rankings generated per query (parameter  +). Specifically, we compared the performances of PAMM(-NDCG) w.r.t. different  + values. Figure 4 shows the performance curve in terms of -NDCG@20. The performance of R-LTR base-

120

-NDCG@20

0.44 12

10 0.42
8

6

0.4

4

PAMM(-NDCG)

PAMMR-(LT-NRDCG) 2

0.38

traiRn-inLTg Rtime

0 1 2 3 4 5 6 7 8 9 10

parameter  +

Figure 4: Ranking accuracies and training time w.r.t.  +.
0.44 8

0.42

6

-NDCG@20

0.4
0.38 5

4
PAMM(-NDCG) 2 PAMMR-(LT-NRDCG)
traiRn-inLTg Rtime 0
10 15 20 25 30 parameter  -

Figure 5: Ranking accuracies and training time w.r.t -.

time (hours) performance

time (hours) -NDCG@20

0.43

0.42

0.41

0.4

0.39
0.38 0.5

PAMM(-NDCG) R-LTR

0.6

0.7

0.8

0.9

-NDCG@20

Figure 6: Ranking accuracies w.r.t. different NDCG@20 values of the negative rankings.

0.4

0.3

0.2 -NDCG@20 ERR-IA@20

0

10 20 30 40 50 60

number of rounds

Figure 7: Learning curve of PAMM(-NDCG).

line is also shown for reference. From the result, we can see that the curve does not change much with different  + val-
ues, which indicates the robustness of PAMM. Figure 4 also shows training time (in hours) w.r.t. different  + values. The training time increased dramatically with large  +, be-
cause more ranking pairs are generated for training. In our experiments  + was set to 5.
We further examined the effect of the number of negative rankings per query (parameter  -). Specifically, we
compared the performances of PAMM(-NDCG) w.r.t. different  - and the results are shown in Figure 5. From the
results, we can see that the performance of PAMM increasing steadily with the increasing  - values until  - = 20,
which indicates that PAMM can achieve better ranking per-
formance with more information from the negative rankings.
As the cost, the training time increased dramatically, be-
cause more training instances are involved in training. In our experiments,  - was set to 20.
We also conducted experiments to show the effect of sam-
pling the negative rankings with different -NDCG values.
Specifically, in each of the experiment, we configured the Al-
gorithm 4 to choose the negative rankings whose -NDCG@20
values are 0.5, 0.6, 0.7, 0.8, and 0.9, respectively. Figure 6
shows the performances of PAMM(-NDCG) w.r.t. different
-NDCG@20 values of the sampled negative rankings. From
the results, we can see that PAMM performs best when the

-NDCG@20 of the sampled negative rankings ranges from 0.6 to 0.9. The results also indicate that PAMM is robust and not very sensitive to different methods of sampling the negative rankings.
5.4.4 Convergence
Finally we conducted experiments to show whether PAMM can converge in terms of the diversity evaluation measures. Specifically, we showed the learning curve of PAMM(-NDCG) in terms of -NDCG@20 and ERR-IA@20 during the training phase. At each training iteration the model parameters are outputted and evaluated on the test data. Figure 7 shows the performance curves w.r.t. the number of training iterations. From the results, we can see that the ranking accuracy of that PAMM(-NDCG) steadily improves in terms of both -NDCG@20 and ERR-IA@20, as the training goes on. PAMM converges and returns after running about 60 iterations. We also observed that in all of our experiments, PAMM usually converges and returns after running 50100 iterations. Similar phenomenon was also observed from the learning curve of PAMM(ERR-IA). The results indicates that PAMM converges fast and conducts the training efficiently.

121

6. CONCLUSION AND FUTURE WORK
In this paper we have proposed a novel algorithm for learning ranking models in search result diversification, referred to as PAMM. PAMM makes use of the maximal marginal relevance model for constructing the diverse rankings. In training, PAMM directly optimizes the diversity evaluation measures on training queries under the framework of Perceptron. PAMM offers several advantages: employs a ranking model that follows the maximal marginal relevance criterion, ability to directly optimize any diversity evaluation measure, and ability to utilize both positive rankings and negative rankings in training. Experimental results based on three benchmark datasets show that PAMM significantly outperforms the state-of-the-art baseline methods including SVM-DIV, structural SVM, and R-LTR.
Future work includes theoretical analysis on the convergence, generalization error, and other properties of the PAMM algorithm, and improving the efficiency of PAMM in both offline training and online prediction.
7. ACKNOWLEDGMENTS
This research work was funded by the 973 Program of China under Grants No. 2014CB340401, No. 2012CB316303, the 863 Program of China under Grants No. 2014AA015204, the National Natural Science Foundation of China under Grant No. 61232010, No. 61425016, No. 61173064, No. 61472401, No. 61203298, and No. 61202214.
We would like to express our gratitude to Prof. Chengxiang Zhai who has offered us valuable suggestions in the academic studies.
8. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of the 2th ACM WSDM, pages 5­14, 2009.
[2] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st ACM SIGIR, pages 335­336, 1998.
[3] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In Proceedings of the 18th ACM CIKM, pages 1287­1296, 2009.
[4] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM CIKM, pages 621­630, 2009.
[5] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of the 31st ACM SIGIR, pages 659­666, 2008.
[6] C. L. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. In Proceedings of the 2nd ICTIR, pages 188­199, 2009.
[7] M. Collins. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In EMNLP '02, pages 1­8, 2002.
[8] V. Dang and W. B. Croft. Diversity by proportionality: an election-based approach to search result diversification. In Proceedings of the 35th ACM SIGIR, pages 65­74, 2012.
[9] S. Gollapudi and A. Sharma. An axiomatic approach for result diversification. In Proceedings of the 18th WWW, pages 381­390, 2009.

[10] S. Guo and S. Sanner. Probabilistic latent maximal marginal relevance. In Proceedings of the 33rd ACM SIGIR, pages 833­834, 2010.
[11] J. He, V. Hollink, and A. de Vries. Combining implicit and explicit topic representations for result diversification. In Proceedings of the 35th ACM SIGIR, pages 851­860, 2012.
[12] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd ACM SIGIR, pages 50­57, 1999.
[13] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422­446, Oct. 2002.
[14] L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. Enhancing diversity, coverage and balance for summarization through structure learning. In Proceedings of the 18th WWW, pages 71­80, 2009.
[15] Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and J. Kandola. The perceptron algorithm with uneven margins. In Proceedings of the 19th ICML, pages 379­386, 2002.
[16] S.-S. Liang, Z.-C. Ren, and M. de Rijke. Personalized search result diversification via structured learning. In Proceedings of the 20th ACM SIGKDD, pages 751­760, 2014.
[17] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.
[18] C. D. Manning, P. Raghavan, and H. SchU¨ tze. An Introduction to Information Retrieval. Cambridge University Press, 2009.
[19] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proceedings of the 28th ACM SIGIR, pages 472­479, 2005.
[20] L. Mihalkova and R. Mooney. Learning to disambiguate search queries from short sessions. In Machine Learning and Knowledge Discovery in Databases, volume 5782, pages 111­127, 2009.
[21] F. Radlinski and S. Dumais. Improving personalized web search using result diversification. In Proceedings of the 29th ACM SIGIR, pages 691­692, 2006.
[22] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse rankings with multi-armed bandits. In Proceedings of the 25th ICML, pages 784­791, 2008.
[23] D. Rafiei, K. Bharat, and A. Shukla. Diversifying web search results. In Proceedings of the 19th WWW, pages 781­790, 2010.
[24] K. Raman, P. Shivaswamy, and T. Joachims. Online learning to diversify from implicit feedback. In Proceedings of the 18th ACM SIGKDD, pages 705­713, 2012.
[25] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proceedings of the 19th WWW, pages 881­890, 2010.
[26] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. J. Mach. Learn. Res., 6:1453­1484, Dec. 2005.
[27] J. Xu, T.-Y. Liu, M. Lu, H. Li, and M. Wei-Ying. Directly optimizing evaluation measures in learning to rank. In Proceedings of the 31th ACM SIGIR, pages 107­114, 2008.
[28] Y. Yue and T. Joachims. Predicting diverse subsets using structural svms. In Proceedings of the 25th ICML, pages 1224­1231, 2008.
[29] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th ICML, pages 1201­1208, 2009.
[30] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proceedings of the 26th ACM SIGIR, pages 10­17, 2003.
[31] Y. Zhu, Y. Lan, J. Guo, X. Cheng, and S. Niu. Learning for search result diversification. In Proceedings of the 37th ACM SIGIR, pages 293­302, 2014.

122

An Eye-Tracking Study of Query Reformulation

Carsten Eickhoff
Dept. of Computer Science ETH Zurich, Switzerland
ecarsten@inf.ethz.ch

Sebastian Dungs
University of Duisburg-Essen Duisburg, Germany
dungs@is.inf.uni-due.de

Vu Tran
University of Duisburg-Essen Duisburg, Germany
vtran@is.inf.uni-due.de

ABSTRACT
Information about a user's domain knowledge and interest can be important signals for many information retrieval tasks such as query suggestion or result ranking. State-ofthe-art user models rely on coarse-grained representations of the user's previous knowledge about a topic or domain. In this paper, we study query refinement using eye-tracking in order to gain precise and detailed insight into which terms the user was exposed to in a search session and which ones they showed a particular interest in. We measure fixations on the term level, allowing for a detailed model of user attention. To allow for a wide-spread exploitation of our findings, we generalize from the restrictive eye-gaze tracking to using more accessible signals: mouse cursor traces. Based on the public API of a popular search engine, we demonstrate how query suggestion candidates can be ranked according to traces of user attention and interest, resulting in significantly better performance than achieved by an attention-oblivious industry solution. Our experiments suggest that modelling term-level user attention can be achieved with great reliability and holds significant potential for supporting a range of traditional IR tasks.
Categories and Subject Descriptors
Information Systems [Information Retrieval]: Query Reformulation
Keywords
Eye-gaze Tracking; Knowledge Acquisition; Domain Expertise; Query Reformulation; Query Refinement; Query Suggestion; Mouse Cursor Tracking.
1. INTRODUCTION
Users of information retrieval systems have been shown to struggle with forming an accurate mental image of their information needs and the resources to satisfy them. Belkin et al. describe this observation as an Anomalous State of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
© 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767703.

Knowledge (ASK), hindering users' query formulation and search success [4]. To mitigate this effect, Web search engines offer query suggestions and recommendations that guide the searcher towards popular queries, frequently issued by other users. It is, however, often unclear how relevant such suggestions are for the individual user, especially for nontransactional information needs. Ideally, we would like to promote those suggestions, that lead the user to relevant, novel and understandable documents rather than just generally popular ones. Personalized generation of query suggestions based on the user's previous search and interaction history has been found as one way to address this problem [10]. The proposed models, however, are coarse-grained and represent only high-level notions of the user's active query vocabulary. They consider, for example, all previously encountered terms (e.g., all terms present on recently visited Web sites) to be known and understandable. While this family of approaches makes a valuable first step towards integrating an understanding of the user's state of knowledge into the query suggestion process, one would require a system that can account for the user's vocabulary at a significantly finer granularity, ideally on the term level.
The same issue plays up at other points of the search process, for example during result ranking. State-of-the-art relevance models often include representations of the user and their specific context such as previous search history [48], preferences in terms of high-level topics [26], or content readability [11]. While such notions of text complexity have been demonstrated to significantly increase retrieval performance by providing users with resources of appropriate reading level, the readability metrics themselves are not personalized and rely on general complexity estimates based on a very diverse audience of users. Instead, it would be strongly desirable to know which exact terms the searcher is able to recognize, understand and actively use.
In this paper, we use eye-gaze fixations and cursor movement information in order to study which concrete terms the user is exposed to on Web pages and search engine result pages (SERPs) and how they are subsequently re-used as query terms. In this way, we make three novel contributions over the state of the art in user modelling. (1) For the first time, we inspect the evolution of users' active query vocabulary during Web search on the term-level in a qualitative user study. (2) Based on the eye-gaze signal, we model the likelihood of the searcher using a given term for query reformulation. (3) Eye-gaze tracking requires expensive hardware that would greatly restrict the exploitation and adaptation of our method. In order to make our in-

13

sights flexibly applicable in most Web search settings, we substitute fixation information with traces of cursor movement.
2. RELATED WORK
Our investigation of related work will be guided by a number of topics that have been pursued in recent years. (1) First of all, we will revisit the body of work dedicated to measuring and tracking domain expertise, both statically as well as over time. (2) Secondly, there is an extensive line of work with the goal of query suggestion, reformulation and expansion. (3) And finally, we will give an overview of those eye-gaze or cursor-trace based studies that investigated user behaviour during the various stages of the search process.
Modern retrieval models rely on a diverse set of features in order to produce the final result ranking. One family of such features is concerned with measuring how familiar the searchers are with the topic of their information need. White et al. [49] investigated the behavior of domain experts and novices during Web search. They report higher likelihoods of search success for experts. Additionally, the authors observed gradual developments in domain expertise over the course of several weeks of search activity. Liu et al. [36] further find characteristic differences in user behaviour depending on the search task at hand. Wildemuth [50] studied concrete strategies and strategy types that domain experts and novices follow during information search. They find that over time, novices "learn" to use the same search patterns as experts if they are exposed to in-domain information for longer periods. This transition was studied in detail by Liu et al. [35], who investigated changes in domain expertise when searchers were following the same overarching tasks across multiple sessions. Eickhoff et al. [15] further showed session-level evidence of domain expertise increases in response to in-domain searches. The authors put a particular focus on the acquisition of new query terms which is explained by previous page visits. Zhang et al. [53] proposed an automatic prediction framework that was able to identify domain experts and novices based on a range of behavioral features. Kim et al. [29] cast the expertise problem as a combination of preferred reading level per topic. In this way, they tracked the notion of topic-normalized resource complexity for different users.
Query formulation can represent one of the cognitively most challenging steps in the information search process [13]. Building on the early investigations of Spink [47] and Saracevic [46], query suggestion functionality aims to aid the user at this initial stage. Chirita et al. [10] rely on information gathered on the user's local desktop in order to expand Web search queries. Kelly et al. [27] investigated query reformulation behaviour by offering query and term suggestions based on clustering and pseudo relevance feedback. They report a clear user preference for query suggestions over term suggestions. Gao et al. [16] rely on information mined from large-scale query log files to provide suggestions. Independently, Song and He [45], as well as Ma et al. [38], propose personalized query suggestions by analysing the user's previous click behaviour within the session in order to mine suggestion terms from skipped and visited documents.
Eye-gaze tracking has been used for unobtrusive tracking of user attention and interest for several decades [24, 43]. In the information retrieval community, a wide array of studies and applications have been proposed in recent years. Cutrell

and Guan [12] compare various degrees of SERP verbosity for different task types, finding that inherently navigational tasks require less information per item than informational ones. Granka et al. [17, 23] conducted an eye-tracking study of users' interaction with search result lists. They confirm several established notions such as the well-known position bias of user attention and note significant potential for using eye gaze signals as implicit relevance indicators. In a number of small-scale (5-8 participants) qualitative studies, Kunze et al. use head-mounted eye gaze tracking devices for inferring language expertise [30] and document types [31] based on reading styles and eye movement patterns. Williams and Morris [51] contrast the fixation duration of familiar and unknown words during silent reading. They find that unfamiliar words receive significantly longer attention windows than known ones. We will revisit this finding in Section 6 of this paper.
Saloj¨arvi et al. [44], as well as Brooks et al. [6] investigate a range of low-level eye-gaze features for inferring passagelevel relevance labels for information retrieval and collaborative filtering[42]. Loboda et al. [37] investigated eye-gaze indicators of sentence-level relevance. They found the overall number of fixations, the number of first pass fixations as well as the total viewing time to carry most indicative power. Interestingly, and somewhat conflicting with the findings of both this paper as well as [8], the authors could not find any clues of term-level relevance. Buscher et al. [8, 7] use fixation length and frequency as relevance indicators for query expansion and search result personalization. Their work is closely related to the application scenario presented in Section 6 of this paper. While their approach relied on shallow term-level feedback mechanisms, we leverage semantic information via related or synonymous terms as well as using a wider array of eye-gaze signals. Ajanki et al. [2, 41] use eye tracking hardware to infer document-level relevance across a manually curated document corpus and, subsequently, generate alternate queries based on salient terms. There is some evidence of the origin of previously encountered Terms being reused in active vocabulary [51, 15], but there has not yet been a dedicated study to further investigate and understand this vocabulary acquisition process.
Independently, Guo and Agichtein [19] as well as Huang et al. [22] propose to infer user eye gaze and, implicitly, user interest, from mouse cursor position and movement. The authors show a substantial overlap between both sources that motivates further exploitation as for example presented later in this work. In a follow-up publication [18], the authors exploit a similar range of signals in order to infer post-click document relevance. Finally, Huang et al. [21] predict clickthrough on the basis of cursor position and movement signals.
Our work differs from the above body of research in that: (1) it measures user vocabulary knowledge and its development over time at a much finer granularity than previous efforts, which mainly concentrate on broad topic verticals. (2) Our model is based on actual observations of user attention to individual terms as evidenced by eye-gaze and mouse-cursor traces. Previous work on domain expertise is based mainly on the posterior analysis of search engine log files in which the mere presence or absence of a term on a page or SERP is regarded as a signal. Information about whether the user actually saw the term, and if so, how long the engagement lasted, are not available. (3) The breadth

14

of existing eye-gaze and mouse cursor movement studies has investigated many aspects of the search process. However, an in-depth study of query reformulation behaviour at the term level, as we propose in this paper, has not yet been attempted. The timeliness and relevance of this study is further evidenced by several pieces of prior related work that explicitly state the need for a more qualitative understanding of query reformulation and term acquisition on the Web (e.g., [15]).
3. METHODOLOGY
The user study took place in a controlled lab environment at a university campus. Participants were recruited through advertisements (flyers and Internet ads) and public announcements. Overall, 17 persons (8 female and 9 male) participated in our study. All participants were students majoring in a range of different, often IT-related, subjects. All participants were between 19 and 27 year old (average 22.7). On average, each participant had 9.8 years of active Internet usage. All participants had experience with Web search engines as well as searching in digital libraries. Experiments lasted for about 60 minutes and participants were compensated by a payment of e 10. 7 (3 female, 4 male) of the participants were part of an initial pilot study while the remaining 10 persons contributed to the final experiments that will be analysed in the further course of this paper. Due to incremental changes to the experimental setup as well as occasional technical glitches during the pilot study, we report only the outcomes of the final experiment.
At the beginning of each experiment, a short introduction to the study, including the eye tracking hardware, was given. Participants were not informed about the concrete research questions and hypotheses of the study. Subsequently, the eye tracking hardware was calibrated and participants were told to maintain a firm yet comfortable seating position for the duration of the experiment.
All sessions were conducted on a Windows 7 system with 22" (1680 × 1150) display, running a Firefox 25.0.1 Web browser. Eye-gaze traces were recorded with an SMI RED remote eye tracking system that is integrated into the monitor. This setup is considered to be less intrusive than headmounted alternatives. The system captures gaze positions at an update frequency of 60Hz and an accuracy of 0.4°. The recordings and analysis were made using iViewX, Experiment Center, and Begaze 3.4. We rely on a number of essential smoothing techniques such as fixation grouping, e.g., described in [5]. Our instrumented Web browser saves screenshots of each accessed page and the final mapping between fixation coordinates and terms rendered on screen is established via OCR technology. This approach has the advantage of making all rendered text accessible, regardless whether it was expressed in plain HTML, or encapsulated in AJAX or JS containers. Previous work [14] found that the inability to parse text contained in such elements can significantly limit the performance of analytical and inference methods.
After the calibration, the Web browser was used to present the questionnaires and tasks to the users. The same tab was used for the questionnaires as well as the input boxes for task completion. Participants were asked to leave the instruction tab open at all times and to use other tabs to their liking. Task presentation and questionnaires were structured as follows:

To ensure task diversity yet obtain a reasonable amount of overlap between tasks, we hand-picked 6 topics (ids 31, 38, 41, 42, 55, 69) from the 2006 and 2007 editions of the TREC Question Answering Track's complex interactive QA task [28]. In the initial pilot study, we found that scientific and biomedical tasks resulted in more frequent query reformulations per session as users gradually acquired the relevant domain vocabulary. Many of the politically and societally motivated tasks have been comprehensively investigated and summarized, due to the time that has passed since the original formulation of the tasks. This resulted in socio-political tasks being mostly answerable with a single page visit. As a consequence, we expanded our pool by several additional topics (ids 53, 70, 72, 73). Our final selection consists of 10 tasks, originating mainly from the bio-medical domain.
Each participant was asked to complete three search tasks. For each task, they were offered two options and were told to choose a task according to their personal preference. As a result, we obtain a total of 30 search sessions (10 participants each choosing 3 tasks). We decided to offer this choice to allow participants to focus on tasks that they found personally interesting, which in turn is expected to spark better engagement and richer interactions during the session. A Web application scheduled the tasks, ensuring that each of the 10 topics was offered for selection equally often and in non-repeating pairings (in our case each topic was shown exactly 6 times to offer 10 participants each 3 choices of two of the 10 available topics). After the task selection, a demographic questionnaire was given to the participants. Subsequently, each of the actual tasks was presented in the browser and accompanied by short pre-task and post-task questionnaires. Pre-task questionnaires covered topic specific aspects like familiarity and confidence as well as perceived difficulty. Post-task questionnaires again covered the aspects of perceived task difficulty as well as perceived completeness and quality of the answers given by the users. We allocated up to 20 minutes time per search task, resulting in an overall duration of up to 60 minutes per participant across all tasks. After all three working tasks had been completed, the searcher's general opinion of the experiment was elicited in a post-experiment questionnaire.
Table 1 discusses some of the salient characteristics in anticipated and actual task difficulty according to the pre- and post session questionnaires. Since we offered the participants to choose their tasks, we can note interesting differences in task frequency. We report averages across all participants that selected and completed a given task. Answers were given on a 5-point scale ranging between settings of 1 (not at all [easy, familiar, . . . ]) to 5 (very [easy, familiar, . . . ]). We can note distinct task-specific levels of difficulty originating from different coverage of the topic on the Web. Due to the mostly bio-medical nature of the tasks and the lack of a relevant formal background knowledge among the participants, we see relatively low scores of prior familiarity. This can be further seen by the fact that our bio-medical laypeople generally overestimated the difficulty of the task (with the exception of Task 31, which also showed the lowest overall participant satisfaction with the results of their search activity).

15

Task Id
31 38 41 42 53 55 69 70 72 73
Overall

Frequency
4 4 1 4 3 5 1 6 0 2
30

familiar (pre)
2.25 2.75 4.00 1.00 2.33 1.20 1.00 1.17
1.50
1.73

Table 1: Lab-study task characteristics.

interesting (pre) easy (pre) easy (post) understandable (post)

3.75

2.50

1.25

3.50

3.50

3.00

2.75

4.50

4.00

2.00

5.00

5.00

2.75

2.50

4.00

4.50

3.67

2.67

3.33

4.00

2.80

3.40

3.20

4.60

4.00

4.00

5.00

5.00

2.5

2.5

2.33

3.5

-

-

-

-

4.50

3.50

4.00

4.50

3.23

2.83

3.00

4.17

found good results (post)
1.75 2.75 5.00 3.25 3.33 3.40 4.00 2.67
4.50
3.06

4. EYE-TRACKING EXPERIMENTS
In this section, we present the outcome of the previously described eye-tracking study. Following previous work [43], our experiments centrally consider eye-gaze fixations, brief periods of time when the reader focuses on a single location, during which no significant eye movement can be noted. The frequency and duration for which the gaze is kept steady are established signals of user attention.
4.1 Literal Term Acquisition
As a starting point to our investigation, let us revisit the findings of Eickhoff et al. [15] who conducted a log-based analysis of query term acquisition. They report that a significant share of all subsequently added query terms in a search session were present on SERPs and previously visited pages earlier in the same session. The authors interpret this observation as evidence of query term acquisition, but already state that, based solely on log files, there is no reliable way of determining which of these co-occurrences are genuine (i.e., the user actually sees and reuses a new term). Intuition suggests that many such cases are due to chance and are never really seen, processed and acquired by the user, e.g., because they were outside of the visible screen area displayed to the reader. To correct for these inaccuracies we reproduce their log-based approach for the search sessions collected in the previous section and contrast it with actual eye-gaze fixations. Similar to previous research, we find a share of 43% of all added query terms to have occurred on previously visited pages and SERPs. The number of actually fixated terms that later on are being used as query terms, however, is much lower (21%). As surmised originally, mere term presence is too coarse an estimator of query vocabulary evolution. The attention-based subset that we capture via eye-gaze tracking, instead, describes what the user has actually seen and potentially adopted from SERPs and Web pages.
To begin our in-depth investigation of term-level user attention and its effect on query reformulation behaviour, let us briefly introduce some necessary notation. Each search session comprises a number of SERPs and visited pages. We break these pages down into white space-separated tokens t. We distinguish between those tokens that appear in any of the session's queries Tq and the much larger remaining set of non-query terms Tn. The overall set of all tokens displayed in the session is given by the union T = Tq  Tn. Besides the displayed tokens, we also collect F , the set of all eye gaze fixations that were measured in the session. Each fixation f  F is described in terms of its duration dur (f ) and screen

Table 2: Term-level fixation statistics show higher-

than-average user attention on query terms.

Non-query terms Query terms

Relative frequency on page

0.83

0.17

Share of overall fixations

0.68

0.32

Rel. number of fixations per token

0.25

0.57*

Share of overall fixation duration

0.61

0.39

Avg. fixation duration per token

58ms

218ms*

coordinate loc(f ). Using the previously described mapping between screen coordinates and display terms, we can now associate fixations with the terms that were rendered at the respective coordinates on the screen. As for display tokens, we can now subdivide fixations into those that rest on query terms (Fq) and those that focus any other terms (Fn). For this study, we disregard any fixations that, after application of a tolerance threshold of 5 pixels, do not coincide with the bounding box coordinates of a display token. This step removes all fixations that fall on browser control elements, images or page margins.

att rel (Tq)

=

|Fq | |Tq |

(1)

dur rel (Fq) =

fFq dur (f ) fF dur (f )

(2)

Let us now compare the way in which users interact with query terms and non-query terms. Table 2 shows an overview of several eye-gaze fixation statistics. We note that the general distribution of tokens in T is heavily biased towards non-query terms. Unsurprisingly, as a consequence of this skewed distribution, Tn also receives the largest share of the session's fixations. If we, however, discount those absolute fixation frequencies by the overall distribution of query terms and non-query terms (see Equation 1), it becomes apparent that tokens in Tq receive a significantly higher relative number of fixations per token than their non-query counterparts Tn. Similarly, we notice that the duration of individual fixations, both in terms of the absolute per-fixation duration dur (f ) as well as the relative share of the overall fixation duration dur rel (Fq), are biased towards giving significantly more attention to Tq. The statistical significance of the differences between query and non-query terms was determined by means of a Wilcoxon signed-rank test at  = 0.05 level.
In practice, these effects result in situations as observed for example in Session 001, during which a participant with-

16

Table 3: Per-term fixation likelihood and duration

show a general upwards trend as the semantic proximity to query terms increases.
fixation duration fixation likelihood

LCH < 0.25

30ms

0.09

0.25  LCH < 0.5

36ms

0.15

0.5  LCH < 0.75

39ms

0.23

0.75  LCH < 1.0

34ms

0.18

1.0  LCH < 1.25

32ms

0.13

1.25  LCH < 1.5

38ms

0.27

1.5  LCH < 1.75

45ms*

0.35*

1.75  LCH < 2.0

53ms*

0.43*

LCH  2.0

86ms*

0.49*

Figure 1: Eye-gaze patterns of Session 001 prior to query reformulation show strong evidence for acquisition of the medical term "prostaglandin" occurring in a paragraph of user-highlighted text.
out formal medical background was solving Task 42: "What effect does Aspirin have on coronary heart diseases?". The participant started with a query that consisted of all nouns taken from the original task descriptor and studied a number of high-ranked results. On one of the visited pages, the participant encountered a text passage discussing the interplay between Aspirin and a particular kind of lipid compounds, so-called prostaglandins. After having read the paragraph, the participant reformulated the query, adding prostaglandin as a new query term. Figure 1 shows a visual representation of the eye-tracker output for this acquisition of medical jargon. The orange lines display gaze patterns and the size of circles depicts the duration of each fixation. This session, as well as many similar examples that we encountered in our experiments, motivate the use of term-level eye-gaze tracking output for modelling user attention. In Section 6, we will demonstrate that exact scenario at the example of re-ranking query suggestion candidates.
4.2 Semantic Proximity of Reformulations
Up to this point, we studied fixated words on SERPs and visited pages that were subsequently picked up as query terms for reformulation. While those literal term acquisitions occur frequently, the majority of reformulations cannot be explained in this way. We will now, instead, inspect the semantic relatedness between reformulation terms and previously fixated ones. The updated hypothesis being that even though eye-gaze fixations do not literally forecast all newly added terms, they describe the user's interest accurately enough to allow for us to infer which semantic cluster of terms will indeed be used. This scenario includes cases such as synonyms or antonyms of fixated terms being employed for reformulation. To measure semantic proximity, we rely on WordNet and the well-known LeacockChodorow similarity [34]. The LCH metric is based on the length of the shortest path between the synsets containing the two terms and the maximum taxonomy depth

Dt. We used the WS4J implementation of LCH (https: //code.google.com/p/ws4j/).

length

LCH = -log(

)

(3)

2 × Dt

To test our hypothesis, we will inspect the distribution of user attention across the spectrum of LCH similarity scores between query terms and fixated terms. Please note that for this experiment, we exclude all direct query term occurrences from the comparison to allow for an exclusive and unperturbed study of semantically related terms. Table 3 shows the per-term likelihood and duration of fixation as functions of semantic proximity in terms of LCH scores. Also note that the LCH scale, in this case, does not reach its full extent (normally around scores of 3.0) because we pruned away all literal query term occurrences for this experiment. We can observe an initial local attention peak at an early point of the scale (LCH scores between 0.5 and 0.75) which is due to the highly frequent, but at the same time hardly related, stop words. As we, however, approach the far end of the LCH scale, we note a significant increase in user attention, bearing evidence of the importance of semantic proximity, even if literal term overlap is not given. For the highest proximity ranges (LCH  1.5), we note a statistically significant increase in both fixation duration and likelihood as compared to each of the lower ranges (LCH < 1.5). Statistical significance of improvements was measured by means of a Wilcoxon signed rank test at  < 0.05-level. Later on, in Section 6 of this paper, we will make use of this observation for model smoothing purposes.
We experimented with a number of alternative measures of semantic proximity including HSO [20], LESK [3], or WUP [52]. All considered metrics show the same initial rise of fixation frequency and duration followed by a monotonic rise as semantic proximity scores increased. There did not seem to be a systematically beneficial choice of metrics. Finally, LCH was chosen due to its low computational complexity.

4.3 Term Length and Complexity
In the previous sections, we showed how future query terms as well as semantically related terms received significantly higher-than-average amounts of user attention. In order to verify that this conclusion is indeed valid and not just due to hidden correlations with other unobserved effects, the

17

Table 4: Query terms receive significantly longer

fixations than unrelated terms of comparable length.

all terms

stop words removed

Length query terms non-query terms query terms non-query terms

<4

30ms

29ms

32ms

30ms

4-7

38ms*

30ms

39ms*

32ms

8-11

49ms*

33ms

49ms*

33ms

12-15

55ms*

38ms

55ms*

38ms

16-19

69ms*

41ms

69ms*

41ms

 20

82ms*

51ms

82ms*

51ms

Table 5: Query terms receive significantly longer

per-term fixations than unrelated terms of compa-

rable complexity.

all terms

stop words removed

AOA query terms non-query terms query terms non-query terms

<4

30ms

31ms

31ms

32ms

4-6

35ms

32ms

35ms

33ms

6-8

49ms*

32ms

49ms*

34ms

8-10

72ms*

36ms

72ms*

36ms

10-12 112ms*

41ms

112ms*

41ms

12-14 173ms*

48ms

173ms*

48ms

14-16 228ms*

59ms

228ms*

59ms

16-18 287ms*

64ms

287ms*

64ms

 18 384ms*

89ms

384ms*

89ms

following paragraphs will discuss the effects of term length, complexity and stop words on user attention.
It is intuitively plausible that term length should play a role in the division of user attention. Longer terms take longer to read and have a greater likelihood of capturing chance fixations. Table 4 shows an overview of fixation duration on terms of various lengths. We note a monotonic, yet mild increase in fixation duration as terms grow longer. This applies to both query terms and non-query terms alike. However, for all but the very shortest length category, we see significantly longer durations for query terms than for arbitrary ones. Statistical significance of improvements was measured by means of a Wilcoxon signed rank test at  < 0.05-level.
Previous work established how complex or unknown terms generally captivate the reader's attention longer than easy or well-known ones. Given the mainly scientific domain of our tasks, it is possible that the increase in attention, that we observed previously, is explained by the terms inherent complexity rather than their relevance. To investigate this hypothesis, we rely on previous work by Kuperman et al. [32], who compiled a list of 50,000 English terms along with their average age of acquisition (AOA). Common words such as "the", "a", or "house" show low ages of acquisition, usually between 1.6 and 4.0. Higher ages of acquisition, on the other hand, indicate greater term specificity and complexity, such as "epithalamium" with an average AOA of 17.67. Table 5 shows the distribution of attention received by query terms and non-query terms of given AOA grades. For both classes, we can note a mild upwards trend as AOA ranks increase. A much greater margin, however, separates the two classes from each other. We can therefore safely assume, that while term complexity plays a role in the distribution of user attention among terms on the screen, the governing factor is indeed topical relevance, as assumed in Sections 4.1 and 4.2.
As a conclusion to our overview of potential hidden factors correlated with user attention, we would like to draw special attention to stop words. This class of highly frequent but

Table 6: The effect of individual query reformulation

strategies on LCH proximity between queries and

user attention.
Strategy Observed Frequency Avg. change in LCH(t)

Specification

48%

+24%

Generalization

16%

+18%

Reformulation

36%

+37%

individually uninformative terms form the "syntactic glue" that ties together the content terms that carry actual meaning. Using the popular Snowball list of stop words [40], we observe that stop words receive much less attention (on average 30 ms per term) than non stop words (65 ms). In order to control for the influence of stop words on the previous investigations of term complexity and length, both Tables 4 and 5 show the respective measurements after stop words were removed. Following intuition, stop words are short (average length of 3.9 characters) and non-complex (average AOA of 5.09). Accordingly, we exclusively observe changes in the early rows that list short and of low complexity. Even in those categories, the changes are only marginal, supporting the conclusion that stop words do not play a special role with respect to user attention but rather follow the general trend dictated by their individual lengths and complexities.
4.4 Alternative Reformulation Strategies
Throughout this section, we investigate various ways of explaining query reformulation in which users add new terms to an existing query. Lau and Horvitz [33] refer to this case as specialization since the focus of the conjunctive query is narrowed down with the addition of each new query term. They introduce 2 additional types of modifications, generalization, during which terms are removed which results in a broader, more diverse set of results, and reformulation, the exchange of one query term for another. This final case can effectively result in a radical topic shift depending on how semantically similar the terms are. To investigate the effect of all three major reformulation strategies, we group all instances of query reformulations depending on whether terms were added (specialization), removed (generalization) or exchanged (reformulation) and measure the average semantic similarity between the terms fixated by the user and the query, before and after the reformulation took place and report the relative difference. Table 6 shows the results of this experiment. As we can see, all three reformulation strategies result in a net gain in LCH scores, increasing the similarity between fixated terms and produced query vocabulary. The highest individual gains were noted for reformulations. Here, typically, mildly related terms are exchanged for highly related ones. Generalizations introduce the least dramatic increase in similarity. Mostly, this strategy corrects for previously added terms that drove the result set in an undesired topical direction.
5. CURSOR-TRACKING EXPERIMENTS
Based on the previously studied eye-gaze signals, we were able to show various forms of evidence of term acquisition during Web search. The main shortcoming of this approach (e.g., for improving the ranking quality) is the need for expensive, non-portable hardware to collect the required eyegaze traces. As a consequence, the number of observations is very limited. Previous work has found a strong correlation

18

Table 7: Term-level cursor hover statistics.
Non-query terms Query terms

Relative frequency

0.79

0.21

Share of overall hovers

0.74

0.26

Rel. number of hovers per token

0.045

0.059*

Share of overall hover duration

0.78

0.22

Avg. hover duration per token

130ms

135ms*

between eye gaze and cursor movements [19, 22]. In this section, we roll out the previously described experimental setup, identically, to a large and diverse audience of crowdsourcing workers on Amazon Mechanical Turk (AMT). Since the availability of eye tracking hardware cannot be ensured in this altered setup, we will instead rely on traces of cursor movement to infer user attention. Where, previously, we investigated duration and frequency of fixations, we will now replace them with the duration and frequency of mouse cursor hovers over terms on the screen. Aside from this substitution, all previously introduced formulas and notations remain the same.
We conducted an initial experiment comprising 500 search sessions. 137 individual workers were presented with a randomly selected topic out of the previously introduced pool of 10 and were redirected to a custom-built Web search engine based on the public API of a popular search engine provider. Each search session was remunerated with $ 0.25. The crowd demographics are more diverse in terms of age, level of education, field of study, and language background. The gender split is comparable to the situation described earlier for the lab study. Table 7 shows cursor hover statistics across 500 individual search sessions. Again, the majority of tokens on the page does not fall into the query term category Tq. Despite this heavily skewed prior distribution, the relative number of hovers per token as well as the time for which the cursor rests on query terms is significantly greater than for non-query terms.
Let us now move on from literal query term occurrences to a final inspection of semantic relatedness. Table 8 compares per-term hover duration and likelihood for varying ranges of LCH proximity scores. While the middle ground of the distribution is less indicative than in the fixation case earlier, we still note a significant increase in hover likelihood and duration between terms of low to moderate semantic proximity (LCH < 1.0) and those of high semantic proximity (LCH > 1.75). Statistical significance of improvements was measured by means of a Wilcoxon signed rank test at  < 0.05-level. Short hovers are much more likely to be caused by unrelated terms while very long hovers occur more frequently for related terms. This finding is supported by recent work by Ageev et al. [1], who investigate the connection between mouse cursor hover durations over relevant document passages to the results of user generated document summaries. Especially, their Figure 3 is in line with our findings here as well as earlier in Table 3.
When comparing our findings to the ones presented earlier in Section 4, it becomes obvious, that fixations are richer and more accurate predictors of user attention than cursor traces. The majority of users only occasionally use the mouse cursor in order to highlight text, mark their current reading position or follow textual hyper links. The result are the previously discussed correlations with topically relevant terms. For a share of 13.6% of the 500 search sessions, how-

Table 8: Per-term cursor hover frequency and duration show a general upwards trend as the semantic proximity to query terms increases.
hover duration hover likelihood

LCH < 0.25

116ms

0.041

0.25  LCH < 0.5

121ms

0.041

0.5  LCH < 0.75

119ms

0.039

0.75  LCH < 1.0

123ms

0.043

1.0  LCH < 1.25

127ms

0.047

1.25  LCH < 1.5

126ms

0.048

1.5  LCH < 1.75

129ms

0.050

1.75  LCH < 2.0 133ms*

0.053*

LCH  2.0

135ms*

0.055*

ever, we observed a stronger connection between eye gaze and mouse movement. Here, users employed the cursor to trace every line of text as they read, creating a pattern that closely mimics the shape of eye gaze traces. Informal discussion with industry researchers from Google revealed that they as well noted this behaviour for 12 - 15% of their user base during an earlier, yet unpublished, large-scale experiment. Having shown that in settings where eye gaze traces are not available, substantial insight into topical relevance of terms can be gained from mouse cursor movements, the next section of this paper will demonstrate the use of this source of evidence for the task of query suggestion.
6. MODELLING QUERY TERM USAGE
In the past, various successful applications integrated attention and interest information in the form of document display times [25, 23], clickthrough features [9, 45], hitting time [39], or the contents of personal data collections [10]. Most notably, Buscher et al. [8] use fixations on different document parts to reorder query expansion candidates. Their work proposed 4 different interest metrics, the best-performing one of which we will include as a baseline for comparison to our own method. Several advances in eye-tracking technology allow us to infer even more fine-grained signals than those studied by previous work. Concretely, (1) we map fixation durations and frequencies to individual terms while Buscher et al. rely on paragraph-level information. (2) In the previous sections, we saw that many reformulations are inspired by fixated terms, but often use related terms rather than the literal fixation term. In order to account for this effect, we include a model of semantic relatedness between candidate terms and fixation terms. Essentially, this broadens the coverage of our method and accounts for reformulations that include previously unseen terms in a fashion similar to language model smoothing.
6.1 Methodology
Buscher et al. [8] report best query expansion performance for their Gaze-Length-Filter (GLF). The method expands the well-known tf-idf formula by a user interest model based on the number of fixations on text segments shorter than 230 characters that contain a word w (SA(w)) and the frequency of longer text segments LA(w) containing that same term. Their approach modifies the standard tf-idf formula in such a way, that only the frequency of w in those segments of the document that were gazed at (cA) is considered. We include

19

Table 9: Query suggestion performance.

Method

MRR RR

API Output

0.76 0.28

Gaze-Length-Filter (GLF)

0.79* 0.26

Term-Attention-Model (TAM)

0.80* 0.24

Term-Attention-Model + Relatedness (TAM-R) 0.86 * 0.29

their model as a performance baseline. Please note that, in the following, we speak about words w rather than the previously discussed tokens t. While tokens are the atomic unit that receives measurable user attention, words represent general concepts. As a consequence, we add up the cumulative attention measured for each occurrence (token) tw of a word w in order to estimate the word's relevancy to the user's information need.
LA(w) GLF (w) = tf (w, cA) × idf (w, C) × LA(w) + SA(w) (4)
Additionally, we propose two novel, term-level attention models. TAM (w) combines F (w), the frequency of attention to term w with the cumulative length for which the attention lasted D(w) = dur (tw ). The mixture parameter  balances the relative contribution of the two terms, biasing the score towards attention frequency, as  increases.

TAM (w) = F (w) + (1 - )D(w)

(5)

At this point, neither the GLF baseline nor the TAM score can account for the addition of previously unseen terms w. In Sections 4 and 5, however, we observed significant shares of added query terms to not have been explicitly present or fixated before the reformulation. To remedy this, TAM-R(w) expands TAM by a semantic similarity metric LCH(w) between candidate term w and the set F of all terms that previously received user attention (fixation or cursor hover, respectively). In this way, we can ensure that the overall model score does not default to zero for unseen terms. The weight vector  defines the concrete relative contributions of semantic relatedness, attention frequency and attention duration.

TAM-R(w) = lLCH(w) + f F (w) + dD(w)s (6)
6.2 Experiments
Our experiments are based on another series of crowdsourcing tasks on our custom search engine interface. The experimental setup is identical to the one described earlier in Section 5. In this case, however, we included the top 7 query suggestions delivered by a commercial search engine API and measured the reciprocal rank (RR) of accepted (clicked) suggestions. We compare the original order of query suggestions with 3 alternative variants, each re-ranked by decreasing averaged scores in the three attention metrics (GLF, TAM, TAM-R) that were computed based on cursor hover information. Table 9 compares the query suggestion performance of the unmodified commercial API output with that of the various, previously introduced user attention-based models in terms of mean RR (MRR) across all accepted suggestions. Additionally, we inspect the stability of the methods in terms of their RR score variance.

For of all models that employ traces of user attention, we can observe consistently and significantly higher ranking performance with respect to the original API output. The mild performance gap, favouring TAM over the passagebased GLF could not be confirmed significant. As we, however, include the proximity-based smoothing functionality of TAM-R, query suggestion performance improves. Statistical significance of improvements was measured by means of a Wilcoxon signed rank test at  < 0.05-level. Significant improvements over the baseline API output are denoted by an asterisk, while significant improvements over the GLF baseline are indicated by the symbol. Manual analysis of the re-orderings introduced by the various models confirms that GLF and TAM incorporate knowledge about literal term acquisitions from previously consumed material. TAM-R, indeed, accounts for the addition of terms that relate to the same topic that the user is interested in, but that did not directly occur in the explored document segments. The best-performing combination of mixture weights (l = 0.3, f = 0.5, d = 0.2) was determined by means of a greedy parameter sweep in the range [0, 1] with step size 0.1, ensuring i = 1 at all times.
6.3 Qualitative Performance Analysis
Besides the mere quantitative performance overview, we manually inspected those cases in which the various systems performed especially well (or badly) in order to give qualitative insight into the respective strengths and weaknesses of the presented methods. As reflected by the solid baseline performance, the raw API output in many cases returns the correct suggestion candidate on the highest ranks. Exceptions to this rule were broad queries that cover many potential aspects of a topic. As one out of several examples that we encountered, take Topic 38, that deals with psychological and emotional consequences of obesity. While our test subjects were mainly interested in mental problems that accompany this medical condition, top ranking query suggestions were concerned with vascular conditions as well as damage to the joints caused by dramatically increased weight.
All three user attention based models successfully placed such off-focus suggestions at the bottom of the candidate list, thereby increasing ranking performance. Both GLF and TAM struggled with previously unseen terms. Users that only briefly explored the available information before reformulating their query saw worst performance issues since their sparse observation vectors were not indicative towards the correct choice of suggestion.
TAM-R, finally, showed significant improvements in such situations. The use of the semantic proximity component served to assign probability mass to previously unseen terms. In practice, this resulted in promoting suggestion candidates that named concrete anabolic steroid substances such as the legally sold Prostanozol rather than the overall class. The remaining performance gap can mostly be accounted to pairwise candidate swaps between the top ranks of our list and the clicked suggestion. In many cases, this happened for near-duplicate candidates such as "steroid consequences" and "steroid substance consequences" in which the added term "substance" does not significantly modify the query semantics.

20

7. CONCLUSION
In this paper, we studied query reformulation by means of an eye-gaze tracking system. Inspired by topics drawn from the TREC QA track, we conduct a series of lab-based user studies. Tracking user attention at the term level, a finer granularity than was previously used in the IR literature, we make a range of interesting observations: (1) A significant share of newly added query terms were previously present on SERPs and visited pages in the same session. Previous work on the log-based recognition of query term acquisition [15] overestimated this effect. With the help of eye tracking hardware, we were able to gain a more realistic impression of how many such term occurrences were indeed seen by the user. (2) We find that literal query term acquisition is often indicated by significantly higher-than-average amounts of prior user attention in the form of frequency and duration of fixations to the prospective query terms. (3) Often, query expansion does not literally re-use previously encountered terms but highly related ones, instead. In a series of experiments we highlighted the importance of semantic proximity between query expansion terms and the center of user attention. (4) To ensure the broad applicability of our results, we replicated our lab-based eye-tracking experiments in a distributed fashion at much larger scale by measuring mouse cursor movements instead of eye gaze fixations. We note that our high-level findings generalize well between the two signals and the conclusions drawn from the lab-based study are confirmed by mouse cursor traces. (5) Finally, we demonstrate the usefulness of our findings for established IR tasks by comparing a passage-level attention model proposed by previous work to two variants of our term-level attention model, finding that term-level models including information about semantic proximity between candidate terms and user interest can deliver significantly better ranking performance of query suggestions than an industrial baseline.
The insights presented in this paper inspire many interesting directions for future work. First of all, the significant performance gains achieved by incorporating estimates of user attention into the query reformulation process motivate an evaluation of other related tasks. User attention models empowered by eye-gaze or cursor movement signals hold potential gains for ranking results to subsequent queries in a session, diversifying result sets, estimating domain expertise or personalized textual complexity. In this work, we used a very short-lived attention model based exclusively on the contents of the current search session. This was mainly due to limited availability of resources of users. Assuming an industrial setting, long-term attention models that include the searcher's general interest in addition to the current session context can be expected to become powerful tools for a wide number of inference tasks. In this way, one could estimate a general user vocabulary model, that describes the searcher's active and passive language use in more than just term frequencies. Such a model could for example describe the ease with which a user generates and consumes a given term, the speed at which they expand their vocabulary of new domains, or gradual shifts in interest. Wide-coverage models like that are especially interesting in query-free environments in which the system pro-actively pushes information about standing queries or upcoming events to the user.

In this work, we showed how models of knowledge acquisition in terms of previously unknown terms can benefit IR tasks. There are many other types of knowledge acquisition, e.g., factual or procedural knowledge, that can greatly benefit retrieval performance. Gaining an understanding of these learning processes holds significant potential for delivering smarter, more user-aware retrieval facilities. Finally, while the focus of this work lies on fixations, there are multiple other signals that can be captured by means of eye-tracking hardware. In our experiments, we additionally measured pupil dilation and saccade patterns. These signals turned out to be rather noisy and inconclusive when broken down on term level. For reasons of space, we omitted the respective results from the paper. In the future, we plan to conduct a dedicated investigation of these adjunct signals.
8. REFERENCES
[1] M. Ageev, D. Lagun, and E. Agichtein. Improving search result summaries by using searcher behavior data. In SIGIR 2013.
[2] A. Ajanki, D. Hardoon, S. Kaski, K. Puolam¨aki, and J. Shawe-Taylor. Can eyes reveal interest? implicit queries from gaze patterns. User Modeling and User-Adapted Interaction, 2009.
[3] S. Banerjee and T. Pedersen. An adapted lesk algorithm for word sense disambiguation using wordnet. In Computational linguistics and intelligent text processing. Springer, 2002.
[4] N. Belkin, R. Oddy, and H. Brooks. Ask for information retrieval: Part i. background and theory. Journal of documentation, 1982.
[5] D. Beymer and D. Russell. Webgazeanalyzer: a system for capturing and analyzing web reading behavior using eye gaze. In CHI 2005. ACM.
[6] P. Brooks, K. Phang, R. Bradley, D. Oard, R. White, and F. Guimbretire. Measuring the utility of gaze detection for task modeling: A preliminary study. In Workshop on Intelligent Interfaces for Intelligent Analysis, 2006.
[7] G. Buscher, A. Dengel, and L. van Elst. Eye movements as implicit relevance feedback. In CHI 2008. ACM.
[8] G. Buscher, A. Dengel, and L. van Elst. Query expansion using gaze-based feedback on the subdocument level. In SIGIR 2008. ACM.
[9] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and H. Li. Context-aware query suggestion by mining click-through and session data. In KDD 2008. ACM.
[10] P. Chirita, C. Firan, and W. Nejdl. Personalized query expansion for the web. In SIGIR 2007. ACM.
[11] K. Collins-Thompson, P. Bennett, R. White, S. de la Chica, and D. Sontag. Personalizing web search results by reading level. In CIKM 2011. ACM.
[12] E. Cutrell and Z. Guan. What are you looking for?: an eye-tracking study of information usage in web search. In CHI 2007. ACM.
[13] E. Efthimiadis. Query expansion. Annual review of information science and technology, 1996.
[14] C. Eickhoff, P. Serdyukov, and A. P. De Vries. A combined topical/non-topical approach to identifying web sites for children. In WSDM 2011.

21

[15] C. Eickhoff, J. Teevan, R. White, and S. Dumais. Lessons from the journey: A query log analysis of within-session learning. In WSDM 2014. ACM.
[16] W. Gao, C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and H. Hon. Cross-lingual query suggestion using query logs of different languages. In SIGIR 2007. ACM.
[17] L. Granka, T. Joachims, and G. Gay. Eye-tracking analysis of user behavior in www search. In SIGIR 2004. ACM.
[18] Q. Guo and E. Agichtein. Beyond dwell time: estimating document relevance from cursor movements and other post-click searcher behavior. In WWW 2012. ACM.
[19] Q. Guo and E. Agichtein. Towards predicting web searcher gaze position from mouse movements. In CHI 2010. ACM.
[20] G. Hirst and D. St-Onge. Lexical chains as representations of context for the detection and correction of malapropisms. WordNet: An electronic lexical database, 1998.
[21] J. Huang, R. White, G. Buscher, and K. Wang. Improving searcher models using mouse cursor activity. In SIGIR 2012. ACM.
[22] J. Huang, R. White, and S. Dumais. No clicks, no problem: using cursor movements to understand and improve search. In CHI 2011. ACM.
[23] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR 2005. ACM.
[24] M. Just and P. Carpenter. A theory of reading: From eye fixations to comprehension. Psychological review, 1980.
[25] D. Kelly and N. Belkin. Display time as implicit feedback: understanding task effects. In SIGIR 2004. ACM.
[26] D. Kelly and C. Cool. The effects of topic familiarity on information search behavior. In JCDL 2002. ACM.
[27] D. Kelly, K. Gyllstrom, and E. Bailey. A comparison of query and term suggestion features for interactive searching. In SIGIR 2009. ACM.
[28] D. Kelly and J. Lin. Overview of the trec 2006 ciqa task. In SIGIR Forum 2007. ACM.
[29] J. Kim, K. Collins-Thompson, P. Bennett, and S. Dumais. Characterizing web content, user interests, and search behavior by reading level and topic. In WSDM 2012. ACM.
[30] K. Kunze, H. Kawaichi, K. Yoshimura, and K. Kise. Towards inferring language expertise using eye tracking. In CHI 2013.
[31] K. Kunze, Y. Utsumi, Y. Shiga, K. Kise, and A. Bulling. I know what you are reading: recognition of document types using mobile eye tracking. In Proceedings of the 17th annual international symposium on wearable computers. ACM, 2013.
[32] V. Kuperman, H. Stadthagen-Gonzalez, and M. Brysbaert. Age-of-acquisition ratings for 30,000 english words. Behavior Research Methods, 2012.
[33] T. Lau and E. Horvitz. Patterns of search: Analyzing and modeling web query refinement. Courses and lectures-International Centre for Mechanical Sciences, 1999.

[34] C. Leacock and M. Chodorow. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 1998.
[35] J. Liu, N. Belkin, X. Zhang, and X. Yuan. Examining users' knowledge change in the task completion process. IPM 2012.
[36] J. Liu, M. J. Cole, C. Liu, R. Bierig, J. Gwizdka, N. J. Belkin, J. Zhang, and X. Zhang. Search behaviors in different task types. In JCDL 2010. ACM.
[37] T. Loboda, P. Brusilovsky, and J. Brunstein. Inferring word relevance from eye-movements of readers. In Proceedings of the 16th international conference on Intelligent user interfaces. ACM, 2011.
[38] H. Ma, H. Yang, I. King, and M. Lyu. Learning latent semantic relations from clickthrough data for query suggestion. In CIKM 2008. ACM.
[39] Q. Mei, D. Zhou, and K. Church. Query suggestion using hitting time. In CIKM 2008. ACM.
[40] M. Porter. Snowball: A language for stemming algorithms, 2001.
[41] K. Puolam¨aki, A. Ajanki, and S. Kaski. Learning to learn implicit queries from gaze patterns. In ICML 2008. ACM.
[42] K. Puolam¨aki, J. Saloj¨arvi, E. Savia, J. Simola, and S. Kaski. Combining eye movements and collaborative filtering for proactive information retrieval. In SIGIR 2005. ACM.
[43] K Rayner. Eye movements in reading and information processing: 20 years of research. Psychological bulletin, 1998.
[44] J. Saloj¨arvi, I. Kojo, J. Simola, and S. Kaski. Can relevance be inferred from eye movements in information retrieval. In WSOM 2003.
[45] Y. Song and L. He. Optimal rare query suggestion with implicit user feedback. In WWW 2010. ACM.
[46] A. Spink and T. Saracevic. Interaction in information retrieval: selection and effectiveness of search terms. JASIS, 1997.
[47] Amanda Spink. Term relevance feedback and query expansion: relation to design. In SIGIR 1994. Springer.
[48] J. Teevan, S. Dumais, and E. Horvitz. Personalizing search via automated analysis of interests and activities. In SIGIR 2005. ACM.
[49] R. White, S. Dumais, and J. Teevan. Characterizing the influence of domain expertise on web search behavior. In WSDM 2009. ACM.
[50] B. Wildemuth. The effects of domain knowledge on search tactic formulation. JASIST 2004.
[51] R. Williams and R. Morris. Eye movements, word familiarity, and vocabulary acquisition. European Journal of Cognitive Psychology, 2004.
[52] Z. Wu and M. Palmer. Verbs semantics and lexical selection. In ACL 1994.
[53] X. Zhang, M. Cole, and N. Belkin. Predicting users' domain knowledge from search behaviors. In SIGIR 2011. ACM.

22

Learning by Example: Training Users with High-quality Query Suggestions

Morgan Harvey
MIS Department Northumbria University
pwhq2@ Nenwocartshtlue,mUbKria.ac.uk

Claudia Hauff
Web Information Systems TU Delft
c.hThaeufNf@ethteurdlaenldfts.nl

David Elsweiler
I:IMSK University of Regensburg Germany
david@elsweiler.co.uk

ABSTRACT
The queries submitted by users to search engines often poorly describe their information needs and represent a potential bottleneck in the system. In this paper we investigate to what extent it is possible to aid users in learning how to formulate better queries by providing examples of high-quality queries interactively during a number of search sessions. By means of several controlled user studies we collect quantitative and qualitative evidence that shows: (1) study participants are able to identify and abstract qualities of queries that make them highly eective, (2) after seeing high-quality example queries participants are able to themselves create queries that are highly eective, and, (3) those queries look similar to expert queries as defined in the literature. We conclude by discussing what the findings mean in the context of the design of interactive search systems.
Categories and Subject Descriptors: H.3.3 Information Search and Retrieval General Terms: Measurement, Experimentation, Human Factors Keywords: Search expertise; Reflection; Behavioural Change; User Study
1. INTRODUCTION
Much of the IR research in the last half century has, with great success, focused on developing improved retrieval models to enhance the utility of retrieval systems for the end user [41]. In this line of research search queries submitted to a retrieval system are considered as a given. The focus is placed on what to do systematically to return relevant documents given this limited representation of the user's information need. A complementary approach with potentially more scope for future performance gains is to focus on giving the system more to work with by assisting users in creating better queries for specific search systems [32, 24].
All three authors contributed equally to this work.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767731.

Considerable evidence exists showing that many users do not know how to generate good queries. Analyses of search transaction logs show that people use short queries, especially on the Web [2] and even in this familiar domain a good proportion of searches fail completely [13]. In many search domains, including Web and Email search (with domainspecific search systems and interfaces), expert users achieve better retrieval eectiveness than novices and demonstrate dierent querying behaviour [3, 12, 43]. Moreover, despite the fact that most users today have to navigate through a range of search systems in their digital life, it has been reported that many users are inflexible in their approach and tend to use the same querying strategies regardless of task and available search system [29].
Typical solutions to assist users in creating eective search queries are the use of search UI features, such as query suggestions [35], related searches [36] or query autocompletion [5]. Alternatively, systems can employ context and personalisation techniques [11], which involve storing (and learning from) personal search histories and preferences to understand what a user knows and likes [18].
A third approach is to educate users about how to become better searchers [28] or to help users reflect on their own behaviour by comparing it to experts [6]. This method has the advantage that it is complementary to technical solutions. Our research continues along this path by investigating to what extent we can teach users how to pose better search queries to a particular search system. In contrast to existing approaches, we aim to understand if users are able to recognise, compare and contrast the properties of their own queries with good queries (provided by the system) and make changes to the queries they generate based on these insights. This is a new way of thinking about query suggestions; instead of providing automated examples for users to simply click on, we present them in a way that leads the user to reflect on his own behaviour, positively influencing his actions as a result.
The two principle research questions we answer are:
RQ1 Are users able to notice dierences between good queries and their own and abstract these dierences to change their own behaviour? If so, what are the noticeable dierences?
RQ2 How eectively can users learn and abstract from good queries; do users who are "trained" perform better than users who did not receive the training? Which properties of their queries do users adapt after training?

133

2. RELATED WORK
It is well-recognised that searchers have di culties communicating their information needs [7, 38, 24]. Taylor writes of a series of stages a user goes through when seeking information. These range from experiencing a visceral need, which is "probably inexpressible in linguistic terms" to a compromised need - a "representation of the inquirer's need within the constraints of the system and its files" [38]. Therefore, in order to generate successful queries, the user must overcome several cognitive challenges: 1) to determine himself what the need is and what kind of document will solve it; 2) to choose terms that describe that document well out of a very large set of possibilities [15] and 3) to communicate using the system's vocabulary and not his own [9].
Many interactive solutions have been designed to help the user overcome these challenges and improve the representations of information needs systems have to work with. The following subsection briefly reviews such work.
2.1 Interactive Query Support
IR systems can attain better descriptions of information needs by explicitly asking for certain details. The I3R system oered a means for users to provide terms and concepts they felt were important and identify relationships between these terms and other concepts in the domain [10]. Similarly, Kelly and Fu [24] used clarification forms to elicit additional information about the search context from users. The forms queried users on what they knew and what they would like to know about the topic and why. These were shown to be helpful in achieving improved retrieval performance.
A second technique is to assist the user to iteratively improve their own queries by adding additional terms suggested by the system, commonly referred to as interactive query expansion (IQE) [17]. This approach gives the user much more control over the search than if the query were to be expanded automatically (i.e. where the system selects expansion terms without user input [31]). Although IQE can oer improved performance [25], it has been shown that users are poor at identifying the terms that will oer the best improvement to their queries [33, 1]. This finding is intriguing with respect to our aims as it begs the question of whether or not users are able to identify qualities of good terms or whether they just assume terms suggested by a system will automatically be of a high quality.
Relevance feedback systems [34] are a further means to expand queries without explicitly choosing terms. Instead, relevance judgements are solicited on the returned documents. In addition to expanding queries, other scholars have investigated the performance of systems suggesting similar or related queries e.g. [36].
Improving user queries need not be achieved via technical solutions. One group in the 2007 SIGIR workshop breakout group identified a spectrum of possible solutions from manually-led approaches (based on improved information literacy and teaching) through to automatic, system-based approaches (based on more intelligent systems) [32]. The following section reviews literature on changing user behaviour via primarily non-technical means.
2.2 Changing Behaviour
Behaviour change support systems are "information systems designed to form, alter, or reinforce attitudes or behaviours or both without using coercion or deception" [30].

Within the context of search, changes can be made to the underlying retrieval engine or to the interface to `nudge' people towards submitting longer or better queries or to look deeper in the results list [6]. Altering the size [14] and wording [8] of the search box, for example, has been shown to influence the length of queries submitted. Moreover providing a simple "Google-like" search interface as opposed to a complicated multi-field catalogue search can radically alter user behaviour [27]. Training users on how to construct queries can improve search behaviour [26]. For example, providing guidance on the advanced features that can help with specific search tasks can improve performance for these tasks and users are able to preserve and use the knowledge gained weeks later [28]. Moreover, allowing users to reflect on their own behaviour and, importantly, compare their behaviour to other, expert users, enables individuals to improve their own habits. In [6] users, after reflection, spent longer considering search results and issued longer queries. They also used a wider range of techniques and search engine features.
We extend some of the ideas in [6] here. Rather than inviting users to compare their behaviour with that of experts, however, we investigate if they are able to learn by comparing their own queries to examples generated by the system to be near optimal for the task at hand. In doing so we relate the kinds of approaches shown in Section 2.1 with the approaches in this section. We attempt to 'nudge' users to improve their queries via high-quality examples shown via widgets similar to those described above.
3. RESEARCH METHODOLOGY
The aim of our work is to establish whether showing users of an unfamiliar search system examples of high-quality queries (for a small number of information needs) enables them to create better-performing queries themselves. We investigate to what extent users learn more successful querying behaviours from those examples.
Based on our two research questions (Section 1) we devised the following research hypotheses:
H1 Users are able to adapt their querying behaviour to pose good queries to an unfamiliar search system.
H2 Users are able to identify characteristics of highperforming queries that allow them to perform so well.
H3 A small number of "training queries" is su cient to enable a user to learn how to pose good queries themselves.
H4 A user who receives training with queries he can relate to (i.e. that are anticipated to perform well), learns better than a user receiving training with queries that are not predicted to perform well.
H5 A user who receives training with queries he can relate to, learns faster than a user receiving training with queries that are not predicted to perform well.
We conducted a number of user studies (Figure 1), each requiring the automatic generation of high-quality queries for a given information need and search system (described in Section 4.1). To address the issue of predicted performance, we performed an initial user study (Section 4.2) to investigate participants' perceptions of the generated queries. In contrast to the later studies, participants were not given access to our search system, their judgement was solely based on their own past experience.

134

Figure 1: Overview of our experimental design.
Concurrently with the User Perception Study, we performed a Pilot Study (Section 4.3) which gave us qualitative insights into the characteristics of good queries that users were able to identify. The results of these two studies then allowed us to conduct a larger Main Study (Section 4.4) and a follow-up Variable Training Size Study (Section 4.5) with a consistent design, but dierent training parameters. The aim here was to better understand how much training is required to achieve an eect. In each of these studies participants were asked to perform a series of ad-hoc retrieval tasks using our search system.
To maintain maximum control over the experiments and have access to complete statistics of the collection the participants were searching over, we used a standard test collection: AQUAINT1 together with the 50 TREC 2005 Robust track queries [40]. As our indexing and search engine we chose Apache SOLR2. To provide our study participants with a familiar user interface for searching the collection, we developed a web-based front-end in PHP (Figure 3).
4. EXPERIMENTS & RESULTS
In this section, we present an overview of each study and its results in turn.
4.1 Generating High-Performing Queries
In generating the "high-performing" query examples, we make the assumption that a query qa is better than another query qb for a given information need if qa returns a higher Average Precision (AP) score. It is also important that the queries are understandable by humans and are not excessively long. Therefore, we are not interested in queries that happen to return good results because of a statistical anomaly or because they are overly verbose and specific.
Candidate queries were obtained via a recursive, greedy search algorithm. For each topic and its corresponding set of relevant documents, a collection was built consisting of only those relevant documents. The query building process is initiated by first considering only queries of length 1 (i.e. single-term queries) and choosing each of the top 100 terms from the topic-specific document collection (after stop words had been removed). Each initialisation of the recursive method takes in a base query and adds each of the top 100 terms to it. All 100 new potential queries are run against the entire collection using the standard SOLR search system and the AP score of the top 50 returned documents is computed. The list of queries is then ranked by their AP values and the top 10 are added to the candidate query list. Subsequently, the algorithm is initiated again with new base query having the newly-selected term added to the end. This recursive process was continued up to a
1We removed duplicate documents in a pre-processing step, to provide a better and more familiar user experience.
2
http://lucene.apache.org/solr/

query length of 4. At the end of the process any duplicate queries were removed and the top 100 queries (according to AP) were selected as the final list of candidates.
Note that this approach diers significantly from previous methods proposed in the literature for generating queries, e.g. [4], as our goal is fundamentally dierent. Rather than generating queries which appear to be samples from the collection (i.e. stochastically drawn from collection statistics), we are specifically interested in queries which yield high performance, are understandable and would, potentially, be posed by real users. Other related approaches used to find optimal queries in Boolean systems (e.g. [37]) were inappropriate due to dierences in the underlying retrieval system. While our greedy approach does not produce globally optimal queries, it quickly produces large numbers of queries with an AP score of around 0.4. Concrete examples of generated queries can be found in the last column of Table 4.
Considering the top 100 queries for each topic, the median AP obtained by the generated queries over the first 20 returned results was 0.389. On a per-topic basis, the median was 0.391, the lowest average achieved was 0.054 and the maximum was 0.948 (IQR=0.31). Overall, 28 out of 50 topics had at least one query with an AP score greater than 0.5 and only 11 topics had any queries in the top 100 with an AP score below 0.2.
4.2 User Perception of Queries
To gain insights into how users perceive our high-quality queries (and as a precursor to answering hypotheses H4 and H5), we conducted a crowd-sourcing experiment on the CrowdFlower3 platform.
4.2.1 Study Overview
Each crowd-sourced task consisted of one search topic (in natural language form) and one of the queries generated in Section 4.1. Specifically, the workers were instructed as follows:
You already know query suggestions from search engines such as Google that present you with suggested queries while you type or show related queries alongside search results.
In this task, you will be given an information need (in natural language form) and a query suggestion that has been derived for this information need. You are asked to judge the query suggestion along three dimensions - surprise, usage and relevance.
Four questions had to be answered on a 5-point Likert scale:
1. How much do you know about the topic of the information need? (1: Very little, 5: A lot)
2. How surprised are you about the generated query suggestion? (1: Not at all surprised, 5: Extremely surprised)
3. Would you use this suggestion in an actual search? (1: No, I would not use it, 5: Yes I would use it)
4. What do you think the search result quality will be if this suggestion is used as query? (1: Very low quality, 5: Very high quality)
3
http://www.crowdflower.com

135

Each job consisted of 10 tasks and workers were paid 12 cents (a standard rate). In this and all following CrowdFlower experiments the participants were restricted to countries where English is a native language.
For each of the Robust track topics, the 15 most eective queries generated were judged by CrowdFlower workers. Each query was judged by 3 workers, and thus, for each topic 45 judgements were collected. Three examples of topics, generated suggestions and worker ratings are shown in Table 4.
4.2.2 Results
Our workers found many of the search topics rather challenging with an average topic knowledge rating of 2.21. The most familiar topics tended to be of broad interest to many dierent communities; the two with the highest average knowledge ratings (3.00 and 2.89 respectively) were What factors contributed to the growth of consumer on-line shopping? (topic 639) and Identify drugs used in the treatment of mental illness. (topic 383). In contrast, search topics focusing on very specific themes or entities tended to elicit the lowest familiarity ratings; the topic with the lowest average knowledge rating (1.58) was What is the status of The Three Gorges Project? (topic 416).
When considering how unexpected the presented suggestions were (i.e. the "surprise" factor) we found that the vast majority of queries (more than 80%) were at least somewhat expected, receiving a rating between 1 and 3 (top-left of Figure 2). Only a small number of suggestions were considered to be extremely surprising and those were mostly found in topics our study participants knew little about. This indicates that our query generation approach is achieving its goal of generating queries understandable to humans.

Number of ratings Number of ratings
Number of ratings

600

500

400

300

200

100

0

1

2

3

4

5

Rating

800

800

600

400

200

0

1

2

3

4

5

Rating

600

400

200

0

1

2

3

4

5

Rating

Figure 2: Histograms of "surprise" (top-left), "search quality" (top-right) and "suggestion usage" (bottom) ratings across the 750 dierent generated query suggestions, each rated by three users.
Of note is that fewer than 7% of judgements estimated the queries to achieve a very high quality of search results (top-right of Figure 2), while in contrast nearly 17% of the judgements were rated as likely to return very low quality search results. This result indicates that users are not able to judge the quality of query suggestions well, corroborating previous findings that users are unable to dierentiate good search terms from bad ones [33, 1]. This result can only be partially explained by their lack of topical domain knowledge as the correlation between knowledge ratings and

search quality ratings was moderate (but significant) with r = 0.35.
Lastly, we consider the question of to what extent users would use the shown suggestions in an actual search (bottom of Figure 2). Not unexpectedly, the correlation between the estimated search result quality and the potential usage is high (r = 0.77). Based on the ratings we have to conclude that many suggestions are not convincing, only a small number would definitely be used (9% of those rated 5) while 30% would definitely not be used (ratings of 1).
In summary, we find that user perception of our highquality queries varies; many of them are not recognised as being eective. We make use of this result in the Main Study: one group of users receives high-quality suggestions recognised as high quality in this study, while another group of users receives high-quality suggestions that were rated as low quality in this study.
4.3 Pilot Study
The pilot study had three goals: (i) to test the validity of our system and task setup, (ii) to learn more about experimental factors such as participant fatigue, and (iii) most importantly, to collect qualitative data in order to establish whether participants are able to notice qualities of example queries that make them so eective as hypothesised in H2.
4.3.1 Study overview
The participants (n=22) consisted of university students and sta members recruited via email lists and announcements in lectures from a major European university. Although the participants were not native English speakers, all had advanced English language skills. They were given access to our search system and asked to complete 10 search tasks. As seen in Figure 3 the information need was prominently displayed to the participants. Each time they issued a query (1), its retrieval eectiveness was displayed (5) in terms of the number of returned relevant documents within the top 20 results and the average precision (which was referred to as "search performance score"). Any relevant documents returned by the search were highlighted in blue (4).
The participants were instructed to submit queries that they believed would return relevant documents (i.e. useful and containing information pertinent to the task). They were told that the documents had already been evaluated for relevance and that each submitted query would be scored in terms of how many relevant documents were returned in the top 20 results and the positions of those documents within the ranked list. This second score is simply average precision as used during the automatic query generation process and users were encouraged to focus on this to determine how well they were doing in the task. Users could move on to the next topic with a click on the New topic, please button (6). Due to the interactive nature of the study, we selected 10 of the 50 Robust TREC topics by first eliminating those that that were either very di cult or very easy for our search system (measured in average precision achieved when using the title of the search topic as query) and then drawing randomly from the remaining topics4.
The study participants were provided with query suggestions as shown in Figure 3 (3) similarly to how Web search
4The topics used were 303*, 362, 367*, 375*, 378, 383*, 401, 426*, 638* and 689. * indicates those that were later also used in the Main Study.

136

2
3
1 6

5 4
Figure 3: Screenshot of search interface showing a list of search results as well as some query suggestions.

engines often present query suggestions. After participants have posed their first two queries to the system for a particular topic, they are shown a number of our high-quality query suggestions. All displayed suggestions are more eective (achieve an AP at least 10% higher) than the participant's previous queries. Thus, dierent participants receive dierent suggestions, depending on the quality of their initial queries. The interface conveys to the participant that these are high-quality queries and they are encouraged to use them (Figure 3 (3)).
To test hypothesis H2, i.e. to establish whether users are able to learn from high quality query examples, after every use of a suggestion participants were prompted to describe in a text box why they considered it to be eective: "You used the suggested query [query]. Considering your previous queries for this topic (shown below), what do you think is it about the suggested query that makes it so eective? ".
4.3.2 Results
The pilot findings help fine-tune our setup for the Main Study. Overall, the setup worked well, however we did establish fatigue to be a considerable factor. Figure 4 plots for each topic in sequence (recall that study participants receive the 10 topics in random order) the AP achieved by all queries submitted for the nth topic across all study participants. It is evident that over time (i.e. queries submitted for later topics) the retrieval eectiveness degrades. In particular after the 7th topic, the median AP is close to zero.
To investigate hypothesis H2 we analysed the free-text explanations from participants describing why they believe the example queries performed so well. The responses show that participants were indeed able to identify positive query characteristics. In total 81 descriptions were supplied and out of the 22 participants, 15 gave at least one description of a suggestion. 3 participants gave descriptions for all of the suggested queries they used.

We analysed the responses qualitatively using an a nity diagramming technique, a process allowing the discovery and validation of patterns in qualitative data [16]. 12 codes were generated describing qualities participants assigned to high-performing suggestions. These are shown in Table 1.
Category
C1: Specific query terms (specification)
C2: More general query terms (generalisation)
C3: Queries not in topic description
C4: Unexpected or surprising vocabulary
C5: Surprising non-use of vocabulary
C6: Uses term the user was surprised at the usefulness of (i.e. perhaps not surprising given the topic, but surprising that it was good for performance)
C7: Thinking creatively
C8: Advanced vocabulary (rare but not on a specialist subject relating to the topic)
C9: Specialist vocabulary (rare and to do with a specialist subject relating to the topic)
C10: Good combination of search terms
C11: Using synonyms and related concepts
C12: Query requires specialist or background knowledge
Table 1: Overview of the query categories identified during the pilot study
Not only does the established coding scheme provide evidence that users are capable of noticing and abstracting dierences between the suggested queries and their own - a prerequisite to learning - but the responses given are similar to those reported in the literature as being useful query reformulation strategies [23] or typical for queries submitted

137

Figure 4: Pilot study: Average precision over sequences of topics showing fatigue. The nth element of the box plot contains the AP achieved over all queries across all users submitted for the nth topic the users worked on (since topics were issued in randomised order, the topic sequence diers per user).

by system and domain experts. For example, a common way to improve queries is to either make them more specfic (C1) or general (C2) [23]. Experts submit queries which are more elaborate [21] (C7, C11, C12), use broader and more varied vocabulary [39] (C1, C2), exploit synonyms and related concepts [22](C11), and include terms not used in topic descriptions [21]. Moreover, domain experts often search with queries containing specialist or domain knowledge [42] (C9, C12).
We take this as evidence to accept hypothesis H2. It is important to point out, however, that some of the participants explicitly mentioned in their responses that they would not be able to create some of the examples due to lack of domain knowledge or vocabulary (C10, C13).
We conclude that, despite the fact that participants are not universally able to recognise good queries (Section 4.2.2), our pilot data show that for many queries people can determine a range of properties that explain good performance.
4.4 Main Study
The main study addresses hypotheses H1, H3, H4 & H5 and draws from the outcomes of the two previously discussed user studies.
4.4.1 Study overview
In this study, we use search topics that our workers in the user perception study considered themselves knowledgeable about to reduce the potential influence of domain knowledge on our results. We base our choice of experimental conditions on the reported perceptions of queries to reflect H2 and we reduced the number of tasks to six in an eort to counteract the fatigue eect observed in the pilot.
We use a between-groups design with participants randomly assigned to one of three experimental conditions:

· Group G

: this experimental group receives high-

exp high

quality query suggestions in the training phase which

were predicted to be eective in the user perceptions

study (Section 4.2).

· Group G

: this experimental group receives high-

exp low

quality query suggestions in the training phase which

were predicted to be ineective in the user perceptions

study (Section 4.2).

· Group G : the control group does not receive any control query suggestions.

For groups G

and G

, where suggestions are

exp high

exp low

given, we split tasks into two phases: the first four top-

ics are considered the training phase, where suggestions are

shown, and the final two tasks are referred to as the test

phase, where no suggestions are presented. Suggestions are

provided using the same approach and interface as in the Pi-

lot Study, i.e. suggestions were only given after two freely-

created queries had been submitted and when there were

queries available that would increase the AP score by at

least 10%. Again, topics were issued in random order.

The participants (n=91, 29 in G

, 34 in G

and

exp high

exp low

28 in G ) were also recruited via CrowdFlower and were

control

paid 50 cents for the completion of a job. A job consisted

of using our search system on six ad-hoc retrieval tasks; the

study participants were not informed about the two phases

of the study, they simply performed six search tasks (after

four of which the query suggestion UI element was removed).

4.4.2 Results

We first compare the eectiveness of the issued queries, before looking at properties of the submitted queries and the fatigue factor.

Average Precision

0.4

0.5

0.4 0.3
0.3 0.2
0.2
0.1 0.1

0.0

0.0

Gexp_high Gexp_low Gcontrol

Gexp_high Gexp_low Gcontrol

Figure 5: Main study: Querying performance over groups. Left: training topics. Right: test topics.

Effectiveness of Submitted Queries.

The fairest way to compare the performance across groups

is to consider only the first 2 queries submitted by each

participant for each topic. Doing so means we only con-

sider queries submitted before suggestions are provided for

a topic. Kruskal-Wallace rank sum tests show no signifi-

cant dierence between the groups on the training topics (p-

value=0.320) but a significant dierence for the test topics

(p-value=0.002), with both experimental groups (G

exp high

and G

) performing significantly better than G .

exp low

control

If we consider all queries submitted for the test topics, not

just the first 2 (as now no suggestions are shown to any user

group), then these results become even clearer as shown in

138

the top half of Table 2; participants of the G

group

exp high

issue on average queries achieving an AP of 0.10, while par-

ticipants of the alternative experimental condition G

exp low

achieve an AP of 0.06. The control group G

at this

control

stage submits queries which are an order of a magnitude

worse, with a mean AP of 0.004.

Figure 5 presents an alternative view of the submitted

queries' eectiveness across groups; the left boxplot shows

the retrieval eectiveness for the training topics whereas on

the right the eectiveness for the testing topics is shown. It

is evident that participants who receive high-quality training

suggestions perform better on average, but also that they

are able to achieve much higher maximum average precision

scores.

Main study
1: G
exp high
2: G
exp low
3: G
control
Training-size study
1: G
exp high
2: G
exp low
3: G
control

Training 2 Queries
0.0560 0.0238 0.025 Training 2 Queries
0.0922 0.014 0.04

Testing 2 Queries
0.043 0.041 0.024
Testing 2 Queries 0.055 0.0343 0.0132

Testing All Queries
0.0998 
0.0571
0.0039
Testing All Queries
0.0591 
0.0666
0.0132

Table 2: Average AP values aggregated across the

first two queries of the training topics (column II),

the first two queries of the test topics (column III)

and all queries submitted for the test topics (column

IV).  indicates a significant improvement over the

G

condition (Kruskal-Wallace rank sum test, p-

control

value  0.01).

If we look at how retrieval eectiveness changes as partic-

ipants query more on the same topic, we see a strong trend

where G

and G

continue to improve while those

exp high

exp low

in G

do not (Figure 6). At query position 1 there is very

control

little dierence between the groups; G

is only scoring

control

on average 0.005 worse than G

. However, this pat-

exp high

tern changes quickly with the experimental groups able to

achieve steadily more eective queries the more they submit,

which is not the case for the control group G . By the

control

4th query the dierence between G

and G

widens

exp high

control

considerably to 0.135.

These findings provide strong evidence of retrieval eec-

tiveness improvements for the experimental groups over the

control group. The analyses so far, however, do not evi-

dence a significant dierence in performance gain between

experimental conditions G

and G

.

exp high

exp low

Properties of Submitted Queries.

Beyond simply considering the retrieval eectiveness at-

tained by a given query, we can also look at other properties

of it that relate to its eectiveness or quality. These prop-

erties (shown in Table 3) go some way towards explaining

the observed improvements in performance achieved by both

experimental groups. We evaluated the submitted queries

with metrics reflecting the literature on expert querying be-

haviour (see Section 4.3.2). On many of these metrics the ex-

perimental groups G

and G

significantly outper-

exp high

exp low

form the control group G . The trend is generally that

control

group G

scores highest, group G

scores slightly

exp high

exp low

0.4 Control
Exp_High
0.3 Exp_Low

Average Precision

0.2

0.1

0

1

2

3

4

5

6

7

8

9

10

Query sequence

Figure 6: Main study: Average precision over sequences of queries on test topics. Each point in the plot represents the mean AP of all queries submitted as nth query. Truncated at query 10 as later queries have very few data points associated with them.

lower, but often not significantly so, and group G

control

achieves the poorest scores. Participants in G

and

exp high

G

, for example, tended to submit longer queries (in

exp low

both words and characters), which is noteworthy as the

example queries they were shown were designed not to be

overly long.

Out of all three groups, participants in G

submitted

exp high

the rarest query terms. We measured this both in terms of

the IDF statistics for the collection (i.e. their query terms

feature significantly less often in the test corpus as a whole)

and in terms of the number of overall participants who sub-

mitted those terms (we refer to this as median UserCount-

Term in Table 3). Comparing the Jaccard-coe cient scores

for query and topic description terms across the experimen-

tal conditions reveals that participants of G

were also

exp high

the most likely to take terms from the topic descriptions

given to them. These results suggest that a good query cre-

ation strategy was to use rare terms and seek inspiration

from the topic descriptions, echoing findings from the liter-

ature [44]. While this could be negatively construed, since

topic descriptions do not exist in real-life and users actu-

ally have di culties in describing what they want [38], this

finding does not explain the whole picture as there is no sig-

nificant correlation (r=0.21) between AP and the overlap of

queries with the topic descriptions (Jaccard score).

G

participants also submitted significantly more

exp high

queries per topic than G

participants. However, this

control

is less likely to explain the performance gains as there is no

significant dierence in the median number of queries sub-

mitted between G

and G

nor between G

exp high

exp low

exp low

and G . From the median time per topics it is also

control

evident that G

and G

spend significantly more

exp high

exp low

time working on each topic than G .

control

Fatigue.

One factor that could potentially aect the results is that

of fatigue; are groups G

and G

doing better be-

exp high

exp low

cause they are feeling less fatigued by the task, perhaps as

a result of getting some assistance in the early topics or by

being shown that high performance was possible and thus in-

creasing motivation? There are a number of metrics we can

consider to try to ascertain if fatigue is present: the amount

of time spent per query (query duration) and the amount

of time spent per topic (topic duration) and the number of

queries submitted. For all 3 groups the median query dura-

tion does seem to decrease slightly over the topics - linear

models show a significant negative coe cient over the top-

139

Median query length (words) Median query length (chars) Median #queries per topic Median time per topic (seconds) Median time per query (seconds) Median query term IDFs Median UserCountTerm Jaccard coe cient (query terms, topic descr. terms)

G1:

G exp high

5.4 29 3, IQR=4 165 13 4.9 43 0.3

G2:

G exp low

4.4 28 2, IQR=4 150 11 5.24 45 0.25

G3:

G control

4.3 23 2, IQR=2 97 13 5.15 51 0.25

Wilcox (p-value) G1/G2 G1/G3 G2/G3

p < 0.05 -- --
p < 0.01 p < 0.01 p < 0.01
-- p < 0.01

p < 0.01 --
p < 0.05 p < 0.01
-- -- p < 0.01 p < 0.01

-- p < 0.05
-- p < 0.01
-- -- -- --

Table 3: Main study: Overview of query properties aggregated for each user group across the two topics issued during the test phase.

ics of between -0.6 and -0.99. This is not the case for topic

duration, however, as there is no significant trend for any of

the 3 groups meaning that they all spend roughly the same

amount of time on each topic. The same consistency is also

present when looking at the number of queries submitted.

There is no significant correlation between topic sequence

and number of queries for any of the groups although groups

G

and G

do submit more queries overall. These

exp high

exp low

factors do not point strongly to fatigue being a factor, al-

though the subtle changes in query duration do suggest that

users are spending less time thinking about each query as

time goes on, which may explain the consistent reduction in

average precision.

4.5 Variable Training Size Study
An obvious question to ask, given these results, is what impact does the number of training topics given to the test groups have on performance. A final study investigated to what extent the number of training topics (hypothesis H3) influences a user's ability to formulate good queries.

0.4 Control
Exp_High
0.3 Exp_Low

Average Precision

0.2

0.1

0

1

2

3

4

5

6

7

8

9

10

Query sequence

Figure 7: Training-size study: Average precision over sequences of queries on test topics. Each point in the plot represents the mean AP of all queries submitted as nth query. Truncated at query 10 as later queries have very few data points associated with them.

4.5.1 Study overview
We used the same setup and experimental design as in the Main Study and varied only the ratio between training and test topics: in this study we used two topics for training, and the remaining four topics for testing. As in the Main Study,

participants (n=57, 19 participants in each condition) were recruited via CrowdFlower.

4.5.2 Results

The results from this study were analysed in the same

fashion as those from the main study as can be seen in the

bottom half of Table 2. The major finding of the Main Study

holds in this experiment as well: both experimental groups

outperform the control group wrt. eectiveness. Thus, even

a very small amount of training (2 topics) is useful and aids

users in learning to formulate better queries.

In contrast to the Main Study, and unsurprising given the

lower amount of training, we observe a smaller dierence in

retrieval eectiveness across the test topics: 0.05 (G

),

exp high

0.054 (G ) and 0.024 (G ) respectively.

exp low

control

These results suggest that some form of learning is tak-

ing place and that the relative improvements are smaller if

less training is given. They also serve to further highlight

the unexpected finding that there is little dierence between

G

and G

.

exp high

exp low

5. SUMMARY AND DISCUSSION

Our findings vs. our research hypotheses.

Hypothesis H1 has been shown to hold - users are in-

deed able to adapt their search behaviour to an unfamiliar

search system. While G

(which received no training)

control

does not adapt, we clearly see significant changes in query-

ing behaviour in both experimental groups (i.e. those who

received training).

Our pilot study served to confirm hypothesis H2; the

study participants were indeed able to determine a set of

characteristics that well-performing queries contain. Recog-

nising such characteristics is a necessary requirement for

learning how to create better queries in general and not just

for specific topics.

The main study and the follow-up focusing on the train-

ing set size provide evidence for hypothesis H3. The two

experimental groups outperform G

significantly, both

control

when being shown two and four training topics respectively.

Thus, even a very small set of training topics is su cient to

improve users' ability to pose good queries.

Our results do not support H4. In terms of AP, although

Figure 5 hints that G

may have outperformed G

,

exp high

exp low

140

ID Information need

av. KNOW av. SUR av. QUAL Query suggestion examples

Identify positive accomplishments

303

2.64

of the Hubble telescope since it was

launched in 1991

Identify drugs used in the treat-

383

2.89

ment of mental illness.

What is the status of The Three

416

1.58

Gorges Project?

universe astronomer faint hubble in-

2.62

2.98

[

], [

frared galaxies universe hubble infrared

], [

stars universe hubble

]

antidepressant risk zoloft prozac zoloft

2.45

3.36

[

], [

studies prozac antidepressant eective

], [

zoloft prozac

]

coerdams damming generating 2009

3.09

2.60

[

],

dam corporation phase 2009 2009 river

[

], [

construction

]

Table 4: Examples of Robust track search tasks and the generated high-quality query suggestions. Columns 3-5 contain user rating data from our study on user perceptions of queries. Column 3 (KNOW) contains the average knowledge rating of the information need across all users of the study. Columns 4 and 5 contain the average rating users assigned to all query suggestions of the topic with respect to the surprise (SUR) factor and the estimated result quality (QUAL).

the dierence is not significant. There were some features of the queries that were statistically distinguishable between these groups, but we feel that the evidence is not strong enough to claim that H4 holds.
Finally, based on the evidence in Figures 6 and 7, we have to reject H5 - our participants in both experimental groups had a comparable learning rate (though with dierent absolute performance scores).
Our findings vs. prior work.
Previous work has presented mixed evidence for people's ability to accurately determine which query terms will have utility. Our findings suggest this is a complex behaviour. Although participants were able to identify positive characteristics of queries shown to be eective (Section 4.3.2), many high-performing queries were not predicted to be such (Section 4.2.2). Perhaps these potentially contradictory findings indicate a potential systems bias, i.e. do users implicitly trust suggestions presented by the system as good? Is it only when doubt is introduced by explicitly questioning users about the queries that they perceive suggestions to be potentially not of good quality? What does this mean for the learning eect? This line of thought opens up many fascinating questions of how query suggestions are presented.
Our work has added to the small base of literature demonstrating means for users to learn how to provide higherquality queries. One limitation of our work has to do with the time period of learning. Our findings support the claim that being shown good suggestions can lead to users learning how to produce better queries, however this is only demonstrated over the period of a session i.e. the test groups achieved better performance for later queries and for later topics. Ideally, however, what we want to show is learning over longer periods of time, such as weeks [28] and months [6] as previous studies have done. This requires a dierent mode of evaluation as crowd-sourcing is not suited to such tasks and represents an important next stage in our project.
A further limitation, with respect to how our findings may be used, is that in a real-life scenario a search system would normally not have access to relevance judgements. This means our method of creating queries cannot typically be applied. We argue that there are situations, though, that may be ideally suited to such an approach. For example in web search we have implicit indicators for di cult tasks (i.e. where better queries might be required) [20] and we also have good models for determining search success based

on user behaviour [19]. When such instances combine (i.e. when users are successful in tasks they have been struggling with), this might be the perfect time to present a query suggestion, perhaps along the line of "The following query would get this page further up the ranking". Another potential use-case might be to present examples when a user switches context or to a new search-engine where new strategies are required. It has been suggested that users tend not to vary their strategy [29] and our approach might help encourage more diverse or tailored behaviour.
6. CONCLUSIONS
The set of user studies described in this paper have demonstrated that it is possible to use high-quality query examples to influence the queries users submit themselves. We have shown that users can recognise and abstract positive qualities of good queries. Users change the properties of the queries they submit and achieve better retrieval performance after seeing good examples for other tasks. Our findings open up a range of interesting questions relating to how query examples should be presented and how this affects learning and the influence of learning duration, i.e. is user behaviour influenced over the longer term? Finally is domain knowledge an important factor? We hope to address these issues in upcoming work.
7. REFERENCES
[1] P. Anick, Using terminological feedback for web search refinement: a log-based study, SIGIR '03, ACM, 2003, pp. 88­95.
[2] A. Arampatzis and J. Kamps, A study of query length, SIGIR '08, ACM, 2008, pp. 811­812.
[3] A. Aula, N. Jhaveri, and M. K¨aki, Information search and re-access strategies of experienced web users, WWW '05, ACM, 2005, pp. 583­592.
[4] L. Azzopardi, M. De Rijke, and K. Balog, Building simulated queries for known-item topics: an analysis using six european languages, SIGIR '07, ACM, 2007, pp. 455­462.
[5] H. Bast and I. Weber, Type less, find more: fast autocompletion search with a succinct index, SIGIR '06, ACM, 2006, pp. 364­371.
[6] S. Bateman, J. Teevan, and R.W. White, The search dashboard: how reflection and comparison impact search behavior, SIGCHI, ACM, 2012, pp. 1785­1794.

141

[7] N.J. Belkin, Helping people find what they don't know, Commun. ACM 43 (2000), no. 8, 58­61.
[8] N.J. Belkin, D. Kelly, G Kim, J-Y Kim, H-J Lee, G. Muresan, M-C Tang, X-J Yuan, and C. Cool, Query length in interactive information retrieval, SIGIR '03, ACM, 2003, pp. 205­212.
[9] J.L. Bennett, The user interface in interactive systems, Annual review of information science and technology 7 (1972), no. 159-196.
[10] W.B. Croft and R.H. Thompson, I3r: A new approach to the design of document retrieval systems, JASIST 38 (1987), no. 6, 389­404.
[11] Z. Dou, R. Song, and J. Wen, A large-scale evaluation and analysis of personalized search strategies, WWW '07, ACM, 2007, pp. 581­590.
[12] D. Elsweiler, Supporting human memory in personal information management, Ph.D. thesis, University of Strathclyde, 2007.
[13] B.M. Evans and E.H. Chi, An elaborated model of social search, IP&M 46 (2010), no. 6, 656­678.
[14] K. Franzen and J. Karlgren, Verbosity and interface design, SICS Research Report (2000).
[15] G.W. Furnas, T.K. Landauer, L.M. Gomez, and S.T. Dumais, The vocabulary problem in human-system communication, Communications of the ACM 30 (1987), no. 11, 964­971.
[16] J. Hackos and J. Redish, User and task analysis for interface design, John Wiley & Sons, Inc., 1998.
[17] D. Harman, Towards interactive query expansion, SIGIR '88, ACM, 1988, pp. 321­331.
[18] M. Harvey, F. Crestani, and M.J. Carman, Building user profiles from topic models for personalised search, CIKM '13, ACM, 2013, pp. 2309­2314.
[19] A. Hassan, R. Jones, and K. L. Klinkner, Beyond dcg: user behavior as a predictor of a successful search, WSDM, ACM, 2010, pp. 221­230.
[20] A. Hassan, R. W. White, S. T Dumais, and Y. Wang, Struggling or exploring?: disambiguating long search sessions, WSDM, ACM, 2014, pp. 53­62.
[21] H.A. Hembrooke, L.A. Granka, G.K. Gay, and E.D. Liddy, The eects of expertise and feedback on search term selection and subsequent learning, JASIST 56 (2005), no. 8, 861­871.
[22] I. Hsieh-yee, Eects of search experience and subject knowledge on the search tactics of novice and experienced searchers, JASIST 44 (1993), 161­174.
[23] B.J. Jansen, D.L. Booth, and A. Spink, Patterns of query reformulation during web searching, Journal of the American Society for Information Science and Technology 60 (2009), no. 7, 1358­1371.
[24] D. Kelly and X. Fu, Eliciting better information need descriptions from users of information search systems, IP&M 43 (2007), no. 1, 30­46.
[25] J. Koenemann and N.J. Belkin, A case for interaction: a study of interactive information retrieval behavior and eectiveness, SIGCHI '96, ACM, 1996, pp. 205­212.
[26] W. Lucas and H. Topi, Training for web search: Will it get you in shape?, JASIST 55 (2004), no. 13, 1183­1198.

[27] D. McKay and G. Buchanan, Boxing clever: how searchers use and adapt to a one-box library search, OzCHIO~ 13, ACM, 2013, pp. 497­506.
[28] N. Moraveji, D. Russell, J. Bien, and D. Mease, Measuring improvement in user search performance resulting from optimal search tips, SIGIR '11, ACM, 2011, pp. 355­364.
[29] J. Nielsen, Incompetent research skills curb users' problem solving http://www.useit.com/alertbox/search-skills.html last accessed january, 2015, Alertbox (2011).
[30] H. Oinas-Kukkonen and M. Harjumaa, Towards deeper understanding of persuasion in software and information systems, ACHI '08, IEEE, 2008, pp. 200­205.
[31] S.E. Robertson, On term selection for query expansion, Journal of Documentation 46 (1990), no. 4, 359­364.
[32] K. Rodden, I. Ruthven, and R.W. White, Workshop on web information seeking and interaction, ACM SIGIR Forum, vol. 41, ACM, 2007, pp. 63­67.
[33] I. Ruthven, Re-examining the potential eectiveness of interactive query expansion.
[34] I. Ruthven and M. Lalmas, A survey on the use of relevance feedback for information access systems, The Knowledge Engineering Review 18 (2003), no. 02, 95­145.
[35] B. Shneiderman, Dynamic queries for visual information seeking, Software, IEEE 11 (1994), no. 6, 70­77.
[36] B. Smyth, E. Balfe, J. Freyne, P. Briggs, M. Coyle, and O. Boydell, Exploiting query repetition and regularity in an adaptive community-based web search engine, User Modeling and User-Adapted Interaction 14 (2004), no. 5, 383­423.
[37] E. Sormunen, A method for measuring wide range performance of boolean queries in full-text databases, Tampere University Press, 2000.
[38] R.S. Taylor, Question-negotiation and information seeking in libraries, College & Research Libraries 29 (1968), no. 3, 178­194.
[39] P. Vakkari, Changes in search tactics and relevance judgements when preparing a research proposal a summary of the findings of a longitudinal study, Information Retrieval 4 (2001), no. 3-4, 295­310.
[40] E.M. Voorhees, The trec 2005 robust track, SIGIR Forum 40 (2006), no. 1, 41­48.
[41] E.M. Voorhees, D.K. Harman, et al., Trec: Experiment and evaluation in information retrieval, vol. 63, MIT press Cambridge, 2005.
[42] R.W. White, S.T. Dumais, and J. Teevan, Characterizing the influence of domain expertise on web search behavior, WSDM '09, ACM, 2009, pp. 132­141.
[43] R.W. White and D. Morris, Investigating the querying and browsing behavior of advanced search engine users, SIGIR '07, ACM, 2007, pp. 255­262.
[44] P. Willett and I. Ruthven, Relevance behaviour in trec, Journal of Documentation 70 (2014), no. 6, 1098­1117.

142

How Many Results Per Page? A Study of SERP Size, Search Behavior and User Experience

Diane Kelly
School of Information and Library Science, University of North Carolina Chapel Hill, NC, USA
dianek@email.unc.edu

Leif Azzopardi
School of Computing Science, University of Glasgow
Glasgow, United Kingdom
leif.azzopardi@glasgow.ac.uk

ABSTRACT
The provision of "ten blue links" has emerged as the standard for the design of search engine result pages (SERPs). While numerous aspects of SERPs have been examined, little attention has been paid to the number of results displayed per page. This paper investigates the relationships among the number of results shown on a SERP, search behavior and user experience. We performed a laboratory experiment with 36 subjects, who were randomly assigned to use one of three search interfaces that varied according to the number of results per SERP (three, six or ten). We found subjects' click distributions differed significantly depending on SERP size. We also found those who interacted with three results per page viewed significantly more SERPs per query; interestingly, the number of SERPs they viewed per query corresponded to about 10 search results. Subjects who interacted with ten results per page viewed and saved significantly more documents. They also reported the greatest difficulty finding relevant documents, rated their skills the lowest and reported greater workload, even though these differences were not significant. This work shows that behavior changes with SERP size, such that more time is spent focused on earlier results when SERP size decreases.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval:Search Process; H.5.2 [Information Interfaces and Presentation]: User Interfaces:Screen Design
Keywords
Search behavior; user studies; search interface; search result page
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
SIGIR'15, August 09 - 13, 2015, Santiago, Chile Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00 DOI: http://dx.doi.org/10.1145/2766462.2767732.

1. INTRODUCTION
Over the years, the standard design for a search engine results pages (SERPs) has converged on a textual listing of ten results per page; this presentation style is often referred to as `ten blue links.' Despite many scholars and designers questioning this convention, this standard persists. While the literature contains many examples of alternatives to this design including variable snippet styles e.g., [31] and layout methods e.g., [15], few works have questioned or considered how the number of results presented per page (SERP Size), impacts search behavior and if a different size might lead to a better user experience. Although many experimental search interfaces, in particular pre-Web interfaces, have presented more than ten results per page, and many current search interfaces allow users to configure the number of results per page, to our knowledge, the size of the SERP has only been systematically investigated in a few studies [19, 23, 25].
Understanding how SERP size influences user behavior is important for several reasons. First, users are now accessing search results via a multitude of devices, including desktop computers, mobile phones, tablets, smart watches and smart glasses. Each device displays varying numbers of results above-the-fold and research has shown that this can impact click behavior [12, 18]. Thus understanding how the behaviors and performance of users changes with respect to SERP size can potentially help guide and inform researchers and practitioners who are interested in designing effective interfaces. Second, many IR evaluations measures [11, 22, 28] make assumptions about the number of results presented to the user and how the user will evaluate these results; specifically, that the user examines results linearly and the likelihood of the user examining a result decreases exponentially with rank. This underlying user model might be insufficient if variable SERP sizes are considered. Finally, little is known about how SERP size impacts the user experience; it might be the case that presenting fewer results per page has positive psychological consequences as found by Oulasvirta et al. [23] who examined user preferences for mocked-up search result sets of varying lengths. On the other hand, fewer results may lead to increased browsing costs and increased mental load as users need to flip between result pages detracting from their search experience.
Thus, the purpose of this paper is to investigate the impact of SERP size on search behavior and user experiences. To do so, we create three search interfaces that vary according to the number of results per SERP (three, six or ten) and conduct a between-subjects laboratory experiment.

183

2. BACKGROUND
Numerous aspects of search engine result pages (SERPs) have been investigated including layout (e.g., ranked list, grid), ordering (ranking of results), components (e.g., aggregated search pages, query suggestion), length and content of snippet summaries (e.g., query-biased), snippet style (e.g., thumbnails) and SERP size. Below we provide a brief overview of some of the research focused on SERP design and SERP size. We also present research on screen size and position effects, which are relevant to this work.
2.1 SERP Design
A range of experiments have been conducted investigating different snippet types, sizes and features, as well as layouts and result combinations. Perhaps the most researched aspect of SERP design has been on the potential advantages of visual snippets (e.g., thumbnails) over textual snippets [2, 13, 24, 31, 34]. For example, Woodruff et al. [34] examined the usefulness of enhanced thumbnail images to represent webpages. The enhanced thumbnail representation magnified salient features of webpages so that searchers could more readily identify whether a page contained relevant material. In comparison with textual summaries and standard thumbnails, it was found that the enhanced thumbnails reduced search time. In more recent work, Teevan et al. [31] explored a variation on generating enhanced thumbnails, referred to as visual snippets, and also found that such an approach decreased the time required to identify relevant documents. TileBars [10] is another example of how researchers have used visual information on the results page to help users make better decisions about the relevance of the result represented by the snippet.
There have also been many studies on the content and size of snippets [6, 8, 32]. For example, Tombros and Sanderson [32] investigated the potential usefulness of query-biased summaries for snippets and found they improved both user accuracy and speed of relevance judgements. Clarke et al. [6] studied different aspects of result snippets which users found attractive and desirable as determined by the number of clicks the snippet received. Clarke et al.'s [6] study showed that well formed snippets with title, summary and URLs that were more readable, contained query terms and query phrases were seen as more desirable.
Cutrell and Guan [8] explored the effect of different snippet lengths (short: 1 line, medium: 2-3 lines and long: 6-7 lines) using eye tracking data. For navigational queries, they found that users examined more results as snippet length increased, but performed slightly worse. They attributed the differences to snippets being a distractor; specifically, as snippets grew longer, users would often ignore the URL. For informational queries, users performed somewhat better as snippet length increased and looked at more results when snippets were medium length.
Alternatives to the linear result list have also been explored, in particular, grid-based layouts [5, 15, 26]. For example, Krammerer and Beinhauer [15] examined differences in user behavior when interacting with a standard list interface (single column of title, URL and snippet stacked vertically), a tabular interface (title, snippet and URL stacked horizontally in 3 columns for each search result) and a grid layout (search results placed in 3 columns). They found users of the grid layout scrutinized the snippets more, and

speculated the interface had promise in overcoming the position bias observed in [12] (see Subsection 2.4).
Finally, researchers investigating aggregated search have studied SERP design [29, 1, 36]. Both Sushmita et al. [29] and Arguello et al. [1] studied the differences between blended and tabbed search result pages. Even these more complicated SERP pages, which blend results from multiple sources, still tend to present about 10 results per page, primarily in list format. However, for specific verticals such as images, the number of results shown per page is often larger and grid formats are more common [20].
2.2 SERP Size
With regard to SERP size, or the number of results presented per page, there has been very little systematic research [23, 25, 19]. Although many experimental search interfaces, in particular pre-Web interfaces, have presented more than ten results per page, and many current search interfaces allow users to configure the number of results per page, to our knowledge, the size of the SERP has only been explicitly studied as a variable in a few of studies. For example, Oulasvirta et al. [23] explored the idea that showing fewer results may be preferable to showing more. In their experiment, they showed participants result lists with 6 results or 24 results, as a way to investigate the so called, "paradox of choice." They found that participants rated their satisfaction, carefulness and confidence higher for the six results per page condition. However, the study was performed using paper based mock-ups of screens, where there was no pagination (i.e., 6 or 24 results were shown on one page), so it is unclear how these findings generalize to on-screen desktop conditions, where typically only about 6-7 results are viewed at one time. Also, it is not clear whether the effect would be significantly different between 6 results per page, and the standard 10 results per page, and how the results would differ in an interactive search context.
In an industry report from Google, Linden [19] stated feedback from Google users indicated they desired more than ten results per page. However, it was observed that when the number of results were increased to 30 per page, traffic dropped by 20%. The drop in traffic was attributed to the increase in time to dispatch the page, since a page with 10 results took approximately 0.4 seconds to generate, while a page with 30 results took approximately 0.9 seconds. This report is nearly ten years old so it is unclear if the results regarding user preference are still applicable to today's searchers and search environments.
2.3 Screen Sizes
While few studies have explicitly examined the impact of SERP size on user interaction, this has been indirectly examined in the context of screen size, since screen size impacts the number of results that are viewable. Jones et al. [14] investigated the differences in search performance when searchers used WAP based mobile phones which displayed 5 title snippets per page, PDA devices with 5 snippets per page and desktop search with 10 snippets per page. They found participants using the devices showing 5 results per page would on average examine 2-3 pages of results.
A similar experiment using more modern mobile phones and PDAs was conducted by Sweeney and Crestani [30], but the focus was on what length snippet to present to searchers on the different devices. They note that because transmis-

184

sion costs were high (at the time), SERPs often contained fewer results per page, thus increasing page to page navigation costs [14]. They varied the length of the snippet summary, but kept the number of results per page on each device constant and set to 10. They hypothesized participants using the larger screen would examine more documents, but found participants examined approximately the same number of documents regardless of the condition.
Jaewon et al. [18] compared the differences in people's interaction styles with two screen sizes (desktop vs mobile) where SERP size was set to 10; on one interface all ten could be seen, while on the other, only 3 could be seen. This latter condition simulated the screen size of a mobile phone. Jaewon et al. [18] found for both informational and navigational tasks, participants using the small screen took longer to click on a link and had longer task completion times (though only the task completion times were significantly different). They also found participants scanned slightly fewer documents on the small screen, but scrutinized each link for a longer duration, especially the first three links. On the smaller screen, links 1-3 attracted more clicks for both informational and navigational tasks: 88% and 91% versus 83% and 90%, respectively. This suggests when interacting with restricted screen space, searchers focus more on the visible set of results and make more clicks on these results.

question is if these findings generalize to situations where the screen and page sizes vary.

2.4 Position Bias and Click-through Rates
How users interact with the standard baseline SERP has been informed by studies examining where users click on the SERP [4, 7, 12, 16, 35]. In one of the first studies of click bias, Joachims et al. [12] found users inspected 70% down to 20% of result snippets in the top 6 (above the fold), and then 10% down to 5% of results snippets at positions 7-10 (below-the-fold). Essentially, the probability of a click decayed exponentially, while the probability of examining a snippet decreased linearly (above-the-fold, and the flattened out). Similar findings have been reported by others [4, 16, 21]. Joachims et al. [12] wondered whether the distribution of clicks could be used as an absolute indicator of relevance. However, they pointed out two problems in interpreting click data stemming from (i) trust bias, where users trust the search engine to deliver the most relevant item first, i.e., following the probability ranking principle [27], and (ii) quality bias, where the behavior depends on the quality of the retrieval system. They concluded users are more likely to click on highly ranked documents and that quality influences click behavior, such that if the relevance of the items retrieved decreases, users click on items that are less relevant, on average.
Craswell et al. [7] noted this as a fundamental problem with click data, and referred to the problem as position bias, where in the top 10 result lists, the probability of a click decreases with the rank of the document. They put forward several hypotheses for why position bias is observed. From their analysis, they showed a cascade model, which assumed the user would click a result snippet with some probability (ps) or skip it (1 - ps), fit click behavior at earlier ranks, and a baseline model, which assumed the probability a user would click a result snippet was proportional to its relevance, fitted click behavior at later ranks. Of note is that the click distributions that have been observed are based on interaction with SERPs that display ten results per page. An open

Figure 1: A standard search interface that has a query box and search button, houses n results per page along with pagination buttons. The number of results returned is also shown.
3. METHOD
We conducted a between-subject experiment where subjects were randomly assigned to one of three interfaces that differed according to how many results were displayed per search result page. Figure 1 displays the basic design of the interfaces. One interface functioned as the baseline and displayed 10 results per page (10RPP). The other two interfaces displayed 3 and 6 results per page (3RPP and 6RPP, respectively). For the 3RPP and 6RPP interfaces, all search results were displayed above-the-fold (i.e., no scrolling was required). For the interface displaying 10RPP, the first six results were visible above-the-fold. Our decision to use 6RPP was to make our work more comparable to past research. Specifically, to Oulasvirta et al.'s [23] study where 6RPP were evaluated through a simulation and to Joachims et al. [12] who described click distributions for results abovethe-fold (those at ranks 1-6). Although we restrict our investigation to desktop search, we choose 3RPP as another comparison point similar to Jaewon et al. [18].
As shown in Figure 1 all three interfaces had a query box at the top and a search button. Below the search button was information displaying the number of retrieved results and the result surrogates. The surrogates displayed the title of the result, metadata about the result and a short summary. At the bottom of the SERP were buttons that allowed subjects to move forward and backward through the set of result pages. All subjects used the same desktop computer when

185

Factors Mental Demand Physical Demand Temporal Demand Effort
Performance
Frustration Level

System How mentally demanding was it to use this system to complete the search tasks? How physically demanding was it to use this system to complete the search tasks? How hurried or rushed did you feel when using this system to complete the search tasks? How hard did you have to work to accomplish your level of performance with this system? How successful were you using this system to complete the search tasks? How insecure, discouraged, etc. were you while using this system?

Navigate Results navigate through the search results? navigate through the search results? navigating through the search results? navigate through the search results? was your navigation through the search results? navigating through the search results?

Assess Results assess and judge documents for relevance? assess and judge documents for relevance? assessing and judging documents for relevance? assess and judge documents for relevance? were you at assessing and judging documents for relevance? assessing and judging documents for relevance?

Table 1: Modified NASA TLX factor definitions for overall system load, navigation load and assessment load.

completing the study, which was in our laboratory and under our control. Subjects used a 19 inch monitor and all aspects of the display, including font size and resolution, were held constant during the study.
3.1 Corpus, Search Topics and System
The Aquaint TREC test collection of over one million newspaper articles was used [33]. We selected three search topics from this collection: 344 (Abuses of E-mail); 347 (Wildlife Extinction) and 435 (Curbing Population Growth). We selected topics that had some contemporary relevance, we thought would be of interest to our target subjects and had a similar number of relevant documents available (123, 165 and 152, respectively). Our selection was also based on evidence from previous user studies with a similar system setup [17] where it was shown that the difficulty of these topics did not significantly differ. Subjects searched all three topics and topics were rotated with a Latin-square.
To situate the search tasks, subjects were instructed to imagine they were newspaper reporters and needed to gather documents to write stories about the provided topics. Subjects were told that there were over 100 relevant documents in the collection for each topic and they should try to find as many of these as possible during the allotted time (10 minutes per topic). While recall-oriented searches are not as common in the general Web population as searches where users are looking for one or a small number of items, these types of searches are still performed by many people, in particular, professionals such as newspaper reporters, patent searchers and business analysts. Because we were interested in examining traditional performance measures, we choose to use a test collection. Since the collection contained newspaper articles, we created a work task situation representing one type of user model (i.e., journalist) and one type of task model (i.e., find as many relevant documents as you can to write a story) that was appropriate given the collection and to which we thought our target participants (i.e., undergraduate students) could relate.
The Whoosh IR Toolkit was used as the core of the retrieval system, with BM25 as the retrieval algorithm, using standard parameters, but with an implicit ANDing of query terms to restrict the set of retrieved documents to only those that contain all the query terms (similar to BM25A used in [3]). This was chosen because most systems, such as web search engines and library catalog systems, implicitly AND terms together. However, subjects could also explicitly use OR, AND, or NOT in their queries. Subjects were not provided with a tutorial of the system.

3.2 Search Behaviors
Search behavior was operationalized with three types of measures: (1) interaction; (2) performance and (3) time spent doing different search activities. All of these measures were computed from log data and TREC q-rels. Interaction measures included: number of queries issued, number of SERPs viewed per query, number of documents viewed, number of documents viewed per query and deepest SERP click. Performance measures included: number of documents marked relevant, number of TREC-relevant documents marked relevant and precision at 3, 5 and 10 documents. Time-based measures included: time spent issuing queries, time spent examining SERPs per query and time spent viewing documents.
3.3 User Experience
To capture user experience, subjects evaluated the search tasks before each search, the system after each search and their experienced workload after completing all search tasks.
Task evaluations were elicited via pre-search questionnaire, which contained five items: (1) How much do you know about this topic? (1=nothing; 5=I know details); (2) How relevant is this topic to your life? (1=not at all; 5=very much); (3) How interested are you to learn more about this topic? (1=not at all; 5=very much); (4) How often have you searched for information related to this topic? (1=never; 5=very often) and (5) How difficult do you think it will be to search for information about this topic? (1=very easy; 5=very difficult).
System evaluations were elicited via post-search questionnaire, which also contained five items: (1) How difficult was it to find relevant documents? (1=very easy; 5=very difficult); (2) How would you rate your skill and ability at finding relevant documents? (1=not good; 5=very good); (3) How would you rate the system's abilities at retrieving relevant documents? (1=not good; 5=very good); (4) How successful was your search? (1=unsuccessful; 5=successful) and (5) How many of the relevant documents do you think you found? (1=a few of them; 5=all of them).
Experienced workload was elicited using a modified version of the NASA Task Load Index (TLX). This instrument elicited ratings of the following: Mental Demand, Physical Demand, Temporal Demand, Performance, Effort and Frustration (Table 1). We modified the TLX statements so they matched the target task (search) and asked subjects to make their evaluations with a 7-point scale. Subjects also completed two other workload questionnaires, which were fo-

186

Queries SERP views
Doc views Docs/query SERP depth

Interface 3RPP 10.86 (8.89)
3.56 (2.35) 20.06 (9.09)
3.67 (3.44) 7.43 (7.34)

6RPP 10.53 ( 6.04)
2.66 ( 3.52) 15.67 ( 7.12)
3.36 ( 5.91) 10.40 (22.24)

10RPP 9.58 ( 6.89) 1.92 ( 0.98)
24.64 (13.25) 5.66 ( 7.08)
10.23 ( 8.78)

F 0.38 4.24* 8.18** 1.95 0.54

Topic 344 15.44 (8.44)
1.80 (0.86) 14.92 (7.63)
1.67 (2.02) 3.84 (3.72)

347 7.64 ( 5.85) 3.79 ( 3.75)
25.11 (13.07) 6.53 ( 6.70)
15.76 (22.14)

435 7.89 (4.24) 2.55 (1.83)
20.33 (8.23) 4.50 (6.24) 8.45 (7.26)

F 16.78** 6.34* 10.58** 7.42** 7.06**

Table 2: Mean (sd) search interactions & F-statistics for interface & topic (df=2, 107) *p<0.01; **p<0.001

Marked rel TREC rel P@3 P@5 P@10

Interface 3RPP 11.08 (8.27) 4.25 (4.00) 0.184 (0.180) 0.160 (0.155) 0.140 (0.130)

6RPP 8.28 (5.88) 3.92 (3.21) 0.200 (0.161) 0.156 (0.129) 0.142 (0.128)

10RPP 16.08 (12.05) 6.17 (4.48) 0.216 (0.198) 0.194 (0.143) 0.163 (0.116)

F 7.70** 3.45 0.28 0.80 0.41

Topic 344 7.44 (6.25) 3.67 (4.23) 0.148 (0.173) 0.112 (0.126) 0.095 (0.108)

347 16.03 (12.12) 5.14 (4.38) 0.220 (0.193) 0.193 (0.168) 0.156 (0.140)

435 11.97 (7.51) 5.53 (3.23) 0.234 (0.164) 0.206 (0.113) 0.193 (1.03)

F 9.08** 2.25 2.36 4.87* 6.15*

Table 3: Mean (sd) performance & F-statistics for to interface & topic (df=2, 107) *p<0.01; **p<0.001

Interface

Topic

Time 3RPP

6RPP

10RPP

F 344

347

435

F

Query 12.21 (10.72) 15.25 (1.24) 12.93 (1.08) 2.35 13.74 (1.08) 14.01 (1.08)

12.64 (1.08) 0.49

SERP 40.20 (27.17) 44.28 (57.19) 56.77 (72.92) 0.88 33.11 (55.79) 60.86 (59.64) 47.29 (49.14) 2.27

Page 300.97 (117.69) 259.81 (73.01) 261.42 (97.91) 2.10 236.22 (97.94) 291.31 (100.21) 294.67 (89.65) 4.16

Table 4: Mean (sd) time and F-statistics according to interface and topic (df=2, 107). Query Time is the time per query, SERP time is the time spent on the SERP per query, and Page time is the total amount of time spent viewing and assessing documents across the session.

cused on the workloads associated with navigating the search results and assessing the relevance of results. We included these two additional scales to isolate any difficulties arising from the varying number of results shown per page.
3.4 Subjects
Subjects were recruited by sending an email solicitation to undergraduate students at a large research university. Thirty-six undergraduate students participated (12 per interface). Twenty-six subjects were female and ten were male. Their average age was 20.4 (SD=2.6). Sixty-seven percent were science majors, 14% were social science majors and 19% were humanities majors. Each subject was compensated with $15 USD and could earn an extra $5 USD per topic for being one of the top three performers in his/her interface condition.
Subjects' search experience was measured using a modified version of the Search Self-Efficacy scale [9]. This instrument contains 14-items describing different search-related activities. Subjects respond to each item by indicating their confidence in completing each activity on a 10-point scale (1=totally unconfident; 10=totally confident). Reliability analysis of these items demonstrated a high Cronbach's alpha (0.934), so responses were averaged. Overall, subjects reported fairly high search self-efficacy (M=7.51, SD=0.97).
4. RESULTS
Both the search behavior and user experience measures were analyzed according to interface and topic. To evaluate these data, ANOVAs were conducted using interface and topic as factors; both main effects and interaction effects were examined with alpha=0.01. Bonferroni tests were used for post-hoc analysis.

4.1 Search Behaviors
Interactions. Table 2 displays the average number of queries submitted by subjects, SERP pages viewed per query, total number of documents viewed, documents viewed per query, and average SERP depth of the last document viewed per query. Participants entered similar numbers of queries in each interface, but they viewed significantly different numbers of SERP pages and documents. Specifically, those who interacted with 3RPP viewed significantly more SERPs than those who interacted with 10RPP and those who interacted with 6RPP viewed significantly less documents than those who interacted with 10RPP. Interestingly, the average number of SERPs viewed per query for those interacting with 3RPP (3.56) corresponded to about 10 results. Table 2 also displays search interactions according to topic. Significant differences were detected for all measures; in all cases the means for Topic 344 significantly differed from those for Topic 347, as well as from Topic 435 for queries and documents viewed. There were no significant interaction effects between interface and topic.
Performance. Table 3 displays the number of documents subjects marked relevant, the number of documents marked relevant that were also TREC relevant and several precision scores that were calculated using the TREC-relevant documents. Subjects who interacted with 10RPP performed the best according to all performance measures, but only significantly so with respect to number marked relevant. Subjects' performances differed significantly according to topic for three measures: number marked relevant, P@5 and P@10. Subjects marked significantly fewer documents as relevant when completing Topic 344 than Topic 347. Their P@5 scores were significantly worse for Topic 344 than the other two topics and their P@10 scores were significantly

187

worse for Topic 344 than Topic 435. There were no significant interaction effects between interface and topic.
Time-Based Measures. Table 4 displays the amount of time subjects spent performing different search activities: issuing queries, viewing SERP pages per query and viewing documents. Subjects who interacted with 3RPP spent the most time viewing documents, those who interacted with 6RPP spent the most time issuing queries and those who interacted with 10RPP spent the most time on SERPs per query, none of these differences were statistically significant. There were also no significant differences in the amount of time spent engaged in different search activities according to topic. Finally, there were no significant interaction effects between interface and topic.

3RPP 0.4

0.3

0.2

0.1

0

5

10

15

20

25

30

6RPP 0.4

0.3

0.2

0.1

0

5

10

15

20

25

30

10RPP 0.4

0.3

0.2

0.1

0

5

10

15

20

25

30

Probability of a Click at Rank i

Figure 2: The probability of clicking on a snippet (yaxis) for each rank i (x - axis). Top: 3RPP, Middle: 6RPP and Bottom: 10 RPP. The distributions for 3RPP and 6RPP were significantly different to the distribution on the 10RPP interface.
Click Distributions. Figure 2 shows the distribution of clicks on each of the three interfaces (3RPP, 6RPP and

10RPP, Top, Middle and Bottom, respectively). The graphs show the average probability of a user clicking on a result snippet at rank position i. Subjects who interacted with 3RPP mainly clicked on the first 3 results (0.41 down to 0.33). A noticeable drop in the probability of a click is observed when subjects went to page 2. Interestingly, the probability stays around 0.22-0.23 for results at ranks 4-6, before another step down in the probability of a click on documents at ranks 7-12. After result 12, i.e., page 5 and onwards, the probability becomes increasingly smaller. Subjects who interacted with 6RPP appeared to be less likely to click on documents overall, but again we observe a small step change after the first page (i.e., 6-12), and again after the second page (i.e., 13-18). In contrast, subjects interacting with 10RPP, clicked with higher probability across most of the ranks, and even after the first page of results (i.e., 1120), there is no noticeable drop in click probability. To determine if the click distributions were drawn from the same underlying distribution a two-sample Kolmogorov-Smirnov test was used. For 3RPP and 6RPP the distributions were not significantly different (p=0.3420, k=0.2333), indicating that they were drawn from the same underlying distribution. However, the click distributions for 3RPP and 6RPP were found to be significantly different from 10RPP (p=0.0046, k=0.4333, and p=0.0006, k=0.5, respectively).
4.2 User Experience
Task Evaluations. Table 5 displays subjects' task evaluations. With the exception of interest in the topic, subjects' responses to these items differed significantly according to topic. Subjects indicated they had significantly more topic knowledge about Topic 435 (Curbing Population Growth) than Topics 344 (Abuses of E-mail) and 347 (Wildlife Extinction), F(2, 107)=6.53, p=0.002 and had searched more often in the past for information about this topic, F(2,107)= 13.44, p<0.0001. They expected it to be significantly more difficult to search for information about Topic 344, than the other two topics, F(2, 107)=8.79, p=0.0003. An analysis was also conducted to see if subjects' responses differed according to interface condition. Since subjects were randomly assigned to condition, we did not expect differences and none were found. There were also no significant interaction effects.

Topic ID knowledge* relevance interesting searched** difficulty**

344 2.17 (1.03) 2.92 (1.27) 2.86 (1.10) 1.31 (0.67) 3.58 (0.77)

347 2.00 (0.93) 2.14 (0.99) 3.14 (0.93) 1.56 (0.74) 2.94 (0.75)

435 2.81 (1.04) 2.61 (1.15) 3.44 (1.05) 2.31 (1.09) 2.94 (0.72)

Table 5: Mean (sd) responses to task evaluation items. *p<0.01; **p<0.0001

System Evaluations. Table 6 displays subjects' system evaluations. For all items, subjects' responses to the items for Topic 344 were significantly different than their responses to the items for the other two topics (note: one subject skipped this questionnaire for Topic 344). This topic was rated as significantly more difficult, F(2, 106)=20.01, p<0.0001, and subjects rated their own skill at finding rele-

188

Topic ID difficulty*
skill* system* success* number found*

344 4.03 (0.92) 2.43 (0.95) 2.40 (1.14) 2.46 (1.12) 2.09 (1.04)

347 2.50 (0.97) 3.42 (0.81) 3.56 (0.81) 3.64 (0.68) 3.00 (0.89)

435 3.03 (1.18) 3.08 (0.77) 3.33 (0.76) 3.39 (0.87) 2.67 (0.79)

Table 6: Mean (sd) responses to system evaluation items according to topic. *p<0.0001

Topic ID difficulty
skill system success number found

3RPP 2.92 (1.25) 3.08 (0.98) 3.25 (0.91) 3.33 (1.04) 2.69 (1.04)

6RPP 3.14 (1.24) 3.03 (0.89) 3.03 (1.20) 2.91 (1.04) 2.51 (1.01)

10RPP 3.47 (1.08) 2.83 (0.94) 3.03 (1.00) 3.25 (1.00) 2.56 (0.91)

Table 7: Mean (sd) responses to system evaluation items according to interface.

vant documents significantly lower, F(2,106)=12.56, p<0.0001, as well as the system's ability at retrieving relevant documents, F(2,106)=12.29, p<0.0001. They also felt they were less successful, F(2,106)=13.71, p<0.0001, and found fewer of the relevant documents, F(2,106)=7.58, p<0.0002.
An analysis was also conducted to see if subjects' responses differed according to interface condition (Table 7). Interestingly, subjects who interacted with 10RPP reported the greatest difficulty finding relevant documents and rated their skills lowest; while those who interacted with 3RPP reported the least difficulty. Those who interacted with 6RPP reported less successful searches. However, no significant differences were found according to interface condition. There were also no significant interaction effects between condition and topic. Taken together, these results suggest subjects had a different search experience with Topic 344 regardless of interface condition, and interface condition did not uniformly impact their system evaluations.
Workload. Figure 3 displays the system, navigation and assessment workloads reported by subjects. With respect to system load, subjects who interacted with 10RPP reported the highest values for all items, with the exception of physical demand. The difference between 10RPP and 3 and 6 RPPs is especially pronounced for mental demand. The same general trends were observed for the navigation and assessment loads, with the exception of performance. Despite these differences, no significant differences were detected.
5. DISCUSSION
This purpose of this paper was to investigate the impact of SERP size on search behavior and user experience. We found that subjects who interacted with 3RPP viewed significantly more SERPs per query, which is perhaps to be expected, as subjects needed to paginate more to see more results. Interestingly, the average number of SERPs they examined per query corresponded to about ten results, so perhaps people have been conditioned to view ten results, even if viewing ten results requires expending a bit more effort through pagination. This finding is consistent with

7 3RPP 6RPP 10RPP
6
5
4
3
2
1
0 mental physical temporal performance effort frustration overall
7
6
5
4
3
2
1
0 mental physical temporal performance effort frustration overall 7
6
5
4
3
2
1
0 mental physical temporal performance effort frustration overall
Figure 3: Top: System Load, Middle: Navigation Load, Bottom: Assessment Load
the findings of Jones et al. [14] who found that participants who used a device that displayed five results per page examined about 2-3 pages of results. However, in our study, those who interacted with 6RPP and 10RPP viewed 2.66 and 1.92 SERPs per query, respectively, which corresponds to about 20 results per query, so there might be an interaction between the number of results subjects desire to see (whether consciously or subconsciously) and the effort required for pagination.
Subjects who interacted with 3RPP examined fewer documents, but seemed to compensate by spending more time on each document they examined. This is interesting because it suggests that the additional cost and effort of paging through results led to deeper inspection of the documents, which is consistent with Jaewon et al.'s [18] results. From the click distributions, it is interesting to note that the results on the first page were treated more or less equally, but for each subsequent page, fewer and fewer subjects visited them. It might be that with smaller sets of results, subjects treat them more independently or treat them as a batch. Subjects who interacted with 6RPP examined significantly fewer documents, but spent more time formulating queries (although not significantly so). This suggests that subjects might have seen enough results to determine their queries were unsuccessful, and thus spent more time querying. It is interesting to consider how the SERP functions as feedback to the user about the quality of his or her query and how the provision of a smaller SERP size might disrupt or change this function.
We found those who interacted with 10RPP viewed significantly more documents than those who interacted with

189

6RPP. This is in contrast to Sweeney and Crestani's [30] findings where there was no difference in the number of documents examined according to the number of results viewable on the device. Subjects who interacted with 10RPP also marked significantly more documents as relevant. This might be because they had cheaper access to more documents compared to subjects in the other conditions. Interestingly though, there were no significant differences in the amount of time subjects in the different conditions spent examining SERP pages or documents. Given that each SERP in the 10RPP condition displayed more snippets, one would have expected this difference to be significant. Seeing more results per page might have allowed those in the 10RPP to more quickly identify or distinguish relevant documents from non-relevant documents, or since there were no significant differences in the number of TREC relevant saved, they might have been more liberal with their assessments. Those who interacted with 3RPP likely faced an additional memory burden of having to internally keep track of what they saw on different pages and make comparisons of the information, while those who interacted with 10RPP likely had fewer things to keep track of internally.
A key finding from this research is that the distribution of clicks changes with SERP size. Specifically, the click distributions for those interacting with 3RPP and 6RPP differed significantly from those who interacted with 10RPP. Essentially these participants gave more time and attention to the top results, which may have been a way to reduce the potential memory load described above. When the SERP size was small, fewer snippets were inspected and more time was spent on the top documents. These findings suggest that precision-oriented search algorithms will be of increasing importance as the page size decreases. While we did not test interfaces that display one or two results per page, our findings suggest that users are likely to be even more focused on the top documents in these situations. Further work is need to examine how screen size (i.e. the number of results viewable) and the page size (the number of results per page) interact with one another and how this is related to the user's task and search goals.
Results regarding user experience were not as revealing, but some interesting, although not significant, trends were observed. Most notably, subjects who interacted with 10RPP reported the greatest difficulty finding relevant documents, rated their skills the lowest and reported greater workload. When considered in light of Oulasvirta et al.'s [23] work these findings suggest that smaller SERP sizes might have positive psychological consequences for users since results are chunked in smaller sets. Oulasvirta et al. [23] work was motivated by the idea of the "paradox of choice" and participants in their study rated their satisfaction, carefulness and confidence higher when interacting with 6 results as opposed to 10. The potential psychological benefits of alternative SERP sizes is an interesting avenue for future research.
Although we tried to select search topics that would not differ according to difficulty, we found that one topic (344) was much more difficult than the other two. Not only did subjects expect this topic to be more difficult, they also described it as more difficult after they searched. They exhibited significantly different search behaviors for this topic and performed significantly worse according to most measures. This topic was about abuses of email at the workplace. Our subjects were likely familiar with the general topic of email,

but since they were all undergraduates, their knowledge and familiarity with the workplace was likely impoverished which might explain why this topic was rated differently in the pre-search task evaluations. The differences in the system evaluations for this topic might be a result of the challenges of searching for information about email, which can be expressed in a variety of ways (e.g., email, e-mail, electronic mail). While we did not find a significant interaction effect between topic and interface condition, future studies might explore this more systematically with a greater range of topics from different points along the difficulty spectrum. The additional clicking required by the interface with 3RPP might intensify people's feelings of stress if they are experiencing difficulty. Alternatively, fewer results per page might help the person stay more focused and in control.
6. CONCLUSION
In this paper, we have studied how the number of results per page impacts search behavior and search experience in the context of ad-hoc topic retrieval. We created three search interfaces that varied according to the number of results per SERP (three, six or ten) and conducted a betweensubjects user-centered evaluation. Our major finding was that subjects' click distributions differed significantly depending on SERP size. Specifically, those interacting with 3RPP and 6RPP spent more time focused on top-ranked results and those interacting with 10RPP. This result has implications for evaluation. Many evaluation measures assume click distributions that neatly decay following an exponential distribution [11, 22, 28]. However, this might be an artifact of the SERP size modeled, and prompts a question as to whether such measures generalize well to SERPs of different sizes. Further work will need to be conducted to determine the impact of different SERP sizes on the distribution of clicks. This is of particular interest as the types of devices used for search are changing from desktop to mobile and wearables devices (such as tablets, mobiles, watches and glasses), where the number of results displayed and the number of results viewable vary.
Furthermore, while we have empirically approached the problem, it would be of interest to formally model how users interact with different screen and page configurations, in order to find optimal or ideal settings for these parameters. Creating a formal model would provide researchers and practitioners with a compact representation of how interaction costs, behaviors and performance are affected when SERP size and display size changes. Such an analytical tool would help in reasoning about the optimal SERP size for different devices and display sizes, as well as other aspects of search context such as type of task and user.
Finally, we noted that different SERP sizes appear to have different psychological effects which may lead to a positive or negative search experience. While none of our user experience measures were significant, the number of users in our study was quite low. Nor did we ask the right questions or study the right constructs to really get at this issue in detail, so further work on this front is also required.
7. ACKNOWLEDGEMENT
We would like to thank to Kathy Brennan from UNC for help with data collection.

190

8. REFERENCES
[1] J. Arguello, W.-C. Wu, D. Kelly, and A. Edwards. Task complexity, vertical display and user interaction in aggregated search. In Proceedings of the 35th International ACM SIGIR Conference, SIGIR '12, pages 435­444, 2012.
[2] A. Aula, R. M. Khan, Z. Guan, P. Fontes, and P. Hong. A comparison of visual and textual page previews in judging the helpfulness of web pages. In Proceedings of the 19th International WWW Conference, pages 51­60, 2010.
[3] L. Azzopardi, D. Kelly, and K. Brennan. How query cost affects search behavior. In Proc. of the 36th International ACM SIGIR Conference, pages 23­32, 2013.
[4] J. Bar-Ilan, K. Keenoy, M. Levene, and E. Yaari. Presentation bias is significant in determining user preference for search results: A user study. J. of the Am. Soc. for Info. Sci. and Tech., 60(1):135­149, 2009.
[5] F. Chierichetti, R. Kumar, and P. Raghavan. Optimizing two-dimensional search results presentation. In Proc. of the 4th Int. ACM WSDM Conference, pages 257­266, 2011.
[6] C. L. A. Clarke, E. Agichtein, S. Dumais, and R. W. White. The influence of caption features on clickthrough patterns in web search. In Proc. of the 30th Annual International ACM SIGIR Conference, pages 135­142, 2007.
[7] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM '08, pages 87­94, 2008.
[8] E. Cutrell and Z. Guan. What are you looking for?: an eye-tracking study of information usage in web search. In Proc. of the SIGCHI conference, pages 407­416, 2007.
[9] S. Debowski, R. Wood, and A. Bandura. The impact of guided exploration & enactive exploration on self-regulatory mechanisms & information acquisition through electronic enquiry. J. of Applied Psych., 86:1129­1141, 2001.
[10] M. A. Hearst. Tilebars: Visualization of term distribution information in full text information access. In Proceedings of the SIGCHI Conference, pages 59­66, 1995.
[11] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422­446, Oct. 2002.
[12] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In Proceedings of the 28th International ACM SIGIR Conference, pages 154­161, 2005.
[13] H. Joho and J. M. Jose. A comparative study of the effectiveness of search result presentation on the web. In Proceedings of the 28th European Conference on Information Retrieval, pages 302­313, 2006.
[14] M. Jones, G. Marsden, N. Mohd-Nasir, K. Boone, and G. Buchanan. Improving web interaction on small displays. In Proceedings of the Eighth International Conference on World Wide Web, WWW '99, pages

1129­1137, New York, NY, USA, 1999. Elsevier North-Holland, Inc.
[15] Y. Kammerer and P. Gerjets. How the interface design influences users' spontaneous trustworthiness evaluations of web search results: comparing a list and a grid interface. In Proc. of the Symp. on Eye-Tracking Research & Applications, pages 299­306, 2010.
[16] M. T. Keane, M. O'Brien, and B. Smyth. Are people biased in their use of search engines? Communications of the ACM, 51(2):49­52, 2008.
[17] D. Kelly, K. Gyllstrom, and E. W. Bailey. A comparison of query and term suggestion features for interactive searching. In Proceedings of the 32nd ACM SIGIR conference, pages 371­378, 2009.
[18] J. Kim, P. Thomas, R. Sankaranarayana, T. Gedeon, and H.-J. Yoon. Eye-tracking analysis of user behavior and performance in web search on large and small screens. J. of the Assoc. for Information Science and Technology, 2014.
[19] G. Linden. Marissa mayer at web 2.0, November 2006. http: // glinden. blogspot. com/ 2006/ 11/ marissa-mayer-at-web-20. html .
[20] H. Liu, X. Xie, X. Tang, Z.-W. Li, and W.-Y. Ma. Effective browsing of web image search results. In Proceedings of the 6th ACM SIGMM International Workshop on Multimedia Information Retrieval, pages 84­90, 2004.
[21] L. Lorigo, B. Pan, H. Hembrooke, T. Joachims, L. Granka, and G. Gay. The influence of task and gender on search and evaluation behavior using google. Information Processing & Management, 42(4):1123­1131, 2006.
[22] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. on Information Systems, 27(1):2:1­2:27, 2008.
[23] A. Oulasvirta, J. P. Hukkinen, and B. Schwartz. When more is less: The paradox of choice in search engine use. In Proceedings of the 32nd International ACM SIGIR Conference, pages 516­523, 2009.
[24] T. Paek, S. Dumais, and R. Logan. Wavelens: A new view onto internet search results. In Proceedings of the SIGCHI Conference, pages 727­734, 2004.
[25] H. Reiterer, G. Tullius, and T. M. Mann. Insyder: a content-based visual-information-seeking system for the web. Int. Journal on Digital Libraries, 5(1):25­41, 2005.
[26] M. L. Resnick, C. Maldonado, J. M. Santos, and R. Lergier. Modeling on-line search behavior using alternative output structures. In Proc. of the Human Factors and Ergonomics Soc. Annual Meeting, volume 45, pages 1166­1170, 2001.
[27] S. E. Robertson. The probability ranking principle in ir. Journal of documentation, 33(4):294­304, 1977.
[28] M. D. Smucker and C. L. Clarke. Time-based calibration of effectiveness measures. In Proceedings of the 35th ACM SIGIR conference, pages 95­104, 2012.
[29] S. Sushmita, H. Joho, M. Lalmas, and R. Villa. Factors affecting click-through behavior in aggregated search interfaces. In Proceedings of the 19th International ACM CIKM Conference, CIKM '10, pages 519­528, 2010.

191

[30] S. Sweeney and F. Crestani. Effective search results summary size and device screen size: Is there a relationship? IPM, 42(4):1056­1074, 2006.
[31] J. Teevan, E. Cutrell, D. Fisher, S. M. Drucker, G. Ramos, P. Andr´e, and C. Hu. Visual snippets: Summarizing web pages for search and revisitation. In Proceedings of the SIGCHI Conference, pages 2023­2032, 2009.
[32] A. Tombros and M. Sanderson. Advantages of query biased summaries in information retrieval. In Proc. of the 21st International ACM SIGIR Conference, pages 2­10, 1998.
[33] E. M. Voorhees. Overview of the trec 2005 robust retrieval track. In Proceedings of TREC-14, 2006.

[34] A. Woodruff, R. Rosenholtz, J. B. Morrison, A. Faulring, and P. Pirolli. A comparison of the use of text summaries, plain thumbnails, and enhanced thumbnails for web search tasks. J. Am. Soc. Inf. Sci. Technol., 53(2):172­185, 2002.
[35] Y. Yue, R. Patel, and H. Roehrig. Beyond position bias: Examining result attractiveness as a source of presentation bias in clickthrough data. In Proc. of the 19th Int. Conference on World Wide Web, pages 1011­1018, 2010.
[36] K. Zhou, R. Cummins, M. Lalmas, and J. M. Jose. Evaluating aggregated search pages. In Proceedings of the 35th International ACM SIGIR Conference, SIGIR '12, pages 115­124, 2012.

192

Bayesian Ranker Comparison Based on Historical User Interactions

Artem Grotov a.grotov@uva.nl

Shimon Whiteson s.a.whiteson@uva.nl

Maarten de Rijke derijke@uva.nl

University of Amsterdam, Amsterdam, The Netherlands

ABSTRACT
We address the problem of how to safely compare rankers for information retrieval. In particular, we consider how to control the risks associated with switching from an existing production ranker to a new candidate ranker. Whereas existing online comparison methods require showing potentially suboptimal result lists to users during the comparison process, which can lead to user frustration and abandonment, our approach only requires user interaction data generated through the natural use of the production ranker. Specifically, we propose a Bayesian approach for (1) comparing the production ranker to candidate rankers and (2) estimating the confidence of this comparison. The comparison of rankers is performed using click model-based information retrieval metrics, while the confidence of the comparison is derived from Bayesian estimates of uncertainty in the underlying click model. These confidence estimates are then used to determine whether a risk-averse decision criterion for switching to the candidate ranker has been satisfied. Experimental results on several learning to rank datasets and on a click log show that the proposed approach outperforms an existing ranker comparison method that does not take uncertainty into account.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords
Ranker evaluation; Learning to rank; Click models
1. INTRODUCTION
Comparing rankers is an essential problem in information retrieval. In an industrial setting, an existing production ranker must often be compared to a new candidate ranker to determine whether to replace the former with the latter. Practical constraints make it difficult to make such decisions well. Because the performance of the candidate ranker is unknown, trying it out to gather data about its performance is often too risky: if it proves to be substantially inferior to the production ranker, the user experience may
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767730.

degrade, leading to abandonment. However, existing historical data (typically collected using the production ranker) may not be sufficiently informative about the candidate ranker. Hence, decisions about when to switch to the candidate ranker that are based on such data can be erroneous, also leading to a degraded user experience.
Controlling the risks associated with switching to a candidate ranker requires ranker comparison methods that can (1) estimate the performance of the production and candidate rankers using only historical data, and (2) quantify their uncertainty about such estimates. Quantification of uncertainty is essential for controlling risk because it enables system designers to switch to a candidate ranker only when they are highly confident that its performance will not be substantially worse than that of the production ranker.
Existing ranker evaluation methods do not fully meet these requirements. Traditional methods rely on explicit relevance labels provided by human assessors to measure metrics such as Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP) [20]. However, such labels are expensive to obtain and may not reflect the preferences of real users. Click-based approaches overcome these limitations by estimating performance from the implicit signals in users' click behavior, using A/B testing or interleaving experiments [5, 6, 12, 13, 15, 16, 18, 19, 21]. Nonetheless, these methods typically require trying out the candidate ranker to gain data about it, which, as mentioned above, may be too risky. While interleaving methods have been developed that use importance sampling to estimate ranker performance given only historical data [14], they have limited generalization properties and require historical data to be gathered stochastically.
Click models [3, 10], i.e., probabilistic models of user behavior, can also estimate ranker performance given only historical data [7] and do not share these limitations. The uncertainty in the resulting estimates depends on the relationship between the historical data and the rankers to be evaluated. E.g., if a candidate ranker differs from the production ranker only in that it swaps a lowly ranked but relevant document with a highly ranked but irrelevant one, then the click model can confidently conclude that the candidate ranker is better so long as the historical data shows which of the two documents is relevant. It can do so even if the result lists containing such a swap do not appear in the historical data. By contrast, if the candidate ranker gives a high rank to a document that does not appear in the historical data, then its relevance cannot be estimated with confidence. A key limitation of existing click models is that they do not distinguish between cases such as these. Because they do not quantify their uncertainty about the comparisons they perform, they cannot judge whether the information in the logs is sufficient to compare rankers. Only methods that can measure the confidence of the performed comparisons can be used to safely decide whether to switch from the production ranker to a candidate one.

273

We present Bayesian Ranker Comparison (BARACO), a click model-based approach to ranker evaluation that compares the performance of ranker pairs using only historical data and quantifies the uncertainty in such comparisons. The key novelty lies in maintaining a full posterior distribution over the relevance of documents for queries. This posterior is used to estimate the probability that the candidate ranker's performance is not substantially worse than that of the production ranker. By switching to the candidate ranker only when this probability is sufficiently high, the risks associated with switching can be controlled in a principled way.
BARACO is able to estimate not only the probability of a candidate ranker being as good as the production ranker, but also the expected difference in performance between two rankers. Measuring the expected difference can be useful in practice, e.g., to select a single ranker from a set of candidates, all of which have a high probability of beating the production ranker.
We present the results of an empirical evaluation on several learning to rank datasets and on a real click log published by Yandex that compares BARACO to an existing non-Bayesian click modelbased ranker evaluation method that estimates only the expected difference in performance between ranker pairs, without quantifying uncertainty. We addresses the following research questions:
RQ1 Can BARACO determine whether the production ranker should be replaced with a candidate ranker better than the non-Bayesian baseline?
RQ2 Does BARACO produce better estimates of the difference in performance of the rankers than the non-Bayesian baseline?
Our results show that BARACO has better performance than the baseline method and can identify more good rankers in a set of candidates more reliably. In addition, the full Bayesian approach enables the production of more precise estimates of the expected differences between rankers than the baseline method.
2. RELATED WORK
Ranker evaluation has long been a central topic in information retrieval. The classical approach is to perform offline evaluation based on the Cranfield paradigm [8, 20], where a collection of documents is manually annotated by human experts. A representative set of queries together with associated user intents are crafted by a group of experts. Then, for each query, documents in the collection are assigned relevance labels. The documents and their relevance labels can then be used in metrics such as NDCG. This approach is widely used, well understood, and benefits from controlled laboratory settings. But it is expensive and assessors' relevance judgements may not adequately reflect real users' opinions.
Online evaluation is a family of techniques that addresses these difficulties by letting users be the judges. In A/B testing [16], the user population is split into two groups, and pairs of rankers are compared by presenting one group with one ranker and the other group with another ranker. The ranker with the best performance on a selected metric (such as CTR) is typically considered to be the winner [16]. Another approach is interleaved comparison: search engine result pages (SERPs) presented to users are obtained by interleaving SERPs of two competing rankers under consideration. The user feedback in the form of clicks is then interpreted as user preference for one ranker over the other [19]. The key problem of online methods is that they require user feedback to evaluate each pair of rankers, which often requires exposing suboptimal SERPs. Consequently, they may be too risky for many real-world settings.
Interleaving methods that exploit importance sampling [14] provide a way to compare rankers using only historical click data.

However, importance sampling requires that the source distribution is non-zero everywhere where the target distribution is non-zero (i.e., every SERP that can result from interleaving has a non-zero probability of occurring in the historical data), a requirement that is often not met in practice. Importance sampling guarantees only that the resulting estimator is unbiased, not that it has low variance: the addition of more historical data is not guaranteed to improve estimates, as new data can actually increase the variance [1]. Additionally, importance sampling has poor generalization properties compared to click-based approaches that infer relevance labels of documents. In particular, the latter can generalize across SERPs that differ in the order of documents, while the former cannot.
In recent years, a number of probabilistic models have been developed to describe, understand, and predict user behavior while interacting with an IR system. In particular, click models such as DBN [3], DCM [10] and UBM [9] infer the relevance of documents for queries and model user clicks on documents in the SERP by analysing search engine log files. These inferred relevance labels can be further used for learning to rank [3] and for ranker comparisons using click model-based metrics [7]. However, to our knowledge, none of these approaches provide a way to quantify the uncertainty in the resulting comparison, which is critical for making informed decisions about when to switch from a production ranker to a candidate ranker. For metrics such as EBU [23], ERR [4] and the utility and effort based metrics in [2, 7], the effect on the comparison of a document seen just once and that of one seen a hundred of times is the same, given that the inferred relevance labels have the same value. Furthermore, these metrics do not take into account the number of previously unseen documents that appear in the SERPs produced by the candidate rankers. Hence, these approaches cannot properly control the risks associated with switching to a candidate ranker, as the uncertainty in the comparison is not measured.
We present a new approach to ranker evaluation that is designed to overcome these limitations. It compares rankers using only the log files collected through natural user interactions with the production ranker. In contrast to importance sampling based interleaving methods [14], our method does not require the data to be obtained stochastically. In contrast to the metrics in [2, 4, 7, 23], our method takes into account the uncertainty associated with the inferred relevance labels and the presence of unseen documents and measures the confidence of the resulting comparison.

3. PROBLEM SETTING
We consider two related problems. The first is the Switching Problem: deciding whether or not to switch from a production ranker Rp to a candidate ranker Rc. The second is the Difference Estimation Problem: estimating the expected difference in performance between the production ranker Rp and candidate ranker Rc.
The Switching Problem is as follows. System designers must decide whether or not to switch from a production ranker Rp to a candidate ranker Rc. They are willing to do so only if they are highly confident that the candidate ranker is at least almost as good as the production ranker, i.e., if

p(Mc +  Mp)  1 - ,

(1)

where Mp and Mc are the expected performance of the production and candidate ranker, respectively, according to some metric; is the degree to which the candidate ranker is allowed to be worse than the production ranker; and  is the probability with which the candidate ranker is allowed to fall outside this threshold. The goal of a ranker comparison method in the Switching Problem is to determine whether (1) holds. Note that this requires explicitly reasoning about the uncertainty of comparisons between Rp and Rc.

274

By contrast, the goal of a ranker comparison method in the Difference Estimation Problem is to accurately estimate the expected difference of the metric values for the two rankers:

E[Mc - Mp].

(2)

Unlike the Switching Problem, the Difference Estimation Problem does not require explicitly reasoning about uncertainty. Nonetheless, it can be useful for, e.g., selecting a single ranker from a set of candidates, all of which satisfy (1).
For both problems, the ranker comparison method is given only a log L = [l1, . . . , l|L|] of user interaction sessions gathered using Rp. Each session lj consists of the following: q, the query the user has submitted; [d0, . . . , dN ], the list of N documents returned by Rp that make up the SERP; and [c0, . . . , cN ], the clicks produced by the user on those documents.

4. CLICK MODEL-BASED METRIC
In this section, we describe the metric used to define Mp and Mc within BARACO, the Bayesian ranker comparison method we introduce in §5. While BARACO can work with any click model that provides relevance estimates, our implementation uses DBN [3] because it has shown high performance in predicting user clicks [7].
DBN, the Dynamic Bayesian Network click model [3], represents the relevance of a document for a query as a combination of two variables: attractiveness and satisfactoriness. In DBN, each user interaction session lj begins with a query q from which the SERP, consisting of titles and snippets of the documents [dq,0,. . . , dq,N ], is generated. By assumption, the user starts by examining the first document's title and snippet. If they find the document attractive, they click on and examine the document, or otherwise proceed to examine the following documents' titles and snippets in order. If a clicked document satisfies the user, they terminate the session. Otherwise, they may or may not continue to examine the SERP. DBN assumes the user will not click a document prior to examining its snippet and cannot be satisfied by a document before clicking on it.
Specifically, for a query q the document at position i in the SERP is modelled with four binary variables: whether it is attractive (Aq,i), satisfactory (Sq,i), examined (Eq,i), and clicked (Cq,i). The following relationships hold between variables for a given query:
· Eq,0 = 1: the first document is examined,
· Aq,i = 1, Eq,i = 1  Cq,i = 1: a document is clicked if, and only if, it is attractive and examined,
· Cq,i = 0 = Sq,i = 0: only clicked documents can be satisfactory,
· Sq,i = 1 = Eq,i+1 = 0: satisfied users abandon the search, and
· Eq,i = 0 = Eq,i+1 = 0: users do not skip documents.
In addition, the following stochastic relationships hold:
· p(Eq,i+1 = 1|Eq,i = 1, Sq,i = 0) = : users continue to examine the list after not being satisfied with probability ,
· p(Aq,i = 1) = aq,i, and
· p(Sq,i = 1|Cq,i = 1) = sq,i.
Thus, attractiveness and satisfactoriness are governed by stationary Bernoulli distributions with unknown parameters aq,i and sq,i, which must be inferred from clicks, the only observed variables. As in [3, 7], we assume  is fixed and known. Fig. 1 depicts the relationships between all variables in a given session.

Ei

Ci

Ai

ai

Si

si

Ei+1

Ci+1

Ai+1

ai+1

Si+1

si+1

Figure 1: Graphical representation of the DBN click model. The grey circles represent the observed variables.

Once aq,i and sq,i have been inferred, they can be used to compute a metric such as EBU [23], ERR [4], and the utility and effortbased metrics described in [2, 7]. In this paper, we use the expected effort metric defined in [7]. For a query q that yields a document list with N documents, the metric is defined as:

N
rrM etric(q) =

sq,i aq,i i

i-1
(1 - sq,j aq,j )

.

(3)

i=1

j=1

For a given ranker Rx, the metric Mx required by (1) can then be defined as the expected value of rrM etric(q) across queries:

Mx = rrM etric(q)p(q).

(4)

qQ

Comparing the production ranker to a candidate ranker is complicated by the presence of unseen documents in the candidate rankings. In order to compute the values of a metric for rankings with unseen documents, the unseen documents can be either assigned some a priori values of attractiveness and satisfactoriness or excluded from the ranking. The latter strategy is taken in [7] and in the baseline method used in our experiments.
Using an approach based on expectation maximisation (EM), a maximum a posteriori estimate of Mx can be computed [3]. Hence, the Difference Estimation Problem can be addressed by simply estimating Mc and Mp using this EM-based method and then computing the difference between them. However, this approach suffers from a key limitation. Because it is based only on maximum a posteriori estimates, the difference computed by such an EM-based approach is not the true expected difference of the metric values for the two rankers. Instead, it is only the first mode of the posterior distribution of the difference of the metric values, which may not be equal to the expectation if the distribution is multimodal or asymmetric. Because BARACO is a fully Bayesian approach, it can estimate the true expected value of the difference between Mc and Mp, which leads to better quality estimates, as we will see.
Furthermore, the Switching Problem cannot be directly addressed using an EM-based approach because such an approach gives no information about the uncertainty in the resulting estimate. That is, it does not estimate the distribution over the metric values of the two rankers. Consequently, it provides no way to estimate the probability that Mc + is greater than or equal to Mp. Instead, addressing the Switching Problem using an EM-based method requires resorting to a heuristic approach, e.g., switching only when the estimated difference exceeds some manually tuned threshold. This forms the core of the baseline method we compare against in §7.

275

Below, we present a method that addresses the shortcomings of an EM-based approach and enables better solutions to both the Switching Problem and the Difference Estimation Problem.

5. BARACO
In this section, we present BARACO. First, §5.1 describes how the posterior distributions over aq,i and sq,i can be inferred from L, the set of user interaction sessions; §5.2 describes how to solve the Switching Problem by evaluating (1) when Mp and Mc are defined according to (4), given posterior distributions over aq,i and sq,i; §5.3 describes how to solve the Difference Estimation Problem.

5.1 Inferring click model posteriors

Evaluating (1) and (2) requires knowing the posterior probabilities p(aq,i|L) and p(sq,i|L) for each ranker, query, and document. In this subsection, we describe how to estimate these posteriors.
The algorithm works by iterating over the sessions. To process the first session, the posteriors p(aq,i|l1) and p(sq,i|l1) are computed given uniform priors p(aq,i) and p(sq,i). Then, for each subsequent session lj, p(aq,i|Lj) and p(sq,i|Lj) are computed given p(aq,i|Lj-1) and p(sq,i|Lj-1), where Lj = [l1, . . . , lj ]. The algorithm terminates when l = |L|, yielding p(aq,i|L) and p(sq,i|L).
We now describe how to compute p(aq,i|Lj) and p(sq,i|Lj) given p(aq,i|Lj-1) and p(sq,i|Lj-1) and the next session lj . Because Aq,i and Sq,i are Bernoulli variables, we can model the distribution over their parameters aq,i and sq,i using a Beta distribution. Focusing on aq,i, this yields:

p(aq,i|Lj )

=

Beta(, )

=

aq,i-1(1 - aq,i)-1 , B(, )

(5)

where  is the number of observations of Aq,i = 1 and  is the number of observations of Aq,i = 0 that have occurred up to and including session j, and B(, ) is a Beta function. Beta(1, 1) corresponds to a uniform distribution, i.e., no prior knowledge about aq,i. If Aq,i is observed in session lj , then p(aq,i|Lj ) can be updated using Bayes' rule:

p(aq,i|Lj ) =

p(aq,i|Aq,i, Lj-1)

=

p(Aq,i|aq,i)p(aq,i|Lj-1) , p(Aq,i|Lj-1)

(6)

In this case, the update reduces to simply incrementing  or . Since the Beta distribution is the conjugate prior for the Bernoulli variable, the posterior remains a Beta distribution.
In our setting, Aq,i is not directly observed. However, when we know Eq,i, we can directly infer Aq,i from Cq,i because the user always clicks on an attractive examined document. Thus, the difficult case is when we do not know Eq,i, which occurs whenever i > c, where c is the index of the last clicked document. The remainder of this subsection describes how to address this case.
There are two steps. Since we do not know Eq,i when i > c, in the first step we must instead compute a posterior over it: p(Eq,i|Lj). Then, in the second step, we use p(Eq,i|Lj) to estimate p(aq,i|Lj ) and p(sq,i|Lj ).
To perform the first step, we use the sum-product message passing algorithm [1]. In particular, we extract the subgraph of the graphical model that represents the documents below the last clicked one and remove the nodes representing the satisfactoriness (Sq,i and sq,i) for documents without clicks. This is because it is not possible to say anything about sq,i for i > c as it is not observed and has no effect on what is observed. Since the resulting graph, shown in Fig. 2, is a polytree, the sum-product algorithm enables us to perform exact inference, which yields p(Eq,i|Lj).

s

S

E1

C1

A1

a1

E2

C2

A2

a2

E3

C3

A3

a3

Figure 2: Graphical representation of the DBN click model for the part of the SERP below and including the last clicked document in a session. Grey circles represent observed variables.

For the second step, we compute p(aq,i|Lj) and p(sq,i|Lj) given p(Eq,i|Lj). Focusing now on p(aq,i|Lj) and starting from Bayes' rule, we have:

p(aq,i|Lj ) =

p(aq,i|Cq,i, Lj-1)

=

p(Cq,i|aq,i)p(aq,i|Lj-1) . p(Cq,i|Lj-1)

(7)

The likelihood term p(Cq,i|aq,i) can be computed by marginalizing across A and E:

p(Cq,i|aq,i) =

p(Cq,i|Eq,i = E, Aq,i = A)
A{0,1} E{0,1}

p(Eq,i = E)p(Aq,i = A|aq,i) .

(8)

Sticking (8) into (7) and ignoring normalization gives: p(aq,i|Cq,i, Lj-1)  p(Cq,i|Eq,i = E, Aq,i = A)
A{0,1} E{0,1}
p(Eq,i = E)p(Aq,i = A|aq,i)p(aq,i|Lj-1)

 p(aq,i|Lj-1)

p(Aq,i = A|aq,i)

A{0,1}

p(Cq,i|Eq,i = 0, Aq,i = A)p(E = 0) +

+ p(Cq,i|Eq,i = 1, Aq,i = A)p(Eq,i) .

Since we are focusing on the case where i > c, we know that Cq,i = 0, i.e., dq,i was not clicked. Furthermore, we know that p(Cq,i = 0|Eq,i = 0, Aq,i = A) = 1 and p(Cq,i = 0|Eq,i = 1, Aq,i = A) = 1 - A, yielding:

p(aq,i|Cq,i = 0, Lj-1) 

p(aq,i|Lj-1)

p(Aq,i = A|aq,i)

A{0,1}

(1 · p(Eq,i = 0) + (1 - Aq,i) · p(Eq,i = 1))

 p(aq,i|Lj-1)

p(Aq,i = A|aq,i)

A{0,1}

276

Algorithm 1 ComputePosteriors(L)

1: InitializePriors()

2: for lj in L do

3: for i in lj do

4:

p(Eq,i)  sumP roduct(lj )

5:

if Cq,i = 1 then

6:

p(aq,i|Cq,i = 1, Lj-1)  p(aq,i|Lj-1)aq,i

7:

p(sq,i|Cq,i+1 = 0, Lj-1)  p(sq,i|Lj-1)

(p(Eq,i+1 = 1) + sq,ip(Eq,i+1 = 0))

8:

else

9:

p(aq,i|Cq,i = 0, Lj-1)  p(aq,i|Lj-1)

(1 - aq,i · p(Eq,i = 1)) return p(a|L), p(s|L)

(1 - p(Eq,i = 1) + p(Eq,i = 1) - A · p(Eq,i = 1))

 p(aq,i|Lj-1)

p(Aq,i = A|aq,i)

A{0,1}

(1 - A · p(Eq,i = 1)) .

(9)

Now we can substitute p(Aq,i = A|aq,i) for the values of A: p(Aq,i = 1|aq,i) = aq,i and p(Aq,i = 0|aq,i) = 1 - aq,i yielding:

p(aq,i|Cq,i = 0, Lj-1) 
p(aq,i|Lj-1)(1 - aq,i · p(Eq,i = 1)). (10)
Thus, (10) is the Bayesian update rule for attractiveness. When p(Eq,i = 1) = 1, e.g., if the document was clicked, the posterior is a Beta distribution because plugging p(Eq,i = 1) = 1 in (10) yields a term conjugate to the parametrization in (5). Otherwise, we use p(Eq,i|Lj), as computed in the first step via a messagepassing algorithm that takes into account the satisfactoriness of the last clicked document and the attractivenesses of the unexamined documents. Instead of a Beta distribution, this yields a more general polynomial function. The normalization constant can be found by integration: since the function is a polynomial with known coefficients, it can be integrated analytically and then the definite integral evaluated on the interval [0, 1]. The cumulative distribution function is therefore also a polynomial and easy to compute. The same holds for the expectation of the distribution.
Analogously, we can derive an update rule for satisfactoriness:

p(sq,i|Cq,i+1 = 0, Lj-1) 
p(sq,i|Lj-1)(p(Eq,i+1 = 1) + sq,ip(Eq,i+1 = 0)). (11)
Algorithm 1 summarizes the steps involved in computing p(aq,i|L) and p(sq,i|L). First, the attractiveness and satisfactoriness of all document query pairs are assigned uniform Beta(1, 1) priors. Then, the log file is processed session by session and the attractiveness and satisfactoriness of all document query pairs in the session are updated using the Bayesian update rules described in this section: (6) and (10) for attractiveness and (11) for satisfactoriness.
5.2 The Switching Problem
To decide whether or not to switch from Rp to Rc, we must determine whether (1) holds. Determining this from L requires evaluating the following double integral:

p(Mc +  Mp | L) =

Mc=M 

Mp =Mc +

p(Mc|L)

p(Mp|L) dMc dMp,

Mc =0

Mp =0

(12)

Algorithm 2 BARACO-SP(Rp, Rc, L, , )

1: p(a|L), p(s|L)  computePosteriors(L)

§5.1

2: for k in 1:K do

§5.2

3: Mpk  sample(Rp, p(a|L), p(s|L))

4: Mck  sample(Rc, p(a|L), p(s|L)) 5: if Mck +  Mpk then

6:

NMc+ Mp  NMc+ Mp + 1

7: p(Mc +



Mp)



NMc+ Mp K

return p(Mc +  Mp)  1 - 

where M  is the maximum possible value of the metric defined in (4). Evaluating this integral requires knowing the posterior probabilities p(Mp|L) and p(Mc|L), which can be computed from (3) and (4) given p(aq,i|L) and p(sq,i|L) for each ranker, query, and document.
Computing (12) analytically is hard, but it can be estimated using a Monte-Carlo sampling scheme. Algorithm 2 describes this scheme, which yields a version of BARACO that solves the Switching Problem.
First, we compute the posteriors p(a|L) and p(s|L) using the approach described in §5.1 (line 1). Then, each sampling iteration k consists of drawing a sample aq,i and sq,i for each ranker, document, and query from p(aq,i|L) and p(sq,i|L). Using these samples, we compute Mck and Mpk, the values of the metrics given the sampled probabilities from the k-th iteration (lines 3­4). Estimating (12) then reduces to computing the fraction of sampling iterations for which Mck +  Mpk (lines 6­7). If this fraction is greater than 1 - , we can safely switch to the candidate ranker.
However, sampling aq,i and sq,i from p(aq,i|L) and p(sq,i|L) is itself hard because their cumulative distribution functions are high degree polynomials that are hard to invert. Therefore, we employ the Metropolis-Hastings algorithm [1] with the expected value of the sampled distribution, which can be calculated analytically through integration, as the starting position of the random walk. The proposal distribution is a Gaussian with fixed variance.

5.3 The Difference Estimation Problem

If we are interested only in the expected value of a ranker Rx, and not the uncertainty of this estimate, we can compute it as fol-

lows:

E[Mx] =

Mx=M  Mx =0

Mxp(Mx)

dMx.

Using the same sampling procedure described above, we can solve

the Difference Estimation Problem by estimating this expected value:

E[Mx]



1 K

K k=1

Mxk

,

(13)

where K is the number of sampling iterations. We can similarly estimate the expected difference between Mc and Mp:

E[Mc

- Mp]



1 K

K k=1

(Mck

-

Mpk ).

(14)

Algorithm 3 summarizes the resulting version of BARACO that solves the Difference Estimation Problem. It is essentially the same as Algorithm 2 except that it estimates the expected difference between the production and the candidate rankers' metrics.

6. EXPERIMENTAL SETUP
In this section, we describe the experimental setup used to evaluate BARACO. This evaluation is complicated by the fact that the user's information need, and therefore the true relevance labels, are unknown. We present two evaluations which deal with this problem

277

Algorithm 3 BARACO-DEP(Rp, Rc, L, , )

1: p(a|L), p(s|L)  computePosteriors(L)

§5.1

2: M  0

3: for k in 1:K do

§5.3

4: Mpk  sample(Rp, p(a|L), p(s|L))

5: Mck  sample(Rc, p(a|L), p(s|L))

6:

M  M +Mck - Mpk

7: E[Mx] 

M K

return E[Mx]

in two different ways. The first evaluation, based on the LETOR datasets [17], uses manual relevance assessments as ground-truth labels and synthetic clicks as feedback to BARACO. The second evaluation, based on the WSDM 2014 Web search personalization challenge,1 uses dwell time as ground-truth labels and real clicks as feedback to BARACO.
Another possibility would be to evaluate BARACO using explicit relevance labels provided by human assessors to measure metrics such as Normalized Discounted Cumulative Gain (NDCG) but this approach is known to have low agreement with metrics based user behavior such as A/B testing or interleaving [4, 6, 19, 24], so it is natural to expect that it would have a low agreement with BARACO as well. It would also be possible to evaluate BARACO using A/B tests or interleavings, but A/B tests have low agreement with interleavings and different metrics collected during A/B tests such as click through rate, clicks@1 and others have low agreement with each other [19]. The only definitive way to evaluate BARACO would be in an industrial setting that measures longterm metrics such as engagement. Such results, however, would be difficult to reproduce. Consequently, the LETOR evaluation is the most reliable and reproducible, as it depends on indisputably unbiased ground truth. In addition, it allows us to explore how the discrepancy between the click model and user behavior affects BARACO's performance. However, the WSDM evaluation, though less reliable, is nonetheless useful because it gives insight into how BARACO performs in a real-world setting with real users.
We compare BARACO to a baseline that, in lieu of our Bayesian approach, uses the EM-based approach described in [3] to compute maximum a posteriori estimates of (4) for Mp and Mc. These estimates can then be directly used in the Difference Estimation Problem. Because this approach does not compute full posteriors, it does not quantify uncertainty in the resulting estimates and therefore cannot be directly used in the Switching Problem. Instead, the baseline method, which we call Manual Thresholding (MT), resorts to a heuristic approach to determine whether (1) holds. In particular, Rc is deemed safe when M^ c - M^ p > m, where M^ c and M^ p are the maximum a posteriori estimates of Mc and Mp produced by the EM-based method and m is a threshold parameter whose value must be tuned manually. Because we want to switch whenever Mc - Mp > - , the quantity + m acts as a buffer, i.e., an extra gap that Rc must exceed to be considered safe. Adding this buffer heuristically accounts for the uncertainty in M^ c and M^ p.
The need to tune m for MT poses significant difficulties in practice. To understand the effect of m on the behavior of MT would require access to ground truth about a set of candidate rankers, i.e., whether they are in fact no more than worse than the production ranker. While such ground truth could be obtained using off-line or
1Personalized Web Search Challenge 2013 https://www.kaggle.com/c/ yandex-personalized-web-search-challenge

on-line evaluations, such evaluations pose exactly the difficulties that motivate the need for methods like BARACO: the former is expensive and may not reflect real user preferences. The latter requires showing potentially poor rankers to real users. Furthermore, while such ground truth, even if it could be obtained, would shed light on how m affects to which candidate rankers MT switches, it would still not make it possible to select the m that is best for a given  supplied by the system designers. Doing so would require a quantification of the uncertainty about each candidate ranker that is inherent to BARACO but absent in MT.
Importantly, BARACO does not require tuning m or any analogous parameter. On the contrary,  and , which are supplied by the system designers, are simply quantifications of their risk tolerance.
6.1 LETOR Evaluation
The first evaluation is based on the LETOR datasets [17], which include manual relevance assessments. However, they do not include user clicks. In addition, even if they did, these would likely correlate only poorly with the relevance assessments, since the assessors may not interpret the users' information need correctly. To address this difficulty, we instead axiomatically define the relevance assessments to be correct ground-truth labels and use click models to generate the clicks. Since BARACO also relies on click models, we evaluate it in settings where the clicks are generated and interpreted using different click models, to assess BARACO's robustness to errors in its modeling assumptions.
LETOR is split into six sub-datasets: HP2003, HP2004, NP2003, NP2004, TD2003, and TD2004. For each run of each algorithm, we use the data from one sub-dataset. First we train a ranker RAda using AdaRank [22] on all the data in this sub-dataset. AdaRank, which performs reasonably on all these datasets [22], trains a linear ranking function, i.e., a linear combination of the ranking features for a given query-document pair. The documents are then sorted based on the values produced by this ranking function.
To ensure that some candidate rankers will be better than the production ranker, we craft Rp by "damaging" RAda, i.e., randomly by adding random vectors to it. These vectors were generated by randomly sampling a normal distribution with mean equal to 0 and standard deviation of 0.2. Finally, to generate a population of candidate rankers Rc, we again perturb Rp 1000 times using the same sampling method. This ranker generation methodology is motivated by the gradient descent algorithm [25]; the standard deviation value was chosen so that some of the rankers would be similar enough to the production ranker and some too different from it for the algorithm to be confident about their performance.
The next step is to generate the log file. To this end, we generate user click interaction data using the DBN, sDBN and UBM click models with three user model settings: perfect, navigational and informational. The clicks are then interpreted using the DBN click model. The parameters of the click models are summarized in Table 1, where p(C|R) and p(C|NR) denote the probability of a user clicking a relevant document and an irrelevant document, respectively, and p(s|R) and p(s|NR) denote the probability of abandoning the SERP after clicking a relevant document and an irrelevant document, respectively. The closer p(C|R) is to p(C|NR) and p(s|R) to p(s|NR), the more noise there is in the feedback and the more difficult inference becomes. The user interaction data is generated for the production ranker by randomly sampling 500 queries and generating the SERPS and clicks.
The perfect user model setting is used to obtain an upper bound in performance: the user clicks all relevant documents and no irrelevant ones. The navigational and informational user models are based on typical user behavior in web search [11]. The navigational

278

Table 1: Overview of the user model settings.

Model
Perfect Navigational Informational

p(C |R)
1.0 0.95 0.9

p(C|N R)
0.0 0.05 0.4

p(s|R)
0.0 0.9 0.5

p(s|N R)
0.0 0.2 0.1

user model reflects user behavior while searching for an item they know to exist, such as a company's homepage. Because it is easy for users to distinguish between relevant and irrelevant documents, the noise levels are low. The informational user model reflects user behavior while looking for information about a topic, which can be distributed over several pages. Because this task is more difficult, there is more noise in the feedback.
The clicks are generated and interpreted using either the same or different click models. When they are generated and interpreted using the same click model, our findings are not affected by the accuracy of the assumptions in the click model, allowing us to focus on the differences between BARACO and the baseline method. Of course, some assumptions made by DBN, which is used to interpret clicks, may not always hold in practice. For example, DBN assumes that the document that was clicked and led to abandonment is the document that satisfied the user. This assumption typically holds for navigational queries, but may not be valid for informational queries. Therefore, experiments in which the clicks are generated and interpreted using different click models help measure the robustness of BARACO to settings whether the assumptions underlying DBN do not always hold.
In the LETOR Evaluation setup, we compare the performance of BARACO and MT using area under roc curves (AUC), Pearson correlation, and the square root of the mean squared error (RMSE). The rankers are compared using the metric rrMetric (3).
6.2 WSDM Evaluation
We additionally evaluate BARACO using an anonymized click log released by Yandex for the WSDM 2014 Web search personalization challenge. The click log contains sessions with queries submitted by a user. For each query there is a list of search results returned by the engine and the clicks produced by the user. The queries and the clicks have timestamps. However, the units of time are not disclosed. The queries, query terms, documents, and users are represented by IDs in order to protect the privacy of users.
The organizers of the challenge have defined three levels of relevance based on the clicks that the documents receive: documents that receive clicks with dwell time less than 50 time units have relevance 0, clicks with dwell time between 50 and 150 units have relevance 1, and clicks with dwell time of more than 2 time units as well as clicks that are the last clicks for a given query have relevance 2. We use all but the last session for a query for training and use the last session for extracting the relevance labels for querydocument pairs.
The candidate rankers are generated by perturbing the result lists observed in the click log in a random way. In order to ensure that some of the candidates are better than the production ranker, the relevant documents have a higher chance to be promoted to top than the irrelevant ones.
In the WSDM Evaluation setup, we compare the performance of BARACO and MT using the following metrics: AUC and Pearson correlation as before. But we do not use RMSE because the graded relevance and the estimated relevance have different scales from 0 to 2, and from 0 to 1 respectively. The candidates are compared using DCG instead of the metric in (3) because (3) requires a mapping from relevance labels to attractiveness and satisfactoriness that

is not available for the graded relevance in the click log--it could be computed using, e.g., DBN but then the evaluation would be less sound because the same click model would be used for training the parameters of the documents and for training the metric.2
6.3 Parameter Settings
Both BARACO and MT are instantiated with the user persistence parameter  = 0.9, as in [3, 7], = 0.01. For the MetropolisHastings sampling procedure described in §5.2, the variance of the proposal distribution was set to 0.1, which was determined experimentally to provide a rejection ratio of around 0.6. For each query/document pair, Nsamples = 1000 samples are drawn and shuffled to reduce autocorrelation in order to evaluate (1). All results are averaged over 30 independent runs for each algorithm. In order to check the significance of observed differences between results, we perform Wilcoxon signed-rank tests; in the result tables,
denotes a significant difference at p = 0.01, and at p = 0.05.
7. RESULTS
In this section, we present our experimental results aimed at answering RQ1 and RQ2. In §7.1, we analyse the performance of BARACO and MT on the LETOR data; in §7.2, we analyse their performance on the WSDM data.
7.1 LETOR Results
In §7.1.1, we compare BARACO and MT on the Switching Problem; in §7.1.2, we compare BARACO and the EM-based approach [3] that underlies MT on the Difference Estimation Problem.
7.1.1 Switching Problem Results
To address RQ1, we compare the ROC curves of BARACO and MT on the Switching Problem. Such curves show how the true and false positives of both methods change when we fix = 0.01 and vary across different values of   [0, 1] for BARACO and m  [-4 , ] for MT. In this context, a true positive occurs when the algorithm recommends switching to Rc and Mc +  Mp, while a false positive occurs when it recommends switching but Mc + < Mp. We then compare the AUC of both methods for different datasets and user and click models.
Note that this comparison is fundamentally unfair to BARACO because its parameter, , does not require tuning but instead is input by the system designers as a quantification of their risk tolerance. By contrast, m is a parameter that requires manual tuning and cannot be derived from , which MT ignores. As discussed in §6, tuning this parameter is quite difficult in practice. Because the ROC curves show performance across different values of  and m, they allow MT to "cheat" by optimizing its critical parameter, a step that is unnecessary in BARACO. In other words, these ROC curves answer the following question: for each value of  that a system designer could input to BARACO, how well can MT match the quality of BARACO's decisions about when to switch to Rc if an oracle provides MT with the best possible value of m? Thus, BARACO can be considered a success if it can match the performance of MT when MT is given this advantage.
Fig. 3 plots the area under the ROC curves for BARACO and MT for all six data sets, three click models, and three user model settings. Fig. 4 shows the ROC curve for the TD2004 dataset, informational user model with the DBN model used for generation. The other ROC curves, omitted for brevity, are qualitatively similar.
2In the case of the LETOR evaluation experiments, this mapping is known and can be read from Table 1.

279

Area Under the ROC Curve

The results in Fig. 3a show that, even at the best value of m, BARACO substantially outperforms MT on four of the six datasets (HP2003, HP2004, NP2003, TD2004). On the other two datasets (NP2003 and TD2003), the two methods perform similarly. Analysis of the latter two datasets shows that the production ranker was at a local minimum. Hence, nearly all candidate rankers are better than the production ranker and the best performance is obtained by always switching. As this degenerate policy can be represented just as well by MT as by BARACO, the two perform similarly.
Furthermore, the fact that BARACO performs nearly as well when clicks are generated and interpreted with different models, as shown in Figs. 3b and 3c, shows that BARACO is robust to violations of its modeling assumptions. The baseline substantially outperforms BARACO only for the perfect sDBN user model on the TD2003 and TD2004 datasets. The baseline shows superior performance because there are many more relevant documents in the TD datasets, and many of them are not presented to the user by the production ranker. In the case of the perfect user model, the user only clicks on relevant documents. Therefore, there are many queries for which no clicks are produced because no relevant documents were shown. The baseline essentially removes such queries from consideration through condensing [6], which may be an effective strategy in this case. Overall, these results demonstrate that BARACO can offer a robust and effective means for deciding when to switch rankers: especially in cases where its modeling assumptions hold, it outperforms MT with a tuned m parameter for nearly all combinations of dataset and user model.
The values of m shown here were chosen because their inflection point lies in the interval   [0, 1]. These are all negative values of m because MT has a consistent negative bias: almost all candidate rankers receive a score lower than the production ranker. This bias is a consequence of condensing [6]: almost all candidate rankers have unseen documents that do not contribute to the metric. This further highlights the brittleness of MT: as more data is collected, the relative number of unseen documents may decrease, which would reduce the effect of condensing and therefore the amount of bias, necessitating a retuning of m.
7.1.2 Difference Estimation Problem Results
To address RQ2, we compare BARACO to the EM-based method [3] that underlies MT, on the Difference Estimation Problem. BARACO uses (14) to estimate the expected difference while the EMbased method uses M^ c - M^ p, where M^ x is the maximum a posteriori estimate of Mx. First, we consider how the RMSE scores of the two approaches differ. Error is defined here to be the difference between the true and estimated value of Mc - Mp. The true values are computed from the expert-generated relevance labels in the datasets. Table 2 summarizes the MSE of BARACO and MT. These results show that BARACO consistently outperforms the EM-based approach, in many cases by an order of magnitude. The only exception is the TD2004 dataset for clicks generated using UBM. This exception occurs because there are many unseen relevant documents in the TD2004 dataset and, when the user model assumptions do not hold, the baseline's condensing strategy [6] may be more effective because it does not rely on these assumptions.
However, the fact that BARACO has lower RMSE scores is important only if we are interested in the absolute values of the metric differences. Instead, if we want to be able to rank the candidate rankers by their metric values, we need a different way to compare the methods. To this end, we measure the Pearson's correlation between the ground truth value Mc - Mp and the estimates produced by BARACO or MT. For example, if the correlation with the ground truth was perfect, ordering all the candidate rankers by

1.0

0.9

0.8 0.7 0.6
PN I

BARACO MT
PN I PN I

PN I

PN I

PN I

HP2003 HP2004 NP2003 NP2004 TD2003 TD2004 (a) Using DBN for generation and interpretation.

Area Under the ROC Curve

1.0

0.9

0.8 0.7 0.6
PN I

BARACO MT
PN I PN I

PN I

PN I

PN I

HP2003 HP2004 NP2003 NP2004 TD2003 TD2004 (b) Using sDBN for generation and DBN for interpretation.

Area Under the ROC Curve

1.0

0.9

0.8 0.7 0.6
PN I

BARACO MT
PN I PN I

PN I

PN I

PN I

HP2003 HP2004 NP2003 NP2004 TD2003 TD2004 (c) Using UBM for generation and DBN for interpretation.

Figure 3: AUC for all data sets, user and click models. The error bars are standard errors of the means. P - perfect user model setting, I - informational, N - navigational (LETOR evaluation).
their ground truth difference with the production ranker would be the same as ordering them by the estimated difference. Thus, the correlation with the ground truth is more informative than RMSE in cases where we care about preserving the ground-truth ranking. This occurs, e.g., when several candidate rankers confidently outperform the production ranker. In such cases, it is desirable to switch to the one that outperforms it by the largest margin, while the exact absolute values of the estimated metrics are not important.
Table 3 summarizes the correlations between the ground truth difference of rankers and the difference of rankers computed by BARACO and the EM-based method. Higher correlations with the ground truth mean that the way the rankers are ranked is closer to the ground truth. These results show that BARACO again outperforms the EM-based method. The negative correlation in the informational setting of NP2003 dataset is due to a heavily skewed distribution of candidate rankers when the production ranker is at a

280

1.0

0.8

True Positive Rate

0.6

0.4

0.2

BARACO ROC (area = 0.88)

0.0 0.0

MT ROC (area = 0.85)

0.2

0.4

0.6

0.8

1.0

False Positive Rate

Figure 4: The ROC curves for BARACO and MT, using DBN for generation and interpretation, TD2004 dataset, informational user model (LETOR evaluation).

Table 2: RMSE between the predicted outcome of the comparison and the ground truth, P - perfect user model setting, I informational, N - navigational (LETOR evaluation).

DBN

sDBN

UBM

Dataset UM BARACO EM BARACO EM BARACO EM

HP2003 P 3.1e-5 3.0e-4 3.8e-5 3.1e-4 3.5e-5 3.2e-4 N 2.8e-5 3.5e-4 2.6e-5 3.1e-4 4.7e-5 3.0e-4 I 6.1e-5 2.7e-4 4.1e-5 2.7e-4 1.1e-4 2.6e-4
HP2004 P 2.7e-5 2.3e-4 4.3e-5 2.0e-4 2.7e-5 2.4e-4 N 1.6e-5 2.4e-4 1.9e-5 2.3e-4 6.0e-5 1.9e-4 I 6.2e-5 2.0e-4 5.3e-5 1.6e-4 2.2e-4 2.4e-4
NP2003 P 1.3e-5 2.9e-4 1.5e-5 2.8e-4 1.4e-5 2.9e-4 N 1.4e-5 3.0e-4 1.3e-5 2.8e-4 3.3e-5 2.5e-4 I 3.7e-5 2.8e-4 3.8e-5 2.7e-4 1.1e-4 2.7e-4
NP2004 P 4.1e-6 6.2e-5 6.6e-6 5.8e-5 4.3e-6 6.6e-5 N 4.9e-6 8.9e-5 3.6e-6 7.8e-5 7.9e-6 6.4e-5 I 1.5e-5 9.8e-5 1.5e-5 1.0e-4 2.5e-5 1.2e-4
TD2003 P 7.0e-5 1.1e-4 3.7e-4 1.2e-4 6.8e-5 1.7e-4 N 8.0e-5 3.5e-4 6.2e-5 3.8e-4 7.6e-5 1.9e-4 I 6.3e-5 2.7e-4 6.9e-5 2.7e-4 1.5e-4 3.0e-4
TD2004 P 4.0e-4 7.8e-4 1.2e-3 5.9e-4 5.1e-4 9.2e-4 N 3.2e-4 2.3e-3 2.5e-4 2.3e-3 6.0e-4 1.2e-3 I 4.7e-4 2.0e-3 4.2e-4 1.7e-3 1.2e-3 1.5e-3

local minimum of the ranker space and almost all candidate rankers are better for the queries in which the production ranker has not presented any relevant documents. This situation is unlikely to occur in the real world since production rankers are typically highly engineered and thus more likely to be local maxima than local minima. As with our earlier results, we see that the performance on the TD2004 dataset using UBM for generation is qualitatively different from the other conditions for the reasons mentioned earlier.
To answer RQ2, we observe that both the RMSE and correlation results show that BARACO outperforms MT: BARACO achieves better estimates in both absolute and relative terms, except on the TD2004 dataset with the UBM click model for generation, whose special nature has been recognized before.
7.2 WSDM Results
In this section, we compare BARACO and MT on the Switching Problem and Difference Estimation Problem, respectively, using the WSDM experimental setup

Table 3: Correlation between the predicted outcome of the comparison and the ground truth, P - perfect user model setting, I - informational, N - navigational (LETOR evaluation)

DBN

sDBN

UBM

Dataset UM BARACO EM BARACO EM BARACO EM

HP2003 P N I
HP2004 P N I
NP2003 P N I
NP2004 P N I
TD2003 P N I
TD2004 P N I

0.961 0.967 0.914 0.963 0.980 0.931 0.979 0.973 0.902 0.922 0.923 0.700 0.738 0.838 0.777 0.846 0.862 0.755

0.846 0.950 0.856 0.961 0.763 0.930 0.933 0.946 0.926 0.981 0.825 0.935 0.890 0.982 0.878 0.982 0.777 0.887 0.841 0.915 0.809 0.943 0.571 0.678 0.678 0.398 0.813 0.873 0.689 0.776 0.783 0.434 0.776 0.890 0.633 0.829

0.858 0.963 0.857 0.856 0.950 0.808 0.744 0.510 0.664 0.937 0.969 0.940 0.943 0.960 0.908 0.825 0.262 0.612 0.901 0.982 0.897 0.891 0.969 0.856 0.769 0.523 0.678 0.871 0.925 0.861 0.835 0.864 0.751 0.546 0.464 0.393 0.619 0.732 0.723 0.830 0.758 0.675 0.702 0.009 67 0.404 0.817 0.887 0.795 0.753 0.858 0.693 0.637 -0.156 0.464

7.2.1 Results for the Switching Problem
To address RQ1, we compare the ROC curves of BARACO and MT on the Switching Problem. The AUC of these curves for both methods are stated in Table 4. The AUCs illustrate that both methods are able to distinguish between strong and weak candidates. Both methods suffer from a weak, systematic bias--they consistently underestimate the quality of the candidates because the relevance labels used in the evaluation are biased towards top results.

Table 4: AUC and Correlation between the predicted outcome of the comparison and the ground truth (WSDM evaluation).

AUC Correlation

BARACO
0.936 0.751

Manual Thresholding
0.934 0.735

The real proportion of candidate rankers that are better than the production ranker across different probability levels computed by BARACO is summarized in Table 5. The probabilities output by BARACO are not perfectly calibrated and instead tend to be underestimates. Thus, not all risk prescribed by  and can be utilized, making the system somewhat overly conservative.
Unfortunately, a limitation of these WSDM experiments is that there is no way to ascertain how much of this bias is due to discrepancies between the relevance labels and the true information needs of the users who generated the clicks and how much is due to discrepancies between BARACO's click model and those users' behavior. However, because the bias is consistent, correcting for this bias, e.g., by learning a constant offset, is straightforward.
7.2.2 Results for the Difference Estimation Problem
The correlations between the true and estimated value of Mc - Mp computed by BARACO and MT are also stated in Table 4. The estimated difference between rankers is strongly correlated with the ground truth in the WSDM dataset, suggesting that both methods can estimate the difference between rankers well given the logged user interactions with the production ranker.

281

Table 5: Proportion of candidate rankers that beat the production ranker across probability levels computed by BARACO (WSDM evaluation).

Probability Level 0­0.1 0.1­0.2 0.2­0.3 0.3­0.4 0.4­0.5 0.5­0.6 0.6­0.7 0.7­0.8 0.8­0.9 0.9­1

Proportion

0.004 0.037 0.139 0.312 0.546 0.742 0.886 0.971 0.993 1.0

In both experimental setups, i.e., the LETOR and WSDM setup, both BARACO and MT have good performance in most cases and high agreement with the ground truth. In the LETOR setup the performance is often superior to that in the WSDM experiments, especially when there is little noise and no discrepancy between the user behavior and the click model. However, when there is much noise and the clicks are generated and interpreted using different click models, the performance drops to levels lower than in the WSDM experiments. Overall, these results show that, given a reasonable click model, BARACO makes it possible to make informed decisions whether or not to switch to a candidate ranker given historical user interaction data obtained using the production ranker.
8. CONCLUSIONS AND FUTURE WORK
We presented BARACO, a new click model-based method of ranker comparison with two key features: (1) it compares the performance of rankers using only historical data, and (2) it quantifies the uncertainty in such comparisons. Using BARACO, it is possible to decide, using only historical data collected with the current production ranker, whether one can confidently replace the production ranker with a candidate ranker. The algorithm takes as input , the degree to which the candidate ranker is allowed to be worse than the production ranker; , the probability with which the candidate ranker is allowed to fall outside this threshold; and user interaction logs collected with the production system.
Our experiments show that BARACO can correctly and confidently identify candidate rankers that are as good as the production ranker. BARACO outperforms or has performance comparable to that of the MT baseline that requires manual tuning of the threshold m through offline assessments or online user experiments.
A natural application of BARACO is within online learning to rank algorithms, many of which require an evaluation subroutine. For example, in Dueling Bandit Gradient Descent (DBGD) [25], the algorithm randomly picks a candidate ranker from the neighborhood of the current ranker, compares it to the current ranker, and, if the candidate ranker appears to be better, updates the current ranker so that is is closer to the candidate ranker. The evaluation step is usually done using interleaving, which requires showing potentially poor rankings to the user. Using BARACO, DBGD could be performed while restricting the rankings shown to users to those generated by production rankers or candidate rankers in which we have sufficient confidence.
Acknowledgements. This research was supported by the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the Netherlands Organisation for Scientific Research (NWO) under project nrs 727.011.005, 612.001.116, HOR11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15, Amsterdam Data Science, the Dutch national program COMMIT, the ESF Research Network Program ELIAS, the Elite Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), the Netherlands eScience Center under project number 027.012.105, the Yahoo! Faculty Research and Engagement Program, the Microsoft Research PhD program, and the HPC Fund.
REFERENCES
[1] C. M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006.
[2] B. Carterette. System effectiveness, user models, and user utility: a conceptual framework for investigation. In SIGIR '11, pages 903­912. ACM, 2011.

[3] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In WWW '09, 2009.
[4] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In CIKM '09, pages 621­630. ACM, 2009.
[5] O. Chapelle, T. Joachims, F. Radlinski, and Y. Yue. Large-scale validation and analysis of interleaved search evaluation. ACM Transactions on Information Systems, 30(1):6, 2012.
[6] A. Chuklin, A. Schuth, K. Hofmann, P. Serdyukov, and M. de Rijke. Evaluating aggregated search using interleaving. In CIKM '13, pages 669­678. ACM, 2013.
[7] A. Chuklin, P. Serdyukov, and M. de Rijke. Click model-based information retrieval metrics. In SIGIR '13, page 493. ACM, 2013.
[8] C. W. Cleverdon, J. Mills, and E. Keen. Factors determining the performance of indexing systems (Volume 1: Design). Cranfield: College of Aeronautics, 1966.
[9] G. E. Dupret and B. Piwowarski. A user browsing model to predict search engine click data from past observations. In SIGIR '08, pages 331­338. ACM, 2008.
[10] F. Guo and Y.-m. Wang. Efficient multiple-click models in web search. In WSDM '09. ACM, 2009.
[11] F. Guo, L. Li, and C. Faloutsos. Tailoring click models to user goals. In Proceedings of the 2009 workshop on Web Search Click Data, pages 88­92. ACM, 2009.
[12] J. He, C. Zhai, and X. Li. Evaluation of methods for relative comparison of retrieval systems based on clickthroughs. In CIKM '09, pages 2029­2032. ACM, 2009.
[13] K. Hofmann, F. Behr, and F. Radlinski. On caption bias in interleaving experiments. In CIKM '12, pages 115­124. ACM, 2012.
[14] K. Hofmann, A. Schuth, S. Whiteson, and M. de Rijke. Reusing historical interaction data for faster online learning to rank for IR. In WSDM '13, pages 183­192. ACM, 2013.
[15] T. Joachims. Optimizing search engines using clickthrough data. In KDD '02, pages 133­142. ACM, 2002.
[16] R. Kohavi, R. Longbotham, D. Sommerfield, and R. Henne. Controlled experiments on the web: survey and practical guide. Data Mining and Knowledge Discovery, 18:140­181, 2009.
[17] T. Qin, T.-Y. Liu, J. Xu, and H. Li. LETOR: A benchmark collection for research on learning to rank for information retrieval. Information Retrieval, 13(4):346­374, 2010.
[18] F. Radlinski and N. Craswell. Comparing the sensitivity of information retrieval metrics. In SIGIR '10, pages 667­674. ACM, 2010.
[19] F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reflect retrieval quality? In CIKM '08, pages 43­52. ACM, 2008.
[20] M. Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4(4):247­375, 2010.
[21] A. Schuth, F. Sietsma, S. Whiteson, D. Lefortier, and M. de Rijke. Multileaved comparisons for fast online evaluation. In CIKM '14, pages 71­80. ACM, 2014.
[22] J. Xu and H. Li. Adarank: A boosting algorithm for information retrieval. In SIGIR '07, pages 391­398, New York, NY, USA, 2007. ACM.
[23] E. Yilmaz, M. Shokouhi, N. Craswell, and S. Robertson. Expected browsing utility for web search evaluation. In CIKM '10, pages 1561­1564. ACM, 2010.
[24] E. Yilmaz, M. Verma, N. Craswell, F. Radlinski, and P. Bailey. Relevance and effort: an analysis of document utility. In CIKM '14, pages 91­100. ACM, 2014.
[25] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In ICML '09, pages 1201­1208. ACM, 2009.

282

Untangling Result List Refinement and Ranking Quality: a Framework for Evaluation and Prediction

Jiyin He
CWI
jiyinhe@acm.org

Marc Bron
Yahoo! Labs London
mbron@yahoo-inc.com

Arjen de Vries
CWI
arjen@acm.org

Leif Azzopardi
University of Glasgow
leif.azzopardi@glasgow.ac.uk

Maarten de Rijke
University of Amsterdam
derijke@uva.nl

ABSTRACT
Traditional batch evaluation metrics assume that user interaction with search results is limited to scanning down a ranked list. However, modern search interfaces come with additional elements supporting result list refinement (RLR) through facets and filters, making user search behavior increasingly dynamic. We develop an evaluation framework that takes a step beyond the interaction assumption of traditional evaluation metrics and allows for batch evaluation of systems with and without RLR elements. In our framework we model user interaction as switching between different sublists. This provides a measure of user effort based on the joint effect of user interaction with RLR elements and result quality.
We validate our framework by conducting a user study and comparing model predictions with real user performance. Our model predictions show significant positive correlation with real user effort. Further, in contrast to traditional evaluation metrics, the predictions using our framework, of when users stand to benefit from RLR elements, reflect findings from our user study.
Finally, we use the framework to investigate under what conditions systems with and without RLR elements are likely to be effective. We simulate varying conditions concerning ranking quality, users, task and interface properties demonstrating a cost-effective way to study whole system performance.
Categories and Subject Descriptors
H.5.2 [User Interfaces]: Evaluation/methodology
Keywords
Simulation; Search behavior; Faceted search; Evaluation
1. INTRODUCTION
Many of today's enterprises require a dedicated search system, i.e., a particular optimal configuration of both a ranking algorithm and interface, to effectively support their specific type of user, task, and collection. To select this optimal configuration an evaluation metric­capturing a particular user behavior and interface combina-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08$15.00 DOI: http://dx.doi.org/10.1145/2766462.2767740.

tion­together with a fixed set of queries, documents, and relevance judgements, is required to determine the quality of various ranking algorithms [33]. Traditional batch evaluation metrics, however, typically assume that after launching a query, user interaction remains limited to scanning down a ranked result list and stops at some rank k [4]. As interfaces of modern search systems are equipped with additional elements, such as result filters, and users become more actively involved in the search process, the whole system effectiveness no longer solely depends on the quality of the ranking, but also on how the interface elements function, as well as on how users operate these elements [3, 14, 16, 26]. Consequently, batch evaluation metrics for traditional search systems no longer accurately reflect system performance when it comes to the combination of a ranking algorithm and interface.
A key problem, then, is how to choose between systems with varying combinations of interface elements and ranking algorithms. As a first step, in this paper, we present a framework for comparing search systems equipped with a particular class of interface elements, i.e., elements supporting result list refinement (RLR). We define RLR search systems as those that provide: (i) a fixed set of filter values that remain visible to the user at all times; and (ii) the filter values operate on a fixed initial result set for a particular query. For example, a system with minimal RLR elements has a single filter value, where elements of increasing complexity are a list of multiple filter values (keywords/entities), and filter values grouped in categories (facets). A relation exists between facets and RLR elements, however, we do not require filter values to be mutually exclusive, exhaustive [34], or orthogonal [13].
We limit the initial framework to search systems with RLR elements for two reasons. The first is pragmatic: without loss of generality, user interactions with these elements can be modeled as switching between a limited number of different subsets of a result list. It allows this work to go beyond the standard user interaction model in batch evaluation, cf. [4], while remaining tractable. The second is methodological: we wish to focus on user interactions with a class of elements that require users only to recognize a suitable filter value to refine a result list with. This in contrast to, for example, query (re)formulations that require additional mental effort on the part of the user [29], thereby allowing greater variability in user interactions depending on individual user characteristics.
Our evaluation framework for systems with RLR elements has two parts: (i) an evaluation measure specified by a model that characterizes how users interact with systems supporting RLR and a specification of how these interactions are associated with user effort and gain (Section 3); and (ii) a simulation strategy, i.e., an instantiation of the interaction model parameters (Sections 4 and 5).

293

The framework has two immediate applications. (i) Prediction: by obtaining estimates of the parameters of the interaction models from usage data, e.g., from online systems or user studies, system performance can be predicted and evaluated off-line. This allows optimization of systems by varying the ranking algorithm and interface elements, for a particular application and user group. (ii) Simulation: our framework allows us to perform "what-if " analyses, i.e., to investigate system performance under varying conditions by simulating different combinations of ranking quality, interface elements, and type of users. Such simulation results can inform decisions about the type of interface, ranking algorithm, and queries to be used when comparing systems in subsequent user studies.
To demonstrate the efficacy of our evaluation framework we apply it to the comparison of two standard snippet-based search systems, one with RLR elements and one without. Here, our first goal is to examine the accuracy of the framework in predicting user effort when interacting with an RLR system. We instantiate the model parameters of the two systems with user data and then compare the model predictions with real usage data. Specifically, we seek answers to the following questions: (i) does the effort predicted by our framework correlate with user effort on a task with the two systems; and (ii) does comparing simulated system performance allow us to accurately predict when RLR elements are beneficial and when they are not?
Having validated its accuracy, our second goal is to investigate system performance under varying conditions, including different ranking quality, filter properties, as well as user behaviors--and how these factors interplay. In short, we study the question: "when does an RLR-enabled system help to improve user performance?"
2. RELATED WORK
User interaction models are closely related to effectiveness metrics developed for batch-evaluation: users invest effort to operate a system in order to gain relevant information. With some abstraction, a batch evaluation metric can be viewed as a function that makes predictions about user effort and gain based on its user interaction model, i.e., assumptions about how users operate the system, cf. [4]. For example, widely used metrics P@K and NDCG@K employ user interaction models of two different types of user search behavior: P@K assumes users examine all top K results in no particular order, and NDCG@K assumes users examine top K results from top-to-bottom and find lower-ranked results of less value. In both cases, we can interpret the number of documents examined as effort, and the number of relevant documents found as gain.
User interaction with a traditional search interface. Modeling user interaction with retrieved results has become a central topic in recent discussions on evaluation methodology [1]. A wide range of stochastic models have been learned from Web search engine click logs [5, 8, 11, 12]. Common to these models are a few assumptions that have been identified as typical user interaction patterns in Web search. For example, the examination assumption states that users are less likely to view lower ranked results [21]; and the cascade model assumes users browse a ranked result list from top to bottom, and stop once a relevant result is found [9].
Meanwhile, several effectiveness metrics have been proposed in an effort to integrate more realistic user interaction models. For example, the Rank Biased Precision (RBP) [28] models the "persistence" of a user, i.e., how likely a user examines a next result when going down a ranked list; the Expected Reciprocal Rank (ERR) [6] is derived from the cascade model; and Chuklin et al. [7] proposed to turn click models into evaluation measures. Time-biased gain

takes into account user variability in terms of the time needed to process information, e.g., reading summaries [31].
An alternative use of user interaction models is to simulate user search activities, in order to evaluate system performance under various conditions such as "what if users do X?" which are not likely to be investigated by a user study. For example, Smucker and Clarke [31] presented a method to simulate time-biased gain. A shared assumption in the studies listed so far is the traditional ranked list search interface.
Studies that go beyond the traditional single ranked list based user interaction is session-based evaluation [20, 22]. Here, in addition to the examination assumption, an extra user decision--whether to reformulate their query--is modeled (cf. Section 3.1).
Beyond traditional search interfaces. Moving away from the traditional ranked list search interface, Fuhr [14] proposed the interactive probabilistic ranking principle (iPRP) aimed at providing a formal description of user interactions and the corresponding optimization strategy for a system. However, instantiation of this general model for practical use remains an open problem.
More concretely, faceted search is a typical example where user interaction models need to go beyond the assumptions of a ranked list interaction model. Often, simulation based evaluation is employed [23­25, 27, 32]. A key notion shared by these simulation models is "utility" [23, 25, 32]--the trade-off between the effort users spend and the benefit gained, e.g., finding a target document.
Various heuristic user interaction models have been proposed for simulation. These models assume different user goals and how users interact with retrieved results and facets. In terms of user goals, Kashyap et al. [23] assumed users examine all filtered results in the context of database queries. Alternatively, in [25, 30, 32] users were assumed to find only one relevant result. In terms of user operation with facets, Koren et al. [25] assumed that users can always recognize the facet that contains relevant document(s), and select facets in one of the following ways: (i) randomly; (ii) facets with least document coverage; (iii) the first facet that contains the target document; or (iv) the optimal facets. Kong and Allan [24] assumed that users sequentially scan facets and skip an entire facet when they find it irrelevant.
Our work. Our goal is to devise a new evaluation method for systems with a search interface enabling RLR elements. What we need is a user interaction model that is able to characterize not only the traditional "examine a result list" interactions, but also interactions with RLR elements.
We evaluate an RLR system under the same notion of utility (user effort and gain) as in the above studies in faceted search. Our work differs in two important ways. First, we model user interactions with RLR elements (including facets) in a more natural way--users scan filtered results without a particular order; and they may and may not recognize a "good" filter value. Second, we do not make explicit assumptions to create categories of users (like in [25]). The variability of users is captured by a probabilistic framework: by varying two model parameters, we are able to simulate a wide range of users. This second property of our model allows us to encompass empirical user interaction models developed for traditional search interfaces, i.e., to fit the model with real usage data and make predictions of system performance with respect to a particular group of users/search tasks.
3. MODELING RLR INTERACTIONS
Our evaluation framework has the following components: (a) a user interaction model that characterizes how users interact with a system that enables result refinement (Section 3.1); (b) associating

294

effort and gain to user interactions for evaluation (Section 3.2); and (c) integration of the above two components (Section 3.3).
We assume that users perform actions to make progress on a search task (e.g., inspect results); every action costs effort; and the user may gain from that action by finding relevant information. With an interaction model, we simulate and predict action paths of users during a search task, which vary across users and search tasks, and are influenced by the quality of the result lists. By associating effort and gain with different paths of search actions, we are able to predict user effort given different types of users, tasks and the quality of result lists.
Overall, the effectiveness of a system can be measured by answering questions such as: How much effort is required to achieve x amounts of gain with system A, as compared to system B?
3.1 User interaction at a conceptual level
User interaction with a basic interface. With a basic search interface, the common assumptions are: users browse a result list from top to bottom; and after examining each result, they make a decision--whether to continue to examine another result, or to give up this result list [12, 15].
User interaction with an RLR-enabled interface. With an RLRenabled interface, apart from examining results in the retrieved ranked list, users may choose to refine the result list by filtering on a particular value. A typical consequence of these RLR interactions is that users switch between different filtered versions of the original result list. We refer to these different versions of the result list, including the original ranked list, as sublists.
Without making assumptions about the specific implementation of these elements, at a functional level, we can model the user interactions with RLR elements as selecting a sublist.
This leads to at least two additional decisions a user needs to make: (1) continue with the current sublist, switch to a different sublist, or quit searching? and (2) if switching, which sublist to select next?
Parameterization of user interactions. Each of the decision points introduces uncertainty in computing user effort and gain during a search task: it is at these points users diverge from each other's action paths. Taking these decision points as variables of our interaction model allows us to capture variability in user behavior.
Specifically, to quantify the uncertain nature of user decisions, we model the outcome of the aforementioned decisions as random variables following specific distributions:
· Continuation decision: we model the decision of user u at rank r to examine the next result in the same result list as a binary variable sr,u  Ber(pr,u). While it looks similar to the persistence probability [28], we do not make the i.i.d. assumption about the continuation behavior at each rank. That is, in [28], a single persistence probability p is shared by results at all ranks; and the probability that a user examines the result at rank r is pr-1. The Bernoulli parameter pr,u in our model, however, is specific to a rank r and a user u, thus leaving more flexibility for setting different hypothesized values for simulation or fitting empirical parameters from log data.
· Switching decision: similarly, we model the decision of user u at rank r to switch sublists as a binary variable lr,u  Ber(plr,u) with parameter plr,u specific to a user and a rank.
· Sublist selection decision: we model user decisions on sublist selection among K candidates as a vector of binary random variables following a categorical distribution f  Cat(K, cu). Sublist k is chosen if fk = 1; 0 otherwise; and only one list is chosen at a time. The parameter cu determines the likeli-

hood that a list is chosen by a user, e.g., according to his/her perception of the quality of the sublists.

These three probabilities can be set to empirical values estimated from usage logs (Section 4), or based on hypotheses about their values (Section 5). Of course, users may decide to quit searching. However, as quitting is complementary to the continuation and switching decisions, there is no need to explicitly define it.

3.2 User actions, effort, and gain
With the conceptual user interaction model in place, we now specify how we can associate effort and gain to user interactions.
User actions. We consider 3 types of action:
· Examine result: Users examine a result to determine its relevance, by inspecting the title, summary, or the document content. While these introduce great variety in the effort needed for an examination, we consider them as a single action of constant cost. We wish to focus on factors that change the result list, e.g., pagination or filtering, which may lead to more examination activities. However, it is straightforward to incorporate fine-grained levels of user interactions with our model, which we leave as future work.
· Pagination: While not explicitly modeled as a decision, a user will need to paginate if he/she decides to examine a result which is on a next page. Effort required by pagination is directly related to result list quality.
· Select candidate result list: This activity involves a series of mental activities such as estimating which sublist is more likely to contain relevant information, e.g., by inspecting the filter names. As with result examination, we abstract away the details and treat the whole process as a single action.

Effort of actions. Each action is associated with an amount of effort. Let A be the possible actions users can perform, and Pa =
a1, ..., at be the action path of a user that starts browsing results of
q until he/she stops, ai  A. The user's effort along Pa is

E(q, Pa) =

t i=1

wiai,

(1)

where wi is the effort needed for action ai. Effort can be implemented in different ways. For example, with
NDCG or P@10, the only actions considered are "examine" a document, and each costs a unit effort. With time-biased gain [31], effort is implemented in a more elaborate way, e.g., the effort required for an "examine" may depend on the document length.

Gain of actions. We assume that user gain is determined by the relevant documents they encounter. Let DPa be the documents a user encounters along action path Pa; its total gain is

G(q, DPa ) = dDPa rel(d, q),

(2)

where rel(d, q) is the relevance judgement of d w.r.t. the query q. This approach to "measuring" effort and gain is of course closely related to the cumulative gain type of evaluation measures [2, 19].

3.3 User action paths
The final ingredient that ties together the above components for measuring user effort/gain when interacting with an RLR system is the user action path Pa. Assuming users examine documents in a ranked list from top to bottom with a basic interface, the order in which users examine documents is deterministic. The only uncertainty is that users may quit, which can easily be handled by computing the gain at a cut-off point, or at an expected search depth [28]. However, with an RLR interface, the possible paths users can take for a query is combinatory given that users can switch

295

between sublists without a particular order. Thus we resort to a Monte Carlo method. The interaction model specified in Section 3.1 allows us to simulate possible user action paths for a given set of parameters, creating a sample of user effort and gain for a particular task and user behavior. System performance can then be compared with these samples using standard statistical tools.
Action path constraints. To further reduce the complexity of the simulation process, we constrain the possible user action paths with the following assumptions.
A1 Users examine results in a ranked list from top to bottom. A2 When switching between sublists, users skip and only skip the
results they have already seen. This prevents inflated counts of user gain (e.g., in terms of relevant documents encountered). A similar design choice has been made in the literature for evaluating faceted search systems [30]. In addition, when switching, users always examine the next (unseen) result in the new sublist, preventing an infinite switching loop. If all documents in a sublist are exhausted then its filter value is no longer selected. A3 Instead of assuming a user quits searching with a certain probability, we assume a deterministic cut-off, leaving two complementary decision points with continuation probability being sufficient. Practically, for gain-based measures, the cut-off is based on a fixed amount of effort, and systems are compared in terms of the amount of gain achieved for a fixed amount of effort (cf., NDCG@K). For effort-based measures, a cut-off can be set to gain and systems are compared in terms of the amount of effort needed to achieve a fixed amount of gain (cf., expected search length).
Steps to simulate a user action path.
1. Specify model parameters (with empirical or hypothesized values), i.e., pr,u for continuation decisions, and cu for sublist selection decisions.
2. At each step, draw sr,u  Ber(pr,u). If sr,u = 1, add examine to Pa; else draw f  Cat(K, cu), add select sublist and examine to Pa. When encountered, add pagination and handle situations specified in A2.
3. Stop when: (1) the total effort/gain meets a predefined cut-off value; or (2) all results are exhausted.
4. VALIDATION OF PREDICTION
The core of our framework consists of a specific mapping y^ = h(x), where y^ is the estimated user effort, aimed at approximating the actual user effort y, given the input variables x. In a typical machine learning scenario h(·) would be selected from a pool of possible hypotheses by fitting example pairs of y and x. In contrast, we have specified in advance a single hypothesis h, i.e., the interaction model motivated in Section 3, and the values of x is determined by specific types of user behavior. Here, we validate our hypothesis h by examining how well its output, y^, approximates actual user effort y.
In this section, we validate our model using usage data gathered for an RLR interface. Specifically, we calibrate our interaction model parameters (pu,r and cu) with empirical values derived from the usage log of a particular group of users (i.e., participants of our experiment), and examine whether the model prediction corresponds to the actual user effort as recorded in the log. We aim to answer the following questions:
Q1 Does the predicted effort (y^) correlate to user effort (y) as computed with usage data? Q2 Can we accurately predict when an RLR interface is beneficial, compared to a basic interface?

4.1 Obtaining usage data
4.1.1 Test collection
We use the TREC Federated Search data [10], for it has two important properties: (1) all document-query pairs in this collection have been assessed. This provides us with a more accurate estimation of system performance; and (2) this collection has been constructed by querying a large number of web search engines all of which are categorized. These categories can be converted to filter values in an RLR system. The complete list contains 24 categories [10, Table 3], including academic, travel, etc.
The collection contains 50 (judged) test topics, their associated web pages, and the summaries (snippets) to represent these pages. We create rankings for each topic based on a standard query likelihood model as implemented in Indri.
We consider two types of system: one with a basic interface, and the other with an RLR interface, where categories are used to construct sublists for each topic. We treat every document as being annotated with the category of its source search engine. Since an engine can be in multiple categories, and documents may have been retrieved by multiple engines, every document is associated via its source(s) to one or more categories.
Relevance judgements for the Federated Search track are graded, 4 levels from highly relevant to non-relevant. We only distinguish between relevant (level 1­3) and non-relevant to ensure more than 10 relevant documents per topic are available (see Section 4.1.3).
4.1.2 Search interfaces
RLR interface. Fig. 1 shows a screenshot of the RLR enabled interface, where numbers 1­6 indicate components of the system. On the left are the filter values (1) as provided by the federated search track [10]. On the right a dashboard (2) is available indicating the number of clicks left for a task and the number of relevant documents found. After 25 clicks a "give up" button (3) would appear providing the option to skip the remainder of the task. The topic description (4) is available at the top of the screen. An additional button allows users to expand the description and review the examples as provided before starting a task. The middle of the screen is devoted to a scrollable result list (5) with 10 snippets. At the bottom of the page a pagination button (6) is available. See [17].
Basic interface. The basic interface is similar to this design except that the filter panel (1) is unavailable. One of the filter values in the RLR interface is "All," which is the unfiltered list as it would be in the basic interface. It is possible for users to ignore the filter values and use the RLR interface exactly as the basic interface.
4.1.3 Setup of user experiment
We investigate the effort it takes users to locate relevant documents. While our interaction model can be applied to both effortbased and gain-based measures, in this study design we focus on measuring effort.
User task. Users were asked to find 10 relevant documents for a topic. It is not as trivial as, e.g., finding 1 relevant document, allowing variability between user action paths. Meanwhile, it limits the effort required to a manageable amount.
Specifically, we asked users to locate by clicking on 10 result summaries of relevant documents within 50 clicks, where a click is counted if it is on a result summary, a pagination button, or a filter value. We use the 50-click limit to prevent users from clicking every result and to force users to make conscious choices instead. We require users to only click on summaries to abstract away from actions such as opening and reading documents as well as to keep the

296

Figure 1: The RLR interface: (1) filter values; (2) dashboard; (3) give up button; (4) topic description; (5) result list; (6) pagination. The basic interface excludes (1).
time necessary to complete a task manageable. To reduce user variability in judgements of relevance, we provide feedback to users whether a clicked result is relevant or not.
Experiment design. We recruited participants via university mailing list and social media. We used a standard between-subject design common to A/B testing, where each new user is randomly assigned to one of the two interfaces and directed to the same interface on subsequent visits. To reduce learning effects, new tasks are randomly assigned to users.
4.1.4 Obtained usage data
In total 145 task instances were completed by 49 users for the system with the basic interface, and 255 by 48 users for the RLR interface. The median number of completed task-instances per task is 2 for the basic interface and 3 for the system with an RLR interface. As some tasks have been completed by more participants than others, we consider median values in our analysis.
4.2 Measuring user & predicted effort
To answer Q1 and Q2, we need to compute the following quantities: (i) user effort y (with basic or RLR interface) as derived from the usage data, and (ii) the predicted effort y^, where the model parameters are calibrated with the actual usage data.
4.2.1 User effort
For simplicity we assume equal effort for user actions. We measure user effort (y) as the number of result summaries users visited and the clicks they spent on choosing filter values and pagination.
To determine which summaries a user has visited we consider mouse hovers over results, which has been shown to correlate with eye-gaze [18]. Fig. 2(a) shows the percentage of total hovers over the 10 ranks of each result page. We observe that the distribution of hovers over the ranks is relatively uniform, i.e., compared to the distribution of hovers over ranks on Web search engine result pages where differences of 38% between the highest and lowest rank are observed [18]. We observe a difference of 3% between the highest and lowest rank for the basic interface (back bar), and 6% for the RLR interface. That the skew is slightly stronger for the RLR interface is expected as not all filter values return 10 results. The hover data suggests that participants tend to visit all result summaries on a page. This is by design as in our systems we reduced the effort needed to judge a result page (i.e., read summary, open/read the page) to judging the result summary. We are therefore able to focus on user effort as introduced by interaction with RLR elements and not by pages and result summaries.

(a) Distribution of hovers over (b) Probability of rank visit for the 10 ranks for basic (black) basic (black) and RLR (red and RLR (white hatched). hatched) interface.
Figure 2: Users' result page rank visiting behavior.
Given these observations, we approximate the number of result summaries visited as follows. We assume that if a user does pagination, they have seen all the results in the previous page and count all results in SERPs before the last visited page as visited. On the last page, we count the number of results up to the last clicked result.
4.2.2 Predicted effort
So far we have obtained user effort y by directly counting the number of actions recorded in the usage log. To compute y^--an approximation of y of our user group--we calibrate our model parameters with empirical values derived from the same log data.
Continuation probability. We compute pr,u using the empirical distributions of the search depths aggregated from all users. The probability that a user will examine the result at rank r is computed as the number of times r has been visited, normalized by the maximum number of times a rank has been visited. In terms of the number of results a user has visited, we take the same approach described above (Section 4.2.1). Fig. 2(b) shows the probability of visiting each rank for the basic and RLR interface. Under the assumption that participants visit all result summaries on a page, ranks 1­10 are visited an equal number of times.
Sublist selection probability. To model user preferences of sublists (filter values), we collect the counts of filter value clicks for each query, and set these as the parameter cu. That is, the expected value of the probability a filter value will be chosen is proportional to how often it is chosen by the users. Since the original result list, i.e., the filter value "All," is always shown to the users as a starting point, we always add 1 count to it.
4.3 Predicted vs. user effort
We simulated a sample of 1000 y^r for an RLR interface. To answer Q1, we perform a correlation analysis between the median of predicted effort (y^r) and the median of user effort (yr) over the 50 topics. We see a significant linear correlation between the two (Fig. 3): Pearson  = 0.79 (p < 0.001). This suggests that our proposed model can be used to reliably predict user effort needed in accomplishing a search task in terms of the number of results visited and the number of filter values they need to explore.
4.4 Predicted vs. user benefit
Next, we investigate how the predicted effort can be used to compare system effectiveness, e.g., between a system with a basic interface and with an RLR interface (Q2). To proceed, we investigate whether and on which topics an RLR interface reduces the effort needed to complete the task, as compared to a basic interface.
We compute two quantities for each topic (see Fig. 4): 1. User difference: effortu = yb - yr 2. Predicted difference: effortp = yb - y^r

297

Figure 3: Correlation between estimated effort and user effort.
Note that yb (effort spent with the basic interface) is fixed as the action path of a user with a basic interface is deterministic.
In terms of user difference, we observe that on 31 out of 50 topics less user effort is needed with the basic interface (effortu < 0); on 16 topics effort is less with the RLR interface and on 3 there is no difference. Of the 31 topics where effortu < 0, on 14 a user is able to save more than 10 actions (points of effort), e.g., paginating to a next page and scanning 10 results. Of the 16 topics where effortu > 0, on 11 a user is able to save more than 10 actions.
On 34 out of 50 topics, effortp agrees with effortu, as in, which interface would reduce user effort in completing a topic. All cases of disagreement are on topics where users of the RLR interface spent more (or the same) amount of effort than users of the basic interface. In these cases participants may have struggled to effectively use filter values or did not use filters at all. If participants did effectively use filters, our interaction model predicted that using filters would save effort.
We observe that our model is able to identify 100% of the cases when an RLR interface is better (i.e., costs less effort) than a basic interface. However, when it predicts that an RLR interface is better, it is only correct in 52% of the cases (0.68 F1-measure). In contrast, it is able to predict when a basic interface is better with 85% precision and 55% recall (0.66 F1-measure). This suggests that if a user would benefit from using the RLR interface, then the model will predict so, and if our model indicates that the basic interface is more beneficial, use of the RLR interface should be avoided.
Further, we find that y^r has significant negative correlation with effortu:  = -0.49, p < 0.001, confirming its predictive power. A negative correlation means that the higher the effort needed with an RLR interface predicted by our model, the more likely that a basic interface is better. The stronger this correlation is, the better the model is at predicting which interface is beneficial.
Figure 4: The black bars show effortu, and the white (hatched) bars show effortp. A negative value indicates more effort is spent with the RLR interface. Difference values exceeding 100 or less than -100 have been cut-off for legibility.

5. WHOLE SYSTEM PERFORMANCE
In Section 4 we have demonstrated that our user interaction model is simple, tractable, and able to accurately predict user effort with empirical parameter settings. We observed that an RLR interface can be useful for some queries while the basic interface is good for others. Many factors may contribute to this observation, ranging from system properties (backend as well as UI) to user properties. However, it is difficult to investigate the exact impact of these different factors with user studies: a large number of experiments are needed given the number of factors considered and their combination; and it is difficult to control user behavior.
In this section, we discuss how our evaluation framework can be used for studying whole system performance, under strictly controlled and varying conditions that may not be attained in real life studies. We use the same test collection as before, and focus on one question: Q3. When does an RLR interface help?
By instantiating our interaction model with parameter values designed to reflect user behavior with desired properties, we generate simulated usage data of large quantity and under strict control. We then study whether and how various factors (and their interactions) affect the advantage of an RLR interface versus a basic one.

5.1 Simulating user browsing behavior
To start with, we describe how we instantiate the interaction model with parameters that characterize different user behaviors.
Examination depth on a ranked list. Users do not visit all documents in a ranked list. The common examination assumption [9] states that the deeper a user investigates a ranked list, the less likely they are to continue examining the list's next document; consistent with the probability ranking principle, deep down the ranked list we expect IR systems to return fewer relevant documents.
In our interaction model, at rank r, users decide either to continue examining another document, or to switch to a different sublist (cf. A3). The assumption that users are more likely to switch when they move deeper down the ranked list can be captured by controlling the parameter pr,u of the Bernoulli distribution with an exponential decay function:

pr,u = e-r,

(3)

where  controls the decay rate. That is, a user with a larger  would decide to switch list at an earlier rank. The resulting exponential decay (Fig. 5(a)) is a good fit for Fig. 2(a).
We simulate the continuation decision of a user with u in the following steps:
1. Compute pu,r with Eq. 3 given u;
2. Draw a decision su,r  Ber(pu,r).
Accurate and inaccurate users. When switching between sublists, some users make better choices than others. A good decision leads to a sublist with many (unseen) relevant documents ranked near the top, potentially reducing the total effort the user needs for

9LVLWSUREDELOLW\ ./'LYIURPRUDFOH







)d 

)(3d



)(3(x





)(3(d



6PRRWKHG





5DQGRP



        

5DQN

$PRXQWVRIVPRRWK

(a) Probability of reaching rank r (b) Amounts of smooth vs. KL-

given different .

divergence from the oracle.

Figure 5: Illustration of parameter setup

298

his/her search task. We simulate users with different levels of accuracy by varying the sublist selection probability (cf. Section 3.1).
Specifically, we sample cu from its conjugate prior distribution, i.e., a Dirichlet distribution Dir(u), with hyperparameters u. By setting u to different values we can simulate users with different types of prior knowledge. E.g., users who do not have a clue which sublist to choose can be simulated by setting u to a uniform distribution; an "accurate" user can be simulated by setting u proportional to the quality of the list, e.g., as measured by NDCG. The properties of the Dirichlet distribution ensure that the expected value of cu,k is k/ j j, i.e., proportional to the performance of the sublists.
In summary, for a given u, we simulate a user's choice of sublist with the following procedure:
1. Draw cu  Dir(K, u);
2. Draw the decision vector (f1, . . . , fK )  Cat(K, cu).
Influence of user behavior on search performance. Before simulating system performance under varying conditions, we conduct a sanity check to examine how the above parameter settings influence estimated user effort given the following setup.
User task We consider an information gathering task where users aim to collect 1, 10, or all relevant documents;
Examination depth We set   {1, .5, .1, .05, .01, .005, .001} to reflect different user examination depths on a ranked list.
User accuracy We consider two cases: a uniform prior u,k = 1/K, and a prior biased on list quality, i.e., k is set to the NDCG value of the corresponding result list.
User effort and gain As before, we assume equal effort for all actions and binary relevance to compute gain.
Given the possible settings above, we run the simulation over all combinations of these parameters, each for 1000 times.
Fig. 6 shows the simulation results. We plot the median of simulated user effort with two different user accuracy priors: NDCG vs. Random. With each prior, the continuation probability is set to different values with varying . From the figure we observe: (i) in all cases, irrespective of the value of  and the task type, good prior knowledge about which sublist to choose is beneficial. A uniform prior corresponding to random selection always leads to more effort. (ii) The continuation probability has a limited effect when fewer relevant documents need to be found. When more relevant documents need to be found, e.g., in the Find-all task, it is better to go deeper down a ranked list (i.e., small ). These observations are intuitive and provide a sanity check on the simulation results using our user interaction model and the proposed simulation strategy.
5.2 Analysis method
Next, we describe the method we employ to analyze conditions when an RLR interface is likely to be beneficial and when it is not.
5.2.1 Factors influencing RLR effectiveness
We identified the following factors that, presumably, determine whether an RLR interface is preferable over a basic one.
Query difficulty for the basic interface (Dq). A priori, if the ranked list in a basic interface is good, i.e., with all relevant documents on top, then users do not need to switch to other sublists for their tasks. We use the effort users need to accomplish a task with the basic interface (effortb) as the indicator of Dq. The higher effortb, the more difficult the query is for a basic interface.
Sublist relevance (Rq). The effectiveness of an RLR element should depend on the quality of the sublists created for result refinement.

If the sublists would filter the relevant documents that were buried deep down in the original ranked list, then it is likely to help users to accomplish their tasks faster. We compute Rq as the averaged NDCG scores over the sublists of a query.
Sublist entropy (Hq). A priori, if few sublists cover most of the relevant documents, these could help to effectively filter out irrelevant documents. Meanwhile, if many sublists contain many relevant documents, then it may be easy for users to find them. In short, we believe the effectiveness of an RLR system is related to how the relevant documents are distributed among sublists, but the exact relation is yet to be explored.
We compute Hq = - i pi log(pi) as the sublist entropy of a query q, where pi is the probability that the sublist i of query q contains relevant documents, derived from the empirical distribution of the relevant documents among the sublists of q.
User accuracy (Ulevel). As shown in Fig. 6, how users choose the sublists makes a big difference on the effectiveness of using RLR. Following the distribution of NDCG scores of the sublists is a good strategy, while choosing randomly leads to inferior performance.
Here, we aim to investigate the impact of user accuracy at a more refined level, i.e., how accurate should the user be in order to make the RLR work? Recall that user sublist selection behavior is controlled by the parameter . Assuming user choices following NDCG scores are "oracles," by gradually smoothing out  with respect to this oracle distribution, we can create user accuracies of different levels between the oracle and the complete random choices (i.e., uniform ). Fig. 5(b) shows the relation between amounts of smoothing added and the median of the KL divergence of the "new" user from the "oracle" user over the 50 test topics. We create 4 user levels, with the amount of smoothing set to 0, 0.1, 0.5, and 1.0, corresponding to the oracle user (level 1), and approximately 15% (level 2), 50% (level 3), and 67% (level 4) less accurate users.
User task. Intuitively, the impact of these factors would be different with respect to different user tasks. For example, when the task is to find all relevant documents, it is not important how good the top of the ranked list is, but rather, where the last relevant document is located. We consider finding 1, 10, and all relevant documents.
As a final note, in Fig. 6 we have observed the influence of user search depth on the RLR performance. Overall its impact is not as obvious as user accuracy, in terms of the magnitude of changes in efforts it leads to. In this analysis, we focus on the sublist selection aspect of the users and fix  to 0.01, which seems to be optimal for finding all relevant documents. For the other two tasks it does not seem to make a major difference when set to a different value.
5.2.2 Analyzing the impact of the identified factors
Let effort = effortb - effortr be the difference between the effort needed to complete a search task with a basic interface and that with an RLR interface. We then make effort > 0 a dependent variable (DV), which takes binary values (1 as yes, 0 as no), and the above four factors (Dq, Rq, Hq, and Ulevel) as independent variables (IVs). Our goal is to investigate how each of the IVs, and their interactions, influence the outcome of whether or not an RLR interface improves over a basic interface.
We apply a generalized linear model (GLM) for this purpose, which allows us to analyze how the IVs contribute to explain the variance observed in the DV. Specifically, given that effort > 0 is a binary variable, we take the form of a logistic regression model. We fit the models with the data simulated with parameters set to different user tasks and different Ulevels. The conditions w.r.t. the rest of the variables Dq, Rq, and Hq are determined at a per-query level, which comes with the test collection.

299

Effort Effort Effort

35

Random

30

NDCG

25

20

15

10

5

01 0.5 0.1 0.05 0.01 0.005 0.001 
(a) Task: find - 1

200
Random

150

NDCG

100

50

01 0.5 0.1 0.05 0.01 0.005 0.001 
(b) Task: find - 10

1200 1000 800 600 400 2001

Random NDCG
0.5 0.1 0.05 0.01 0.005 0.001 
(c) Task: find - all

Figure 6: Influence of user patience and their accuracy in choosing result lists. The line curves represent the median of the simulation results, and the shades indicate their upper and lower quartiles.

To determine which IVs and interaction terms should be included in the model, we conducted model selection based on the Bayesian information criterion (BIC) using both forward and backward selection. Further, we expect that for different types of tasks (e.g., Find-1 vs. Find-all), the importance of these factors would differ dramatically, and different models would be appropriate. Therefore, we fit a model for each of the user tasks individually.
5.3 When does RLR help?
Table 1 shows the parameters for models best able to predict whether the RLR interface will be effective based on combinations of the four factors for each task.
5.3.1 Main effect
We observe that for Find-1 none of the main factors in the model have a significant effect on the dependent variable (RLR effectiveness). However, for Find-10 and Find-all tasks we see that the Ulevel has a significant effect. The negative coefficients for the Ulevel variables indicate that, as users deviate from the "oracle" sublist selection behavior, the log-odds of the RLR interface being effective decrease. Those user level effects for Find-10 and Find-all but not Find-1 suggest that if a user's task is to locate 1 document, then just

Table 1: Estimated coefficients of the selected models and their effects on the odds that an RLR interface helps. The overall effect of user accuracy (ulevel) is tested by Wald test. The model goodness-of-fit (GOF) is tested by Hosmer-Lemeshow test. Significance codes: 0.001 ( ); 0.01();  0.05().

Coefficients

Find-1 Find-10

Find-all

(Intercept) Dq Ulevel2 Ulevel3 Ulevel4 Hq Rq Dq : Ulevel2 Dq : Ulevel3 Dq : Ulevel4 Dq : Hq Dq : Rq Hq : Rq Dq : Hq : Rq
Overall effect of Ulevel
Model GOF (p-value)

-7.3401 0.1058 3.2234 1.5590 -2.3189 -1.0443
­ -1.6547 -2.0041 -2.0683 1.3103
­ ­ ­
2=1.6 (df=3)
0.9339

-10.4365 -0.0686 -2.1309 -5.5278 -8.1936 3.6347 -49.7916
­ ­ ­ -0.0968 3.2363 13.9680 -0.8422
2 = 16.0 (df=3)
0.9928

-0.5337 0.0017 -5.1061 -8.0140 -8.0140 -1.6492 114.9398
­ ­ ­ ­ 0.0906 -57.2773 ­
2 = 25.6 (df=3)
0.9213

the accuracy with which users select sublists is not enough to predict whether an RLR interface will be beneficial. One explanation is that, since most sublists will have at least one relevant document ranked highly, users do not need to be accurate in their choice of sublist to achieve the task. When collecting more relevant documents however, knowing which sublist to pick is important.
Regarding Rq we find that it has a significant effect only for Find-all. As the average relevance of sublists increases the logodds of the RLR interface being effective increase as well. That is, having sublists with relevant documents ranked high is essential for the RLR interface to be effective for the Find-all task. For the Find-1 and Find-10 tasks sublist relevance alone is not enough to predict RLR effectiveness and the effect depends on the interaction between two or more of the main factors. We look into these interaction terms in more detail next.
5.3.2 Effect of interaction terms
To investigate the effect of the interaction terms on the probability of RLR effectiveness, we express the relation visually. Due to space limitation, we focus here on the model for the Find-10 task.
To visualize interaction terms between continuous variables, we plot the predicted value of the DV against one varying IV, and recenter the other IVs to a fixed level. We take the 25% and 75% quantile of the values of a variable as its low and high level, respectively. A model fitting the re-centered data then shows the effect of the varying IV on the DV with respect to the different levels of the re-centered IVs. For Find-10 there are two interactions terms that have significant effect: Dq:Rq and Dq:Rq:Hq. For Dq:Rq, we recenter Dq to its low and high levels, and fix Hq to its median. For Dq:Rq:Hq we re-center both Dq and Rq to a low and high level.
Fig. 7(a) shows the effect of increasing Rq on the probability P (effort > 0) when Dq is high for varying levels of user accuracy. At relatively low levels (0.15 to 0.25) of Rq there is a steep increase in the probability of RLR being more effective for all user levels. This suggests that when a query is difficult (i.e., the quality of the original ranked list is low), the sublists and the users do not need to be very accurate for an RLR interface to be more effective than a basic interface. In Fig. 7(b) we see that when query difficulty is low, high quality sublists (relevant documents ranked high) and higher user accuracy are necessary for an RLR interface to be helpful.
The relation between Dq, Rq, and Hq in the final interaction term for task Find-10 is shown in Fig. 8. It shows the effect of increasing Hq on P (effort > 0) under conditions in terms of combinations of different levels of Dq (high/low), Rq (high/low), and varying levels of user accuracy. We see that a high level of Dq combined with low/medium levels of Hq result in a relative high P (effort > 0) for both high (Fig. 8(c)) and low (Fig. 8(a)) levels of Rq. This interaction aligns with our intuition of when sublists

300

3UREDELOLW\WKDW5/5KHOSV



OHYHO



OHYHO



OHYHO

OHYHO





        6XEOLVWUHOHYDQFH
(a) Dq: high; Hq: median

3UREDELOLW\WKDW5/5KHOSV



OHYHO



OHYHO



OHYHO

OHYHO





        6XEOLVWUHOHYDQFH
(b) Dq: low; Hq: median

Figure 7: Effect of interaction terms query difficulty : sublist relevance for task Find-10.





3UREDELOLW\WKDW5/5KHOSV

3UREDELOLW\WKDW5/5KHOSV





 OHYHO



OHYHO



OHYHO

OHYHO

 







6XEOLVWHQWURS\

(a) Dq: high; Rq: high

 OHYHO



OHYHO



OHYHO

OHYHO

 







6XEOLVWHQWURS\

(b) Dq: low; Rq: high

3UREDELOLW\WKDW5/5KHOSV

3UREDELOLW\WKDW5/5KHOSV





 OHYHO



OHYHO



OHYHO

OHYHO

 







6XEOLVWHQWURS\

(c) Dq: high; Rq: low





 OHYHO



OHYHO



OHYHO

OHYHO

 







6XEOLVWHQWURS\

(d) Dq: low; Rq: low

Figure 8: Effect of interaction terms query difficulty : sublist relevance : sublists entropy for task Find-10.

are beneficial, i.e., having a single high quality sublist when the original ranked list is of low quality. When Dq is low, we observe that P (effort > 0) decreases at low levels of Hq when Rq is low (Fig. 8(d)) and at medium levels of Hq when Rq is high (Fig. 8(b)). This suggests that at lower levels of query difficulty very specific conditions need to be met for an RLR interface to be beneficial.
Hq plays a role in different interaction terms depending on the task as well (cf. Table 1). For Find-1 we find that as both Dq and Hq increase the log-odds of the RLR interface being beneficial increases. Since the task requires a single relevant document, the ranking within the vertical is less important. Having more sublists with a relevant document allows users to complete the task effectively with the RLR interface even when users select sublists randomly. For Find-all, an increase in both Hq and Rq result in a decrease in the log-odds of the RLR interface being beneficial. As we saw in Fig. 8, for Find-10 high sublist entropy results in low probability of an RLR interface being helpful. Sublist relevance only plays a role when query difficulty is low; however, as the number of relevant documents to be found increases, it is less likely that enough documents are available at the top of the ranking.
5.3.3 Summary
Query difficulty alone is not a good predictor for the probability of an RLR interface being helpful. Depending on the task, different factors determine whether users will be more effective with the basic or RLR interface. In the case of Find-1, sublists do not have to be of high quality for an RLR interface to be helpful; it becomes

more likely to be beneficial when the query is difficult and the entropy of the sublists is high. For Find-10, high query difficulty and low entropy are conditions for the RLR interface to be beneficial. The importance of sublist relevance depends on user accuracy; when they are accurate, lower levels of sublist relevance are necessary. For Find-all, the conditions necessary for an effective RLR interface are high query difficulty, high user accuracy, high sublist relevance and low entropy.
6. RELATION TO TRADITIONAL METRICS
We have illustrated how our evaluation framework can be used for simulating and predicting RLR system performance in two ways, and its efficacy has been validated with usage data. Next, we discuss how it relates to metrics for evaluation of traditional search systems, i.e., normalized Discounted Cumulative Gain [19, nDCG], Expected Reciprocal Rank [6, ERR], normalized Rank-biased precision [28], average precision (AP), and precision@10 (P@10).
From a modeling perspective, Carterette [4] has proposed a conceptual framework for analyzing and comparing different effectiveness measures (for traditional systems). Our framework is close to the category of Model 3 under his classification, i.e., computing the effort a user needs to achieve a particular amount of utility. Further, all metrics discussed in [4] compute an expected value (utility, effort or gain). Computing the expected performance directly is rather intractable in our setting, as the order in which sublists are selected and the number of results viewed in each of these lists is nondeterministic. To compute the expected performance, one needs to obtain the distribution of all possible orders in which sublists are selected. Here, simulation provides samples of possible sequences from which performance can be approximated (Section 3.3).
We now move on to an empirical investigation. By examining to what extent metrics for traditional systems are able to predict the performance of an RLR system, we investigate whether our framework offers new insights. Table 2 shows the correlation between traditional measures and actual user effort (column 1, 2; obtained in Section 4), predicted user effort (column 3, 4), and the difference between user effort with the basic and RLR interface (effortu in column 5, 6). We observe that nDCG measured at low cut-offs (10) has no significant correlation with user or predicted effort. When all relevant documents are taken into account (nDCG@all), the correlation is significant at  = -.42 (negative, for gain vs. effort). We observe a similar pattern for binary nDCG (BnDCG); however, in this case the correlation is stronger than for nDCG. When collecting usage data we did not distinguish between highly relevant and relevant documents. Other measures have a negative correlation with user effort and our model in the range between nDCG and BnDCG. We focus on BnDCG here, as it is most strongly correlated.
The correlation between user effort and BnDCG@all is  = -.72, which indicates that for topics with high BnDCG@all scores, i.e., with many relevant documents at the top of the ranking, user effort is low. The magnitude of the correlation of both BnDCG and our model with actual user effort is high. The negative correlation between BnDCG@all and our model is lower than that of either measure with user effort ( = -0.59, p < 0.001), indicating that these measures disagree on the effort needed for some topics.
We apply BnDCG@all to the task of predicting whether a topic would cost a user more effort with the basic or the RLR interface (effortu). We compute the correlation between BnDCG@all and effortu. Results are listed in Table 2 (last two columns). There is no significant correlation ( = 0.04, p = 0.776): BnDCG@all cannot differentiate between topics suitable for a basic or RLR interface. In comparison, the correlation of effortu with the pre-

301

Table 2: Correlation of traditional metrics with user effort, predicted effort, and the effortu (cf. Fig. 4).

user effort simulated effort effortu

measure

 p-value  p-value  p-value

nDCG@10 nDCG@all NRBP ERR@10 P@10 AP BnDCG@10 BnDCG@all
our model

-0.21 0.142 -0.42 0.002 -0.41 0.003 -0.45 0.001 -0.56 <0.001 -0.63 <0.001 -0.54 <0.001 -0.72 <0.001 0.79 <0.001

-0.19 -0.34 -0.33 -0.36 -0.46 -0.54 -0.44 -0.59
­

0.185 0.016 0.018 0.010 <0.001 <0.001 0.001 <0.001
­

0.02 0.896 0.00 0.994 0.08 0.568 0.08 0.567 0.00 0.980 0.02 0.875 0.02 0.875 0.04 0.776
-0.49 <0.001

dicted effort (by our model) is significant ( = -0.49, p < 0.001). That is, simulated effort tells us which interface is the best.
As a final remark, the lack of correlation between traditional metrics and effortu confirms our observation that query difficulty, i.e., the quality of the original ranked list alone, is not sufficient to predict whether an RLR interface is preferable over a basic interface (cf. Section 5.3).

7. CONCLUSION
We have developed a simulation-based evaluation framework that measures the effectiveness of systems enabling result refinement, e.g., facets or filters. Its key component is an interaction model that characterizes the user's search behavior in the presence of result list refinement features. Using this framework, we investigate whole system performance, under various conditions. Instantiating the parameters of the user interaction model, corresponding to properties of search task and user type, allows us to predict system performance for specific groups of users. We validated the predictions made using data collected with two search systems, re-using the TREC Federated Search test collection.
We found that user effort estimated by our model is correlated significantly with actual user effort measured in the user data. We applied our interaction model to the task of predicting when a user should or should not use an RLR interface, and found a significant correlation between the predictions we made and observations in the user data. We did not find such correlations when applying traditional retrieval metrics to this task, demonstrating the value of the proposed user interaction model for search with result refinement.
Our study extends user interaction models beyond the classic "10 blue links." It provides a means to evaluate retrieval systems while considering the interaction effects between non-standard search UI features and search system effectiveness and the variability in how different people use the search UI.
Acknowledgements. This research was partially supported by the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the Netherlands Organisation for Scientific Research (NWO) under project nrs. 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15, Amsterdam Data Science, the Dutch national program COMMIT, the ESF Research Network Program ELIAS, the Elite Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), the Netherlands eScience Center under project nr. 027.012.105, the Yahoo! Faculty Research and Engagement Program, the Microsoft Research PhD program, the HPC Fund, and the Dutch Technology Foundation (STW) under project nr. 13675.
8. REFERENCES
[1] M. Agosti, N. Fuhr, E. Toms, and P. Vakkari. Evaluation methodologies in information retrieval dagstuhl seminar 13441. In ACM SIGIR Forum, volume 48, pages 36­41. ACM, 2014.

[2] L. Azzopardi. Modelling interaction with economic models of search. In SIGIR'14, pages 3­12, 2014.
[3] M. Bron, J. Van Gorp, F. Nack, L. B. Baltussen, and M. de Rijke. Aggregated search interface preferences in multi-session search tasks. In SIGIR'13, pages 123­132. ACM, 2013.
[4] B. Carterette. System effectiveness, user models, and user utility: a conceptual framework for investigation. In SIGIR'11, 2011.
[5] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In WWW'09, pages 1­10, 2009.
[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In CIKM'09, 2009.
[7] A. Chuklin, P. Serdyukov, and M. de Rijke. Click model-based information retrieval metrics. In SIGIR '13, 2013.
[8] A. Chuklin, I. Markov, and M. de Rijke. Click Models for Web Search. Morgan & Claypool Publishers, 2015.
[9] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In WSDM'08, 2008.
[10] T. Demeester, D. Trieschnigg, D. Nguyen, and D. Hiemstra. Overview of the trec 2013 federated web search track. In TREC'14.
[11] D. Downey, S. Dumais, and E. Horvitz. Models of searching and browsing: languages, studies, and applications. In IJCAI'07, 2007.
[12] G. Dupret and B. Piwowarski. A user browsing model to predict search engine click data from past observations. In SIGIR '08, 2008.
[13] J. English, M. Hearst, R. Sinha, K. Swearingen, and K.-P. Yee. Flexible search and navigation using faceted metadata. In University of Berkeley. Citeseer, 2002.
[14] N. Fuhr. A probability ranking principle for interactive information retrieval. Information Retrieval, 11(3):251­265, 2008.
[15] F. Guo, C. Liu, and Y. Wang. Efficient multiple-click models in web search. In WSDM '09, 2009.
[16] J. He, M. Bron, and A. P. de Vries. Characterizing stages of a multi-session complex search task through direct and indirect query modifications. In SIGIR'13, pages 897­900, 2013.
[17] J. He, M. Bron, L. Azzopardi, and A. de Vries. Studying user browsing behavior through gamified search tasks. In GamifIR'14, pages 49­52, 2014.
[18] J. Huang, R. W. White, and S. Dumais. No clicks, no problem: using cursor movements to understand and improve search. In SIGCHI'11.
[19] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422­446, 2002.
[20] K. Järvelin, S. L. Price, L. M. Delcambre, and M. L. Nielsen. Discounted cumulated gain based evaluation of multiple-query IR sessions. In ECIR'08, pages 4­15, 2008.
[21] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR'05, pages 154­161. ACM, 2005.
[22] E. Kanoulas, B. Carterette, P. D. Clough, and M. Sanderson. Evaluating multi-query sessions. In SIGIR'11, 2011.
[23] A. Kashyap, V. Hristidis, and M. Petropoulos. Facetor: cost-driven exploration of faceted query results. In CIKM '10, 2010.
[24] W. Kong and J. Allan. Extending faceted search to the general web. In CIKM'14, 2014.
[25] J. Koren, Y. Zhang, and X. Liu. Personalized interactive faceted search. In WWW '08, pages 477­486, 2008.
[26] R. L. Kumar, M. A. Smith, and S. Bannerjee. User interface features influencing overall ease of use and personalization. Information & Management, 41(3):289­302, 2004.
[27] C. Li, N. Yan, S. B. Roy, L. Lisham, and G. Das. Facetedpedia: dynamic generation of query-dependent faceted interfaces for wikipedia. In WWW '10, pages 651­660. ACM, 2010.
[28] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):Article 2, 2008.
[29] P. Morville and J. Callender. Search patterns. O'Reilly Media, 2010. [30] A. Schuth and M. Marx. Evaluation methods for rankings of
facetvalues for faceted search. In CLEF'11, pages 131­136, 2011. [31] M. D. Smucker and C. L. Clarke. Stochastic simulation of
time-biased gain. In CIKM'12, pages 2040­2044, 2012. [32] D. Vandic, F. Frasincar, and U. Kaymak. Facet selection algorithms
for web product search. In CIKM '13, pages 2327­2332. ACM, 2013. [33] E. Voorhees, D. K. Harman, et al. TREC: Experiment and evaluation
in information retrieval. MIT, 2005. [34] B. S. Wynar, A. G. Taylor, and J. Osborn. Introduction to cataloging
and classification. Libraries Unlimited, 1985.

302

Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks

Aliaksei Severyn
Google Inc.
aseveryn@gmail.com

Alessandro Moschitti
Qatar Computing Research Institute
amoschitti@qf.org.qa

ABSTRACT
Learning a similarity function between pairs of objects is at the core of learning to rank approaches. In information retrieval tasks we typically deal with query-document pairs, in question answering ­ question-answer pairs. However, before learning can take place, such pairs needs to be mapped from the original space of symbolic words into some feature space encoding various aspects of their relatedness, e.g. lexical, syntactic and semantic. Feature engineering is often a laborious task and may require external knowledge sources that are not always available or difficult to obtain. Recently, deep learning approaches have gained a lot of attention from the research community and industry for their ability to automatically learn optimal feature representation for a given task, while claiming state-of-the-art performance in many tasks in computer vision, speech recognition and natural language processing. In this paper, we present a convolutional neural network architecture for reranking pairs of short texts, where we learn the optimal representation of text pairs and a similarity function to relate them in a supervised way from the available training data. Our network takes only words in the input, thus requiring minimal preprocessing. In particular, we consider the task of reranking short text pairs where elements of the pair are sentences. We test our deep learning system on two popular retrieval tasks from TREC: Question Answering and Microblog Retrieval. Our model demonstrates strong performance on the first task beating previous state-of-the-art systems by about 3% absolute points in both MAP and MRR and shows comparable results on tweet reranking, while enjoying the benefits of no manual feature engineering and no additional syntactic parsers.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval; I.5.1 [Pattern Recognition]: Models--Neural nets
Keywords
Convolutional neural networks; learning to rank; Question Answering; Microblog search
The work was carried out at University of Trento. Professor at University of Trento, DISI.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767738.

1. INTRODUCTION
Encoding query-document pairs into discriminative feature vectors that are input to a learning-to-rank algorithm is a critical step in building an accurate reranker. The core assumption is that relevant documents have high semantic similarity to the queries and, hence, the main effort lies in mapping a query and a document into a joint feature space where their similarity can be efficiently established.
The most widely used approach is to encode input text pairs using many complex lexical, syntactic and semantic features and then compute various similarity measures between the obtained representations. For example, in answer passage reranking [31] employ complex linguistic features, modelling syntactic and semantic information as bags of syntactic and semantic role dependencies and build similarity and translation models over these representations.
However, the choice of representations and features is a completely empirical process, driven by the intuition, experience and domain expertise. Moreover, although using syntactic and semantic information has been shown to improve performance, it can be computationally expensive and require a large number of external tools -- syntactic parsers, lexicons, knowledge bases, etc. Furthermore, adapting to new domains requires additional effort to tune feature extraction pipelines and adding new resources that may not even exist.
Recently, it has been shown that the problem of semantic text matching can be efficiently tackled using distributional word matching, where a large number of lexical semantic resources are used for matching questions with a candidate answer [33].
Deep learning approaches generalize the distributional word matching problem to matching sentences and take it one step further by learning the optimal sentence representations for a given task. Deep neural networks are able to effectively capture the compositional process of mapping the meaning of individual words in a sentence to a continuous representation of the sentence. In particular, it has been recently shown that convolutional neural networks are able to efficiently learn to embed input sentences into low-dimensional vector space preserving important syntactic and semantic aspects of the input sentence, which leads to state-of-the-art results in many NLP tasks [18, 19, 38]. Perhaps one of the greatest advantages of deep neural networks is that they are trained in an end-to-end fashion, thus removing the need for manual feature engineering and greatly reducing the need for adapting to new tasks and domains.
In this paper, we describe a novel deep learning architecture for reranking short texts, where questions and documents are limited to a single sentence. The main building blocks of our architecture are two distributional sentence models based on convolutional neural networks. These underlying sentence models work in parallel, mapping queries and documents to their distributional vectors, which are then used to learn the semantic similarity between them.

373

The distinctive properties of our model are: (i) we use a stateof-the-art distributional sentence model for learning to map input sentences to vectors, which are then used to measure the similarity between them; (ii) our model encodes query-document pairs in a rich representation using not only their similarity score but also their intermediate representations; (iii) the architecture of our network makes it straightforward to include any additional similarity features to the model; and finally (iv) our model does not require manual feature engineering or external resources. We only require to initialize word embeddings from some large unsupervised corpora 1
Our sentence model is based on a convolutional neural network architecture that has recently showed state-of-the-art results on many NLP sentence classification tasks [18, 19]. However, our model uses it only to generate intermediate representation of input sentences for computing their similarity. To compute the similarity score we use an approach used in the deep learning model of [38], which recently established new state-of-the-art results on answer sentence selection task. However, their model operates only on unigram or bigrams, while our architecture learns to extract and compose n-grams of higher degrees, thus allowing for capturing longer range dependencies. Additionally, our architecture uses not only the intermediate representations of questions and answers to compute their similarity but also includes them in the final representation, which constitutes a much richer representation of the question-answer pairs. Finally, our model is trained end-to-end, while in [38] the output of the deep learning model is used to learn a logistic regression classifier.
We test our model on two popular retrieval tasks from TREC: answer sentence selection and Microblog retrieval. Our model shows a considerable improvement on the first task beating recent stateof-the-art system. On the second task, our model demonstrates that previous state-of-the-art retrieval systems can benefit from using our deep learning model.
In the following, we give a problem formulation and provide a brief overview of learning to rank approaches. Next, we describe our deep learning model and describe our experiments.
2. LEARNING TO RANK
This section briefly describes the problem of reranking text pairs which encompasses a large set of tasks in IR, e.g., answer sentence selection in question answering, microblog retrieval, etc. We argue that deriving an efficient representation of query-document pairs required to train a learning to rank model plays an important role in training an accurate reranker.
2.1 Problem formulation
The most typical setup in supervised learning to rank tasks is as follows: we are given a set of retrieved lists, where each query qi  Q comes together with its list of candidate documents Di = {di1 , di2 , ..., din }. The candidate set comes with their relevancy judgements {yi1 , yi2 , . . . , yin }, where documents that are relevant have labels equal to 1 (or higher) and 0 otherwise. The goal is to build a model that for each query qi and its candidate list Di generates an optimal ranking R, s.t. relevant documents appear at the top of the list.
More formally, the task is to learn a ranking function:
h(w, (qi, Di))  R,
1Given a large training corpora our network can also optimize the embeddings directly for the task, thus omitting the need to pre-train the embeddings.

where function (·) maps query-document pairs to a feature vector representation where each component reflects a certain type of similarity, e.g., lexical, syntactic, and semantic. The weight vector w is a parameter of the model and is learned during the training.
2.2 Learning to Rank approaches
There are three most common approaches in IR to learn the ranking function h, namely, pointwise, pairwise and listwise.
Pointwise approach is perhaps the most simple way to build a reranker where the training instances are triples (qi, dij, yij) and it is enough to train a binary classifier: h(w, (qi, dij))  yij, where  maps query-document pair to a feature vector and w is a vector of model weights.
The decision function h(·) typically takes a linear form simply computing a dot product between the model weights w and a feature representation of a pair generated by (·). At test time, the learned model is used to classify unseen pairs (qi, dij), where the raw scores are used to establish the global rank R of the documents in the retrieved set. This approch is widely used in practice because of its simplicity and effectiveness.
A more advanced approaches to reranking, is pairwise, where the model is explicitly trained to score correct pairs higher than incorrect pairs with a certain margin:
h(w, (qi, dij ))  h(w, (qi, dik)) + ,
where document dij is relevant and dik is not. Conceptually similar to the pointwise method described above, the pairwise approach exploits more information about the ground truth labelling of the input candidates. However, it requires to consider a larger number of training instances (potentially quadratic in the size of the candidate document set) than the pointwise method, which may lead to slower training times. Still both pointwise and pairwise approaches ignore the fact that ranking is a prediction task on a list of objects.
The third method, referred to as a listwise approach [6], treats a query with its list of candidates as a single instance in learning, thus able to capture considerably more information about the ground truth ordering of input candidates.
While pairwise and listwise approaches claim to yield better performance, they are more complicated to implement and less effective train. Most often, producing a better representation () that encodes various aspects of similarity between the input querydocument pairs plays a far more important role in training an accurate reranker than choosing between different ranking approaches. Hence, in this paper we adopt a simple pointwise method to reranking and focus on modelling a rich representation of query-document pairs using deep learning approaches which is described next.
3. OUR DEEP LEARNING MODEL
This section explains our deep learning model for reranking short text pairs. Its main building blocks are two distributional sentence models based on convolutional neural networks (ConvNets). These underlying sentence models work in parallel mapping queries and documents to their distributional vectors, which are then used to learn the semantic similarity between them.
In the following, we first describe our sentence model for mapping queries and documents to their intermediate representations and then describe how they can be used for learning semantic matching between input query-document pairs.
3.1 Sentence model
The architecture of our ConvNet for mapping sentences to feature vectors is shown on Fig. 1. It is mainly inspired by the architectures used in [18, 19] for performing various sentence classifica-

374

Figure 1: Our sentence model for mapping input sentences to their intermediate feature representations.

tion tasks. However, different from previous work the goal of our distributional sentence model is to learn good intermediate representations of the queries and documents, which are then used for computing their semantic matching.
Our network is composed of a single wide convolutional layer followed by a non-linearity and simple max pooling.
The input to the network are raw words that need to be translated into real-valued feature vectors to be processed by subsequent layers of the network. In the following we give a brief explanation of the main components of our convolutional neural network: sentence matrix, activations, convolutional and pooling layers.

3.1.1 Sentence matrix
The input to our sentence model is a sentence s treated as a sequence of words: [w1, .., w|s|], where each word is drawn from a vocabulary V . Words are represented by distributional vectors w  Rd looked up in a word embeddings matrix W  Rd×|V | which is formed by concatenating embeddings of all words in V . For convenience and ease of lookup operations in W, words are mapped to integer indices 1, . . . , |V |.
For each input sentence s we build a sentence matrix S  Rd×|s|, where each column i represents a word embedding wi at the corresponding position i in a sentence (see Fig. 1):
| | |  S = w1 . . . w|s|
|| |
To learn to capture and compose features of individual words in a given sentence from low-level word embeddings into higher level semantic concepts, the neural network applies a series of transformations to the input sentence matrix S using convolution, nonlinearity and pooling operations, which we describe next.

3.1.2 Convolution feature maps
The aim of the convolutional layer is to extract patterns, i.e., discriminative word sequences found within the input sentences that are common throughout the training instances.
More formally, the convolution operation  between two vectors s  R|s| and f  Rm (called a filter of size m) results in a vector c  R|s|+m-1 where each component is as follows:

i+m-1

ci = (s  f )i = sT[i-m+1:i] · f =

sk fk

(1)

k=i

The range of allowed values for i defines two types of convolution: narrow and wide. The narrow type restricts i to be in the range [1, |s| - m + 1], which in turn restricts the filter width to be  |s|. To compute the wide type of convolution i ranges from 1 to |s| and sets no restrictions on the size of m and s. The benefits of one type of convolution over the other when dealing with text are discussed in detail in [18]. In short, the wide convolution is able to better handle words at boundaries giving equal attention to all words in the sentence, unlike in narrow convolution, where words close to boundaries are seen fewer times. More importantly, wide convolution also guarantees to always yield valid values even when s is shorter than the filter size m. Hence, we use wide convolution in our sentence model. In practice, to compute the wide convolution it is enough to pad the input sequence with m - 1 zeros from left and right.
Given that the input to our ConvNet are sentence matrices S  Rd×|s|, the arguments of Eq. 1 are matrices and a convolution filter is also a matrix of weights: F  Rd×m. Note that the convolution filter is of the same dimensionality d as the input sentence matrix. As shown in Fig. 1, it slides along the column dimension of S producing a vector c  R|s|-m+1 in output. Each component ci is the result of computing an element-wise product between a column slice of S and the filter matrix F, which is then flattened and summed producing a single value.
It should be noted that an alternative way of computing a convolution was explored in [18], where a series of convolutions are computed between each row of a sentence matrix and a corresponding row of the filter matrix. Essentially, it is a vectorized form of 1d convolution applied between corresponding rows of S and F. As a result, the output feature map is a matrix C  Rd×|s|-m+1 rather than a vector as above. While, intuitively, being a more general way to process the input matrix S, where individual filters are applied to each respective dimension, it introduces more parameters to the model and requires a way to reduce the dimensionality of the resulting feature map. To address this issue, the authors apply a folding operation, which sums every two rows element-wise, thus effectively reducing the size of the representation by 2.
So far we have described a way to compute a convolution between the input sentence matrix and a single filter. In practice, deep learning models apply a set of filters that work in parallel generating muliple feature maps (also shown on Fig. 1). The resulting filter bank F  Rn×d×m produces a set of feature maps of dimension n × (|s| - m + 1).
In practice, we also add a bias vector2 b  Rn to the result of a convolution ­ a single bi value for each feature map ci.
3.1.3 Activation units
To allow the network learn non-linear decision boundaries, each convolutional layer is typically followed by a non-linear activation function () applied element-wise to the output of the preceding layer. Among the most common choices of activation functions are the following: sigmoid (or logistic), hyperbolic tangent tanh, and a rectified linear (ReLU) function defined as simply max(0, x) to ensure that feature maps are always positive. The choice of activation function has been shown to affect the convergence rate and the quality of obtained the solution. In particular, [22] shows that rectified linear unit has significant benefits over sigmoid and tanh overcoming some of the their shortcomings.
3.1.4 Pooling
The output from the convolutional layer (passed through the activation function) are then passed to the pooling layer, whose goal
2bias is needed to allow the network learn an appropriate threshold

375

is to aggregate the information and reduce the representation. The result of the pooling operation is:

 pool((c1 + b1  e)) 

cpooled = 

...



pool((cn + bn  e))

where ci is the ith convolutional feature map with added bias (the bias is added to each element of ci and e is a unit vector of the same size as ci) and passed through the activation function ().
There are two conventional choices for the pool(·) operation: average and max. Both operations apply to columns of the feature map matrix, by mapping them to a single value: pool(ci) : R1×(|s|+m-1)  R. This is also demonstrated in Fig. 1.
Both average and max pooling methods exhibit certain disadvantages: in average pooling, all elements of the input are considered, which may weaken strong activation values. This is especially critical with tanh non-linearity, where strong positive and negative activations can cancel each other out.
The max pooling is used more widely and does not suffer from the drawbacks of average pooling. However, as shown in [40], it can lead to strong overfitting on the training set and, hence, poor generalization on the test data. To mitigate the overfitting issue of max pooling several variants of stochastic pooling have been proposed in [40].
Recently, max pooling has been generalized to k-max pooling [18], where instead of a single max value, k values are extracted in their original order. This allows for extracting several largest activation values from the input sentence. As a consequence deeper architectures with several convolutional layers can be used. In [18], the authors also propose dynamic k-max pooling, where the value of k depends on the sentence size and the level in the convolution hierarchy [18].
Convolutional layer passed through the activation function together with pooling layer acts as a non-linear feature extractor. Given that multiple feature maps are used in parallel to process the input, deep learning networks are able to build rich feature representations of the input.
This ends the description of our sentence model. In the following we present our deep learning network for learning to match short text pairs.

3.2 Our architecture for matching text pairs
The architecture of our model for matching query-document pairs is presented in Fig. 2. Our sentence models based on ConvNets (described in Sec. 3.1) learn to map input sentences to vectors, which can then be used to compute their similarity. These are then used to compute a query-document similarity score, which together with the query and document vectors are joined in a single representation.
In the following we describe how the intermediate representations produced by the sentence model can be used to compute querydocument similarity scores and give a brief explanation of the remaining layers, e.g. hidden and softmax, used in our network.

3.2.1 Matching query and documents

Given the output of our sentence ConvNets for processing queries and documents, their resulting vector representations xq and xd, can be used to compute a query-document similarity score. We follow the approach of [2] that defines the similarity between xq and xd vectors as follows:

sim(xq, xd) = xTq Mxd,

(2)

where M  Rd×d is a similarity matrix. The Eq. 2 can be viewed

as a model of the noisy channel approach from machine translation, which has been widely used as a scoring model in information retrieval and question answering [13]. In this model, we seek a transformation of the candidate document xd = Mxd that is the closest to the input query xq. The similarity matrix M is a parameter of the network and is optimized during the training.

3.2.2 Hidden layers
The hidden layer computes the following transformation:

(wh · x + b),
where wh is the weight vector of the hidden layer and () is the non-linearity. Our model includes an additional hidden layer right before the softmax layer (described next) to allow for modelling interactions between the components of the intermediate representation.

3.2.3 Softmax

The output of the penultimate convolutional and pooling layers is flattened to a dense vector x, which is passed to a fully connected softmax layer. It computes the probability distribution over the labels:

p(y = j|x) =

exT j

K k=1

exT

k

,

where k is a weight vector of the k-th class. x can be thought of as a final abstract representation of the input example obtained by a series of transformations from the input layer through a series of convolutional and pooling operations.

3.2.4 The information flow
Here we provide a full description of our deep learning network (shown on Fig. 2) that maps input sentences to class probabilities.
The output of our sentence models (Sec. 3.1) are distributional representations of a query xq and a document xd. These are then matched using a similarity matrix M according to Eq. 2. This produces a single score xsim capturing various aspects of similarity (syntactic and semantic) between the input queries and documents. Note that it is also straight-forward to add additional features xfeat to the model.
The join layer concatenates all intermediate vectors, the similarity score and any additional features into a single vector:
xjoin = [xTq ; xsim; xTd ; xTfeat]
This vector is then passed through a fully connected hidden layer, which allows for modelling interactions between the components of the joined representation vector. Finally, the output of the hidden layer is further fed to the softmax classification layer, which generates a distribution over the class labels.

3.3 Training

The model is trained to minimise the cross-entropy cost function:

C

= -log

N i=1

p(yi

|qi

,

di

)

+





2 2

=-

N i=1

[yi

log

ai

+

(1

-

yi)log(1

-

ai)]

+





d 2

,

(3)

where a is the output from the softmax layer.  contains all the

parameters optimized by the network:

 = {W; Fq; bq; Fd; bd; M; wh; bh; ws; bs},
namely the word embeddings matrix W, filter weights and biases of the convolutional layers, similarity matrix M, weights and biases of the hidden and softmax layers.

376

Figure 2: Our deep learning architecture for reranking short text pairs.

The parameters of the network are optimized with stochastic gradient descent (SGD) using backpropogation algorithm to compute the gradients. To speedup the convergence rate of SGD various modifications to the update rule have been proposed: momentum, Adagrad [12], Adadelta [39], etc. Adagrad scales the learning rate of SGD on each dimension based on the l2 norm of the history of the error gradient. Adadelta uses both the error gradient history like Adagrad and the weight update history. It has the advantage of not having to set a learning rate at all.
3.4 Regularization
While neural networks have a large capacity to learn complex decision functions they tend to easily overfit especially on small and medium sized datasets. To mitigate the overfitting issue we augment the cost function with l2-norm regularization terms for the parameters of the network.
We also experiment with another popular and effective technique to improve regularization of the NNs -- dropout [30]. Dropout prevents feature co-adaptation by setting to zero (dropping out) a portion of hidden units during the forward phase when computing the activations at the softmax output layer. As suggested in [14] dropout acts as an approximate model averaging.
4. EXPERIMENTS AND EVALUATION
We evaluate our deep learning model on two popular retrieval benchmarks from TREC: answer sentence selection and TREC microblog retrieval.
4.1 Training and hyperparameters
The parameters of our deep learning model were (chosen on a dev set of the answer sentence selection dataset) as follows: the width m of the convolution filters is set to 5 and the number of convolutional feature maps is 100. We use ReLU activation function and a simple max-pooling. The size of the hidden layer is equal

to the size of the xjoin vector obtained after concatenating query and document vectors from the distributional models, similarity score and additional features (if used).
To train the network we use stochastic gradient descent with shuffled mini-batches. We eliminate the need to tune the learning rate by using the Adadelta update rule [39]. The batch size is set to 50 examples. The network is trained for 25 epochs with early stopping, i.e., we stop the training if no update to the best accuracy on the dev set has been made for the last 5 epochs. The accuracy computed on the dev set is the MAP score. At test time we use the parameters of the network that were obtained with the best MAP score on the development (dev) set, i.e., we compute the MAP score after each 10 mini-batch updates and save the network parameters if a new best dev MAP score was obtained. In practice, the training converges after a few epochs. We set a value for L2 regularization term to 1e-5 for the parameters of convolutional layers and 1e - 4 for all the others. The dropout rate is set to p = 0.5.
4.2 Word embeddings
While our model allows for learning the word embeddings directly for a given task, we keep the word matrix parameter W static. This is due to a common experience that a minimal size of the dataset required for tuning the word embeddings for a given task should be at least in the order of hundred thousands, while in our case the number of query-document pairs is one order of magnitude smaller. Hence, similar to [11, 19, 38] we keep the word embeddings fixed and initialize the word matrix W from an unsupervised neural language model. We choose the dimensionality of our word embeddings to be 50 to be on the line with the deep learning model of [38].
4.3 Size of the model
Given that the dimensionality of the word embeddings is 50, the number of parameters in the convolution layer of each sentence

377

model is 100 × 5 × 50. Hence, the total number of parameters in each of the two convolutional networks that map sentences to vectors is 25k. The similarity matrix is M  R100×100, which adds another 10k parameters to the model. The fully connected hidden layer is and a softmax add about 40k parameters. Hence the total number of parameters in the network is about 100k.
5. ANSWER SENTENCE SELECTION
Our first experiment is on answer sentence selection dataset, where answer candidates are limited to a single sentence. Given a question with its list of candidate answers the task is to rank the candidate answers based on their relatedness to the question.
5.1 Experimental setup
Data and setup. We test our model on the manually curated TREC QA dataset3 from Wang et al. [36], which appears to be one of the most widely used benchmarks for answer reranking. The dataset contains a set of factoid questions, where candidate answers are limited to a single sentence. The set of questions are collected from TREC QA tracks 8-13. The manual judgement of candidate answer sentences is provided for the entire TREC 13 set and for the first 100 questions from TREC 8-12. The motivation behind this annotation effort is that TREC provides only the answer patterns to identify if a given passage contains a correct answer key or not. This results in many unrelated candidate answers marked as correct simply because regular expressions cannot always match the correct answer keys.
To enable direct comparison with the previous work, we use the same train, dev and test sets. Table 1 summarizes the datasets used in our experiments. An additional training set TRAIN-ALL provided by Wang et. al [36] contains 1,229 questions from the entire TREC 8-12 collection and comes with automatic judgements. This set represents a more noisy setting, nevertheless, it provides many more QA pairs for learning. Word embeddings. We initialize the word embeddings by running word2vec tool [20] on the English Wikipedia dump and the AQUAINT corpus4 containing roughly 375 million words. To train the embeddings we use the skipgram model with window size 5 and filtering words with frequency less than 5. The resulting model contains 50-dimensional vectors for about 3.5 million words. Embeddings for words not present in the word2vec model are randomly initialized with each component sampled from the uniform distribution U [-0.25, 0.25].
We minimally preprocess the data only performing tokenization and lowercasing all words. To reduce the size of the resulting vocabulary V , we also replace all digits with 0. The size of the word vocabulary V for experiments using TRAIN set is 17,023 with approximately 95% of words initialized using wor2vec embeddings and the remaining 5% words are initialized at random as described in Sec. 4.2. For the TRAIN-ALL setting the |V | = 56, 953 with 85% words found in the word2vec model. Additional features. Given that a certain percentage of the words in our word embedding matrix are initialized at random (about 15% for the TRAIN-ALL) and a relatively small number of QA pairs prevents the network to directly learn them from the training data, similarity matching performed by the network will be suboptimal between many question-answer pairs.
Additionally, even for the words found in the word matrix, as noted in [38], one of the weaknesses of approaches relying on dis-
3http://cs.stanford.edu/people/mengqiu/data/ qg-emnlp07-data.tgz 4https://catalog.ldc.upenn.edu/LDC2002T31

Table 1: Summary of TREC QA datasets for answer reranking.

Data

# Questions # QA pairs % Correct

TRAIN-ALL TRAIN DEV TEST

1,229 94 82
100

53,417 4,718 1,148 1,517

12.0% 7.4% 19.3% 18.7%

tributional word vectors is their inability to deal with numbers and

proper nouns. This is especially important for factoid question an-

swering, where most of the questions are of type what, when, who

that are looking for answers containing numbers or proper nouns.

To mitigate the above two issues, we follow the approach in [38]

and include additional features establishing relatedness between

question-answer pairs. In particular, we compute word overlap

measures between each question-answer pair and include it as an

additional feature vector xfeat in our model. This feature vector

contains only four features: word overlap and IDF-weighted word

overlap computed between all words and only non-stop words. Com-

puting these features is straightforward and does not require addi-

tional pre-processing or external resources.

Evaluation. The two metrics used to evaluate the quality of our

model are Mean Average Precision (MAP) and Mean Reciprocal

Rank (MRR), which are common in Information Retrieval and Ques-

tion Answering.

MRR

is computed as

follows:

M RR

=

1 |Q|

|Q| q=1

1 rank(q)

,

where

rank(q) is the position of the first correct answer in the candidate

list. MRR is only looking at the rank of the first correct answer,

hence it is more suitable in cases where for each question there is

only a single correct answer. Differently, MAP examines the ranks

of all the correct answers. It is computed as the mean over the av-

erage precision scores for each query q

 Q:

1 Q

Q q=1

AveP

(q).

We use the official trec_eval scorer to compute the above met-

rics.

5.2 Results and discussion
We report the results of our deep learning model on the TRAIN and TRAIN-ALL sets also when additional word overlap features are used. Additionally, we report the results from a recent deep learning system in [38] that has established the new state-of-the-art results in the same setting.
Table 2 summarises the results for the setting when the network is trained using only input question-answer pairs without using any additional features. As we can see our deep learning architecture demonstrates a much stronger performance compared to the system in [38]. The deep learning model from [38], similarly to ours, relies on a convolutional neural network to learn intermediate representations. However, their convolutional neural network operates only on unigram or bigrams, while in our architecture we use a larger width of the convolution filter, thus allowing for capturing longer range dependencies. Additionally, along with the question-answer similarity score, our architecture includes intermediate representations of the question and the answer, which together constitute a much richer representation. This results in a large improvement of about 8% absolute points in MAP for TRAIN and almost 10% when trained with more data from TRAIN-ALL. This emphasizes the importance of learning high quality sentence models.
Table 3 provides the results when additional word overlap features are added to the model. Simple word overlap features help to improve the question-answer matching. Our model shows an improvement of about a significant improvement over previous state-

378

Table 2: Results on TRAIN and TRAIN-ALL from Trec QA.

Model

MAP MRR

TRAIN Yu et al. [38] (unigram) Yu et al. [38] (bigram) Our model

.5387 .5476 .6258

.6284 .6437 .6591

TRAIN-ALL Yu et al. [38] (unigram) Yu et al. [38] (bigram) Our model

.5470 .5693 .6709

.6329 .6613 .7280

Table 3: Results on TREC QA when augmenting the deep learning model with word overlap features.

Model

MAP MRR

TRAIN Yu et al. [38] (unigram) Yu et al. [38] (bigram) Our model
TRAIN-ALL Yu et al. [38] (unigram) Yu et al. [38] (bigram) Our model

.6889 .7058 .7329
.6934 .7113 .7459

.7727 .7800 .7962
.7677 .7846 .8078

Table 4: Survey of the results on the QA answer selection task.

Model

MAP MRR

Wang et al. (2007) [36] Heilman and Smith (2010) [15] Wang and Manning (2010) [35] Yao et al. (2013) [37] Severyn & Moschitti (2013) [26] Yih et al. (2013) [33] Yu et al. (2014) [38]

.6029 .6091 .5951 .6307 .6781 .7092 .7113

.6852 .6917 .6951 .7477 .7358 .7700 .7846

Our model (TRAIN) Our model (TRAIN-ALL)

.7329 .7962 .7459 .8078

of-the-art in both MAP and MRR when training on TRAIN and TRAIN-ALL. Note that the results are significantly better than when no overlap features are used. This is possibly due to the fact that the distrubutional representations fail to establish the relatedness in some cases and simple word overlap matching can help to drive the model in the right direction.
Table 4 reports the results of the previously published systems on this task. Our model trained on a small TRAIN dataset beats all of the previous state-of-the-art systems. The improvement is further emphasized when the system is trained using more questionanswer pairs from TRAIN-ALL showing an improvement of about 3% absolute points in both MAP and MRR. The results are very promising considering that our system requires no manual feature engineering (other than simple word overlap features), no expensive preprocessing using various NLP parsers, and no external semantic resources other than using pre-initialized word embeddings that can be easily trained provided a large amount of unsupervised text.
In the spirit, our system is most similar to a recent deep learning architecture from Yu et al. (2014) [38]. However, we employ a more expressive convolutional neural network for learning inter-

Table 5: Summary of TREC Microblog datasets.

Data

# Topic # Tweet pairs % Correct # Runs

TMB2011

49

TMB2012

59

60,129 73,073

5.1% 184 8.6% 120

mediate representations of the query and the answer. This allows for performing a more accurate matching between question-answer pairs. Additionally, our architecture includes intermediate question and answer representations in the model, which result in a richer representation of question-answer pairs. Finally, we train our system in an end-to-end fashion, while [38] use the output of their deep learning system as a feature in a logistic regression classifier.
6. TREC MICROBLOG RETRIEVAL
To assess the effectiveness and generality of our deep learning model for text matching, we apply it on tweet reranking task. We focus on the 2011 and 2012 editions of the ad-hoc retrieval task at TREC microblog tracks [23, 29]. We follow the setup in [27], where they represent query-tweet pairs with a shallow syntactic models to learn a tree kernel reranker. In contrast, our model does not rely on any syntactic parsers and requires virtually no preprocessing other than tokenizaiton and lower-casing. Our main research question is: Can our neural network that requires no manual feature engineering and expensive pre-processing steps improve on top of the state-of-the-art learning-to-rank and retrieval algorithms?
To answer this question, we test our model in the following settings: we treat the participant systems in the TREC microblog tasks as a black-box, and implement our model on top of them using only their raw scores (ranks) as a single feature in our model. This allows us to see whether our model is able to learn information complementary to the approaches used by such retrieval algorithms. Our setup replicates the experiments in [27] to allow for comparing to their model.
6.1 Experimental setup
Data and setup. Our dataset is the tweet corpus used in both TREC Microblog tracks in 2011 (TMB2011) and 2012 (TMB2012). It consists of 16M tweets spread over two weeks, and a set of 49 (TMB2011) and 59 (TMB2012) timestamped topics. We minimally preprocess the tweets--we normalize elongations (e.g., sooo  so), normalize URLs and author ids. Additionally, we use the system runs submitted at TMB2011 and TMB2012, which contain 184 and 120 models, respectively. This is summarized in Table 5. Word embeddings. We used the word2vec tool to learn the word embeddings from the provided 16M tweet corpus, with the following setting: (i) we removed non-english tweets, which reduces the corpus to 8.4M tweets and (ii) we used the skipgram model with window size 5 and filtering words with frequency less than 5. The trained model contains 330k words. We use word embeddings of size 50 -- same as for the previous task. To build the word embedding matrix W , we extract the vocabulary from all tweets present in TMB2011 and TMB2012. The resulting vocabulary contains 150k words out of which only 60% are found in the word embeddings model. This is due to a very large number of misspellings and words occurring only once (hence they are filted by the word2vec tool). This has a negative impact on the performance of our deep learning model since around 40% of the word vectors are randomly initialized. At the same time it is not possible to tune the word embeddings on the training set, as it will overfit due to the small number of the query-tweet pairs available for training.

379

Training. We train our system on the runs submitted at TMB2011, and test it on the TMB2012 runs. We focus on one direction only to avoid training bias, since TMB2011 topics were already used for learning systems in TMB2012. Submission run as a feature. We use the output of participant systems as follows: we use rank positions of each tweet rather than raw scores, since scores for each system are scaled differently, while ranks are uniform across systems. We apply the following transformation of the rank r: 1/ log (r + 1). In the training phase, we take the top 30 systems from the TMB2011 track (in terms of P@30). For each query-tweet pair we compute the average transformed rank over the top systems. This score is then used as a single feature xfeat by our model. In the testing phase, we generate this feature as follows: for each participant system that we want to improve, we use the transformed rank of the query-tweet taken from their submission run. Evaluation. We report on the official evaluation metric for the TREC 2012 Microblog track, i.e., precision at 30 (P@30), and also on mean average precision (MAP). Following [4, 23], we regard minimally and highly relevant documents as relevant and use the TMB2012 evaluation script. For significance testing, we use a pairwise t-test, where and denote significance at  = 0.05 and  = 0.01, respectively. Triangles point up for improvement over the baseline, and down otherwise. We also report the improvement in the absolute rank (R) in the official TMB2012 ranking.
6.2 Results and discussion
Table 6 reports the results for re-ranking runs of the best 30 systems from TMB2012 (based on their P@30 score) when we train our system using the top 30 runs from TMB2011.
First, we note that our model improves P@30 for the majority of the systems with a relative improvement ranging from several points up to 10% with about 6% on average. This is remarkable, given that the pool of participants in TMB2012 was large, and the top systems are therefore likely to be very strong baselines.
Secondly, we note that the relative improvement of our model is on the par with the STRUCT model from [27], which relies on using syntactic parsers to train a tree kernel reranker. In contrast, our model requires no manual feature engineering and virtually no preprocessing and external resources. Similar to the observation made in [27], our model has a precision-enhancing effect. In cases where MAP drops a bit it can be seen that our model sometimes lowers relevant documents in the runs. It is possible that our model favours query-tweet pairs that exhibit semantic matching of higher quality, and that it down-ranks tweets that are of lower quality but are nonetheless relevant. Another important aspect is the fact that a large portion of the word embeddings (about 40%) used by the network are initialized at random, which has a negative impact on the accuracy of our model.
Looking at the improvement in absolute position in the official ranking (R), we see that, on average, our deep learning model boosts the absolute position in the official ranking for top 30 systems by about 7.8 positions.
All in all, the results suggest that our deep learning model with no changes in its architecture is able to capture additional information and can be useful when coupled with many state-of-the-art microblog search algorithms.
While improving the top systems from 2012 represents a challenging task, it is also interesting to assess the potential improvement for lower ranking systems. We follow [27] and report our results on the 30 systems from the middle and the bottom of the official ranking. Table 7 summarizes the average improvements for three groups of systems: top-30, middle-30, and bottom-30.

Table 7: Comparison of the averaged relative improvements for the top, middle (mid), and bottom (btm) 30 systems from TMB2012.
STRUCT [27] Our model band MAP P@30 MAP P@30
top 3.3% 5.3% 2.0% 6.2% mid 12.2% 12.9% 9.8% 13.7% btm 22.1% 25.1% 18.7% 24.3%
We find that the improvement over underperforming systems is much larger than for stronger systems. In particular, for the bottom 30 systems, our approach achieves an average relative improvement of 20% in both MAP and P@30. The performance of our model is on the par with the STRUCT model [27].
We expect that learning word embeddings on a larger corpora such that the percentage of the words present in the word embedding matrix W should help to improve the accuracy of our system. Moreover, similar to the situation observed with answer selection experiments, we expect that using more training data would improve the generalization of our model. As one possible solution to getting more training data, it could be interesting to experiment with training our model on much larger pseudo test collections similar to the ones proposed in [4]. We leave it for the future work.
7. RELATED WORK
Our learning to rank method is based on a deep learning model for advanced text representations using distributional word embeddings. Distributional representations have a long tradition in IR, e.g., Latent Semantic Analysis [10], which more recently has also been characterized by studies on distributional models based on word similarities. Their main properties is to alleviate the problem of data sparseness. In particular, such representations can be derived with several methods, e.g., by counting the frequencies of co-occurring words around a given token in large corpora. Such distributed representations can be obtained by applying neural language models that learn word embeddings, e.g., [3] and more recently using recursive autoencoders [34], and convolutional neural networks [8].
Our application of learning to rank models concerns passage reranking. For example, [17, 24] designed classifiers of question and answer passage pairs. Several approaches were devoted to reranking passages containing definition/description, e.g., [21, 28, 31]. [1] used a cascading approach, where the ranking produced by one ranker is used as input to the next stage.
Language models for reranking were applied in [7], where answer ranks were computed based on the probabilities of bigram models generating candidate answers. Language models were also applied to definitional QA in [9, 25, 32].
Our work more directly targets the task of answer sentence selection, i.e., the task of selecting a sentence that contains the information required to answer a given question from a set of candidates (for example, provided by a search engine). In particular, the state of the art in answer sentence selection is given by Wang et al., 2007 [36], who use quasi-synchronous grammar to model relations between a question and a candidate answer with the syntactic transformations. Heilman & Smith, 2010 [15] develop an improved Tree Edit Distance (TED) model for learning tree transformations in a q/a pair. They search for a good sequence of tree edit operations using complex and computationally expensive Tree Kernel-based heuristic. Wang & Manning, 2010 [35] develop a probabilistic

380

Table 6: System performance on the top 30 runs from TMB2012, using the top 10, 20 or 30 runs from TMB2011 for training.

TMB2012

STRUCT [27]

Our model

# runs

MAP P@30

MAP

P@30

R%

MAP

P@30

R%

1 hitURLrun3

0.3469 0.4695 0.3328 (-4.1%) 0.4774 (1.7%)

0 0.3326 (-4.1%) 0.4836 (3.0%)

0

2 kobeMHC2

0.3070 0.4689 0.3037 (-1.1%) 0.4768 (1.7%)

1 0.3052 (-0.6%) 0.4899 (4.5%)

1

3 kobeMHC

0.2986 0.4616 0.2965 (-0.7%) 0.4718 (2.2%)

2 0.2999 (0.4%) 0.4830 (4.6%)

2

4 uwatgclrman

0.2836 0.4571 0.2995 (5.6%) 0.4712 (3.1%)

3 0.2738 (-3.5%) 0.4516 (-1.2%)

-1

5 kobeL2R

0.2767 0.4429 0.2744 (-0.8%) 0.4463 (0.8%)

0 0.2677 (-3.3%) 0.4409 (-0.5%)

-2

6 hitQryFBrun4

0.3186 0.4424 0.3118 (-2.1%) 0.4554 (2.9%)

2 0.3220 (1.1%) 0.4849 (9.6%)

5

7 hitLRrun1

0.3355 0.4379 0.3226 (-3.9%) 0.4525 (3.3%)

2 0.3188 (-5.0%) 0.4610 (5.3%)

3

8 FASILKOM01 0.2682 0.4367 0.2820 (5.2%) 0.4531 (3.8%)

3 0.2622 (-2.2%) 0.4346 (-0.5%)

-1

9 hitDELMrun2

0.3197 0.4345 0.3105 (-2.9%) 0.4424 (1.8%)

4 0.3246 (1.5%) 0.4723 (8.7%)

8

10 tsqe

0.2843 0.4339 0.2836 (-0.3%) 0.4441 (2.4%)

5 0.2917 (2.6%) 0.4660 (7.4%)

7

11 ICTWDSERUN1 0.2715 0.4299 0.2862 (5.4%) 0.4582 (6.6%)

7 0.2765 (1.8%) 0.4484 (4.3%)

6

12 ICTWDSERUN2 0.2671 0.4266 0.2785 (4.3%) 0.4475 (4.9%)

7 0.2786 (4.3%) 0.4478 (5.0%)

7

13 cmuPrfPhrE

0.3179 0.4254 0.3172 (-0.2%) 0.4469 (5.1%)

8 0.3321 (4.5%) 0.4585 (7.8%)

9

14 cmuPrfPhrENo 0.3198 0.4249 0.3179 (-0.6%) 0.4486 (5.6%)

9 0.3359 (5.0%) 0.4591 (8.1%)

10

15 cmuPrfPhr

0.3167 0.4198 0.3130 (-1.2%) 0.4379 (4.3%)

8 0.3282 (3.6%) 0.4572 (8.9%)

11

16 FASILKOM02 0.2454 0.4141 0.2718 (10.8%) 0.4508 (8.9%)

11 0.2489 (1.4%) 0.4201 (1.5%)

1

17 IBMLTR

0.2630 0.4136 0.2734 (4.0%) 0.4441 (7.4%)

10 0.2703 (2.8%) 0.4346 (5.1%)

8

18 otM12ihe

0.2995 0.4124 0.2969 (-0.9%) 0.4322 (4.8%)

7 0.2900 (-3.2%) 0.4239 (2.8%)

3

19 FASILKOM03 0.2716 0.4124 0.2859 (5.3%) 0.4452 (8.0%)

14 0.2740 (0.9%) 0.4270 (3.5%)

7

20 FASILKOM04 0.2461 0.4113 0.2575 (4.6%) 0.4294 (4.4%)

9 0.2414 (-1.9%) 0.4220 (2.6%)

5

21 IBMLTRFuture 0.2731 0.4090 0.2808 (2.8%) 0.4311 (5.4%)

10 0.2785 (2.0%) 0.4415 (8.0%)

14

22 uiucGSLIS01

0.2445 0.4073 0.2575 (5.3%) 0.4260 (4.6%)

9 0.2478 (1.4%) 0.4233 (3.9%)

7

23 PKUICST4

0.2786 0.4062 0.2909 (4.4%) 0.4514 (11.1%) 18 0.2832 (1.7%) 0.4491 (10.6%) 18

24 uogTrLsE

0.2909 0.4028 0.2977 (2.3%) 0.4282 (6.3%)

9 0.3131 (7.6%) 0.4484 (11.3%) 19

25 otM12ih

0.2777 0.3989 0.2810 (1.2%) 0.4175 (4.7%)

10 0.2752 (-0.9%) 0.4119 (3.3%)

5

26 ICTWDSERUN4 0.1877 0.3887 0.1985 (5.8%) 0.4164 (7.1%)

10 0.2040 (8.7%) 0.4220 (8.6%)

11

27 uwatrrfall

0.2620 0.3881 0.2812 (7.3%) 0.4136 (6.6%)

9 0.2942 (12.3%) 0.4314 (11.2%) 16

28 cmuPhrE

0.2731 0.3842 0.2797 (2.4%) 0.4136 (7.7%)

12 0.2972 (8.8%) 0.4352 (13.3%) 19

29 AIrun1

0.2237 0.3842 0.2339 (4.6%) 0.4102 (6.8%)

5 0.2285 (2.2%) 0.4157 (8.2%)

13

30 PKUICST3

0.2118 0.3825 0.2318 (9.4%) 0.4119 (7.7%)

14 0.2363 (11.6%) 0.4415 (15.4%) 23

Average

3.3%

5.3%

7.3

2.0%

6.2%

7.8

model to learn tree-edit operations on dependency parse trees. They cast the problem into the framework of structured output learning with latent variables. The model of Yao et al., 2013 [37] applies linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. Severyn and Moschitti [26] applied SVM with tree kernels to shallow syntactic representation, which provide automatic feature engineering. Yih et al. [33] use distributional models based on lexical semantics to match semantic relations of aligned words in QA pairs.
More recently, Bordes et al. [5] used siamese networks for learning to project question and answer pairs into a joint space whereas Iyyer et al. [16] modelled semantic composition with a recursive neural network for a question answering task. The work closest to ours is [38], where they apply deep learning to learn to match question and answer sentences. However, their sentence model to map questions and answers to vectors operates only on unigrams or bigrams. Our sentence model is based on a convolutional neural network with the state-of-the-art architecture, we use a relatively large width of the convolution filter (5), thus allowing the network to capture longer range dependencies. Moreover, the architecture of deep learning model along with the question-answer similarity score also encodes question and answer vector representations in the model. Hence, our model constructs and learns a richer representation of the question-answer pairs, which results in superior results on the answer sentence selection dataset. Finally, our deep learning reranker is trained end-to-end, while in [38] they use the output of their neural network in a separate logistic scoring model.
Regarding learning to rank systems applied to TREC microblog datasets, recently [27] have shown that richer linguistic representa-

tions of tweets can improve upon state of the art systems in TMB2011 and TMB-2012. We directly compare with their system, showing that our deep learning model without any changes to its architecture (we only pre-train word embeddings) is on the par with their reranker. This is remarkable, since different from [27], which requires additional pre-proccesing using syntactic parsers to construct syntactic trees, our model requires no expensive pre-processing and does not rely on any external resources.
8. CONCLUSIONS
In this paper, we propose a novel deep learning architecture for reranking short texts. It has the benefits of requiring no manual feature engineering or external resources, which may be expensive or not available. The model with the same architecture can be successfully applied to other domains and tasks.
Our experimental findings show that our deep learning model: (i) greatly improves on the previous state-of-the-art systems and a recent deep learning approach in [38] on answer sentence selection task showing a 3% absolute improvement in MAP and MRR; (ii) our system is able to improve even the best system runs from TREC Microblog 2012 challenge; (iii) is comparable to the syntactic reranker in [27], while our system requires no external parsers or resources.
Acknowledgments. This work has been supported by the EC project CogNet, 671625 (H2020-ICT-2014-2, Research and Innovation action). The first author was supported by the Google Europe Doctoral Fellowship Award 2013.

381

REFERENCES
[1] A. Agarwal, H. Raghavan, K. Subbian, P. Melville, D. Gondek, and R. Lawrence. Learning to rank for robust question answering. In CIKM, 2012.
[2] J. W. Antoine Bordes and N. Usunier. Open question answering with weakly supervised embedding models. In ECML, Nancy, France, September 2014.
[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137­1155, 2003.
[4] R. Berendsen, M. Tsagkias, W. Weerkamp, and M. de Rijke. Pseudo test collections for training and tuning microblog rankers. In SIGIR, 2013.
[5] A. Bordes, S. Chopra, and J. Weston. Question answering with subgraph embeddings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615­620, Doha, Qatar, October 2014. Association for Computational Linguistics.
[6] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: From pairwise approach to listwise approach. In Proceedings of the 24th International Conference on Machine Learning, ICML '07, pages 129­136, New York, NY, USA, 2007. ACM.
[7] Y. Chen, M. Zhou, and S. Wang. Reranking answers from definitional QA using language models. In ACL, 2006.
[8] R. Collobert and J. Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. In ICML, pages 160­167, 2008.
[9] H. Cui, M. Kan, and T. Chua. Generic soft pattern models for definitional QA. In SIGIR, Salvador, Brazil, 2005. ACM.
[10] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 1990.
[11] M. Denil, A. Demiraj, N. Kalchbrenner, P. Blunsom, and N. de Freitas. Modelling, visualising and summarising documents with a single convolutional neural network. Technical report, University of Oxford, 2014.
[12] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121­2159, 2011.
[13] A. Echihabi and D. Marcu. A noisy-channel approach to question answering. In ACL, 2003.
[14] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. C. Courville, and Y. Bengio. Maxout networks. In ICML, pages 1319­1327, 2013.
[15] M. Heilman and N. A. Smith. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In NAACL, 2010.
[16] M. Iyyer, J. Boyd-Graber, L. Claudino, R. Socher, and H. Daumé III. A neural network for factoid question answering over paragraphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 633­644, Doha, Qatar, October 2014. Association for Computational Linguistics.
[17] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar questions in large question and answer archives. In CIKM, 2005.
[18] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A convolutional neural network for modelling sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June 2014.
[19] Y. Kim. Convolutional neural networks for sentence classification. In EMNLP, pages 1746­1751, Doha, Qatar, October 2014.

[20] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111­3119, 2013.
[21] A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar. Exploiting syntactic and shallow semantic kernels for question/answer classification. In ACL, 2007.
[22] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807­814, 2010.
[23] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. Overview of the TREC-2011 microblog track. In TREC, 2011.
[24] F. Radlinski and T. Joachims. Query chains: Learning to rank from implicit feedback. CoRR, 2006.
[25] Y. Sasaki. Question answering as question-biased term extraction: A new approach toward multilingual qa. In ACL, 2005.
[26] A. Severyn and A. Moschitti. Automatic feature engineering for answer selection and extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 458­467, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.
[27] A. Severyn, A. Moschitti, M. Tsagkias, R. Berendsen, and M. de Rijke. A syntax-aware re-ranker for microblog retrieval. In SIGIR, 2014.
[28] D. Shen and M. Lapata. Using semantic roles to improve question answering. In EMNLP-CoNLL, 2007.
[29] I. Soboroff, I. Ounis, J. Lin, and I. Soboroff. Overview of the TREC-2012 microblog track. In TREC, 2012.
[30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929­1958, 2014.
[31] M. Surdeanu, M. Ciaramita, and H. Zaragoza. Learning to rank answers to non-factoid questions from web collections. Comput. Linguist., 37(2):351­383, June 2011.
[32] J. Suzuki, Y. Sasaki, and E. Maeda. Svm answer selection for open-domain question answering. In COLING, 2002.
[33] W. tau Yih, M.-W. Chang, C. Meek, and A. Pastusiak. Question answering using enhanced lexical semantic models. In ACL, August 2013.
[34] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res., 11:3371­3408, Dec. 2010.
[35] M. Wang and C. D. Manning. Probabilistic tree-edit models with structured latent variables for textual entailment and question answer- ing. In ACL, 2010.
[36] M. Wang, N. A. Smith, and T. Mitaura. What is the jeopardy model? a quasi-synchronous grammar for qa. In EMNLP, 2007.
[37] P. C. Xuchen Yao, Benjamin Van Durme and C. Callison-Burch. Answer extraction as sequence tagging with tree edit distance. In NAACL, 2013.
[38] L. Yu, K. M. Hermann, P. Blunsom, and S. Pulman. Deep learning for answer sentence selection. CoRR, 2014.
[39] M. D. Zeiler. Adadelta: An adaptive learning rate method. CoRR, 2012.
[40] M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional neural networks. CoRR, abs/1301.3557, 2013.

382

Learning to Extract Local Events from the Web


John Foley
Center for Intelligent Information Retrieval University of Massachusetts
jfoley@cs.umass.edu

Michael Bendersky
Google
bemike@google.com


Vanja Josifovski
Pinterest
vanja@pinterest.com

ABSTRACT
The goal of this work is extraction and retrieval of local events from web pages. Examples of local events include small venue concerts, theater performances, garage sales, movie screenings, etc. We collect these events in the form of retrievable calendar entries that include structured information about event name, date, time and location.
Between existing information extraction techniques and the availability of information on social media and semantic web technologies, there are numerous ways to collect commercial, high-profile events. However, most extraction techniques require domain-level supervision, which is not attainable at web scale. Similarly, while the adoption of the semantic web has grown, there will always be organizations without the resources or the expertise to add machine-readable annotations to their pages. Therefore, our approach bootstraps these explicit annotations to massively scale up local event extraction.
We propose a novel event extraction model that uses distant supervision to assign scores to individual event fields (event name, date, time and location) and a structural algorithm to optimally group these fields into event records. Our model integrates information from both the entire source document and its relevant sub-regions, and is highly scalable.
We evaluate our extraction model on all 700 million documents in a large publicly available web corpus, ClueWeb12. Using the 217,000 unique explicitly annotated events as distant supervision, we are able to double recall with 85% precision and quadruple it with 65% precision, with no additional human supervision. We also show that our model can be bootstrapped for a fully supervised approach, which can further improve the precision by 30%.
In addition, we evaluate the geographic coverage of the extracted events. We find that there is a significant increase in the geo-diversity of extracted events compared to existing explicit annotations, while maintaining high precision levels.
Work done while at Google.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author(s). Copyright is held by the owner/author(s). SIGIR'15, August 09-13, 2015, Santiago, Chile. ACM 978-1-4503-3621-5/15/08. DOI: http://dx.doi.org/10.1145/2766462.2767739 .

Categories and Subject Descriptors
H3.3 [Information Storage And Retrieval]: Information Search and Retrieval
Keywords
Information Retrieval; Information Extraction
1. INTRODUCTION
With the increasing trend toward personalized mobile applications and user experiences, there is a need for information systems that react to user preferences and location. In the past few years, this challenge has gathered more attention in the research community. Lagun et al. find that not only is local context useful in search, but that users are interested in explicit feedback in locality-sensitive tasks [16]. The Contextual Suggestion Track in the Text Retrieval Conference (TREC) presents the task of recommending establishments or venues to users given the preferences of a user in another city [5]. In this work, we explore a similar task, presenting users with events near them, rather than locations. Unlike the contextual suggestion track, we move away from wholepage relevance judgments toward extracting relevant atomic event records.
We define an event as an object having three mandatory properties, keeping in mind our goal: to recommend, display, and make searchable all events that can be extracted from the web.
Definition An event occurs at a certain location, has a start date and time, and a title or description. In other words, to be useful to a user, an event must be able to answer the questions: What?, When?, and Where?
Succintly, we are interested in events that users may want to add to their calendars to be reminded of and potentially attend. This is in contrast to many other definitions of an event, such as those in works discussing real-time detection of natural disasters, riots, pandemics or terrorist attacks in microblog streams [26, 36, 20], or the classic event definition in computational linguistics, which can be as broad as "a situation that occurs" [25].
Before a recommendation system for events can be created and evaluated, there is the information extraction challenge of discovering and collecting all available events in all areas.
Simple approaches to this problem include:
· Mining and recommending events from social media [14].

423

· Leveraging semantic web annotations like Schema.org1.
· Traditional wrapper induction and data mining.
Unfortunately, both semantic web and social media approaches require organizations to maintain their data in a particular format. With social media, this means an updated organization page, and with semantic web technologies, this means marking up the page with microdata (e.g., Schema.org). Unfortunately, smaller businesses, charities, and truly local organizations will lack the funding or the expertise required to fully and correctly adopt semantic web technologies.
Similarly, most existing approaches to information extraction require supervision at either the page or domain level, or some sort of repeated element structure [34]. As it would be infeasible and costly to annotate all such pages - or even a single page per domain, existing systems that mine for events or other structured data fall short of our goal of local event extraction from all web pages.
Examples of events that we consider local and that are unlikely to have existing markup, or sufficient social media presence are farmer's markets, poetry readings, library book sales, charity dinners, garage sales, community band concerts, etc. These events are of interest to a smaller, local community, and are unlikely to be selling tickets on high-profile sites or paying for advertisement.
In this work, our goal is to leverage the well-advertised, high-profile events to learn to extract a greater variety and depth of events, including the kinds of local events described above. Specifically, we leverage the existing Schema.org microdata annotations (there is an example of how these annotations appear in Figure 1) as a source of data for distant supervision, allowing us to learn to extract events that do not have semantic web annotations, including local events, without actually collecting judgments specifically for our task.
We introduce a model for scoring event field extractions and an algorithm that groups these fields into complete event records. We scale up our technique to the entire ClueWeb12 corpus (700 million pages), extracting 2.7 million events. We evaluate our precision at various recall-levels, and show that we can double event coverage of a system with respect to the available semantic web annotations at an 85% precision level. We briefly explore using our judgments for a supervised approach to this task and are able to improve precision by 30% on another million events with only 30 annotator-hours.
We also explore the geographic diversity of our dataset, finding that existing markup is heavily biased toward large cities (e.g. New York, Chicago, and Los Angeles) and that the results of our extraction cover a wider variety of locations. We validate this hypothesis via visual mapping, and by showing that we have good precision in a random sample of 200 cities across the world.
In the next section, we introduce related work in detail. In Section 3, we introduce our event field extraction and scoring model, and our event record grouping algorithm. In Section 4, we discuss our corpus and our judgments in more detail, and we present the results of our experiments in Section 5. We end with our conclusions in Section 6.
1http://schema.org/Event

Figure 1: Example Microdata adapted from Schema.org
<div itemscope itemtype=``http://schema.org/Event''> <span itemprop=``name''> Miami Heat at Philadelphia 76ers </span> <meta itemprop=``startDate'' content=``2016-04-21T20:00''> Thu, 04/21/16 8:00 p.m. <div itemprop=``location'' itemscope itemtype=``http://schema.org/Place''> Wells Fargo Center <div itemprop=``address'' itemscope itemtype=``http://schema.org/PostalAddress''> <span itemprop=``addressLocality''> Philadelphia </span>, <span itemprop=``addressRegion''>PA</span> </div> </div>
</div>
2. RELATED WORK
Our work is characterized by using the explicitly annotated Schema.org as training data to learn to extract local events from the web. While there is work looking at events on social media, work leveraging semantic web annotations, and work on extraction in general, to our knowledge, our work is the first to leverage this data in this way, and the first to attempt this task.
2.1 Similar Tasks
The Contextual Suggestion Track [5] considers the task of a known user spending time and looking for entertainment in a new city. Evaluation is done on the snippet and page level, with users judging sites as interesting or not, effectively making the task about retrieving venues or establishments. In a similar motivation, we would like to consider the task of finding events relevant to a user in their current location, but because no large corpora of events exist, we consider first the task of extracting local events.
There are numerous works that identify the availability of semantic web information [27, 21, 3] but there is very little prior work on using this information as a source of distant supervision. Petrovski et al. use Schema.org annotations for products to learn regular expressions that help identify product attributes such as CPU speed, version, and product number [22]. Gentile et al. work on dictionary-based wrapper induction methods that learn interesting XPaths using linked data [7, 8]. Because Linked Data websites like DBPedia and Freebase are not typical web pages as those with Schema.org data, the structural features we are able to learn are not available in these works. We also attempt to learn about less structured fields, in particular, our What? aspect of events.
Another similar work comes from the historical domain. Smith worked on detecting, and disambiguating places and dates within historical documents in a digital libraries setting. He looked at collocations between these places and dates as events, and ranked them to identify significant date-place collocations that might merit further study by historians [32]. In a follow-up work, he looked at associating terms with these collocations and building interfaces for browsing [31].

424

2.2 Event Detection in other Domains
There is an entire class of work on detecting events within microblogs or real-time social media updates [26, 36, 20]. Becker describes identification of unknown events and their content in her thesis, but focuses on trending events on social media sites, and classification is used to separate event from non-event content clusters [2]. Our work, in contrast, is looking to find events in an offline manner that are explicitly described so as to present them in a search or recommendation system to users.
Kayaalp et al. discuss an event-based recommendation system integrated with a social media site, but focus on events already available through Facebook and Last.fm [14]. In this work, we consider these social media sites as two examples of the larger concept of "head domains" which are likely to have or adopt semantic web technologies, or be worthwhile candidates for supervised wrapper induction techniques.
Extraction of named events in news is another topic in which there has been work, e.g., [15]. Again, the definition of these news events is different from our local event definition.
2.3 Information Extraction
In the extraction domain, there is an immense amount of work on reducing boilerplate, whether directly, through template extraction [10, 23, 18], or through the more general idea of region extraction and classification. Sleimen and Corchuelo provide a detailed survey of region extractors in web documents, including those that leverage visual features [29]. Region extractors, in general, attempt to classify portions of a page into their purpose, i.e. navigation links, main content, and are studied in part to index only the content present on web pages. At least part of our task could be phrased within this context: an attempt to classify regions as either containing an event or not.
Work on temporal knowledge bases and extraction needed for automatic construction is similar to our task [11], except in a different domain and context. Most popular knowledge bases, have some ties to Wikipedia, which requires articles to be notable. By definition, we are seeking to extract events that do not have global relevance.
Additionally, there are a number of works that focus on repeated structure for extraction. Taking cues from HTML tables [9, 17, 4, 1, 39], or repetitive command tags or terms in general [34], these techniques do not require microdata, but require repetitive structure and make the assumption that there will always be a multitude of records to extract [35]. Recently, there has been a number of works about extracting data from web documents that were generated from the same templates [12, 30, 28]. In contrast to these works, we are interested in extracting local event records, which may or may not be generated from a pre-specified template.
3. EVENT EXTRACTION MODEL
We approach the task of extracting structured event information as a scoring or ranking method. We do this because we do not expect our target data ­ the listings of events on the web ­ to be particularly clean or perfectly aligned with any schema. Therefore, we wish to have a technique that joins several field scoring functions, which together give a multi-faceted score of how well the extracted structure fits our model of an event. The scoring functions take into account the source web page (Section 3.1), extraction

sub-region (Section 3.2), and the individual extracted fields (Section 3.3).
The fact that there may be multiple extractions from overlapping regions on the page, poses an additional challenge of grouping the extracted fields into disjoint event records that contain event name, date, time and location. To this end, we propose a greedy selection and grouping algorithm that provides an efficient way to optimize the quality of extracted event records. This algorithm is described in detail in Section 3.4.
Formally, we represent each extracted event record as a set of fields (F = {f1, f2 . . . fn}), their enclosing region (R), and the source document (D).
Figure 2: Example of an HTML document structure, presented visually. The boxes present represent nodes, the white boxes are the ones that are plausibly detected as fields in this example. Subsets of these fields represent possible event extractions.
An example HTML document is shown in Figure 2. If we consider the case where this document represents only one event, it is likely that Fields A,B, and C (F ) are the key fields in our event, and that the date in the navigation bar and in the copyright are false-positive fields. An event extraction only including the copyright field should get a low score, and similarly, the event containing all the fields should be considered too big and also receive a low score. We define the region of a fieldset to be the first shared parent of its fields, so for fields A and B, the R should be the box that contains all lettered fields, the image and the text node.
We formulate our scoring function, , which relates all our available context, as follows
(F , R, D) = (D)(R)(F ) We discuss the scoring components in order, and note where we adapted this object-general model to our event domain. Our document scoring, (D), our region scoring (R), and our field scoring (F ) are discussed in depth in the following sections.
3.1 Document Scoring
We want our event scoring function to take into account the page's likelihood of discussing an event, therefore we include a document-level score ((D)).

425

We take a naive-Bayes approach to this problem, and examine the probability of a web page discussing an event (belonging to the event class). We denote the candidate document D and the event class E, and consider the following ratio
P (E|D) > 1.
P (E|D)
In other words, we consider a page worth investigating if the probability of it belonging to the event class (E) is higher than the probability that it does not belong to the event class (E). In practice, the division of small fractions can lead to underflow, so we consider the equivalent relationship, asking whether the so-called log-odds of a page belonging to an event class is greater than zero.

logP (E|D) - logP (E|D) > 0
We estimate these probabilities based upon the commonlyused language modeling framework introduced by Ponte and Croft [24]. In the language modeling framework, we estimate the probability of a word given a model X (which will be one of {E, E}) as the probability of drawing it randomly from the bag of words that is that model.
tf (w, X) P (w  X) =
tf (, X)
The bag of words assumption means that we can treat all our terms as being independent, and we estimate the probability of a whole document by the probability of all its component terms (w  D) under the model.
tf (w, X) P (D  X) =
tf (, X)
wD
Because our event class may be sparse in comparsion to any given document, we apply linear smoothing [13] to our positive class to avoid zero probabilities.

P (E|D) = P (w  E) + (1 - )P (w  C)
wD
In contrast, because we approximate our non-event class by the language model of the entire collection, E = C, no smoothing is needed because all terms are present by construction.

P (E|D) = P (w  C)
wD
Since we run our algorithm on millions of pages, we chose to use this page scoring mechanism as a first pass filter, restricting our calculation of other scoring components to those whose (D) > 0, where  is defined as follows:

(D) =

1 logP (E|D) - logP (E|D) > 0

0

otherwise

What remains now is identifying an initial set of documents that are used to construct the language model for the event class E. For this, we bootstrap existing semantic annotations on the web. Specifically, we consider all the

pages that contain any http://schema.org/Event annotations (see Figure 1 for an example) as belonging to the event class E, since mark-up by their creators suggests that they discuss events. Overall, we have close to 150,000 such pages, which allows creating a robust event model.

3.2 Region Scoring
Region scoring ((R)) is considered on the enclosing region R in the document. Since this region encloses all of event potential fields, it is a good unstructured representation of the extracted event. In fact, we present this enclosing region to annotators to understand our event prediction performance separately from our field prediction.
Therefore, we decided on a simple region filtering approach, which simply removed from consideration regions above certain length

(R) =

1 |R| <  0 otherwise

 is set to 212 in all the subsequent experiments. Our empirical evaluation have shown that this simple approach effectively removed the majority of bad regions. In fact, we considered a number of more sophisticated approaches, including learning probability distributions of the size of the region and enclosing region tags. However, in experiments not listed here due to space constraints, we found the effects of such additional information negligible, especially since region features were included on a per-field basis within our field-scoring functions (see Section 3.3).

3.3 Field Set Scoring
We explore field scoring in a way that requires at least one of each required field to be part of our extracted field set (F ). Therefore our formulation for (F ) includes both a scoring function S and an indicator function that tests for required fields, R.

(F ) =

S(F ) R(F )

0

otherwise

We will discuss the breakdown of the S and R functions below. Generally, we jointly score field occurrences, and this joint scoring considers the independent scores, k(f ), assigned to each field f of type k  {What, When, Where}.

3.3.1 Independently Scoring Fields
In this work, we consider a number of ways to assign independent scores to fields that allows us to define k(f ) for any f and k  {What, When, Where}. Formally, we define f as a tuple with a (begin, end) index range within the source document D. Because the document itself has some HTML structure, the offsets within the field allow us to understand where in the structure of the page it occurs and leverage that as part of our scoring features.
Pattern-based approaches to tagging of dates, times and postal addresses in text yields results of reasonable accuracy, as evidenced by the approaches in HeidelTime [33], StanfordNLP [19], as well as some prior work on address detection [38]. Therefore, we consider pattern-based baselines for our extraction of event `When' and `Where' fields. While dates, times, and places have inherent and sometimes obvious structure, patterns do not make sense as a baseline for the `What' of an event, so we assign an equal score to all candidates.

426

Table 1: Features used in Field Classification.

Category Feature

Description

Text

Unigrams

Stopped and stemmed

unigrams, hashed to

10,000 count-features.

Bigrams

Stopped and stemmed

bigrams, hashed to

10,000 count-features.

NLP

Capitalization Ratio of terms capital-

ized, first term capital-

ized.

Address overlap Number of address fields

that overlap the target

span.

Date overlap

Number of date fields

that overlap the target

span.

Time overlap

Number of time fields

that overlap the target

span.

Structural Size

Ratio of size to parent

and to page.

Location

Ratio of start and end

locations to page.

Attribute Text Unigrams present in at-

tributes; can capture

style information.

Parent tag

Hashed vocabulary of

size 10 of lower-case par-

ent tag.

GrandParent tag Hashed vocabulary of

size 10 of lower-case par-

ent's parent tag.

Sibling tags

Hashed vocabulary of

size 1000 sibling tags.

Reverse XPath 10 features for each

XPath entry going back-

wards toward HTML.

No classification uses baseline approaches for all fields.
What(f ) = 0.5 Where(f ) = matches(f, Address) When(f ) = matches(f, Date/Time)

What classification uses baseline approaches except for What fields.
What(f ) = WW T hat · Xf Where(f ) = matches(f, Address) When(f ) = matches(f, Date/Time)

What-When-Where classification uses multiclass classification to rescore the boolean baseline approaches for all fields.
What(f ) = WW T hat · Xf Where(f ) = matches(f, Address) · WW T here · Xf When(f ) = matches(f, Date/Time) · WW T hen · Xf

Our baseline methods are implemented by the function matches(f, rk) where f is a field, and r is a set of regular expressions for field type k, returning 1 if a field f is considered a match for field type k, and 0 otherwise.
Our classification methods leverage features Xf extracted
from the candidate field f and weights Wk learned using LI-

BLINEAR [6]. The features used encompass textual, naturallanguage, and structural features that are more fully described in Table 1. Evaluation of these prediction methods is discussed in Section 5.1. All other evaluations consider only the What-When-Where classification method for independent field scoring.
In order to train our classification methods we turn once again to the pages in the event class E, described in Section 3.1. Using these pages, we label all the HTML tags with semantic mark-up related to one of our three target fields (`What', `When', `Where') with their respective field type. In addition, we label all other textual HTML tags on these pages as `Other'. We then use this bootstrapped training data to construct a multiclass classifier with label set
K = [ W hat , W hen , W here , `Other ],
and learn a weight vector Wk, for each k  K. See Section 4.3 for more details on this process.
3.3.2 Jointly Scoring Fields
Given F R = {What, When, Where} as the set of required fields for an event, a field set F has all its required fields if and only if R(F ) is true. We make the assumption that the field type k, with the maximum score k(f ) is the PredictedType of the given field f . Formally:
PredictedType(f ) = argmax k(f )
kF R
We test for the presence of all required fields by containment; the required fields should be a subset of the predicted types of the entire field set we are scoring. As an example, a set of fields may comprise an event if it has an extra date or time, but it may not if it doesn't have a date or time at all.

R(F ) = F R  {PredictedType(f )|f  F }
We combine the individual field scores within S(F ), our field set scoring function, using the same notation as before for per-field-type scorers (k(f )).

S(F ) =

max k(f )

kF R

f F

The individual score for each field is still labeled with a function, k(f ), which computes the score for the maximallylikely class for each field, reusing F R to describe the set of required classes.
In this formulation, we expect the output range of k to be [0, 1]. Since our independent scores are all in this range, it means that our function for S will tend to prefer field sets with fewer higher-scored fields, with R ensuring that we do not consistently predict incomplete events.

3.4 Event Record Grouping Algorithm
We consider all HTML tags that contain any baseline (regular expression pattern-based) matches on the page to be candidates for field scoring, rather than exhaustively iterating over all the subsets of text fields on the page. Even with this relatively-smaller number of candidate fields, the prediction algorithm is computationally difficult. Recall that we have formulated our field scoring as a ranking problem. Therefore, grouping these ranked fields into complete event records is a variation of the general subset selection problem, which is known to be NP-hard in most cases [37].

427

To ameliorate this problem, we choose to add a constraint that no field may be used twice (predicted events should not overlap) and to use a greedy approach to assigning fields to event records, so that we at least are able to predict the best events correctly. This greedy algorithm for event record grouping is presented in Algorithm 1. We rank our predicted nodes by score, and greedily select the highest scoring events, whose enclosing regions (R) do not overlap.
Algorithm 1 Greedy event record grouping algorithm.
# All tags and all regex matches on the page: candidate fields = {span, . . .} # All tags on the page containing candidatef ields. candidate tags = {span, . . .}
# Score and collect candidates possible events =  for R in each candidate tag:
F = set(candidate fields inside R) score = (F , R, D) if score > 0:
add (score, F , R, D) to possible events
# Greedily select highest scoring, non-overlapping tags output =  for event in sorted(possible events):
if event fields do not overlap with any fields in output: add event to output
return output;
As another nod to efficiency, we implement  by considering the parts independently, in terms of cost. Because (D), (R), and R(F ) are simple to calculate and may return zero scores, we calculate these parts first and skip the relatively-more expensive independent field calculations (S(F )). Additionally, this reduces the number of field sets that must be considered, lowering the overall cost of the algorithm in practice.
An example HTML structure is shown in Figure 3. Nodes D,E,F, and G are textual nodes, which contain fields. D has been detected as a What field, E has been detected as a When field, F has both a Where and a partial When (only a relative date) and maybe a What field. Node G is an example of a date that is nearby but irrelevant to the event itself.
Running on this example, our algorithm would walk through the nodes, assigning  scores to each node. Node D would receive a score of zero, because it is missing required field types (R). Similarly, nodes E,F, and G would be removed from consideration due to missing fields. The best selection for this event would either be Node B or Node C, depending on whether Node D is required to describe the event or if Node F is considered sufficient What explanation. Because of our joint field scoring, even if we do not apply a What field classifier, Node A will receive a lower score since it has an extra date field, which will lower its S score and overall score.
3.4.1 Deduplication
During development, our annotators reported seeing numerous instances of duplicate events, particularly of high-profile events that were advertised on many sites. Therefore, we

Figure 3: Example document structure handled by our algorithm. The boxes labeled with letters represent nodes in the HTML structure. Nodes D-G are textual nodes whose text is present. The lines display the parent-child relationships in the tree.
applied an exact-string matching duplicate detection method that ignored differences in capitalization, HTML tags and whitespace as a baseline and found that it was difficult to improve upon this baseline as our annotators no longer reported finding duplicates in our sampling of events. In all following analyses, we deduplicate all the extracted events using this method. We leave using extracted date/time and place information to perform a more domain-specific duplicate detection for future work.
4. EXPERIMENTAL SETUP
We evaluate our technique for extracting events on the ClueWeb122 corpus, which contains about 733 million pages. We extract http://schema.org/event records as microdata and rDFa in a first pass. This generates 145,000 unique web pages with events, and about 900,000 events on those pages. After deduplication, there are 430,000 unique events. Of these unique events, we map Schema.org properties to our three fields: What, Where, and When, and we are left with 217,000 complete events without any further processing.
We use incomplete events to train our field classifiers, but we will later consider recall in respect to the number of unique, complete Schema.org events, and we remove all pages with Schema.org events from our processing of the corpus moving forward, except for training data generation purposes.
4.1 Collecting Judgments
During the development of our system, we collected 2,485 binary judgments for understanding precision of event predictions. Each of these judgments contains the ClueWeb12 id, the start and end offsets of the events (in bytes after the WARC header), and a judgment: 1 if it is an event, and 0 otherwise.
For a large number of events in our best method, and in our method comparsion, we evaluate at the field level as well as at the event level, but only if the extraction actually is an event. For these judgments, as well as the agreement judgments, we turn to a crowdsourcing platform (Amazon
2http://lemurproject.org/clueweb12/

428

Mechanical Turk) to rapidly accumulate high-quality perfield judgments. We paid annotators $0.05 for event-level judgments and $0.10 for field-level judgments. We allowed annotators to mark I can't tell from the available information for any field or event, and removed these from our evaluation (Only 46 events in total out of the 2500).
For the fields, we asked annotators if they believed the selected snippet answered the given question about the event. As an example, we asked annotators to be picky about the "When" field in particular: two of our options were No, it only includes a date and No, it only includes a time.
4.2 Annotator Agreement
Figure 4: Agreement on Events and Fields
We calculated agreement on a set of thirty events randomly selected from our best-performing technique. Results are displayed in Figure 4. Because we asked our annotators for reasons when a field was inaccurate, we have some data on which the annotators agreed the field was incorrect, but disagreed on the reason: for example, one reviewer said the field was too short and the other said that they could not tell.
Our agreement is relatively high (>80%) on all but the "What" field, and our local reviewers found that agreement tended to be correlated with quality. The relative field performance here displays the ambiguity inherent in deciding what should describe an event in comparison to the stricter definitions of location and time.
4.3 Training Field Classifiers
Earlier, in Section 3.3.1, we discuss our approach to assigning scores to fields, and our classification methods. We list the features that we extract in Table 1, and we have already mentioned that we were able to parse 430,000 unique events from websites with Schema.org markup.
The question remains - how do we use this data as training for What Classification and What-When-Where Classification? Our first attempt to learn from this data involved a cross-fold evaluation setup, where we split events by domain into train, validate and test splits, extract features, and try to predict the fields in our validate and test sets. We split by domain here to better prepare ourselves for new data in our cross-validation setup, to avoid overtfitting to head domains that would have been present in all cross-validation sets.
While Schema.org gives us plenty of positive examples for each field, we don't necessarily have obvious negative examples. In the end, we found that taking all the tags, including other Schema.org properties, gave us the richest, most accurate representation of our problem space. We want

our field classifiers to be able to select tags that actually correspond to an event from all the other tags on a page that has events. In early experiments, we found that using a singleclass classifier was a mistake - the best negative examples for What include the positives for When and Where, as judged by the way our What field initially predicted dates and times and places. We moved to a multiclass setup, and this seemed to improve on this problem. Even for our What Classification setup, we emitted training, validation, and test data. We used our validation data to tune the C and parameters for LIBLINEAR and to choose between L1 and L2 normalization.
Even with the care taken to select negative examples, our cross-validation estimate of field classifier performance was still extremely high (F1 > 75%) for our three fields, which as we discuss later, is not the case when evaluated on pages without Schema.org.

5. RESULTS

5.1 Evaluating our Field Prediction Methods

Table 2: Fields Classified by Method. (N,W) denote significant

improvements, where 99% of bootstrap samples of the target

method have higher precision than 99% of samples from the None

and What methods, respectively.

None What What-Where-When

Event 0.54 0.51

0.76 (N,W)

What 0.09 0.30 (N) 0.36 (N)

When 0.17 0.20

0.32

Where 0.32 0.32

0.66 (N,W)

Figure 5: Comparison of Precision of Field Detection Methods
A key part of our overall event extraction system is the ability to assign meaningful independent scores to fields. We introduced three field scoring systems in Section 3.3.1 based on our baseline ability to detect Where/When aspects of events through regular expressions, and combining these baselines with classification.
In the No Classification system, we don't use any training data, and end up leveraging text near our Where/When fields for our What predictions. In our What Classification system, we use classification on only the What field, for which the baseline is weakest, and in our What/When/Where Classification system, we combine our baseline and classification approaches for all fields, leveraging the semi-supervision of our Schema.org extracted fields to learn weights for features as described earlier.

429

Table 3: Precision Levels used in evaluation.

V. High High Medium Low

Predicted 25,833 201,531 452,274 1,575,909

Increase 12%

93%

208% 725%

Judgments 155

567

482

491

Precision 0.92

0.85

0.65

0.55

In order to compare these methods, we first attempted to use our semi-supervised data - the Schema.org events and fields. Unfortunately, on such an event-focused dataset, all of our methods performed well. We believe that because of the format of microdata, and how it requires that each field be represented succinctly and separately in its own tag, these training instances were actually too simple. As a result, we were not able to leverage the Schema.org data to compare methods, and we had to create our own judgments.
We chose the top 30,000 pages by our document scoring method: (D), and ran our algorithm three times on each page, once with each of our classification methods, and generated a pool of events from each. From each pool we sampled 100 random events (any attempt at a pairwise comparison would not have accurately compared the different methods).
We used bootstrap sampling with 10,000 samples to obtain 99% confidence intervals for each of our evaluations. We mark improvements where the 1% (worst-case) of the treatment is greater than the 99% (best-case) of the baseline. The improvements are marked in terms of the None baseline (N) and the What baseline (W), and the raw, mean values are presented in the Table 2.

5.2 Exploring Precision and Recall

In Figure 6, precision for each component of the event is illustrated. As overall precision drops, we can see how the weakest part of our detection is still the Event title, or the What field. Intuitively, this makes sense, as it is less structured and even our annotators had trouble agreeing on What field with respect to the other fields. Another finding that's interesting here is that even though the field precision degrades rapidly, (partly because it is conditioned on the event being correct) our event precision is significantly higher, particularly in the "Low" precision level. This suggests that even our noisy field detection has value, particularly to a selection algorithm like ours.
5.3 Supervised Evaluation
As a qualitative observation, when we had false-positives for event classification, they were often near-misses, in the form of album releases, band histories, obituaries, or biographies. They tended to be rich in field information, but from the wrong domain. In general, our technique introduced some false positives as the domain broadened from the focus on commercial music events of the Schema.org events.
Since we had collected a large number of judgments, we decided to briefly explore a human-supervised approach. In all other sections, we discuss distant supervision results, where we had no gold-truth human assigned labels, but only the silver-truth of the Schema.org data. This evaluation serves two purposes: (1) determining whether our impression of easily-detected domain mismatches is true, and (2) evaluating the amount of effort needed to apply a supervised approach to this problem.
Since we were interested in understanding annotator effort, we only used our 1100 Mechanical Turk annotations for this part, as they were automatically timed. We break our judgments into two parts, taking 800 events from medium to very-high precision, as our training and validation data, and 300 events in our low-precision bracket (original Precision of 0.55). We build a simple unigram classifier with 10,000 hashed term features as well as document, region, and individual field scores for the best field of each kind from those predictions, again using LIBLINEAR.

Figure 6: Precision Evaluated at Recall Levels
As we discussed earlier, there were only about 217,000 unique, complete events used in our semi-supervised approach for training. We cannot evaluate our recall perfectly because we simply do not know how many events are on the internet, or even in ClueWeb12's 700 million pages. Additionally, we think the most interesting way to evaluate recall is to consider a number of interesting recall points (precision categories) for evaluation based on the ratio of events predicted to the size of our semi-supervised input (Schema.org) events).
We consider a "Very High" precision category, chosen because it was a cutoff found to have over 90% precision. Our "High" precision category is chosen because it represents about a 1:1 prediction to training ratio. Our "Medium" precision category captures the case where we predict two events for each training instance, and our "Low" precision category contains the lowest score range in which we collected judgments, at about a 7:1 prediction ratio. These pseudo-recall levels are listed in Table 3.

Figure 7: Supervised Event Prediction of Low-precision data based on other judgments. On the left, we have the training/validation data, and their precision represented (Very High, High, and Medium), and on the right, we have the test data, and its original precision (Low) and its new precision under supervision (Supervised).
This classifier works extremely well over our 300 evaluation events from the low range, boosting the precision from 0.55 to 0.72, with a 92% relative recall (the new classifier rejected another 8% of the 300 events). This experiment is displayed

430

in Figure 7, where the blue bars correspond to the training data, the red bar with the original performance of the lowprecision events, and the green bar with the supervised precision of the low-precision events.
This experiment supports our claim that a large amount of false-positives from our technique are simple to eliminate with minimal training data and annotation effort (~30 total hours of annotator time for the 1100 train/validate/evaluation judgments used).
5.4 Geographic Evaluation
We use a publicly-available geocoding and address API3 to convert our extracted "Where" fields into full street addresses, and latitude/longitude information. The street addresses are used to evaluate our performance on future retrieval tasks, and the latitude and longitude pairs are used to generate maps to better understand our coverage and that of the Schema.org data.
5.4.1 City Coverage around the World

Table 4: Retrieval Metrics for 200 random cities.

MRR@5 P@1 P@2 P@3 P@4 P@5

0.78

0.71 0.70 0.69 0.70 0.70

The TREC Contextual Suggestion Track considers the task of recommending venues or destinations to users in new cities [5]. In this work, we briefly evaluate the suitability of our dataset to be applied to a similar task: the task of recommending events to users based on their location. Because the work we present is not about the recommendation task itself, but rather the extraction of events, we evaluate our ability to provide event coverage in a random city.
We group our events by city, and select events from 200 cities at random. We then judge, using Amazon Mechanical Turk, the 5 highest scoring events from each city. We evaluate as a retrieval task, where our query is the random city, and our ranking function for now is our confidence in the event prediction. The mean reciprocal rank (MRR) of finding an event is 0.78 on this data: on average there is an event at either rank 1 or rank 2. Our event precision at 1 is 0.71. Full results are listed in Table 4. The precision numbers here seem flat because there tend to be five or so valid events for each location. It is nice to see this consistency, because it suggests that our system is able to find many events for randomly selected locations.
5.4.2 Geographic Display
In Figure 8, we display a small world map where our events are plotted to understand our international coverage. Since the dataset we used, ClueWeb12 is biased toward English pages, it is understandable that the events we have extracted are mostly found in the United States, and north-western Europe. There is also some token coverage in other parts of the world, including India and Australia, but we focus on areas that we expect to have good coverage based on the ClueWeb12 English-speaking bias.
In Figure 9, you can see our events plotted on a US Map in two colors: green for high and very-high confidence P >= 0.65, and yellow for 0.55 < P < 0.65. The blue dots are events parsed from Schema.org pages. The key takeaway from this
3https://developers.google.com/maps

Figure 8: Events across the world. Note that we have good coverage of countries where English is the dominant language, in a reflection of our corpus, ClueWeb12.
Figure 9: Events within the United States. Blue data points indicate Schema.org data, Green represents high confidence (P=0.65..0.85+), and Yellow represents low to medium confidence points (P=0.55..0.65). All points have more than 5 unique events, and are logarithmically scaled according to weight. map is that Schema.org events are definitely more prevalent in large cities, specifically in New York, Los Angeles, and Chicago, and visually, our technique does quite a good job of expanding coverage geographically. Note that we only plot a point if there are more than five unique event occurrences at that location, and we have restricted our sampled precision to be above 0.55, and there are many more events at the larger dots, as their size is logarithmically scaled.
6. CONCLUSION
In this work, we introduce a new task inspired by the TREC Contextual Suggestion Track and the increasing population of mobile users: to retrieve and recommend events that users might want to attend. Because we are particularly interested in local events, our work focuses on the identification and extraction of events on the open internet.
We show that using semantic web technologies, and specifically Schema.org microdata to collect distant supervision data can be useful for training semi-supervised approaches to extraction problems like ours. This setup has the advantage that as more organizations adopt such technologies, the performance of our extractions will increase over time without any additional labeling effort.
We propose a novel model of event extraction based on independent field scoring and a greedy algorithm for grouping these fields into complete event records, with confidence scores assigned to them. Our features for field classification

431

allow us to combine elements commonly seen in region extraction approaches with both information retrieval-based and linguistically-inspired features. As the proposed model depends heavily upon the ability to assign individual field scores, we evaluate a number of score assignment methods, and find that classification that learns from using the distant supervision of the Schema.org data is significantly beneficial.
7. ACKNOWLEDGEMENTS
This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
8. REFERENCES
[1] M. D. Adelfio and H. Samet. Schema extraction for tabular data on the web. VLDB'13, 6(6):421­432, 2013.
[2] H. Becker. Identification and characterization of events in social media. PhD thesis, Columbia University, 2011.
[3] C. Bizer, K. Eckert, R. Meusel, H. Mu¨hleisen, M. Schuhmacher, and J. V¨olker. Deployment of rDFa, microdata, and microformats on the web­A quantitative analysis. ISWC, pages 17­32, 2013.
[4] M. J. Cafarella, A. Halevy, and J. Madhavan. Structured data on the web. CACM'11, 54(2):72­79, 2011.
[5] A. Dean-Hall, C. L. Clarke, J. Kamps, P. Thomas, N. Simone, and E. Voorhees. Overview of the trec 2013 contextual suggestion track. Technical report, DTIC Document, 2013.
[6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871­1874, 2008.
[7] A. L. Gentile, Z. Zhang, I. Augenstein, and F. Ciravegna. Unsupervised wrapper induction using linked data. In K-CAP '13, pages 41­48, New York, NY, USA, 2013. ACM.
[8] A. L. Gentile, Z. Zhang, and F. Ciravegna. Self training wrapper induction with linked data. In Text, Speech and Dialogue, pages 285­292. Springer, 2014.
[9] R. Gupta and S. Sarawagi. Answering table augmentation queries from unstructured lists on the web. VLDB'09, 2(1):289­300, 2009.
[10] S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. Dom-based content extraction of html documents. In WWW'03, pages 207­214. ACM, 2003.
[11] J. Hoffart, F. M. Suchanek, K. Berberich, and G. Weikum. Yago2: a spatially and temporally enhanced knowledge base from wikipedia. Artificial Intelligence, 194:28­61, 2013.
[12] J. L. Hong, E.-G. Siew, and S. Egerton. Information extraction for search engines using fast heuristic techniques. Data & Knowledge Engineering, 69(2):169­196, 2010.
[13] F. Jelinek and R. L. Mercer. Interpolated estimation of markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, 1980.
[14] M. Kayaalp, T. Ozyer, and S. Ozyer. A collaborative and content based event recommendation system integrated with data collection scrapers and services at a social networking site. In ASONAM'09, pages 113­118. IEEE, 2009.
[15] E. Kuzey, J. Vreeken, and G. Weikum. A fresh look on knowledge bases: Distilling named events from news. In CIKM'14, pages 1689­1698. ACM, 2014.
[16] D. Lagun, A. Sud, R. W. White, P. Bailey, and G. Buscher. Explicit feedback in local search tasks. In SIGIR'13, pages 1065­1068. ACM, 2013.
[17] G. Limaye, S. Sarawagi, and S. Chakrabarti. Annotating and searching web tables using entities, types and relationships. VLDB'10, 3(1-2):1338­1347, 2010.

[18] R. Manjula and A. Chilambuchelvan. Extracting templates from web pages. In Green Computing, Communication and Conservation of Energy (ICGCE), 2013 International Conference on, pages 788­791. IEEE, 2013.
[19] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky. The Stanford CoreNLP natural language processing toolkit. In ACL, pages 55­60, 2014.
[20] D. Metzler, C. Cai, and E. Hovy. Structured event retrieval over microblog archives. In ACL'12, pages 646­655. Association for Computational Linguistics, 2012.
[21] H. Mu¨hleisen and C. Bizer. Web data commons-extracting structured data from two large web corpora. LDOW, 937, 2012.
[22] P. Petrovski, V. Bryl, and C. Bizer. Learning regular expressions for the extraction of product attributes from e-commerce microdata. 2014.
[23] J. Pomik´alek. Removing boilerplate and duplicate content from web corpora. Disertacni pra´ce, Masarykova univerzita, Fakulta informatiky, 2011.
[24] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR'98, pages 275­281. ACM, 1998.
[25] J. Pustejovsky, P. Hanks, R. Sauri, A. See, R. Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro, et al. The timebank corpus. In Corpus linguistics, volume 2003, page 40, 2003.
[26] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake shakes twitter users: real-time event detection by social sensors. In Proceedings of the 19th international conference on World wide web, pages 851­860. ACM, 2010.
[27] M. Schmachtenberg, C. Bizer, and H. Paulheim. Adoption of the linked data best practices in different topical domains. ISWC, pages 245­260, 2014.
[28] H. Sleiman and R. Corchuelo. Trinity: on using trinary trees for unsupervised web data extraction. 2013.
[29] H. A. Sleiman and R. Corchuelo. A survey on region extractors from web documents. Knowledge and Data Engineering, IEEE, 25(9):1960­1981, 2013.
[30] H. A. Sleiman and R. Corchuelo. Tex: An efficient and effective unsupervised web information extractor. Knowledge-Based Systems, 39:109­123, 2013.
[31] D. A. Smith. Detecting and browsing events in unstructured text. In SIGIR'02, pages 73­80. ACM, 2002.
[32] D. A. Smith. Detecting events with date and place information in unstructured text. In Proceedings of the 2nd ACM/IEEE-CS joint conference on Digital libraries, pages 191­196. ACM, 2002.
[33] J. Str¨otgen and M. Gertz. Heideltime: High quality rule-based extraction and normalization of temporal expressions. In Workshop on Semantic Evaluation, ACL, pages 321­324, 2010.
[34] W. Thamviset and S. Wongthanavasu. Information extraction for deep web using repetitive subject pattern. WWW'13, pages 1­31, 2013.
[35] W. Thamviset and S. Wongthanavasu. Bottom-up region extractor for semi-structured web pages. In ICSEC'14, pages 284­289. IEEE, 2014.
[36] K. Watanabe, M. Ochi, M. Okabe, and R. Onai. Jasmine: a real-time local-event detection system based on geolocation information propagated to microblogs. In CIKM'11, pages 2541­2544. ACM, 2011.
[37] W. J. Welch. Algorithmic complexity: three np-hard problems in computational statistics. Journal of Statistical Computation and Simulation, 15(1):17­25, 1982.
[38] Z. Yu. High accuracy postal address extraction from web pages. Master's thesis, Dalhousie University, Halifax, Nova Scotia, 2007.
[39] Z. Zhang. Towards efficient and effective semantic table interpretation. In ISWC'14, pages 487­502. Springer, 2014.

432

Representative & Informative Query Selection for Learning to Rank using Submodular Functions

Rishabh Mehrotra
Dept of Computer Science University College London, UK
r.mehrotra@cs.ucl.ac.uk

Emine Yilmaz
Dept of Computer Science University College London, UK
emine.yilmaz@ucl.ac.uk

ABSTRACT
The performance of Learning to Rank algorithms strongly depend on the number of labelled queries in the training set, while the cost incurred in annotating a large number of queries with relevance judgements is prohibitively high. As a result, constructing such a training dataset involves selecting a set of candidate queries for labelling. In this work, we investigate query selection strategies for learning to rank aimed at actively selecting unlabelled queries to be labelled so as to minimize the data annotation cost. In particular, we characterize query selection based on two aspects of informativeness and representativeness and propose two novel query selection strategies (i) Permutation Probability based query selection and (ii) Topic Model based query selection which capture the two aspects, respectively. We further argue that an ideal query selection strategy should take into account both these aspects and as our final contribution, we present a submodular objective that couples both these aspects while selecting query subsets. We evaluate the quality of the proposed strategies on three real world learning to rank datasets and show that the proposed query selection methods results in significant performance gains compared to the existing state-of-the-art approaches.
Categories and Subject Descriptors
H.3.3 [Information Storage And Retrieval]: Information Search and Retrieval--Learning to Rank
Keywords
Learning to Rank, Query Selection, Active Learning, Submodularity
1. INTRODUCTION
Most modern search technologies are based on machine learning algorithms that learn to rank documents given a query, an approach that is commonly referred to as "learning to rank". Learning to Rank algorithms aim to learn ranking functions that achieve good ranking objectives on test data.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767753.

Such learning methods require labelled data for training. As is the case with many supervised learning algorithms, the performance of Learning to Rank algorithms are often highly correlated with the amount of labelled training data available[1][17][7].
Constructing such labelled training data for learning-torank tasks incurs prohibitive costs since it requires selecting candidate queries, extracting features from query-document pairs and annotating documents in terms of their relevance to these queries (annotations are used as labels for training). The major bottleneck in constructing learning-to-rank collections is annotating documents with query specific relevance grades. It is essential therefore, both for the efficiency of the construction methodology and for the efficiency of the training algorithm, that only a small subset of queries be selected. The query selection, though, should be done in a way that does not harm the effectiveness of learning.
Active Learning algorithms help reduce the annotation costs by selecting a subset of informative instances to be labelled. Unlike traditional algorithms, active learning strategies for ranking algorithms are more complex because of the inherent query-document pair structure embodied in ranking datasets, non-smooth cost functions, etc., hence these cannot be applied directly in ranking setting.
Existing approaches for active learning for ranking have focused on selecting documents [1], selecting queries [17] or balancing number of queries with depth of documents judged using random query selection [27].
In this work, we focus on selecting subset of queries to be labelled so as to minimize the data annotation cost. Prior work on selecting queries made use of expected loss optimization [17] to estimate which queries should be selected but their approach is limited to rankers that predict absolute graded relevance which is not the case with modern Learning to Rank algorithms since many of them induce a ranking and not absolute labels [4]. Apart from the learning to rank setting, query selection has also received significant attention for evaluation setting [13] wherein the goal was to find a subset of queries that most closely approximates the system evaluation results that would be obtained if instead documents for the full set of queries was judged instead. However, it was shown by Aslam et a.l [1] that learning to rank and evaluation of retrieval systems are quite different from each other and that datasets constructed for evaluating quality of retrieval systems are not necessarily good for training and vice versa. Therefore, query selection strategies that are directly devised for learning to rank purposes are needed.

545

Intuitively, an optimal subset of queries constructed for learning to rank should have two characteristics: (i) informativeness, which measures the ability of an instance (query) in reducing the uncertainty of a statistical model (ranking model) and (ii) representativeness, which measures if an instance (query) well represents the possible input patterns of unlabelled data (unlabelled queries) [22]. Most existing active learning for ranking algorithms solely focus on the informativeness aspect of queries without considering the representativeness aspect which can lead to possible selection of noisy queries, not quite representative of the whole population of queries; thus, significantly limiting the performance of query selection.
In this work, we focus on query selection strategies for learning to rank and propose novel query selection algorithms aimed at finding an optimal subset of queries to be labelled. Since problems associated with subset selection are generally NP-Hard or NP-Complete[12], we approximate the solution by an iterative query selection process so as to minimize the data annotation cost without severely degrading the performance of the ranking model.
We describe two paradigms of query selection strategies based on the aspects of informativeness and representativeness described above and propose novel query selection techniques: Permutation Probability based query selection and query selection based on topic models which capture these two aspects, respectively. We further present a new algorithm based on defining a submodular objective that combines the powers of the two paradigms. Submodular functions have the characteristic of diminishing returns [19], which is an important attribute of any query-subset selection technique since the value-addition from individual queries should ideally decrease as more and more queries are selected. Thus, not only are submodular functions natural for query subset selection, they can also be optimized efficiently and scalably such that the result has mathematical performance guarantees.
We show that our proposed algorithms result in significant improvements compared to state-of-the-art query selection algorithms thereby helping in reducing data annotation costs.
2. RELATED WORK
Active Learning for Labelling Cost Reduction: A number of active learning strategies have been proposed for the traditional supervised learning setting, a common one being uncertainty sampling which selects the unlabelled example about which the model is most uncertain how to label. Some of the others adopt the idea of reducing the generalization error and select the unlabelled example that has the highest effect on the test error, i.e. points in the maximally uncertain and highly dense regions of the underlying data distribution[9]. A comprehensive active learning survey can be found in [22].
Reducing judgment effort for learning to rank has received significant amount of attention from the research community. Learning to rank methods are quite different than approaches used for classification as they require optimizing nonsmooth cost functions such as NDCG and AP [24]. Moreover, owing to the unique query-document structure which inherent to the learning to rank setting, it is not straightforward to extend the models devised for traditional supervised learning settings to ranking problems. In recent

years, active learning has been actively extended to rank learning and can be classified into two classes of approaches: document level and query level active learning.
Document Selection for Learning to Rank: Based on uncertainty sampling, Yu et al [28] selected the most ambiguous document pairs, in which two documents received close scores predicted by the current model, as informative examples. Donmez et al.[8] chose those document pairs, which if labelled could change the current model parameters significantly. Silva et al [23] proposed a novel document level active sampling algorithm based on association rules, which does not rely on any initial training seed.
Query Selection for Learning to Rank: For query level active learning, Yilmaz et al. [27] empirically showed that having more queries but shallow documents performed better than having less queries but deep documents. They balance number of queries with depth of documents judged using random query selection. Cai et al. [5] propose the use of Query-By-Committee (QBC) based method to select queries for ranking adaptation but omit the evaluation of the query selection part and focussed on the ranking adaptation results instead. Long et al. [17] introduced an expected loss optimization (ELO) framework for ranking, where the selection of query and documents were integrated in a principled 2 staged active learning framework and most informative queries selected by optimizing the expected DCG loss but the proposed approach is limited to rankers that predict absolute graded relevance and hence not generalizable to all rankers. Authors in [2] adapt ELO to work with any ranker by introducing a calibration phase where a classification model is trained over in the validation data. Moreover, they show that estimating expected loss in DCG is more robust than NDCG even when the final performance measure is NDCG.
Thus, QBC attempts to capture the informativeness aspect of queries by selecting queries which minimize the disagreement among a committee of rankers while the Expected loss optimization based approach formulates informativeness in terms of expected DCG loss; both these approaches fail to capture the representativeness aspect of queries which we show outperforms both these approaches.
Submodular Maximization: Submodularity is a property of set functions with deep theoretical and practical consequences. Submodular maximization generalizes to many well-known problems, e.g., maximum weighted matching, max coverage, and finds numerous applications in machine learning and social networks. In Information Retrieval, submodular objectives have been majorly employed for diversified retrieval[29] & learning from implicit feedback[21]. A seminal result of Nemhauser et al. [19] states that a simple greedy algorithm, based on a submodular objective, produces solutions competitive with the optimal (intractable) solution. In fact, if assuming nothing but submodularity, no efficient algorithm produces better solutions in general [10].
3. QUERY SELECTION STRATEGIES
Our aim is to actively select the optimal subset of unlabelled queries for obtaining relevance judgements so as to reduce data annotation costs. Intuitively, the selected queries

546

should have two major properties: informativeness & representativeness. We describe both these properties below and provide intuitions motivating each.
3.1 Informativeness
Informativeness measures the ability of an instance in reducing the uncertainty of a statistical model[22]. Ideally, the selected queries should be maximally informative to the ranking model. In learning to rank setting, Informativeness based query selection focusses on greedily selecting queries which are most informative to the current version of the ranking model.
Different notions of informativeness can be encapsulated by different techniques depending on how query-level informativeness is quantified. Two possible measures of capturing a query's informativeness include: (i) Uncertainty based informativeness & (ii) Disagreement based informativeness.
Uncertainty based informativeness quantifies the querylevel information as the uncertainty associated with the optimal document ranking order for that query. Query selection strategies focusing on uncertainty reduction would greedily select the query instance about which the current ranking model is most uncertain about, thereby trying to reduce the overall uncertainty associated with the ranking model.
Disagreement based informativeness, on the other hand, quantifies the query-level informativeness as the disagreement in this query's document rankings among a committee of ranking models. The key idea here is that the maximally informative query is one about whose document rankings, the committee of ranking models maximally disagree; hence obtaining relevance labels for such a query would provide the maximum information. Among the existing approaches for query selection for ranking models, the Queryby-Committee [5] attempts to capture the Informativeness aspect of queries based on a disagreement measure.
3.2 Representativeness
Representativeness measures if an instance well represents the overall input patterns of unlabelled data [22]. Web search queries can span a multitude of topics and information needs, with even a small dataset containing a broad set of queries ranging from simple navigational queries to very specific domain-dependent queries. In learning to rank settings, this implies that selected queries should have strong correlation with the remaining queries, as without this correlation there is no generalizability and predictive capability. Different notions of representativeness can be defined covering different characteristics of individual queries. Improving the representativeness of the selected query subset improves the coverage aspect of the query collection - the more representative selected queries are, the more they cover the entire query collection.
3.3 Informativeness vs Representativeness
Selecting queries solely based on their informativeness aspects could possibly lead to selection of noisy queries. In line with the Meta-Search Hypothesis [14][15], rankers tend to agree on relevant documents and disagree about nonrelevant docs. Hence, the queries that a ranker is unsure about or there is big disagreements across rankers are likely to be the ones that contain a lot of nonrelevant documents. Such noisy, outlier queries which majorly have non-relevant documents would lead to maximal disagreement and uncertainty

among ranking models, and thus would be wrongly labelled maximally informative. Also, the set of informative queries might not necessarily represent the set of all possible queries, which lead to less coverage of the unlabelled query set.
On the other hand, selecting queries based on representativeness aspects could lead to the selection of a query that is very similar to the a query already in the labelled set and hence, does not provide much information to the ranking model. Despite being representative, such queries possibly offer redundant information to the ranking models. Ideally, a query selection algorithm should take into account both these aspects while selecting queries. Existing work has majorly looked into selecting queries by considering informativeness based on disagreement among rankers (Query-byCommittee) or informativeness in terms of expected DCG loss (Expected loss optimization). Both these approaches fail to capture the representativeness aspect of queries. In addition to a novel informativeness approach based on uncertainty reduction, we present a representativeness based approach and finally couple both these aspects for query selection via a joint submodular objective which jointly incorporates informativeness & representativeness.
As our first contribution, we present a novel informativeness based query selection scheme (§ 4) based on permutation probabilities of document rankings which tries to reduce uncertainty among rankers. While no existing query selection scheme for learning to rank incorporates the representativeness aspect of queries, we propose a LDA topic model based query selection scheme (§ 5) which captures the representative aspect of queries while constructing the query subset. An ideal query subset would have both informative & representative queries. As our third contribution, we combine the two paradigms of representativeness & informativeness by proposing a coupled model based on submodular functions(§ 6).
4. CAPTURING INFORMATIVENESS VIA PERMUTATION PROBABILITIES
Our first novel query selection scheme is aimed at capturing the informative-aspect of queries. We maintain a committee of ranking models C = {1, 2, ..., C } which are trained on a randomly selected subset from the current labelled set, and thus contain different aspects of the training data depending on the queries in their subset. It is to be noted that these ranking models could be generated using any learning to rank algorithm. Given the set of currently labelled query instances, our goal is to pick the next query (q) from the set of unlabelled queries by selecting the maximally informative query instance. The query-level informativeness is defined in terms of the uncertainty associated with the optimal document ranking orders among the |C| ranking models. We follow a similar approach as outlined by Cai et al.[5] to maintain a committee of rankers. However, unlike Query-By-Committee [5] which encapsulates informativeness via ranker disagreements, our approach presents an alternate view of informativeness based on uncertainty reduction wherein a ranking model's uncertainty for the query's document ranking order is defined based on the concept of permutation probabilities.
More specifically, each committee ranking model is allowed to score the documents associated with each query following which a permutation probability is calculated on the

547

ranking obtained on sorting these document scores. Thus, each query gets a permutation probability score by each committee member. The most informative query is considered to be the query instance which minimizes the maximum permutation probability of document scores given by each ranking model committee member.
We postulate that a query which has the minimum permutation probability score from among the maximum scores assigned between the different ranking models is maximally informative in the sense that even the best ranker among the committee is highly uncertain about its document rankings and hence this query obtained the least permutation probability score among the set of unlabelled candidate queries. We select a query for which the probability with respect to the most certain (maximum permutation probability) model is minimal, i.e., a query for which even the most certain committee member has minimum confidence.
To define permutation probabilities, we make use of the Plackett-Luce model [20]. The Plackett-Luce (P-L) model is a distribution over rankings of items (documents) which is described in terms of the associated ordering of these items (documents). We define P (|) as the probability of obtaining the ranking order () based on the score (k) assigned to each document (k) by the ranking model learnt thus far. For each query, we rank the documents based on the scores assigned by model learnt so far and calculate the probability of the ranking order obtained () using the permutation probability defined as follows:

P (|) =

i

i=1,...,K i + · · · + K

(1)

where each ranking  has an associated ordering of document scores  = (1, · · · , K ) and an ordering is defined as a permutation the K document indices with i representing the score assigned to document i (at rank i) by the ranking model. We make use of a committee of ranking models and select the maximally informative query based on a greedy min-max algorithm described next.

4.1 Min-Max PL Probability Algorithm
Building a Min-Max PL Probability based selection system involves two components: (i) building a committee of ranking models that are well diversified and compatible with the currently labelled data and (ii) computing permutation probabilities by each committee member for each query in the unlabelled set of queries & selecting maximally informative query as per the min-max score.

Committee Construction: Following the work of [5], we use query-by-bagging approach to construct the members. Given the set of currently labelled instances, bagging generates C partitions of sub samples by sampling uniformly with replacement, and then the committee can be constructed by training each of its members on one portion of the sub-sample partitions. We randomly initialize the initial set of labelled queries with a small base set of queries and their labelled documents. We sample with replacement for C times in the set of labelled queries and train a ranking model on each subset of queries. Such a sampling procedure allows us to create various different training datasets that each represent a subset of the data possibly having very different characteristics than each other. These C models represents our C committee mem-

bers. We set the size of each subset to be 50 % of the current labelled subset size at each step. The maximally informative query q is selected for annotation which obtains the lowest min-max score, the calculation of which is described below.
Calculating min-max score: For each query q in the candidate set of unlabelled queries, the C committee members return C ranked lists. Following the construction of |C| ranking models, for each ranking model per query, we sort the documents based on the scores given by the ranking model and compute the permutation probability of obtaining this ranking order.
Thus, each query has |C| permutation probability scores. In order to minimize the overall uncertainty associated with the ranking models, we select the maximally informative query q, i.e., the query that has the minimum value of the permutation probability assigned by its most certain committee member, i.e., the committee member that has the highest permutation probability score associated with the query's document ranking order. Thus,
q = argminqDu maxcC P (qc|qc)
c k k=1,...,K c k + · · · + c K
(2)
where each ranking qc has an associated ordering  = (1c, · · · , Kc ) and an ordering is defined as a permutation the K document indices with c k representing the score assigned to document k by the ranking model c.
5. CAPTURING REPRESENTATIVENESS VIA LDA TOPICS
A major drawback associated with pure-Informativeness based models is that often they tend to select outlier queries. As is confirmed by the Meta-Search Hypothesis [14][15], rankers tend to agree on relevant documents but disagree on non-relevant documents. In such a scenario, an outlier query which majorly has non-relevant documents would lead to maximal disagreement and uncertainty in the ranking model, and thus will be wrongly labelled maximally informative. This motivates the need for considering the representativeness aspect of queries.
The information-seeking behaviour of users tend to vary based on the search task at hand [25] which suggests that the importance of feature weights for queries belonging to different tasks or topics are likely to be very different. The relative importance of different features are likely to be very different for different tasks. For example, queries belonging to a topic such as news would warrant high authority websites to be ranked higher (i.e., larger weight on the pagerank score) while queries belonging to (say) educational informational content would prefer the documents better matched with their query terms be ranked higher (i.e., larger weight on the relevance features such as BM25). To capture these diverse variations in the feature weights, the training set should ideally be composed of representative queries from different tasks. This makes it necessary that the labelled set of queries have representative queries spanning the entire

548

array of different topics. We propose a Latent Dirichlet Allocation (LDA) [3] topic model based query selection scheme which tries to capture this insight by selecting representative queries which are most topically similar to the set of unlabelled queries.
Based on this intuition, we conjecture that representative queries would be those that are most similar to the set of unlabelled queries in terms of their topical distribution. To capture the heterogeneity among all queries in the search logs, we make use of the concept of latent topics. We learn these latent topics from the collection of queries and represent each query as a probability distribution over these latent topics. We train an LDA model, a generative model which posits that each document (query in our case) is a mixture of a small number of topics and that each word's (query term's) creation is attributable to one of the document's (query's) topics. Each query is represented as a feature vector corresponding to its distribution over the LDA topics. To find representative queries, we select the query with the maximum average similarity from among the unlabelled set of queries, i.e.,

q

=

argmaxq

1 |Du|

qi Du

sim(Tq ,

Tqi )

(3)

where |Du| represents the number of queries in the unlabelled set Du; Tq represents the query q's feature vector in the LDA topic space and sim(Tq, Tqi ) can be any similarity score between queries; we use the cosine similarity between
the topic-space representations of queries q and qi.

6. COMBINING REPRESENTATIVENESS & INFORMATIVENESS
The approaches discussed so far have looked at either the informativeness of queries and selected queries which are most informative in terms of their ability reduce the uncertainty of the ranking model or they have focussed on representativeness of queries and selected representative queries spanning the entire array of different topics. As we discussed earlier in subsection 3.3, optimizing for only one of the two criteria for query selection could significantly limit the performance of query selection by selecting suboptimal query subsets. In this section we present a way of combining the two objectives by means of submodular functions and propose a submodular objective which jointly captures the notions of representativeness and informativeness.

6.1 Submodular Functions
Submodular functions are discrete functions that model laws of diminishing returns and can be defined as follows:[19]: Given a finite set of objects (samples) Q = {q1, ..., qn} and a function f : 2S  + that returns a real value for any subset S  Q, f is submodular if given S  S , and q / S

f (S + q) - f (S)  f (S + q) - f (S )

(4)

That is, the incremental "value" of q decreases when the set in which q is considered grows from S to S . A function is monotone submodular if S  S , f (S)  f (S ). Powerful guarantees exist for such subtypes of monotone submodular function maximization. Though NP-hard, the problem of maximizing a monotone submodular function subject to a cardinality constraint can be approximately solved by a simple greedy algorithm [19] with a worst-case approximation

factor (1 - e-1). This is also the best solution obtainable in polynomial time unless P=NP [10].

6.2 Problem Formulation
Submodularity is a natural model for query subset selection in Learning to Rank setting. Indeed, an important characteristic of any query-subset selection technique would be to decrease the value-addition of a query q  Q based on how much of that query has in common with the subset of queries already selected (S). The value f (q|S) of a query in the context of previously selected subset of queries S further diminishes as the subset grows S  S. In our setting, each q  Q is a distinct query, Q corresponds to the entire collection of queries and S corresponds to the subset of queries already selected from Q.
Mathematically, the query subset selection problem can be formulated as selecting the subset of queries S which maximizes the value of f (S) where f (S) captures both the representativeness aspect as well as the informativeness aspects of queries. We next describe in detail the construction of such a monotone submodular function and later present a greedy algorithm to approximately solve the problem of query subset selection.

6.3 Submodular Query Selection
We model the quality of the query subset in terms of both the representativeness & informativeness. To capture both these traits, we model the quality of the query subset as:

F (S) = (S) + (1 - )(S)

(5)

where (S) captures the representativeness aspect of the query subset (S) with respect to the entire query set Q while (S) rewards selecting informative queries. The parameter  controls the trade-off between the importance of representativeness & informativeness while selecting queries. A single weighting scheme would not be suitable for all problems since depending on the constituent queries, size of the overall dataset and the size of the subset that needs to be selected, different weighting schemes would produce different results. The function F (S) will be monotone submodular if each of (S) and (S) are individually monotone submodular. We defer an in-depth analysis of the trade-off between representativeness & informativeness aspects to subsection 8.1 and next describe the details of both these functions.

6.3.1 Representativeness: (S)
(S) can be interpreted either as a set function that measures the similarity of query subset S to the overall query set Q, or as a function representing some form of "representation" of Q by S. Most naturally, (S) should be monotone, as representativeness improves with a larger subset. (S) should also be submodular: consider adding a new query to two query subsets, one a subset of the other. Intuitively, the increment when adding a new query to the small subset should be larger than the increment when adding it to the larger subset, as the information carried by the new query might have already been covered by those queries that are in the larger subset but not in the smaller subset. Indeed, this is the property of diminishing returns.
We employ the same functional form of (S) as was adopted by Lin et al.[16]. Specifically, a saturated coverage function

549

is defined as follows:

(S) = min {Cq(S), Cq(Q)}

(6)

qQ

where Cq(S) is a set based function defined as Cq(S) : 2S  and 0    1 is a threshold co-efficient. Intuitively,
Cq(S) measures how topically similar S is to query q or how much of the query q is covered by the subset S. Building on top of the earlier proposed LDA topic model based query selection, we define the coverage function Cq(S) in terms of the topical coverage of queries. More specifically,

Cq(S) =

wq,q

(7)

q S

where wq,q  0 measures the topical similarity between queries q and q . Since Cq(S) measures how topically similar S is to query q, summing Cq(S) q  Q would measure how similar the current subset S is to the overall set of queries Q. It is important to note that Cq(Q) is just the largest value Cq(S) can ever obtain because Q is the set of all the queries we have and it maximally represents all the information we have. We call a query q saturated by the subset of queries S when min {Cq(S), Cq(Q)} = Cq(Q). When q is saturated in this way, any new query cannot further improve the coverage even if it is very similar to the query q. Thus, this gives other queries which are not yet saturated a higher chance of being better covered and hence the resulting subset tends to better cover the entire set of queries Q.

6.3.2 Informativeness: (S)
The (S) function described above intuitively captures the notion of coverage or representativeness by selecting subset of queries S which are topically most representative of the entire set of queries Q. While representativeness is an important trait, we also wish to capture the informativeness aspect of queries and select queries which are most informative to the current version of the ranking model. We formulate the functional form of (S) based on top of the earlier proposed ways of encapsulating query-level informativeness in terms of either ranker disagreements or model uncertainity, or both. As a precursor, it is worth mentioning that to define the function (S) we make use of LDA topic model which gives us k-topics and we associate each query to one of these k-topics. Formally, we define the (S) function as follows:

K

(S) =

q

(8)

i=1 qPiS

where Pi, i = 1, ..., K is the topical-partition of the set of queries Q into K-topics and q captures the informativeness carried by the query q based on the current ranking model. The function (S) rewards topical-diversity along with valuing informativeness since there is usually more benefit to selecting a query from a topic not yet having one of its query already chosen. As soon as a query is selected from a topic, other queries from the same topic starthaving diminishing gain owing to the square root function ( 2+ 1 > 3+ 0). It is easy to show that (S) is submodular by the composition rule. The square root is non-decreasing concave function. Inside each square root lies a modular function with non-negative weights (and thus is monotone). Applying the square root to such a monotone submodular function yields

a submodular function, and summing them all together retains submodularity.
The informativeness of a query q can be defined based on the metrics proposed earlier. To incorporate the informativeness aspects of queries, we experiment with various different formulations of the singleton-query rewards (q) include the following::

· Disagreement Score for a query - this allows us to capture information about the disagreement about the document rankings for a query among a committee of ranking models [5]

· Uncertainty associated with the query - this allows us to capture the ranking model's uncertainty about the query's document rankings 4

· Combination of uncertainty & disagreement.

Based on empirical analysis, we find that the disagreement based reward functions perform better than the rest of the formulations across all datasets, so we skip the performance comparisons among these.

6.4 Greedy Optimization

Having defined the individual functions based on the different paradigms, we formulate the overall query subset selection problem as the selection of the subset S of queries which maximizes the following function:







F (S) =  min

wq,q , 

 wq,q

qQ

q S

q Q K

 (9)

+ (1 - )

q

i=1 qPiS

Modelling the query selection problem in such an objective provides many advantages. Firstly, the submodular formulation provides a natural way of coupling the different aspects of query selection. Secondly, the above formulation can be optimized efficiently and scalably given the monotone submodular form of the function F (S). Assuming we wish to select a subset of N queries from the total unlabelled set of Q queries, the problem reduces to solving the following optimization problem:

S = argmax F (S)
SQ,|S|N

(10)

While solving this problem exactly is NP-complete [10], tech-

niques like ILP [18] can be used but scaling it to bigger

datasets becomes prohibitive. Since the function F (S) is

submodular, it can be shown that a simple greedy algorithm

will have a worst-case guarantee of f (S)  0.63F (Sopt) where Sopt is the optimal and

(S1-is1e

)F (Sopt)  the greedy

solution [10]. This constant factor guarantee has practical

importance. First, a constant factor guarantee stays the

same as N grows, so the relative worst-case quality of the

solution is the same for small and for big problem instances.

Second, the worst-case result is achieved only by very con-

trived and unrealistic function instances - the typical case is

almost always much better. The greedy solution works by

starting with an empty set and repeatedly augmenting the

set as

S  S  argmax F (q|S)

(11)

qQ\S

550

nQueries 30 50 100 150 250 350 400 500

SF 0.496& 0.502&
0.509 0.518& 0.528&
0.527 0.531& 0.535&

MQ2007 Dataset

LDA

PL ELO

0.495

0.493 0.493

0.501 0.504

0.496 0.494 0.510& 0.506

0.517

0.510 0.511

0.527 0.528 0.531&

0.519 0.523 0.526

0.517 0.520 0.523

0.531

0.530 0.528

QBC 0.493 0.490 0.500 0.506 0.513 0.525 0.526 0.527

RDM 0.482 0.485 0.501 0.507 0.516 0.523 0.524 0.526

nQueries 30 50 100 150 250 350 400

SF 0.730 0.735 0.741& 0.743
0.745 0.751& 0.753&

MQ2008 Dataset
LDA PL ELO 0.728 0.722 0.716 0.731 0.731 0.720 0.740 0.739 0.724 0.742 0.740 0.729 0.745 0.748& 0.735 0.749 0.745 0.749 0.750 0.748 0.746

QBC 0.728 0.734 0.735 0.742 0.746 0.747 0.747

RDM 0.714 0.721 0.733 0.734 0.740 0.744 0.745

nQueries 25 30 35 40 45 50

SF 0.466
0.464
0.463 0.478& 0.481& 0.484&

OHSUMED Dataset

LDA

PL ELO QBC

0.459
0.466 0.478&

0.463 0.473&
0.476

0.463 0.454 0.467

0.465 0.455 0.458

0.460

0.468 0.455 0.469

0.473

0.456 0.455 0.464

0.466

0.467 0.467 0.472

RDM 0.432 0.462 0.463 0.460 0.473 0.464

Figure 1: Performance evaluation based on NDCG@10 scores for
the different algorithms; SF: Submodular function based query selection, LDA: LDA Topic Model based query selection, PL: Permutation Probability Based Query Selection, ELO: expected loss minimization baseline, QBC: Query-By-Committee baseline, RDM: Random query selection baseline. nQueries is the number of queries in the labelled set = base set + actively labelled queries. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.
until we select the N number of queries in the subset we intended.
Overall, we select query subsets based on the aforementioned formulations; we next describe in detail the experimental evaluation performed to compare the performances of the three proposed approaches against state-of-the-art baselines.

7. EXPERIMENTAL EVALUATION
We evaluate the proposed query selection strategies on web search ranking and show that the proposed techniques can result in good performance with much fewer labelled queries. We next describe our experimental settings along with the baselines, dataset and evaluation metrics used.
7.1 Compared Approaches
We compare the performance of the proposed query selection strategies against existing state-of-the-art approaches. The compared approaches include:
· Query-By-Committee (QBC): The Query-ByCommittee (QBC) approach involves maintaining a committee of models wherein each member is then allowed to vote on the labellings of query candidates. The most informative query is considered to be the instance about which the committee members most disagree. QBC based query selection strategy was used in [5] for ranking adaptation.
· Expected Loss Optimization (ELO): Based on the ELO framework described by Long et al [17], we im-

plemented the query-selection phase of the originally proposed 2-phase active learning framework to select queries wherein the most informative queries are selected by optimizing the expected DCG loss. As is mentioned in the original paper, we use score-range normalization to calculate the gain function. For details, please refer to [17].
· Random Query Selection (RDM): Queries are selected randomly for labelling from among the set of unlabelled queries. It is to be noted that random query selection is the primary method used in most settings [6].
· Permutation Probability Model (PL): Our first proposed approach (§ 4) based on capturing informativeness of queries via the uncertainty reduction principle.
· Topic Model (LDA): Our second proposed approach (§ 5) based on selecting representative queries which are most topically similar to the set of unlabelled queries.
· Submodular Model (SF): Our final proposed approach (§ 6) based on the coupled submodular objective which incorporates both the aspects of query informativeness & representativeness.
7.2 Dataset
We use three commonlyused real-world learning to rank datasets: (i) MQ2007; (ii) MQ2008 from LETOR 4.0 which uses query sets from Million Query track of TREC 2007, TREC 2008 and (iii) the OHSUMED test collection, a subset of the MEDLINE database, which is a bibliographic database of important, peer-reviewed medical literature maintained by the National Library of Medicine. It is worth mentioning that the proposed approaches make use of query term information which is not available in many other ranking datasets, hence we restrict our evaluation to these three datasets having query term information. There are 1700 queries in MQ2007, 800 queries in MQ2008 and 100 queries in the OHSUMED dataset. The MQ2007 & MQ2008 datasets are of notable size and query selection indeed makes sense in the such datasets; the OHSUMED dataset, on the other hand, has too few queries to select from which isn't ideal for a query selection scenario. Nevertheless, we compare performances across all datasets.
We adopt a 5-fold cross validation scheme with each fold divided into three parts, one each for training, validation and testing in the ratio 3:1:1. Each query-document pair is represented using 46 features [45 in case of the OHSUMED dataset) along with the relevance score from among {0,1,2}. The test set is used to evaluate the different query selection strategies while active learning is performed on queries from the training set.
7.3 Experimental Setting
We start with a base set of 40 labelled queries randomly sampled from the entire query set; the rest of the queries form the candidate set. We make use of C = 4 committee members (where applicable) each of which is constructed based on the procedure described earlier (subsection 4.1). To learn the initial ranking models for each of the committee members, we randomly select a sample of 20 queries from the base set of 40 queries and build a ranking model based

551

% Queries 5% 10% 25% 50%

SF
0.726 0.735 0.738& 0.745&

MQ2008 Dataset
LDA PL ELO 0.731 0.721 0.729 0.737& 0.733 0.734 0.732 0.733 0.731 0.731 0.734 0.735

QBC 0.730 0.734 0.730 0.734

RDM 0.728 0.726 0.727 0.728

SF
0.514& 0.508 0.513& 0.516&

MQ2007 Dataset LDA PL ELO QBC 0.505 0.486 0.501 0.498 0.498 0.501 0.503 0.507 0.509 0.511 0.507 0.511 0.510 0.505 0.509 0.514

RDM 0.498 0.496 0.504 0.505

Table 1: Generalizability across different Learning to Rank algorithm: NDCG performance based on ADARANK algorithm. Performance
evaluation based on NDCG@10 scores for the different algorithms; SF: Submodular function based query selection, LDA: LDA Topic Model based query selection, PL: min-max Plackett-Luce Based Query Selection, ELO: expected loss minimization baseline, QBC: Query-By-Committee baseline, RDM: Random query selection baseline. % Queries is the % of queries in the labelled set = base set + actively labelled queries. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.

on these queries as training data. We first focus on LambdaMART [11], (a state-of-the-art learning to rank algorithm that was the winner of the Yahoo! Learning to Rank challenge [4]) to build ranking models used in the initial part of our experiments. We later show (subsection 8.2) that the queries selected by this method could also be used by other Learning to Rank algorithms.
The entire experiment is repeated multiple times over the 5 folds on each dataset. We perform batch mode Active Learning for queries by selecting a batch of top 10 queries from the candidate set of queries based on the query selection criterion at each round and iteratively add them to our base set. Queries having no relevant documents were ignored while calculating the different metrics. Based on empirical estimation, the threshold parameter in equation 6 was initialized as  = 0.8. For our initial results, we evaluate the performance of the proposed query selection strategies based on their NDCG@10 values in the test set. We later analyse the generalizability of our approach on a different metric (MAP).
8. RESULTS
We compare the NDCG@10 performance of the test set against the number of queries in training set (base queries plus actively selected) in Figure 1 for the different datasets and compare the performance of the proposed query selection schemes against the QBC, ELO and Random baselines (statistically significant results are highlighted in the respective tables). For all the methods, the NDCG@10 values tends to increase with the number of iterations which is in line with the intuition that the quality of the ranking model is positively correlated with the number of examples in the training set.
While min-max PL based query selection stems from the same class of approaches (informativeness based) like the two baselines ELO & QBC, it performs better than both these baselines in most cases; this is in line with our initial claim of capturing informative queries from an alternate view of informativeness based on uncertainty reduction. We observe that LDA Topic Model based query selection performs better than existing baselines as well as the PL model which suggests that the quality of the queries selected by this scheme is better than those selected by other strategies which are mostly based on the informativeness aspect. Perhaps selecting queries based on the informativeness results in some noisy outlier queries getting selected, a case which LDA topic model based query selection avoids by selecting representative queries. The minor fluctuations and occasional dip in the NDCG values on adding more queries

0.54

0.535

0.53

0.525

NDCG 10

0.52

0.515

0.51

0.505

0.5
0.495
0.49 0

=0.1 =0.2 =0.3 =0.4 =0.5

50

100

150

200

250

300

350

400

450

500

no of queries

Figure 2: Tradeoff analysis between Informativeness & Representativeness for the MQ2007 datasets. The  coefficient in equation 5 controls the relative importance of the two aspects.

to the labelled set could be explained by the fact that some queries are indeed noisy and selecting such queries induces noise in the ranking models, which results in a slightly worse ranker performance.
Finally, we observe from the results that the submodular objective (SF) outperforms the baselines as well as (in most cases) our own proposed purely informativeness & purely representativeness based query selection schemes across the different datasets. While purely informativeness based methods tend to select noisy queries, purely representativeness based methods might possibly select queries which are representative but add redundant information. Hence, selecting queries based on the coupled aspects selects queries which are not only representative of other unselected queries, but are also informative to the ranking model.
8.1 Trade-Off between Informativeness & Representativeness
Our main motivation behind introducing the submodular objective was to couple the notions of informativeness and representativeness in a joint coherent manner. Indeed, an ideal subset of queries would be a fine blend of queries which convey the maximal amount of information to the ranking model while at the same time, be characteristic of the unselected set of queries. In Figure 2, we present a example analysis on one of the datasets of the relative importance of the two aspects and how they contribute to the overall ranking performance. As can be seen in the figure, a relative weight-

552

% Queries 5% 10% 25% 50%

SF 0.354& 0.371&
0.362 0.360

MQ2008 Dataset

LDA PL ELO

0.354& 0.344 0.341

0.328 0.369& 0.371&

0.354 0.357 0.367

0.362 0.357 0.369

QBC 0.340 0.361 0.362 0.346

RDM 0.342 0.352 0.361 0.365

SF
0.164 0.164& 0.165& 0.166&

MQ2007 Dataset LDA PL ELO
0.147 0.147 0.154 0.146 0.155 0.148 0.158 0.161 0.157 0.166& 0.160 0.153

QBC 0.163 0.160 0.161 0.135

RDM 0.149 0.159 0.160 0.159

Table 2: Generalizability across different Learning to Rank algorithm: AP performance based on Adarank algorithm. Performance evaluation
based on NDCG@10 scores for the different algorithms; SF: Submodular function based query selection, LDA: LDA Topic Model based query selection, PL: min-max Plackett-Luce Based Query Selection, ELO: expected loss minimization baseline, QBC: Query-By-Committee baseline, RDM: Random query selection baseline. % Queries is the % of queries in the labelled set = base set + actively labelled queries. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.

ing scheme of  = 0.3 (which weighs representativenessvs-informativeness in 3:7 proportions) works best for query selection. This highlights that while representativeness is important, selecting informative queries from the different topics indeed helps. Also, it must be noted that the informativeness term in Equation 9 not only contains contributions from query's singleton informativeness reward, but also has contributions from the topical segregation of queries into partitions. Overall, we chose the coefficient  = 0.3 to weigh the contributions from the two aspects while reporting results. It is to be noted that domain knowledge about the dataset in consideration can be used to vary  accordingly, depending on the desired proportion of representativeness & informativeness.
For a milder sized dataset (MQ2008), putting more weight on informativeness helps initially while the relative contributions tend toequal out once a certain threshold of queries have been selected. Overall, the general weighting factor or  = 0.3 works well consistently across different datasets.
8.2 Generalizability Across Learning Algorithms & Metrics
For initial results shown before, the query selection method uses LambdaMART as the learning to rank algorithms optimized for the NDCG metric. Since the labelled learning to rank dataset generated as a result of the query selection process could potentially be used in any future ranking systems, the selected queries should ideally be usable by any learning to rank algorithm, optimized for any metric. We analyze such generalization performance in these sets of experiments. While the initial set of results presented above were NDCG values based on LambdaMART ranking algorithm optimizing for NDCG metric, we divert from our original setting and present results on a different ranker: AdaRank [26] in table 1. Similar results for the OHSUMED dataset can be seen in Fig 3. Additionally, we demonstrate the performance of the proposed query selection strategies on a different metric (MAP) and report results in Table 2. Overall, we can see that the proposed query selection methodologies consistently perform better than the baselines across different ranking algorithms and metrics.
8.3 Labelling Cost Reduction
We next analyse the reduction in labelling cost achieved as compared to the case where the entire set of unlabelled queries were labelled. The performance of the ranking function trained with the whole labelled data set is referred to as the optimal performance. When the performance of the active learning model obtained with the proposed algorithms

Algorithm SF LDA PL ELO
QBC RDM

MQ2007
SS LCR  370 63%  390 61%  490 51%  560 44%  620 39%  720 29%

MQ2008
SS LCR  330 57%  400 48%  510 35%  520 34%  540 31%  570 27%

OHSUMED
SS LCR  45 29%  55 14%  55 14%  60 6%  60 6%  60 6%

Table 3: The performance in terms of the Labelling Cost Reduction (LCR) and the Saturated Size (SS) for the various compared approaches.

nQueries 30 40 50

SF 0.473 0.478 0.478

OHSUMED Dataset
LDA PL ELO QBC 0.469 0.478 0.477 0.478 0.478 0.475 0.472 0.477 0.466 0.469 0.477 0.477

RDM 0.466 0.467 0.473

Figure 3: Results on the OHSUMED dataset with LamdaMART
Learning to Rank algorithm. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.

is comparable to the optimal performance, we call the size of training data as the saturated size (SS). Table 3 highlights the approximate labelling cost reduction (LCR) results obtained via the proposed query selection techniques. The %ages were calculated based on the average number of queries in the training set. The corresponding values were calculated using the LambdaMART implementation with NDCG metric. Experimental evaluation shows the proposed query selection algorithms indeed require less number of queries to be labelled than baseline methods to achieve comparable ranking performance. It is worth mentioning that at some point, adding more queries to the labelled training set doesn't help improve ranking performance, as can be seen by the results of the RDM algorithm in the table: with about 720 labelled queries out of 1015 queries, the algorithm is able to demonstrate comparable ranking performance.

9. CONCLUSION & FUTURE WORK
We formulated approaches to the query selection problem into two classes: informativeness based and representativeness based strategies and proposed two novel query selection strategies, one from each class respectively: permutation probability based and LDA Topic Model based query selection. Additionally, we argued that an ideal query selection scheme should incorporate insights from both the aspects and presented a principled way of coupling information from the two aspects. Based on rigorous experiments

553

we demonstrated the efficacy of the proposed query selection schemes. A possible line of future work could look at enriching the representativeness aspect by adding document level information to the topic model.
10. REFERENCES
[1] J. A. Aslam, E. Kanoulas, V. Pavlu, S. Savev, and E. Yilmaz. Document selection methodologies for efficient and effective learning-to-rank. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 468­475. ACM, 2009.
[2] M. Bilgic and P. N. Bennett. Active query selection for learning rankers. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 1033­1034. ACM, 2012.
[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993­1022, 2003.
[4] C. J. Burges, K. M. Svore, P. N. Bennett, A. Pastusiak, and Q. Wu. Learning to rank using an ensemble of lambda-gradient models. In Yahoo! Learning to Rank Challenge, 2011.
[5] P. Cai, W. Gao, A. Zhou, and K.-F. Wong. Relevant knowledge helps in choosing right teacher: active query selection for ranking adaptation. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 115­124. ACM, 2011.
[6] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge overview. In Yahoo! Learning to Rank Challenge, 2011.
[7] O. Chapelle, Y. Chang, and T.-Y. Liu. Future directions in learning to rank. In Yahoo! Learning to Rank Challenge, 2011.
[8] P. Donmez and J. G. Carbonell. Optimizing estimated loss reduction for active sampling in rank learning. In Proceedings of the 25th international conference on Machine learning, pages 248­255. ACM, 2008.
[9] P. Donmez, J. G. Carbonell, and P. N. Bennett. Dual strategy active learning. In Machine Learning: ECML 2007, pages 116­127. Springer, 2007.
[10] U. Feige. A threshold of ln n for approximating set cover. Journal of the ACM, 1998.
[11] Y. Ganjisaffar, R. Caruana, and C. V. Lopes. Bagging gradient-boosted trees for high precision, low variance ranking models. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 85­94. ACM, 2011.
[12] Y. Hamo and S. Markovitch. The compset algorithm for subset selection. In IJCAI, pages 728­733, 2005.
[13] M. Hosseini, I. J. Cox, N. Milic-Frayling, M. Shokouhi, and E. Yilmaz. An uncertainty-aware query selection model for evaluation of ir systems. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 901­910. ACM, 2012.
[14] J. H. Lee. Combining multiple evidence from different properties of weighting schemes. In Proceedings of the 18th annual international ACM SIGIR conference on

Research and development in information retrieval, pages 180­188. ACM, 1995.
[15] J. H. Lee. Analyses of multiple evidence combination. In ACM SIGIR Forum. ACM, 1997.
[16] H. Lin and J. Bilmes. Multi-document summarization via budgeted maximization of submodular functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010.
[17] B. Long, O. Chapelle, Y. Zhang, Y. Chang, Z. Zheng, and B. Tseng. Active learning for ranking through expected loss optimization. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 267­274. ACM, 2010.
[18] G. L. Nemhauser and L. A. Wolsey. Integer and combinatorial optimization, volume 18. Wiley New York, 1988.
[19] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functions-i. Mathematical Programming, 1978.
[20] R. L. Plackett. The analysis of permutations. Applied Statistics, pages 193­202, 1975.
[21] K. Raman, P. Shivaswamy, and T. Joachims. Online learning to diversify from implicit feedback. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2012.
[22] B. Settles. Active learning literature survey. University of Wisconsin, Madison, 52:55­66, 2010.
[23] R. Silva, M. A. Gonc¸alves, and A. Veloso. Rule-based active sampling for learning to rank. In Machine Learning and Knowledge Discovery in Databases, pages 240­255. Springer, 2011.
[24] M. Taylor, J. Guiver, S. Robertson, and T. Minka. Softrank: optimizing non-smooth rank metrics. In Proceedings of the 2008 International Conference on Web Search and Data Mining. ACM, 2008.
[25] R. W. White and D. Kelly. A study on the effects of personalization and task information on implicit feedback performance. In Proceedings of the 15th ACM international conference on Information and knowledge management, pages 297­306. ACM, 2006.
[26] J. Xu and H. Li. Adarank: a boosting algorithm for information retrieval. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007.
[27] E. Yilmaz and S. Robertson. Deep versus shallow judgments in learning to rank. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 662­663. ACM, 2009.
[28] H. Yu. Svm selective sampling for ranking with application to data retrieval. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 354­363. ACM, 2005.
[29] Y. Yue and C. Guestrin. Linear submodular bandits and their application to diversified retrieval. In Advances in Neural Information Processing Systems, 2011.

554

Impact of Surrogate Assessments on High-Recall Retrieval

Adam Roegiest Gordon V. Cormack Charles L.A. Clarke Maura R. Grossman
University of Waterloo University of Waterloo University of Waterloo Wachtell, Lipton, Rosen & Katz

ABSTRACT
We are concerned with the effect of using a surrogate assessor to train a passive (i.e., batch) supervised-learning method to rank documents for subsequent review, where the effectiveness of the ranking will be evaluated using a different assessor deemed to be authoritative. Previous studies suggest that surrogate assessments may be a reasonable proxy for authoritative assessments for this task. Nonetheless, concern persists in some application domains--such as electronic discovery--that errors in surrogate training assessments will be amplified by the learning method, materially degrading performance. We demonstrate, through a re-analysis of data used in previous studies, that, with passive supervised-learning methods, using surrogate assessments for training can substantially impair classifier performance, relative to using the same deemed-authoritative assessor for both training and assessment. In particular, using a single surrogate to replace the authoritative assessor for training often yields a ranking that must be traversed much lower to achieve the same level of recall as the ranking that would have resulted had the authoritative assessor been used for training. We also show that steps can be taken to mitigate, and sometimes overcome, the impact of surrogate assessments for training: relevance assessments may be diversified through the use of multiple surrogates; and, a more liberal view of relevance can be adopted by having the surrogate label borderline documents as relevant. By taking these steps, rankings derived from surrogate assessments can match, and sometimes exceed, the performance of the ranking that would have been achieved, had the authority been used for training. Finally, we show that our results still hold when the role of surrogate and authority are interchanged, indicating that the results may simply reflect differing conceptions of relevance between surrogate and authority, as opposed to the authority having special skill or knowledge lacked by the surrogate.
The views expressed herein are solely those of the author and should not be attributed to her firm or its clients.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author(s). Copyright is held by the owner/author(s). SIGIR'15, August 09-13, 2015, Santiago, Chile. ACM 978-1-4503-3621-5/15/08. http://dx.doi.org/10.1145/2766462.2767754.

Categories and Subject Descriptors: H.3.4 Systems and Software Performance evaluation (efficiency and effectiveness).
Keywords: recall; assessor error; evaluation; eDiscovery; electronic discovery; relevance ranking; supervised learning.

1. INTRODUCTION

In high-recall information retrieval tasks--such as elec-

tronic discovery ("eDiscovery") in civil litigation [11], sys-

tematic review in evidence-based medicine [24], and prepa-

ration of test collections for information retrieval research

[14]--supervised learning is often used to separate relevant

from non-relevant documents [37]. In supervised learning,

each of a pre-selected set of documents (the "training set")

is labeled as relevant or not by a human assessor, and used

to train a machine-learning algorithm, which then classifies

or ranks the documents in a corpus (the "evaluation set")

according to their likelihood of relevance. We focus here on

a type of supervised learning, which, in eDiscovery is re-

ferred to as "simple passive learning" [12], to distinguish it

from active learning, where the training set is selected in-

crementally, using feedback from the learning algorithm [38,

11].

To measure the effectiveness of a high-recall retrieval ef-

fort, it is necessary to estimate the number r of relevant

documents from the evaluation set that are retrieved, as

well as the number m that are missed. If r and m were

known with certainty, it would be a simple matter to com-

pute

an

effectiveness

measure:

For

example,

recall

=

r r+m

.

However, the very notion of relevance is subjective [31, 32,

33], and necessarily relies on imperfect human judgement to

determine m and r, and hence recall.

For many high-recall tasks, the opinion of a single subject-

matter expert ("the authority") provides the ultimate deter-

mination of relevance. In the eDiscovery domain, the au-

thority might be a senior lawyer representing the respond-

ing party; in the intellectual property domain, the authority

might be a patent examiner; in the medical domain, the au-

thority might be a senior researcher. In all cases, obtaining

authoritative opinions for even a small set of documents may

be impossible, or may incur unacceptable costs and delays.

Previous studies have investigated the impact of replacing

the opinion of the authority with that of a surrogate asses-

sor [41, 46, 28]. The results of those studies suggest that,

at least under certain experimental conditions, a surrogate

can reasonably replace the authority, thereby increasing the

allure of using cheaper, more readily available surrogates as

proxies for authorities. On the other hand, some practition-

555

ers claim that the use of authoritative assessors is of critical importance in training the machine-learning algorithms used for eDiscovery tasks. One commentator [19], writing in a trade magazine, expressed the concern that errors in relevance assessments will be amplified by a chosen learning method to an irrecoverable extent. Others, including one court [2], have referred to this training issue more generally as "garbage in, garbage out."
1.1 Overview of experiments
After a review of related work in Section 2, and a discussion of our general experimental methodology in Section 3, in Section 4, we describe our studies using the TREC-4 test collection and supplementary assessments described by Voorhees [41]. Using the official and supplementary sets of assessments, we trained a support vector machine ("SVM") on each set and ranked the entire corpus through the use of ten-fold cross validation. From these rankings, we determined the depth in the ranking required (i.e., the number of documents reviewed) to achieve a particular level of recall, when each of the three assessors was deemed to be the authority, while the others were treated as surrogates.
Extending these experiments, we explored whether the inclusion of training assessments that reflected greater diversity in the interpretation of relevance would improve ranking performance. We implemented this diversification strategy in two different ways. First, we created three new surrogateassessment sets, each corresponding to a pair of the three original assessors, which we imagined as working together to judge the training set. We generated the merged set for a surrogate pair by randomly dividing the documents 50/50 between the two surrogates, with each determining relevance for their half. An SVM was then trained on each of these merged surrogates, and the results evaluated using cross-validation, treating the third assessor's opinion as authoritative.
As an alternate means of diversification, we took the union of each surrogate pair, such that a document was deemed relevant if either constituent had designated it as relevant. Again, an SVM was trained on each of these union surrogates and evaluated using cross-validation, treating the third assessor's opinion as authoritative.
In Section 5, we directly explore the impact of a more liberal interpretation of relevance on passive supervised learning. The University of Waterloo, in the course of participating in the TREC-6 adhoc task, created an independent set of relevance assessments that included a third relevance category --"iffy"--denoting documents which they believed to be of borderline relevance. The availability of three relevance categories (i.e., relevant, non-relevant, and iffy) allowed us to take two views of relevance: a "conservative" view, which considered only those documents actually labeled as "relevant" to be relevant; and a "liberal" view, which considered documents labeled as "iffy" to be relevant, in addition to documents actually labeled as "relevant."
We continue in Section 6 with an experiment investigating the applicability of liberal and diverse interpretations of relevance in the legal domain. The TREC 2009 Legal interactive task used initial assessments generated by volunteer law students and contract attorneys, that were subsequently adjudicated by senior lawyers ("topic authorities"). In addition, the University of Waterloo generated sets of assessments using a high-recall retrieval system. An SVM

was trained using each of: the assessments generated by Waterloo; the initial TREC assessments; a combination of Waterloo and the initial TREC assessments; and the final assessments. All classifiers were evaluated with respect to the final assessments. After this, we conducted a brief followup experiment to test the hypothesis that adding judgments from a third assessor for documents not originally included in the training set could reduce recall depth.
In Section 7, we discuss our findings and the limitations of our results, and in Section 8, we offer our conclusions.
2. RELATED WORK
Voorhees observed that the retired professional analysts who assessed the TREC-4 adhoc task agreed on relevance, as measured by the Jaccard index, less than 50% of the time [41]. As a consequence, when one assessor's judgements were assumed to be correct and used to evaluate the other's, the other's judgements were found to have both recall and precision on the order of 65%, leading Voorhees to opine, "a practical upper bound on retrieval system performance is 65% precision at 65% recall since that is the level at which humans agree with one another." While Voorhees' primary measure, mean average precision ("MAP"), is a reasonable choice for ad hoc retrieval, it provides little insight into the performance of high-recall retrieval due to the focus on early precision present in the measure. We know of no study that has investigated the applicability of Voorhees' results to a measure suitable for evaluating high-recall retrieval.
Voorhees is not alone in noting that relevance judgements differ for different assessors, and for the same assessor at different times [34, 30, 16, 45, 20, 21], or that differences in assessors, while resulting in different estimates of effectiveness measures, have little impact on determining the relative effectiveness of retrieval methods [3, 26, 7, 9, 40].
Webber and Pickens [46], using the same relevance assessments as Voorhees, deemed Voorhees' "primary" assessor for each topic to be "authoritative." Webber and Pickens reported that, on average, using a non-authoritative assessor for training resulted in a 14% decrease in F1 and a 24% increase in the number of top-ranked documents that must be retrieved to achieve a recall of 75%.
Cheng et al. [8] and Scholtes et al. [35] have similarly observed that non-authoritative training assessments have a significant but moderate negative impact on high-recall effectiveness. In contrast, Pickens [28] has suggested that nonauthoritative training assessments may improve high-recall effectiveness when active (instead of passive) supervisedlearning methods are used.
The issue of mitigating training and evaluation error is one of general interest in machine learning [47]. None of these studies specifically considered the interaction between assessor judgements for training and evaluation. A different source of interaction--inclusion bias introduced by the pooling method--is also a subject of current interest [6].
The TREC routing task [29, 44] bears substantial resemblance to high-recall retrieval, but differs by focusing on disjoint training and test collections, and evaluation using precision, rather than recall. Voorhees has suggested that examining the effect of differing relevance assessments on routing-like tasks is an important area of study [41]. To the best of our knowledge, this has yet to be investigated.
The effect of label noise (i.e., incorrect relevance assessments) is an ongoing topic of investigation within the IR

556

Corpus TREC 4 TREC 6 Legal 2009

Documents 713,049 556,077 723,386

Min. Prevalence 0.002% 0.0002% 0.58%

Avg. Prevalence 0.017% 0.0097% 0.72%

Max. Prevalence 0.028% 0.057% 0.94%

Table 1: Summary statistics of all three corpora. The official NIST assessor was deemed as the gold standard for these statistics.

community [25, 13, 36], as well as in other communities [5, 17]. A comprehensive study by Fr´enay and Verleysen [17] outlined research on various facets of label noise (e.g., sources of label noise, its effects on classification, etc). Brodley and Friedl found that removing labels identified as incorrect, primarily by majority or consensus voting filters, improved overall classifier performance [5]. The e-mail spam filtering community has also noticed that label noise can drastically affect spam-filter performance [25, 36, 13].
3. EXPERIMENTAL METHODOLOGY
Our experiments follow the Cranfield paradigm [42], using test collections consisting of documents, topics, and relevance assessments from several TREC tracks. Table 1 offers summary statistics, including the number of documents and the average prevalence of topics, for each of the test collections used in this work; further details on each collection are provided in subsequent sections. In this section, we provide an overview of the general experimental methodology used in all of our experiments.
In these experiments, we used assessments generated by several independent assessors to train our classifiers and evaluate their performance. For the experiments described in Sections 4 and 5, we used assessments only for documents that were assessed by all of the assessors for the particular collection; any document for which there was not a complete set of assessments was treated as non-relevant. This choice was made to control for the fact that some assessors rendered (many) more assessments than others, and using the extra assessments would confound comparison. In addition, using the additional assessments would change the number of training examples and would result in testing a different hypothesis than the one in which we were interested (i.e., the effect of quantity versus quality of assessments).
In contrast, for the experiments reported in Section 6, we used assessments for all documents in the TREC judging pool, while maintaining the roles (initial assessor, topic authority, and independent assessor) established in the original experiment. In cases where there was no independent assessment for a document in the pool, we evaluated two different methods: (i) deeming the assessment to be "not relevant," and (ii) deeming the assessment to be the same as the initial assessment.
The TREC judging pools used for training form convenience samples of the full collections since they are composed of the top-ranked documents from participant submissions. To mitigate any effects from training on a narrowly selected set of documents (i.e., those that appeared relevant to some participant system), we augmented each training set with 1,000 randomly selected documents that were treated as non-relevant. This step broadened the representativeness of documents in each training set, to ensure that the resultant classifier was not focused exclusively on fine-grained distinctions between relevant and non-relevant documents in

the judging pool, to the exclusion of non-relevant documents outside the pool.
To rank the documents, we used SVMlight[18], with default parameters. The features supplied were tf-idf term scores for all alphabetic words. Scores were generated after the Porter stemmer and case folding were applied.
Because our training and evaluation sets were not disjoint, we used ten-fold cross validation to approximate the effect of an independent evaluation set. Documents appearing in both sets were evenly distributed among 10 splits, as were documents appearing only in the evaluation set. The documents in each split were scored by a classifier whose training set was the union of the other nine splits, and a ranking was formed by sorting documents in the evaluation set according to score. We first tested our experimental methodology by replicating the Webber and Pickens study [46], successfully reproducing their results.
Our primary evaluation measure was Recall Depth, which is the size of the shortest prefix of the ranking that achieved a particular level of recall, expressed as percentage of the size of the corpus.
Our graphical results show, for each method, the average over all topics of (log transformed) recall depth, as a function of recall. In addition, for direct comparison, we show the average of (log transformed) relative recall depth--the ratio of recall depths between pairs of interest. Our tabular results show the same recall depth for 75% recall--a previously reported recall target [12]. We computed the significance of the surrogate-trained classifiers relative to the authoritytrained classifier, applying a t-test to the log-transformed difference. In our tables,  denotes p < 0.05;  denotes p < 0.0001.
4. INDEPENDENT JUDGMENTS
In this section, we describe our experiments using documents, topics, and relevance assessments from the TREC-4 adhoc task [22]. For this test collection, the official relevance assessments were augmented by two independent sets of relevance assessments rendered by different assessors within the course of Voorhees' experiments [41]. We labeled these assessment sets as J1, J2, and J3. While the assessments in J1 were (a subset of) those used for the official TREC evaluation, our experiments treated J1, J2, and J3 equally, treating each in turn as the "authority," and the others as surrogates. We restricted our experiments to topics where all three assessors found at least eight relevant documents, with the intent of reducing variance created by very low prevalence topics, consistent with previous work [46].
J1, J2, and J3 each reflect a single interpretation of relevance. To explore our hypothesis that a more diverse interpretation of relevance derived from several assessors would result in better training, we took each pair of surrogates and merged their assessments by randomly splitting the training set in half and assigning each half to one of the surrogates. The resulting merged-surrogate sets, which we denote J1|J2, J1|J3, and J2|J3, might then be viewed as the result of the two surrogate assessors working together to assess a single set of documents. Classifiers trained using each of the merged-surrogate sets were evaluated using J3, J2, and J1, respectively, as the authoritative assessor. Each merged-surrogate set contains the same documents as the single-surrogate set, reflecting the same amount of training effort.

557

To explore our hypothesis that a more liberal interpretation of relevance would result in better training, we evaluated training using the union of each pair of surrogates, denoted J1+J2, J1+J3, and J2+J3, in which a document was considered relevant if either of two surrogates considered it relevant. The classifiers constructed using these union surrogates were evaluated using J3, J2, and J1, respectively, as the authoritative assessor. Each of the union-surrogate sets contained the same documents as the single-surrogate and merged-surrogate sets, but reflected twice as much assessment effort. We do not believe that such a practice would necessarily be cost prohibitive, given the assumption that surrogate assessments are substantially less expensive than authoritative assessments.
4.1 Results
Figure 1 shows that the single-surrogate-trained classifiers are generally inferior to the corresponding authority-trained classifiers, requiring greater recall depth to achieve any particular level of recall. This result is reiterated in the relative recall depth plots in Figure 2, and the 75% recall depth values presented in Table 2. The differences among surrogates are most apparent at high levels of recall; as Figure 2 illustrates, some individual surrogates are substantially better than others.
Table 2 shows that with J1 and J3 as the authority, the authority-trained classifiers significantly outperform classifiers trained by individual surrogates. However, with J2 as the authority, the difference is not significant, particularly with respect to the case in which J1 is used as the surrogate assessor. While this reduced difference may be due to chance, it may also be an artifact of the assessment process. J1 corresponds to the official NIST assessments, for which the assessor reviewed the entire TREC-4 pool. This pool was much larger and had a lower prevalence of relevant documents than the pool reviewed by J2. An inverse relationship between prevalence and recall [39] might account for J1's assessments being more liberal than they would have been had J1 assessed only the documents that were assessed by J2.
Figures 1 and 2, as well as Table 2, show that the merged surrogates achieve effectiveness close to the better of the individual surrogates, occasionally exceeding both.
The union surrogates trained substantially and significantly (p<0.01) superior classifiers compared to the individual surrogates, as is evident in Figures 1 and 2, as well as Table 2.

Authority
J1 J2 J3
J1 J2 J3
J1 J2 J3

J1 0.082% (0.058 - 0.115) 0.103% (0.061 - 0.174) 0.146% (0.077 - 0.278)
J1|J2 -
0.182% (0.096 - 0.346) J1+J2 -
0.094% (0.054 - 0.164)

Training J2
0.542% (0.254 - 1.156) 0.087% (0.051 - 0.149) 0.359% (0.162 - 0.797)
J1|J3 -
0.092% (0.059 - 0.143) -
J1+J3 -
0.062% (0.043 - 0.091) -

J3 0.284% (0.139 - 0.584) 0.161% (0.083 - 0.312) 0.066% (0.044 - 0.101)
J2|J3 0.321% (0.162 - 0.636)
J2+J3 0.094% (0.054 - 0.164) -

Table 2: 75% recall depth values for the TREC-4 experiments, with 95% confidence intervals. Significance is determined by comparing surrogate-trained classifiers to the authority-trained classifier. ( denotes p < 0.05;  denotes p < 0.0001.)

5. LIBERAL ASSESSMENT
In this section, we describe our experiments using documents, topics, and relevance assessments from the TREC-6 adhoc task [43], augmented by assessments rendered independently by the University of Waterloo in the course of their participation in TREC-6, using a process of interactive search and judging [10]. While TREC-6 used binary assessments, Waterloo used three categories of relevance: relevant, not relevant, and "iffy." This "iffy" label was used to identify documents for which the Waterloo assessors were unsure of the true relevance (i.e., they were of borderline relevance).
In Voorhees' study, these "iffy" assessments were treated as non-relevant. One of our hypotheses was that a more liberal interpretation of relevance would result in better classifier performance with respect to an independent third party (i.e., in this case, NIST). To this end, we compared the two classifiers trained by treating these "iffy" documents, alternatively, as non-relevant and as relevant. These different sets of assessments are labeled WaterlooRel and WaterlooRel+Iffy, respectively. In this experiment, WaterlooRel+Iffy represents a "liberal" assessor, while WaterlooRel represents a "conservative" assessor.
We used the NIST assessments to evaluate classifiers trained using each of: the NIST assessments, the WaterlooRel assessments, and the WaterlooRel+Iffy assessments. While our primary interest was in the relative effectiveness of using the liberal versus conservative Waterloo assessments for training, we also reversed the roles of surrogate and authority, as for our previous experiment. We did not investigate the use of the conservative Waterloo assessments as the surrogate and the liberal Waterloo assessments as the authority, or vice versa, as these sets of assessments were not independent.
5.1 Results
Our hypothesis--that surrogate assessors taking a more liberal view of relevance would produce better classifiers-- is supported by the results presented in Figures 3a and 4a, where training using the liberal assessor is seen to achieve significantly better recall depth than both the conservative assessor and the NIST assessor. Table 3 shows the difference at 75% recall depth, with 95% confidence intervals. Across all recall levels, the liberally-trained classifier generally performs as well as, or better than, the authority, while the conservatively-trained classifier performs significantly worse.
Table 3, as well as panels (b) and (c) of Figures 3 and 4, show that, consistent with our previous results, classifiers trained using the NIST assessments fall short when evaluated using either the liberal or conservative Waterloo assessments as the authority. It is no surprise that the shortfall is greater with respect to the liberal assessments.

a a

Training

a a

NIST

Authoritay aa

NIST aa 0.110% (0.065 - 0.185)

WaterlooRel 0.244% (0.130 - 0.458)

WaterlooRel+Iffy 0.882% (0.515 - 1.511)

WaterlooRel
0.261% (0.142 - 0.481) 0.152% (0.094 - 0.246)
-

WaterlooRel+Iffy
0.072% (0.049 - 0.105) -
0.129% (0.094 - 0.177)

Table 3: 75% recall depth values for the TREC-6 experiments for Waterloo and NIST-trained classifiers, evaluated using NIST assessments, with 95% confidence intervals. Significance is shown relative to the NIST-trained classifier. ( denotes p < 0.05;  denotes p < 0.0001.)

558

(a) Authority: J1

(a) Authority: J1

Recall Depth

J1

10

J2

J3

1

J2|J3 J2+J3

0.1

0.01

0.001 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall

(b) Authority: J2

Relative Depth

10

1

J2/J1

J2|J3/J1

J3/J1

J2+J3/J1

J1/J1

0.1

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Recall

(b) Authority: J2

Recall Depth

J1

10

J2

J3

1

J1|J3 J1+J3

0.1

0.01

0.001 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall

(c) Authority: J3

Relative Depth

10

1

J1/J2

J1|J3/J2

J3/J2

J1+J3/J2

J2/J2

0.1

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Recall

(c) Authority: J3

Recall Depth

J1

10

J2

J3

1

J1|J2 J1+J2

0.1

0.01

0.001 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall

Relative Depth

10

1

J1/J3

J1|J2/J3

J2/J3

J1+J2/J3

J3/J3

0.1

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Recall

Figure 1: Recall depth plots for the TREC-4 experiments, using (a) JI, (b) J2, and (c) J3 as the authority.

Figure 2: Relative recall depth plots for the TREC-4 experiments, using (a) J1, (b) J2, and (c) J3, as the authority.

559

(a) Authority: NIST

(a) Authority: NIST

Recall Depth

10

1

0.1

0.01

NIST WaterlooRel

WaterlooRel+Iffy

0.001

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Recall

(b) Authority: WaterlooRel

Relative Depth

10
1
WaterlooRel/NIST NIST/NIST
WaterlooRel+Iffy/NIST
0.1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall
(b) Authority: WaterlooRel

Recall Depth

10

1

0.1

0.01

NIST

WaterlooRel

0.001

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Recall

(c) Authority: WaterlooRel+Iffy

Relative Depth

10
1
NIST/WaterlooRel WaterlooRel/WaterlooRel
0.1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall (c) Authority: WaterlooRel+Iffy

10

10

Relative Depth

Recall Depth

1

0.1

1

0.01

NIST

WaterlooRel+Iffy

0.001

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Recall

NIST/WaterlooRel+Iffy WaterlooRel+Iffy/WaterlooRel+Iffy
0.1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall

Figure 3: Recall depth plots for the TREC-6 experi- Figure 4: Relative recall depth plots for TREC-6 exper-

ments, using classifiers trained by each surrogate, and iments, using classifiers trained by each surrogate, and

evaluated by each authority.

evaluated by each authority.

560

Topic 201 202 203 207

Initial 1.056% 1.005% 6.542% 1.314%

Waterloo 0.214% 0.977% 0.955% 1.401%

Waterloo w/ Initial 0.254% 0.993% 0.816% 1.324%

Final 0.215% 0.936% 0.456% 1.236%

Table 4: 75% recall depth values for the TREC 2009 Legal experiments, using classifiers trained by Waterloo and initial assessments, and evaluated using final assessments.

Topic 201 202 203 207

Judging Pool

Precision Recall

0.40

0.70

0.93

0.93

0.47

0.25

0.98

0.88

Full Corpus

Precision Recall

0.05

0.76

0.27

0.80

0.13

0.25

0.89

0.79

Table 5: Recall and Precision of initial assessments in the TREC 2009 Legal Track judging pool versus the full corpus.

6. TREC 2009 LEGAL TRACK
The TREC 2009 Legal interactive task [23], simulated a high-recall eDiscovery task. For each topic in this task, the judging pool was a stratified sample of the document collection. An initial assessment of the judging pool was rendered using volunteer law students or contract attorneys. The initial assessments were provided to participating teams, who were invited to appeal assessments with which they disagreed. The appealed assessments were adjudicated by a TREC-designated Topic Authority, a senior lawyer who rendered the final, authoritative relevance assessments that were used to evaluate submissions. During the course of their participation in TREC 2009, the University of Waterloo developed an independent set of assessments using their own interactive, high-recall retrieval system for four of the task topics (Topics 201, 202, 203, and 207) [15].
Because the TREC judging pool included a large random sample of the document population, only a relatively small fraction (17.7%) of the documents in the pool were included in the Waterloo assessments; the rest were excluded by Waterloo's search method, as unlikely to be relevant. We investigated two approaches to determine the relevance of these documents for training purposes. The "Waterloo" surrogate assessments deemed the excluded documents to be "not relevant" for the purpose of training, and used the final TREC assessments as the sole authority for evaluation. The "Waterloo w/Initial" surrogate assessments used the initial NIST assessment for each excluded document. Thus, the "Waterloo" assessments were fully independent of the initial assessments, whereas the "Waterloo w/Initial" assessments, while not fully independent, might better model the situation in which the excluded documents had been manually assessed.
We evaluated the effect of training using the two sets of Waterloo surrogate assessments, as well as the initial and final assessments, using the final assessments as authoritative.
6.1 Results
Figure 5 shows relative recall depth plots, with respect to the final assessments, for each of the four topics. Table 4 shows 75% recall depth values for the same four topics. Overall, the surrogate-trained results appear inferior for

Topic 201

10

Topic 202

Topic 203 Topic 207

Relative Depth

1

0

5000 10000 15000 20000 25000

Number of Additional Assessments

Figure 6: Per-topic 75% relative recall depth plots for the retrospective TREC 2009 Legal experiment, using classifiers trained on initial assessments, progressively augmented by Waterloo assessments, and evaluated using final assessments.

high recall, but substantially so only when using the initial assessments as surrogate, and only for Topics 202 and 207. We note that Topics 202 and 207 have much higher prevalence than Topics 201 and 203, and due to the stratified sampling used to select the judging pool, the initial assessments achieved much higher precision and recall within the pool than in the collection at large, as illustrated in Table 5.
Figure 5 and Table 4 further indicate that the combination of assessments generally yields results as good as, and often superior to, the better of the individual surrogates.
6.2 Interactive Training
Retrospectively, we conducted one final supplemental experiment in an effort to shed some light on the applicability of our results to a more interactive, high-recall retrieval effort. Our supplemental experiment tracked improvement in relative recall depth as the initial assessments were supplemented incrementally with batches of 500 Waterloo assessments.
Figure 6 shows 75% relative recall depth, as a function of the number of Waterloo assessments. For Topics 201, 203, and 207, we see a dramatic gain from supplementing the assessments with a small fraction of the Waterloo assessments. For Topic 202, we see little improvement over the near-perfect initial assessments. The result suggests that having an independent assessor judge a fairly small fraction of the documents can result in a dramatic improvement in effectiveness, but further study is needed.
7. DISCUSSION
Our results show that it matters who assesses relevance; in particular, it matters whether the assessors whose judgements are used to train the system are the same as those whose judgements are used to evaluate the result. A statistic like "75% recall" conveys little meaning without considering, "according to whom?"
In one of the first cases where a court ruled in favor of the responding party's use of machine learning for eDiscovery, over the requesting party's objection, the responding party's brief [1] asserts:

561

(a) Topic 201 10

(b) Topic 202 10

Relative Depth

Relative Depth

1
Initial/Final Waterloo w/ Initial/Final
Waterloo/Final Final/Final
0.1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall
(c) Topic 203
10

1
Initial/Final Waterloo w/ Initial/Final
Waterloo/Final Final/Final
0.1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall
(d) Topic 207
10

Relative Depth

Relative Depth

1
Initial/Final Waterloo w/ Initial/Final
Waterloo/Final Final/Final
0.1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall

1
Initial/Final Waterloo w/ Initial/Final
Waterloo/Final Final/Final
0.1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall

Figure 5: Relative recall depth plots for the TREC 2009 Legal experiments, using classifiers trained by each surrogate, and evaluated by the final assessments.

Given that recall for linear review averages only 59.3%, [the responding party] proposes an acceptable recall criterion of 75%. In other words, predictive coding will conclude once the sampling program establishes that at least 75% of the relevant documents have been retrieved from the [responding party's electronically stored information] and are available to [them] for discovery purposes.
The 59.3% recall average was derived from Grossman and Cormack's analysis of the TREC 2009 Legal Track results [20], calculating the precision and recall of the initial assessments, evaluated with respect to the final (i.e., independent) authoritative assessments. The acceptance criterion of 75% recall, however, was established using the responding party's own reviewers, who were, we presume, also involved in training the system. Our results suggest that, had recall been evaluated using an independent assessor, the calculated recall value might have been considerably lower.
The designation of a particular assessor to be "authoritative" is, in many ways, an artificial construct designed to sidestep well-known uncertainties in the definition of relevance, and hence recall (see Webber et al. [45]). While some assessors may be more knowledgeable or skillfull than others, it is well known that even expert assessors will disagree on a substantial number of assessments [3]. The IR literature suggests than an exhaustive assessment effort by one such expert would be unlikely to achieve more than 65% recall and 65% precision in the eyes of another, equally skilled

and knowledgeable expert [41]. The surrogates used for our TREC-4 and TREC-6 experiments achieved comparable recall and precision levels: When assessed by J1, J2 achieved recall of 52.9% and precision of 80.8%, and J3 achieved recall of 63.1% and precision of 78.1%; when assessed by NIST, the conservative Waterloo surrogate achieved recall of 62.8% and precision of 65.2%, while the liberal Waterloo surrogate achieved recall of 86.6% and precision of 50.0%.
Our results do not support the proposition that the use of machine learning "amplifies" inconsistencies between the surrogate and authority. The classifier trained using a surrogate's assessments achieves higher recall--at a recall depth corresponding to a fraction of the corpus--than the surrogate would achieve by assessing the entire corpus. For example, Figure 1 shows that a classifier trained using J2's assessments achieves 55% recall at a recall depth corresponding to 0.1% of the corpus, while a classifier trained using J2's assessments achieves 75% recall at a recall depth corresponding to 0.542% of the corpus.
Notwithstanding the discussion above, it is a worthwhile objective to try to maximize recall with respect to an authoritative assessor, either because that assessor has been stipulated to be the purveyor of true relevance, or because that assessor acts as a proxy for an as-yet-unavailable authority, such as a judge or regulator. Presumably, if a system achieves high recall at low recall depth with respect to one reasonable independent assessor, it is likely to achieve similar results with respect to another.
The results from our three experiments indicate that using a surrogate assessor instead of the authoritative assessor for

562

training can dramatically increase recall depth. While the effect is not universally large, the instances in which it is, cannot be attributed to chance (p<0.05, corrected for multiple hypothesis testing). On the other hand, there are several instances where surrogate training appears to be as good as, or better than, authoritative training. The same general effect is observed, regardless of which assessor is deemed to be authoritative.
Our TREC-4 experiments suggest that randomly intermingling the assessments of two surrogates achieves recall depth similar to that of the better surrogate, while using the union of the two surrogates' assessments improves on both, approaching the effectiveness of using the authority's assessments.
Our TREC-6 experiments show that, when the surrogate assessor makes the deliberate choice to label marginally relevant documents as relevant, recall depth is substantially and significantly reduced, relative to the case in which such documents are labeled as non-relevant. Furthermore, training using this liberal assessment strategy yields a materially lower 75% recall depth than authoritative training. While caution must be exercised in extrapolating this result to other assessors' efforts, it strongly suggests that a surrogate can, at least in this instance, train a classifier as effectively as the authority, even when the authority's assessments are deemed to be the gold standard.
Our TREC 2009 Legal experiments show the same pattern as the others: For two topics, training using the initial assessor yields substantially inferior results to training using the authority; for the other two topics, the difference was small and not significant. This apparent dissonance might be explained by the fact that, for the latter two topics, the recall and precision of the surrogate--with respect to the judging pool--were exceptionally high. For the former two topics, they were substantially inferior.
7.1 Limitations
Our experiments study only the case of simple passive learning, where a fixed training set is to used to train a learning method to rank the entire corpus, and the topranked documents are reviewed until high recall is achieved. Although this practice appears to be widely employed in eDiscovery today, the state of the art is perhaps better represented by interactive, active-learning approaches [12, 27]. Accordingly, our results are applicable only to the former method; their utility in guiding individual stages of an interactive or active approach has not been established.
Our "convenience sample" of available assessments and collections may not be representative of a typical application of passive supervised machine learning. In our experiments, the judging pool was far from a random sample of the collection, as evidenced in Table 5. Nor was it the result of uncertainty sampling as commonly used in active learning. The judging pools for the TREC-4 and TREC-6 experiments might be construed to be representative of relevance feedback, because the documents were those ranked highly by TREC submissions. The judging pool for the TREC 2009 Legal experiments might be construed to represent queryby-committee, as it was constructed using strata to illuminate disagreements among the TREC submissions.
While our findings indicate quite strongly that some combinations of surrogates and authorities fare poorly, while others fare very well, more research is needed--with a larger

population of assessors--to gain a thorough understanding of the causal factors. That said, our results clearly suggest that, when other factors are held constant, increasing the diversity or liberality of training assessments increases the quality of ranking, relative to that produced by a single surrogate.
8. CONCLUSIONS
Situations where assessments from a single authoritative assessor are unavailable, expensive, or limited, may occasion the use of a surrogate assessor to train a passive supervisedlearning method to rank documents. In many situations, the resulting ranking is significantly and substantially inferior to that which would have occurred, had the authoritative assessor been used for training. Our experiments indicate that this effect can be mitigated, and sometimes overcome, by merging the assessments of multiple surrogates, or by instructing the surrogate to use a more liberal interpretation of relevance.
We question whether it is possible to sweep away uncertainties in relevance determination simply by arbitrarily deeming relevance to be the judgment of a single authoritative assessor. It is well known that informed, qualified assessors disagree, and even the same assessor will disagree with him or herself, at different times and in different circumstances. We wonder whether it is useful to expend heroic efforts to anticipate the judgments of one particular assessor, and posit, instead, that it might be better to target a hypothetical "reasonable authority," selected from a pool of equally competent choices. In any event, it is important when evaluating the recall of a retrieval effort, to ask, "according to whom?" 75% recall measured through independent assessment is a formidable achievement, but the same 75% recall measured through self-assessment is unremarkable.
9. REFERENCES
[1] Memorandum in Support of Motion for Protective Order Approving the Use of Predictive Coding, Global Aerospace v. Landow Aviation, No. CL 61040, 2012 WL 1419842 (Va. Cir. Ct., Loudoun Cnty., Apr. 9, 2012).
[2] Da Silva Moore v. Publicis Groupe, 287 F.R.D. 182 (S.D.N.Y., 2012).
[3] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P. de Vries, and E. Yilmaz. Relevance assessment: are judges exchangeable and does it matter? In Proc. SIGIR, 2008.
[4] T. Barnett, S. Godjevac, J.-M. Renders, C. Privault, J. Schneider, and R. Wickstrom. Machine learning classification for document review. In ICAIL DESI III Workshop, 2009.
[5] C. E. Brodley and M. A. Friedl. Identifying mislabeled training data. J. of A.I. Research, 11, 2011.
[6] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Information Retrieval, 10(6), 2007.
[7] R. Burgin. Variations in relevance judgments and the evaluation of retrieval performance. Information Processing & Management, 28(5), 1992.

563

[8] J. Cheng, A. Jones, C. Privault, and J.-M. Renders. Soft labeling for multi-pass document review. In ICAIL DESI V Workshop, 2013.
[9] C. W. Cleverdon. The Effect of Variations in Relevance Assessments in Comparative Experimental Tests of Index Languages. Cranfield Library, 3, 1970.
[10] G. V. Cormack, C. L. A. Clarke, C. R. Palmer, and S. S. L. To. passage-based refinement (MultiText experiments for TREC-6). In Proc. TREC-6, 1997.
[11] G. V. Cormack and M. R. Grossman. The Grossman-Cormack glossary of technology-assisted review with foreword by John M. Facciola, Magistrate Judge. Fed. Courts Law Rev., 7(1), 2013.
[12] G. V. Cormack and M. R. Grossman. Evaluation of machine-learning protocols for technology-assisted review in electronic discovery. In Proc. SIGIR, 2014.
[13] G. V. Cormack and A. Kolcz. Spam filter evaluation with imprecise ground truth. In Proc. SIGIR, 2009.
[14] G. V. Cormack and T. R. Lynam. Spam corpus creation for TREC. In Proc. 2nd CEAS, 2005.
[15] G. V. Cormack and M. Mojdeh. Machine learning for information retrieval: TREC 2009 Web, Relevance Feedback and Legal Tracks. In Proc. TREC-18, 2009.
[16] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 14(5), 2011.
[17] B. Fr´enay and M. Verleysen. Classification in the presence of label noise: a survey. IEEE Trans. Neural Networks and Learning Systems, 25(5), 2013.
[18] T. Joachims. Making large-scale SVM learning practical. In Advances in Kernel Methods - Support Vector Learning. MIT Press, 1999.
[19] D. Gonsowski. A look into the e-discovery crystal ball. Inside Counsel, Dec. 2, 2011.
[20] M. R. Grossman and G. V. Cormack. Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Rich. J. Law & Tech., 17, 2011.
[21] M. R. Grossman and G. V. Cormack. Inconsistent responsiveness determination in document review: Difference of opinion or human error? Pace Law Rev., 32, 2012.
[22] D. Harman. Overview of the Fourth Text REtrieval Conference (TREC-4). In Proc. TREC-4, 1995.
[23] B. Hedin, S. Tomlinson, J. R. Baron, and D. W. Oard. Overview of the TREC 2009 Legal Track. In Proc. TREC-18, 2009.
[24] J. P. Higgins, S. Green, eds. Cochrane Handbook for Systematic Reviews of Interventions, Wiley Online Library, 2008.
[25] A. Kolcz and G. V. Cormack. Genre-based decomposition of email class noise. In Proc. KDD, 2009.
[26] M. E. Lesk and G. Salton. Relevance assessments and retrieval system evaluation. Information Storage and Retrieval, 4(4), 1968.
[27] C. Li, Y. Wang, P. Resnick, and Q. Mei. ReQ-ReC: High Recall Retrieval with Query Pooling and Interactive Classification. In Proc. SIGIR, 2014.

[28] J. Pickens. In TAR, wrong decisions can lead to the right documents (a response to Ralph Losey). http://web.archive.org/save/http://www. catalystsecure.com/blog/2014/02/in-tar-wrongdecisions-can-lead-to-the-right-documents-a-responseto-ralph-losey/.
[29] S. Robertson and I. Soboroff. The TREC 2002 Filtering Track report. In Proc. TREC-11, 2002.
[30] H. L. Roitblat, A. Kershaw, and P. Oot. Document categorization in legal electronic discovery: Computer classification vs. manual review. J. Am. Soc. for Info. Sci. and Tech., 61(1), 2010.
[31] T. Saracevic. Relevance: A review of the literature and a framework for thinking on the notion in information science. J. Am. Soc. Info. Sci., 26(6), 1975.
[32] T. Saracevic. Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: Nature and manifestations of relevance. J. Am. Soc. for Info. Sci. and Tech., 58(13), 2007.
[33] T. Saracevic. Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. J. Am. Soc. for Info. Sci. and Tech., 58(13), 2007.
[34] L. Schamber. Relevance and information behavior. Annual Rev. Info. Sci. and Tech. (ARIST), 29, 1994.
[35] J. C. Scholtes, T. van Cann, and M. Mack. The impact of incorrect training sets and rolling collection on technology assisted review. In ICAIL DESI V Workshop, 2013.
[36] D. Sculley and G. V. Cormack. Filtering email spam in the presence of noisy user feedback. In Proc. CEAS, 2008.
[37] F. Sebastiani. Machine learning in automated text categorization. ACM Comput. Surv., 34(1), 2002.
[38] B. Settles. Active learning literature survey. TR 1648, University of Wisconsin, Madison, 2010.
[39] M. D. Smucker and C. P. Jethani. Human performance and retrieval precision revisited. In Proc. SIGIR, 2010.
[40] A. Trotman and D. Jenkinson. IR evaluation using multiple assessors per topic. In Proc. ADCS, 2007.
[41] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5), 2000.
[42] E. M. Voorhees. The philosophy of information retrieval evaluation. In Evaluation of Cross-Language Information Retrieval Systems. Springer, 2002.
[43] E. M. Voorhees and D. Harman. Overview of the Sixth Text REtrieval Conference (TREC-6). In Proc. TREC-6, 1997.
[44] E. M. Voorhees and D. K. Harman, eds. TREC: Experiment and Evaluation in Information Retrieval. MIT Press, 2005.
[45] W. Webber, D. W. Oard, F. Scholer, and B. Hedin. Assessor error in stratified evaluation. In Proc. CIKM, 2010.
[46] W. Webber and J. Pickens. Assessor disagreement and text classifier accuracy. In Proc. SIGIR, 2013.
[47] X. Zhu and X. Wu. Class noise vs. attribute noise: A quantitative study. A. I. Rev., 22(3), 2004.

564

The Benefits of Magnitude Estimation Relevance Assessments for Information Retrieval Evaluation

Andrew Turpin
University of Melbourne, Australia
aturpin@unimelb.edu.au

Falk Scholer
RMIT University, Australia
falk.scholer@rmit.edu.au

Stefano Mizzaro
University of Udine, Italy
mizzaro@uniud.it

Eddy Maddalena
University of Udine, Italy
eddy.maddalena@uniud.it

ABSTRACT
Magnitude estimation is a psychophysical scaling technique for the measurement of sensation, where observers assign numbers to stimuli in response to their perceived intensity. We investigate the use of magnitude estimation for judging the relevance of documents in the context of information retrieval evaluation, carrying out a large-scale user study across 18 TREC topics and collecting more than 50,000 magnitude estimation judgments. Our analysis shows that on average magnitude estimation judgments are rank-aligned with ordinal judgments made by expert relevance assessors. An advantage of magnitude estimation is that users can chose their own scale for judgments, allowing deeper investigations of user perceptions than when categorical scales are used.
We explore the application of magnitude estimation for IR evaluation, calibrating two gain-based effectiveness metrics, nDCG and ERR, directly from user-reported perceptions of relevance. A comparison of TREC system effectiveness rankings based on binary, ordinal, and magnitude estimation relevance shows substantial variation; in particular, the top systems ranked using magnitude estimation and ordinal judgments differ substantially. Analysis of the magnitude estimation scores shows that this effect is due in part to varying perceptions of relevance, in terms of how impactful relative differences in document relevance are perceived to be. We further use magnitude estimation to investigate gain profiles, comparing the currently assumed linear and exponential approaches with actual user-reported relevance perceptions. This indicates that the currently used exponential gain profiles in nDCG and ERR are mismatched with an average user, but perhaps more importantly that individual perceptions are highly variable. These results have direct implications for IR evaluation, suggesting that current assumptions about a single view of relevance being sufficient to represent a population of users are unlikely to hold. Finally, we demonstrate that magnitude estimation judgments can be reliably collected using crowdsourcing, and are competitive in terms of assessor cost.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '15, August 09­13, 2015, Santiago, Chile Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 . . . $15.00. DOI: http://dx.doi.org/10.1145/2766462.2767760.

Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and software--performance evaluation.
Keywords
Magnitude estimation; evaluation; relevance assessments; relevance
General Terms
Measurement, performance, experimentation
1. INTRODUCTION
Relevance is an important concept in information retrieval (IR), and relevance judgments form the backbone of test collections, the most widely-used approach for the evaluation of IR system effectiveness. Document relevance judgments are typically made using ordinal scales, historically at a binary level, and more recently with multiple levels [28]. However, despite its importance, operationalizing relevance for the evaluation of IR systems remains a complicated issue; for example, when using an ordinal scale it is unclear how many relevance categories should be chosen [26].
Magnitude estimation is psychophysical technique for measuring the sensation of a stimulus. Observers assign numbers to a series of stimuli, such that the numbers reflect the perceived difference in intensity of each item. The outcome is a ratio scale of the subjective perception of the stimulus; if a magnitude of 50 is assigned to one stimulus, and 10 to another, then it can be concluded that the two items are perceived in a 5:1 ratio of sensation [18]. While initially developed for the measurement of the sensation of physical stimuli such as the intensity of a light source, magnitude estimation has been successfully applied to the measurement of non-physical stimuli including the usability of interfaces [17].
Being able to derive ratio scales of subjective perceptions of the intensity of stimuli, magnitude estimation may be a useful tool for measuring and better understanding document relevance judgments. The application of magnitude estimation in the IR field has been limited to the consideration of the relevance of carefully curated abstracts returned from bibliographic databases [10], and our own small-scale pilot study [15, 21]. While suggestive, these studies have been limited in terms of demonstrating the broader utility of the approach, or its direct application to IR evaluation. In this work, we investigate the larger-scale application of magnitude estimation to document relevance scaling, reporting on a user study over 18 TREC topics and obtaining judgments for 4,269 documents. This is also the first work to consider the direct application

565

of magnitude estimation to the evaluation of IR systems. Specifically, we consider three research questions.
RQ1. Is the magnitude estimation technique suitable for gathering document-level relevance judgments, and are the resulting relevance scales reasonable with respect to our current knowledge of relevance judgments?
RQ2. How does IR system evaluation change when ratio scale magnitude estimation relevance judgments are used to calibrate the gain levels of the widely-used nDCG and ERR evaluation metrics, compared to using arbitrarily set gain values for a pre-chosen number of ordinal levels?
RQ3. Can magnitude estimation relevance scores provide additional insight into user perceptions of relevance, into actual gain values, and into individual gain profiles?
In the next section, background material and related work are presented. Details of our user study and experimental methodology are provided in Section 3, and the analysis and discussion of our results, corresponding to the three research questions, is presented in Sections 4 to 6. Our conclusions and directions for future work are included in the final section of the paper.
2. BACKGROUND
Relevance: Relevance is an important concept in information science and information retrieval [19]. IR system effectiveness is most often measured with reference to a test collection, consisting of a set of search topics, a static set of documents over which to search, and human-generated assessments that indicate, for an answer document returned by a search system in response to a topic, whether the document is relevant [28]. Typically, these assessments are made based on "topical" relevance, that is, a judgment of whether the document contains any information that is "about" the material that the search topic is asking for. Other aspects of relevance, such as novelty or contextual factors, are not considered [19]. Based on the relevance judgments, each ranked answer list returned by a system is aggregated into a number that reflects the system's performance, using a chosen effectiveness metric. The metrics themselves reflect either implicit or explicit assumptions about searcher behavior, for example that a relevant document that is returned by a system lower down a ranked list is of less value than if the same document had been returned earlier.
Historically, relevance judgments were often made using a binary scale, where a document is classed as either being relevant to the search topic or not, and many effectiveness metrics use this notion of relevance, including precision, recall, mean average precision (MAP), and precision at a specified cutoff [28]. More recently, based on the observation that searchers can generally distinguish between more than two levels of relevance, evaluation metrics that incorporate multi-level relevance have been proposed [12]. Here, relevance is typically measured on an ordinal scale; metrics that include this more fine-grained notion of relevance include normalized discounted cumulative gain (nDCG) [12] and expected reciprocal rank (ERR) [5]. Some examples of previous choices for the number of levels in ordinal relevance scales include 3 (TREC Terabyte Track [6]), 4 (TREC newswire re-assessments by Sormunen [23]), and 6 (TREC Web Track [7]). Tang et al. [26] studied the self-reported confidence of psychology students when assessing the relevance of bibliographic records on ordinal scales from 2 to 11 levels, and observed that for this specific task, confidence is maximized with a 7-point scale. However, the issue is far from settled: in a broader survey of the optimal number of levels in an ordinal

response scale, Cox [8] concludes that there is no single number of response alternatives that is appropriate for all situations.
Magnitude estimation: Magnitude estimation is a psychophysical technique for the construction of measurement scales for the intensity of sensations. An observer is asked to assign numbers to a series of presented stimuli. The first number can be any value that seems appropriate, with successive numbers then being assigned so that their relative differences reflect the observer's subjective perceptions of differences in stimulus intensity [9]. A key advantage of using magnitude estimation is that the responses are on a ratio scale of measurement [11], meaning that all mathematical operations may be applied to such data, and parametric statistical analysis can be carried out. In contrast, for ordinal (or ranked category) scales certain operations are not defined; for example, the median can be used as a measure of central tendency for ordinal data, but the mean is not meaningful since the distance between the ranked categories is not defined [22].
Proposed by Stanley Stevens in the 1950s, magnitude estimation has a long history, and is the most widely-used psychophysical ratio scaling technique [11]. Initially developed to measure perceptions of physical stimuli, such as the brightness of a light or the loudness of a sound, magnitude estimation has also been successfully applied to a wide range of non-physical stimuli in the social sciences (including occupational preferences, political attitudes, the pleasantness of odors, and the appropriateness of punishments for crimes [25]), in medical applications (such as levels of pain, severity of mental disorders, and emotional stress from life events [11]), in user experience research (for example, as a measure of usability in HCI [17], and for healthcare applications [13]), and in linguistics (including judging the grammaticality of sentences [2]).
In information retrieval research, Eisenberg [10] investigated magnitude estimation in the context of judging the relevance of document citations from a library database (including fields such as author, title, keywords and abstract), and concluded that participants are able to effectively use magnitude estimation in such a scenario. A related technique was used by Spink and Greisdorf [24], where participants in a user study were required to fill in a worksheet with information about the relevance of resources that were retrieved from a library database for personal research projects; this included indicating the level of relevance on a 4-level ordinal scale, providing feedback about other levels of relevance such as utility and motivation, and marking the level of relevance on a 77mm line. The line was then decoded into numbers at a 1mm resolution so was in effect a 78-level ordinal scale.
To the best of our knowledge, the only previous investigation of magnitude estimation for document-level relevance judging in the context of IR evaluation was our own small pilot study [15, 21]. That study indicated that magnitude estimation could be useful for measuring document-level relevance, but used only 3 information need statements and 33 documents.
3. EXPERIMENTAL METHODOLOGY
In this section we describe the details of the experiments that were carried out to gather the needed relevance judgments.
3.1 Topics and Documents
Document-level relevance assessments have traditionally been made using binary or multi-level ordinal scales. To compare magnitude estimation judgments to these approaches, we selected a set of search tasks and documents from the TREC-8 ad hoc collection, which studies informational search over newswire documents. The TREC-8 collection includes binary relevance judgments made by

566

Topic # # docs units
402 278 460 403 111 182 405 214 354 407 212 350 408 188 310 410 212 350 415 179 295 416 174 287 418 243 402 420 164 270 421 342 567 427 195 322 428 253 419 431 203 335 440 264 437 442 408 677 445 210 347 448 419 695
Sum: 4269 7059 Percentage:

H

N

k

k

Back

Fail

0 1 2+ 0 1 2+

LA111689-0162 FBIS3-10954

452 8 0 426 18 16

LA092890-0067 LA071290-0133 178 4 0 120 19 43

LA061490-0072 FBIS3-13680

347 6 1 322 20 12

FT921-6003 FR940407-2-00084 346 3 1 324 9 17

FT923-6110 LA062290-0070 306 4 0 266 16 28

FBIS4-64577 FBIS4-44440

346 4 0 326 10 14

FBIS3-60025 FBIS4-10862

289 4 2 255 23 17

FBIS4-49091 LA112590-0107 286 1 0 267 12 8

LA102189-0167 FT924-6324

394 8 0 372 15 15

LA121590-0108 LA112690-0001 266 4 0 256 4 10

FT941-428 LA073189-0033 554 11 2 537 16 14

FT943-5736 LA080590-0077 315 7 0 214 52 56

FT943-9226 FBIS3-20994

412 6 1 396 13 10

FBIS3-46247 FT944-5962

333 2 0 296 27 12

FT942-3471 LA020589-0074 427 10 0 397 19 21

LA011390-0057 FT923-4524

672 5 0 629 25 23

FT924-8156 LA031989-0092 334 11 2 334 10 3

LA080190-0139 FBIS3-16837

687 8 0 652 21 22

6944 106 9 6389 329 341 98 2 0 90 5 5

Table 1: Topics, number of documents, number of units, Hk, Nk, number of back buttons usage, number of failures in the (c)
and (d) checks, as detailed in the text.

NIST assessors [27]. Subsequently, Sormunen [23] carried out a re-judging exercise where a subset of topics and documents were judged by a group of six Master's students of information studies, fluent in English although not native speakers, on a 4-level ordinal relevance scale: N­not relevant (0); M­marginally relevant (1); R­relevant (2); H­highly relevant (3).
The 18 search topics used in our study are the subset of TREC8 topics which also have Sormunen ordinal judgments available. They are listed in the first column of Table 1. The documents for which we collected magnitude estimation judgments are the set of the top-10 items returned by systems that participated in TREC-8. This gives a total of 4,269 topic-document pairs, of which 3881 have binary TREC relevance judgments available, and 805 of which have Sormunen ordinal judgments available. The number of documents for each topic is shown in Table 1 (2nd column).
3.2 User Study
We carried out a user study through the CrowdFlower crowdsourcing platform during December 2014 and January 2015. The experimental design was reviewed and approved by the RMIT University ethics review board. Participants were paid $0.2 for each task unit, defined as a group of magnitude estimation judgments for 8 documents in relation to one topic. The number of units for each topic is shown in Table 1 (3rd column).
Practice task: After agreeing to take part in the study, a participant was shown a first set of task instructions, including a brief introduction to the experiment and explanation of the magnitude estimation process. Since magnitude estimation may not be familiar to participants, they were first asked to complete a practice task, making magnitude estimations of three lines of different lengths, shown one at a time. If they successfully completed this practice task (success was defined as assigning magnitudes such that the numbers were in ascending order when the lines were sorted from shortest to longest), participants were able to move on to the main part of the experiment.
Main task: The main task of the experiment required making magnitude estimations of the relevance of documents. Participants were informed, by means of a second set of instructions, that they would

be shown an information need statement, and then a sequence of eight documents that had been returned by a search system in response to the information need statement, presented in an arbitrary order, and that: Your task is to indicate how relevant these documents appear to you, in relation to the information need.
For the main task, the title, description and narrative of a TREC topic were first displayed at the top of the screen. After reading the information need, participants had to respond to a 4-way multiplechoice question, to test their understanding of the topic. The test questions focused on the main information concepts presented in the topic statements, and were intended to check that participants were engaging with the task, and as a mechanism to remove spammers. Participants who were unable to answer the test question correctly were unable to continue with the task.
Next, participants were presented with eight documents, one at a time. For each document, the participant was required to enter a magnitude estimation number in a text box displayed directly below the document, and a brief justification of why they entered their number into a larger text field. They were then able to proceed to the next document. A back button was available in the interface, with participants being advised that this should only be used if they wish to correct a mistake; under 3% of submitted jobs included use of this feature (details on the occurrences of clicks on the back button are shown in Table 1, where the number of units with zero, one, or two or more back button clicks are shown).
After entering responses for eight documents, the task was complete. Participants were able to complete further tasks for other topics, up to a maximum of 18 tasks, as they were not able to reassess the same topic.
Magnitude estimation assignments: Regarding the assignment of ME scores, participants were instructed that: You may use any numbers that seem appropriate to you ­ whole numbers, fractions, or decimals. However, you may not use negative numbers, or zero. Don't worry about running out of numbers ­ there will always be a larger number than the largest you use, and a smaller number than the smallest you use. Try to judge each document in relation to the previous one. For example, if the current document seems half as relevant as the previous one, then assign a score that is half of your previously assigned score. (The complete instructions that were shown to participants are available at http://cs.rmit.edu. au/~fscholer/ME/SIGIR15.) While some applications of magnitude estimation use a fixed modulus (specifying a particular number that is to be assigned to the first stimulus that is presented), this has been found to promote clustering of responses and the potential over-representation of some numbers [18]; we therefore allowed participants to freely choose their own values.
Document ordering: As explained above, each participant task unit required the judging of a set of eight documents for a particular search topic statement. The experiments used a randomized design, with documents presented in random order, to avoid potential ordering effects and first sample bias [18]. The document sets were constructed such that two of the documents in each set were a known ordinal H and N document for the topic; these documents (henceforth Hk and Nk) were the same for all the participants working on the same topic (the TREC document identifiers for each topic are shown in Table 1). This was to ensure that each participant saw at least one document from the high and low ends of the relevance continuum. Ensuring that stimuli of different intensity levels are included in a task has been found to be important for the magnitude estimation process [11], and also has an impact on the score normalization process, described below. Moreover, including the two Nk and Hk documents with "known" ordinal relevance

567

values enabled a further data collection quality control check: after
judging all eight documents, participants who had assigned magnitude estimation scores for the Nk document that were larger than for the Hk document were not able to complete the task. In total, at
least 10 ME scores were gathered for each topic-document pair.

Quality checks: Three quality checks were included in the data gathering phase of our experiments:
(a) a practice task requiring magnitude estimation of line lengths;

(b) a multiple-choice test question to check a participant's understanding of the topic; and
(c) a check that the magnitude estimations score for Hk was greater than that assigned to Nk.
In addition, a time-based quality check was also used:
(d) each participant had to spend at least 20 seconds on each of the 8 documents.
If any of (a) or (b) was unsuccessful, the participant could not continue with the task (they were allowed to restart from scratch, on a different unit and therefore on a randomly selected topic, if willing to do so; the same quality checks were applied again). If checks (c) or (d) were unsuccessful, the participant received the message "Your job is not accurate enough. You can revise your work to finish the task", and was allowed to use the back button and revise their previously assigned scores if they wished (the same two checks (c) and (d) were performed again in such a case); however, participants were not made aware which documents or scores were the "offending" responses. Less than 10% of units resulted in conditions (c) or (d) being triggered (see details in Table 1, where the number of eventually successful units with 0, 1, or two or more failed checks are shown), and as already mentioned the back button was used in less than 3% of units. Finally, there was a syntax check that the numeric scores input by the participants were in the (0, +1) range. If any of the checks were not successfully completed, no data for that unit was retained. As such, we did not do any further filtering once the data had been collected.

Crowd Judging: As detailed in Table 1, with 7,059 units com-

prised of 8 documents each, we collected more than 50,000 judg-

ments

in

total,

at

a

cost

of

around

, $1

700

(CrowdFlower

fees

in-

cluded). This is in the order of magnitude of $0.4 for each docu-

ment, which is broadly competitive when compared with the cost of

TREC assessors or other similar crowdsourcing initiatives; for ex-

ample, according to Alonso and Mizzaro [1, Footnote 2], gathering

ordinal scale judgments cost around . per document. However, $0 1
the comparison is more favorable when considering that in our ex-

periments 10 redundant judgments per document are collected, and

the Nk and Hk documents receive multiple judgments, whereas in

Alonso and Mizzaro's work only 5 redundant judgments were col-

lected, and there was nothing similar to our Nk vs. Hk check.

3.3 Score Normalization
Magnitude estimation is a highly flexible process, with observers being free to assign any positive number, including fractions, as a rating of the intensity of a presented stimulus. The key requirement is that the ratio of the numbers should reflect the ratio of the differences in perception between stimulus intensities. As the stimuli are presented in a randomized order, and observers are free to assign a number of their choice to the first presented item, it is natural that different participants may make use of different parts of the positive number space. Magnitude estimation scores are therefore normalized, to adjust for these differences. Geometric averaging is the recommended approach for the normalization of magnitude estimation scores [11, 17], and was applied in our data analysis.

Normalised ME score (median per document)

20



15



 

 

10 5







0

U

N

M

R

H

Sormunen

U

0

1

TREC

Figure 1: ME score distribution by Sormunen and TREC levels.
Recall that for a given search topic, participants made magnitude estimation judgments for groups of eight documents (a unit). To normalize the scores, first the log of each raw score is taken. Next, the arithmetic mean of these log scores is calculated for each unit (µu), and for the topic as a whole (µ). Each individual log score is then adjusted by the unit and topic means, and the exponent is taken of the resulting quantity, giving the final normalized score:
s0i = exp(log(si) µu + µ)
Intuitively, normalization by geometric averaging means that the raw magnitude estimation scores are moved along the number line, both in terms of location and spread. The log transformation is theoretically motivated by the fact that magnitude estimation scores of perceived stimulus intensities have been found to be approximately log-normal [16]. Importantly, normalization through geometric averaging has the property of preserving the ratios of the original scores, the essential feature of the magnitude estimation process. Unless otherwise noted, all analysis reported in this paper uses normalized magnitude estimation scores.
4. MAGNITUDE ESTIMATION RELEVANCE JUDGMENTS
Having obtained magnitude estimation scores for 4,269 topicdocument pairs, we analyze the user-perceived relevance to answer the first research question, whether the magnitude estimation technique is suitable for gathering document level relevance judgments, and whether the resulting relevance scales are reasonable with respect to our current knowledge of relevance judgments.
4.1 Consistency of Magnitude Estimation and Ordinal Relevance
The distribution of the median normalized magnitude estimation scores for each document are shown in Figure 1, aggregated across all 18 topics, and split by Sormunen ordinal relevance levels (left side of figure, levels are N, M, R, H, and U, the group of docu-

568

Normalised ME score (mNeodriamnalipseerddoMcEusmceornte) (mNeodriamnalipseerddoMcEusmceornte) (median per document) Normalised ME score (mNeodriamnalipseerddoMcEusmceornte) (mNeodriamnalipseerddoMcEusmceornte) (median per document) Normalised ME score (mNeodriamnalipseerddoMcEusmceornte) (mNeodriamnalipseerddoMcEusmceornte) (median per document) Normalised ME score (mNeodriamnalipseerddoMcEusmceornte) (mNeodriamnalipseerddoMcEusmceornte) (median per document) Normalised ME score (mNeodriamnalipseerddoMcEusmceornte) (mNeodriamnalipseerddoMcEusmceornte) (median per document)

20 402

403

405

407

408

410

15

10

 





5



0 20 415 

 
416

   
418

    
420

   
421

   
427

15



10



5





0

20 428



15



10





5



0 UNMRHU 0 1

    
431
 

UNMRHU 0 1

 
440
 
UNMRHU 0 1

  
442
    
UNMRHU 0 1

   
445
     
UNMRHU 0 1

 
448

  
UNMRHU 0 1

Figure 2: ME score distribution by Sormunen (U, N, M, R, H) and TREC (U, 0, 1) levels: breakdown for individual topics.

ments that were not judged by Sormunen), and by TREC binary levels (right side of figure, levels are 0, 1, and U, the documents that were not judged by TREC, as not all participating runs were included in the judging pool). This boxplot (and subsequent boxplots) show the median as a solid black line; boxes show the 25th to 75th percentile; and whiskers show the range, up to 1.5 times the inter-quartile range. There is a clear distinction between each of the four adjacent Sormunen levels, (two-tailed t-test, p < 0.002), with the magnitude estimation scores on average following the ordinal scale rank ordering. The differences between the two TREC levels are also significant (p < 0.001), with the magnitude estimation scores on average again being aligned with the binary levels. This is strong evidence for the overall validity of the magnitude estimation approach.
Figure 2 shows the magnitude estimation score distributions for each of the 18 individual topics. Although there is some variability across topics, overall the figure confirms that the magnitude estimation scores are usually aligned with ordinal categories even when considering individual topics: the medians of the median magnitude estimation scores (the solid black lines) generally follow the ordinal categories, for all categories and for all topics (there are some exceptions for some of the Sormunen adjacent categories in topics 403, 405, 410, 420, 421, and 440; there are no exceptions for non-adjacent categories, nor for TREC categories). Since for each topic there could potentially be 3 exceptions for adjacent categories, and 6 exceptions in general, plus one exception for the two TREC categories, the 6 exceptions found are out of 318+18 = 72 possible cases when considering only adjacent categories, or out of 6  18 + 18 = 126 cases when considering also non-adjacent categories. Such a limited fraction of exceptions (in the 5%-8% range) is further strong evidence for the validity of our approach, even at the single topic level.

Regarding the set of documents that were not judged by Sormunen (left-most boxes in Figure 1 and sub-plots in Figure 2), based on the magnitude estimation scores it can be inferred that the bulk of this class are likely to be non-relevant; however there are also instances that occur across the central parts of the marginal to highly relevant score distributions. There were also a handful of documents unjudged by TREC that seemed to be rated highly by our judges. It can also be observed that while the overall distributions of magnitude estimation scores are strongly consistent with the ordinal and binary categories, there are also documents in each class where the ME scores fall into the central region of a different class. We therefore next investigate judge agreement.
4.2 Judge Agreement
It is well-known that relevance is subjective, even when focusing on "topical" relevance as is typically done in evaluation campaigns such as TREC, and that judges will therefore not perfectly agree. To investigate the level of agreement when using different scales for judging relevance, we compare the pairwise orderings between binary, ordinal and magnitude estimation relevance judgments. Figure 3 shows the proportion of document pairs that agree on the order of relevance. For example, when comparing ordinal and magnitude estimation ratings, if two documents are rated N and M, and the corresponding magnitude estimation score of the first is lower than or equal to the score for the second, this would be an agreement, while if the magnitude estimation score was higher for the second document, it would indicate disagreement. The figure shows the pairwise agreement proportions between magnitude estimation (ME), Sormunen (S) and TREC (T), for each of the six possible combinations of ordinal relevance levels. Red circles indicate the mean score for each group. It can be seen that the rates of agreement are highly consistent when comparing any of the three relevance scales. In particular, ME leads to a higher average agree-

569

Proportion of doc-pairs that agree on order of relevance (%)

100

80

60

40

20

0

NH

NR

MH

RH

MR

NM

ME ME S ME ME S ME ME S ME ME S ME ME S ME ME S STTSTTSTTSTTSTTSTT

Judgements compared

Figure 3: Agreement on the ordering of relevance of all document pairs (one small dot per topic) between judges as indicated on the x-axis (ME: magnitude estimation, S: Sormunen and T: TREC). The large (red) dot is the mean over all topics.

ment with Sormunen than TREC with Sormunen for documents in the NH, NR, MH, and MR pairs, and slightly lower for RH and NM: it can be said that normalized ME scores agree, in terms of ranks, with Sormunen at least as well as TREC agrees with Sormunen. Across all groups, the overall average agreement rates are 77% between magnitude estimation and Sormunen, 64% between magnitude estimation and TREC, and 65% between Sormunen and TREC. This further supports the validity of the magnitude estimation approach for gathering relevance judgments.
4.3 Failure Analysis
Despite the similar overall levels of agreement between the magnitude estimation method and ordinal relevance, Figures 1 and 2 show that some individual documents appear to be "misjudged". We therefore conducted a failure analysis, manually examining a subset of documents for which the Sormunen relevance level and the median magnitude estimation scores were substantially different (for example, where a particular document was assigned an ordinal Sormunen relevance level of N, but the median magnitude estimation score for the document was closer to the magnitude estimation scores assigned to H documents for the same topic, and substantially higher than the magnitude estimation scores assigned to other N documents for the same topic). Based on the manual examination of 34 documents where there appeared to be a significant flip between the ordinal and magnitude estimation scores, we found two broad classes of disagreements: those where one group of assessors appeared to be clearly wrong; and a class where the topic statement itself is so unclear as to be open to interpretation.
Of 34 documents that were examined, we found 14 cases (41.2%) where the Sormunen ordinal judgments appeared clearly wrong, and 9 (26.5%) of cases where the crowd-based magnitude estimation assessments appeared clearly wrong. For this class of clear disagreements, where some assessors appear to be clearly wrong in the assignment of relevance (whether ordinal or magnitude estima-

tion), the cause mostly appears to be that the assessors have missed or ignored a specific restriction included as part of the TREC topic. For example, the narrative of topic 410, "Schengen agreement", includes the statement that: "Relevant documents will contain any information about the actions of signatories of the Schengen agreement such as: measures to eliminate border controls...". Document FT932-17156 makes clear reference to nine signatories of Schengen, and the process of removing passport checks. As such, it seems implausible that the document should be classed as N, or completely non-relevant. The original TREC binary judgment supports this view, having assigned a rating of 1 (indicating that the document is at least marginally relevant).
For the remaining 11 (32.4%) of cases, it was not possible to determine that one assessment was clearly correct and the other wrong. Here, the original TREC topic statement itself was ambiguous, preventing a clear conclusion to be drawn based on the limited information that the topic statement provided. For example, a number of topics list several concepts in the narrative about what is or is not deemed relevant. However, they introduce ambiguity about whether the document must meet all of the listed criteria, or whether a subset is sufficient. For example, topic 407, "poaching, wildlife preserves", states that "A relevant document must discuss poaching in wildlife preserves, not in the wild itself. Also deemed relevant is evidence of preventive measures being taken by local authorities." This raises the ambiguity of whether preventative measures by authorities against poaching, but not specifically in wildlife preserves, should be considered as being at least somewhat relevant, or completely non-relevant. We note that further ambiguity is introduced due to the temporal mismatch between the time when the documents and topics were written (1990s), and when the magnitude estimation judgments are being made (2010s). This is particularly the case for topics that include terms such as "current".
The above failure analysis must also be interpreted in the context that it is known that assessors make mistakes when judging, perhaps due to fatigue or other lapses in attention, leading to selfinconsistencies [4, 20]; or they may display systematic errors due to a misunderstanding of the relevance criteria, or relevance drift [29]. Clearly, assessor errors will lower overall agreement rates when comparing assessments. Determining whether magnitude estimation relevance assessments lead to higher or lower error rates compared to using ordinal or binary scales is left for future work.
Overall, the examination of a set of clear disagreements demonstrates that there are cases where both groups of assessors (ordinal or magnitude estimation) are at odds with certain details of the TREC topic statements, and that these appear to occur at broadly similar rates. Moreover, the topic statements themselves are sometimes a cause of ambiguity, placing a practical upper-limit on the agreement that can be achieved. We conclude therefore that the magnitude estimation relevance judgments are sound and sensible, having similar agreement rates with the ordinal Sormunen judgments as the Sormunen judgments have with TREC assessments.
5. MAGNITUDE ESTIMATION FOR SYSTEM-LEVEL EVALUATION
The second research question concerns the direct application of magnitude estimation relevance judgments for the evaluation of IR systems, by considering their use for calibrating gain levels in two widely used gain-based IR evaluation metrics, nDCG and ERR.
5.1 Gain in the nDCG and ERR Metrics
Magnitude estimation provides scores that reflect ratios of human perceptions of the intensity of relevance of different docu-

570

ments in relation to a topic. This is directly related to the notion of gain in effectiveness metrics such as normalized discounted cumulative gain (nDCG). For example, Järvelin and Kekäläinen [12] describe "cumulative relevance gain" that a user receives by examining a search results list, and discuss setting "relevance weights at different relevance levels". That is, weights are applied to each level of an ordinal relevance scale, and can be chosen to reflect different assumptions about searcher relevance behavior. However, the "standard" approach that has been adopted when using nDCG is to simply assign ascending integer values to the ordinal levels, starting with 0 for the lowest (non-relevant) level; for a 3-level ordinal scale, the default gains would be 0, 1 and 2, as for example implemented in trec_eval.1
In addition to modeling different levels of gain, or relevance, the discounted cumulative gain metric also includes a discounting function, so that documents that are retrieved further down in a ranked search results list contribute smaller amounts of gain, reflective of factors such as the effort or time that a user must invest while working their way through the list [12]. Using a logarithmic discount, discounted cumulative gain at cutoff N is calculated as

DCG@N

=

G1

+

X N
i=2

Gi log2(i)

,

where Gi is the gain value for the document at position i in the ranked list. To enable fair comparisons across topics with different numbers of relevant documents, the DCG@N score is divided by an ideal gain vector, a ranking of documents in decreasing relevance order, to obtain normalized discounted cumulative gain, nDCG@N.
Expected reciprocal rank is a metric based on a cascade model of searcher behavior, where the probability of continuing on to the next position in the ranked results list is influenced by the relevance of previous items. The metric calculates the expected reciprocal rank at which the searcher will stop [5], and is defined as

X N R(Gi) iY1

ERR@N =

i

(1

i=1

j=1

R(Gi

, ))

where R G

G i

/

G max

.

In

the

analysis

that

follows,

we

( ) = (2 1) 2

calculate both nDCG and ERR up to a depth of N = 10.

For both nDCG and ERR, gain is based on the relevance of a doc-

ument, which has previously been measured on an ordinal scale,

typically with 3 [14] or 4 [12] levels, depending on the test collec-

tion being used. Since magnitude estimation relevance judgments

are continuous rather than ordinal in nature, and reflect the per-

ceived level of relevance for individual topic-document combina-

tions, it is possible to assign more fine-grained gain values, poten-

tially reflecting different gains for individual documents, or even

for individual searchers.

5.2 Comparative System Rankings
Given that the agreement between our magnitude scores, TREC judgments and Sormunen's judgments is not perfect, it is reasonable to expect that relative system effectiveness orderings computed with each of these as a basis may differ. We first examine the correlation between system orderings using magnitude estimation scores and TREC relevance as gain values, as for both these judgment sets we have nearly complete coverage of the top 10 documents of all runs submitted to TREC-8, for our 18 judged topics. Figure 4 shows the system scores obtained using median magnitude estimation scores per document (x-axis), and binary TREC categories (y-axis). It can be seen that there are definite changes in
1http://trec.nist.gov/trec_eval

system rankings when using the different relevance scales. When using the nDCG@10 metric, there is only small movement of the top ranked systems (cluster of points towards the top right), but when using the ERR@10 metric there is a large perturbation of the top ranked systems using the different judgments.
As the aim of system evaluation is to identify top-performing systems, we also consider changes in the top set, defined as the group of systems that are statistically indistinguishable from the system with the highest mean effectiveness, using a paired Wilcoxon signed-rank test with p < 0.05. The overlap of the top set, based on the TREC and magnitude estimation relevance judgments is 44% for nDCG@10 and 76% for ERR@10, confirming that the perturbation in system ordering has an impact on which systems are identified as being the best performers.
Since the Sormunen judgments only cover around 19% of the documents that occur in the top 10 of all TREC-8 ad hoc runs, evaluating the original runs using these relevance judgments is problematic, due to the large number of unjudged documents. However, the judgment coverage of individual ranked lists (that is, for particular topics within a run) varies substantially. Therefore, to enable a comparison between the ordinal judgment and magnitude estimation judgments on system orderings, we simulate runs that only include ranked lists for topics that are fully judged by Sormunen.
For each topic, we identify all ranked lists within the full set of runs that have Sormunen judgments for all top 10 documents for that topic; we call each of these a complete sub-run. There are 12 topics where there are at least two complete sub-runs out of all of the TREC-8 ad hoc runs for that topic. To form a simulated retrieval system, we then randomly choose one complete sub-run for each of the 12 topics to give a "full run" over all 12 topics. Using this method, we construct 100 simulated system runs that are random merges of complete sub-runs from real runs. While these simulated systems are not actual TREC submissions, they are plausible in that each individual topic ranking is from a real TREC run.
Figure 5 shows that there is also a large discordance between the system rankings using the Sormunen categories and median magnitudes as gain values. However, note that the scale is different in this figure than in Figure 4, as the simulated systems all have relevant documents in the top 10, and so are high scoring. The overlaps in top sets are 51% for nDCG@10 and 81% for ERR@10. The relatively high performance of systems on this simulated collection is to be expected, as only complete sub-runs were selected, and around half of the Sormunen judgments are for documents that were originally rated as relevant by TREC, and so the runs in the simulated systems have a high number of relevant documents. It is interesting that there is still a sizable change in the top set using both metrics, however.
5.3 Judgment Variability
There is substantial variation in the magnitudes assigned to documents by our judges. As we have at least 10 judgments per topicdocument pair, rather than using the median score per document as in the previous section, we can resample individual scores many times, recompute system orderings, and get a distribution of  values. The results are shown in Figure 6: the correlations between system orderings using sampled magnitudes as gains are generally lower than when using the median magnitudes. This reflects the wide range of individual variation ­ differences in perceptions of relevance ­ that in turn leads to a wide range of  values.
An alternate explanation for the low  values is that gains set as magnitudes are generally much higher than the gains set by Sormunen's categories. From Figure 1 it is apparent that magnitudes are generally in the range of 2 to 15 (although some are as large as

571

TREC category as gain

nDCG@10 0.8

0.6

0.4

0.2

0.0

 = 0.814

0.0 0.2 0.4 0.6 0.8 Median-ME as gain

ERR@10 0.6 0.5 0.4 0.3 0.2 0.1 0.0

 = 0.515

0.0 0.1 0.2 0.3 Median-ME as gain

Figure 4: System scores using TREC categories (y-axis) and magnitudes (x-axis) as gains in the nDCG@10 and ERR@10 metrics. There is one dot per system participating in the TREC-8 ad hoc track. Kendall's  is shown.

Sormunen category as gain

nDCG@10 0.70 0.68 0.66 0.64 0.62 0.60

 = 0.529

0.62 0.64 0.66 0.68 Median-ME as gain

ERR@10 0.75

0.70

0.65

0.60

 = 0.267

0.25 0.30 0.35 Median-ME as gain

Figure 5: System scores using Sormunen categories (y-axis) and magnitudes (x-axis) as gains in the nDCG@10 and ERR@10 metrics. There is one dot per simulated system (see text). Kendall's  is shown.

100 or 1000), whereas using categories as gains the values are 0, 1, 2 or 3. However, because both nDCG and ERR are normalized, this scale effect is nullified to a degree. For example, multiplying gains by a constant has no effect on nDCG, and even altering the category scores using a small exponential or additive constant has little effect. Table 2 shows this for our 12 topics on the simulated systems. The final row hints that using magnitudes that are gathered on a wide scale may alter system rankings using nDCG@10 compared with simply using the category values as gains. Similarly, the first row suggests that using gains gathered on a wide scale may alter rankings using ERR@10. One way to examine this further is to split our data into narrow units, those whose Hk/Nk ratios are less

Kendall's 

0.8 0.6 0.4 0.2 0.0 -0.2



 

nDCG@10

ERR@10

TREC vs Magnitudes



nDCG@10

ERR@10

Sormunen vs Magnitudes

Figure 6: Kendall's  between system rankings obtained using gains from the judgments and metrics indicated on the x-axis, where magnitudes are randomly sampled for each document 1000 times. Compare with  in Figures 4 and 5.

Mapping

Gi = 100  Ci

Gi = 1 + Ci

Gi = 10 + Ci

Gi

=

1+C i
2

Gi

=

1+C i
10

nDCG@10
1.0000 0.9994 0.9950 0.9568 0.7833

ERR@10
0.8719 0.9985 0.9949 0.8738 0.8719

Table 2: Correlation (Kendall's  ) between system orderings when using Sormunen categories as gain (Gi = Ci), and other mappings on the simulated systems, when using the nDCG@10 and ERR@10 metrics.

Source of gains
TREC, 
nDCG@10 ERR@10
TREC, top set
nDCG@10 ERR@10
Sormunen-Simulated, 
nDCG@10 ERR@10
Sormunen-Simulated, top set
nDCG@10 ERR@10

Narrow
0.780 0.537
89% 56%
0.491 0.179
67% 100%

All
0.814 0.515
44% 76%
0.529 0.267
51% 81%

Wide
0.765 0.434
100% 95%
0.361 -0.010
82% 100%

Table 3: Correlation (Kendall's  ) and overlap of the top set between system orderings when median document magnitude scores are based on narrow, wide or all units.

than 5 (the median value), and wide units, where the ratios Hk/Nk are 5 or more.
Table 3 shows the correlation (Kendall's  ) between system orderings, and overlap of runs in the top set, when using TREC and Sormunen judgments, and magnitude estimation scores when the median is obtained from units that used a narrow scale, a wide scale, or from all units (note that the  values in the All column match those in Figures 4 and 5). When there was no narrow or wide judgment for a topic-document pair (all units containing that topicdocument pair had Hk/Nk above or below the median), the minimum or maximum magnitude over all units was taken, respectively. System orderings based on narrow or wide units show greater differences (lower  values) than when considering the full data set. This is consistent with the results in Table 2. The overlap of the top set is generally less than 100% for the narrow/wide data sets, indicating that some perturbation of the set of "equivalent" top systems is occurring, but the pattern is not as clear as for  values. This may be due to the relatively small number of topics that is being considered, especially for the simulated systems, and more topics are required before a definitive conclusion about the effect of narrow and wide units on the top set can be made.
Overall, there are clear differences in system effectiveness orderings as measured using Kendall's  , when using narrow and wide units. In particular, it appears that using the median of narrow units, or of all units, leads to higher correlations with existing measurement techniques, compared to when using wide units. We

572

Hk Nk Ratio (rounded to nearest 5)

10000 1000 200 100 95 90 85 80 75 70 65 60 55 50 45 40 35 30 25 20 15 10 5 0
1

4

16

64

256

1024

Number of units (log)

Figure 7: Number of units with a particular Hk/Nk ratio over all topics. There were a total of 7,059 units. 37 units with frequency less than 4 are not shown.

n 8478
>20 1200
20

n 2252 >20 160

n 2630 >20 227

n 7662 >20 646

Magnitudes (normalised by Nk)

15

10

5 
0 N

  

M

R

H

Histograms of magnitudes for each Sormunen level

Figure 8: Distribution of magnitudes (normalized by Nk)

for each Sormunen level. Circles are linear gain values,

{1, 2, 3, 4}, while triangles represent exponential gain values,

{1 2

,

2
2

,

3
2

,24

}.

Black

lines

are

medians;

the

text

"n"

gives

the

total number of magnitudes in the level, and "> 20" shows the

count of scores above 20.

can therefore infer that current evaluations using nDCG@10 and ERR@10 do not assume wide units as their underlying user model. This is important because it shows that nearly half of our participants are not behaving according to the user model assumed by current evaluation techniques. Figure 7 shows the full distribution of the "width" (Hk/Nk ratio) of units rounded to the nearest 5 (with bars containing less than 4 units omitted).
6. INVESTIGATING GAIN USING MAGNITUDE ESTIMATION
The previous section used perturbations in system orderings to examine the difference between using magnitude estimate relevance scores and the usually employed ordinal relevance levels as gains. This section directly considers gain profiles, answering our third research question by considering what additional insights magnitude estimation can provide into user perceptions of relevance.
The gain weights of the nDCG metric allow for the modeling of different user relevance preferences. However, in practice gains are usually set to one of two profiles: a linear setting (as defined in Section 5); or an exponential setting, where the ordinal relevance level forms a power of 2 [3], placing more emphasis on highly relevant documents. Figure 8 shows the distribution of magnitude estimation scores, normalized by the Nk score in each unit (intu-

237

62

119

3

Ratio

2

1

0

H/R

H/mean(R,M)

mean(H,R)/M

Sormunen relevance levels

Figure 9: Ratio of the magnitude scores assigned to the Sormunen categories highly relevant (H) to relevant (R) and marginally relevant (M) over all pairs of documents. Numbers indicate the count of document pairs that are greater than 3.5 for each box.

itively, a measure of relevance standardized for each user). Black lines show the median magnitudes for each group. Superimposed are the two default profiles, linear (white circles), and exponential (white triangles). It can be seen that, compared to actual user perceptions of the relevance space, the linear gain profile is fairly close to the gain that should be allocated at each level to satisfy the (mythical) median user. The exponential profile is further from the median, overestimating the gain for the R and H levels, but catering for some judgments.
It is readily apparent, however, that the large spread of the distributions of magnitudes cannot be captured in a single gain formulation. There is simply too much individual variation to warrant the simplification of relevance perceptions to a single profile.
Our final piece of analysis is given in Figure 9, which shows the ratio of magnitudes for document pairs in the Sormunen levels highly relevant and partially relevant. We combine the three relevance levels into two to allow comparison with Kanoulas and Aslam [14]. They recommend a ratio of 1.1 for relevant to highly relevant, which is supported by the medians in Figure 9. However, again, we see a wide variation from this median.
7. DISCUSSION AND CONCLUSIONS
This is the first study to have collected large scale, real user data about relevance perceptions using magnitude estimation. Our first research question investigated the suitability of using magnitude estimation for gathering relevance judgments. As shown in Section 4, magnitude estimation scores are consistent with classical ordinal scores, both on single topics and overall on the 18 aggregated topics; ordering agreement between magnitude estimation judgments and ordinal judgments is higher than between ordinal and binary judgments; and failure analysis demonstrated that disagreement is often due not to problems with magnitude estimation per se but rather to different judging context. It is likely that using a classical scale one would get a similar agreement level.
Considering the impact of magnitude estimation relevance judgments on IR system evaluation, the second research question, our analysis showed that system orderings vary substantially when different relevance scales are used, with a  of 0.81 for nDCG@10 when comparing magnitude estimation scores with binary relevance across TREC runs, and 0.53 when comparing with ordinal relevance on simulated runs. When individual variability is considered, these correlations are even lower. The analysis suggests that

573

it is important to incorporate different judging scales ­ users who have wider or thinner perceptions of the differences in relevance.
We also employed magnitude estimation to directly investigate gain profiles, the third research question. Typically, gains are set as linear or exponential profiles. Our analysis of user-reported relevance perception showed that the linear profile is close to the "average" user, but the distribution of magnitudes suggests that attempting to fit a single profile, or view of relevance, for system evaluation is unlikely to be sufficient.
While on average our magnitudes were broadly equivalent to previous ordinal scales, the outstanding feature of our data was the wide range of scores that participants chose to employ in the judging task. In particular, at least half of the participants chose gain values that are not consistent with currently used values. Section 5.2 shows that using judgments made on a wide scale leads to different system rankings than judgments collected on a narrow scale. Recall that these scales are not imposed on the judge, as they are in all previous relevance judgment tasks in the literature, but are chosen by the participants themselves.
This is another key contribution of this study: when a priori categorical scales are used for relevance judgment tasks, there is no possible way to capture variance in human perception of the scale of relevance. In turn, this limits our understanding of how gain should be set in DCG-like metrics, and hence our ability to accurately evaluate systems.
Throughout the paper we have assumed that topical relevance collected using magnitude estimation can be used directly as gain in DCG-like measures. While perhaps being more representative of user perceptions than other relevance scales, this still makes many assumptions about the user's search process which are probably untrue: for example, ignoring the interdependence of documents and other aspects of relevance. In future work, we plan to investigate whether magnitude estimation can be used to reliably scale other aspects of relevance such as novelty, as well as search outcome measures such as satisfaction, and to examine whether this can lead to more meaningful gain representations.
Acknowledgments This work was supported by the Australian Research Council (Discovery Project DP130104007).
References
[1] O. Alonso and S. Mizzaro. Using crowdsourcing for TREC relevance assessment. Information Processing and Management, 48(6):1053­ 1066, 2012.
[2] E. G. Bard, D. Robertson, and A. Sorace. Magnitude estimation of linguistic acceptability. Language, 72(1):32­68, 1996.
[3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning, pages 89­96, Bonn, Germany, 2005.
[4] B. Carterette and I. Soboroff. The effect of assessor error on IR system evaluation. In Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, pages 539­546, Geneva, Switzerland, 2010.
[5] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the ACM CIKM International Conference on Information and Knowledge Management, pages 621­630, Hong Kong, 2009.
[6] C. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2004 terabyte track. In The Fourteenth Text REtrieval Conference (TREC 2005), Gaithersburg, MD, 2005. NIST.
[7] K. Collins-Thompson, P. Bennett, F. Diaz, C. L. Clarke, and E. M. Voorhees. TREC 2013 Web Track Overview. In 22nd Text REtrieval Conference (TREC 2013), Gaithersburg, MD, 2014.
[8] E. P. Cox,III. The optimal number of response alternatives for a scale: A review. Journal of marketing research, 17(4):407­422, 1980.

[9] W. H. Ehrenstein and A. Ehrenstein. Psychophysical methods. In U. Windhorst and H. Johansson, editors, Modern techniques in neuroscience research, pages 1211­1241. Springer, 1999.
[10] M. Eisenberg. Measuring relevance judgements. Information Processing and Management, 24:373­389, 1988.
[11] G. Gescheider. Psychophysics: The Fundamentals. Lawrence Erlbaum Associates, 3rd edition, 1997.
[12] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422­ 446, 2002.
[13] K. Johnson and A. Bojko. How much is too much? Using magnitude estimation for user experience research. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 54(12):942­946, 2010.
[14] E. Kanoulas and J. A. Aslam. Empirical justification of the gain and discount function for nDCG. In Proceedings of the ACM CIKM International Conference on Information and Knowledge Management, pages 611­620, Hong Kong, 2009.
[15] E. Maddalena, S. Mizzaro, F. Scholer, and A. Turpin. Judging relevance using magnitude estimation. In Advances in Information Retrieval - Proceedings of 37th ECIR, volume 9022 of LNCS, pages 215­ 220, Vienna, Austria, 2015.
[16] L. Marks. Sensory Processes: The new Psychophysics. Academic Press, 1974.
[17] M. McGee. Usability magnitude estimation. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 47(4):691­ 695, 2003.
[18] H. R. Moskowitz. Magnitude estimation: notes on what, how, when, and why to use it. Journal of Food Quality, 1(3):195­227, 1977.
[19] T. Saracevic. Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. JASIST, 58(13):1915­1933, 2007.
[20] F. Scholer, A. Turpin, and M. Sanderson. Quantifying test collection quality based on the consistency of relevance judgements. In Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, pages 1063­1072, Beijing, China, 2011.
[21] F. Scholer, E. Maddalena, S. Mizzaro, and A. Turpin. Magnitudes of relevance: Relevance judgements, magnitude estimation, and crowdsourcing. In The Sixth International Workshop on Evaluating Information Access (EVIA 2014), Tokyo, Japan, 2014.
[22] D. Sheskin. Handbook of parametric and nonparametric statistical procedures, 4th ed. CRC Press, 2007.
[23] E. Sormunen. Liberal relevance criteria of TREC: Counting on negligible documents? In Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, pages 324­330, Tampere, Finland, 2002.
[24] A. Spink and H. Greisdorf. Regions and levels: Measuring and mapping users' relevance judgments. JASIST, 52(2):161­173, 2001.
[25] S. S. Stevens. A metric for the social consensus. Science (New York, NY), 151(3710):530­541, 1966.
[26] R. Tang, W. M. Shaw, and J. L. Vevea. Towards the identification of the optimal number of relevance categories. JASIS, 50(3):254­264, 1999.
[27] E. M. Voorhees and D. K. Harman. Overview of the Eighth Text REtrieval Conference (TREC-8). In The Eighth Text REtrieval Conference (TREC-8), pages 1­24, Gaithersburg, MD, 1999.
[28] E. M. Voorhees and D. K. Harman. TREC: experiment and evaluation in information retrieval. MIT Press, 2005.
[29] W. Webber and J. Pickens. Assessor disagreement and text classifier accuracy. In Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, pages 929­ 932, Dublin, Ireland, 2013.

574

Learning to Reweight Terms with Distributed Representations

Guoqing Zheng
School of Computer Science Carnegie Mellon University
5000 Forbes Avenue Pittsburgh, PA 15213, USA
gzheng@cs.cmu.edu
ABSTRACT
Term weighting is a fundamental problem in IR research and numerous weighting models have been proposed. Proper term weighting can greatly improve retrieval accuracies, which essentially involves two types of query understanding: interpreting the query and judging the relative contribution of the terms to the query. These two steps are often dealt with separately, and complicated yet not so effective weighting strategies are proposed. In this paper, we propose to address query interpretation and term weighting in a unified framework built upon distributed representations of words from recent advances in neural network language modeling. Specifically, we represent term and query as vectors in the same latent space, construct features for terms using their word vectors and learn a model to map the features onto the defined target term weights. The proposed method is simple yet effective. Experiments using four collections and two retrieval models demonstrates significantly higher retrieval accuracies than baseline models.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms
Algorithms, Experimentation
Keywords
Query term weighting, distributed representations, word vectors
1. INTRODUCTION
Performance of text search engines relies heavily on query understanding, of which one important problem is how to
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767700.

Jamie Callan
School of Computer Science Carnegie Mellon University
5000 Forbes Avenue Pittsburgh, PA 15213, USA
callan@cs.cmu.edu
weight the contribution of each individual term to the retrieval score1. When proper weights (such as ground truth term recall weights [22]) are used, they can boost retrieval accuracies by up to 30% given relevance judgments. Properly setting the query term weights requires accurately interpreting and properly representing the query first. This is no easy task as query intent understanding itself is a diffcult problem in IR research [7].
In this paper, we attempt to address query interpretation and term weighing from a different angle, with a unified framework built upon recent advances in neural network language modeling [13, 3]. Recent research in the application of neural network to text problems exploits the cooccurrence of words to represent words by multidimensional vectors. Distributed representations learned from neural network based models [13, 3] are designed and shown to be effective for measuring semantic similarity among words and identifying similar neighbors for a given word. The mapping from words to vectors gives the ability to not only measure word-word similarity but also ways to represent the query in the same vector space from word vectors from its terms (such as taking the average of the word vectors of all terms as the vector representation for the query).
As proper query term weights reflects the relative importance of the term with respect to the query, specifically we propose to construct features from word vectors representations of terms and the query and to learn the relationship between the feature vectors and target term weight (such as the term recall weight [22] estimated from relevance judgments). A regularized linear regression problem from the feature vectors onto term weights is formulated and the predicted term weights are used for both bag-of-words queries and term dependency queries. We demonstrate the effectiveness of our method using two popular retrieval models, four standard test collections, word vectors developed from a variety of sources, and three baseline methods.
The contributions of our work are three fold. First, we join the work of distributed word vectors to the prediction of query term weights in IR, and propose a simple yet effective framework to predict effective term weights. Second, we observe significant improvement over the baseline mod-
1People use "term weight" to describe two different settings: one is the matching score a retrieval function assigns to a term and a document pair; the other is the relative importance of the term to the query. The first setting is handled by the retrieval model while the query model deals with the second one; in this paper, we refer "term weight" or "term reweighting" to that in the second setting.

575

els with two retrieval models over four standard collections when using predicted term recall as term weights. Third, the proposed method is much more efficient than previous work on query term weight prediction, i.e., term recall weight, which requires an initial retrieval and a local SVD for every new incoming query to obtain features for predicting term recall [22]. The proposed framework derives feature vectors directly from precomputed distributed word vectors; simple computations are sufficient for predicting term weights for new queries.
The remainder of this paper is organized as follows: Section 2 introduces prior research related to query term weighting and term recall weight. Section 3 discusses the preliminaries for term recall prediction and distributed word vectors. Section 4 formally presents our approach in term weight modeling and estimation. Section 5 describes the data sets and the experimental settings. Experimental results as well as data analysis are presented in Section 6. At last, we conclude the paper and discuss some future work in Section 7.

2. RELATED WORK
Query term weighting has been extensively studied in IR literature and a retrieval model reflects its choice of query term weights used. Conceptually, any retrieval model can be abstracted as the following scoring function:

Score(q, D) =

w(t)f (t, D)

(1)

tqD

where f (t, D) is the matching score of term t and document
D (for example, term frequency), and w(t), the query term
weight, which doesn't specifically depend on D (for example,
inverse term frequency), is the quantity we are interested
in this paper. By formulating this way, existing retrieval
models make different choices about term weight. The most
commonly used query term weights in the literature is idf,
for example, the vector space model, language model [21],
BM25, etc. Another well known term weight, the term recall weight,
is closely related to idf and also attracts broad attention. It is originally captured by the Binary Independence Model (BIM)[16] to emphasize the importance of query term weights, as shown below:

Score(q, D) 

log

ti qD

1

P(ti |Rq ) - P(ti|Rq)

×

1

- P(ti|R¯q) P(ti |R¯q )

(2)

where Rq is the set of relevant documents, R¯q is the set of non-relevant documents, d is a document to be ranked, q is the query, and ti is a query term. The probability P(t|Rq) provides one way to weight query terms, known as the RSJ term weight, to improve retrieval performance. However, due to involvement of the relevant set of documents of query q, it is hard to give a reliable estimation of P(t|Rq) when relevance information is unavailable. Indeed, researchers recognized that the use of term recall weight could lead to huge retrieval gain as P(t|Rq) is actually the only term about relevance in the ranking function [22], and proposed several ways to predict it.
Croft and Harper [5] modeled the query term weight as a tuned constant (the Croft/Harper Combination Match model). Greiff [6] tried to predict term weight and P(t|R¯q) with a linear function of idf. His experiments showed some improvement over the BIM model. More recently, Metzler

[12] modeled term weight as a linear function of document frequency. The above modeling of term weight only used df or idf features. The predictions were inadequate as they did not reflect the insight that P(t|Rq) is a query dependent quantity and that query dependent features are needed to estimate term weight. Recently, Zhao et al. [22] proposed a framework to construct features for query terms from pseudo relevance feedback [20] and use them to predict term weight.
Retrieval models that capture term dependencies have attracted research attention and also demonstrated their retrieval effectiveness compared to unigram query models. One widely used model is the sequential dependency model (SD) [10], which features three types of query concepts: terms, bigrams and proximity expression. An example of a sequential dependency query in the Indri query language for the bag-of-words query apple pie recipe is:
#weight( 0.8 #combine( apple pie recipe )
0.1 #combine( #1(apple pie)
#1(pie recipe) )
0.1 #combine( #uw8(apple pie)
#uw8(pie recipe) )
)
#1(apple pie) matches when apple and pie form a bigram. #uw8(apple pie) matches when apple and pie occur in any order within a window of 8 words. And #combine is a probabilistic AND operator.
The sequential dependence model provides basic weighting for different types of query concepts. Broad empirical results have validated the effectiveness of SD over the unweighted bag-of-words query model [1, 2, 10]. However, concepts of the same type share the same weight, which is not optimal. Recent research proposed to predict term weights for each concept using features computed from collection statistics, an adaptive model, and an optimization goal of maximizing an evaluation metric such as Mean Average Precision (MAP) [1, 2]. Term weighting strategy emphasizing query aspects is also proposed [23].
Our approach differs from the above methods in that we represent terms and queries using distributed word vectors in a semantic vector space learnt from a global corpus and we construct novel feature vectors from the distributed representations to automatically learn term weights for efficient retrieval.
3. PRELIMINARIES
In this section, we briefly introduce recent work on distributed representations learning from neural network language models and also defines the target term weights we use in our framework.
3.1 Distributed word vectors
Neural network based language models aim to learn the word vector representations and a statistical language model for the underlying text. These models can be mainly attributed to two categories. Models from the first category try to learn the word vector representations and the language model jointly. One example is the neural network language model (NNLM) [3], where a linear projection layer and a non-linear hidden layer are adopted to form a feedforward neural network. Models in the second category learn the word vector representations first and then train the language model with the word vectors. For example, Mikolov,

576

et al. [14] used one neural network with a single hidden layer to learn word vector representations and then trained an Ngram language model on the word vectors. Typically the simple structure of the neural networks used by the second approach makes it less computationally expensive, thus we confine the discussion to this type of system.
Recently, Mikolov et al. [13] proposed two new models, the Continuous Bag-of-Words model (CBOW) and the Continuous Skip-gram model, that have greater training efficiency. CBOW tries to maximize classification of a word by building a log-linear classifier with word vectors of several history words and future words around that location as input, while the Continuous Skip-gram model tries to predict words within a range before and after the current word given the word vector of the current word. Both CBOW and the Continuous Skip-gram model have only one projection layer between the input and output layer without any hidden layer, which significantly reduces the computational cost caused by the non-linear hidden layers in prior neural network based language models. Negative sampling is used to learn the CBOW and the Continuous Skip-gram models [15]. Word vector representations on a Google News corpus with 100 billion words for a vocabulary of 3 million words can be learned in less than one day using modest hardware.
Word vectors learned by both models have performed well in several semantic related task evaluations [13]. Mikolov et al. released the software for training CBOW and Continuous Skip-gram models and a set of pre-trained 300-dimensional word vector representations on the above mentioned Google News corpus2. Table 1 presents an example of the 10 closest words to `Chinese river' in terms of cosine similarity in the word vector representation space.

Table 1: Example of closest n-gram terms to the

phrase `Chinese river'.

Word

Cosine similarity

Yangtze_River

0.667376

Yangtze

0.644091

Qiantang_River

0.632979

Yangtze_tributary 0.623527

Xiangjiang_River 0.615482

Huangpu_River

0.604726

Hanjiang_River

0.598110

Yangtze_river

0.597621

Hongze_Lake

0.594108

Yangtse

0.593442

It can be seen that the neighbors given by word vectors are indeed semantically related to the input. Also, in this example, unlike Yangtze_River, the phrase Chinese_river does not belong to the vocabulary of the model; there is no word vector representation for Chinese_river. Instead, word vectors for both Chinese and river are fetched and averaged to represent Chinese_river in the search for the closest neighbors, which yields meaningful results. A recent analysis [8] justifies the above results by showing that the learning of distributed representations is essentially factorizing a word-context matrix which ensures that words sharing similar context (thus similar meaning) will have similar vector representations.
This word vector addition property is the key moti-
vation to represent query and terms as word vectors and
thus to derive term features from them to predict target term

2https://code.google.com/p/word2vec/

weights. See [15] for more examples of word vector addition property. In this paper, we adopt the CBOW framework to learn word vectors and use them to build a term weight prediction framework. The proposed framework can also be applied to word vectors learned by the Continuous Skipgram model, which we leave as future work.

3.2 Target term weights

As discussed in Section 1, proper query term weights re-

flects the relative importance of a term to a query and should

also help improve retrieval performance. We choose term re-

call weight as our target term weight to predict in our frame-

work for its simplicity to compute and also great potential

to improve retrieval performance [22]. Given relevance judg-

ments,

it

can

be

estimated

as

P(t|Rq )

=

|Rq,t | |Rq |

,

where

Rq

is the set of relevant documents to q and Rq,t  Rq is the

subset of relevant documents that contain term t.

3.3 Term weights and retrieval models
In this section, we present how true or estimated target term weights can be integrated into different retrieval models.

3.3.1 Probabilistic language model
One commonly used retrieval model in IR is the language model, often together with Dirichlet smoothing as shown in Eq. (3),

Score(q, D) =

log

tft,D

+

µ

cft |C|

(3)

tqD

|D| + µ

where tft is the term frequency of term t in document D,
cft is the collection frequency of t, |D| is the length of doc-
ument D, |C| is the total length of the collection and µ is
the smoothing parameter. True term recall weight or term recall weight estimates
can used by a language model in similar fashion as [9], as shwon in Eq. (4).

Score(q, D) =

 P(t|R)

·

log

tft,D

+

µ

cft |C|

 

(4)

tqD

|D| + µ

The term recall weighted language model shown above is equivalent to the relevance model under the assumption of binary term occurrences and uniform document length [22].

3.3.2 BM25
Another widely adopted retrieval model is BM25, as shown in Eq. (5)

Score(q, D) =

log
tqD

N - dft + 0.5 dft + 0.5

·

tft,D

tft,D · (k1 + 1)

+

k1(1

-

b

+

b

|D| avgdl

)

(5)

where dft is the document frequency of term t, avgdl is the average document length over the collection and k1 and b are free parameters. BM25 is actually more directly connected
to term recall weight, as the above original BM25 model is an extension of the BIM [16]. By inserting the recall weight estimates to BM25, we get the recall-enhanced BM25 as
shown in Eq. (6).

Score(q, D) =

log P(t|R) · N - dft + 0.5

tqD

1 - P(t|R) dft + 0.5

·

tft,D

tft,D · + k1(1

(k1 -b

+ +

1)

b

|D| avgdl

)

.

(6)

577

4. TERM WEIGHTS LEARNING WITH

DISTRIBUTED WORD VECTORS

In this section, we present our model for estimating term weights with distributed word vectors.

Table 2: Notations M number of queries N total number of terms in all M queries ni number of terms in the ith query qi the ith query tij the jth term of the ith query rij true term weight for tij p dimension of distributed word vector wij distributed word vector for tij xij feature vector for tij X feature matrix y regression labels vector  regularization parameter  feature weights vector

Suppose we have a set of M queries Q = {q1, q2, ..., qM }

and each query qi has ni terms for i = 1, 2, ..., M . Let

tij represent the jth term of query qi for j = 1, 2, ..., ni,

rij denote the true term weight estimated from relevance

judgments for tij, hence rij  [0, 1] and N =

M i=1

ni

denote

the total number of query terms. (Refer to Table 2 for a

summary of notations.)

Let wij  Rp denote the continuous distributed word vec-

tor representation for term tij, where p is the dimension of

the word vector. In this paper, we propose to directly con-

struct the feature vector xij for term tij from the distributed

word vector representations of term tij and other terms in

the same query qi as

xij = wij - wqi

(7)

where

wqi

=

1 ni

ni
wik
k

(8)

is the mean of all word vectors of terms in qi. Hence, the feature vectors have the same dimension as the word vectors. An naive example of feature vector construction with p = 2 is illustrated in Figure 1.
Intuitively, proper term weights measures the relative importance of a term w.r.t. the whole query (hence w.r.t. the other terms in the same query). In other words, a term with a higher term weight means that it's more important for the term to represent the meaning of the query. Meanwhile, the feature vector of a term defined above is the difference of the term to the center of distributed representations for all terms in the query, which also serves as the vector representation for the entire query (by applying the word vector addition properties). Hence, the feature vector measures the semantic difference of a term to the whole query. Believing that the above two measures are somewhat related, we propose to construct features above and learn a model to map from the feature vectors to term weight labels.
As a side note, we can see that the above constructed feature vectors are hence query dependent, which is also necessary in that term weight, from its definition, is also query dependent. As we will see in the experiments, the above construction is simple and yet effective.
Having defined the features, we now turn to formulate the model to map from feature to term weight labels. As the

6

5.5

5

4.5

4

3.5

3

2.5

2

1.5

1

0

1

2

3

4

5

6

7

8

9

10

Figure 1: Demonstration of transforming global 2dimensional word vectors into feature vectors local to the query. In the graph, blue solid circles represent terms in one query and green diamonds represent terms in another query. Note that the two queries share one common term located at (3, 4). The red solid circle represents the center for the circles, and the red diamond represents the center for the diamonds. The dashed and solid arrows are then the transformed feature vectors for all the terms.

adopted target term weight is a probability, we first map it from (0, 1) onto the real line R with a logit function to avoid numerical issues, as shown below.

yij

=

logit(rij )

=

log

1

rij - rij

(9)

We then formulate the "transformed" term weight y as a linear function of the term feature vectors defined above, that is y = x, and employ 1-norm regularization to learn the feature weights  (We also tried 2-norm regularization and 1-norm regularization works a little better). Formally, the optimization problem is

^

=

arg

min
Rp

1 2

M

ni
(yij - xij )2 +   1

i=1 j=1

(10)

where   0 is the regularization parameter controlling the balance between prediction error on training data and model complexity, and needs to be tuned by cross validation. In matrix form, the above optimization problem can be expressed as

^

=

arg

min
Rp

1 2

y - X

2 2

+





1

(11)

where y  RN is
y = (y11, y12, ..., y1n1 , y21, y22, ...y2n2 , ..., yM1, yM2, ..., yMnM ) (12)

is target term weight vector with term weights of all terms in the training queries stacked, and X  RN×p is
X = (x11, x12, ..., x1n1 , x21, x22, ...x2n2 , ..., xM1, xM2, ..., xMnM ) (13)

It is easy to verify that optimization problem (11) is equivalent to (10) and is of standard form of LASSO regression [19].

578

Specifically, the subgradient of the objective function w.r.t  in problem (11) is

f () 

=

X  (X 

- y)

+ s

(14)

where

si 

sign(i) [-1, 1]

if i = 0 if i = 0

(15)

Efficient gradient based methods can be applied to solve the above problem and after we get the optimum ^, when a new query term t with feature vector x arrives, we predict its term weight as

P(t|R) = sigmoid

^x

exp ^x

= 1 + exp

^x

(16)

We refer to the above proposed method as DeepTR for it using distributed representations of words to model term weights.
DeepTR is a very efficient method of predicting term weights, which enables it to be used in online services where latency (the time required to respond to a query) must be kept low. The word vectors and the regression model are trained offline. Predicting P(tij|R) for a new query involves loading the word vectors for the query terms, transforming them into feature vectors (averaging and subtraction), an inner product with the learned feature weights, and a sigmoid function. This is far more efficient than some prior methods that were effective but computationally complex (e.g., [22]).
After we gain estimates of the term weights, we construct the following query model variations in Indri query language (with the above apple pie recipe keyword query as an example):

· DeepTR-BOW:
#weight( P(apple|R) apple P(pie|R) pie P(recipe|R) recipe )
)

· DeepTR-SD:
#weight( 0.8 #weight( P(apple|R) apple
P(pie|R) pie P(recipe|R) recipe 0.1 #combine(#1(apple pie) #1(pie recipe) ) 0.1 #combine( #uw8(apple pie) #uw8(pie recipe) ) )
Note that in DeepTR-SD, we mainly focus on reweighting the unigrams, as unigram reweighting plays a much more significant role than bigrams and proximity expressions in improving retrieval accuracies, which is also confirmed by previous studies [22, 1, 2]. The proposed framework can be directly extended to bigrams, proximity expressions and other query concepts that can be represented by a standard inverted list containing positions, however the training data

are more sparse for more complex concepts, thus we leave that for future study.
The two query model variations proposed above can be used with language modeling and BM25 retrieval models; in the next section, we present experiments with both types of retrieval model.

5. EXPERIMENTAL METHODOLOGY
This section describes how we evaluated our work experimentally. We conduct experiments on 4 TREC test collections consisting of one Robust track dataset and three Web Track datasets. The document collections differ in size, from a small and traditional TREC Robust Track collection to large Web Track collections of web documents. The dataset sizes and the queries used with each dataset are shown in Table 3. The first three datasets are standard datasets used without change. The ClueWeb09B dataset is the standard ClueWeb09 Category B dataset after spam documents are removed. The Waterloo Fusion spam scores3 were used, and the filtering threshold was set to 70%.

Table 3: Collections and topics. Spam documents

were removed from the ClueWeb09 Category B

dataset (Waterloo Fusion spam scores, 70% thresh-

old).

Collection # Docs # Words TREC Topics

ROBUST04

528K

253M 351-450, 601-700

WT10g

1,692K

1,076M

451-550

GOV2

25,205K 24,007M

701-850

ClueWeb09B 29,038K 23,890M

1-200

We use the Indri search engine4 to index the corpus. The Krovetz stemmer was used on queries and documents. A standard list of English stop words is maintained for query processing. Additionally, for thee web track datasets, anchor texts from in-links is treated as part of the document and hence is indexed. We use descriptions of the TREC topics as queries stopped by a list of standard stop words and several stop phrases such as "find information".
We use DeepTR-BOW to denote the re-weighted keyword queries and DeepTR-SD to denote the re-weighted sequential dependency queries. All of the above query models can be expressed in the Indri query language. When constructing the sequential dependency model queries, we use weights 0.8, 0.1 and 0.1 for terms, bigrams and unordered windows, respectively as they have shown to be effective in prior research and practical applications [10, 11].
For retrieval, we use a language model with Dirichlet smoothing [21] and BM25 to test both types of weighted queries. Although sequential dependency model queries are not typically used with the BM25 retrieval model, they are not incompatible with BM25. It is straightforward to calculate the tf and df statistics that BM25 needs in order to calculate scores for bigrams (#1(apple pie)) and proximity expressions (#uw8(apple pie)) [2]. We show the results for these query concepts with BM25 to demonstrate the effectiveness of our method. A minor change is made to Indri to allow for the insertion of term weight estimates to the BM25 retrieval model, as shown in Eqn. (6). Retrieval parameters are all set to Indri default values, which is µ = 2500 for language model and k1 = 1.2, b = 0.75 for BM25.

3http://plg.uwaterloo.ca/~gvcormac/clueweb09spam 4http://lemurproject.org/indri/

579

For DeepTR, we use several sets of distributed word vector representations. The first one is the pre-trained 300dimensional vectors released by Google, which is trained on a Google News corpus of about 100 billion words.5 The second set of word vectors was trained on a subset of the ClueWeb09 Category B corpus.6 Spam documents were removed, as described above; the remaining documents were processed by Boilerpipe7 to remove the "unimportant" parts of each page. Mikolov's Continuous Bag-of-Words Model software [13, 15] was used to learn the word vectors with 100, 300, 500 dimensions, repectively. We also trained word vectors on the ROBUST04 and WT10g collections with 300 dimensions to enable comparison of corpus-specific word vectors trained from small datasets with corpus-independent word vectors trained on larger web datasets.
We train DeepTR on the set of queries with 5-fold cross validation and report the averaged performance. That is, we divide the queries to 5 folds and on each fold, we train the model using three of them, validates the performance on one fold of them as development set to pick the regularization parameter  from {0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100} and report the performance on the rest fold as testing data. After the model is trained on each fold, term weights for queries in the testing fold are predicted and retrieval performances for the reweighted queries are reported.
We use the trec_eval8 tool provided by TREC to assess the retrieval results. We focus mainly on the Mean Average Precision (MAP) and Precision at 10 (P@10) metrics. For web collections such as GOV2 and ClueWeb09B, where graded relavence judgments are available, we also report Normalized Discounted Cumulative Gain (NDCG) at 20 and Expected Reciprocal Rank (ERR)[4] at 20 values using the gdeval.pl9 tool provided by TREC. Statistical significance of model differences in terms of retrieval performance is judged by a two-sided paired randomization test with  < 0.05 rather than the Wilcoxon signed rank test that was used in prior research [1], which is prone to type I errors and considered not reliable for ad hoc IR [17, 18].
We compare the retrieval performance of DeepTR to three baseline query models. The first is the unstructured bag-ofwords query model (BOW). The second is the original sequential dependency model (SD) [10]. The third is the weighted sequential dependence model (WSD) [1], which is the state of the art model for query reweighting. For WSD, we implemented the model with coordinate ascent [11] to directly optimize MAP on the training queries for WSD. However, we got slightly worse results than reported in [1, 2], possibly due to that we didn't have the MSN query log feature used in the original paper. As we use an overlapping set of collections and set of queries, and our BOW and SD baseline results are very close to the ones reported in the WSD paper, we decide to show and compare with the performance numbers of WSD reported in the original paper [1, 2]. We also implemented the method proposed by Zhao et al. [22] to predict term recall, but the experimental results on our datasets were worse than our unweighted bag-of-words query baselines, thus we omit the comparison from this paper.
5https://code.google.com/p/word2vec/ 6http://lemurproject.org/clueweb09/ 7https://code.google.com/p/boilerpipe/ 8http://trec.nist.gov/trec_eval/trec_eval_latest. tar.gz 9http://trec.nist.gov/data/web/10/gdeval.pl

6. EXPERIMENTAL RESULTS
This section presents the results of experiments that compare the DeepTR method of setting term weights in two retrieval models (language models, BM25) to three baseline methods (unweighted queries, sequential dependency models, weighted sequential dependency models); the effects of word vectors of varying dimensionality; and the effects of word vectors trained from different types of data.
6.1 Retrieval results with Language Model
The first experiment compared query term weights provided by DeepTR in the language model retrieval model to three baseline methods (unweighted queries, sequential dependency models, weighted sequential dependency models). Experimental results are shown in Table 4 for both DeepTR-BOW and DeepTR-SD. In this experiment, we fix the dimension of word vectors to be 300 and compare the performances of the proposed models with different sources of word vectors.
DeepTR-BOW term weights perform better than the unweighted bag-of-words query model (BOW) over all collections, significantly outperforming BOW on ROBUST04 and GOV2 in terms of MAP and on ROBUST 04 in terms of P@10. DeepTR-BOW even achieves higher MAP than the sequential dependency model (SD) on WT10g, however the results are not statistically significant. DeepTR-BOW gains comparable MAPs to WSD in ROBUST04, WT10g and ClueWeb09B. This suggests that DeepTR-BOW which models no bigrams and proximity expressions is able to achieve comparable retrieval performances with the sequential dependency models (both unweighted and weighted).
The DeepTR-SD query model performs better than the standard sequential dependency model over all collections with all sources of word vectors. Particularly, significant differences are observed on ROBUST04, WT10g and GOV2 with all sets of word vectors; DeepTR-SD with word vectors trained on the Google News corpus achieves significantly higher MAPs over BOW and SD on all four collections. The significant gains of DeepTR-SD with respect to SD range from 4.2% to 12.1% in MAP. Similar significant improvements for P@10 are also observed. While we cannot draw statistical significance conclusions as we have no performance on individual query of WSD, on ROBUST04 and WT10g, DeepTR-SD attains better MAPs than WSD. This validates the effectiveness of using distributed representations of words as feature sources to predict term weights in improving both overall retrieval performance and top-rank results over the baseline models.
6.2 Retrieval results with BM25
We also show the retrieval results with BM25 in Table 5 for bothDeepTR-BOW and DeepTR-SD to demonstrate the boost in retrieval performance with different retrieval models. Prior research on weighted sequential dependency models [1] did not publish results for BM25, so we are unable to compare against that baseline in this experiment.
DeepTR-BOW term weights perform better than the unweighted bag-of-words query model (BOW) over all collections, significantly outperforming BOW for MAP over all data sets and all word vector variations except for ClueWeb09B with the Google word vectors; although not statistically significant, DeepTR-BOW also performs better than SD with all word vectors settings on ROBUST04, WT10g and GOV2.

580

Table 4: Retrieval performance of DeepTR-BOW using language model retrieval. In the first column, the paren-

thesis indicates the word vector source. Other parentheses show improvements over BOW and SD, respectively.

(For WSD, there were no P@10 values reported and no results on ClueWeb09B were reported in [2])

Query Model

ROBUST04 P@10 MAP

WT10g P@10 MAP

GOV2 P@10 MAP

ClueWeb09B P@10 MAP

BOW
SD WSD (Table 7 in [2])

0.4245 0.4414 -

0.2512 0.2643 0.2749

0.3290 0.3400 -

0.1943 0.2032 0.2260

0.5054 0.5342 -

0.2488 0.2688 0.2946

0.2667 0.2798 -

0.0702 0.0745 -

DeepTR-BOW (Corpus-specific 300)
DeepTR-BOW (GOV2 300)
DeepTR-BOW (ClueWeb09B 300)
DeepTR-BOW (Google 300)

0.4430b 0.4430b 0.4454b 0.4450b

0.2591b (+3.2/-1.9) 0.2650b (+5.5/+0.3) 0.2657b (+5.8/+0.5) 0.2673b (+6.4/+1.2)

0.3280 0.3330 0.3270 0.3380

0.2103 (+8.2/+3.5) 0.2111b (+8.7/+3.9)
0.2129 (+9.6/+4.8) 0.2122b (+9.3/+4.5)

0.5208 0.5208 0.5121 0.5221

0.2646b (+6.3/-1.6) 0.2646b (+6.3/-1.6) 0.2685b (+7.9/-0.1) 0.2630b (+5.7/-2.2)

0.2682 0.2667 0.2682 0.2667

0.0718 (+2.2/-3.6)
0.0741 (+5.6/-0.5)
0.0718 (+2.2/-3.6)
0.0732 (+4.2/-1.8)

DeepTR-SD (Corpus-specific 300)
DeepTR-SD (GOV2 300)
DeepTR-SD (ClueWeb09B 300)
DeepTR-SD (Google 300)

0.4558bs 0.4610bs 0.4659bs 0.4627bs

0.2754bs (+9.6/+4.2)

0.3510

0.2182bs (+12.3/+7.4)

0.5490b

0.2781bs

0.3700bs 0.2223bs

(+10.7/+5.2)

(+14.4/+9.4)

0.5490b

0.2810bs

0.3610bs 0.2279bs

0.5597bs

(+11.9/+6.3)

(+17.3/+12.1)

0.2842bs

0.3560

(+13.1/+7.5)

0.2256bs

0.5497b

(+16.1/+11.0)

b : Statistically significant difference with BOW

s : Statistically significant difference with SD

0.2831bs (+13.8/+5.3) 0.2831bs (+13.8/+5.3) 0.2890bs (+16.2/+7.5) 0.2814bs (+13.1/+4.7)

0.2879b 0.2854 0.2879b 0.2869

0.0748 (+6.5/+0.5) 0.0806bs (+14.8/+8.2)
0.0748 (+6.5/+0.5) 0.0780bs (+11.0/+4.7)

The DeepTR-SD query model performs better than unweighted queries and the standard sequential dependency model, delivering significantly higher MAPs over all collections and all sources of word vectors except for ClueWeb09B with the Google word vectors. The significant gains of DeepTR-SD over SD range from 1.9% to 7.3%. Similar trends over P@10 are also observed. DeepTR-SD with word vectors trained on ClueWeb09B achieves significantly higher P@10 than SD on ROBUST04, WT10g and GOV2.
6.3 Word vector dimensionality
Word vectors of different dimensionality provide different levels of granularity that may or may not be useful for setting term weights; they may also require different amounts of training data. Our third experiment investigates the effects of word vectors containing 100, 300, and 500 dimensions on term weights produced for language models. The ClueWeb09B corpus is used for these experiments due to its size, availability, and strong performance in the experiments above. Experimental results for DeepTR-BOW and DeepTR-SD are shown d together in Table 6.
Word vectors of 100 dimensions work best for unstructured BOW queries on ROBUST04, WT10g and ClueWeb09B, while the best MAP for GOV2 is achieved with word vectors of 300 dimensions. Similar trends are observed for the DeepTR-SD query model. Notably, DeepTR-SD with word vectors of 100 dimensions attains a significant 7.9% MAP improvement over SD on ROBUST04, and a significant 16.8% MAP improvement over SD on WT10g. On GOV2, word vectors of 300 dimensions help DeepTR-SD significantly improve over SD by 7.5%; last on ClueWeb09B, word vectors of 500 dimensions achieve the best result over all three dimensions, by a 4.9% over SD. We also observe similar results on P@10 when varying the dimensionality of word vectors. We also evaluated the effects of word vector dimensionality on term weights for BM25 retrieval and trends are similar to those for language model retrieval; the results are omitted due to space constraints.

Our results suggest that 100 dimensions is sufficient for estimating very effective term weights.
6.4 Corpus effects
We turn back to Tables 4 and 5 to compare retrieval results obtained with corpus-specific word vectors and word vectors trained from larger, more general, and external corpora (GOV2, ClueWeb09B, and Google News).
DeepTR-BOW performed about equally well with all three external corpora; the differences among them were too small and inconsistent to support any conclusion about which is best. However, although no external corpus was best for all datasets, the language model and BM25 retrieval models agreed on which source of word vectors was best for a particular corpus. This agreement suggests that the learned term weights are independent of a particular retrieval model, which was the purpose of training with term weight values.
The corpus-specific word vectors were never best in these experiments, even for GOV2 and ClueWeb09, which are large and provided `best' performance for other datasets. However, given the wide range of training data sizes ­ varying from 250 million words to 100 billion words ­ it is striking how little correlation there is between search accuracy and the amount of training data.
Similar trends are observed for the DeepTR-SD query models.
6.5 Robustness analysis
Figure 2 presents the detailed number of queries improved or hurt by proposed method using word vectors trained from various sources compared to the LM baseline. It's clear that for all methods using the predicted term weights, the numbers of improved queries are significantly larger than that of hurt queries. Moreover, when compared to sequential dependency model (SD), all the methods using the estimated weights help more and hurt fewer queries than sequential dependency model (SD), especially in the [-20%, 0) range as shown in the figure.

581

Table 5: Retrieval performance of DeepTR-BOW and DeepTR-SD using BM25 retrieval. In the first column, the

parenthesis indicates the word vector source. (No WSD results are reported as no BM25 experiments are

conducted in [2])

Query Model

ROBUST04 P@10 MAP

WT10g P@10 MAP

GOV2 P@10 MAP

ClueWeb09B P@10 MAP

BOW

0.4189 0.2460

0.3330 0.1783

0.4926 0.2334

0.2030 0.0566

SD

0.4394 0.2546

0.3350 0.1817

0.5215 0.2409

0.2030 0.0600

DeepTR-BOW (Corpus-specific 300)
DeepTR-BOW (GOV2 300)
DeepTR-BOW (ClueWeb09B 300)
DeepTR-BOW (Google 300)

0.4341b 0.4390b 0.4426b 0.4382b

0.2566b (+4.3/+0.8) 0.2561b (+4.1/+0.6) 0.2590b (+5.3/+1.8) 0.2600b (+5.7/+2.1)

0.3280 0.3340 0.3390 0.3450

0.1903b (+6.7/+4.7) 0.1910b (+7.1/+5.1) 0.1932b (+8.3/+6.3) 0.1922b (+7.8/+5.7)

0.5107 0.5107 0.5081 0.5181b

0.2430b (+4.1/+0.9) 0.2430b (+4.1/+0.9) 0.2462b (+5.5/+2.2) 0.2449b (+4.9/+1.7)

0.2086 0.2136b 0.2086 0.2091

0.0593b (+4.7/-1.2) 0.0596b (+5.3/-0.7) 0.0593b (+4.7/-1.2)
0.0584 (+3.1/-2.7)

DeepTR-SD (Corpus-specific 300)
DeepTR-SD (GOV2 300)
DeepTR-SD (ClueWeb09B 300)
DeepTR-SD (Google 300)

0.4406b 0.4450b 0.4486bs 0.4458b

0.2595bs

0.3470s 0.1944bs

0.5356bs

(+5.5/+1.9)

(+9.0/+7.0)

0.2609bs

0.3500s 0.1941s

0.5356bs

(+6.1/+2.5)

(+8.8/+6.8)

0.2630bs

0.3510s 0.1951bs

0.5349bs

(+6.9/+3.3)

(+9.4/+7.3)

0.2627bs

0.3510s 0.1938s

0.5322b

(+6.8/+3.2)

(+8.7/+6.7)

b : Statistically significant difference with BOW

s : Statistically significant difference with SD

0.2478bs (+6.1/+2.9) 0.2478bs (+6.1/+2.9) 0.2502bs (+7.2/+3.9) 0.2461b (+5.4/+2.2)

0.2066 0.2091 0.2066 0.2030

0.0613bs (+8.2/+2.1) 0.0616bs (+8.8/+2.6) 0.0613bs (+8.2/+2.1)
0.0604 (+6.8/+0.7)

Table 6: Retrieval performance of DeepTR-BOW and DeepTR-SD using language model retrieval. In the first

column, the parenthesis indicates the word vector source. (For WSD, there were no P@10 values reported and

no results on ClueWeb09B were reported in [2])

Query Model

ROBUST04 P@10 MAP

WT10g P@10 MAP

GOV2 P@10 MAP

ClueWeb09B P@10 MAP

BOW
SD WSD (Table 7 in [2])

0.4245 0.4414 -

0.2512 0.2643 0.2749

0.3290 0.3400 -

0.1943 0.2032 0.2260

0.5054 0.5342 -

0.2488 0.2688 0.2946

0.2667 0.2798 -

0.0702 0.0745 -

DeepTR-BOW (ClueWeb09B 100)
DeepTR-BOW (ClueWeb09B 300)
DeepTR-BOW (ClueWeb09B 500)

0.4410b 0.4454b 0.4398b

0.2681b (+6.7/+1.4) 0.2657b (+5.8/+0.5) 0.2679b (+6.6/+1.4)

0.3440 0.3270 0.3480

0.2239b (+15.3/+10.2)
0.2129 (+9.6/+4.8) 0.2179b (+12.1/+7.2)

0.5228 0.5121 0.5054

0.2667b (+7.2/-0.8) 0.2685b (+7.9/-0.1) 0.2655b (+6.7/-1.2)

0.2727 0.2682 0.2707

0.0727 (+3.6/-2.4)
0.0718 (+2.2/-3.6)
0.0726 (+3.4/-2.5)

DeepTR-SD (ClueWeb09B 100)
DeepTR-SD (ClueWeb09B 300)
DeepTR-SD (ClueWeb09B 500)

0.4711bs 0.4659bs 0.4606bs

0.2851bs

0.3700bs 0.2373bs

0.5557bs 0.2848bs

(+13.5/+7.9)

(+22.1/+16.8)

(+14.4/+5.9)

0.2810bs

0.3610bs 0.2279bs

0.5597bs 0.2890bs

(+11.9/+6.3)

(+17.3/+12.1)

(+16.2/+7.5)

0.2817bs

0.3660bs 0.2306bs

0.5550b 0.2860bs

(+12.2/+6.6)

(+18.7/+13.5)

(+14.9/+6.4)

b : Statistically significant difference with BOW

s : Statistically significant difference with SD

0.2874b 0.2879b 0.2894b

0.0778bs (+10.8/+4.5)
0.0748 (+6.5/+0.5) 0.0781bs (+11.3/+4.9)

6.6 Query length
We investigate how DeepTR-SD performs on queries with different lengths. We divide the queries into three groups: with no more than 3 terms (Len -3), 4 to 5 terms (Len 4-5) and 6 terms or more (Len 6+). Figure 3 shows the relative gains of SD and DeepTR-SD (using vectors pre-trained from Google News) over BOW. It's clear that DeepTR-SD works significantly better than SD on mid-range and long queries (Len 4-5 and Len 6+) on all collections. It's worth noting that SD performs worse than BOW for long queries on WT10g, while in contrast DeepTR-SD improves BOW by 14%. For short queries (Len -3), DeepTR-SD outperforms SD over 3 out of 4 collections, which also renders DeepTR-SD a better alternative for SD.
6.7 Graded relevance
Our final experiment investigates retrieval quality using the graded relevance judgments that are available for web

collections such as GOV2 and ClueWeb09B. Table 7 presents the experimental results using the NDCG@20 and ERR@20 metrics that have been widely used in web search scenarios. Similar to prior experiments, we see that DeepTR-SD produces more accurate rankings than BOW and SD on both GOV2 and ClueWeb09.
7. CONCLUSIONS AND FUTURE WORK
In this paper, we present a novel framework for learning term weights using distributed representations of words from the deep learning literature. We motivate the framework by adopting the word vectors to represent terms and further to represent the query due to the ability to represent things semantically of word vectors. Therefore we propose to construct features for terms directly from the word vectors and model the target term weight as a regression problem.
We conducted experiments with two retrieval models, the language model and BM25, over four text collections of vary-

582

Number of queries

90

ROBUST04

SD

DeepTR-SD (trec7)

80

DeepTR-SD (wt10g)

DeepTR-SD (gov2)

70

DeepTR-SD (clueweb100)

DeepTR-SD (clueweb300)

DeepTR-SD (clueweb500)

60

DeepTR-SD (google300)

50

40

30

20

10

0

<-1[-0100%0%, -8[0-%80)%, -6[0-%60) %, -4[0-%40)%, -20%) [-20%, 0)

0

(0,

20%[2) 0%,

40%[4) 0%,

60%[6) 0%,

80%) [80%,

100%]

>100%

Relative gain

60

GOV2

SD

DeepTR-SD (trec7)

DeepTR-SD (wt10g)

50

DeepTR-SD (gov2)

DeepTR-SD (clueweb100)

DeepTR-SD (clueweb300)

DeepTR-SD (clueweb500)

40

DeepTR-SD (google300)

30

20

10

0

<-1[-0100%0%, -8[0-%80)%, -6[0-%60) %, -4[0-%40)%, -20%) [-20%, 0)

0

(0,

20%[2) 0%,

40%[4) 0%,

60%[6) 0%,

80%) [80%,

100%]

>100%

Relative gain

Number of queries

Number of queries

35

WT10g

SD

DeepTR-SD (trec7)

30

DeepTR-SD (wt10g)

DeepTR-SD (gov2)

DeepTR-SD (clueweb100)

DeepTR-SD (clueweb300)

25

DeepTR-SD (clueweb500)

DeepTR-SD (google300)

20

15

10

5

0

<-1[-0100%0%, -8[0-%80)%, -6[0-%60) %, -4[0-%40)%, -20%) [-20%, 0)

0

(0,

20%[2) 0%,

40%[4) 0%,

60%[6) 0%,

80%) [80%,

100%]

>100%

Relative gain

45

ClueWeb09B

SD

DeepTR-SD (trec7)

40

DeepTR-SD (wt10g)

DeepTR-SD (gov2)

35

DeepTR-SD (clueweb100)

DeepTR-SD (clueweb300)

DeepTR-SD (clueweb500)

30

DeepTR-SD (google300)

25

20

15

10

5

0

<-1[-0100%0%, -8[0-%80)%, -6[0-%60) %, -4[0-%40)%, -20%) [-20%, 0)

0

(0,

20%[2) 0%,

40%[4) 0%,

60%[6) 0%,

80%) [80%,

100%]

>100%

Relative gain

Figure 2: Robustness analysis over all data sets in terms of MAP over LM (best viewed in color)

Number of queries

ing sizes to demonstrate the effectiveness of the proposed framework. Specifically, the proposed framework predicts term weight for query terms that can be used to weight unstructured bag-of-words queries and queries that are a slight variant of the sequential dependency model. We observed significant improvements at high precision and throughout the rankings over the unweighted bag-of-words queries and the original unweighted sequential dependency model queries.
The proposed method is time efficient. Once the word vectors are trained in an offline step (that is itself relatively efficient), online prediction of term weight for new query terms involves only a simple inner product with the learned feature weights.
There are several promising directions for further research. One can extend the method from creating vectors for bigrams and proximity terms by averaging the word vectors of their constituents to a direct modeling of bigrams and proximity terms; although one might expect sparse training data for these query terms to be a problem, our results with small corpora suggest that it may not be necessary to have massive amounts of information for each term. Word vec-

tors may also be useful for identifying terms that should be the focus of query expansion or terms that would be good expansion terms.
Acknowledgements
This work is supported by National Science Foundation under grant IIS-1018317. We thank the anonymous reviewers for their helpful comments.
8. REFERENCES
[1] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In Proceedings of the third ACM international conference on Web search and data mining, pages 31­40. ACM, 2010.
[2] M. Bendersky, D. Metzler, and W. B. Croft. Parameterized concept weighting in verbose queries. In SIGIR, pages 605­614, 2011.
[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137­1155, 2003.
[4] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In CIKM, pages 621­630, 2009.

583

Table 7: Retrieval performance of DeepTR-SD using graded relevance judgments. Parenthesis indicates the

word vector source. Query Model

Language Model Retrieval

GOV2

ClueWeb09B

NDCG@20 ERR@20 NDCG@20 ERR@20

BM25 Retrieval GOV2 NDCG@20 ERR@20

ClueWeb09B NDCG@20 ERR@20

BOW

0.3732

0.1493

0.1597

0.1253

0.3789

0.1487

0.1188

0.1075

SD

0.4059

0.1607

0.1694

0.1243

0.3940

0.1554

0.1294

0.1059

DeepTR-SD (GOV2 300) DeepTR-SD (ClueWeb09B 300) DeepTR-SD (Google 300)

0.4121b 0.4146b 0.4112b

0.1626b 0.1614b
0.1600

0.1743b 0.1663 0.1698

0.1312s 0.1255 0.1302

0.4008bs 0.4033bs 0.4010bs

b : Statistically significant difference with BOW

s : Statistically significant difference with SD

0.1590bs 0.1587bs 0.1586s

0.1307 0.1305 0.1298

0.1068 0.1067 0.1050

Relative improvement over LM

ROBUST04 0.25
SD DeepTR-SD
0.2

0.15

0.1

0.05

0 Len -3 (7%) 0.25
0.2

Len 4-5 (27%) Len 6+ (66%) GOV2
SD DeepTR-SD

0.15

0.1

0.05

0 Len -3 (11%) Len 4-5 (41%) Len 6+ (48%)

Relative improvement over LM

Relative improvement over LM

WT10g 0.25
SD DeepTR-SD 0.2

0.15

0.1

0.05

0

Len -3 (26%) 0.25
0.2

Len 4-5 (39%) Len 6+ (35%) ClueWeb09B
SD DeepTR-SD

0.15

0.1

0.05

0 Len -3 (26%) Len 4-5 (40%) Len 6+ (34%)

Relative improvement over LM

Figure 3: Performance gains of SD and DeepTR-SD over BOW w.r.t to query length. Y-axis shows the relative gain in MAP over BOW and parentheses in X-axis present the percentage of queries in each group. All retrievals are performed using LM. (best viewed in color)

[5] W. B. Croft and D. J. Harper. Using probabilistic models of document retrieval without relevance information. Journal of Documentation, 35:285­295, 1979.
[6] W. R. Greiff. A theory of term weighting based on exploratory data analysis. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 11­19. ACM Press, 1998.
[7] B. J. Jansen, D. L. Booth, and A. Spink. Determining the user intent of web search engine queries. In Proceedings of the 16th International Conference on World Wide Web, Banff, Alberta, Canada, May 8-12, 2007, pages 1149­1150, 2007.
[8] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2177­2185, 2014.
[9] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Inf. Process. Manage., 40(5):735­750, 2004.
[10] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proceedings of the 28th annual international ACM SIGIR conference on Research and

development in information retrieval, pages 472­479. ACM, 2005.
[11] D. Metzler and W. B. Croft. Linear feature-based models for information retrieval. Information Retrieval, 10(3):257­274, 2007.
[12] D. Metzler, V. Lavrenko, and W. B. Croft. Formal multiple-bernoulli models for language modeling. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540­541. ACM, 2004.
[13] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013.
[14] T. Mikolov, J. Kopecky´, L. Burget, O. Glembek, and J. Cernocky´. Neural network based language models for highly inflective languages. In ICASSP, pages 4725­4728, 2009.
[15] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111­3119, 2013.
[16] S. E. Robertson and K. S. Jones. Relevance weighting of search terms. Journal of the American Society for Information science, 27(3):129­146, 1976.
[17] M. Sanderson and J. Zobel. Information retrieval system evaluation: Effort, sensitivity, and reliability. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '05, pages 162­169, New York, NY, USA, 2005. ACM.
[18] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM '07, pages 623­632, New York, NY, USA, 2007. ACM.
[19] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267­288, 1994.
[20] J. Xu and W. B. Croft. Query expansion using local and global document analysis. In In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 4­11, 1996.
[21] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334­342. ACM, 2001.
[22] L. Zhao and J. Callan. Term necessity prediction. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management, pages 259­268. ACM, 2010.
[23] W. Zheng and H. Fang. Query aspect based term weighting regularization in information retrieval. In Advances in Information Retrieval, 32nd European Conference on IR Research, ECIR 2010, Milton Keynes, UK, March 28-31, 2010. Proceedings, pages 344­356, 2010.

584

A Probabilistic Model for Information Retrieval Based on Maximum Value Distribution
Jiaul H. Paik
University of Maryland, College Park, USA
jia.paik@gmail.com

ABSTRACT
The main goal of a retrieval model is to measure the degree of relevance of a document with respect to the given query. Probabilistic models are widely used to measure the likelihood of relevance of a document by combining within document term frequency and term specificity in a formal way. Recent research shows that tf normalization that factors in multiple aspects of term salience is an effective scheme. However, existing models do not fully utilize these tf normalization components in a principled way. Moreover, most state of the art models ignore the distribution of a term in the part of the collection that contains the term. In this article, we introduce a new probabilistic model of ranking that addresses the above issues. We argue that, since the relevance of a document increases with the frequency of the query term, this assumption can be used to measure the likelihood that the normalized frequency of a term in a particular document will be maximum with respect to its distribution in the elite set. Thus, the weight of a term in a document is proportional to the probability that the normalized frequency of that term is maximum under the hypothesis that the frequencies are generated randomly. To that end, we introduce a ranking function based on maximum value distribution that uses two aspects of tf normalization. The merit of the proposed model is demonstrated on a number of recent large web collections. Results show that the proposed model outperforms the state of the art models by significantly large margin.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Retrieval: Retrieval Models
General Terms
Algorithm; Experimentation; Performance
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767762.

Keywords
Document ranking; Retrieval model; Extreme value theory
1. INTRODUCTION
To measure the weight of a term in a document, most well known functions combine three major components the term frequency, the inverse document frequency and the document length. The term frequency factor is a key evidence for determining a term's salience in a document, while inverse document frequency is used for attenuating the effect of terms that occur too often in the collection to be meaningful for relevance determination. On the other hand, term frequency is closely related with document length, since long documents tend to use a term repeatedly. Thus, term frequency normalization, in accordance with the document length, is necessary to remove the advantage that the long documents have in retrieval over the short documents.
Given these three major components, the key question is then how these components can be integrated to produce a composite weight for each query term in each document and that is where one model differs from the other. Most well known weighting functions under the vector space model compute the composite weight by taking the product of the tf factor and the idf factor, where the tf factor is some combination of tf and the document length. Classical probabilistic models (for example BM25 [24]), adopt somewhat the same strategy. Although, they have the same objective, the two models have very different ways of determining the functional form of the tf factor. The nature of the tf functions under the vector space framework are generally constructed empirically, which are primarily guided by the experimental results, while BM25 formula is derived by approximating the logarithm of odds ratio of two Poisson distributions- one for relevant documents and the other for non-relevant documents. On the other hand, language models (LM) [21] differ from the above models in a fundamental way in the sense that the documents are ranked based on the likelihood that the query has been generated from the document in consideration. In addition, unlike tf.idf models, language models do not use explicit length normalization. The length of the document is an integral part of the probability estimation. Non-parametric probabilistic models are also known to be very effective in information retrieval. One of the widely used non-parametric probabilistic model is divergence from randomness (DFR) [1] based approaches, where the term weight is computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution. One major deficiency with

585

these models is that they consider only the document length normalized tf and ignore within document relative tf distributions. Recent research [20] shows that integration of within document relative tf into scoring model improves performance significantly. However, it is yet not clear how this variable can be added into the existing models formally.
This article describes a probabilistic retrieval model that obviates empirical way of determining a ranking function, unlike existing tf-idf models [29, 20]. The model introduces a tf factor based on the distribution of maximum values of normalized tf. The model achieves a number of important goals. First, it integrates the recent multi-aspect tf normalization schemes into a probabilistic framework. Second, the model automatically factors in the distribution of normalized tf in a term specific way, unlike many standard models. Third, it uses a mixture of two maximum value distribution to better model distributions of terms having varying heaviness of tails. To the best of our knowledge, this work is the first to address the ranking problem using the distribution of maximum values.
The effectiveness of the proposed model is evaluated on a number of recent web test collections containing millions of documents. We compare the performance of the proposed method to the state of the art representative baselines from tf-idf model, classical probabilistic model, language model and divergence from randomness model. Our primary experimental results show that the proposed model almost always outperforms the state of the art baselines by a significantly large margin. We carry out additional set of experiments to compare the performance of the proposed model against a log-logistic (LL) based model that uses multi-aspect tf normalization. Once again, the results suggest that the proposed model is often significantly better than LL model. Moreover, the results demonstrate that our model is more precise than the state of the art models, thereby making it a potential choice for web search.
We organize the article as follows. Section 2 reviews the state of the art. The proposed approach is described in Section 3. The experimental setup is detailed in Section 4. In Section 5, we present the experimental results. Finally, we conclude in Section 6.
2. PRIOR WORK
Modeling term weight is the central issue in an information retrieval system. Three widely used models in IR are the probabilistic models [23], the vector space model [28, 27], and the inference network based model [31]. Furthermore, probabilistic models can be broadly classified into three groups, namely the classical probabilistic model, language model and a non-parametric divergence from randomness model. A large number of instances of these models exist in the literature. In this section we mainly review the state of the art representatives from each of these categories.
2.1 Classical Probabilistic Model
The key part of the probabilistic models is to estimate the probability of relevance of the documents for a query. This is where most probabilistic models differ from one another. Since the introduction of full text search, a large number of weighting formulae have been developed that attempt to measure document relevance probabilistically and BM25 [22] seems to be the most effective weighting function from among them. BM25 model approximates the two Pois-

son model of relevance. The approximation is done using a increasing asymptotic tf function. Although, structurally, BM25 and tf-idf functions are very similar (in the sense that they both use tf and idf factor), they differ in many respects. First, BM25 has a well grounded theory, while most of the tf-idf models have an empirical background. Second, anatomically, IDF factor of BM25 discounts the collection size by the document frequency of the term, which is different from the standard IDF factor. Third, BM25 uses a different query term frequency function, unlike tf-idf models where that function is linear. The length normalization factor uses the average document length and a parameter has been introduced to control the relative length effect.

2.2 Language Model
Probabilistic language modeling approaches [21, 15] follow a different principle in estimating the relevance of a document, unlike classical probabilistic models. Typically, language modeling approaches compute the probability of generating a query from a document, assuming that the query terms are chosen independently. Unlike TF-IDF models, language modeling approaches do not explicitly use document length factor and the idf component. It seems that the length of the document is an integral part of this formula and that automatically takes care of the length normalization issue. However, smoothing is crucial and it has very similar effect as the parameter that controls the length normalization factor and term specificity in pivoted normalization or BM25 model. Three major smoothing techniques (Dirichlet, Jelinek-Mercer and Two-stage) are commonly used in this model [32].
Although, query likelihood model is reasonably effective, one major deficiency with using a multinomial distribution as a language model is that all term occurrences are treated independently. The term-independence assumption in information retrieval is often adopted in theory and practice, as it renders the retrieval problem tractable. It is well known that once a term occurs in a document, it is more likely to reappear in the same document. This phenomenon is known as word burstiness [18] and is a type of dependency that is not modelled in the multinomial language model. Cummins et al. [8] present a Smoothed Polya Urn Document language model, which incorporates word burstiness only into the document model. They use the Dirichlet compound multinomial (DCM) to model documents in place of the standard multinomial distribution, whereas the standard multinomial is used to model query generation.

2.3 Divergence from Randomness Model
Amati and Rijsbergen [1] proposed a class of non-parametric probabilistic approaches to term weighting called divergence from randomness (DFR) model. The weight of a term in DFR models is the amount of divergence between a term distribution produced by a random process and the actual term distribution. The anatomy of the weighting function of DFR is defined as follows

w(t, d) = -log2(P rob1) · (1 - P rob2).

(1)

The left factor measures the information content of the term in a document based on its distribution in the entire collection, while the right factor measures the information gain of the term with respect to its occurrence in the elite set (set of documents that contains the term). P rob1 is com-

586

puted using various well known distributions (such as BoseEinstein statistics, Poisson distributions etc), while P rob2 is measured using Laplace law of succession or the ratio of two Binomial distributions. Like other models, DFR models use the same basic components. However, the integration of various component are derived theoretically. DFR models use explicit length normalization and following standard practice, average document length is considered as the ideal document length.
2.4 Vector Space Models
In vector space model, the search problem is viewed in a different way. Queries and documents are represented as the vectors of terms. To compute a score between a document and a query, the model measures the similarity between the query and document vector using cosine function. The central part of the vector space model is to determine the weight of the terms that are present in the query and the documents. Salton and Buckley [26] summarize a number of term weighting approaches which use various types of normalization. It is evident that document length is an important component in effective term weighting. Singhal et al. [29] identify a number of weaknesses of cosine and maximum tf normalization and they observe that a weighting formula that retrieves documents with chances similar to their probability of relevance performs better. Following this observation, they propose a pivoted normalization scheme that acts as a correction factor of old normalization and is one of the most effective term weighting schemes in the vector space framework. Typically, the term weighting functions in vector space model are constructed empirically. Several work tried to go beyond purely empirical approaches and use the data instead to learn the patterns that satisfy the data. For example, Greiff [12] uses exploratory data analysis to uncover some important relationship between the document frequency and the relevance of a document.
Most of the earlier work on vector space model normalizes the term frequency in accordance with the length of the documents. Paik [20] argued that the length based normalization alone is not sufficient to capture the different aspects of term salience and that within document distribution of the terms plays an important role. He then proposed a two-aspect normalization scheme. An asymptotic bounded increasing function (much in spirit with BM tf function) is then used to transform the normalized tf values. Two tf components are then combined using query length information. However, the main weakness of the model is its highly empirical nature and that is where the model proposed in this article differs from [20]. The proposed model has a formal probabilistic foundation that directly produces the weighting function.
2.5 Other Models
In inference network, document retrieval is modeled as an inference process [31]. A document instantiates a term with a certain strength and given a query, the credit from multiple terms is accumulated to compute a relevance, which is very much equivalent to the similarity score of vector space model. From an operational angle, the strength of instantiation of a term for a document can be considered as weight of the term in a document. The strength of instantiation of a term can be computed using any reasonable formula.

Some models go beyond the use of bag of words features only and incorporates the proximity/phrases of query terms in the documents [6, 9]. Metzler and Croft [19] develop a general formal framework for modeling term dependencies via Markov Random Fields. The model allows arbitrary text features, such as occurrence of single term, ordered phrases and unordered phrases to be incorporated as the potential evidences of relevance. They explore full independence (bag of words) , full dependence (between every pair of query terms) and sequential dependence (between consecutive query terms) in the language modeling framework. Since, the model has to compute the positional information during query processing time, it is more computationally complex than our model.
Fang et al. [10] give a comprehensive analysis of four retrieval models by defining a set of constraints that needs to be satisfied for effective retrieval. Using these constraints the strengths and weaknesses of some well known models are analyzed and some of the models are modified. There are also a number of recent works that focus on the constraint based analysis of the retrieval models [4, 7].

3. PROPOSED WORK
In this section we describe the proposed ranking model. We first revisit the key variables used in a typical ranking model and describe the roles they play. We then describe how maximum value can be used for ranking. Finally, we turn on to present the maximum value based models and their parameter estimation.

3.1 TF-IDF Model: A Probabilistic View
Within document Term frequency and inverse document frequency (idf) are the two main building blocks of information retrieval models that measure query-document similarity. These two variables play a complementary role in ranking documents in response to a query. The idf factor of a term t (idf (t)) measures the information gain of randomly picking a document that will fall in the elite set for t (the set of document that contains t and henceforth we denote it as E(t)). On the other hand, tf factor of t for a document d, (tf f (t, d)) measures the relative weights of documents within E(t). Thus, from an operational perspective, idf (t) balances the weight between different E(t), while tf f (t, d) adjusts the relative weights of documents within the same elite set. Term frequency hypothesis suggests that tf f (t, d) is an increasing function of normalized term frequency. Intuitively, this means, if the rank of a document d having ntf (t, d) (normalized tf of t in d) is relatively high in E(t), the contribution made by tf f (t, d) is also high. Hence, given the distribution of normalized tf of a term in E(t), a natural way to measure tf f (t, d) is to take the percentage of documents in E(t) having normalized tf not higher than ntf (t, d). Thus, tf f (t, d) can be defined as follows:

tf f (t, d)  P (X  ntf (t, d))

(2)

where X is the random variable on normalized tf values in E(t).
Lv and Zhai [17] argued that straightforward non-parametric (plain percentile based) way of estimating this probability does not fully factor in the main objective of tf hypothesis, since it ignores the quantum of differences of normalized tf values. Thus, they advocate the use of parametric probability distribution functions to circumvent this limitation.

587

They use log-logistic distribution for computing tf f (t, d) as follows

tf f (t, d) = P (X  ntf (t, d)|c, ) = F (ntf (t, d)|c, ) ntf (t, d)
= c + ntf (t, d) (3)

where c > 0 and  > 0 are the model parameters which can be estimated from the normalized tf values in E(t). The main issue in this approach is to choose the right distribution function that captures the distribution of normalized values properly. We use maximum value distribution of two aspect normalized tf values in the above framework to measure the tf f (t, d). In the next two sections, we describe multi-aspect tf normalization scheme followed by the maximum value based model.

3.2 Term Frequency Normalization

Raw term frequencies are known to be less effective because of its correlation with the document length. Thus, a long document enjoys preference over a short document if the term frequency is used as is. A document becomes longer if it contains many unrelated contents together. Therefore, although the frequency of a term may not increase in this case, the document uses many distinct terms. Since, the chance of a random match of a term between a query and a document is approximately proportional to the number of distinct terms in the document, long documents get an additional advantage over shorter documents. On the other hand, documents also become longer if they repeat the same content, thereby resulting in higher term frequencies without giving any additional useful information.
Therefore, to enhance retrieval accuracy, it is imperative to regularize the term frequency in accordance with the document length. A standard and successful approach for doing this is to compare the length of the concerned document to the length of an ideal document (pivotal document). Both, pivoted tf-idf and BM25 effectively use this strategy where the length of the pivotal document is the average document length of the retrieval collection. Thus, the tf of an average length document remain unchanged, while tf of the documents longer (shorter) than average length document are punished (rewarded).
Recently, Paik [20] argued that the traditional length based normalization alone is not sufficient to capture the different aspects of term importance and proposed two normalization formulae- one is based on within document average term frequency, while the other makes use of the traditional length based approach. These two normalized tf s are then combined. We use the same normalization schemes as described in [20], since it gives state of the art results. For convenience, the normalization factors are called ritf (t, d) (relative intra-document frequency of term t in the document d) and lrtf (t, d) (length normalized frequency of term t in the document d). The following equations formally define the normalization schemes.

ritf (t, d) = log(1 + tf (t, d))

(4)

log(k + mtf (d))

adl

lrtf (t, d) = tf (t, d) log(1 + )

(5)

l(d)

The terms mtf , adl and l(d) denote the mean term frequency of the document that contains t, the average docu-

ment length of the collection and the length of the document d, and k ( 1) is a smoothing parameter. The proposed model combines these frequency normalizations in a probabilistic framework.

3.3 Limitations of Existing Models

In the last section we have described multi-aspect tf nor-

malization scheme. In this section we discuss the potential

limitations of existing methods and the major difficulties in

integrating multi-aspect tf normalization into the state of

the art probabilistic models.

We start our discussion with the MATF model. We reiter-

ate that, although, idf function does not vary much from one

model to the other, it is the tf function that often makes

the main difference.

In

[20],

the

function

x 1+x

is

used

to

transform the normalized tf values to enforce term cover-

age. However, the function has a number of notable short-

comings. First, the choice of the function is purely empirical

in nature. Second, the function does not have the knowl-

edge of the distribution of tf in the elite set. Third, since

the function operates on the tf values having incompatible

range (range of ritf is much smaller than that of lrtf ), one

component overpowers the other component, thereby com-

promising the ultimate effectiveness.

BM25 model is a nice bridge between tf.idf and probabilis-

tic model. Anatomically, BM25 is clearly separable into tf

and idf component, where the tf function is a special case of

log-logistic model and is guided by 2-Poisson model. BM25

normalizes tf in accordance with the document length where

average document length is used as an ideal (or pivotal) doc-

ument. However, it is not clear how to integrate relative

intra-document tf into this model, since the notion of pivot

for relative intra-document tf is hard to define. Moreover,

BM25's tf function is also distribution independent.

Unlike the previous two models, divergence from random-

ness model (DFR) takes a more principled approach in terms

of factoring in the term distribution. Once again, it is yet

unknown how relative intra document tf (ritf (t)) can be

added to this model that will be theoretically consistent with

DFR's basic principle. Moreover, normalized tf values are

continuous valued random variable and thus, an attempt to

integrate it into DRF will give rise to theoretical anomaly,

since DFR uses discrete distributions to measure informa-

tion gain.

Language model is very different from all the models dis-

cussed above primarily because it neither uses idf explicitly

nor it uses length normalization. Thus we confine our dis-

cussion on the models that have explicit tf and idf factors.

In the next section we describe the maximum value based

model and how it can be used to circumvent some of the

problems outlined above, followed by the development of a

model that uses two aspect tf normalization in a probabilis-

tic framework.

3.4 Maximum Value Model
Unlike existing ranking models, we attempt to measure tf f (t, d) based on the nature of some of the largest values of normalized tf for that term. A natural consequence of using maximum value based ranking is that it makes the weight of a term in a document dependent upon the distribution of normalized tfs in E(t).
To that direction, the simplest possible approach could be to take the maximum value of normalized tf for a term t

588

and then measure tf f (t, d) relative to the maximum value. Clearly, this scoring is perfectly consistent with tf hypothesis, where the document having highest normalized tf gets highest weight. We can easily think of two naive approaches to measure tf f (t, d) that are based on maximum values. One potentially feasible approach can be percentile based scoring that we have outlined before, while the other simple approach can be to measure tf f (t, d) as a ratio of ntf (t, d) (or some increasing function of ratio) and the maximum normalized tf for that term in the collection. To understand the limitations of these two approaches, let us consider the following examples.
Let x1, x2, . . . , xn-1, xn be the normalized tf values for a term t in ascending order. As our first case, let us assume that (xn - xn-1)  0. The percentile based method may give higher weight for xn compared to xn-1 even if they are nearly the same. This happens because percentile based method does not factor in the magnitude of difference, which consequently violates the tf hypothesis. As a second case, if it happens that xn xn-1, scoring based on ratio gives too much priority on the maximum value alone, which results in sharp discount of scores of other documents. As a consequence, a document even if genuinely relevant, is undesirably punished.
These problems are addressed using a sampling based technique which exclusively focuses on maximum values of samples. Rather than relying on a single value, we attempt to measure the distribution of values at the right tail where some of the largest values fall. Hence, our main goal is to model the nature of the right tail of ntf (t, d). We hypothesize that the most potentially relevant documents for a term fall on that part of the distribution. Quite clearly, this hypothesis is consistent with the standard tf hypothesis. Thus, the main challenge is to model the nature of the right most tail as accurately as possible. In other words, this model measures the likelihood that ntf (t, d) will fall on the right most tail. Thus, if the probability is higher, likelihood of d being relevant will also be higher.
We now focus on the models for maximum values. We reiterate that in order to avoid the influence of a single quantity (maximum value), the following sampling based approach is taken to derive maximum value distributions. Let us assume that N samples, each of size n are drawn from the same population. From each sample we can get the largest value. Thus in nN observations we have N largest values corresponding to each random sample. The distribution of the largest values in nN observations will tend to follow the same asymptotic expression as the distribution of the largest value in samples of size n. Consequently, the asymptote must be such that the largest value of a sample of size n taken from it must have the same asymptotic distribution. Formally, the maximum value distribution is defined as follows. Let X1, X2, . . . , Xn be independent and identically distributed random variable with distribution F . Let Mn = max(X1, X2, . . . Xn). Then,

P r(Mn  x) = P r(X1  x, X2  x . . . Xn  x) (6)

= F n(x)

(7)

Since a linear transformation does not change the form of the distribution, the probability that the largest value is less than x should be equal to the probability of a linear function

prob

0.6 mitchell travel
0.5

0.4

0.3

0.2

0.1

0

1

2

3

4

5

6

7

8

9

10

normalized tf

Figure 1: Distributions of random samples of normalized elite set term frequency of mitchell and travel.

of x. Thus, the above equation is equivalent to

P r( Mn - bn an

 x) = F n(anx + bn).

(8)

Fisher-Tippett-Gnedenko theorem [11] states that if a pairs of real numbers (an, bn) (an and bn must be functions of n) exist such that an > 0 and

lim F n(anx + bn)  D(x)

(9)

n

for a distribution F , then D(x) can be Type I or Type II distribution defined below.
The type I distribution [13] (known as Gumbel distribution) is defined as

x-µ

Fg(x) = exp(- exp(-  )), µ  R;  > 0.

(10)

while type II distribution [13] (Frechet distribution) for pos-

itive random variable is defined as

µ

Ff (x) = exp(- x ), x  0; µ > 0;  > 0.

(11)

Having defined the maximum value distribution, our next major goal is to verify that the maximum value distributions satisfy the mandatory preconditions in order to be applicable in our task. Specifically, the data must be coming from a distribution F that satisfies Fisher-Tippett-Gnedenko theorem. Thus, our primary goal is to fix the underlying distribution function from which the data have been supposedly generated. In order to guess F , we first examine the distributions of normalized frequencies for a few randomly chosen terms. We noticed that the density graphs near the extreme right tail are not monotonically decreasing and it happens primarily because of the presence of random noise or extreme outliers. We empirically (by plotting) identify the points at which the density graphs violate this smoothness for the first time and ignore all the data larger than this particular point. On Clueweb collections, our analysis suggests that normalized tf values between 70-80 seem to be

589

a reasonable cut-off point and thus, in our experiments we set it to 75 empirically (but that value may depend on the nature of the collection). We then plot the distributions of the truncated data. As an example, Figure 1 shows distributions for two selected terms. To better understand the relationship between the pattern of distributions and term's collection level occurrence, we choose two terms ("mitchell" and "travel") of varying specificity. Figure 1 clearly shows that both the terms seem to be following long tail distributions with monotonically decreasing density functions. We consider two such long tail distributions ­ namely, exponential distribution and Pareto distribution. Note that the nature of the tails are different in these two cases.

Case 1.
Suppose the data have been distributed from exponential distribution. Thus, F (x) = 1 - exp(-x/),  > 0. If we choose an = 1 and bn = ln n, Then

F n(anx + bn) =

-x - ln n n

1 - exp(-

)

(12)



exp(-x/) n

= lim 1 -

(13)

n

n

= exp(- exp(-x/)).

(14)

Thus, if the data is generated from exponential distribution, for an = 1 and bn = ln n, maximum value distribution converges to Gumbel distribution.

Case 2.
Suppose now the data have Pareto tail. Thus, 1 - F (x) = cx- as x  , with c > 0 and  > 0. Again if we set
1
an = n  and bn = 0, then for x > 0 we have

F n(anx) = 1 - c(anx)- n

(15)

x- n

= lim 1 - c

n

n

(16)

= exp(-( µ )) (setting c = µ) (17) x

which turned out to be Frechet distribution. Hence, the above results provide us the necessary evidence that the maximum value distributions can be applied on our data.

Mixture Model.
Although, Fg and Ff are the asymptotic approximations to maximum value models, the shapes of their distributions are not identical. Frechet distribution has longer right tail (Pareto tail) than Gumbel. This has some interesting corelation with the distribution of term frequencies in a large collection. If a term is more general (but not really stopwords), the frequency distribution for that term likely to have a longer tail than that of more specific term. Figure 1 illustrates this point clearly: the density curve of "mitchell " (which is a rare term) touches the x-axis much before that of "travel " (which is a more general term). Thus, an attempt to model the distribution of a term using only one of Gumbel and Frechet may lead to lower accuracy. Any real query contains terms having varying collection frequency and this motivates us to use a weighted mixture of the two distributions. Thus, our resulting distribution is defined as
G(x) = p · Fg(x) + (1 - p) · Ff (x), 0 < p < 1 (18)

where p can be considered as prior of Fg(x). A straightforward way to estimate p is to use a standard method such as gradient ascent method that directly optimizes a target retrieval metric (such as NDCG@20). Indeed, we adopt such an approach, but not directly on p. As we have discussed earlier, Fg (Gumbel) distribution seems better in modeling the distribution of a term having relatively smaller df values (more specific). Thus, instead of optimizing the value of p independently, we make the value of p dependent on df . Specifically, if a term has low df (high idf ) we give higher weight to Fg(x). In other words, p should be higher for high idf terms. We formalize this intuition using the following well known linear model

p =  · idf

(19)

1-p

which gives the following solution for p

p =  · idf .

(20)

1 +  · idf

where  (> 0) is a free parameter.

3.5 Scoring Function
We are now ready to define our final scoring function. Our scoring function uses two aspect tf normalization in maximum value distribution framework. Formally, if X and Y be the random variables corresponding to ritf (t) and lrtf (t) in E(t) respectively, then tf f (t, d) is defined as

tf f (t, d) = ·P (X  ritf (t, d))+(1-)·P (Y  lrtf (t, d)) =  · G(ritf (t, d)) + (1 - ) · G(lrtf (t, d)) (21)

where 0 <  < 1, is the interpolation parameter. Consequently, the final scoring function for a query Q = q1q2 . . . qn and a document d is defined as

S(Q, d) = tf f (t, q) · idf (q)

(22)

qQ

where idf (t) = log(N/df (t)). The parameter   (0, 1) in Equation 21 is set empirically.

3.6 Model Parameter Estimation
In this section we detail our method for estimating the parameters of the two maximum value distribution models described in the last section. These parameters play important role in determining the actual shape of the distributions which in turn make them term dependent. There are many methods for parameter estimation including maximum likelihood estimation (MLE), which perhaps is an obvious choice. However, in our case, MLE does not seem to be a good choice for the reason we detail next.
We explain the difficulty with Gumbel distribution only (similar argument holds for Frechet). The log-likelihood function of Gumbel based on random sample x1, x2, . . . , xn is given by

L(, µ) = - n xi - µ - n ln  - n exp(- xi - µ ). (23)





i=1

i=1

The system of differential equations (used for MLE)

L = L = 0

(24)

µ 

590

yields the following estimates for µ and 

µ = (ln n - ln n exp(- xi ))

(25)



i=1

and

n

xi

exp(-

xi 

)

x¯

=



+

i=1 n

.

exp(-

xi 

)

i=1

(26)

Clearly, Equation 26 shows that  does not have closed form expression. Thus, we need to apply iterative numerical methods to find value of . Iterative methods may take substantial amount of time for very large collection such as Clueweb, since it needs to iterate over the set of maximum values from each random sample for each distinct term in the collection. This is precisely the reason we use point estimates (with a somewhat empirical transformation) of central tendencies for these models.

3.6.1 Parameter Estimation for Gumbel
The mean of Gumbel distribution is

µ + 0.57 · 

(27)

while the standard deviation is

 .

(28)

6

To estimate the values of  and µ we equate them with

corresponding sample mean and standard deviation, which

finally gives the following estimates.





6

6

 = s and µ = x¯ - 0.58 · s

(29)





where x¯ and s are sample mean and standard deviation respectively. Since our data is positive random variable and originates from exponential distribution we use Equation 14 for final ranking. Thus, we do not need to worry about the parameter µ. Our only concern is the parameter . Surprisingly, point estimate of  as is does not perform well in practice. Thus, in practice, we use a linear transformation,  = z1 + z2 · s, where z1 and z2 are set empirically to 2.5 and 0.04 respectively.

3.6.2 Parameter Estimation for Frechet
Mean and variance for Frechet are defined respectively as

µ(1 - 1/),  > 1

(30)

and

µ2((1 - 2/) - 2(1 - 1/)),  > 2.

(31)

Once again, the above two expressions are not very convenient to use since the improper integral (.) needs to be evaluated in order to compute the parameter. Fortunately, median and mode of Frechet distribution have much manageable expressions. Median is defined as

µ0.69-1/

(32)

and mode is defined as

µ(1 + 1 )-1/.

(33)



As in Gumbel, we can equate these two expressions to sample median and mode to estimate the model parameters.

However, unlike Gumbel, the parameters do not have closed form solution, which can be achieved by using any standard numerical method. Note that, in this case we do not need to iterate over the sample of maximum values, instead mode and median computed once for a term is enough. It is also important to note that although median for a sample is easy to determine, we need to do a little processing to compute mode from a set of real numbers. To compute mode of a sample, we create non-overlapping bins of numbers having 0.5 as the interval. We then take the median of the bin having highest frequency as our sample mode. We have adopted computationally efficient parameter estimation methods. However, a large number of other methods exist in the literature. Thus, it may be interesting to see whether other estimation strategies can improve the retrieval results without sacrificing efficiency too much.
4. EXPERIMENT SETUP
In this section, we describe the experiment setup used to evaluate the proposed model. Our experiments have the following two major objectives.
1. To compare the performance of the model against the state of the art probabilistic models (Section 5.1).
2. To compare against a recently proposed multi-aspect tf-idf weighting scheme [20] (Section 5.2).

Table 1: Summary of the test collections and topics

used in our experiments. `M' stands for million.

Collection

# doc topics

# topics

Clueweb.B-09 & 10 Clueweb.B-11 & 12 MQ-2009 Clueweb.A-09 & 10 Clueweb.A-11 & 12

50M 50M 50M 500M 500M

1-100

100

101-200

100

20001-30000 684

1-100

100

101-200

100

We summarize the test collections used in our experiments in Table 4. The test collections are taken from TREC web tasks of recent years (2009-2012) as well as from million query 2009 (MQ-2009). The collections contain web documents and real web queries sampled from a search engine log. The documents are crawled from web and hence they have variety of content quality. Clueweb.B collection contains nearly 50 million documents, while ClueWeb.A collection contains approximately 500 million web pages. In MQ-2009 collection, although many queries available, not all queries have been judged. Thus, we use 684 queries for which judgments are available. All the collections have graded relevance assessment. It is important to note that, MQ-2009 queries have incomplete relevance assessment. Therefore, our evaluation methodology skips the unjudged documents from the ranked lists in order to compute the values of well known metrics following the recommendation made in [25].
Documents and queries are stemmed via Porter stemmer. Stopwords are removed from documents and queries. Statistically significant performance differences are determined using a paired t-test at 95% confidence level (p < 0.05). All our experiments are done using title field of the topics.

591

Table 2: Retrieval effectiveness of the proposed method (MVD) compared to probabilistic models. Statisti-

cally significant improvements are indicated using the first letter of the less effective method. The highest

value per column is boldfaced. The numbers in parenthesis indicate relative improvement over LM, PL2 and

BM25, respectively.

Clueweb.B-09 & 10 Clueweb.B-11 & 12 MQ-2009

Clueweb.A-09 & 10 Clueweb.A-11 & 12

LM 0.309

0.264

0.367

0.254

0.219

PL2 0.312

0.263

0.373

0.256

0.219

ERR@20

BM25 0.306 MVD 0.337lpb

0.253 0.286lpb

0.372 0.408lpb

0.248 0.286lpb

0.221 0.257lpb

(8.9, 7.9, 9.9)

(8.2, 8.8, 12.9)

(11.2, 9.5, 9.8) (12.7, 11.4, 15.0) (17.8, 17.5, 16.4)

NDCG@10

LM PL2 BM25 MVD

0.282 0.285 0.284 0.332lpb (17.9, 16.5, 16.8)

0.228 0.231 0.222 0.268lpb (17.3, 16.1, 20.4)

0.395 0.393 0.391 0.422lpb (7.0, 7.4, 7.9)

0.200 0.205 0.208 0.261lpb (30.7, 27.0, 25.3)

0.194 0.196 0.191 0.231lpb (19.0, 17.8, 21.3)

NDCG@20

LM PL2 BM25 MVD

0.275 0.278 0.280 0.325lpb (18.4, 17.0, 16.4)

0.228 0.228 0.225 0.265lpb (15.9, 16.2, 17.8)

0.459 0.458 0.453 0.479b (4.5, 4.5, 5.8)

0.193 0.195 0.208 0.248lpb (28.7, 27.4, 19.0)

0.196 0.198 0.186 0.228lpb (16.4, 15.2, 22.7)

4.1 Baselines
The performance of the proposed model is compared to a number of state of the art retrieval models from different families. BM25 [24] is chosen as the representative baseline from the classical probabilistic model. From language model, we choose Dirichlet smooth version [32], since it is known to be the most effective among the language models [10]. From divergence from randomness family, we choose PL2 [1] as the baseline, following recent work [10, 14].
Pivoted document length normalization is chosen as a basic TF-IDF baseline. MATF [20] is chosen as another state of the art tf-idf model. Note that, MATF is a highly effective empirical tf-idf model and one of the major objectives of the proposed model is to advance the multi-aspect TF model using a probabilistic foundation. Finally, since our model attempts to capture the distribution of normalized tf, we also compare to multi-aspect TF normalization with a log-logistic distribution which has similar purpose. Thus, our set of baselines contains members from all state of the art families.
4.2 Free parameters and evaluation metrics
All the baseline models (except MATF) and the proposed model contain one or more free parameters. It is important to note that the parameters of these models often influence the performance to a statistically significant degree. Hence, for the sake of reliable and competitive comparison, the parameters are optimized using 5-fold cross validation with the corresponding evaluation measure (ERR@20 [2], NDCG@10 [16] or NDCG@20) as the target metric.
We choose expected reciprocal rank (ERR), NDCG@10, and NDCG@20 as our evaluation measures. ERR has been the primary evaluation metric for recent TREC web tracks [3]. NDCG@k leverages graded relevance and also has a position wise discounting. Thus, it reflects the overall quality of the documents at top k. On the other hand, ERR@k is a precision bias metric that leverages graded relevance assessment. Thus, ERR is more suitable metric for web search.

Clueweb collections contain substantial number of spam documents. Thus, following previous work [5], we have filtered out spam documents from the collections. Specifically, documents assigned by Waterloo's spam classifier [5] with a score below 70 were filtered out from the initial corpus. The score indicates the percentage of all documents in ClueWeb that are presumably "spammier" than the document at hand. The models are then run on the residual corpus to produce final ranked lists.
5. RESULTS
In this section we summarize retrieval performance of the proposed method and the baseline methods. Throughout the result section MVD denotes the proposed model.
5.1 Comparison to Probabilistic Models
Table 2 compares the performance of MVD to that of the three probabilistic models, namely, language model with Dirichlet prior, BM25 and PL2. First, we compare the performances measured by ERR@20. Table 2 shows that, on two Clueweb.B collections, MVD outperforms LM, PL2 and BM25 by a margin of 8% to 12% and all the differences are statistically significant. On MQ-2009 collection, MVD is once again always statistically significant compared to all the baselines with a margin more than 9%. Similarly, on two Clueweb.A datasets, MVD is unequivocally superior to the baselines and quite clearly the performance differences are even larger than that on Clueweb.B and MQ-2009. The baseline methods seem to be performing nearly equally and in none of the cases, the performance differences among the baselines found to be statistically significant.
Our next goal is to analyze the results measured in terms of NDCG@10. Once again, MVD gives consistent performance improvement over LM, BM25 and PL2 on Clueweb.B collections. The performance differences are always statistically significant with more than 15% relative improvements. Results on MQ-2009 collection also show that MVD is significantly more effective than all the baselines, however the relative differences are smaller compared to Clueweb collec-

592

Table 3: Retrieval effectiveness of the proposed method (MVD) compared to tf-idf models. Statistically

significant improvements are indicated using the first letter of the less effective method. The highest value

per column is boldfaced. The numbers in parenthesis indicate relative improvement over PIVOT, MATF and

LL, respectively.

Clueweb.B-09 & 10 Clueweb.B-11 & 12 MQ-2009

Clueweb.A-09 & 10 Clueweb.A-11 & 12

PIVOT 0.263

0.234

0.367

0.169

0.196

MATF 0.283

0.282

0.388

0.227

0.251

ERR@20

LL MVD

0.290 0.337pml

0.275 0.286pl

0.391 0.408pml

0.244 0.286pml

0.240 0.257pl

(27.9, 18.8, 16.0) (22.0, 1.5, 3.9)

(11.2, 5.1, 4.4) (69.6, 26.1, 16.9) (31.2, 2.7, 7.1)

NDCG@10

PIVOT MATF LL MVD

0.219 0.276 0.287 0.332pml (51.6, 20.5, 15.8)

0.196 0.240 0.234 0.268pml (36.6, 11.4, 14.5)

0.381 0.402 0.418 0.422pm (10.8, 5.0, 1.0)

0.177 0.197 0.207 0.261pml (47.5, 32.3, 25.8)

0.175 0.213 0.206 0.231pml (32.5, 8.7, 12.5)

NDCG@20

PIVOT MATF LL MVD

0.212 0.286 0.284 0.325pml (53.5, 13.5, 14.4)

0.200 0.243 0.235 0.265pml (32.0, 9.0, 12.7)

0.442 0.466 0.477 0.479pm (8.3, 2.8, 0.4)

0.181 0.202 0.201 0.248pml (37.2, 22.7, 23.0)

0.171 0.209 0.198 0.228pml (33.2, 9.4, 15.0)

tions. One reason for smaller difference is that the baseline NDCG@10 numbers are very high, which makes the relative improvements smaller. The effectiveness of MVD on Clueweb.A collections is even more encouraging. MVD surpasses the baselines on Clueweb.A-09 & 10 collection by more than 20% margin which is clearly highly significant. We observe similar trend on the other Clueweb.A collection. As in ERR@20, the baselines seem to be performing with equal effectiveness.
We notice very similar (as in ERR@20 and NDCG@20) behavior of MVD on Clueweb.B collections measured by NDCG@20. Once again, MVD is consistently and significantly better than all the baselines with noticeably large margin of relative improvement. The picture is slightly different on MQ-2009 collection. Although, MVD is better than all the baselines, difference against BM25 only found to be significant. We suspect that sparser relevance judgements of MQ-2009 collection is a possible reason behind smaller differences. Finally, MVD beats the baselines by a convincingly large margin thereby maintaining its consistency as in the previous cases.
Overall, the results indicate that the proposed model based on distribution of maximum values yields consistent and significant retrieval performance improvement over the three state of the art probabilistic baselines from different categories measured by NDCG measures. We conclude that the proposed model is significantly more precise than the baselines on all the collections, thereby making it a very suitable for web search. The experiments also reveal that the performance of the baselines are very similar to each other, irrespective of the collection, which corroborates earlier findings that if parameters of the models are properly optimized, language model, BM25 and divergence from randomness model are closely comparable.
5.2 Comparison to TF-IDF Models
The experiments in this section are designed to compare the proposed method to a number of tf-idf models. By this set of experiments, we intend to achieve the following major goals.

1. How does the proposed model perform compare to a basic tf-idf model that uses only pivoted document length normalization?
2. Since MVD is based on multi-aspect term frequency normalization in a new probabilistic framework, how does it compare against a recent tf-idf model (MATF) that introduced multi-aspect tf normalization? We reiterate that this is the main issue we sought to address using maximum value based model.
3. We mentioned before that MATF combines the two normalized tf using an empirical tf function that transforms normalized tf values. Moreover, it does not factor in the distribution of normalized tf in the elite set for the particular term. Thus, in this section we compare the performance of MVD to a method that uses log-logistic probability distribution of two normalized tfs. The method is denoted as LL in the table. The parameters of this model is estimated using the method detailed in [17].
Table 5.2 compares the performance of tf-idf methods and MVD. First, it is clear from the table that MVD is highly significantly better than PIVOT. This holds for all collection and measured by all three evaluation measures. The performance differences are unequivocally statistically significant. On Clueweb (both A and B) collections, MVD gives upto 50% relative improvement over PIVOT. Second, MATF, which is based on relative intra-document tf normalization and length based normalization (which we call multi-aspect tf normalization), is always poorer than MVD and the differences are almost always statistically significant. More importantly, the margin of improvement by MVD is often noticeably high.
Thus, we conclude that maximum value distribution has large impact on retrieval performance. Finally, we compare the proposed method to log-logistic distribution based method denoted as LL in the table. Note that LL uses distribution of multi-aspect tf normalization for estimating rel-

593

evance and thus has probabilistic interpretation. Table 5.2 once again shows that MVD often significantly surpasses LL.
6. CONCLUSION
In this paper we introduce a probabilistic information retrieval model. The proposed model is guided by the principle that given the normalized frequency of a term in a document, the score is proportional to the likelihood that the normalized tf is maximum with respect to its distribution in the elite set for the corresponding term. We use a mixture of two maximum value distribution, that factors in varying specificity of query terms. The proposed model, integrates multi-aspect tf normalization scheme proposed recently in a probabilistic framework. Unlike many existing models, the proposed model takes into account the term specific distribution in the elite set. However, the unique contribution is that the model measures the likelihood of relevance focussing on the maximum values of the distribution, which we believe the first such effort to view ranking problem from this perspective. An empirical evaluation on large web collections containing millions of documents and hundreds of real world web queries demonstrates that the model significantly outperforms the state of the art probabilistic models from different families. As a future work, we plan to incorporate term proximity (ordered and un-ordered bigram) information into our model.
Acknowledgments
I thank Doug Oard for useful discussions and suggestions. Without his support and advice this work would not have been possible. This research was supported in part by DARPA contract HR0011-12-C-0015 and NSF award 1065250.
7. REFERENCES
[1] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 2002.
[2] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In ACM CIKM, 2009.
[3] C. L. A. Clarke, N. Craswell, I. Soboroff, and E. M. Voorhees. Overview of the trec 2011 web track. In TREC, 2011.
[4] S. Clinchant and E. Gaussier. Retrieval constraints and word frequency distributions a log-logistic model for ir. Inf. Retr., 2011.
[5] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval, 2011.
[6] W. B. Croft, H. R. Turtle, and D. D. Lewis. The use of phrases and structured queries in information retrieval. In ACM SIGIR, 1991.
[7] R. Cummins and C. O'Riordan. A constraint to automatically regulate document-length normalisation. In ACM CIKM, 2012
[8] R. Cummins, J. H. Paik, and Y. Lv. A polya urn document language model for improved information retrieval. ACM Trans. Inf. Syst., 2015.
[9] J. Fagan. Automatic phrase indexing for document retrieval. In ACM SIGIR, 1987.

[10] H. Fang, T. Tao, and C. Zhai. Diagnostic evaluation of information retrieval models. ACM Trans. Inf. Syst., 2011.
[11] R. A. Fisher and L. H. C. Tippett. Limiting forms of the frequency distribution of the largest or smallest member of a sample. In Mathematical Proceedings of the Cambridge Philosophical Society, 1928.
[12] W. R. Greiff. A theory of term weighting based on exploratory data analysis. In ACM SIGIR, 1998.
[13] E. J. Gumbel. Statistics of extremes. Courier Dover Publications, 2012.
[14] B. He and I. Ounis. A study of the dirichlet priors for term frequency normalisation. In ACM SIGIR, 2005.
[15] D. Hiemstra, S. Robertson, and H. Zaragoza. Parsimonious language models for information retrieval. In ACM SIGIR, 2004.
[16] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4), Oct. 2002.
[17] Y. Lv and C. Zhai. A log-logistic model-based interpretation of tf normalization of bm25. In ECIR, 2012
[18] R. E. Madsen, D. Kauchak, and C. Elkan. Modeling word burstiness using the dirichlet distribution. In ICML, 2005.
[19] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In ACM SIGIR, 2005.
[20] J. H. Paik. A novel tf-idf weighting scheme for effective ranking. In ACM SIGIR, 2013.
[21] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In ACM SIGIR, 1998.
[22] S. Robertson and H. Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4), Apr. 2009.
[23] S. E. Robertson. Readings in information retrieval. chapter The probability ranking principle in IR. 1997.
[24] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In ACM SIGIR, 1994.
[25] T. Sakai. Alternatives to bpref. In ACM SIGIR, 2007. [26] G. Salton and C. Buckley. Term-weighting approaches
in automatic text retrieval. Inf. Process. Manage., 24(5), Aug. 1988. [27] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., 1986. [28] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11), Nov. 1975. [29] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In ACM SIGIR, 1996. [30] K. Sparck Jones. Document retrieval systems. chapter A statistical interpretation of term specificity and its application in retrieval. 1988. [31] H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Trans. Inf. Syst., 9(3), July 1991. [32] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2), Apr. 2004.

594

Non-Compositional Term Dependence for Information Retrieval

Christina Lioma
Department of Computer Science University of Copenhagen, Denmark
c.lioma@di.ku.dk
Birger Larsen
Department of Communication Aalborg University Copenhagen, Denmark
birger@hum.aau.dk
ABSTRACT
Modelling term dependence in IR aims to identify co-occurring terms that are too heavily dependent on each other to be treated as a bag of words, and to adapt the indexing and ranking accordingly. Dependent terms are predominantly identified using lexical frequency statistics, assuming that (a) if terms co-occur often enough in some corpus, they are semantically dependent; (b) the more often they co-occur, the more semantically dependent they are. This assumption is not always correct: the frequency of co-occurring terms can be separate from the strength of their semantic dependence. E.g. red tape might be overall less frequent than tape measure in some corpus, but this does not mean that red+tape are less dependent than tape+measure. This is especially the case for non-compositional phrases, i.e. phrases whose meaning cannot be composed from the individual meanings of their terms (such as the phrase red tape meaning bureaucracy).
Motivated by this lack of distinction between the frequency and strength of term dependence in IR, we present a principled approach for handling term dependence in queries, using both lexical frequency and semantic evidence. We focus on non-compositional phrases, extending a recent unsupervised model for their detection [21] to IR. Our approach, integrated into ranking using Markov Random Fields [31], yields effectiveness gains over competitive TREC baselines, showing that there is still room for improvement in the very well-studied area of term dependence in IR.
Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous; H.3.3 [Information Search and Retrieval]
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767717.

Jakob Grue Simonsen
Department of Computer Science University of Copenhagen, Denmark
simonsen@di.ku.dk
Niels Dalum Hansen
Department of Computer Science University of Copenhagen, Denmark
nhansen@di.ku.dk
General Terms
Theory, Experimentation
1. INTRODUCTION
Frege's principle of compositionality posits that the meaning of an expression is a function of the meanings of its constituent expressions and the ways they combine [59]. Applied to linguistics by Montague, this principle implies that the meaning of some text is not just the collective meaning of its words, but also a function of how these words are arranged. Whereas this holds most of the times, occasionally language is non-compositional, i.e. the meaning and arrangement of words alone is not enough to convey the overall semantics. E.g. the phrase red tape (meaning bureaucracy) is not a tape of type red. This linguistic phenomenon is known as non-compositionality.
The challenges posed by non-compositionality have spurred Natural Language Processing (NLP) research in automatic non-compositionality detection, e.g. in nouns [1, 47], verb-noun [20] and verb-particle [30] combinations, using techniques such as latent semantic analysis [20], compositional translations to multiple languages [44], sense induction [22] and word space models [23, 42]. An active line of research focuses on distributional and vector-based models of word and phrase meaning leading to vector-space models for compositionality [8, 35, 41]. These advances have not penetrated IR research notably (with the exception of [33], discussed in Section 2), despite long and persistent IR interest in term dependence. A resulting risk is that the strength of term dependence may be consistently miscalculated in IR. We explain this next.
In IR, dependent terms are predominantly identified using lexical frequency statistics: if terms co-occur often enough in some typically large dataset, they are assumed to be dependent, and the strength of their dependence is typically assumed proportional to their frequency of co-occurrence, e.g. see [10, 34]. Simply stated, the more frequently two terms co-occur, the more dependent we assume they are. This assumption is not always correct. In linguistics, the frequency of term co-occurrence can be somewhat separate from the strength of semantic dependence. Even though the former can be indicative to some extent of the latter, their relation is not symmetric. E.g. red tape might be overall less frequent than tape measure in some corpus, but this does not

595

mean that red+tape are semantically less dependent than tape+measure; quite the contrary. Non-compositionality lies at the heart of this because non-compositional terms are maximally dependent, regardless of their frequency of co-occurrence. So, whereas the strength of term dependence within compositional phrases, e.g., tape measure, white horse, can be reasonably approximated by their frequency of co-occurrence in a corpus, this is not true for non-compositional phrases, like red tape, dark horse.
Motivated by this lack of distinction in IR between frequency of term co-occurrence and strength of term dependence, we present a principled approach for treating term dependence in queries. This approach extends a recent unsupervised model for detecting non-compositional phrases using lexical frequency and semantic evidence [21]. The main idea consists of (a) substituting a term in a phrase by a synonym (e.g. red tape would become scarlet tape) and (b) measuring the semantic divergence of the replacement phrase from the original phrase. If their meanings diverge, the original phrase is more likely to be non-compositional. If however their meanings do not diverge much (e.g. tax office would become tax bureau), then the original phrase is less likely to be non-compositional. We extend the vector space model proposed for measuring this divergence in [21] with a probabilistic model that measures the KullbackLeibler divergence between the language models of the original and replacement phrase (Sections 3-4). We apply both approaches to detect strongly dependent query terms, which we then treat in a non-bag of words fashion during ranking (Section 6). Experiments with 350 TREC queries show that our approaches consistently outperform competitive baselines, and are particularly effective for 2-, 3-, and 4-term queries in the web search task.
2. RELATED WORK
Broadly speaking, efforts to model term dependence, also known as term co-occurrence, adjacency and lexical affinities1 in IR, typically model phrases found in queries and/or documents, motivated by the intuition to consider as more relevant those documents in which terms appear in the same order and patterns as they appear in the query, and as less relevant those documents in which terms are separated [51]. These efforts were initiated mainly in the 1980s, and intensified in the 1990s, reporting retrieval benefits. Later, efforts decreased: baseline performance improved, and the cost associated with linguistic processing was not worth the small benefits over the already improved baselines [54].
Generally, term dependence is detected using either statistical or linguistic information. Research began with the early work on statistical term associations [6, 14, 24, 56] and syntax-based approaches [3, 7, 45], continuing with work on probabilistic term dependence models [15, 46, 60, 61, 63], syntactic methods [5, 32, 49, 50] and statistical approaches [10, 25, 26]. From the mid-1990s onwards, research focused on hybrid methods combining syntactic and statistical approaches of phrase processing [9], phrase-based enhancement of the indexed term representations [64], and phrase-based term weighting [37, 39, 57]. More recent research has focused on statistical methods, primarily using language modelling
1Dependence, co-occurrence, adjacency and lexical affinities are not synonyms [16], but in IR they are used interchangeably.

[31, 34, 36, 52, 55] but not exclusively [28, 40], while attention has also been given to term dependence and efficient large-scale indexing [12, 27]. The Markov Random Field (MRF) model of term dependence [31] reported significant improvements in retrieval effectiveness.
Several more recent studies address term dependence, for instance using heuristics [58], formalising the term position in the document [29], or extending the MRF model to concepts [4], all reporting positive findings. This continued interest in term dependence may indicate that it is still an open problem. However, to our knowledge, none of these approaches addresses non-compositionality, except Michelbacher et al. [33], who focus primarily on the automatic detection of the head modifier inside non-compositional phrases and use IR as a task illustrating that the information they detect can be useful. They experiment with a small nonTREC dataset and report statistically significant gains in retrieval precision.
3. NON-COMPOSITIONALITY DETECTION (NCD)
Non-Compositionality Detection (NCD) aims to identify the presence and strength of non-compositional phrases in language. This is typically realised as a measurement, i.e. through some function that outputs a compositionality score for a phrase. Given a scale of such scores, the minimum and maximum reflect the total absence of compositionality (noncompositionality), e.g. red tape, and complete compositionality, e.g. tax office, respectively. Sliding along such a scale corresponds to moving across phrases of various levels of compositionality, practically facilitating the comparison of phrases on the grounds of their term dependence. We reason that such a comparison may be useful to IR, where systems need to process differently queries at different positions of this scale, i.e. keyword-based (= compositional) queries such as London transportation, and queries containing heavily dependent terms (=non-compositional) such as red tape AL register car.
This section presents how we use non-compositionality to model term dependence in IR. Among the various NCD approaches outlined in Section 1, we use the recent approach of [21] because it is unsupervised, resource-efficient, and performs competitively on benchmark tests. We extend this NCD approach, which uses vector spaces, by adding a second estimation of non-compositionality, this time probabilistic. In addition, we formally express the methodological description of [21] as a model of query perturbation, and we model non-compositional term dependence specifically for IR queries, not general phrases like [21], with considerations to data constraints in an IR context.
3.1 Non-compositionality in queries
Given a query, we aim to detect the presence and strength of non-compositionality in it. Kiela and Clark [21] posit that non-compositional phrases can be identified by substituting each of the original words in them, one at a time, by some other relevant/synonymous term, and comparing the meaning of each phrase resulting from the substitution to the meaning of the original phrase. The more they diverge, the less compositional the original phrase is. E.g. replacing car by vehicle in import car gives import vehicle, which is semantically similar to the original, but replacing red by

596

scarlet in red tape gives scarlet tape, which is semantically different from the original. Hence, red tape is less compositional than import car. The core idea is that such substitutions are likely to have a low impact on the semantics of compositional phrases, but a high impact on the semantics of non-compositional phrases. The resulting semantic divergence is then approximately inversely proportional to compositionality.
Conceptually, we see this approach as applying perturbations over some signal in order to study the resulting effects upon the signal. In our case, the signal is the query and the perturbation is the replacement of a query term by another term. We express this perturbation as follows: Let Sq(I; T ) be the semantic space S of query q containing the ordered set of terms T , where I is the information conveyed by T . Let T¯ denote the ordered set of query terms where one of them has been replaced by another term (e.g., a synonym). I.e., T¯ is the perturbation of T . Then, the non-compositionality Nq, and the compositionality Cq of query q, can be expressed as a function of the divergence  of the resulting semantic spaces (Sq(I; T ), Sq(I; T¯)), for all m = |T | divergences resulting from all substitutions:

Nq = f {(Sq(I; T ), Sq(I; T¯) : T¯  {T1, . . . , Tm})} (1)

Cq = g(Nq)

(2)

where f is typically some summation or averaging function over the set of divergences, and g is some decreasing function, e.g. g(x)  1/x . Thus, non-compositionality increases with semantic divergence, but compositionality decreases with semantic divergence.
Unless constrained, such perturbations risk drifting semantically further away than intended, e.g., if two out of all three terms in a query are replaced simultaneously. Kiela & Clarke address this using two constraints, which we also adopt: (a) only one term is replaced at a time, and (b) a term is replaced by its synonym or a closely related term such as a hyper- or hyponym2. We identify a further risk of degrading performance by considering too many synonyms: The set of perturbations for query q consisting of terms t1 · · · tm, where sj is a synonym of tj, is:



s1t2 · · · tm







  

t1s2 · · · tm

  

    
{p1, . . . , pm} =


... t1 · · · tj-1sj tj+1 · · · tm

    


     

...

     







t1 · · · tm-1sm



where pm denotes the mth perturbation. As a term may have more than one synonym, of various grades of synonymity, the set of perturbations can grow to include all synonyms of the query terms. A selection process must control the perturbations so that: (a) "best possible" synonyms (as opposed to near-synonyms) are used, and (b) the number of perturbations is minimised, i.e. we perturb the queries no more than necessary for computing their compositionality. We thus use one perturbation per query term and experimentally show (in Section 6) that this suffices for IR. Other tasks, including NCD per se, may require more perturbations per term.

2We henceforth refer to all forms of closely related terms as synonyms.

3.2 Semantic divergence
Computing the divergence in Equation 1 requires that both the query and perturbations be represented in some semantic space that is tractable and amenable to measurement. Kiela & Clarke propose vector spaces (Section 3.2.1). We propose probability spaces as a complementary representation (Section 3.2.2). We present and experiment with both.

3.2.1 Vector Space

We re-express the vector space representation of Kiela & Clarke for queries and their perturbations as follows. Let v(q) and v(pj) be the vector of query q and its perturbation pj respectively. The semantic divergence  between the query and its perturbation can be modelled as the distance d between their vectors (  d), where d is some appropriate distance function. This d can be chosen as any vector distance function, e.g. Euclidean, Chebychev, or the better-known Cosine we use here. Then, assuming a summation function f in Equation 1, the non-compositionality of a query containing terms t1 · · · tm of k synonyms is:

1 mk

mk

d(v(q), v(pij))

(3)

j=1 i=1

where pij is the perturbation t1 · · · tj-1sij tj+1 · · · tm and sij is the ith synonym of term tj. Using one synonym per term
only (as we do) reduces this to:

1m

m d(v(q), v(pj))

(4)

j=1

The main idea is to represent a query and its perturbation as vectors, so that we can interpret their distance as semantic divergence. Practically this means mapping  from Equation (1) to d above. Dating back to Salton, the IR and NLP literature abounds with variations of how the above vector representation can be implemented and interpreted, any of which can be used here. We describe how we build the vectors and how we compare their distance in Section 4.

3.2.2 Kullback-Leibler Divergence
We now present our probabilistic representation of queries and their perturbations. The high-level difference from the previous representation is that instead of representing a query as a vector of terms, we represent it as a distribution of events, where the events correspond to terms. Such representations are called probabilistic because they allow computing the probability of an event occurring, i.e. the probability of a term occurring in the query. When these probabilities are interpreted in a frequentist way, they are approximated by relative frequencies (i.e. normalised word counts). In text processing, this is known as language modelling.
We reason that, if queries and their perturbations are represented as event distributions, then their divergence can be computed using standard methods, one of the better known being their Kullback-Leibler divergence (KLD). Even though KLD is not a distance metric (it is not symmetric), it is widely used in IR to approximate the semantic distance between texts, where higher KLD values indicate more divergence. We apply this to compute the semantic divergence  in Equation 1, by building a language model for the query and each perturbation. Then, their KLD should be proportional to the semantic divergence  in Equation (1), i.e.

597

(  KLD). Let LMq and LMp denote the language models of query q and perturbation p respectively. Their KLD is:

K LD(LMq ||LMp )

=

LMq

log

LMq LMp

(5)

We next describe how we build LMq, LMp and how we operationalise Equation (5).

4. MODEL INDUCTION
Both vector and probability space representations presented above approximate how different a perturbation is from the original query, albeit in different ways. This section describes their exact mechanics.
We start by describing what the above vectors and language models actually consist of. As the approach is the same for both queries and their perturbations, we henceforth refer to their union as Q. For each term t  Q, we build a context window as follows: we extract bags of terms occurring within a window of maximum n terms away from t in some large document corpus, so that the window consists of 2n + 1 terms. E.g., if n=5, then we consider 11 terms in total: 5 (immediately preceeding t) + t + 5 (immediately succeeding t). The underlying assumption is that all the terms in a document have some relationship to all other terms in the document, modulo window size, outside of which the relationship is not taken into consideration. In statistical NLP this is a standard way of inducing word semantics from "the company they keep", a.k.a. distributional semantics [11]. These context windows provide the ingredients of the vector and probabilistic representation of our queries and perturbations, explained next.
4.1 Vector representation
After all context windows of a term t  Q are extracted, we compute a term weight vector wt for t with the aim of capturing the salience of term t. Kiela & Clarke show that such weights can function in a discriminative way for the task of NCD. For each query, we generate a term weight vector by combining the term weight vectors of the terms in the query. Next we explain how we compute the weights of the individual query terms and the weight of the whole query or perturbation.
4.1.1 Individual Term Weights
Kiela & Clarke experiment with these five well-known weighting schemes, adapted to the context window scenario, (even though they only report results from LTU), which we also use:
ATC [43]:

wit =

0.5

+

0.5

×

fit maxf

)

log(

N n(t)

(6)

N i=1

0.5 + 0.5

×

fit maxf

log

N n(t)

2

LTU [48]:

wit

=

(log(fit) + 1.0) log

0.8

+

0.2

Mi av.M

N n(t)

(7)

Mutual Information (MI) [38]:

fjt

wit = log

N

× N
j=1

fjt

N

Mi k=1

fik

N

(8)

Okapi [18]:

wit =

fit

0.5 + 1.5 ×

Mi av.M

+ fit

× log

N - n(t) + 0.5 fit + 0.5 (9)

TFxIDF [53]:

N

wit = log(fit) × log n(t)

(10)

where wit is the weight of term t in context window i; fit is the frequency of t in context window i; N is the total number of context windows; n(t) is the number of context windows containing t; Mi is the number of terms in context window i; av.M is the average number of terms in all context windows; and maxf is the maximum frequency of any term in any context window.
To construct a vector v(t) for each t  Q, we extract the context windows for t, which we denote cwt. For each term, t , represented by an entry in v(t), the corresponding weight is computed as the average of wit for i  cwt.

4.1.2 Query/Perturbation Weights
Having built such a vector for each t  Q, the vector of the entire query or perturbation can be constructed in several ways, for instance as the element-wise sum of the vectors of its terms, or as their dilation, or as their pointwise multiplication. We choose the latter because it has been shown more effective for semantic vector representations in NLP [21, 35]. The final query vector q for query q consisting of terms t1 · · · tm is:

v(q) = v(t1) · · · v(tm)

(11)

where is the binary operator on equal-length vectors of real numbers defined by (x1, . . . , xn) (y1, . . . , yn) = (x1 × y1, . . . , xn × yn). The perturbation vectors are built identically to this. Note that as is associative and commutative, the jth component of v(q) is simply the product of all the jth components of the vectors v(t1), . . . , v(tm).
As Kiela & Clarke point out, using pointwise multiplication has a somewhat `reverse' effect on the semantic distance: overlapping components (i.e. terms appearing in common contexts) are stressed; since their vectors have little overlap outside the non-compositional meaning, their perturbations also have little overlap, resulting in a smaller change in distance when perturbed. Another effect of pointwise multiplication is that the frequency of terms occurring in the context windows of a query term will be strengthened: if a term t has a high weight in both v(t) and v(t ), it will have a high weight in v(t) v(t ); however, low weight in either one of v(t) or v(t ) will correspond to low weight in v(t) v(t ). This means that the vectors of the terms of non-compositional queries, which will in general occur in very different contexts, will have entries with fairly low absolute values. In contrast, for compositional queries, substituting a term by its synonym may yield constructions that

598

can be expected to occur in a number of contexts wildly different from the original, hence will have markedly different contextual statistics and thus greater distance d.

4.2 Language modelling representation

The alternative representation we propose for queries and perturbations is to use the set of all context windows of the terms in a query or perturbation to build a respective language model LMq, LMp (introduced in Equation 5). There exist various ways of building language models from term counts, involving some sort of smoothing of the counts; we use two among the best known, Laplace and Simple GoodTuring.
Laplace (or add-one) estimates the probability of a term t in the language model of query q, PLP (q, t), as:

PLP (q, t)

=

cq,t + 1 Cq + V

(12)

where cq,t is the count of t in q, Cq is the count of all terms in the context windows of q, and V is the number of terms in the language model of q. We compute it identically for perturbations (replacing q by p above).
For sparse data over large vocabularies, Laplace tends to make a very big change to the counts and resulting probabilities because it moves too much probability mass to all unseen events (zero counts). We could move a bit less mass by adding a fractional count rather than 1 (e.g. add "smoothing" [17]), but that would require choosing  dynamically, risking inappropriate discounting for many counts, and producing overall counts with poor variances [19]. For these reasons, we also apply Simple Good-Turing [13] smoothing, which uses (i) the counts of hapax legomena (events occurring once) to estimate the counts of unseen events, and (ii) double counts, i.e. the frequency of a frequency. Simple Good-Turing estimates the probability of a term t with frequency r in the language model of query q, PGT (q, t), as:

PGT (q, t)

=

(r + 1) · S(f fr+1) Cq · S(f fr)

for

r>0

(13)

where f f is a vector with frequencies for term frequencies, Cq is as defined as in Equation 12, and S is a function fitted through the observed values of f f to get the expected count of these values (see [13] for more). For zero count values the probability is calculated as follows:

PGT (q, t)

=

f f1 Cq

for r = 0

(14)

where f f1 is the frequency of frequency of hapax legomena. We normalise the resulting language model to sum to 1. Simple Good-Turing is known to perform well, especially for large numbers of observations drawn from large vocabularies.
The above two smoothing methods produce a language model for each term per query or perturbation. To produce one language model for the whole query or perturbation, we sort the language models of their terms and combine them in four different ways: (1) summing their values in quantiles 2 & 33; (2) averaging their values in quantiles 2 & 3; (3) multiplying their values; (4) using the median of their values. Overall, the above 2 smoothing methods × 4 combinations produce 8 language modelling variations of NCD.

3We use quantiles 2 & 3 to avoid outliers.

5. DISCUSSION OF OUR NCD APPROACH
Both representations (vector and probability space) of the NCD approach we present are parameterised over the notion of semantic divergence, which we operationalise with different weightings, each corresponding to some variation of computing this divergence. Our use of semantic divergence, measured typically as a real number in the model, corresponds to the observation that compositionality is not dichotomous: phrases in general are not only compositional or non-compositional; rather, a fine-grained range of compositionality exists, a fact corroborated by human raters asked to score degrees of compositionality [2, 30]. Suitable divergence functions that could mimic the scores of human raters may exist, but we have not attempted to do so.
We have also not attempted to estimate the semantic `accuracy' of the phrases resulting from each perturbation, i.e. the extent to which they are non-sensical, even though Kiela & Clarke state that this is possible with their approach [21]. We estimate solely the divergence between the query and a perturbation, and not how much sense the perturbed phrase makes, for two reasons: (a) we reason that the semantic divergence should in principle suffice for indicating compositionality as we intend to use it in IR; (b) to our knowledge, no scalable automatic approach can adequately approximate such a semantic assessment for query logs.
Another point of departure from Kiela & Clarke is our treatment of query terms as a list, i.e. a set endowed with a strict order. In principle, all computations presented, both by Kiela & Clarke and by us, can be used with ordinary (i.e., unordered) sets of terms too, as has also been done with term dependence models in IR [31]. We use strictly ordered sets because non-compositionality is never manifested in language in any other way, for instance by mixing the order of non-compositional terms, or by interrupting them by another term. E.g., red tape can function non-compositonally (and mean bureaucracy) only when the terms red and tape appear adjacent and in that specific order. Ergo, no variation of red .+ tape or tape .+ red (in RegEx notation) can have the non-compositional meaning of bureaucracy.
Finally, perturbations are common in science, and the practice of perturbing queries has even been used in IR before, albeit for different reasons. For instance Vinay et al. [62] employ different query (and document) perturbations for query performance prediction: by altering the query term weights, they observe the documents retrieved, and study the relationship between the amount, or sensitivity of perturbation and the quality of the ranking. Our approach, apart from having a different overall scope, namely term dependence as opposed to query performance prediction, also differs from [62] in that it applies a linguistically informed selection process for each perturbation: we replace query terms by their synonyms, not by varying their respective term weights within some range.
6. EVALUATION
6.1 Using NCD for selective term dependence
This section presents experiments aiming to quantify the effectiveness of processing query term dependence, not as a bag of words, but as a `set phrase' of strict ordered adjacency, i.e. matching documents that contain an identical (ordered & uninterrupted) sequence of terms. The main idea is to use NCD to select which among a batch or stream

599

of queries contain dependent terms, and process only those queries as a `set phrase'; the rest of the queries can be processed as a bag of words. For this initial study, we focus on the non-compositionality of the whole query, not of phrases within queries.
We use the non-compositionality score of each query (computed with any of the 5 vector space or 8 language modelling variants presented in Section 4) as a proxy of term dependence. This allows to detect queries more likely to be noncompositional, hence more likely to contain highly dependent terms, rather than those queries that are strictly noncompositional. We do this by ranking queries by their noncompositionality and selecting the  least compositional. These  queries are processed with the MRF model of fully dependent query terms; the rest of the queries in the batch are treated as a bag of words.
6.2 Experimental Setup
6.2.1 Baselines & Our Methods
We use three baselines: (1) bag of words for all queries, which allows for no term dependence; (2) the MRF model of sequentially dependent query terms [31], which treats as a `set phrase' only adjacent query terms; (3) the MRF model of fully dependent query terms [31], which treats as a `set phrase' the whole query. We compare these baselines against our selective term dependence approach that treats as a `set phrase' the whole query iff the NCD score of this query indicates that it is likely to be non-compositional; this is controlled by the threshold  presented above.
All three baselines and our 13 NCD variants use a unigram, query likelihood, Dirichlet-smoothed language model for ranking. Note that we use `language model' in two different ways in this work, for two entirely different computations: (a) to estimate the semantic divergence between queries and perturbations (in Section 3.2.2), and (b) to rank documents with respect to queries.
6.2.2 Data & Tuning
We use the TREC 6-8 queries (301-450, title only) of the AdHoc track with Disks 4-5 (minus the Congressional Records for TREC7-8), and queries 1-200 of the Web AdHoc tracks of TREC 2009-2012 with ClueWeb09B4 (see Table 1). We extract the distributional semantics of the NCD model (i.e. build the context windows) from Disks4-5 for queries 301-450, and from ClueWeb09B for queries 1-200. We use no stemming and remove stop words from the queries only (as in [31]). We use Indri 5.85 for indexing and retrieval of at most 1000 documents per query. We evaluate retrieval effectiveness using standard measures of early and deep precision (MAP, NDCG@10, P@10).
The Dirichlet ranking model includes a parameter µ that we tune as follows: µ  {100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000, 10000}. We also vary the number  of least compositional queries selected each time:   1 . . . 45 per TREC batch of 50 queries. All tuning is done per evaluation measure using 3-fold cross validation. We report the average of the three test folds. For NCD we extract the first synonym suggested by WordNet6 (to be used for perturbing the query). For these initial experiments, we do not vary the
4http://lemurproject.org/clueweb09.php/ 5http://www.lemurproject.org/ 6http://wordnet.princeton.edu

# Documents # Queries TREC track

Table 1: Datasets

Disks4-5

ClueWeb09B

556077

50220423

301-450

1-200

TREC6-8 AdHoc Web09-12 AdHoc

Table 2: Query length (without stopwords) #1 #2 #3 #4 #5
DISKS4-5 11 56 76 7 CWEB09B 57 65 63 13 2

value of the window of co-occurrence described in Section 4: we set n = 5, i.e. the context window size is 11.
6.3 Findings
Table 3 shows the retrieval precision of our baselines and NCD approaches. Each cell also displays the % of queries that are treated as a `set phrase'. For the MRF models, 100% means that all queries are treated as a `set phrase', including single-term queries, for which this treatment makes no difference over a bag of words treatment. Overall, our NCD approaches outperform all baselines at all times. The improvement over the strongest baseline is modest (up to >+3.5% for MAP with ATC, >+3.3% for NDCG@10 with MI, and >+4.5% for P@10 with MI), however it is consistent for both datasets and for all evaluation measures (deep and early precision). This means that the performance gain spans across the range of relevant documents (those retrieved in the top ranks, but also those retrieved further down). Unlike earlier findings that the use of co-occurrence information tends to reduce retrieval effectiveness [46], possibly due to the fact that the term relationships modelled may have little discriminating power [31], we notice an overall modest but clear gain in effectiveness.
Breaking this down to a per-query basis (cf. the two top plots in Fig. 1), the following two findings emerge. (I) The scale of improvement is higher than that of deterioration: between +0.13 and -0.07 for MAP; and between +0.68 and -0.4 for NDCG@10, for our Laplace sum approach (chosen illustritatively) from the strongest baseline (MRF with full dependence). (II) More queries improve than deteriorate by our approach. Hence, the improvements in Table 3 are not artificially inflated by outliers that might affect the means of the evaluation measures, but are rather representative of the whole body of queries.
Furthermore, we show examples of queries yielding the highest and lowest precision difference from the strongest baseline in Table 5. The best queries are not strictly noncompositional; however they do have strongly contextualised semantics and term co-dependence. E.g. french lick resort casino does not denote some other meaning than a particular casino, but it is presumably irrelevant to the semantics of the verb to lick and french as a language or nationality. Most of the best queries in Table 5 are web queries, which often tend to include abbreviations and acronyms, e.g. vbart sf. These are not non-compositional either, but rather idiomatic or colloquial phrases of strong term dependence, and are selected by our NCD approach because they are likely to diverge in meaning if perturbed (i.e. it is not possible to express their meaning alternatively, for instance by near-synonyms). Hence, using NCD to approximate strong term dependence is effective in these

600

Table 3: Retrieval precision of the 3 baselines (in grey rows) vs. our 13 non-compositionality approaches.

Bold marks >highest baseline. The star * marks best overall per measure & collection. %DQ is the % of

queries processed as dependent (the rest of the queries in the batch are processed as bags of words).

METHOD

DISKS4-5

CWEB09B

DISKS4-5

CWEB09B

DISKS4-5

CWEB09B

MAP %DQ MAP %DQ NDCG@10 %DQ NDCG10 %DQ P@10 %DQ P@10 %DQ

Bag of words

.1905

­ .1151

­ .4276

­ .3502

­ .3907

­ .4167

-

Sequential Dependence [31] .1814 100% .1077 100% .3983

100% .3463

100% .3687 100% .4120 100%

Full Dependence [31]

.1933 100% .1151 100% .4341

100% .3514

100% .4007 100% .4176 100%

VECTOR LANG. MODEL

Laplace sum

.1948 63% .1188 34% .4406

70% .3596

18% .4047 67% .4317 30%

Laplace average

.1948 63% .1186 34% .4406

70% .3596

18% .4047 67% .4307 36%

Laplace median

.1947 67% .1176 51% .4390

48% .3585

18% .4060 67% .4278 31%

Laplace multiplication

.1948 48% .1182 46% .4388

36% .3617

22% .4040 59% .4303 22%

GoodTuring sum

.1940 81% .1168 50% .4402

57% .3618

31% .4040 79% .4288 28%

GoodTuring average

.1940 81% .1167 50% .4402

57% .3618

29% .4040 79% .4288 26%

GoodTuring median

.1949 73% .1168 56% .4422

51% .3583

15% .4067* 51% .4283 32%

GoodTuring multiplication .1943 71% .1171 29% .4390

59% .3623

28% .4053 72% .4302 22%

ATC

.1950* 77% .1191* 47% .4446*

55% .3604

31% .4053 56% .4308 53%

LTU

.1948 75% .1184 40% .4444

51% .3592

29% .4053 52% .4278 33%

MI

.1946 81% .1188 51% .4445

59% .3631* 32% .4053 52% .4364* 52%

Okapi

.1948 73% .1180 48% .4427

57% .3597

20% .4040 57% .4293 21%

TFIDF

.1941 56% .1175 30% .4422

61% .3605

39% .4053 53% .4294 30%

Table 4: Retrieval precision for 2/3/4-term queries with our three best non-compositionality approaches. (±

%): difference from the strongest baseline. Rest of notation as in Table 3.

METHOD
Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI

2 terms (56 queries)

MAP

%DQ

.1994

­

.1953

100%

.2022

100%

.2115* (+4.6%) 48%

.2114 (+4.5%) 48%

.2114 (+4.5%) 48%

DISKS4-5

3 terms (76 queries)

MAP

%DQ

.1985

­

.1722

100%

.1976

100%

.2046* (+3.1%) 45%

.2046* (+3.1%) 46%

.2046* (+3.1%) 46%

4 terms (7 queries)

MAP

%DQ

.1181

­

.1120

100%

.1143

100%

.1245* (+5.4%) 29%

.1245* (+5.4%) 29%

.1245* (+5.4%) 29%

METHOD
Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI

2 terms (56 queries)

NDCG@10

%DQ

.4331

­

.4183

100%

.4174

100%

.4855* (+12.1%) 32%

.4855* (+12.1%) 32%

.4855* (+12.1%) 32%

DISKS4-5

3 terms (76 queries)

NDCG@10

%DQ

.4699

­

.3685

100%

.4421

100%

.4968* (+5.7%) 33%

.4968* (+5.7%) 33%

.4968* (+5.7%) 33%

4 terms (7 queries)

NDCG@10

%DQ

.3549

­

.3394

100%

.3768

100%

.3902* (+3.6%) 29%

.3902* (+3.6%) 29%

.3902* (+3.6%) 29%

METHOD
Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI

2 terms (56 queries)

P@10

%DQ

.4018

­

.3909

100%

.3873

100%

.4545* (+13.1%) 20%

.4527 (+12.7%) 20%

.4527 (+12.7%) 20%

DISKS4-5

3 terms (76 queries)

P@10

%DQ

.4286

­

.3429

100%

.4208

100%

.4649* (+8.5%) 30%

.4649* (+8.5%) 30%

.4649* (+8.5%) 30%

4 terms (7 queries)

P@10

%DQ

.3000

­

.3000

100%

.3400*

100%

.3400* (±0.0%) 29%

.3400* (±0.0%) 29%

.3400* (±0.0%) 29%

METHOD
Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI

2 terms (65 queries)

MAP

%DQ

.1290

­

.1126

100%

.1234

100%

.1371 (+6.3%) 43%

.1368 (+6.0%) 48%

.1368 (+6.0%) 48%

CWEB09B

3 terms (63 queries)

MAP

%DQ

.1391

­

.1235

100%

.1377

100%

.1480 (+6.4%) 43%

.1519 (+9.2%) 46%

.1520* (+9.3%) 46%

4 terms (13 queries)

MAP

%DQ

.1046

­

.0949

100%

.0982

100%

.1120 (+7.1%) 15%

.1128* (+7.8%) 23%

.1128* (+7.8%) 23%

METHOD
Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI

2 terms (65 queries)

NDCG@10

%DQ

.4003

­

.3671

100%

.3412

100%

.4142 (+3.5%) 32%

.4143 (+3.5%) 34%

.4142 (+3.5%) 34%

CWEB09B

3 terms (63 queries)

NDCG@10

%DQ

.2907

­

.2902

100%

.2958

100%

.3267 (+10.4%) 33%

.3291* (+11.3%) 35%

.3291* (+11.3%) 35%

4 terms (13 queries)

NDCG@10

%DQ

.3552

­

.2409

100%

.3213

100%

.3873* (+9.8%) 31%

.3838 (+8.1%) 31%

.3838 (+8.1%) 31%

METHOD
Bag of words Sequential Dependence [31] Full Dependence [31] GoodTuring median ATC MI

2 terms (65 queries)

P@10

%DQ

.4894

­

.4318

100%

.4167

100%

.5152 (+5.3%) 20%

.5167* (+5.6%) 25%

.5167* (+5.6%) 25%

CWEB09B

3 terms (63 queries)

P@10

%DQ

.3600

­

.3550

100%

.3617

100%

.4067 (+12.4%) 21%

.4100* (+13.4%) 19%

.4100* (+13.4%) 19%

4 terms (13 queries)

P@10

%DQ

.3538

-

.2692

100%

.3615

100%

.4154* (+14.9%) 38%

.4154* (+14.9%) 38%

.4154* (+14.9%) 38%

601

cases. Our worst performing queries consist of phrases for which many more variants that denote the same meaning exist. E.g. tv show, television programme/broadcast, signs/symptoms/indications heart attack/failure, etc. Restricting this type of queries to strict `set phrase' matching limits the retrieval scope significantly with resulting drops in performance.
Next we focus the analysis on two pertinent aspects of our approach: the number of strongly term dependent queries selected and retrieval performance for 2-4 term queries.
6.3.1 Number of least compositional queries
The number of queries treated as a `set phrase' is lower for our approach than for MRF by 1/3 for Disks4-5 and 2/3 for ClueWeb09B, or by 1/4 for Disks4-5 and 1/3 for ClueWeb09B if we ignore 1-term queries (statistics in Table 2). Compositionality and term dependence in general cannot be measured for single terms, hence 1-term queries are ignored.
Since we treat the number  of least compositional queries as a tuneable parameter, one may wonder to what extent the gains we report are due to tuning as opposed to the inherent strength of our approach in detecting term dependence. To answer this, Fig. 2 shows the MAP and NDCG@10 of our MI approach across the range of  values for ClueWeb09B (we can confirm similar trends for P@10 and Disks4-5, and our other NCD approaches). We see that our approach outperforms the strongest baseline (marked by a horizontal line) consistently across the range of , peaking when roughly  =80 least compositional queries (out of 200, or 143 if one excludes 1-term queries) are treated as strongly term dependent. Practically this means that our approach can be used without necessarily tuning  and is likely not to give large fluctuations in both early and deep precision.
6.3.2 Queries of 2-4 terms
Finally, we focus on queries of 2, 3 and 4 terms because these are the most likely to include strong term dependence, hence they are ideal for comparing our approaches to the MRF models.
Table 4 shows the retrieval precision of our baselines and our three best NCD approaches (marked by * in Table 3) specifically for queries of these lengths. Again all our approaches outperform all baselines at all times. The only exception is for 4-term queries in Disks4-5 and P@10, where our methods perform equally to the strongest baseline (no gain, no loss). Overall, our NCD approaches outperform the strongest baseline by up to >+5% for MAP, >+6% for NDCG@10, and >+8% for P@10, on average. The two middle and lower plots in Fig. 1 show that these improvements are not due to outliers, but are instead spread over the queries. Fig. 1 illustrates this for 2- and 3-term queries w.r.t. MAP and NDCG@10, but we confirm that the same trend applies to 4-term queries and P@10. Hence, for queries of length 2-4, i.e. predominantly phrasal queries, our approaches outperform all baselines notably. This finding, combined with the relative robustness of the threshold  discussed above, mean that our approach could be used as part of the IR pipeline, e.g. for 80% of the incoming queries of length 2-4. Note that these types of queries form the majority of all queries, at least in our TREC data (see Table 2), hence they are not a negligible sample.

MAP all 350 queries

NDCG@10 all 350 queries

0.1 0.05
0 -0.05
MAP 121 2-term queries
0.15
0.1
0.05
0 MAP 139 3-term queries
0.12 0.1 0.08 0.06 0.04 0.02
0 -0.02

0.6 0.4 0.2
0 -0.2 -0.4
NDCG@10 121 2-term queries 0.4 0.3 0.2 0.1
0 -0.1
NDCG@10 139 3-term queries
0.3
0.2
0.1
0
-0.1

Figure 1: Sorted per-query difference (y-axis) in MAP/NDCG@10 between the strongest baseline (Full Dependence) and our Laplace sum method, for all, 2-term, & 3-term queries in DISKS4-5 & CWEB09B. The horizontal line marks the baseline (points above are gains). Each point is a query.

0.1188

MAP

NDCG@10 0.36

0.1151

0.3514

0 20 40 60 80 100 120

0 20 40 60 80 100 120

Figure 2: MAP & NDCG@10 (y-axis) vs.  most non-compositional queries in CWEB09B according to MI (x-axis). The horizontal line marks the baseline. Each point is a query.

Table 5: Queries with most gain/loss from NCD.

Best

Worst

bart sf

tv show

ct jobs

industrial espionage

french lick resort casino export controls cryptography

civil right movement

signs heartattack

602

7. DISCUSSION
The relative gains in retrieval precision reported above should not be considered as indications of accurate noncompositionality detection. The suitability of our proposed probabilistic representation of queries and their perturbations in particular remains to be evaluated for NCD accuracy. Moreover, several of our choices of NCD settings can be further explored, e.g. synonymy selection or smoothing choices. In this initial study we opted for default or popular settings, where possible. For these reasons, we have refrained from making a quantitative comparison between the vector space and probability space NCD variations, other than reporting the retrieval precision they yield. This means that the NCD variations we present are not necessarily calibrated to this domain or task. Calibrating them could potentially improve performance even more, but would incur some computational cost, the major bulk of which would likely lie in the extraction of context windows from some large dataset. In an IR scenario, this can be done offline, and is perhaps not too distant from the query analytics widely used.
Regarding our data, the query sets we use are `curated' by TREC, in the sense that those queries that are perhaps not understood by human assessors, or for which no relevant documents are easily found during pooling, may have been omitted. This selection may have affected non- or lowcompositionality queries. This agrees with the finding that the number of IR benchmark queries that contain strongly dependent terms in general is small [65]. Unfiltered query logs may contain more such queries, making our approach potentially even more useful in such a practical setting.
8. CONCLUSIONS
We presented an approach for detecting strongly dependent query terms using the linguistic property of non-compositionality. Non-compositional meaning cannot be induced from the meanings of individual words or their arrangement in a query. E.g., hot dog is not a type of dog that is hot, but rather a type of food. We used unsupervised measurement of non-compositionality to approximate the detection of strongly dependent query terms. Such queries are challenging to IR because they cannot be processed to some reasonable accuracy by bag of words approaches. Motivated by this, we focussed not on how these queries can be treated during ranking (there is a lot of literature in this area generally for term dependence, which can be applied here), but on how these queries can be selected from a batch or stream of incoming queries. This specific question has so far been addressed by assuming that the more frequently terms co-occur in a query, the more dependent they are. This assumption is however not always true, because frequency is not always proportional to the strength of semantic association. The unsupervised method for measuring non-compositionality that we used is recent and uses vector spaces [21]. We extended it by adding a probabilistic representation that uses Kullback-Leibler divergence. We experimentally showed that all variants of our approach were effective in selecting which queries to treat as term dependent and resulted in gains for both early and deep precision (> 5%) with respect to a range of baselines (standard bag of words and competitive MRF with sequential and full dependence [31]).

In the future we plan to analyse the amount of non- or low-compositionality queries in real-life query logs, as opposed to TREC data. As discussed in Section 7, there may be more low-compositionality queries in those samples. We also intend to investigate optimal ways of measuring noncompositionality within a query, as opposed to considering the non-compositionality of a query as a whole as we did here. Another interesting direction is the direct mapping of the non-compositionality score of a query into the strength of its term dependence used during ranking. In this initial study we treated all queries selected as least-compositional in the same way as fixed phrases processing them identically; in doing so, we ignored their grades of non-compositionality. Modelling this may yield further improvements and is an interesting research question in its own right.
Acknowledgments.
Partially funded by the first author's FREJA research excellence fellowship (grant no. 790095).
9. REFERENCES
[1] T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows. An empirical model of multiword expression decomposability. In ACL Multiword Expressions Wksh., pages 89­96. 2003.
[2] C. Bannard, T. Baldwin, and A. Lascarides. A statistical approach to the semantics of verb-particles. In ACL Multiword Expressions Wksh., pages 65­72, 2003.
[3] P. B. Baxendale. Machine-made index for technical literature. IBM Journal for R&D, 2:354­361, 1958.
[4] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In WSDM, pages 31­40, 2010.
[5] M. Dillon and A. Gray. FASIT - a fully automatic syntactically based indexing system. JASIS, 34:99­108, 1983.
[6] L. B. Doyle. Indexing and abstracting by association. Part I. Am. Doc., 13:378­390, 1962.
[7] L. L. Earl. The resolution of syntactic ambiguity in automatic language processing. Information Storage and Retrieval, 8:277­308, 1972.
[8] K. Erk. Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6(10):635­653, 2012.
[9] D. A. Evans and C. Zhai. Noun-phrase analysis in unrestricted text for IR. In ACL, pages 17­24, 1996.
[10] J. L. Fagan. The effectiveness of a non syntactic approach to automatic phrase inducing for document retrieval. JASIS, 40:115­132, 1989.
[11] J. R. Firth. A synopsis of linguistic theory. Selected papers of J.R. Firth 1952-1959, pages 168­205, 1968.
[12] S. Fujita. More reflections on aboutness TREC-2001 evaluation experiments at Justsystem. In TREC, pages 331 ­ 338, 2001.
[13] W. Gale and G. Sampson. Good-Turing frequency estimation without tears. J. of Quant. Ling., 2(3):217­237, 1995.
[14] V. E. Giuliano and P. E. Jones. Linear associative IR. Vistas in Information Handling: The Augmentation of Man's Intellect by Machine, 1:30­54, 1963.
[15] D. J. Harper and C. J. K. van Rijsbergen. An evaluation of feedback in document retrieval using concurrence data. J. of Doc., 34:189­216, 1978.
[16] F. Heylighen and J. Dewaele. Variation in the contextuality of language. F. of Sci., 7(3):293­340, 2002.
[17] H. Jeffreys. Theory of Probability. Clarendon, 1948. [18] R. Jin, C. Falusos, and A. G. Hauptmann. Meta-scoring:
Automatically evaluating term weighting schemes in IR without precision-recall. In SIGIR, pages 83­89, 2001.

603

[19] D. Jurafsky and J. Martin. Speech and Language Processing. Pearson, 2009.
[20] G. Katz and E. Giesbrecht. Automatic identification of non-compositional multi-word expressions using LSA. In ACL Multiword Expressions Wksh., pages 12­19. 2006.
[21] D. Kiela and S. Clark. Detecting compositionality of multi-word expressions using nearest neighbours in vector space models. In EMNLP, pages 1427­1432, 2013.
[22] I. Korkontzelos and S. Manandhar. Detecting compositionality in multi-word expressions. In ACL-IJCNLP, pages 65­68, 2009.
[23] L. Krcma´r, K. Jezek, and P. Pecina. Determining compositionality of expresssions using various word space models and methods. In Continuous Vector Space Models and their Compositionality Wksh., pages 64­73, 2013.
[24] M. E. Lesk. Word-word associations in document retrieval systems. Am. Doc., 20:27­38, 1969.
[25] D. D. Lewis. An evaluation of phrasal and clustered representations on a text categorization task. In SIGIR, pages 37­50, 1992.
[26] D. D. Lewis and W. B. Croft. Term clustering of syntactic phrases. In SIGIR, pages 385­404, 1990.
[27] J. Lin. Indexing & Retrieving Natural Language Using Ternary Expressions. Master's thesis, U. of Maryland, USA, 2001.
[28] R. Losee. Term dependence: Truncating the Bahadur Lazarsfeld expansion. IPM, 30(2):293­303, 1994.
[29] Y. Lv and C. Zhai. Positional language models for information retrieval. In SIGIR, pages 299­306, 2009.
[30] D. McCarthy, B. Keller, and J. Caroll. Detecting a continuum of compositionality in phrasal verbs. In ACL Multiword Expressions Wksh., pages 73­80. 2003.
[31] D. Metzler and B. Croft. A MRF model for term dependencies. In SIGIR, pages 472­479, 2005.
[32] D. P. Metzler, T. Noreault, L. Richey, and P. B. Heidorn. Dependency parsing for information retrieval. In SIGIR, pages 313­324, 1984.
[33] L. Michelbacher, A. Kothari, M. Forst, C. Lioma, and H. Schu¨tze. A cascaded classification approach to semantic head recognition. In EMNLP, pages 793­803, 2011.
[34] G. Mishne and M. de Rijke. Boosting web retrieval through query operations. In ECIR, pages 502­516, 2005.
[35] J. Mitchell and M. Lapata. Composition in distributional models of semantics. Cognitive Science, 34(8):1388­1429, 2010.
[36] R. Nallapati and J. Allan. Capturing term dependencies using a language model based on sentence trees. In CIKM, pages 383­390, 2002.
[37] M. Narita and Y. Ogawa. The use of phrases from query texts in IR. In SIGIR, pages 318­320, 2000.
[38] P. Pantel and D. Lin. Document clustering with committees. In SIGIR, pages 199­206. ACM, 2002.
[39] J. Pederson, C. Silverstein, and C. Vogt. Verity at TREC-6: out-of-the-box and beyond. In TREC-6, pages 259 ­ 274, 1997.
[40] V. Plachouras and I. Ounis. Multinomial randomness models for retrieval with document fields. In ECIR, pages 28­39, 2007.
[41] S. Reddy, I. Klapaftis, D. McCarthy, and S. Manandhar. Dynamic & static prototype vectors for semantic composition. In IJCNLP, pages 705­713, 2011.
[42] S. Reddy, D. McCarthy, S. Manandhar, and S. Gella. Exemplar-based word-space model for compositionality detection: shared task system description. In DiSCo, pages 54­60. 2003.

[43] J. W. Reed, Y. Jiao, T. E. Potok, B. A. Klump, M. T. Elmore, and A. R. Hurson. TF-ICF: A new term weighting scheme for clustering dynamic data streams. In ICMLA, pages 258­263, 2006.
[44] B. Salehi and P. Cook. Predicting the compositionality of multiword expressions using translations in multiple languages. In SEM, pages 266­275. 2013.
[45] G. Salton. Automatic phrase matching. Readings in Automatic Language Processing, pages 169­188, 1966.
[46] G. Salton, C. Buckley, and C. T. Yu. An evaluation of term dependence models in information retrieval. In SIGIR, pages 151­173, 1982.
[47] S. Schulte im Walde, S. Mu¨ller, and S. Roller. Exploring vector space models to predict the compositionality of German noun-noun compounds. In SEM, pages 255­265. 2013.
[48] A. Singhal. AT&T at TREC-6. In TREC-6, pages 215­225, 1997.
[49] A. Smeaton and K. van Rijsbergen. Experiment on incorporation syntactic processing of user queries into a document retrieval strategy. In SIGIR, pages 31­51, 1988.
[50] A. F. Smeaton. Incorporating syntactic information into a document retrieval strategy: An investigation. In SIGIR, pages 103­113, 1986.
[51] F. J. Smith and K. Devine. Storing and retrieving word phrases. IPM, 21(3):215­224, 1985.
[52] F. Song and W. B. Croft. A general language model for IR. In CIKM, pages 316­321, 1999.
[53] K. Sp¨arck-Jones. A statistical interpretation of term specificity and its application in retrieval. J. of Doc., 28(1):132­142, 1972.
[54] K. Sp¨arck-Jones and J. Tait. Charting a New Course: MLP and IR: Essays in Honour of Karen Sp¨arck Jones. Springer, 2005.
[55] M. Srikanth and R. K. Srihari. Incorporating query term dependencies in language models for document retrieval. In SIGIR, pages 405­406. ACM, 2003.
[56] H. E. Stiles. The association factor in information retrieval. Journal of the ACM, 8:271­279, 1961.
[57] T. Strzalkowski and F. Lin. Natural language IR TREC-6 report. In TREC, pages 347 ­ 366, 1997.
[58] T. Tao and C. Zhai. An exploration of proximity measures in ir. SIGIR, pages 295­302. ACM, 2007.
[59] R. H. Thomason. Formal Philosophy. Selected Papers of Richard Montague. Yale University Press, 1974.
[60] H. Turtle and B. Croft. Evaluation of an inference network-based retrieval model. TOIS, 9(3):187­222, 1991.
[61] C. J. K. van Rijsbergen. A theoretical basis for the use of co-occurrence data in information retrieval. J. Doc., 33:106­119, 1977.
[62] V. Vinay, I. J. Cox, N. Milic-Frayling, and K. R. Wood. On ranking the effectiveness of searches. In SIGIR, pages 398­404, 2006.
[63] C. T. Yu, C. Buckley, K. Lam, and G. Salton. A generalised term dependence model in IR. Information Technology: R&D, 2:129­154, 1983.
[64] C. Zhai, X. Tong, N. Milic-Frayling, and D. A. Evans. Evaluation of syntactic phrase indexing - CLARIT NLP track report. In TREC-5, pages 347­358, 1997.
[65] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Comput. Surv., 38(2):6, 2006.

604

Assessor Differences and User Preferences in Tweet Timeline Generation

Yulu Wang1, Garrick Sherman2, Jimmy Lin1, and Miles Efron2
1 University of Maryland, College Park 2 University of Illinois, Urbana-Champaign {ylwang,jimmylin}@umd.edu, {gsherma2,mefron}@illinois.edu

ABSTRACT
In information retrieval evaluation, when presented with an effectiveness difference between two systems, there are three relevant questions one might ask. First, are the differences statistically significant? Second, is the comparison stable with respect to assessor differences? Finally, is the difference actually meaningful to a user? This paper tackles the last two questions about assessor differences and user preferences in the context of the newly-introduced tweet timeline generation task in the TREC 2014 Microblog track, where the system's goal is to construct an informative summary of non-redundant tweets that addresses the user's information need. Central to the evaluation methodology is humangenerated semantic clusters of tweets that contain substantively similar information. We show that the evaluation is stable with respect to assessor differences in clustering and that user preferences generally correlate with effectiveness metrics even though users are not explicitly aware of the semantic clustering being performed by the systems. Although our analyses are limited to this particular task, we believe that lessons learned could generalize to other evaluations based on establishing semantic equivalence between information units, such as nugget-based evaluations in question answering and temporal summarization.
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation
Keywords: TREC evaluation; microblog search; user study
1. INTRODUCTION
In response to information needs, search systems should strive to return as much relevant content as possible. However, there are other factors that systems might consider beyond topical relevance: often, users may not wish to see multiple pieces of text that "say the same thing"; frequently, users desire diverse results that cover multiple aspects of the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767699.

topic they are interested in. These considerations are particularly important in the context of searching tweets on Twitter. Due to the nature of the medium, there are frequently duplicate, near-duplicate, and highly-similar posts--for example, a breaking news event may be reported by multiple outlets at roughly the same time, each using slightly different language. In turn, these posts are retweeted, sometimes with additional commentary, copy-and-pasted, and discussed as part of a global conversation. This creates a cacophony of voices that obscures the underlying information content.
The Microblog track at TREC began in 2011 to explore information retrieval challenges in the context of social media services such as Twitter. The main task since 2011 has been temporally-anchored ad hoc retrieval ("At time T , give me the most relevant tweets about an information need expressed as query Q"). In response to the considerations above, the tweet timeline generation (TTG) task was introduced in 2014, where the goal is to construct a "timeline" of relevant and non-redundant tweets that best addresses the user's information need.
This paper presents a meta-evaluation of the TTG task from two perspectives: First, do assessor differences affect evaluation stability? Ultimately, TTG evaluation boils down to judgments about the semantic content of tweets, for which we would expect inter-assessor disagreements. Do these differences prevent us from drawing conclusions about the relative effectiveness of systems? Second, do user preferences correlate with our evaluation metrics? That is, are our metrics meaningful in being able to capture aspects of what users care about in timelines for making system comparisons? Our major findings are summarized as follows:
· We find that assessors do exhibit substantial differences in semantic clustering. However, these differences do not impact the stability of system comparisons.
· We find that user preferences correlate with metric differences, and for precision and unweighted F1, agreement increases as the magnitude of the difference increases.
Although this work focuses on a specific TREC task, tweet timeline generation is representative of a class of information retrieval evaluations based on identifying atomic units of information and establishing semantic equivalences between these units. Thus, we believe that lessons learned from our study can be applied to similar types of evaluations.
2. BACKGROUND AND RELATED WORK
At a high level, the tweet timeline generation task bears a family resemblance to topic detection and tracking (TDT)

615

[33, 3], "other" nuggets in question answering [32, 9, 8], nugget-based evaluations in DARPA's BOLT program,1 and temporal summarization at TREC [12, 5]. These tasks all share the insight that atomic units of information should be grouped together into semantic equivalence classes to provide more useful responses to users (we refer to this generically as "clustering"). In TDT the atomic units were documents, which were grouped into those discussing the same event. For question answering, nuggets were short phrases that provided interesting information about a target entity. In temporal summarization, sentences formed the atomic units that conveyed essential information about a particular event. These notions are also reflected in variants of ad hoc retrieval such as aspect retrieval [21] (or, alternatively, facet or sub-topic retrieval [34]) and result diversification [1, 28], although less explicitly.
The two biggest challenges of formulating such an evaluation are (1) defining the atomic unit of information and (2) defining semantic equivalence between those units. Once these two issues are resolved, computing a metric is reasonably straightforward, although variations abound. The first issue is more difficult than it initially seems if the atomic unit does not correspond to a logical entity such as a document. Experience from question answering evaluations has shown that users disagree about the granularity of nuggets--for example, whether a piece of text encodes one or more nuggets and how to treat partial semantic overlap between two pieces of text [20]. The solution that most evaluations adopt today is to simply declare the atomic information unit by fiat: in tweet timeline generation, tweets form the atomic units, and in temporal summarization [5], they are sentences. This sweeps many nuances under the rug, but yields workable evaluations in practice.
The second issue--semantic equivalence between atomic information units--is challenging because making such judgments requires taking into account context and fine-grained distinctions in meaning. In the same way that assessors disagree over relevance judgments (see [6] for a nice summary), humans also disagree about whether two pieces of text have the same semantic content. This issue is typically resolved by acknowledging these assessor differences and simply accepting the opinion of a single assessor. In the same way that Voorhees [31] demonstrated the stability of system rankings with respect to assessors' divergent document-level relevance judgments, the implicit assumption is that for these semantic clustering tasks, assessor differences don't matter if our goal is to induce a set of system comparisons.
To our knowledge, however, this assumption has never been validated at scale. For example, "ground truth" nuggets for question answering evaluations were generated by a single assessor (for each entity) [32, 9].2 Thus, it is unclear if an alternative set of nuggets (i.e., generated by another assessor) would have altered system rankings. A more recent iteration of the nugget-based evaluation methodology [22] has not to our knowledge examined assessor differences in the "nuggetization" process either. One of the major contributions of this paper is that, for tweet timeline generation,
1 http://www.nist.gov/itl/iad/mig/bolt_p1.cfm 2Subsequent "nugget pyramid" extensions [17, 9, 8] had multiple assessors assigning importance weights, but the underlying nuggets were still the same; similarly, researchers have studied how assessors identify nuggets in a piece of text [20], but the analyses were restricted to a single set of nuggets.

we devoted the resources necessary to answer this question by procuring two independent sets of semantic clusters for all topics in the test collection. Our results verify that system rankings are stable with respect to assessor differences in the TTG task, which gives us some confidence that the same results will carry over to similar tasks as well.
With respect to the second part of our paper--an exploration of user preferences in tweet timeline generation-- there is a long history of research that examines the correlation between effectiveness metrics from system-oriented evaluations with task-based metrics from user-oriented evaluations [13, 29, 4, 30, 14, 2, 26, 25, 27]. The broader goal of this thread of work is to understand when system effectiveness improvements are meaningful or useful in improving users' information seeking abilities in practice. Early studies suggest that "better" systems (as measured by systemoriented metrics) don't necessarily translate into better task performance [13, 29, 30]. Smith and Kantor [26] explained that users "do well" with poor search engines because they reformulate queries. Along these lines, Lin and Smucker [19] suggested that browsing can compensate for poor search results. However, more recent work has been able to detect a correlation between various system effectiveness metrics and human preferences [2, 25] (by using larger sample sizes) and between precision and user performance [27] (by more carefully controlling the experimental setup). These results give the community some degree of confidence that systemoriented evaluations can guide progress in producing more useful systems.
Our study follows the general setup of Sanderson et al. [25], i.e., asking users which of two system outputs they prefer. There are, however, a few important differences. Whereas their work examined ad hoc retrieval, we explore the more complex task of tweet timeline generation. Whereas they restricted system comparisons to those with "large" differences and "small" differences in effectiveness, we sampled our comparison conditions and analyzed the results in a way that allows us to characterize user sensitivity to different magnitudes of differences. Finally, whereas Sanderson et al. used judgments from Amazon's Mechanical Turk service, we took the route of training local assessors,
3. TWEET TIMELINE GENERATION
3.1 Task Definition
Tweet timeline generation (TTG) was introduced at the TREC 2014 Microblog track to supplement the existing ad hoc retrieval task. The putative user model is as follows: "At time T , I have an information need expressed by query Q, and I would like a summary that captures relevant information." The system's task is to produce a "summary" timeline, operationalized as a list of non-redundant, chronologically ordered tweets. It is imagined that the user would consume the entire summary (unlike a ranked list, where the user might stop reading at any time).
We conceived of a reference architecture in which a TTG module processes the output of an ad hoc retrieval system to generate the timeline. Thus, tweet timeline generation introduces two additional challenges beyond ad hoc retrieval:
· Systems must detect (and eliminate) redundant tweets. This is equivalent to saying that systems must detect if a tweet contains novel information.

616

Figure 1: Screenshot of the annotation interface. Tweets are presented one at a time in chronological order (bottom). For each tweet, the assessor can add it to an existing cluster or create a new cluster.
· Systems must determine how many tweets to return. Some topics have more relevant and non-redundant tweets than others and a system must be able to automatically infer this. Systems can make different precision/recall tradeoffs along these lines.
We operationalized redundancy as follows: for every pair of tweets, if the chronologically later tweet contains substantive information that is not present in the earlier tweet, the later tweet is considered novel; otherwise, the later tweet is redundant with respect to the earlier one. In our definition, redundancy and novelty are antonyms, and so we use them interchangeably, but in opposite contexts.
Due to the temporal constraint, redundancy is not symmetric. If tweet A precedes tweet B and tweet B contains substantively similar information found in tweet A, then B is redundant with respect to A, but not the other way around. We also assume transitivity. Suppose A precedes B and B precedes C: if B is redundant with respect to A and C is redundant with respect to B, then by definition C is redundant with respect to A. In this task setup, redundancy boils down to the definition of "contains substantively similar information", which is more precisely defined below.
3.2 Annotation Methodology
The TTG definition of redundancy and the assumption of transitivity means that the task can be viewed as semantic clustering--that is, we wish to group relevant tweets into clusters in which all tweets share substantively similar information. Within each cluster, the earliest tweet is novel; all other tweets in the cluster are redundant with respect to all earlier tweets.
Our annotation methodology builds exactly on this idea. We begin with a list of all relevant tweets, ordered chronologically, from earliest to latest. These tweets are presented, one at a time, to a human assessor. For each tweet, the assessor can add it to an existing cluster if she thinks the tweet contains substantively similar information with respect to tweets in the existing cluster, or she can create a new cluster for the tweet. We have developed a JavaScript-based annotation interface to help assessors accomplish this task. A screenshot is shown in Figure 1.

In the interface, the next tweet to be clustered is shown at the bottom of the screen. The assessor can either add the tweet to an existing cluster by clicking the "Add" button next to the cluster or create a new cluster by hitting the space bar. At any time, the assessor can expand a cluster to show all tweets contained in it, or collapse the cluster to show only the first tweet. The interface also implements an undo feature that allows the assessor to reverse the action taken and go back to the previous tweet.
The TTG evaluation methodology boils down to this central question: what exactly does "substantively similar information" mean? Like document relevance in ad hoc retrieval, assessors make the final determination and we expect natural variations among humans. However, pilot studies helped us devise a set of guidelines, which were provided as instructions to the assessors. We told them: a good rule of thumb is that if two tweets "say the same thing", then they're substantively similar. To speed up the clustering process, the annotators were asked not to consider external content (e.g., follow links in the tweets).
To provide further guidance, we devised a few questions that the assessors might consider in determining whether two tweets should be in the same cluster:
· If I had already seen the first tweet, would I have missed out on some information if I didn't see the second tweet?
· If two tweets are similar but the second contains an addition to or endorsement of the first, is the addition or endorsement important enough that I would be interested in seeing both tweets?
· Sometimes two tweets look similar but actually narrate the development of an event. Are the tweets different enough from each other that I would want to see both tweets to understand how an event develops or unfolds?
Since TTG was originally conceived as a stage following ad hoc retrieval, the track guidelines asked TTG participants to also submit ad hoc runs. For the evaluation, NIST assessors developed 55 topics and used a standard pooling methodology to generate tweet-level relevance judgments; see the track overview for more details [18]. Tweets in the pool were assigned one of three judgments: not relevant, relevant, and highly relevant. In the cluster annotation process, assessors at UMD and UIUC worked on the relevant and highly-relevant tweets from the NIST judgment pools. Due to resource constraints, NIST assessors were not able to perform the clustering, and thus a weakness of our setup is that the individual with the information need was not the one who created the clusters. The two assessors at UMD were graduate students in computer science (both male). The two assessors at UIUC were graduate students in library and information science (one male, one female). All were familiar with Twitter.
Assessors were first trained in the laboratory: the session included an introduction to the task and an overview of the annotation interface. After that, assessors were free to perform annotations at their own pace on their own machines, at any location of their choosing. This was possible because the annotation interface was implemented in JavaScript and hence accessible over the web. All assessors began with a throwaway "practice topic" (although they were not aware of the throwaway nature) and then proceeded to annotate topics in batches (roughly ten topics per batch). Topics were

617

Preicision vs. Unweighted Recall 1.0

Precision vs. Weighted Recall 1.0

0.8

0.8

0.6

0.6

Precision Precision

0.4 0.2 0.0
0.0

F1=0.05

F1=0.10

F1=0.45 F1=0.40 F1=0.1F51=0F.12FF=0110==.020.5.3305

0.2

0.4

0.6

0.8

1.0

Unweighted Recall

0.4 0.2 0.0
0.0

F1=0.05

F1=0.10

F1=0.55 F1=0.50 F1=0.45 F1=0.40 F1=0.1F51=0F.12FF=0110==.020.5.3305

0.2

0.4

0.6

0.8

1.0

Weighted Recall

Figure 2: Scatter plots showing precision vs. unweighted recall (left) and precision vs. weighted recall (right) for all submitted runs in the TREC 2014 TTG task, overlaid with iso-F1 contours.

grouped into batches of roughly equal size (in terms of the number of relevant tweets) prior to the beginning of the annotation process. When an assessor completed a batch, he or she could request another batch to work on.
Each site annotated the topic batches in the opposite order, and when a covering set for all topics had been obtained, we designated those clusters to be the "official" judgments. However, the annotation process continued until both sites had processed all topics, giving us alternate judgments. Thus, both the official and alternate clusters contained annotations generated from both sites.
3.3 Metrics and Results
The output of the human annotation process is an ordered list of tweet clusters. Within each cluster, the tweets are sorted by temporal order (earliest to latest). The clusters themselves are sorted by the temporal order of their earliest tweet. Following the heuristic of using the most straightforward metric when defining a new task (and then subsequently refining the metric as needed), we decided to measure cluster-based precision and recall. The measure is cluster-based in the sense that systems only receive credit for returning one tweet from each cluster--that is, once a tweet is retrieved, all other tweets in the cluster are automatically considered not relevant. From this, we can compute precision, recall, and F-score in the usual way (lacking any basis for setting the  parameter, we simply computed F1). Since the user model assumes that the user will consume the entire summary, set-based metrics seemed appropriate.
The only additional refinement is that we computed both weighted and unweighted variants of recall. In weighted recall, each cluster is assigned a weight proportional to the sum of relevance grades from every tweet in the cluster (relevant tweets receive a weight of one and highly-relevant tweets receive a weight of two). This weighting scheme implements the heuristic that larger clusters and those containing more highly-relevant tweets are more important, and the denominator in the weighted recall computation is the sum of all cluster weights. In unweighted recall, all clusters are considered equally important, and the denominator is simply the total number of clusters.
Note that this setup gives equal credit to retrieving any tweet from a cluster. Intuitively, however, this seems overly

simplistic--users would certainly prefer seeing certain tweets over others, even if they contain substantively similar information [15]. For example, users might prefer the earliest tweet, a tweet from the most "authoritative" user (e.g., a verified news account), or a tweet from someone close by in their network (e.g., a tweet from someone they follow). We currently do not have sufficient understanding to accurately model such preferences, and thus explicitly made the decision not to tackle this challenge.
The evaluation metrics for TTG represent straightforward extensions of previous work: aspect recall [21], sub-topic recall [34], and the "nugget pyramid" approach from the TREC question answering evaluations [17]. Alternative metrics we had considered include those based on gain [7, 5] and the extension of mean average precision to graded relevance judgments [24]. The challenge with gain-based approaches is the complex parameterization necessary to fully instantiate a particular model; we lacked the empirical data to properly develop such a model. The graded extension to mean average precision is elegant, but our underlying user model is better captured by set-based metrics.
In total, 13 groups submitted 50 runs to the tweet timeline generation task at TREC 2014. Recognizing that systems make different choices with respect to balancing precision and recall, it is illustrative to visualize the tradeoffs in a scatter plot. Figure 2 shows precision vs. unweighted recall (left) and precision vs. weighted recall (right) for all runs. Iso-F1 contours are plotted in blue; points on the same contour line have the same F1 score, but with different precision/recall tradeoffs. Note that the effectiveness of individual runs is not relevant for the purposes of our meta-analysis, so we refer readers to the track overview for additional details [18].
4. ASSESSOR DIFFERENCES
Our first research question revolves around assessor differences in the semantic clustering task for tweet timeline generation and their impact on evaluation stability. For the TREC 2014 evaluation, we devoted the necessary resources to generate two independent sets of reference clusters, which we refer to as the "official" and "alternate" judgments. Note that the official judgments were simply the ones obtained first and used to report evaluation results at TREC; there is no implication that they are somehow more "authoritative".

618

Cluster Number

MB179: Official Judgments 20 15 10

MB225: Official Judgments 20
15
10

Cluster Number

5

5

10

20

30

40

50

60

Day of Collection

MB179: Alternate Judgments

10

20

30

40

50

60

Day of Collection

MB225: Alternate Judgments

20 20

15

15

10

10

Cluster Number

5

5

10

20

30

40

50

60

Day of Collection

10

20

30

40

50

60

Day of Collection

Cluster Number

Figure 3: Visualization of the clusters for two topics.

4.1 Descriptive Characterization
We begin with a descriptive characterization of the semantic clusters generated by the assessors. In this and subsequent analyses, relevant and highly-relevant tweets are both considered "relevant". Each topic (55 in total) contains 194 relevant tweets on average: the official judgments averaged 89 clusters per topic, while the alternate judgments averaged 73 clusters per topic. These differences suggest that humans perform the semantic clustering task at different levels of granularity.
In Figure 3 we attempt to visualize these differences. In each plot, a point represents a tweet, and its x-axis position denotes the time when it was posted. Tweets that were assigned to the same cluster are at the same y-axis position and connected by horizontal lines. Thus, each horizontal "level" represents a semantic cluster, ordered by the first tweet; each cluster is assigned a different color for clarity. On the left, we show topic 179 "care of Iditarod dogs" and on the right, we show topic 225 "Barbara Walters, chicken pox". The top row shows the official judgments and the bottom row shows the alternate judgments. These topics were selected primarily for visual clarity: enough relevant tweets to show interesting clusters, but not too many relevant tweets as to be overly cluttered. From these visualizations, it is apparent that there are substantial assessor differences in the formation of the clusters. For example, the alternate assessor created fewer clusters for topic 179, and in topic 225, the alternate assessor did not seem to agree with two early clusters found by the official assessor.
To quantitatively characterize the differences between the official and alternate clusters, we computed the Adjusted Rand Index [23], which is a measure of similarity between two clusterings that is corrected for chance groupings (ranging from -1 to +1.). This metric has two other desirable properties: first, it is symmetric, which is appropriate since the official judgments aren't any more "correct" than the alternate judgments; second, it ignores permutations in that the similarity values don't depend on the "cluster labels" (which, in our case, are just arbitrary numeric identifiers). We computed the Adjusted Rand Index on a per topic basis (N = 55) between the official and alternate judgments and obtained a mean of 0.445, a median of 0.492, and a standard

Correlation: Precision

0.8

Official

0.7

Alternate

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0

10

20

30

40

50

Correlation: Unweighted Recall

Correlation: Weighted Recall

0.8

Official

0.8

Official

0.7

Alternate

0.7

Alternate

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0.0

0.0

0

10

20

30

40

50

0

Correlation: Unweighted F1

0.8

Official

0.8

0.7

Alternate

0.7

0.6

0.6

10

20

30

40

50

Correlation: Weighted F1

Official Alternate

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0.0

0.0

0

10

20

30

40

50

0

10

20

30

40

50

Figure 4: Comparison between scores based on the official judgments and the alternate judgments for various metrics. Runs are sorted by score based on the official judgments in descending order.

deviation of 0.225. As a point of reference, for topic 179 (left, Figure 3), the Adjusted Rand Index is 0.687, and for topic 225 (right, Figure 3), the Adjusted Rand Index is 0.305. We observe that the topics exhibit a wide range of similarity values, ranging from a minimum of 0.007 to a maximum of 0.977, which suggests that agreement is to a large extent dependent on the nature of the information need. However, it would be fair to say that there is substantial disagreement between assessors overall.
4.2 Stability Analysis
With the two independent sets of judgments, we can conduct a stability analysis of the evaluation to determine the extent to which assessor differences impact our ability to make system comparisons, i.e., that system X is more effective than system Y . An evaluation is considered stable if system rankings and pairwise system comparisons are insensitive to assessor differences. Here, we follow the wellestablished methodology of Voorhees [31], who examined assessor differences in document-level relevance judgments for ad hoc retrieval.
Figure 4 shows scores for all runs based on the official judgments and the alternate judgments for each of the five metrics. Results are sorted by scores based on the official judgments. We see that the rankings produced by both sets of judgments are highly correlated, with the exception of a few cases in weighted F1. Furthermore, the absolute values of the metrics are also quite similar (particularly the unweighted metrics).

619

Metric
precision unweighted recall
weighted recall unweighted F1
weighted F1

Rank Swaps
30 27 22 46 90

Kendall's 
0.951 0.956 0.964 0.925 0.853

Table 1: Count of rank swaps and Kendall's  correlation based the official and alternate judgments for each metric.

Rank Swap Count Rank Swap Count

Histogram of Rank Swaps: Unweighted F1
40 35 30 25 20 15 10 5 0

Histogram of Rank Swaps: Weighted F1
40 35 30 25 20 15 10 5 0

(0, 0.01] (0.01, 0.02] (0.02, 0.03] (0.03, 0.04] (0.04, 0.05] (0.05, 0.06] (0.06, 0.07]
(0, 0.01] (0.01, 0.02] (0.02, 0.03] (0.03, 0.04] (0.04, 0.05] (0.05, 0.06] (0.06, 0.07]

Figure 5: Histogram of rank swaps for unweighted F1 and weighted F1, binned by score differences.
In Table 1, we show the Kendall's  correlation between rankings induced by the two different sets of judgments. These values are in the same range as those reported by Voorhees [31] for ad hoc retrieval, which is generally regarded by the IR community to be stable with respect to assessor differences in document-level relevance judgments. Table 1 also shows the number of rank swaps for each metric. A rank swap is a pairwise comparison where, according to one set of judgments, run A scores higher than run B, but according to the other set of judgments, run B scores higher than run A. There are a total of (50 × 49)/2 = 1225 pairwise comparisons, so the numbers of rank swaps observed in Table 1 are quite small.
A tally of the rank swaps does not tell the complete story because rank swaps do not capture the magnitude of the score differences. In particular, we are less concerned with rank swaps in which the absolute score differences between the two conditions are small. Histograms of the rank swaps for unweighted F1 and weighted F1 binned by absolute score differences are shown in Figure 5. We see that, indeed, most of the rank swaps are small. For space considerations, we only show the histograms for these two metrics, which have the lowest Kendall's  correlations. The histograms for the other metrics show even larger fractions of rank swaps where the score differences are small.
The conclusion from these analyses is fairly clear: assessor differences do not appear to impact the stability of the evaluation, at least in terms of the metrics we have examined. Although our findings are limited to tweet timeline generation, there is no reason to believe that these results would not carry over to other types of evaluations built around the notion of semantic clustering.
5. USER PREFERENCES
The second major research question we tackle in this paper concerns user preferences: when comparing TTG sys-

tems, do our metrics capture differences that are actually meaningful to users? Following previous work, we operationalize the notion of "meaningful" in terms of user preferences [2, 25]. If the evaluation tells us that system X is better than system Y , and a human can actually detect this difference (better than chance), we might reasonably claim that the improvement is meaningful from a user perspective.
5.1 Analysis Methodology
Unlike most studies discussed in Section 2, which focus on ad hoc retrieval (and variants), our analyses have a more complex setup because tweet timelines are variable in length. We are specifically interested in three distinct types of effectiveness differences:
· For two systems that exhibit roughly the same recall, how sensitive are users to differences in precision?
· For two systems that exhibit roughly the same precision, how sensitive are users to differences in recall?
· For two systems that exhibit similar precision-recall tradeoffs, how sensitive are users to differences in F1?
Our general strategy was to sample system output from the submitted TREC 2014 TTG runs and to generate system comparisons on a per-topic basis for user preference assessment. Unlike some previous work that artificially manipulated system output to generate results of a particular level of effectiveness (e.g., [4, 30]), our procedure yields more realistic comparisons.
The sampling process proceeded as follows: First, we selected a set of 30 topics, biased toward "typical" topics that have neither too many nor too few relevant tweets. Let xi = ri - r¯ be the difference between ri, the number of relevant tweets for topic i, and r¯, the median number of relevant tweets over all topics: the probability of "drawing" topic i is proportional to the density at xi of a normal distribution with µ = 0 and  = 20 (chosen heuristically).
Next, we performed some filtering of the submitted runs: we discarded all runs that contained unjudged tweets to eliminate the effects of missing relevance judgments. We also discarded all runs longer than 41 tweets, which is the median length of submitted runs (after the first filter). A pilot study indicated that long runs are very difficult to judge, and this is roughly the point after which the judgments become too onerous to make.
After filtering, we randomly (i.e., uniformly) selected 20 "base" runs, and for each, sampled up to 20 different "comparison" runs per metric based on the following criteria:
· For what we call precision sampling, we selected runs that differed by less than 0.1 in recall, but more than 0.1 in precision. Previous work suggests that users have a hard time distinguishing small differences in effectiveness metrics, so we did not want to waste assessor effort.
· For what we call recall sampling, we selected runs that differed by less than 0.1 in precision, but more than 0.1 in recall. This is the opposite of precision sampling.
· For what we call F1 sampling, we want the two comparison runs to make approximately the same tradeoff in terms of precision and recall. We selected runs where the value of T = |P - R| for the base run is within 0.1 of the T for the comparison run, as long as the difference in either precision or recall between the two runs exceeds 0.1.

620

Figure 6: A screenshot of the web-based assessment interface for eliciting preference judgments. System outputs are presented in the left and right columns, with shared content in the middle column.
Note that all sampling was performed on the cluster-based TTG metrics, i.e., taking into account redundancy. We used the unweighted variants of the metrics for simplicity. Finally, we discarded all pairs where the two sampled runs differ in length by more than 20 tweets. Based on the pilot study, we found that assessors struggled to compare runs that differed significantly in length.
Each trial of the above sampling procedure yields a large number of comparisons, from which we further sampled 180 pairs comprised of roughly an equal number of pairs from precision, recall, and F1 sampling. This represents a single batch that we gave to assessors to judge; assessors could request additional batches if they desired after completing a batch. The first batch for each assessor contained a shared set of 30 comparisons (i.e., all assessors judged the same pairs), but for the remaining pairs in the first batch (and all subsequent batches), each assessor worked on unique pairs, which we ensured in the batch preparation process.
For eliciting user preference judgments, we designed a web-based assessment interface (see screenshot in Figure 6). The comparison screen displays the sampled TTG outputs for a given topic side by side: System #1 on the left and System #2 on the right. Sampled pairs were mapped to the two positions randomly to avoid introducing any systematic biases. The topic is shown at the top. The output of each system is ordered chronologically, and each tweet is displayed with its timestamp. To help the assessor compare the two timelines, tweets returned by both systems are displayed in the middle "Shared" column. This means that an output might have gaps, indicating that one system returned more tweets than the other prior to the shared tweets (as is the case in Figure 6). We converged on this design after trying various display alternatives in a pilot study--we learned that assessors wanted an easy way to compare the two systems in terms of the tweets they both returned, so we designed the interface to facilitate this comparison.
In the assessment instructions, we asked the assessors which of the two system outputs they thought was better. They were reminded to evaluate system outputs as timelines, rather than as ranked lists. To be consistent with the clustering task, assessors were told to evaluate tweets based solely on their contents, ignoring any links they may contain. The assessor could click "Prefer System #1" or "Prefer

System #2" above each of the two conditions, to indicate a preference, or a button labeled "Prefer Neither" in the middle if the assessor could not decide.
We avoided prescriptively dictating what it meant for a timeline to be "good". Specifically, the instructions made no reference to precision, recall, or redundancy. This is an important feature in our evaluation design to mitigate the effects of demand characteristics.3 However, we tried to help the assessors frame their task with a few neutral reminders:
· We are asking you about relative quality. Even if you feel that both systems do a bad job, do your best to determine which does a better job.
· The tweets are presented in chronological order (earliest first), so you should keep in mind the timeframe covered by each result.
· The timelines are displayed with a "shared" column in the middle. This column contains all tweets that were returned by both systems, and should help you more quickly determine how similar the results are. Since the timelines are aligned by their shared tweets, this middle column will also help you compare the results according to chronological blocks.
Assessors were first trained in the laboratory. They were given an introduction to the task and an explanation of the web interface, which included time spent with practice topics that were discarded for analysis. After this training session, the assessors could proceed at their own pace anywhere they had an internet connection. The server that hosted the web assessment interface recorded all interactions: both the preference judgments and their timestamps.
Evaluation proceeded concurrently at UMD and UIUC. Judgments at UMD were performed by two male computer science graduate students. One of these assessors completed two batches of comparisons, while the other completed one. The assessors at UIUC were two graduate students in library and information science (one male, one female) and one former library and information science graduate student (male) who was working nearby. One of the assessors completed two batches, while the remaining assessors completed one each. These assessors were not the same as the annotators who created the clusters (at both sites).
5.2 Results
After the assessments were completed, the user preferences were correlated against the preferences implied by the sampled metric, using Cohen's  as the agreement metric. Cohen's  ranges from -1 to +1, where 0 indicates that any agreement is attributable to chance. To compute , we first discarded "prefer neither" judgments. In the precision sampling case, we correlated the human preferences against the implied preference based on the precision scores. In the recall sampling case, we correlated the human preferences against the implied preference based on both unweighted recall and weighted recall. In the F1 sampling case, we correlated the human preferences against the implied preference based on unweighted F1 and weighted F1.
Figure 7 shows these correlations binned by differences in the sampled metric, aggregated across all assessors. The
3An experimental artifact where participants form an interpretation of the experiment's purpose and unconsciously change their behavior to fit that interpretation.

621

Precision

Cohen's Kappa
-0.5 0.0 0.5 1.0

0 0 18 106 11 112 8 83 22 141

[0, 0.1) [0.1, 0.2) [0.2, 0.3) [0.3, 0.4) [0.4, 1)

Metric difference

Unweighted Recall

Weighted Recall

Cohen's Kappa
-0.5 0.0 0.5 1.0

Cohen's Kappa
-0.5 0.0 0.5 1.0

0 0 34 163 15 79 3 40 8 28
[0, 0.1) [0.1, 0.2) [0.2, 0.3) [0.3, 0.4) [0.4, 1)
Metric difference Unweighted F1

19 89 27 109 2 35 1 25 13 50
[0, 0.1) [0.1, 0.2) [0.2, 0.3) [0.3, 0.4) [0.4, 1)
Metric difference Weighted F1

Cohen's Kappa
-0.5 0.0 0.5 1.0

Cohen's Kappa
-0.5 0.0 0.5 1.0

28 82 25 123 16 85 1 40 0 3
[0, 0.1) [0.1, 0.2) [0.2, 0.3) [0.3, 0.4) [0.4, 1)
Metric difference

26 61 25 101 12 79 5 53 2 39
[0, 0.1) [0.1, 0.2) [0.2, 0.3) [0.3, 0.4) [0.4, 1)
Metric difference

Figure 7: Cohen's  for each metric binned by differences in the sampled metric. Error bars show 95% confidence intervals. The numbers in the bottom left of each bar show the number of "prefer neither" while those in the bottom right show the number of preference judgments in that condition.

error bars show the 95% confidence intervals, computed following Fleiss et al. [11]. The numbers in the bottom of each bar show the number of "prefer neither" (on the left) and the number of preference judgments, i.e., users preferred one system over the other (on the right). Note that by the design of the sampling procedure, no comparisons fell into the [0, 0.1) bucket for precision and unweighted recall, although some samples did fall into that bucket for the other metrics.
In Figure 8, we show agreement broken down by each individual assessor in a "small multiples" visualization scheme. Each row shows a particular metric, and each column shows the agreement for an individual assessor for that metric. For clarity, the bar charts are shown without labels, but they are organized in exactly the same manner as in Figure 7. Note that confidence intervals here are much larger because fewer samples fall into each bin.
In both Figures 7 and 8, the bars are arranged by putative difficulty of the preference judgments, from left to right. Since there were generally fewer samples in which the metric differed by more than 0.4, all difference greater than 0.4 were placed in the same bin. The leftmost bar represents those runs where the metric difference is less than 0.1, for which we would expect humans to have the most difficulty distinguishing differences in output quality. Similarly, the rightmost bar should represent the "easiest" comparisons, in that the metric differences are the largest. We can intuitively think of these charts as quantifying how "sensitive" users are to differences in the underlying system-oriented metrics. Put another way, they tell us how much "better" or "worse" a system would have to be in order for users to notice.
Overall, we do see a general trend of the agreement increasing from left to right, both at the aggregate and at the individual level. At the aggregate level, we see that there tend to be more "prefer neither" judgments in cases where

 <0.00 0.00-0.20 0.21-0.40 0.41-0.60 0.61-0.80 0.81-1.00

Strength of Agreement Poor Slight Fair
Moderate Substantial Almost perfect

Table 2: Interpretations of  from [16].

Metric
precision unweighted recall
weighted recall unweighted F1
weighted F1

F-value
F(3, 16) = 3.397 F(3, 16) = 1.749 F(4, 20) = 1.475 F(3, 16) = 5.093 F(4, 20) = 1.322

p-value
0.0437 0.1970 0.2470 0.0115 0.2960

Table 3: ANOVA results for each metric. Metrics displayed in bold show a statistically significant difference (p < 0.05) in mean  across bins.

the metric difference is small, which makes sense. However, there are clearly individual differences that buck the overall trend, as shown in Figure 8. For example, assessor 4 (fourth column from the left) does not appear to be sensitive to precision at all--agreement remains near and sometimes worse than chance, even for very large differences in precision.
One of the downsides of  is that it lacks an intuitive interpretation. To address this, Landis and Koch [16] proposed a guide, which we replicate in Table 2. Under this rubric, the minimum  values we observe per metric range from "poor" (for precision) to "fair" (for weighted F1), whereas the maximum  values we observe for each metric range from "fair" (for precision) to "almost perfect" (for unweighted and weighted F1). The differences between maximum and minimum  values per metric support the apparent relationship between metric differences and annotator preferences.
It is worth emphasizing here that all our metrics already take into account redundancy (i.e., they are cluster-based). Since we did not explicitly ask the assessors to consider redundancy in the assessment instructions, these results show that humans are nevertheless sensitive to the redundancy removal performed by the systems.
To add statistical rigor to our analyses about the relationship between agreement and metric differences, we performed analysis of variance (ANOVA) comparing the  values across bins. That is, we treat the  of each individual assessor for a particular bin as a point estimate of the "true"  for that bin (magnitude of metric difference), and want to know if the mean  values across the bins are significantly different. The results are shown in Table 3. We found statistically significant differences in mean  across bins for precision and unweighted F1 at p < 0.05. From this, we can conclude that increases in metric differences are associated with better agreement for those metrics.
A surprising result is the lack of statistical significance for both unweighted and weighted recall. What does this mean? The recall bar charts in Figure 7 show that differences in recall between pairs of runs are detectable by annotators; however, the magnitude of the difference does not appear to affect the agreement, i.e., the  values are not significantly different across the bins. We believe that this is due to the nature of evaluating recall, which requires knowledge of the

622

Weighted F1 Unweighted F1 Weighted Recall Unweighted Recall Precision

Figure 8: Cohen's  for each metric (row), for each individual assessor (column) arranged in "small multiples". Each bar chart is organized in the same manner as those in Figure 7.

complete relevance judgments. When comparing two runs, it is relatively easy to detect recall differences (i.e., if one system returned a relevant tweet that was absent from the other system), but without access to all the relevance judgments, it is difficult to tell how much better one is than the other (i.e., the marginal recall increase from returning one more relevant tweet is dependent on the number of relevant tweets for that topic). Note that in contrast, for precision, annotators do appear capable of detecting the magnitude of the difference. This is again unsurprising since precision can be assessed directly from the displayed tweets.
We were surprised at the high p-value for weighted F1, particularly in comparison to unweighted F1. This result indicates that although assessors consistently achieved better than chance agreement, they either had difficulty assessing the importance of individual tweets or our weighting scheme does not accurately capture users' notion of importance. The second possibility points to future work in developing more meaningful metrics for modeling importance.
Figure 7 shows another surprising result: we observe relatively low agreement for precision (compared to the other metrics). We believe that this may be an artifact of our sampling strategy, where we discarded overly verbose runs. This choice was made out of necessity, as our pilot study suggested that it was not feasible to ask assessors to compare timelines with, say, hundreds of tweets. The problem is this: while it is certainly possible to obtain low precision in a short timeline, the amount of "pain" associated with such low precision timelines is relatively low (bounded by the total number of returned tweets). However, if a timeline were an order of magnitude longer, say, the burden imposed by poor precision would be much heavier. In other words,

our sampling procedure selected only those runs where differences in precision were "not a big deal" from the assessor's point of view because the timelines were relatively short.
We also analyzed the relationship between the time assessors spent on each comparison (reconstructed from our logs) and the observed agreement. This is shown in Figure 9, binned in intervals of 30 seconds for each metric. The numbers at the top of each bar show the number of "prefer neither" and preference judgments. "Prefer neither" judgments were not included in the agreement calculation. Missing bars indicate an undefined  while dashes indicate a  of zero. We see no clear relationship between  and the amount of time it takes to make a judgment; this is somewhat surprising as we would have expected "easy judgments" to result in higher agreement.
Finally, we analyzed agreement on the set of 30 shared comparisons provided to each of the five annotators at the beginning of the first batch. We calculated Fleiss'  [10], a variant of Cohen's  intended for more than two raters, and found a value of 0.294 for this shared set. Under Landis and Koch's scheme, this represents only a fair level of agreement among annotators. This finding is noteworthy because it indicates that user preferences correlate with metric differences, despite the fact that humans only achieve "fair" agreement among themselves.
Summarizing these results, our user study suggests that user preferences do agree with system preferences as measured by the TTG metrics--that users are sensitive to differences in precision, recall, and F1, although to different degrees. We can therefore conclude that system-oriented metrics are meaningful in being able to capture aspects of what users care about in timeline summaries.

623

Cohen's Kappa
-0.5 -0.1 0.3

Precision

1/

44/ 6/ 30

276 73

1/6

3/

13 0/2 2/2 0/3 1/2 0/6 0/2

[0, 3[300), 6[600),[9900,[)112200,[)115500,[)118800,[)221100,[)224400,[)227700,[)330000,)600)

Time spent (seconds)

Unweighted Recall

Weighted Recall

6/

26/ 164

13/ 67

23

6/

10

1/3 0/4 1/ 10

2/5 1/2 0/3

2/2

27/

13/

7/ 22

6/

163 67

10

1/3 1/

0/4

10

2/5 1/2 0/3

2/2

Cohen's Kappa
-0.8 0.0 0.6

Cohen's Kappa
-0.8 0.0 0.6

Cohen's Kappa
-0.1 0.3 0.7

[0, 3[300), 6[600),[9900,[)112200,[)115500,[)118800,[)221100,[)224400,[)227700,[)330000,)600)
Time spent (seconds)

Unweighted F1

5/

1/4

23/ 26/ 80 167

20

4/ 12

0/ 13

6/8

1/6

1/7

2/1 0/2

[0, 3[300), 6[600),[9900,[)112200,[)115500,[)118800,[)221100,[)224400,[)227700,[)330000,)600)

Time spent (seconds)

Cohen's Kappa
-0.1 0.3 0.7

[0, 3[300), 6[600),[9900,[)112200,[)115500,[)118800,[)221100,[)224400,[)227700,[)330000,)600)
Time spent (seconds)

Weighted F1

5/

1/4

26/ 23/ 167 80

20

4/ 12

0/ 13

6/8

1/6

1/7

2/1 0/2

[0, 3[300), 6[600),[9900,[)112200,[)115500,[)118800,[)221100,[)224400,[)227700,[)330000,)600)

Time spent (seconds)

Figure 9: Cohen's  binned by time spent on each comparison (at 30 second intervals). Numbers at the top of each bar show the number of "prefer neither" and preference judgments. Missing bars indicate an undefined  while dashes indicate a  of zero.

6. CONCLUSIONS
As primarily an empirical discipline, progress in information retrieval is built on system comparisons. This paper explores assessor differences and user preferences in the context of the tweet timeline generation task in the TREC 2014 Microblog track. Analyses show that the evaluation methodology appears to be sound, which strengthens our confidence in existing and future results. Although our findings are limited to this particular task, we believe that lessons learned could generalize to other "cluster-based" evaluations, informing related tasks such as question answering and temporal summarization.

7. ACKNOWLEDGMENTS
This work was supported in part by the National Science Foundation under IIS-1217279 and IIS-1218043. Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsor. We are grateful to Ellen Voorhees and the assessors at NIST for making TREC possible.

8. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. WSDM, 2009.
[2] A. Al-Maskari, M. Sanderson, P. Clough, and E. Airio. The good and the bad system: Does the test collection predict users' effectiveness? SIGIR, 2008.
[3] J. Allan. Topic Detection and Tracking: Event-Based Information Organization. Kluwer, 2002.
[4] J. Allan, B. Carterette, and J. Lewis. When will information retrieval be "good enough"? User effectiveness as a function of retrieval accuracy. SIGIR, 2005.
[5] J. Aslam, M. Ekstrand-Abueg, V. Pavlu, R. McCreadie, F. Diaz, and T. Sakai. TREC 2014 temporal summarization track overview. TREC, 2014.

[6] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. de Vries, and E. Yilmaz. Relevance assessment: Are judges exchangeable and does it matter? SIGIR, 2008.
[7] C. Clarke, M. Kolla, G. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. SIGIR, 2008.
[8] H. Dang and J. Lin. Different structures for evaluating answers to complex questions: Pyramids won't topple, and neither will human assessors. ACL, 2007.
[9] H. Dang, J. Lin, and D. Kelly. Overview of the TREC 2006 question answering track. TREC, 2006.
[10] J. Fleiss. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378­382, 1971.
[11] J. Fleiss, J. Cohen, and B. Everitt. Large sample standard errors of kappa and weighted kappa. Psychological Bulletin, 72(5):323­327, 1969.
[12] Q. Guo, F. Diaz, and E. Yom-Tov. Updating users about time critical events. ECIR, 2013.
[13] W. Hersh, A. Turpin, S. Price, B. Chan, D. Kramer, L. Sacherek, and D. Olson. Do batch and user evaluations give the same results? SIGIR, 2000.
[14] S. Huffman and M. Hochster. How well does result relevance predict session satisfaction? SIGIR, 2007.
[15] J. Hurlock and M. Wilson. Searching Twitter: Separating the tweet from the chaff. ICWSM, 2011.
[16] J. Landis and G. Koch. The measurement of observer agreement for categorical data. Biometrics, 33(1):159­174, 1977.
[17] J. Lin and D. Demner-Fushman. Will pyramids built of nuggets topple over? NAACL, 2006.
[18] J. Lin, M. Efron, Y. Wang, and G. Sherman. Overview of the TREC-2014 Microblog track. TREC, 2004.
[19] J. Lin and M. Smucker. How do users find things with PubMed? Towards automatic utility evaluation with user simulations. SIGIR, 2008.
[20] J. Lin and P. Zhang. Deconstructing nuggets: The stability and reliability of complex question answering evaluation. SIGIR, 2007.
[21] P. Over. TREC-6 interactive report. TREC, 1997.
[22] V. Pavlu, S. Rajput, P. Golbus, and J. Aslam. IR system evaluation using nugget-based test collections. WSDM, 2012.
[23] W. Rand. Objective criteria for the evaluation of clustering methods. JASA, 66(336):846­850, 1971.
[24] S. Robertson, E. Kanoulas, and E. Yilmaz. Extending average precision to graded relevance judgments. SIGIR, 2010.
[25] M. Sanderson, M. Paramita, P. Clough, and E. Kanoulas. Do user preferences and evaluation measures line up? SIGIR, 2010.
[26] C. Smith and P. Kantor. User adaptation: Good results from poor systems. SIGIR, 2008.
[27] M. Smucker and C. Jethani. Human performance and retrieval precision revisited. SIGIR, 2010.
[28] K. Tao, C. Hauff, and G.-J. Houben. Building a microblog corpus for search result diversification. AIRS, 2013.
[29] A. Turpin and W. R. Hersh. Why batch and user evaluations do not give the same results. SIGIR, 2001.
[30] A. Turpin and F. Scholer. User performance versus precision measures for simple search tasks. SIGIR, 2006.
[31] E. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. SIGIR, 1998.
[32] E. Voorhees. Overview of the TREC 2004 question answering track. TREC, 2004.
[33] C. Wayne. Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation. LREC, 2000.
[34] C. Zhai, W. Cohen, and J. Lafferty. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. SIGIR, 2003.

624

User Variability and IR System Evaluation

Peter Bailey
Microsoft, Australia
pbailey@microsoft.com
Falk Scholer
RMIT University, Australia
falk.scholer@rmit.edu.au

Alistair Moffat
The University of Melbourne, Australia
ammoffat@unimelb.edu.au
Paul Thomas
CSIRO, Australia
paul.thomas@csiro.au

ABSTRACT
Test collection design eliminates sources of user variability to make statistical comparisons among information retrieval (IR) systems more affordable. Does this choice unnecessarily limit generalizability of the outcomes to real usage scenarios? We explore two aspects of user variability with regard to evaluating the relative performance of IR systems, assessing effectiveness in the context of a subset of topics from three TREC collections, with the embodied information needs categorized against three levels of increasing task complexity. First, we explore the impact of widely differing queries that searchers construct for the same information need description. By executing those queries, we demonstrate that query formulation is critical to query effectiveness. The results also show that the range of scores characterizing effectiveness for a single system arising from these queries is comparable or greater than the range of scores arising from variation among systems using only a single query per topic. Second, our experiments reveal that searchers display substantial individual variation in the numbers of documents and queries they anticipate needing to issue, and there are underlying significant differences in these numbers in line with increasing task complexity levels. Our conclusion is that test collection design would be improved by the use of multiple query variations per topic, and could be further improved by the use of metrics which are sensitive to the expected numbers of useful documents.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--performance evaluation.
Keywords
User behavior, test collections, relevance measures
1. INTRODUCTION AND BACKGROUND
In the Cranfield and TREC paradigm, information retrieval test collections (consisting of a corpus, topics, relevance judgments, and
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '15, August 09­13, 2015, Santiago, Chile Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 . . . $15.00. http://dx.doi.org/10.1145/2766462.2767728.

a relevance measure ­ collectively representing a sample of some population of a real-world information retrieval task) allow comparative system performance experiments to be carried out. This approach, sometimes referred to as batch evaluation, assesses the system or algorithmic aspect of relevance, as defined by Saracevic [26]. Almost all sources of variability are removed in this classical design of test collections, including users and tasks, leaving topics as the primary source of variability within the collection. The relevance measure encodes (either explicitly or implicitly) an abstracted model of user behavior, and rewards systems which deliver relevant material more efficiently or comprehensively according to the model. Statistical assessments of comparative effectiveness as determined by the relevance measure can be used to determine improvements in algorithm design. Statistical power analysis calculations can be used to determine the number of topics needed to quantify the probabilities of making type I and type II errors.
An important aspect of test collection use that has perhaps been under-investigated is the degree to which they have external validity. Crudely put, external validity characterizes the extent to which an experiment (typically relative system effectiveness according to the relevance measure in the case of test collections) generalizes to other real world circumstances. Such circumstances might encompass use by different users (stay-at-home parents vs retired intelligence analysts) or over different document sets (a subset of the Web vs a collection of news articles) or for different interaction tasks (factoid question answering vs ad hoc topical information discovery). We propose some potential properties of a test collection that relate to the degree to which they have external validity. These include: fidelity ­ whether the relative effectiveness of systems is consistent when a population of users/topics/documents/task (of which the test collection is a sample) uses the systems; corpus-generalizability ­ whether outcomes from this test collection are consistent across other test collections modeling the same task, as investigated by Robertson and Kanoulas [24]; task-generalizability ­ whether outcomes from this test collection are consistent across other test collections with different tasks (i.e., generalizability across situations); and user-generalizability ­ whether outcomes are consistent in the presence of different user behaviors for the same task and topics (i.e., generalizability across people). We provide brief definitions for some other key concepts in Figure 1.
In this work, we are motivated to improve the user-generalizability property of test collections. In particular, we seek to understand how introducing some simple sources of variability in users, namely individual query formulation and expectations of the quantities of relevant information needing to be found, might affect how test collections are constructed, and how batch evaluations are carried

625

Task Topic Task complexity T Q Q02, R03, T04
AP, ERR, NDCG, RBP p, Q

An information seeking activity.
A description of information content or subject matter required for some task.
A categorized model of cognitive information processing for some task.
Expected number of useful documents required to satisfy a task for some topic.
Expected number of queries required to satisfy a task for some topic.
Our subsets of the topics and judgments from TREC test collections corresponding to Question Answering 2002, Robust 2003, Terabyte 2004 tracks.
Various relevance measures: Average Precision, Expected Reciprocal Rank, Normalized Discounted Cumulative Gain, Rank-Biased Precision, and the Q-Measure.

Figure 1: Definitions of significant terms and abbreviations used.

out. We use the lens of task complexity (discussed below) to help assess these issues across a range of information seeking scenarios.
To examine user-generalizability in a batch evaluation setting, we pose a series of research questions assessed in the sections below.
RQ1 Does the existence of individual variation in initial query formulation for a single information need alter the evaluation of system performance? (Section 4)
RQ2 Is there significant variation among users of the anticipated effort in terms of the number of documents viewed and queries to be issued, and is there a relationship between a user's anticipated effort and the information task complexity? (Section 5)
RQ3 Does incorporating anticipated effort within adaptive metrics lead to changes in relative system performance assessments? (Section 6)
The overarching issue we consider is: to what extent do measures of system effectiveness depend on (lack of) variation in user behavior and thus do test collections have insufficient user-generalizability?
We are not the first to consider this issue. In a 1977 report on the design for an ideal test collection, Spärck Jones and Bates [29] recommend that:
The effects on the retrieval of relevant documents of such variations over requests should be counteracted by the use of additional queries specifically designed to exhaust the relevant document set.
The 1999 TREC Query Track examined sets of queries for topics, and the coordinators Buckley and Walz [8] similarly conclude that:
We've reaffirmed the tremendous variation that sometimes gets hidden underneath the averages of a typical IR experiment. Topics are extremely variable; queries dealing with the same topic are extremely variable. . . ; and systems were only somewhat variable.
In a comprehensive study examining different types and sets of judges as the source of user variability, Voorhees [32] found that the TREC-4 and TREC-6 collections were reasonably stable in relative outcomes for participating systems, both for similar users' judgments and different users' judgments. She also observed that inter-system comparisons required more substantial differences in measure scores than for intra-system comparisons. More recently,

Bailey et al. [4] examined consequences of using relevance labels originating from judges of differing task and topic expertise. They found that variation in expertise levels led to consistent differences in relevance outcomes and also to questions about the robustness of relative system performance measures over the TREC Enterprise 2007 test collection. Kazai et al. [16] confirmed that such systematic bias between different kinds of judge may exist.
The project described here encompasses exploration of just two (among many) aspects of user variability, thereby to connect user experiences more closely with batch evaluation outcomes.
2. RELATED WORK
Task complexity In information science, the complexity of a search task has long been recognized as having an important impact on information seeking behavior and use, including for example the type and complexity of information needed, and the number and diversity of sources consulted [31].
Byström and Järvelin [9] proposed a five-level task complexity taxonomy, ranging from automatic information processing tasks (tasks that are completely determinable so that they could in theory be automated) to genuine decision tasks (unexpected, unstructured tasks). This taxonomy was refined into three levels by Bell and Ruthven [7], with the distinction between levels being based primarily on the initial determinability and clarity of the task.
Focusing more directly on task complexity in the context of interactive information retrieval, Wu et al. [35] proposed a hierarchy based on the Cognitive Process Dimension of Krathwohl's Taxonomy of Learning Objectives [17], which is itself a refinement of Bloom's Taxonomy of educational objectives. Through a user study, Wu et al. demonstrated a tendency for participants to spend more time, issue a greater number of queries, and click on more search results for tasks with greater cognitive complexity. We use three levels of this taxonomy for our experiments, explained below.
Factors that influence searcher behavior Wu et al. [36] investigated the relationship between information scent (signals of relevance on a search results page) and search behavior such as query reformulation, search depth and stopping, demonstrating that a higher density of relevant items on the first page increases the probability of query reformulation, and decreases that of pagination.
The relationship between constraints and searcher behavior was studied by Fujikawa et al. [13], who showed that when the number of queries that a searcher can enter is restricted, greater attention is given to query formulation and more time is invested in viewing search results pages. Similar effects were observed when constraints were placed on the number of documents that can be viewed.
Azzopardi et al. [3] studied the effect of query cost on the behavior of searchers, examining the influence of different interfaces, designed to require differing amounts of effort. Users of the "structured" (highest cost) interface displayed different behavior, submitting fewer queries and spending longer when examining search result pages. A strong relationship between searcher behavior and task type and structure was also reported by Toms et al. [30], with users showing different rates of query reformulation and page views.
In a focused study, White and Kelly [34] varied the threshold acquired from individual document examination times as an input to an implicit relevance feedback algorithm, across a number of individuals and search tasks. They found that there was substantial variation in individual examination times, and that it was possible to improve relevance performance by using task information to determine the threshold. Attempts to tailor the threshold on a perindividual basis led to degraded performance however, suggesting intra-task-consistency was higher than intra-individual-consistency.

626

Gwizdka and Spence [14] examined observable measures of information seeking activities (such as documents viewed, time spent etc.) of a set of psychology students within a laboratory setting. They characterized relationships between the objective operationalized task complexity (in a manner influenced by [7]) and subjective searcher assessments of task difficulty with respect to these observable measures, and analyzed which measures were more important in predicting the difficulty experienced by the searcher. They found that task complexity affected both the relative importance of these predictors and the subjective assessment of difficulty. They also observed that individual variation (in factors like experience, verbal ability, other cognitive abilities etc.) played an important part in affecting performance and relative assessment of difficulty. We use individual variation in query formulation and expected goals of search to examine how batch evaluation outcomes change, and use task complexity as an analysis factor.
Query variability Searchers use an IR system to resolve an information need. To do so they need to translate their internal information requirement into an explicit query that is submitted to the search system. Multiple queries can represent a single information need, and indeed a single user may issue multiple queries within a single search session. Finally, interactive query (re-)formulation systems are increasingly common and have been demonstrated to assist in improving retrieval performance by (among others) Kumaran and Allan [18]. In that work, the authors also demonstrate how programmatic query expansion or relaxation can lead to significant increases in performance, across a selection of TREC test collections.
The 1999 TREC Query Track [8] investigated the issue of query variability through the creation of 23 query "sets", alternative query statements corresponding to 50 TREC topics. Analysis confirmed previous research showing that differences between topics introduces substantial variability into IR experimental results, and further showed that the variability of queries dealing with the same topic also introduced significant variability, typically greater than differences between retrieval systems. However, Buckley and Walz note that formal conclusions cannot be drawn from the full data set, due to the presence of "blundered queries" and the presence of multiple versions of the same basic system [8]. Other investigations of query variability in the TREC setting were shown to improve query performance through data fusion [5, 6].
Modave et al. [20] carried out a study of the quality of healthrelated information related for people seeking information about weight-loss using Google. While measuring query variability was not a focus of the study, this effect was accounted for by generating a range of queries about the weight-loss topic, eliciting specific queries from 20 study participants as well as the Google auto-complete feature.
Evaluation metrics Batch evaluations rely on objective scoring of search response listings. Long-standing mechanisms include Reciprocal Rank (RR); Precision at depth k; and Average Precision (AP), the average of the precisions achieved at the depths in the ranking of the relevant documents. A wide range of further alternatives have been developed over the last decade, including Normalized Discounted Cumulative Gain (NDCG) [15]; Rank-Biased Precision (RBP) [22]; Expected Reciprocal Rank (ERR) [10]; and the Q-Measure [25]. Per-query scores from one or more of these metrics are then averaged in some way, and paired statistical tests applied in order to draw experimental conclusions.
Metrics are sensitive to system performance in different ways. Precision at 10 and RBP with parameters less than about p = 0.8 are "shallow" metrics, and hence better match the behavior of a typical web search user than do "deep" metrics such as AP, NDCG, and

the Q-Measure. In terms of judgment effort, shallow metrics are also cheaper to evaluate than deep metrics. On the other hand, deep metrics tend to lead to a higher fraction of statistically significant system differences being identified (the discrimination ratio), and to be just as predictive of the behavior of shallow metrics as are the shallow metrics themselves [33]. Moffat [21] provides further commentary on ways effectiveness metrics can be categorized.
User goals and persistence Users vary in the way they process search response pages, and hence if a metric is to reflect the user's perception of their experience, should be sensitive to that variation. Moffat and Zobel [22] argued that a metric should match a user model, a description of the behavior of the presumed user; and parameterized their RBP metric with a persistence parameter p. Rather than quantifying persistence in terms of documents, Smucker and Clarke [28] used time as the primary persistence factor in the model, and make use of data from a user study to calibrate their gain calculations. Moffat et al. [23] note that users may have differing goals, even for the same query or same information need, and introduce the notion of an "expected goal of search", their parameter T , and use it to shape predictions about what happens when that user is viewing a page, thereby creating a more refined user model that in turn leads to further alternative effectiveness metrics. A user study provided evidence to support that hypothesis, bringing user and batch evaluations a step closer. We build on their work by examining how user variation in queries and expected goals can be combined. Next we describe our overall experimental framework.
3. EXPERIMENTAL FRAMEWORK
Search can be viewed as a process that starts with an information need, out of which a particular query is formulated by a user and submitted to a retrieval system. However, batch evaluations typically start with a single query per information need and regard the system as being the primary variable that impacts on effectiveness. Our experimental framework attempts to reintroduce two aspects of user variability into the batch evaluation process. We start by describing the process we adopted for formulating information need statements that could then be used to investigate user-generalizability.
Information needs To investigate user generalizability, several aspects of searcher behavior were studied through a crowd-sourced experiment. We first required a set of labeled search tasks for the experimental participants to carry out. To obtain a broad crosssection of information-seeking tasks, a set of 180 TREC topics was selected:
· Q02 Question Answering Track 2002, 70 topics (1824­1893)
· R03 Robust Track 2003, 60 topics (selected from 303­610)1
· T04 Terabyte Track 2004, 50 topics (701­750)
For each topic, a backstory was created; this was a short information need statement that was intended to motivate and contextualize the search request, making the topic statements less abstract and more engaging. Four annotators created the backstories, based on the full original TREC title, description and narrative fields. They were also free to explore related background information using online resources. An example topic from each of the three TREC tracks is shown in Figure 2. To encourage our eventual experimental participants to engage more fully with these search tasks, and to treat them as personal searches rather than abstract impersonal ones,
1The topic numbers are non-contiguous because half of the topics selected for the Robust Track 2003 were chosen as they were known to be difficult from previous Ad-Hoc tracks.

627

Q02.1828, Remember; "What was Thailand's original name?" While visiting Thailand for a beach holiday last year, you decided to visit some local museums to learn more about Thailand's history. You learned many interesting things about the country, including that it was not always called Thailand. What was it called originally?
R03.356, Understand; "postmenopausal estrogen Britain" A friend, who lives in Britain, has started estrogen treatment. This surprises you as you thought it's no longer recommended. You want to find out more about the use of hormone replacement therapy or estrogen treatment in the U.K.
T04.734, Analyze; "Recycling successes" Your city has recently embarked on an ambitious zero-waste policy for household and industrial garbage. Recycling is going to be a big component of the program. You wish to find out what recycling projects have been successful, including the places or product programs that have worked, and what they understood success to mean.
Figure 2: Backstory associated with three TREC topics from different tasks in different years, together with the task type.
Number: 734
Recycling successes
Description: What recycling projects have been successful?
Narrative: Guidelines by themselves are not relevant. Titles in a table of contents are relevant if they identify places or product programs which have had success. Must be declared successful or success should be clearly assumed from the description. Name of state identified as successful recycler is relevant. Listing of recycled products for sale are relevant.
Figure 3: Topic 734 from the TREC 2004 Terabyte Track.
the backstories were written to speak directly to the reader, and to include hypothetical family members or friends. Figure 3 shows the original TREC presentation of one of the topics shown in Figure 2.
The original topic statements from the Terabyte and Robust tracks contain substantial detail about what information a document should or should not contain to be considered relevant, and the created backstories aimed to reflect the bulk of these requirements. Nevertheless, we acknowledge that there is potential for drift between the interpretations of the backstory and the original TREC topic description that led to relevance judgments being created. Topics from the QA Track were more difficult as they are typically presented simply as question statements, such as "How much gravity exists on mars?" (Q02.1871). Simply posing the question statement to the experimental participants might lead to these being entered directly as a search query, rather than then being read as an information need statement, so the QA topics are also presented with a backstory. When possible, pronouns or other indirect references to the query subject were used, to reduce the likelihood that participants would simply copy and paste the final question as their query.
Task complexity Different information-seeking tasks have different characteristics, and task complexity is a key feature that may influence searcher behavior. For our experiments we adapt three levels from the cognitive complexity hierarchy proposed by Wu et al. [35], derived from a taxonomy of learning objectives presented by

Anderson and Krathwohl [2]. This hierarchy considers a spectrum of information needs, with the lowest level consisting of searches that involve "retrieving, recognizing, and recalling relevant knowledge". Such Remember queries therefore involve finding a fact in response to a simple "when", "where" or "what" question, such as "How did Eva Peron die?". The next level in the hierarchy, Understand, involves "constructing meaning from oral, written, and graphic messages through interpreting, exemplifying, classifying, summarizing, inferring, comparing, and explaining". We also use a third level, Analyze; tasks at this level of the hierarchy involve "breaking material into constituent parts, determining how the parts relate to one another and to an overall structure or purpose through differentiating, organizing, and attributing". Each of the example tasks in Figure 2 indicates the corresponding complexity category.
Based on the created backstories, each of the 180 selected TREC topics were assigned to one of the task complexity types. Four annotators independently rated each topic: broadly speaking, topics that required a simple factoid answer tended to be assigned to the Remember category; where topics required the production of list of things, even if relatively complex and sourced from different pages, they tended to be assigned as Understand; and, where topics required synthesis of disparate information, and eventual summary, or balancing of competing viewpoints and opinions, they were allocated to the Analyze category.2 The overall inter-annotator agreement among the four judges for the initial ratings was 0.664, measured by Fleiss'  [12], a statistic that measures agreement across multiple raters and corrects for agreement by chance. It is interesting to note that the per-category agreement varied substantially, from 0.456 for the highest of the three hierarchy levels (Analyze) to 0.907 for the lowest level (Remember), indicating that Remember tasks are relatively easy to identify and agree on, while differentiating between Remember and Analyze tasks is more difficult. For all cases where there was no majority rating among the four annotators, the tasks were carefully discussed until agreement was reached, resulting in a single confirmed type for each.
Gathering data To investigate user variability in a test collection setting, an experiment was carried out using the CrowdFlower crowd-sourcing platform.3 The experiment was reviewed and approved by the <anonymous institution> ethics board.
On signing up for the experiment, a participant was first presented with an information need statement, one of the created backstories. They were then required to answer three questions. First, participants were asked: How many useful web pages do you think you would need to complete the search task?. Responses were selected from the following: 0 useful pages (I'd expect to find the answer in the search results listing, without reading any of the pages); 1 useful page (I'd expect to find the answer in the first useful page I found); 2 useful pages; 3-5 useful pages; 6-10 useful pages; 11-100 useful pages; 101+ useful pages. Second, they were asked: In total, how many different queries do you think you would need to enter to find that many useful pages? with answers selectable from the following: 1 query (I'd expect to be able to complete the search task after the first query); 2 queries; 3-5 queries; 6-10 queries; 11+ queries. Third, participants were asked: What would your first query be?; answers to this question were entered in a textbox. Participants were free to complete as many topics as they liked, from one to a maximum of 180. The resulting data set had 10,800 responses from 115 workers,
Cleaning crowd data It cannot be expected that all anonymous
2To promote reproducibility, the full set of 180 topic backstories and corresponding task complexity labels will be available on request. 3http://www.crowdflower.com

628

number of topics average queries per topic average query length (chars) average query length (words) average query entropy (bits)

Task complexity

Remem. Under. Analyze

70

81

29

44.3 44.4 44.0

25.9 32.9 37.1

5.6

6.0

6.5

19.9 26.0 30.5

Table 1: Query properties after normalization: average query length in characters, not counting white-space characters; average query length in words; and average query entropy in bits. To calculate the last, the frequency distribution of words appearing in the queries for each topic was computed, and then the average information cost of representing the queries for that topic computed using that frequency distribution, and averaged over task complexities.

workers took their task seriously, and where it was possible to identify clearly inappropriate responses, those workers were removed from further analysis (but still paid). First, if any worker suggested the same first query for two or more tasks, they were considered unreliable and all their responses were removed ­ recall that no worker got the same task twice, so it is extremely unlikely that two tasks would attract identical queries. This rule removed 15 of 115 workers. Two further workers who had copy/pasted apparently nonsensical parts of the topic statement as their first query were also identified and removed. This left 7,971 responses from 98 workers, covering all 180 topics with 41­48 responses per topic (median 44).
4. VARIATION IN FIRST QUERIES
Having described the data collection process, we first examine the sets of queries suggested by the experimental subjects.
Normalization One of the components in each interaction pane asked "What would your first query be?" Workers then entered text in to a textbox. As with all web queries, the resultant strings are noisy, with a wide range of spelling and grammatical errors. In this regard, the behavior of the crowd workers probably corresponds closely to other users. To ameliorate this type of behavior, web search systems include a "did you mean?" query modification feature. To faithfully reflect that behavior, the query strings typed by the crowd-sourced subjects were converted to US English, and corrections applied whenever they could be unambiguously identified. For example, "theropy" was changed to "therapy" in the context of topic R03.356 (Figure 2). In some cases the correction was not clearcut, or the erroneous word was actually a correct spelling of something different. Manual interactions with a major search engine were used to decide whether to alter these queries. For example, "cheapskate bay" was altered to "chesapeake bay", because that is what happened at a web search interface. On the other hand, "calgary provicence" was altered to "calgary providence" rather than "calgary province", which would have better fitted the topic in question, because the first alteration was what was suggested at the same search interface. As a further part of the normalization process all punctuation characters were removed, including periods. Finally, two queries ("zdvfdzfvg" and "fxghfsdg") not caught by the earlier quality-control mechanisms were removed. The resulting query set contained 7,969 queries, of which 5,046 were unique.
Query diversity Table 1 lists some properties of the queries received, averaged over the three query classes after quality control and normalization mechanisms were applied. The table shows a clear trend to longer queries as the information need becomes more

city recycling projects (2) city recycling scheme progress council website most successful recycling programs recycling policy update recycling projects (2) recycling projects for household and industrial garbage recycling projects program recycling projects successes and effects recycling projects that have been successful recycling successes reducing waste to zero success stories successful city recycling policies successful municipal recycling projects successful recycling programs (2) successful recycling projects (11) successful recycling projects place product programs successful zero waste what are the recycling projects that have been successful what does it take to make a successful recycling program what recycling projects have been successful (6) where have recycling projects been successful and
how do they define success zero waste policy (2) zero waste policy for household and industrial garbage zero waste policy for household and industrial garbage programs
Figure 4: The 44 user-generated queries for Topic 734 (Figure 3). Numbers in parentheses indicate multiplicity.
complex, both in terms of characters typed and in terms of words typed. The final row of the table represents the average diversity of the terms across the pool of queries generated for each topic, by computing the term frequencies of all terms used in queries for that topic, then calculating the entropy of each query relative to that distribution, and finally averaging those average entropies. The entropy of a query increases as the length of the query increases, and is also high if a broad set of term is being used across the pool of queries for that topic ­ if queries are less predictable. This measure confirms that the more complex the information need, the more expressive are the queries posed to resolve it.
As a single example, Figure 4 lists the complete set of queries generated for one of the 180 information need statements (see Figure 2). One query dominates ­ an extended version of the TREC title-only query for this topic, "recycling successes" ­ but nearly half of the queries generated by the subjects occur only once.
Query effectiveness ­ In the small Two different retrieval systems were then used to execute each query against the corresponding document collection: Indri4 with an Okapi similarity computation, and Indri with a sequential dependency computation [19]. Using Indri for both ranking algorithms ensures the system effects are due to fundamental differences in the retrieval algorithms, rather than other factors related to query or document processing. Rankings of length 200 documents were generated and scored; with documents for which no judgment was available deemed to be not relevant.
Figure 5 shows the range of scores that resulted when four standard relevance measures were applied to the rankings for the set of 44 queries generated in response to Topic 734 (Figures 2 and 4), with the Indri Okapi BM25 and SDM ranking functions. The blue diamonds show the corresponding scores for the canonical TREC
4http://www.lemurproject.org/indri/.

629

1.0

1.0

Okapi SDM  Mean Title

Queries - SDM Systems  Mean Title

0.8

0.8

0.6

0.6

Score

Score

0.4

0.2

 

 



 










0.0

AP

NDCG

Q

RBP

Figure 5: Retrieval effectiveness measured by AP, NDCG, Q 1 and RBP 0.85 for Topic 734. Green and blue boxes show scores obtained from running the different user queries using BM25 and SDM, respectively. Grey points indicate the mean for each column, black bars the median, and the blue diamonds show the effectiveness of the corresponding TREC title-only query.

title-only query (again, see Figure 2) when evaluated using the same two retrieval mechanisms. The results for this one query reflect a trend that we also saw more widely ­ that for typical user queries the mean performance of SDM is superior to that of Okapi. That difference is consistent, but not absolute, and for most combinations of metric and topic there are also queries for which Okapi out-scored SDM. On Topic 734 the title-only query was the highest-scoring query (of the 44) for AP, NDCG, and Q 1 for both Okapi and SDM models, and also the highest-scoring for RBP 0.85 for Okapi (the query "successful recycling projects place product programs" scored 0.703 when the SDM similarity model is used), but this topic was unusual in that regard. For example, for Topic 356 (Figure 2), more than half of the user-generated queries outperformed the canonical title-only query. The omission from the corresponding backstory of the word "postmenopausal", which appears in the TREC topic description ("identify documents discussing the use of estrogen by postmenopausal women in Britain"), may have had an effect. Some level of unintentional topic drift is always possible in our process.
A risk factor in any experimentation in which judgments are reused is the extent to which they provide coverage of the documents retrieved by the systems being compared. For Topic 736, the RBP residuals when p = 0.85 are 0.518 and 0.469 for Okapi and SDM, respectively. These represent the assessment weight of the unjudged documents in RBP [22], with 0.0 representing a situation with all required judgments available, and larger values indicating that the RBP score would increase by that much if all unjudged documents were in fact relevant. For Topic 734 the average residual for the usergenerated queries was in excess of 0.25, and the available judgments covered less than 75% of the RBP probability mass. That is, the relativities shown in Figure 5 need to be taken cautiously. With more complete judgment coverage, the RBP scores for this topic (and hence the scores for other metrics) might change considerably. Similar situations were encountered for several other topics. On the other hand, the title-only queries have consistently low residuals, because they were used by some of the systems that contributed to the pools from which the judgments were created.
Query effectiveness ­ In the large The Q02 queries were especially prone to the problems arising from sparse judgments, with

0.4

 

 

 

 

 

 

 

 

0.2

0.0

AP NDCG Q R03

RBP

AP NDCG Q T04

RBP

Figure 6: Retrieval effectiveness as measured by AP, NDCG, Q 1 and RBP 0.85, for two subcollections R03 and T04. Blue boxes show scores obtained from running different user queries with SDM retrieval, while red boxes show scores achieved by different TREC contributing system runs. Grey points indicate the mean for each column, black bars the median, and the blue diamonds show the effectiveness of an Indri SDM run using the corresponding TREC title-only query. The average residuals for the four RBP measurements were (left to right) 0.153, 0.038, 0.235, and 0.059. The Indri SDM runs had RBP residuals of 0.017 for R03, and 0.056 for T04.

RBP 0.85 residuals that averaged around 0.5 over that set of 70 topics. One reason may be that QA relevance was focused on answer fragments, not document-level relevance. Coverage was somewhat better on the R03 and T04 queries (but with considerable variation, as already noted).
Figure 6 incorporates score information derived from all of the TREC participating systems that contributed runs for the Robust-03 and Terabyte-04 tracks. The boxes show variation over the corresponding R03 and T04 topics of score responses to user-generated queries (blue) and score responses across the set of contributed TREC runs for that year (red), factoring in all of the user-generated queries when evaluated using the Indri SDM model. The average of the Indri scores for the title-only queries for those topics is also marked on each bar. It is clear that query-derived variations are just as broad as are the variations caused by system diversity, and hence that improved performance relative to the Indri SDM title-only runs is thus equally likely to be derived from query reformulation as it is from system improvement. Note also that for the user-generated queries (blue boxes) there is a considerable amount of metric weight still sitting in the residuals, which might be released with further judgments, and result in higher scores.
Variability analyzed The effect of query choice is illustrated further in an analysis of variance for each metric, modeling score as a response to topic, system, and query. In this analysis "topic" is a nominal variable, one level per TREC topic; "system" has one level for each TREC system, plus two levels for our Indri runs; and "query" has one level for all TREC systems plus one for each query processed by Indri. (We do not know the exact query used by each TREC system, but by assuming it is always the same we will underestimate the variability due to query phrasing and overestimate that due to system.) Q02 looks very different, as discussed above, and those runs are not included in this analysis since these measures are document-relevance centric.

630

Metric

2

SS

df

F

AP

query 0.55 158.40 4977 4.58

system 0.20 32.61 147 31.90

topic 0.14 22.05 179 17.72

NDCG

query 0.59 279.02 4977 5.38 system 0.28 75.64 147 49.35
topic 0.16 36.56 179 19.59

Q1

query 0.57 145.79 4977 5.05

system 0.19 24.83 147 29.12

topic 0.18 23.63 179 22.76

RBP 0.85 query 0.53 341.80 4977 4.23 system 0.20 77.12 147 32.33 topic 0.11 38.90 179 13.39

Table 2: ANOVA for four metrics, modeled as a response to system, topic, and query string. Partial 2 values reported; all F statistics
significant at p  .001. In each case, the effect due to query phrasing
is substantially larger than that due to topic or system.

Table 2 summarizes the results. Each of query, system, and topic has a statistically significant effect (p  .001 in all cases, Wald test) and the effect of each factor is medium/large, with the possible exception of topic for RBP, but the effects are of very different scales. The variation due to system is slightly larger than that due to topic (e.g. partial 2 of 0.20 and 0.14 for AP), so slightly more variation in final score is explained by changes to system than by changes to topic. The variation due to query phrasing, however, dwarfs other effects and over 50% of variation in final score can be attributed to phrasing even after system and topic are taken into account (partial 2 in the range 0.53­0.59).
Observations Particular choices of query clearly can lead to widely different scores, independent of the topic, the system, or the metric. We commonly want to use variation in scores to say something about differences between systems (i.e., "system B is better"); less commonly, we want to use variation in scores to say something about topics (i.e., "topic 734 is hard"). In either case we need to be aware of query wording as a confound, and an extra source of variation which in fact dominates system and topic.
Two macro implications can be drawn from this analysis regarding test collection design and development. First, since this approach supplies between one and two orders of magnitude more queries for a given set of topics within a collection, even given some crossquery document overlap when judging pools on a per topic basis, this would sharply increase the required judging budgets. Given finite budgets, this implies that measures that accommodate missing judgments such as RBP or the suite of inferred AP and NDCG measures developed by Yilmaz and Aslam [37] are required and/or more cost-effective judgment acquisition methods such as crowdsourcing approaches (e.g., as discussed by Alonso et al. [1]) should be employed. Second, systems could be provided with each topic's collection of queries, and can then make use of any methods desired to create a single top-K ranking for the topic. Document pools would be formed in the usual way, but on the scale of number of topics, not number of queries. In the absence of search engine logs, this might provide some partial subset of the data that is available to commercial search providers about variant phrasing, and hence techniques such as pseudo relevance feedback or query reformulation merging [27] could be employed.

Factor
Worker Author Remember (baseline) Understand Analyze

Effect (mult. odds)

T

Q

0.005­7520.3 10-9­22316.9

0.8­1.3

0.9­1.5

1.0

1.0

14.1

11.2

21.9

18.9

Table 3: Significant factors in fitted models for estimates of T and Q. Effect sizes > 1 correspond to higher values of T or Q being more likely. All effects significant at p < 0.05, Wald test.

5. VARIATION IN EXPECTATIONS
As well as differing in their behavior (issuing different queries, in our example), users may have different expectations of a search system and of a task and it would be appropriate to consider this when evaluating search systems. For example, if one user expects to issue a query then read three or four documents ­ perhaps to compare information from different sources ­ then it would not be appropriate to evaluate based on the rank of only the first relevant result. If another expects to issue several queries in succession, then it may be appropriate to evaluate a session rather than a single question. Other, similar, scenarios are easy to imagine. Two questions in our instrument aimed to understand some of these varied expectations.
Expected number of documents Cooper [11] noted that (p.31) "most measures do not take into account a crucial variable: the amount of material relevant . . . which the user actually needs". Following Moffat et al. [23], we denote this quantity by T . Cooper further observes (p.33):
A search request is therefore to be conceived in the abstract as involving two parts: a relevance description (normally a subject specification) and a quantity specification. To put it another way, every search request has a definite quantification.
To understand this quantification and how it varies, we asked: "how many useful web pages do you think you would need to complete the search task?". We plot the responses for each task complexity category in Figure 7. The distribution of responses across the three types of task are significantly different (2 = 2067.0, df = 12, p  .001), and it seems that descriptions of more complex tasks prompt people to expect more reading.
Clearly estimates of T vary with task complexity, and they may vary with other factors as well. Some of these factors are captured in our instrument, and some are external and not captured. To clarify some of the instrument-captured factors, we used cumulative logistic regression (also called ordinal regression) to model T as a response to a number of potential explanatory variables: complexity (three levels), worker (98 levels, or one per worker), topic author (four levels), and CrowdFlower run (two levels). Model selection was performed to minimize the Akaike information criterion (AIC), which measures likelihood but with a penalty for complex models.5
The model is summarized in Table 3, where effects are given as multipliers to odds ratios. Effects greater than 1 represent a higher probability of answers higher up the scale. For example, a multiplier of 2 would mean the odds of estimating T as 1 vs estimating T as 0 are twice as high as the baseline; likewise, the odds of estimating T as 2 vs either 1 or 0 would be twice the baseline, and so on.
The largest effects are due to the CrowdFlower workers. Our workers varied substantially, with a worker at one extreme claiming
5Modeling used R's ordinal::clm and ordinal::step.clm functions.

631

Proportion of cases

Remember 0.4 0.2 0.0

Understand

Analyze

Proportion of cases

Remember 0.8 0.6 0.4 0.2 0.0

Understand

Analyze

1 2 3-5 6-10 11+ 1 2 3-5 6-10 11+ 1 2 3-5 6-10 11+

0 1 2 3-5 6-10 11-100 101+ 0 1 2 3-5 6-10 11-100 101+ 0 1 2 3-5 6-10 11-100 101+

Estimate of T

Estimate of Q

Figure 7: Judges' estimates of T , the number of relevant documents they expect to read, vary with task complexity (left); as do their estimates of Q, the number of queries they expect to issue (right). Judges expect to need more interactions for more complex tasks.

they expected to read documents for only five out of 65 topics and one worker at the other extreme expecting to read 11 or more documents in every case. This is reflected in the model, where per-worker effects are highly variable and again dominate all other effects ­ odds ratios change by six orders of magnitude for T .
There is a smaller but still notable effect due to task complexity, with Understand tasks more likely to prompt higher estimates of T and Analyze more likely still ­ the difference between Remember and Understand being larger than that between Understand and Analyze. Finally, there is an effect due to topic author: even after controlling for task complexity and worker, some authors provoked higher T estimates, an effect that was statistically significant, but practically negligible. We also checked for batch effects (the tasks were released to workers in two rounds), but they were not evident.
Expected number of queries Users may also vary in the number of queries they expect to issue ­ be it a single query, if they think a task is simple or well-supported, or very many in succession, if they think the task is more complex. We denote this expected number of queries Q and plot it in the right-hand side of Figure 7. As before, per-judge effects dominate ­ the variability here is even more pronounced than for T , with odds ratios varying across thirteen orders of magnitude. Clearly different users have very different expectations of their search engine interactions, even for the same topic. There are again significant differences across complexity levels, with similar effects to those seen for T . Again the difference between a Remember and an Understand task is larger than the difference between Understand and Analyze. We also note a significant but small effect due to topic author, and no significant batch effect.
Observations Just as with individual variation in query formation, we observed significant individual variation in expectations of documents and queries to satisfy that need. Given that T is correlated with task complexity and is strongly influenced by user-specific expectations, a natural question to ask is whether it is possible to include T ­ and progress made towards attaining T as a ranked list is consumed ­ into an relevance measure in a meaningful way. In the next section, we develop this possibility using the data we collected.
6. SENSITIVE EVALUATION
Current effectiveness metrics are insensitive to both T , the initial expectation of the user, and to the evolving expectation of T as the search is prosecuted. For example, while the p parameter of RBP [22] provides adjustment for user persistence and can be adjusted so as to influence the expected depth in the ranking that the user will examine, the user model associated with RBP requires

that the user proceeds to the next document with fixed probability, regardless of how much information has already been accumulated, or what depth has been reached. We believe that as more relevance is accumulated, the user becomes less likely to continue their search.

Expected search length Cooper's definition of ESL [11] is simple: it is the total number of documents inspected before T relevant ones have been found. As such, it is always greater than or equal to T , with larger values representing inferior performance. To obtain a metric with the usual behavior (bounded by zero and one, with larger values indicating better performance) we scale by T and invert, to obtain an ESL- and RR-inspired metric:

RRT(T )

=

rank

of

T

th

T relevant

document

.

(1)

As is the case with RBP, Prec, and RR, the score returned by this metric corresponds to the average rate at which utility (relevance) is acquired per document inspected, with a user model defined by a person who seeks exactly T relevant documents, and stops their search immediately upon finding a T th answer in the ranking. Queries that have fewer than T answers in a system's ranking are scored as zero. There is a clear connection between RRT and precision ­ RRT is equal to precision at the T th relevant document. Note also that RRT(1) is exactly reciprocal rank, RR.

Probabilistic users A second option is to form a probabilistic composite of RBP and ERR [10]. Suppose that the user makes a biased decision after encountering each relevant document, continuing to scan with probability p = (T - 1)/T , and ending their search with probability (1 - p) = 1/T . In this user model the expected utility per document inspected is given by:

ERRT(T )

=

1




T -1

t-1
· RRT(t)

.

(2)

T t=1

T

The geometric distribution means that the average number of relevant documents identified by the time the user stops scanning is 1/(1 - p) = T , and that the value of ERRT is non-zero even if fewer than T relevant documents appear in the run. There is also a clear relationship between ERRT and AP, the latter being an unweighted sum of all R precision scores: AP = (1/R) tR=1 RRT(t), where R is the number of relevant documents for that topic. A key difference between AP and ERRT is that computation of the latter does not require knowledge of R. Nor is AP sensitive to T , of course.
Moffat et al. [23] have also considered effectiveness metrics that are sensitive to T . Their INSQ and INSQ functions are weighted precision metrics defined in terms of the conditional probability C(i)

632

T Upper

Lower

INSQ INSQ INST

1 2.58 2.58 1.64 1.33 3 6.53 6.53 4.36 3.27 10 20.51 20.51 13.93 10.26 30 60.50 60.50 41.29 30.25

Table 4: Expected search length for INSQ-based metrics for different values of T , when no documents in the ranking are relevant (column "upper"); and when every document is (columns "lower").

of the user continuing from the document at depth i in the ranking to the document at depth i + 1. They define INSQ and INSQ via

C(i) =

i + 2T - 1 i + 2T

2
and C(i) =

i + T + Ti - 1 i + T + Ti

2
,

respectively, where Ti is the amount of relevance (or gain) that has not yet been accumulated by depth i, Ti = T - ij=1 ri, and where 0  ri  1 is the relevance of the i th document in the ranking. Both versions of INSQ are sensitive, in that higher values of T lead to
more patient search behavior and a greater expected depth in the ranking. In addition, INSQ is adaptive ­ as relevant documents are
identified, the expected remaining search cost decreases.

INST: An improved INSQ In the formulation of Moffat et al. [23],
Ti is required to be positive. We remove that restriction, and allow Ti to be negative too, covering situations in which more gain has been
accrued than was initially anticipated. The altered metric, still using the continuation function C(i), is denoted as INST. Table 4 shows why we prefer this change: it lists expected search depth for INSQ, INSQ and INST in two extreme situations ­ when no documents in
the ranking are relevant, and when every document in the ranking
is relevant. As is evident in the table, INSQ is not adaptive, and has the same behavior in both extreme situations, examining an average of 2T + 0.5 documents. On the other hand, the expected search length in INSQ and INST decreases if relevant documents are encountered. Our preference for INST over INSQ is based on its expected search length of approximately T + 0.25 when all documents are relevant, intuitively a better fit than the approximately 1.4T expectation of INSQ. That is, INST anticipates that a user seeking T relevant documents will examine, on average, between
T and 2T documents before leaving the ranking, with the actual exit depth depending on the number (and locations) of the relevant
documents. In addition, INST retains the other features that made
INSQ a more representative model for user behavior [23]. Compared
to the INSQ/INST variants, RRT and ERRT give rise to models in which the user may only exit the ranking as they encounter each
relevant document. On rankings that do not contain any relevant
documents at all, the models associated with RRT and ERRT (like
RR and AP before them) have the user scanning the full collection.

Retrieval effectiveness We use RRT(T ), ERRT(T ), INSQ(T ), and INST(T ) in our experimentation. To set the parameter T , the distribution of T -bands indicated by the crowd-workers for each topic is employed, and the mapping 0  1, 1  1, 2  2, 3­5  3, 6­10  6, 11­100  11, and 101+  101. That is, each score computed for RRT, ERRT, INSQ, and INST is a weighted average of up to six different T -based scores.
Table 5 lists scores for the 110 R03 and T04 queries topics and the user-generated queries, as measured by the four T -sensitive metrics; together with the expected depth at which users exit the result ranking. All of these metrics allow residuals to be computed;

Metric
RRT ERRT INSQ INST

Score

Residual Depth

0.421±0.267 0.453±0.267 0.310±0.213 0.366±0.249

0.195±0.188 0.114±0.113 0.251±0.185 0.206±0.186

48.98 44.17 10.46
8.57

Table 5: Averages and standard deviations of topic means for 4,871 user-generated queries over 110 R03 and T04 topics, using Indri SDM retrieval, and weighted distributions of T for each query. The final column lists the expected retrieval depth in the corresponding user models, also as a weighted average.

the variation in queries means that these are again relatively high compared to title-only queries. Note that the residuals associated with INST are smaller than those of INSQ, a consequence of the shallower expected depth; and the relatively implausible evaluation depths associated with RRT(T ) and ERRT(T ) (these would be even higher if the rankings were extended beyond 200 documents).
Kendall's  for TREC systems Table 6 lists Kendall's -b coefficients, computed by scoring TREC systems using a total of nine metrics, and ordering the 70 Terabyte 2004 systems (the coefficients above the diagonal) and the 78 Robust 2003 systems (below the diagonal) according to the average metric score across topics, using the R03 and T04 topic sets, and the same weighted-by-user-T computation as was employed for Table 5. The four T -aware metrics have relatively high similarity to each other in terms of the system orderings they induce, and fit in to the middle of the spectrum, in terms of being neither deep metrics (like NDCG) nor shallow (like RRT 1, which is equivalent to RR). They also yield system orderings that are similar to the ordering generated by RBP 0.85, for which the corresponding user model has an expected search depth of 6.7.
7. CONCLUSIONS
We have demonstrated that query variability among individuals leads to substantial changes across a range of standard relevance measures, and the effect of this source of variability is substantially more than that arising from topic or system effects. We also found that variation in expected goals of search in the number of documents (and number of queries) arises substantially from user-based factors, and is broadly correlated with increasing task complexity. Finally, we found that relevance measures that capture expectations of relevant documents and are adaptive to individual behavior are more similar to each other in terms of system orderings, and sharply dissimilar from deep metrics like AP and shallow metrics like RR.
We conclude that the aspects of variability among users regarding individual query formulation and expected goals of search can be incorporated within a batch evaluation process. The use of multiple queries per topic arising from different searchers provides a more representative characterization of the mapping from information need than just one. Systems which can perform well across such a range of queries per topic are more likely to exhibit usergeneralizability. Incorporating estimates of variance due to user query-factors in a statistical power calculator would help determine the number of topics needed to reliably detect certain effect sizes.
We also suggest that the adaptive and expectation-sensitive measures we presented (especially INST) display potential in having more user-generalizability and more task-generalizability than existing measures, which tend to overemphasize either shallow or deep recall user behaviors. We hope to build a test collection in future to carry out more conclusive experiments on this matter.

633

NDCG AP Q1 INSQ RBP 0.85 INST RRT ERRT RRT 1

NDCG AP

­

0.95

0.84

­

0.80

0.81

0.79

0.69

0.78

0.71

0.77

0.67

0.78

0.67

0.71

0.61

0.52

0.49

Q 1 INSQ RBP 0.85 INST RRT ERRT RRT 1

0.93

0.83

0.81

0.79

0.79

0.78

0.68

0.93

0.83

0.80

0.79

0.78

0.77

0.66

­

0.84

0.82

0.81

0.81

0.78

0.67

0.75

­

0.96

0.96

0.94

0.92

0.79

0.68

0.87

­

0.95

0.95

0.92

0.79

0.74

0.97

0.85

­

0.96

0.95

0.82

0.76

0.94

0.86

0.95

­

0.94

0.81

0.64

0.87

0.79

0.89

0.85

­

0.86

0.38

0.59

0.59

0.60

0.57

0.71

­

Table 6: Kendall's -b coefficients for the 70 Terabyte 2004 system runs ordered by the scores computed for the T04 queryset (above the lead diagonal); and for the 78 Robust 2003 system runs ordered according to the scores computed for the R03 queryset (below the diagonal). Metrics are ordered according to their -b coefficients relative to NDCG in the T04 comparison. Bold is for scores  0.9.

Acknowledgment This work was supported by the Australian Research Council's Discovery Projects Scheme (projects DP110101934 and DP140102655). We thank Alec Zwart and Xiaolu Lu.
References
[1] O. Alonso, D. E. Rose, and B. Stewart. Crowdsourcing for relevance evaluation. In SIGIR Forum, volume 42, pages 9­15, 2008.
[2] L. W. Anderson and D. A. Krathwohl. A Taxonomy for Learning, Teaching and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives. Longman, New York, 2001.
[3] L. Azzopardi, D. Kelly, and K. Brennan. How query cost affects search behavior. In Proc. SIGIR, pages 23­32, 2013.
[4] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P. de Vries, and E. Yilmaz. Relevance assessment: are judges exchangeable and does it matter? In Proc. SIGIR, pages 667­674, 2008.
[5] N. J. Belkin, C. Cool, W. B. Croft, and J. P. Callan. Effect of multiple query representations on information retrieval system performance. In Proc. SIGIR, pages 339­346, 1993.
[6] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. Combining the evidence of multiple query representations for information retrieval. Inf. Proc. Man., 31(3):431­448, 1995.
[7] D. J. Bell and I. Ruthven. Searchers' assessments of task complexity for web searching. In Proc. ECIR, pages 57­71, 2004.
[8] C. Buckley and J. Walz. The TREC-8 query track. In Proc. TREC, 1999. NIST Special Publication 500-246.
[9] K. Byström and K. Järvelin. Task complexity affects information seeking and use. Inf. Proc. Man., 31(2):191­213, 1995.
[10] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proc. CIKM, pages 621­630, 2009.
[11] W. S. Cooper. Expected search length: A single measure of retrieval effectiveness based on the weak ordering action of retrieval systems. Amer. Doc., 19(1):30­41, 1968.
[12] J. L. Fleiss. Measuring nominal scale agreement among many raters. Psych. Bull., 76(5):378, 1971.
[13] K. Fujikawa, H. Joho, and S. Nakayama. Constraint can affect human perception, behaviour, and performance of search. In Proc. Int. Conf. Asia-Pacific Digital Libraries, pages 39­48. 2012.
[14] J. Gwizdka and I. Spence. What can searching behavior tell us about the difficulty of information tasks? A study of Web navigation. Proc. Amer. Soc. Inf. Sc. Tech., 43(1):1­22, 2006.
[15] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Sys., 20(4):422­446, 2002.
[16] G. Kazai, N. Craswell, E. Yilmaz, and S. Tahaghoghi. An analysis of systematic judging errors in information retrieval. In Proc. CIKM, pages 105­114, 2012.
[17] D. R. Krathwohl. A revision of Bloom's taxonomy: An overview. Theory Into Practice, 41(4):212­218, 2002.
[18] G. Kumaran and J. Allan. Adapting information retrieval systems to user queries. Inf. Proc. Man., 44(6):1838­1862, 2008.

[19] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proc. SIGIR, pages 472­479, 2005.
[20] F. Modave, N. K. Shokar, E. Peñaranda, and N. Nguyen. Analysis of the accuracy of weight loss information search engine results on the internet. Amer. J. Public Health, 104(10):1971­1978, 2014.
[21] A. Moffat. Seven numeric properties of effectiveness metrics. In Proc. AIRS, pages 1­12, 2013.
[22] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Sys., 27(1):2:1­2:27, 2008.
[23] A. Moffat, P. Thomas, and F. Scholer. Users versus models: What observation tells us about effectiveness metrics. In Proc. CIKM, pages 659­668, 2013.
[24] S. E. Robertson and E. Kanoulas. On per-topic variance in IR evaluation. In Proc. SIGIR, pages 891­900, 2012.
[25] T. Sakai and N. Kando. On information retrieval metrics designed for evaluation with incomplete relevance assessments. J. Inf. Ret., 11(5): 447­470, 2008.
[26] T. Saracevic. Relevance reconsidered. In Proc. Conf. Conceptions of Library and Inf. Sc., pages 201­218, 1996.
[27] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. Lambdamerge: merging the results of query reformulations. In Proc. WSDM, pages 795­804, 2011.
[28] M. D. Smucker and C. L. A. Clarke. Time-based calibration of effectiveness measures. In Proc. SIGIR, pages 95­104, 2012.
[29] K. Spärck Jones and R. Bates. Report on the Design Study for the "Ideal" Information Retrieval Test Collection. British Library Research and Development Report, 5428, 1977.
[30] E. G. Toms, H. O'Brien, T. Mackenzie, C. Jordan, L. Freund, S. Toze, E. Dawe, and A. Macnutt. Task effects on interactive search: The query factor. In Focused Access to XML Documents, pages 359­372. Springer, 2008.
[31] P. Vakkari. Task complexity, problem structure and information actions: Integrating studies on information seeking and retrieval. Inf. Proc. Man., 35(6):819 ­ 837, 1999.
[32] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Inf. Proc. Man., 36(5):697­716, 2000.
[33] W. Webber, A. Moffat, J. Zobel, and T. Sakai. Precision-at-ten considered redundant. In Proc. SIGIR, pages 695­696, 2008.
[34] R. W. White and D. Kelly. A study on the effects of personalization and task information on implicit feedback performance. In Proc. CIKM, pages 297­306, 2006.
[35] W.-C. Wu, D. Kelly, A. Edwards, and J. Arguello. Grannies, tanning beds, tattoos and NASCAR: Evaluation of search tasks with varying levels of cognitive complexity. In Proc. IIiX, pages 254­257, 2012.
[36] W.-C. Wu, D. Kelly, and A. Sud. Using information scent and need for cognition to understand online search behavior. In Proc. SIGIR, pages 557­566, 2014.
[37] E. Yilmaz and J. A. Aslam. Estimating average precision with incomplete and imperfect judgments. In Proc. CIKM, pages 102­111, 2006.

634

An Entity Class-Dependent Discriminative Mixture Model for Cumulative Citation Recommendation

Jingang Wang
School of Computer Science Beijing Institute of Technology
bitwjg@bit.edu.cn
Zhiwei Zhang
Dept. of Computer Science Purdue University
zhan1187@purdue.edu

Dandan Song
School of Computer Science Beijing Institute of Technology
sdd@bit.edu.cn
Luo Si
Dept. of Computer Science Purdue University
lsi@purdue.edu
Chin-Yew Lin
Knowledge Mining Group Microsoft Research
cyl@microsoft.com

Qifan Wang
Dept. of Computer Science Purdue University
wang868@purdue.edu
Lejian Liao
School of Computer Science Beijing Institute of Technology
liaolj@bit.edu.cn

ABSTRACT
This paper studies Cumulative Citation Recommendation (CCR) for Knowledge Base Acceleration (KBA). The CCR task aims to detect potential citations of a set of target entities with priorities from a volume of temporally-ordered stream corpus. Previous approaches for CCR that build an individual relevance model for each entity fail to handle unseen entities without annotation. A baseline solution is to build a global entity-unspecific model for all entities regardless of the relationship information among entities, which cannot guarantee to achieve satisfactory result for each entity. In this paper, we propose a novel entity class-dependent discriminative mixture model by introducing a latent entity class layer to model the correlations between entities and latent entity classes. The model can better adjust to different types of entities and achieve better performance when dealing with a broad range of entities. An extensive set of experiments has been conducted on TREC-KBA-2013 dataset, and the experimental results demonstrate that the proposed model can achieve the state-of-the-art performance.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
This work was partially done when the first author was visiting Purdue University and Microsoft Research Asia. Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09-13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767698.

Keywords
Cumulative Citation Recommendation; Knowledge Base Acceleration; Mixture Model; Information Filtering
1. INTRODUCTION
In recent years, we have witnessed a proliferation of open domain Knowledge Bases (KBs) such as Freebase1 and Yago2. They have been used in many applications such as query answering, entity search and entity linking and have shown great promises. These KBs are usually organized around entities such as persons, organizations, locations, and so on. Currently, the maintenance of a KB mainly relies on human editors. However, with the explosion of information, largescale KBs are hard to be kept up-to-date solely by human editors. Taking English Wikipedia for example, there are approximately 4.7 million entities but merely 132,938 active editors3. The less popular entities cannot be updated in time because they are not spotlighted. As reported in [14], the median time delay between a cited document's publishing and its citation in Wikipedia is almost one year. An outdated KB severely limits the effectiveness of applications depending on it. This gap could be bridged if relevant documents of KB entries can be automatically detected as soon as they emerge online and then be recommended to the editors with various levels of relevance. This is called the Cumulative Citation Recommendation (CCR). Formally, given a KB entity, CCR is a task to filter highly relevant documents from a chronological stream corpus and evaluate their citation-worthiness to the target entity.
Most previous approaches (e.g., [2, 25]) for CCR are highly supervised and require sufficient training data to build an individual relevance model for each entity. These approaches are infeasible when dealing with a large-scale KB, since the
1https://www.freebase.com/ 2http://www.mpi-inf.mpg.de/departments/ databases-and-information-systems/research/ yago-naga/yago/ 3http://meta.wikimedia.org/wiki/List_of_ Wikipedias#1_000_000.2B_articles

635

labeling work is labor intensive. One solution is to build a global entity-unspecific discriminative model and optimize it to achieve an overall optimal performance for all entities [29, 36]. However, these models ignore the distinctions between different entities and learn a set of fixed model parameters for all entities, which leads unsatisfactory performance when dealing with a diverse entity set. For instance, it is not intuitve to apply the same discriminative model for Geoffery Hinton and Appleton Museum of Art. The former entity is a computer scientist, while the latter one is a museum. Nevertheless, the global model treats them equally without considering the prior entity class knowledge. We assume that entities from a same class have similar tastes and preferences when citing relevant documents, which means they have similar combination weights in the discriminative model. Therefore, for an entity with little training data, the training data of its similar entities from the same class can be utilized to learn the combination weights. In comparison to the global model, more accurate combination weights are learned for each entity by this manner.
Based on this observation, we build an adaptive discriminative model for different types of entities by utilizing the underlying entity class information, i.e. entity class dependent discriminative mixture model. We introduce an intermediate latent entity class layer and define a joint distribution over the entity-document pairs and latent classes conditioned on the observations. The aim is to achieve relevance estimation through learning a mixture model which is expected to outperform the global model, while maintaining the capability to reveal the hidden correlations between entities and entity classes. The model can be viewed as a hierarchical combination of a discriminative component and a mixing component, so two types of features are required: entity-document features for the discriminative component and entity-class features for the mixing component.
For the discriminative component, we develop a set of bursty features as temporal features in addition to semantic features. The bursty features are detected from two independent data sources: the stream corpus (internal) and certain third-party data (external) like Google Trends.
For the mixing component, we explore two types of entityclass features to model the correlations between entities and hidden classes, including profile-based features and categorybased features. Profile-based features are constructed from the entity's profile in KB, while category-based features rely on the existing category labels for the entity in KB.
To the best of our knowledge, this is the first research work that focuses on modeling correlations between entities and hidden entity classes in discriminative model for CCR. Our model is capable of tackling less popular entities with little training data and unseen entities that do not exist in the training set, which is indispensable in a practical CCR system. Empirical studies have been conducted on TREC-KBA-2013 dataset to show the effectiveness and robustness of the proposed mixture model. Experimental results demonstrate that our model achieves the state-ofthe-art performance on TREC-KBA-2013 dataset.
The rest of this paper is organized as follows. Section 2 summarizes related works. Section 3 introduces an entity class-dependent discriminative mixture model for CCR. Section 4 describes features required in our model, especially the temporally bursty features and their detection methods. Section 5 presents the detailed experimental results and pro-

vides some discussion. Section 6 concludes this paper and points out possible future work.
2. RELATED WORK
Although CCR was first proposed in TREC-KBA tracks, the similar research problem has been studied in several topics of information retrieval.
Topic/Event Detection and Tracking.
Topic Detection and Tracking (TDT) is a track hosted by TREC from 1997 to 2004 [1]. A similar research topic in recent years is event detection. Both TDT and event detection are concerned with the development of techniques for finding and following events in broadcast news or social media. The techniques adopted for TDT and event detection can be broadly classified into two categories: (1) clustering documents based on the semantic distance between them [34], or (2) grouping the frequent words together to represent events [22]. In [22], a finite automaton model is proposed to detect events in stream by modeling events as state transitions. This method has been validated widely by lots of other studies [18, 17, 35]. We also adopt this model to detect KB entities' bursts in the stream corpus and then extract bursty features for them. Different from above works, we model entities' occurrences to capture bursty activities instead of words' occurrences. Another difference between CCR and TDT is that CCR needs to make fine-grained citation-worthiness distinctions between relevant documents further.
Cumulative Citation Recommendation.
TREC has launched the KBA-CCR track since 2012. Participants treat CCR as either a ranking problem [3, 2, 4] or a classification problem [3, 5, 29]. Classification and Learning to Rank methods have been compared and evaluated [2, 15], and both of them can achieve the state-of-art performance with a powerful feature set. Several supervised learning techniques, such as SVM [21], language models [26, 10], Markov Random Fields [7], and Random Forests [4, 5, 29] are utilized. Meanwhile, a variety of relevance scoring methods have been tried, including standard Lucene scoring [6], and custom ranking functions based on entity cooccurrences [25]. A time-aware evaluation paradigm is developed to study time-dependent characteristics of CCR [9].
However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. Entity-unspecific methods, regardless of entity distinctions, are employed to address this problem [29, 28]. Nevertheless, characteristics of different entities are lost in the entity-unspecific methods. Some other researchers employ transfer learning techniques to learn across entities by using entity-unspecific meta-features [36], or utilize a semi-supervised approach to profile an entity by leveraging its related entities and weighting them with the training data [23]. These methods have demonstrated that the correlations between entities are useful for CCR. Nevertheless, all these methods are empirically designed and the performance can be improved further.
What's more, query expansion is often employed because the name of the target entity is too sparse to be a good query. Other name variants and contextual information of terms or related entities from Wikipedia or from the document stream

636

[7, 6] are used to enrich the semantic features of entities. In addition to semantic features, temporal features have been proved especially helpful in CCR [2, 5, 29].
Mixture Model.
Mixture model has been proved effective to address the problem of data insufficiency in several information retrieval tasks, including expert search [11], federated search [19], collaborative filtering [20] and image retrieval [30]. By introducing latent layers to learn flexible combination weights for different feature vectors, mixture model can always outperform simple discriminative models with fixed combination weights. Hence, we propose an entity class-dependent discriminative mixture model to deal with the entities with little training data, which will be described in next section.

3. DISCRIMINATIVE MODEL FOR CCR
This section proposes a novel learning framework by modeling each entity's distribution across hidden entity classes and combining it with a logistic regression model to form a final discriminative model. First we provide a formal definition of the research problem and model it as a classification task, and then present two discriminative models: a global model and an entity class-dependent mixture model.

3.1 Problem Statement
We consider CCR as a binary classification problem that treats the relevant entity-document pairs as positive instances and irrelevant ones as negative instances. Many probabilistic classification techniques in the literature generally fall into two categories: generative models and discriminative models. Discriminative models have attractive theoretical properties [24] and generally perform better than their generative counterparts in the field of information retrieval [16, 32]. Therefore, we adopt discriminative probabilistic models in this paper.
Given a set of KB entities E = {eu}(u = 1, · · · , M ) and a document collection D = {dv}(v = 1, · · · , N ), our objective is to estimate the relevance of a document d to a given entity e. In other words, we need estimate the conditional probability of relevance P (r|e, d) with respect to an entity-document pair (e, d). Each entity-document pair (e, d) is represented as a feature vector f (e, d) = (f1(e, d), · · · , fK (e, d)), where K indicates the number of entity-document features. Moreover, to model the hidden entity class information, each entity can be represented as an entity-class feature vector g(e) = (g1(e), · · · , gL(e)), where L indicates the number of entity-class features. The entity-document features and entity-class features will be introduced in Section 4 later.

3.2 Global Discriminative Model
This paper utilizes logistic regression, a traditional discriminative model, to estimate the conditional probability P (r|e, d), in which r(r  {1, -1}) is a binary label to indicate the relevance of the entity-document pair (e, d). The value of r is 1 if the document d is relevant to the entity e, otherwise r = -1. Formally, the parametric form of P (r = 1|e, d) can be expressed as follows in terms of logistic functions over a linear combination of features,

K

P (r = 1|e, d) = ( ifi(e, d))

(1)

i=1

where (x) = 1/(1 + exp(-x)) is the standard logistic function, and i is the combination parameter for the ith entry of the feature vector. For the irrelevant class, we have

K
P (r = -1|e, d)=1 - P (r = 1|e, d)=(- ifi(e, d)) (2)
i=1

It is worth noting that for different values of r, the only

difference in P (r|e, d) is the sign within the logistic func-

tion. Therefore, we adopt the general representation of

P (r|e, d) = (r

K i=1

i

fi(e,

d))

in

the

following

sections.

The conditional probability of relevance P (r|e, d) represents

the extent to which the document d is relevant to the entity

e. The entity-documents pairs are then classified as positive

or negative according to the value of P (r = 1|e, q). Since the

learned weights are identical for all entity-document pairs

and regardless of specific entities, this model is also denoted

as global discriminative model (GDM) in this paper.

Several other approaches for CCR [5, 29] can be deemed as

global discriminative models adopting different classification

functions such as decision trees and Support Vector Machine

(SVM).

3.3 Entity Class-Dependent Mixture Model
In the GDM introduced in Subsection 3.2, a fixed set of combination weights (i.e., ) are learned to optimize the overall performance for all entities. However, the best combination strategy for a given entity is not always the best for others. The entities stored in KBs are extremely diverse, including persons, organizations, locations, events, etc. Different entities have personalized criteria to detect relevant documents.
We propose an entity class-dependent discriminative mixture model (ECDMM) by introducing an intermediate latent class layer to capture the entity class information in the learning framework. A latent variable z is utilized to indicate which entity class the combination weights z· = (z1, · · · , zK ) are drawn from. The choice of z depends on the target entity e in the entity-document pair (e, d). The joint probability of relevance r and the latent variable z is represented as

P (r, z|e, d; , ) = P (z|e; )P (r|e, d, z; )

(3)

where P (z|e; ) is the mixing coefficient, representing the probability of choosing hidden entity class z given entity e, and  is the corresponding parameter. P (e, d, z; ) denotes the mixture component which takes a logistic functions for r = 1 (or r = -1).  = {zi} is the set of combination parameters where zi is the weight for the ith feature vector entry for the given training instance (e, d) under the hidden class z. By marginalizing out the latent variable z, the corresponding mixture model can be written as

Nz

K

P (r|e, d; , ) = P (z|e; ) r zifi(e, d) (4)

z

i=1

where Nz is the number of latent entity classes. If P (z|e; )

follows the multinomial distribution, the model cannot eas-

ily generalize the combination weights to unseen entities

beyond the training set since each parameter in multino-

mial distribution specifically corresponding to a training en-

tity. To address this problem, we adopt a soft-max function

1 Ze

exp(

Lz j=1

zj

gj

(e))

to

model

P (z|e; )

instead.

zj is

the weight parameter associated with the jth entity feature

637

in the latent entity class z and Ze is the normalization factor that scales the exponential function to be a proper probability distribution. In this representation, each entity e is denoted by a bag of entity-class features (g1(e), · · · , gLz (e)) where Lz is the number of entity features. By plugging the soft-max function into Eq. 4, we can get

1 Nz

Lz

K

P (r|e, d; , )=

exp

Ze z=1

zj gj (e)
j=1



r zifi(e, d)
i=1

(5)

Because zj is associated with each entity feature instead of each entity, the above model allows the estimated zj to be applied to less popular entities and even unseen entities.
Suppose entity-document pairs in training set are represented as T = {(eu, dv)}, and R = {ruv} denotes the corresponding relevance judgment (i.e., +1 or -1) of (eu, dv), where u = 1, · · · , M and v = 1, · · · , N . Assume training instances in T are independently generated, the conditional likelihood of the training data is written as follows.

MN

P (R|T )=

P (ruv|eu, dv)=

u=1 v=1

MN u=1v=1

1 Nz

Lz

K

Zeu

exp(

z=1

j=1

zj gj (eu))(ruv

zifi(eu, dv))
i=1

(6)

3.4 Parameter Estimation
The parameters (i.e.  and ) in Eq. 6 can be estimated by maximizing the following data log-likelihood function,

MN

Nz 1

L(, )=

log

u=1 v=1

( z=1 Zeu

exp( zj gj (eu)))
j=1

(7)

K

(ruv zifi(eu, dv))

i=1

where M is the number of the entities and N is the number of the documents in training set. gj(eu) denotes the jth feature for the uth entity and ruv denotes the relevance judgment for the pair (eu, dv). A typical approach to maximize Eq. 7 is to use Expectation-Maximization (EM) algorithm [8].

E-Step.
The E-step can be derived as follows by computing the posterior probability of z given entity eu and document dv.

P (z|eu, dv) =

exp(

Lz j=1

zj

gj

(eu))(ruv

K i=1

zifi(eu,

dv ))

(8)

z exp(

Lz j=1

zj

gj

(eu))(ruv

K i=1

zifi(eu,

dv ))

M-Step.
By optimize the auxiliary Q function, we can derive the following parameter update rules.

z =

K

(9)

arg max P (z|eu, dv) log (ruv zifi(eu, dv))

z uv

i=1

z = arg max
z u

1

Lz

P (z|eu, dv)
v

log

Zeu

exp(
j=1

zj

gj

(eu))

(10)

The M-step can be optimized by any gradient descent method. To optimize Eq. 9 and Eq. 10, we employ the minFunc toolkit4, a collection of Matlab functions for solving optimization problems using Quasi-Newton strategy. When the value of L(, ) converges to a local optima, the estimated parameters can be plugged back into the model to compute the probability of relevance for entity-document pairs. Since EM is only guaranteed to converge to local optima given different starting points, we try several starting points and choose the model that leads to the greatest log-likelihood.

3.5 Discussion
The ECDMM can exploit the following two advantages over the GDM: (1) the combination weights are able to change across entities and hence lead to a gain of flexibility. (2) it offers probabilistic semantics for the latent entity classes and thus each entity can be associated with multiple classes.

Determining the number of latent Variables.
The number of hidden entity classes can be determined by some model selection criterion. We choose Akaike Information Criteria (AIC), which has been shown suitable in determining the number of latent classes in mixture models. As a measure of the goodness of fit of an estimated statistical model, AIC is defined as

2m - 2L(, )

(11)

where m is the total number of parameters in the model. AIC offers a relative estimation of the information loss when a given model is used to represent the process that generates the data. Given a set of models, the preferred model is the one with the minimum AIC value.

4. FEATURES
In this section, we present the two types of features used in the discriminative models. Entity-document features f (e, d) are used in the discriminative components of GDM and ECDMM. In addition, ECDMM requires entity-class features g(e) to learn the mixing coefficients in the mixture component.
4.1 Entity-Document Features
Entity-document features (i.e., f (e, d)) are composed of semantic and temporal features.
4.1.1 Semantic Features
We adopt the semantic features listed in Table 1, which have been proved effective in CCR [28, 29]. Semantic features can model semantic characteristics of document-entity pairs.
4.1.2 Temporal Features
Entities are evolving in the stream corpus as time goes by, yet semantic features are not capable of portraying the
4http://www.cs.ubc.ca/~schmidtm/Software/minFunc. html

638

Table 1: Semantic features.

Feature

Description

N (erel)

# of entity e's related entities found in its profile page

N (d, e)

# of occurrences of e in document d

N (d, erel)

# of occurrences of the related entities in document d

F P OS(d, e)

First occurrence position of e in d

F P OSn(d, e)

F P OS(d, e) normalized by the document

length

LP OS(d, e)

Last occurrence position of e in d

LP OSn(d, e)

LP OS(d, e) normalized by the document

length

Spread(d, e)

LP OS(d, e) - F P OS(d, e)

Spreadn(d, e) Spread(d, e) normalized by document

length

Simcos(d, si(e)) Cosine similarity between d and the ith section of e's profile

Simjac(d, si(e)) Jaccard similarity between d and the ith section of e's profile

Simcos(d, ci)

Cosine similarity between d and the ith citation of e in the KB

Simjac(d, ci)

Jaccard similarity between d and the ith citation of e in the KB

dynamic characteristics of entities. So we resort temporal features to make up this deficiency. Previous work [2, 5, 29] considering temporal features can be summarized as a straightforward strategy that counts the daily (or hourly) occurrences of target entities in the stream corpus and calculates some statistical indicators as temporal features. To exploit the effectiveness of temporal features, novel bursty features are introduced in this paper. The underlying intuition is that the occurrences of entities in the stream do not distribute uniformly. If the amount of documents referring to an entity increases sharply in a short time period when something important is happening around the entity, this time period is detected as one bursty period of this entity. We make an assumption that documents occur in a bursty period of an entity are more likely to be related to it than those not.
The bursty periods of an entity can be detected either from stream corpus or from third-party data sources, denoted as internal bursty periods and external bursty periods respectively. Due to the heterogeneity of data sources, we use different burst detection methods to identify internal bursty periods and external bursty periods for entities.

Internal Burst Detection.

Burst detection from a stream of documents have been

thoroughly investigated in TDT and event detection [22, 17,

31].

Since our goal is not to develop a new burst detection algo-

rithm, we simply adopt Kleinberg's 2-state finite automaton

model [22] to identify bursty periods of entities. There are

two states q0 and q1 in the finite automaton A. For every

target entity e, when A in state q0, it has low emission rate

0

=

|Rd (e)| T

,

where

Rd(e)

is

the

number

of

all

documents

referring to e over the whole time range T . When A in state

q1,

the

rate

is

increased

to

1

=

s·

|Rd (e)| T

,

where

1

>

0

be-

cause s is a scaling factor larger than 1.0 and s is empirically

set as 2.0 in our work. The larger the number of documents

referring to entity e at time t, the higher the likelihood of e

being identified as a bursty entity at t.

After performing the burst detection algorithm, if the au-
tomaton of entity e is in the state q1 during a time period [tstart, tend], [tstart, tend] is a burtsy period of e with a bursty weight bw(tstart,tend)(e). The bursty weight is defined as the cost improvement incurred by assigning state q1 over the bursty period instead of q0, and can be found in [22].

External Burst Detection.
External resources, such as daily view statistics of entities' profile pages, are utilized as temporal features in previous work [2, 29]. Since some KBs do not provide page view statistics for entities as Wikipedia, we also include Google Trends5 to detect external bursts. Akin to Wikipedia statistics, Google Trends can provide a numeric sequence v = (v1, · · · , vT ) for each entity e, where vi denotes the normalized search volume of e in the ith day.
We detect external bursts of entity e from v with a tailored moving average (MA) method [27]. More concretely, for each vi in v,

1. Calculate a moving average sequence of length w as

M Aw(i)

=

vi

+ vi-1

+ · · · + vi-w+1 w

2. Calculate a cutoff c(i) based on previous MA sequences P reMA = (M Aw(1), · · · , M Aw(i)) as
c(i) = mean(P reMA) +  · std(P reMA)

3. Detect bursty day sequence d, where d = {i|M Aw(i)  c(i)}

4. Calculate the bursty weight sequence w = (w1, · · · , wT ) for e as follows.

 0, i  d



wi

=



M Aw(i) , c(i)

i



d

5. Compact each segment of consecutive days in d into a
bursty period [tstart, tend] of entity e, and the bursty weight bw(tstart,tend) is calculated as the average weight of all the bursts in this period.

The moving average length can be varied to detect long-term or short-term bursts. We set the moving average length as 7 days (i.e., w = 7). The cutoff value is empirically set as 2 times the standard deviation of the M A (i.e.,  = 2).

Bursty Feature Representation.
Given an entity-document pair (e, d), we define a bursty value b(e, d) to represent the temporal correlation between d and e. Let t be the timestamp of d. If t falls in one of e's bursty periods, say [tstart, tend], then b(d, e) is calculated as Eq. 12 shows. If t is not in any bursty period of e, b(d, e) is set as 0.

b(d, e)

=

(1

-

t - tstart ) tend - tstart

·

bw(tstart ,tend ) (e),

(12)

t  [tstart, tend]

In

Eq. 12, 1 -

t-tstart tend -tstart

is

a decaying coefficient

reflecting

the intuition that the documents appear at the beginning

of a bursty period are more informative than those appear

5http://www.google.com/trends/

639

Geoffery Hinton

Labeled Categories
Canadian Computer Scientists
AI Researchers
Fellows of AAAI

Parent Categories
Computer Scientists
Researchers
Fellows of Learned Societies

Labeled Categories
American Computer Scientists
Programming Language Researchers
Fellows of ACM

Barbara Liskov

Figure 1: Two entities without common labeled categories but with shared parent categories.

at the end. Please note that b(d, e) can be calculated based on external bursts and internal bursts respectively. To avoid using future information during burst detection, we carefully perform burst detection algorithm (either internal or external) in a daily incremental manner. When dealing with an entity-document pair, the bursty periods are determined by the data before the timestamp of this document.
4.2 Entity-Class Features
In ECDMM, besides entity-document features, entity-class features (i.e., g(e) in Eq. 5) are required to learn the mixing coefficients. Here we consider two types of prior knowledge to design entity-class features.
4.2.1 Profile-based features
Each entity in KBs is uniquely identified by its profile page, which contains the basic information of this entity, such as name, address and experiences. We crawl the profile pages of all the entities as a profile collection. After removing stop words, we represent each entity as a feature vector with the bag-of-words model, where term weights are determined by the TF-IDF scheme.
4.2.2 Category-based features
Some KBs like Wikipedia organize entities with hierarchical categories. For example, Geoffrey Hinton in Wikipedia, is labeled with categories such as Canadian computer scientists, Artificial intelligence researchers, and Fellows of AAAI. Besides these labeled categories, we take the parent categories of the labeled categories into consideration to deal with the circumstance in Figure 1. The two alike entities can not be correlated if we only consider labeled categories.
Similar to profile-based feature vector, we leverage a "bagof-categories" model to represent each entity as a categorybased feature vector. Given an entity without category information, we manually assign a meta-category for it according to its profile. We supplement three meta-categories: person, facility and organization, which can cover all the entities in our dataset. The category-based feature vector of entity e is denoted as gc(e) = (c1(e), · · · , cN (e)), where N is the total number of categories. ci(e) equals to 1 if e is labeled with category ci, otherwise ci(e) is 0.
Therefore, given a target entity set E, we can generate two feature vectors for each e  E: profile-based vector gp(e) and category-based vector gc(e) respectively.

5. EXPERIMENTS
In this section, we first introduce the dataset for experiments. After that, we report an extensive set of experimental results of our proposed models and baselines in two scenarios of CCR. At last, analysis and discussion are presented based on the experimental results.
5.1 Dataset
We conduct our experiments on TREC-KBA-2013 dataset6, a standard test bed provided by TREC. The data set is composed of a target entity set and a document collection called stream corpus.
Entity Set.
The target entity set includes 121 Wikipedia entities and 20 Twitter entities, more specifically, 98 people, 19 organizations, and 24 facilities from 14 inter-related communities such as small towns like Danville, KY and academic communities like Turing award winners.
Stream Corpus.
The temporally-ordered stream corpus, containing approximately 1 billion documents crawled from October 2011 to the end of February 2013. Each document is associated with a timestamp indicating its time of crawling. The corpus have been split with documents from October 2011 to February 2012 as training instances and the remainder for evaluation. We adopt the same training/test range setting in our experiments.
Annotation.
The relevance of entity-document pairs are labeled following a four-point scale relevance setting, including vital, useful, neural and garbage. The definitions are listed in Table 2.
Table 2: Four-point scale relevance estimation in TREC-KBA-2013.
Vital timely info about the entity's current state, actions, or situation. This would motivate a change to an already up-to-date KB article.
Useful possibly citable but not timely, e.g., background biography, secondary source information.
Neutral informative but not citable, e.g., tertiary source like Wikipedia article itself.
Garbage no information about the target entity could be learned from the document, e.g., spam.
The details of the annotations for Wikipedia and Twitter entities are demonstrated in Table 3.
5.2 Evaluation Scenarios
According to different granularity settings, we evaluate the proposed models in two classification scenarios respectively.
6http://trec-kba.org/kba-stream-corpus-2013.shtml

640

Table 3: The number of training and test instances (entity-document pairs) for Wikipedia and Twitter entities

respectively.

Training

Test

Vital Useful Neutral Garbage Vital Useful Neutral Garbage

Wikipedia 2096 2257 1162

1756 8639 16053 5649 18694

Twitter 182 326

72

569 1808 2953 1491

4103

Total 2278 2583 1234

2325 10447 19006 7140 22797

Vital Only.
Only vital entity-document pairs are treated as positive instances, and the others are negative instances. This scenario is the essential task of CCR.
Vital + Useful.
Both vital and useful entity-document pairs are treated as positive instances, and the others are negative ones.
5.3 Experimental Methodology
Experiments in this section investigate the effectiveness of our proposed mixture model and baseline methods in the two scenarios. The following methods are compared:
· Global Discriminative Model (GDM). As presented in Subsection 3.2, this approach learns a set of fixed weights for all entity-documents pairs.
· Na¨ive Entity Class-Dependent Discriminative Mixture Model (Na¨ive ECDMM). This approach uses entitydocument features instead of entity-class features for the mixing component (i.e., g(e) := f (e, d) in Eq. 5) of ECDMM.
· Profile-based Entity Class-Dependent Discriminative Mixture Model (profile ECDMM). This approach utilizes profile-based features as entity-class features for the mixing component of ECDMM.
· Category-based Entity Class-Dependent Discriminative Mixture Model (category ECDMM). This approach utilizes category-based features as entity-class features for the mixing component of ECDMM.
· Combination Entity Class-Dependent Discriminative Mixture Model (combine ECDMM). This approach utilizes profile-based and category-based features together as entity-class features for the mixing component of ECDMM. In our experimental setting, we simply union the two feature vectors together into an integral feature vector.
For reference, we also include three top-ranked approaches in the TREC-KBA-2013 track as baselines.
· Official Baseline [13]. A string matching approach implemented by TREC-KBA organizers. For each target entity, they split the entity's name into different tokens and manually composite them into reliable aliases of the entity. These alias are utilized to filter relevant documents from the stream corpus with the strategy that documents referring to any alias are rated as vital to the target entity. A relevance score is estimated according to the length of matched string.

· BIT-MSRA [29]. An entity-unspecific random forests classification model, which is the first place approach in TREC-KBA-2013 track. This approach can be considered as a variant of GDM utilizing a different kernel.
· UDEL [23]. An entity-centric query expansion approach that achieves the second best performance in TREC-KBA-2013 track. Given a target entity, the approach first detect related entities from the profile page of the entity. Then, these related entities are utilized as expansion terms and combine with the target entity as a new query to detect and rank the relevant documents. The optimal weights of query terms are learned from the training data. The relevance score of a document is estimated according to its position in the ranking list.
5.4 Hidden Classes Analysis
For all mixture models, the number of hidden classes are determined according to AIC value. The optimal numbers of latent classes of all variants of ECDMM are reported in Table 4. The number of optimal classes of category ECDMM is obviously larger than the optimal numbers of the other mixture models, which possibly caused by the hierarchical structures of categories in our category-based feature set. Although the incorporation of parent categories can build the correlation between two similar entities without common labeled categories, it brings some noisy correlations in the meantime. For instance, a politician and a business man both living in Florida share a common parent category "Living people from Ocala, FLorida", this correlation will mislead the model and come to an non-optimal fit to the data.

Table 4: Number of hidden classes determined by

AIC for each mixture model.

Model

Vital Vital + Useful

na¨ive ECDMM

9

10

profile ECDMM 7

6

category ECDMM 13

12

combine ECDMM 9

8

5.5 Overall Results
This section presents the overall performance of all experimental methods. We adopt F1 (harmonic mean between precision and recall), accuracy and AUC (Area Under Curve) [12] as the evaluation measurements. All the measurements are computed in an entity-insensitive manner. In other words, the measurements are computed based on the test pool of all entity-document pairs regardless of specific entities. The results are reported in Table 5.

641

Table 5: Overall classification results of evaluated models.

Methods

Vital Only

Vital + Useful

P

R F1 Accu AUC P

R F1 Accu

Official Baseline .171 .942 .290 .175 .475 .540 .972 .694 .532

BIT-MSRA

.214 .790 .337 .445 .580 .589 .974 .734 .615

UDEL

.169 .806 .280 .259 .473 .573 .893 .698 .579

GDM

.218 .507 .304 .587 .556 .604 .913 .727 .565

na¨ive ECDMM .223 .400 .286 .644 .548 .627 .912 .744 .656

profile ECDMM .332 .376 .353 .754 .606 .669 .866 .755 .692

category ECDMM .316 .422 .362 ..734 .612 .672 .894 .767 .704

combine ECDMM .397 .418 .407 .783 .640 .703 .877 .780 .731

AUC .488 .578 .547 .588 .631 .675 .685 .716

We notice that combine ECDMM achieves best on all measurements except recall. The official baseline achieve the best recall of all methods, which is not surprising since the official baseline is a manual method to detect as many relevant documents as possible by manually selecting reliable aliases of an entity in advance.
Compared with GDM regardless of entity class information, all the mixture models employing entity-class features explicitly (i.e., profile ECDMM, category ECDMM and combine ECDMM) achieve better classification performance in both scenarios. Even Na¨ive ECDMM which does not employ entity-class features explicitly can outperform GDM and other three baselines. This reveals that the mixture model is an effective strategy to enhance the straightforward discriminative model. Na¨ive ECDMM is not robust in two scenarios. Although it outperforms GDM in vital + useful scenario, it cannot beat GDM in vital only scenario. This is possibly caused by its implicitly employment of entityclass features. The entity-document features are noisy, because the document-related counterpart contributes nothing to capture hidden entity classes.
Both profile ECDMM and category ECDMM outperform na¨ive ECDMM remarkably, revealing that profile-based features and category-based features are effective in modeling hidden entity classes. Category-based features are more promising than profile-based features, which is reasonable because the category labels in KBs contain prior human knowledge on entity class and taxonomy information. Even though combine ECDMM combines profile-based features and category-based features in a straightforward manner, it achieves the best performance. In comparison to GDM, combine ECDMM improve F1 more than 10 percent and AU C approximately 10 percent. We believe that the performance can be enhanced further with more comprehensive entity class information and combination strategy.

5.6 Fine-grained Results

This section compares the methods in a fine-grained level.

We need guarantee our mixture models not only achieve

remarkable overall performance, but also perform well in

entity-level. Hence, we recomputed the measurements in an

entity-sensitive manner.

Based on the classification results for each entity ei (i =

1, · · · , M ) in the test set, precision and recall of each model

are first calculated as P (ei) and R(ei). Then, we compute

the macro-averaged precision and recall over all entities, de-

noted as macro P =

M 1

P (ei)

M

and

macro

R

=

M 1

R(ei )

M

respectively. At last, macro-averaged F1 is computed ac-

cording to macro P and macro R.

The macro-averaged measurements in two scenarios are reported in 2(a) and 2(b) respectively. The three baselines are labeled with red color, and the blue dots represent our proposed methods. The best method is labeled with pentagram in both figures. The parallel solid curves are contour lines of F1 value, which means the dots in the same curve achieve same F1 values. The dots lying in upper right achieve higher F1 than the lower left ones. Obviously, combine ECDMM achieves the best F1 in both scenarios. We also find our mixture models (i.e., blue dots) achieve higher precision in vital only scenario, demonstrating our models can detect vital documents more accurately than the baselines.
5.7 Performance on Unseen Entities
This section evaluates the generalization ability of our proposed models to handle unseen entities in the training set. A robust model is able to handle unseen entities as well as training entities. As listed in Table 6, there are 10 unseen entities in the TREC-KBA-2013 dataset. We evaluate the performance of our models on the unseen entity set composed of these 10 entities. We choose macro-averaged accuracy as the evaluation measurement. Due to the sparse positive instances for some unseen entities, it is improper to adopt precision, recall and F1 for evaluation because they possibly become 0, in which case these measurements cannot reflect the performance suitably. The results are reported in Table 7.

Table 7: The averages of accuracies over 10 unseen

entities. Methods

accu@(vital) accu@(vital + useful)

Official Baseline

.175

.532

BIT-MSRA

.445

.614

UDEL

.259

.579

GDM

.552

.565

na¨ive ECDMM

.587

.608

profile ECDMM

.623

.647

category ECDMM

.565

.431

combine ECDMM

.580

.582

In both scenarios, the best classification results are achieved by profile ECDMM, which outperforms category ECDMM and combine ECDMM. A possible explanation for the unsatisfactory performance of category ECDMM is that the category information of unseen entities are not covered well in the training set, especially the Twitter entities. For these entities, there is too little category information to model their hidden classes accurately.

642

Recall Recall

0.9

Official Baseline

BIT-MSRA

UDEL

GDM

0.8

naïve_ECDMM

profile_ECDMM

category_ECDMM

combine_ECDMM

0.7

0.6

0.5

0.4

0.3

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

Precision

(a) Vital Only

0.8 0.75
0.7 0.65
0.6 0.45

Official Baseline BIT-MSRA UDEL GDM naïve_ECDMM profile_ECDMM category_ECDMM combine_ECDMM

0.5

0.55

0.6

0.65

0.7

Precision

(b) Vital + Useful

Figure 2: Macro-averaged recall VS. macro-averaged precision over all test entities. The best approach is dotted as pentagram.

Table 6: The statistics of test instances for 10 unseen entities.

Entity

KB vital useful neutral/garbage

The Ritz Apartment (Ocala,Florida) Wiki 4

1

5

Keri Hehn

Wiki 3

0

0

Chiara Nappi

Wiki 2

3

55

Chuck Pankow

Wiki 7

0

10

John H. Lang

Wiki 2

0

1

Joshua Boschee

Wiki 191 23

5

MissMarcel

Twitter 52 13

3

evvnt

Twitter 1

3

40

GandBcoffee

Twitter 0

2

2

BartowMcDonald

Twitter 1

18

9

total 10 3 60 17 3 219 68 44 4 28

In vital only scenario, all the variants of our mixture model can achieve better classification performance than GDM and the other baselines. The results validate the flexibility of our mixture model as expectation, which is essential for a practical CCR system. Our mixture model is not only good at handling existing entities in the training set, but also capable of dealing with unseen entities.
5.8 Bursty Feature Analysis
To further validate the effectiveness of the proposed bursty features, we evaluate them with the help of Information Gain (IG). Table 8 reports the IGs of the proposed features in two scenarios. All the IGs are computed following the method proposed in [33]. The higher of the IG achieved by a feature, the more powerful role it plays in the classification. The maximum, mean and median IGs of semantic features are also presented for reference. Since the bursty features are only used in the discriminative counterpart of ECDMM, we evaluate them with GDM.
In Table 8, external bursty features perform best out of all features in both scenarios, conforming that external bursts of an entity are accompanied with occurrences of its relevant documents. However, internal bursts are not so helpful in vital + useful scenario as in vital only scenario. This is possibly caused by the incompleteness of the stream corpus. As we know, the stream corpus are crawled from the web, so it is possibly a biased snapshot of the true web. In addition, we only utilize the occurrences of a target entity itself in the stream corpus to detect its internal bursts currently. We can include more evidences to improve the accuracy of

Table 8: Information gain values of features.

Information Gain

Feature

Vital Only Vital + Useful

external bursty feature 0.130

0.286

internal bursty feature 0.020

0.008

max1 mean 2 median3

0.121 0.046 0.039

0.175 0.081 0.067

1 maximum IG of all semantic features 2 mean IG of all semantic features 3 median IG of all semantic features

internal burst detection. For instance, contextual related entities can be resorted to enhance the detection accuracy of internal burst.
6. CONCLUSIONS AND FUTURE WORK
The objective of Cumulative Citation Recommendation (CCR) is to detect citation-worthy documents for a set of KB entities from a chronological stream corpus. To address the problem of training data insufficiency for less popular entities, we propose an entity class-dependent discriminative mixture model (ECDMM) by introducing a latent entity class layer to model the hidden entity class information. The model can be adjusted to different types of entities by learning flexible combination parameters according to underlying entity classes. Experimental results demonstrate that ECDMM can improve the performance of CCR. Entity-

643

document features and entity-class features are developed for the discriminative and mixing components of ECDMM respectively. In terms of entity-class features, profile-based and category-based features are validated separately and in a combination strategy. The novel bursty features developed as entity-document features are proved rewarding. Our ECDMM with proposed semantic and temporal features can achieve the state-of-the-art performance on TREC-KBA2013 dataset.
For future work, we wish to explore more useful entityclass features and apply more proper combination strategies to improve the entity class-dependent mixture model.
Acknowledgement
The authors would like to thank Jing Liu and Ning Zhang for their valuable suggestions and the anonymous reviewers for their helpful comments. This work is funded by the National Program on Key Basic Research Project (973 Program, Grant No. 2013CB329600), National Natural Science Foundation of China (NSFC, Grant Nos. 61472040 and 60873237), and Beijing Higher Education Young Elite Teacher Project (Grant No. YETP1198).
7. REFERENCES
[1] J. Allan. Introduction to topic detection and tracking. In Topic Detection and Tracking, volume 12 of The Information Retrieval Series, pages 1­16. Springer US, 2002.
[2] K. Balog and H. Ramampiaro. Cumulative citation recommendation: classification vs. ranking. In SIGIR, pages 941­944. ACM, 2013.
[3] K. Balog, H. Ramampiaro, N. Takhirov, and K. Nørv°ag. Multi-step classification approaches to cumulative citation recommendation. In OAIR, pages 121­128. ACM, 2013.
[4] R. Berendsen, E. Meij, D. Odijk, M. d. Rijke, and W. Weerkamp. The university of amsterdam at trec 2012. In TREC. NIST, 2012.
[5] L. Bonnefoy, V. Bouvier, and P. Bellot. A weakly-supervised detection of entity central documents in a stream. In SIGIR, pages 769­772. ACM, 2013.
[6] Z. W. C. Tompkins and S. G. Small. Sawus: Siena's automatic wikipedia update system. In TREC. NIST, 2012.
[7] J. Dalton and L. Dietz. Bi-directional linkability from wikipedia to documents and back again: Umass at trec 2012 knowledge base acceleration track. In TREC. NIST, 2012.
[8] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), pages 1­38, 1977.
[9] L. Dietz and J. Dalton. Time-aware evaluation of cumulative citation recommendation systems. In SIGIR 2013 Workshop on Time-aware Information Access (TAIA2013), 2013.
[10] L. Dietz, J. Dalton, and K. Balog. Umass at trec 2013 knowledge base acceleration track. In TREC. NIST, 2013.
[11] Y. Fang, L. Si, and A. Mathur. Discriminative probabilistic models for expert search in heterogeneous information sources. Information Retrieval, 14(2):158­177, 2011.
[12] T. Fawcett. An introduction to roc analysis. Pattern Recogn. Lett., 27(8):861­874, June 2006.
[13] J. Frank, S. J. Bauer, M. Kleiman-Weiner, D. A. Roberts, N. Triouraneni, C. Zhang, and C. R`e. Evaluating stream filtering for entity profile updates for trec 2013. In TREC. NIST, 2013.
[14] J. R. Frank, M. Kleiman-Weiner, D. A. Roberts, F. Niu, C. Zhang, C. Re, and I. Soboroff. Building an Entity-Centric Stream Filtering Test Collection for TREC 2012. In TREC. NIST, 2012.

[15] G. G. Gebremeskel, J. He, A. P. d. Vries, and J. Lin. Cumulative citation recommendation: A feature-aware comparison of approaches. In Database and Expert Systems Applications (DEXA), pages 193­197. IEEE, 2014.
[16] A. Genkin, D. D. Lewis, and D. Madigan. Large-scale bayesian logistic regression for text categorization. Technometrics, 2007.
[17] Q. He, K. Chang, and E.-P. Lim. Using burstiness to improve clustering of topics in news streams. In ICDM, pages 493­498. IEEE, 2007.
[18] Q. He, K. Chang, E.-P. Lim, and J. Zhang. Bursty feature representation for clustering text streams. In SDM, pages 491­496. SIAM, 2007.
[19] D. Hong and L. Si. Mixture model with multiple centralized retrieval algorithms for result merging in federated search. In SIGIR, pages 821­830. ACM, 2012.
[20] R. Jin, L. Si, and C. Zhai. A study of mixture models for collaborative filtering. Information Retrieval, 9(3):357­382, 2006.
[21] B. Kjersten and P. McNamee. The hltcoe approach to the trec 2012 kba track. In TREC. NIST, 2012.
[22] J. Kleinberg. Bursty and hierarchical structure in streams. In KDD, pages 91­101. ACM, 2002.
[23] X. Liu, J. Darko, and H. Fang. A related entity based approach for knowledge base acceleration. In TREC. NIST, 2013.
[24] A. Y. Ng and M. I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 841­848. MIT Press, 2002.
[25] A. D. O. Gross and H. Toivonen. Term association analysis for named entity filtering. In TREC. NIST, 2012.
[26] J. H. C. B. S. Araujo, G. Gebremeskel and A. de Vries. Cwi at trec 2012 kba track and session track. In TREC. NIST, 2012.
[27] M. Vlachos, C. Meek, Z. Vagena, and D. Gunopulos. Identifying similarities, periodicities and bursts for online search queries. In SIGMOD, pages 131­142. ACM, 2004.
[28] J. Wang, L. Liao, D. Song, L. Ma, C.-Y. Lin, and Y. Rui. Resorting relevance evidences to cumulative citation recommendation for knowledge base acceleration. In WAIM, 2015.
[29] J. Wang, D. Song, C.-Y. Lin, and L. Liao. Bit and msra at trec kba ccr track 2013. In TREC. NIST, 2013.
[30] Q. Wang, L. Si, and D. Zhang. A discriminative data-dependent mixture-model approach for multiple instance learning in image classification. In ECCV, pages 660­673. 2012.
[31] J. Weng and B.-S. Lee. Event detection in twitter. In ICWSM, volume 11, pages 401­408. AAAI, 2011.
[32] Y. Yang and X. Liu. A re-examination of text categorization methods. In SIGIR, pages 42­49. ACM, 1999.
[33] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization. In ICML, pages 412­420, 1997.
[34] Y. Yang, T. Pierce, and J. Carbonell. A study of retrospective and on-line event detection. In SIGIR, pages 28­36. ACM, 1998.
[35] W. X. Zhao, R. Chen, K. Fan, H. Yan, and X. Li. A novel burst-based text representation model for scalable event detection. In ACL, pages 43­47. ACL, 2012.
[36] M. Zhou and K. C.-C. Chang. Entity-centric document filtering: boosting feature mapping through meta-features. In CIKM, pages 119­128. ACM, 2013.

644

Evaluating Streams of Evolving News Events

Gaurav Baruah
Computer Science
University of Waterloo gbaruah@uwaterloo.ca

Mark D. Smucker

Charles L. A. Clarke

Management Sciences

Computer Science

University of Waterloo

University of Waterloo

mark.smucker@uwaterloo.ca claclark@plg.uwaterloo.ca

ABSTRACT
People track news events according to their interests and available time. For a major event of great personal interest, they might check for updates several times an hour, taking time to keep abreast of all aspects of the evolving event. For minor events of more marginal interest, they might check back once or twice a day for a few minutes to learn about the most significant developments. Systems generating streams of updates about evolving events can improve user performance by appropriately filtering these updates, making it easy for users to track events in a timely manner without undue information overload. Unfortunately, predicting user performance on these systems poses a significant challenge. Standard evaluation methodology, designed for Web search and other adhoc retrieval tasks, adapts poorly to this context. In this paper, we develop a simple model that simulates users checking the system from time to time to read updates. For each simulated user, we generate a trace of their activities alternating between away times and reading times. These traces are then applied to measure system effectiveness. We test our model using data from the TREC 2013 Temporal Summarization Track (TST) comparing it to the effectiveness measures used in that track. The primary TST measure corresponds most closely with a modeled user that checks back once a day on average for an average of one minute. Users checking more frequently for longer times may view the relative performance of participating systems quite differently. In light of this sensitivity to user behavior, we recommend that future experiments be built around clearly stated assumptions regarding user interfaces and access patterns, with effectiveness measures reflecting these assumptions.
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software -- Performance evaluation (efficiency and effectiveness)
Keywords: search; streams; evaluation
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767751.

1. INTRODUCTION
On December 4, 2012, Typhoon Bopha made landfall on the eastern coast of the Philippine island of Mindanao. The strongest typhoon in the island's recorded history, the storm system had formed in the Pacific in late November, growing into a tropical storm by November 28th, and into a severe tropical storm by November 30th. On December 1st, as the storm moved west towards the island, it rapidly intensified into a category 4 typhoon. Two days later, only miles from shore, it reached category 5, a super typhoon, with winds up to 150 mph. At 4:45AM it hit. Heavy rains and waves exceeding 25 feet washed across villages and towns, destroying homes, flooding roads, and cutting communications. Initial reports limited the death toll to one, but by the next day it had increased to over 200, mostly killed by flooding and debris. Over the next week, the number of fatalities grew to a final total of over 600, with more than 200,000 people displaced or otherwise impacted.
In this paper we consider how to evaluate a search engine that aggregates a stream of information and provides a means for people to obtain the latest, most relevant information regarding an event of interest. As time permits, a user of such a search engine will check back multiple times in order to find out new information about the event.
We envision that there will be a wide variety of users of such a search engine. Government and health care workers, as well as people in the immediate vicinity, would likely need to learn of new information soon after it becomes available. People interested in the event, but not directly affected, would likely have a less urgent need for new information. Some people would be willing to consume a large amount of content in the hope of not missing any relevant information, while others may desire a minimal amount of content.
One way of designing an information retrieval system for streams of evolving events would be to allow users to visit and query the system whenever they want the latest information. In this type of system, the user pulls information. An alternative approach would be to create a system that pushes select pieces of information to users that have set up a search query or profile. In either case, users would want to control the frequency and duration of their interaction with the system. In the pull scenario, the user controls the frequency by how often they visit and controls the duration by how long they consume content during a visit's session. In the push scenario, the user would need to specify up front how often content should be sent to them as well as the amount of content to be sent.

675

In this paper, we propose a new effectiveness measure for the evaluation of retrieval systems that produce streams of content for evolving events. We call this measure modeled stream utility (MSU). This new effectiveness measure models users in terms of the time spent away from the system and the time spent with the system. In effect, we model users in terms of their frequency and duration of interaction with the system, which is a model applicable to either push or pull variants of this type of retrieval system. We measure utility in the number of relevant information nuggets the user is likely to find. Search users naturally vary, and we model this variation as well by modeling a population of users and simulating many different users' interaction with the retrieval system. A significant advantage of MSU as an effectiveness measure is that by making it user-centered, the measure has the potential to be easily calibrated using recorded user behavior.
We demonstrate MSU using the TREC 2013 Temporal Summarization Track [4]. Participating groups in the temporal summarization track (TST) take a large stream of content and attempt to produce a filtered stream of content relevant to a given search topic. For each topic, the track has determined a set of relevant nuggets of information and has judged a pool of content and determined which nuggets are present in the content.
To evaluate the runs submitted to TST, we use MSU in a scenario where users pull information from the retrieval system. MSU simulates user interaction with the retrieval system. When a simulated user visits the retrieval system and queries it for a given search topic, the user is shown the most recently filtered content in reverse chronological order. In TST, filtered content consists of sentence length material extracted from a stream of larger documents. After a simulated user has read an element of content, we measure the user's gain in terms of the number of nuggets the user is likely to have found relevant in the content. If a user has read a nugget earlier, MSU assumes that a repeated nugget will not be found relevant and has a gain value of zero. The temporal summarization track has estimated when nuggets are first known in the aggregate stream, and in the MSU simulation, nuggets delivered late to a user are less likely to be found relevant.
In addition to demonstrating MSU applied to the TREC 2013 temporal summarization track, we compare MSU to the track's existing primary measure, ELG, as well as conduct a parameter sweep over a range of the MSU user model's parameters to better understand how the measure behaves. We find that:
· When we set MSU's parameters to that of a reasonably interested user who visits the system for 2 minutes every 3 hours on average, MSU's ranking of TST submitted runs is significantly different from TST's primary measure, ELG (Kendall's  = 0.47).
· Of the parameter settings included in our sweep, ELG correlates best with a model of users who visit for only 1 minute per day (Kendall's  = 0.62). In terms of content consumed, a minute-long visit amounts to a user reading approximately 4-5 sentences.
· The ranking of systems produced by the MSU effectiveness measure appears to be most sensitive to the amount of content consumed by the simulated users.

Given that the amount of content consumed appears to affect system rankings the most, this result has implications for the design of retrieval systems that generate streams of information updates. The temporal summarization track gave participating groups the task of filtering a stream, but different users are going to be interested in different amounts of material. Rather than filter a stream, retrieval systems for evolving events may be better designed as best-match rankers that must balance recency and relevancy. No matter how such systems are designed, MSU provides a usercentered measure that allows for calibration with actual measured user behavior.
2. BACKGROUND
The news about an event, such as Typhoon Bopha, has an impact on large sections of society. The sudden onset of such events can generate related information needs among governments, aid agencies, observer groups, as well as those affected and their families. Usually such events are unexpected and can be dynamic and evolve over time.
A large number of documents will be authored in connection with significant events and made available for consumption via the Internet in near real time. Publishers of this content include social networking services such as Twitter and Facebook as well as newspapers, news feeds, and blogs. Authors can range from journalists to government agencies to persons experiencing the event first hand. We consider all newly created content in aggregate as a stream of material from which a person might want to find relevant information.
The closest analogues to this type of retrieval are microblog search [12] and temporal summarization [2, 4]. The TREC 2011 Microblog track [12] included a single adhoc retrieval task where the retrieval system receives a query at time t and must retrieve the most relevant and recent Twitter postings prior to time t. The track organizers used precision at rank 30 to evaluate submissions, but found that the task was under-specified with respect to how results should be ranked in terms of relevance and recency, making comparisons between participating groups difficult.
In the TREC 2012 Microblog track [14], the adhoc retrieval task remained and was adjusted so that participants were simply to score the microblog posts for ranking purposes. The only notion of recency was that the postings had to have occurred before the time of the query. Relevance of a posting was formulated with respect to the informativeness of the posting without considerations of recency or novelty. To evaluate a ranking, the organizers computed its precision at 30 and its receiver operating characteristic (ROC). For TREC 2013, the microblog track continued to use precision at 30 as its primary retrieval measure [11].
Temporal summarization systems attempt to extract relevant information from an aggregate stream and relay this information to the user as soon as possible. In contrast to a microblog search task, a temporal summarization task extends over a period that roughly corresponds to the period that an event is in the news. Being a summarization task, the evaluation of temporal summarization systems has focused on determining the overall quality of the resulting summary even though the nature of temporal summarization means that the summary grows over time and is likely being consumed by an end user at regular intervals, e.g. for

676

a multi-day event, we would expect a user to seek new information daily.
The temporal summarization work of Allan, Gupta, and Khandelwal [2] defined new precision and recall measures that incorporate notions of usefulness and novelty. A useful sentence is one that would be helpful for use in a summary and a novel sentence is one that is new and contains information not previously seen. In addition, they examine the importance of summary size and recency of information.
Guo et al. [10] primarily focus on finding updates for rapidly evolving news events for which the information need is urgent (or time-critical). They consider an event to be composed of a number of subtopics. Each update may contain one or more subtopics as well. Accordingly, the precision for an update is the fraction of the update's subtopics that are also subtopics for the event, and, the recall for an update is the the fraction of the event's subtopics contained in the update. They define measures expected precision and expected recall to be the average precision (and recall respectively) over the complete set of updates.
The TREC 2013 Temporal Summarization Track (TST) [4] follows the temporal summarization framework defined by Allan et al. [2] with two significant changes. The first change is that while Allan et al. required that summarization happen on-the-fly as each news story was received, in TST, systems are only restricted to be certain that summaries produced at time t only use content available at t or earlier. For example, if a system wanted to, it could wait to process several days of content before producing a single result. The TST directly evaluates the recency of information and thus does not have to restrict systems in when or how they produce summaries. The second change is that the TST uses a web-scale aggregate stream of world wide web content as opposed to the comparatively small collection available to Allan et al. in 2001.
The TST uses sentences (updates in TST terminology) as the primary retrieval unit. Groups participating in TST had the task of filtering the aggregate stream and emitting sentences containing timely and novel information regarding a news event. The TST judges the relevance of sentences by noting the presence of individual nuggets of information. Sentences are annotated such that it is known what pieces (nuggets) of relevant information the sentences contain.
The TST assessors identified nuggets about an event from Wikipedia. For example, Typhoon Bopha has its own detailed Wikipedia page1. TST attached a timestamp to every nugget as determined by the first occurrence of the nugget in the Wikipedia edit history for the topic's article.
The TST organizers crafted effectiveness measures that are nugget-based variants of precision and recall that incorporate several key criteria for the updates produced by a system. In addition to an update needing to contain novel nuggets, the nuggets should be emitted as early as possible (latency) and shorter updates are better than longer updates (verbosity). Each update is assigned a score based on these criteria and the track's set-based effectiveness measures are computed as averages over all updates.
Participating groups produced runs consisting of emitted updates. When an update is emitted, it is given a timestamp equal to the most recent material consumed by the system from the aggregate stream of material. Thus, a system that
1http://en.wikipedia.org/wiki/Typhoon_Bopha

delayed emitting any updates until it had consumed all of the material in the period of interest, would have all of its updates given a timestamp equal to the very end of the period. If an update contains a nugget, the nugget's latency is the difference between the update's timestamp and the nugget's Wikipedia assigned time. The gain for a reported nugget is discounted as a function of its latency, such that, with increasing latency, the gain for a nugget drops substantially within the first 24 hours, after which its discounted gain stays under 20%. It is possible for an update to be "earlier" than the nugget's timestamp, in which case the latency discount function awards the update a bonus for reporting the nugget early. Reflecting novelty, repeated nuggets have no value, providing zero gain.
Finally, sentences are penalized for being overly verbose. To incorporate verbosity into the track's measures, an update may count as more than a single update in the computation of the average. The verbosity of an update is based on the extra number of nuggets the update could have contained. For example, if an update contains zero nuggets and is 60 words long, and if the average number of words per nugget is 15, then this update has a verbosity penalty of 60 / 15 = 4 and when averaged counts as 5 updates, i.e. itself (1) plus its verbosity penalty (4). If the number of words in an update equals the sum of the words in the nuggets in the update, then the update counts as a single update.
TST's evaluation has two primary metrics, expected latency gain (ELG), which measures the gain per update while discounting latency and penalizing verbosity, and latency comprehensiveness (LC), which measures coverage of nuggets relating to the topic. ELG and LC are analogous to precision and recall respectively. While both ELG and LC are important to be considered together, we consider the ELG measure to be the primary measure of the track given that runs were presented from best to worst ELG in the track overview [4]. ELG for a set D of system generated updates is formulated as,

ELGV(D) =

1

G(d, D)

dD V(d) dD

(1)

where, V(d) is the verbosity normalization of update d, G(d, D) is the latency discounted gain for update d, and G(d, D) is non zero when d is the earliest update from the set D to report one or more nuggets for the topic. Once a nugget is reported, it does not contribute to gain if it appears again in later updates. The LC metric replaces the denominator in equation (1), with nN R(n), where N is the set of nuggets identified by the assessors and R(n) is the relevance for n based on its importance (R(n) = 1 for binary relevance). Essentially LC computes the recall of relevant material by the system. Unjudged sentences are elided from run submissions.
It is clear that users are kept in mind in the formulation of effectiveness measures for temporal summarization. For example, an update should be timely and contain relevant, novel information without extraneous material. In addition to these qualities, we believe that it important to also consider the user's desired frequency and duration of interaction with the system. A government agency may well assemble a team of people to monitor a stream of updates 24 hours a day for the duration of a crisis. An individual in the midst of a crisis may be limited by extrinsic circumstances to only having a few moments each day to check for updates.

677

ID Time Nugget n9 12/05/12 The typhoon destroyed 70-80% of planta-
15:13:56 tions, mostly bananas for export. n10 12/06/12 Damage to agriculture and infrastructure in
17:45:12 Compostela Valley province could reach at least 4 billion pesos, equivalent to 75 million or $98 million U.S.
n11 12/04/12 About 40 people were killed or missing in 18:31:18 flash floods and landslides near a mining area on Mindanao.
n12 12/04/12 Typhoon Bopha made landfall on Mindanao 03:17:18 early on December 4 as a category 4.
n13 12/05/12 Late on December 3, Bopha made landfall 10:31:21 over Baganga, Mindanao, as a category 5 super typhoon.
n14 12/05/12 As of 5 December, 238 deaths had been re13:55:42 ported on Mindana with hundreds missing.
Table 1: Nuggets reported to user (read by user) in the example session (Table 2). Times for each nugget are their time of first occurrence in Wikipedia's edit history.
3. MODELED STREAM UTILITY
We have designed our new effectiveness measure, modeled stream utility (MSU), to be an easily calibrated usercentered effectiveness measure. A user-centered effectiveness measure must take into consideration both the user interface of the retrieval system and user behavior with the user interface. For an effectiveness measures to be easily calibrated to user behavior, the measure's user model must be designed such that the model's parameters can be set based on actual measurements of user behavior.
3.1 User Interface and User Behavior
Our hypothetical user interface provides a means for the user to query the system and receive a ranked list of results (i.e. updates about an event). Each result is a short piece of text. The retrieval system produces a stream of information that is consumed by the user in the order produced. In other words, after issuing a query and receiving the ranked list of results, the user reads the results in rank order. No other interaction with the retrieval system is possible.
Our simulated user is not limited to one visit with the retrieval system. Our simulated user will repeatedly visit the search engine with a frequency reflective of their interest in the evolving event or reflective of their availability to visit. On each visit, the simulated user enters the same query, and expects to receive the most recent and relevant updates concerning their event of interest. As with their frequency of visits, the simulated user will read during a visit for an amount of time reflective of their interest or time available. We will call visits sessions, with each having a duration.
No two users are the same. Each user will visit the search engine with different frequencies and durations. Each user will have their own reading speed. We model frequency and duration by modeling the time a user spends away from the system and by separately modeling the time a user spends with the system. We model each user with three parameters:
· A: Average time away.
· D: Average session duration.
· V : Reading speed, e.g. words per minute.

To estimate user performance with the retrieval system, we draw multiple simulated users from population distributions and average user performance over all simulated users. We model the user population's time away and their session duration with two separate log-normal distributions. Lognormal distributions are such that when one takes the natural log of the data, a normal distribution fits the resulting distribution. Usually one describes a log-normal distribution in terms of the mean, , and standard deviation, , of the normal distribution fit to the log of the data.
For both the time-away and the session-duration distributions, we will describe them in this paper in terms of the underlying distribution's mean and standard deviation. If the underlying data has mean M and standard deviation S, then, for the log-normal distribution, the variance is 2 = log(1 + S2/M 2), and the mean is  = log(M ) - 0.52. As we describe later in the paper, we consider a wide range of user behavior by varying the time away and session duration population distributions. To model the population's reading speed, we use the log-normal parameters from Clarke and Smucker [8] wherein, the distribution of reading speed across users is described by a log-normal distribution with  = 1.29 and  = 0.558.
Given these three population distributions, we can generate as many simulated users as needed for numerical precision in our estimate of average user performance. For example, a possible reasonable setting of the population parameters is:
· Time away mean, MA = 3 hours, and standard deviation, SA = 1.5 hours.
· Session duration mean, MD = 2 minutes, and standard deviation, SD = 1 minute.
· Reading speed log-normal (words per second):  = 1.29 and  = 0.558, i.e. a mean reading speed of 255 words per minute.
A random user drawn from this distribution will spend more or less time away, have longer or shorter session durations, and read faster or slower than the above population averages. Overall, the population of simulated users will have average behavior equal to the population means. To simulate a single user's interaction with the search engine over a given period of time, we construct a user-trace (Figure 1d) consisting of alternating sessions with the search engine and time intervals spent away from the search engine.
It would be unusual for a user to visit on a fixed frequency and visit for the exact same amount of time on each visit. To model variation in a user's time away and session duration, we use their mean time away and mean session duration as parameters to two separate exponential distributions. An exponential distribution is commonly used to model phenomena such as the time between radioactive decay of nuclei or the time between events in a Poisson process. For example, to simulate random session duration times with a mean of D, we draw random deviates from an exponential distribution [9] with a rate parameter of  = 1/D.
3.2 Gain
A simulated user will repeatedly visit the search engine. During a visit, the user will read the search results in rank order. Each search result has an amount of gain (possibly zero) associated with it. The simulated user will accumulate

678

Time 9:52
9:52
9:52 9:52
9:50
9:15 7:45
7:31

Conf. 0.95
0.95
0.95 0.91
0.87
0.91 0.87
0.87

Update Sentence Produced by System at Time Shown in Column 1
Typhoon Bopha , with central winds of 75 mph and gusts of up to 93 mph, battered beach resorts and dive spots in northern Palawan on Wednesday, but there was little damage as the storm began to weaken.
Hardest hit were the coastal, farming and mining towns in the southern Mindanao region, where Bopha made landfall on Tuesday, destroying homes, causing landslides and flash flooding and killing at least 230 people.
About 60 people died in the municipality of New Bataan alone and around 245 were still missing, Uy said, adding the area was initially cut off by road blocks.
Damage to agriculture and infrastructure in Compostela Valley province could reach at least 4 billion pesos ($98 million), with the typhoon destroying 70-80 percentof plantations, mostly bananas for export, Uy said.
A man looks at the dead bodies of relatives killed by landslides after Typhoon Bopha hit Compostela town. / Getty Disaster-response agencies reported 13 other typhoon-related deaths elsewhere.
Typhoon Bopha : Philippines Storm Kills 238 - Yahoo!
Most Popular Today's five most popular stories World's oldest person dies at age 116 Snake on a plane forces emergency landing What city has world's best quality of life?
( AP Photo)&lt;/em&gt ; May 1960 A magnitude 9.5 earthquake in southern Chile and ensuing tsunami kill at least 1,716 people.&lt; br&gt; &lt;em&gt ;Caption: A soldier stands guard nearrubble strewn around an electrical shop which was shattered by an earthquake in Concepcion , Chile , on May 24, 1960.

TtR 10.1 s
8.8 s
7.7 s 8.5 s
7.5 s
2.4 s 7.7 s
13.1 s

CTtR 10.1 s
18.9 s
26.7 s 35.2 s
42.7 s
45.1 s 52.8 s
65.9 s

Nuggets n11, n12, n13, n14 n9, n10
n14

Table 2: Example user session: User reads at 225 words per minute, spending 60 seconds reading, starting at 9:55, on Dec 06, 2012. The column heading "Time" indicates the time at which the update was emitted, "Conf." is short for "Confidence", "TtR" means "Time to Read" and "CTtR" means "Cumulative Time to Read".

gain at the end of each search result. If the simulated user does not finish reading a search result during a session, the result is considered unread and no gain is recorded. We consider unjudged updates as non-relevant, and we do not elide them.
We measure the gain of a search result as the number of nuggets of relevant information contained in the result. A nugget can be considered an atomic piece of information. Example nuggets from the TST 2013 qrels are shown in Table 1. Our simulated users consider only novel nuggets to be relevant. Previously seen nuggets provide no gain.
Users interested in evolving events want the update produced by the search engine to be recent as well as relevant. Nuggets delivered late to a user are less likely to be considered relevant by the user. Given that different users will visit the search engine at different times, our notion of nugget timeliness must be relative to the simulated user. To make nugget timeliness relative to the user, we define a nugget as being late if it existed in the aggregate stream at a time equal to or before the start of the previous visit. In other words, if a user reads a result that contains a nugget that the search engine could have delivered during a previous visit, the nugget is late. To measure how late a nugget is, we define a function (n) that returns how many sessions ago the nugget could have been reported (Figure 1f). A nugget that is reported on time has an (n) of zero.
While we do not know how the probability that a user will consider a nugget relevant changes with its lateness, an exponential decay seems reasonable. Therefore we compute the gain for every read nugget n as:

g(n) = 1 × L(n)

(2)

where, L is the decay parameter for late reporting of nuggets and can vary between 0 and 1. L is a pre-determined value representing how much less a user is likely to consider the nugget to be relevant if it is reported late. For our exper-

iments, we vary L from 0 to 1, where 0 indicates that a nugget loses all value if it is reported late, and 1 indicates that the nugget does not lose any value ever regardless of how late the reporting.
The gain from each read nugget is summed to get the cumulative gain for a user over the user trace (Figure 1d). Thus a simulated user's MSU for a search topic is given by:

M SU = g(n)

(3)

n

where n is the set of nuggets read by the user. We compute the MSU for a system by computing for each simulated user their mean MSU over all topics and then averaging all users' mean MSU to produce a system mean MSU. To increase the numerical precision of our system's MSU estimate, we only need to increase the number of simulated users.

3.3 MSU and TST
To demonstrate modeled stream utility (MSU), we use the TST 2013 test collection. The TST is a close but imperfect fit for MSU. The primary issue with using TST to demonstrate MSU is that the runs submitted by the participating groups were guided by the set-based effectiveness measures of TST.
As described in section 2, each participating group submitted runs to the TST that consisted of a set of updates. Each update has both a timestamp and a confidence associated with it. The timestamp is when the system emitted the update and the confidence is a system generated score indicating how relevant the update is.
For MSU, the simulated users check the retrieval system as per their user-trace to read the system's updates. At the start of every session in the user-trace, the updates emitted between the end of the last session and the start of the current session are presented to the user in a reverse chronological order, so that most recent information is read first.

679

(a) Input: A time ordered document stream.
(b) Output: Stream of updates d1..d10 emitted at various times by a system.
(c) Update-trace: Times of first occurrence of nuggets are identified. Updates containing nuggets are noted.
(d) User-trace: Simulated behavior of a user who reads updates from the stream from time to time.
(e) Reading-trace: Determines which updates are available to read for every session. e.g. d3, d2 are available to read at the start of session 2. The user's reading speed V determines which
nuggets are actually read. Reading updates that contain nuggets adds to gain.
(f) (n): Gain is discounted by L(n). (n) is the number of sessions between the first occurrence of nugget n and the
current session within which an update reporting n is read by the user.  is only computed only if the update containing n is
read by the user. Figure 1: Evaluation Model

Updates with the same timestamp are shown in descending order given their confidence. The simulated user starts reading the latest update and then reads the next older update, and so on, until the user runs out of time or encounters an already read update and stops reading further. In case the session ends, the last update that is partially read in the session, is considered as unread. The first user session always begins at the start of the query duration. To determine if a nugget is late, we use the TST assigned Wikipedia times as described in section 2. As with the TST, the gain of a novel, on-time nugget is 1.
3.4 MSU Example for User Gain in a Session
Figure 1 illustrates the MSU model of evaluation. Figures 1a and 1b show the input to and output of a system that generates a stream of updates. The evaluation process begins at Figure 1c, where the nuggets, their time of first occurrence and the updates containing the nuggets are identified. The user-trace in Figure 1d is generated for a modeled user by alternatively sampling the exponential distributions for the session duration and the away time respectively. With the user-trace overlaid over the update-trace we get a readingtrace (Figure 1e). The reading-trace identifies updates available to read at each user session, and by incorporating the reading speed of the user it determines which updates are actually read. Thus the reading-trace determines if any relevant updates were read by the user or not, for every user session.
For a more concrete example, consider a modeled user with A of 1 day, D of 60 seconds and a reading speed V , of 225 words per minutes. Let us also assume that this user considers late information to be half as likely to be relevant, for every session where it is unreported (L = 0.5). Suppose that this user checks for updates at about 10:00 am every day and had previously checked for updates at 10:02 am on Dec 4, at 10:11 am on Dec 5, and at 9:50 am on Dec 6, 2012.
On Dec 7, 2012, the user starts a session at 9:55 am. The user finds the updates listed in Table 2 at the start of the session. The times at which the updates were emitted by the system are listed in the first column. In this case, all updates were emitted on the morning of Dec 7 before the user started the session. The updates are presented to the user in reverse chronological order. In case multiple updates are emitted at the same time, the updates are ordered by their system assigned confidence (as for the 4 updates emitted at 9:52).
The user starts by reading the most recent update (delivered at 9:52 am). The user continues on to read the second update and finds 4 nuggets n11, n12, n13, n14. Table 1 lists the nuggets found in the session. These are nuggets that the user had not seen before and therefore experiences an increase in gain.
However, as per each nugget's Wikipedia time, n11 first occurred at 6:31 p.m. on Dec 4, 2012. It should ideally have been reported to the user at the session starting at 10:11 am on Dec 5. The system further missed reporting the nugget at the session at 9:50 on Dec 6. Therefore (n11) is 2. By similar calculation (n12) = 3, as it should have been reported for the user session at 10:02 on Dec 4. For nuggets n13 and n14, (n13) and (n14) are 1.
As the user reads further down the list, more nuggets (n9, n10) are read and gain increases accordingly. n10 is reported in time at the current session and (n10) = 0. The user gets no gain from reading the update emitted at 9:15 as

680

RunID (GroupID)
cluster5 (PRIS) run2 (ICTNET) run1 (ICTNET) TuneExternal2 (hltcoe) TuneBasePred2 (hltcoe) cluster3 (PRIS) cluster2 (PRIS) uogTrNMTm1MM3 cluster1 (PRIS) cluster4 (hltcoe) BasePred (PRIS) Baseline (hltcoe) uogTrNSQ1 EXTERNAL (hltcoe) uogTrNMTm3FMM4 uogTrNMM uogTrEMMQ2 SUS1 (wim GY 2013) rg4 (UWaterlooMDS) rg3 (UWaterlooMDS) rg2 (UWaterlooMDS) rg1 (UWaterlooMDS) UWMDSqlec4t50 UWMDSqlec2t25 CosineEgrep (UWMDS) NormEgrep (UWMDS)

#Upd. / topic
21.9 93.8 97.8 799.4 2,696.1 42.3 122.1 358.8 164.8 163.0 8,790.7 12,743.0 139.0 22,476.1 168.3 954.7 2,077.8 2,338.7 41,863.3 42,534.1 299,559.6 312,863.3 213,735.7 230,056.0 11.9 151.3

ELG
0.136 0.127 0.125 0.118 0.114 0.103 0.074 0.069 0.067 0.067 0.067 0.063 0.060 0.055 0.049 0.045 0.040 0.036 0.028 0.026 0.022 0.021 0.018 0.017 0.010 0.001

Reason. MSU
4.35 9.45 9.46 5.34 5.49 5.99 9.31 7.28 9.57 9.55 5.84 5.87 6.85 5.60 6.33 7.63 6.88 3.62 1.44 1.45 0.32 0.30 1.02 0.61 0.55 0.73

ELG Rank
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

Reason. MSU Rank
17 4 3 16 15 11 5 7 1 2 13 12 9 14 10 6 8 18 20 19 25 26 21 23 24 22

Best Rank
8 1 1 13 11 5 1 6 1 1 1 1 4 1 5 1 2 17 1 2 18 19 6 14 19 19

Best Ranks achieved by Systems

MSU Parameter values for Best Rank

@Best MA

SA MD

SD L

4.06 1 d 12 h 30 s 30 s 0.9

9.27 1 d 12 h 5 m 2.5 m 0.1

14.65 1 d 12 h 30 m 15 m 0.9

9.77 3 h 1.5 h 30 m 15 m 0.9

9.89 1 h 30 m 15 m 7.5 m 0.9

5.83 1 d 12 h 30 s 15 s 1.0

12.02 3 h 1.5 h 30 s 30 s 1.0

12.79 1 h 30 m 30 s 15 s 1.0

16.45 5 m 10 m 30 s 30 s 1.0

16.15 10 m 20 m 30 s 30 s 1.0

14.17 6 h 6 h 30 m 15 m 0.9

15.02 30 m 15 m 30 m 15 m 0.9

11.35 3 h 3 h 30 s 15 s 1.0

20.38 30 m 1 h 30 m 15 m 1.0

10.52 3 h 1.5 h 30 s 15 s 1.0

19.07 30 m 15 m 15 m 7.5 m 1.0

18.55 30 m 15 m 15 m 15 m 1.0

7.44 1 h 1 h 2 m 60 s 1.0

22.11 5 m 10 m 30 m 15 m 1.0

21.54 5 m 5 m 30 m 15 m 1.0

14.02 5 m 10 m 30 m 15 m 1.0

13.39 5 m 10 m 30 m 15 m 1.0

12.20 5 m 5 m 30 m 15 m 0.9

15.87 5 m 10 m 30 m 15 m 1.0

0.30 1 d 12 h 60 s 30 s 0.3

0.52 3 h 1.5 h 30 s 15 s 0.5

Table 3: Main results. The 26 runs submitted to the TREC 2013 Temporal Summarization Track (TST) are ordered by the TST primary measure, ELG. The column "#Upd./topic" shows the average number of updates produced by the run per topic. The "Reason. MSU" column shows the score for modeled stream utility (MSU) with a reasonable set of parameters as described in section 4.2. On the right side of the table we report an experiment where we selected parameter settings from our parameter sweep that give a run the best rank possible relative to the other runs (section 4.3).

nugget n14 was read earlier. Since the session duration was 60 seconds, the last update was partially read and the user gets no gain on reading it. Thus the total gain (MSU) for this user, from this session on Dec 7, is 2.875.
4. EXPERIMENTS
We explore the parameter space of MSU by applying it to evaluate the runs submitted to the TST 2013. We compare MSU's ranking of the TST runs to the TST's track's primary measure, ELG. The ELG measure is described in section 2.
The 2013 TST evaluated 26 runs from 6 groups over 9 topics. Topics in the track are instances of event types from the set {accident, bombing, earthquake, shooting, storm}. Each topic had a time period (query duration) of 10 days. In other words, a run must summarize 10 days of material from the TREC KBA stream corpus [1] for each topic.
The KBA corpus is essentially a time ordered document stream spanning from October 2011 to January 2013, containing over 1 billion documents crawled from the web. The corpus contains documents with 3 categories, i.e. the documents were crawled from URLs of public newswires (news), blogs and forums (social), and the URLs submitted for shortening at www.bitly.com. Documents are segmented into sentences and some documents are tagged with named entities using NLP tools. There are on average 93,037 documents per hour [5] in the corpus.
4.1 Parameter Sweep
Keeping the user population's reading speed distribution the same across all parameter sets, we sweep the parameter

space by setting MSU's parameters to the following values:
· User population mean session duration, MD = {0.5, 1, 2, 5, 15, 30} minutes.
· User population mean time away, MA = {5, 10, 30 minutes, 1, 3, 6, 24 hours}).
· Lateness decay parameter, L = {0, 0.1, 0.25, 0.5, 0.75, 0.9, 1}.
For the standard deviations of the session duration, SD, and the time away, SA, we multiplied the mean by the values 0.5, 1, 2. For example, a mean session duration of 30 seconds, was associated with standard deviations of 15, 30 and 60 seconds. In total we generated 7 (MA) x 3 (SA) x 6 (MD) x 3 (SD) x 7 (L) = 2646 parameter-tuples (points in the parameter space). For instance, the parameter tuple (MA= 6 hours, SA= 3 hours, MD= 5 minutes,SD= 5 minutes, L = 1), is a point in the parameter space that represents users who spend an average of 5 minutes every 6 hours reading updates, unaffected by late reporting of nuggets.
For each tuple (selected point from the parameter space) in the parameter sweep, we simulated 1000 users. For each simulated user, we generated their user-trace, determined which updates they read, and computed their average MSU across the topics for every run submitted to TST 2013.
4.2 Reasonable Parameters
We believe that a reasonable setting of MSU's parameters is a user population where the users visit on average for 2 minutes every 3 hours and for whom late material quickly

681

8

Modeled Stream Utility (MSU)

6

Modeled Stream Utility (MSU)

MSU vs ELG Kendall's tau = 0.471 , tau_AP = 0.405
cluster4  cluster1 cluster2 

run1  run2

uogTrNMM 

uogTrNMTm1MM3 

uogTrEMMQ2 

 uogTrNSQ1

uogTrNMTm3FMM4 

BaselinBeasePred cluster3 

EXTERNAL 

TuneBasePred2  

TuneExternal2

cluster5 

SUS1 

4

6

8

MSU vs LC Kendall's tau = -0.108 , tau_AP = -0.244
rruucnnl1u2stecrl4usctelur2ster1

uogTrNMM 

uogTrNMTm1MM3 

uogTrNSQ1 

 uogTrEMMQ2

uogTrNMTm3FMM4  cluster3 

TuneBasePred2




TuneExternal2

BasePred   Baseline  EXTERNAL

cluster5 

SUS1 

4

2

2

0

rg3  rg4 ormEgrep  UWMDSqlec4t50
CosineEgreUpWrgM2DSqlec2t25 rg1

0.00

0.02

0.04

0.06

0.08

0.10

0.12

0.14

Expected Latency Gain (ELG)

(a) Reasonable Users, MSU vs ELG Correlation.

0

NormEgrep
 
CosineEgrep

0.0

0.1

rg3  rg4

UWMDSqlec4t50 
UWMDSqlec2t25  rg2  rg1

0.2

0.3

0.4

0.5

Latency Comprehensiveness (LC)

(b) Reasonable Users, MSU vs LC Correlation.

Figure 2: MSU with reasonable parameter settings (MA= 3 hours, SA= 1.5 hours, MD= 2 minutes, SD= 1 minute, L= 0.5), compared to TST 2013 measures, ELG and LC.

becomes less likely to be considered relevant. This corresponds to the parameter tuple (MD = 2 minutes, SD = 1 minute, MA = 3 hours, SA = 1.5 hours, L = 0.5).
Both Figure 2a and Table 3 show the results of MSU vs. the TST measure ELG, given these reasonable parameter settings. For both MSU and ELG, larger values indicate better runs. At these parameter settings, MSU and ELG do not rank systems the same (Kendall's  = 0.471). As Figure 2a shows, the runs in the middle-lower positions of the ELG ranking jump to the top positions. A similar observation can be made in Figure 2b for MSU vs. the LC measure. Here as well, MSU and LC rank systems differently (Kendall's  = -0.108). ELG and LC are themselves negatively correlated (Kendall's  = -0.28). The AP correlation coefficient (AP ) [16, 13], which is more sensitive to changes in higher ranks is also reported in Figure 2.
Across the entire set of swept parameter settings, the maximum Kendall's  correlation was found to be 0.625 for the parameters: MA= 24 hours, SA= 12 hours, MD= 1 minute, SD= 2 minutes, L = 0.1. These results are shown in Figure 3. While not a high correlation, this result shows us that ELG best correlates with a simulated user population where users on average visit once a day for 1 minute. In other words, over the 10 days of the query period, an average user reads about 10 minutes of material. If we were to understand ELG in terms of the users that would prefer its top ranked runs, then ELG is tuned for highly time constrained and selective users with low tolerance for late reporting.
A correlation of 0.625 correlation is not high, and MSU and ELG are behaving quite differently. We believe that there are likely two main reasons for the different rank order of the runs. The first reason is that run performance with MSU is affected by the amount of material that the simulated user reads. If a run does not supply enough ma-

terial for a session visit, then the simulated user will stop reading and also stop accumulating gain.
For example, Table 3 shows that the top ELG run cluster5 averages only 21.9 updates per topic over a 10 day query duration. With the reasonable parameter settings, MSU simulated users spend about 2 minutes every 3 hours, i.e. about 160 minutes reading, on average, over a 10 day query duration. With an average length of each TST update being 63 words, and the average reading speed being 4.3 words per second, it takes on average just 319 seconds to read all the 21.9 updates of cluster5. A simulated user, who is willing to read for 160 minutes in total over 10 days, thus derives very low gain from cluster5 because such a user's visit is cut short from a lack of material to read.
Figure 4 shows the user performance at the point (MA= 5 minutes, SA= 10 minutes, MD= 30 minutes, SD= 15 minutes, L = 1) that has the minimum correlation of MSU with ELG (Kendall's  = -0.04), in our parameter set. This set of users seem inclined to spend almost all their time with the system reading updates taking a 5 minute break every 30 minutes on average, without any dissatisfaction for late reporting of information. Unsurprisingly, these users achieve the highest amount of MSU (22.11) with run rg4. While seemingly an unreasonable parameter setting for MSU, such user behavior could be achieved by a team interested in constant monitoring of a stream of updates.
This result also shows that MSU scores are related to the amount of material read by the simulated users. MSU measures gain as the number of relevant nuggets read in the total content consumed by a user, while discounting for lateness if required. The amount of material read depends on the characteristic behavior of the user (or user population) as well as the number of updates emitted by the system. MSU for a system is simply the average over all simulated users.
A second reason for differences between ELG and MSU is likely that ELG is a set-based measure that measures av-

682

4

MSU vs ELG Kendall's tau = 0.625 , tau_AP = 0.522
cluster2  cluster4  cluster1

uogTrNSQ1 

run1  run2

cluster3 

uogTrNMTm3FMM4



uogTrNMTm1MM3


cluster5 

3

Modeled Stream Utility (MSU)

2

uogTrNMM 

Baseline


BasePred  EXTERNAL  uogTrEMMQ2 

TuneExternal2 TuneBasePred2  

SUS1 

1

0

DorSmCqEolesgcirn2eetp2E5grreUgp3WrgM2rDg4Sqlec4t50 rg1

0.00

0.02

0.04

0.06

0.08

0.10

0.12

0.14

Expected Latency Gain (ELG)

Figure 3: Maximum correlation of MSU with ELG is obtained with parameters (MA= 24 hours, SA= 12 hours, MD= 1 minute, SD= 2 minutes, L= 0.1).

20

15

Modeled Stream Utility (MSU)

MSU vs ELG Kendall's tau = -0.04 , tau_AP = 0.0362

rg4  rg3  EXTERNAL 

Baseline   BasePred

UWMDSqlec4t50




 uogTrNMM

uogTrEMMQ2

cluster4  cluster1

UWMDSqlec2t25



uogTrNMTm1MM3 

 cluster2

run1  run2

uogTrNMTm3FMM4

rg2 



rg1 

uogTrNSQ1 

TuneBasePred2  TuneExternal2 

SUS1 

cluster3 

cluster5 

10

5

 NormEgrep  CosineEgrep

0

0.00

0.02

0.04

0.06

0.08

0.10

0.12

0.14

Expected Latency Gain (ELG)

Figure 4: Minimum correlation of MSU with ELG is obtained with parameters (MA= 5 minutes, SA= 10 minutes, MD= 30 minutes, SD= 15 minutes, L= 1).

erage quality of an update. In contrast, MSU is estimating the total number of nuggets read and considered by the simulated user to be relevant. To see how much the set-based nature of ELG is causing it to be different than MSU, we transformed MSU into a similar measure by taking MSU and dividing it by the time the simulated user spent reading (MSU/second).
In our experiments, MSU/second has the highest correlation (Kendall's  = 0.754) with ELG (Figure 5), with parameters (MA= 24 hours, SA= 12 hours, MD= 1 minute,SD= 1 minutes, L = 0.1). The correlation between MSU/second and ELG is greater than the maximum correlation between MSU and ELG. Of note, the top ranked run for MSU/second is now the same as for ELG: cluster5. The MSU/second top 4 runs in Figure 5 are 4 of the 5 runs with the lowest number of updates submitted (Table 3).
4.3 Everyone's a Winner (Almost)
Noting how the ranking of systems can change greatly based on the wide range of parameter settings in our sweep, we decided to find the instances in our parameter sweep for which a particular system was ranked the highest across all parameter sets. Table 3 shows the results of this analysis on its right side. Some systems achieved their best rank for multiple parameter sets. In such cases, we chose the parameter set for which the system had the highest MSU.
The relationship between time spent and user performance can be seen as we look at the parameter settings from top to bottom of Table 3. Systems that had very few updates submitted, performed well for users who might visit a system about once a day for 30 seconds to 30 minutes on average. The run cluster2 seems to be the best performing system for users who return to the system about every 3 hours for 30 seconds on average. As we go lower in the table, we see that spending more time reading (larger MD) and taking shorter breaks (smaller MA) improves performance of systems that

ranked lower on ELG. These systems are typically those that submitted a large number of updates.
As Table 3 shows, almost all groups have at least one run that is able to achieve a rank 1 performance under some setting of MSU's parameters. If we want to know which system is the best, we must calibrate MSU given actual user behavior. This result may also show that submitted TST systems had very different notions of the appropriate amount of material to make available to users. TST's ELG measure is set-based, but the decision of how large of a set to return to users is left to system designers. In contrast, MSU guides system designers to return a suitable amount of material based on the model of user behavior.
5. OTHER RELATED WORK
Our work builds directly on recent research in modeling user behavior for improved evaluation of information retrieval systems [7]. Of these new effectiveness measures, Clarke and Smucker's [8] Time Well Spent (TWS) measure is the most similar. Like modeled stream utility (MSU), TWS provides a means to evaluate systems that produce a stream of content. Unlike TWS, MSU models multiple visits to the same system, and MSU models variation in each simulated user's behavior. MSU should be amenable to the same statistical analyses that TWS enables.
Other researchers have also evaluated systems by defining a hypothetical user interface and then simulating user behavior with the interface. Of the more recent work that goes beyond a single query and results list is that of Baskaya, Keskustalo, and J¨arvelin [6] who conducted an experiment that considered many different parameter settings for their model and investigated various query reformulation strategies that could be utilized by a user. Yang and Lad [15] also employ a user model based evaluation for information distillation systems and it would be interesting to compare MSU with their method in future work.

683

MSU/second vs ELG Kendall's tau = 0.754 , tau_AP = 0.599

cluster5 

0.08

0.06

Modeled Stream Utility per Second (MSU/second)

cluster3  run1  run2

0.04

0.02

cluster2 

cluster4  cluster1 Baseline

uogTrNSQ1BasePrueodgTrNMTm1MMTu3neExternal2

uogTrNMTm3FMM4 

EXTERNAL 


TuneBasePred2

SUS1   uogTrNMM mDDESCSgqqorleelsecpinc42et5tE20g5rerrpgg12rgr3g4  uogTrEMMQ2

0.00

0.02

0.04

0.06

0.08

0.10

0.12

0.14

Expected Latency Gain (ELG)

0.00

Figure 5: Maximum correlation of MSU/second with ELG is obtained with parameters (MA= 24 hours, SA= 12 hours, MD= 1 minute, SD= 1 minutes, L= 0.1).

6. DISCUSSION
ELG has aspects that try to capture characteristics of good updates, i.e. updates should be on time, not long, and have relevant material. ELG seems to be a measure oriented towards measuring the performance of systems that push highly relevant updates with low frequency to the user. ELG does not provide an easy means to be calibrated to known user behavior.
Our experiments and analysis show that it matters how much material is read. By specifying the amount of material in user terms, we have a way of then calibrating a measure once we know actual user behavior. Observing actual user behavior while evolving events are actually taking place would involve monitoring users' browsing histories. The sudden nature of news events makes a live user study difficult to organize. Search log-analysis may provide some indirect insight into user behavior when such events are running. Our analysis also shows that there may be a case for personalization of stream filtering systems for different user behaviors. As other future work, we hope to extend our understanding of MSU, both in terms of its formal properties and in terms of empirical meta-evaluation criteria, including robustness to noise, discriminativeness between systems, and strictness [3].
7. CONCLUSION
We introduced an effectiveness measure that utilizes a model of user behavior for evaluating systems producing streams of information about evolving events. Our measure is designed to be calibrated based on actual usage data. Our user model simulates a user checking back with the system to read the most recent information from time to time. Users can check back with different frequencies and for different amounts of time depending on various factors. By modeling user behavior, our effectiveness measure produces a score that is easily interpretable as the number of relevant nuggets

of information read by the user. As would be expected, we found that for streams of updates, the gain is sensitive to the amount of time a user spends for reading updates. While temporal summarization systems have traditionally been evaluated in term of their precision and recall, we believe that it is important to consider both the user interface and the user behavior with this interface when evaluating such systems. Given the degree to which the rankings of systems changed as we modified the behavior of the user population, we believe that an effectiveness measure such as ours when calibrated with user data will allow system developers and researchers to build better performing systems tuned to user behavior.
8. ACKNOWLEDGMENTS
This work was made possible by the facilities of SHARCNET (www.sharcnet.ca) and Compute/Calcul Canada, and was supported in part by GRAND NCE, in part by an Amazon AWS in Education Research Grant, in part by NSERC, in part by a Google Founders Grant, and in part by the University of Waterloo.
9. REFERENCES
[1] KBA Stream Corpus 2013. http://trec-kba.org/kba-stream-corpus-2013.shtml.
[2] J. Allan, R. Gupta, and V. Khandelwal. Temporal Summaries of New Topics. In SIGIR, pp. 10­18, 2001.
[3] E. Amigo´, J. Gonzalo, and S. Mizzaro. A Formal Approach to Effectiveness Metrics for Information Access: Retrieval, Filtering, and Clustering. In ECIR, pp. 817­821. 2015.
[4] J. Aslam, F. Diaz, M. Ekstrand-Abueg, V. Pavlu, and T. Sakai. TREC 2013 Temporal Summarization. In TREC, 2013.
[5] G. Baruah, A. Roegiest, and M. D. Smucker. The Effect of Expanding Relevance Judgements with Duplicates. In SIGIR, pp. 1159­1162, 2014.
[6] F. Baskaya, H. Keskustalo, and K. J¨arvelin. Modeling Behavioral Factors in Interactive Information Retrieval. In CIKM, pp. 2297­2302, 2013.
[7] C. L. Clarke, L. Freund, M. D. Smucker, and E. Yilmaz. Report on the SIGIR 2013 workshop on Modeling User Behavior for Information Retrieval Evaluation (MUBE 2013). SIGIR Forum, 47(2):84­95, Jan. 2013.
[8] C. L. A. Clarke and M. D. Smucker. Time Well Spent. In IIiX, pp. 205­214, 2014.
[9] B. P. Flannery, W. H. Press, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C, pp. 214­215, Cambridge University Press, 1988.
[10] Q. Guo, F. Diaz, and E. Yom-Tov. Updating Users About Time Critical Events. In ECIR, pp. 483­494, 2013.
[11] J. Lin and M. Efron. Overview of the TREC-2013 Microblog Track. In TREC, 2013.
[12] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. Overview of the TREC-2011 Microblog Track. In TREC, 2011.
[13] M. D. Smucker, G. Kazai, and M. Lease. Overview of the TREC 2013 Crowdsourcing Track. In TREC, 2013.
[14] I. Soboroff, I. Ounis, J. Lin, and I. Soboroff. Overview of the TREC-2012 Microblog Track. In TREC, 2012.
[15] Y. Yang and A. Lad. Modeling Expected Utility of Multi-Session Information Distillation. In ICTIR, pp. 164­175, 2009.
[16] E. Yilmaz, J. A. Aslam, and S. Robertson. A new Rank Correlation Coefficient for Information Retrieval. In SIGIR, pp. 587­594, 2008.

684

Document Comprehensiveness and User Preferences

in Novelty Search Tasks

Ashraf Bah
University of Delaware Newark, Delaware, USA
ashraf@udel.edu

Praveen Chandar
University of Delaware Newark, Delaware, USA
pcr@del.edu

Ben Carterette
University of Delaware Newark, Delaware, USA
carteret@udel.edu

ABSTRACT
Different users may be attempting to satisfy different information needs while providing the same query to a search engine. Addressing that issue is addressing Novelty and Diversity in information retrieval. Novelty and Diversity search models the retrieval task wherein users are interested in seeing documents that are not only relevant, but also cover more aspects (or subtopics) related to the topic of interest. This is in contrast with the traditional IR task in which topical relevance is the only factor in evaluating search results. In this paper, we conduct a user study where users are asked to give a preference between one of two documents B and C given a query and also given that they have already seen a document A. We then test a total of ten hypotheses pertaining to the relationship between the "comprehensiveness" of documents (i.e. the number of subtopics a document is relevant to) and real users' preference judgments. Our results show that users are inclined to prefer documents with higher comprehensiveness, even when the prior document A already covers more aspects than the two documents being compared, and even when the less preferred document has a higher relevance grade. In fact, users are inclined to prefer documents with higher overall aspect-coverage even in cases where B and C are relevant to the same number of novel subtopics.
Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]
Keywords: diversity; preference judgment; user study
1. INTRODUCTION
In the recent past, more and more researchers in information retrieval (IR) evaluation have directed their attention to evaluation measures that account for both redundancy and ambiguity in search. Novelty aims at dealing with redundancy in search results, while diversity aims at handling ambiguity in queries. Research in that direction has seen such an interest that two different IR evaluation conferences have organized diversity retrieval tasks. The Text Retrieval Conference (TREC) included a diversity retrieval task as part of the Web track between 2009 and 2012 [8]. Unlike its ad-hoc counterpart in the same track, the diversity task judged documents based on subtopics as well as to the topic as a whole. In the same manner, the NTCIR11-IMine task incorporates a subtask focused on diversified document retrieval for which different user intents must be taken into account [11].
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '15, August 09 - 13, 2015, Santiago, Chile Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08...$15.00 DOI: http://dx.doi.org/10.1145/2766462.2767820

Popular evaluation metrics such as precision and recall do not account for intents and subtopics. Both assume binary relevance.
Newer metrics go beyond simple binary relevance, and use graded relevance instead. The shortcoming of binary relevance addressed by graded relevance is that some documents are simply more useful to users than others. Two of the most widely used metrics that make use of graded judgments are nDCG [10] and ERR [7]. What these two metrics do not account for, however, are different user intents and subtopics.
The simplest metric that accounts for different subtopics is subtopic recall [12]. Perhaps the more widely used measures in that category are -nDCG [9] and ERR-IA [6]. These measures produce specific hypotheses about user preferences, and their recapitulative idea is that a user is always interested in seeing more novel subtopics, with decreasing interest in seeing redundant subtopics. Although this idea is sound, we argue that there are other factors that impact users' preferences. Specifically, we adopt an experimental design described by Chandar and Carterette [4] to show that users are interested in seeing more subtopics, but also that users are biased towards more "comprehensive" documents-- that is, they prefer documents that cover the most subtopics regardless of how novel the subtopics are.
2. PREFERENCE JUDGMENTS
Absolute judgments in IR such as Boolean relevance and graded relevance have been used widely in the literature. An alternative is pairwise preference judgments, in which an assessor is presented with two documents and gives a preference to one document over the other. Early work on preference judgments in IR involved inferring preferences from those absolute judgments [2]. However more recently, pairwise (i.e. binary) preference judgments have been adopted as an alternative that may offer advantages in terms of alleviating the burden on assessors by reducing the complexity of the assessment, rendering the assessment task easier than assigning grades and reducing assessor disagreement [3].
The preference judgment scheme we adopt here, however, is a bit different and was first proposed by Chandar et al. [4]. This scheme is based on the so-called triplet framework, wherein given a query and a document A that the assessor is to pretend contains everything they know about the topics, the assessor chooses the next document they would like to see between two documents.
2.1 Data
For our experiments, we use 10 queries from the TREC 2012 Web track dataset [8]. The 10 queries selected are a sample of broad queries (i.e. queries good for intrinsic diversity tasks). Documents were selected for preference assessment from those judged for

735

Figure 1. Screenshot of the HIT Layout

the Web track. All documents are therefore from the ClueWeb09 collection of millions of web pages. We use the publicly available subtopic relevance judgments produced by experienced NIST assessors and based on graded relevance. We use the publicly available subtopic relevance judgments produced by experienced NIST assessors and based on graded relevance. Since documents were not judged for subtopic relevance, we consider the maximum subtopic relevance grade of a document to be its topical relevance. For each one of the 10 queries, we obtained triplets from several users. In the next section, we describe the experimental design that yielded the triplets.
2.2 Experimental Design
The framework we adopt in our study for preference judgment is based on the work of Chandar and Carterette [4]. In the framework, an assessor (i.e. user) is shown three documents, one appearing at the top of the page, one appearing at the bottom left, and the third appearing at the bottom right. We will refer to the top document as DT, and the bottom ones as D1 and D2, in concordance with naming conventions of Chandar and Carterette. Given DT and a query, the assessor is asked to choose between D1 and D2. Essentially the assessor would need to indicate their preference for the second document they would like to see in a ranking by selecting either D1 or D2.
Since we have graded topical and subtopic relevance judgments for the documents, we can use that information to determine what subtopics each document is relevant to, as well as the corresponding relevance grades. A document can thus be represented as the set of subtopics it has been judged relevant to. For instance, Di = {Sj, Sk} means document i is relevant to subtopics j and k.
We used Amazon Mechanical Turk (AMT) [1]; an online labor marketplace to collect user judgments. AMT works as follow: a requestor creates a group of Human Intelligence Task (HITs) with various constraints and workers from the marketplace work to complete the tasks. Workers were instructed to assume that everything they know about the topic is in the top document, and they are now trying to find a document that would be most useful for learning more about the topic. No mentions of subtopics, novelty, or redundancy were given to them except as examples of properties assessors might take into account in their preferences (along with recency, ease of reading, and relevance). Each preference triplet consists of three documents, all of which were relevant to the topic; the documents were picked randomly from the data described in Section 2.1. One document appeared at the top followed by two documents below it. The HITs layout

design, the quality control decisions and the HIT properties were the same as described by Carterette and Chandar [5]. Figure 1 shows an example of a triplet as it appeared in a HIT.
3. HYPOTHESES AND RESULTS
In this section we enumerate some specific hypotheses about relationships between document "comprehensiveness", ad-hoc relevance, and user preferences. Our goal is to test the degree to which comprehensiveness (in the sense of covering more aspects in relevance judgments) is more important to users than relevance; in general, we hypothesize that it is the more important of the two factors in their preferences.
3.1 Comprehensiveness Hypotheses
In the following, D1 > D2 means document D1 is preferred to document D2. R1 > R2 means document D1 was judged by NIST assessors to have a higher ad-hoc relevance grade than document D2. S1 > S2 means document D1 contains more subtopics than document D2. S1new > S2new means document D1 contains more novel subtopics (with respect to the subtopics already seen in DT) than document D2. Regardless of whether a document was placed on the left or right of a triplet, we will refer to the more comprehensive one as D1.
3.1.1 Hypotheses Set 1 (H1 through H3).
This first set of hypotheses posits that a document D1 with higher aspect coverage, in general, tends to be preferred by users ­ regardless of whether D1 covers more novel aspects than the document it is being compared to. The three hypotheses are:
H1: If S1 > S2 and R1 > R2 then D1 > D2. This means users prefer a document with higher aspect coverage and higher ad-hoc relevance grade than a document with lower aspect coverage and lower ad-hoc relevance grade.
H2: If S1 > S2 and R1 < R2 then D1 > D2. This means users prefer a document with higher aspect coverage but lower ad-hoc relevance grade than a document with lower aspect coverage but higher ad-hoc relevance grade.
H3: If S1 > S2 and R1 = R2 then D1 > D2. This means that for documents with equal ad-hoc relevance grade, users prefer a document with higher aspect coverage than a document with lower aspect coverage.
We expected H1 to be largely true, and H2 and H3 to be more mitigated (or possibly inconclusive for H2) due to the fact that relevance grades are an important factor as well. The results in

736

Table 1. Results for all the hypotheses, all results in the last column are significant (++) at p<0.01, except for H5 and H9

Q 152 Q 157 Q 158 Q 167 Q171 Q 173 Q178 Q 184 Q 196 Q199 All Q

H1 true/false 196/ 22 87/ 13 148/19 97/ 28 181/31 155/30 119/51 157/19 108/53 136/22 1384/288 (82.78% true)++

H2 true/false 0/0

0/0

4/1

10/2 5/0

0/0

1/1

4/4

0/0

6/1

30/9 (76.92% true)++

H3 true/false 10/15 0/0

33/13 18/6 12/1

6/11

14/11 21/14 18/15 12/14 144/90 (61.54% true)++

H4 true/false 40/3

20/2 49/14 31/6 42/15 82/21 37/17 56/8

59/32 41/10 457/128 (78.12% true)++

H5 true/false 0/0

0/0

0/0

0/0

0/0

0/0

1/1

2/2

0/0

6/1

9/4 (69.23% true)

H6 true/false 4/1

0/0

21/4

0/0

0/0

2/7

6/1

1/6

5/2

8/6

47/27 (63.51% true)++

H7 true/false 181/21 67/11 97/5

79/24 149/18 73/9

105/50 115/11 49/21 91/11 1006/181 (84.75% true)++

H8 true/false 0/0

0/0

4/2

10/10 5/0

0/0

H9 true/false 10/5

0/0

9/24

18/6 12/1

4/4

1/1

2/2

13/17 16/8

0/0

0/0

13/13 4/8

22/7 (75.86% true)++ 99/86 (53.51% true)

H10 true/false 15/1

20/2 63/18 20/5 32/13 84/28 18/5

49/16 64/34 59/18 424/140 (75.18% true)++

All H's T/F

293/68 194/28 400/100 283/87 306/79 406/110 315/155 423/90 316/170 363/91 3622/960++

Table 1 support our hypotheses that, a document D1 with higher aspect coverage, in general, tend to be preferred by users ­ regardless of whether D1 covers more novel aspects than the document D2 it is being compared to. In fact, even H2 and H3 are true far more often than we expected them to be. According to the results, when D1 covers more aspects than D2 and D1 also has a higher ad-hoc relevance grade than D2, D1 was by far preferred by users. In our experiment this happened 1384 times (82.78%), and failed to happen 288 times (17.22%).
The results also confirm H3 which posits that, for documents with equal ad-hoc relevance grade, users prefer the document with higher aspect coverage. And this happened 144 times (61.54%), and failed to happen 90 times (38.46%). H2 is also true more often than not, i.e. 30 times (76.92%) against 9 times (23.08%). The results, while proving H2 and H3 to be true, also suggest that when the least-comprehensive document has a higher relevance grade than the most-comprehensive document, the bias against the least-comprehensive document is reduced.
3.1.2 Hypotheses Set 2 (H4 through H6).
The second set of hypotheses zooms into special cases where the prior document DT (i.e. document shown at the top) has higher aspect coverage than each of D1 and D2 and posits that, even then, the document with higher aspect coverage is preferred by users. The three hypotheses are:
H4: If S1 > S2 | (ST > S1 and ST > S2) and R1 > R2 then D1 > D2. This means, given the prior document has higher aspect coverage than each of D1 and D2, users still prefer a document with higher aspect coverage and higher ad-hoc relevance grade than a document with lower aspect coverage and lower ad-hoc relevance grade.
H5: If S1 > S2 | (ST > S1 and ST > S2) and R1 < R2 then D1 > D2. This means, given the prior document has higher aspect coverage than each of D1 and D2, users prefer a document with higher aspect coverage but lower ad-hoc relevance grade than a document with lower aspect coverage but higher ad-hoc relevance grade.
H6: If S1 > S2 | (ST > S1 and ST > S2) and R1 = R2 then D1 > D2. This means given the prior document has higher aspect coverage than each of D1 and D2, for documents with equal ad-hoc relevance grade, users prefer a document with higher aspect coverage than a document with lower aspect coverage.

The results in Table 1 support the claims made by Hypotheses H4 through H6. That is, even when the prior document DT has higher aspect coverage than each of D1 and D2, the document D1 with higher aspect coverage is preferred by users. And here again, whether documents D1 and D2 have the same relevance grade or not, it is the one with the highest aspect coverage that gets selected by users as most preferred. Although, as we expected, H5 and H6 are more mitigated than H4. H4 is very often true (in 78.12% of qualifying user preferences): given DT with higher aspect coverage than D1 and D2, the document D1 with higher aspect coverage and higher ad-hoc relevance grade is preferred. Also, H5 is often true (69.23% of qualifying user preferences, but this is not significant): given DT with higher aspect coverage than D1 and D2, the document D1 with higher aspect coverage but lower ad-hoc relevance grade is preferred. And finally, H6 is also often true (63.51% true and 36.49% false): given DT with higher aspect coverage than D1 and D2, the document D1 with higher aspect coverage but same ad-hoc relevance grade as the other document, is preferred. However the proportions in which H5 and H6 are true are not as strong as that of H4, which suggests that the bias against the leastcomprehensive document is reduced in the cases of H5 and H6.
3.1.3 Hypotheses Set 3 (H7 through H9).
This second set of hypotheses focuses on "novelcomprehensiveness". Given a prior document DT, a document D1 is more "novel-comprehensive" than D2 if D1 covers more novel subtopics (with respect to the subtopics already seen in DT). We posit that a document D1 with higher novel aspect coverage, in general, tends to be preferred by users. The three hypotheses are:
H7: If S1new > S2new and R1 > R2 then D1 > D2. This means users prefer a document with higher novel-aspect coverage and higher ad-hoc relevance grade than a document with lower novel-aspect coverage and lower ad-hoc relevance grade.
H8: If S1new > S2new and R1 < R2 then D1 > D2. This means users prefer a document with higher novel-aspect coverage but lower ad-hoc relevance grade than a document with lower novel-aspect coverage but higher ad-hoc relevance grade.
H9: If S1new > S2new and R1 = R2 then D1 > D2. This means that for documents with equal ad-hoc relevance grade, users prefer a document with higher novel-aspect coverage than a document with lower novel-aspect coverage.

737

The results shown in Table 1 support hypotheses H7 through H9. In fact, H7 is true in 1006 cases (84.75%), and fails 181 times (15.25%). This means when D1 covers more novel aspects than D2 and D1 also has a higher ad-hoc relevance grade than D2, D1 was by far preferred by users. H9 is true in 99 cases (53.51%), and fails 86 times (46.49%). This means when D1 covers more novel aspects than D2 but has same ad-hoc relevance grade as D2, D1 was still preferred by users, but not significantly. Also, H8 is true only slightly more often than it is false. It is true 22 times (75.86%), and false 7 times (24.14%). These results
suggest that when the least novel-comprehensive document has
less than or equal ad-hoc relevance grade as the most novel-
comprehensive document, the bias towards the most novel-
comprehensive document is reduced.
3.1.4 Hypothesis H10.
This final hypothesis puts an emphasis on cases where the two documents being compared are equally "novel-comprehensive" ­ i.e. cover the same number of new subtopics ­ and posits that even in that case, users are more likely to prefer the one that covers the most number of subtopics.
H10: If S1 > S2 | (S1new = S2new) then D1 > D2. This means users are more likely to prefer a document that covers the most number of subtopics, even when both documents contain the same number of novel-aspects. In other words, users are biased towards more comprehensive documents, even in cases where both documents have the same number of novel-aspects.
The result for this hypothesis is perhaps the most interesting one. It shows that, even when the two documents being compared are relevant to an equal number of novel aspects, users are more inclined to choose the one with the highest overall subtopic coverage. And this happens in 75.18% of cases.
It should be noted that there are very few cases (17 cases) where the preferred document covers more aspects but fewer novelaspects; and even fewer cases (2 cases) where it contains more novel-aspects but fewer aspects.

100% 80%
60% 40% 20%
0%

H False H true

Figure 2. Comparison of proportions in which hypotheses are true/false for three cases (considering all-pref, left-pref
only and right-pref only).
It is important to note that, in most cases, the document with higher aspect coverage (i.e. more comprehensive) is either more relevant or equally relevant to the other document. There were not many cases where one of the document being compared is more comprehensive but with lower ad-hoc relevance. So those cases are underrepresented, possibly due to the fact that documents with high coverage tend to be very relevant.

But is it the case that users prefer left docs to right docs (or vice versa) even when the preferred document has lower aspect coverage? That is, does the position of the document have an effect on it being preferred by a user? The results in Figure 2 suggest that is not the case. In fact, the proportions in which the hypotheses are true/false in both situations are relatively close. The triplets were indeed placed randomly in either left or right, that is, they are not placed according to any factor.
4. CONCLUSION AND FUTURE WORK
In this paper, we have used the triplet framework to empirically show that users tend to prefer in large proportions documents with high aspect coverage, regardless of the topical relevance grade. We asked users to choose, given a prior document DT, between two documents D1 and D2 the one that is most useful for learning more about the topic. According to the results, users overwhelmingly prefer documents that are relevant to the largest number of aspects (i.e. highest aspect coverage), even when the prior document DT already covers more subtopics than each of D1 and D2. In fact, even in cases where D1 and D2 are relevant to the same number of novel subtopics, the one that is relevant to the largest overall subtopics tends to be preferred.
ACKNOWLEDGMENTS
This material is based upon work supported by the National Science Foundation under Grant No. IIS-1350799. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
REFERENCES
[1] Amazon mechanical turk. http://www.mturk.com.
[2] Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., Hullen-der, G.: Learning to rank using gradient descent. In ICML. (2005)
[3] Carterette, B., Bennett, P. N., Chickering, D. M., & Dumais, S. T.: Here or there. In ECIR. (2008).
[4] Chandar, P. & Carterette, B.: What Qualities Do Users Prefer in Diversity Rankings? In Proc. WSDM Workshop on Diversity in Document Retrieval (2012)
[5] Chandar, P., & Carterette, B.: Using preference judgments for novel document retrieval. In SIGIR. (2012)
[6] Chapelle, O., Ji, S., Liao, C., Velipasaoglu, E., Lai, L., Wu, S. L.: Intent-based diversification of web search results: metrics and algorithms. IR14(6) (2011) 572-592
[7] Chapelle, O., Metlzer, D., Zhang, Y., ...: Expected reciprocal rank for graded relevance. In CIKM. (2009)
[8] Clarke, C. L., Craswell, N., & Soboroff, I.: Overview of the trec 2012 web track. In TREC (2012).
[9] Clarke, C. L., Kolla, M., Cormack, G. V., Vechtomova, O., Ashkan, A., Büttcher, S., ...: Novelty and diversity in information retrieval evaluation. In SIGIR. (2008)
[10] Järvelin, K., & Kekäläinen, J.: Cumulated gain-based evaluation of IR techniques. TOIS 20(4) (2002) 422-446
[11] Liu, Y., Song, R., Zhang, M., Dou, Z., Yamamoto, T., ....: Overview of the ntcir-11 imine task. In NTCIR. (2014).
[12] Zhai, C. X., Cohen, W. W., & Lafferty, J. (2003, July). Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR (2003)

738

The Best Published Result is Random: Sequential Testing and its Effect on Reported Effectiveness

Ben Carterette
Department of Computer and Information Sciences University of Delaware
Newark, DE, USA 19716
carteret@udel.edu

ABSTRACT
Reusable test collections allow researchers to rapidly test different algorithms to find the one that works "best". But because of randomness in the topic sample, or in relevance judgments, or in interactions among system components, extreme results can be seen entirely due to chance, particularly when a collection becomes very popular. We argue that the best known published effectiveness on any given collection could be measured as much as 20% higher than its "true" intrinsic effectiveness, and that there are many other systems with lower measured effectiveness that could have substantially higher intrinsic effectiveness.
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval] Performance Evaluation
General Terms: Experimentation, Measurement
Keywords: information retrieval; test collections; evaluation; statistical analysis
1. INTRODUCTION
Statistical significance testing is an important aspect of experimentation in IR. Without it, differences in effectiveness on the order of 5% would be difficult to interpret: they could represent a "real" improvement in effectiveness, or they could be the product of random noise. Significance testing helps us differentiate between the two [4, 5, 3].
That we use significance testing implies that we accept there is randomness in measuring effectiveness. There is randomness due to a topic sample, due to documents in a collection, due to relevance judgments, and other factors of a test collection. Significance testing asks whether the variance in effectiveness that can be ascribed directly to differences in the systems being tested outweighs those other sources of variance [1].
A full accounting of variance (such as that done for an ANOVA) could compute the total variance due to collection factors, suggesting that there is a "baseline" level of effectiveness for a given collection close to which we should expect
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767812.

Density 0 5 10 15 20 25 30 35

0.24

0.26

0.28

0.30

0.32

0.34

maximum MAP in sample

Figure 1: The distribution of the maximum of 100 samples from a normal distribution with mean 0.20 and standard deviation 0.027.

to find most reasonable IR systems. It is the systems that are well above that baseline that we are most interested in, and in particular, the one with the maximum measured effectiveness is widely considered the best possible system to compare against.
But just as two systems can have different measured effectiveness due to chance, a system can have effectiveness higher than the expected collection baseline due to chance. This randomness in turn means that there must be randomness in our determination of which system has produced the maximum value. It is actually likely that whatever method produces the largest measured effectiveness on a given collection has lower intrinsic effectiveness than reported. This is because of sequential testing: the more tests are done with a given collection, the more likely it becomes that an extreme effectiveness value will be observed.
Rather than take the maximum value at face value, we argue that we should analyze it in the context of other known results and the likelihood that such a value could be produced by a system whose intrinsic effectiveness is much lower simply due to random factors. Of course, the opposite is true as well: any given result could be produced by a system whose intrinsic effectiveness is much higher. We investigate both sides to argue that the intrinsic effectiveness of the best known system could be as much as 20% lower than reported, and systems as much as 20% less effective than the best could have intrinsic effectiveness much higher.
In Section 2 we introduce extreme value theory, and in Sections 3 and 4 we show how we can use it for deeper analysis of IR experiments. We conclude in Section 5 with some discussion about the implications of this work.

747

expected maximum value 0.24 0.25 0.26 0.27 0.28

10

20

50

100

200

number of samples

500

1000

Figure 2: Increase in the expected maximum as the number of samples increases.

2. EXTREME VALUE DISTRIBUTIONS
Suppose we have 100 normal distributions, each of which has an identical population mean µ and population standard deviation . From each of these we sample 50 values and average them so that we have 100 sample means µ1 · · · µ100. We can think of these as 100 measures of mean effectiveness over 50 topics from systems with the same intrinsic effectiveness. Each of them will be "close to" the population mean. But some will be further away than others, and in particular, one of them must be the maximum of the 100. Given that we've sampled 100 values, it is likely that whatever the maximum is, it will be more than two standard deviations above the population mean; that is, it will "look like" a significant improvement over the population mean even though it was sampled from exactly the same distribution.
Figure 1 shows the distribution of maximum MAP when 100 values are sampled from normal distributions with mean 0.20 and standard deviation 0.027. The mean of the distribution of maximums is 0.267, which, if taken at face value, would look like a 34% improvement over the mean of 0.2! Furthermore, 0.267 is outside of the 95% confidence interval around 0.2; in fact only 0.6% of the distribution is greater than 0.267. A full 99% of the distribution of maximums is greater than the upper bound of the 95% confidence interval, meaning it is a near-certainty that out of 100 systems with identical intrinsic effectiveness, at least one will be measured above the 95% confidence interval around the mean.
Figure 2 shows the increase in expected maximum MAP as the number of samples increases. From this we argue that as the number of experiments performed on a particular collection increases, the expected maximum effectiveness reported on that collection will increase logarithmically, even if the "real" effectiveness of the systems being experimented on is not significantly different from the overall mean.
2.1 Extreme value theory
Extreme value theory is the area of statistics devoted to distributions of maximum and minimum sampled values [2]. The Gumbel distribution is an example of an extreme value distribution (EVD) that is useful for normally-distributed random variables. Its cumulative density function is:
P (X  x|, ) = e-e-(x-)/
This could be the distribution of random variable X representing the maximum value of N samples from a normal distribution with mean µ and variance 2. In that case, the

parameter  is equivalent to µ, and the parameter  is an increasing function of both 2 and N . The expected value of the maximum is then given as  + , where  is Euler's constant, which has a value of about 0.5772. Since  = µ,  is constant, and  is an increasing function of N , this means that the expected maximum increases with the number of identically-distributed samples.
2.2 Estimating an extreme value distribution
The exact relationship between , 2, N has no closed form1. It is easy enough to estimate a Gumbel distribution using sampling, however. Given a population mean µ and population standard deviation , we sample n values from a normal distribution with those parameters N times (n represents the number of topics; N the number of systems), average those n values for each of the N samples, and take the maximum average. Over many trials, this produces an approximation of the Gumbel distribution.
We can simplify this further by just taking N samples from a normal distribution with mean µ and standard deviation / n--the latter is known as the standard error or the sampling distribution of the mean. Figure 1 was generated this way, as was Figure 2.
3. ANALYSIS USING EVD
We typically answer statistical questions such as "is one algorithm better than another?" using statistical hypothesis testing. Procedures for hypothesis testing start by forming a reference distribution for the statistic in question (say, difference in mean effectiveness), then checking whether the measured value is in the tail of that distribution. If so, we say the systems are significantly different.
The extreme value question is "is this algorithm's effectiveness better than the maximum expected among N algorithms with the same intrinsic effectiveness on the same collection?" 2 Note that the question includes the number of samples N ; this is a key difference between a one-off test of significance versus a test that accounts for the history of experiments done.
We would answer that question by forming a reference distribution for maximum effectiveness rather than mean effectiveness. That distribution must be based on variation across a sample of systems as well as topics, and must also be based on the number of systems N . It must be specific to a collection and an effectiveness measure, since different collections and measures exhibit different variability --average precision typically has low variance compared to other measures, while P@10 has high variance; more recent collections (which tend to be larger and more heterogeneous) like ClueWeb12 tend to exhibit higher variance than older (smaller and less heterogeneous) collections like WSJ or AP.
To form a reference distribution for a given collection and measure, we will first need to obtain a set of mean effectiveness values. Once obtained, we will assume that all of those values came from the same distribution: a normal distribution centered at their means, with variance equal to the variance of those means divided by the number of topics in
1Closed forms exist only for N  5 [7]. 2We use the phrase "intrinsic effectiveness" as a shorthand for "population effectiveness", which refers to the system's effectiveness measured over the full population of queries. In practice there may not be a finite population of queries that could be measured even in principle.

748

the collection (this is the variance of the sampling distribution). Then the maximum mean effectiveness has a Gumbel distribution, parameterized by  (the mean of the original normal distribution) and  (which is a function of the original variance as well as the number of means in the set); we estimate that distribution as described above.
Though we have described how to estimate a reference distribution that could be used in a significance test, we are not actually going to propose a significance test. Instead, we will use a reference distribution to analyze results reported using different collections.
3.1 TREC-7 run analysis
There were 103 submissions to TREC-7, so we have N = 103 mean average precisions (MAPs). The mean of means, which we will use for µ, is about 0.2; we consider this the "baseline" effectiveness by MAP for the collection. The standard deviation among means, which we will use for , is 0.08. We assume that each mean is drawn from a normal distribution with mean 0.2 and standard deviation 0.08/ 50, which is the standard deviation of the sampling distribution of the mean, also known as the standard error. To generate the reference maximum value distribution (MxVD), we repeatedly sample 103 values from a normal distribution with those parameters and take the maximum of those values.
One possible analysis similar to a significance test is as follows: identify the MAP in the MxVD such that 5% of the distribution is greater than or equal to that value. That represents the minimum MAP a system would need for us to conclude with high confidence that it is not just a random extreme value from a distribution with mean 0.2. In TREC7, that value is about 0.2375. 35 of the 103 submitted runs have a MAP greater than 0.2375; we would say that it is likely 33 (95% of 35, since we expect 5% to be false positives) of those have intrinsic effectiveness above the overall mean. We could do the same for minimum MAP. We find that 5% of the minimum value distribution (MnVD) is less than 0.1625, and 34 TREC-7 submissions have MAPs lower than that. This leaves 34 systems with MAPs within the bounds of what would be expected given that we've sampled 103 total MAPs, the highest and lowest of which are nearly 20% different from the mean.
For the systems outside those bounds, we might also ask what distribution they could have reasonably come from. What is the minimum mean that could generate the maximum observed MAP with high enough probability that we do not consider it significant? Let us take the maximum MAP of any automatic TREC-7 run, since we expect a priori that manual runs will have higher MAPs. That is 0.303 for the ok7ax run [6]. Then the question is how low a population mean could produce an extreme value of 0.303 or higher (with the same variance and N ) with probability 0.2. Applying a linear search, we find that value to be 0.2705, which is 11% lower than 0.303.
We might also ask how low a MAP we could measure when 103 are sampled from a distribution centered at 0.2705 rather than 0.2, or, what is the minimum MAP that we could observe with probability greater than 0.2 sampled from that distribution? It turns out that it could be as low as 0.2378. Therefore any MAP between 0.2378 and 0.303 could have come from a distribution centered at 0.27 if 103 values are sampled, and the probability of observing a MAP between those values is 60%.

20 40 60 80

60% interval containing likely non-extreme values 95% confidence interval

Density

0

0.22

0.24

0.26

0.28

0.30

0.32

MAP

Figure 3: A normal distribution with mean 0.2705 and standard deviation 0.0114 (solid line), along with its maximum value distribution and minimum value distribution for N = 103. Blue lines show the 95% confidence interval of the normal distribution; red lines show the 60% interval in which non-extreme values are likely to fall.

venue SIGIR ECIR CIKM

years 1995­2014 2005­2014 2005­2014

papers 2,413 759 2,620

stats 1,159 short, 1,254 long 346 short, 413 long 1,015 short, 1,605 long;

Table 1: IR research paper corpora.

To summarize, if we sample 103 mean average precisions from a distribution centered at 0.2705, there is a 20% chance the maximum sampled MAP would be greater than 0.303, and a 20% chance the minimum sampled MAP would be less than 0.2378. These bounds are outside of the 95% confidence interval around 0.2705, so would likely be considered statistically significantly different than 0.2705, even if the systems turned out to be equivalent over a much larger sample of topics. Figure 3 illustrates this, comparing the normal distribution and its 95% confidence interval to its extreme value distributions and the corresponding 60% interval in which non-extreme values are likely to fall.
The conclusion of this example is that there is a wide range of possible MAPs that are likely to be observed when sampling 100 from this distribution, significantly wider than is implied from its 95% confidence interval. The fact that the largest of them is 0.303 is random; under slightly different conditions that same system could have produced a MAP closer to 0.2378, and a system with a MAP of 0.24 could have produced a MAP of 0.3. Yet the change from 0.2378 to 0.303 represents a 27% improvement in effectiveness.

4. ANALYSIS OF IR EXPERIMENTS
In this section we analyze the IR literature to find distributions of effectiveness for different standard collections. We have a corpus of IR conference papers from 1995­2014, some statistics of which are shown in Table 1.We searched this corpus for papers using some standard collections: the Wall Street Journal (WSJ) and Associate Press (AP) collections on TREC disks, the GOV2 collection, the WT10g collection, and the TREC Robust 2004 track collection. We transcribed results from these papers, specifically mean effectiveness results. Then for a collection and a measure, we have a set of mean effectiveness values that we can analyze.

749

collection N

µ

 max

Robust '04 55 0.2660 0.0024 0.3591

WSJ

31 0.2577 0.0108 0.4033

AP

31 0.2091 0.0096 0.2982

GOV2

17 0.2523 0.0144 0.3806

WT10g

17 0.1721 0.0059 0.2352

Table 2: Summary statistics on mean average precisions reported in published IR papers for different standard reusable collections.

collection Robust '04 WSJ AP GOV2 WT10g

µ 0.3448 0.3768 0.2747 0.3489 0.2227

±20% c.i. (0.3384, 0.3591) (0.3502, 0.4033) (0.2513, 0.2982) (0.3170, 0.3806) (0.2096, 0.2352)

# above l.b. 3 5
12 6 1

Table 3: Means of distributions that could produce the maximum values in Table 2 along with the 60% confidence interval for non-extreme values. The final column is the number of MAPs greater than the lower bound.

Table 2 shows summary statistics about data sets. Each row gives the number of results we transcribed (N ), the mean of those results (µ), their standard deviation (), and the maximum MAP in our sample. Our N s are fairly low, and necessarily a lower bound on the actual value of N . (In fact N cannot be known, since it includes experiments done but never published.)
Table 3 shows results of the analysis we described in Section 3.1. For each collection we report the mean µ of the normal distribution for which the maximum value reported in Table 2 has 80% cumulative probability in the MxVD--so the upper 20th percentile value in this table is the same as the maximum in Table 2. The lower 20th percentile is the value of MAP at the 20th percentile in the MnVD for the distibution centered at µ . These numbers are essentially a mean and 60% confidence bound (like the one in Figure 3) for a distribution that reasonably could have produced the maximum observed value for each collection.
The lower limit of the confidence interval is the value we are most interested in, as it gives an idea of how low measured effectiveness could be while intrinsic effectiveness is still competitive with the best observed effectiveness. Note that the ranges are wide, with the upper bound up to 20% higher than the lower (for GOV2) and over 15% for three of the collections (WSJ, AP, GOV2). The range is lowest for Robust '04, because that collection has a much larger number of topics (249) and therefore lower standard error.
The last column in Table 3 gives the number of systems in our sample with effectiveness greater than or equal to the minimum value. In all but WT10g there is more than one system that could reasonably be a candidate for "best performing" on the collection. If one performs slightly worse than another, it is most likely due to randomness.

5. CONCLUSION
We have argued that the best known result on any given test collection has a component of randomness due to the number of times the collection has been experimented with-- something that is out of the control of and unknown to most

researchers. Moreover, we have shown that the best known result could come from a system whose intrinsic effectiveness is as much as 20% lower than its observed mean, while a system with much lower observed effectiveness could have intrinsic effectiveness up to 20% greater. This means that there is a wide range of possible overlap in effectiveness, more than what is implied by the standard deviation normally computed for significance testing, due solely to the effect of reusing the collection, and enough that results that are statistically significantly different may actually not be once the extreme value distributions are taken into account.
This means there are extra considerations when reusing test collections and when comparing to best known results. The danger of reusable test collections is that the longer they are used, the more likely it is that an extreme value will be observed by chance alone. This implies that we must mentally adjust reported results downward some, particularly for older or very popular collections, and especially for methods that have not been shown to consistently work across collections. A relatively simple retrieval approach like BM25 that we know to work well in many different settings, is likely to be a better point of comparison than a much more complex model that happens to have the highest effectiveness on a single collection.
It also suggests that it is not always beneficial to rely on reusable test collections to advance the field. Proprietary collections can be beneficial in that they will not be used by as many different researchers, and thus their N may remain relatively small. Strictly non-reusable collections can never have N > 1, and therefore will never have an issue with extreme values being observed due to large N . Therefore it is probably best for the field to publish a portfolio of results across reusable test collections (which will always be good for unit testing, for prototyping, for training, and for failure analysis), proprietary collections (which can include data unavailable outside of the group that owns it, and therefore suggest new avenues of discovery), and non-reusable collections (which should be considered the true "test set").
Acknowledgments This material is based upon work supported by the National Science Foundation under Grant No. IIS-1350799. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
6. REFERENCES
[1] B. Carterette. Multiple testing in statistical analysis of systems-based information retrieval experiments. ACM TOIS, 30(1), 2012.
[2] S. Coles. An Introduction to Statistical Modeling of Extreme Values. Springer, 2001.
[3] M. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of CIKM, pages 623­632, 2007.
[4] J. Tague. The pragmatics of information retrieval evaluation. pages 59­102. Buttersworth, 1981.
[5] C. J. van Rijsbergen. Information Retrieval. Butterworths, London, UK, 1979.
[6] E. M. Voorhees and D. Harman. Overview of the Seventh Text REtrieval Conference (TREC-7). In Proceedings of the Seventh Text REtrieval Conference (TREC-7), pages 1­24, 1999.
[7] E. W. Weisstein. Gumbel distribution. From MathWorld­A Wolfram Web Resource. http://mathworld.wolfram.com/GumbelDistribution.html.

750

Load-sensitive CPU Power Management for Web Search Engines

Matteo Catena1,3, Craig Macdonald2, Nicola Tonellotto3
1 Gran Sasso Science Institute, 67100 L'Aquila, Italy 2 University of Glasgow, Glasgow, G12 8QQ, UK
3 National Research Council of Italy, 56124 Pisa, Italy
{matteo.catena, nicola.tonellotto}@isti.cnr.it, craig.macdonald@glasgow.ac.uk

ABSTRACT
Web search engine companies require power-hungry data centers with thousands of servers to efficiently perform searches on a large scale. This permits the search engines to serve high arrival rates of user queries with low latency, but poses economical and environmental concerns due to the power consumption of the servers. Existing power saving techniques sacrifice the raw performance of a server for reduced power absorption, by scaling the frequency of the server's CPU according to its utilization. For instance, current Linux kernels include frequency governors i.e., mechanisms designed to dynamically throttle the CPU operational frequency. However, such general-domain techniques work at the operating system level and have no knowledge about the querying operations of the server. In this work, we propose to delegate CPU power management to search engine-specific governors. These can leverage knowledge coming from the querying operations, such as the query server utilization and load. By exploiting such additional knowledge, we can appropriately throttle the CPU frequency thereby reducing the query server power consumption. Experiments are conducted upon the TREC ClueWeb09 corpus and the query stream from the MSN 2006 query log. Results show that we can reduce up to 24% a server power consumption, with only limited drawbacks in effectiveness w.r.t. a system running at maximum CPU frequency to promote query processing quality.
Categories and Subject Descriptors: H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval Keywords: Power Consumption, CPU Frequency Scaling,
Search Engines
1. INTRODUCTION
Nowadays search engines are a fundamental part of the Web, due to its enormous size. In fact, most users turn to a search engine to look for the information they need, producing billions of searches every day. And yet, users want to quickly receive answers to their questions, and are not willing
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767809.

to wait long for queries to be served [15]. To achieve low latencies, user queries are processed in a distributed fashion by thousands of query servers, which are organized in clusters and hosted by a data center [1]. This distributed architecture promotes raw performance but also raises environmental and economical issues. In fact, data centers have high electricity consumption due to servers and telecommunications. Moreover, as servers generate heat, data centers need additional energy for thermal cooling.
Research in Information Retrieval on energy-efficiency for search engines has only started comparatively recently. Chowdhury was the first to explicitly write about Green Information Retrieval and to propose a research agenda to reduce power consumption in IR systems [5]. Recently, the research community has proposed some approaches for improving the energy efficiency of search engines data centers. These can involve workload consolidation among multiple query servers [6], caching mechanisms [8, 12, 14] or query routing between different data centers [10, 18]. These works focus on the behavior of either the whole distributed infrastructure or the single data-centre of a Web search engine. Conversely, our work focuses on the energy efficiency optimization of a query server within the data-centre's cluster.
The power consumption of a typical server is dominated by its CPU. This is particularly true at low utilization levels, where CPUs consume a fixed amount of power without performing any demanding computations. For this reason, Hoelzle and Barroso [9] report the data centers' need for energy-proportional computing, i.e., for hardware components with power consumption proportional to utilization.
Dynamic Frequency Scaling (DFS) technologies sacrifices CPU performance for lower power consumptions, by throttling processor's frequency [17]. By doing so, CPUs operating at low frequencies absorb less power but have lower performance than CPUs working at higher frequencies. Operating system (OS) kernels can exploit DFS to achieve energy savings, for instance by throttling the server CPU frequency accordingly to the processor utilization [3]. When the processor is under-utilized, a low CPU frequency is selected. Conversely, a high CPU frequency is picked when the processor is heavily utilized by the system. However, the OS misses domain-specific information on the search engine software and its interactions with the incoming user queries. We advocate that this information can help improving the energy proportionality of a search engine, as identified in [9].
We propose search engine-specific frequency governors that can better adapt to varying query workloads by leveraging domain-specific information. While DFS states are typically

751

managed by the OS, their functionality can be (partially) controlled by application-level code [3]. We build upon this functionality to develop our proposed solution i.e., search engine-specific frequency governors which control the CPU frequency from within the query server. Indeed, knowledge of query server utilization and load facilitate a more refined control of the processor to achieve power savings.
Recently, Lo et al. propose a centralized feedback-based DFS controller for search clusters [11]. Their approach achieves considerable power savings by trading off performance, so that latency constraints are barely met for any workload. However, the authors report several challenges in deploying their centralized solution on large clusters. On the other hand, our approach is decentralized as it works at the single query server level.
The contribution of this paper are as follows: (1) we propose to exploit query servers' knowledge (e.g., utilization, load) to throttle the CPU frequency via search engine-specific frequency governors (2) we experimentally demonstrate that our solutions can achieve significant power savings without markedly damaging the query processing quality w.r.t. standard frequency governors.
2. PROBLEM STATEMENT
We model a query server as a first-come first-served queue, where incoming queries wait to be processed upon arrival to the search engine. As soon as a processing thread is available, it picks the next query from the queue and starts processing it, in disjunctive mode. Queries arrive to the system with an arrival rate  and are processed at a processing rate µ (both expressed in qps, i.e., queries per second). The query arrival rate can vary over time, due to fluctuations in the query load [16]. The query processing rate may change as well, because of the DFS mechanism: for lower CPU frequencies we expect lower power consumption but also lower µ values, as the CPU speed is reduced and query processing takes longer.
Search engines users are impatient [15], so we assume that query servers must process queries within a short time threshold  since their arrival, e.g., 1 second. However, since a query can spend some time waiting in the queue, processing threads may actually have less than  seconds to solve certain queries. Additionally, execution times are variable and some queries may require more time than others to be processed [4]. If our system cannot complete a query processing within the time budget, the retrieval phase is terminated early and results computed so far are returned [10]. This partial processing will likely have a negative impact on the effectiveness of the returned results; however, they can benefit from subsequent effectiveness-improving processing stages, e.g., machine-learned ranking [19]. Conversely, when a query exceeds  seconds waiting in the queue, the server just drops it. In this case, the system returns no results.
The power consumed by a query server (measured in Watts) can be divided into two components: a static part which is continuously consumed to operate the hosting machine, and a dynamic (or operational) part which depends on the CPU usage to perform query processing activities. In this work, we propose to exploit DFS technology to dynamically change the query server CPU frequency to reduce the operational power consumption. Indeed, as shown in Figure 1, the average operational power consumed by the query server is directly correlated to the CPU frequency. Of course, the operational power consumed by the server varies for differ-

Power (Watt)

50

30 qps

27 qps

24 qps

40

21 qps

18 qps

15 qps 30
12 qps

20

9 qps

6 qps
10 3 qps

0 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.1 2.3 2.5 2.7 2.9 3.1 3.3 3.5 CPU Frequency (GHz)
Figure 1: Average operational power (measured in Watts) consumed by a query server at different CPU frequencies and different workloads.

ent incoming workloads, since the machine load varies with the number of queries requiring processing. Clearly, lower CPU frequencies consume less power but also increases the number of unanswered and partially processed queries, as lower frequencies decrease the query processing rate µ.
Hence, our goal is to reduce the power consumed by a query server to process queries, while providing an acceptable query processing quality.

3. PROPOSED SOLUTION
To achieve our goal, we propose to move CPU power management from the OS directly to the query server application code. The Linux OS leverages DFS to reduce power consumption. A CPU exposes a finite set of available operational frequencies to the OS kernel, which exploits software modules called frequency governors [3] to dynamically select the current operational frequency. OS governors vary the processor frequency according to metrics like the CPU utilization, i.e., the fraction of time a processor is busy performing computations. For instance, the os-conservative governor steps up the processor frequency when the CPU utilization is above a tunable threshold  (e.g., 0.8). Conversely, the CPU frequency is scaled down if utilization is below a tunable threshold  (e.g., 0.2). In any case, the OS governors select the CPU frequency using kernel-level information. They do not use application-specific information from the search engine, which we argue can help to improve the system energy efficiency. For this reason, we propose to delegate frequency scaling decisions to application-level modules, by implementing governors inside the search engine.
We develop two search engine-specific frequency governors, based on query server knowledge: se-conservative and se-load. The se-conservative governor is inspired by the os-conservative module, but it exploits the query server utilization instead of the raw CPU utilization. By using the query arrival and processing rates, the query server utilization  is computed as

= 

(1)

k·µ

752

where k is the number of threads processing queries [7]. The idea behind this governor is to maintain an acceptable query server utilization (e.g., 0.7 [7]) so that incoming queries can be easily processed without consuming too much power. Periodically, se-conservative computes the query server utilization and adjusts the CPU frequency. Frequency throttling is performed if  is above (resp. below) the tunable threshold  (resp. ). If an adjustment is required, se-conservative changes the processor frequency to obtain the desired utilization. The governor assumes that it will receive in the immediate future the same number of queries received during the last period. Using Equation (1), it computes the query processing rate necessary to obtain the target utilization. Finally, this governor selects the lowest frequency capable of producing such query processing rate, assuming processing rate directly proportional to CPU speed.
Our second governor, se-load, bases its frequency scaling decisions upon the number of queries N populating the query server, i.e., the queries currently queued or being processed. Given N , we define the query server load as:

N

=

(2)

k

Here, the principle is to reduce the query population in the server as fast as possible, when the query server load is too high (e.g. > 0.7). Periodically, se-load observes the query server load and accordingly adjust the CPU frequency. If is greater than , the processor is set to its maximum frequency. When is below , the CPU frequency is stepped down from its current frequency to the next smaller one.
In the following section, we experiment in order to evaluate (a) how much power can be saved by using the search enginespecific frequency governors and (b) the corresponding impact on query processing quality.

4. EXPERIMENTAL SETUP
Experiments are conducted using the Terrier IR platform [13]. The platform is hosted on a dedicated Ubuntu 14.04 server; Linux kernel version is 3.13.0-45-generic. The machine is equipped with 32GB RAM and an 8-core Intel i7-4770K processor, which exposes 16 operational frequencies ranging from 800 MHz to 3.5 GHz. The ClueWeb09 (Cat. B) document collection is indexed to represent the first tier of a Web search engine. Stopwords are removed and the Porter stemmer is applied to all terms. The index stores document identifiers and term frequencies. The index is compressed with Elias-Fano encoding [20], and is kept in memory, shared among 8 query processing threads.
Queries are taken from the MSN 2006 query log and are submitted in real time to our system, while halving their original interarrival time. Since we use the first day of the dataset, every experiment take 12 hours to run; the average query load in 11.28 qps instead of the original 5.14, with a peak of 44 qps instead of 28.
For each query, we use BM25 to retrieve the top 1000 documents using WAND [2]. Upon arrival, queries are queued and have 1 second to be processed1. When this time expires before processing completion, query processing is early terminated and partial results are returned. If a query spends all its time in the queue, the system drops the query. The query is unanswered and an empty result list is returned.
1Approximatively 7% of the queries take at least one second to be solved.

For each experiment, we measure (a) the % of unanswered queries (%UQ), (b) the mean recall, relatively to an ideal system which has infinite time to process every query (RR) and (c) the power consumed by the query server (P). In particular, power consumption is measured at the server power socket by using an Alciom PowerSpy2 wattmeter. Measurements consider only the dynamic power consumption, i.e., we remove the power consumed by the server when idle (41.8 Watt). Power is measured every 30 milliseconds and the mean value is reported for each experiment.
Our baselines are given by standard Linux frequency governors. In particular, we compare our approach to two baselines: os-performance which processes every query at the maximum CPU frequency; and os-conservative which adjusts the CPU frequency based on the processor utilization.
For os-conservative, frequency throttling decisions are taken every 0.08 milliseconds (the default value). Instead, our governors take decisions at every second, since we observe from the query log that the query arrival rate fluctuates every 1.2 seconds in average.
While os-performance is parameterless, other governors are tested under two different configurations. One has relaxed thresholds ( = 0.8,  = 0.2 ­ as in the os-conservative default setting), so that the query server is likely to maintain a certain CPU frequency for longer periods. The other configuration has tighter thresholds (selected as  = 0.8,  = 0.6), so that the query server will promptly react to changes in utilization or load.
We consider our search engine-specific governors successful if they show reduced power consumption (P) than the baselines, without marked degradation to query processing quality, as measured by relative recall (RR) and % of unanswered queries (%UQ).

5. EXPERIMENTAL ANALYSIS

Governor

  %UQ RR

P

os-performance - - 0.342 0.931 41.765

os-conservative

0.8 0.8

0.2 0.6

0.283 0.295

0.929 0.927

38.569 35.381

se-conservative

0.8 0.8

0.2 0.6

0.352 0.315

0.911 36.016 0.900 31.727

se-load

0.8 0.2 0.312 0.913 35.455 0.8 0.6 0.292 0.912 32.888

Table 1: Percentage of unanswered queries (%UQ), mean relative recall (RR) and mean consumed power (P, in Watt) for different frequency governors under various settings of , .

Experiments results are reported in Table 1. We observe that the os-performance baseline consumes the highest operational power ( 42 Watt) and causes the query server to drop more than 0.3% of the incoming queries. However, under this configuration the system shows the best relative recall (0.931). Relative recall values are statistically significant according to paired t-tests (p < 0.01).
The query server using the os-conservative governor shows reduced power consumption w.r.t. a server equipped with the os-performance governor. Indeed, os-conservative can consume from 8% to 15% less power than a governor which always maintains the CPU at the maximum frequency.

753

Also, os-conservative drops less queries but provides a slightly worse relative recall.
Our first search engine-specific governor, se-conservative, leads to reduced power consumption if compared to osconservative runs. In fact, our governor saves more than 6% in power consumption when relaxed thresholds are set ( = 0.8,  = 0.2); and more than 10% using tight thresholds ( = 0.8,  = 0.6). These power savings come at the price of small degradation in query processing quality: the percentage of unanswered queries increases by 6% while the relative recall decreases by almost 2%, if we compare se-conservative to os-conservative with tight thresholds. Under the relaxed threshold, se-conservative drops 24% more queries than os-conservative, while its relative recall diminishes of 3% in comparison. When compared to osperformance, se-conservative can help saving from 14% to 24% in power consumption. Relative recall decreases by slightly more than 2% when using relaxed thresholds, and by almost 3% with tight ones. The percentage of unanswered queries increases of 3% w.r.t. os-performance when seconservative uses relaxed thresholds. However, dropped queries decrease by 8% when tight thresholds are set.
Our second governor, namely se-load, obtains power savings similar to the se-conservative governor, but with a better query processing quality. Indeed, se-load saves more than 8% in power consumption when compared to osconservative with relaxed thresholds. However, the relative recall detriment is less than 2% and the unanswered queries increment by just 10%. When the governors are configured with tight thresholds, se-load saves 7% in power consumption w.r.t. os-conservative. At the same time, relative recall is damaged for less than 2% and no additional queries are dropped. When compared to os-performance, se-load saves from 15% to 21% in power consumption. Relative recall is damaged by 2% under both threshold configurations. Instead, the percentage of unanswered queries benefits from our governor. Under relaxed thresholds, se-load drops 9% less queries than os-performance and 15% less queries remain unanswered by using tight thresholds.
Overall, experiments confirm that our approach is successful, as the search engine-specific governors show reduced power consumption than the two baselines. In particular, seconservative provides the highest power saving. Relative recall (RR) and percentage of unanswered queries (%UQ) are not markedly damaged, especially when se-load is used.
6. CONCLUSIONS
In this work, we advocate that search engines infrastructures can save power at query server level, by leveraging knowledge on the server querying operations. We develop two search engine-specific frequency governors, se-conservative and se-load, which perform processor frequency throttling according to the query server utilization and load. By extensive experimentation, we evaluate the benefits and drawbacks of our approaches, compared to standard OS-level frequency governors. We find that se-conservative can help saving up to 24% power w.r.t. a system which operates at maximum CPU frequency to promote query processing quality. Indeed, se-conservative damages by just 3% both relative recall and percentage of unanswered queries. When compared to systems that use more energy efficient configurations, we find that our governors can still save at least 7% in power consumption. This gain costs only a limited

detriment in relative recall (less than 2%) when se-load is used. Moreover, such power savings are important at data center-level too. Indeed, reduced CPU frequencies reduces heat output, and thus reduces thermal cooling expenditure. Greater power savings can be achieved by accepting more substantial degradation in query processing quality.
7. REFERENCES
[1] L. A. Barroso, J. Dean, and U. H¨olzle. Web Search for a Planet: The Google Cluster Architecture. IEEE Micro, 23(2):22­28, 2003.
[2] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In CIKM, 2003, pages 426­434.
[3] D. Brodowski. CPU frequency and voltage scaling code in the Linux kernel: Linux CPUFreq. https://www.kernel. org/doc/Documentation/cpu-freq/index.txt, Visited: 2015-02-16.
[4] S. Bu¨ttcher, C. L. A. Clarke and G. V. Cormack. Information retrieval: Implementing and evaluating search engines, 2010, MIT Press.
[5] G. Chowdhury. An agenda for green information retrieval research. Information Processing & Management, 48(6):1067­1077, 2012.
[6] A. Freire, C. Macdonald, N. Tonellotto, I. Ounis, and F. Cacheda. A Self-adapting Latency/Power Tradeoff Model for Replicated Search Engines. In WSDM, 2014, pages 13­22.
[7] M. Harchol-Balter. Performance Modeling and Design of Computer Systems: Queueing Theory in Action. Cambridge University Press, 2013.
[8] N. Hidalgo, E. Rosas, V. Gil-Costa, and M. Marin. Assessing Energy Efficiency in ISP and Web Search Engine Collaboration. In WAINA, 2014, pages 299­304.
[9] U. Hoelzle and L. A. Barroso. The Datacenter As a Computer: An Introduction to the Design of Warehouse-Scale Machines. Morgan and Claypool Publishers, 1st edition, 2009.
[10] E. Kayaaslan, B. B. Cambazoglu, R. Blanco, F. P. Junqueira, and C. Aykanat. Energy-price-driven Query Processing in Multi-center Web Search Engines. In SIGIR, 2011, pages 983­992.
[11] D. Lo, L. Cheng, R. Govindaraju, L. A. Barroso and C. Kozyrakis. Towards energy proportionality for large-scale latency-critical workloads. In ISCA, 2014, pages 301­312.
[12] M. Marin, V. Gil-Costa, and C. Gomez-Pantoja. New Caching Techniques for Web Search Engines. In HPDC, 2010, pages 215­226.
[13] C. Macdonald, R. McCreadie, R. Santos, and I. Ounis. From Puppy to Maturity: Experiences in Developing Terrier. In OSIR Workshop, 2012.
[14] F. B. Sazoglu, B. B. Cambazoglu, R. Ozcan, I. S. Altingovde, and O. Ulusoy. A Financial Cost Metric for Result Caching. In SIGIR, 2013, pages 873­876.
[15] E. Schurman and J. Brutlag. Performance related changes and their user impact. In Proc. Velocity, 2009, page 1.
[16] F. Silvestri. Mining Query Logs: Turning Search Usage Data into Knowledge. Foundations and Trends in IR, Now Publishers Inc., 2010.
[17] D. C. Snowdon, S. Ruocco, and G. Heiser. Power Management and Dynamic Voltage Scaling: Myths and Facts. In PARC Workshop, 2005.
[18] A. Teymorian, O. Frieder, and M. A. Maloof. Rank-energy Selective Query Forwarding for Distributed Search Systems. In CIKM, 2013, pages 389­398.
[19] N. Tonellotto, C. Macdonald, and I. Ounis. Efficient and effective retrieval using selective pruning. In WSDM, 2013, pages 63­72.
[20] S. Vigna. Quasi-succinct indices. In WSDM, 2013, pages 83­92.

754

Retrieval from Noisy E-Discovery Corpus in the Absence of Training Data

Anirban Chakraborty, Kripabandhu Ghosh and Swapan Kumar Parui
Computer Vision and Pattern Recognition Unit Indian Statistical Institute, Kolkata, West Bengal, India
{chakraborty.abhi89, kripa.ghosh, swapan.parui}@gmail.com

ABSTRACT
OCR errors hurt retrieval performance to a great extent. Research has been done on modelling and correction of OCR errors. However, most of the existing systems use language dependent resources or training texts for studying the nature of errors. Not much research has been reported on improving retrieval performance from erroneous text when no training data is available. We propose a novel algorithm for detecting OCR errors and improving retrieval performance on an E-Discovery corpus. Our contribution is two-fold : (1) identifying erroneous variants of query terms for improvement in retrieval performance, and (2) presenting a scope for a possible error-modelling in the erroneous corpus where clean ground truth text is not available for comparison. Our algorithm does not use any training data or any language specific resources like thesaurus. It also does not use any knowledge about the language except that the word delimiter is blank space. The proposed approach obtained statistically significant improvements in recall over state-of-the-art baselines.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Clustering; Query formulation
General Terms
Algorithms, Legal Aspects
Keywords
Noisy Data, Co-occurrence, E-Discovery
1. INTRODUCTION
Erroneous text collections have posed challenges to the researchers. Many such collections have been created and researchers have tried several error modelling and correcting techniques on them. The techniques involve training models on sample pairs of correct and erroneous variants. But such
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767828.

exercise is possible only if the training samples are available. There are several text collections which are created directly by scanning hard copies and then OCRing them. The Illinois Institute of Technology Complex Document Information Processing Test Collection, version 1.0, referred to here as "IIT CDIP" [4] and informally in the TREC community as the "tobacco collection" is one such collection. It was created for the TREC Legal track. The text version was created by Optical Character Recognition (OCR) from the original document images. However, it contains a widespread of OCR errors which made it very difficult for the participants to achieve meaningful improvements on it. The size and the presence of OCR errors discouraged participation in the Legal task to an extent that the organizers decided not to use it further.
The unavailability of training data presents a different problem premise. Parapar et al. [7] combined the results of 5, 4, 3 and 2-grams with the actual terms. The parameters were trained on the TREC Confusion Track collection and tested on the TREC legal IIT CDIP 1.0 dataset. The method failed to produce significant improvement over a weak baseline in MAP. Moreover, they did not consider any recall specific evaluation measure which is vital for a legal IR retrieval. Also, a major drawback with the n-gram based methods is the increase in the size of the inverted index, which poses serious resource crisis and also slows down the retrieval process considerably. Ghosh et al. [2] proposed an algorithm based on word similarity and context information and an improved version was proposed by Chakraborty et al. [1]. We consider the later version as a baseline in our paper. A string matching technique (e.g., edit distance, n-gram overlaps, etc.) alone is not reliable in finding the erroneous variants of an error-free word due to homonymy. For example, word pairs like industrial and industrious, kashmir (place) and kashmira (name), etc. have very high string similarity and yet they are unrelated. Such mistakes are even so likely when we do not have a parallel error-free text collection to match the erroneous variants with the correct ones using the common context. However, context information can be used to get more reliable groups of erroneous variants. Context information can be harnessed effectively by word co-occurrence. We say that two words co-occur if they occur in a window of certain words between each other. Word co-occurrence has been successfully used in identifying better stems ([5], [6]) than methods that use string similarity alone [3].

755

The rest of the paper is organized as follows: we describe the proposed approach in Section 2, provide the results in Section 3, and conclude in Section 4.
2. PROPOSED APPROACH
We first describe two key terms used in our algorithm. Then we describe our algorithm.
2.1 Key Terms
2.1.1 Word Co-occurrence
We say that two words w1 and w2 co-occur if they appear in a window of size s (s > 0) words in the same document d. In our experiment we take the whole document as the window. Word co-occurrence gives a reliable measure of association between words as it reflects the degree of context match between the words. This association measure gets more strength when it is used in conjunction with a string matching measure. For example, two words with high string similarity are likely to be variants of each other if they share the same context as indicated by a high co-occurrence value between them. The word industrious is highly similar to industrial. But, they are not variants of each other. They can be easily segregated by examining their context match as they are unlikely to have a high co-occurrence frequency.
2.1.2 Longest Common Subsequence (LCS) similarity
Given a sequence X = x1, x2,....,xm, another sequence Z = z1, z2,....,zk is a subsequence of X if there exists a strictly increasing sequence i1, i2,....,ik of indices of X such that for all j = 1,2,...,k, we have xij = zj. Now, given two sequences X and Y , we say that Z is a common subsequence of X and Y if Z is a subsequence of both X and Y . A common subsequence of X and Y that has the longest possible length is called a longest common subsequence or LCS of X and Y . For example, let X = A, B, C, B, D, A, B and Y = B, D, C, A, B, A. Then, the sequence B, D, A, B is an LCS of X and Y . Note that LCS of X and Y is not in general unique.
In our problem, we consider sequences of characters or strings. For strings industry and industrial, an LCS is industr. Now, we define a similarity measure as follows :
LCS similarity(w1, w2) = StringLength(LCS(w1,w2))
M aximum(StringLength(w1),StringLength(w2))

So, LCS similarity(industry, industrial)

=

StringLength(LCS(industry,industrial)) M aximum(StringLength(industry),StringLength(industrial))

=

StringLength(industr) M aximum(8,10)

=

7 10

= 0.7

Note that the value of LCS similarity lies in the interval

[0,1].

2.2 The Proposed Algorithm
Our algorithm has two major parts :

1. Grouping, and

2. Binding

2.2.1 Grouping
This part of our algorithm can be divided into the following sub-parts:

· Filtering :

Let L be the lexicon or the set of all unique words

in the documents in C. Let q  Q be a query such

that q = {w1, w2,...,wn}, where wi, i  {1,2,...,n},

is a query word. We construct L: LCS similarity(w, wi) > }.

a In

soettheLrwwi o=rds{,wLwi

contains all the words in L that has LCS similarity of

more than a selection threshold  with the query word

wi. Since,  is a value of LCS similarity,  belongs

to the interval (0, 1).

· Graph formation :
We now define a graph G = (V ,E) where V is the set Lwi of words. For two words w1, w2 in Lwi , let cf (w1, w2) denote the co-occurrence frequency over all the documents. If cf (w1, w2) > 0 then (w1, w2) defines an edge in E and the weight of the edge is defined as cf (w1, w2).

· Trimming : Let the maximum edge weight of graph G be maxew. Then, we eliminate those edges in G whose weight is less than  (  (0, 100)) per cent of maxew. Let this new graph be called Gr. Then Gr = (V , Er) where Er = {e  E : weight of e   % of maxew}. This step is done to eliminate the chance co-occurrences of words which are otherwise unrelated but happen to occur together in the same document by chance. The frequencies of such co-occurrence are very low. An example of such a situation can be - a lady named Kashmira visited the place Kashmir to spend her summer vacation. Note that the words Kashmira and Kashmir are not semantic variants of each other.
· Clustering : We now cluster the vertices of graph Gr based on the edge weights. The clustering algorithm is as follows :
Two vertices v1 and v2 will belong to the same cluster if
­ either v1 is the strongest neighbour of v2
­ or v2 is the strongest neighbour of v1
v1 is the strongest neighbour of v2 if out of all the neighbouring (adjacent) vertices of v2, the edge weight of edge joining v1 and v2 is the maximum. This clustering algorithm was used by Paik et al. [6]. It is more convenient to use this algorithm over popular clustering algorithms like single-linkage, complete-linkage, k means and k nearest neighbour since it is parameterfree.
· Weight assignment : Given a cluster, the weight of each node is the degree of the node (i.e., the number of edges connected to the node). In Figure 1, we see a cluster containing the words Tobacc, obacc and Tobac0. The degrees of these three vertices are 2, 1 and 1 respectively. So, the weights of Tobacc, obacc and Tobac0 are 2, 1 and 1 normalized into 0.5, 0.25 and 0.25 respectively.

756

Figure 1: Weight assignment

2.2.2 Binding
Now, given a query word wi, we need to find the erroneous variants from the OCRed corpus. Let C = {Cl1, Cl2,...,Clk} be the set of all clusters formed from Lwi (lexicon of the erroneous corpus) by the clustering algorithm discussed in the last subsection. So, each cluster Clj is of the form {(wj1 , wtj1 ), (wj2 , wtj2 ),..,(wjm , wtj1 )}, where (wjt , wtjt ) is a word-weight pair in cluster Clj. In the clusters of C, we look for the word that has the maximum LCS similarity with wi. Let wclosest  Lwi be the word such that LCS similarity(wclosest, wi) > LCS similarity(wt, wi), for all wt  Lwi - {wclosest}. Let Clclosest  C be the cluster containing wclosest. Then, we choose all the words in Clclosest as the erroneous variants of wi. If there are more than one such wclosest having maximum similarity with wi, we do not choose any cluster.
A pictorial view of the algorithm is provided in Figure 2. For a Query Word, we get a Subset of Lexicon after filtering out from Lexicon based on an  threshold. Cooccurrence Pairs' Block represents the repository of all the co-occurrence information between all the word pairs in the document collection. For the Subset of Lexicon, the Cooccurrence Pairs' Block is used to read the co-occurrence values for this subset of words and form the Graph. Next, the Graph is trimmed using the  threshold and we get Trimmed Graph. Then, Trimmed Graph is clustered to get Clusters. After weight assignment, we get Clusters (weighted nodes). Now, for the Query Word, we choose the appropriate cluster from Clusters (weighted nodes). We call this process Binding. The chosen cluster (if any cluster is chosen), along with the Query Word, forms the Expanded Query. The Expanded Query is then used for retrieval.
3. RESULTS
We run our experiments on the IIT CDIP 1.0 dataset created for TREC Legal track. The collection statistics are shown in Table 1. We use the 43 topics of TREC Legal Ad Hoc 2007. We use only the FinalQuery field for our experiments. This query was prepared for Boolean retrieval and hence it contains Boolean operators like OR and AND. But, since we use it for ranked retrieval, we ignore the Boolean operators. We also convert the words in wildcard format like fertiliz!, phosphat!, agricultur! etc. to the shortest correct words like fertilize, phosphate, agriculture etc. respectively. The proximity operators like "w/15" are also dropped.
3.1 Performance
We have compared our approach with two baselines:
· The unexpanded query (No Expansion): Here retrieval is done solely on the basis of the original query terms. In other words, no query expansion is done here.

Figure 2: Algorithm : a pictorial view
· Method proposed by Chakraborty et al. [1] (KDIR): Both co-occurrence and string similarity are used here. But the condition on co-occurrence of two words is quite relaxed resulting in more error variants.
Table 2 shows the results of the proposed approach. We see that the proposed approach produces superior performance in comparison to No Expansion and KDIR. The improvements in terms of MAP are only numerical. However, in Recall@100, the numerical improvements are also found to be statistically significant at 95% confidence level (p-value < 0.05) based on Wilcoxon signed-rank test [8]. The values of the parameters  and  are chosen by simple grid search in the intervals (0, 1) and (0, 100) respectively and the best result obtained is reported.
3.2 Error variants
Table 3 shows the error variants of some correct words in the corpus. We see that the proposed method is able to identify genuine error variants. For example, for the correct word legal the variants egal, iegal, lega and legai are identified. Several variants of health are also identified. All these variants are genuine error variants of health. Note here that LCS similarity between health and iiealth is 0.7143 and that between health and wealth (present in the corpus) is 0.8333. However, iiealth is identified as a variant while wealth is not. This is because the first pair has high co-occurrence frequency while the second pair has low co-occurrence frequency. So, both LCS similarity and co-occurrence have a role to play in identifying the error variants of a word. The proposed method is thus useful in the cleaning and errormodelling of the corpus where the clean text samples are not available for comparison and error-modelling.

757

No Expansion KDIR
Proposed

MAP 0.0899 0.0898
0.0947 (+5.34%, +5.45%)

Recall@100 0.1574 0.1658
0.1741nk(+10.6%, +5%)

Table 2: The table shows comparison of the proposed method with No Expansion and KDIR. Bold font shows the best value. Percentage improvements are shown in bracket. The super-scripts show significant improvements. The first letter of the less effective method appears in the super-script.

Term tobacco smoking
mice oncology polonium sewage
legal health securities insurance natural

Error variants lobacco, obacco, tabacco, tobacc, tobacca, tobaceo, tobacoo, tobaeco, tobaoco, tobecco, tohacco smcking, smeking, smnking, smokiag, smokin, smokina, smokinc, smokine, smokinp, smokinq,
smokins, smoktng, smolcing, smuking, snoking micee, miice, milce
0ncology, oncoiogy, oncolog, oncologv, oneology, onoology olonium, poionium, polonlum ewage, sewaqe egal, iegal, lega, legai
iiealth, heaith, hcalth, healt, healthy, bealth, healtb curities, secunties, securitles, seeurities insuranee, lnsurance, nsurance atural, natura, naturai, naturall

Table 3: Error variants : IIT CDIP 1.0

Feature number of documents
number of tokens number of unique terms average document length

Value 6,910,192 6,524,494,574 135,985,661 944.184

Table 1: Collection statistics for IIT CDIP 1.0

4. CONCLUSIONS
In this paper we have proposed a new paradigm which has not been well explored - improving IR performance from erroneous text without the availability of training data or language-specific resources. We have also proposed a novel algorithm to solve the problem on a large and extremely noisy E-Discovery corpus. The results show that we have achieved statistically significant improvements over the baselines in recall. Also, we have shown that the use of context information is extremely beneficial and reliable in agglomerating semantically related erroneous variants. In addition, the automatic identification of error variants lays a sound platform for the modelling of error patterns for the text corpora where the clean text sample is not available for comparison. We have seen that it is feasible to obtain useful error variants of the terms in the noisy corpus. This may be extremely useful in automatic cleaning of the IIT CDIP 1.0 corpus which, in its cleaned version, will be very important to the E-Discovery community.

[2] K. Ghosh and A. Chakraborty. Improving ir performance from ocred text using cooccurrence. FIRE RISOT track 2012 working notes, Dec. 2012.
[3] P. Majumder, M. Mitra, S. K. Parui, G. Kole, P. Mitra, and K. Datta. Yass: Yet another suffix stripper. ACM Trans. on Information Systems, 25(4):18:1­18:20, Oct. 2007.
[4] D. W. Oard, J. R. Baron, B. Hedin, D. D. Lewis, and S. Tomlinson. Evaluation of information retrieval for e-discovery. Artificial Intelligence and Law, 18(4):347­386, 2010.
[5] J. H. Paik, M. Mitra, S. K. Parui, and K. J¨arvelin. Gras: An effective and efficient stemming algorithm for information retrieval. ACM Trans. on Information Systems, 29(4):19:1­19:24, Dec. 2011.
[6] J. H. Paik, D. Pal, and S. K. Parui. A novel corpus-based stemming algorithm using co-occurrence statistics. In ACM SIGIR, SIGIR '11, pages 863­872, 2011.
[7] J. Parapar, A. Freire, and A. Barreiro. Revisiting n-gram based models for retrieval in degraded large collections. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR '09, pages 680­684, Berlin, Heidelberg, 2009. Springer-Verlag.
[8] S. Siegel. Nonparametric statistics for the behavioral sciences. McGraw-Hill series in psychology. McGraw-Hill, 1956.

5. REFERENCES
[1] A. Chakraborty, K. Ghosh, and U. Roy. A word association based approach for improving retrieval performance from noisy ocred text. KDIR '14, pages 450­456, Rome, Italy, Oct. 2014. SCITEPRESS.

758

Multi-Faceted Recall of Continuous Active Learning for Technology-Assisted Review

Gordon V. Cormack
University of Waterloo
gvcormac@uwaterloo.ca


Maura R. Grossman
Wachtell, Lipton, Rosen & Katz
mrgrossman@wlrk.com

ABSTRACT
Continuous active learning achieves high recall for technology-assisted review, not only for an overall information need, but also for various facets of that information need, whether explicit or implicit. Through simulations using Cormack and Grossman's TAR Evaluation Toolkit (SIGIR 2014), we show that continuous active learning, applied to a multi-faceted topic, efficiently achieves high recall for each facet of the topic. Our results assuage the concern that continuous active learning may achieve high overall recall at the expense of excluding identifiable categories of relevant information. Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Search process, relevance feedback. Keywords: Technology-assisted review; TAR; predictive coding; electronic discovery; e-discovery; test collections; relevance feedback; continuous active learning; CAL.
1. INTRODUCTION
The objective of technology-assisted review ("TAR"), first described in the context of electronic discovery ("eDiscovery") in legal matters [6], is to bring to the attention of a document reviewer substantially all relevant documents, and relatively few non-relevant ones, thereby maximizing recall and minimizing reviewer effort. The best reported results for TAR employ continuous active learning ("CAL"), in which a learning method presents the most-likely relevant documents to the reviewer in batches, the reviewer labels each document in each successive batch as relevant or not, and the labels are fed back to the learning method [6]. While CAL has been shown to achieve high recall with less effort than competing methods (including exhaustive manual review [7] and non-interactive supervised learning [6]), it has been suggested that CAL's emphasis on the most-likely relevant documents may bias it to prefer documents like the
The views expressed herein are solely those of the author and should not be attributed to her firm or its clients.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author(s). Copyright is held by the owner/author(s). SIGIR'15, August 09-13, 2015, Santiago, Chile. ACM 978-1-4503-3621-5/15/08. DOI: http://dx.doi.org/10.1145/2766462.2767771 .

ones it finds first, causing it to fail to discover one or more important, but dissimilar, classes of relevant documents [11, 8].
In legal matters, an eDiscovery request typically comprises between several and several dozen requests for production ("RFPs"), each specifying a category of information sought. A review effort that fails to find documents relevant to each of the RFPs (assuming such documents exist) would likely be deemed deficient. In other domains, such as news services, topics are grouped into hierarchies, either explicit or implicit. A news-retrieval effort for "sports" that omits articles about "cricket" or "soccer" would likely be deemed inadequate, even if the vast majority of articles ­ about baseball, football, basketball, and hockey ­ were found. Similarly, a review effort that overlooked relevant short documents, spreadsheets, or presentations would likely also be seen as unsatisfactory.
We define a "facet" to be any identifiable subpopulation of the relevant documents, whether that subpopulation is defined by relevance to a particular RFP or subtopic, by file type, or by any other characteristic. Our objective is to determine whether CAL is able to achieve high recall over all facets, regardless of how they are identified. To this end, we used Cormack and Grossman's TAR Evaluation Toolkit ("Toolkit"),1 grouping together the four RFPs supplied with the Toolkit as one overall topic, and treating each of the four RFPs as facets. We then computed recall as a function of review effort for the overall topic, as well as for each of the facets. We also computed recall separately for facets consisting of short documents, word-processing files, spreadsheets, and presentations.
We repeated our experiments, importing the Reuters RCV1-v2 dataset [10] into an adapted version of the Toolkit, using each of the RCV1-v2 top-level subject categories as an overall information need, and treating each of the 82 bottomlevel subject categories as a facet.
2. TREC LEGAL TRACK EXPERIMENTS
In the Toolkit, we created a new overall topic, "all," by combining the topics supplied with the Toolkit (topics 201, 202, 203, and 207 from the TREC 2009 Legal Track [9]) as follows: As a seed set, we used 1,000 documents selected at random from the "seed query" hits in the Toolkit for the four topics [6, Table 3, p. 155]; as training and gold standards, we used the union of the respective standards supplied for the four topics, with the effect that, for both training and
1http://cormack.uwaterloo.ca/cormack/tar-toolkit/.

763

Overall and Topical Facet Recall vs. Review Effort 1

Overall and File-Property Facet Recall vs. Review Effort 1

0.8

0.8

0.6

0.6

Recall Recall

0.4

0.4

0.2
0 0

Overall

Topic 201

0.2

Topic 202

Topic 203

Topic 207

0

5

10

15

20

25

30

35

0

Review Effort (Thousands of Documents)

Overall Short docs
.doc files .ppt files .xls files

5

10

15

20

25

30

35

Review Effort (Thousands of Documents)

Figure 1: Overall and facet recall as a function of effort for the four TREC 2009 Legal Track topics. The left panel shows recall for an overall review effort, as well as recall with respect to each of four facets represented by the TREC topics. The right panel shows overall recall, as well as recall for facets represented by short documents, word-processing files, presentations, and spreadsheets.

evaluation, a document was considered relevant to the overall topic, "all," if it was relevant to any of the four facets represented by the Toolkit-supplied topics. We ran the "actkeysvm" CAL implementation, without modification, and captured the ordered list of documents presented for review. We computed recall for the "all" topic, and for each facet, at each position in the list.
We further evaluated recall with respect to four other facets: "short," ".doc," ".xls," and ".ppt," representing short documents (< 1K bytes), word-processing files, spreadsheets, and presentations, respectively. Our results are shown in Figure 1. While it is apparent that, at the outset, topics 201 and 203, as well as short documents and presentations, lag behind; at high recall levels, there is little variance among the recall levels of the facets.
3. RCV1-V2 EXPERIMENTS
We next adapted the Toolkit to work with the RCV1-v2 dataset [10]. We used tf-idf Porter-stemmed word features, and SVMlight, following accepted practice [10]. We used as information needs the four top-level categories in the RCV1v2 subject hierarchy, titled "corporate, industrial," "economics," "government and social," and "markets," with the corresponding subject codes, "CCAT," "ECAT," "GCAT," and "MCAT," respectively. As facets, we used the bottomlevel categories in the RCV1-v2 subject hierarchy. We ignored the intermediate levels, as they are simple unions of the bottom-level categories. For seed queries, we used the titles of the top-level categories. We used the RCV1-v2 labels as both the training and gold standards.
Figure 2 shows overall and facet recall, as a function of review effort, for each of the four RCV1-v2 overall information needs. The results mirror those for the TREC 2009 dataset; however, a much higher level of recall is achieved, and convergence occurs as that higher level is approached. It appears that convergence coincides with a decline in marginal precision which, for these experiments, takes place in the neighborhood of 90% recall.

Among the results in Figure 2, one facet ­ "GMIL" ­ is an obvious outlier. Its recall reaches 80% only with double the review effort required for all other facets. In the RCV1v2 collection, only five documents are labeled relevant to GMIL (bottom-level topic title: "Millennium Issues"). We examined these documents and found that four of them contain the phrase "millennium bug"; see, for example, the left panel of Table 1. We then searched the dataset and found 141 documents containing this same phrase, of which four were labeled relevant to GMIL, 48 were labeled relevant to some other facet of GCAT, and 93 were not labeled relevant to any facet. The right panel of Table 1 shows one such document. On reviewing these and other examples, we were unable to glean the criteria used to distinguish relevant from non-relevant documents, and thus attribute this outlier result to apparent mislabeling in the RCV1-v2 dataset.
4. WHEN TO STOP REVIEW
The objective of finding substantially all relevant documents suggests that CAL ­ or any other review effort ­ should continue until high recall has been achieved, and achieving higher recall would require disproportionate effort. Measuring recall is problematic, due to imprecision in the definition and assessment of relevance [3, 12, 8], and the effort, bias, and imprecision associated with sampling [2, 1, 8]. Accordingly, it is difficult to specify an absolute threshold value that constitutes "high recall," or to determine reliably that that such a threshold had been reached. Arguably, 75% to 80% recall would be sufficient for the TREC review detailed in Figure 1, because at that level, the recall for the facets is uniformly high, and a higher level can be achieved only with disproportionate effort. On the other hand, 90% or higher recall would be necessary to establish the adequacy of the RCV1-v2 reviews detailed in Figure 2. Only above 90% overall recall is the recall for facets uniformly high, and 90% recall is achievable with proportionate effort.
We suggest that, as an alternative to a fixed recall target, marginal precision might be a better indication of the completeness of a CAL review. In all of our experiments,

764

Overall and Facet Recall vs. Review Effort 1

Overall and Facet Recall vs. Review Effort 1

0.8

0.8

0.6

0.6

Recall

Recall

0.4

0.4

0.2
0 0
1

CCAT Others

50

100

150

200

250

300

350

400

450

Review Effort (Thousands of Documents)

Overall and Facet Recall vs. Review Effort

0.2
0 0
1

ECAT Others

50

100

150

200

Review Effort (Thousands of Documents)

Overall and Facet Recall vs. Review Effort

0.8

0.8

0.6

0.6

Recall

Recall

0.4

0.4

0.2
0 0

GCAT GMIL Others

50

100

150

200

250

300

350

400

450

Review Effort (Thousands of Documents)

0.2
0 0

MCAT Others

50

100

150

200

250

300

Review Effort (Thousands of Documents)

Figure 2: Overall and facet recall as a function of effort for the RCV1-v2 dataset.

we observed that the precision of each successive batch of 1,000 documents rose rapidly to nearly 100%, was sustained at nearly 100%, and then fell off. Table 2 illustrates that, in these experiments, stopping the review when marginal precision falls below one-tenth of its previously sustained value is a good predictor of high recall for the overall information need, as well as the facets, with proportionate effort.
5. DISCUSSION
A sign test shows our result to be significant (p < 0.03), by virtue of being observed for six of six separate experiments.
CAL is a greedy method that always chooses the mostlikely relevant documents for review. It is to be expected that, at the outset, it chooses documents representing the easiest-to-identify facets, due to their subject matter, file properties, or abundance. As those documents are exhausted, others representing new facets become the mostlikely relevant documents, until no more likely relevant documents remain. Only when the most-likely relevant documents from all facets have been exhausted, does marginal precision drop to a de minimus level.
While our findings suggest that it may be unnecessary, neither our theory of CAL's operation nor our results suggest that it would be harmful to train the learning method using additional seed documents ­ found by ad hoc means

­ to represent important facets that are are known to the reviewer at the outset, or become known during the course of the review process [5].
Our experiments suggest that when a review achieves sustained high precision, and then drops off substantially, one may have confidence that substantially all facets of relevance have been explored. In addition to offering a potentially better prediction of completeness, precision can be readily calculated throughout the review, while recall cannot. Further research is necessary to determine the extent to which marginal precision may afford a reliable quantitative estimate of review completeness, including coverage of different facets of relevance.
While sharing general motivation with efforts to achieve novelty and diversity in ad hoc retrieval [4], CAL seeks to achieve high recall rather than to reduce redundancy, and does so using a depth-first rather than breadth-first approach. We conducted an auxiliary experiment to investigate whether a strategy of using separate reviews for each facet would improve on the combined review strategy reported here. We found that the overall effort required to achieve 75% recall for every facet was higher for the separate review strategy. It remains to be seen whether other diversity-focused methods might improve on the purely depth-first results presented here.

765

Labeled "Relevant" in RCV1-v2 Okura up on millennium bug software demand. TOKYO 1997-08-15 Shares of Okura & Co Ltd surged on Friday afternoon due to the expectation that its software business would benefit from the so-called millennium bug problem. The stock was the top percentage gainer on the Tokyo Stock Exchange's first section in the afternoon session. Okura's shares were up 55 yen at 455 yen as of 0435 GMT. (c) Reuters Limited 1997-06-16

Labeled "Not relevant" in RCV1-v2 Complete Business Solutions gets contract. FARMINGTON HILLS, Mich. 1997-06-16 Complete Business Solutions Inc said early Monday that South Carolina Electric & Gas Co has awarded it a contract to manage and implement its Year 2000 code conversion project and deal with issues related to the "millennium bug". The project is expected to require changes to over three million lines of code at the utility and to be in the multi-million dollar range, Complete Business said. The company received the contract as part of a bidding process that included six other vendors. SCE&G is the principal subsidiary of SCANA Corp. ((­ New York Newsdesk 212-859-1610)) (c) Reuters Limited 1997

Table 1: Conflicting relevance labels for category GMIL (bottom-level topic title: "Millennium Issues"). Of 141 documents in the RCV1-v2 collection containing the phrase "millennium bug," four were labeled "relevant," while 137 were labeled "not relevant."

Review Effort (K-docs) Overall Precision Overall Recall Overall F1
Lowest Facet Recall

TREC 34
0.587 0.818 0.684 0.766

CCAT 436 0.856 0.979 0.913 0.924

ECAT 166 0.664 0.919 0.771 0.890

GCAT 281 0.848 0.996 0.916 0.200

GCAT (excl. GMIL) 281 0.848 0.996 0.916 0.863

MCAT 237 0.840 0.972 0.901 0.958

Table 2: Review effort and effectiveness when marginal precision (measured on the last batch of 1,000 documents) falls below 10%. Effort is measured in terms of thousands of documents reviewed; effectiveness is measured in terms of overall precision, recall, and F1, as well as the lowest recall obtained for any facet.

6. CONCLUSION
For all experiments, our results are the same: CAL achieves high overall recall, while at the same time achieving high recall for the various facets of relevance, whether topics or file properties. While early recall is achieved for some facets at the expense of others, by the time high overall recall is achieved ­ as evidenced by a substantial drop in overall marginal precision ­ all facets (except for a single outlier case that we attribute to mislabeling) also exhibit high recall. Our findings provide reassurance that CAL can achieve high recall without excluding identifiable categories of relevant information.
7. REFERENCES
[1] M. Bagdouri, D. D. Lewis, and D. W. Oard. Sequential testing in classifier evaluation yields biased estimates of effectiveness. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 933­936, 2013.
[2] M. Bagdouri, W. Webber, D. D. Lewis, and D. W. Oard. Towards minimizing the annotation cost of certified text classification. In Proceedings of the 22nd ACM International Conference Information and Knowledge Management, pages 989­998, 2013.
[3] D. C. Blair. STAIRS redux: Thoughts on the STAIRS evaluation, ten years after. Journal of the American Society for Information Science, 47(1):4­22, Jan. 1996.
[4] C. L. A. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkann, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research

and Development in Information Retrieval, pages 659­666, 2008.
[5] G. Cormack and M. Mojdeh. Machine learning for information retrieval: TREC 2009 Web, Relevance Feedback and Legal Tracks. In Eighteenth Text REtrieval Conference, 2009.
[6] G. V. Cormack and M. R. Grossman. Evaluation of machine-learning protocols for technology-assisted review in electronic discovery. In Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 153­162, 2014.
[7] M. R. Grossman and G. V. Cormack. Technologyassisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Richmond Journal of Law and Technology, 17(3):1­48, 2011.
[8] M. R. Grossman and G. V. Cormack. Comments on "The implications of rule 26(g) on the use of technology-assisted review". Federal Courts Law Review, 7:285­313, 2014.
[9] B. Hedin, S. Tomlinson, J. R. Baron, and D. W. Oard. Overview of the TREC 2009 Legal Track. In The Eighteenth Text REtrieval Conference, 2009.
[10] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361­397, 2004.
[11] K. Schieneman and T. Gricks. The implications of Rule 26(g) on the use of technology-assisted review. Federal Courts Law Review, 7(1):239­274, 2013.
[12] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5):697­716, 2000.

766

Time Pressure and System Delays in Information Search

Anita Crescenzi
School of Information and Library Science, University of
North Carolina Chapel Hill, NC, USA
amcc@email.unc.edu

Diane Kelly
School of Information and Library Science, University of
North Carolina Chapel Hill, NC, USA
dianek@email.unc.edu

Leif Azzopardi
School of Computing Science, University of Glasgow
Glasgow, United Kingdom
leif@dcs.gla.ac.uk

ABSTRACT
We report preliminary results of the impact of time pressure and system delays on search behavior from a laboratory study with forty-three participants. To induce time pressure, we randomly assigned half of our study participants to a treatment condition where they were only allowed five minutes to search for each of four ad-hoc search topics. The other half of the participants were given no task time limits. For half of participants' search tasks (n=2), five second delays were introduced after queries were submitted and SERP results were clicked. Results showed that participants in the time pressure condition queried at a significantly higher rate, viewed significantly fewer documents per query, had significantly shallower hover and view depths, and spent significantly less time examining documents and SERPs. We found few significant differences in search behavior for system delay or interaction effects between time pressure and system delay. These initial results show time pressure has a significant impact on search behavior and suggest the design of search interfaces and features that support people who are searching under time pressure.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Search Process
Keywords
Search Behavior; Time Pressure; System Delays
1. INTRODUCTION
Recently, increased attention has been paid to how system delays impact search behavior and the user experience in the context of interactive information retrieval (IIR) [1, 8, 15, 16]. Users of search systems with slower query response times have been found to have lower perceptions of system usability and helpfulness [1]. Studies examining system delays have found users are tolerant of search delays up
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
SIGIR'15, August 09 - 13, 2015, Santiago, Chile Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00 DOI: http://dx.doi.org/10.1145/2766462.2767817.

to a "tipping point" beyond which behaviors and perceptions change [15]. User sensitivity to delay varies across individuals, the average speed of the system used [1], and how the information is presented [15]. It has also been found that searchers who experience both query and document response delays view fewer documents, but mark a higher percentage as relevant, although the delay had no impact on the number of queries issued, the number of documents marked relevant, the depth of results inspected or search accuracy [8].
Although there has been a resurgence of interest in the impact of search system delay on user experience, a long line of research precedes this interest. In 1968, Miller [9] described system response times exceeding two seconds as changing the character of "man-computer conversational transactions" and provided a taxonomy of acceptable response times depending on the type and complexity of system interactions and the extent to which the response delay would disrupt the user's task. Since this early work, researchers have found that perceived wait time and the impact of delays can be reduced by informing users of the system response status [11], incrementally loading pages [2], or displaying filler text or images [6] while the system is processing its response.
While system delays have been investigated in IIR, one variable that has not been investigated much is time pressure. Research on time pressured decision-making has found that people adapt under time pressure: they may spend less time in each decision stage, spend less time processing individual pieces of information, accelerate the pace of the task or information processing, or even satisfice in their decisions [12]. Time has been shown to be an important contextual factor in search [13]; however, the impact of time pressure on search behaviors and perceptions is less well understood. Recent studies have have shown when searchers have less time to complete tasks, they have lower perceptions of their search performance [7] and task completion [17], less satisfaction with results [7, 17] and less knowledge gained [7]. Searchers also report lower pre-task search confidence [7] and more negative affect [7] and stress [17]. Time pressure has also been found to impact search satisfaction even when the same amount of time was given to complete tasks [4]. Finally, Mishra et al. [10] found dissatisfaction among people who searched for time-critical information.
In this paper, we report preliminary results of a study investigating the impact of time pressure, system delays and interaction effects on search behaviors and user experience. We present the results of our analyses of interaction measures as well as time pressure and delay manipulation checks.

767

2. METHOD
We conducted a laboratory experiment where we manipulated task time limit and system response time. To manipulate task time limit, half of the participants were given an unexpected task time limit just before they started the experimental search tasks. This time limit was presented via pop-up message after they completed the practice task and informed them they had only 5 minutes to complete each experimental search task (Figure 1). These instructions were intended to simulate a situation in which an urgent notification changes one's expectations about the time available to complete a task. The 5 minute time limit represented approximately half of the mean task completion times from a prior study [5] where different participants used the same system to complete three of the same tasks without time limits. The system tracked the time, and after participants had been searching for 5 minutes, popped-up another message indicating time was up.
Figure 1: Task time limit instructions
Participants who were not given a task time limit were not provided with any instructions regarding the amount of time to spend on each task, although all participants were aware that the experimental session would last 1hr and 15min. If these participants were still searching after 1hr and 5min (i.e., with approximately 10 minutes remaining in the experimental session), we stopped them so they could complete the exit questionnaire and interview. Based on our previous experiences, we did not expect we would need to do this; however, this occurred in seven cases. To minimize time pressure caused by researcher presence [14], the researcher was stationed in an adjacent room while participants completed the study.
System response time was a within-subjects variable with two levels: delay and no delay. Participants completed two tasks in which we introduced 5 second delays after they submitted queries and clicked on SERP results and two tasks in which the system was not delayed. We selected a 5 second delay based on prior research which indicated that a delay of 4 seconds changed behaviors but not attitudes and is usually considered acceptable to users (delays of 7-10 seconds have been found to be unacceptable by users [15]). We presented delays in 2 orders: half of the participants experienced delays during their first and third tasks while the other half experienced delays during their second and fourth tasks.
The basic procedure was as follows. Participants were first shown the experiment instructions on screen including the simulated work task scenario and then completed a practice task to become familiar with the system; we did not impose a time limit or system delay during the practice task. Once participants completed the practice task, those with a task time limit were shown the instructions in Figure 1 before being presented with their first search task. For each search task, participants were shown the topic, completed

a pre-search questionnaire, conducted their search and then completed a post-search questionnaire. After conducting all four searches, participants completed an exit questionnaire. Following this, the researcher returned to the room for a final interview and debriefing.
2.1 Scenario and Tasks
Following Borlund's [3] guidelines, search tasks were situated in the context of a project in which a research team needs to compare news coverage about four topics during the (consecutive) U.S. presidential terms of Clinton, Bush and Obama. Participants were told their job was to examine news during Clinton's second term, 1996-2000, which corresponded to the date range of the test collection [18]. We wanted the scenario to provide participants with a rationale for searching articles from this time period and to make this characteristic of the collection salient to help ground participants' relevance judgments. The scenario indicated that participants should find 8-12 articles per topic.
We selected topics from the collection with contemporary relevance where prior studies have reported that participants found the tasks at least moderately interesting and difficult [5]. We aimed to minimize potential variability in interest and difficulty in order to examine the impact of our independent variables. Four topics were used: wildlife extinction (347), journalist risks (354), piracy (367) and population growth (435). Participants were presented with the topic descriptions before each search.
To minimize order effects, we created a set of 24 topic rotations. We assigned the topic rotations to time limit and delay order combinations such that each topic rotation was used twice: once in each time limit condition and once in each delay order. We randomly assigned participants to time limit, delay order and topic rotation.
2.2 Search Questionnaires
Our search questionnaires measured several constructs including system usability, task difficulty, metacognitive actions and satisfaction. Several items functioned as manipulation checks to see if participants felt time pressure and noticed system delays (see Table 1). Participants responded to these items with a 7-point scale (1 = strongly disagree, 7 = strongly agree). In this paper, we only present items related to the manipulation checks and will analyze the additional measures in future work.
2.3 Search System
All participants used the same search system which resembled a standard search engine. The Whoosh IR Toolkit was used as the core of the retrieval system, with BM25 as the retrieval algorithm, using standard parameters, but with an implicit ANDing of query terms to restrict the retrieval set. Only a portion of the collection was indexed to ensure that the natural response time of the system was quick and constant. This was to control the effects of both the natural and experimental delays as much as possible. The documents included in the index consisted of all the TREC pooled documents for the given topics, along with the top 100 documents retrieved from the full index given a set of user generated queries from a past study (approximately 100-200 queries per topic). In total, over 200,000 documents were indexed. In all conditions, the search system displayed a spinning wheel when it was busy.

768

2.4 Participants
Forty-five people from a major research university participated including undergraduate students (n=13), graduate students (n=8) and staff (n=22) representing many different majors and occupations. Participants ranged in age from 18-59 years (M=32.1, SD=12.8) and 31 were female. Participants were compensated $20 USD.
3. RESULTS
We analyzed 163 tasks completed by 43 participants using a mixed ANOVA. We excluded data from 2 participants with no task time limit who spent most of the time completing a single search task, 7 tasks where participants with no task time limit were stopped in their third task or final task, and 2 tasks where participants accidentally ended the tasks by pressing the wrong button.
3.1 Time and Delay
We conducted a series of manipulation checks to make sure our experimental manipulations were successful. We found those with the task time limit reported feeling significantly more time pressure, a greater need to work fast and more rushed (Table 1). We found significant differences in perceived quickness of SERP and document display between the delay conditions. We found no interaction effects, but note a significant effect of delay on perceived time pressure.
We examined task completion times and found significant effects of task time limit and system delay on the total task time as shown in Table 1. The introduction of time limits and system delays complicates comparison of interaction measures as the total delay experienced by any one person is a function of the number of queries issued and documents viewed by this person. For this reason, we also examined adjusted task times where we subtracted the cumulative delay experienced by each participant for each task from the total task time; we found significant main effects of time limit and no main effect of delay. Interaction effects were significant (F(2,118)=7.03, p<.01).
To check the delay manipulation, we examined query and document processing times (in seconds). As expected, we found no significant differences in organic system response times and significant differences in the mean total processing time (organic + delay) according to delay condition. It took longer for the system to return requests for tasks with delay condition (query: M=5.92, SD=0.96; document: M=5.20, SD=1.16) than for tasks without delay (query: M=0.94, SD=0.84; document: M=0.29, SD=0.34). We found no main effect for time limit or interaction effects.
3.2 Search Behaviors
Table 1 shows the means and standard deviations of participants' search behaviors by time limit and response time condition. With one exception, there were no significant interaction effects, so we only report main effects. We normalized time-based measures to facilitate comparison. The adjusted rate-based measures reflect the number of queries, document views and documents marked relevant per minute using adjusted times; that is, total time minus time spent in delay as in Taylor, Dennis & Cummings [15]. While participants who experienced a task time limit had higher rates of querying, document viewing and marking documents as relevant, only the difference for query rate was significant. There were no significant differences according to whether or

not participants experienced delays; in fact, these measures were very similar in these two conditions.
When we examine actions per query, we see those who experienced the task time limit examined search results more shallowly. They viewed fewer SERPs and documents and also did not navigate as deeply in the search results list. All of these differences were statistically significant. Participants with a task time limit also spent significantly less time viewing documents and SERPs. Although the trend was for participants to view fewer SERPs and documents per query when they experienced delays and not go as deeply in the search results list, these differences were not significant. Participants spent significantly more time viewing documents for delayed tasks, but they spent roughly equal amounts of time viewing SERPs for their delayed and non-delayed tasks.
With respect to performance, there were no differences in participants' save rates (% viewed marked relevant) according to time limit or response time conditions. Although we have not examined the content of the documents participants saved, we examined whether participants met the scenario goal of finding 8-12 relevant articles. This goal was achieved more often for tasks where no task time limit was present (96% vs. 84% of tasks) and for tasks where no delays were present (92% vs. 86% of tasks). Only 76% of tasks with both a delay and a task time limit had at least 8 pages marked relevant compared to 100% of tasks with no task time limit and delay.
4. DISCUSSION AND CONCLUSIONS
We investigated the impact of time pressure and system delays on search behaviors. Results showed participants who were given task time limits queried at a significantly higher rate, viewed significantly fewer documents per query, had significantly shallower hover and view depths, and spent significantly less time SERPs. They also spent significantly less time examining documents, and presumably, making judgments about which documents to save.
We did not find significant differences in search behavior with respect to system delays beyond the time spent completing the task (total and adjusted) and the time spent per document. While the delay was noticed by most participants, it did not significantly impact their search behaviors. These findings are consistent with other studies [1, 8]. Of course, it is likely that larger delay times would produce different results: Taylor et al. [15] found a tipping point between 7 and 11 seconds for behavioral impacts but some changes to satisfaction were observed at lower thresholds. Whether the delay is consistent or intermittent also seems to play a role [1]. Further analysis of the questionnaires and debriefings will likely provide additional insight about how time pressure and system delays impacted the user experience.
Interestingly, we found considerable variability in the task completion times for those without task time limits and two participants, whose data were excluded, used nearly the entire experimental session to search for a single task. For those tasks included in analyses, task completion times ranged from 1.76min to 36.27min. For 10 tasks, the total task completion time exceeded 15min. These variations surprised us because they differ considerably from what we have observed in previous laboratory studies, where the general challenge is often to create tasks that will require sustained interaction.

769

Manipulation checks felt time pressure
needed to work fast felt hurried or rushed displayed search results quickly displayed articles quickly Interaction measures
total task time (m) adjusted task time (m)
adjusted query rate adjusted view rate adjusted mark relevant rate SERPs viewed per q docs viewed per q hover depth per q
view depth per q total time per doc (s) total SERP time per q (s)
save rate

Time Limit (main effects)

Session

Task

F

3.43 (1.62) 3.56 (1.56) 3.03 (1.49) 4.31 (1.67) 4.39 (1.54)

5.25 (1.62) 5.62 (1.50) 4.91 (1.75) 4.65 (1.78) 4.68 (1.92)

15.27*** 27.30*** 17.93*** 0.99 0.63

9.04 (6.12) 8.25 (5.63) 0.61 (0.44) 2.28 (0.93) 1.64 (0.98) 2.25 (1.67) 6.09 (5.75) 17.52 (16.70) 14.88 (15.95)
14.05 (6.50) 61.53 (60.35)
0.70 (0.18)

4.60 (0.98) 4.08 (1.02) 1.04 (0.61) 2.63 (1.24) 2.05 (1.24) 1.64 (0.79) 3.90 (3.47) 11.34 (8.78) 8.89 (7.98)
8.82 (5.46) 34.86 (23.44)
0.73 (0.21)

21.82**** 21.29**** 16.72*** 2.55 3.22 5.36* 4.81* 5.23* 6.51*
17.26*** 13.18*** 0.64

Response Time (main effects)

No Delay

Delay

F

4.29 (1.90) 4.70 (1.82) 4.03 (1.96) 4.96 (1.58) 5.00 (1.68)

4.61 (1.80) 4.71 (1.86) 4.11 (1.82) 4.01 (1.76) 4.09 (1.74)

4.12* 0.00 0.18 19.73*** 17.72***

6.13 (4.00) 6.13 (4.00) 0.85 (0.61) 2.52 (1.18) 1.85 (1.19) 1.98 (1.54) 5.08 (5.04) 14.77 (15.46) 11.91 (14.46)
10.22 (5.54) 46.73 (52.80)
0.69 (0.20)

7.01 (5.27) 5.70 (4.67) 0.85 (0.54) 2.42 (1.07) 1.89 (1.12) 1.83 (0.96) 4.65 (4.41) 13.34 (10.43) 11.15 (10.11)
12.07 (7.23) 46.55 (36.98)
0.74 (0.20)

7.61** 0.03 0.03 1.60 0.00 0.90 0.71 0.77 0.20
10.63*** 0.00 2.30

Table 1: Manipulation check and interaction signals. Means (sd) and F-test results by time constraint (df= 1, 41) and response time (df=1,118), *p<0.05; **p<0.01; ***p<0.001; ****p<0.0001

While preliminary, our initial results open a new line of inquiry into how time pressure impacts search behaviors and how search tools might be designed to support people who are searching under time pressure. For example, interface features that make it easier for people to query, assess relevance and monitor progress might be especially beneficial. Displaying fewer results per SERP might also help keep people who are searching under time pressure stay focused.
5. REFERENCES
[1] I. Arapakis, X. Bai, and B. B. Cambazoglu. Impact of response latency on user behavior in web search. In Proc. of the 37th ACM SIGIR conference, pages 103­112, 2014.
[2] N. Bhatti, A. Bouch, and A. Kuchinsky. Integrating user-perceived quality into Web server design. Computer Networks, 33(1-6):1­16, 2000.
[3] P. Borlund. The IIR evaluation model: A framework for evaluation of interactive information retrieval systems. Information Research, 8(3):1­34, 2003.
[4] A. Crescenzi, R. Capra, and J. Arguello. Time pressure, user satisfaction and task difficulty. Proc. of the American Society for Info. Sci. and Tech., 50(1):1­4, 2013.
[5] A. Edwards, D. Kelly, and L. Azzopardi. The impact of query interface design on stress, workload and performance. In Proc. of the 37th ECIR Conference, pages 691­702, Vienna, Austria, 2015.
[6] Y. Lee, A. N. K. Chen, and V. Ilie. Can online wait be managed? The effect of filler interfaces and presentation modes on perceived waiting time online. MIS Quarterly, 36(2):365­394, 2012.
[7] C. Liu, F. Yang, Y. Zhao, Q. Jiang, and L. Zhang. What does time constraint mean to information searchers? In Proc. of the 5th IIIX Conference, pages 227­230, 2014.

[8] D. Maxwell and L. Azzopardi. Stuck in traffic: How temporal delays affect search behavior. In Proc. of the 5th IIIX Conference, pages 155­164, 2014.
[9] R. B. Miller. Response time in man-computer conversational transactions. In Proc. of the 1968 Fall Joint Computer Conference, pages 267­277, 1968.
[10] N. Mishra, R. W. White, S. Ieong, and E. Horvitz. Time-critical search. In Proc. of the 37th ACM SIGIR conference, pages 747­756, 2014.
[11] R. Molich and J. Nielsen. Improving a human-computer dialogue. Comm. of the ACM, 33(3):338­348, 1990.
[12] J. W. Payne, J. R. Bettman, and E. J. Johnson. The Adaptive Decision Maker. Cambridge Uni. Press, 1993.
[13] R. Savolainen. Time as a context of information seeking. Library & Info. Sci. Research, 28(1):110­127, Mar. 2006.
[14] D. N. Stone and K. Kadous. The joint effects of task-related negative affect and task difficulty in multiattribute choice. Organizational Behavior and Human Decision Processes, 70(2):159­174, May 1997.
[15] N. J. Taylor, A. R. Dennis, and J. W. Cummings. Situation normality and the shape of search: The effects of time delays and information presentation on search behavior. J. of the A. Soc. for Info. Sci. and Tech., 64(5):909­928, 2013.
[16] J. Teevan, K. Collins-Thompson, R. W. White, S. T. Dumais, and Y. Kim. Slow search: Information retrieval without time constraints. In Proc. of the 7th HCIR Symposium, pages 1­10, 2013.
[17] A. Tombros, I. Ruthven, and J. M. Jose. How users assess web pages for information seeking. J. of the American Society for Info. Sci. and Tech., 56(4):327­344, 2005.
[18] E. M. Voorhees. Overview of the trec 2005 robust retrieval track. In Proceedings of TREC-14, 2006.

770

How Random Decisions Affect Selective Distributed Search

Zhuyun Dai, Yubin Kim, Jamie Callan
Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA
{zhuyund, yubink, callan}@cs.cmu.edu

ABSTRACT
Selective distributed search is a retrieval architecture that reduces search costs by partitioning a corpus into topical shards such that only a few shards need to be searched for each query. Prior research created topical shards by using random seed documents to cluster a random sample of the full corpus. The resource selection algorithm might use a different random sample of the corpus. These random components make selective search non-deterministic.
This paper studies how these random components affect experimental results. Experiments on two ClueWeb09 corpora and four query sets show that in spite of random components, selective search is stable for most queries.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Search and Retrieval
Keywords
distributed retrieval; selective search; variance
1. INTRODUCTION
A selective search architecture reduces search costs by organizing a large corpus into topical index shards and searching only the most likely shards for each query. As defined by prior research, selective search has several non-deterministic steps. Kulkarni [4] used sample-based k-means clustering to create topical shards, which is non-deterministic due to random sampling and cluster seeds. Some resource selection algorithms, such as Rank-S [5], use a random sample of the corpus to decide which shards to search for each query, which introduces additional non-determinism.
Prior research showed that selective search can be as accurate as a typical `search all shards' distributed architecture but at a substantially reduced computational cost [1, 4, 5]. However, it was based on a single partitioning of the corpus. A different partitioning might yield different results.
We are not the first to notice this problem. Jayasinghe, et al. [3] proposed a linear model to statistically compare non-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767796 .

deterministic retrieval systems, using selective search as an example. Their model took into consideration the multidimensional variance in selective search. However, their work focused on testing whether selective search has equivalent mean effectiveness as a baseline search architecture. In our work, we focus on the variance itself.
This paper investigates the effect of random decisions on selective search accuracy by comparing results obtained with different partitionings of two ClueWeb datasets. It examines the variance of a system across four query sets (Section 3.1), and for individual queries (Section 3.2).
2. EXPERIMENTAL METHOD
We define a system instance to be a partition of a corpus index and its corresponding resource selection database for algorithms such as Rank-S [5] or Taily [1]. Our system instances were defined by a slight adaptation of a process defined by Kulkarni [4]. First, 500K documents were randomly sampled from the corpus, k of those documents were selected to be seeds, and k-means clustering was used to form clusters. Second, the remaining documents were projected onto the most similar clusters to form index shards. Third, spam documents were removed and a resource selection index was constructed. The Rank-S resource selection index was formed from a 1% random sample of each cluster. The Taily resource selection index is created deterministically. Thus, each system instance involved 2-3 random processes.
The parameters used for Taily and Rank-S were suggested by Aly et al. [1] and Kulkarni et al. [5]. They were n = 400, v = 50 for Taily and B = 5, CSI = 1% for Rank-S.
The experiments also tested Relevance-Based Ranking (RBR), an oracle resource selection algorithm that ranks shards by the number of relevant documents that they contain. RBR makes it possible to distinguish between different types of variance. For query q, RBR searched the average number of shards that Rank-S and Taily searched for q.
Experiments were conducted on the ClueWeb09 Category A (CW09-A) corpus, which contains 500 million English web pages, and ClueWeb09 Category B (CW09-B ), which contains 50 million English web pages. Queries were from the TREC 2009-2012 Web Tracks: 4 sets of 50 queries.
Parameters were set to produce shards of 500K documents on average. Each CW09-A system instance had 1000 shards; CW09-B system instances had 100 shards.
3. EXPERIMENT RESULTS
The effects of random decisions during partitioning and indexing can be measured across query sets or individual queries. Measuring across query sets provides information

771

P@10

Rank-S CW09-B

0.60

exhaustive search

0.55

0.50

0.45

0.40

0.35

0.30

0.25

0.20 2009 2010 2011 2012 Query Set

Rank-S CW09-B

0.40

exhaustive search

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00 2009 2010 2011 2012 Query Set

Rank-S CW09-A

0.60

exhaustive search

0.55

0.50

0.45

0.40

0.35

0.30

0.25

0.20 2009 2010 2011 2012 Query Set

Rank-S CW09-A

0.40

exhaustive search

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00 2009 2010 2011 2012 Query Set

RBR CW09-B

0.60

exhaustive search

0.55

0.50

0.45

0.40

0.35

0.30

0.25

0.20 2009 2010 2011 2012 Query Set

RBR CW09-B

0.40

exhaustive search

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00 2009 2010 2011 2012 Query Set

RBR CW09-A

0.60

exhaustive search

0.55

0.50

0.45

0.40

0.35

0.30

0.25

0.20 2009 2010 2011 2012 Query Set

RBR CW09-A

0.40

exhaustive search

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00 2009 2010 2011 2012 Query Set

MAP@500

Figure 1: Accuracy distribution of Rank-S and RBR system instances. Mid line in boxes represents median. Outer edges of the boxes represent lower and upper quartiles, while whiskers indicate 1.5 interquartile range. + indicates outliers. Blue diamond is the performance of the typical `search all shards baseline.

about the reliability of metrics such as MAP and NDCG that are typically reported in research papers. Measuring across individual queries provides information about the behavior that people will observe when they use the system. The experiments below provide both types of measurement.
3.1 Variability for Query Sets
The first experiment examined how random decisions affect the `average case' metrics typically reported in research papers. It used three resource selection algorithms: Rank-S, which has a random component; Taily, which is deterministic; and RBR, the oracle algorithm. Ten random partitions were created for each dataset. Thus there were 2 datasets × 10 random partitions × 3 resource selection algorithms = 60 system instances. On average, Rank-S searched 3.91 CW09-A shards and 3.75 CW09-B shards; Taily searched 2.58 CW09-A shards and 2.53 CW09-B shards; and RBR searched 3.13 CW09-A shards and 2.80 CW09-B shards.
Retrieval accuracy was evaluated using three metrics: Precision at rank 10 (P@10), Normalized Discounted Cumulative Gain at rank 100 (NDCG@100), and Mean Average Precision at rank 500 (MAP@500) for compatibility with prior research [1]. Results for Taily and NDCG@100 were similar to results for Rank-S and other metrics, thus they are omitted due to space limitations.
The distributions of measurements over each type of system instance and the `search all shards' baseline are shown in Figure 1. Tables 1 and 2 report the standard deviation and variance coefficient of Rank-S and RBR system accuracy

scores. The standard deviation is a common measure, however it has two flaws: i) values produced by different metrics (e.g., P@10, MAP@500) cannot be compared because it is not dimensionless; and ii) the importance of a given level of deviation depends upon the value of the mean. The variance coefficient (VC) normalizes the standard deviation by the mean, thus eliminating these flaws.
The Relevance-Based Ranking (RBR) was generally more accurate and had lower variance across system instances than Rank-S and Taily, as expected. The one exception was the 2011 query set and the MAP@500 metric; Rank-S had lower variance than RBR under this condition. This exception was caused by a small set of queries. Rank-S had consistently poor accuracy on these queries, whereas RBR had greater accuracy but also greater variance.
The experimental results suggest that there are two sources of variance in selective search: Partitioning, and resource selection. RBR always selects the best shards, thus variance in RBR results is due to differences in partitioning effectiveness. Rank-S and Taily use models (resource selection indices) to make decisions about which shards to select for each query, however those models are necessarily incomplete. The increases in variance for Rank-S and Taily as compared to RBR are due to resource selection errors caused by weaknesses in the models of shard contents.
Selective search instances displayed differing levels of variability across different metrics. P@10 overall had a lower variance coefficient than MAP@500 (Tables 1 and 2). Similar behavior is observed for Taily instances. This behavior

772

Table 1: Standard Deviation (SD) and Variance Coefficient (VC) of accuracy scores for Rank-S instances.

Query Set 2009 2010 2011 2012

a) P@10

CW09-B

CW09-A

SD

VC

SD

VC

2.22 × 10-2 5.06% 2.00 × 10-2 7.30%

2.11 × 10-2 5.57% 1.50 × 10-2 4.90%

1.52 × 10-2 4.84% 1.51 × 10-2 5.06%

1.79 × 10-2 5.44% 1.54 × 10-2 6.14%

Query Set 2009 2010 2011 2012

b) MAP@500

CW09-B

CW09-A

SD

VC

SD

VC

7.77 × 10-3 10.04% 6.65 × 10-3 10.65%

7.56 × 10-3 7.16% 3.31 × 10-3 3.66%

8.52 × 10-3 8.89% 7.67 × 10-3 7.62% 7.17 × 10-3 6.37% 7.96 × 10-3 8.23%

indicates that selective search is more stable at the top of the ranking.
Rank-S is affected by one more random component than Taily, thus it might be expected to have greater variability across system instances. Table 3 shows the variance coefficient of MAP@500 for Rank-S and Taily across ten CW09-A system instances; similar behavior was observed across all metrics for both collections. Taily only had lower variance than Rank-S on 1 of the 4 query sets. These results might be considered surprising. The additional random component does not appear to cause greater instability in Rank-S results. As far as we know, prior research has not investigated the variability of results produced by document-based and model-based algorithms such as Rank-S and Taily.
3.2 Variability for Queries
Ideally selective search would provide similar results for a given query regardless of which system instance is used. The second experiment examined variability on a query-by-query basis.
We examined the variance of Average Precision of each query across Rank-S and Taily instances on CW09-A and observed some highly variant queries in both Rank-S and Taily. Rank-S had 14 queries with AP standard deviation higher than 0.1; Taily had 17 such high variance queries.
High variance could be due to errors from the partitioning process or errors from resource selection. To discover the source of the variance, a query-by-query experiment was conducted with RBR on the same partitions. RBR does not make resource selection errors but is affected by partitioning errors. Thus, if Rank-S, Taily, and RBR all have trouble with a query, the problem is likely to be partitioning.
Figure 2 shows the average precision (AP) of each query for ten RBR system instances. A notable observation is that most of the queries have stable AP; 177 of the 200 queries had a standard deviation of less than 0.05. However, there were a few queries with very high variance that contributed most of the average variance.
Whether the query is variable or stable remained mostly consistent across RBR, Rank-S, and Taily. While there were some highly variable queries in RBR that were not so in Taily and Rank-S, these were difficult queries (low `search all shards' search accuracy), and had nearly zero MAP@500

Table 2: Standard Deviation (SD) and Variance Coefficient (VC) of accuracy scores for RBR instances.

Query Set 2009 2010 2011 2012

a) P@10

CW09-B

CW09-A

SD

VC

SD

VC

1.38 × 10-2 2.71% 1.05 × 10-2 2.95%

1.16 × 10-2 2.15% 9.73 × 10-3 2.04%

1.36 × 10-2 3.53% 1.31 × 10-2 3.04%

5.39 × 10-3 1.39% 9.73 × 10-3 2.63%

Query Set 2009 2010 2011 2012

b) MAP@500

CW09-B

CW09-A

SD

VC

SD

VC

3.56 × 10-3 3.25% 3.54 × 10-3 2.97%

3.87 × 10-3 2.24% 2.46 × 10-3 1.35%

1.10 × 10-2 7.21% 9.47 × 10-3 4.40% 3.53 × 10-3 2.41% 4.97 × 10-3 2.91%

Table 3: Comparison of MAP@500 scores (CW09-A) for Rank-S and Taily instances.

Query Set 2009 2010 2011 2012

Mean Rank-S Taily 0.062 0.065 0.090 0.087 0.101 0.084 0.097 0.085

Variance Coefficient Rank-S Taily 10.65% 10.98% 3.66% 6.89% 7.62% 15.67% 8.23% 5.36%

in Rank-S and Taily. All easy queries with high variance in RBR also had high variance in Rank-S and Taily. The consistency across RBR, Rank-S and Taily suggests that errors from the partitioning stage are a major source of variance of selective search accuracy; however, most of that variation comes from a small number of queries.
We investigated why some partitions performed much better than others on the high variance queries. One might expect that `good' partitions group relevant documents into fewer shards. However, in all of our partitions, relevant documents were distributed across a small number of shards. Typically the 3 most relevant shards contained more than 60% of the relevant documents, which is consistent with the standards of most prior federated search research. Furthermore, in this experiment, all ten instances retrieved 100% of the relevant documents for 4 of the 5 most variant queries. However, these queries had AP values ranging from values similar to exhaustive search to as much as ten times better than exhaustive search.
An examination of the instances with unusually high AP in the RBR experiment revealed that the relevant documents and the most likely false positives were in different shards. Thus, the combination of partitioning and resource selection `filtered out' non-relevant documents that otherwise would have been ranked highly. Although this behavior might seem desirable, it occurred because the partitioning process incorrectly assigned relevant documents to shards that contained documents that were largely dissimilar to the relevant documents. This poor clustering made it nearly impossible for Rank-S and Taily to identify which shards contained relevant documents due to the overwhelming number of dissimilar documents in the shard. RBR, which knows the number of relevant documents in each shard, had no such problem.

773

Average Precision

1.0

2009

0.8

0.6

0.4

0.2

0.0 1 5 10 15 20 25 30 35 40 45 50

1.0

2011

0.8

0.6

0.4

0.2

0.0 105 110 115 120 125 130 135 140 145 150 Query ID

Average Precision

Average Precision

1.0

2010

0.8

0.6

0.4

0.2

0.0 55 60 65 70 75 80 85 90 95 100

1.0

2012

0.8

0.6

0.4

0.2

0.0 155 160 165 170 175 180 185 190 195 200 Query ID

Average Precision

Figure 2: Query Average Precision of RBR instances. Each data point is a box and whisker plot with the edges of the boxes representing the upper and lower quantiles, the mid line the median, and the whiskers the 1.5 interquartile range. + are outliers.

Figure 2 also indicates that different query sets produced different levels of variability. The 2011 query set was notably more unstable than the others. Among the 24 queries with AP standard deviation higher than 0.05, 14 were from TREC 2011. We believe that this difference is due to the other three query sets using topics of medium-to-high frequency, while TREC 2011 used more obscure topics [2]. Our results indicate that the type of query has an impact on not only the average accuracy of a system, but also the variance of a system with random components. Topic-based partitioning may produce stable results for common topics, but may need improvement for queries about rare topics.
4. CONCLUSION
Understanding the variance of selective search effectiveness is critical for evaluating and improving selective search. This paper explores selective search variance and the effects of document collections, resource selection algorithms, and query characteristics. Partitioning and resource selection processes both introduce variance into search behavior, however in our experiments the effect on most queries was not large. Results were more stable at the top of the ranking, but variance at rank 500 was not unreasonable, especially for an architecture that avoids searching most of the index.
Rank-S and Taily produced nearly equal variance, which might be considered surprising given that Rank-S has one more random component than Taily.
Most of the variance observed in our per-query experiments was caused by a small number of queries ­ typically, rare queries. In some instances the partitioning process correctly grouped relevant documents together, but placed

them in unrepresentative shards, which caused poor resource selection. This behavior may indicate the need for partitioning processes more sophisticated than simple k-means clustering.
5. ACKNOWLEDGMENTS
This research was supported by National Science Foundation (NSF) grant IIS-1302206 and Natural Sciences and Engineering Research Council of Canada (NSERC) Postgraduate Scholarship-Doctoral. Any opinions, findings, conclusions, and recommendations expressed in this paper are the authors' and do not necessarily reflect those of the sponsors.
6. REFERENCES
[1] R. Aly, T. Demeester, and D. Hiemstra. Taily: Shard selection using the tail of score distributions. In Proceedings of SIGIR, 2013.
[2] C. L. Clarke, N. Craswell, I. Soboroff, and E. M. Voorhees. Overview of the trec 2011 web track. In Proceedings of TREC 2011, 2011.
[3] G. K. Jayasinghe, W. Webber, M. Sanderson, L. S. Dharmasena, and J. S. Culpepper. Evaluating non-deterministic retrieval systems. In Proceedings of SIGIR, 2014.
[4] A. Kulkarni. Efficient and Effective Large-scale Search. PhD thesis, Carnegie Mellon University, 2013.
[5] A. Kulkarni, A. Tigelaar, J. Callan, and D. Hiemstra. Shard ranking and cutoff estimation for topically partitioned collections. In Proceedings of CIKM, 2012.

774

Modelling Term Dependence with Copulas

Carsten Eickhoff
Dept. of Computer Science ETH Zurich, Switzerland
ecarsten@inf.ethz.ch

Arjen P. de Vries
CWI Amsterdam Amsterdam, The Netherlands
arjen@acm.org

Thomas Hofmann
Dept. of Computer Science ETH Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

ABSTRACT
Many generative language and relevance models assume conditional independence between the likelihood of observing individual terms. This assumption is obviously na¨ive, but also hard to replace or relax. There are only very few term pairs that actually show significant conditional dependencies while the vast majority of co-located terms has no implications on the document's topical nature or relevance towards a given topic. It is exactly this situation that we capture in a formal framework: A limited number of meaningful dependencies in a system of largely independent observations. Making use of the formal copula framework, we describe the strength of causal dependency in terms of a number of established term co-occurrence metrics. Our experiments based on the well known ClueWeb'12 corpus and TREC 2013 topics indicate significant performance gains in terms of retrieval performance when we formally account for the dependency structure underlying pieces of natural language text.
Categories and Subject Descriptors
Information Systems [Information Retrieval]: Retrieval models
Keywords
Relevance models; Multivariate relevance; Ranking; Probabilistic framework; Language models.
1. INTRODUCTION & RELATED WORK
Generative n-gram language models are frequently used tools for representing document or collection vocabulary in the form of probability distributions over (spans of) textual tokens. They are popular for a wide array of tasks, including sentiment analysis, machine translation, content based classification and document retrieval. Most state-of-the-art models assume individual terms to be independently drawn from the underlying distribution. While this independence
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DhtOtpI:://hdtxtp.d:/o/id.xo.rdgo/1i.0o.r1g1/1405./12174656/426726.62X76X7X83.X1 XXXXXX.

assumption greatly simplifies the computation of conditional probabilities, it is also rather na¨ive. It is easy to find examples of proper names such as "Barack Obama" or "Hong Kong", but also other fixed expressions ("tax evasion") as well as non-consecutive constructs ("dollar " , "stock "), that should benefit from an explicit model of term interdependency. It is easy to see that some of these examples go beyond the capabilities of mere higher-order language models. In the past, there have been a number of attempts to formally integrate term dependency structures into generative models.
Van Rijsbergen's early work on dependency trees [19] theoretically establishes the use of maximum spanning trees and term co-occurrence statistics in order to establish local term dependency structures. Yu et al. [20] present a comparison of tree and cluster-based methods for dependency modelling in information retrieval. Srikanth and Srihari [18] investigate dependency-aware relevance models by using higher order n-gram models and comparing to the unigram setting. They report consistent improvements for the dependency models. Croft et al. [7] propose the use of inference networks trained on the basis of term proximity information to model the relevance of entire phrases. In a related effort, Lossee [12] confirms the important role of proximity in dependency modelling. The author reports optimal performance using context windows of 3-5 terms surrounding each candidate term. In a comparison study of multiple dependency models, Bruza and Song [4] achieve best results using a matrix representation of co-occurrence contexts to describe terms. Gao et al. [11] explicitly decouple the dependency structure of a sentence from the concrete term generation probabilities in the form of linkages.
Nallapati and Allan [14] relax the independence assumption by modelling documents as groups of (still independent) sentences. Within each sentence, however, they condition the probability of observing a term on all previous terms in the same sentence. Their sentence model is based on the maximum spanning tree over the fully connected sentence graph. The individual strength of dependency within term pairs is measured in terms of the Jaccard co¨efficient. They later refine this model by advancing from dedicated sentence trees to entire forests [15] of trees for each connected component in the sentence graph. Cao et al. [5] use Bayesian networks to combine two sources of term dependence: cooccurrence and semantic relatedness. The latter is expressed in terms of proximity in the WordNet graph. While most approaches define document language models and compare their respective likelihoods of having generated the query,

783

Bai et al. [1] propose a term-dependency query model in order to account explicitly for query expansion. Metzler and Croft [13] learn Markov Random Fields that account for various forms of term dependence on the basis of arbitrary feature vectors. Bendersky et al. [3, 2] propose a learning to rank approach that accounts for higher order term and concept dependencies using hypergraphs. Shi and Nie [17] investigate different dependence weighting schemes based on the concept's utility in the respective retrieval task.
In this paper, we present the use of copulas, a robust statistical model family that is able to explicitly decouple marginal observations (i.e., the individual likelihoods of generating terms) from their underlying dependency structures. Comparing to a number of well-known baseline methods and evaluating on a large-scale standard dataset, we show the competitive performance of this novel approach to dependency modelling.

2. METHODOLOGY
In this section, we will give a brief overview of the formal copula framework, introducing the relevant notation and conventions. For a more comprehensive introduction to the topic, please refer to the previous IR applications by Eickhoff et al. [8, 9], or the surveys by Embrechts [10] and Schmidt [16]. Let X be a k-dimensional random vector of observations that we wish to use as input to our copula model:
Xk = (x1, x2, . . . , xk)

The copula allows us to model the likelihood of observing X by offering computationally efficient approximations to the true joint probability distribution in the high-dimensional space of cardinality k. As a first step, copulas require input scores U k to be uniformly distributed in the [0 . . . 1] interval. We can achieve this by defining a set of transformations F (X) between raw marginal observations X and their normalized equivalents on the unit cube U .
U k = (u1, u2, . . . , uk) = F (X) = (f1(u1), f2(u2), . . . , fk(uk))

An easy example of such a function is the empirical distribution function f^:

f^(t)

=

1 n

1 {xi  t}

The cumulative distribution function C for all copulas is fully defined in terms of a generator  and its inverse -1:

C(u1, u2, . . . , uk) = -1((u1) + (u2) + . . . + (uk))

There are many concrete instantiations of such copula functions. Each copula family defines their own generator and inverse. A previous study [8] compared a wide range of copula families for the task of Web retrieval, finding Gumbel copulas to be the most adequate choice in this setting. For reasons of space, this paper builds on the previous findings and concentrates exclusively on Gumbel copulas. Their generators are given in the following form:

-1(t)

=

exp(-t

1 

)

(t) = (-log(t))

The resulting distribution function for a 2-dimensional Gumbel copula is, for example:

C (u1 ,

u2)

=

exp(-((-log(u1))

+

(-log(u2 )) )

1 

)

Once the choice of copula family is made, we are left with just a single parameter  that allows us to control the strength of dependency between the individual marginal observations u. If we, for example, set  = 1, our distribution function defaults to the case of conditional independence:

C=1 = exp(-(-log(u1)) + (-log(u2))) = u1  u2

Any choice of  > 1 results in an increasing degree of conditional dependency between the k dimensions of our observation. At this point, we have introduced all relevant components for our original use case of statistical language modelling. A traditional unigram language model describes the likelihood of observing a string of text T under a given class c as the product of the individual likelihoods of each term:

P (T |c) = P (t1|c)P (t2|c) . . . P (t|T ||c)

The same can be achieved under the copula framework by considering the class-conditional probabilities of observing individual terms t1, t2, . . . as our marginal observations, making the dimensionality of our copula k = |T |:

P (T |c) = C=1(P (t1|c), P (t2|c), . . . P (t|T ||c))

By choosing  = 1, we ensure conditional independence between the marginal term observation likelihoods, giving us the standard unigram language model. As we however increase , the strength of dependency between the individual terms increases. This ability to account for term dependence makes the copula framework a powerful alternative to the standard language modelling scheme. At this point, any setting of  globally describes the relationship between all terms. In practice, however, we much rather want a select few terms to depend on each other, while the majority of terms occur indeed independently.
This is easily achieved by using nested copulas. Instead of combining all dimensions in a single step as described earlier, they allow for a nested hierarchy of multiple copulas that estimate joint distributions for sub sets of the full term space and subsequently combine scores until one global model is obtained. Generally, an example of a fully nested copula with k dimensions is given by:

C0(u1, C1(u2, C2(. . . , Ck-2(uk-1, uk))))

By means of the structure of the nesting "tree", nested copulas can explicitly model which dimensions depend on each other directly. Instead of the global  parameter discussed earlier, each of the constituent copulas defines their respective i, determining the strengths of these (per-dimension) dependencies. This mechanism gives nested copulas a theoretical advantage in flexibility over their non-nested counterparts. Effectively, this allows us to describe formally grounded probabilistic models under which select term pairs show dependencies ( > 1) while the majority of terms oc-

784

C=1

C>1

P("is") P("in") P("greenland")

P("winter") P("harsh")
Figure 1: The copula dependence tree for the sentence "Winter is harsh in Greenland." shows significant dependence between the terms "winter " and "harsh" while the remaining terms occur independently.

cur independently of each other ( = 1). Figure 1 shows an example of such a situation.
At this point, the final missing component in our language modelling scheme is a way to determine the concrete settings of  for a pair of terms. To this end, we define conditional dependency in terms of frequency of co-occurrence in a document corpus and rely on two widely used co-occurrence metrics. The point-wise mutual information between terms t1 and t2 as well as their Jaccard coefficient measure in which fraction of sentences the terms co-occur.

PMI

(t1,

t2)

=

log2

P (t1,t2) P (t1)P (t2)

J (t1, t2)

=

|t1 |t1

 

t2 | t2 |

Finally, the dependency parameter t1,t2 is defined on the basis of the concrete choice of metric m  {PMI , J} and its collection-wide metric mean (µPMI , µJ ) across all potential term pairs. All those pairs of higher-than-average co-occurrence frequency are assigned values of  proportionally to their relative co-occurrence rate. Since  is defined in the range [1, ) and the resulting scores scale in a nonlinear fashion, there is no need to further address or remove outlier pairs of extremely high frequency.

m,t1,t2 =

m(t1 ,t2 ) µm
1

if m(t1, t2) > µm else

3. EXPERIMENTS
To empirically test the performance of the previously presented copula-based language modelling scheme, we investigate its performance at the task of adhoc document retrieval. Instead of modelling the likelihood of observing a given document under a topic specific language model, we will now establish one distinct model per document and compare their respective likelihoods of having generated the query q.
P (rel|q, d)  P (q|d)
|q|
P (q|d)indep = P (wi|d)
i=1
P (q|d)cop = Cd(w1, w2, . . . , wn)|w  q
For our experimental comparison, we rely on the widely used ClueWeb'12 corpus, a collection of 730 million authentic Web documents. Our 50 topics originate from TREC's 2013 Adhoc retrieval task [6]. We contrast our method's

Table 1: Retrieval performance on ClueWeb'12 and

TREC 2013 Adhoc topics at a cut-off threshold of

20 retrieved documents.

Model

Precision Recall F1 MAP

Unigram LM 0.31

0.22 0.26 0.41

Bigram LM

0.34

0.26 0.3 0.45

SenTree

0.35

0.28 0.31 0.47

MRF

0.38

0.31 0.34 0.51

Copula LM 0.41* 0.35* 0.38* 0.52

performance with a number of established as well as stateof-the-art baselines such as standard unigram and bigram language models, Nallapati's sentence trees [14], as well as the Markov Random Field model [13] and apply Laplace smoothing to all LM variants in order to account for previously unseen query terms. Table 1 details the respective performances obtained by the various methods in terms of precision, recall, F1 and MAP, each computed at a cut-off rank of 20 retrieved documents. Statistically significant improvements over all baseline methods are indicated by the asterisk character. Statistical significance was tested using a Wilcoxon signed-rank test at   0.05-level. We can note that, due to their wider context, the classification performance of bigram language models significantly exceeds that of the lower-order model. SenTrees as well as the MRF model which explicitly capture term dependence show even higher classification performance. Finally, our copula language model yields significant performance improvements across most metrics and baselines. The improvements over the MRF model were only significant for some of the considered metrics.
4. CONCLUSION
In this paper, we demonstrated the use of the copula framework, a model family from the field of robust statistics, for representing term dependencies in language models. The main advantages of the proposed model are its formal rigour, the low model complexity in terms of training effort as well as disk space requirements and its high degree of flexibility. As an additional advantage, copulas have been previously shown [9] to be beneficial for qualitative manual inspection of results.
Our experiments, based on a sizeable document collection (ClueWeb'12) confirm the competitive performance of the proposed model in comparison with a number of state-ofthe-art baselines.

785

The present paper describes early results of an ongoing body of research. Consequently, there are numerous directions for future work that are interesting to explore: (1) In this paper, we investigated "single-layer" dependency structures with a nesting depth of 1. The nested copula framework, however, is able to capture arbitrarily complex structures. Given a modified dependency estimation scheme, the model can easily account for cases of fine-grained multilevel dependencies. (2) Similarly, the current model regards only dependencies of degree 2. Since the copula framework is able to account for higher-degree dependencies (i.e., between three or more terms) this is another promising alley for continued research. (3) Previous work has investigated different forms of inter-term dependency, including, for example, semantic proximity. It would be easy to integrate such additional sources of evidence into our  estimation step. (4) We would like to draw from the existing wealth of topic modelling techniques in order to describe not merely the dependency structure between individual terms but also between terms and more high-level (latent) concepts, allowing for exciting new insights. (5) Finally, we would like to explore means of representing local term context into the dependency model. Take for instance the two terms "new" and "york". In the query [affordable real estate on the US east coast] these terms clearly have some dependency, whereas they probably are less dependent when the query is [affordable real estate in yorkshire, UK]. The current paper highlights the applicability of the context invariant method for general queries. In the future we will additionally investigate the importance of the immediate context.
5. REFERENCES
[1] Jing Bai, Dawei Song, Peter Bruza, Jian-Yun Nie, and Guihong Cao. Query expansion using term relationships in language models for information retrieval. In Proceedings of the 14th ACM international conference on Information and knowledge management, pages 688­695. ACM, 2005.
[2] Michael Bendersky and W Bruce Croft. Modeling higher-order term dependencies in information retrieval using query hypergraphs. In Proceedings of the 35th international ACM SIGIR conference, pages 941­950. ACM, 2012.
[3] Michael Bendersky, Donald Metzler, and W Bruce Croft. Learning concept importance using a weighted dependence model. In Proceedings of the third ACM international conference on Web search and data mining, pages 31­40. ACM, 2010.
[4] Peter Bruza and Dawei Song. A comparison of various approaches for using probabilistic dependencies in language modeling. In Proceedings of the 26th annual international ACM SIGIR conference, pages 419­420. ACM, 2003.
[5] Guihong Cao, Jian-Yun Nie, and Jing Bai. Integrating word relationships into language models. In Proceedings of the 28th annual international ACM SIGIR conference, pages 298­305. ACM, 2005.
[6] Kevyn Collins-Thompson, Paul Bennett, Fernando Diaz, Charles LA Clarke, and Ellen M Voorhees. Trec 2013 web track overview. In 22nd Text REtrieval Conference, Gaithersburg, Maryland, 2014.
[7] W Bruce Croft, Howard R Turtle, and David D Lewis. The use of phrases and structured queries in

information retrieval. In Proceedings of the 14th annual international ACM SIGIR conference, pages 32­45. ACM, 1991.
[8] Carsten Eickhoff and Arjen P de Vries. Modelling complex relevance spaces with copulas. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1831­1834. ACM, 2014.
[9] Carsten Eickhoff, Arjen P de Vries, and Kevyn Collins-Thompson. Copulas for information retrieval. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, pages 663­672. ACM, 2013.
[10] P. Embrechts, F. Lindskog, and A. McNeil. Modelling dependence with copulas and applications to risk management. Handbook of heavy tailed distributions in finance, 8(329-384):1, 2003.
[11] Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and Guihong Cao. Dependence language model for information retrieval. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 170­177. ACM, 2004.
[12] Robert M Losee. Term dependence: truncating the bahadur lazarsfeld expansion. Information processing & management, 30(2):293­303, 1994.
[13] Donald Metzler and W Bruce Croft. A markov random field model for term dependencies. In Proceedings of the 28th annual international ACM SIGIR conference, pages 472­479. ACM, 2005.
[14] Ramesh Nallapati and James Allan. Capturing term dependencies using a language model based on sentence trees. In Proceedings of the eleventh international conference on Information and knowledge management, pages 383­390. ACM, 2002.
[15] Ramesh Nallapati and James Allan. An adaptive local dependency language model: Relaxing the naive bayes' assumption. 2003.
[16] T. Schmidt. Coping with copulas. Risk Books: Copulas from Theory to Applications in Finance, 2007.
[17] Lixin Shi and Jian-Yun Nie. Using various term dependencies according to their utilities. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 1493­1496. ACM, 2010.
[18] Munirathnam Srikanth and Rohini Srihari. Incorporating query term dependencies in language models for document retrieval. In Proceedings of the 26th annual international ACM SIGIR conference, pages 405­406. ACM, 2003.
[19] Cornelis Joost van Rijsbergen. A theoretical basis for the use of co-occurrence data in information retrieval. Journal of documentation, 33(2):106­119, 1977.
[20] Clement T Yu, Chris Buckley, K Lam, and Gerard Salton. A generalized term dependence model in information retrieval. Technical report, Cornell University, 1983.

786

A Word Embedding based Generalized Language Model for Information Retrieval

Debasis Ganguly
ADAPT Centre, School of Computing Dublin City University Dublin, Ireland
dganguly@computing.dcu.ie
Mandar Mitra
CVPR Unit Indian Statistical Institute
Kolkata, India
mandar@isical.ac.in
ABSTRACT
Word2vec, a word embedding technique, has gained significant interest among researchers in natural language processing (NLP) in recent years. The embedding of the word vectors helps to identify a list of words that are used in similar contexts with respect to a given word. In this paper, we focus on the use of word embeddings for enhancing retrieval effectiveness. In particular, we construct a generalized language model, where the mutual independence between a pair of words (say t and t ) no longer holds. Instead, we make use of the vector embeddings of the words to derive the transformation probabilities between words. Specifically, the event of observing a term t in the query from a document d is modeled by two distinct events, that of generating a different term t , either from the document itself or from the collection, respectively, and then eventually transforming it to the observed query term t. The first event of generating an intermediate term from the document intends to capture how well a term fits contextually within a document, whereas the second one of generating it from the collection aims to address the vocabulary mismatch problem by taking into account other related terms in the collection. Our experiments, conducted on the standard TREC 6-8 ad hoc and Robust tasks, show that our proposed method yields significant improvements over language model (LM) and LDA-smoothed LM baselines.
Categories and Subject Descriptors
H.3.3 [INFORMATION STORAGE AND RETRIEVAL]: Information Search and Retrieval--Retrieval models, Relevance Feedback, Query formulation
General Terms
Theory, Experimentation
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'15, Aug 9­13, 2015, Santiago, Chile. Copyright 2015 ACM ISBN 978-1-4503-3621-5/15/08 ...$15.00. http://dx.doi.org/10.1145/2766462.2767780 .

Dwaipayan Roy
CVPR Unit Indian Statistical Institute
Kolkata, India
dwaipayan_r@isical.ac.in
Gareth J.F. Jones
ADAPT Centre School of Computing Dublin City University Dublin, Ireland
gjones@computing.dcu.ie
Keywords
Generalized Language model, Word Embedding, Word2Vec
1. INTRODUCTION
Word embedding as technique for representing the meaning of a word in terms other words, as exemplified by the Word2vec approach [7]. The embedding of the word vectors enables the identification of words that are used in similar contexts to a specufic word. a list of words that are used in similar contexts with respect to a given word. While word Embedding has gained significant interest among researchers in natural language processing (NLP) in recent years, there has to date been little exploration of the potential for use of these methods in information retrieval (IR).
This paper explores the use of word embeddings of enhance IR effectiveness. We begin with a brief introduction to word embedding techniques and then motivate how can these be applied in IR.
A brief introduction to word embedding. Word embedding techniques seek to embed representations of words. For example, two vectors t and t , corresponding to the words t and t , are close in an abstract space of N dimensions if they have similar contexts and vice-versa, (i.e. the contexts in turn having similar words) [4]. Use of a cosine similarity measure on this abstract vector space of embedded words can be used to identify a list of words that are used in similar contexts with respect to a given word. These semantically related words may be used for various natural language processing (NLP) tasks. The general idea is to train moving windows with vector embeddings for the words (rather than training with the more conventional word count vectors), and classify the individual windows [2]. This finds application for examples in applications such as POS tagging, semantic role labeling, named-entity recognition and other tasks. The state-of-the-art word embedding approaches involve training deep neural networks with the help of negative sampling [7]. It is reported that this process of negative sampling (commonly known as word2vec1) produces reliable word embeddings in a very efficient manner [7].
Potential use in IR. We now discuss how word embeddings can potentially be helpful in improving retrieval quality. In the context of IR, vocabulary mismatch, i.e. the inherent characteristic of using different but semantically similar terms across documents about
1The name word2vec comes from the name of the software tool released by Micholov et. al. (https://code.google.com/ p/word2vec/

795

the same topic or between a query and its relevant documents, is a difficult problem to solve.
However, the working principle of most standard retrieval models in IR involves an underlying assumption of term independence, e.g. the vector space model (VSM) assumes that the documents are embedded in a mutually orthogonal term space, while probabilistic models, such as the BM25 or the language model (LM) assume that the terms are sampled independently from documents. Standard approaches in IR take into account term association in two ways, one which involves a global analysis over the whole collection of documents (i.e. independent of the queries), while the other takes into account local co-occurrence information of terms in the top ranked documents retrieved in response to a query. The latter approach corresponds to the relevance feedback step in IR which we do not investigate in this paper. Existing global analysis methods such as the latent semantic indexing (LSA) [3] or latent Dirichlet allocation (LDA) [1] only take into account the co-occurrences between terms at the level of documents instead of considering the context of a term. Since the word embedding techniques that we introduced in the beginning of this section, leverage the information around the local context of each word to derive the embeddings (two words have close vector representations if and only if they are used in similar contexts), we believe that such an approach can potentially improve the global analysis technique of IR leading to better retrieval effectiveness.
The rest of the paper is organized as follows. Section 2 discusses related work. In Section 3, we propose the generalized LM, which is evalaued in Section 4. Finally, Section 5 concludes the paper.
2. RELATED WORK
Latent semantic analysis (LSA) [3] is a global analysis technique in which documents are represented in a term space of reduced dimensionality so as to take into account inter-term dependencies. More recent techniques such as the latent Dirichlet allocation (LDA) represent term dependencies by assuming that each term is generated from a set of latent variables called the topics [1]. A major problem of these approaches is that they only consider word co-occurrences at the level of documents to model term associations, which may not always be reliable. In contrast, the word embeddings take into account the local (window-based) context around the terms [7], and thus may lead to better modeling of the term dependencies.
Moreover, most of these global analysis approaches, e.g. LDA, have been applied in IR in an ad-hoc way for re-assigning term weights without explicitly representing the term dependencies as an inherent part of an IR model. For example, an LDA document model (term sampling probabilities marginalized over a set of latent topic variables) is linearly added as a smoothing parameter to the standard LM probability [9], as a result of which the term dependencies are not clearly visible from the model definition. Contrastingly, in this paper, we intend to directly model the term dependencies as a part of an IR model.
3. A GENERALIZED LANGUAGE MODEL
In this section, we propose the generalized language model (GLM) that models term dependencies using the vector embeddings of terms.
3.1 Language Modelling
In LM, for a given query q, documents are returned as a ranked list sorted in decreasing order by the posterior probabilities P (d|q). These posterior probabilities are estimated for each document d during indexing time with the help of the prior probability (P (q|d))

according to the Bayes rule [8, 6, 10].

P (q|d).P (d)

P (d|q) =

 P (q|d).P (d) = P (d). P (t|d)

d C P (q|d ).(d )

tq

= P^(t|d) + (1 - )P^(t|C) =  tf (t, d) + (1 - ) cf (t)

|d|

cs

tq

tq

(1)

In Equation 1, the set C represents a universe of documents (commonly known as the collection), P^(t|d) and P^(t|C) denote the maximum likelihood estimated probabilities of generating a query term t from the document d and the collection respectively, using frequency statistics. The probabilities of these two (mutually exclusive) events are denoted by  and 1 -  respectively. The notations tf (t, d), |d|, cf (t) and cs denote the term frequency of term t in document d, the length of d, collection frequency of the term t and the total collection size respectively.

3.2 Term Transformation Events
As per Equation 1, terms in a query are generated by sampling them independently from either the document or the collection. We propose the following generalization to the model. Instead of assuming that terms are mutually independent during the sampling process, we propose a generative process in which a noisy channel may transform (mutate) a term t into a term t. More concretely, if a term t is observed in the query corresponding to a document d, according to our model it may have occurred in three possible ways, shown as follows.
· Direct term sampling: Standard LM term sampling, i.e. sampling a term t (without transformation) either from the document d or from the collection.
· Transformation via Document Sampling: Sampling a term t (t = t) from d which is then transformed to t by a noisy channel.
· Transformation via Collection Sampling: Sampling the term t from the collection which is then transformed to t by the noisy channel.
Transformation via Document Sampling. Let P (t, t |d) denote the probability of generating a term t from a document d and then transforming this term to t in the query.

P (t, t |d) = P (t|t , d)P (t |d)

(2)

In Equation 2, P (t |d) can be estimated by maximum likelihood with the help of the standard term sampling method as shown in Equation 1. For the other part, i.e. transforming t to t, we make use of the cosine similarities between the two embedded vectors corresponding to t and t respectively. More precisely, this probability of selecting a term t, given the sampled term t , is proportional to the similarity of t with t . Note that this similarity is independent of the document d. This is shown in Equation 3, where sim(t, t ) is the cosine similarity between the vector representations of t and t and (d) is the sum of the similarity values between all term pairs occurring in document d, which being the normalization constant, can be pre-computed for each document d.

sim(t, t )

sim(t, t )

P (t|t , d) =

=

(3)

t d sim(t, t )

(d)

Consequently, we can write Equation 2 as

sim(t , t) tf (t , d)

P (t, t |d) = (d) |d|

(4)

Equation 4 favours those terms t s that are not only tend to co-occur with the query term t within d, but are also semantically related to

796

C d

1---





t

Noisy Channel

t



Figure 1: Schematics of generating a query term t in our proposed Generalized Language Model (GLM). GLM degenerates to LM when  =  = 0.

it. Thus, words that are used in similar contexts with respect to the query term t over the collection, as predicted by the vector embeddings, are more likely to contribute to the term score of t. In other words, Equation 4 takes into account how well an observed query term t contextually fits into a document d. A term contextually fits well within a document if it co-occurs with other semantically similar terms. Terms, score high by Equation 4, potentially indicate a more relevant match for the query as compared to other terms with low values for this score.
Transformation via Collection Sampling. Let the complementary event of transforming a term t , sampled from the collection instead of a particular document, to the observed query term t be denoted by P (t, t |C). This can be estimated as follows.

cf (t )

P (t, t |C) = P (t|t , C).P (t |C) = P (t|t , C). cs

(5)

Now P (t|t , C) can be estimated in a way similar to computing P (t|t , d), as shown in Equation 3. However, instead of considering all (t, t ) pairs in the vocabulary for computation, it is reasonable to restrict the computation to a small neighbourhood of terms around the query term t, say Nt because taking too large a neighbourhood may lead to noisy term associations. This is shown in Equation 6.

sim(t, t )

sim(t, t )

P (t|t , C) =

=

(6)

t Nt sim(t, t )

(Nt)

While P (t, t |d) measures the contextual fitness of a term t in a document d with respect to its neighbouring (in the vector space of embedded terms) terms t in d, P (t, t |C), on the other hand, aims to alleviate the vocabulary mismatch between documents and queries in the sense that for each term t in d it expands the document with other related terms t s. From an implementation perspective, P (t, t |d) reweights existing document terms based on their contextual fit, whereas P (t, t |C) expands the document with additional terms with appropriate weights.
Combining the Events. Finally, for putting all the events together in the LM generation model, let us assume that the probability of observing a query term t without the transformation process (as in standard LM) be . Let us denote the probability of sampling the query term t via a transformation through a term t sampled from the document d with , and let and the complementary probability of sampling t from the collection be then , as shown schematically in Figure 1. The LM term generation probability in this case can thus be written as shown in Equation 7. This is a generalized version of the standard LM, which we now henceforth refer to as generalized language model (GLM), that takes into account term relatedness with the help of the noisy channel transformation model, which in turn uses the word embeddings to derive the likelihood of term transformations. Note that the GLM degenerates to standard LM by setting  and  to zero, i.e. not using the

Table 1: Dataset Overview

TREC Qry Fields Qry Avg. qry Avg. #

disks set

Ids length rel. docs

TREC 6 title 301-350 2.48

4&5

TREC 7 TREC 8

title title

351-400 401-450

2.42 2.38

Robust title 601-700 2.88

92.22 93.48 94.56 37.20

transformation model in the term generation process.
P (t|d) = P (t|d) +  P (t, t |d)P (t )+
t d
(7)  P (t, t |C)P (t ) + (1 -  -  - )P (t|C)
t Nt
3.3 Implementation Outline
An efficient approach to get the neighbours of a given term is to store a pre-computed list of nearest neighbours in memory for every word in the vocabulary. After this step, for each document d in the collection, we iterate over term pairs (t, t ) and assign a new term-weight to the term t representing the document sampling transformation according to Equation 4. Then we iterate again over every term t in d and use the pre-computed nearest neighbours of t (Nt) to compute a score for the collection sampling transformation, as shown in Equation 6. To account for the fact that these transformation probabilities are symmetrical, we add the term t to d. Note that it is not required to add the term t in case of the document sampling transformation event because t is already present in d.
4. EVALUATION
Experimental Setup. Our experiments were conducted on the standard TREC ad hoc tasks from TREC 6, 7, 8 and the Robust track. Information about the document and the query sets is outlined in Table 1. We implemented GLM using the Lucene2 IR framework. As one of our baseline retrieval models, we used standard LM with Jelinek Mercer smoothing [6, 10], which is distributed as a part of Lucene. Additionally, we also used LM with LDA smoothing [9] as our second baseline to compare against. In contrast to [9], which reports retrieval results with LDA smoothed LM (LDA-LM) on individual document subsets (and their corresponding relevance judgements) from the TREC collection as categorized by their sources, i.e. the "LA Times" and the "Financial Times", we instead executed LDA on the whole TREC collection. The rationale for using LDA as a baseline is that analogous to our model, LDA also attempts to model term dependencies by taking into account latent variables (called the topics). This baseline was also implemented in Lucene.
Parameters. The parameter  of the LM baseline was empirically set to 0.2 (after varying it within a range of [0.1, 0.9]). This value of  for the TREC collection agrees with the observations reported in [6]. According to the findings of [9], the number of topics in LDA, i.e. K, was set to 800. As prescribed in [5], we set the LDA hyper-parameters  and  (note that these are different from the GLM parameters) to 50/K and 0.01 respectively. Obtaining effective word embedding is an integral part of the GLM. The word embeddings for the experiments reported in this section were obtained on the TREC document collection with the parameter settings as prescribed in [7], i.e., we embedded the word vector in a 200 dimensional space, and used continuous bag-of-words
2http://lucene.apache.org/core/

797

MA P

0.2288

0.1955

0.2284 0.228

0.1945

MAP

0.2276

0.1935

0.2272

=0.1

=0.2

=0.1

=0.2

=0.3

=0.4

=0.3

=0.4

0.2268

0.1925

0.1

0.2

0.3

0.4

0.1

0.2

0.3

0.4





(a) TREC-6

(b) TREC-7

MAP

0.2505

0.2495

=0.1

=0.2

0.2485

=0.3

=0.4

0.1

0.2

0.3

0.4



(c) TREC-8

MAP

0.2865 0.2855 0.2845 0.2835

=0.1 =0.3

0.1

0.2

0.3



(d) Robust

=0.2 =0.7
0.4

Figure 2: Effect of varying the GLM parameters  and  on the MAP values for the TREC query sets.

with negative sampling. The neighbourhood Nt of the GLM (see Equation 7) was set to 3, i.e., for each given term in a document, we consider adding at most 3 related terms from the collection.
Results. First, we varied the GLM parameters, namely  and  within the range [0.1, 0.4] so as to ensure that  +  +  < 1 ( being set to 0.2) for all the query sets used in our experiments. The results are shown in Figure 2. It can be seen that the optimal values of  and  depend on the query set, e.g. for the TREC 8 query set (Figure 2c, the optimal results are obtained for (, ) = (0.3, 0.2), whereas this combination does not produce the optimal results for the other query sets. It can be observed that a reasonable choice for these parameters is in the range [0.2, 0.3], which means imparting more or less uniform weights to all the term generation events, namely ,  and . In Table 2, we show the optimal results obtained with GLM for each individual query set and compare the results with the baselines, i.e. the LM and the LDA-LM. It can be observed that for each query set, GLM significantly3 outperforms the baselines. It turns out that the LDA-LM (almost) consistently outperforms the standard LM. However, the results (as measured by the percentage gains in comparison to standard LM) do not seem to be as high as reported in [9] (about 3% as compared to about 8%). We believe that the reason for this is due to the diversity in the LDA topics caused by the news articles from different sources.
From Table 2, we observe that GLM consistently and significantly outperforms both LM and LDA-LM for all the query sets. Not only does it increase the recall values in comparison to LM, but it also increases precision at top ranks by always outperforming LDA in terms of MAP. Although LDA achieves higher recall than GLM in two cases (TREC-6 and Robust), the higher recall in the case of LDA does not significantly increase the MAP, which is indicative of the fact that the precision at top ranks does not improve. For GLM however, an increase in the recall value is always associated with a significant increase in MAP as well, which indicates that precision at top ranks remains relatively stable in comparison to LDA.
5. CONCLUSIONS AND FUTURE WORK
We proposed a generalized version of the language model for IR. Our model considers two possible cases of generating a term from
3Measured by Wilcoxon statistical significance test with 95% confidence.

Table 2: Comparative performance of LM, LDA and GLM on the TREC query sets.

Metrics

Topic Set Method

MAP GMAP Recall

TREC-6

LM LDA-LM GLM

0.2148 0.2192 0.2287

0.0761 0.0790 0.0956

0.4778 0.5333 0.5020

TREC-7

LM LDA-LM GLM

0.1771 0.1631 0.1958

0.0706 0.0693 0.0867

0.4867 0.4854 0.5021

TREC-8

LM LDA-LM GLM

0.2357 0.2428 0.2503

0.1316 0.1471 0.1492

0.5895 0.5833 0.6246

Robust

LM LDA-LM GLM

0.2555 0.2623 0.2864

0.1290 0.1712 0.1656

0.7715 0.8005 0.7967

either a document or the collection and then changing it to another term after passing it through a noisy channel. The term transformation probabilities of the noisy channel, in turn, are computed by making use of the distances between the word vectors embedded in an abstract space. We argue that this model has two fold advantage, firstly it is able to estimate how well a term fits in the context of a document, and secondly it is able to decrease the vocabulary gap by adding other useful terms to a document. Empirical evaluation shows that our method significantly outperforms the standard LM and LDA-LM. Possible future work will be to investigate compositionality of terms from the vector embeddings of words.
Acknowledgement. This research is supported by SFI through the CNGL Programme (Grant No: 12/CE/I2267) in the ADAPT Centre (www.adaptcentre.ie) at Dublin City University, and by a grant under the SFI ISCA India consortium.
6. REFERENCES
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993­1022, March 2003.
[2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493­2537, Nov. 2011.
[3] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. Indexing by latent semantic analysis. JASIS, 41(6):391­407, 1990.
[4] Y. Goldberg and O. Levy. word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method. CoRR, abs/1402.3722, 2014.
[5] T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of Sciences (PNAS), 101(suppl. 1):5228­5235, 2004.
[6] D. Hiemstra. Using Language Models for Information Retrieval. PhD thesis, Center of Telematics and Information Technology, AE Enschede, 2000.
[7] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Proc. of NIPS '13, pages 3111­3119, 2013.
[8] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR, pages 275­281. ACM, 1998.
[9] X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In SIGIR '06, pages 178­185, 2006.
[10] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, Apr. 2004.

798

A Head-Weighted Gap-Sensitive Correlation Coefficient

Ning Gao
College of Information Studies/UMIACS University of Maryland, College Park
ninggao@umd.edu

Douglas W. Oard
College of Information Studies/UMIACS University of Maryland, College Park
oard@umd.edu

ABSTRACT
Information retrieval systems rank documents, and shared-task evaluations yield results that can be used to rank information retrieval systems. Comparing rankings in ways that can yield useful insights is thus an important capability. When making such comparisons, it is often useful to give greater weight to comparisons near the head of a ranked list than to what happens further down. This is the focus of the widely used AP measure. When scores are available, gap-sensitive measures give greater weight to larger differences than to smaller ones. This is the focus of the widely used Pearson correlation measure (). This paper introduces a new measure, GAP, which combines both features. System comparisons from the TREC 5 Ad Hoc track are used to illustrate the differences in emphasis achieved by AP, , and the proposed GAP.
Categories and Subject Descriptors
H.3.4 [Systems and Software]: Performance evaluation
Keywords
Evaluation Metric; Rank Correlation Coefficient
1. INTRODUCTION
In information retrieval evaluation, we often wish to compare alternative systems based on some single-valued evaluation measures. For example, we might want to know whether comparing systems using relevance judgments created by one user (to whom we have access) can be used to compare systems in ways that are predictive of what we would have seen had some other user made the judgments [9]. Alternatively, we might want to know whether we can better approximate the system comparison results we could compute with very extensive relevance judgments on a large number of topics by reducing the number of topics or by reducing the number of judgments per topic [2]. In such cases, we can formulate our research question as asking about the correlation between two ranked lists of scores, where the scores result from some evaluation metric such as F1, Expected Reciprocal Rank (ERR), Normalized
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767793.

Discounted Cumulative Gain (NDCG), or Mean Average Precision (MAP).
When making such comparisons, we focus on two desiderata. First, we prefer that those differences to be large, since small differences may not reflect any meaningful degree of impact on the user experience [3]. Second, we prefer that those differences be statistically significant, since unreliable measurements of differences could be misleading [7]. When extending our comparison from pairs to large sets of systems, as is common in shared-task evaluations such as TREC, CLEF, NTCIR and FIRE, we often care more about distinguishing systems that are very different from each other, which requires the evaluation metric to be gap-sensitive. We also care more about the comparisons among the best systems than we do about comparisons between, for example, the best and the worst systems, which requires the evaluation metric to be headweighted.
Perhaps the most widely used measure of rank correlation in information retrieval research is Kendall's  [4] in which swapping systems is penalized. Yilmaz et al. [10] introduced a headweighted variant of  that they call AP that penalizes the misranking of the best systems (i.e., those near the head of the list), and that measure is now also often reported. Buckley and Voorhees have observed, however, that when systems receive very similar scores we should care less about swaps than when those scores are very different [1]. They therefore created a gap-sensitive measure by suppressing the effect of small swaps by treating any swap within a "fuzziness value" (e.g., 5% relative to the smaller value) as not being large enough to be counted. In this paper, we propose to generalize that approach to penalize swaps in proportion to the difference in scores, so that large swaps will gave the greatest influence on the measure, but small swaps are not completely ignored. This approach thus potentially offers greater insight, without the need to commit to a specific "fuzziness" threshold. Moreover, we combine this more nuanced approach with the head-weighted design of AP to produce a new correlation measure for ranked lists of scores that is both head-weighted and gap-sensitive. We call our new measure GAP (for Gap And Position).
The remainder of the paper is organized as follows, Section 2 reviews the prior work on the use of correlation measures for system comparison. Section 3 then define GAP and establishes that it has a number of desirable properties. Section 4 complements this analytic perspective with empirical results for system scores from the TREC 5 Ad Hoc task, and then further analyses the focus of different correlation coefficient metrics through the heatmap of system pair weights. Section 5 concludes the paper with some remarks on limitations of the GAP measure that may inspire further work on this important problem.

799

2. RELATED WORK

The Pearson correlation coefficient between two items is defined as the covariance of the two items divided by the product of their standard deviations:



=

E [(X

- µX )(Y X · Y

- µY )]

(1)

where X and Y are the vectors of ranked lists; E is the expectation;  is the standard deviation; and µ is the mean [6]. Given two ranked lists of items, the Spearman correlation coefficient [11] is defined as the Pearson correlation coefficient between the ranks (i.e., with the ranks used in place of scores). The most widely used measure in information retrieval research is neither Pearson nor Spearman correlation, however, but rather Kendall's  [4]. Kendall's  evaluates the correlation of two lists of items by counting their concordant and discordant pairs.
To fulfill the specific evaluation needs for different tasks, various definitions of correlation coefficients derived from Kendall's  have been proposed. Kendall's tau rank distance [5] measures the disagreements between two ranking lists by counting the swaps that the bubble sort algorithm needed to sort one list in the same order as the other. The AP Correlation coefficient (AP), proposed by Yilmaz et al. [10], focuses and penalizes more on the errors at high rankings. Given a ground-truth list and prediction list, AP of the two lists is defined as:

 AP

=

2 (N - 1)

N
·
i=2

Ci i-1

-1

(2)

where N is the number of items in the list; Ci is the number of items above rank i in the prediction list and correctly ranked with respect to the item at rank i in the ground-truth list. For each item at rank i, AP only checks the positions of the i - 1 items with ranks above i, and calculates the proportion of the correctly ordered items with respect to the item at rank i. Finally, the value of AP is a linear combination of all ranks i in the prediction list.

3. THE TAU GAP COEFFICIENT
GAP is a non-parametric correlation coefficient, particularly sensitive to errors at high rankings and errors for item pairs with large score differences (gaps). In this section, we present the definition of GAP. Since the definition of AP and GAP are both based on swapped item pairs counting, we further explore the mathematical properties of GAP comparing with AP.

 GAP

=

(N

2 - 1)

N
·
i=2

 j<i |CG ji|  j<i |G ji|

-1

(3)

Given two ranked lists, a ground truth list and a prediction list, GAP is defined as in Equation 3, where N is the number of items in the lists; i represents the item at rank i in the prediction list; j is the item ranked higher than i in the prediction list; G ji is the gap between the item at rank j and item at rank i in ground-truth list; CG ji returns the same value as G ji when the item pair ( j, i) in the prediction list are ranked in the same order as in the ground-truth list, otherwise, return 0. For each item at rank i in the prediction list, GAP only considers the rank relation of the i - 1 item pairs ( j, i).  j<i |G ji| is the sum of all the gaps for the i - 1 item pairs, and  j<i |CG ji| is the sum of the gaps for correctly ordered pairs.

THEOREM 1. The value of GAP is always between -1 and 1.

PROOF. For each item at rank i, there will be i - 1 pairs of items ( j, i) taken into the consideration by GAP.  j<i |G ji| is the sum of the gaps of these i - 1 pairs of items.  j<i |CG ji| is the sum of the

gaps that the prediction list ranks the two items ( j, i) in the same or-

der

as

the

ground-truth

list.

Therefore,

 j<i |CG ji|  j<i |G ji|

is

always

between

0 and 1.

Normalized across all ranks i,

1 (N-1)

·

Ni=2

 j<i |CG ji|  j<i |G ji|

is

always

between

0

and

1;

2 (N-1)

· Ni=2

 j<i |CG ji|  j<i |G ji|

is

always

between

0 and 2. Then the value of GAP is always between -1 and 1.

THEOREM 2. If the gaps between the items follow the uniform distribution, then GAP is equal to AP.
PROOF. Let the uniform gap be g, then  j<i |G ji| = (i - 1) · g, and  j<i |CG ji| = Ci · g. Therefore,

 GAP

=

(N

2 - 1)

N
·
i=2

Ci · g (i - 1) · g

-1

 =

(N

2 - 1)

N
·
i=2

Ci (i - 1)

-1

=

AP

(4)

Moreover, if the errors are uniformly distributed over the all the ranks in prediction list, then GAP, AP and  are equivalent.

THEOREM 3. For two prediction lists with same number of items N and same number of errors located at the same ranks, if the errors of one list have large gaps and the errors of the other list have small gaps, then the value of GAP for the list with large error gaps GAP-Large will be smaller than the GAP for the list with small error gaps GAP-Small .
PROOF. If we define FG ji as the gap between items at rank i and rank j when the item pair ( j, i) in the prediction list are ranked in the converse order as in the ground-truth list, or otherwise 0, then GAP could be represented as:

 GAP

=

(N

2 - 1)

N
·
i=2

 j<i

 j<i |CG ji| |CG ji| +  j<i |FG ji|

-1

(5)

If two prediction lists have the same number of errors located at the same ranks, then their will have the same N, i, j and CG ji. The only difference between GAP-Large and GAP-Small is that GAP-Large has larger values for FG ji, so that the value of GAP-Large will be always smaller than GAP-Small .

THEOREM 4. For a prediction list, if the swapped item pairs always have small gaps, then the value of GAP is larger than AP; if the swapped item pairs always have large gaps, then the value of GAP is smaller than AP.
PROOF. The expectation of the difference between GAP and AP is:

 E [GAP

- AP]

=

N

2 -1

N
·
i=2

 j<i CG ji  j<i G ji

-

Ci i-1

 =

N

2 -1

N
·
i=2

(i - 1)  j<i CG ji -Ci  j<i G ji (i - 1)  j<i G ji

(6)

Let Pi be the probability that the items before item i in the prediction list are ranked as the same order as in ground-truth list, then we have

Ci = Pi · (i - 1)

(7)

800

Then

 E [GAP

- AP]

=

N

2 -1

N
·
i=2

(i - 1)( j<i CG ji - Pi  j<i G ji) (i - 1)  j<i G ji

   2 N

=

N

-1

·
i=2

1  j<i G ji ·

CG ji - Pi G ji

j<i

j<i

(8)

The overall expectation of E[GAP - AP] is a linear combination

over all ranks i  [2, N]. However, for each rank position i in the

predict list, the expectation of the difference between GAP and AP

is:

  Ei[GAP

- AP]

=

2  j<i G ji

·

CG ji - Pi G ji

j<i

j<i

(9)

Since  j<i G ji is always positive, the relation between GAP and AP at rank i depends on ( j<i CG ji - Pi  j<i G ji). For rank i, there are (i - 1) pairs of items and their corresponding gaps in the prediction list taken into consideration, and there are Pi(i - 1) pairs ordered correctly.  j<i CG ji is the sum of the gaps for these correct ordered pairs, and Pi  j<i G ji is a definite proportion of the total gaps of the (i - 1) pairs. If the Pi(i - 1) correctly ordered pairs with large gaps, then  j<i CG ji > Pi  j<i G ji, and Ei[GAP - AP] > 0; if the Pi(i - 1) correctly ordered pairs focus on the pairs with small gaps, then Ei[GAP - AP] < 0; ideally, if the Pi(i - 1) correctly ordered pairs are distributed randomly across all levels of gaps, then Ei[GAP - AP] = 0. Since the expectation of E[GAP - AP] is a linear combination over all Ei[GAP - AP], we could conclude that if the error gaps of a prediction list are relatively small, then GAP is larger than AP; if the error gaps of a prediction list are relatively large, then GAP is smaller than AP.

4. COMPARISON OF THE METRICS
In this section, we use 61 participating systems from TREC 5, ranked by MAP, as a case study to compare the different emphases of the correlation coefficients , AP and our proposed GAP.
4.1 Correlation Scores for Prediction Lists
Figure 1 shows the result of a two-sided paired t-test for each pair of systems based on Average Precision (AP) scores for each of the 50 topics as samples, with both axes sorted in the same order. In that figure, system pairs with p < 0.05 are plotted as white (37%) and system pairs with p  0.05 are plotted as black (63%). This illustrates clearly that swaps among many of the systems (near the main diagonal, where score differences are smallest) would offer little insight into whether one evaluation framework yielded system comparisons that we meaningfully different from another.
Figures 2 and 3 then each illustrate two ways of making ranked lists to compare. Each dot represents a system, with the true (TREC5) rank of that system plotted on the X-axis, and a randomly permuted rank on the Y-axis. We produce the random permutations by randomly selecting five system pairs each time and swapping them. In Figure 2, we randomly select systems pairs that are statistically significantly different from each other (i.e., five of what were black dots in Figure 1). We do these five random draws 50 times (Figure 2 shows only one of the 50 times). The correlation coefficient metrics that result are  = 0.70; AP = 0.68; GAP = 0.65, averaging over 50 such random draws of five pairs to swap. In Figure 3, we randomly select system pairs from among the set of pairs that are not statistically significantly different from each other (i.e., five of what were white dots in Figure 1). We do this 50 times. The correlation coefficient metrics that result are  = 0.99;

AP = 0.92; GAP = 0.97, averaging over 50 such random draws of pairs to swap. In general, we can see that swapping system pairs with large gaps yields lower correlation scores with any measure than swapping system pairs with small gaps. But the key observation to make is that, as proven in theorem 4, when the swapped system pairs have relative large gaps, GAP = 0.65 is lower than AP = 0.68; whereas when the swapped system pairs have relatively small gaps, GAP = 0.97 is larger than AP = 0.92.
4.2 Weights for System Pairs
Figures 4, 5 and 6 show the weight for each system pair in evaluating the correlation coefficient of two ranked lists under the measurement of , AP and GAP respectively. The systems are also ranked by their ground truth MAP scores. So cell (1, 61) represents the weight of only swapping the systems at rank 1 and rank 61. In detail, taking GAP in Figure 6 as example, the value of cell (1, 61) is calculated by: (1) produce the prediction list by only swapping the systems at rank 1 and 61 in ground truth list; (2) calculate the value of GAP between the ground truth list and the prediction list; (3) fill the value of (1 - GAP) to cell (1, 61); (4) after filling in all the cells, rescale the matrix to (0, 1) and build the heatmap. Therefore, in general, darker cells represent system pairs with higher weight in evaluating the correlation coefficient.
Figure 4 shows the heatmap of system pair weights by using . Since  is only sensitive to the score difference, not the ranks of items, we can observe that the value of  is dominated by the swapping pairs with large differences, probably composed of a system at very high rank and a system at very low rank. The system pairs along the diagonal with non-significant difference get lower weights as expected. However, the swap of top ranked systems also get lower weights because of their relatively small difference. The heatmap of AP in figure 5 shows a progressively decreasing from the top-left corner of (1, 1) to the bottom-right corner of (61, 61) due to its sensitivity over ranks. However, comparing Figure 5 with Figure 1, we can see that the 37% non-significant system pairs still get non-ignorable weights in calculating AP. Figure 6 shows the heatmap using GAP. Since GAP is sensitive to both top ranked systems and the gap between system pairs, we can also observe a decreasing of weights for the system pairs from top-left corner to the bottom-right corner. At the same time, we can see a expansion of the lower weight area (bright-ish area) along the diagonal line comparing with the heatmap of AP. This is due to GAP's sensitivity to the system pairs with small difference. Overall, the value of GAP is dominated by the top ranked system pairs with large difference.
5. CONCLUSION
In this paper, we proposed a new metric GAP for evaluating the correlation coefficient between two ranked lists of scores. We have shown that GAP is sensitive to both top ranked items and to swapped item pairs that exhibit larger differences. Through analysis, we have shown that GAP compares favorably with both  and AP, and using the TREC 5 Ad Hoc track as a case study we have illustrated that the swaps that GAP is most sensitive to the ones we have argued we should care the most about. Although we have introduced GAP in the context of system comparison, it is of course a general correlation coefficient for ranked lists of scores that could be applied in any case in which both a head-weighted and gap-sensitive measure would be useful. There are, however, some characteristics of GAP that might be further improved upon. For example, it might be useful to completely discount differences in scores that are not statistically reliable indicators of real differences in system behavior. As Figure 1 illustrates, such cases can

801

Ground Truth Rank

1

30

60 60

30 Ground Truth Rank

Prediction Rank

1 20 40 60



Prediction Rank

1 20 40 60



1

1

20

40

60

1

20

40

60

Ground Truth Rank

Ground Truth Rank

Figure 1: Significance of the MAP differ- Figure 2: Significant difference between Figure 3: No significant difference be-

ence on TREC 5.

system pairs.

tween system pairs.

1

1

1

Ground Truth Rank

Ground Truth Rank

Ground Truth Rank

30

30

30

60 1

30

60

Ground Truth Rank

Figure 4: Gap-Sensitive: 

60 1

30

60

Ground Truth Rank

Figure 5: Head-Weighted: AP

60 1

30

60

Ground Truth Rank

Figure 6: The proposed GAP

be common. As another example, GAP implicitly presumes that the scores are represented on a meaningful interval scale, meaning that (for example) a user would prefer a difference of 0.2 twice as much as they would prefer a difference of 0.1. User studies have shown that some current evaluation measures do not exhibit anywhere near this degree of correlation to extrinsic measures of success such as task completion rates [8]. Future extensions that more closely model extrinsic measures of satisfaction or success might therefore be useful. Nonetheless, we see the progression from  to AP and now to GAP to be a useful one, and one that can perhaps serve as a basis for future extensions of these and other kinds.
6. ACKNOWLEDGEMENT
This work has been supported in part by NSF award 1065250. Opinions, findings, conclusions and recommendations are those of the authors and may not reflect NSF views.
References
[1] C. Buckley and E. M. Voorhees. Evaluating evaluation measure stability. In SIGIR, pages 33­40, 2000.
[2] N. Gao et al. Reducing reliance on relevance judgments for system comparison by using expectation-maximization. In ECIR, pages 1­12. 2014.

[3] K. S. Jones. Automatic indexing. Journal of Documentation, 30(4):393­432, 1974.
[4] M. G. Kendall. A new measure of rank correlation. Biometrika, pages 81­93, 1938.
[5] M. G. Kendall. Rank correlation methods. Griffin, 1948.
[6] K. Pearson. Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58(347-352):240­242, 1895.
[7] M. Smucker et al. A comparison of statistical significance tests for information retrieval evaluation. In CIKM, 2007.
[8] A. Turpin and W. R. Hersh. Why batch and user evaluations do not give the same results. In SIGIR, pages 225­231, 2001.
[9] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Inf. Process. Manage., 36(5):697­716, 2000.
[10] E. Yilmaz et al. A new rank correlation coefficient for information retrieval. In SIGIR, pages 587­594, 2008.
[11] G. U. Yule. An introduction to the theory of statistics. C. Griffin, limited, 1919.

802

On the Reusability of Open Test Collections

Seyyed Hadi Hashemi1 Charles L.A. Clarke2 Adriel Dean-Hall2 Jaap Kamps1 Julia Kiseleva3
1University of Amsterdam, Amsterdam, The Netherlands 2University of Waterloo, Waterloo, Canada
3Eindhoven University of Technology, Eindhoven, The Netherlands

ABSTRACT
Creating test collections for modern search tasks is increasingly more challenging due to the growing scale and dynamic nature of content, and need for richer contextualization of the statements of request. To address these issues, the TREC Contextual Suggestion Track explored an open test collection, where participants were allowed to submit any web page as a result for a personalized venue recommendation task. This prompts the question on the reusability of the resulting test collection: How does the open nature affect the pooling process? Can participants reliably evaluate variant runs with the resulting qrels? Can other teams evaluate new runs reliably? In short, does the set of pooled and judged documents effectively produce a post hoc test collection? Our main findings are the following: First, while there is a strongly significant rank correlation, the effect of pooling is notable and results in underestimation of performance, implying the evaluation of non-pooled systems should be done with great care. Second, we extensively analyze impacts of open corpus on the fraction of judged documents, explaining how low recall affects the reusability, and how the personalization and low pooling depth aggravate that problem. Third, we outline a potential solution by deriving a fixed corpus from open web submissions.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-- Query formulation, Search process, Selection process
General Terms: Algorithms, Measurement, Experimentation
1. INTRODUCTION
Controlled test collections remain crucial for evaluation and tuning of retrieval systems, both for offline testing in industry and for public benchmarks in academia. The TREC Contextual Suggestion Track experimented with an open test collection, where participants were allowed to submit any web page result for a personalized venue recommendation task. This option proved exceedingly popular amongst participants of the track, e.g., in 2014 the track received 25
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767788.

open web submissions against 6 runs based on ClueWeb12. We focus here exclusively on the open web submissions, and investigate the reusability of the resulting open web test collection. There are at least three factors that may impede the reusability of the resulting test collection. First, the open nature may result in little to no overlap between the submissions, frustrating the pooling effect and limiting its evaluation power. Second, the track includes personalization of results to a specific user profile, hence a "topic" consists of the main statement of request (in this case a North American city) and a profile of the requester. Third, the resulting pooling depth over submissions per topic (i.e., a unique context and profile pair) are limited to rank 5. It is well known that low pool depth affects reusability [7]. The key factor in case of sparse judgments is the presence or absence of pooling bias [1].
In this paper, our main aim is to study the question: How reusable are open test collections? Specifically, we answer the following research questions:
1. How does the open nature affect the evaluation of nonpooled systems?
(a) What is the effect of leave out uniques on the score and ranking over all systems?
(b) What is the effect of leave out uniques on the score and ranking over top ranked systems?
2. How does the open nature affect the fraction of judged documents?
(a) What is the fraction of judged documents over ranks?
(b) What is the effect of personalization on the fraction of judged documents?
2. EXPERIMENTAL DATA
The TREC Contextual Suggestion Track asks participants to submit venue recommendations (in the form of a valid URL). We give some statistics of the open web submissions in 2014. There were a total of 25 submissions by 14 teams (with 11 teams submitting 2 runs). A topic consists of a pair of both a context (a North American city) and a profile of the requester (consisting of likes and dislikes of venues in another city). For example, to recommend venues to visit in the unknown city of Buffalo, NY, based on a profile with ratings of attractions in Chicago, IL. Runs were pooled at depth 5 and in total 299 context-profile pairs were judged, with an average of 28.2 unique judged venues per pair, hence 8,441 judgments in total. Details of the run and their P@5 scores are shown later in Table 2.

827

LORO bpref

LORO MAP

LORO P@5

1

Kendall  = 0.62 ap corr = 0.44

avg diff. = 0.42

0.5

0.2

Kendall  = 0.66 ap corr = 0.58 avg diff. = 0.36

0.1

0.2

Kendall  = 0.70 ap corr = 0.52 avg diff. = 0.17

0.1

0

0

0.5

1

Actual P@5

0 0

0.1

0.2

Actual MAP

0 0

0.1

0.2

Actual bpref

Figure 1: Difference in P@5, MAP, and bpref based on the leave one run out (LORO) test.

1

Kendall  = 0.39 ap corr = 0.34

avg diff. = 0.78

0.5

0.2

Kendall  = 0.48 ap corr = 0.48 avg diff. = 0.73

0.1

0.2

Kendall  = 0.64 ap corr = 0.56 avg diff. = 0.44

0.1

LOTO bpref

LOTO MAP

LOTO P@5

0

0

0.5

1

Actual P@5

0 0

0.1

0.2

Actual MAP

0 0

0.1

0.2

Actual bpref

Figure 2: Difference in P@5, MAP, and bpref based on the leave one team out (LOTO) test.

3. IMPACT ON REUSABILITY
This section studies the reusability of the test collection, aiming to answer our first research question: How does the open nature affect the evaluation of non-pooled systems?

3.1 Leave Out Uniques Analysis

We first look at the question: What is the effect of leave

out uniques on the score and ranking over all systems? Specif-

ically, we perform both the leave-one-run-out [7] and leave-

one-team-out [1] experiments to see what would have hap-

pened if a run had not contributed to the pool of judged

documents. We also measure the effect on the runs' scores

as well as their system ranking--as the main goal of a test

collection is to determine the system ranking rather than

absolute scores. The standard system rank correlation mea-

sure

in

IR

research

is

Kendall's



(i.e.



=

N

C-D (N -1)/2

),

where

C is the number of concordant pairs, D is the number of dis-

cordant pairs, and N is the number of systems in the given

two rankings [6]. However, there are a number of researches

studied that the Kendall's  is not promising in some con-

ditions [2, 3, 6]. In order to more precisely measure the test

collection reusability, we also use AP Correlation Coefficient

n

(i.e.,

AP

=

2 N -1

·

(

C(i) i-1

)

-

1),

where

C (i)

is

the

number

i=2

of systems above rank i and correctly ranked [6].

Leave One Run Out.
In a leave-one-run-out (LORO) experiment, we exclude a pooled run's unique judgments from the test collection, and

evaluate the run based on the new test collection in terms of P@5, MAP, or bpref metrics. This test is done for all of the pooled runs--hence for each run we obtain the score as if it had not been pooled and judged. Then, the ranking correlation of the official ranking of runs with the new one is estimated. In Figure 1, reusability of the test collection is evaluated based on the mentioned metrics. The Kendall's  of this experiment based on P@5, MAP and bpref metrics are much lower than 0.9 that is the threshold usually considered as the correlation of two effectively equivalent rankings [4].
Moreover, difference of actual P@5, MAP and bpref and the ones based on LORO test is shown in Figure 1. As it is shown in this figure, average difference of MAP is 0.36 which is much higher than the ones reported for reusable test collections (e.g., from 0.5 to 2.2 [1, 5]). Figure 1 shows that bpref is a more reliable metric in comparison to (mean average) precision.
Leave One Team Out.
The LORO experiment can be biased in case teams' submit closely related runs containing many mutual venues. In reality, a non-pooled system might use completely different collection than the ones used by the pooled runs. Hence, we also conduct a leave-one-team-out (LOTO) experiment. Figure 2 demonstrates the same pattern as observed above for the LORO experiment, with somewhat lower rank correlations, and larger differences in scores. Again, bpref remains the most stable of the three measures.

828

Table 1: Reusability in top of the ranking

Metric

Depth P@5 MAP bpref 5 All

Kendall  Kendall sig Bias

0.800 0.800 0.000 1.000 0.777 1.000 0.000 0.111 0.000

Kendall  Kendall sig Bias

0.393 0.480 0.646 0.418 0.572 0.691 0.290 0.213 0.154

3.2 Top Ranked Systems
The leave out uniques experiments give a clear call to caution on the reuse of the open web judgments, but we observe in the scatter plots that the top ranked runs seem to fare slightly better. Hence, we look at the question: What is the effect of leave out uniques on the score and ranking over top ranked systems? We look both at Kendall's  and the sig, which only consider significant inversions [3]. We also look at bias, which is the fraction of all significant pairs that are significant inversions [3]. We use a paired Student's t-test with  = 0.05 is used to find significant inversions (i.e., p < ). Table 1 reports the more critical LOTO test. Over all runs, we see that sig is somewhat better than  but still low enough to be very careful with using the resulting test collection for evaluating non-pooled runs. Over the top ranked systems (based on P@5 as reported in Table 2), the bias,  and sig correlations are substantially better.
In this section we looked at the leave out uniques analysis for the open test collection in both leave run and leave team out experiments. The outcome is mixed at best, while there is a strongly significant rank correlation, the effect of pooling is notable, and results in underestimation of score and hence affects the ranking. Although we observe a somewhat more reliable evaluation of the better scoring systems, this means that the judgments should be used with caution, and evaluating non-pooled systems requires great care.

4. IMPACT ON JUDGED DOCUMENTS
This section studies in more detail the factors contributing to the observed low reusability, trying to answer our second research question: How does the open nature affect the fraction of judged documents?

4.1 Fraction of Judged Documents
We first look at the question: What is the fraction of judged documents over ranks? We define Overlap@N as the fraction of the top - N suggestions that is judged for the given set of topics:

Overlap@N (

C, P

)=

1 | C,P

|

#Judged@N ( N

c,p

),

c,p  C,P

where #Judged@N ( c, p ) corresponds to the count of judged suggestions for the given context and profile pair c, p in the top-N suggestions, and C, P is a set of judged context and profile pair. Table 2 shows the overlap@N of runs submitted to the contextual suggestion track in 2014. We see a significant drop after the pooling cut-off at rank 5, signaling that the recall base may be incomplete and the overlap between

Table 2: Overlap@N and P@5 of each pooled open web run based on the official TREC judgments

Run

Overlap@N (%)

P@5 (%)

N=5 N=10 N=25 N=50

BJUTa

100.00 61.43 32.38 20.65

BJUTb

100.00 60.26 32.10 20.23

BUPT PRIS 01 44.88 23.24 09.36 04.68

BUPT PRIS 02 47.02 25.21 10.27 05.13

cat

99.93 59.06 31.90 19.83

choqrun

97.85 57.52 31.47 16.19

dixlticmu

100.00 59.49 32.33 21.46

gw1

97.99 51.97 24.98 14.21

lda

100.00 53.57 24.73 14.31

RAMARUN2 100.00 57.99 27.78 15.53

run DwD

99.53 61.00 35.30 24.68

run FDwD

99.59 79.79 37.61 23.68

RUN1

99.93 58.56 28.58 16.00

simpleScore

100.00 58.82 28.60 16.25

simpleScoreImp 100.00 59.43 28.86 16.34

tueNet

99.86 52.64 23.90 14.33

tueRforest

99.86 52.64 23.90 14.33

UDInfoCS2014 1 100.00 57.45 28.64 17.35

UDInfoCS2014 2 100.00 59.36 31.41 19.83

uogTrBunSumF 100.00 55.75 22.38 11.20

uogTrCsLtrF

100.00 55.75 27.30 16.40

waterlooA

99.79 64.28 31.10 19.67

waterlooB

99.79 59.53 30.50 19.36

webis 1

98.59 56.75 27.50 15.16

webis 2

98.59 56.75 27.50 15.16

50.57 50.37 14.45 14.25 20.94 22.47 39.13 10.99 08.43 49.97 31.44 42.41 49.36 44.88 45.22 22.81 22.81 40.74 55.72 48.63 39.26 42.21 43.08 45.69 45.69

Average

95.33 55.93 27.62 16.48 36.06

0.6

0.6

0.4

0.4

P@5 P@5

0.2

0.2

0.1 0.2 0.3 0.4

0.1 0.2

Overlap@25

Overlap@50

Figure 3: Overlap@N versus P@5 for open web runs.

the different runs is relatively low. Clearly the lack of a fixed collection will have contributed to this.
In order to investigate the relation of the fraction of judged pages with the pooled runs' effectiveness, we plot Overlap@N vs. P@5 (i.e. the main official metric in this track) in Figure 3. Points in the graph represent pooled runs. Arguably, evaluating the best runs reliably is more important than separating the blatant failures. As it is shown in Figure 3, runs having higher P@5 usually have higher Overlap@N. This explains why for the evaluation is more reliable for the better performing runs. This figure also shows two runs that are outliers in terms of low fractions of judged documents. These two runs did usually provide fewer than 5 venues for the given topics.

829

1 0.8

Overlap@N OverlapL @N

0.6

0.4

0.2

0 0

10

20

30

40

50

N Figure 4: Effect of lenient judgments on Overlap@N.

1 0.8

Overlap@[m - n] OverlapL @[m - n]

0.6

0.4

0.2

0 1-5 6-10 11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 m-n
Figure 5: Overlap@N over rank intervals.

4.2 Impact of Personalization
We now look at the question: What is the effect of personalization on the fraction of judged documents? Specifically, we exploit the fact that contexts (i.e., cities) are judged for multiple profiles of the same (and other) submissions: in the case that the relevance of a venue to the given context is not judged for the given profile, judgments made for other profiles will be used. We define Lenient Overlap (i.e., OverlapL@N) that is an instance of Overlap@N, in which #Judged@N is calculated by ignoring profile assumption. The results are shown in Figure 4, which shows that ignoring the exact profile substantially improves the fraction of judged pages.
To highlight the number of judged pages after the pooling depth, we show the same data in an interval level analysis in Figure 5. Obviously, for pooled runs, the Overlap@5 is guaranteed to be 1, making Overlap@10 guaranteed to be at least 0.5, etc. This shows the drop in fraction of judged paged for the personalized runs in an even more dramatic way. The lenient profile-ignorant overlap measure however remains more stable over the intervals. This signals that the relatively low fractions of judged pages can be attributed for some part to the low pool depth and personalization, rather than the open nature of the test collection.
This section looked at the fraction of judged pages in the open web submissions. The outcome clearly show the low recall: after the pooling depth the fraction plummets down, explaining the relatively low reusability of the open web judgments. We looked in the relative contribution of the open nature of the collection and the personalization and pool depth, which suggested that the latter play a major role in explaining the low fraction of overlap.

5. CONCLUSIONS
We have studied reusability of the TREC 2014 Contextual Suggestion open test collection in terms of the reusability of the judgments to evaluate non-pooled runs and in terms of fraction of judged venues. We analyzed the effectiveness of the pool for building a reusable test collection. Experimental results of leave out uniques (i.e., run or a team) tests based on various metrics, including Kendall's  , AP correlation and average difference, showed that the test collection should be used with extreme care: non-pooled systems tend to be underestimated. However, for the high quality runs (i.e., top-5 of the ranking), the test collection performs somewhat better and had the highest correlation with the official ranking in terms of the  based on significant inversions. Our empirical investigation has also shown that using an open collection tends to produce a diverse pool and consequently less fraction of judged venues at ranks deeper than the pool cut-off (e.g., only 16% overlap at ranks between 6 and 10). In addition, we looked at the role of personalization and low pooling depth, and showed that the lenient profileignorant fractions of judged page leads to considerable larger fractions of judged documents.
Our general observation is that the open collection leads to significantly lower recall, and low fraction of judged results, over individual runs. There are several ways in which this could be addressed. First, it is still an open question on whether we can derive a post hoc corpus and test collection from the open web submissions, by constructing a corpus based on the combined retrieved pages, and use this to evaluate runs over the combined set. We have done an initial analysis of this approach showing promising results. Second, the organizers of the TREC 2015 contextual suggestion track aim to collect open web results as a pre-task in early 2015, and use these submissions to construct a fixed open web collection shared to all track participants. The results of this paper give support to the creation of a fixed collection of open web results, and suggest that this will substantially increase the reusability of the benchmark for non-pooled runs in follow up experiments.
Acknowledgments This research is party funded by the European Community's FP7 (project meSch, grant # 600851).
References
[1] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Information retrieval, 10(6):491­508, 2007.
[2] B. Carterette. On rank correlation and the distance between rankings. In SIGIR, pages 436­443, 2009.
[3] G. V. Cormack and T. R. Lynam. Power and bias of subset pooling strategies. In SIGIR, pages 837­838, 2007.
[4] E. M. Voorhees. Evaluation by highly relevant documents. In SIGIR, SIGIR '01, pages 74­82. ACM, 2001.
[5] E. M. Voorhees, J. Lin, and M. Efron. On run diversity in evaluation as a service. In SIGIR, pages 959­962, 2014.
[6] E. Yilmaz, J. A. Aslam, and S. Robertson. A new rank correlation coefficient for information retrieval. In SIGIR, pages 587­594, 2008.
[7] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In SIGIR, pages 307­314, 1998.

830

Towards Quantifying the Impact of Non-Uniform

Information Access in Collaborative Information Retrieval

Nyi Nyi Htun
SEBE Glasgow Caledonian University Glasgow, G4 0BA, Scotland, UK
nyinyi.htun@gcu.ac.uk

Martin Halvey
Department of CIS University of Strathclyde Glasgow, G1 1XQ, Scotland, UK
martin.halvey@strath.ac.uk

Lynne Baillie
Department of CS Heriot-Watt University Edinburgh, EH14 4AS, Scotland, UK
lynne.baillie@hw.ac.uk

ABSTRACT
The majority of research into Collaborative Information Retrieval (CIR) has assumed a uniformity of information access and visibility between collaborators. However in a number of real world scenarios, information access is not uniform between all collaborators in a team e.g. security, health etc. This can be referred to as Multi-Level Collaborative Information Retrieval (MLCIR). To the best of our knowledge, there has not yet been any systematic investigation of the effect of MLCIR on search outcomes. To address this shortcoming, in this paper, we present the results of a simulated evaluation conducted over 4 different non-uniform information access scenarios and 3 different collaborative search strategies. Results indicate that there is some tolerance to removing access to the collection and that there may not always be a negative impact on performance. We also highlight how different access scenarios and search strategies impact on search outcomes.
Categories and Subject Descriptors
H.3.3 Information Search and Retrieval
General Terms
Measurement, Performance, Experimentation.
Keywords
Collaborative search; non-uniform access; effectiveness measures
1. INTRODUCTION
Collaborative Information Retrieval (CIR) involves people with common information needs working together, exploring and collecting useful information, and collectively making decisions that help them move toward their common goal. A simple example might be of a group of colleagues collaborating for a project where they may, individually or together, go through a number of information resources and then discuss their results, exchanging information and knowledge in order to contribute to the project.
A common assumption in much of the research in CIR is that all members of a team have equal access to the information sources, tools etc., and that they may share any relevant information they find with each other without any restriction [4, 5, 11]. However, in reality it may not always be the case that all searchers have equal information access. There are numerous situations where societal, legal or security reasons may prevent a searcher from
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '15, August 09 - 13, 2015, Santiago, Chile © 2015 ACM. ISBN 978-1-4503-3621-5/15/08...$15.00 DOI: http://dx.doi.org/10.1145/2766462.2767779

sharing information within or out with a group. Handel and Wang [6] presented an example of such a scenario involving two intelligence analysts engaged in collaborative search, where one analyst is a signal intelligence specialist and the other a human intelligence specialist. Despite their unequal access to intelligence databases and underlying intelligence, as well as differing information needs and shareability, the two analysts must collaborate to achieve an outcome. This type of scenario was referred to as Multi-Level Collaborative Information Retrieval (MLCIR) [6]. Similar scenarios have been examined by other researchers who have looked at the effect of organisational structure in legal search [2], crisis management [3] and healthcare [10] to gain a better understanding of how these can impede collaboration. Others have considered how different roles within a search team might be leveraged to assist with CIR. For example, Pickens et al. [12] studied the impact of having two different roles in a collaborative exploratory search team, and looked into developing algorithms to support this. However, the main focus of these studies has been on the division of labour in CIR and although, to date, having different roles has been viewed as positive in collaborative search tasks, it might not always be. In fact, MLCIR is different from division of labour in that any system that supports MLCIR has to be aware of information flow, accessibility and shareability between collaborators [6]. Thus many of the concepts previously used to support CIR such as awareness, sense-making and persistence [4, 5, 11] may need to be revised.
Previous research [2, 3, 9, 10] has focused primarily on qualitative observations which may not be completely applicable in all nonuniform information access scenarios. To the best of our knowledge, there has yet to be a systematic evaluation on the impact of non-uniform information access within a team of searchers. We attempt to overcome this shortcoming by conducting a simulated user evaluation where we investigate the impact of two different kinds of non-uniformity in access, namely removing document access and search-term blacklisting for team members (Details are presented in Section 2.2). There are three main research questions that we attempt to answer in this paper:
1. What is the impact of non-uniform information access on the outcomes of CIR?
2. Do different types of non-uniformity have different impacts on CIR outcomes?
3. Are there scenarios where non-uniform access may be beneficial to CIR outcomes?
2. EXPERIMENTAL DESIGN
As there are a number of potential parameters for collaboration and non-uniformity in information access, we decided to use a simulated study. This approach means that we can more easily compare different variables and combinations than in a user evaluation. In future work, we anticipate exploring the findings from this study in more depth with a user evaluation.

843

2.1 Data, Topic and Search Strategies
Our evaluation followed the same procedure as Joho et al.'s simulation of collaborative search [7], with some small changes as outlined below. We utilised the TREC HARD 2005 [1] collection (AQUAINT corpus) and topics. For their study, Joho et al. [8] generated a query pool through a user evaluation for 13 of the topics. We were provided with this query pool and thus use the same 13 topics (303, 344, 363, 367, 383, 393, 397, 439, 448, 625, 651, 658 689). The query pool has a total of 1157 queries across the 13 topics and each query contains up to 9 terms.
Joho et al. [7] simulated teams of searchers (of variable size from 2 to 5) to carry out collaborative search tasks. Each team had 20 search iterations per topic. During each iteration, a team member selected a random query from the query pool and was assumed to judge 20 documents per iteration. For simplicity in our evaluation we simulate a pair of users rather than vary team size, as this would introduce extra complexity, whereby combining a multitude of possible access combinations could become intractable. In other words, we assume that there are always 2 people in a search team for any given search session and the team performs 20 search iterations. Thus each individual in a team would judge a maximum of 400 documents per topic, with a team judging a maximum of 800 documents. One of the goals of Joho et al. was to compare a number of collaborative search strategies [7]; we utilise 3 of these search strategies for our study. These 3 strategies are:
1) Independent Search (IS): team members judge documents independently without any interaction between each other, and have their results merged at the end of each search iteration.
2) Independent Relevance Feedback (IRF): same as (1) but query expansion is performed based on their independent relevance feedback and then the expanded queries are resubmitted independently to the system. Team members do not share any knowledge on relevancy of documents.
3) Shared Relevance Feedback (SRF): same as (2) but the query expansion is performed based on the relevance feedback of both members. Thus, team members share knowledge on relevancy of the documents.
For Joho et al. [7], IS was the most basic and simplest search strategy whereas the other two were the most effective. Due to its simplicity, IS is also the easiest to compare directly with any other search strategies in terms of performance, collection coverage, etc. The other two strategies chosen were the best performing in their experiments.
2.2 Access Scenarios and Combinations
We devised 4 scenarios to simulate non-uniform information access amongst team members completing a collaborative search task; these are summarised in Table 1 and outlined in detail below. For each scenario, we assumed that each of the two searchers have access to more or less of the collection relative to their search partner. For example, in one case, one searcher might be able to access only 10% of the collection while their partner can access 20% of the collection. Also, there is a possibility that one searcher cannot retrieve any documents that contain certain phrases or terms.
Therefore, starting with S1 (document removal), we began by indexing a random selection of 10% of the documents from the document collection. Then an iterative process was adopted whereby we increased the percentage of documents indexed by 10% until 100% of the collection had been indexed. This resulted

in 10 different indexes for each person and 55 possible access combinations of indexes for two people (i.e. combinations of 10%-10%, 10%-20%, 10%-30%, 10%-40%; up to 100%-100%). This simulates a scenario laid out by Handel and Wang [6] where a person with higher security clearance may have access to more documents than a subordinate.

Table 1. Information access scenarios

Code S1 S2
S3
S4

Scenario Remove access to documents from collection
Term blacklisting ­ remove access to random terms from the collection
Term blacklisting ­ remove access to terms based on their frequency in documents
Term blacklisting - remove access to terms based on their frequency in query pool

Scenarios S2, S3 and S4 simulate term blacklisting, this is a major problem highlighted by Handel and Wang [6]. For S2, we began by analysing the collection for a list of terms. After that, we indexed the entire corpus meaning there is complete access. We then created other indexes by iteratively removing 10% of the terms randomly, until only 10% remained. This also resulted in 55 possible combinations of indexes for 2 individuals. Scenarios S3 and S4 took a more systematic approach. We analysed term frequencies in both collection and query pool, which contain 841498 and 591 unique terms respectively. We then followed the same procedure as S2 but instead of removing random terms we removed terms based on their frequencies in the collection and in the query pool respectively for S3 and S4. Therefore, for S3 the first 10% removed were the most frequent terms in the collection whereas for S4 those were the most frequent terms in the query pool. In each scenario we had 10 indexes for each team member and 55 different access combinations, although the indexes in S4 are of different size to S1, S2 and S3 because in S1, S2 and S3 we can theoretically exclude everything from the collection whereas for S4 this is dependent on the query pool.

Thus for each scenario, there are 55 possible combinations; for each of these combinations, we conducted each search simulation 10 times in order to reduce randomness and inconsistencies. In total, there were 1,716,000 search sessions performed by teams in our simulation (i.e. 3 search strategies x 4 access scenarios x 10 runs x 55 combinations x 13 topics x 20 iterations). For all of the indexing and retrieval, we used the Inverted File indexing method and BM25 retrieval algorithm, these were developed using the Terrier1 library with out of the box settings.

2.3 Evaluation Measures
For the evaluation we utilised traditional IR evaluation metrics: recall, precision and f-measure in conjunction with specific metrics for CIR proposed by Shah and González-Ibáñez [13]: coverage, relevant coverage, unique coverage and unique relevant coverage. Coverage is the average number of distinct documents discovered by the team throughout the entire search session. Relevant coverage is the average number of documents in coverage that are actually relevant. Unique coverage is the average number of distinct documents that are only discovered in a given access combination, and not in any other. Unique relevant coverage is the average number of documents in unique coverage that are actually relevant.

1 http://terrier.org

844

3. RESULTS
Table 2 shows the access combinations which yield the highest values for recall, precision and f-measure across all access scenarios and search strategies and Table 3 shows those for coverage, relevant coverage, unique coverage and unique relevant coverage. As our data was not normally distributed, for each measure across 4 access scenarios and 3 search strategies, we conducted a Friedman analysis to compare the 55 access combinations (i.e. 10-10, 20-10, 20-20, 30-10, etc.) and found that there was a statistically significant difference in every case. Post hoc analysis with Wilcoxon signed-rank tests was conducted with a Bonferroni correction applied, resulting in a significance level set at p<0.00003367. We present more detailed results of the pairwise comparisons in the following sub-sections. For reasons of space as there were many comparisons we do not present all of these comparisons.
3.1 Search Performance
Our first research question examined the impact of non-uniform information access on the outcomes of CIR. First of all, statistical analysis of recall, precision and f-measure values showed a number of access combinations that were not significantly different from the best performing access combinations. However, what was interesting among these is that for S1, S2 and S4, relevance feedback search strategies had a very high number of combinations that are not significantly different from their best performing access combinations (ranging from 50-20 to 90-60 for S1, 70-70 to 90-80 for S2, and 70-70 to 100-80 for S4) whereas the IS strategy had only a few (90-80, 90-90, 100-80, 100-100 for S1; 90-90, 100-10, 100-60 100-90 for S2; 90-90, 100-90 for S4). It suggests that in terms of recall, precision and f-measure non-uniform access for S1, S2 and S4 had very little effect when relevance feedback strategies were employed.

Table 2. Highest recall, precision and f-measure values with their respective access combinations. * indicates those values
at full access (i.e. 100-100)

Recall

Precision

Independent Search

S1

0.0859 (100-90) 0.0829*

0.2459 (100-90) 0.23898*

S2

0.0813 (100-100)

0.2349 (100-100)

S3

0.0818 (100-100)

0.2446 (100-20) 0.2353*

S4

0.0830 (100-100)

0.2389 (100-100)

Independent Relevance Feedback

S1

0.1210 (90-90) 0.0383*

0.3576 (90-90) 0.1302*

S2

0.1110 (90-90) 0.0376*

0.3273 (90-90) 0.1266*

S3

0.1241 (90-90) 0.0370*

0.3931 (90-90) 0.1244*

S4

0.0904 (90-90) 0.0376*

0.2711 (90-90) 0.1295*

Shared Relevance Feedback

S1

0.1001 (90-30) 0.0325*

0.3317 (80-70) 0.1756*

S2

0.0836 (90-90) 0.0324*

0.4197 (90-90) 0.1748*

S3

0.1006 (90-90) 0.0323*

0.5208 (90-90) 0.1745*

S4

0.0762 (100-90) 0.0324*

0.3570 (90-90) 0.1748*

F-measure
0.1270 (100-90) 0.1227*
0.1204 (100-100)
0.1210 (100-100)
0.1228 (100-100)
0.1802 (90-90) 0.0604*
0.1653 (90-90) 0.0588*
0.1878 (90-90) 0.0572*
0.1350 (90-90) 0.0580*
0.1502 (90-30) 0.0548*
0.1391 (90-90) 0.0554*
0.1683 (90-90) 0.0544*
0.1173 (100-90) 0.0551*

Looking at Table 2, we found that when the IS strategy was employed for S1, the values of the 3 measures (recall, precision and f-measure) were highest at non-full access (i.e. 100-90) whereas for the rest of the scenarios (S2, S3 and S4) the values reached the highest at full access. When relevance feedback strategies were employed, however, it was found that the values reached the highest at non-full access (mostly at 90-90) for all 4 scenarios (S1, S2, S3 and S4). This suggests that there is some tolerance to removing access from the collection, and while it was expected that there would be a decrease in performance when access had been reduced, there were some cases which indicate that there may not always be a negative impact on performance. In addition, as mentioned earlier, our statistical test results revealed a number of combinations that are not significantly different from the best performing access combinations, which suggests that there are certain combinations that allow search performance to be comparable to the best performing access combination regardless of the users' unequal, or equal but not full (e.g. 90-90) access to the collection. This finding addresses our third research question. Moreover, the statistical test results also showed us that depending on the type of access scenario and search strategies being utilised, the resulting combinations were different, and thus resulted in different outcomes, addressing our second research question.
3.2 Collection Coverage
In terms of coverage for the document removing scenario (S1), statistical test results showed that in all 3 search strategies, there were many access combinations which were not significantly different from the best performing access combination and also represent the case where team members had access to a very diverse amount of the collection from each other (these are 50-10, 60-10, 70-10, 80-10, 80-20, 90-10, 90-20, 100-10, 100-20, 10030). It appears that regardless of the search strategy, reducing access to documents for one member of the team means that a different member can make judgements about different parts of the collection thereby covering similar amount of documents as they would in the best performing access combinations. This finding is in contrast to term blacklisting scenarios (S2, S3 and S4) in which most combinations that are not significantly different from the best performing access combination represent the case where both team members had a higher access to the collection (e.g. 60-60, 100-80, etc.). Next, looking at coverage in Table 3, the fact that the highest values were obtained at non-full access again indicates that there may not always be a negative impact on performance when access has been reduced, addressing our third research question. In addition, statistical test results of coverage also showed that the resulting access combinations are different depending on the type of access scenario and search strategy being utilised which addresses our second research question.
In terms of relevant coverage, Table 3 indicates that when the IS strategy was utilised, the highest values were obtained at full access (100-100) for all of the term blacklisting scenarios (S2, S3 and S4). However, statistical test results also indicated that there were non-full-access combinations where relevant coverage was as high as the full access. Besides, it also showed that the resulting access combinations and their outcomes are different depending on the type of access scenario and search strategy being utilised, again addressing our second research question. With respect to unique coverage for S1, it can be seen in Table 3 that across all search strategies the access combination that has highest value is the lowest access (10-10), and this is opposite to S3 where the full access has the highest unique coverage. In addition, it is interesting to note that for all 4 scenarios (S1, S2, S3 and S4) the SRF strategy was able to obtain very high unique coverage in all access

845

combinations compared to the other two strategies. Statistical test results showed that for S2, when the IS and IRF strategies were utilised, many of the access combinations ranging from 20-10 to 100-100 showed no significant difference from the best performing access combinations (i.e. 50-40 and 10-10 respectively). A similar outcome was also found for S4, but across all 3 search strategies. Unique relevant coverage in Table 3 shows that for all scenarios (other than for S3 of the IS strategy), the highest values were not obtained at full access. However, it appears that reducing access to the collection has little or no effect in terms of unique relevant coverage as statistical test results indicated that for almost every access scenario and search strategy, none of the access combinations showed any significant difference from the best performing access combinations.
Table 3. Highest values of different CIR measures with their respective access combinations. * indicates values of those measures at full access (i.e. 100-100)

Coverage

Relevant Coverage

Independent Search

S1

365.7769 (100-10) 297.7461*

44.6461 (100-80) 42.0769*

S2

355.7153 (80-80) 296.2615*

42.1615 (100-100)

S3

304.2384 (100-90) 297.9615*

42.4769 (100-100)

S4

418.6461 (90-60) 296.1538*

42.4307 (100-100)

Independent Relevance Feedback

S1

349.2769 (100-80) 290.8846*

48.3769 (90-60) 19.4846*

S2

349.8692 (100-50) 292.4385*

47.5231 (90-80) 19.0538*

S3

326.6077 (100-90) 277.8692*

42.7538 (90-80) 18.2769*

S4

407.6769 (100-60) 281.6154*

40.3308 (90-90) 18.6*

Shared Relevance Feedback

S1

353.6153 (100-10) 244.5308*

43.5 (90-40) 17.1*

S2

361.1615 (100-40) 241.3*

41.4308 (100-90) 17.2385*

S3

304.0615 (100-90) 242.5308*

43.1615 (100-90) 17.0231*

S4

387.4077 (100-30) 249.3769*

40.1077 (100-90) 17.1308*

Unique Coverage
8.4923 (10-10) 2.0307*
14.7615 (50-40) 0.3769*
9.8846 (100-100)
4.8461 (10-10) 1.8923*
81.5231 (10-10) 12.4615*
12.5923 (10-10) 7.9692*
17.1385 (100- 100)
8.7231 (100-100)
133.3615 (10-10) 58.6692*
74.7538 (40-40) 42.777*
67.2769 (100- 100)
47.7692 (100-10) 45.7692*

Unique Rele- vant Coverage
0.0923 (80-20) 0.0461*
0.1923 (80-70) 0.0*
0.1615 (100-100)
0.1153 (100-30) 0.0538*
0.3923 (10-10) 0.0*
0.3 (90-60) 0.0*
0.2231 (90-90) 0.0846*
0.0846 (100-80) 0.0615*
0.7923 (10-10) 0.0385*
1.2462 (80-20) 0.0769*
1.2692 (100-90) 0.2769*
0.4308 (100-90) 0.3077*

4. CONCLUSION AND FUTURE WORK
While a great deal of research has focused on CIR, only a few papers have considered the impact of non-uniform information access on CIR outcomes. This paper is one of the first attempts to quantify the impact of non-uniform information access on CIR outcomes. To that end, we conducted a simulated user evaluation using established scenarios [6] and search strategies [7].
In relation to our first research question it was found that in terms of recall, precision and f-measure that non-uniform access for S1, S2 and S4 had very little impact when relevance feedback strategies were employed. In addition, it was also found that in some cases, one member of the team having a high level of access can compensate for the other team member. Besides, our results have also highlighted that there is some tolerance to removing access from the collection and that there may not always be a negative

impact on performance. This leads us into our second and third research questions. We have found that depending on the type of access scenario and search strategy, access combinations yield different outcomes. Removing access to documents and term blacklisting had different impacts in terms of coverage: for removing document access, coverage remained stable where at least one team member had high access, whereas for blacklisting both members needed high access to retain high coverage. We have also found that in some scenarios, performance is even increased due to non-uniformity. This may in part be because this ensures that parts of the collection which might otherwise be ignored due to overlap in retrieved documents are now examined. Thus, there can be some benefits to non-uniform access depending on the search task.
To address our research questions in this paper we used 3 search strategies, 4 access scenarios, 7 different measures and teams of 2 simulated users. We anticipate extending this study in various ways to be able to produce findings that greatly generalise to a number of real situations. Thus, we intend to look at more complex strategies and access scenarios, and incorporate more users within each team. Furthermore, the findings from this study will be examined further via a user evaluation. To conclude, our findings provide a better understanding on the impact of non-uniform information access amongst searchers in collaborative information retrieval, as well as a roadmap for further user studies.
5. REFERENCES
[1] Allan, J: HARD Track Overview in TREC 2005: High Accuracy Retrieval from Documents. TREC 2005, pp. 1-17
[2] Attfield, S., Blandford, A., Makri, S.: Social and interactional practices for disseminating current awareness information in an organisational setting. IPM, 46(6), (2008)
[3] Bjurling, B., Hansen, P.: Contracts for Information Sharing in Collaborative Networks. ISCRAM 2010 (Vol. 1)
[4] González-Ibáñez, R., Shah, C.: Coagmento: A system for supporting collaborative information seeking. JASIST, 48(1), 1-4 (2011)
[5] Halvey, M., Vallet, D., Hannah, D., Feng, Y., Jose, J. M.: An asynchronous collaborative search system for online video search. IPM, 46(6), 733-748 (2010)
[6] Handel, M. J., Wang, E. Y.: I can't tell you what i found: problems in multi-level collaborative information retrieval. 3rd international workshop on Collaborative information retrieval, pp. 1-6. ACM CIKM 2011
[7] Joho, H., Hannah, D., Jose, J. M.: Revisiting IR techniques for collaborative search strategies. ECIR 2009, pp. 66-77
[8] Joho, H., Hannah, D., Jose, J. M.: Comparing collaborative and independent search in a recall-oriented task. ACM IIiX, 2008 pp. 89-96
[9] Karunakaran, A., Reddy, M.: Barriers to collaborative information seeking in organizations. JASIST, 49(1), (2012)
[10] Karunakaran, A., Reddy, M.: The Role of Narratives in Collaborative Information Seeking. ACM SIGGROUP 2012, pp. 273­276
[11] Morris, M. R., Horvitz, E.: SearchTogether: an interface for collaborative web search. ACM UIST 2007, pp. 3-12
[12] Pickens, J., Golovchinsky, G., Shah, C., Qvarfordt, P., Back, M.: Algorithmic mediation for collaborative exploratory search. ACM SIGIR 2008, pp. 315-322
[13] Shah, C., González-Ibáñez, R.: Evaluating the synergic effect of collaboration in information seeking. ACM SIGIR 2011, pp. 913-922

846

Features of Disagreement Between Retrieval Effectiveness Measures

Timothy Jones

Paul Thomas

RMIT University

CSIRO

timothy.jones@rmit.edu.au paul.thomas@csiro.au

Falk Scholer

Mark Sanderson

RMIT University

RMIT University

falk.scholer@rmit.edu.au mark.sanderson@rmit.edu.au

ABSTRACT
Many IR effectiveness measures are motivated from intuition, theory, or user studies. In general, most effectiveness measures are well correlated with each other. But, what about where they don't correlate? Which rankings cause measures to disagree? Are these rankings predictable for particular pairs of measures? In this work, we examine how and where metrics disagree, and identify differences that should be considered when selecting metrics for use in evaluating retrieval systems.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and software--performance evaluation
Keywords
IR evaluation; effectiveness measures; binary relevance
1. INTRODUCTION
Since information retrieval systems were first evaluated, system effectiveness measures have been a topic of discussion, study and controversy. When a new evaluation metric is introduced­or existing measures are criticised or praised­the discussion is usually motivated either by theoretical concerns with earlier metrics [5, 8, 9], or by user studies [1]. Sometimes, both approaches are combined [4].
Most IR effectiveness measures assume that the quality of results returned by a search engine can be calculated as a function of the gain vector inferred by a ranked list­ sometimes also including additional knowledge such as the total number of relevant documents available [2]. In this paper we use:
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '15, August 09­13, 2015, Santiago, Chile Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 . . . $15.00. http://dx.doi.org/10.1145/2766462.2767824.

· k as the depth to which a particular effectiveness metric has been evaluated.
· R as the total number of relevant documents available.
In the case of binary relevance (where documents are assumed to either be relevant, or not), the gain vector can be represented as a bit string. In simple cases, it is easy to examine these bitstrings by eye and reason about which list should be preferred. For example, where k = 3, {1, 0, 0} is almost always better than {0, 0, 1}.
However, some cases become more contentious. Consider two vectors with k = 10 and R = 10:
A = {1100010000} B = {1000101100}
If asked which list is better, most researchers would say "it depends on the task". However, it's not always immediately clear whether particular effectiveness metrics would agree on which list is better. In this particular example, AP prefers ranking A over B, while DCG prefers ranking B over A. Put another way, it's not always clear which metric prefers which type of task, or whether particular metrics are consistent in their preferences. In this work, we perform an exhaustive search of the possible binary relevance vectors where k = 10, and investigate where the disagreement between metrics lies.
2. RELATED WORK
It's worth noting that many metric descriptions don't completely specify the details of implementation. For example, an implementation of DCG requires a selection of both a gain and a discount function. Similarly, an implementation of RBP requires a selection of the p parameter [9]. Kanoulas and Aslam examine several different possible choices for the gain and discount functions in NDCG [6]. They used a Generalisability Theory approach to find choices that produced a stable ranking of systems. In this environment, they show that the optimal discount function is less steep than previously thought, and also that the optimal gain function gives nearly equal weight to relevant and highly relevant documents.
Although producing a single score for a retrieved list is convenient, a single score is not a particularly effective way of capturing system performance [7]. One illustration of this is

847

Metric
DCG SP SN-DCG SN-AP Precision NDCG SDCG RR/ERR Recall AP RBP

Bounced
No No Yes Yes Yes Yes Yes Yes Yes Yes Yes

Monoton.
Yes Yes No No No No No Yes Yes Yes Yes

Converg.
Yes Yes No No Yes Yes Yes No Yes Yes Yes

Top-wgt
Yes Yes Yes Yes No Yes Yes No No Yes Yes

Localis.
Yes Yes Yes Yes Yes No Yes Yes No No Yes

Complete
Yes Yes No No Yes No Yes Yes No No Yes

Realis.
No No Yes Yes No Yes No Yes No No No

Table 1: Properties of effectiveness metrics used (table reproduced and slightly modified from Moffat [8]. See original paper for footnotes and full discussion). All metrics are assumed to be an @k version.

the idea of 1-equivalence [10], which is the set of binary result vectors that receive the same score as a single document. The authors note that many 1-equivalent sets are contentious or at least counter-intuitive.
An interesting approach to measuring the quality of an effectiveness measure is to use the Maximum Entropy Method to infer the probabilities that each rank contains a relevant document, given the effectiveness score [2]. Once this probability vector is computed from the effectiveness score, a predicted precision­recall curve can be produced. The error between this and the actual precision­recall curve can be used to determine the quality of the metric. This approach asks the question "how good is the effectiveness score at inferring the original ranked list?".
Motivated by the wide variety of effectiveness metrics available, Moffat [8] introduces seven properties for describing and comparing metrics. They are:
· Boundedness: Whether the range of scores a metric can produce is bounded or not. For example, NDCG is bounded in the range 0­1, while DCG is unbounded in the upper range.
· Monotonicity: If extra documents are appended to the tail end of a ranking, the new score produced by a monotonic ranking is never less than the previous score. For example, Recall-at-k and DCG are monotonic, while AP is not.
· Convergence: If a document within the top k is swapped with a more relevant document outside the top k, then the score always increases if the metric is convergent. For example, DCG is convergent, while RR is not.
· Top-weighted: If a document within the top k is swapped with a more relevant document also within the top k but lower down the ranking, then the score always increases if the metric is top-weighted. For example, AP is top-weighted, while Precision-at-k is not.
· Localisation: If a score at depth k can be produced using only the information about the documents down to depth k, then a metric is localised. For example, RR is localised, while NDCG and AP both require additional information about the relevant documents that did not make it in to the ranking.

· Completeness: If a score can be produced when a query returns no relevant documents, then the metric is complete. For example, DCG is complete, while NDCG and AP both produce a division by zero when there are no relevant documents.
· Realisability: If a collection has any relevant documents, then a metric is realisable if it is possible to achieve the maximum value for that metric. For example, RR is realisable, but RBP is not.
Moffat notes that it is impossible for a metric to satisfy all seven properties, as some property combinations exclude others (for example, a metric that is monotonic and convergent cannot also be realisable if k < R).
3. METRIC DISAGREEMENT
Many previous comparisons between effectiveness metrics have looked at correlation between metrics. In this work, we are interested in the specific cases where metrics do not agree. That is, the cases where we have two binary relevance vectors A and B where­for example­AP says that A is the most effective result list, while DCG says that B is the most effective result list. In this work, we only focus on rankings where k = 10, since many of the metrics above include a user model, and it is common for users to only examine the first page of results in a web setting.
3.1 Method
We generate all possible combinations for binary relevance rankings where k = 10 and R = 10. This yields a total of 1,023 possible ranked lists, containing all possible rankings of 10 documents with up to 10 relevant documents (the all 0's case is ignored). There are 1,045,506 pairs of rankings, ignoring the pairs where both rankings are identical. Each ranked list is evaluated for each metric, and for each pair of ranked lists and pair of metrics, agreement or disagreement is recorded. All metrics were computed using 64-bit floating point numbers, and equality was checked using an epsilon of 2-53.
3.2 Metrics
We use the same metrics as Moffat [8], with the exception of HIT, which would receive the same score for every ranked list described above.

848

RBP 0.95 DCG NDCG SDCG RBP 0.85 AP SP MRR ERR SNAP RBP 0.50 SNDCG

P@k/R@k RBP 0.95 DCG/NDCG/SDCG RBP 0.85 AP/SP MRR/ERR SNAP RBP 0.50

17.57

23.99 8.65

23.75 6.87 5.32

23.06 7.02 3.01 2.88

54.09 50.79 43.02 45.73 45.04

36.45 20.41 13.25 13.59 13.43 38.31

39.99 24.95 16.30 18.29 17.99 33.27
8.05

38.34 23.00 14.35 16.31 16.05 35.24
4.25 4.41

Table 2: Disagreement between metrics (%, lower is better).

Property
Bounded Monoton. Converg. Top-wgt Localis. Complete Realis.

Has-prop
25.57 25.63 22.23 21.45 21.97 22.40 22.81

No-prop
3.00 19.25 24.70 22.15 21.13 20.95 18.33

Cross-prop
16.98 20.60 24.14 25.82 25.26 24.65 24.98

Table 3: Disagreement between metrics with particular properties (%, lower is better). Bolded groups show significance.

The metrics used are DCG, SP, SN-DCG [8], SN-AP [8], Prec@k, NDCG [6], SDCG [8], RR, Recall@k, AP [3] and RBP [9]. For RBP, 3 different values of p were used: 0.5, 0.85 and 0.95. As recommended by the authors [9], the lower bound of the score was taken to be the RBP score. ERR was also implemented, but it scores identically to RR in binary relevance [4]. Similarly, where metrics are identical except for normalisation (eg DCG and NDCG), metrics completely agree. See Table 1 for the breakdown of properties of each measure. Where choices for implementation were available, we implement the metric as described by Moffat [8].
4. RESULTS
The disagreement between each pair of measures can be seen in Table 2. In some cases (such as AP vs RR), the disagreement is not surprising, but in other cases where one would expect metrics to agree, they do not­such as the high disagreement with SNDCG and most other metrics. Also of note is the high disagreement between RBP variants­ although this is not surprising, it is worth mentioning that simply changing the discount parameter dramatically affects the behaviour of the metric.
4.1 Realistic disagreement
The disagreement percentages shown in Table 2 are assuming that each possible ranking with k = 10 can be returned by some system. However, retrieval systems aren't random; the results returned depend on collection statistics and other

Metric combination
AP DCG
DCG RBP85
RBP85 RBP95
RBP50 RBP95

Recall
52.41 22.09
33.24 37.71
0.00 89.96
0.00 89.89

First-rel
4.07 71.63
62.38 11.27
75.00 1.38
70.24 0.00

First-irrel
65.54 14.34
11.27 62.38
1.38 75.00
0.00 70.24

Table 4: Features of rankings where metric pairs disagree, in percentages. Since features are sometimes tied, they do not add to 100.

features which are not independent of relevance. To investigate whether these vectors are reasonable, we produced the k = 10 bitstrings for all runs submitted to the TREC4-8 adhoc tracks, and the 2005 and 2006 TREC robust tracks. All of the 1024 possible vectors were returned by real runs submitted to the track. This validates this exhaustive method of investigation.
4.2 Properties of metrics
To investigate whether the properties of effectiveness measures affect the disagreement between metrics, each disagreement score was sorted in to one of three buckets for each property: has-prop, where both metrics have the property; no-prop, where neither metric has the property, and crossprop, where one metric has the property, and one does not. Table 3 shows the mean agreement between metrics in each bucket. Surprisingly, a single property is not enough to determine whether two metrics agree. Of note is the case of metrics which are not bounded, but as Table 1 shows, there are only two measures in the no-prop category for that property. With the exception of monotonicity, boundedness, and realisability, the has-prop category contains the lowest mean disagreement. However, a one way ANOVA test with a Tukey post-hoc analysis shows only the convergent/has-prop group and the realisable/no-prop group to be statistically significant predictors of agreement.

849

4.3 Task behaviour
To investigate whether metrics disagreed in a predictable way that might be correlated with a task goal, all disagreeing pairs of ranked lists from each pair of metrics were examined to find out which metric preferred: recall, the list with the most relevant documents (ignoring ties); first-rel, the list with the first relevant document (ignoring ties); and firstirrel, the list with the first irrelevant document (ignoring ties). Some selected results are in Table 4. Each pairing is divided by the space in the table. When AP and DCG disagree about which ranked list is better, this table shows that in 52.41% of disagreeing cases, AP will prefer the list with more relevant documents, while DCG prefers the higher recall list only 22.09% of the time. These numbers do not sum to 100%, as sometimes there is a tie in the recall between two ranked lists, and sometimes a disagreement means that one of the two metrics gives the same score to two ranked lists (as is the case with AP when comparing {1, 0, 0, 0} and {0, 1, 0, 1} when R = 10).
This approach provides an alternative way to think about metric selection. When choosing say between AP and DCG (two metrics with strong agreement), this approach allows us to drill down into the differences between the metrics. If we prefer finding a relevant document quickest, then Table 4 suggests that DCG is the metric to choose, with 71.63% of cases preferring the ranking with the highest ranked initial relevant document. Alternatively, if recall is preferred, then AP is the better metric. Note that although Table 2 shows only a 3.01% disagreement between AP and DCG, this is still over 15,000 pairs of ranked lists.
The difference between the user model of RBP and DCG can also be seen in this data­DCG prefers the list with the earliest relevant document more often than RBP. And, as the weighting factor in RBP decreases, the metric prefers earlier relevant documents. Conversely, as the weight in RBP increases, the list with higher recall is preferred.
5. CONCLUSION
In this work we have introduced a novel strategy for investigating the disagreement between effectiveness metrics­by counting and examining the pairs of hypothetical rankings where the metrics disagree with each other. We validated our strategy by demonstrating that all possible rankings of 10 binary relevant documents have appeared in search results submitted to two of the TREC tracks, and we performed an initial investigation into whether the properties of effectiveness measures can be used to predict agreement. We found that two of the properties appeared to be weak predictors of agreement between metrics. Then, we used a feature-based approach to investigate whether the disagreement between a pair of metrics could be described in terms of task features. This approach allows statements like "if I select AP over DCG, I am preferring recall over highly ranked documents" to be made, allowing fine-grained selection between metrics.
6. FUTURE WORK
As mentioned above, search engines do not produce a random result set. Although all possible rankings for k = 10 did appear in real search results during the TREC ad-hoc and robust tracks, the frequency with which each ranking appears is not uniform. It would be valuable to examine the likelihood of disagreement that each result list has. It is

possible that the gain vectors produced by systems could be used to determine how contentious they are­how sensitive evaluation would be to metric selection.
An obvious extension to Section 4.2 is to consider multiple groupings of properties. Although it seems that individual properties of effectiveness measures are not enough to predict agreement, perhaps some combination of the properties might be. In future work, we intend to include more variants of metrics, such as alternative discounts and gain functions. This may lead to discovery of further properties.
In Section 4.3, we only consider three features of ranked lists that map to task goals, but there are many more we could consider (such as longest consecutive run of relevant documents, or lowest ranked relevant document). Additionally, user preference experiments could be constructed using pairs of vectors where metrics disagree.
As noted in Section 4, many of the DCG variants completely agree on individual ranked lists, as all that changes is the normalisation. However, if multiple queries (and therefore multiple ranked lists) are used for evaluation­as in the case of TREC tracks­then different normalisation strategies may well cause further disagreement between metrics.
Finally, this work has only considered the binary relevance case. Many of these metrics behave differently when there are multiple grades of relevance. An important next step is to repeat this analysis using graded relevance.
Acknowledgements
In addition to the anonymous reviewers, we would like to thank Alistair Moffat for helpful comments and a correction.
7. REFERENCES
[1] A. Al-Maskari, M. Sanderson, and P. Clough. The relationship between IR effectiveness measures and user satisfaction. In Proc SIGIR, pages 773­774, 2007.
[2] J. A. Aslam, E. Yilmaz, and V. Pavlu. The maximum entropy method for analyzing retrieval measures. In Proc SIGIR, pages 27­34, 2005.
[3] C. Buckley and E. M. Voorhees. Evaluating evaluation measure stability. In Proc SIGIR, pages 33­40, 2000.
[4] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proc CIKM, pages 621­630, 2009.
[5] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422­446, Oct. 2002.
[6] E. Kanoulas and J. A. Aslam. Empirical justification of the gain and discount function for nDCG. In Proc. CIKM, pages 611­620, 2009.
[7] S. Mizzaro. The good, the bad, the difficult, and the easy: Something wrong with information retrieval evaluation? In Proc. ECIR, volume 4956, pages 642­646, 2008.
[8] A. Moffat. Seven numeric properties of effectiveness metrics. In AIRS, volume 8281, pages 1­12, 2013.
[9] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):2:1­2:27, Dec. 2008.
[10] P. Thomas, D. Hawking, and T. Jones. What deliberately degrading search quality tells us about discount functions. In Proc SIGIR, pages 1107­1108, 2011.

850

Evaluating Retrieval Models through Histogram Analysis

Kriste Krstovski,§ kriste@cs.umass.edu

David A. Smith dasmith@ccs.neu.edu

Michael J. Kurtz§ kurtz@cfa.harvard.edu

College of Information and Computer Sciences University of Massachusetts Amherst Amherst, MA, 01003
College of Computer and Information Science Northeastern University Boston, MA, 02115
§Harvard-Smithsonian Center for Astrophysics Cambridge, MA, 02138

ABSTRACT
We present a novel approach for efficiently evaluating the performance of retrieval models and introduce two evaluation metrics: Distributional Overlap (DO), which compares the clustering of scores of relevant and non-relevant documents, and Histogram Slope Analysis (HSA), which examines the log of the empirical distributions of relevant and non-relevant documents. Unlike rank evaluation metrics such as mean average precision (MAP) and normalized discounted cumulative gain (NDCG), DO and HSA only require calculating model scores of queries and a fixed sample of relevant and non-relevant documents rather than scoring the entire collection, even implicitly by means of an inverted index. In experimental meta-evaluations, we find that HSA achieves high correlation with MAP and NDCG on a monolingual and a cross-language document similarity task; on four ad-hoc web retrieval tasks; and on an analysis of ten TREC tasks from the past ten years. In addition, when evaluating latent Dirichlet allocation (LDA) models on document similarity tasks, HSA achieves better correlation with MAP and NCDG than perplexity, an intrinsic metric widely used with topic models. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models, Selection process Keywords: efficient evaluation; retrieval models; topic models
1 Introduction
Evaluating retrieval models with ranking metrics, such as mean average precision (MAP) and normalized discounted cumulative gain (NDCG), requires computing, in the worst
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767821.

case, the relevance score of each query against each member of a collection of n documents and then sorting the results. In practice, of course, most retrieval systems achieve vastly better performance by: exploiting inverted indices containing sufficient statistics on features such as terms and n-grams; by performing lossless or lossy pruning of posting lists; and by keeping track of only the top k documents for each query. For large collections, however, building even one index can be costly, and evaluating multiple models may require the creation of multiple indices. Reindexing can become even more common when working with continuous representations, as in image retrieval or in using topic models for text.
Before building new indices and tuning other efficiency parameters of an IR system, researchers may want some validation that a new feature, such as skip n-grams or LDA [1] topics, will positively impact effectiveness on the target task. Rescoring ranked lists generated by a baseline system provides one such check on model validity; however, new models will be most useful when they identify relevant results outside the output of the baseline system.
To alleviate these drawbacks, this paper proposes a novel approach for efficiently evaluating retrieval models by analyzing the relationship between histograms computed over the empirical distribution of relevance scores of query relevant and non-relevant documents. We analyze the performance of two metrics based on these histograms--Distributional Overlap (DO) and Histogram Slope Analysis (HSA)--in three different settings: monolingual and crosslanguage document-similarity retrieval; ad-hoc web retrieval; and a meta-evaluation of the ranked lists of ten TREC tracks from the past ten years. HSA achieves better correlation with MAP and NDCG on full collections than does MAP or NCDG on the subcollections used by HSA. In addition, when evaluating LDA models on document similarity tasks, HSA achieves better correlation with MAP and NCDG than perplexity, an intrinsic metric widely used with topic models.
2 Histogram Analysis
Our evaluation approach is based, first of all, on the simple notion that a good retrieval model should cluster relevant documents together with higher scores than non-relevant

859

documents (We discuss below the relation to other such "cluster hypotheses".). Better retrieval models, under this hypothesis, will have a relatively small overlap between the scores of relevant and non-relevant documents. We use the volume of the distributional overlap between the histograms of query relevant and non-relevant documents to define our first evaluation metric--Distributional Overlap (DO).
Computing the log ratio between the histograms of the query relevant and non-relevant documents gives us the proportion of relevant documents that we would expect to find at certain relevance values. Across different retrieval models, as the performance of the model improves, the proportion of relevant documents found at higher relevance scores should increase thus making the slope of the histogram analysis steeper going down from higher to lower values on the relevance scale. We define this slope as an evaluation metric that we call Histogram Slope Analysis (HSA).
By computing DO and HSA from the full set of relevant documents and a relatively small sample of non-relevant documents, we are able to evaluate retrieval models much more efficiently than with metrics computed over ranked lists of documents. We simply need to summarize the empirical distribution of relevant and non-relevant documents using histograms with some fixed number of bins.

3 Computing DO and HSA

For a test collection of k queries Q = q1, q2, q3, ...qk and sets of query relevant Rqi = r1i , r2i , r3i , ...rm i and non-relevant N Rqi = nr1i , r2i , r3i , ...rni documents, we evaluate retrieval model's scoring function Scoref to get VR = Scoref (qi, Rqi ) and VNR = Scoref (qi, N Rqi ). Relevance scores have range of values that vary depending on the scoring function of
the retrieval model. Different retrieval models generate rel-
evance values on different scales. In order to compare and
rank their performance, relevance values generated by each
model are normalized to a range of [0,1].
Using a set B of equally spaced bins, histograms are com-
puted over the two sets of relevance scores to give HR = [hr1, hr2, hr3, ..., hr|B|] and HNR = [hn1 r, hn2 r, hn3 r, ..., hn|Br|]. The hrb and hnb r are the counts of the number of times relevant and non-relevant scores fall within the bin centered at b: hrb = #(VR  b), hnb r = #(VNR  b). Since in typical real document collections the portion of query relevant docu-
ments is significantly smaller than non-relevant documents,
we use log scale for the frequency axis when creating the
two histograms. We further define a set of supported bins B = {b : hrb , hnb r = 0}. Distributional Overlap measures the volume of the overlap between the two histograms:

DO =

log(min(hrb , hnb r))

(1)

b B

Figure 1 illustrates an example of computing DO. In this

figure we show the joint histogram plots across two different

document similarity models on the task of finding document

translation pairs. Both similarity models are based on the

Polylingual Topic Model (PLTM) [5] configured with num-

ber of topics T=50 and 500. We detail this experimental

setup in §5.1.

For HSA, we take the log ratio of the two histograms

for the bins b  B where they are both observed Ob =

log(

HRb HN Rb

) and then fit a linear function Ob

= +b +

b

Figure 1: Computing DO to evaluate PLTM models on the task of retrieving document translation pairs. PLTM with 50 topics: acc.=94.3%; with 500 topics, acc.=99.3%.

Figure 2: Computing HSA to evaluate ad-hoc retrieval models on the TREC Web Track 2009. NDCG(BM25)=0.106, NDCG(SDM)=0.293.

using linear least squares regression. We define HSA to be the estimated slope:

HSA = ^ =

b B (b - B¯ )(Ob - O¯) b B (b - B¯ )2

(2)

Figure 2 shows an example of computing HSA across two retrieval models on the TREC Web Track 2009. In this example we compute HSA over the relevance scores of query relevant and non-relevant web pages obtained using BM25, which gave the worst NDCG, and the sequential dependence model (SDM), which achieved the highest NDCG. See §5.2 for experimental details. The first two subplots show the log histograms of the query relevant (R) and non-relevant documents (NR). The bottom two subplots show the log ratio of the empirical distributions of R and NR scores Ob along with the linear fit O^b .

4 Previous Work
In their original work, Jardine and van Rijsbergen [3] used histograms to define one of the prominent concepts in information retrieval: the cluster hypothesis, which states that "closely associated documents tend to be relevant to the same requests". The hypothesis introduced the cluster based retrieval [8] and many of its variants [2]. To measure the potential of the cluster based retrieval on a particular document collection, Jardine and van Rijsbergen [3] proposed the cluster hypothesis test. For a given collection and a set of queries the test measures the similarity between all query

860

relevant documents and between all query relevant and nonrelevant documents. Aside from the cluster hypothesis test, researchers have proposed other measures of the cluster hypothesis [7, 9]. More recently, Raiber and Kurland [6] analyzed how these measures correlate with the performance of cluster based retrieval. While different measures exist for the cluster hypothesis they have not found their use in evaluating the performance of different retrieval models.
Although developed independently, the DO metric is consistent with the cluster hypothesis. Unlike cluster hypothesis tests, which asks whether two relevant documents are similar to each other, with DO we analyze the similarity of relevance scores between query relevant and non-relevant documents. Since DO is reflecting on the cluster hypothesis one may also consider DO as an intuitive implementation of the cluster hypothesis test in the space of query relevance values. As we shall see, however, HSA is better correlated with established ranking metrics than DO.
5 Experimental Results
The purpose of developing DO and HSA was to be able to evaluate and predict the performance of retrieval models. To demonstrate this ability we compare the values of DO and HSA with existing IR metrics and evaluate their predictive power by performing linear correlation using Pearson correlation coefficient (R). Using Spearman's rank correlation coefficient () we compute the correlation between the ranked list of models' performance sorted by existing IR metrics and the ranked list obtained using DO and HSA. We demonstrate the generality of our evaluation approach across different retrieval tasks, models, and scoring functions, which we group into three experimental setups: (1) document similarity models where the scoring function computes similarity between two documents, (2) ad-hoc retrieval where the scoring functions represents the relevance score of the document given a query, and (3) a meta-evaluation of ranked lists submitted to ten TREC tracks in the past ten years. With our last experimental setup, we also demonstrate that DO and HSA can be computed using the ranks of the retrieved documents as relevance values.
5.1 Document Similarity Tasks
We first showcase DO and HSA on two document similarity tasks: prior-art patent search [10] and the cross-language IR (CLIR) task of finding document translations [4]. Both tasks use topic models to retrieve similar documents. Experiments were performed with 7 different topic configurations. More specifically, the prior-art patent search task uses LDA with number of topics set to T=50, 100, 200, 500, 1k, 2k and 5k. While on the CLIR task PLTMs were configured with T=100, 200, 300, 400, 500, 700 and 1k. On the patent retrieval task, following the experimental setup of [10], model performance was evaluated using MAP computed over 372 queries and a test collection of 70k patents. In [4], performance of PLTMs was evaluated on a test collection of 14k English-Spanish Europarl speeches based on the percentage of true document translation pairs (out of the whole test set) that the model ranked as most similar. This metric is referred to as "percentage at rank one" (P@1).
Table 1 shows the correlation coefficients computed between our evaluation metrics and MAP and P@1. Topic models are typically evaluated intrinsically using perplexity

Correlation
R 

MAP(LDA)
DO HSA Perp. -0.75 0.92 -0.84 0.64 0.93 0.89

P@1(PLTM)
DO HSA Perp. 0.00 0.71 -0.63 0.11 0.64 0.25

Table 1: Predicting document similarity model performance using DO, HSA and perplexity: Pearson (R) and Spearman's () coef. computed over MAP and P@1.

Model
LDA T=50 LDA T=100 LDA T=200 LDA T=500 LDA T=1k LDA T=2k LDA T=5k

MAP[s]
288.1 224.8 240.1 345.9 405.6 559.5 1037.4

DO[s]
28.2 33.0 47.5 99.2 176.3 333.6 816.2

HSA[s]
28.2 33.0 47.6 99.2 176.3 333.6 816.2

Perplexity[s]
11.1 27.2 53.6 143.1 3166.7 32930.0 46340.0

Table 2: Absolute computation time for MAP, DO, HSA and perplexity for evaluating patent retrieval.

on held-out data, which is considered as a good predictor of the model performance on an extrinsic task. Correlation coefficients were also computed for this metric in order to compare its predictive power with DO and HSA.
On both retrieval tasks, HSA exhibits better linear and rank correlation compared to perplexity, while the linear and rank correlation of all three metrics is higher with MAP compared to P@1. In practice this allows us to compute HSA only on the set of query relevant and non-relevant patents and predict the performance of the document similarity model without the need of processing all patents in the collection.
Table 2 shows the absolute computation times for each evaluation metric when computed on the patent search task across different LDA model configurations. Computation time between DO and HSA differs in the second step where computing the volume requires 10ms while computing the slope takes 60ms. It is evident from this table that, both DO and HSA, are the most efficient metrics to compute compared to MAP and perplexity. On the CLIR task, due to the nature of the evaluation metric, the computation time for MAP, DO and HSA, while being different for each metric, is equal across the different model configurations. While perplexity, as in the case with LDA, grows linearly with the number of topics. Computing DO and HSA on the PLTM model we achieve a relative speed improvement of 5.12 times over MAP.
5.2 Ad-Hoc Retrieval Tasks
We conducted experiments using query sets from 4 previous TREC Web Tracks (2009-2012). Experiments were performed on the ClueWeb09 Category-B with spam filtering (a threshold of 60 using the Waterloo spam scores) collection using the open source retrieval engine Galago1 with 7 different retrieval models: BM25, BM25RF, RM, SDM and three QL models with various parameter settings. Table 3 and Table 4 show the correlation coefficient values for DO and HSA computed across three IR metrics: MAP, precision at ten (P@10), and NDCG. All evaluation metrics were
1http://www.lemurproject.org/galago.php

861

Web Track 2009 2010 2011 2012

MAP 0.89 0.88 0.91 0.89

HSA P@10
0.91 0.71 0.93 0.87

NDCG 0.95 0.93 0.99 0.97

MAP 0.84 0.64 0.77 0.79

DO P@10
0.88 0.60 0.81 0.79

NDCG 0.94 0.79 0.92 0.92

Table 3: Evaluating ad-hoc retrieval models using DO and HSA: Pearson (R) coef. computed across MAP, P@10 and NDCG.

Web Track 2009 2010 2011 2012

MAP 0.82 0.93 1.00 0.71

HSA P@10
0.86 0.79 0.96 0.82

NDCG 0.86 0.96 1.00 0.75

MAP -0.86 -0.46 -0.82 -0.46

DO P@10 -0.89 -0.68 -0.79 -0.61

NDCG -0.89 -0.50 -0.82 -0.39

Table 4: Evaluating ad-hoc retrieval models using DO and HSA: Spearman's () coef. computed across MAP, P@10 and NDCG.

computed using the top 10k retrieved documents and their relevance scores.
Across all evaluation sets, HSA has a high linear and rank correlation with all three IR metrics. While DO has a high linear correlation, its rank correlation is negative, since a large overlap between the distributions of relevant and nonrelevant documents is undesirable.
5.3 TREC Ranked Lists
So far in our experiments, we have computed DO and HSA using a relatively large set of query based relevance scores (e.g. 70k patents and 10k ClueWeb documents). However, typical IR systems, across various tasks, are configured to return the top k documents, which is usually a relatively smaller percentage of all the documents in the collection. For example, across different TREC tracks, ranked lists submitted by participants typically consist of the top 1k retrieved documents. Relevance values obtained from such ranked lists are a very small subset on the values across the whole collection. To measure the correlation when DO and HSA are computed over such small sample sets of relevance values we used ranked lists submitted on ten TREC tracks from the past ten years (2004-2013). From each year's TREC we randomly picked a track and for the selected track we randomly chose 7 submitted ranked lists. Unlike previous experimental settings where we computed DO and HSA using the relevance values generated by the retrieval models, in this experimental setup we compute DO and HSA over the normalized values of the document ranks. This is due to the fact that in some instances the relevance scores in the ranked lists are not properly formatted or missing and moreover there is no information on the relevance function used. Table 5 and Table 6 show the linear and rank correlation coefficients computed across various TREC tracks. Results in these tables shown that over all ten tracks HSA has a high linear and rank correlation with MAP and NDCG.
6 Conclusion and Future Work
We presented two evaluation metrics, DO and HSA, that use a novel approach for evaluating retrieval models performance through histogram analysis. We showed that HSA

TREC Track

HSA

DO

MAP P@10 NDCG MAP P@10 NDCG

Microblog '13 0.98 0.95 0.93 -0.82 -0.80 -0.70

Medical '12 0.90 0.84 0.94 0.30 0.10 0.50

Web '11

0.75 0.86 0.79 0.80 0.51 0.90

Session '10

0.82 0.75 0.84 0.46 0.27 0.68

Chemical '09 0.85 0.95 0.82 0.66 0.84 0.65

Enterprise '08 0.93 -0.14 0.93 0.99 0.21 0.99

Million '07

0.85 0.93 0.88 0.87 0.89 0.94

Terabyte '06 0.97 0.98 1.00 0.94 0.97 0.97

Robust '05

0.75 0.60 0.74 0.82 0.83 0.91

Web '04

0.87 0.63 0.88 -0.41 -0.02 -0.36

Table 5: Evaluating TREC track submissions using DO and HSA: Pearson (R) coef. computed across MAP, P@10, and NDCG.

TREC Track

HSA

DO

MAP P@10 NDCG MAP P@10 NDCG

Microblog '13 0.82 0.79 0.86 -0.53 -0.64 -0.51

Medical '12 0.96 0.82 0.96 -0.32 -0.14 -0.32

Web '11

0.71 0.86 0.75 -0.68 -0.43 -0.71

Session '10

0.71 0.86 0.64 -0.54 -0.32 -0.75

Chemical '09 0.82 0.86 0.79 -0.96 -0.89 -0.93

Enterprise '08 0.86 -0.14 0.75 -0.86 -0.04 -0.89

Million '07

0.86 0.68 0.86 -0.89 -0.89 -0.89

Terabyte '06 0.86 0.43 0.86 -0.18 -0.29 -0.12

Robust '05

0.78 0.57 0.79 -0.67 -0.54 -0.68

Web '04

0.68 0.39 0.71 0.00 -0.07 -0.07

Table 6: Evaluating TREC track submissions using DO and HSA: Spearman's () coef. computed across MAP, P@10, and NDCG.

has a high linear and rank correlation with MAP and NDCG while being more efficient to compute. These metrics can predict the performance of document retrieval models without the need for indexing and searching entire collections. In the future, we plan to explore the relationship between the number of query relevant and non-relevant score values used to compute HSA and its effect on the correlation coefficients.
References
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 3:993­1022, 2003.
[2] W. Croft. A model of cluster searching based on classification. Information Systems, 5(3):189­195, 1980.
[3] N. Jardine and C. J. van Rijsbergen. The use of hierarchical clustering in information retrieval. Information Storage and Retrieval, 7:217­240, 1971.
[4] K. Krstovski and D. A. Smith. Online polylingual topic models for fast document translation detection. In WMT'11, pages 252­ 261, 2013.
[5] D. Mimno, H. Wallach, J. Naradowsky, D. A. Smith, and A. McCallum. Polylingual topic models. In EMNLP'09, pages 880­ 889, 2009.
[6] F. Raiber and O. Kurland. The correlation between cluster hypothesis tests and the effectiveness of cluster-based retrieval. In SIGIR '14, pages 1155­1158, 2014.
[7] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In CIKM '07, pages 623­632, 2007.
[8] C. van Rijsbergen. Automatic Information Structuring and Retrieval. PhD thesis, University of Cambridge, 1972.
[9] E. M. Voorhees. The cluster hypothesis revisited. In SIGIR '85, pages 188­196, 1985.
[10] X. Xue and W. B. Croft. Transforming patents into prior-art queries. In SIGIR '09, pages 808­809, 2009.

862

Modeling Multi-query Retrieval Tasks Using Density Matrix Transformation

Qiuchi Li1,2, Jingfei Li1, Peng Zhang1, Dawei Song 1,3
1School of Computer Science and Technology, Tianjin University, China 2Department of Electronic Engineering, Tsinghua University, China 3The Computing Department, The Open University, United Kingdom
liqiuchi2015@gmail.com, jingfl@foxmail.com, {pzhang, dwsong}@tju.edu.cn

ABSTRACT
The quantum probabilistic framework has recently been applied to Information Retrieval (IR). A representative is the Quantum Language Model (QLM), which is developed for the ad-hoc retrieval with single queries and has achieved significant improvements over traditional language models. In QLM, a density matrix, defined on the quantum probabilistic space, is estimated as a representation of user's search intention with respect to a specific query. However, QLM is unable to capture the dynamics of user's information need in query history. This limitation restricts its further application on the dynamic search tasks, e.g., session search. In this paper, we propose a Session-based Quantum Language Model (SQLM) that deals with multi-query session search task. In SQLM, a transformation model of density matrices is proposed to model the evolution of user's information need in response to the user's interaction with search engine, by incorporating features extracted from both positive feedback (clicked documents) and negative feedback (skipped documents). Extensive experiments conducted on TREC 2013 and 2014 session track data demonstrate the effectiveness of SQLM in comparison with the classic QLM.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query formulation, Relevance feedback, Retrieval Models
Keywords
Quantum Language Model, Session Search, Density Matrix Transformation
1. INTRODUCTION
Recently, various quantum theory (QT) based IR models are developed under the inspiration of the pioneering work
Corresponding Author: Dawei Song, Email: dwsong@tju.edu.cn
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767819 .

of van Rijsbergen [8], which draws a clear connection between the QT and IR. Piwowarski et al. [5] proposed that queries and documents can be modeled as density operators and subspaces respectively, but the tensor space based representation method has not led to good performance. The advent of Quantum Language Model (QLM) [7] , a representative QT-based IR model, successfully solved this issue. In QLM, both single terms and compound term dependencies are represented as projectors in a vector space, while queries and documents are represented as density matrices defining a quantum probability distribution in the space. An EMbased training method for the estimation of density matrix is then devised [7]. The advantages of QLM over traditional language models have been demonstrated from both theoretical and experimental perspectives.
Despite its success in the ad-hoc retrieval, QLM (referred to as classical QLM in the rest of the paper) is solely targeted on single ad-hoc queries. It is insufficient to capture the dynamics of users' information need in response to the user's interaction with the search engine. As a result, it is difficult for the classical QLM to be applied in more complex search tasks, such as multi-query session search.
To address this challenge, we propose to integrate user' short-term interaction information into the estimation of QLM for the current query, and correspondingly a novel Session-based QLM (SQLM) is proposed. The evolution of the user's information need within a search session is modeled by the density matrix transformation, i.e., transforming the original density matrices (for single queries) by some principled rules based on user interactions (e.g., the click and dwell time). We also put forward the concepts of positive projectors and negative projectors extracted from the positive feedback documents (clicked documents) and negative feedback documents (skipped documents), respectively, to enhance the representation ability of the QLM. Specially, a novel training algorithm for QLM with different projectors is devised. Although there exists a body of related work [3][9] for integrating users' interaction information in IR models, they did not model term dependencies in queries and documents, compared with the SQLM proposed in this paper.
2. QLM PRELIMINARIES
2.1 Quantum Probability
In the field of IR, the quantum probability is defined on a real finite space Rn [7] for simplicity (originally, defined on the infinite Hilbert space). In this paper, we use

871

the Dirac's notation to represent a unit column vector u  Rn as |u and its transpose uT as u|, respectively. An

elementary quantum event can be uniquely represented by a projector onto a 1-dimensional subspace of Rn. For a unit

vector |u , the corresponding elementary quantum event, or

the projector, is denoted as |u u|. Suppose |e1 ,|e2 ,...,|en forms an orthonormal basis for Rn, then each unit vector

|v can be uniquely expressed as the superposition of |ei : |v = i vi|ei , where i vi2=1.
A measure µ is introduced to define the quantum proba-
bility on Rn. It satisfies two conditions: (I) for every pro-

jector |v v|, µ(|v v|)  [0, 1] and (II) for any orthonormal

basis {|ui } for Rn, we have

n i=1

µ(|ui

ui|) = 1.

The

Gleason's Theorem [2] can prove the existence of a mapping

function µ(|v v|) = tr(|v v|) for any vector v given a density matrix   Sn (Sn is the density matrix space con-

taining all n-by-n positive semi-definite matrices with trace

1, i.e., tr() = 1). Formally, any density matrix  assigns a

quantum probability for each quantum event in vector space Rn , thereby uniquely determining a quantum probability

distribution over the vector space.

2.2 Classical Quantum Language Model
The classical Quantum Language Model(QLM) aims at modeling term dependencies in the principled quantum theory formulation. Different from traditional language models, QLM extracts term dependencies in each document as projectors in the quantum probabilistic space. The single words correspond to projectors |ei ei|, and the compound terms (with two or more words for each term) correspond to projectors |v v| (refer to Section 2.1). The projectors are used to estimate density matrices q and d for a query and each document by maximizing a likelihood function with the EMbased iterative approach, i.e., RR algorithm [7]. Then, the top retrieved documents in the initial search results returned by the traditional language model are re-ranked according to the negative VN-Divergence between q and d. For details of the classical QLM, please refer to [7].

3. SESSION QUANTUM LANGUAGE MODEL (SQLM)

3.1 Framework

In QLM, a single query can be represented by a density matrix  over a vector space for a certain vocabulary. The positive definite matrix with unitary trace can be decomposed as follows:

n

 = i(|ui ui|)

(1)

i=1

where |ui is a eigenvector, and i is the eigenvalue. Corre-

spondingly, i = |ui ui| can be interpreted as an elemen-

tary quantum event or projector, and i is the corresponding

quantum probability for the elementary event (

n i=1

i=1).

By obtaining a density matrix, we actually obtain a set of

mutually orthogonal quantum elementary events along with

the corresponding discrete probability distribution, and vice

versa. In a real search scenario, a user often interacts with

the search engine many times before achieving his/her actual

information need. We propose a density matrix transforma-

tion framework to model the interaction process, which is

mathematically formulated as a mapping function T in the

density matrix space Sn.

T : Sn  Sn

(2)

In session search, we assume that there exists an "ideal" transformation T for density matrices which can model dynamic query intention in the historical interactions. Specifically, T is a transformation that for any two consecutive queries qi-1 and qi, the estimated density matrix ^i= T i-1 represents the user's information need for qi, where i-1 is a representation of qi-1. This implies we further make a 1st order Markov assumption that a query is dependent solely on its last previous query. This assumption is reasonable because the dependency can continuously back-propagate in the session.
Theoretically, from Eq.(1), we can easily find that T can be divided into two separate transformation process: T = T1T2. T1 is the transformation operator for quantum events |ui . Since |ui forms an orthogonal basis, T1 can be any standard transition matrix. T2 changes the original probability distribution for the events (namely, change the values of i), and it is be a diagonal matrix. In this sense, the transformation of density matrix is basically a transformation of main quantum events, and a reallocation of quantum probability for each event.
In practice, however, this consideration seems infeasible due to its high degree of freedom. Suppose a vocabulary V with |V| distinct words, T1 will have a freedom of O(|V|2), and T2 will have a freedom of O(|V|). Thus the model is prone to be overfiting and computationally expensive. Moreover, it is hard to draw a clear and reasonable connection from the training of T1 and T2 to the extraction of projectors. Therefore, we propose an iterative training approach to represent the transformation process, inspired by the updating method of the classical QLM. Specially in this paper, we use the density matrix i-1 for query qi-1 as the initial density matrix to train the density matrix i for query qi.
To facilitate subsequent discussions, we define notations for the session search. In a search session, we have a set of historical interaction units {Qi, Ri, Ci}Ni=-11, where Qi, Ri and Ci represent the query, returned documents and clicked documents for the ith interaction unit respectively. We need to use the historical interaction information to retrieve documents for the current query QN . To this end, we first obtain the top N retrieved documents returned by the traditional language model (LM), denoted as RN . {i}Ni=-11 denote a set of |V|-order density matrices representing user's information need for each historical query, where |V| is the size of the vocabulary containing all distinct terms in the historical queries and the current query.

3.2 Modeling a Single Query
For a historical interaction of a search session, the first clicked document of the query is not always the first one in the search results list. In other words, users often skip some irrelevant results before clicking the first assumingly relevant document. Therefore, we assume that the "skip" behavior is a strong negative feedback signal of users, since the user would have otherwise clicked them. In our assumption, some extreme cases are neglected. For example, the user may gain the right information only by reading the snippets without detecting any click behaviors. Based on this point, we form a positive documents set Rpos(i) with all clicked documents as well as a negative set Rneg(i) with all skipped documents

872

in Ri for each query qi. Note that, Rpos(i) is equivalent to

parameter in [0,1], which we will further discuss in Section

Ci, and Rneg(i) is null for the queries whose first returned

4.2. The objective function (3) can therefore be updated as

document is clicked.

From

Rpos(i)

and

Rneg (i),

positive

projectors

Ppos

=

{i

}Mpos
i=1

and

negative

projectors

Pneg

=

{j

}Mneg
j=1

are

extracted

us-

ing the method discussed in Section 2.2, where Mpos and

Mpos

Mneg

LP () =

WD(i) log tr(i)

log tr(j) (8)

i=1

j=1

Mneg is the number of positive and negative projectors re-

spectively. Note that some details when extracting projec-

where D(i) is the document containing the projector i.

tors: i) only single words, bi-grams and tri-grams are con-

The new objective function is similar to Eq.5, and the only

sidered as possible compound dependencies, since otherwise

difference is that the new one multiplies each projector in

the computational complexity will be exponential to the vocabulary size; ii) we use TFIDF to assign the superposition

clicked documents by a weight WD(i). Thus the updating methods discussed for Eq.6 can still be applied to the new

weight vi rather than the IDF or UNIFORM weights intro-

objective function in Eq.8.

duced in the original paper [7], since TFIDF is a document specific measure and has better distinguishability across

3.3 Density Matrix Transformation

documents. In order to maximize the probability that all

In this paper, we do not train the quantum events trans-

positive events happen while all negative events not happen

formation operator T1 and the quantum probability change

with respect to the quantum probability distribution (i.e.,

operator T2 for density matrix transformation operator T ,

the density matrix ), the Maximum Likelihood Estimation

because of the high freedom. Instead, we propose an itera-

(MLE) problem can be formulated as

tive training algorithm to approximate the process of density

Mpos

Mneg

matrix transformation between two subsequent queries:

^ = argmax( log tr(i) log(1 - tr(j))) (3)

i=1

j=1

where i and j denote a positive projector and a negative projector. Since

1 - tr(j) = tr((I - j))

= (|V| - 1)  tr(j))

(4)

Algorithm 1 : Density Matrix Transformation.
1: 0  diag(LM ); // Initiate the density matrix 0 with the traditional unigram language model.
2: for k = 1; k  N - 1; k += 1 do 3: Extract projectors from Rkpos, Rkneg (Section 3.2); 4: Estimate k with initial density matrix k-1 with
T rainingSteps(k) = Sk-1 iterative steps;

where

j

=

I -j |V |-1

is

also

a

legal

density

matrix

and

|V| - 1

is a constant. Then Eq.(3) can be rewritten as

5: end for 6: Return the desired density matrix for interactions N-1.

Mpos

Mneg

^ = argmax( log tr(i) log tr(j))

(5)

i=1

j=1

Eq.5 is similar to the objective function in classical QLM. Thus we can apply the similar updating method used in [7] to update the density matrix . Since the RR algorithm in [7] dose not guarantee convergence, we revise it by utilizing the updating method in [4]:

~(m+1)

=

(1

-

 )^(m)

+



^(m)R((m^ ))

+ 2

R((^m))^(m)

(6)

It can be strictly proved in [4] that for a sufficiently small value of , Eq.(6) guarantees global convergence. Although this updating method guarantees the global convergence theoretically, it requires a sufficiently small value of parameter , resulting in a slow training speed. Therefore, in this paper we do not target on training the density matrix to its convergence, but control an appropriate iterative steps (will be discussed in Section 3.3).
In SQLM, we also model the dwelling time and click sequence for each clicked document. The assumption is that a longer dwelling time and an earlier click mean that the document is more likely to be relevant. Specifically, the weight for a clicked document d is calculated as

Wd

td
= e tall

 cSeqd-1

(7)

where td is the dwelling time for the document d, tall is the lasting time of the whole interaction, Seqd denotes the rank of d in the returned document list, and c is a decaying

The training steps are different for different queries, since we believe nearer queries to the current query will have stronger influence on the estimation of current query. The initial training steps S and the discount factor  are free parameters which need to be further discussed. The more steps the density matrix is trained, the closer it moves towards the current query density matrix and away from the initial matrix. Thus, gaining an appropriate training steps can achieve a balance between the current query information and historical interaction information.

3.4 Ranking

We use the top K (we set K = 50 in this paper) retrieved
documents (pseudo feedback documents) returned by traditional LM to train a pseudo feedback QLM pN for current query. The representation of user's search intention can be formulated as the linear combination of N-1 and pN :

^ = pN + (1 - )N-1

(9)

where  controls the extent to which the history influence on the query representation. After obtaining ^, it can be used to re-rank the retrieved documents following the same method in [7].

4. EMPIRICAL EVALUATION
4.1 Experimental Setup
Empirical evaluations are conducted on the TREC 2013 and 2014 session track data shown in Table 1. The corpus

873

Table 1: Statistics For TREC 2013 and 2014

Datasets (TREC 2014's official ground truth only

contains the first 100 sessions).

Items

TREC 2013

TREC 2014

#Sessions

87

100

#Queries

442

453

#Avg. session length

5.08

4.53

#Max. session length

21

11

Table 2: Performance on TREC 2013 and 2014.

TREC2013 NDCG@10 chg%

MAP

chg%

QLM

0.0763

+0.00 0.01708 +0.00

SQLM

0.0847

+11.01 0.01799 +5.32

SQLM+LM 0.0967

+26.74 0.01994 +16.74

TREC2014 NDCG@10 chg%

MAP

chg%

QLM

0.0909

+0.00

0.0164

+0.00

SQLM

0.0950

+4.51

0.0170

+3.66

SQLM+LM 0.1033

+13.64 0.0180

+9.76

used in our experiments is the ClueWeb12 full corpus1 which consists of 733,019,372 English webpages collected from the Internet. We index the corpus with Indri search engine2. In the indexing process, we filtered out all documents with Waterloo's spam scores [1] less than 70, removed the stop words and stemmed all words with Porter stemmer [6].
To verify the effectiveness of the proposed model, we compared the following models: (i) QLM, the classic quantum language model which is regarded as the baseline model; (ii) SQLM, the proposed session-based quantum language model; (iii) SQLM+LM, the combination model of SQLM and traditional language model (LM), which takes the feature of LM into consideration (the linear combination parameter is ). We employ the official evaluation metrics MAP and NDCG@10 to evaluate the models.
A number of parameters are involved in the proposed models, and they are summarized as follows: c in Eq.7, S and  in Algorithm 1,  in Eq.9, and  in model SQLM +LM . For the global setup, we select  = 0.01 in Eq.6. The selection of best parameters will be discussed in next section.
4.2 Results and Discussion
Table 2 reports the experimental results for TREC 2013 and 2014 datasets respectively. In the tables, "chg%" means the improvement percentage over the baseline, i.e., QLM.
Since the modeling process of SQLM only involves matrix addition and multiplication, the computing complexity is low, allowing us to conduct a grid search to find the best parameter configuration. For TREC 2013, the best parameter configuration is {c = 0.95, S = 10,  = 1.05,  = 0.7} for SQLM; and {c = 0.95, S = 30,  = 1.05,  = 0.6 ,  = 0.9} for SQLM+LM. For TREC 2014, the best parameter configuration is {c = 0.95, S = 30,  = 1.0,  = 0.7} for SQLM, and {c = 0.95, S = 30,  = 1.15,  = 0.9,  = 0.9} for SQLM+LM.
The results indicate that the proposed SQLM achieves improvements over the classical QLM, on both TREC 2013 (11.01% improvement for NDCG and 5.33% for MAP), and TREC 2014 Session data (4.51% relative improvements for NDCG and 3.66% for MAP). Moreover, a linear combination
1http://www.lemurproject.org/clueweb12/index.php 2http://www.lemurproject.org

of SQLM and LM can further enhance the performance of SQLM, suggesting that SQLM is adaptive to other features such as the traditional LM. It also indicates that SQLM has a large potential for further improvements.
5. CONCLUSION AND FUTURE WORK
In this paper, we present a novel quantum theory based probabilistic framework for multi-query retrieval task, i.e., session search. By extending the classical Quantum Language Model (QLM), our proposed Session-based Quantum Language Model (SQLM) incorporates the sound mechanism of the density matrix transformation to approximate the dynamics of information need entailed in historical interactions, for re-ranking the initial results generated by the search engine. At the operational level, we utilise the information from both clicked documents and top unclicked documents, and devise a new training algorithm. Extensive experiments on both TREC 2013 and 2014 Session track datasets demonstrate that SQLM does perform better than classical QLM for multi-query retrieval systems, and also show its potential of being further improved for session search.
Therefore, it is safe and reasonable to conclude that the proposed Session-based Quantum Language Model(SQLM) is a feasible expansion of classical Quantum Language Model(QLM) on the multi-query session search tasks. As for future work, we believe that a better retrieval result could be achieved if one can find a better realization of density matrix transformation based on the quantum inference, and incorporate more features into the framework. We will also apply the model to data closer to real-time retrieval systems.
6. ACKNOWLEDGMENTS
This work is supported in part by them Chinese National Program on Key Basic Research Project (973 Program, grant No.2013CB329304, 2014CB744604), the Chinese 863 Program (grant No. 2015AA015403), the Natural Science Foundation of China (grant No. 61402324, 61272265), and the Research Fund for the Doctoral Program of Higher Education of China (grant no. 20130032120044). Any comments from anonymous reviewers are appreciated.
7. REFERENCES
[1] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5):441­465, 2011.
[2] A. M. Gleason. Measures on the closed subspaces of a hilbert space. J. Math. Mech, 6(6):885­893, 1957.
[3] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In SIGIR, pages 453­462. ACM, 2013.
[4] M. G. A. Paris and J. Reh´l´ccek. Quantum State Estimation. 649, 2004.
[5] B. Piwowarski, I. Frommholz, M. Lalmas, and K. Van Rijsbergen. What can quantum theory bring to information retrieval. In CIKM, pages 59­68. ACM, 2010.
[6] M. F. Porter. An algorithm for suffix stripping. Program: electronic library and information systems, 14(3):130­137, 1980.
[7] A. Sordoni, J.-Y. Nie, and Y. Bengio. Modeling term dependencies with quantum language models for ir. In SIGIR, pages 653­662. ACM, 2013.
[8] C. J. Van Rijsbergen. The geometry of information retrieval. Cambridge University Press, 2004.
[9] S. Zhang, D. Guan, and H. Yang. Query change as relevance feedback in session search. In SIGIR, pages 821­824. ACM, 2013.

874

Using Term Location Information to Enhance Probabilistic Information Retrieval

Baiyan Liu, Xiangdong An, Jimmy Xiangji Huang
Information Retrieval and Knowledge Management Research Lab School of Information Technology
York University, Toronto, ON M3J 1P3, Canada
{baiyan, xan, jhuang}@yorku.ca

ABSTRACT
Nouns are more important than other parts of speech in information retrieval and are more often found near the beginning or the end of sentences. In this paper, we investigate the effects of rewarding terms based on their location in sentences on information retrieval. Particularly, we propose a novel Term Location (TEL) retrieval model based on BM25 to enhance probabilistic information retrieval, where a kernel-based method is used to capture term placement patterns. Experiments on f ve TREC datasets of varied size and content indicate the proposed model signif cantly outperforms the optimized BM25 and DirichletLM in MAP over all datasets with all kernel functions, and excels the optimized BM25 and DirichletLM over most of the datasets in P@5 and P@20 with different kernel functions.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Retrieval
Keywords
Term location, probabilistic information retrieval, noun
1. INTRODUCTION
English has 5 basic sentence patterns [1, 10]: (1) Subject + Verb (e.g., Joe swims); (2) Subject + Verb + Object (e.g., Joe plays the guitar); (3) Subject + Verb + Complement (e.g., Joe becomes a doctor); (4) Subject + Verb + Indirect Object + Direct Object (e.g., I give her a gift); and (5) Subject + Verb + Object + Complement (e.g., We elect him president). Most English simple sentences follow these 5 patterns and exceptions are rare (compound and complex sentences can be split into simple sentences) [10], where nouns and noun-phrases are mostly located in the beginning or the end of sentences. On the other hand, past research has indicated that nouns and noun-phrases are more information-bearing than the other parts of speech in information retrieval (IR) [6, 3, 8, 12]. Therefore, integrating term location information into IR may help improve retrieval performances.
Two illustrative experiments are conducted to verify the hypothesis. The f rst illustrative experiment on WT2g dataset (247,491 web documents, TREC'99 Web track) indicates nouns do concen-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specif c permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767827.

trate on both ends of sentences as shown in Table 1, where AvgDis is the average of the normalized distances of a set T of nouns from the middle of their sentences as def ned by Eq. 1. In Eq. 1, |T | means the cardinality of set T (In this paper, except as explicitly

noted, |x| means absolute value of x).

AvgDis =

tT avg(|M id(t) - P os(t)|/M id(t)) |T |

(1)

where M id(t) is the middle position of the sentence that noun t is in, P os(t) is the position of t, and T is the set of nouns. Since a term may appear in a document more than once, average function avg(.) is used. AvgDis has a range of [0, 1], and is closer to 0 if all nouns are gathered in the middle of sentences.
Table 1 shows that AvgDis > 0.5 on both halves of the sentences, which means that the nouns are nearer to the beginning or the end of sentences than to the middle of sentences.

End Left Right

Table 1: Noun placement in sentences

AvgDis # Nouns Avg. sent. len. # Sentences

0.5901 24,918,926 0.613 26,286,542

10.5619

14,360,676

Table 2: Relevant noun placement in sentences

Term

Score Term Score

louisa

43 head

-24

gradgrind 27 ladi

-17

tom

23 hand

-16

bounderbi 22 countri -16

slackbridg 15 time

-16

The second illustrative experiment on Hard Times by Charles
Dickens [2] illustrates that relevant terms are more likely located in
the beginning or the end of than in the middle of sentences as shown in Table 2, where Score > 0 if a term is more often found near the beginning or the end of sentences, and Score < 0 otherwise. To obtain Score of a term t in a document D, sentences in D are each partitioned into three parts, {p1 p2 p3}, where |p1| = |p3| and |p2| = |p1| + |p3|, and a score to t for its each occurrence in D is
assigned by Eq. 2:

Score(t) =

1 -1

if t  p1  p3 if t  p2

(2)

Then Score of t for all of its occurrences in D is given by Eq.

3:

Score(t, D) = Score(ti)

(3)

ti D

where ti is the ith occurrence of t in D. Table 2 shows that the highest scoring terms "louisa", "grad-
grind", "bounderbi", "tom", and "slackbridg" turn out to be the

883

main or minor characters in the book, whereas the lowest scoring terms are not particularly related with the novel.
The results from the two illustrative experiments above indicate the hypothesis deserves a deeper investigation. The main contributions of this paper are as follows:
· We extend BM25 naturally with term location information to enhance probabilistic information retrieveal;
· In order to reward terms that are more likely to be nouns, we propose a novel kernel-based Term Location (TEL) retrieval model to capture term placement patterns;
· Experiments on f ve TREC datasets of varied size and content indicate the proposed model is highly promising.

2. RELATED WORK

Jing and Croft [6] proposed PhraseFinder to automatically construct collection-dependent association thesauri, and then used the association thesauri to assist query expansion and found that nouns and noun-phrases were most effective in improving IR. Liu et al. [8] classif ed noun-phrases into four types ­ proper names, dictionary phrases, simple phrases, and complex phrases ­ and ranked documents based on phrase similarity. Zheng et al [17] used nounphrases and semantic relationships to represent documents in order to assist document clustering, where noun-phrases were extracted with the assistance of WordNet. Yang et al [14] used a parse tree to transform the sentences in legal agreements into subject-verbobject (SVO) representations, which are then used in conjunction with cue terms to identify the provisions provided by the sentences. However, they found that provision extraction using the SVO representation resulted in high precision but low recall, which could be due to the specif city of SVO sentence patterns and the diff culty in parsing complex sentences. Hung et al. [4] used syntactic pattern matching to extract syntactically complete sentences that express event-based commonsense knowledge from web documents, and then semantic role labeling was used to tag the semantic roles of the terms in the sentences, such as the subject and the object. Ibekwe-SanJuan et al. [5] built f nite state automaton with syntactic patterns and synonyms from WordNet, which was used to tag the sentences in scientif c documents according to its category. It is diff cult to f nd syntactic patterns that are effective in all the documents of a single corpus, and rule-based part-of-speech taggers are less effective in unseen text [7]. Terms were rewarded based on their locations in a document in [15, 16].

3. OUR APPROACH 3.1 Term Location

We assume that the most important terms in the documents are
near the beginning or the end of the sentences. We determine the importance (relevancy) of a term t by examining its distance from the middle of its sentence in document D as def ned by Eq. 4:

q(t, D) = |M id(t, D) - P os(t, D)|

(4)

where M id(t, D) = (SL(t, D) - 1)/2,
SL(t, D) is the length of the sentence in D that contains t, and P os(t, D) is the location of t in the sentence. We use the average distance of t from the middle of its sentences in D if t appears more than once in D, which is def ned as r(t, D) by Eq. 5:

r(t, D) = ( q(ti, D))/tf (t, D)

(5)

ti D

where ti is the ith occurrence of t in D and tf (t, D) is the term frequency of t in D. We def ne m(t, D) to be the average length of the sentence(s) that contain t in D as Eq. 6:

m(t, D) =

tiD SL(ti, D)   tf (t, D)

+



(6)

where parameter  has a larger effect for longer sentences since it is proportional to the lengths of the sentences, and parameter  has a proportionally smaller effect for longer sentences since it is the same for all sentences. These two parameters are used to balance term weights in particularly short or long sentences.

3.2 Kernel Functions

In order to measure the distances of the terms from the middle

of their sentences, we f t a kernel function over each sentence. We

adjust the weight of each term based on its average distance to the

middle of its sentences. In this paper, we explore the following

kernel functions for our location based reward function RN (t, D)

used in Eq. 9:

r2

Gaussian - Kernel(r, m) = 1 - e -2m2

Triangle

-

Kernel(r, m)

=

r m

Cosine

-

Kernel(r, m)

=

1

-

1

+

cos 2

r m

Circle - Kernel(r, m) = 1 -

1-

r2 m

Quartic - Kernel(r, m) = 1 -

1-

r2 m

2

Epanechnikov - Kernel(r, m) =

r2 m

Triweight - Kernel(r, m) = 1 -

1-

r 23 m

Among them, Gaussian kernel is widely used in statistics and machine learning such as Support Vector Machines, Triangle kernel, Circle Kernel, and Cosine Kernel are applied to estimate the proximitybased density distribution for the positional language model [9]. Since the kernel functions are not maturely applied in IR, we also explore Quartic kernel, Epanechnikov kernel and Triweight kernel in this work. In these kernel functions, r and m are def ned by Eqs. 5 and 6, respectively. With these kernel functions, the number of terms that are given maximum reward decreases as m(t, D) increases.

3.3 Integration into BM25

In this experiment, we use the BM25 weighting model as our base weighting model. BM25 is def ned as follows:

Score(t, D) = T F (t, D)  IDF (t)

(7)

where

T F (t, D)

=

(k3

+ 1) (k3

 +

tf (t, D)  qtf qtf (t))  K

(t)

,

IDF

(t)

=

log2

N

- n(t) n(t) +

+ 0.5 0.5

,

K = k1 

1

-

b

+

b  |D| AvgDL

+ tf (t, D),

k1, k3, and b are tuning parameters for BM25, qtf (t) is the frequency of t in the query, |D| is the number of terms in D, AvgDL is the average length of the documents in the collection, N is the number of documents in the collection, and n(t) is the number of documents in the collection that contain t. We modify T F (t, D) to
account for the reward given to the terms based on their locations in
the sentences. We def ne the Term Location score (TL) as follows:

T L(t, D)

=

(k3

+

1)

 RN(t, D)  tf (t, D) (k3 + qtf (t))  KT L



qtf (t)

(8)

884

Performance improvement (%)

MAP improvements of TEL over best BM25 with different kernel functions 3.5

3

2.5

2 1.5
1 0.5
0 WT2G

DISK4&5

Gaussian MAP Triangle MAP Circle MAP Cosine MAP Quartic MAP Epanechnikov MAP Triweight MAP

WT10G Dataset

BLOGS06

GOV2

Performance improvement (%)

P@5 improvements of TEL over best BM25 with different kernel functions 7

6

Gaussian P@5

Triangle P@5

5

Circle P@5

Cosine P@5

4

Quartic P@5

Epanechnikov P@5

3

Triweight P@5

2

1

0

-1

-2 WT2G

DISK4&5

WT10G Dataset

BLOGS06

GOV2

Performance improvement (%)

P@20 improvements of TEL over best BM25 with different kernel functions 2.5

Gaussian P@20 2
Triangle P@20

Circle P@20

1.5

Cosine P@20

Quartic P@20

1

Epanechnikov P@20

Triweight P@20

0.5

0

-0.5

-1 WT2G

DISK4&5

WT10G Dataset

BLOGS06

GOV2

Figure 1: Performance improvements of TEL over best BM25 with different kernel functions.

Performance improvement (%)

MAP improvements of TEL over best DirichletLM with different kernel functions 6

P@5 improvements of TEL over best DirichletLM with different kernel functions 14

P@20 improvements of TEL over best DirichletLM with different kernel functions 5

Gaussian LM MAP

5

Triangle LM MAP

Circle LM MAP

4

Cosine LM MAP

Quartic LM MAP

Epanechnikov LM MAP

3

Triweight LM MAP

2

1

Performance improvement (%)

12

4.5 Gaussian LM P@20

Performance improvement (%)

4

Triangle LM P@20

10

Circle LM P@20

3.5

Cosine LM P@20

8

3

Quartic LM P@20

Gaussian LM P@5

Epanechnikov LM P@20

6

Triangle LM P@5

2.5

Triweight LM P@20

Circle LM P@5

4

Cosine LM P@5

2

Quartic LM P@5

1.5

2

Epanechnikov LM P@5

Triweight LM P@5

1

0

0.5

0 WT2G

DISK4&5

WT10G Dataset

BLOGS06

GOV2

-2 WT2G

DISK4&5

WT10G Dataset

BLOGS06

GOV2

0 WT2G

DISK4&5

WT10G Dataset

BLOGS06

GOV2

Figure 2: Performance improvements of TEL over best DirichletLM with different kernel functions.

where

KT L = k1 

1

-

b

+

b  |D| AvgDL

+ RN (t, D)  tf (t, D) (9)

We integrate our model into BM25 to form the Term Location Score (TEL) as follows: T EL(t, D) = ((1 - )  T F (t, D) +   T L(t, D))  IDF (t)
(10) where  controls the contribution of our model.
4. EXPERIMENTAL RESULTS
We conduct experiments on f ve standard TREC collections: WT2G, DISK4&5, WT10G, BLOGS06, and GOV2. These datasets vary in both size and content, where WT2g contains 247,491 general Web documents (TREC'99 Web track), DISK4&5 is comprised of 528,155 newswire documents from sources such as the Financial Times and the Federal Register (TREC'97-99 Ad hoc track), WT10G has 1,692,096 general web documents (TREC'00-01 Web track), BLOGS06 consists of 3,215,171 feeds from late 2005 to early 2006 with associated permalink and homepage documents (TREC'06-08 Blog track), and GOV2 holds 25,178,548 documents crawled from .gov sites (TREC'04-06 Terabyte track).
We compare our model against the following weighting models when they perform best on each dataset with parameters obtained as follows:
1. BM25, with k1 = 1.2 and k3 = 8. We adjust b in the range of [0.1, 0.9] in steps of 0.1 for each dataset to f nd the value of b that gives the best MAP for that dataset.
2. DirichletLM. We adjust µ in the range of [100, 3000] in steps of 100. We f nd the optimal value of µ for each dataset.
The proposed model T EL uses the same values as BM25 for k1, k3, and b, and sets  = 0.2,  = 3, and  = 3 for all datasets.

In the future, we would study the optimal values of the model parameters and their relations with the characteristics of the datasets. We use the TREC off cial evaluation measures in our experiments, namely the topical MAP on BLOGS06 [11], and Mean Average Precision (MAP) on all the other datasets [13]. To stress the top retrieved documents, we also include P@5 and P@20 as the evaluation measures. All statistical tests are based on Wilcoxon Matchedpairs Signed-rank test.
The experimental results are presented in Table 3. To illustrate the performance differences graphically, we plot the results in Figures 1 and 2. As shown by the two f gures, our model TEL outperforms optimized BM25 and DirichletLM in M AP over all datasets with all kernel functions, and outperforms the two optimized baseline models over most of the datasets in P @5 and P @20 with different kernel functions. The performance improvements of our model TEL against DirichletLM are greater than those against BM25. According to the two f gures, each kernel function has its advantage on some datasets. There is no single kernel function that outperforms others on all the datasets.
5. CONCLUSIONS AND FUTURE WORK
In this paper, we extend BM25 and reward the terms based on their locations in the sentences with kernel functions. Experimental study shows that the proposed model performs signif cantly better than BM25 and DirichletLM on MAP over all datasets, and significantly better on P@5, p@10, and p@20 over most datasets.
In the future, more experiments will be conducted to further investigate the proposed model. We would investigate non-symmetric kernel functions and kernel functions with negative values since the placement of the terms at the beginning of the sentences is different from that at the end of the sentences as indicated in the f rst illustra-

885

Model BM 25 DirichletLM TEL Gaussian TEL Triangle TEL Circle TEL Cosine TEL Quartic TEL Epanechnikov TEL Triweight

Eval Metric M AP P @5 P @20 M AP P @5 P @20 MAP P@5 P@20 MAP P@5 P@20 MAP P@5 P@20 M AP
P @5
P @20
MAP P@5 P@20 MAP P@5 P@20 MAP P@5 P@20

WT2G
0.3167 0.5120 0.3870
0.3059 0.5080 0.3870
0.3223*+ (+1.77%,+5.36%)
0.5200* (+1.56%,+2.36%)
0.3960 (+2.33%,+2.33%)
0.3179 (+0.38%,+3.92%)
0.5040 (-1.56%,-0.79%)
0.3890* (+0.52%,+0.52%)
0.3235*+ (+2.15%,+5.75%)
0.5280* (+3.13%,+3.49%)
0.3950 (+2.07%,+2.07%)
0.3186 (+0.60%,+4.15%)
0.5160* (+0.78%,+1.57%)
0.3890* (+0.52%,+0.52%)
0.3199*+ (+1.01%,+4.58%)
0.5160* (+0.78%,+1.57%)
0.3900* (+0.78%,+0.78%)
0.3201*+ (+1.07%,4.64%)
0.5200* (+1.56%,+2.36%)
0.3930 (+1.55%,+1.55%)
0.3179 (+0.38%,+3.92%)
0.5080 (-0.78%,0.00%)
0.3880* (+0.26%,+0.26%)

DISK4&5
0.2176 0.4680 0.3613
0.2190 0.4560 0.3627
0.2201 (+1.15%,+0.50%)
0.4627+ (-1.13%,+1.47%)
0.3653* (+1.11%,+0.72%)
0.2209* (+1.52%,+0.87%)
0.4613+ (-1.43%,+1.16%)
0.3640*+ (+0.75%,+0.36%)
0.2201 (+1.15%,+0.50%)
0.4627+ (-1.13%,+1.47%)
0.3647* (+0.94%,+0.55%)
0.2209* (+1.52%,+0.87%)
0.4613+ (-1.43%,+1.16%)
0.3643*+ (+0.83%,+0.44%)
0.2209* (+1.52%,+0.87%)
0.4613+ (-1.43%,+1.16%)
0.3643*+ (+0.83%,+0.44%)
0.2206 (+1.38%,+0.73%)
0.4640+ (-0.85%,1.75%)
0.3660* (+1.30%,0.91%)
0.2205* (+1.33%,+0.68%)
0.4613+ (-1.43%,+1.16%)
0.3640*+ (+0.75%,+0.36%)

WT10G
0.2134 0.3918 0.2776
0.2168 0.3531 0.2745
0.2202* (+3.19%,+1.57%)
0.3898+ (-0.51%,10.39%)
0.2765 (-0.40%,+0.73%)
0.2196* (+2.91%,+1.29%)
0.3898+ (-0.51%,+10.39%)
0.2755+ (-0.76%,+0.36%)
0.2202* (+3.19%,+1.57%)
0.4000* (+2.09%,+13.28%)
0.2770 (-0.22%,+0.91%)
0.2197* (+2.95%,+1.43%)
0.3898+ (-0.51%,+10.39%)
0.2770+ (-0.22%,+0.91%)
0.2199* (+3.05%,+1.43%)
0.3918+ (+0.00%,+10.96%)
0.2755 (-0.76%,+0.36%)
0.2206* (+3.37%,+1.75%)
0.3918+ (+0.00%,+10.96%)
0.2760 (-0.58%,+0.55%)
0.2198* (+3.00%,+1.38%)
0.3898+ (-0.51%,+10.39%)
0.2765+ (-0.40%,+0.73%)

BLOGS06
0.3195 0.6380 0.6095
0.3125 0.6080 0.5935
0.3238+ (+1.35%,+3.62%)
0.6640* (+4.08%,+9.21%)
0.6180 (+1.39%,+4.13%)
0.3241*+ (+1.44%,+3.71%)
0.6800* (+6.58%,+11.84%)
0.6225 (+2.13%,+4.89%)
0.3238+ (+1.35%,+3.62%)
0.6680* (+4.70%,+9.87%)
0.6185 (+1.48%,+4.21%)
0.3239*+ (+1.38%,+3.65%)
0.6760* (+5.96%,+11.18%)
0.6210 (+1.89%,+4.63%)
0.3239*+ (+1.38%,+3.65%)
0.6760* (+5.96%,+11.18%)
0.6220 (+2.05%,+4.80%)
0.3239+ (+1.38%,+3.65%)
0.6740* (+5.64%,+10.86%)
0.6195 (+1.64%,+4.38%)
0.3239*+ (+1.38%,+3.65%)
0.6760* (+5.96%,+11.18%)
0.6220 (+2.05%,+4.80%)

GOV2
0.3008 0.6094 0.5406
0.2983 0.5919 0.5272
0.3045*+ (+1.23%,+2.08%)
0.6174*+ (+1.31%,+4.31%)
0.5383 (-0.43%,+2.11%)
0.3041*+ (+1.10%,+1.94%)
0.6067+ (-0.44%,,+2.50%)
0.5356 (-0.92%,+1.59%)
0.3045*+ (+1.23%,+2.08%)
0.6148*+ (+0.89%,+8.37%)
0.5376 (-0.55%,+1.97%)
0.3043*+ (+1.16%,+2.01%)
0.6054+ (-0.66%,+2.28%)
0.5383 (-0.43%,+2.11%)
0.3043*+ (+1.16%,+2.01%)
0.6040+ (-0.89,+2.04%)
0.5393 (-0.24%,+2.30%)
0.3044*+ (+1.20%,+2.04%)
0.6107*+ (+0.21%,+3.18%)
0.5383 (-0.43%,+2.11%)
0.3043*+ (+1.16%,+2.01%)
0.6054+ (-0.66%,+2.28%)
0.5379 (-0.50%,+2.03%)

Table 3: Comparison between TEL and two baselines BM25 and DirichletLM with different kernel functions: Parameter b of BM25 and parameter µ of DirichletLM are obtained and set individually for each dataset for their best performances, and "*" and "+" denote statistically signif cant improvements over BM25 and DirichletLM (Wilcoxon signed-rank test with p < 0.05), respectively.
The best result obtained on each dataset is in bold. The two percentages below each value are the percentage improvement of TEL
over BM25 and DirichletLM, respectively.

tive experiment. It is also worthwhile to analyze the optimal values of the model parameters and their relations with the characteristics of the datasets. Different term proximity measures would also be explored to improve the performance of our model.
6. ACKNOWLEDGMENTS
This research is supported by the research grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada and NSERC CREATE Award. We thank anonymous reviewers for their thorough review comments on this paper.
7. REFERENCES
[1] H. Ann. The Essentials of English -- a writer's handbook. New York, Pearson Education, 2003.
[2] C. Dickens. Hard Times. Bradbury & Evans, 1854. [3] D. A. Evans and C. Zhai. Noun-phrase analysis in unrestricted text for
information retrieval. In ACL 1996, pages 17­24. [4] S.-H. Hung, C.-H. Lin, and J.-S. Hong. Web mining for event-based
commonsense knowledge using lexico-syntactic pattern matching and semantic role labeling. Expert Systems with Applications, 37(1):341­347, 2010. [5] F. Ibekwe-SanJuan and et al. Annotation of scientif c summaries for information retrieval. CoRR, 2011.

[6] Y. Jing and W. B. Croft. An association thesaurus for information retrieval. In RIAO'94, pages 146­160.
[7] K. Liu and et al. Effectiveness of lexico-syntactic pattern matching for ontology enrichment with clinical documents. Meth. of info. in med., 50(5):397, 2011.
[8] S. Liu, F. Liu, C. Yu, and W. Meng. An effective approach to document retrieval via utilizing wordnet and recognizing phrases. In SIGIR'04, pages 266­272.
[9] Y. Lv and C. Zhai. Positional language models for information retrieval. In SIGIR'09, pages 299­306.
[10] C. F. Meyer. Introducing English Linguistics. Cambridge University Press, 2010.
[11] I. Ounis and et al. Overview of the TREC-2006 blog track. [12] O. Vechtomova. Noun phrases in interactive query expansion and document
ranking. Info. Retrieval, 9:399­420, 2006. [13] E. Voorhees and D. Harman. TREC: Experiment and evaluation in information
retrieval. MIT Press, 2005.
[14] D. Yang and et al. A natural language processing and semantic-based system for contract analysis. In ICTAI 2013, pages 707­712.
[15] J. Zhao, X. Huang, and S. Wu. Rewarding term location information to enhance probabilistic information retrieval. In SIGIR'12.
[16] J. Zhao, X. Huang, and Z. Ye. Modeling term associations for probabilistic information retrieval. ACM Trans. Inf. Syst., 32(2), 2014.
[17] H.-T. Zheng and et al. Exploiting noun phrases and semantic relationships for text document clustering. Info. Sci., 179(13):2249­2262, 2009.

886

An Initial Investigation into Fixed and Adaptive Stopping Strategies

David Maxwell and Leif Azzopardi
School of Computing Science University of Glasgow Glasgow, Scotland
d.maxwell.1@research.gla.ac.uk Leif.Azzopardi@Glasgow.ac.uk
ABSTRACT
Most models, measures and simulations often assume that a searcher will stop at a predetermined place in a ranked list of results. However, during the course of a search session, real-world searchers will vary and adapt their interactions with a ranked list. These interactions depend upon a variety of factors, including the content and quality of the results returned, and the searcher's information need. In this paper, we perform a preliminary simulated analysis into the influence of stopping strategies when query quality varies. Placed in the context of ad-hoc topic retrieval during a multi-query search session, we examine the influence of fixed and adaptive stopping strategies on overall performance. Surprisingly, we find that a fixed strategy can perform as well as the examined adaptive strategies, but the fixed depth needs to be adjusted depending on the querying strategy used. Further work is required to explore how well the stopping strategies reflect actual search behaviour, and to determine whether one stopping strategy is dominant.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval:Search Process H.3.4 [Information Storage and Retrieval]:Systems and Software:Performance Evaluation
Keywords Search Strategies; Search Behaviour; Stopping Strategies; Evaluation
1. INTRODUCTION
Most models, simulations and measures that examine or evaluate searcher interaction typically rely on the assumption that searchers will reach a fixed depth, with precisionat-n (P @n) being a prime example. In practice, this assumption is unlikely to hold. Indeed, searchers are likely to vary their interaction and the depth to which they inspect snippets and documents, depending on the performance of the system, their information need/task, and the amount of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
SIGIR'15, August 09 - 13, 2015, Santiago, Chile Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00 DOI: http://dx.doi.org/10.1145/2766462.2767802

Kalervo Järvelin and Heikki Keskustalo
School of Information Sciences University of Tampere Tampere, Finland
kalervo.jarvelin@uta.fi heikki.keskustalo@uta.fi
time they have available [4, 17, 18, 20, 22]. For example, a searcher may issue a query that does not return any relevant documents (i.e. a `dud' query). It is then likely that once the searcher inspects a few snippets and/or documents, (s)he will conclude that the issued query was unsuccessful. In this case, the searcher is then likely to issue a new query rather than continue down the current results list. This intuition has been confirmed by empirical analysis, where research shows that searchers examined significantly fewer documents when the search system failed to retrieve any relevant material in the top ten results, in contrast to when it did [1]. Thus, real searchers are inherently adaptive, and their behaviour is conditioned on the quality of the ranked lists that they interact with. In this paper, we examine the aforementioned fixed depth assumption, and perform a preliminary analysis to determine the impact of assuming a fixed depth when interacting with a search system over the course of a session. To this end, we will propose two adaptive stopping strategies - motivated by various stopping rules and compare them to the fixed depth stopping strategy.
While we assume that adaptive approaches may perform better, we hypothesise that this depends on the quality of the queries issued. For example, if all queries issued by a searcher are `duds', then the stopping strategy may be irrelevant. However, what if all their queries are successful, or if their queries are of varying quality? What then is the influence of the stopping strategy on overall performance?
2. RELATED WORK
Knowing when to stop is considered a fundamental aspect of human thinking [15]. Consequently, IR researchers have examined stopping behaviours in a bid to understand why, and when, searchers stop. Many studies investigating when people stop searching have concluded that the decision was mainly based on intuition, or the subjective notion of "feeling good enough" [22] - often termed satisficing [9, 21, 22]. Furthermore, decisions have also been shown to be highly dependent upon the task type being undertaken, time constraints, and various actions that the searcher performs [4, 17, 20, 22]. However, in this work, we are interested in the stopping rules and heuristics that have been proposed [5, 6, 7, 11, 15] and how to operationalise them.
When formulating such rules, researchers have considered stopping behaviours with respect to the overall search task (e.g. ceasing the search when enough information has been acquired to meet some threshold, or satisfy some task or goal) [5, 6, 15]. For example, Nickles et al. [15] proposed a

903

number of rules investigating the sufficiency of information: the mental list rule, where searchers construct a mental list of criteria about a given item (such as a car) that must be satisfied before stopping; the representational stability rule, where a searcher continues to examine information until the underlying mental model that they possess of the topic begins to stabilise; the difference threshold rule, where a searcher sets an a priori difference level to gauge when he or she is not learning anything new; and the magnitude threshold rule, where a searcher has a cumulative amount of information that must be reached before he or she stops.
Other work focuses on stopping behaviour at the query level (e.g. whether the searcher continues to examine documents, or decides to submit a new query) [7, 11]. For example, Cooper [7] devised two stopping rules for examining a list of ranked results: the frustration point rule, where a searcher stops after a certain number of non-relevant documents are encountered; and the satisfaction stopping rule, where searchers would stop when a certain number of relevant documents were obtained. Later, Cooper [8] developed rules using utility theory, positing that searchers stop examining documents once the effort of examining another document outweighs the benefit of moving to a new results list. Similar rules can be obtained from Search Economic Theory [1] and Information Foraging Theory [16].
In this work, we focus on query level stopping rules and implement two variations of the frustration point rule to explore the relationship between stopping strategies and query performance. This rule was also implemented by Lin and Smucker [12]. When navigating similar documents, their simulated searchers stopped after seeing two contiguous nonrelevant documents. Our work however considers different implementations, and explores a range of stopping thresholds for ad-hoc topic retrieval.
3. EXPERIMENTAL METHOD
To explore the influence of the stopping strategy on overall performance, we conducted a number of simulations where we varied the stopping strategy and querying strategy. This was to determine which stopping strategy achieved the best performance when the quality of queries was varied. Our simulations consisted of `searchers' performing ad-hoc topic retrieval over a series of topics on two TREC collections.
Corpora, Topics and System: We used two test collections: TREC AQUAINT with the TREC 2005 Robust Track topics, and TREC WT2g with the TREC Ad-Hoc and Small Web topics. Both topic sets were comprised of 50 topics. Each collection was indexed using the Whoosh IR toolkit1, where stopwords2 were removed and Porter stemming applied. The retrieval model used was PL2 (c = 10.0).
3.1 Simulations
While various methods have been proposed to model or simulate session-based retrieval [2, 3, 14, 19], we utilise an adaption of the method proposed by Baskaya et al. [3] as follows. A searcher (1) issues a query to the system, and then (2) proceeds to examine the first/next result snippet, or decides to issue a new query (back to (1)). If a given snippet is considered relevant, (3) the document is examined. If said document is considered relevant, (4) it is marked as
1 https://pypi.python.org/pypi/Whoosh/ 2Fox's classical stopword list was used. Refer to http://git.io/vT3So for the complete list.

Table 1: Summary of interaction times (in seconds) used for the simulations in this study.

Time Required to...

Seconds

...issue a query ...undertake an initial results page examination ...examine an individual snippet ...examine a document ...mark a document as relevant

15.1 1.1 1.3 21.45 2.57

such, and the searcher returns to (2). If either a snippet or document are considered non-relevant, the searcher returns to (2) - with the document remaining unmarked.
Experimental Setup: At (2), Baskaya et al. [3] assumed in one of their baselines a fixed depth of ten. In this paper, we consider a range of fixed depths n. In addition, we also include two adaptive stopping strategies (see Section 3.2). To generate queries of varying quality, we will employ a range of strategies as suggested by Keskustalo et al. [10] (see Section 3.3). To determine the relevance of a document, the corresponding TREC relevance assessments are typically used. Here, the action/decision of clicking on a relevant snippet or marking a relevant document is determined in a probabilistic manner. In this work, we used the probabilities of clicking on a (non)relevant snippet and the probabilities of marking a document as (non)relevant from the study performed by Smucker and Clarke [18]. In previous work, for each run, whether a document is examined or marked relevant is determined on the fly. This means that for the same query (or even a different query), the same snippet/document can be considered relevant and then non-relevant, or vice versa. In this paper, we pre-compute whether a document is considered relevant or not a priori, so that for different thresholds, depths and other factors, the same judgements are made for each run. This means we can perform a pairwise comparison, thus reducing the total number of simulations required.
Finally, the goal of the search task is to find as many relevant documents in a fixed time period of 1200 seconds (20 minutes). For each action performed during the simulation, the times in Table 1 were used. The estimates for each action were obtained from a user study we performed with 48 subjects over the TREC 2005 Robust Track [13].

3.2 Stopping Strategies
We considered three stopping strategies - the default fixed depth strategy (SS1 ), and two other strategies based on the frustration point rule (SS2 & SS3 ) [7].
SS1 (Fixed Depth): This fixed stopping strategy encodes the heuristic that a searcher will stop examining a results list after they have viewed x1 snippets, regardless of their relevance to the given topic.
SS2 (Total Non-Relevant): Under this stopping strategy, the searcher will stop once they have observed x2 nonrelevant snippets. If a snippet has been previously seen and was considered non-relevant, it is included in the count.
SS3 (Contiguous Non-Relevant): Similar to SS2 above, the searcher will stop using this strategy when they observe x3 non-relevant documents in a row. As above, previously seen non-relevant snippets are included in the count.
For this analysis, we set the thresholds (x1, x2 & x3) to be 1-20 in steps of 1, and 25-50 in steps of 5. The final value of 50 was sufficiently deep enough such that if a simulated

904

searcher only issued one query and examined all documents, they would run out of time. Note that for SS1, x1 corresponds to the maximum depth per query, whereas for SS2 and SS3, x2 and x3 represent the minimum depth per query. For example, when x2 = 3, a searcher is willing to tolerate three non-relevant snippets. However, they may see two relevant snippets in the process, and thus stop at a depth of five. In our results, we will report the average depth per query for each xi. This will therefore allow us to compare across the three implemented stopping strategies.
3.3 Querying Strategies and Selection
Keskustalo et al. [10] define and analyse a number of different querying strategies. For the purposes of this paper, we have selected the best performing querying strategy (referred to as QS3 , consisting of two pivot terms and one other term), and the worst performing querying strategy (referred to as QS1 , consisting of a series of single terms) [3]. These querying strategies were used to determine how the different stopping strategies performed when combined with differing querying quality. We also implemented a blended querying strategy QS1+3 , which interleaved the queries generated by QS1 and QS3 . QS1+3 was implemented to determine how robust the different stopping strategies were against poor performing (or `dud') queries.
Queries were generated as follows. For each topic, the title and description were used to create a Maximum Likelihood Estimate (MLE) language model, i.e. p(term|topic). For QS1 , we then extracted a list of all single terms, ranking them according to this probability. For QS3 , we took all two term combinations of the title terms, and selected the pair with the highest joint probability as the pivot. A list of three term candidate queries q was then constructed by appending another term from the topic to the pivot. These were then ranked according to p(q|topic).
4. RESULTS
Figure 1 plots the mean depth per query3 versus the rate of gain per second (averaged over all sessions and topics), given each threshold value for the AQUAINT collection4. All nine combinations of the aforementioned querying strategies (QS1 , QS1+3 & QS3 ) and stopping strategies (SS1 , SS2 & SS3 ) are shown. From the plots, we can see that the adaptive stopping strategies (SS2 & SS3 ) generally outperformed the fixed depth strategy (SS1 ), regardless of the querying strategy employed, or the depth attained.
Table 2 reports the maximum gain attained across each of the stopping and querying strategies for both AQUAINT and WT2g, together with the thresholds (xi) and mean depth per query (d). To determine whether one stopping strategy outperformed another, we performed a series of paired t-tests comparing each stopping strategy with a given querying strategy (e.g. SS1 & QSx vs. SS2 & QSx ). We found that there were no significant differences at p = 0.05.
To examine why this was the case, given that adaptive strategies should be intuitively more successful, we evaluated the retrieval performance of the queries that were issued during the simulation. Table 3 reports the mean retrieval performance metrics (P @5, P @10 & P @20) for
3The depth for a query is the lowest item in the results list for which a simulated searcher examined the associated snippet of. Mean depth is averaged for all simulated searchers over each query issued. 4Similar plots were obtained for the WT2g collection.

x 10-3 AQUAINT: Mean Depth/Query vs. Mean Rate of Gain
9

8

7

Mean Rate of Gain/Second

6

5

4

SS1 w/ QS1

3

SS2 w/ QS1

SS3 w/ QS1

SS1 w/ QS1+3

2

SS2 w/ QS1+3

SS3 w/ QS1+3

SS1 w/ QS3

1

SS2 w/ QS3

SS3 w/ QS3

0

5

10

15

20

25

Mean Depth/Query

Figure 1: The mean depth per query versus the

mean rate of gain per second for the AQUAINT col-

lection, for each query and stopping strategy. Each

point represents a particular threshold value, i.e. xi.

both collections and for each of the three querying strategies used (QS1 , QS1+3 & QS3 ). In line with previous work, the three-term querying strategy (QS3 ) outperformed the single-term strategy (QS1 ) [10], while blended QS1+3 queries performed better than those generated by QS1 , but worse than QS3 queries. Notably, each querying strategy had a high variance. This resulted in 35-40% of queries issued under QS1 achieving P @10 = 0 (`dud' queries), while approximately 25% of queries under QS1+3 and QS3 were `duds'. This suggests that our manipulation provided a mixture of highly performing and underperforming queries, yet the performance of the best cases for the fixed strategy were similar in overall gain to the adaptive strategies. This may be an artefact of the simulation as fixed interaction probabilities were used, whereas searchers may be more or less likely to click depending on the quality of the list (and the information scent [21]). In future work, we will examine how the behaviour of a searcher changes given the quality of the ranked list to provide a more grounded simulation.
When we examined the threshold for the fixed depth stopping strategy (SS1 ), we observed that it was quite high, ranging from 25-50. This range is much deeper than inspecting ten results per query, which is typically assumed in simulations [3]. Furthermore, real searchers are unlikely to know in advance the average performance of their queries or adopt only one particular querying strategy, so it is unlikely that a real searcher would subscribe to a fixed depth strategy. Similarly to SS1 , SS2 also requires a range of thresholds in order to achieve maximum gain. In contrast, SS3 is more consistent, where thresholds range from three to five over both collections, depending on the querying strategy used.

905

Table 2: Maximum cumulative gain values with corresponding thresholds and depths for each stopping and querying strategy for AQUAINT and WT2g. Significance tests indicate that there are no differences between stopping strategies.

QS1

QS1+QS3

QS3

AQUAINT

4.5 ± 1.1

7.6 ± 1.1

10.6 ± 1.8

SS1 x1 = 45, d = 18.7 x1 = 25, d = 14.3 x1 = 30, d = 15.9

4.7 ± 0.9

7.8 ± 1.1

10.9 ± 0.9

SS2 x2 = 19, d = 17.9 x2 = 9, d = 12.8 x2 = 9, d = 13.9

4.7 ± 0.7

8 ± 1.1

10.5 ± 0.7

SS3 x3 = 5, d = 18.5 x3 = 3, d = 12.5 x3 = 3, d = 13.7

3.9 ± 1

4.9 ± 0.9

6.6 ± 1.4

SS1 x1 = 45, d = 18.2 x1 = 25, d = 13.6 x1 = 50, d = 18.9

3.9 ± 0.7

5 ± 0.7

6.7 ± 0.9

SS2 x2 = 17, d = 17 x2 = 9, d = 12.8 x2 = 30, d = 19.6

3.8 ± 0.6

4.9 ± 0.6

6.6 ± 0.8

SS3 x3 = 5, d = 19 x3 = 3, d = 12.2 x3 = 4, d = 16.9

WT2g

Table 3: Means (and standard deviations) of the queries issued during the simulation for each querying strategy, both for AQUAINT (AQ.) and WT2g.

QS1

QS1+3

QS3

WT2g AQ.

P@5 P@10 P@20

0.2 ± 0.27 0.21 ± 0.26 0.13 ± 0.18

0.32 ± 0.31 0.31 ± 0.29
0.2 ± 0.2

0.41 ± 0.31 0.41 ± 0.28 0.32 ± 0.25

P@5 P@10 P@20

0.19 ± 0.26 0.19 ± 0.22 0.15 ± 0.21

0.25 ± 0.29 0.26 ± 0.25 0.18 ± 0.21

0.32 ± 0.33 0.35 ± 0.29 0.29 ± 0.25

This strategy is also more in line with intuition. Here, the searcher moves to the next query after encountering three to five contiguous non-relevant documents. This suggests that SS3 is more robust across query performance, but further research is required to confirm this.
5. SUMMARY AND FUTURE WORK
In this paper, we used simulations to examine different stopping strategies. Overall, the adaptive stopping strategies tend to outperform the fixed stopping strategy. However, to our surprise, this was not significantly so. The caveat being that for the fixed stopping strategy to provide similar performance, the right threshold needs to be chosen. In practice, this would require a different type of adaptive behaviour, where the searcher changes the depth they are willing to go to based on their querying strategy. This seems unlikely. The most robust stopping strategy appeared to be SS3 (contiguous non-relevant), with a threshold of around three to five non-relevant documents. This stopping strategy seems to match better with intuition, but whether real searchers adopt such a strategy is an open question. In future work, we will examine a greater variety of querying strategies/selection methods (such as lower and higher precision) to determine whether the adaptive strategies result in greater gains. We shall also explore which strategy, if any, best characterises real searcher stopping behaviour, and explore whether there is a relationship between results list quality and interaction probabilities. Other adaptive stopping strategies will be examined, as well as comparing proposed strategies against observed stopping behaviours.

Acknowledgments: We would like to thank the ESF-funded
MUMIA COST Action (ref. ECOST-STSM-IC1002-080914-049840).
We would also like to thank Hora¸tiu Bota, Sean McKeown, Alas-
tair Maxwell and Paul Thomas for their input.
References
[1] L. Azzopardi. Modelling interaction with economic models of search. In Proc. 37th ACM SIGIR, pages 3­12, 2014.
[2] F. Baskaya, H. Keskustalo, and K. J¨arvelin. Time drives interaction: Simulating sessions in diverse searching environments. In Proc. 35th ACM SIGIR, pages 105­114, 2012.
[3] F. Baskaya, H. Keskustalo, and K. J¨arvelin. Modeling behavioral factors in interactive information retrieval. In Proc. 22nd ACM CIKM, pages 2297­2302, 2013.
[4] M. J. Bates. The fallacy of the perfect thirty-item online search. RQ, 24(1):pp. 43­50, 1984.
[5] G. Browne, M. Pitts, and J. Wetherbe. Stopping rule use during web-based search. In 38th HICSS, page 271, 2005.
[6] G. J. Browne, M. G. Pitts, and J. C. Wetherbe. Cognitive stopping rules for terminating information search in online tasks. MIS Quarterly, 31(1):89­104, 2007.
[7] W. S. Cooper. On selecting a measure of retrieval effectiveness part ii. implementation of the philosophy. J. of the American Society for Info. Sci., 24(6):413­424, 1973.
[8] W. S. Cooper. The paradoxical role of unexamined documents in the evaluation of retrieval effectiveness. Info. Processing and Management, 12(6):367 ­ 375, 1976.
[9] M. Dostert and D. Kelly. Users' stopping behaviors and estimates of recall. In Proc. 32nd ACM SIGIR 2009, pages 820­821, 2009.
[10] H. Keskustalo, K. J¨arvelin, A. Pirkola, T. Sharma, and M. Lykke. Test collection-based ir evaluation needs extension toward sessions -- a case of extremely short queries. In Proc. 5th AIRS, pages 63­74, 2009.
[11] D. Kraft and T. Lee. Stopping rules and their effect on expected search length. IPM, 15(1):47 ­ 58, 1979.
[12] J. Lin and M. D. Smucker. How do users find things with pubmed?: Towards automatic utility evaluation with user simulations. In Proc. 31st ACM SIGIR, pages 19­26, 2008.
[13] D. Maxwell and L. Azzopardi. Stuck in traffic: How temporal delays affect search behaviour. In Proc. 5th IIiX, pages 155­164, 2014.
[14] A. Moffat, P. Thomas, and F. Scholer. Users versus models: What observation tells us about effectiveness metrics. In Proc. 22nd ACM CIKM, pages 659­668, 2013.
[15] K. R. Nickles, S. P. Curley, and P. G. Benson. Judgment-based and reasoning-based stopping rules in decision making under uncertainty. Technical report, U. of Minnesota, 1995.
[16] P. Pirolli and S. Card. Information foraging. Psychological Review, 106:643­675, 1999.
[17] C. Prabha, L. S. Connaway, L. Olszewski, and L. R. Jenkins. What is enough? Satisficing information needs. J. of Documentation, 63(1):74­89, 2007.
[18] M. D. Smucker and C. L. Clarke. Time-based calibration of effectiveness measures. In Proc. 35th ACM SIGIR, pages 95­104, 2012.
[19] P. Thomas, A. Moffat, P. Bailey, and F. Scholer. Modeling decision points in user search behavior. In Proc. 5th IIiX, pages 239­242, 2014.
[20] E. G. Toms and L. Freund. Predicting stopping behaviour: A preliminary analysis. In Proc. 32nd ACM SIGIR, pages 750­751, 2009.
[21] W. C. Wu, D. Kelly, and A. Sud. Using information scent and need for cognition to understand online search behavior. In Proc. 37th ACM SIGIR, pages 557­566, 2014.
[22] L. Zach. When is "enough" enough? modeling the information-seeking and stopping behavior of senior arts administrators: Research articles. J. of the American Society for Info. Sci. and Tech., 56(1):23­35, 2005.

906

On the Cost of Phrase-Based Ranking
Matthias Petri Alistair Moffat
Department of Computing and Information Systems The University of Melbourne
matthias.petri, ammoffat@unimelb.edu.au

ABSTRACT
Effective postings list compression techniques, and the efficiency of postings list processing schemes such as WAND, have significantly improved the practical performance of ranked document retrieval using inverted indexes. Recently, suffix array-based index structures have been proposed as a complementary tool, to support phrase searching. The relative merits of these alternative approaches to ranked querying using phrase components are, however, unclear. Here we provide: (1) an overview of existing phrase indexing techniques; (2) a description of how to incorporate recent advances in list compression and processing; and (3) an empirical evaluation of state-of-the-art suffix-array and inverted file-based phrase retrieval indexes using a standard IR test collection.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: content analysis and indexing--indexing methods; H.3.4 [Information Storage and Retrieval]: systems and software--performance evaluation.
1. INTRODUCTION
Postings list processing schemes such as WAND [2] have greatly decreased the cost of similarity computations for bag-of-words retrieval; and postings list storage schemes such as Elias-Fano codes have also reduced the cost of manipulating document-level postings lists [12]. But phrases are sometimes a further part of similarity computations such as BM25 and language models, and including them in similarity computations requires additional information, which must be either explicitly stored in the index, or computed at query time. A range of storage schemes have been proposed that provide different query time, storage space, and retrieval effectiveness trade-offs [4, 11, 19]. Of especial interest is the relative performance of traditional inverted index-based schemes and suffix array-based indexes. The latter have received significant recent attention because they support arbitrary phrase searching.
We compare storage schemes that support phrases as part of ranked queries, when evaluated via list processing schemes such as WAND. Specifically, we define a collection D of N documents D = d0, . . . , dN-1. Each document consists of a non-empty string
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09-13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. http://dx.doi.org/10.1145/2766462.2767769.

of symbols drawn from an alphabet  of size  consisting of the parsed word tokens of the collection. We additionally define a sequence C = d0$d1$d2 . . . dN-1$ consisting of the concatenation of the documents in D, separated by a symbol $  . The length of C is given by n. A phrase P consists of a pattern P[0 . . . m - 1] drawn from , where m  2. The frequency of P in document di is denoted by fi,P . We seek to address the following requirement:
Sorted Document Listing with Frequencies ­ Preprocess a document collection D such that, given an arbitrary phrase P, an increasing sequence of NP document identifiers i, . . . , j can be efficiently computed, together with the frequencies fi,P , . . . , fj,P , where dk  D and fk,P > 0 for each k  i, . . . , j .
The sorted document listing can then be used as a component in a similarity computation using algorithms such as WAND, exactly as if it had been drawn directly from a pre-computed phrase index.
2. PHRASE INDEXING SCHEMES
Five broad categories of phrase indexing scheme have been proposed, with combinations providing other trade-offs.
Positional. A list of term positions is stored, relative to the start
of each document (hierarchical), or as absolute positions within D [17]. The relative representation requires less space, but at the cost of increased run-time performance, since intersection has to be performed both at the document and at the positional level. To answer phrase queries using absolute positions, the positional lists of the terms are intersected using an algorithm such as SvS [4], then converted to an increasing list of document identifiers and frequencies by mapping each position to a corresponding document number.
Pre-computation. Instead of performing list intersection at query
time, the final set of i, fi,P pairs can be stored in the index and accessed when needed by queries. Storage limits mean that precomputing postings lists for all phrases is impossible, and techniques have been explored to choose lists to be computed, including analyzing query logs [3, 19] and using collection statistics [13]. Indexing only a subset of the phrases implies that either other ways of creating lists at query time must be provided too, or that retrieval effectiveness must be sacrificed.
Suffix-Array. Suffix arrays and suffix trees have primarily been
used to support regular pattern searches over a continuous text; but suffix array-based structures can also be used for the sorted document listing problem [10, 11]. All suffix array-based indexes are structured as follows. First, a suffix array SA[0 . . . n - 1] over C is constructed. Then an array DA[0 . . . n - 1] is added, to store the document number in which each suffix appears. To compute a document listing, the interval SA[sp, ep] containing all suffixes prefixed by P is determined, by searching the suffix array. The unique

931

document identifiers and their frequencies in DA[sp, ep] are then determined, using one of several possible methods. In this work we compare three suffix array-based indexes that provide a range of time-space trade-offs.
As a suffix array baseline we use the SORT [6] index. It replaces SA by a compressed suffix array (CSA) [14], which reduces the space required; and stores DA as an array using n log2 N bits. To determine document identifiers and frequencies for a range DA[sp, ep], it is copied to a temporary vector and sorted using a standard integer sorting algorithm. One additional pass produces the list of all NP pairs i, fi,P matching P. The second option, Sadakane's (SADA) index [15] also uses a CSA to find sp, ep . But instead of storing DA explicitly, it is emulated using a bitvector of length n that marks the document boundaries. Two range minimum query (RMQ) data structures are used, and each unique document identifier in the range is processed only once. In addition, for each document di a separate CSA is stored, to determine fi,P . The third variant (WT) stores DA using a wavelet tree [5]. After determining DA[sp, ep], the wavelet tree supports retrieval of the NP pairs i, fi,P in sorted order in O(NP log ) time [16].
Navarro [11] provides details of these mechanisms.
Next Word Indexing. Instead of storing position lists for all unique
terms in , positions for all unique bi-grams in C can be stored [19]. This requires additional space, but intersection of long phrases can be performed efficiently, as the bi-gram lists are shorter than the corresponding term lists. Williams et al. [19] explore additional trade-offs where only some bi-gram lists are stored explicitly, based on common phrases identified in a query log.
Document Surrogates. Direct sequential search for phrases can be
carried out in parsed documents if a small number of candidates is identified via an initial document-level search process. These methods make use of the "direct" file that is maintained by some search systems. Other indexes tailored towards proximity queries also cannot efficiently be used to answer phrase queries [1, 20].
3. LISTS AND INTERSECTION
Indexes that make use of postings lists store sorted sequences of integers, both as part of the underlying document level inverted index, and as part of the positional index. Compression techniques such as OptPforDelta [4] and representations based on Elias-Fano codes [12, 18] allow compact storage of sorted lists, and support fast execution. One of our purposes in this work is to explore the benefits of these new structures in the context of phrase processing.
Packed Binary. One of the most effective encoding schemes for
postings lists is OptPforDelta [21], which compresses blocks of integers using a fixed width b, storing values larger than 2b - 1 as exceptions using a secondary codec. The underlying structure of the code, and the availability of SIMD instructions to assist the process [9], means that implementations can be very fast, and well-suited to hierarchical indexes. On the other hand, absolute positional indexes require encoding schemes which support large integers, and while all of the methods described can be adapted to support 64-bit values, underlying SIMD instructions may not be available, and decoding may be slower. Similarly, compression schemes tuned for positional information may be slower than document level compression codecs [20].
Bitvector Hybrids. Kane and Tompa [7] combine bitvector repre-
sentations and standard compression techniques. Each postings list in the index is either stored completely as a bitvector; partially using a bitvector for the first part and then a compressed representation for the tail; or fully compressed. But bitvectors are only viable

if the set being represented is dense across the domain. For example, a bitvector representing a domain of N  25 million documents requires  3 MiB. The domain is much sparser when storing positional information; and for a collection containing n  23 billion words, a bitvector would be 2.8 GiB. That is, the methods of Kane and Tompa (and the earlier work of Culpepper and Moffat [4]) are not suited to storing positional information.
Elias-Fano Codes. The Elias-Fano (EF) representation for increas-
ing sequences has been rediscovered recently and applied to postings lists [18]. Elias-Fano codes provide a principled way of including bitvectors into postings list representations. Each number is split up into low and high parts. The high parts of the numbers in a sequence are stored as unary encoded differences in a bitvector; the low parts in binary using a fixed number of bits. Ottaviano and Venturini [12] show that a block-partitioned two-level EF representation can compete in both space and time with other forms of posting list compression. Like some other coding methods, the partitioned EF scheme exploits locality within postings lists. Document reordering, typically based on source URLs, produces clusters of similar documents and hence non-uniform postings lists, and can improve compression effectiveness and list processing speed by up to 40% [7]. However, these clusterings schemes are not applicable to improving compression for lists of term positions [20].
Intersection. Conjunctive bag-of-words queries and phrase search-
ing using positional indexes both rely on list intersection, with skip information or bitvectors used to improve intersection speed [4, 7]. Elias-Fano codes directly include bitvectors, and Vigna [18] demonstrates how the bitvector representing the high parts of the sequence can be used as part of the compressed representation of the sequence, as well as an auxiliary structure to support fast skipping. Ottaviano and Venturini [12] show that Boolean conjunctive queries using EF codes outperform OptPforDelta schemes, support 64-bit integers, and are competitive to other representations optimized to support fast list intersection.
4. EXPERIMENTS
Dataset and Methodology. We use the standard GOV2 test collec-
tion of the TREC 2004 Terabyte Track, stored in URL-sort order. To ensure reproducibility we extract the integer token sequence C from Indri1 using default parameters without removing stop-words. We index sequences of length |C| = n = 23,468,782,575 consisting of N = 25,205,179 documents and  = 39,177,922 unique word tokens. The raw collection uses  426 GiB, which is reduced to 71 GiB of 26-bit binary term identifiers after tokenization. All experiments were run on a server equipped with 148 GiB of RAM and two Intel Xeon E5640 processors each with a 12 MiB L3 cache.
To evaluate query performance, we traverse the suffix tree over C and assign phrases according to: a band 1  b  5 where 10b  NP < 2 × 10b; a band b  l  7, where the phrase's least frequent term occurs between 10l and 2 × 10l times in C; and by phrase length 2  m  6. Each bucket was capped at 1,000 phrases; because of the three dimensional nature of the categorization, some had fewer. All reported timings are median per-query elapsed times, with all index components fully memory-resident.
Index Sizes and Implementation. Figure 1 shows space use. The
suffix array-based indexes are roughly the size of C, or larger. The CSA shared by all suffix array methods requires 21 GiB. The SORT method adds an uncompressed DA array of 70 GiB, whereas the WT method uses a wavelet tree over DA (WTD), which is stored
1http://www.lemurproject.org/indri/

932

RELPOS

|C|

EF

NW

SADA

WT

SORT

0

25

50

75

100

Index Size [GiB]

INVIDX

PLISTS

CSA LCSA WTD DA METADATA

Figure 1: Space usage for indexes for GOV2. The dotted line shows

the size of the tokenized collection.

using hybrid bitvectors [8], and requires only 49 GiB. The SADA method uses a local CSA (denoted LCSA in Figure 1) for each document; these are implemented using the method described by Gog et al. [6]. The metadata for SADA includes the two RMQ structures. These index arrangements reflect the current (practical) state-ofthe-art for succinct indexes for the sorted document listing with frequencies problem. The components were partially provided by and implemented using the current version of the sdsl library [6]; our additional code is also available.2
The three inverted file-based indexes share the same documentlevel inverted index (INVIDX), implemented with uniform partitioned EF lists (UEF) with blocksize 128. The document identifiers and frequencies require 5.5 GiB, matching the values reported by Ottaviano and Venturini [12]3. The inverted indexes additionally require 2 GiB metadata used for list offsets, document permutations, and WAND list max scores. The positional data is represented using absolute offsets, but for comparison we also include the size of an positional index that uses relative positions (RELPOS). This approach reduces the size significantly, but is slower than the absolute positional index [17]. The absolute index requires additional metadata to map absolute position offsets in C to document identifiers, implemented using a uncompressed bitvector of length n which marks each document boundary in C. Constant time rank operations are used to achieve the mapping [11]. The nextword index (NW) stores absolute positions for all 473,366,430 bi-grams in GOV2. While there are techniques that only partially store lists [3, 13, 19], we measure the exhaustive case in which all bi-grams are indexed. The position lists of NW are stored using UEF codes, and require 55 GiB, still less than the three suffix-based indexes.
Document Level Retrieval. As a preliminary, we compare the per-
formance of suffix array and inverted file-based methods in the context of bag-of-words conjunctive Boolean queries. Four indexes are used: the UEF and WT methods already described; plus regular Elias-Fano (EF) and OptPforDelta (OP4) encoded postings lists. The latter two require indexes of 8.3 GiB and 5.7 GiB respectively; and in the EF, UEF, and OP4 methods, intersection is achieved via the set-versus-set (SvS) approach. The WT index supports conjunctive queries by performing intersection operations, as described by Gagie et al. [5]. For each query term, the range SA[sp, ep] is determined. These are then processed simultaneously using the wavelet tree over DA, to determine document identifiers which contain all terms. This approach has not yet been compared empirically to inverted file-based intersection approaches, hence our interest.
2https://github.com/mpetri/pos-cmp 3Yan et al. [21] report 4.1 GiB but the basis of this is unclear, and may involve stopping or other index reduction techniques.

Queries

EF

UEF

OP4

WT

TREC 2005 0.92 1.51 1.28 77.78 TREC 2006 2.32 3.71 3.12 148.90

Table 1: Median conjunctive Boolean bag-of-words retrieval times, in milliseconds, over GOV2.

Band
b=1 b=2 b=3 b=4 b=5

EF
0.09 0.69 6.43 62.69 522.20

NW
0.08 0.38 1.20 2.09 52.66

SORT
0.11 0.12 0.22 1.34 13.61

WT
0.37 1.59 9.09 34.36 226.17

SADA
0.37 2.65 25.36 222.41 1922.13

Table 2: Median phrase materialization times, in milliseconds, over
GOV2, using 1,000 queries in each bucket except when b = 5 (421 queries), with 10b  NP < 2 × 10b in the b th band. The pattern length is fixed at m = 3, and the smallest list size band at l = b + 2
for queries in the b th band.

All queries of length m  2 in the TREC 2005 and 2006 Terabyte Track efficiency tasks are used in this experiment, 34,495 and 94,253 queries respectively. Table 1 shows median query times, in milliseconds. The relative times of the inverted file-based indexes are broadly similar to those reported in recent studies [7, 12], except that our OP4 index is faster than the UEF index. This is in contrast to what was reported by Ottaviano and Venturini [12], a difference that we attribute to optimizations done in the NEQ skip method they used. All studies to date agree that EF is faster than OP4, but requires more space.
The three inverted file-based indexes outperform the WT index by a considerable margin. This is a consequence of the random memory accesses required to work the wavelet tree, compared to fast sequential processing of postings lists in the inverted file indexes. That is, the WT index is not competitive in either space or time. However, the WT index can also answer phrase queries, whereas additional positional information is required before inverted file indexing schemes can do the same (Figure 1).
Phrase Materialization. We turn to our main interest ­ the query-
time generation of postings lists for phrases, ready for incorporation in ranked retrieval such as WAND-based evaluation of the BM25 scoring regime. Similarity computations using WAND can be performed efficiently, meaning that materializing additional postings lists must also be fast if it is not to dominate execution times. For example, Ottaviano and Venturini [12] report average BM25 computation times of  9 milliseconds for GOV2.
Table 2 shows median query times to materialize phrase lists for synthetic queries of length m = 3, categorized according to the band corresponding to the result size NP . For each band b, we fix the smallest position list size l to be b + 2. That is, the smallest postings list for each query is  100 times larger than NP for that query. For small b, the cost of phrase materialization using all methods is within the cost of performing a WAND computation. As b becomes larger, the WT method becomes uncompetitive; once b = 5, both SADA and EF require too much time to be included in a similarity computation. The simple SORT method and NW index remain competitive. Surprisingly, the fastest index for bands b  2 is the simple SORT index, which copies and sorts parts of DA. Unlike SADA and WT, the performance of SORT is dependent on ep - sp, which can be much larger than NP . But SORT does not perform random accesses, and instead makes use of fast localized

933

Time [ms]

Output Band b = 1

1k

EF SADA

100

10

1

0.1

Output Band b = 3

1234567 1234567
Smallest List Band [l] Figure 2: Phrase materialization times for EF and SADA, in milliseconds over GOV2, for queries where 10b  NP < 2 × 10b for b = 1 and b = 3 for smallest list sizes l  b . . . 7. Patterns lengths are in the range m  2 . . . 6.
integer sorting. Its drawback is the space required ­ 98 GiB, more than the other indexes.
In the previous experiment the size of the input for EF is fixed to be a constant ratio of the output size. The performance of list intersection-based methods depends on the size of the smallest list to be intersected, whereas all suffix array based methods only depend on the size of NP . Figure 2 shows the phrase materialization cost for patterns with b = 1 and b = 3, varying l, the minimum list length band, over b  l  7. The time to process a phrase query using SADA depends primarily on the output size, rather than the lengths of the terms' position lists. In contrast, the EF run time significantly increases as l grows. In particular, EF is faster than SADA for small l (for example, for b = 3 and l = 4, EF takes 1.22 milliseconds, compared to 18.16 for SADA), but is more than an order of magnitude slower for phrases containing no infrequent terms.
Additional Trade-offs. We have evaluated a selection of index types
in our phrase list comparison. Other combinations have also been proposed, with different time and space trade-offs. The nextword index (NW) in our experiments stores positional lists for all unique word bi-grams in C. Williams et al. [19] examine hybrid schemes, in which only certain bi-gram lists are stored, reducing the space required. Similarly, Petri et al. [13] propose a mixed arrangement which pre-computes document/frequency lists up to a certain size threshold. Smaller lists are materialized at query time by processing DA, similar to the SORT index. A nextword index could also be used to materialize missing lists in this scheme. Another option is to avoid storing DA explicitly, and extract document numbers from the CSA by storing an additional bitvector marking document boundaries. This change would greatly reduce the storage cost of the index, but would add ep-sp calls to suffix array values and rank operations, adversely affecting query time. That is, this method can only be viable for small sp, ep ranges.
Construction Cost. Our primary focus has been on phrase list ma-
terialization times. But construction costs are also a factor to be considered when choosing an index. The cost of building the suffix array-based indexes is an order of magnitude higher than inverted file-based indexing methods [6], and constructing SA for large parsings requires RAM not available in commodity hardware.
5. CONCLUSION
We compare inverted file indexes to suffix-based alternatives. The WT index is uncompetitive in both time and space for conjunctive Boolean retrieval; and all suffix-based indexes are larger than their inverted file-based counterparts. For phrase components, the SADA and WT methods are fast to materialize short lists, but slow

considerably when there are many answers. Regular positional list intersection can produce phrase lists efficiently if the smallest intersected list is short. The nextword index (NW) is smaller than all suffix array indexes, and provides materialization times which are reasonably fast. The simple SORT index processes phrase queries rapidly even for large b, but uses nearly 100 GiB RAM. These various relativities are determined by the number of answers and the frequency of the smallest input term, and in future work we will more fully categorize the respective zones of applicability.
Acknowledgment. This work was funded by the Australian Re-
search Council's Discovery Project scheme (project DP140103256), and by the Victorian Life Sciences Computation Initiative (grant VR0052), an initiative of the Victorian Government, Australia.
References
[1] D. Arroyuelo, S. González, M. Marín, M. Oyarzún, and T. Suel. To index or not to index: Time-space trade-offs in search engines with positional ranking functions. In Proc. SIGIR, pages 255­264, 2012.
[2] A. Z. Broder, D. Carmel, H. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In Proc. CIKM, pages 426­434, 2003.
[3] A. Broschart and R. Schenkel. High-performance processing of text queries with tunable pruned term and term pair indexes. ACM Trans. Inf. Sys., 30(1):5, 2012.
[4] J. S. Culpepper and A. Moffat. Efficient set intersection for inverted indexing. ACM Trans. Inf. Sys., 29(1):1, 2010.
[5] T. Gagie, G. Navarro, and S. J. Puglisi. New algorithms on wavelet trees and applications to information retrieval. Theor. Comp. Sc., 426427:25­41, 2012.
[6] S. Gog, T. Beller, A. Moffat, and M. Petri. From theory to practice: Plug and play with succinct data structures. In Proc. SEA, pages 326­ 337, 2014.
[7] A. Kane and F. W. Tompa. Skewed partial bitvectors for list intersection. In Proc. SIGIR, pages 263­272, 2014.
[8] J. Kärkkäinen, D. Kempa, and S. J. Puglisi. Hybrid compression of bitvectors for the FM-index. In Proc. DCC, pages 302­311, 2014.
[9] D. Lemire and L. Boytsov. Decoding billions of integers per second through vectorization. Soft. Prac. & Exp., 45(1):1­29, 2015.
[10] S. Muthukrishnan. Efficient algorithms for document retrieval problems. In Proc. SODA, pages 657­666, 2002.
[11] G. Navarro. Spaces, trees and colors: The algorithmic landscape of document retrieval on sequences. ACM Comp. Surv., 46(4.52), 2014.
[12] G. Ottaviano and R. Venturini. Partitioned Elias-Fano indexes. In Proc. SIGIR, pages 273­282, 2014.
[13] M. Petri, A. Moffat, and J. S. Culpepper. Score-safe term-dependency processing with hybrid indexes. In Proc. SIGIR, pages 899­902, 2014.
[14] K. Sadakane. New text indexing functionalities of the compressed suffix arrays. J. Alg., 48(2):294­313, 2003.
[15] K. Sadakane. Succinct data structures for flexible text retrieval systems. J. Disc. Alg., 5(1):12­22, 2007.
[16] T. Schnattinger, E. Ohlebusch, and S. Gog. Bidirectional search in a string with wavelet trees. In Proc. CPM, pages 40­50, 2010.
[17] D. Shan, W. X. Zhao, J. He, R. Yan, H. Yan, and X. Li. Efficient phrase querying with flat position index. In Proc. CIKM, pages 2001­2004, 2011.
[18] S. Vigna. Quasi-succinct indices. In Proc. WSDM, pages 83­92, 2013.
[19] H. E. Williams, J. Zobel, and D. Bahle. Fast phrase querying with combined indexes. ACM Trans. Inf. Sys., 22(4):573­594, 2004.
[20] H. Yan, S. Ding, and T. Suel. Compressing term positions in web indexes. In Proc. SIGIR, pages 147­154, 2009.
[21] H. Yan, S. Ding, and T. Suel. Inverted index compression and query processing with optimized document ordering. In Proc. WWW, pages 401­410, 2009.

934

Relevance-aware Filtering of Tuples Sorted by an Attribute Value via Direct Optimization of Search Quality Metrics

Nikita Spirin1, Mikhail Kuznetsov2, Julia Kiseleva3, Yaroslav Spirin4, Pavel Izhutov5
1UIUC, Urbana IL, USA; 2MIPT, Dolgoprudny, Russia; 3Eindhoven University of Technology, Eindhoven,
Netherlands; 4Datastars, Moscow, Russia; 5Stanford University, Palo Alto CA, USA
spirin2@illinois.edu1, mikhail.kuznecov@phystech.edu2, j.kiseleva@tue.nl3, izhutov@stanford.edu5

ABSTRACT
Sorting tuples by an attribute value is a common search scenario and many search engines support such capabilities, e.g. price-based sorting in e-commerce, time-based sorting on a job or social media website. However, sorting purely by the attribute value might lead to poor user experience because the relevance is not taken into account. Hence, at the top of the list the users might see irrelevant results. In this paper we choose a different approach. Rather than just returning the entire list of results sorted by the attribute value, additionally we suggest doing the relevance-aware search results (post-)filtering. Following this approach, we develop a new algorithm based on the dynamic programming that directly optimizes a given search quality metric. It can be seamlessly integrated as the final step of a query processing pipeline and provides a theoretical guarantee on optimality. We conduct a comprehensive evaluation of our algorithm on synthetic data and real learning to rank data sets. Based on the experimental results, we conclude that the proposed algorithm is superior to typically used heuristics and has a clear practical value for the search and related applications.
Keywords
Search Metric; Attribute; Filtering; Dynamic Programming
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information filtering, Retrieval models, Search process, Selection process
1. INTRODUCTION
Many search engines support sorting of the search results by an attribute value, e.g. sort items by price in e-commerce or sort resumes by the update time on the job websites. A similar scenario exists in the social domain when the goal is to construct a chronologically sorted social feed, e.g. Twitter, Facebook. However, sorting purely by the attribute value might not be the best approach since at the top of the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767822.

list users might find irrelevant results. For example, see the screenshots of the search user interfaces for Indeed.com and Amazon.com on Figure 1. In both cases the results sorted by the attribute values are hardly relevant for the queries.
To evaluate how such search scenarios are supported today, we conducted the ad hoc evaluation of ten popular search engines from the e-commerce and job industries1. For each search engine we submitted 25 queries (different queries for different industries), applied the sorting based on one of the attributes (relevance, date, price), and judged the quality of results2. The ranking by relevance is of very high quality. The average Precision@10 is 0.86. On the other hand, we found that the search results are far from relevant when the attribute-based sorting is done. For instance, across the sites the average Precision@1 is 0.44, Precision@5 is 0.45, and 61% of queries have the Precision@10 below 0.5. We think that it is mainly due to the relevance not being taken into account when the attribute-based sorting is requested. Therefore, our research questions are: (RQ1) Can the quality of results sorted by the attribute value be improved by incorporating the relevance into the ranking process? (RQ2) What is the best way to accomplish it?
In this paper we propose a new principled approach to perform relevance-aware search results (post-)filtering via direct optimization of a given search quality metric. Our algorithm uses the ideas from dynamic programming and is guaranteed to deliver the optimal solution. The algorithm is presented in Section 3. The experiments on synthetic and real learning to rank (L2R) data sets are described in Section 4.
2. RELATED WORK
This work is related to the research on search user behavior analysis, search metrics, and learning to rank. The proposed algorithm is based on the dynamic programming [1].
Researchers studied the way people interact with search engines by analyzing mouse movements, eye-tracking and click logs. Joachims et al. [9] discovered the position bias phenomenon, i.e. the results at the first two positions receive most attention, and then it quickly drops. Plus, on average users tend to read the results in a linear order from top to bottom. Craswell et al. [4] explored how the position bias might arise and proposed four hypotheses and the corresponding probabilistic click models. They found that the "cascade" model, where users view results from top to bottom and leave as soon as they see a worthwhile document, is
1Amazon, Walmart, Target, Etsy, BestBuy, NewEgg for products and Indeed, LinkedIn, SuperJob, Monster for jobs. 2we don't describe the exact setup due to the page limit.

979

Figure 1: (A) Indeed.com resume search results for the query "product manager" sorted by "date" and (B) Amazon.com search results for the query "bicycle" sorted by "price". While sorting by relevance is accurate, the results sorted by the attribute value are hardly relevant for the query, which leads to poor user experience.

the best explanation for position bias in early ranks. Dupret et al. [5] generalized this model by allowing for the possibility that a user skips a document without examining it.
Complementary to the work on search models, a lot of attention has been devoted to the design and analysis of search metrics. Thus, in addition to the traditional metrics, like the Precision and the Recall, J¨arvelin and Kek¨al¨ainen proposed the (Normalized) Discounted Cumulative Gain (DCG) [8], Chapelle et al. -- the Expected Reciprocal Rank (ERR) [2], to name just a few. Recently, Chuklin et al. [3] developed a unified framework to convert any click model into the evaluation metric. Essentially, all search metrics model the position bias and penalize the top ranked irrelevant results.
Numerous ranking algorithms have been developed to accurately predict the relevance of documents. Typically, these algorithms are based on machine learning and find the optimal parameters by optimizing the "surrogate" objective function. However, the solution to the approximation is not always optimal for the original ranking problem. Therefore, recently several approaches have been proposed that directly optimize a given search metric. For instance, Xu et al. [12] focus on the algorithms that optimize the objectives upperbounding the original non-smooth search metrics. Tan et al. [11] proposed DirectRank, which is based on the iterative coordinate ascent with the smart line search procedure.
Attribute-based ranking, however, has been handled very differently. Rather than taking the relevance into account, search engines return the list of results sorted by the attribute value or suboptimal heuristics are used (Section 4.1). Inspired by the recent advancements in L2R, in this work we bridge the gap between the relevance-based ranking and the attribute-based ranking by proposing to do relevanceaware search results filtering, which directly optimizes a given search metric, when the sorting by the attribute value is requested. It is worth highlighting the difference between the proposed algorithm and a famous TA algorithm by Fagin et al. [6]. While TA algorithm finds the top-k most relevant tuples by scoring them individually, we return the tuples, which cumulatively optimize a given search quality metric. The ordering of the tuples is as crucial as their relevance.
3. OUR APPROACH
We consider the scenario when a user requests the sorting of the search results by the attribute value, e.g. by date (Fig-

ure 1, A) or by price (Figure 1, B). Our goal is to produce the final ranking that both satisfies the strict ordering constraints and optimizes a given search quality metric (in turn it minimizes the user's effort on finding relevant results). We only focus on the results filtering and assume that the relevance scores are already predicted by the ranking algorithm. Therefore, the formalization of our problem looks as follows.
Input: a list of tuples {(ti, ri)}li=1, where ti is the attribute value and ri  R+ is the relevance score predicted by the ranking algorithm; a search quality metric Q.
Output: a (sub)list of indices J delivering the maximum to the metric Q and totally ordered based on the attribute value, i.e. J = arg max Q(rji |ji  J ), s.t. tj1 < ... < tjl .
Throughout the paper, we consider the DCG as the search quality metric (although ERR or other metric can be used), date as the attribute, and the input sorted chronologically. It is worth mentioning that the formalization above covers the post-filtering scenarios as well, i.e. the input might consist of tuples that passed some other filtering algorithm.
Currently, this problem is solved heuristically. Mainly there are two approaches built around the same idea of thresholding. We can take only the results that have the relevance score above the threshold. We can also sort the results by the relevance score, take the top-k elements, and finally re-sort the list by date. While these approaches are easy to implement, they have two major drawbacks. First, it is not clear how to set the threshold. Second, the described approaches are the approximate solutions of our problem. Even the result set constructed from the top-k tuples sorted by relevance, being re-sorted by the attribute value, gets ordered randomly if we look at the relevance component.
The solution that guarantees optimality is to enumerate all possible subsequences, compute the metric for each one, and take the best one. However, this approach is not tractable as the number of subsequences is exponential. We propose a polynomial algorithm based on the dynamic programming [1]. There are three key observations behind our algorithm: (1) natural enumeration order for subsequences; (2) additivity of the metric; (3) optimality of subproblems.
First, all subsequences can be partitioned into the factor classes based on their length, i.e. in each factor class there will be the subsequences of the same length. To enumerate all subsequences, we can iterate over the factor classes and

980

Algorithm 1 (A1) Relevance-aware filtering of totally ordered set via direct optimization of a search quality metric

Input: DCG and {(ti, ri)}li=1, s.t. ti < ... < tl and ri  R+ Output: J = arg max DCG(rji |ji  J ), s.t. tj1 < ... < tjl 1: M  M atrix(l + 1, l + 1); M (:, 0)  0; M (0, :)  0;

2: P ath  M atrix(l + 1, l + 1); # to recover max sequence

3: for i in 1, . . . , l

4: for j in 1, . . . , i

5:

gain



2ri -1 log(j+1)

;

6: if M (i - 1, j - 1) + gain > M (i - 1, j)

7:

M (i, j)  M (i - 1, j - 1) + gain;

8:

P ath(i, j)  (i - 1, j - 1);

9: else

10:

M (i, j)  M (i - 1, j);

11:

P ath(i, j)  (i - 1, j);

12: (i, j)  arg max M (l, :); # last element of solution

13: J  List(); J.append(j);

14: while i > 1 and j > 1

15: if P (i, j).last < j

16: J.append(P (i, j).last);

17: (i, j)  P (i, j); # jump to shorter subsequence

18: return J.reverse()

within each factor class enumerate all subsequences. Second, the search metrics are additive and can be computed in linear time from the beginning of the list to the end [2]. It means that having a partial metric value for the prefix, we can compute the new metric value by simply adding the gain/utility provided by the current element. Third, the optimal subsequence for the prefix of length k is one of the optimal subsequences from each of the factor classes for the prefix of length k - 1 with or without the current element appended (proof by induction for the prefix length).
Combining the observations above, we present our algorithm and its analysis. It starts by initializing the memoization matrix to store the optimal DCG values for subproblems and the transition matrix to reconstruct the optimal subsequence. Then, it iterates over the prefixes of the input sequence in the outer loop and over the factor classes in the inner loop. The cell (i, j) is for the optimal subsequence of length j for the prefix of length i. At each step we decide whether we should append the current element of the input sequence i to the optimal subsequence of length j - 1 for the prefix of length i - 1 (the recursion on lines 6-11). If we append the current element, we go diagonal. If we don't ap-

Figure 2: Dependencies in the memoization matrix, a legal evaluation order, and the optimal path.

pend, we keep the existing optimal subsequence of length j and stay on the same column. A legal evaluation order and the dependencies between the cells are shown in Figure 2, A. Finally, to reconstruct the optimal subsequence, we find the maximum in the last row (the last element is always "in" since the elements are non-negative) and go backwards in the P ath matrix. If the line is diagonal, we take the element in the next cell. Otherwise, we skip. The P ath matrix is depicted in Figure 2, B. The complexity (both time and space) of the algorithm is O(l2) because we have two nested loops, costing us O(1) time at each iteration, and the square memoization matrices. It is guaranteed to deliver the optimum because we "virtually" enumerate all subsequences within the dynamic programming framework. For a toy example problem {(0, 0), (1, 3), (2, 1), (3, 2), (4, 1), (5, 3))} the optimal solution is {1, 3, 4, 5} with the DCG equal to 12.40.
4. EXPERIMENTS AND RESULTS
In this section we study how our approach contributes to the ranking quality using two real LETOR [10] (MQ2007, MSLR-WEB10K) and synthetically generated data sets.
4.1 Learning to Rank Data Sets
To answer our research questions, we do the simulations using the real learning to rank data sets. We extend MQ2007 and MSLR-WEB10K data sets by assigning a random timestamp to each document to model the sorting by the attribute value. Scikit-learn3 implementation of the Gradient Boosted Regression Trees (GBRT ) [7] is used to predict the relevance scores. The optimal parameters for the final GBRT model are picked using cross validation for each data set. We use the 5-fold cross validation partitioning from LETOR [10].
Three popular baselines are considered, which are typically used to perform the filtering of the search results: Baseline 1 (B1): sort by the attribute value, no filtering; Baseline 2 (B2): sort by the attribute value, keep the results with the predicted relevance scores above the threshold (we normalized the scores to [0,1] and set the threshold=0.5); Baseline 3 (B3): sort results by the predicted relevance score, take the top-k (where k is the cutoff point for the metric value calculation), and re-sort by the attribute value.
The evaluation procedure works as follows. First, we train the GBRT on the training folds. Second, we predict the relevance scores using the trained GBRT model for the documents in the testing fold. Third, we apply a baseline filtering algorithm to the documents in the testing fold by working with the relevance scores from the step two and the randomly generated timestamps. Fourth, we apply our filtering algorithm to the tuples that passed the baseline filtering. Finally, knowing the true relevance labels, we calculate the N DCG@k for the filtered result list sorted by the timestamp. To make sure that the conclusions are not due to randomness, we average the results from 1000 runs.
The results of the experiment are presented in Table 1 and 2. We can see that the output (post-)filtered with our algorithm is regularly better than the baselines. We applied the binomial test and found that almost all differences in the N DCG values are statistically significant (marked in bold), p-value is below 0.001. One average the increase in the metric value is around 2-4%. Moreover, since the data sets used have very different characteristics (e.g. the average query
3http://scikit-learn.org/stable/index.html

981

NDCG B1 only A1  B1 B2 only A1  B2 B3 only A1  B3

@1 0.226 0.299 0.289 0.315 0.433 0.433

@5 0.245 0.287 0.318 0.326 0.417 0.417

@10 0.273 0.304 0.357 0.364 0.418 0.420

@20 0.336 0.363 0.448 0.453 0.451 0.455

@40 0.496 0.511 0.450 0.454 0.498 0.512

Table 1: The demonstration of effectiveness of the proposed approach on MQ2007 data set.

length for MQ2007 is 40 and for MSLR-WEB10K -- 120),

the experiment suggests that the algorithm achieves good

performance for a wide range of input problems. Yet, one

should note that the increase in the ranking quality comes

with the extra computational cost because the complexity of

our

algorithm

is

O(

l log

l

)

times

larger

than

for

the

baselines.

4.2 Synthetic Data Sets

In this section we focus on the filtering only (both rele-

vance labels and timestamps are generated) and study how

the algorithm behavior changes for different input sizes and

relevance label distributions. We consider the following four

label distributions modeling the real situations: (a) uniform

integer in the range [0, 5]; (b) uniform real in the range [0, 5];

(c) power law,

the slope 

=

2.0;

(d)

3x2 125

with

the

support

in

the range [0, 5]. We generate the input lists for the filtering

algorithm by sampling from the corresponding distribution.

Similarly to the previous experiment, we simulate each com-

bination of conditions 1000 times and average the runs. Only

the Baseline 1 is used in this experiment for simplicity. The

data from the simulation is presented in Figure 3.

There are several observations that could be made with

the help of this figure. First, the output size is linearly pro-

portional to the input size (Figure 3, C). DCG also grows

linearly with the growth of the input size (Figure 3, A). Sec-

ond, the proposed algorithm always outperforms the Base-

line 1 (Figure 3, B), which is expected because we do the

filtering directly optimizing a given search quality metric.

Third, both the graph for the ratio of the DCG values and

the graph for the ratio of the output sequence lengths for the

proposed algorithm and the baseline monotonically converge

for the longer input lists (Figure 3, B and D). This means

Figure 3: The behavior of the algorithm (A1) for different input sizes and relevance label distributions.

NDCG B1 only A1  B1 B2 only A1  B2 B3 only A1  B3

@1 0.131 0.173 0.170 0.192 0.390 0.390

@5 0.161 0.183 0.208 0.215 0.362 0.362

@10 0.190 0.208 0.244 0.250 0.365 0.366

@20 0.236 0.250 0.300 0.304 0.380 0.382

@40 0.309 0.321 0.379 0.383 0.418 0.421

Table 2: The demonstration of effectiveness of the proposed approach on MSLR-WEB10K data set.

that our algorithm works better when the original hit list is shorter. Fourth, higher gains in DCG over the baseline are characteristic for the relevance label distributions, where relevant results are more probable (Figure 3, B). The observations above are valid for non-degenerate cases, e.g. not all labels are the same or sorted in a special order.
5. CONCLUSIONS AND FUTURE WORK
In this paper we addressed the important problem in search, that is, how to increase the relevance of the search results sorted by an attribute value. Our solution is based on the idea to perform relevance-aware search results filtering by directly optimizing a given search quality metric. We developed a simple, yet effective algorithm based on the dynamic programming, which consistently outperforms typically used heuristic approaches and is guaranteed to deliver the optimal solution. In the future, we plan to integrate the proposed algorithm in a real search engine and instrument an A/B test to see how such a modification will affect the user engagement and satisfaction with the search results.
6. ACKNOWLEDGEMENTS
We thank Karrie Karahalios, ChengXiang Zhai, and anonymous reviewers for their valuable comments and suggestions.
7. REFERENCES
[1] R. Bellman. Dynamic Programming. Dover Books on Computer Science, USA, 2003.
[2] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. CIKM'09.
[3] A. Chuklin, P. Serdyukov, and M. de Rijke. Click model-based information retrieval metrics. SIGIR'13.
[4] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In Proceedings of Web Search and Data Mining 2008.
[5] G. E. Dupret and B. Piwowarski. A user browsing model to predict search engine click data from past observations. In Proceedings of ACM SIGIR 2008.
[6] R. Fagin, A. Lotem, and M. Naor. Optimal aggregation algorithms for middleware. In Proceedings of PODS '01.
[7] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 2001.
[8] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4).
[9] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. SIGIR'05.
[10] T. Qin, T.-Y. Liu, J. Xu, and H. Li. LETOR: A benchmark collection for research on learning to rank for information retrieval. Inf. Retr., 13(4).
[11] M. Tan, T. Xia, L. Guo, and S. Wang. Direct optimization of ranking measures for learning to rank models. KDD'13.
[12] J. Xu, T.-Y. Liu, M. Lu, H. Li, and W.-Y. Ma. Directly optimizing evaluation measures in L2R. SIGIR'08.

982

Towards Understanding the Impact of Length in Web Search Result Summaries over a Speech-only Communication Channel

Johanne R. Trippas johanne.trippas@rmit.edu.au

Damiano Spina damiano.spina@rmit.edu.au

Mark Sanderson mark.sanderson@rmit.edu.au

Lawrence Cavedon lawrence.cavedon@rmit.edu.au

School of Computer Science and Information Technology RMIT University, Melbourne, Australia

ABSTRACT
Presenting search results over a speech-only communication channel involves a number of challenges for users due to cognitive limitations and the serial nature of speech. We investigated the impact of search result summary length in speech-based web search, and compared our results to a text baseline. Based on crowdsourced workers, we found that users preferred longer, more informative summaries for text presentation. For audio, user preferences depended on the style of query. For single-facet queries, shortened audio summaries were preferred, additionally users were found to judge relevance with a similar accuracy compared to text-based summaries. For multi-facet queries, user preferences were not as clear, suggesting that more sophisticated techniques are required to handle such queries.
Categories and Subject Descriptors
H.5.1 [Multimedia Information Systems]; H.3.3 [Information Search and Retrieval]
Keywords
Spoken Retrieval; Search Result Summarisation; Crowdsourcing
1. INTRODUCTION
Speech-based web search (i.e., posing search queries using voice rather than a keyboard/touchscreen interface) is increasingly ubiquitous, particularly through the use of mobile devices. Several systems (e.g., Siri, Google Now, Cortana) can speak a reply to "factoid"-style search queries (e.g., "How high is Everest?"). If no factoid answer exists, the systems revert to displaying a ranked list of results on screen. However, there are situations where a full speech-only interface is preferable, such as while driving a car [6, 7], when there is no screen or keyboard available [20], when users are mobile [12, 16, 18], or when using wearable devices [4].
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09­13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767826.

Moreover, visually presented results may be inaccessible for certain populations, such as visually impaired users [14, 17] or people with limited literacy skills.
Presenting lists of search results over a speech-only communication channel presents a number of challenges; in particular, simply speaking the textual component of a standard search results list has been shown to be ineffectual [17]. The serial nature of the speech/audio channel makes it difficult for users to "skim" back and forth over a list of results (a standard process in browsing a visual list).
Few studies have investigated techniques for effective presentation of web search results via a speech-only communication channel (hereafter referred to as audio) [7]. The present study seeks to address this. In particular, we seek a better understanding of how to present search results over audio while not overwhelming the users with information [18], nor leaving users uncertain as to whether what they heard covered the information space [19].
The length of a spoken search result summary plays a crucial role in the success or failure of presenting search results over audio. A short summary might not yield enough information to judge whether the retrieved document is relevant or not; in contrast, a more descriptive summary might take too long to be played and thereby diminish user experience. Thus a trade-off is necessary between a short summary and a longer, more descriptive summary.
The present study investigated these trade-offs via a crowdsourcebased interactive experimental design. The aim of the study was to develop a baseline of understanding about the audio result summary length users prefer. This study aimed to answer the following research questions:
· What is the impact of search result summary length in a spoken retrieval scenario?
· Do users prefer a longer or shorter summary?
2. METHODOLOGY
Our experiments used a crowdsourcing platform to present queries and search results to users. Result summaries of various length were presented in text or audio form. Summary length was either a full Google-length summary or a truncated version extracted from the original summary. Users were asked to select a result that best addressed the query, and were also presented with post-task and exit questionnaires. The present study used the CrowdFlower crowdsourcing tool [9, 10] to provide user data for the tasks and questionnaires.

991

Table 1: Examples of full and truncated summaries for faceted and single-faceted queries.

Query

Type

old town scottsdale
Find restaurants in Old Town Scottsdale, AZ.

faceted

what was the name of elvis presley's home
What was the name of Elvis Presley's home?

single-facet

Full Summary
Downtown Scottsdale / Old Town Scottsdale. Downtown Scottsdale pulses with a vibrant energy all its own. By day, shopping, gallery-hopping and dining. . .
Where the Old West Meets the New West. Downtown Scottsdale is the ultimate destination for visitors and residents alike to enjoy art galleries, specialty retail. . .
The unique association between Elvis Presley and Graceland, his home in Memphis, . . . Early on, Elvis became known around the world by his first name alone. . .
Graceland? Learn fun Graceland facts, and get the official answers to common questions about the home of Elvis Presley! . . . How did Graceland get its name?

Truncated Summary
and residents alike to enjoy art galleries, specialty retail. . .
all its own. By day, shopping, gallery-hopping and dining. . .
Elvis Presley and Graceland, his home in Memphis, . . . Early
Graceland facts, and get the official answers to common

2.1 Experimental Design
We first describe the task users undertook, followed by the queries, search engine results summaries, post-task and exit questionnaires, and use of text as a baseline for audio. The crowdsourcing setup for presenting results lists to users and collecting judgements is also described.1
Users were presented with a task which consisted of three queries, corresponding lists of result summaries, a post-task questionnaire, and an exit questionnaire. Users were asked to read three query descriptions and read/listen to the summaries, before stating their preferred result description in the questionnaires. One set of summaries was the full length, one set was truncated.
2.1.1 Queries
The task was designed to reflect common search tasks on the Web: query topics from the Text REtrieval Conference (TREC) 2013 Web Track were used [5]. The queries in the track were manually selected from logs of a commercial search engine [5]. Since this was a preliminary study, a subset of twenty queries from the TREC 2013 Web Track dataset were used. An assessment of the queries indicated two categories, single-facet queries (queries with a clear intent) and faceted queries (typically broader in intent and represented with subtopics). It was decided to investigate whether these categories impacted on result summary preference.
The present study included seven single-facet and thirteen faceted queries. Table 1 shows an example for each type of query. Only informational subtopics were selected for the present study, since they have a primary interpretation which is reflected in the description field.
2.1.2 Search Engine Results Summaries
Each query was sent to the Google search engine and Googlegenerated text summaries were extracted for the top five search results.2 The summaries were converted into a spoken synthetic voice (audio). Instructions were added to each summary set: These are summaries of the top results. Select the summary that leads to the information you are looking for. A list-rank number was added to the front of each summary to allow easy identification of the users' selection. Table 1 shows a sample summary before and after the conversion for the task.
Truncated versions of the original Google-generated summaries were created manually. Here, a contiguous subset of nine words was selected from each full summary. Nine was found to be a little less than half the length of a standard Google-generated summary.
1All experiments were performed under Ethics Application BSEH 10-14 at RMIT University. 2Only the top five were presented to keep the audio task manageable.

For this initial work, manual summaries were created to avoid bias introduced by poor automatic truncation, which may negatively impact user perception. Human judgement was assumed to be the best way to preserve the meaning of the summary.
The presentation of the twenty search queries was randomized with the use of a Latin square design. Each user saw three queries per task. The order in which the users were presented with the result description (original summary vs. truncated summary) was rotated. These steps were implemented to avoid learning effects triggered by usage order and by users becoming accustomed to a synthetic voice [1, 11].
A problem reported with crowdsourcing is that users try to receive payment without completing the task properly [3]. Therefore, every task was populated with a Gold Question to help with data integrity and to detect if the participant was paying attention to the task [2]. The Gold Questions in this study used queries with clear pre-determined answers. Users were presented with three query descriptions and corresponding summaries. However, one of these summaries was populated with unrelated summary results. A user that was not able to identify that the summaries were not related to the query had their judgements discarded.
Though crowdsourcing provides a significant and easily accessible sample, we discovered that despite the precaution of releasing the tasks at different times and on different days, the results showed that many users completed every task. As a result, a modification was made and instead of running a new task for every set of queries, the queries were grouped into batches of ten and restrictions placed on how many batches users could judge.
2.1.3 Post-task and Exit Questionnaires
Post-task questionnaires are frequently implemented to assess the system­task interaction and gather user feedback on their experiences with using a particular system to complete a particular task [11]. Since no validated questionnaire has been published for studying user reaction to audio summaries, we used questionnaires adapted from previous studies.
Users completed the post-task questionnaire three times for each of the queries given. The post-task questionnaire (see Table 2) consisted of five questions on a five-point Likert scale (1­5); one question on query judgement with multiple choice answers (6); one question on how the participant listened to the audio with tick boxes (7); and a text box for further comments (8).
In addition, [11] suggests conducting a questionnaire at the end of the completed task to capture comparisons for within-subjects studies. Thus users were also presented with an exit questionnaire. Using a dynamic panel, the exit questionnaire was available only to users who were successful in answering a Gold Question. The exit questionnaire was used to measure users' preferences for information exploration using different result description configurations.

992

Table 2: Post-task questionnaire questions.
Post-task Questionnaire
1. The search results I heard are informative. 2. The search results give me a good overview of the available options. 3. The search results give me enough information to select the most relevant result. 4. The search results are presented in a way that is easy to understand. 5. I am confident I can recall the search results that I heard. 6. Which search result would you select to hear further information for? 7. Which statement describes how you listened to the audio? 8. Further comments.

Table 3: Exit questionnaire results for preferences in the search engine result summaries. p < .01.

Exit Question
Recommend to a friend Easier to find relevant result Gave better result More efficient to use

Text Summary

Full

Truncated

572 (57%) 434 (43%)

548 (54%) 458 (46%)

576 (57%) 430 (43%) 529 (53%) 477 (47%)

Audio Summary

Full

Truncated

529 (51%) 512 (49%)

514 (49%) 527 (51%)

539 (52%) 502 (48%) 499 (48%) 542 (52%)

The exit questionnaire was analysed with the help of responses from the post-task questionnaire.
2.1.4 Using Text as Baseline for Audio
Tasks were paired, whereby one task's summaries were audio and the other's were text. The text output was used to create a baseline measure of the system, facilitating analysis of the difference in preference between audio and text [11], enabling us to compare audio against the text baseline.
2.2 Users
CrowdFlower allows contributors (users who submit tasks to CrowdFlower) to place constraints on the users assigned to a task. The following constraints were put in place for the present study:
· Only users with an IP address from Australia, Ireland, New Zealand, the UK, and the USA were allowed to participate in order to maximise the likelihood that users were native English speakers or or had a high level of English.
· Users were able to participate only once in a particular task to maximise the worker pool.
· Users who took less than sixty seconds to complete the whole task were discarded on the basis that it would take users more than sixty seconds to listen to/read the summaries.
Although users were not permitted to participate more than once in a task which had the same set of queries, they were allowed to participate in tasks with different queries. A minimum of 36 users were recruited for each given task [13].
When a participant did not answer the Gold Question successfully, that participant's submission was discarded. These users were also not allowed to participate in later tasks. It was found that 11.8% of all users did not answer the Gold Question successfully, their submissions were discarded.
3. RESULTS
The exit questionnaire was analyzed using the 2 goodness-of-fit test to compare the distribution of scores across two levels. Results are shown in Table 3.
The 2 goodness-of-fit test [11] was used to assess whether changing the result summary had an effect on user preference.
Users were asked (in the post-task questionnaire) which summary made the users want to know more about the underlying document.3 These judgements were analysed with the two-sample Kolmogorov-Smirnov test (KS test) to determine whether two given samples follow the same distribution [15]. The tasks compared the result `click' distributions where the length of the summary
3This is equivalent to asking which result they would click in a traditional search engine result page (SERP).

was manipulated. The tasks were conducted in pairs: audio and text [11].
The KS test showed that for truncated summaries, only two out of twenty queries were there different distributions for audio-based versus text-based summaries. For full-length summaries and three out of twenty summaries resulted in different distributions for audio versus text. In general, the same distribution was found for query judgements between the text baseline and audio indicating that users made similar query judgements regardless of the presentation style being audio or text.
3.1 Preferred Length of Text Summaries
Table 3 shows that users tend to prefer full text summaries rather than their truncated counterpart. For instance, 57% of users would recommend full text summaries to a friend and 57% indicated that full summaries gave better results. The 2 goodness-of-fit tests were statistically significant (p < .01) for three exit questions in relation to the use of the original summary for presenting text results, indicating that this information exploration style was preferred.
3.2 Preferred Length of Audio Summaries
Results in audio summaries do not indicate a clear preference between full and truncated (preferences differ at most by only 2%). The 2 goodness-of-fit tests were not statistically significant (p > .05) for any of the exit questions about presenting audio results. No statistically significant difference were found for faceted queries. However, for single-facet queries using audio, all exit questions where statistically significant (p < .05) with a user preference for truncated summaries.
The KS test revealed that only one out of seven single-facet queries judgements was statistically significant (p < .05) when comparing truncated audio to the truncated text baseline. The KS test was not statistically significant for any faceted queries in audio, with one exception.
Users reported that overall it was easier to recall truncated audio summaries (54.4%) than full audio summaries (49.9%). Moreover, fewer users reported that they had to listen to the audio more than once (16.8%) for truncated summaries than for full-length audio summaries (23.7%). Only three users reported that they stopped the audio for truncated summaries, possibly indicating that both the information presented and the length of the information were short enough to avoid cognitive overload.
4. DISCUSSION
The present study investigated whether summaries of shorter length would be preferred for audio presentation as they could avoid overloading users' memory [20]. The exit questionnaire responses demonstrated that, for text summaries, full-length were preferred. However, for audio, no significant length preference was found.

993

Users reported that truncated summaries for single-facet queries were preferred. Thus for simpler, less ambiguous queries, shorter audio summaries were both effective and preferred. However, for faceted queries, users may have benefited from a more informative audio response even at the cost of listening time.
The single-facet query judgement distribution for both audio and text followed the distribution reported in past work [8] where query results ranked first and second receive most user attention. However, this expected distribution was not reflected in the faceted query judgements; rather, summaries ranked first and last obtained the most attention. This is also of interest: the serial nature of audio seems to lead to a bias towards most-recently-heard results, a behavior not found in visual presentation.
Users left comments in questionnaires. For summaries of faceted queries, they indicated that the summaries were missing key information. This suggests that the way of presenting summaries may differ depending on query intent: short audio summaries may be appropriate for clear intent queries (single-facet), whereas broader intent queries (faceted) may need more complex techniques (e.g., interactive/conversational approaches).
5. CONCLUSIONS
The present paper describes an initial investigation into result summaries for audio-based search. This study aimed to answer the following research questions:
· What is the impact of search result summary length in a spoken retrieval scenario?
· Do users prefer a full or truncated summary?
Differences were observed when result summary lengths were presented in the spoken retrieval scenario. In general, there was no preference for fuller descriptive summaries or for truncated summaries. However, results revealed that different kinds of queries (single-facet vs. faceted) benefited from an optimised summary depending on the type of query.
Extensions of the research include testing based on a larger number of queries and using automated techniques for truncating summaries for audio presentation. (The current method used manual truncation to avoid poor truncation from confounding the results.) More significantly, the results suggest a need for developing more sophisticated approaches to handling result-presentation over audio for faceted queries.
Acknowledgments
This research was partially supported by Australian Research Council Project LP130100563 and Real Thing Entertainment Pty Ltd. The authors wish to thank Bruce Croft who provided valuable feedback.
6. REFERENCES
[1] W. Albert, T. Tullis, and D. Tedesco. Beyond the Usability Lab: Conducting Large-Scale Online User Experience Studies. Morgan Kaufmann, 2009.
[2] S. Buchholz, J. Latorre, and K. Yanagisawa. Crowdsourced assessment of speech synthesis. Crowdsourcing for Speech Processing, pages 173­216, 2013.

[3] C. Callison-Burch and M. Dredze. Creating speech and language data with amazon's mechanical turk. In Proc. of NAACL HLT CSLDAMT'10 Workshop, 2010.
[4] E. Chang, F. Seide, H. M. Meng, C. Zhuoran, S. Yu, and L. Yuk-Chi. A system for spoken query information retrieval on mobile devices. Transactions on Speech and Audio Processing, 10(8):531­541, 2002.
[5] K. Collins-Thompson, P. Bennett, F. Diaz, C. L. Clarke, and E. M. Voorhees. TREC 2013 Web Track Overview. In TREC'13, 2014.
[6] V. Demberg and A. Sayeed. Linguistic cognitive load: implications for automotive uis. In Proc. of AutomotiveUI 2011, 2011.
[7] V. Demberg, A. Winterboer, and J. D. Moore. A strategy for information presentation in spoken dialog systems. Computational Linguistics, 37(3):489­539, 2011.
[8] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In Proc. of SIGIR '05, 2005.
[9] G. Jones. An introduction to crowdsourcing for language and multimedia technology research. Information Retrieval Meets Information Visualization, 7757:132­154, 2013.
[10] F. Jurcícek, S. Keizer, M. Gasic, F. Mairesse, B. Thomson, K. Yu, and S. Young. Real user evaluation of spoken dialogue systems using amazon mechanical turk. In Proc. of INTERSPEECH'11, 2011.
[11] D. Kelly. Methods for evaluating interactive information retrieval systems with users. Foundations and Trends in Information Retrieval, 3(1­2):1­224, 2009.
[12] S. R. Klemmer, A. K. Sinha, J. Chen, J. A. Landay, N. Aboobaker, and A. Wang. Suede: a wizard of oz prototyping tool for speech user interfaces. In Proc. of UIST'00, 2000.
[13] B. P. Knijnenburg, M. C. Willemsen, and A. Kobsa. A pragmatic procedure to support the user-centric evaluation of recommender systems. In Proc. of RecSys'11, 2011.
[14] J. Lai and N. Yankelovich. Speech Interface Design. Elsevier, 2006.
[15] F. J. Massey. The Kolmogorov-Smirnov test for goodness of fit. Journal of the American Statistical Assoc., 46(253):68­78, 1951.
[16] L. J. Najjar, J. J. Ockerman, and J. C. Thompson. User interface design guidelines for speech recognition applications. In Proc. of VRAIS '98, 1998.
[17] N. G. Sahib, D. Al Thani, A. Tombros, and T. Stockman. Accessible information seeking. In Proc. of Digital Futures'12, 2012.
[18] M. Turunen, J. Hakulinen, N. Rajput, and A. A. Nanavati. Evaluation of mobile and pervasive speech applications. In Speech in Mobile and Pervasive Environments, pages 219­ 262. 2012.
[19] S. Varges, F. Weng, and H. Pon-Barry. Interactive question answering and constraint relaxation in spoken dialogue systems. In Proc. of ACL'06, 2006.
[20] N. Yankelovich, G.-A. Levow, and M. Marx. Designing speechacts: Issues in speech user interfaces. In Proc. of the SIGCHI'95, 1995.

994

